{
    "title": "Few-shot Adaptation of Multi-modal Foundation Models: A Survey",
    "url": "https://openalex.org/W4388103918",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2101170733",
            "name": "Fan Liu",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2114021793",
            "name": "Tianshu Zhang",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2120253759",
            "name": "Wenwen Dai",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2134739613",
            "name": "Wenwen Cai",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2118657910",
            "name": "Xiaocong Zhou",
            "affiliations": [
                "Hohai University"
            ]
        },
        {
            "id": "https://openalex.org/A2148350493",
            "name": "Delong Chen",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4381304672",
        "https://openalex.org/W4386072228",
        "https://openalex.org/W4283378900",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2047643928",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4293824280",
        "https://openalex.org/W4294808066",
        "https://openalex.org/W2166049352",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2964194231",
        "https://openalex.org/W3037492894",
        "https://openalex.org/W4224545477",
        "https://openalex.org/W4312376733",
        "https://openalex.org/W4309396952",
        "https://openalex.org/W4285595687",
        "https://openalex.org/W4319319893",
        "https://openalex.org/W4303648967",
        "https://openalex.org/W2138011018",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4324321325",
        "https://openalex.org/W4386076681",
        "https://openalex.org/W4327810635",
        "https://openalex.org/W4320855021",
        "https://openalex.org/W4229453513",
        "https://openalex.org/W4321021726",
        "https://openalex.org/W4312277011",
        "https://openalex.org/W2533598788",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W1977295328",
        "https://openalex.org/W4310513152",
        "https://openalex.org/W4297677947",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4300979858",
        "https://openalex.org/W4309864938",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W4361194507",
        "https://openalex.org/W4310513153",
        "https://openalex.org/W4297676396",
        "https://openalex.org/W4292945941",
        "https://openalex.org/W3198675127",
        "https://openalex.org/W4312091889",
        "https://openalex.org/W2017814585",
        "https://openalex.org/W3173233571",
        "https://openalex.org/W4300989019",
        "https://openalex.org/W4308241978",
        "https://openalex.org/W4309801531",
        "https://openalex.org/W4386065763",
        "https://openalex.org/W4306313147",
        "https://openalex.org/W4317669196",
        "https://openalex.org/W4362598512",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W4312242596",
        "https://openalex.org/W4312310776",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W4281987380",
        "https://openalex.org/W4306820534",
        "https://openalex.org/W1846799578",
        "https://openalex.org/W2953937638",
        "https://openalex.org/W3126337491",
        "https://openalex.org/W2947707615",
        "https://openalex.org/W4224232320",
        "https://openalex.org/W4297795751",
        "https://openalex.org/W4286906902",
        "https://openalex.org/W4320813149",
        "https://openalex.org/W4304699759",
        "https://openalex.org/W1628114069",
        "https://openalex.org/W4225307291",
        "https://openalex.org/W3213647938",
        "https://openalex.org/W24089286",
        "https://openalex.org/W3158631574",
        "https://openalex.org/W3133825286",
        "https://openalex.org/W3201114717",
        "https://openalex.org/W2998385486",
        "https://openalex.org/W4285429066",
        "https://openalex.org/W4226027139",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2994749257",
        "https://openalex.org/W3173909648",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4281838252",
        "https://openalex.org/W2148440006",
        "https://openalex.org/W3177096435",
        "https://openalex.org/W4224246420",
        "https://openalex.org/W4382463911",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3212456749",
        "https://openalex.org/W4302305810",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4386790226",
        "https://openalex.org/W4226255121",
        "https://openalex.org/W4286897344",
        "https://openalex.org/W4221145109",
        "https://openalex.org/W4296151208",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W4382458283",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3170863103"
    ],
    "abstract": "<title>Abstract</title> Multi-modal models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of foundational visual models. These multi-modal models with robust and aligned semantic representations from billions of internet image-text pairs and can be applied to various downstream zero-shot tasks. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for multi-modal foundation models, gradually deriving three technical approaches: 1) prompt-based fine-tuning adaptation methods, 2) adapter-based fine-tuning adaptation methods, and 3) adaptation methods based on external knowledge. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this paper, we provide an extensive survey and analysis of the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) domain distribution adaptation, 2) model selection adaptation, and 3) knowledge utilization adaptation.",
    "full_text": "Few-shot Adaptation of Multi-modal Foundation\nModels: A Survey\nFan Liu \nHohai University\nTianshu Zhang \nHohai University\nWenwen Dai \nHohai University\nWenwen Cai \nHohai University\nXiaocong Zhou \nHohai University\nDelong Chen \nHong Kong University of Science and Technology\nResearch Article\nKeywords: Multi-modal foundation models, Vision-language pre-training, Few-shot adaptation, Parameter\ne\u0000cient \u0000ne-tuning\nPosted Date: October 30th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3497100/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Arti\u0000cial Intelligence Review on August\n27th, 2024. See the published version at https://doi.org/10.1007/s10462-024-10915-y.\nFew-shot Adaptation of Multi-modal Foundation\nModels: A Survey\nFan Liu · Tianshu Zhang · Wenwen Dai ·\nWenwen Cai · Xiaocong Zhou · Delong Chen*\nReceived: date / Accepted: date\nAbstract Multi-modal models, such as CLIP, are replacing traditional su pervised\npre-training models (e.g., ImageNet-based pre-training) as th e new generation of\nfoundational visual models. These multi-modal models with ro bust and aligned se-\nmantic representations from billions of internet image-text pai rs and can be applied\nto various downstream zero-shot tasks. However, in some ﬁne-grai ned domains like\nmedical imaging and remote sensing, the performance of multi-mo dal foundation\nmodels often leaves much to be desired. Consequently, many rese archers have be-\ngun to explore few-shot adaptation methods for multi-modal foun dation models,\ngradually deriving three technical approaches: 1) prompt-based ﬁne-tuning adapta-\ntion methods, 2) adapter-based ﬁne-tuning adaptation metho ds, and 3) adaptation\nmethods based on external knowledge. Nevertheless, this rapid ly developing ﬁeld\nhas produced numerous results without a comprehensive survey to s ystematically\norganize the research progress. Therefore, in this paper, we provide an extensive\nsurvey and analysis of the research advancements in few-shot ada ptation methods\nfor multi-modal models, summarizing commonly used datasets a nd experimental\nsetups, and comparing the results of diﬀerent methods. In addit ion, due to the lack\nof reliable theoretical support for existing methods, we derive t he few-shot adapta-\ntion generalization error bound for multi-modal models. The the orem reveals that\nthe generalization error of multi-modal foundation models is co nstrained by three\nfactors: domain gap, model capacity, and sample size. Based on this, we propose\nthree possible solutions from the following aspects: 1) domain d istribution adap-\ntation, 2) model selection adaptation, and 3) knowledge uti lization adaptation.\nKeywords Multi-modal foundation models · Vision-language pre-training ·\nFew-shot adaptation · Parameter eﬃcient ﬁne-tuning\nF. Liu, T. Zhang, W. Dai, W. Cai, X. Zhou\nCollege of Computer and Information, Hohai University, Nan jing, China\nD. Chen\nDepartment of Electronic & Computer Engineering, Hong Kong University of Science and\nTechnology, Hong Kong, China\nE-mail: delong.chen@connect.ust.hk\n2 Fan Liu † et al.\n1 Introduction1\nArtiﬁcial intelligence is increasingly being applied to a wid e range of key indus-2\ntries, including voice recognition, image recognition, auton omous driving, intel-3\nligent manufacturing, medical diagnosis, ﬁnancial risk contro l and so on. In the4\nprocess of empowering various ﬁelds with artiﬁcial intelligenc e technology, there5\nare often challenges related to fragmented and diversiﬁed demands . In the past,6\nmodels often had small parameter sizes and limited generalizat ion capabilities.7\nAnd one model could only cope with a single scenario, resulting in high costs and8\npoor generalization performance. Recently, an increasing numbe r of researchers9\nhave started focusing on pre-trained foundation models with great er generaliza-10\ntion.11\nSince 2018, the training data and parameter sizes of foundation models such12\nas BERT [14], Pangu [93], PaLM [10], GPT-4 [57], etc. have grow n exponentially,13\nresulting in signiﬁcant performance improvements in various natu ral language un-14\nderstanding tasks. Meanwhile, the development of foundation models is gradually15\nevolving from single modalities such as text, speech, vision, etc. to multi-modal16\nfusion. More and more research organizations have turned their atte ntion to multi-17\nmodal pre-trained foundation models, such as ViLBERT [49], CLI P [62], DeCLIP18\n[43], FILIP [90], PyramidCLIP [20], OFA [6], BEiT-3 [81], ERN IE-ViL [68] and19\nData2vec [1].20\nIn early 2021, OpenAI released CLIP, a large-scale multi-modal model for align-21\ning images and texts, which is pre-trained using billions of in ternet data to obtain22\nrich visual language knowledge through contrastive learning. W hile the pre-trained23\nCLIP model can achieve zero-shot predictions by employing tex t features as clas-24\nsiﬁcation weights during the inference stage, this approach typ ically excels only in25\ngeneral domains like ImageNet and tends to underperform when deal ing with data26\nfrom certain ﬁne-grained domains. The reason behind this is that s uch models pri-27\nmarily utilize data from the general domain during their pre-traini ng phase, and28\nwhen confronted with speciﬁc downstream tasks, the data distrib ution often di-29\nverges from pre-training data. Hence, it becomes necessary to ﬁne- tune the model30\nusing the speciﬁc data of downstream tasks. To improve the gene ralization perfor-31\nmance of the model through ﬁne-tuning, researchers ﬁrst proposed a prompt-based32\nﬁne-tuning adaptation method (e.g., CoOp [101]), which trea ts the ﬁxed text in-33\nputs of the CLIP text side as learnable vectors and then ﬁne-tune s them with a34\nsmall number of samples to adapt to the downstream tasks. Anot her method com-35\nmonly employed to enhance the few-shot adaptation capabilit y is adapter-based36\nﬁne-tuning, like CLIP-Adapter [19]. This method involves a dding simple adapter37\nstructures within the pre-trained model and then ﬁne-tuning the adapter parame-38\nters using a small amount of sample data, enabling the foundati on model to adapt39\nto downstream tasks. In addition, methods that introduce found ation language40\nmodels or external knowledge such as knowledge graphs (e.g., C uPL [61]) can help41\nthe model to handle unseen samples better, enhance its semant ic comprehension42\nand robustness, and thus improve its performance in few-shot adapt ation tasks.43\nThe above-mentioned three kinds of methods have been widely us ed in various44\ndownstream adaptation tasks, but there is a lack of a comprehens ive survey that45\nsystematically sorts out the methods. Therefore, we elaborate an d compare these46\nmethods in detail and explore their future directions to further im prove the per-47\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 3\nformance and generalization ability of pre-trained models. The m ain contributions48\nof this paper are as follows:49\n– We comprehensively review and sort out multi-modal few-shot ada ptation50\nmethods, and classify existing methods into prompt-based ﬁne -tuning adap-51\ntation methods, adapter-based ﬁne-tuning adaptation metho ds, adaptation52\nmethods based on external knowledge, and other methods. With in the prompt-53\nbased ﬁne-tuning adaptation methods, we further subdivide th em into text54\nprompt ﬁne-tuning, visual prompt ﬁne-tuning, multi-modal pro mpt, and multi-55\ntask prompt methods. Regarding adapter-based ﬁne-tuning adap tation meth-56\nods, we categorize them into single-modal adapter ﬁne-tunin g and multi-modal57\nadapter ﬁne-tuning. As for methods employing external knowle dge, we distin-58\nguish between pre-training methods with external knowledge an d downstream59\nadaptation methods leveraging external knowledge.60\n– We review 11 commonly used datasets for evaluating the downstre am general-61\nization performance of multi-modal foundation models. We provid e a detailed62\ndescription of four experimental setups for verifying the adaptat ion perfor-63\nmance of multi-modal foundation models under few-shot condit ions. The ex-64\nperimental results for the four diﬀerent setups are presented, and a comparative65\nanalysis of these results is performed. We highlight the reasons w hy diﬀerent66\ntypes of methods can eﬀectively enhance the generalization p erformance of67\nmulti-modal foundation models.68\n– We discuss the common shortcomings of few-shot adaptation met hods for69\nexisting multi-modal foundation models and analyze the doma in adaptation70\nproblem. Starting from the error bound in cross-domain generalizati on from71\nstatistical machine learning theory, we derive the error bound for few-shot72\nadaptation with multi-modal foundation models, which reveal s that the main73\nchallenges faced by existing methods are ineﬀective adaptati on of upstream74\nand downstream domain distributions, lack of adaptability in model selection75\nand insuﬃcient utilization of data and knowledge.76\n2 Pre-training of Multi-modal Foundation Models77\nIn recent years, large-scale pre-training models have received ex tensive attention78\nfrom academia and industry. Initially, the related works of founda tion model pre-79\ntraining mainly focus on the ﬁeld of natural language processing , in which the80\nself-supervised language models such as BERT [14] and GPT [63] have shown bet-81\nter natural language understanding and generation capabiliti es than traditional82\nmethods. In the ﬁeld of computer vision, the paradigm has also shifted from83\nsupervised pre-training to self-supervised pre-training. The perfo rmances of self-84\nsupervised pre-trained visual models have signiﬁcantly improv ed, evolving from85\ninitial models based on data augmentation like SimCLR [9] an d MoCo [23] to86\nmore recent approaches based on random masking methods such as MA E [22] and87\nBEiT [3]. However, pre-trained language models are unable to rece ive visual in-88\nputs, resulting in an inability to extend their advantage in l anguage understanding89\nto multi-modal downstream tasks such as visual question answ ering (VQA). On90\nthe other hand, the supervised signals used for visual pre-train ing are often limited91\nto data augmentation and stochastic masks, which prevents th em from learning92\n4 Fan Liu † et al.\nricher semantic representations in the open world. As a result, we have witnessed93\na recent surge in the development of large-scale pre-trained mult i-modal models94\nthat combine visual and language modalities, as illustrated in Table 1.95\nTable 1 Multi-modal pre-trained foundation models\nDate Institution Publication Method\n2021.02 OpenAI ICML 2021 CLIP [62]\n2021.06 Google ICML 2021 ALIGN [30]\n2021.07 Renmin University of China ArXiv BriVL [29]\n2021.09 Kuaishou Technology ArXiv EﬃﬁcientCLIP [79]\n2021.09 SenseTime Research ICLR 2022 DeCLIP [43]\n2021.11 HUAWEI ICLR 2022 FILIP [90]\n2021.12 Facebook ArXiv BoW [76]\n2021.12 Facebook ECCV 2022 SLIP [55]\n2022.02 Salesforce ICML 2022 BLIP [41]\n2022.02 JKU NeurIPS 2022 CLOOB [18]\n2022.06 MEGVII Arxiv ProtoCLIP [7]\n2022.09 IDEA Arxiv Taiyi-CLIP [80]\n2022.11 DAMO Academy ArXiv ChineseCLIP [87]\n2023.03 BAAI ArXiv EVA-CLIP [75]\nA notable characteristic of the above multi-modal pre-trained fo undation mod-96\nels lies in the ability to eﬃciently learn visual concepts from l arge-scale natural lan-97\nguage supervision and embed image and text features into a shared semantic space,98\nthus obtaining zero-shot prediction capability. However, whe n the downstream99\ntask’s data belongs to some speciﬁc domains, such as remote se nsing, healthcare,100\ne-commerce, etc., which diﬀer greatly from the pre-training data, the zero-shot101\nprediction accuracy of the multi-modal foundation models will drop sharply. At102\nthis point, it is necessary to ﬁne-tune the model with the help of the downstream103\ntask’s data, for example, using linear probing or global ﬁne-t uning methods. How-104\never, such methods often require a large number of samples for eﬀec tive training,105\nand the number of samples available in the actual downstream t ask is often lim-106\nited by the tagging cost. To address this problem, there have bee n some initial107\nexplorations in the academic community that attempt to ﬁne-t une multi-modal108\nfoundation models using small amounts of data so that they can be eﬃciently gen-109\neralized to speciﬁc downstream applications. For example, th ere have been some110\nworks [44,19] to ﬁne-tune CLIP, such as using linear classiﬁe rs, adapter layers,111\netc. The work on ﬁne-tuning CLIP can achieve very good results on few-shot im-112\nage recognition tasks, even surpassing some algorithms design ed speciﬁcally for113\nfew-shot tasks.114\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 5\n3 Few-shot Adaptation Methods for Multi-modal Foundation M odels115\nTo eﬀectively enhance the model’s generalization performance in speciﬁc domains,116\nit is necessary to ﬁne-tune multi-modal foundation models usi ng limited sam-117\nples, enabling them to have broader applications. These meth ods can be deﬁned118\nas few-shot adaptation methods for multi-modal foundation mod els. This chap-119\nter will be divided into four sections to provide a detailed ove rview of existing120\nmethods for multi-modal foundation models, namely: prompt-ba sed ﬁne-tuning121\nadaptation methods, adapter-based ﬁne-tuning adaptation m ethods, adaptation122\nmethods based on external knowledge, and other methods.123\n3.1 Prompt-based Fine-tuning Adaptation Methods124\n3.1.1 Textual Prompt-based Fine-tuning Adaptation125\nIn the ﬁeld of natural language processing, prompt-based ﬁne-t uning adaptation126\n[38,71,42,65,47] is a classic approach to addressing the issu e of few-shot general-127\nization in large language models. It involves using a ﬁxed part of the text input128\nas a learnable vector and ﬁne-tuning its parameters using downs tream task data,129\nenabling the model to adapt to speciﬁc downstream tasks. The a dvantage of this130\nmethod lies in its ability to avoid the manual design of textu al prompts, eﬀec-131\ntively mitigating the risk of overﬁtting by ﬁne-tuning only a s peciﬁc portion of the132\nmodel input. Inspired by this, some researchers have also begun t o design prompt-133\nbased ﬁne-tuning adaptation methods for multi-modal foundat ion models. CoOp134\n[101] for the ﬁrst time incorporates the idea of prompt learning int o downstream135\ntask adaptation for multi-modal pre-trained foundation models . It uses learnable136\nword embeddings to automatically construct context prompts in stead of manually137\ndesigning prompt templates for each task. As illustrated in Fig ure 1, the individ-138\nual category label {object} is transformed into a comprehensive textual prompt139\n’[V ]1 , [V ]2 , . . . , [V ]m , {object}’. Here, [ V ]i represents the adaptable word vectors.140\nThe classiﬁcation loss is then computed to ﬁne-tune these wo rd vectors using data141\nfrom the downstream task, enabling the model to autonomously ac quire text in-142\nputs adapted to the downstream task.143\nSubsequently, Zhou et al. [100] introduced Conditional Cont extual Optimiza-144\ntion (CoCoOp), which constructs a meta-network to learn features from images.145\nThese features are then combined with prompt vectors to enhance Co Op’s gener-146\nalization performance on new category data. To leverage the zero-s hot ability of147\npre-trained models eﬀectively, Huang et al. [28] proposed Unsu pervised Prompt148\nLearning (UPL). It selects zero-shot prediction results with hi gh conﬁdence as149\npseudo-labels to supervise prompt vector learning. Similarly, Prompt-aligned Gra-150\ndient (ProGrad) [102] uses the zero-shot prediction results to co nstrain the direc-151\ntion of the model gradient update, thus avoiding the conﬂict b etween few-shot152\nmodels and the generalized knowledge, and mitigating the prob lem of overﬁtting.153\nHowever, due to the rich diversity of visual information, learning only one tex-154\ntual prompt makes it challenging to match complex visual data . To address this,155\nChen et al. [8] proposed Prompt Learning with Optimal Transport (P LOT). It is156\nused to learn multiple distinct textual prompts, where diﬀerent textual prompts157\nare regarded as descriptions of image locations, and the optimal transport theory158\n6 Fan Liu † et al.\nFig. 1 Schematic diagram of prompt-based ﬁne-tuning adaptation m ethods.\nis employed to match textual prompts with local image features. Lu et al. [50]159\nintroduced Prompt Distribution Learning (ProDA) to learn prompt dis tributions160\nand sample diﬀerent textual prompts from these distributions. In addition, to161\nmake full use of the correlation between multi-task data, Ding et al. [15] proposed162\nSoft Context Sharing for Prompt Tuning (SoftCPT), which designs a task-sharing163\nmeta-network that splices predeﬁned task names and learnable m eta-prompts as164\ninputs to ﬁne-tune the prompts with the help of multi-task dat a.165\n3.1.2 Visual Prompt-based Fine-tuning Adaptation166\nAll of the above methods only ﬁne-tune the textual side of CLI P, whereas CLIP,167\nas a multi-modal model, places equal importance on both visua l and textual sides.168\nFine-tuning only the textual prompts cannot improve the abili ty of the visual169\nencoder to extract features, and the extracted visual features are l ikely to mis-170\nmatch the target features of downstream tasks. Therefore, inspired b y the textual171\nprompt ﬁne-tuning adaptation, a series of visual prompt ﬁne-tu ning adaptation172\nmethods have emerged. Existing visual prompt ﬁne-tuning adap tation methods173\nmainly include token-level ﬁne-tuning adaptation and pixe l-level ﬁne-tuning adap-174\ntation. Visual Prompt Tuning (VPT) [31] introduces learnable v isual prompts in175\ntoken form. Class-Aware Visual Prompt Tuning (CAVPT) [86] further includes176\na cross-attention module on this basis to make visual prompts m ore focused on177\nthe objectives of downstream tasks. In contrast to token-based methods, Bahng178\net al. [2] suggested adding pixel-level visual prompts direct ly around the image179\nin a padding format to enhance visual prompts. Wu et al. [83] furthe r proposed180\nEnhanced Visual Prompting (EVP) by scaling and padding inste ad of padding181\ndirectly around the original image.182\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 7\n3.1.3 Multi-modal Prompt-based Fine-tuning Adaptation183\nIn addition to separately learning textual and visual prompts, it is also possi-184\nble to simultaneously learn multi-modal prompts to better ali gn textual and vi-185\nsual features. Textual and visual features have inherent diﬀeren ces, and in order186\nto strengthen the connection between them when learning multi -modal prompts,187\nMulti-modal Prompt Learning (MAPLE) [36] uses copula function s to transform188\ntextual prompts into visual prompts. Uniﬁed Prompt Tuning (UPT ) [92] on the189\nother hand, ﬁrst learns a universal prompt and then decomposes it into textual and190\nvisual prompts. On the other hand, Multi-task Visual Languag e Prompt Tuning191\n(MVLPT) [70] introduces the concept of multi-task learning, ﬁ ne-tuning textual192\nand visual prompts using cross-task knowledge.193\n3.2 Adapter-based Fine-tuning Adaptation Methods194\n3.2.1 Single-modal Adapter-based Fine-tuning Adaptation195\nIn the ﬁeld of Natural Language Processing (NLP), the concept o f Adapters was196\nﬁrst introduced by the Google team in 2019 for ﬁne-tuning large la nguage models197\n[27]. During training on downstream tasks, this method freezes the parameters of198\nthe original language model and only updates a small number of parameters added199\nas adapter modules. Due to its advantages such as parameter eﬃc iency, ﬂexibil-200\nity in design, and high robustness, this approach has received e xtensive research201\nattention in the NLP ﬁeld in recent years [16]. More recently, the adapter-based202\napproach has also been applied to Vision Transformers (ViTs) in th e computer203\nvision domain. Jie et al. [33] addressed the issue of the lack of inductive bias204\nof adapter structures in ViTs by introducing Convolutional Byp asses (Convpass).205\nAdditionally, they proposed Factor-Tuning (FacT, cited as [3 4]) to further improve206\nthe eﬃciency of parameter-eﬃcient transfer learning to meet stora ge constraints207\nin practical applications.208\n3.2.2 Multi-modal Adapter-based Fine-tuning Adaptation209\nThe above adapter-based methods are all applicable to single- modal foundation210\nmodels in natural language processing or computer vision. In re cent years, adapter-211\nbased methods have also been extended to multi-modal foundat ion models to en-212\nhance downstream generalization ability. Gao et al. [19] intro duced CLIP-Adapter,213\nwhich adds a fully connected layer adapter after freezing the bac kbone network to214\nlearn additional knowledge. It then merges this knowledge wit h zero-shot predic-215\ntion results based on residual connections, as illustrated in F igure 2.216\nBuilding upon these developments, Zhang et al. introduced Ti p-Adapter [96].217\nThis method constructs classiﬁers based on downstream few-shot training data218\nand combines their predictions with the original zero-shot cla ssiﬁers’ results in219\na linear weighted manner to enhance the model’s prediction pe rformance. And220\nSVL-Adapter [58] fuses a pre-trained self-supervised visual enc oder before the221\nadapter to extract more robust visual features. However, the above methods only222\nuse cross-modal contrastive loss and do not consider visually speciﬁc contrastive223\n8 Fan Liu † et al.\nFig. 2 Schematic diagram of adapter-based ﬁne-tuning adaptation methods.\nloss for few-shot datasets. To address this issue, Peng et al. [6 0] proposed Semantic-224\nguided Visual Adapting (SgVA-CLIP), which guides the parame ter update of the225\nvisual adapter through implicit knowledge distillation to e nsure the consistency of226\nthe image-text relationship. To enhance the cross-modal inte raction capabilities of227\nadapters, CALIP [21] leverages attention maps to fuse text and i mage features and228\ninserts two ﬁne-tunable linear layers before and after fusion. In a ddition, Cross-229\nModal Adapter (CMA) [32] and Multimodal Video Adapter (MV-A dapter) [94]230\nachieve cross-modal interaction by sharing adapter weights be tween two modali-231\nties. These methods consider both single-modal and multi-m odal scenarios but do232\nnot fully integrate the advantages of each modality. To address this, Lu et al. [48]233\nproposed UniAdapter to unify single-modal and multi-modal ad apters.234\n3.3 External Knowledge-based Adaptation Methods235\n3.3.1 External Knowledge-based Pre-training Methods236\nPre-trained foundation models have the ability to learn general re presentations237\nby mining relevant information from vast amounts of data on the int ernet. How-238\never, in such data-driven models, knowledge is often implicit a nd not explicitly239\nlinked to human understanding of the world or common sense know ledge. In re-240\ncent years, data and knowledge-driven pre-training methods hav e been emerging,241\nand researchers have started exploring the incorporation of more com prehensive242\nexternal knowledge, such as knowledge graphs, into foundation models. This inte-243\ngration aims to make these models more robust, reliable, and inte rpretable. ERNIE244\n[98] incorporates a knowledge encoder for entity knowledge ext raction and hetero-245\ngeneous information fusion. K-BERT [45] retrieves external knowl edge relevant to246\nthe model input and constructs sentence trees with rich context ual knowledge as247\nmodel input. In recent years, some eﬀorts have also started to inje ct knowledge248\ninto pre-training for multi-modal foundation models. For examp le, ERNIE-ViL249\n[91] integrates knowledge from scene graphs, KM-BART [85] model s general visual250\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 9\nknowledge by creating additional pre-training tasks, and K-LI TE [69] incorporates251\nvarious external knowledge sources, including WordNet and Wiki pedia deﬁnitions.252\n3.3.2 External Knowledge-based Downstream Adaptation Meth ods253\nFig. 3 Schematic diagram of external knowledge-based ﬁne-tuning adaptation methods.\nThe methods mentioned above introduce external knowledge duri ng the pre-254\ntraining phase. However, in downstream few-shot adaptation sce narios with lim-255\nited data samples, it is also necessary to enhance external kno wledge to ensure the256\nmodel’s performance. One of the most common approaches is to gene rate richer257\ntextual descriptions for each category by querying a large langua ge model. An258\nillustration of this method is shown in Figure 3. Customized Pro mpts via Lan-259\nguage models (CuPL) [61] is the ﬁrst method to integrate externa l knowledge260\ninto the downstream generalization process of multi-modal foun dation models.261\nCuPL achieves this by asking GPT-3 questions to generate mult iple descriptive262\nstatements for each category, enriching the semantics of categ ories and thereby im-263\nproving zero-shot classiﬁcation performance. However, the sente nces generated by264\nCuPL using GPT-3 may have issues with poor descriptiveness an d reliability. To265\naddress these issues, Menon et al. [54] further reﬁned the knowle dge enhancement266\nprocess based on GPT-3. They prompted GPT-3 to generate semanti c attribute de-267\nscriptions in the form of phrases, enhancing the model’s interpret ability. To strike268\na balance between interpretability and performance, Language Gu ided Bottlenecks269\n(LaBo) [88] uses GPT-3 to generate a large candidate feature desc riptor space, tak-270\ning into account both the discriminability of features with resp ect to other classes271\nand the coverage of the current class. It ﬁlters out the optimal su b-descriptor space272\nfor classiﬁcation decisions, thereby uncovering the model’s d ecision rationale. EL-273\nEVATER [39] also incorporates deﬁnitions from sources like GPT-3 , WordNet, and274\nWiktionary. Experimental results indicate that external knowl edge can enhance275\ndownstream generalization performance for multi-modal foundatio n models. How-276\never, diﬀerent sources of knowledge have diﬀerent emphases and p roperties. For277\ninstance, WordNet has relatively rich and accurate knowledge bu t lower coverage,278\nwhile GPT-3 has a broader knowledge coverage but may lack reliab ility. Addition-279\n10 Fan Liu † et al.\nally, unlike the methods mentioned above that use external kn owledge to enhance280\ntextual semantics, SuS-X [77] focuses on enhancing visual sa mples for multi-modal281\nmodels. It augments the few-shot training sets for downstream ta sks through im-282\nage retrieval from the LAION-5B dataset [67] or generates image sa mples based283\non Stable Diﬀusion [66], which aims to model the true distributi on of downstream284\ndata more accurately and reliably.285\n3.4 Other Methods286\nIn addition to the three categories of methods mentioned above , there are some287\napproaches to ﬁne-tuning multi-modal foundation models from th e perspectives of288\nweight parameter fusion, model reconstruction, and cross-atten tion. Speciﬁcally,289\nWise-FT [82] fuses the original and ﬁne-tuned model parameters b y linear inter-290\npolation, which enables the model to acquire speciﬁc knowled ge from downstream291\ndata while retaining as much generic knowledge as possible. Ma skCLIP [99] di-292\nrectly modiﬁes the structure of the CLIP image encoder by removin g the query293\nembedding layer and the key embedding layer. It replaces the va lue embedding294\nlayer and the last linear layer with a 1 × 1 convolutional layer, allowing the model295\nto extract denser image features. VT-Clip [97] introduces a visu al-guided attention296\nmechanism, which enhances the semantic correlation between t he textual features297\nand the image data of the downstream tasks, thus eﬀectively im proving the gen-298\neralization performance of the multi-modal foundation models.299\n4 Datasets and Comparison of Experimental Results300\nThere are 11 commonly used datasets for evaluating the downstrea m generalization301\nperformance of multi-modal foundation models, namely: 2 general target datasets302\n(ImageNet [13] and Caltech101 [17]), 5 ﬁne-grained classiﬁc ation datasets (Oxford-303\nPets [59], StanfordCars [37], Flowers102 [56], Food101 [4] and F GVCAircraft [52]),304\n1 scene recognition dataset (SUN397 [84]), 1 action recogniti on dataset (UCF101305\n[74]), 1 texture dataset (DTD [11]) and 1 satellite image datas et (EuroSAT [24]).306\nThese datasets cover a range of diﬀerent visual tasks and colle ctively form a more307\ncomprehensive benchmark for evaluating multi-modal foundati on model perfor-308\nmance in various scenarios.309\nTo evaluate the generalization performance of multi-modal found ation models310\nunder few-shot conditions, four experimental settings are comm only used, namely:311\nFew-shot Learning: Building upon the 11 datasets mentioned above, the312\ntraining and test sets are partitioned. For each class in the trai ning set, 1, 2, 4,313\n8, and 16 samples are extracted and used for training. Subsequent ly, the model’s314\nperformance is evaluated on the test set. The primary aim of this ex periment is315\nto assess the impact of limited samples on the generalization performance.316\nBase-to-new Generalization: To evaluate the eﬀectiveness of adaptation317\nmethods for multi-modal foundation models on previously unsee n classes, all the318\nclasses from the 11 datasets are evenly divided into two groups. O ne group is319\ncalled ‘base classes’ and the other group is called ‘new class es’. The multi-modal320\nfoundation model is trained only on the data from the base classes . Subsequently,321\nevaluations are performed separately on both the base class and ne w class data.322\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 11\nPerformance on the base classes reﬂects the discriminability of fe atures learned by323\nthe model, while performance on the new classes reﬂects the model ’s generalization324\nability. The harmonic mean of the results obtained on the base a nd new class data325\nis adopted as a balance between discriminability and generali zation ability.326\nDomain Generalization: To validate the generalization and domain shift327\ncapabilities of multi-modal foundation models’ adaptation methods when dealing328\nwith Out-of-Distribution (OOD) data, ImageNet is selected as th e source dataset,329\nand the other four datasets (ImageNetV2 [64], ImageNet-Sket ch [78], ImageNet-A330\n[26] and ImageNet-R [25]) are selected as the target datasets. The target datasets331\nhave the same category information as the source dataset but diﬀe rent data dis-332\ntributions. The model is trained solely on the source dataset an d subsequently333\nevaluated on the target datasets.334\nCross-Dataset Transfer: To validate the generalization performance on dif-335\nferent datasets, ImageNet is chosen as the source dataset, and t he remaining 10336\ndatasets are selected as target datasets. The model is trained o n ImageNet and337\nthen tested on the target datasets. The source dataset and the t arget datasets338\nhave almost no overlap in classes, which can test the model’s g eneralization ability339\non diﬀerent class datasets.340\nTable 2 Comparison of few-shot learning experimental results [95]\nMethod Image Encoder\nAverage\nImageNet [13]\nCaltech101 [17]\nPets [59]\nCars [37]\nFlowers102 [56]\nFood101 [4]\nAircraft [52]\nSUN397 [84]\nDTD [11]\nEuroSAT [24]\nUCF101 [74]\nBaseline [62] ViT-B/16 71.7 70.2 95.4 94.1 68.6 74.8 90.6 31. 1 72.2 56.4 60.6 73.5\nBaseline [62] ViT-L/14 73.7 76.2 92.8 93.5 78.8 78.3 93.8 37. 2 68.4 55.7 59.6 76.9\nCoOp [101] ViT-B/16 71.6 71.9 93.7 94.5 68.1 74.1 85.2 28.7 72 .5 54.2 68.7 67.5\nCoCoOp [100] ViT-B/16 75.8 73.1 95.8 96.4 72.0 81.7 91.0 27.7 78.3 64.8 71.2 77.6\nUPL [28] ResNet-50 68.4 61.1 91.4 89.5 71.0 76.6 77.9 21.7 66. 4 55.1 71.0 70.2\nProDA [50] ResNet-50 - 65.3 91.3 90.0 75.5 95.5 82.4 36.6 - 70. 1 84.3 -\nProGrad [102] ResNet-50 67.9 62.1 91.5 93.4 62.7 78.7 81.0 21 .9 70.3 57.8 59.0 68.5\nPLOT [8] ResNet-50 73.9 63.0 92.2 87.2 72.8 94.8 77.1 34.5 70. 0 65.6 82.2 77.3\nCAVPT [86] ViT-B/16 83.2 72.5 96.1 93.5 88.2 97.6 85.0 57.9 74 .3 72.6 92.1 85.3\nUPT [92] ViT-B/16 76.2 73.2 96.1 96.3 71.8 81.0 91.3 34.5 78.7 65.6 72.0 77.2\nTPT [72] ViT-B/16 64.8 69.0 94.2 87.8 66.9 69.0 84.7 24.8 65.5 47.8 42.4 60.8\nTip-Adapter [96] ViT-B/16 - 70.8 - - - - - - - - - -\nSgVA-CLIP [60] ViT-B/16 - 73.3 - - - - - - 76.4 - - -\nCALIP [21] ResNet-50 59.4 60.6 87.7 58.6 77.4 66.4 56.3 17.7 8 6.2 42.4 38.9 61.7\nCuPL [61] ViT-L/14 - 76.6 93.4 93.8 77.6 - 93.3 36.1 61.7 - - -\nSuS-X [77] ResNet-50 - 61.8 - - - - - - - - 45.6 50.6\nSubPT [51] ResNet-50 66.4 63.4 91.7 91.8 60.7 73.8 81.0 20.3 7 0.2 54.7 54.5 68.1\nVT-Clip [97] ResNet-50 - - - 93.1 - - - - - 65.7 - -\nWe collect the few-shot learning experimental results for select ed methods on341\nthe 11 datasets from various sources, as shown in Table 2. The back bone networks342\nprimarily used are CNN-based ResNet50, as well as Transformer-base d ViT-B and343\nViT-L. All methods are trained with only 16 samples per class an d then tested344\nfor image classiﬁcation accuracy on the test set. The ”Baselin e” refers to the345\nclassiﬁcation results of Linear-probe CLIP.346\n12 Fan Liu † et al.\nBased on Table 2, the following conclusions can be drawn:1)Th ree multi-modal347\nfoundation models’ adaptation methods for few-shot learning eﬀ ectively improve348\nthe adaptability of foundation models to downstream tasks und er few-shot condi-349\ntions. Methods such as CoOp based on prompts, SgVA-CLIP based on adapters,350\nand CuPL based on external knowledge show performance improvemen ts of 1.7%351\n(ViT-B/16), 3.1% (ViT-B/16), and 0.1% (ViT-L/14), respecti vely, on the Ima-352\ngeNet dataset. 2) For prompt-based ﬁne-tuning methods, some unsupervised train-353\ning methods yield results similar to supervised training metho ds. The accuracy of354\nthe unsupervised method UPL is only 0.4% higher than that of Co Op trained355\nwith 2 samples, while the accuracy of TPT (69.0%) is not signiﬁ cantly diﬀer-356\nent from CoOp trained with 16 samples (71.9%). This is because un supervised357\ntraining methods can leverage unlabeled data eﬀectively and ca n avoid overﬁtting358\ncompared to supervised training with only a small number of samp les.359\nTable 3 Comparison of base-to-new generalization experimental re sults\nMethod Image Encoder Average\nBase New H\nCLIP [62] ViT-B/16 69.34 74.22 71.70\nCoOp [101] ViT-B/16 82.63 67.99 74.60\nCoCoOp [100] ViT-B/16 80.47 71.69 75.83\nProDA [50] ViT-B/16 81.56 72.30 76.65\nProGrad [102] ViT-B/16 82.48 70.75 76.16\nMAPLE [36] ViT-B/16 82.28 75.14 78.55\nGRAM [40] ViT-B/16 78.74 74.93 76.79\nKgCoOp [89] ViT-B/16 80.73 73.60 77.00\nCPBPrompt [46] ViT-B/16 80.88 74.74 77.69\nUNIGRAM [40] ViT-B/16 80.34 75.92 78.07\nLASP-V [5] ViT-B/16 83.18 76.11 79.48\nAs shown in Table 3, we also collect the experimental results from base-to-360\nnew generalization for existing methods from various sources. From the table, we361\ncan conclude that: 1) Methods that perform well on the base class es often sacriﬁce362\ntheir generalization performance on the new classes. This may b e due to the model363\noverﬁtting to the base classes. Meanwhile, disrupting or forget ting the potential364\ngeneralization knowledge acquired during pre-training leads to a drop in perfor-365\nmance when the model generalizes to unseen classes. 2) LASP-V and CPL perform366\nwell in terms of base class discriminability and new class gene ralization. Across367\nthe 11 datasets, LASP-V outperforms CLIP by 13.76%, 1.89%, and 7 .78% for the368\nthree metrics of the base class, new class, and harmonic mean, res pectively. On the369\nImageNet dataset, CPL outperforms CLIP by 6.38%, 5.03%, and 5.6 7% respec-370\ntively. Notably, both of these methods exhibit signiﬁcantl y better performance on371\nunseen classes compared to CLIP, which has strong zero-shot cap abilities.372\nThe collected domain generalization experimental results for existing methods373\nfrom various sources are shown in Table 4, and we can ﬁnd that: 1) TPT can374\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 13\nTable 4 Comparison of domain generalization experimental results\nMethod Source Domain Target Domain\nImageNet [13] -V2 [64] -Sketch [78] -A [26] -R [25]\nzero shot CLIP [62] 66.73 60.83 46.15 47.77 73.96\nlinear probe CLIP [62] 65.85 56.26 34.77 35.68 58.43\nCoOp [101] 71.51 64.20 47.99 49.71 75.21\nCoCoOp [100] 71.02 64.07 48.75 50.63 76.18\nVPT-shallow [31] 68.98 62.10 47.68 47.19 76.10\nVPT-deep [31] 70.57 63.67 47.66 43.85 74.42\nMAPLE [36] 70.72 64.07 49.15 50.90 76.98\nUPT [92] 72.63 64.35 48.66 50.66 76.24\nKgCoOp [89] 71.20 64.10 48.97 50.69 76.70\nPBPrompt [46] 70.90 64.40 49.10 51.00 76.40\nUNIGRAM [40] 71.65 64.81 49.54 51.51 77.34\nTPT+CoOp [72] 73.61 66.83 49.29 57.95 77.27\nTPT+CoCoOp [72] 71.07 64.85 48.47 58.47 78.65\neﬀectively combine with CoOp and CoCoOp and achieve state-of- the-art (SOTA)375\nperformance. When using ViT-B/16 as the backbone, the combinat ion of TPT376\nand CoOp outperforms zero-shot CLIP by 6.88%, 6%, 3.14%, 10.18%, and 3.31%377\non the ImageNet, -V2, -Sketch, -A, and -R datasets, respectiv ely. 2) Compared to378\nboth textual-only prompt-based ﬁne-tuning adaptation meth ods (e.g., CoOp, Co-379\nCoOp) and visual-only prompt-based ﬁne-tuning adaptation m ethods (e.g., VPT),380\nmulti-modal prompt-based ﬁne-tuning adaptation methods li ke UPT and MAPLE381\nachieve more improvements. This suggests that ﬁne-tuning the two modalities si-382\nmultaneously is both suﬃcient and necessary for better performan ce.383\nTable 5 Comparison of cross-dataset transfer experimental result s\nMethod\nImage\nEncoder\nStructure\nSource\nDataset\nTarget\nDataset\nImageNet [13]\nCaltech101 [17]\nPets [59]\nCars [37]\nFlowers102 [56]\nFood101 [4]\nAircraft [52]\nSUN397 [84]\nDTD [11]\nEuroSAT [24]\nUCF101 [74]\nAverage\nCoOp [101] ViT-B/16 71.5 93.7 89.1 64.5 68.7 85.3 18.5 64.2 41 .9 46.4 66.6 63.9\nCoCoOp [100] ViT-B/16 71.0 94.4 90.1 65.3 71.9 86.1 22.9 64.4 45.7 45.4 68.2 65.7\nMAPLE [36] ViT-B/16 70.7 95.5 90.5 65.6 72.2 86.2 24.7 67.0 46 .5 48.1 68.7 66.3\nSubPT [51] ResNet-50 62.6 88.3 87.4 56.2 63.4 77.8 16.7 61.8 3 9.7 29.1 61.7 58.6\nVPT [31] ViT-L/14 70.6 95.8 92.9 76.1 95.0 86.2 41.0 71.6 69.8 91.5 82.8 79.4\nUPT [92] ViT-L/14 72.6 95.9 93.0 84.3 97.1 85.0 46.8 75.9 70.7 90.5 84.0 81.4\nTPT [72] ResNet-50 60.7 87.0 84.5 58.5 62.7 74.9 17.6 61.5 40. 8 28.3 60.8 57.7\nUNIGRAM [40] ViT-B/16 71.7 94.7 90.8 66.8 73.1 86.7 25.3 68.0 48.1 52.6 71.0 67.7\nCPBPrompt [46] ViT-B/16 70.9 94.9 90.8 65.3 72.4 86.4 24.6 67 .8 45.2 45.1 68.8 66.1\n14 Fan Liu † et al.\nThe collected results of cross-dataset adaptation experiment s for existing meth-384\nods from various sources are shown in Table 5. The purpose of this exp eriment is385\nto validate the generalization performance of adaptation metho ds across diﬀerent386\ndatasets. Using the ImageNet dataset with 1000 classes as th e source dataset, the387\nmethods are initially trained with 16 samples from each class of t he source dataset.388\nSubsequently, these methods are tested on 10 diﬀerent target da tasets.389\nFrom the data in the table, the following observations can be mad e: 1) For390\nSubPT and TPT with ResNet50 as the backbone, as well as other m ethods with391\nViT-based backbones, they achieve similar results on the sou rce dataset but ex-392\nhibit signiﬁcant variations in performance on diﬀerent target dat asets. For exam-393\nple, they achieve higher accuracy on datasets like Caltech10 1 and Pets, which have394\ncategories similar to ImageNet. However, their performances are m uch lower on395\ndatasets like Aircraft and DTD, which contain ﬁne-grained data rela ted to various396\naircraft and textures. These datasets have greater category diﬀere nces from Ima-397\ngeNet, and hence, the accuracies on these datasets are much low er than 50%, indi-398\ncating that transferring speciﬁc knowledge learned from ImageNet t o downstream399\ntasks with signiﬁcantly diﬀerent categories is challenging. 2) Whether it is prompt-400\nbased ﬁne-tuning methods or adapter-based ﬁne-tuning metho ds, ﬁne-tuning both401\nmodalities simultaneously tends to yield better results tha n ﬁne-tuning only one402\nmodality. For instance, the multi-modal prompt learning meth od MAPLE achieves403\nhigher accuracies on 10 target datasets compared to the textual -only prompt learn-404\ning method CoCoOp (ViT-B/16), and UPT achieves higher accura cies compared405\nto the visual-only prompt learning method VPT (ViT-L/14). Thi s suggests that406\nfor multi-modal foundation models like CLIP, ﬁne-tuning both textual and visual407\naspects is essential for improved generalization performance.408\n5 Analysis and Discussion409\nThe current researches on few-shot adaptation for multi-modal foun dation models410\nprimarily include prompt-based ﬁne-tuning adaptation method s, adapter-based411\nﬁne-tuning adaptation methods, and external knowledge-bas ed adaptation meth-412\nods. Based on the current research status on few-shot adaptation, we summarise413\nthe following issues and challenges:414\n1) Ineﬀective Adaptation of Upstream and Downstream Domain Dis-415\ntributions: Existing few-shot adaptation methods for multi-modal foundat ion416\nmodels mostly focus on the category information in downstream tas k data while417\nneglecting domain distribution information. Additionally, i n the domain adapta-418\ntion scenarios of multi-modal foundation models, the scarcity of samples in the419\ntarget domain makes modeling challenging, while the abundan ce of samples in the420\nsource domain results in high modeling costs. Furthermore, current domain adap-421\ntation methods are tailored for a single modality, ignoring cross -modal information422\ninteraction, which does not meet the requirements of adaptatio n for multi-modal423\nfoundation models.424\n2) Lack of Adaptability in Model Selection: There is a wide variety of425\nexisting adapter structures with diﬀerent ﬁne-tuning characte ristics, learning ca-426\npabilities, and model capacities. Diﬀerent downstream tasks o ften require diﬀerent427\noptimal adapters or combinations of multiple adapters, and th e feature extraction428\nprocesses vary signiﬁcantly across diﬀerent modalities. Current ly, adapter selection429\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 15\nis typically reliant on heuristic methods or exhaustive search -based approaches,430\nwhich are costly and challenging to guarantee performance.431\n3) Insuﬃcient Utilization of Data and Knowledge: Although existing432\ndata augmentation strategies can enrich the downstream trainin g set and reduce433\nthe risk of model overﬁtting to some extent. However, this empiri cally manually434\ndesigned augmentation approach is costly and does not ensure t hat the selected435\ndata augmentation can be eﬀectively adapted to the speciﬁc d ownstream task. Al-436\nthough new textual descriptions can be generated by introducin g external knowl-437\nedge, there is no guarantee that the textual knowledge describi ng the category438\nattributes can eﬀectively collaborate with the image data.439\nTo address the above-mentioned issues and challenges, we sum marize and reﬁne440\nthe existing theories related to cross-domain adaptation for mul ti-modal founda-441\ntion models, which makes the work of few-shot adaptation more sy stematic and442\nguides few-shot cross-domain adaptation.443\nTheorem 1 Deﬁne the expected error of the adapted ﬁne-tuned model in the tar-444\nget domain as ∈ T (hN ) and the empirical error of the ﬁne-tuned model in the target445\ndomain as ∈ T (hN ). Introduce the notion of adaptability for both the upstream and446\ndownstream tasks as ˜λS,T. Characterize the gap between the source and target do-447\nmains as E[d(S, T )], and quantify the dissimilarity between the original model and448\nthe ﬁne-tuned model by KL(hN ||h0), which encapsulates the ﬁne-tuned model’s449\ncapacity – referring to the extent of parameter adjustments relative to the original450\nmodel. Here, N signiﬁes the volume of data samples. Conseque ntly, the expected451\nerror bound ∈ T (hN ) for the adapted ﬁne-tuned model in the target domain is re-452\nlated to its empirical error ∈ T (hN ), the inter-domain discrepancy E[d(S, T )], the453\nmodel’s capacity KL(hN ||h0), the number of samples N, and the upstream and454\ndownstream tasks’ adaptability ˜λS,T:455\nϵT (hN ) ≤ ϵT (hN ) + 2 E [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N + 2 ˜λS,T.\n(5)\nThe proof of Theorem 1 is given in the Appendix.456\nThe adaptability of the upstream and downstream tasks mentione d in the457\nabove theorem is determined by the nature of the tasks themselve s. Once the458\ntasks are ﬁxed, this factor remains constant. However, by adjusti ng domain dis-459\ncrepancies, model capacity, and sample sizes, it is possible to enhance the model’s460\ngeneralization performance and reduce empirical errors. Therefore, dom ain dis-461\ncrepancies between the source and target domains, model capaci ty, and sample462\nsizes are three fundamental factors that inﬂuence the adaptatio n of multi-modal463\nfoundation models.464\nTaking guidance from the theory of generalization error bound in few -shot465\ncross-domain adaptation, We start from three aspects, namely: do main distribu-466\ntion adaptation, model selection adaptation, and knowledg e utilization adaptation,467\nas shown in Figure 4, to study the few-shot adaptation methods fo r multi-modal468\nfoundation models and propose corresponding solutions to the afo rementioned469\nproblems:470\n1) Domain Distribution Adaptation: To address the issue of high mod-471\neling cost in domain adaptation, a possible approach is to con sider source-free472\n16 Fan Liu † et al.\nFig. 4 Key issues of the few-shot adaptation methods for multi-mod al foundation models and\ncorresponding solutions.\ndomain adaptation methods for multi-modal foundation models . The reconstruc-473\ntion error of a pre-trained autoencoder-based model can be used as a measure of474\ndomain distribution discrepancy and the advantages of prompt- based ﬁne-tuning475\nadaptation methods can also be combined, which can obtain mo re convenient and476\neﬃcient adaptation methods. Additionally, to avoid the risk of modality gap during477\ndomain alignment and the loss of cross-modal semantic relevanc e, a multi-modal478\nautoencoder can be introduced in the prompt reconstruction proce ss to constrain479\nthe cross-modal joint distribution of data. This will help main tain the semantic480\nconsistency of textual and visual features during the domain ad aptation process.481\n2) Model Selection Adaptation: Neural Architecture Search (NAS) [35]482\nis a promising approach to address the problem of adaptive select ion of adapter483\nstructures in multi-modal foundation models. It automaticall y explores diﬀerent484\nnetwork architectures in the search space to ﬁnd the best-performin g structure for485\na given task. However, due to the complexity of multi-modal fou ndation model486\nstructures, NAS-based methods need to search for a wide variety of adapter types487\nsimultaneously, resulting in a large search space and high comp utational costs. In488\nsuch cases, it is necessary to adopt a coarse-to-ﬁne search strat egy to design more489\neﬃcient NAS-based search methods.490\n3) Knowledge Utilization Adaptation: Traditional image augmentation491\ntechniques can generate a large number of augmented samples bu t may not eﬀec-492\ntively adapt to speciﬁc downstream tasks. In contrast, the con tinuous diﬀerentiable493\nimage augmentation paradigm can solve for optimal image augme ntation param-494\neters through derivation and backpropagation. Therefore, it is wort h exploring a495\ndiﬀerentiable image augmentation approach to achieve adapti ve image augmen-496\ntation for downstream tasks. Additionally, introducing extern al knowledge can497\nprovide richer textual descriptions for multi-modal foundation models. However,498\nit is essential to ensure that the introduced knowledge is high ly relevant to visual499\nsemantics. One approach could involve using a large language m odel to generate500\nreliable visual descriptions as references and then training visu al ﬁlters through501\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 17\nadversarial learning to ensure that the ﬁltered textual descripti ons contain valid502\nvisual semantics.503\nDue to the diversity of downstream tasks in the real world, they vary in do-504\nmain distribution, task attributes, available sample quanti ties, and so on. Current505\nadaptation methods of foundation models do not possess the ca pability to adapt506\nto these factors eﬀectively, resulting in limited model performan ce. This limita-507\ntion has become a bottleneck hindering the further adoption of fo undation models508\nin various industries. Therefore, endowing multi-modal foundatio n models with509\nadaptability in the context of downstream few-shot adaptatio n is crucial. This can510\nbe achieved through domain distribution adaptation, model se lection adaptation,511\nand knowledge utilization adaptation. These adaptations h ave the potential to512\nsigniﬁcantly improve model performance and may represent importan t research513\ndirections in this ﬁeld in the future.514\n6 Conclusion515\nWe have comprehensively summarized the methods for multi-moda l foundation516\nmodels in the context of few-shot adaptation tasks, includin g prompt-based ﬁne-517\ntuning adaptation, adapter-based ﬁne-tuning adaptation, a nd external knowledge-518\nbased ﬁne-tuning adaptation. Prompt-based ﬁne-tuning adap tation methods avoid519\nthe tediousness of manually designing the textual prompt and require only a small520\nnumber of parameters to be ﬁne-tuned, thus eﬀectively mitigat ing the overﬁt-521\nting problem. Adapter-based ﬁne-tuning adaptation methods o nly need to update522\na small number of parameters and have the advantages of high eﬃc iency, de-523\nsign ﬂexibility, and robustness. External knowledge-based ﬁ ne-tuning adaptation524\nmethods can alleviate the problem of insuﬃcient prior knowled ge and scarce train-525\ning samples in downstream few-shot scenarios to a certain extent . However, these526\nmethods still have some limitations, such as ineﬀective ada ptation of upstream527\nand downstream domain distributions, lack of adaptability in model selection,528\nand insuﬃcient utilization of data and knowledge. Therefore, w e believe that in529\nthe future, we need to take three perspectives: domain distributi on adaptation,530\nmodel selection adaptation, and knowledge utilization ada ptation in order to im-531\nprove the performance in multi-modal few-shot adaptation. In add ition, we review532\n11 commonly used datasets for evaluating the downstream genera lization per-533\nformance of multi-modal foundation models and adopt four experim ental setups534\nto test the generalization performance of multi-modal foundatio n models under535\nfew-shot conditions. We hope that the summary and analysis of t his survey can536\nprovide some insights and guidance for future research on the few-sh ot adaptation537\nof multi-modal foundation models.538\nAcknowledgements This work was partially supported by National Nature Scienc e Foun-539\ndation of China (62372155), Joint Fund of Ministry of Educat ion for Equipment Pre-research540\n(8091B022123), Research Fund from Science and Technology o n Underwater Vehicle Tech-541\nnology Laboratory (2021JCJQ-SYSJJ-LB06905), Key Laborat ory of Information System Re-542\nquirements, No: LHZZ 2021-M04, Water Science and Technolog y Project of Jiangsu Province543\nunder grant No.2021063, Qinglan Project of Jiangsu Provinc e.544\n18 Fan Liu † et al.\nConﬂict of interest545\nThe authors declare that they have no conﬂict of interest.546\nA Appendix547\nLemma 1 Maurer et al. [53] focused on the relationship between mean err or and expected548\nerror in the target domain in 2004:549\n∆hN (T, T) ≤\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N . (1)\nwhere ∆hN (T, T) = |ϵT (hN ) − ϵT (hN )|, ϵT (hN ) denotes the mean error in the target do-550\nmain, ϵT (hN ) represents the expected error in the target domain, T stands for the data in the551\ntarget domain dataset, T represents all the possible data that may exist in the target domain,552\nwhile h0 represents the prior model, hN represents the adapted model, and N represents the553\nnumber of samples used from the target domain dataset.554\nLemma 2 Anthony Sicilia et al. [73] derived an empirical calculation of the domain gap from555\nsource and target domain data:556\n∆hN (S, T ) ≤ ˜λS,T + E [d (S, T )] . (2)\nwhere ˜λS,T denotes the minimum value of the model’s error sum over the source and target do-557\nmains in the ﬁne-tuning space to indicate the adaptability of up stream and downstream tasks,558\nand E[d(S, T )] denotes the domain gap as the H-divergence between two distin ct domains.559\nLemma 3 Crammer et al. [12] proposed the triangle inequality for erro rs:560\n∆hN (S, T) ≤ ∆hN (S, T ) + ∆hN (T, T) . (3)\nLemma 4 Based on the above lemmas, Anthony Sicilia et al. proposed th e PAC-Bayesian561\ndomain adaptation bound theory for multi-class classiﬁers:562\nϵT (hN ) ≤ ˜λS,T + ϵS (hN ) + E [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N . (4)\nhowever, the term ϵS (hN ) in the theory represents the empirical error of the ﬁne-tuning563\nmodel on the source domain data, which is independent of the ad aptation to the downstream564\ntarget domain. Conversely, the empirical error ϵT (hN ) on the target domain, which aﬀects the565\nadaptation, is not captured. To make the theory better repres ent the impact of target domain566\ndata on the adaptation, we propose the following theorem.567\nTheorem 1. The expected error ∈ T (hN ) of the adapted ﬁne-tuned model in the target568\ndomain is deﬁned, with the empirical error of the ﬁne-tuned mod el in the target domain569\ndenoted as ∈ T (hN ). The adaptability of the upstream and downstream tasks is deﬁ ned570\nas ˜λS,T, with E[d(S, T )] representing the gap between the source and target domains, a nd571\nKL(hN ||h0) indicating the diﬀerence between the original model and the ﬁne-tuned model.572\nHere, N represents the number of data samples. Therefore, the expected error bound ∈ T (hN )573\nof the ﬁne-tuned model in the target domain, for adaptation, de pends on its empirical error574\n∈ T (hN ), the domain diﬀerence E[d(S, T )] between the source and target domains, model ca-575\npacity KL(hN ||h0), sample size N, and the adaptability of the upstream and downstream576\ntasks ˜λS,T:577\nϵT (hN ) ≤ ϵT (hN ) + 2 E [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N + 2 ˜λS,T. (5)\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 19\nProof We choose to replace ϵS (hN ) in Lemma 4 with ϵS (hN ) − ϵT (hN ) + ϵT (hN ), which is578\norganized into the form of Equation 6:579\nϵT (hN ) ≤ ˜λS,T + ϵS (hN ) − ϵT (hN ) + ϵT (hN ) +\nE [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N .\n(6)\nas ϵS (hN ) − ϵT (hN ) ≤ | ϵS (hN ) − ϵT (hN ) | is constant, we can derive that:580\nϵT (hN ) ≤ ˜λS,T + |ϵS (hN ) − ϵT (hN ) |+ ϵT (hN ) +\nE [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N .\n(7)\naccording to the deﬁnition of ∆hN (S, T ) = |ϵS (hN ) − ϵT (hN ) |, it can be obtained:581\nϵT (hN ) ≤ ˜λS,T + ∆hN (S, T ) + ϵT (hN ) +\nE [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N .\n(8)\nsubstituting Lemma 2 into Equation 8, the collation gives:582\nϵT (hN ) ≤ ϵT (hN ) + 2 E [d (S, T )] +\n√\nKL(hN ||h0) + ln\n√\n4N − ln(δ)\n2N + 2 ˜λS,T. □\nReferences583\n1. Baevski, A., Hsu, W., Xu, Q., Babu, A., Gu, J., Auli, M.: data 2vec: A general frame-584\nwork for self-supervised learning in speech, vision and lan guage. In: K. Chaudhuri,585\nS. Jegelka, L. Song, C. Szepesv´ ari, G. Niu, S. Sabato (eds.) International Confer-586\nence on Machine Learning, ICML 2022, 17-23 July 2022, Baltim ore, Maryland, USA,587\nProceedings of Machine Learning Research, vol. 162, pp. 1298–1312. PMLR (2022). URL588\nhttps://proceedings.mlr.press/v162/baevski22a.html589\n2. Bahng, H., Jahanian, A., Sankaranarayanan, S., Isola, P. : Exploring visual prompts for590\nadapting large-scale models (2022)591\n3. Bao, H., Dong, L., Piao, S., Wei, F.: Beit: BERT pre-traini ng of image trans-592\nformers. In: The Tenth International Conference on Learnin g Representations,593\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.ne t (2022). URL594\nhttps://openreview.net/forum?id=p-BhZSz59o4595\n4. Bossard, L., Guillaumin, M., Gool, L.V.: Food-101 - minin g discriminative components596\nwith random forests. In: D.J. Fleet, T. Pajdla, B. Schiele, T . Tuytelaars (eds.) Computer597\nVision - ECCV 2014 - 13th European Conference, Zurich, Switz erland, September 6-12,598\n2014, Proceedings, Part VI, Lecture Notes in Computer Science, vol. 8694, pp. 446–461.599\nSpringer (2014). DOI 10.1007/978-3-319-10599-4 29. URL https://doi.org/10.1007/978-600\n3-319-10599-4 29601\n5. Bulat, A., Tzimiropoulos, G.: LASP: text-to-text optimi zation for language-aware soft602\nprompting of vision & language models. In: IEEE/CVF Confere nce on Computer603\nVision and Pattern Recognition, CVPR 2023, Vancouver, BC, C anada, June 17-24,604\n2023, pp. 23232–23241. IEEE (2023). DOI 10.1109/CVPR52729 .2023.02225. URL605\nhttps://doi.org/10.1109/CVPR52729.2023.02225606\n6. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-al l: Train one network and607\nspecialize it for eﬃcient deployment. In: 8th Internationa l Conference on Learning Rep-608\nresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-3 0, 2020. OpenReview.net609\n(2020). URL https://openreview.net/forum?id=HylxE1HKw S610\n7. Chen, D., Wu, Z., Liu, F., Yang, Z., Huang, Y., Bao, Y., Zhou , E.: Prototypi-611\ncal contrastive language image pretraining. CoRR abs/2206.10996 (2022). DOI612\n10.48550/arXiv.2206.10996. URL https://doi.org/10.485 50/arXiv.2206.10996613\n20 Fan Liu † et al.\n8. Chen, G., Yao, W., Song, X., Li, X., Rao, Y., Zhang, K.: PLOT: prompt learning with op-614\ntimal transport for vision-language models. In: The Eleven th International Conference on615\nLearning Representations, ICLR 2023, Kigali, Rwanda, May 1 -5, 2023. OpenReview.net616\n(2023). URL https://openreview.net/pdf?id=zqwryBoXYnh617\n9. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.E.: A simp le framework for con-618\ntrastive learning of visual representations. In: Proceedi ngs of the 37th Interna-619\ntional Conference on Machine Learning, ICML 2020, 13-18 Jul y 2020, Virtual Event,620\nProceedings of Machine Learning Research, vol. 119, pp. 1597–1607. PMLR (2020). URL621\nhttp://proceedings.mlr.press/v119/chen20j.html622\n10. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra , G., Roberts, A., Barham, P.,623\nChung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsv yashchenko, S., Maynez,624\nJ., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V ., Reif, E., Du, N., Hutchin-625\nson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-A ri, G., Yin, P., Duke, T.,626\nLevskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garci a, X., Misra, V., Robinson,627\nK., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph , B., Spiridonov, A., Sepassi,628\nR., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz,629\nA., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wa ng, X., Saeta, B., Diaz, M.,630\nFirat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D ., Dean, J., Petrov, S., Fiedel,631\nN.: Palm: Scaling language modeling with pathways. CoRR abs/2204.02311 (2022).632\nDOI 10.48550/arXiv.2204.02311. URL https://doi.org/10. 48550/arXiv.2204.02311633\n11. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi , A.: Describing textures in the634\nwild. In: 2014 IEEE Conference on Computer Vision and Patter n Recognition, CVPR635\n2014, Columbus, OH, USA, June 23-28, 2014, pp. 3606–3613. IE EE Computer Society636\n(2014). DOI 10.1109/CVPR.2014.461. URL https://doi.org/ 10.1109/CVPR.2014.461637\n12. Crammer, K., Kearns, M.J., Wortman, J.: Learning from mu ltiple sources.638\nIn: B. Sch¨ olkopf, J.C. Platt, T. Hofmann (eds.) Advances in Neural In-639\nformation Processing Systems 19, Proceedings of the Twenti eth Annual640\nConference on Neural Information Processing Systems, Vanc ouver, British641\nColumbia, Canada, December 4-7, 2006, pp. 321–328. MIT Pres s (2006). URL642\nhttps://proceedings.neurips.cc/paper/2006/hash/0f21f0349462cacdc5796990d37760ae-643\nAbstract.html644\n13. Deng, J., Dong, W., Socher, R., Li, L., Li, K., Fei-Fei, L.: Imagenet: A large-scale hi-645\nerarchical image database. In: 2009 IEEE Computer Society C onference on Computer646\nVision and Pattern Recognition (CVPR 2009), 20-25 June 2009 , Miami, Florida, USA,647\npp. 248–255. IEEE Computer Society (2009). DOI 10.1109/CVP R.2009.5206848. URL648\nhttps://doi.org/10.1109/CVPR.2009.5206848649\n14. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre- training of deep bidirectional650\ntransformers for language understanding. In: J. Burstein, C. Doran, T. Solorio (eds.)651\nProceedings of the 2019 Conference of the North American Cha pter of the Association652\nfor Computational Linguistics: Human Language Technologi es, NAACL-HLT 2019, Min-653\nneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Pa pers), pp. 4171–4186.654\nAssociation for Computational Linguistics (2019). DOI 10. 18653/v1/n19-1423. URL655\nhttps://doi.org/10.18653/v1/n19-1423656\n15. Ding, K., Wang, Y., Liu, P., Yu, Q., Zhang, H., Xiang, S., P an, C.: Prompt tuning657\nwith soft context sharing for vision-language models. CoRR abs/2208.13474 (2022).658\nDOI 10.48550/arXiv.2208.13474. URL https://doi.org/10. 48550/arXiv.2208.13474659\n16. Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., Hu, S ., Chen, Y., Chan, C., Chen,660\nW., Yi, J., Zhao, W., Wang, X., Liu, Z., Zheng, H., Chen, J., Liu, Y., Tang, J., Li, J., Sun,661\nM.: Delta tuning: A comprehensive study of parameter eﬃcien t methods for pre-trained662\nlanguage models. CoRR abs/2203.06904 (2022). DOI 10.48550/arXiv.2203.06904. URL663\nhttps://doi.org/10.48550/arXiv.2203.06904664\n17. Fei-Fei, L., Fergus, R., Perona, P.: Learning generativ e visual models from few training665\nexamples: An incremental bayesian approach tested on 101 ob ject categories. Com-666\nput. Vis. Image Underst. 106(1), 59–70 (2007). DOI 10.1016/j.cviu.2005.09.012. URL667\nhttps://doi.org/10.1016/j.cviu.2005.09.012668\n18. F¨ urst, A., Rumetshofer, E., Lehner, J., Tran, V.T., Tan g, F., Ramsauer, H.,669\nKreil, D.P., Kopp, M., Klambauer, G., Bitto, A., Hochreiter , S.: CLOOB: mod-670\nern hopﬁeld networks with infoloob outperform CLIP. In: Neu rIPS (2022). URL671\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/8078e76f913e31b8467e85b4c0f0d2 2b-672\nAbstract-Conference.html673\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 21\n19. Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., L i, H., Qiao, Y.: Clip-adapter:674\nBetter vision-language models with feature adapters. CoRR abs/2110.04544 (2021).675\nURL https://arxiv.org/abs/2110.04544676\n20. Gao, Y., Liu, J., Xu, Z., Zhang, J., Li, K., Ji, R., Shen, C. : Pyramidclip: Hierarchical677\nfeature alignment for vision-language model pretraining. In: NeurIPS (2022). URL678\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/e9882f7f7c44a10acc01132302bac9 d8-679\nAbstract-Conference.html680\n21. Guo, Z., Zhang, R., Qiu, L., Ma, X., Miao, X., He, X., Cui, B .: CALIP: zero-shot en-681\nhancement of CLIP with parameter-free attention. In: B. Will iams, Y. Chen, J. Neville682\n(eds.) Thirty-Seventh AAAI Conference on Artiﬁcial Intell igence, AAAI 2023, Thirty-683\nFifth Conference on Innovative Applications of Artiﬁcial I ntelligence, IAAI 2023, Thir-684\nteenth Symposium on Educational Advances in Artiﬁcial Inte lligence, EAAI 2023,685\nWashington, DC, USA, February 7-14, 2023, pp. 746–754. AAAI Press (2023). URL686\nhttps://ojs.aaai.org/index.php/AAAI/article/view/25152687\n22. He, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P., Girshick, R .B.: Masked autoen-688\ncoders are scalable vision learners. In: IEEE/CVF Conferen ce on Computer Vi-689\nsion and Pattern Recognition, CVPR 2022, New Orleans, LA, US A, June 18-24,690\n2022, pp. 15979–15988. IEEE (2022). DOI 10.1109/CVPR52688 .2022.01553. URL691\nhttps://doi.org/10.1109/CVPR52688.2022.01553692\n23. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.B.: Momentum contrast for unsupervised693\nvisual representation learning. In: 2020 IEEE/CVF Confere nce on Computer Vision and694\nPattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-1 9, 2020, pp. 9726–9735.695\nComputer Vision Foundation / IEEE (2020). DOI 10.1109/CVPR 42600.2020.00975. URL696\nhttps://doi.org/10.1109/CVPR42600.2020.00975697\n24. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep learning698\nbenchmark for land use and land cover classiﬁcation. IEEE J. Sel. Top. Appl. Earth699\nObs. Remote. Sens. 12(7), 2217–2226 (2019). DOI 10.1109/JSTARS.2019.2918242. URL700\nhttps://doi.org/10.1109/JSTARS.2019.2918242701\n25. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F. , Dorundo, E., Desai, R., Zhu,702\nT., Parajuli, S., Guo, M., Song, D., Steinhardt, J., Gilmer, J.: The many faces of ro-703\nbustness: A critical analysis of out-of-distribution gene ralization. In: 2021 IEEE/CVF704\nInternational Conference on Computer Vision, ICCV 2021, Mo ntreal, QC, Canada, Oc-705\ntober 10-17, 2021, pp. 8320–8329. IEEE (2021). DOI 10.1109/ ICCV48922.2021.00823.706\nURL https://doi.org/10.1109/ICCV48922.2021.00823707\n26. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Son g, D.: Natural adversarial ex-708\namples. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition709\n(CVPR), pp. 15257–15266 (2021). DOI 10.1109/CVPR46437.20 21.01501710\n27. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Ges-711\nmundo, A., Attariyan, M., Gelly, S.: Parameter-eﬃcient tra nsfer learning for NLP. In:712\nK. Chaudhuri, R. Salakhutdinov (eds.) Proceedings of the 36 th International Confer-713\nence on Machine Learning, ICML 2019, 9-15 June 2019, Long Bea ch, California, USA,714\nProceedings of Machine Learning Research, vol. 97, pp. 2790–2799. PMLR (2019). URL715\nhttp://proceedings.mlr.press/v97/houlsby19a.html716\n28. Huang, T., Chu, J., Wei, F.: Unsupervised prompt learnin g for vision-language717\nmodels. CoRR abs/2204.03649 (2022). DOI 10.48550/arXiv.2204.03649. URL718\nhttps://doi.org/10.48550/arXiv.2204.03649719\n29. Huo, Y., Zhang, M., Liu, G., Lu, H., Gao, Y., Yang, G., Wen, J., Zhang, H., Xu, B.,720\nZheng, W., Xi, Z., Yang, Y., Hu, A., Zhao, J., Li, R., Zhao, Y., Z hang, L., Song, Y.,721\nHong, X., Cui, W., Hou, D.Y., Li, Y., Li, J., Liu, P., Gong, Z., J in, C., Sun, Y., Chen, S.,722\nLu, Z., Dou, Z., Jin, Q., Lan, Y., Zhao, W.X., Song, R., Wen, J.: Wenlan: Bridging vision723\nand language by large-scale multi-modal pre-training. CoR R abs/2103.06561 (2021).724\nURL https://arxiv.org/abs/2103.06561725\n30. Jia, C., Yang, Y., Xia, Y., Chen, Y., Parekh, Z., Pham, H., Le, Q.V., Sung, Y., Li,726\nZ., Duerig, T.: Scaling up visual and vision-language repre sentation learning with noisy727\ntext supervision. In: M. Meila, T. Zhang (eds.) Proceedings of the 38th Interna-728\ntional Conference on Machine Learning, ICML 2021, 18-24 Jul y 2021, Virtual Event,729\nProceedings of Machine Learning Research, vol. 139, pp. 4904–4916. PMLR (2021). URL730\nhttp://proceedings.mlr.press/v139/jia21b.html731\n31. Jia, M., Tang, L., Chen, B., Cardie, C., Belongie, S.J., H ariharan, B., Lim, S.: Visual732\nprompt tuning. In: S. Avidan, G.J. Brostow, M. Ciss´ e, G.M. F arinella, T. Hassner733\n(eds.) Computer Vision - ECCV 2022 - 17th European Conferenc e, Tel Aviv, Israel,734\n22 Fan Liu † et al.\nOctober 23-27, 2022, Proceedings, Part XXXIII, Lecture Notes in Computer Science,735\nvol. 13693, pp. 709–727. Springer (2022). DOI 10.1007/978- 3-031-19827-4 41. URL736\nhttps://doi.org/10.1007/978-3-031-19827-4 41737\n32. Jiang, H., Zhang, J., Huang, R., Ge, C., Ni, Z., Lu, J., Zho u, J., Song, S., Huang, G.:738\nCross-modal adapter for text-video retrieval. CoRR abs/2211.09623 (2022). DOI739\n10.48550/arXiv.2211.09623. URL https://doi.org/10.485 50/arXiv.2211.09623740\n33. Jie, S., Deng, Z.: Convolutional bypasses are better vis ion transformer adapters.741\nCoRR abs/2207.07039 (2022). DOI 10.48550/arXiv.2207.07039. URL742\nhttps://doi.org/10.48550/arXiv.2207.07039743\n34. Jie, S., Deng, Z.: Fact: Factor-tuning for lightweight a daptation on vision transformer.744\nIn: B. Williams, Y. Chen, J. Neville (eds.) Thirty-Seventh AA AI Conference on Artiﬁcial745\nIntelligence, AAAI 2023, Thirty-Fifth Conference on Innov ative Applications of Artiﬁcial746\nIntelligence, IAAI 2023, Thirteenth Symposium on Educatio nal Advances in Artiﬁcial747\nIntelligence, EAAI 2023, Washington, DC, USA, February 7-1 4, 2023, pp. 1060–1068.748\nAAAI Press (2023). URL https://ojs.aaai.org/index.php/A AAI/article/view/25187749\n35. Kang, J., Kang, J.K., Kim, J., Jeon, K., Chung, H., Park, B .: Neural architecture750\nsearch survey: A computer vision perspective. Sensors 23(3), 1713 (2023). DOI751\n10.3390/s23031713. URL https://doi.org/10.3390/s23031 713752\n36. Khattak, M.U., Rasheed, H.A., Maaz, M., Khan, S., Khan, F .S.: Maple: Multi-modal753\nprompt learning. CoRR abs/2210.03117 (2022). DOI 10.48550/arXiv.2210.03117. URL754\nhttps://doi.org/10.48550/arXiv.2210.03117755\n37. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object re presentations for ﬁne-756\ngrained categorization. In: 2013 IEEE International Confe rence on Computer Vi-757\nsion Workshops, ICCV Workshops 2013, Sydney, Australia, De cember 1-8, 2013,758\npp. 554–561. IEEE Computer Society (2013). DOI 10.1109/ICC VW.2013.77. URL759\nhttps://doi.org/10.1109/ICCVW.2013.77760\n38. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-eﬃcient prompt761\ntuning. In: M. Moens, X. Huang, L. Specia, S.W. Yih (eds.) Proc eedings of the 2021 Con-762\nference on Empirical Methods in Natural Language Processin g, EMNLP 2021, Virtual763\nEvent / Punta Cana, Dominican Republic, 7-11 November, 2021 , pp. 3045–3059. Asso-764\nciation for Computational Linguistics (2021). DOI 10.1865 3/v1/2021.emnlp-main.243.765\nURL https://doi.org/10.18653/v1/2021.emnlp-main.243766\n39. Li, C., Liu, H., Li, L.H., Zhang, P., Aneja, J., Yang, J., J in, P., Hu, H.,767\nLiu, Z., Lee, Y.J., Gao, J.: ELEVATER: A benchmark and toolki t for768\nevaluating language-augmented visual models. In: NeurIPS (2022). URL769\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/3c4688b6a76f25f2311daa0d75a58f 1a-770\nAbstract-Datasets and Benchmarks.html771\n40. Li, J., Gao, M., Wei, L., Tang, S., Zhang, W., Li, M., Ji, W., T ian, Q., Chua, T.,772\nZhuang, Y.: Gradient-regulated meta-prompt learning for g eneralizable vision-language773\nmodels. CoRR abs/2303.06571 (2023). DOI 10.48550/arXiv.2303.06571. URL774\nhttps://doi.org/10.48550/arXiv.2303.06571775\n41. Li, J., Li, D., Xiong, C., Hoi, S.C.H.: BLIP: bootstrappi ng language-image pre-training776\nfor uniﬁed vision-language understanding and generation. In: K. Chaudhuri, S. Jegelka,777\nL. Song, C. Szepesv´ ari, G. Niu, S. Sabato (eds.) Internatio nal Conference on Ma-778\nchine Learning, ICML 2022, 17-23 July 2022, Baltimore, Mary land, USA, Proceedings779\nof Machine Learning Research, vol. 162, pp. 12888–12900. PMLR (2022). URL780\nhttps://proceedings.mlr.press/v162/li22n.html781\n42. Li, X.L., Liang, P.: Preﬁx-tuning: Optimizing continuo us prompts for generation. In:782\nC. Zong, F. Xia, W. Li, R. Navigli (eds.) Proceedings of the 59t h Annual Meeting of the783\nAssociation for Computational Linguistics and the 11th Int ernational Joint Conference784\non Natural Language Processing, ACL/IJCNLP 2021, (Volume 1 : Long Papers), Vir-785\ntual Event, August 1-6, 2021, pp. 4582–4597. Association fo r Computational Linguistics786\n(2021). DOI 10.18653/v1/2021.acl-long.353. URL https:// doi.org/10.18653/v1/2021.acl-787\nlong.353788\n43. Li, Y., Liang, F., Zhao, L., Cui, Y., Ouyang, W., Shao, J., Y u, F., Yan, J.: Su-789\npervision exists everywhere: A data eﬃcient contrastive la nguage-image pre-training790\nparadigm. In: The Tenth International Conference on Learni ng Representations,791\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.ne t (2022). URL792\nhttps://openreview.net/forum?id=zq1iJkNk3uN793\n44. Lin, Z., Yu, S., Kuang, Z., Pathak, D., Ramanan, D.: Multi modality helps unimodality:794\nCross-modal few-shot learning with multimodal models. In: IEEE/CVF Conference on795\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 23\nComputer Vision and Pattern Recognition, CVPR 2023, Vancou ver, BC, Canada, June796\n17-24, 2023, pp. 19325–19337. IEEE (2023). DOI 10.1109/CVP R52729.2023.01852. URL797\nhttps://doi.org/10.1109/CVPR52729.2023.01852798\n45. Liu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., Wang , P.: K-BERT:799\nenabling language representation with knowledge graph. In : The Thirty-Fourth800\nAAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The T hirty-Second Innova-801\ntive Applications of Artiﬁcial Intelligence Conference, I AAI 2020, The Tenth AAAI802\nSymposium on Educational Advances in Artiﬁcial Intelligen ce, EAAI 2020, New803\nYork, NY, USA, February 7-12, 2020, pp. 2901–2908. AAAI Pres s (2020). URL804\nhttps://ojs.aaai.org/index.php/AAAI/article/view/5681805\n46. Liu, X., Wang, D., Li, M., Duan, Z., Xu, Y., Chen, B., Zhou, M.: Patch-token aligned806\nbayesian prompt learning for vision-language models. CoRR abs/2303.09100 (2023).807\nDOI 10.48550/arXiv.2303.09100. URL https://doi.org/10. 48550/arXiv.2303.09100808\n47. Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., Ta ng, J.: GPT understands,809\ntoo. CoRR abs/2103.10385 (2021). URL https://arxiv.org/abs/2103.10385810\n48. Lu, H., Ding, M., Huo, Y., Yang, G., Lu, Z., Tomizuka, M., Z han, W.:811\nUniadapter: Uniﬁed parameter-eﬃcient transfer learning f or cross-modal model-812\ning. CoRR abs/2302.06605 (2023). DOI 10.48550/arXiv.2302.06605. URL813\nhttps://doi.org/10.48550/arXiv.2302.06605814\n49. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretrain ing task-agnostic visiolinguistic rep-815\nresentations for vision-and-language tasks. In: H.M. Wall ach, H. Larochelle, A. Beygelz-816\nimer, F. d’Alch´ e-Buc, E.B. Fox, R. Garnett (eds.) Advances in Neural Information Pro-817\ncessing Systems 32: Annual Conference on Neural Informatio n Processing Systems 2019,818\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, p p. 13–23 (2019). URL819\nhttps://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-820\nAbstract.html821\n50. Lu, Y., Liu, J., Zhang, Y., Liu, Y., Tian, X.: Prompt distr ibution learning. In:822\nIEEE/CVF Conference on Computer Vision and Pattern Recogni tion, CVPR 2022,823\nNew Orleans, LA, USA, June 18-24, 2022, pp. 5196–5205. IEEE ( 2022). DOI824\n10.1109/CVPR52688.2022.00514. URL https://doi.org/10. 1109/CVPR52688.2022.00514825\n51. Ma, C., Liu, Y., Deng, J., Xie, L., Dong, W., Xu, C.: Underst anding and mitigating826\noverﬁtting in prompt tuning for vision-language models. IE EE Trans. Circuits Syst.827\nVideo Technol. 33(9), 4616–4629 (2023). DOI 10.1109/TCSVT.2023.3245584. U RL828\nhttps://doi.org/10.1109/TCSVT.2023.3245584829\n52. Maji, S., Rahtu, E., Kannala, J., Blaschko, M.B., Vedald i, A.: Fine-grained visual classi-830\nﬁcation of aircraft. CoRR abs/1306.5151 (2013). URL http://arxiv.org/abs/1306.5151831\n53. Maurer, A.: A note on the PAC bayesian theorem. CoRR cs.LG/0411099 (2004). URL832\nhttp://arxiv.org/abs/cs.LG/0411099833\n54. Menon, S., Vondrick, C.: Visual classiﬁcation via descr iption from large language834\nmodels. In: The Eleventh International Conference on Learn ing Representa-835\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview .net (2023). URL836\nhttps://openreview.net/pdf?id=jlAjNL8z5cs837\n55. Mu, N., Kirillov, A., Wagner, D.A., Xie, S.: SLIP: self-s upervision meets language-838\nimage pre-training. In: S. Avidan, G.J. Brostow, M. Ciss´ e, G.M. Farinella, T. Hass-839\nner (eds.) Computer Vision - ECCV 2022 - 17th European Confer ence, Tel Aviv, Is-840\nrael, October 23-27, 2022, Proceedings, Part XXVI, Lecture Notes in Computer Science,841\nvol. 13686, pp. 529–544. Springer (2022). DOI 10.1007/978- 3-031-19809-0 30. URL842\nhttps://doi.org/10.1007/978-3-031-19809-0 30843\n56. Nilsback, M., Zisserman, A.: Automated ﬂower classiﬁca tion over a large num-844\nber of classes. In: Sixth Indian Conference on Computer Visi on, Graphics &845\nImage Processing, ICVGIP 2008, Bhubaneswar, India, 16-19 D ecember 2008, pp.846\n722–729. IEEE Computer Society (2008). DOI 10.1109/ICVGIP .2008.47. URL847\nhttps://doi.org/10.1109/ICVGIP.2008.47848\n57. OpenAI: GPT-4 technical report. CoRR abs/2303.08774 (2023). DOI849\n10.48550/arXiv.2303.08774. URL https://doi.org/10.485 50/arXiv.2303.08774850\n58. Pantazis, O., Brostow, G.J., Jones, K.E., Aodha, O.M.: S vl-adapter: Self-supervised851\nadapter for vision-language pretrained models. In: 33rd Br itish Machine Vision Con-852\nference 2022, BMVC 2022, London, UK, November 21-24, 2022, p . 580. BMVA Press853\n(2022). URL https://bmvc2022.mpi-inf.mpg.de/580/854\n59. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.V. : Cats and dogs. In:855\n2012 IEEE Conference on Computer Vision and Pattern Recogni tion, Providence,856\n24 Fan Liu † et al.\nRI, USA, June 16-21, 2012, pp. 3498–3505. IEEE Computer Soci ety (2012). DOI857\n10.1109/CVPR.2012.6248092. URL https://doi.org/10.110 9/CVPR.2012.6248092858\n60. Peng, F., Yang, X., Xu, C.: Sgva-clip: Semantic-guided v isual adapting of vision-language859\nmodels for few-shot image classiﬁcation. CoRR abs/2211.16191 (2022). DOI860\n10.48550/arXiv.2211.16191. URL https://doi.org/10.485 50/arXiv.2211.16191861\n61. Pratt, S.M., Liu, R., Farhadi, A.: What does a platypus loo k like? generating cus-862\ntomized prompts for zero-shot image classiﬁcation. CoRR abs/2209.03320 (2022).863\nDOI 10.48550/arXiv.2209.03320. URL https://doi.org/10. 48550/arXiv.2209.03320864\n62. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., A garwal, S., Sastry, G., Askell,865\nA., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Lear ning transferable visual models866\nfrom natural language supervision. In: M. Meila, T. Zhang (e ds.) Proceedings of the867\n38th International Conference on Machine Learning, ICML 20 21, 18-24 July 2021, Virtual868\nEvent, Proceedings of Machine Learning Research, vol. 139, pp. 8748–8763. PMLR (2021).869\nURL http://proceedings.mlr.press/v139/radford21a.htm l870\n63. Radford, A., Narasimhan, K.: Improving language unders tanding by generative pre-871\ntraining (2018). URL https://api.semanticscholar.org/C orpusID:49313245872\n64. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imag enet classiﬁers generalize to im-873\nagenet? In: K. Chaudhuri, R. Salakhutdinov (eds.) Proceedi ngs of the 36th International874\nConference on Machine Learning, ICML 2019, 9-15 June 2019, L ong Beach, California,875\nUSA, Proceedings of Machine Learning Research, vol. 97, pp. 5389–5400. PMLR (2019).876\nURL http://proceedings.mlr.press/v97/recht19a.html877\n65. Reynolds, L., McDonell, K.: Prompt programming for larg e language models: Beyond878\nthe few-shot paradigm. In: Y. Kitamura, A. Quigley, K. Isbis ter, T. Igarashi (eds.)879\nCHI ’21: CHI Conference on Human Factors in Computing System s, Virtual Event /880\nYokohama Japan, May 8-13, 2021, Extended Abstracts, pp. 314 :1–314:7. ACM (2021).881\nDOI 10.1145/3411763.3451760. URL https://doi.org/10.11 45/3411763.3451760882\n66. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer , B.: High-resolution im-883\nage synthesis with latent diﬀusion models. In: IEEE/CVF Con ference on Computer884\nVision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24,885\n2022, pp. 10674–10685. IEEE (2022). DOI 10.1109/CVPR52688 .2022.01042. URL886\nhttps://doi.org/10.1109/CVPR52688.2022.01042887\n67. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wight man, R., Cherti, M.,888\nCoombes, T., Katta, A., Mullis, C., Wortsman, M., Schramows ki, P., Kundurthy, S.,889\nCrowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAIO N-5B: an open large-scale890\ndataset for training next generation image-text models. In : NeurIPS (2022). URL891\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c 25-892\nAbstract-Datasets and Benchmarks.html893\n68. Shan, B., Yin, W., Sun, Y., Tian, H., Wu, H., Wang, H.: Ernie -vil 2.0: Multi-view con-894\ntrastive learning for image-text pre-training. CoRR abs/2209.15270 (2022). DOI895\n10.48550/arXiv.2209.15270. URL https://doi.org/10.485 50/arXiv.2209.15270896\n69. Shen, S., Li, C., Hu, X., Xie, Y., Yang, J., Zhang, P., Gan, Z., Wang, L., Yuan,897\nL., Liu, C., Keutzer, K., Darrell, T., Rohrbach, A., Gao, J.: K-LITE: learning898\ntransferable visual models with external knowledge. In: Ne urIPS (2022). URL899\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/63fef0802863f47775c3563e18cbba 17-900\nAbstract-Conference.html901\n70. Shen, S., Yang, S., Zhang, T., Zhai, B., Gonzalez, J.E., K eutzer, K., Darrell, T.:902\nMultitask vision-language prompt tuning. CoRR abs/2211.11720 (2022). DOI903\n10.48550/arXiv.2211.11720. URL https://doi.org/10.485 50/arXiv.2211.11720904\n71. Shin, T., Razeghi, Y., IV, R.L.L., Wallace, E., Singh, S. : Autoprompt: Eliciting knowledge905\nfrom language models with automatically generated prompts . In: B. Webber, T. Cohn,906\nY. He, Y. Liu (eds.) Proceedings of the 2020 Conference on Emp irical Methods in Natural907\nLanguage Processing, EMNLP 2020, Online, November 16-20, 2 020, pp. 4222–4235. As-908\nsociation for Computational Linguistics (2020). DOI 10.18 653/v1/2020.emnlp-main.346.909\nURL https://doi.org/10.18653/v1/2020.emnlp-main.346910\n72. Shu, M., Nie, W., Huang, D., Yu, Z., Goldstein, T., Anandku -911\nmar, A., Xiao, C.: Test-time prompt tuning for zero-shot gen er-912\nalization in vision-language models. In: NeurIPS (2022). U RL913\nhttp://papers.nips.cc/paper ﬁles/paper/2022/hash/5bf2b802e24106064dc547ae9283bb 0c-914\nAbstract-Conference.html915\n73. Sicilia, A., Atwell, K., Alikhani, M., Hwang, S.J.: Pac- bayesian domain adaptation916\nbounds for multiclass learners. In: J. Cussens, K. Zhang (ed s.) Uncertainty in917\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 25\nArtiﬁcial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty918\nin Artiﬁcial Intelligence, UAI 2022, 1-5 August 2022, Eindh oven, The Netherlands,919\nProceedings of Machine Learning Research, vol. 180, pp. 1824–1834. PMLR (2022). URL920\nhttps://proceedings.mlr.press/v180/sicilia22a.html921\n74. Soomro, K., Zamir, A.R., Shah, M.: UCF101: A dataset of 10 1 human actions classes from922\nvideos in the wild. CoRR abs/1212.0402 (2012). URL http://arxiv.org/abs/1212.0402923\n75. Sun, Q., Fang, Y., Wu, L., Wang, X., Cao, Y.: EVA-CLIP: imp roved training techniques924\nfor CLIP at scale. CoRR abs/2303.15389 (2023). DOI 10.48550/arXiv.2303.15389.925\nURL https://doi.org/10.48550/arXiv.2303.15389926\n76. Tejankar, A., Sanjabi, M., Wu, B., Xie, S., Khabsa, M., Pi rsiavash, H., Firooz, H.: A927\nﬁstful of words: Learning transferable visual models from b ag-of-words supervision. CoRR928\nabs/2112.13884 (2021). URL https://arxiv.org/abs/2112.13884929\n77. Udandarao, V., Gupta, A., Albanie, S.: Sus-x: Training- free name-only transfer of vision-930\nlanguage models. CoRR abs/2211.16198 (2022). DOI 10.48550/arXiv.2211.16198. URL931\nhttps://doi.org/10.48550/arXiv.2211.16198932\n78. Wang, H., Ge, S., Lipton, Z.C., Xing, E.P.: Learning robu st global representations by933\npenalizing local predictive power. In: H.M. Wallach, H. Lar ochelle, A. Beygelzimer,934\nF. d’Alch´ e-Buc, E.B. Fox, R. Garnett (eds.) Advances in Neu ral Information Processing935\nSystems 32: Annual Conference on Neural Information Proces sing Systems 2019, NeurIPS936\n2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 10506 –10518 (2019). URL937\nhttps://proceedings.neurips.cc/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-938\nAbstract.html939\n79. Wang, J., Wang, H., Deng, J., Wu, W., Zhang, D.: Eﬃcientcli p: Eﬃcient cross-modal pre-940\ntraining by ensemble conﬁdent learning and language modeli ng. CoRR abs/2109.04699941\n(2021). URL https://arxiv.org/abs/2109.04699942\n80. Wang, J., Zhang, Y., Zhang, L., Yang, P., Gao, X., Wu, Z., D ong, X., He, J., Zhuo,943\nJ., Yang, Q., Huang, Y., Li, X., Wu, Y., Lu, J., Zhu, X., Chen, W. , Han, T., Pan, K.,944\nWang, R., Wang, H., Wu, X., Zeng, Z., Chen, C., Gan, R., Zhang, J.: Fengshenbang 1.0:945\nBeing the foundation of chinese cognitive intelligence. Co RR abs/2209.02970 (2022).946\nDOI 10.48550/arXiv.2209.02970. URL https://doi.org/10. 48550/arXiv.2209.02970947\n81. Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., A ggarwal, K., Mo-948\nhammed, O.K., Singhal, S., Som, S., Wei, F.: Image as a foreig n language: Beit pre-949\ntraining for all vision and vision-language tasks. CoRR abs/2208.10442 (2022). DOI950\n10.48550/arXiv.2208.10442. URL https://doi.org/10.485 50/arXiv.2208.10442951\n82. Wortsman, M., Ilharco, G., Kim, J.W., Li, M., Kornblith, S ., Roelofs, R., Lopes, R.G.,952\nHajishirzi, H., Farhadi, A., Namkoong, H., Schmidt, L.: Rob ust ﬁne-tuning of zero-shot953\nmodels. In: IEEE/CVF Conference on Computer Vision and Patt ern Recognition, CVPR954\n2022, New Orleans, LA, USA, June 18-24, 2022, pp. 7949–7961. IEEE (2022). DOI955\n10.1109/CVPR52688.2022.00780. URL https://doi.org/10. 1109/CVPR52688.2022.00780956\n83. Wu, J., Li, X., Wei, C., Wang, H., Yuille, A.L., Zhou, Y., X ie, C.: Unleashing the957\npower of visual prompting at the pixel level. CoRR abs/2212.10556 (2022). DOI958\n10.48550/arXiv.2212.10556. URL https://doi.org/10.485 50/arXiv.2212.10556959\n84. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A .: SUN database: Large-scale scene960\nrecognition from abbey to zoo. In: The Twenty-Third IEEE Con ference on Computer961\nVision and Pattern Recognition, CVPR 2010, San Francisco, C A, USA, 13-18 June 2010,962\npp. 3485–3492. IEEE Computer Society (2010). DOI 10.1109/C VPR.2010.5539970. URL963\nhttps://doi.org/10.1109/CVPR.2010.5539970964\n85. Xing, Y., Shi, Z., Meng, Z., Lakemeyer, G., Ma, Y., Watten hofer, R.: KM-BART: knowl-965\nedge enhanced multimodal BART for visual commonsense gener ation. In: C. Zong,966\nF. Xia, W. Li, R. Navigli (eds.) Proceedings of the 59th Annual Meeting of the As-967\nsociation for Computational Linguistics and the 11th Inter national Joint Conference on968\nNatural Language Processing, ACL/IJCNLP 2021, (Volume 1: L ong Papers), Virtual969\nEvent, August 1-6, 2021, pp. 525–535. Association for Compu tational Linguistics (2021).970\nDOI 10.18653/v1/2021.acl-long.44. URL https://doi.org/ 10.18653/v1/2021.acl-long.44971\n86. Xing, Y., Wu, Q., Cheng, D., Zhang, S., Liang, G., Zhang, Y .: Class-aware visual prompt972\ntuning for vision-language pre-trained model. CoRR abs/2208.08340 (2022). DOI973\n10.48550/arXiv.2208.08340. URL https://doi.org/10.485 50/arXiv.2208.08340974\n87. Yang, A., Pan, J., Lin, J., Men, R., Zhang, Y., Zhou, J., Zh ou, C.: Chinese CLIP: con-975\ntrastive vision-language pretraining in chinese. CoRR abs/2211.01335 (2022). DOI976\n10.48550/arXiv.2211.01335. URL https://doi.org/10.485 50/arXiv.2211.01335977\n26 Fan Liu † et al.\n88. Yang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison- Burch, C., Yatskar, M.: Lan-978\nguage in a bottle: Language model guided concept bottleneck s for interpretable image979\nclassiﬁcation. CoRR abs/2211.11158 (2022). DOI 10.48550/arXiv.2211.11158. URL980\nhttps://doi.org/10.48550/arXiv.2211.11158981\n89. Yao, H., Zhang, R., Xu, C.: Visual-language prompt tunin g with knowledge-982\nguided context optimization. In: IEEE/CVF Conference on Co mputer Vision983\nand Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24,984\n2023, pp. 6757–6767. IEEE (2023). DOI 10.1109/CVPR52729.2 023.00653. URL985\nhttps://doi.org/10.1109/CVPR52729.2023.00653986\n90. Yao, L., Huang, R., Hou, L., Lu, G., Niu, M., Xu, H., Liang, X., Li, Z., Jiang, X., Xu, C.:987\nFILIP: ﬁne-grained interactive language-image pre-train ing. In: The Tenth International988\nConference on Learning Representations, ICLR 2022, Virtua l Event, April 25-29, 2022.989\nOpenReview.net (2022). URL https://openreview.net/foru m?id=cpDhcsEDC2990\n91. Yu, F., Tang, J., Yin, W., Sun, Y., Tian, H., Wu, H., Wang, H. : Ernie-vil:991\nKnowledge enhanced vision-language representations thro ugh scene graphs. In:992\nThirty-Fifth AAAI Conference on Artiﬁcial Intelligence, A AAI 2021, Thirty-Third993\nConference on Innovative Applications of Artiﬁcial Intell igence, IAAI 2021, The994\nEleventh Symposium on Educational Advances in Artiﬁcial In telligence, EAAI 2021,995\nVirtual Event, February 2-9, 2021, pp. 3208–3216. AAAI Pres s (2021). URL996\nhttps://ojs.aaai.org/index.php/AAAI/article/view/16431997\n92. Zang, Y., Li, W., Zhou, K., Huang, C., Loy, C.C.: Uniﬁed vis ion and language prompt998\nlearning. CoRR abs/2210.07225 (2022). DOI 10.48550/arXiv.2210.07225. URL999\nhttps://doi.org/10.48550/arXiv.2210.072251000\n93. Zeng, W., Ren, X., Su, T., Wang, H., Liao, Y., Wang, Z., Jian g, X., Yang, Z., Wang,1001\nK., Zhang, X., Li, C., Gong, Z., Yao, Y., Huang, X., Wang, J., Y u, J., Guo, Q., Yu,1002\nY., Zhang, Y., Wang, J., Tao, H., Yan, D., Yi, Z., Peng, F., Jia ng, F., Zhang, H.,1003\nDeng, L., Zhang, Y., Lin, Z., Zhang, C., Zhang, S., Guo, M., Gu , S., Fan, G., Wang,1004\nY., Jin, X., Liu, Q., Tian, Y.: Pangu- α: Large-scale autoregressive pretrained chinese1005\nlanguage models with auto-parallel computation. CoRR abs/2104.12369 (2021). URL1006\nhttps://arxiv.org/abs/2104.123691007\n94. Zhang, B., Jin, X., Gong, W., Xu, K., Zhang, Z., Wang, P., Sh en, X., Feng, J.: Multimodal1008\nvideo adapter for parameter eﬃcient video text retrieval. C oRR abs/2301.07868 (2023).1009\nDOI 10.48550/arXiv.2301.07868. URL https://doi.org/10. 48550/arXiv.2301.078681010\n95. Zhang, J., Huang, J., Jin, S., Lu, S.: Vision-language mo dels for vision tasks: A1011\nsurvey. CoRR abs/2304.00685 (2023). DOI 10.48550/arXiv.2304.00685. URL1012\nhttps://doi.org/10.48550/arXiv.2304.006851013\n96. Zhang, R., Fang, R., Zhang, W., Gao, P., Li, K., Dai, J., Qia o, Y., Li, H.: Tip-adapter:1014\nTraining-free clip-adapter for better vision-language mo deling. CoRR abs/2111.039301015\n(2021). URL https://arxiv.org/abs/2111.039301016\n97. Zhang, R., Qiu, L., Zhang, W., Zeng, Z.: VT-CLIP: enhancin g vision-language1017\nmodels with visual-guided texts. CoRR abs/2112.02399 (2021). URL1018\nhttps://arxiv.org/abs/2112.023991019\n98. Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.: ER NIE: enhanced language1020\nrepresentation with informative entities. In: A. Korhonen , D.R. Traum, L. M` arquez (eds.)1021\nProceedings of the 57th Conference of the Association for Co mputational Linguistics,1022\nACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 1441–1023\n1451. Association for Computational Linguistics (2019). D OI 10.18653/v1/p19-1139.1024\nURL https://doi.org/10.18653/v1/p19-11391025\n99. Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels fr om CLIP. In: S. Avidan, G.J.1026\nBrostow, M. Ciss´ e, G.M. Farinella, T. Hassner (eds.) Compu ter Vision - ECCV 20221027\n- 17th European Conference, Tel Aviv, Israel, October 23-27 , 2022, Proceedings, Part1028\nXXVIII, Lecture Notes in Computer Science, vol. 13688, pp. 696–712. Springer (2022).1029\nDOI 10.1007/978-3-031-19815-1 40. URL https://doi.org/10.1007/978-3-031-19815-1 401030\n100. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prom pt learning for vision-language1031\nmodels. In: IEEE/CVF Conference on Computer Vision and Patt ern Recognition, CVPR1032\n2022, New Orleans, LA, USA, June 18-24, 2022, pp. 16795–1680 4. IEEE (2022). DOI1033\n10.1109/CVPR52688.2022.01631. URL https://doi.org/10. 1109/CVPR52688.2022.016311034\n101. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to promp t for vision-language models.1035\nInt. J. Comput. Vis. 130(9), 2337–2348 (2022). DOI 10.1007/s11263-022-01653-1. U RL1036\nhttps://doi.org/10.1007/s11263-022-01653-11037\nFew-shot Adaptation of Multi-modal Foundation Models: A Su rvey 27\n102. Zhu, B., Niu, Y., Han, Y., Wu, Y., Zhang, H.: Prompt-alig ned gradient for prompt1038\ntuning. CoRR abs/2205.14865 (2022). DOI 10.48550/arXiv.2205.14865. URL1039\nhttps://doi.org/10.48550/arXiv.2205.148651040"
}