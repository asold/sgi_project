{
  "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
  "url": "https://openalex.org/W4393105094",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3092461845",
      "name": "Jiahang Zhou",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2137745707",
      "name": "Yanyu Chen",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2920666542",
      "name": "Zicong Hong",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2100588147",
      "name": "Wuhui Chen",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2096181728",
      "name": "Yue Yu",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1973217115",
      "name": "Tao Zhang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A63849503",
      "name": "Hui Wang",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2251072804",
      "name": "ChuanFu Zhang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2715018527",
      "name": "Zibin Zheng",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W6854474899",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4379929801",
    "https://openalex.org/W6852989508",
    "https://openalex.org/W4385801730",
    "https://openalex.org/W6850462617",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3086105743",
    "https://openalex.org/W4386768656",
    "https://openalex.org/W6776671032",
    "https://openalex.org/W6799372109",
    "https://openalex.org/W3159351344",
    "https://openalex.org/W2056999868",
    "https://openalex.org/W4294433898",
    "https://openalex.org/W201315547",
    "https://openalex.org/W6795846692",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W4220741164",
    "https://openalex.org/W3132107458",
    "https://openalex.org/W6779863968",
    "https://openalex.org/W3206832494",
    "https://openalex.org/W6856663864",
    "https://openalex.org/W4386763939",
    "https://openalex.org/W4321466422",
    "https://openalex.org/W4394923298",
    "https://openalex.org/W6810520696",
    "https://openalex.org/W6797242466",
    "https://openalex.org/W6780805062",
    "https://openalex.org/W2150884987",
    "https://openalex.org/W6793032698",
    "https://openalex.org/W4220967350",
    "https://openalex.org/W6811726652",
    "https://openalex.org/W4376652719",
    "https://openalex.org/W4386396242",
    "https://openalex.org/W6810296985",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W6811928498",
    "https://openalex.org/W4310282800",
    "https://openalex.org/W6703652217",
    "https://openalex.org/W6768723914",
    "https://openalex.org/W6745245109",
    "https://openalex.org/W6753209298",
    "https://openalex.org/W2962747323",
    "https://openalex.org/W2489529491",
    "https://openalex.org/W3012479151",
    "https://openalex.org/W3010830594",
    "https://openalex.org/W4321636583",
    "https://openalex.org/W6790981935",
    "https://openalex.org/W4318541551",
    "https://openalex.org/W4387723591",
    "https://openalex.org/W3189259198",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W6787953186",
    "https://openalex.org/W3205803342",
    "https://openalex.org/W4226057994",
    "https://openalex.org/W4312060029",
    "https://openalex.org/W4318541673",
    "https://openalex.org/W4220703317",
    "https://openalex.org/W6853287796",
    "https://openalex.org/W7046032890",
    "https://openalex.org/W4318541692",
    "https://openalex.org/W4386763875",
    "https://openalex.org/W6857551316",
    "https://openalex.org/W6853192989",
    "https://openalex.org/W4318541554",
    "https://openalex.org/W4366341968",
    "https://openalex.org/W6779103662",
    "https://openalex.org/W4321636575",
    "https://openalex.org/W6849514112",
    "https://openalex.org/W6852691820",
    "https://openalex.org/W3095488153",
    "https://openalex.org/W6798686915",
    "https://openalex.org/W6843821762",
    "https://openalex.org/W6850927664",
    "https://openalex.org/W4372263604",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W3082549344",
    "https://openalex.org/W4388874804",
    "https://openalex.org/W6847386241",
    "https://openalex.org/W6855874919",
    "https://openalex.org/W4381551031",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W3021234081",
    "https://openalex.org/W4386617904",
    "https://openalex.org/W4376167141",
    "https://openalex.org/W3167220634",
    "https://openalex.org/W4388725733",
    "https://openalex.org/W2884711234",
    "https://openalex.org/W4404658388",
    "https://openalex.org/W4404356490",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3138895808",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4362515116"
  ],
  "abstract": "Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-<inline-formula><tex-math notation=\"LaTeX\">$\\Sigma$</tex-math></inline-formula>) have demonstrated extraordinary performance in key technological areas, such as natural language processing and visual recognition, and have become the mainstream trend of artificial general intelligence. This has led more and more major technology giants to dedicate significant human and financial resources to actively develop their foundation model systems, which drives continuous growth of these models&#x0027; parameters. As a result, the training and serving of these models have posed significant challenges, including substantial computing power, memory consumption, bandwidth demands, etc. Therefore, employing efficient training and serving strategies becomes particularly crucial. Many researchers have actively explored and proposed effective methods. So, a comprehensive survey of them is essential for system developers and researchers. This paper extensively explores the methods employed in training and serving foundation models from various perspectives. It provides a detailed categorization of these state-of-the-art methods, including finer aspects such as network, computing, and storage. Additionally, the paper summarizes the challenges and presents a perspective on the future development direction of foundation model systems. Through comprehensive discussion and analysis, it hopes to provide a solid theoretical basis and practical guidance for future research and applications, promoting continuous innovation and development in foundation model systems.",
  "full_text": "Received XX Month, XXXX; revised XX Month, XXXX; accepted XX Month, XXXX; Date of publication XX Month, XXXX; date of\ncurrent version XX Month, XXXX.\nDigital Object Identifier 10.1109/OJIM.2022.1234567\nTraining and Serving System of\nFoundation Models: A Comprehensive\nSurvey\nJIAHANG ZHOU1, YANYU CHEN1, ZICONG HONG3, WUHUI CHEN2, 4, YUE YU4,\nTAO ZHANG1, HUI WANG4, CHUANFU ZHANG1, AND ZIBIN ZHENG2.\n1School of Systems Science and Engineering, Sun Y at-sen University, Guangzhou, 528406, China\n2School of Software Engineering, Sun Y at-sen University, Guangzhou, 510275, China\n3Department of Computing in The Hong Kong Polytechnic University, Hong Kong, 999077, China\n4Peng Cheng Laboratory, Shenzhen, 518000, China\nCORRESPONDING AUTHOR: Wuhui Chen (e-mail: chenwuh@mail.sysu.edu.cn).\nABSTRACT Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-Σ) have demonstrated\nextraordinary performance in key technological areas, such as natural language processing and visual\nrecognition, and have become the mainstream trend of artificial general intelligence. This has led more\nand more major technology giants to dedicate significant human and financial resources to actively develop\ntheir foundation model systems, which drives continuous growth of these models’ parameters. As a result,\nthe training and serving of these models have posed significant challenges, including substantial computing\npower, memory consumption, bandwidth demands, etc. Therefore, employing efficient training and serving\nstrategies becomes particularly crucial. Many researchers have actively explored and proposed effective\nmethods. So, a comprehensive survey of them is essential for system developers and researchers. This\npaper extensively explores the methods employed in training and serving foundation models from various\nperspectives. It provides a detailed categorization of these state-of-the-art methods, including finer aspects\nsuch as network, computing, and storage. Additionally, the paper summarizes the challenges and presents\na perspective on the future development direction of foundation model systems. Through comprehensive\ndiscussion and analysis, it hopes to provide a solid theoretical basis and practical guidance for future\nresearch and applications, promoting continuous innovation and development in foundation model systems.\nINDEX TERMS Foundation Model System, Training, Serving, Network, Computing, Storage\nI. INTRODUCTION\nT\nHe combination of deep learning techniques and pow-\nerful computational capabilities continuously drives the\ndevelopment of artificial general intelligence, ushering us\ninto the era of foundation models. However, achieving suc-\ncessful applications of foundation models is inseparable from\ncomprehensive support at the system level. A foundation\nmodel system is built upon extensive training data, state-of-\nthe-art models, high-performance computing resources, and\nmeticulously optimized training and serving algorithms. The\nprimary purpose of this system is to handle complex tasks\nwith heightened precision, such as GPT3 [1], LLaMA [2],\nPanGu-Σ [3], PengCheng Mind [4] etc.\nFoundation models have demonstrated extraordinary per-\nformance in many tasks. This has led more and more major\ntechnology giants to dedicate significant human and financial\nresources to actively develop their foundation model systems,\nwhich increases the parameter size (Figure 1). However, as\nthe parameter size of foundational model systems continues\nto grow, challenges are posed throughout the lifecycle of\nfoundation models, particularly during the training and serv-\ning phases. In the training phase, the substantial parameter\nsize results in significant demands for computation and\nstorage, creating immense pressure on hardware resources\nand computational efficiency. Consequently, training these\nmodels usually takes a long time and requires efficient\nutilization of computational resources. In the serving phase,\nwith the widespread application of foundation models, the\nsignificant increase in workload has become an unavoidable\nchallenge. This heightened demand may lead to issues for\nserving systems, such as latency, performance decline, or\nresource bottlenecks. Therefore, employing highly efficient\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVOLUME , 1\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\n0\n200\n400\n600\n800\n1000\n1200\n2019 2020 2021 2022 2023\nT5(11B)\nGPT-3(175B)\nGopher(280B)\nMT-NLG(530B)\nPanGu-Σ(1085B)\nCPM-2(198B)\nPaLM(540B)\nFlan-PaLM(540B)\nMT5(13B)\nModel Size (B)\nYears\nFIGURE 1. Evolutionary Chart of Model Sizes Over Time.\ntraining and serving strategies becomes particularly crucial.\nMany researchers have actively explored and proposed ef-\nfective methods for training and serving. However, different\napproaches have different application scenarios. So, it poses\na challenge for system developers who struggle to identify\nthe most suitable method for their problems. This challenge\nis precisely why this paper was proposed.\nAlthough there have been some surveys on foundation\nmodels, Most surveys [5]–[11] predominantly focus on\nmodel design and downstream task adaptation, with only a\nminority delving into foundation model training. However,\nthere are two notable shortcomings in these training-centric\nsurveys [12]: firstly, they lack in-depth exploration from the\nperspective of updates in network, computing, and storage;\nsecondly, their primary emphasis is on the training phase,\nneglecting considerations for the serving phase. Therefore,\na comprehensive survey of foundation model training and\nserving methods is essential for system developers and\nresearchers. Accordingly, this paper presents an in-depth\nanalysis of the state-of-the-art methods in this domain. This\npaper provides systems developers and researchers valuable\ninformation through comprehensive analysis and compari-\nson. It assists them in making the right decisions when\nconfronted with the challenges associated with foundation\nmodel systems.\nII. BASIC CONCEPTS\nThis section comprehensively explains the fundamental con-\ncepts in foundation model systems.\nA. The lifecycle of the foundation model system\nThe lifecycle of the foundation model system (Figure 2)\nencompasses several crucial stages. ➊ Initially, the collection\nand preprocessing of data ensure the quality and availability\nrequired for model training. Subsequently, choosing an ap-\npropriate model. ➋ Transitioning to the training phase, the\nmodel undergoes adjustments through the backpropagation\nalgorithm, demanding substantial computational resources to\nenhance its fitting capability to the training data. ➌ Model\nevaluation and fine-tuning involve assessing performance\nTraining Dataset Foundation model\n➊ Data collection and \nmodel selection\nDatasets and models\n➋ Training\n➌ Tuning\n➍ Serving ChatGPT\n LLaMA\n Bard\n PanGu\nTuning\nServing\nComputing Computing Computing Computing\nStorage\n Storage\n Storage\nNetwork\n Network\n Network\nFinal model\nFIGURE 2. The lifecycle of the foundation model system.\nwith test data and adjusting for improved generalization.\nOnce the model performs satisfactorily, it can be deployed\ninto practical applications. ➍ In the serving stage, effective\ndeployment and integration are crucial to ensuring harmo-\nnious collaboration with existing systems. The primary focus\nin this phase centers on performance optimization, aiming to\nenhance serving speed and reduce latency through strategies\nsuch as model quantization and hardware acceleration.\nB. Transformer for foundation models\nTransformer [13] is a deep learning model architecture\ncomprised of encoders and decoders. Its core innovation\nlies in the self-attention mechanism, an important component\nwidely utilized in foundational models. The main idea is to\nenable the model to focus on dynamic associations between\ndifferent positions, thereby better capturing long-distance\ninterdependent features in a sentence. In the current field\nof deep learning, the Transformer architecture has become\nthe preferred choice for numerous foundational models.\nThis architecture stands out for its outstanding performance\nand flexibility, particularly in excelling at natural language\nprocessing tasks. Many pivotal foundational models, such\nas GPT, LLaMA, and PengCheng Mind, have adopted the\ndesign of the Transformer. The successful applications of\nthe Transformer architecture demonstrate its universality in\nfoundational models, providing powerful modeling tools for\nvarious tasks.\nIII. MODEL TRAINING\nIn foundation model training, the most significant challenges\nare the high demands for memory and computational power.\nTherefore, this section explores the implementation of opti-\nmization strategies in foundation model training from three\nperspectives, network, computing, and storage, to address\nthese challenges, as shown in Table 1.\nA. Advanced Techniques in Parallel Computing\n1) Data Parallelism: Accelerating Workloads Effectively\nIn data parallelism, each computational node possesses a\nreplica of the model and independently processes a subset\n2 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 1. Overview of network, computing, and storage optimization strategies in foundation model training.\nParallel Computing\nParallelism Type Specific Strategy Year Main Features Scales (M, B, 10B, 100B+)Open Resource\nData Parallelism\nDDP [14] 2020 Bucketing gradients, Skipping gradient synchronization M ✓\nXu et al. [16] 2020 Automatic cross-replica sharding, Efficient weight-update shardingM ×\nFSDP [15] 2023 Fully sharded data parallel 100B+ ✓\nTensor Parallelism\nMegatron-LM [17] 2021 Weight matrix partitioned 100B+ ✓\nOptimus [18] 2023 2D partition gradient computation, Systematic buffering technique100B+ ✓\nTesseract [20] 2023 2.5-Dimensional Matrix Multiplication, Reducing communication overheadB ×\n3D Tensor Parallelism [22]2021 3-dimensional model parallelism, Perfect load balance M ×\nPipeline Parallelism\nGPipe [23] 2019 Novel batchsplitting pipelining 10B ✓\nPipeDream [24] 2019 1F1B, Weight stashing, Vertical sync M ×\nMegatron-LM [17] 2021 Schedule with Interleaved Stages 100B+ ✓\nChimera [28] 2021 Bidirectional pipelines, More balanced activation memory consumptionB ✓\nPipeDream-2BM [27] 2021 Memory-efficient pipeline parallelism, Weight gradient coalescing strategyB ×\nFTPipe [35] 2021 Mixed-pipe partitioning, Better balance of the compute- and memory-load10B ✓\nDAPPLE [26] 2022 Synchronous training, Topology-aware device assignment B ×\nVaruna [25] 2022 Recomputation, Correctness-preserving job morphing 100B+ ✓\nHanayo [29] 2023 Wave-like pipeline, Unified framework for pipeline parallelism100B+ ×\nMixpipe [30] 2023 Mixed scheduling, Flexible bidirectional pipeline 10B ×\nAvgpipe [31] 2023 Elastic averaging training method M ×\nBpipe [33] 2023 Memory-balanced pipeline parallelism 100B+ ×\nBamboo [34] 2023 Redundant computation, Use of preemptible instances M ×\nDynapipe [32] 2024 Dynamic micro-batching pipeline 10B ✓\nExpert Parallelism\nGShard [36] 2021 Sparsely-Gated Mixture-of-Experts 100B+ ×\nFastMoE [38] 2021 Distributed MoE training system, Open-source system based on PyTorch100B+ ✓\nFasterMoE [40] 2022 Dynamic shadowing method, Novel roofline-like model B ✓\nLina [44] 2023 Tensor partitioning, Two-phase scheduling B ×\nJanus [45] 2023 Data-centric paradigm, Topology-aware priority strategy M ×\nDeepSpeed-MoE [41] 2022 Pyramid-Residual MoE architecture 10B ✓\nDeepSpeed-TED [42] 2023 3D hybrid parallel algorithm 10B ✓\nSmartMoE [43] 2023 Expert placement strateg, Two-stage adaptive auto-parallelization approach10B ×\nHybrid Parallelism\nAlpa [48] 2022 Two-level parallel execution, ILP Formulation 100B+ ✓\nSmith et al. [46] 2022 3D parallelism methodology 100B+ ×\nGalvatron [49] 2022 Decision tree approach, Dynamic programming search algorithm10B ✓\nGPU Memory Optimization\nCategory Specific Strategy Year Main Features Scales (M, B, 10B, 100B+)Open Resource\nCheckpointing and RecomputationChen et al. [50] 2016 Checkpointing, Recomputation M ×\nCheckmate [51] 2022 Integer linear program, Off-the-shelf MILP solvers M ✓\nMixed Precision TrainingMixed Precision Training [52]2018 Mixed precision M ×\nJia et al. [53] 2018 LARS algorithm M ×\nMemory Swapping\nSwapAdvisor [56] 2020 Custom-designed genetic algorithm, Memory allocation, Swap decisionsM ×\nAutotm [57] 2020 Integer linear programming M ✓\nFlashNeuron [59] 2021 SSDs for data offloading and prefetching, The first buffering-on-SSD solutionB ✓\nStronghold [58] 2022 Work window method, CPU-GPU offloading 10B ✓\nPatrickstar [62] 2022 Chunk-based memory management, Dynamic memory management10B ✓\nG10 [61] 2023 Amalgamating GPU, host, flash memory into a unified memory spaceB ✓\nDeepUM [60] 2023 Enhances Unified Memory by prefetching techniques B ×\nZero Redundancy Optimization\nZeRO [63] 2020 Zero memory redundancy 100B+ ✓\nZeRO-Offload [64] 2021 Parameters is offloaded to CPU memory 10B ✓\nZeRO-Infinity [66] 2021 Parameters is offloaded to CPU, and NVMe memory 100B+ ✓\nCommunication Optimization\nCategory Specific Strategy Year Main Features Scales (M, B, 10B, 100B+)Open Resource\nCommunication Optimization\nBagua [67] 2021 MPI-style communication library M ✓\nOut-Of-Order BackProp [70]2022 Out-Of-Order Computation 100B+ ✓\nZeRO++ [71] 2023 Block-quantization, Data remapping, Quantized gradient averaging paradigm100B+ ✓\nWang et al. [68] 2023 Decomposing the original communication and computational operations100B+ ×\nMobius [69] 2023 Cross-mapping strategy, Communication-efficient B ×\nOptimus-CC [73] 2023 Inter-node communication compression, Selective stage compressionB ×\nCOCKTAILSGD [72] 2023 Random sparsification, Top-K sparsification, Quantization 10B ×\nof data assigned to it. As shown in Figure 3a, each node\nuses its model replica for forward and backward propagation\nand gradient calculation. So, it requires gradient aggregation\nand synchronization operations to update the global model\nparameters. This distributed approach significantly reduces\nthe computational load on individual nodes and speeds up\nthe training process by parallelizing the workload.\nDistributed Data Parallel (DDP) [14] utilizes gradient\nbucketing, computation-communication overlap, and gradi-\nent synchronization skipping to enhance the efficiency of dis-\ntributed data parallelism. In DDP, storing the entire model’s\nparameters on each node simplifies training but significantly\nincreases memory demand, especially for foundation models.\nTo solve this problem, several solutions have been proposed.\nFacebook introduced a technique called Fully Sharded Data\nParallel (FSDP) [15] to tackle this issue. It divides model\nparameters into smaller units, restoring the complete model\nparameters through communication before computation and\ndiscarding them immediately after the calculation. Similarly,\nXu et al. [16] proposed an automatic cross-replica sharding\nVOLUME , 3\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nFIGURE 3. Schematic diagram of parallelization strategies in foundation model systems. Different color blocks indicate different layers in the network.\ntechnique for weight updates in data-parallel training to op-\ntimize memory and communication efficiency during model\ntraining.\n2) Tensor Parallelism: Scaling Foundation Models\nTensor parallelism is developed to address the challenges of\ntraining foundation models that exceed the memory capacity\nof a single device. In tensor parallelism (As shown in\nFigure 3b), the parameters and computations of the model\nare divided (or sliced) across multiple computing devices,\neffectively reducing the memory load on each device.\nMegatron-LM [17] introduced an efficient form of 1D\ntensor parallelism. For a given computational task, it in-\nvolves two GEMM operations and a GeLU non-linearity:\nY = GeLU( XA), Z = Y B. A can be partitioned into\u0002\nA1, A2\n\u0003\n. So each processor can independently compute\nthe Yi:\n[Y1, Y2] = [GeLU (XA1) ,GeLU (XA2)] .\nThe second weight matrix B can be split into\n\u0014\nB1\nB2\n\u0015\n,\nso Z is equal to\n\u0002\nY1, Y2\n\u0003 \u0014\nB1\nB2\n\u0015\n. With this approach,\nYiBi can be computed separately on individual processors.\nIn Transformer models, the method of 1D tensor parallelism\nis effectively applied to the computation of multi-head atten-\ntion. This allows multiple processing units to simultaneously\ncalculate different attention heads without waiting for the\nresults of others.\nOptimus [18] proposed an efficient and scalable 2D tensor\nparallelism. It is introduced based on the scalable universal\nmatrix multiplication algorithm (SUMMA) [19]. Compared\nto 1D tensor parallelism, 2D parallelism distributes the com-\nputational load across more processing units, significantly\nenhancing overall computational efficiency. Although 2D\nparallelism offers a more fine-grained model partitioning\napproach, it can introduce higher communication overhead.\nTo solve this problem, Tesseract [20] introduces the 2.5D\ntensor parallelism mechanism, which is based on the de-\nvelopment of the 2.5D matrix multiplication [21]. Tesseract\nalso develops different parallelization schemes for the ma-\ntrix multiplication portion and the non-matrix multiplication\nportion, respectively, which minimize the communication by\nleveraging additional device requirements, thereby reducing\nthe communication overhead and lowering the memory re-\nquired per GPU.\nTo balance computation, memory, and communication\nloads effectively, 3D tensor parallelism [22] employs a 3-D\nParallel Matrix Multiplication algorithm to accurately map\nand execute the computation process of the Transformer\nmodel. This algorithm optimizes the use of computational\nresources by intelligently distributing and computing differ-\nent parts of the input and weight matrices on designated\nprocessors.\n3) Pipeline Parallelism: Enhancing Foundation Model\nScalability\nIn pipeline parallelism (Figure 3c), the entire model is\ndivided into several stages, with each part allocated to\nan independent GPU. However, a typical issue in pipeline\nparallel processing is the idle time created due to waiting\nfor dependent data or processing results, commonly referred\nto as the bubble phenomenon. Therefore, effectively reduc-\ning these bubbles to enhance GPU utilization in pipeline\nparallelism becomes a critical issue.\nGPipe [23] is one of the first significant works to apply\nthe concept of pipeline parallelism to the training of foun-\ndation models. However, GPipe requires waiting for each\nmicro-batch to complete forward propagation before starting\nbackward propagation, as shown in Figure 4a. Therefore,\nthe intermediate results (activations) produced during the\nforward computation of each micro-batch need to be cached\nin memory for subsequent backpropagation, resulting in\nincreased memory usage. Meanwhile, this approach can also\nlead to the creation of a significant number of bubbles.\nSo PipeDream [24] utilizes a one-forward-one-backward\n(1F1B) strategy, as shown in Figure 4b, to solve these\nproblems, in which the backward propagation process im-\nmediately follows the completion of forward propagation\nfor a micro-batch. Varuna [25] improves upon PipeDream\nby performing recomputation earlier during the backward\n4 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\npass, effectively reducing bubbles and memory usage. How-\never, the training mode based on PipeDream introduces\ntwo types of parameter inconsistency. PipeDream utilizes\nweight stashing and vertical sync methods to address these\nissues. Instead, DAPPLE [26] performs synchronization after\ncompleting the forward and backward propagation for micro-\nbatches. This synchronization ensures the consistency of\nmodel parameters across micro-batches, avoiding parameter\ninconsistency issues.\nPipeDream utilizes a weight storage scheme to use the\nsame weight version in forward and backward propagation\nfor the same input. In the worst case, the number of stored\nweight versions equals the pipeline depth. Therefore, this can\nresult in increased memory consumption. PipeDream-2BM\n[27] maintains only two versions of model weights within\nthe pipeline. By storing only two versions, it significantly\nreduces memory usage. In Megatron-LM [17], pipeline par-\nallelism is implemented using an Interleaved Schedule ap-\nproach. To reduce pipeline bubbles, the Interleaved Schedule\nmethod assigns each device to two sets of model chunks,\nallowing each device to handle multiple stages. By utilizing\nthe idle time of devices that would otherwise be waiting for\nbackward computation, it can perform forward computation\nfor the second set of model chunks, effectively reducing the\nsize of bubbles in the pipeline. In contrast, Chimera [28]\nutilizes a bidirectional pipeline by deploying multiple stages\nof the model on a single GPU. Chimera minimizes idle\ntime and maximizes GPU utilization by interleaving the\ncomputation of forward and backward propagation across\ndifferent stages. Different from Chimera, Hanayo [29] avoids\nthe strategy of model replication. Instead, it employed a\nwave-like pipeline scheme, further reducing bubble rates and\nenhancing performance. Similarly employing a bidirectional\npipeline, MixPipe [30] achieves a better balance between\npipeline and device utilization by adjusting the number\nof micro-batches. Additionally, MixPipe designs a hybrid\nscheduling strategy that combines 1F1B and 2F1B to achieve\na more balanced memory usage, further decreasing bubble\nrates. To further reduce bubbles, AvgPipe [31] employs the\napproach of multiple parallel pipelines, with each pipeline\nhandling a batch of data in each iteration. It trains par-\nallel models using an elastic averaging training method.\nBy processing more batches, AvgPipe can subdivide each\nbatch into finer-grained micro-batches, effectively reducing\nthe generation of bubbles.\nIn traditional pipelines, feeding a batch of samples with\nequal lengths into the GPU for training is common. In\neach batch, padding is applied to the input sequences to\naccommodate the length of the longest sequence, leading\nto evident memory wastage. Dynapipe [32] introduces a\nmethod of dynamic micro-batching, where the core idea is\nto ensure consistent sequence lengths among samples within\neach micro-batch without requiring uniformity across micro-\nbatches. This approach reduces the padding overhead in\nmicro-batch processing, effectively lowering memory con-\nFIGURE 4. Schematic diagram of pipeline parallelization and 1F1B\npipeline parallelization.\nsumption. To address memory balancing in the pipeline,\nBpipe [33] uses an activation balancing approach that ensures\nthat all GPUs can fully utilize comparable amounts of\nmemory by transferring intermediate activations between\ndifferent GPUs during training. This innovation solves the\nproblem that some GPUs may face high memory pressure\nwhile others fail to fully utilize the performance.\nThe continuous growth of the foundation’s scale has\ntriggered a substantial demand for training resources. In\naddressing this issue, Bamboo [34] significantly reduces\ntraining costs by using preemptible instances optimally.\nWhen idle, these instances are available at a lower cost\nbut may be preempted once users submit priority requests.\nBamboo optimizes the training pipeline by introducing re-\ndundant computations to overcome this challenge. Specifi-\ncally, each node performs computations not only on its layer\nbut also on adjacent layers. Bamboo cleverly incorporates\nthese additional computations into redundant layers, thus\nproviding greater flexibility at a lower cost. The pipeline\nmethods previously discussed primarily involve a simplistic\npartitioning of a model’s adjacent layers. This approach\ncan lead to imbalanced workload distribution across GPUs.\nAs an improvement, FTPipe [35] introduces mixed-pipe\npartitioning technology. It employs a heuristic algorithm to\nallocate GPUs based on any computational blocks in the\ncomputation graph, not just adjacent layers.\n4) Expert Parallelism: Enhancing Specialized Computing\nCapabilities\nExpert parallelism, as depicted in Figure 3d, involves seg-\nmenting a specific part of a model into several specialized\nsub-models, referred to as experts, and distributing them\nacross various computational devices. A gating network is\nused to determine how to allocate input data efficiently\namong these different experts.\nGoogle’s GShard [36] introduces the MoE [37] structure\nfor the first time in training foundation Transformer-based\nmodels, aiming to address scalability issues in foundation\nmodel training. To optimize the performance of the MoE\nmodel, FastMoE [38] was proposed. It is the first high-\nVOLUME , 5\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nperformance MoE open-source system that supports the\nPyTorch [39] framework. FastMoE pairs the FFN layer in\nthe Transformer and adopts a finer parallelization strategy.\nThis strategy significantly speeds up the computation of the\nFFN part of the Transformer model. During the training\nprocess of MoE systems, challenges such as dynamic load\nimbalance and congested end-to-end communication need\nto be addressed. To tackle these challenges, FasterMoE\n[40] proposes a dynamic shadowing method to handle load\nimbalances. By dynamically adjusting task allocation and\nscheduling, system resources are utilized more evenly, im-\nproving overall efficiency. DeepSpeed-MoE [41] achieves\nsignificant breakthroughs in model parameter efficiency and\nperformance cost optimization through the novel Pyramid-\nResidual MoE architecture and model compression tech-\nniques. Concurrently, DeepSpeed-TED [42] presents a novel\n3D hybrid parallel algorithm that integrates data, tensor,\nand expert parallelism to further scale the training of MoE\nmodels. In addition, it incorporates memory optimization\nduring the optimizer phase and enhances communication\nefficiency to reduce redundant data transfers.\nOn the other hand, to address the dynamic computational\nworkload of MoE models, the SmartMoE [43] system in-\ntroduces a unique expert placement strategy. Building upon\nthe classic combination of parallel strategies, this strategy\nachieves dynamic load balancing. By intelligently adjusting\nthe deployment positions of various experts in the model,\nthe SmartMoE system effectively balances the computational\nworkload and improves overall system efficiency. In dis-\ntributed data parallelism, contention may occur between the\nall-to-all communication among MoEs and the all-reduce\noperations, leading to prolonged training times. Therefore,\nLina [44] integrates tensor partitioning and pipelining to\nperform micro-operation scheduling, reducing blocking pe-\nriods in distributed training. All of the above approaches\nuse an expert-centric paradigm, keeping the expert in place\nand providing information to the expert through an all-to-all\nprocess. However, Janus [45] proposes a new data-centric\nparadigm: maintaining the data in place and moving the ex-\nperts between GPUs. Janus hides the communication time by\nscheduling the requests of fetching experts in a fine-grained\nmanner, thus reducing cross-node traffic. Moreover, Janus\ndevelops a topology-aware priority strategy, ensuring smooth\nintra-node expert exchanges without resource contention.\n5) Hybrid Parallelism: Combining the Power of Different\nParallel Computing Approaches\nAlthough various parallel technologies have shown signifi-\ncant effects in theoretical and experimental research, a single\nparallel strategy often fails to meet the growing computa-\ntional demands and complexity in actual deep learning model\ntraining. Therefore, hybrid parallelism becomes critical to\naddressing this challenge. The core of hybrid parallelism lies\nin its ability to make customized strategy choices based on\nthe specific requirements of the task and available hardware\nresources, thereby maximizing training efficiency while en-\nsuring model performance.\nCombining multiple parallelization techniques for en-\nhanced efficiency is common when conducting pre-training\nof foundation models with parameter scales in tens to\nhundreds of billions. Smith et al. [46] utilized a combination\nof pipeline and tensor parallelism techniques to parallelize\nthe Transformer block in Megatron-Turing NLG during their\ntraining using DeepSpeed [47] and Megatron-LM. They\nexpanded the training scale by incorporating data parallelism,\nallowing for training on more GPUs. To simplify the appli-\ncation and enhance the efficiency of parallelization strate-\ngies, Alpa [48] integrates all parallelization strategies into a\nsingle framework, establishing a compiler that automatically\ngenerates optimal parallelization strategies. Similarly, Galva-\ntron [49] introduces a decision tree approach that leverages\nlogical intuition for pruning, thereby significantly cutting\ndown the search space. In addition, Galvatron employs a\ndynamic programming search algorithm to determine the\nmost effective hybrid parallelization strategy.\nB. GPU Memory Optimization In Training\nAs the model size increases, the demand for GPU mem-\nory grows exponentially. However, limited by hardware\nresources, insufficient GPU memory often becomes a bottle-\nneck, restricting the scale and performance of the training.\nTherefore, developing effective GPU memory optimization\ntechniques is essential to reduce memory consumption.\nSubsequent sections will explore various innovative GPU\nmemory optimization techniques targeting these overheads.\n1) Checkpointing and Recomputation for Memory Efficiency\nIn foundation model training, activation checkpointing tech-\nnology reduces memory consumption by only saving key\nactivation values and uses recomputation technology to re-\ngenerate these values during backpropagation.\nIt was Chen et al. [50] who first proposed the con-\ncept of activation checkpointing to tackle the high mem-\nory consumption in foundation model training. By selec-\ntively removing unneeded intermediate activations in the\nforward propagation process and reconstructing them during\nbackward propagation through additional computations, this\nmethod significantly reduces GPU memory usage while\nallowing for the training of more extensive networks. How-\never, recomputation imposes an additional time overhead.\nTherefore, it requires a trade-off between training time\nand memory requirements. To address this problem, Jain\net al. proposed the Checkmate [51], which models the\nproblem to minimize computation time while ensuring that\ntask scheduling does not exceed the memory limit of the\ndevice. The Checkmate effectively manages memory usage\nby dynamically determining when to store activations and\nrecompute them. This enables the training of larger-scale\n6 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nnetworks within the constraints of limited memory resources,\nproviding an effective solution to address memory limitations\nin foundation model training.\n2) Optimizing with Mixed Precision Training\nMixed Precision Training [52] is a technique used in founda-\ntion models that simultaneously employs both low-precision\nand high-precision data types. Representing the training data\ntypes as 16-bit floating-point numbers can reduce the amount\nof computation while lowering the memory requirement.\nHowever, the 16-bit floating-point representation will in-\nevitably impact model convergence. Jia et al. [53] utilized the\nLARS algorithm [54] to solve this problem. The algorithm\nworks by using different learning rates for different layers.\nHowever, the test found that applying the LARS algorithm to\nthe training of half-precision models directly caused a great\nloss of accuracy. This is because after multiplying by the\nLARS coefficients, many parameters directly go to zero due\nto the small range of the half-precision values, so Jia et al.\nconverted the half-precision parameters into single-precision\nand then combined them with the LARS.\n3) Memory Swapping Techniques in Optimization\nThe basic idea of memory swapping technology is to offload\nthe computational burden from the GPU to other devices,\nsuch as CPUs or NVMe. It migrates some model parameters\nand computational tasks from the GPU to other devices. This\nrelieves the GPU’s workload and enables it to handle the\nremaining computational tasks more efficiently.\nThis idea was first introduced in vDNN [55], which aims\nto reduce the pressure on the GPU memory by moving data\nthat does not require immediate access from the GPU to the\nCPU memory. The implementation of vDNN represents an\ninitial application of swapping technology. However, with\ntechnological advancements, more sophisticated methods\nhave emerged. SwapAdvisor [56] employs genetic algo-\nrithms to automatically search for the best data transfer strat-\negy as an alternative to manual judgment-based approaches.\nThe benefit of this automated approach is that it reduces the\nneed for human intervention, thereby increasing efficiency.\nIn contrast, Autotm [57] uses an integer linear programming\napproach to search for suitable transfer strategies.\nStronghold [58] introduces a work window method, which\nkeeps only part of the model’s layers and parameters in the\nGPU. Under this mechanism, the GPU processes only the\nmodel layers within the work window, transferring the rest\nto the CPU. The corresponding resources are only moved\nfrom the CPU to the GPU when the work window shifts.\nAdditionally, Stronghold models the window size, and lever-\nages computation and communication overlap to hide the\ncommunication costs between the CPU and GPU effectively.\nMeanwhile, FlashNeuron [59] considers that offloading data\ndirectly to the CPU might interfere with other tasks running\non the CPU and thus uses SSDs for data offloading and\nprefetching. DeepUM [60] enhances Unified Memory (UM)\nby incorporating prefetching techniques, effectively reducing\nthe additional overhead caused by address translations and\npage faults. Similarly, G10 [61] innovatively extends the\nUnified Memory of GPUs, amalgamating GPU memory,\nhost memory, and flash memory into a unified memory\nspace. This fusion is achieved by storing flash memory page\naddresses in the UM page table. Consequently, a unified page\ntable can point to host, GPU, or flash memory addresses.\nBy preemptively analyzing the lifecycle of tensors, G10\nenables efficient tensor swapping when needed, maximizing\nthe overlap between GPU computation and tensor migration.\nFurthermore, Patrickstar [62] proposes a memory manage-\nment method based on chunks, a series of consecutive tensors\nof the same size. This method is similar to storing files in\nfixed-sized disk blocks in a distributed file system. During\ntraining, chunks with different lifecycles can share memory,\nreducing memory usage. Additionally, Patrickstar collects\nmemory usage information during the warm-up iteration\nphase to optimize memory management.\nBeyond the methods above, other works like ZeRO-\nOffload and ZeRO-Infinity have also employed Memory\nSwapping Techniques. To comprehensively introduce the\nZeRO series of research, this paper includes these additional\nworks in the next section.\n4) Zero Redundancy Optimizers\nMicrosoft has developed a technology called Zero Re-\ndundancy Optimization (ZeRO) [63] as the core of the\nDeepSpeed distributed training framework. The core idea of\nZeRO is to reduce the GPU memory by sacrificing some\nof the communication overhead. ZeRO divides the model\nparameters, gradients, and optimizer states into multiple\nparts, with each GPU maintaining only a portion of them\nduring training and obtaining the rest when needed through\nan AllGather operation. Building upon the foundation laid\nby ZeRO, ZeRO-Offload [64] leverages the idea of Hetero-\ngeneous DL training [65] to alleviate the pressure on GPU\nmemory by effectively utilizing CPU memory. It divides the\nmodel parameters into two parts. One part of the parameters\nis kept in GPU memory for efficient computation during\nforward and backward propagation. The other part of the\nparameters is offloaded to CPU memory and accessed when\nneeded. Further advancing these concepts, ZeRO-Infinity\n[66], similar to ZeRO-Offload, leverages GPU, CPU, and\nNVMe memory to enable the training of foundation models\non limited resources without the need for code refactoring.\nWith ZeRO-Infinity, the model parameters and gradients are\nstill computed on the GPU, while the optimizer state and\nactivations are offloaded to more suitable NVMe memory\nand CPU, respectively.\nVOLUME , 7\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nC. Communication Optimization\nAs demonstrated in the previous sections, communication\noverhead is a significant bottleneck in the distributed training\nof foundation deep learning models. This issue is especially\npronounced when synchronizing model parameters, gradi-\nents, and optimizer states across multiple GPUs or nodes.\nThe mainstream solutions focus on reducing the amount\nof communication, optimizing communication patterns, and\nenhancing the overlap between computation and communi-\ncation.\nFor instance, Gan et al. developed an MPI-style communi-\ncation library called Bagua [67]. The library provides a series\nof flexible and modular primitives to support state-of-the-art\nsystem relaxation techniques of distributed training. Bagua\nachieves efficient implementation and scalability through this\ndesign for various cutting-edge distributed learning algo-\nrithms. The method proposed by Wang et al. [68] involves\ndecomposing the original communication and computational\noperations into more fine-grained tasks, thereby achieving\nan overlap between communication and computation that\neffectively reduces data communication overhead. Mobius\n[69] introduces a pipeline strategy for heterogeneous mem-\nory, which overlaps communication with computation by\nprefetching data from the CPU to the GPU memory for the\nnext stage. Additionally, it employs a Cross-mapping strat-\negy to reduce communication contention, further optimizing\noverall performance. Simultaneously, Out-Of-Order Back-\nProp [70] maximizes the overlap between communication\nand computation by optimizing the sequence of computing\noutput gradients, weight gradients, and parameter updates.\nZeRO achieves parallel computation by distributing model\nweights, gradients, and optimizer states across multiple\nGPUs, increasing communication volume and frequency. As\nan improvement, ZeRO++ [71] employs weight quantization,\nmeaning model parameters are compressed into smaller\ndata types (such as INT8) in real-time before communica-\ntion, reducing the required communication bandwidth and\ntime. Moreover, ZeRO++ maintains a complete model copy\non each machine, enhancing intra-machine communication\nbandwidth. COCKTAILSGD [72] integrates various commu-\nnication compression techniques, cleverly overlapping com-\nmunication with local gradient computation. During the com-\nmunication steps, it combines three different compression\ntechniques (random sparsification, top-K sparsification, and\nquantization) to achieve more excellent compression than\neach method individually. Lastly, Optimus-CC [73] utilizes\nthree techniques: compression of back-propagation gradients,\nmerging of embedding layer synchronization operations, and\nselective phase compression to reduce inter-node commu-\nnication volume. Optimus-CC selectively compresses based\non the communication needs of different training stages,\nthus minimizing unnecessary communication overhead and\nenhancing overall training efficiency.\nIV. MODEL SERVING\nThis section discusses five principal areas of optimization\nin foundation model serving systems: batch processing opti-\nmization, sparse acceleration techniques, resource scheduling\noptimization, GPU memory optimization, and multi-model\ninference (As shown in Table 2). It presents various innova-\ntive techniques and technologies designed to enhance pro-\ncessing efficiency, minimize latency, and improve memory\nusage. These strategies are categorized within the “network-\ncomputing-storage” optimization framework.\n• Network optimization is accomplished through effi-\ncient batch processing and resource scheduling, opti-\nmizing data flow and task execution.\n• Computing optimization is characterized by multi-\nmodel inference, enabling the efficient utilization of\ncomputational resources.\n• Storage optimization involves GPU memory man-\nagement and the application of sparse acceleration\ntechniques, collectively reducing memory footprint and\ncomputational overhead.\nIntegrating these “network-computing-storage” principles\nensures a comprehensive optimization approach, which is\ncrucial for the performance of foundation model serving\nsystems.\nA. Batch Processing Optimization\nBatch processing allows models to handle multiple requests\nefficiently by grouping input data into batches. This method\nallows for more efficient use of computational resources\nby leveraging parallel processing capabilities, significantly\nimproving the throughput and reducing the latency of model\ninferences. DV ABatch [74] proposes a multi-entry and multi-\nexit strategy that employs operations like new, split, and\nstretch to dynamically customize batch sizes for different\nstages of the model, thereby optimizing efficiency, through-\nput, and reducing latency. In addition to this approach,\nOrca [75] introduces a selective batching mechanism that\nstrategically applies batch processing and padding to fully\nconnected layers, maximizing efficiency. Simultaneously,\nit refrains from applying this method to attention layers,\nminimizing memory overhead. Furthermore, Orca presents\nan iterative-level scheduling strategy that offers adaptability\nby enabling batch size adjustments after each processing\niteration. The evaluation of Orca on Azure’s high-end hard-\nware may not necessarily apply to typical environments with\nlimited resources. On the other hand, DV ABatch’s testing on\nNVIDIA Titan RTX GPUs, despite being more accessible,\nfailed to consider multi-GPU configurations.\nB. Sparse Acceleration Techniques\nSparse acceleration techniques play a crucial role in op-\ntimizing the performance of Transformer-based foundation\nmodels when faced with limited computational and memory\nresources. These methods leverage the inherent sparsity in\n8 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 2. Summary of Optimization Techniques in Foundation Model\nServing\nMethod/Framework Main Features Year\nBatch Processing Optimization\nDV ABatch [74] Dynamic Batching 2022\nOrca [75] Selective Batching 2022\nSparse Acceleration Techniques\nSparseAttention [76] Sparse attention masks 2023\nDeja Vu [77] Use MLP to predict sparsity 2023\nH2O [78] Sparse KV Cache 2023\nSTI [79] Sharded Models 2023\nOliVe [80] Outlier Quantization 2023\nResource Scheduling Optimization\nClockwork [81] Latency Predict 2020\nDeepSpeed Inference [82] Integrated Scheduling 2022\nREEF [83] Real-Time Scheduling 2022\nAlphaServe [84] Automated Model Parallelism 2023\nFastServe [85] Preemptive Scheduling 2023\nSHEPHERD [88] Predictive Load Management 2023\nOSML [89] Predictive Resource Allocation 2023\nGPU Memory Optimization In Inference\nGpulet [90] Virtual GPU Partitioning 2022\nFlexGen [91] Zig-Zag Block Scheduling 2023\nDHA [92] Direct GPU Access 2023\nvLLM [93] PageAttention Mechanism 2023\nMulti-Model Inference\nPetS [94] Selective Model Sharing 2022\nTabi [96] Multi-Level Model Inference 2023\nSpeculative Decoding [97] Speculative decoding 2023\nLLMCad [98] Model collaboration 2023\nmodel parameters, attention mechanisms, and KV Cache to\nprioritize computation and storage. By focusing on the most\ninfluential components of the model and reducing the over-\nhead on less critical areas, sparse acceleration approaches\nenable high model performance while accommodating de-\nployment in resource-constrained environments.\nAddressing the computational intensity of self-attention\nmechanisms in Transformers, Dai et al. [76] introduce a\nmethod that leverages the intrinsic sparsity within self-\nattention matrices. The method applies structured sparse\nattention masks, refined through entropy-aware fine-tuning,\nto concentrate computation on crucial attention parameters\nextracted from the model’s attention distribution. These\nattention masks shape the parameters into patterns such as\nblocks or stripes, enhancing computational efficiency. An-\nother notable work is Deja Vu [77], which offers a strategic\nsolution by leveraging the concept of contextual sparsity.\nThe proposed approach effectively identifies and activates\nselective attention heads and FFN parameters that are crucial\nin processing the given input. The Deja Vu utilizes an MLP\nto predict the critical attention heads and FFN parameters\nprecisely. In cases where the KV Cache retrieval fails, it\ntriggers a recomputation process. Another relevant work is\nthat the H2O [78] method suggests that in attention blocks,\nthe cumulative attention scores of tokens follow a power-law\ndistribution, with only a few tokens playing a pivotal role in\ngeneration. To conserve memory, H2O retains only the KV\nCache for these pivotal tokens, which are identified as highly\ncontributive KV Cache based on their elevated cumulative\nattention scores. This approach substantially reduces the\nmemory requirement while retaining the essential KV Cache\nof the attention mechanism.\nIn low-resource scenarios on edge devices, both computa-\ntion and memory are constrained. STI [79] partitions the\nmodel into manageable shards. To balance accuracy and\nlatency, a central scheduler dynamically allocates shards at\noptimal precision levels, taking into account their importance\nand resource availability. In the realm of model quantization\nacceleration, OliVe [80] presents the concept of Outlier-\nVictim Pair (OVP) quantization. This approach recognizes\nthe importance of outliers for model accuracy, identifying\nadjacent normal values, termed ’victim values’, that can be\npruned without significant performance degradation. OliVe\nstrategically retains outliers while selectively pruning adja-\ncent normal values, aligning this approach with hardware\ndesign principles. Studies such as Deja Vu demonstrate that\nmodels can maintain high accuracy even when subjected\nto 75% sparsity, indicating that performance is not directly\nproportional to sparsity levels. Furthermore, the strategic\nimplementation of sparsity, as seen in the STI study, can de-\ncrease computational requirements while preserving or even\nimproving model performance. This emphasizes the potential\nof sparsity in optimizing performance within resource limi-\ntations without significantly impacting task-specific results.\nC. Resource Scheduling Optimization\nEffective resource scheduling is essential in optimizing ser-\nvice delivery. To effectively manage variable workloads in\nGPU-based inference services, several systems incorporate\ntechniques such as multi-GPU optimization, dynamic task\ndistribution, and advanced scheduling algorithms. These\nmethods ensure adherence to Quality of Service (QoS) by\nminimizing latency through operator fusion, adapting to\ndemand surges through model parallelism, and employing\npreemptive and adaptive queuing to maintain throughput.\nDeepSpeed Inference [82] offers a multi-GPU inference\nsolution designed to handle foundation models while ad-\nhering to limited GPU memory constraints. DeepSpeed\nInference presents a tailored pipeline-parallel schedule for\nautoregressive decoders, which effectively reduces latency\nwhile harnessing the combined power of GPU, CPU, and\nNVMe storage resources. Additionally, it incorporates oper-\nator fusion within Transformer modules to optimize mem-\nory utilization and improve overall throughput. AlphaServe\n[84] utilizes model parallelism in order to distribute the\ninference of foundation models across multiple GPUs. This\napproach effectively mitigates memory limitations and re-\nVOLUME , 9\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\nduces latency. Additionally, AlphaServe implements a two-\nlayer algorithm that facilitates efficient cluster distribution\nand ensures compliance with service-level objective (SLO)\nrequirements. These processes are automated within the Al-\npha framework. Taking optimization further, FastServe [85]\nemploys preemptive scheduling with a skip-join Multi-Level\nFeedback Queue to reduce job times and minimize request\nwait times. This approach efficiently directs jobs to the most\nsuitable queue, avoiding unnecessary transitions and delays.\nHowever, despite these sophisticated approaches, Current\nmethods [86], [87] that make decisions for each request\nindividually often result in GPU over-provisioning during\nshort-term high loads, leading to low resource utilization.\nTo address this issue, Shepherd [88] improves predictability\nby batching individual requests and employs a two-stage\nalgorithm. This algorithm utilizes load data to partition the\nGPU cluster for service groups and incorporates preemptive\nscheduling to prioritize large batches that meet the service-\nlevel objective (SLO) for optimized throughput.\nIn scenarios with strong latency requirements, some works\nhave addressed the issue. Clockwork [81] ensures predictable\nDNN inference times, combating tail latency from diverse\ntasks, hardware, and inputs to satisfy SLOs and minimize\ndelays. By limiting options at each computational layer\nfor uniform execution times and deploying a central con-\ntroller that assigns tasks with known durations, Clockwork\nmaintains strict timing assurances. REEF [83] is a sys-\ntem designed for efficient and timely DNN inference on\nGPUs. It schedules tasks in a way that prioritizes real-time\ntasks, quickly interrupts other tasks if needed, and allocates\ncomputing units first to real-time kernels, then distributes\nthe remaining units to best-effort kernels. And OSML [89]\npredicts QoS fluctuations by analyzing architectural metrics.\nIt consists of three models: Model A is used to allocate\nresources and detect scenarios where resources are running\nlow, referred to as ”resource cliff” situations. Model B\nreallocates resources to prioritize QoS-sensitive services,\nwhile Model C makes real-time adjustments to allocation\nto ensure sustainable service performance.\nD. GPU Memory Optimization In Inference\nDuring the process of inference, the weight parameters of a\nmodel significantly consume GPU memory. Various studies\nhave concentrated on optimizing these model parameters.\nFor instance, FlexGen [91] is a throughput-oriented gener-\native inference system that optimizes offloading strategies.\nA standout characteristic of FlexGen is its zig-zag block\nscheduling strategy. The zig-zag block scheduling strategy\nexplores the computation graph by advancing column-by-\ncolumn and reusing weights within each column to minimize\nloading times. When the memory limits for activations\nare reached, the process transitions to the next column,\noptimizing GPU memory utilization and efficiently process-\ning the model through a zig-zag pattern. Additionally, it\ndynamically loads and unloads activation values and KV\nCache as required. In another study, Jeong et al. [92] utilized\nDirect-Host-Access (DHA) for direct GPU memory access,\nreducing latency for layers like the embedding layer. They\nalso applied Parallel Model Transmission, dividing the model\nper GPU for parallel loading via PCIe. The sections are\nthen quickly transferred to the primary GPU using NVLink,\noptimizing layer execution.\nInference of foundation models faces challenges when\nit comes to GPU memory limits, particularly due to the\nincreased size of the KV Cache as token counts grow,\nwhich can lead to potential memory overflows. To address\nthis issue, frameworks often limit iteration lengths and pre-\nallocate memory for the KV Cache. However, these measures\ncan result in memory fragmentation and hinder the efficiency\nof inference. Several techniques have been proposed to op-\ntimize this aspect. The PageAttention mechanism proposed\nby vLLM [93] addresses the issues of GPU memory over-\nallocation and fragmentation. It accomplishes this by emu-\nlating OS page table mapping and segmenting GPU memory\ninto blocks. A block mapping table is then used to ensure\nlogically sequential but physically discrete storage. This\ndynamic approach effectively meets the demand of the KV\nCache, reducing memory fragmentation and improving infer-\nence throughput. Drawing inspiration from the virtual nature\nof operating systems, The gpulet [90] concept introduces an\nabstraction for partitioning GPUs, creating virtual GPUs that\npossess a fraction of the physical GPU resources. The pro-\nposed multidimensional search-based scheduling framework\noptimizes GPU tasks by considering data batch sizes along\nwith the temporal and spatial sharing of resources.\nE. Multi-Model Inference\nMulti-model inference involves utilizing multiple models\nfor serving. An important research question in this context\nis how to effectively combine these diverse models and\noptimize resource allocation to achieve optimal performance.\nPetS [94] introduces a multi-task Parameter Efficient Trans-\nformers (PET) framework that fine-tunes a shared core model\nwith task-specific Adapters [95], enabling unified task pro-\ncessing while saving memory and simplifying deployment.\nIn the context of hierarchical models ranging from small to\nlarge, one approach is presented by Tabi [96]. It employs\nwell-calibrated confidence scores using temperature scaling\nto determine whether a query can be promptly resolved\nusing the smaller model or if it should be escalated to the\nlarger model. For escalated queries, Tabi reduces system\noverhead by employing attention-based word pruning and a\nweighted ensemble approach. Another technique introduced\nby Google Research is Speculative Decoding [97], which\nutilizes a smaller model to generate tokens sequentially while\na larger model concurrently verifies the correctness of each\ntoken in parallel. This approach allows for the generation\nof multiple tokens in a single iteration of the larger model.\nLLMCad [98] differs from Google’s Speculative Decoding\nby employing a tree-based token generation approach that\n10 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nfacilitates the concurrent evaluation of multiple tokens. To\naccomplish this, LLMCad utilizes a smaller language model\nto construct a comprehensive vocabulary tree comprising\nvarious word paths. The larger LLM then efficiently and\nconcurrently evaluates these paths.\nIn a resource-constrained environment, maintaining per-\nformance in multi-model inference necessitates strategic re-\nsource management. One approach to achieve this is through\nthe deployment of resource isolation. Another strategy is\ndynamic allocation, which adjusts resources in real-time\nbased on usage to optimize system efficiency. A critical\nfacet of this strategy is priority scheduling, which entails\nthe dynamic adjustment of resource allocation based on\ntask urgency, business priorities, and other indicators. This\nensures that crucial tasks have sufficient resources to meet\nperformance requirements. Additionally, load balancing is\nemployed as an intelligent request distribution mechanism\nto evenly assign workload across various model instances.\nThis not only prevents individual models from becoming\noverloaded but also enhances overall resource utilization.\nV. CHALLENGE AND FUTURE DIRECTIONS\n➊ Privacy protection. Regarding privacy protection, the\nkey challenge for foundation models lies in the potential\nunauthorized collection, usage, and inadvertent disclosure\nof personal information. Future efforts should focus on\nincorporating privacy protection mechanisms into the design\nand application of models to ensure robust safeguards for\nuser data, preventing unauthorized use and disclosure threats.\n➋ Security. Foundation models exhibit a relatively weak\nability to defend against malicious attacks, making them sus-\nceptible to activities such as command injection and prompt\ninjection. Particularly in critical domains such as politics,\nmilitary, finance, and healthcare, any form of malicious\nattack could severely affect the stability of national society\nand the safety of people’s lives and property. Therefore,\nfuture efforts must focus on enhancing security measures\nfor foundation models to ensure their reliable protection in\ncritical domains.\n➌ Energy sustainability. Foundation systems face a sig-\nnificant challenge in terms of energy sustainability during\nboth training and serving. This entails a high demand for\nsubstantial computational resources, which may result in ad-\nverse environmental impacts. The key to future efforts lies in\nenhancing the energy efficiency of models and adopting more\nenergy-efficient hardware innovations. Through innovative\ngreen computing and sustainable development, these efforts\naim to make foundation model systems more environmen-\ntally friendly and efficient, reducing energy dependence and\nmitigating environmental impact.\nVI. CONCLUSION\nThe primary contribution of this paper is that it offers a com-\nprehensive view on the training and serving of foundational\nmodel systems, especially through its detailed analysis in the\naspects of networking, storage, and computing. In the train-\ning section, it discusses various parallel computing strategies.\nEach strategy has unique advantages and application sce-\nnarios. Additionally, it explores GPU memory optimization\nand communication optimization techniques. The serving\nsection discusses key technologies such as batch process-\ning, sparse acceleration, resource scheduling, GPU memory\noptimization, and multi-model inference. These strategies are\nessential for ensuring the efficiency and practicality of the\nfoundation model system in real-world scenarios. In sum-\nmary, the training and serving of foundation model systems\nis an evolving field. With the emergence of new technologies,\nit anticipates solving more challenges and further advancing\nthe field of artificial general intelligence.\nREFERENCES\n[1] Brown T, et al. ”Language models are few-shot learners.”, NeurIPS,\n2020.\n[2] Touvron H, et al. ”Llama: Open and efficient foundation language\nmodels.”, arXiv, 2023.\n[3] Ren X, et al. ”PanGu- Σ: Towards Trillion Parameter Language Model\nwith Sparse Heterogeneous Computing.”, arXiv, 2023.\n[4] https://cloudbrain.pcl.ac.cn/\n[5] Chang Y , et al. ”A survey on evaluation of large language models.”\narXiv, 2023.\n[6] Hadi M U, et al. ”Large Language Models: A Comprehensive Survey\nof its Applications, Challenges, Limitations, and Future Prospects.”\narXiv,2023.\n[7] Zhao H, et al. ”Explainability for Large Language Models: A Survey.”\narXiv, 2023.\n[8] Wang X, et al. ”Large-scale multi-modal pre-trained models: A com-\nprehensive survey.” Machine Intelligence Research, 2023.\n[9] Yin S, Shukang, et al. ”A Survey on Multimodal Large Language\nModels.” arXiv, 2023.\n[10] Zhou Y , et al. ”Vision+ Language Applications: A Survey.” CVPR.\n2023.\n[11] Zhou C, et al. ”A comprehensive survey on pretrained foundation\nmodels: A history from bert to chatgpt.”, arXiv, 2023.\n[12] Zhao W X, et al. ”A survey of large language models.” arXiv ,2023.\n[13] Vaswani A, et al. ”Attention is all you need.” NeurIP, 2017.\n[14] Li S, et al. ”PyTorch distributed: experiences on accelerating data\nparallel training.” VLDB, 2020.\n[15] Zhao Y , et al. ”PyTorch FSDP: Experiences on Scaling Fully Sharded\nData Parallel.”,VLDB, 2023.\n[16] Xu Y , et al. ”Automatic cross-replica sharding of weight update in\ndata-parallel training.” arXiv, 2020.\n[17] Narayanan D, et al. ”Efficient large-scale language model training on\ngpu clusters using megatron-lm.”, SC, 2021.\n[18] Xu Q, et al. ”An efficient 2d method for training super-large deep\nlearning models.”, IPDPS, 2023.\n[19] Van De Geijn R A, et al. ”SUMMA: Scalable universal matrix multi-\nplication algorithm.” Concurrency: Practice and Experience, 1997.\n[20] Wang B, et al. ”Tesseract: Parallelize the Tensor Parallelism Effi-\nciently.”, ICPP, 2022\n[21] Solomonik E, et al. ”Communication-optimal parallel 2.5 D matrix\nmultiplication and LU factorization algorithms.” European Conference\non Parallel Processing. Berlin, Heidelberg: Springer Berlin Heidelberg,\n2011.\n[22] Bian Z, et al. ”Maximizing parallelism in distributed training for huge\nneural networks.” arXiv, 2021.\n[23] Huang Y , et al. ”Gpipe: Efficient training of giant neural networks\nusing pipeline parallelism.”, NeurIPS, 2019.\n[24] Narayanan D, et al. ”PipeDream: generalized pipeline parallelism for\nDNN training.”, SOSP, 2019.\n[25] Athlur S, et al. ”Varuna: scalable, low-cost training of massive deep\nlearning models.”, EuroSys, 2022.\n[26] Fan S, et al. ”DAPPLE: A pipelined data parallel approach for training\nlarge models.”, PPoPP, 2021.\nVOLUME , 11\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE OPEN JOURNALS\n[27] Narayanan D, et al. ”Memory-efficient pipeline-parallel dnn training.”,\nICML, 2021.\n[28] Li S, et al. ”Chimera: efficiently training large-scale neural networks\nwith bidirectional pipelines.”, SC, 2021.\n[29] Liu Z, et al. ”Hanayo: Harnessing Wave-like Pipeline Parallelism for\nEnhanced Large Model Training Efficiency.”, SC, 2023.\n[30] Zhang W, et al. ”MixPipe: Efficient Bidirectional Pipeline Parallelism\nfor Training Large-Scale Models.” DAC, 2023.\n[31] Chen Z, et al. ”Elastic Averaging for Efficient Pipelined DNN Train-\ning.”, PPoPP, 2023.\n[32] Jiang C, et al. ”DynaPipe: Optimizing Multi-task Training through\nDynamic Pipelines.”, EuroSys, 2024.\n[33] Kim T, et al. ”BPIPE: memory-balanced pipeline parallelism for\ntraining large language models.”, ICML, 2023.\n[34] Thorpe J, et al. ”Bamboo: Making Preemptible Instances Resilient for\nAffordable Training of Large DNNs.”, NSDI, 2023.\n[35] Eliad S, et al. ”Fine-tuning giant neural networks on commodity\nhardware with automatic pipeline model parallelism.”, ATC 2021.\n[36] Lepikhin D, et al. ”Gshard: Scaling giant models with conditional\ncomputation and automatic sharding.”, ICLR, 2021.\n[37] Jacobs R A, et al. ”Adaptive mixtures of local experts.” Neural\ncomputation, 1991\n[38] He J, et al. ”Fastmoe: A fast mixture-of-expert training system.”, arXiv,\n2021.\n[39] Paszke A, et al. ”Pytorch: An imperative style, high-performance deep\nlearning library.”, NeurIPS, 2019.\n[40] He J, et al. ”FasterMoE: modeling and optimizing training of large-\nscale dynamic pre-trained models.”, PPoPP, 2022.\n[41] Rajbhandari, et al. ”Deepspeed-moe: Advancing mixture-of-experts\ninference and training to power next-generation ai scale.”, ICML, 2022.\n[42] Singh, Siddharth, et al. ”A Hybrid Tensor-Expert-Data Parallelism\nApproach to Optimize Mixture-of-Experts Training.”, ICS, 2023.\n[43] Zhai M, et al. ”SmartMoE: Efficiently Training Sparsely-Activated\nModels through Combining Offline and Online Parallelization.”, ATC\n2023.\n[44] Li J, et al. ”Accelerating Distributed MoE Training and Inference with\nLina.”, ATC, 2023.\n[45] Liu J, et al. ”Janus: A Unified Distributed Training Framework for\nSparse Mixture-of-Experts Models.”, SIGCOMM, 2023.\n[46] Smith S, et al. ”Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model.” arXiv, 2022.\n[47] Rasley J, et al. ”Deepspeed: System optimizations enable training deep\nlearning models with over 100 billion parameters.”, KDD, 2020.\n[48] Zheng L, et al. ”Alpa: Automating inter-and Intra-Operator parallelism\nfor distributed deep learning.”, OSDI, 2022.\n[49] Miao X, et al. ”Galvatron: Efficient Transformer Training over Multi-\nple GPUs Using Automatic Parallelism.”, VLDB, 2022.\n[50] Chen T, et al. ”Training deep nets with sublinear memory cost.” arXiv,\n2016.\n[51] Jain P, et al. ”Checkmate: Breaking the memory wall with optimal\ntensor rematerialization.”, MLSys, 2022.\n[52] Micikevicius P, et al. ”Mixed precision training.”, ICLR, 2018\n[53] Jia X, et al. ”Highly scalable deep learning training system with mixed-\nprecision: Training imagenet in four minutes.” arXiv, 2018.\n[54] You Y , et al. ”Imagenet training in minutes.”, ICPP, 2018.\n[55] Rhu M, et al. ”vDNN: Virtualized deep neural networks for scalable,\nmemory-efficient neural network design.”,MICRO, 2016.\n[56] Huang C C, et al. ”Swapadvisor: Pushing deep learning beyond the\ngpu memory limit via smart swapping.”, ASPLOS, 2020.\n[57] Hildebrand M, et al. ”Autotm: Automatic tensor movement in hetero-\ngeneous memory systems using integer linear programming.”, ASP-\nLOS, 2020.\n[58] Sun X, et al. ”Stronghold: fast and affordable billion-scale deep\nlearning model training.”, SC, 2022.\n[59] Bae J, et al. ”FlashNeuron:SSD-Enabled Large-Batch Training of Very\nDeep Neural Networks.”, FAST, 2021.\n[60] Jung J, et al. ”DeepUM: Tensor Migration and Prefetching in Unified\nMemory.”, ASPLOS, 2023.\n[61] Zhang H, et al. ”G10: Enabling An Efficient Unified GPU Memory\nand Storage Architecture with Smart Tensor Migrations.”, MICRO,\n2023.\n[62] Fang J, et al. ”Parallel Training of Pre-Trained Models via Chunk-\nBased Dynamic Memory Management.”, TPDS, 2022.\n[63] Rajbhandari S, et al. ”Zero: Memory optimizations toward training\ntrillion parameter models.” SC, 2020.\n[64] Ren J, et al. ”ZeRO-Offload: Democratizing Billion-Scale model\ntraining.”, ATC, 2021.\n[65] Huang C C, et al. ”Swapadvisor: Pushing deep learning beyond the\ngpu memory limit via smart swapping.”, ASPLOS, 2020.\n[66] Rajbhandari S, et al. ”Zero-infinity: Breaking the gpu memory wall\nfor extreme scale deep learning.”,SC, 2021.\n[67] Gan S, et al. ”Bagua: scaling up distributed learning with system\nrelaxations.”, arXiv,2021.\n[68] Wang S, et al. ”Overlap communication with dependent computation\nvia decomposition in large deep learning models.”, ASPLOS, 2023.\n[69] Feng Y , et al. ”Mobius: Fine tuning large-scale models on commodity\ngpu servers.”,ASPLOS, 2023.\n[70] Oh H, et al. ”Out-of-order backprop: An effective scheduling technique\nfor deep learning.”, EuroSys, 2022.\n[71] Wang G, et al. ”ZeRO++: Extremely Efficient Collective Communi-\ncation for Giant Model Training.” arXiv, 2023.\n[72] Wang J, et al. ”CocktailSGD: Fine-tuning foundation models over\n500Mbps networks.”, ICML, 2023.\n[73] Song J, et al. ”Optimus-CC: Efficient Large NLP Model Training with\n3D Parallelism Aware Communication Compression.”,ASPLOS, 2023.\n[74] Cui W, et al. ”DV ABatch: Diversity-aware Multi-Entry Multi-Exit\nBatching for Efficient Processing of DNN Services on GPUs.”, ATC,\n2022.\n[75] Yu G I, et al. ”Orca: A distributed serving system for Transformer-\nBased generative models.”, OSDI, 2022.\n[76] Dai S, et al. ”Efficient Transformer Inference with Statically Structured\nSparse Attention.”, DAC, 2023.\n[77] Liu Z, et al. ”Deja Vu: Contextual Sparsity for Efficient LLMs at\nInference Time.”, ICML, 2023.\n[78] Zhang Z, et al. ”H2O: Heavy-Hitter Oracle for Efficient Generative\nInference of Large Language Models”, ICML, 2023.\n[79] Guo L, et al. ”STI: Turbocharge NLP Inference at the Edge via Elastic\nPipelining.”,ASPLOS, 2023.\n[80] Guo C, et al. ”OliVe: Accelerating Large Language Models via\nHardware-friendly Outlier-Victim Pair Quantization.”, ISCA, 2023.\n[81] Gujarati A, et al. ”Serving DNNs like Clockwork: Performance\nPredictability from the Bottom Up”, OSDI, 2020\n[82] Aminabadi R Y , et al. ”DeepSpeed-inference: enabling efficient infer-\nence of transformer models at unprecedented scale.” SC, 2022.\n[83] Han M, et al. ”Microsecond-scale Preemption for Concurrent GPU-\naccelerated DNN Inferences”, OSDI, 2022\n[84] Li Z, et al. ”AlpaServe: Statistical Multiplexing with Model Paral-\nlelism for Deep Learning Serving.”, OSDI, 2023.\n[85] Wu B, et al. ”Fast Distributed Inference Serving for Large Language\nModels.” arXiv, 2023.\n[86] Crankshaw D, et al. ”InferLine: latency-aware provisioning and scaling\nfor prediction serving pipelines.”, SoCC, 2020.\n[87] Romero F, et al. ”INFaaS: Automated Model-less Inference Serving.”,\nATC, 2021.\n[88] Zhang H, et al. ”SHEPHERD: Serving DNNs in the Wild.”, NSDI,\n2023.\n[89] Liu L, et al. ”Intelligent Resource Scheduling for Co-located Latency-\ncritical Services: A Multi-Model Collaborative Learning Approach.”,\nFAST, 2023.\n[90] Choi S, et al. ”Serving Heterogeneous Machine Learning Models on\nMulti-GPU Servers with Spatio-Temporal Sharing”, ATC, 2022\n[91] Sheng Y , et al. ”FlexGen: High-Throughput Generative Inference of\nLarge Language Models with a Single GPU.”, ICML, 2023.\n[92] Jeong J, et al. ”Fast and Efficient Model Serving Using Multi-GPUs\nwith Direct-Host-Access.”, EuroSys, 2023.\n[93] Kwon W, et al. ”Efficient memory management for large language\nmodel serving with pagedattention.”,SOSP, 2023.\n[94] Zhou Z, et al. ”PetS: A Unified Framework for Parameter-Efficient\nTransformers Serving.”, ATC, 2022.\n[95] Lin Z, et al. ”The adapter-bot: All-in-one controllable conversational\nmodel.”, AAAI, 2021.\n[96] Wang Y , et al. ”Tabi: An Efficient Multi-Level Inference System for\nLarge Language Models.”, EuroSys, 2023.\n[97] Leviathan Y , et al. ”Fast Inference from Transformers via Speculative\nDecoding”, ICML, 2023\n[98] Xu D, et al. ”LLMCad: Fast and Scalable On-device Large Language\nModel Inference.”, arxiv, 2023.\n12 VOLUME ,\nThis article has been accepted for publication in IEEE Open Journal of the Computer Society. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/OJCS.2024.3380828\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.772040605545044
    },
    {
      "name": "Training (meteorology)",
      "score": 0.5963002443313599
    },
    {
      "name": "Medical education",
      "score": 0.3976533114910126
    },
    {
      "name": "Computer science",
      "score": 0.3694421648979187
    },
    {
      "name": "Medicine",
      "score": 0.2769601345062256
    },
    {
      "name": "Geography",
      "score": 0.14181339740753174
    },
    {
      "name": "Archaeology",
      "score": 0.06319200992584229
    },
    {
      "name": "Meteorology",
      "score": 0.04851973056793213
    }
  ]
}