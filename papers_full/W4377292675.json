{
    "title": "ACCT is a fast and accessible automatic cell counting tool using machine learning for 2D image segmentation",
    "url": "https://openalex.org/W4377292675",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2276067242",
            "name": "Theodore J. Kataras",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A4284160566",
            "name": "Tyler J. Jang",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A2765982970",
            "name": "Jeffrey Koury",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A2138251241",
            "name": "Hina Singh",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A4377957786",
            "name": "Dominic Fok",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A2129194426",
            "name": "Marcus Kaul",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A2276067242",
            "name": "Theodore J. Kataras",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A4284160566",
            "name": "Tyler J. Jang",
            "affiliations": [
                "University of California, Riverside"
            ]
        },
        {
            "id": "https://openalex.org/A2765982970",
            "name": "Jeffrey Koury",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2138251241",
            "name": "Hina Singh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4377957786",
            "name": "Dominic Fok",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2129194426",
            "name": "Marcus Kaul",
            "affiliations": [
                "University of California, Riverside"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2404695164",
        "https://openalex.org/W1535485604",
        "https://openalex.org/W2099540110",
        "https://openalex.org/W2601810315",
        "https://openalex.org/W2093530590",
        "https://openalex.org/W2036091553",
        "https://openalex.org/W1988785687",
        "https://openalex.org/W2139815913",
        "https://openalex.org/W3216149514",
        "https://openalex.org/W2136482257",
        "https://openalex.org/W2975634117",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3007268491",
        "https://openalex.org/W3196897371",
        "https://openalex.org/W2467414813",
        "https://openalex.org/W3045852227",
        "https://openalex.org/W2167279371",
        "https://openalex.org/W3003257820",
        "https://openalex.org/W2342249984",
        "https://openalex.org/W3035965352",
        "https://openalex.org/W2011301426",
        "https://openalex.org/W6912918802",
        "https://openalex.org/W2997591727",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W1817561967",
        "https://openalex.org/W2124260943",
        "https://openalex.org/W4226215591",
        "https://openalex.org/W3099878876",
        "https://openalex.org/W3103145119"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports\nACCT is a fast and accessible \nautomatic cell counting tool using \nmachine learning for 2D image \nsegmentation\nTheodore J. Kataras 1,2, Tyler J. Jang 1,2, Jeffrey Koury 2, Hina Singh 2, Dominic Fok 2 & \nMarcus Kaul 1,2*\nCounting cells is a cornerstone of tracking disease progression in neuroscience. A common approach \nfor this process is having trained researchers individually select and count cells within an image, which \nis not only difficult to standardize but also very time-consuming. While tools exist to automatically \ncount cells in images, the accuracy and accessibility of such tools can be improved. Thus, we \nintroduce a novel tool ACCT: Automatic Cell Counting with Trainable Weka Segmentation which \nallows for flexible automatic cell counting via object segmentation after user-driven training. ACCT \nis demonstrated with a comparative analysis of publicly available images of neurons and an in-house \ndataset of immunofluorescence-stained microglia cells. For comparison, both datasets were manually \ncounted to demonstrate the applicability of ACCT as an accessible means to automatically quantify \ncells in a precise manner without the need for computing clusters or advanced data preparation.\nQuantifying cells in immunofluorescent images has long been a limiting step in both time and required effort for \nthe analysis of microscopy data used in research. These selective image analysis techniques can provide valuable \nphysiological information and manual counts by trained professionals have been held up as the “gold standard” \nfor  quantification1,2.\nHere we used multiple separate observers’ complete manual counts for comparison to an automatic cell \ncounting methodology. Traditionally, an important aspect of maintaining consistency in cell quantification has \nbeen ensuring that a dataset is counted by a single observer who strives for accuracy and reproducibility while \nideally being blinded to the experimental conditions. This massively limits the speed at which cell counting data \ncan be processed, as increases in manpower do not always translate to increased speed. Manual counting can \nstruggle with reproducibility and consistency across a dataset due to human error and fatigue. Such issues can \nbe avoided by utilizing computational models which remain consistent over any number of images.\nFor that purpose we introduce here ACCT: Automatic Cell Counting with Trainable Weka Segmentation \n(TWS) hosted on GitHub at https:// github. com/ tkata ras/ Autom atic- Cell- count ing- with- TWS. git. TWS provides a \nmachine learning basis for our accessible automatic cell counting methodology, with additional image processing \npotential provided by scripts in ImageJ, Python, and  BeanShell3,4. The TWS program provides a graphical user \ninterface (GUI) for training and applying a machine learning classifier that differentiates between cell and non-\ncell pixels, which are then grouped into cell objects and counted. ACCT is built around this pixel segmentation \nto provide quantitative validation at the cellular level and assist in optimal classifier selection and application \n(Fig. 1). ACCT processes single-channel images provided by users. Images with multiple channels can be analyzed \nusing image copies showing one channel at a time and processing image sets for each channel separately.\nTwo datasets are used in this study to demonstrate performance in varied imaging contexts. The first \ndataset used is comprised of imaged microglia in mice with and without immune-and-inflammation-activating \nconditions brought on by the transgenic expression of the envelope protein gp120 of human immunodeficiency \nvirus-1 (HIV-1) 5. This model of NeuroHIV (HIVgp120tg mouse) provides an observable outcome from the \nmanual counts, an increase in microglia in the presence of HIVgp120 (referred to hereafter as Activated) versus \nthe absence of the viral protein (non-transgenic littermate control, referred to as Resting). ACCT was used to \nassess the difference in microglia cell numbers from images represented in Fig.  2. For an automatic counting \nOPEN\n1Graduate Program of Genetics, Genomics and Bioinformatics, University of California, Riverside, Riverside, \nCA 92521, USA. 2School of Medicine, Division of Biomedical Sciences, University of California, Riverside, Riverside, \nCA 92521, USA. *email: Marcus.Kaul@medsch.ucr.edu\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nFigure 1.  A visual overview of ACCT components and process. (A) Weka and a set of training images are \nemployed to create multiple classifiers through iterative training. (B) These classifiers are then evaluated in bulk \nagainst validation images and the best classifier is chosen by the user. (C) The chosen classifier is applied to the \nexperimental dataset for cell quantification, producing a set of counted images and the number of cells in each \nimage, as well as information about the cell morphology. This information includes the area, position, minimum \nand maximum intensity, circularity, skew, and more details about each cell, available on GitHub in the data \navailability section.\nFigure 2.  Images of immunofluorescence-labeled microglia before and after segmentation. An example \nof processed paired images of Iba-1 immunolabeled microglia in cerebral cortex (layer III; Upper panel) \nof wild-type, non-transgenic (‘Resting’) and of HIVgp120tg mice (‘ Activated’), and the accompanying final \nsegmentation images generated via ACCT (lower panel). Resulting object segmentations are color-coded (blue \n= true positive, red = false positive, yellow = false negative). Segmented objects from images in the same field of \nview were projected with a size exclusion minimum of 50 pixels for counting. Immunofluorescence staining and \nacquisition of images are described in previous publications and the Methods  Section17. Scale bar: 100 µm.\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nmethodology to be effective in an experimental context, it must be able to accommodate the variability in \ndata presentation resultant from experimental  conditions6. Microglia are known to undergo morphological \nchanges during activation that alter their morphology and appearance when imaged through immunofluorescent \n staining7,8. We focus on a dataset of images of cells immunofluorescence-labeled for ionized calcium-binding \nadaptor protein-1 (Iba-1) which is a cell type-specific marker and enables visualizing microglia. However, the \nmethodology and accompanying scripts allow for automatic quantification of cells in a wide array of imaging \ncontexts.\nThe second dataset used is a publicly available set of images of monosynaptic retrograde tracer stained \nneurons at 200x magnification (Fig.  3). This dataset, which we refer to as the Fluocell dataset, was used in the \ngeneration of novel additions to the U-net neural network for cell  segmentation9,10. Thus, our study tests the \nvalidity of ACCT against another published dataset.\nAccessibility. The existence of software tools for use in the life sciences does not inherently lead to an \nimprovement in  function11. The prerequisite technical knowledge to operate new software tools effectively can \ncreate barriers to novel methodologies based on their accessibility. The goal of ACCT is to reduce the barrier \nto entry for the execution of full semi-supervised imaging studies. ACCT provides the tools to leverage user \nexpertise in handcrafting training data, while providing quantitative tools to efficiently assess training accuracy \nfrom a variety of approaches. By reducing the programming knowledge required from users with GUI elements, \nACCT increases accessibility of automatic cell counting. Additionally, ACCT performs statistical analysis from \nthe counted images, reducing the technical workload and additionally increasing the tool’s accessibility.\nRelated works. There are many ways to address an image segmentation problem. This complex problem \ncenters on assigning an appropriate label for every pixel in an image. The TWS program we utilize is just one of \nseveral software tools, including machine learning implementations such as  Ilastik12 and neural nets like U-Net, \nResUNet, and c-ResUnet9,13,14.\nWe have chosen to work with  TWS4 over  Ilastik12 due to the increased breadth of default available features, \nas well as the integration with Fiji and ImageJ that streamlines automated image processing and analysis. This \nintegration with ImageJ made TWS more accessible to build upon for this and future automatic imaging tools.\nWhile programs like TWS and Ilastik provide excellent pixel segmentation with an accessible interface, there \nis an additional need to assess accuracy and performance at the cell level, rather than the pixel level. ACCT \nprovides a framework for users to accomplish this with minimal file manipulation at the command line. Ilastik’s \nsegmentation does not test models against a validation stage following training of their machine learning model, \nwhich increases the risk of overfitting to the training dataset. Thus, we compare the performance of Ilastik to \nACCT in our study.\nAdditionally, we compare the performance of ACCT against CellProfiler, which is a tool commonly used \nfor image analysis which allows users to create modular  pipelines15. This tool provides pixel level segmentation, \nalthough it does not provide automated cell counting with machine learning models without its companion tool \nCellProfiler  Analyst16. However, CellProfiler Analyst requires the users to manually modify text and database files \nFigure 3.  Image of fluorescence-labeled neurons before and after segmentation. A cropped image taken \nfrom the Fluocell dataset paired with cell segmentation (a). This depicts segmented cell objects from image \nMAR38S1C3R1_DMR_20_o reported in the publicly available Fluocell dataset segmented by c-ResUnet10 (b) \nand ACCT with classifierBayes3 (c). Resulting object segmentations are color-coded (blue = true positive, red \n= false positive, yellow = false negative). ACCT was set to filter out objects smaller than 250 pixels and greater \nthan 5000 pixels to remove noise. We applied the watershed algorithm to this dataset. ACCT correctly identified \n84.6% of the hand counts in this image and c-ResUnet 86.2%, while ACCT was correct with 86.9% of all \npredictions and c-ResUnet with 93.3%. Scale bar: 50 µm.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nin SQL, which requires user knowledge of code editors. For this reason, we do not compare against CellProfiler \nAnalyst.\nFinally, ResUNet is a convolutional neural net approach (CNN) to image segmentation and exists as a \ngeneral tool for image labeling. It was demonstrated to make effective use of training data to make accurate \ncell segmentation on images with a large variance in the number of cells, as well as the presence of non-cell \nartifacts. This is a development on U-Net, which has proven effective at bulk cell counting tasks in a variety of \ncontexts. Further, c-ResUnet is an extension of  ResUNet9. However, CNN models require high processing power \nto generate results in a reasonable amount of time, which may require accessing expensive computational centers. \nACCT is designed to be efficiently functional on commercially available consumer laptops and computers.\nMethods\nIba-1 Microglia dataset. A dataset comprised of images of Iba-1 positive microglial cells was generated \nfollowing procedures recently published by our  group17. In brief, the dataset was derived from brain sections \nof a model for HIV-induced brain injury (HIVgp120tg), which expresses soluble gp120 envelope protein in \nastrocytes under the control of a modified GFAP  promoter5. The mice were in a mixed C57BL/6.129/SJL genetic \nbackground, and two genotypes of 9 month old male mice were selected: wild type controls (Resting, n = 3) and \ntransgenic littermates (HIVgp120tg, Activated, n = 3). No randomization was performed. HIVgp120tg mice \nshow among other hallmarks of human HIV neuropathology an increase in microglia numbers which indicates \nactivation of the cells compared to non-transgenic littermate  controls17. All experimental procedures and \nprotocols involving animals were performed in compliance with National Institutes of Health (NIH) guidelines \nand approved by the Institutional Animal Care and Use Committees (IACUC) of the Sanford Burnham Prebys \nMedical Discovery Institute (SBP), The Scripps Research Institute (TSRI), and the University of California \nRiverside (UCR). The study follows ARRIVE guidelines.\nThe procedures for brain tissue harvest, immunofluorescence staining, and microscopy of microglia have \nbeen described in a recent publication by our  group17. In brief, mice were terminally anesthetized with isoflurane \nand transcardially perfused with 0.9% saline. The mouse brains were removed and fixed for 72 hours at 4◦C in \n4%  paraformaldehyde17. Brain sections were obtained using a vibratome (Leica VT1000S, Leica Biosystems, \nBuffalo Grove, IL) and cerebral cortex in 40 µm thick sagittal sections spaced 320 µm apart medial to lateral from \nbrains of each genotype. Staining was performed with rabbit anti-ionized calcium-binding adaptor molecule \n1 (Iba-1) IgG (1:125; Wako) with secondary antibody Fluorescein isothiocyanate (FITC). For quantification \nof Iba-1 stained microglia, cell bodies were counted in the cerebral cortex from three fields of view for three \nsections each per animal. Between 2 and 3 images were collected per field of view to capture as many cells \nas possible in sufficient focus for identification. Microscopy was performed with a Zeiss 200 M fluorescence \ndeconvolution microscope with a computer-controlled 3D stage and FITC filter. All images were collected using \nSlidebook software (version 6, Intelligent Imaging Innovations, Inc., Denver, CO). Images were acquired at 10X \nmagnification and pixel resolution 1280x1280 and cropped to 1280x733 pixel area to exclude irregular tissue \nedges. Representative examples are shown in Fig. 2.\nManual counts. Manual counts were performed by three observers, who were allowed to adjust the image \nbrightness to best facilitate counting accuracy. Images were collected as a Z-stack consisting of two to three \nplanes of focus 0.5 µm apart per field in order to allow the observer to confirm the presence of Iba-1 positive cell \nbodies that were only partially in focus. The plane showing most cells in focus was used as the primary plane for \ncounting. The observers used different visualization software during counting. Observer A used the Slidebook \nsoftware (Intelligent Imaging Innovations, Denver, CO) paired with the microscope and Observers B and C \nused the Fiji distribution of ImageJ 2.1.0 for manual counting. Additionally, Observer A ’s count was performed \nprior to the start of this project, and count markers were placed on images in close proximity to cell bodies for \nrapid total summation. Observer B and C placed counts within cell bodies to allow for later cell-level accuracy \nassessment. Microglia counts were normalized to area in cases where a part of the image area was unsuitable for \ncell detection, due to tissue damage or thickness irregularity (n = 3 images out of 62 total in study).\nFluocell public dataset. To further examine and develop the effectiveness of ACCT on a variety of data, we \nalso performed a cell counting study using a publicly available image  set9,10. The 283 1600x1200 pixel images were \ntaken at 200x magnification of 35 µm thick slices of mouse brain tissue with neurons stained via a monosynaptic \nretrograde tracer (Cholera Toxin b, CTb). This tracer highlighted only neurons connected to the toxin injection \nsite.\nThis dataset contains images with both high and low cell density, as well as varying amounts of noise and \nartifacts (Supplementary Fig. S2). We also observed that many images contain overlapping or touching cells. \nThe Fluocell dataset presents different challenges when compared to our Iba-1 positive microglia dataset where \ncells are more evenly distributed and the number of cells per image is more consistent. A representative example \nof Fluocell data is shown in Fig. 3.\nIn the Fluocell analysis of this data, a subset of the images were manually counted by the authors, and the \nremaining images were counted via automatically  thresholding9. Since we wish to compare ACCT to human \nplaced cell counts as ground truth labels to assess performance of our tool versus human cell counting, we \nmanually counted the entire 283 image dataset (one observer). This allows us to validate our tool against manual \nobserver cell counts rather than another automatic process. In addition, the authors of the Fluocell dataset \nwrote their own automated cell counting program using a CNN approach named c-ResUnet which builds upon \n ResUNet9. Thus, we also compare the performance of ACCT versus c-ResUnet and Ilastik on the Fluocell dataset \nwith our manual counts. We acknowledge that better classifiers may be possible to be generated for Ilastik and \n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nCellProfiler for our dataset, however we maintain that these programs lack the functionality for users to generate \nand assess multiple classifiers at large scale. Therefore, we generated fewer classifiers for these programs and \nselected the optimal classifier during training.\nAutomatic counting methodology. ACCT is open source, available on GitHub at https:// github. com/ \ntkata ras/ Autom atic- Cell- count ing- with- TWS. git. Our machine learning classifier was built using the TWS \nplugin version 3.2.34 in ImageJ 2.1.0 included in the Fiji  distribution18. In addition, the open source Python \npackages: scipy, pandas, numpy, matplotlib, imageio, and scikit-learn were used in ACCT 19–24.\nACCT allows for the selection of several different types of machine learning approaches. Machine learning \nhere refers to dynamic models trained on user specified input data to select cell pixels within an image. Users \ncan also upload additional machine learning approaches compatible with Weka if desired. For this paper, we use \nan implementation of the Random Forest approach, called Fast Random  Forest4,25. This is the default machine \nlearning approach in TWS and the following default features were used:\n• Gaussian blur\n• Sobel filter\n• Hessian\n• Difference of Gaussian\n• Membrane Projections\n• Membrane thickness = 1\n• Membrane patch size = 19\n• Minimum sigma = 1.0\n• Maximum sigma = 16.0\nWe additionally use a Bayesian Network model, which is also implemented in  Weka4. This approach, called \nBayesNet, follows a Bayesian statistical model to determine the probability that observed features are conditionally \ndependent, or caused, by the object of  interest26. For this study, we use the following parameters for Bayesian \npixel classification, in addition to the above listed features:\n• Variance\n• Mean\n• Minimum\n• Maximum\n• Median\n• Anisotropic diffusion\n• Bilateral\n• Lipschitz\n• Kuwahara\n• Gabor\n• Entropy\n• Neighbors\nNoise removal. During cell detection, small and large cellular processes or artifacts can be classified as cell \nbodies given similar enough appearance to cells. We address this noise by implementing a minimum and \nmaximum cell object size parameter when counting cells. Thus, objects outside the specified size range are \nexcluded from the automatic count. This range is empirically determined by observed cell bodies during model \ntraining and validation.\nAn additional challenge for cell detection is when two or more cells abut or overlap. This causes multiple \ncells to be identified as one large cell, so ACCT must separate these objects to increase accuracy. Thus, we \noptionally enable a watershed algorithm post pixel  segmentation27. This algorithm is used to separate objects by \ncontour, which allows for separated objects to be counted independently. We use the default implementation of \nthe watershed algorithm provided in  ImageJ27. We utilized the watershed segmentation strategy in the Fluocell \ndataset which had closely grouped cells, and compared ACCT’s performance with and without watershed to \ndemonstrate the effect of the algorithm in Supplementary Fig. S3.\nCell body detection. The machine learning models in TWS generate a probability map for each image which is a \nrepresentation of each pixel in the image as the probability that it is part of an object of interest. This probability \nis compared to a confidence threshold, which is the minimum probability a pixel must be to be considered part \nof an object. The user can set different threshold values, which affects how conservative or liberal the program \nwill be in identifying objects. By default, ACCT starts at a threshold of 0.5 which can be modified by users \nthrough the user interface. Conventionally, stricter thresholds lead to fewer false positives but also fewer true \npositives. The inverse also holds with a more relaxed threshold identifying more true positives, but also more \nfalse positives. The performance of ACCT at various thresholds is represented visually on a Receiver Operator \nCharacteristic (ROC) curve. However, some models usable with TWS in ACCT only give a binary zero or one \nfor their confidence values which prevents generation of meaningful ROC curves.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nIterative training and validation. Training on the Iba-1 microglia dataset was drawn from 10 randomly \nselected images not used in the counting analysis. These images were collected using the above described methods \nfrom mice distributed equally between experimental genotypes (Fig. 2). We note that ACCT is currently able to \nonly handle single-channel images, rather than multi-channel images. Incremental adjustments to training data \nand resulting changing pixel classification was observed in real time and the classifiers were saved sequentially.\nTo avoid overtraining, classifiers are updated a few pixels at a time with new training data and the updated \npixel segmentations on training data are observed immediately in TWS. Subsequent training data is selected to \naddress areas of incorrect segmentation. We continue this over successive iterations of classifiers, saving a version \nof the classifier after each addition of training data. This iterative classifier creation scheme continues until the \nclassifier does not appear to be improving on the data. ACCT then performs accuracy assessment on validation \ndata and ground truth markers, accounting for experimental conditions, to help the user select the classifier \niteration with the greatest accuracy and consistency across the validation dataset.\nThis strategy was applied to create multiple sequential classifiers which were subsequently applied to the \nvalidation dataset (Fig. 1, Supplementary Fig. S1). Ultimately, 25 sequential classifiers were trained on the Iba-1 \nmicroglia data. The Iba-1 microglia validation dataset was comprised of 10 images (Resting n = 5, Activated n \n= 5) from the main dataset which was then excluded from all further analyses. Cell body location specific count \nmarkers were placed in these images by Observer B, and performance was calculated via precision, recall, F1 \nscore, accuracy, as well as a Student’s T-Test of differential accuracy between the Resting and Activated images. \nACCT is also able to perform ANOV A calculations for further analyses including more than two experimental \ngroups.\nIn the Fluocell data, 10 training images were selected from the dataset to represent the variety of segmentation \nchallenges within the dataset: highly variable intensity, highly variable cell density, overlapping cell images, and \nimages with non-cell artifacts. The validation dataset was made with a different set of 10 images selected to \nrepresent a similar distribution of challenges.\nClassifier selection. True positive scores for each image are determined by the localization of manual count \nmarkers which are checked against the pixel location values of each object for the automatic counting process. \nFalse positives for each image are represented as the total number of automatically generated cell objects that \ndo not contain a single manual count. False negatives for each image are determined by the total number of \nmanual count markers that are not contained inside of an automatically generated object by the program, plus \nthe number of manual count markers inside of a single cell object in excess of one, indicating insufficient object \nseparation. As we assess accuracy based on cell location and do not differentiate between background pixels, \nACCT does not include determination of true negative cell locations. This methodology was used in Morelli \net al.9, and we calculate accuracy as TP\nTP+FP+FN .\nWe assess the performance of our classifiers using measures of precision, recall, F1 score, and accuracy. \nPrecision is the proportion of automatic counts that are correct based on manually placed markers, and recall \nis the proportion of the total manually placed cell markers that were successfully identified by the automatic \ncount. The F1 score is the harmonic average of precision and recall. The accuracy is assessed specifically as the \nnumber of true positive cell counts as a proportion of all manual and automatic counts, including false negatives. \nMultiple classifiers can be evaluated through automatically calculated statistical analysis. Statistical measures, \nsuch as mean absolute error (MAE), are additionally calculated through ACCT to evaluate the performance \nof different classifiers. We used these statistics to assess the performance of ACCT against other automatic cell \ncounting tools based on these metrics.\nThis statistical information is shown in Figs. 4 and 9. Figure 4 is a subset of the full data which can be found \nin Supplementary Table 1. Figure 9 shows selected accuracy statistics at different confidence thresholds. Classifier \nselection via the best F1 score or different weighting of precision and recall, are all valid metrics for selecting a \nclassifier. However, for this study we have selected the classifier based on the highest F1 score.\nExperimental dataset analysis. As the next step, the selected classifier is applied to the experimental \ndataset of images. This experimental dataset excludes images used in training and validation. The automated \ncounting methodology is repeated in this analysis and reports the total number of cells counted in addition to \nother statistical information per image. Morphological information about each identified cell is reported by \nACCT to users. An example of this can be found in Supplementary Table 2, which lists some of the reported \nmorphological information generated from analysis.\nSelected classifier performance audit. We additionally have the option for users to audit the \nperformance of their selected classifier. The audit requires further manual counting and is identical to how we \nassess the performance of classifiers during the validation stage. This step is intended to determine how similarly \nthe classifier performed on the experimental dataset compared to the validation set in situations where all images \nhave not been manually counted. The audit can be performed using a subset of the experimental dataset, or even \nthe whole dataset, if the user chooses to complete an entire manual count to assess model accuracy. These images \nare known as the audit set. We randomly selected 5 images each of Activated and Resting microglia experimental \nimages for the Iba-1 audit set. An audit of the Fluocell data was performed on a sample of 10 images from the \nFluocell experimental dataset  (Fig. 5).\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nResults\nMicroglia density. Iba‑1 microglia dataset. All statistical tests on Iba-1 microglia images include all \nimages except those used for training and validation (Resting n = 22, Activated n = 20). We report classifier \n10’s automatic count instead of classifier 4 because classifier 10 provided the maximum performance on the \nexperimental dataset and audit set that we observed. The significant increase in microglia density in images of \ngp120 positive (Activated) mice was consistent across the dataset via two way ANOV A ( p = 3.39E−16 ; Resting \nn = 22, Activated n = 20) (Fig. 6).\nAccounting for genotype variance within the dataset no significant differences were found between the ACCT \ncount and Observers (Observer A p = 0.060; Observer B p = 0.514; Observer C p= 0.440). Additionally, the per \nimage microglia density demonstrated significant correlations among the automatic count and all observers with \nstronger correlations in Resting microglia images (Fig. 5).\nThe following represents the total cell counts:\n• Validation: Observer A/B 1263/1380 cells over 10 images.\n• Experimental Dataset: Observer A/B/C 5158/5035/5056 cells over 42 images.\n• Audit Set: Observer A/B/C 1239/1207/1262 cells over 10 images.\nPrecision and recall. Iba‑1 microglia dataset. The overall precision and recall achieved by the TWS \nmethodology were similar in the validation dataset, experimental dataset, and audit dataset with overall \naccuracy and F1 increased in the experimental dataset compared to validation as shown for classifier 10 in \nFig.  7. However, within the experimental dataset, the TWS classifier was more conservative in the Resting \nimages compared to Observer B’s manual counts, with the automatic count having higher precision in images \nof Resting than Activated samples (Precision p = 0.007477) (Fig. 7). When compared to Ilastik and CellProfiler, \nACCT had a similar, and slightly stronger performance than both tools in each set of Iba-1 images, with Ilastik \nslightly outperforming CellProfiler. We additionally compared these tools against basic functionality that users \ncan manually select in Fiji, to illustrate how ACCT builds upon existing Fiji functionality. We used the subtract \nbackground with rolling ball, adjust threshold tools in Fiji for this analysis, with background subtraction at 25 \npixel area and pixel intensity threshold of 90. Without applying minimum and maximum object size this analysis \nresulted in near 0 precision, thus we used a 50 minimum and 1000 maximum pixel size as in the other tools. \nClassifier 10 outperforms the basic Fiji tools in most metrics except for precision. The basic Fiji application also \nnarrowly outperforms Ilastik and CellProfiler in most metrics in the Iba-1 images.\nFluocell dataset.  In contrast to the Iba-1 microglia dataset, the Fluocell dataset does not compare two \ndifferent experimental conditions. All Fluocell statistical tests include all Fluocell images except those used in \ntraining and validation (n = 263). In Fig.  8, we compared the performance of the Fast Random Forest and \nBayesNet models implemented within ACCT versus c-ResUnet, Ilastik and basic Fiji  usage9,12. Figure 8 shows \nClassifierRandomForest3 outperforming BayesNet on most statistical metrics. Additionally, the c-ResUnet \nmodel outperformed on most metrics compared to these two classifiers. In contrast to the other tools, Ilastik \nhas much greater recall than precision in the experimental and audit datasets, with ACCT and c-ResUnet \noutperforming on precision. However, Ilastik has the greatest F1 score in the experimental dataset. Basic Fiji \nFigure 4.  Summary of individual classifier performance on the Iba-1 microglia dataset during the validation \nstage. A chart of the most and least accurate three incrementally trained classifiers, ranked by F1 score of 25 \ntrained classifiers (n = 10 images). Error bars represent standard error of the mean on calculated performance \nstatistics from each image, where the statistic itself is calculated from total cells in the dataset. The parameters \nused in ACCT: 0.5 threshold, 50 minimum pixel size, and 1,000 maximum pixel size.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nmethodology struggled in much the same way as the other classifiers, with higher precision and low recall, \nit provided comparable accuracy to the ACCT classifiers on the experimental dataset. For basic Fiji we used \nbackground subtraction at 50 pixel area and pixel intensity threshold of 70. In the context of this dataset, the \nfollowing represents the total cell count:\n• Validation: 137 cells over 10 images.\n• Experimental Dataset: 3307 cells over 263 images.\n• Audit Set: 247 cells over 10 images.\nReceiver operator characteristic. ACCT automatically generates ROC curves for each trained classifier. \nThis visualizes the tradeoffs between precision and recall as well as the true positive rate and the false positive \nrate. Figure  9 demonstrates a ROC curve of ACCT classifier 10 applied to the Iba-1 microglia dataset. The \nthreshold represents the required probability from the classifier to determine if a pixel will be designated as \na cell pixel. The data represented in these graphs were generated using the scikit-learn Python library which \nperformed statistical  analysis24.\nFigure 9 demonstrates the tradeoff in which the false positive rate decreases at a faster rate than the true \npositive rate when a higher threshold is applied. For example, increasing the threshold for pixel segmentation \nin the Iba-1 dataset reduced the false positive rate compared to the default of 0.5. In this study, the 0.5 threshold \nwas used for reported calculations, as overall accuracy did not increase due to a decrease in true positive cell \nidentifications.\nDiscussion\nACCT is a step towards more accessible computation tools for cell counting and image segmentation. The main \ncurrent advantage of this strategy is the shorter time required to train and apply the automatic counting strategy \ncompared to manually counting each image. Our study demonstrates the general applicability of this tool to \nquickly explore large amounts of data.\nFigure 5.  Correlation analysis of microglial density scatter plots with regression lines for all correlative \ncomparisons between observer counts and the automatic count from classifier 10 for the experimental dataset. \nAll relationships showed significant, positive overall correlations (p and Adj. R2 = values included in the figures; \nn = 42).\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nThe training process is critical for the success of this automatic cell counting methodology and relies on \na researcher’s specific knowledge of their imaged cell type. Each image set comes with its own unique set of \nchallenges due to variability in cell and media characteristics, so providing accurate training data requires a \nfirm and consistent understanding of the images in question. ACCT is able to help users adjust for these features \nin their images. Users can select specific features to be analyzed in their selected machine learning models to \nbetter represent their image data. Since every user has different data, this added flexibility improves the ability \nfor ACCT to analyze users-specific images.\nThe results demonstrate that these ACCT performs strongest on precision, indicating that most of the ’ called’ \ncells were real cells. Recall tends to be substantially lower than precision, leading to decreased F1 scores and \naccuracy in all ACCT classifiers tested on these datasets. This indicates that these models tend to be more \nconservative than expert manual counts. However, the results indicate that when models classify an object as a \ncell, they tend to be correct based on the high precision.\nAdditionally, microglia density analysis in Fig. 6 and in Fig. 5 demonstrate that ACCT counts cells similarly \nto expert observers. Counts of all observers identified a similar mean difference in microglia density between \nFigure 6.  Mean microglia density by experimental genotype in manual and automated counts. All counting \nmethods found an increase in microglia density in Activated microglia images by two way ANOV A with \ninteraction effect and Tukey HSD post-hoc analysis. The difference between counting method and the \ninteraction effect did not display statistical significance. (Genotype: p = 3.39E−16 ; Counting Method: p = 0.096; \nGenotype:Counting Method: p = 0.224; Resting n = 22, Activated n = 20). There were not significant differences \nbetween the automatic count and Observer B and C. However, the ACCT count density trended lower overall \nthan Observer A ’s (p = 0.0599; Resting n = 22, Activated n = 20). This suggests that classifier 10 may have \nexcluded some faintly stained or not well-in-focus cells in the Resting group images that Observer A did count. \nError bars represent standard deviation.\nFigure 7.  ACCT vs Ilastik vs CellProfiler vs Basic Fiji on images of Iba-1 positive microglia. The performance \nof ACCT classifier 10 versus Ilastik, CellProfiler, and basic use of Fiji tools. The audit set is a selection of 10 \nimages, which is equal to the size of the validation image set, chosen from the experimental dataset and evenly \ndistributed between experimental groups. Error bars represent standard error of the mean on calculated \nperformance statistics from each image, where the statistic itself is calculated from total cells in the dataset. The \nparameters used by ACCT in this analysis were: a 0.5 threshold, 50 minimum pixel size, 1,000 maximum pixel \nsize for objects.\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nexperimental genotypes. ACCT correlates strongly with human cell counting results and can replicate the \ndifference between experimental conditions similar to manual counting. Thus, it is a useful tool for image \nanalysis between multiple experimental conditions.\nWe acknowledge that in the future more accurate automatic cell counting tools are likely to evolve from \nACCT or other software packages. However, currently ACCT shows strong performance while being a more \naccessible tool to researchers than all approaches which require large computer networks or computing clusters. \nIn some cases ACCT outperforms other cell counting tools, but not in every dataset. However, users may find \nthe accessibility in the ImageJ environment and speed of ACCT worth trading for the slight loss in performance \nin some image data sets.\nIn terms of computational power, all work was performed on commercially available consumer laptops such as \na Dell Inspiron 15-7559 (released Feb. 2017). Other automatic cell counting tools are often designed to make use \nof large computational resources. For example, Morelli et al. used 4 V100 GPUs to process their CNN  approach9. \nCNN based tools recommend using a cluster, or network of multiple computers, which allows access to greater \ncomputational power. However, computer clusters are not available for all researchers and they additionally \nmay require knowledge of command lines for effective utilization. ACCT is not limited by substantial computer \nspecifications, additionally it is more accessible to less command line-oriented researchers.\nReproducible results are a major concern in scientific research, and ensuring reproducibility via manual cell \ncounting can be costly and time consuming. Since ACCT stores classifiers as single model files, they are easy to \nshare and download. Thus, researchers can share reproducible results and statistical analysis of a cell counting \nstudy by sharing the model file and set of analyzed images. Since analysis generated by ACCT is stored as files \neditable in Excel, it is easy for users to share and communicate their results.\nBuilding ACCT around the graphical interface implemented by TWS broadens usability by providing \ninfrastructure for quantitative classifier validation and application in a full experimental context onto the flexible \nFigure 8.  ACCT vs c-ResUnet vs Ilastik vs Basic Fiji on the Fluocell dataset. ClassifierRandomForest3 and \nClassifierBayes3 are the third trained iterations of a Fast Random Forest and BayesNet model in ACCT, \nrespectively. The three tools’ automatic counts of the Fluocell images and basic use of Fiji tools are compared \nto our manual count of the Fluocell dataset. Error bars represent standard error of the mean on calculated \nperformance statistics from each image, where the statistic itself is calculated from total cells in the dataset. \nThe audit set is a selection of 10 images, which is equal to the size of the validation image set, chosen from the \nexperimental dataset. The parameters used are: a 0.5 threshold, 250 minimum pixel size, 5000 maximum pixel \nsize, and with the watershed algorithm applied.\nFigure 9.  An ROC curve generated by ACCT following the validation stage on Iba-1 microglia images. \nIt depicts the false positive, true positive, recall, and precision rates of classifier 10 at different confidence \nthresholds on the Iba-1 stained microglia images. Objects were filtered to a minimum size of 50 pixels and a \nmaximum size of 1000 pixels. The watershed algorithm was not applied to the Iba-1 dataset, due to consistent \ncell separation in the sample tissues.\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\nand intuitive training  apparatus4. Since ACCT makes use of existing tools for cell imaging analysis such as TWS, \nresearchers familiar with the program should find it easier to learn how to use ACCT as well.\nACCT includes accessible documentation, with an instruction manual that explains the program’s function \nand usage found on its GitHub page. Documentation is important for users to understand and learn how to \nuse software tools. Many other software tools document their components’ functions inside of the tool itself, \nrequiring users to navigate code to understand and use the tool. This is avoided by having detailed instructions \nwritten on the website for ACCT which explain the use of the tool without ever requiring the users to manually \naccess the code itself.\nACCT could also be applied more generally to image segmentation problems. While the focus of our study \nis on cell counting in the context of neuroscience, so long as an image has object characteristics that can be \nseparated from its background, ACCT is able to quantify the objects. However, more complex object shapes \nand less distinctive backgrounds may require selecting more complex models than demonstrated in this study. \nWe provide a simple example of this in Supplementary Fig. S4 using the Fast Random Forest model, which is \nan image segmented for buildings against a  field28. While not as distinct as cells, it demonstrates that ACCT is \napplicable beyond the biological context. Overall, ACCT should greatly increase the accessibility of automatic \nanalysis involving cell counting for a wide audience in neuroscience research and beyond.\nData availability\nThe Iba-1 dataset, its analysis, and the Fluocell dataset analysis during the current study is available in the \nACCT-Data-Repository, https:// github. com/ tkata ras/ ACCT- Data- Repos itory. git. The Fluocell dataset analyzed \nis publicly available as published in http:// amsac ta. unibo. it/ 6706/.\nCode availability\nACCT can be accessed and downloaded from GitHub at https:// github. com/ tkata ras/ Autom atic- Cell- count ing- \nwith- TWS. git.\nReceived: 9 June 2022; Accepted: 10 May 2023\nReferences\n 1. von Bartheld, C. S., Bahney, J. & Herculano-Houzel, S. The search for true numbers of neurons and glial cells in the human brain: \nA review of 150 years of cell counting. J. Comp. Neurol. 524, 3865–3895. https:// doi. org/ 10. 1002/ cne. 24040 (2016).\n 2. Jensen, E. C. Quantitative analysis of histological staining and fluorescence using ImageJ. Am. Assoc. Anat. 296, 378–381. https:// \ndoi. org/ 10. 1002/ ar. 22641 (2013).\n 3. Schneider, C. A., Rasband, W . S. & Eliceiri, K. W . Nih image to imagej: 25 years of image analysis. Nat. Methods 9, 671–675. https:// \ndoi. org/ 10. 1038/ nmeth. 2089 (2012).\n 4. Arganda-Carreras, I. et al. Trainable Weka Segmentation: A machine learning tool for microscopy pixel classification. Bioinformatics \n33, 2424–2426. https:// doi. org/ 10. 1093/ bioin forma tics/ btx180 (2017).\n 5. Toggas, S. M. et al. Central nervous system damage produced by expression of the hiv-1 coat protein gpl20 in transgenic mice. \nNature 367, 188–193. https:// doi. org/ 10. 1038/ 36718 8a0 (1994).\n 6. Lynch, M. A. The multifaceted profile of activated microglia. Mol. Neurobiol. 40, 139–156. https:// doi. org/ 10. 1007/ s12035- 009- \n8077-9 (2009).\n 7. Karperien, A., Ahammer, H. & Jelinek, H. Quantitating the subtleties of microglial morphology with fractal analysis. Front. Cell. \nNeurosci. 7, 3. https:// doi. org/ 10. 3389/ fncel. 2013. 00003 (2013).\n 8. Gomez-Nicola, D. & Perry, V . H. Microglial dynamics and role in the healthy and diseased brain: A paradigm of functional \nplasticity. The Neuroscientist 21, 169–184. https:// doi. org/ 10. 1177/ 10738 58414 530512 (2015).\n 9. Morelli, R. et al. Automating cell counting in fluorescent microscopy through deep learning with c-resunet. Sci. Rep. 11, 1–11. \nhttps:// doi. org/ 10. 1038/ s41598- 021- 01929-5 (2021).\n 10. Clissa, L. et al. Automating Cell Counting in Fluorescent Microscopy Through Deep Learning with c‑resunet. AlmaDL http:// amsac \nta. unibo. it/ 6706/ (2021).\n 11. Goecks, J., Nekrutenko, A. & Taylor, J. Galaxy: A comprehensive approach for supporting accessible, reproducible, and transparent \ncomputational research in the life sciences. Genome Biol. 11, 1–13. https:// doi. org/ 10. 1186/ gb- 2010- 11-8- r86 (2010).\n 12. Berg, S. et al. Ilastik: interactive machine learning for (bio) image analysis. Nat. Methods 16, 1226–1232. https:// doi. org/ 10. 1038/ \ns41592- 019- 0582-9 (2019).\n 13. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International \nConference on Medical Image Computing and Computer‑assisted Intervention 234–241 (Springer, 2015). https:// doi. org/ 10. 1007/ \n978-3- 319- 24574-4_ 28.\n 14. Diakogiannis, F . I., Waldner, F ., Caccetta, P . & Wu, C. Resunet-a: A deep learning framework for semantic segmentation of remotely \nsensed data. ISPRS Journal of Photogrammetry and Remote Sensing 162, 94–114. https:// doi. org/ 10. 1016/j. isprs jprs. 2020. 01. 013 \n(2020).\n 15. Stirling, D. R. et al. Cellprofiler 4: Improvements in speed, utility and usability. BMC Bioinform. 22, 1–11. https:// doi. org/ 10. 1186/ \ns12859- 021- 04344-9 (2021).\n 16. Dao, D. et al. Cellprofiler analyst: Interactive data exploration, analysis and classification of large biological image sets. \nBioinformatics 32, 3210–3212. https:// doi. org/ 10. 1093/ bioin forma tics/ btw390 (2016).\n 17. Singh, H. et al. A pivotal role for interferon-α receptor-1 in neuronal injury induced by hiv-1. J. Neuroinflamm. 17, 226. https:// \ndoi. org/ 10. 1186/ s12974- 020- 01894-2 (2020).\n 18. Schindelin, J. et al. Fiji: An open-source platform for biological-image analysis. Nat. Methods 9, 676–682. https:// doi. org/ 10. 1038/ \nnmeth. 2019 (2012).\n 19. Virtanen, P . et al. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nat. Methods  17, 261–272. https:// doi. \norg/ 10. 1038/ s41592- 019- 0686-2 (2020).\n 20. McKinney, W . et al. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference vol. \n445, 51–56 (2010).\n 21. Harris, C. R. et al. Array programming with NumPy. Nature 585, 357–362. https:// doi. org/ 10. 1038/ s41586- 020- 2649-2 (2020).\n 22. Hunter, J. D. Matplotlib: A 2d graphics environment. Comput. Sci. Eng. 9, 90–95 (2007).\n 23. Klein, A. et al. imageio/imageio: v2.16.1. Zenodo https:// doi. org/ 10. 5281/ zenodo. 63020 89(2022).\n12\nVol:.(1234567890)Scientific Reports |         (2023) 13:8213  | https://doi.org/10.1038/s41598-023-34943-w\nwww.nature.com/scientificreports/\n 24. Pedregosa, F . et al. Scikit-learn: Machine learning in python. J. Mach. Learn. Res. 12, 2825–2830. https:// doi. org/ 10. 5555/ 19530 \n48. 20781 95 (2011).\n 25. Breiman, L. Random forests. Mach. Learn. 45, 5–32. https:// doi. org/ 10. 1023/A: 10109 33404 324 (2001).\n 26. Friedman, N., Geiger, D. & Goldszmidt, M. Bayesian network classifiers. Mach. Learn. 29, 131–163. https:// doi. org/ 10. 1023/A: \n10074 65528 199 (1997).\n 27. Vincent, L. & Soille, P . Watersheds in digital spaces: An efficient algorithm based on immersion simulations. IEEE Trans. Pattern \nAnal. Mach. Intell. 13, 583–598. https:// doi. org/ 10. 1109/ 34. 87344 (1991).\n 28. Y an, Q., Zheng, J., Reding, S., Li, S. & Doytchinov, I. Crossloc: Scalable aerial localization assisted by multimodal synthetic data. \nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 17358–17368, https:// doi. org/ 10. 48550/ \narXiv. 2112. 09081(2022).\nAcknowledgements\nThe authors thank Dr. Nina Yuan and Dr. Deepika Bhullar for images of mouse brain tissues. This work was \nsupported by funds from the National Institute of Health NIH, R01 MH104131, MH105330, MH087332, \nDA052209 to MK.\nAuthor contributions\nT.K. and M.K. conceived of this project. T.J. and T.K. programmed the tool, performed the image analysis study, \nand wrote the manuscript. J.K. participated in image acquisition. User testing of the tool was done by T.K., T.J., \nand D.F . who provided feedback during ACCT development. Iba-1 microglia images by H.S. and manually \ncounted by T.K., H.S., and D.F .. M.K. supervised the project and edited the manuscript. All authors reviewed \nthe manuscript and approved of the manuscript before submission.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 34943-w.\nCorrespondence and requests for materials should be addressed to M.K.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023"
}