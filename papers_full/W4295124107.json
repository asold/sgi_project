{
    "title": "W-Transformers: A Wavelet-based Transformer Framework for Univariate Time Series Forecasting",
    "url": "https://openalex.org/W4295124107",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4295168018",
            "name": "Sasal, Lena",
            "affiliations": [
                "Sorbonne University Abu Dhabi"
            ]
        },
        {
            "id": "https://openalex.org/A3168754096",
            "name": "Chakraborty, Tanujit",
            "affiliations": [
                "Sorbonne University Abu Dhabi"
            ]
        },
        {
            "id": "https://openalex.org/A2744878029",
            "name": "Hadid Abdenour",
            "affiliations": [
                "Sorbonne University Abu Dhabi"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4230410911",
        "https://openalex.org/W3022122691",
        "https://openalex.org/W4378979428",
        "https://openalex.org/W3111294584",
        "https://openalex.org/W3129433613",
        "https://openalex.org/W2034032998",
        "https://openalex.org/W2903162817",
        "https://openalex.org/W4241574976",
        "https://openalex.org/W2145856394",
        "https://openalex.org/W2980994438",
        "https://openalex.org/W6749825310",
        "https://openalex.org/W4206189171",
        "https://openalex.org/W3022643593",
        "https://openalex.org/W2971724044",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964199361",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W4385763767",
        "https://openalex.org/W6795611752",
        "https://openalex.org/W3188872815",
        "https://openalex.org/W6804297678",
        "https://openalex.org/W6773071120",
        "https://openalex.org/W2036681246",
        "https://openalex.org/W4247761207",
        "https://openalex.org/W2067984959",
        "https://openalex.org/W4298352105",
        "https://openalex.org/W2035876214",
        "https://openalex.org/W2952301058",
        "https://openalex.org/W3212024433",
        "https://openalex.org/W2936479322",
        "https://openalex.org/W6802185124",
        "https://openalex.org/W2947017472",
        "https://openalex.org/W4388215482",
        "https://openalex.org/W3011590044",
        "https://openalex.org/W2811507150",
        "https://openalex.org/W3002709689",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3017052115",
        "https://openalex.org/W4387092445",
        "https://openalex.org/W2124459709",
        "https://openalex.org/W4287181557",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3122820950",
        "https://openalex.org/W2768348081",
        "https://openalex.org/W1965737908",
        "https://openalex.org/W1544613517",
        "https://openalex.org/W3132782787",
        "https://openalex.org/W2106445473",
        "https://openalex.org/W4286908664",
        "https://openalex.org/W4292483811",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W3213421678",
        "https://openalex.org/W3008964617",
        "https://openalex.org/W4283375115",
        "https://openalex.org/W638887343"
    ],
    "abstract": "Deep learning utilizing transformers has recently achieved a lot of success in many vital areas such as natural language processing, computer vision, anomaly detection, and recommendation systems, among many others. Among several merits of transformers, the ability to capture long-range temporal dependencies and interactions is desirable for time series forecasting, leading to its progress in various time series applications. In this paper, we build a transformer model for non-stationary time series. The problem is challenging yet crucially important. We present a novel framework for univariate time series representation learning based on the wavelet-based transformer encoder architecture and call it W-Transformer. The proposed W-Transformers utilize a maximal overlap discrete wavelet transformation (MODWT) to the time series data and build local transformers on the decomposed datasets to vividly capture the nonstationarity and long-range nonlinear dependencies in the time series. Evaluating our framework on several publicly available benchmark time series datasets from various domains and with diverse characteristics, we demonstrate that it performs, on average, significantly better than the baseline forecasters for short-term and long-term forecasting, even for datasets that consist of only a few hundred training samples.",
    "full_text": "Knowledge-based Deep Learning for Modeling\nChaotic Systems\nZakaria Elabid\nSorbonne Center for Artiﬁcial Intelligence\nSorbonne University Abu Dhabi\nAbu Dhabi, UAE\nZakaria.Alabid@sorbonne.ae\nTanujit Chakraborty\nDept. of Science and Engineering\nSorbonne University Abu Dhabi\nAbu Dhabi, UAE\ntanujit.chakraborty@sorbonne.ae\nAbdenour Hadid\nSorbonne Center for Artiﬁcial Intelligence\nSorbonne University Abu Dhabi\nAbu Dhabi, UAE\nAbdenour.Hadid@sorbonne.ae\nAbstract—Deep Learning has received increased attention due\nto its unbeatable success in many ﬁelds, such as computer\nvision, natural language processing, recommendation systems,\nand most recently in simulating multiphysics problems and\npredicting nonlinear dynamical systems. However, modeling and\nforecasting the dynamics of chaotic systems remains an open\nresearch problem since training deep learning models requires\nbig data, which is not always available in many cases. Such deep\nlearners can be trained from additional information obtained\nfrom simulated results and by enforcing the physical laws of\nthe chaotic systems. This paper considers extreme events and\ntheir dynamics and proposes elegant models based on deep\nneural networks, called knowledge-based deep learning (KDL).\nOur proposed KDL can learn the complex patterns governing\nchaotic systems by jointly training on real and simulated data\ndirectly from the dynamics and their differential equations. This\nknowledge is transferred to model and forecast real-world chaotic\nevents exhibiting extreme behavior. We validate the efﬁciency\nof our model by assessing it on three real-world benchmark\ndatasets: El Ni ˜no sea surface temperature, San Juan Dengue\nviral infection, and Bjørnøya daily precipitation, all governed\nby extreme events’ dynamics. Using prior knowledge of extreme\nevents and physics-based loss functions to lead the neural network\nlearning, we ensure physically consistent, generalizable, and\naccurate forecasting, even in a small data regime.\nIndex Terms—Chaotic systems, long short-term memory, deep\nlearning, extreme event modeling.\nI. I NTRODUCTION\nCenturies-old efforts to comprehend and forecast the dy-\nnamics of chaotic systems have spurred developments in\nlarge-scale simulations, dimensionality reduction techniques,\nand a multitude of forecasting techniques [1, 2]. Chaotic\nsystems describe deterministic dynamical systems that ex-\nhibit random behavior after a while and cannot be predicted\nsometimes [3, 4, 5]. These systems can be modeled with\nphysical laws to express their temporal and spatial depen-\ndencies [6, 7]. Some examples of these chaotic systems are:\ndaily weather forecasts, viral diseases (e.g., Covid-19), trafﬁc\njams, double pendulum [8], etc. Modeling chaotic systems\nhas always been an important study in various ﬁelds, from\ngeophysics [9], atmospheric physics [10], electronics [11],\nand biochemistry [12] among many others. Various dynamical\nsystems evolved in order to understand natural phenomena\nand their unpredictable behavior and how a perturbation\non initial conditions in real-world problems can randomly\nchange their outcome [2]. The goals of understanding and\npredicting chaotic systems using physical laws and data-driven\napproaches have complemented each other for the last few\ndecades. In recent years, we have observed a convergence of\nthese approaches towards modern data-driven methodologies\ndue to advances in computing power, algorithmic innovations,\nand the ample availability of data. A major beneﬁciary of this\nconvergence are physics-informed machine learning methods\n[6, 13, 14], dynamical learning with deep neural networks\n[15] and representation learning tools to study continuous\ndynamical systems [2] and molecular properties [16]. All these\nmethods aim to provide precise short-term predictions while\ncapturing the long-term trajectories of the systems. Successful\ndeep learning-based forecasting methods (such as long short-\nterm memory networks (LSTM) [17], convolutional neural\nnetworks (CNN) [18], feed-forward neural networks (FFNN)\n[19], reservoir computing (RC) [20]) can capture the evolution\nof time series in chaotic systems but usually do not use\nphysical laws in their architecture.\nChaotic systems are widely used to describe extreme events,\nsuch as natural catastrophes and weather dynamics, which\nare highly sensitive to ﬂuctuations [21]. While the weather\ncan be predicted for the near future, some random factors\nmake it impossible to forecast as time goes on. Currently,\nchaotic systems are modeled through dynamical systems [2]\n- a differential equation of the function describing the spatio-\ntemporal dependency between the systems’ variables. For\nexample, sea surface temperature is governed by mass and\nenergy conservation laws: Navier–Stokes equations and heat\ntransport equation [22]. While physics-driven approaches man-\nage to predict chaotic systems in the short-term, they fail\nto accurately forecast real-world phenomena when noise or\nperturbation is present, which is always the case in chaotic sys-\ntems [23]. In addition, due to the complexity of the dynamics,\nsome chaotic systems cannot be solved analytically and instead\nare modeled through direct numerical simulation, decreasing\nthe reliability of the prediction and increasing the uncertainty\nof the analysis of the physical processes [24]. On the other\nhand, data-driven approaches to model and forecast chaotic\nsystems using deep neural networks require large amounts\nof data to train the models. They are difﬁcult to generalize\nand mostly fail to emulate long-time dependencies when the\narXiv:2209.04259v1  [cs.LG]  9 Sep 2022\nsystem’s behavior is random. Multiple studies focused on\nmodeling chaotic systems using deep learning, especially when\nexperimental data is small, by integrating prior knowledge\n[1, 2, 14, 15].\nIncorporating physical knowledge to model real-life phe-\nnomena using deep learning models is a promising ﬁeld\nof study, potentially capable of overcoming the everlasting\nchallenges of modeling chaotic systems. Physics-informed\nmachine learning (PIML), a new variant of machine learn-\ning models, embeds prior physical knowledge in data-driven\nalgorithms through the use of differential equations [6, 14, 25].\nThis method is aptly used when a large amount of temporal\ndata is unavailable for making accurate and reliable forecasts\nof chaotic systems containing extreme events. Instead of\nentirely relying on the data to capture the complex underlying\nphenomena, it incorporates physical knowledge into data-\ndriven deep learning systems, see for detailed surveys [13, 14]\n(see also references therein). In recent seminal work, physics-\ninformed neural networks (PINN) have been introduced as a\nfunction approximator [6]. By providing several collocation\npoints, the initial and boundary conditions, PINN manages to\nprovide an accurate solution to the governing partial derivative\nequations (PDE) through a regularization term in the loss\nfunction. PINN can also be used to solve inverse problems\nwhen coefﬁcients of the PDE, initial or boundary conditions\nare rebuilt and the solution of the governing PDE is partially\nknown. Among many applications, PINN was used to simulate\nthe ﬂow in an expresso cup [26], modeling 3D temperature\ndata using physical laws (mass, momentum, and energy con-\nservation) in a fully connected network architecture. To predict\nthe effect of cloud processes on climate, authors enforced\nphysical constraints to machine precision (hard-enforcement)\nby replacing layers of a neural network with the help of\nconservation laws [27]. Another similar architecture, namely\nphysics-guided neural networks (PGNN), is used to model\nlake temperature as a spatio-temporal forecasting problem in\nwhich LSTM networks build the model using the dynamics of\nsea surface temperature [25]. More precisely, the conservation\nlaws enforce the monotonicity of density with depth and\nincorporate the dynamics by hard encoding them on a machine\nprecision in a PGNN architecture for a small data regime.\nHowever, the models described above fall short when it\ncomes to modeling chaotic systems. PINN can only be used\nto emulate data, i.e., it numerically simulates the solution to\nPDEs governing the dynamics of the system. Furthermore,\nPINN requires collocation points, boundary, and initial condi-\ntions as input. Therefore, these models (e.g., PINN) only work\non simulated data and cannot be directly generalized to real-\nworld problems in modeling chaotic systems. On the other\nhand, physics-guided architectures can forecast lake surface\ntemperature [25] since the equation governing the dynamics\nof temperature and density is of order zero and does not\nrequire their spatio-temporal derivatives. Therefore, such a\nmodel cannot be used to emulate more complex dynamics\ngoverned by non-linear partial differential equations of higher\norder.\nMotivated by the shortcomings of the aforementioned mod-\nels, we aim to design a physically consistent and generalizable\ndeep learning model for modeling chaotic systems that seam-\nlessly integrate real data and simulated data directly from the\ndynamics and their differential equations. Hence, we introduce\nthe knowledge-based deep learning (KDL) approach. Our\nproposed KDL approach learns the extreme events’ dynamics\nand temporal patterns for the observed chaotic systems. Our\nmain contributions in this paper are described as follows:\n• We propose a new framework called KDL (knowledge-\nbased deep learning), in which the network is trained on\nsynthetic data simulated from the dynamics and transfers\nthe knowledge to real-world data governed by the same\nphysical laws. Incorporating the dynamics through a\nregularization term in the loss function corresponding\nto the physical law governing the system makes the\ndeep learning framework suitable for small data regime\nproblems in chaotic systems.\n• KDL algorithm computes discrete temporal derivatives\nand includes them during the backpropagation in the loss\nfunction of the LSTM network.\n• We evaluate our proposed approach on several time series\nproblems containing extreme events and demonstrate\nsuperior performance compared to several state-of-the-art\ndeep learning models.\nII. P RELIMINARIES\nThis section discusses some necessary mathematical prelim-\ninaries on the Li ´enard-type system, followed by a summarized\nintroduction on the long-short term memory network and\ntransfer learning used in building our proposed KDL approach.\nA. Li ´enard-type System\nNonlinear oscillator systems are omnipresent in physics and\nare used to model numerous physical phenomena ranging from\natmospheric physics, condensed matter, and nonlinear optics\nto electronics, plasma physics, biophysics, and evolutionary\nbiology, among many others [3, 4, 5]. For example, Li ´enard-\ntype nonlinear oscillator systems are very present in chaos\ntheory and extreme events studies [28]. A Li´enard equation is a\nsecond-order differential equation in mathematics speciﬁcally\nused in the study of dynamical systems. For any two functions\nf and g on R, the second-order differential equation of the\nfollowing form is called a Lin ´eard equation:\nd2x\ndt2 + f(x)dx\ndt + g(x) = 0.\nLi´enard-type system has a unique limit cycle, i.e., when time\nis close to inﬁnity, at least one trajectory spirals into a closed\ntrajectory [29]. The Li ´enard-type oscillator with a periodic\nforcing, exhibits extreme events for an admissible set of\nparameter values, as\ndx\ndt = y\ndy\ndt = −αxy− γx− βx3 + fsin(ωt), (1)\nwhere α and β are the nonlinear dampings and the strength\nof nonlinearity, respectively, and γ is related to the internal\nfrequency of the autonomous system. f and ω are the am-\nplitude and the frequency of an external sinusoidal signal,\nrespectively [30]. An extremely large amplitude of intermittent\nspikings in a dynamical variable of a periodically forced\nLi´enard-type oscillator can be observed, and thus, it can be\ncharacterized as extreme events, which are rare but recurrent\nand larger in amplitude than a threshold.\nB. Long-Short Term Memory and Transfer Learning\nLong-Short Term Memory networks (LSTM) are an evo-\nlution of recurrent neural networks (RNN) conceived to\novercome the stability and speed issues in classical RNN\n[31]. They are omnipresent in speech recognition problems,\nmachine translation, image captioning, and time series fore-\ncasting, among many other applications. They are also widely\nused to model chaotic systems due to their ability to memorize\nlong-term dependencies in the training data, which helps deal\nwith the problem of vanishing gradients and the long-term\nrandomness [32]. LSTM regulates the ﬂow of sequential inputs\nthrough the memory cells of the network by employing three\nmain gates: input gate , output gate , and a forget gate that\ndecide which bits of long-term dependencies are relevant\nwhen given previous hidden state and current state input\ndata. However, the time and computational cost required to\ntrain this sequential neural network become an overarching\nproblem. Among many solutions to this problem, using pre-\ntrained models is a popular approach to decrease time, increase\nperformance, and improve the model’s generalization. One\nsuch approach is Transfer Learning (TL), which focuses on\nstoring knowledge gained from solving one problem and\napplying it to different but related problems [33]. We used TL\nmethodology in our application to transfer the knowledge from\na synthetic dataset simulated from a Li ´enard-type differential\nequation to real-world datasets, exhibiting extreme events and\nfollowing the same dynamics.\nIII. P ROPOSED METHOD : KNOWLEDGE BASED DEEP\nLEARNING (KDL)\nWe consider a memristor Li ´enard system with external\nharmonic perturbation from Eqn. 1 where the parameters\nf,α,γ,ω,β are known. We generate our synthetic dataset\nby simulating data points from the nonlinear differential\nequations using the Runge-Kutta method [34]. We train an\nLSTM network on the simulated time series while enforcing\nthe physical law on the network as a regularization term.\nThe network learns complex patterns from historical values\nand the physical distribution of the target values. In the\nstandard PINN model, the time index is considered an input,\nand the output is the solution of the differential equation.\nComputing the regularization term amounts to differentiating\nthe multilayered perceptron (MLP) network and computing the\ntime derivatives using auto-differentiation [35]. However, time\nseries are discrete observations and there is not a substantial\nmathematical equation governing the observed variables in\ntime, and subsequently, it is difﬁcult to compute the derivatives\nin time. To overcome this, we compute discrete derivatives of\nthe time series. For an observed time series x(t) indexed over\ntime t, the discrete-time derivative of order one can be written\nas:\ndx\ndt = x(t+ δt) − x(t)\nδt ,\nwhere δt is the lag. For real-world time series datasets, time\nis recorded as a timestamp (date) in chronological order.\nThus, we choose δt = 1 in our work. To adjust the loss\nof the network, the standard PINN model backpropagates the\ntime derivatives since the mapping function used in MLP is\ndifferentiable.\nFor instance, if x= g(t,θ = (w,b)), where g is the neural\nnetwork function of the MLP, then y = dx\ndt = dg(t,θ=(w,b))\ndt .\nThe backpropagation writes:\ndLphy\ndθ = dLphy\ndy\ndy\ndθ = dLphy\ndy\nd2x\ndtdθ\n= dLphy\ndy\nd2x\ndtdθ = dLphy\ndy\nd2g(t,θ)\ndtdθ ,\nwhere Lphy is the physical loss corresponding to the regular-\nization term and θ= (w,b) represents the weights and biases\nof the network. In our case, we computed the time derivatives\nduring the pre-processing step.\nOur network forecasts x(t), dx\ndt and d2x\ndt2 in time from\ntheir historical data. The physics is enforced on the system\nthrough regularization of the output, i.e., the predicted values\nof time series x(t) and its derivatives have to satisfy the\nLi´enard equation as in Eqn 1. After pre-training, we transfer\nthis knowledge to real-life data following the same dynamics.\nThe importance of transferring synthetic data knowledge to\nreal data is further experimentally explored in sections IV\nand V. In the case when harmonic forcing (second-term of\nthe differential equation) is not known, we assume it can be\napproximated by the 1st term of the differential equation for\nthe real data distribution.\nLphy = RMSE(d2ypred\ndt2 + αypred\ndypred\ndt + γypred + βy3\npred,\nfsin(ωt))\n(2)\nIn our case, we minimize the gap between the 1st term of\nthe differential equation of the real and the predicted data as\nfollows:\nLphy = RMSE(d2yreal\ndt2 + αyreal\ndyreal\ndt + γyreal + βy3\nreal,\nd2ypred\ndt2 + αypred\ndypred\ndt + γypred + βy3\npred),\n(3)\nwhere RMSE is the root mean squared error. A schematic\ndiagram showing the various steps of the KDL approach is\npresented in Figure 1. An algorithmic version of the proposed\nframework is also given in Algorithm 1.\nFig. 1: Proposed knowledge-based deep learning architecture. Differential equation is simulated into a time series t,x(t). The\nﬁrst and second-order derivatives of xare computed and fed along with xto our model that forecasts the next instances through\na backpropagation mechanism of the losses: 1) Data loss 2) Physics loss: Predicted data should satisfy the dynamics. The same\nprocess is used on real-world datasets with the addition of transferred knowledge from the pre-training.\nAlgorithm 1 KNOWLEDGE -BASED DEEP LEARNING (KDL)\nData: X = x(t− k),...,x (t− 1),x(t),dx\ndt(t− k),..., dx\ndt(t),\nd2x\ndt2 (t− k),..., d2x\ndt2 (t)\nResult: Y = x(t+ 1), dx\ndt(t+ 1), d2x\ndt2 (t+ 1)\nInitialize network parameters: (weights and biases)\nif Pre-training then\nwhile epoch<max epochsor Termination Condition do\nCompute Y = KDL(X)\nCompute the loss Ldata = RMSE(Ypredicted,Yreal)\nCompute the loss Lphy from Equation (2)\nBackpropagate Ldata + λ1Lphy (using ADAM opti-\nmizer) and λ1 is a tuning parameter chosen by cross-\nvalidation\nSave Network State (weights, biases)\nend\nelse\nwhile epoch<max epochsor Termination Condition do\nLoad pretrained network state (weights, biases)\nCompute Y = KDL(X)\nCompute the loss Ldata = RMSE(Ypredicted,Yreal)\nCompute the loss Lphy from Equation (3)\nBackpropagate Ldata + λ2Lphy (using ADAM opti-\nmizer) and λ2 is a tuning parameter chosen by cross-\nvalidation\nend\nend\nIV. E XPERIMENTAL SETUP\nWe evaluate KDL on three datasets containing extreme\nevents to validate our proposed model. Extreme events can\nbe modelled as a Li ´enard system with external forcing from\nEquation 1, and for a set value of parameters [30]: f = 0.2,\nα= 0.45, γ = −0.5, ω= 0.642, β = 0.5.\nA. Real-world Datasets\n1) El Ni ˜no Dataset: El Ni˜no dataset, also known as ENSO,\nis a periodic ﬂuctuation of sea surface temperature (SST)\nacross the Paciﬁc Ocean [15]. El Ni ˜no dataset contains 1634\nweekly observations of SST in El Nino region 1 across the\nPaciﬁc Ocean, from January 3, 1990 to April 21, 2021.\n2) San Juan Dengue infections: San Juan dataset [36] is a\nunivariate time series data portraying the evolution of cases\nof Dengue infection. This infection exhibits extreme events\ndue to the randomness of the transmission. Dengue infection\nis a mosquito-borne viral disease. The dataset contains 1197\nobservations from week 17 of 1990 to week 16 of 2013. We\nconsider only the univariate time series with the number of\ncases across this time period.\n3) Bjørnøya Precipitation: This is an extreme event dataset\ntaken from the Norwegian Climate Service Center [37] obser-\nvation of rainfall in the Bjørnøya region. The dataset consists\nof 15320 daily observations from June 16, 1980 to June 16,\n2022.\nB. Performance Measures\nTo evaluate our models, we considered three commonly\nused metrics as given below [38]. The lower their value is,\nthe better the forecasting model. We have used the following\nmeasures in this study: (a) Root Mean Squared Error (RMSE)\n[38]; (b) Mean Absolute Error (MAE); and (c) Physical\ninconsistency (PIC) [25].\nAmong the three performance measures, physical inconsis-\ntency estimates the physical meaning of a variable. It is com-\nputed by comparing the dynamics of real data to the predicted\ndata. This metric is relevant in problems revolving around\nphysics. For example, to predict temperature T at a time\npoint using two different models knowing that Treal = 25,\nif the model 1 predicted Tpred = 24.9 and model 2 predicted\nTpred = 26.1, model 1 outperforms model 2. Nonetheless,\nunder the hypothesis that temperature T ≥ 25 (from the\ndynamics of the system), then model 2 is more relevant, even\nwhen the average error is higher.\nC. Implementation of the proposed KDL framework\nData is generated from the differential equation using the\nRunge-Kutta method and ODE-4 Solver in Matlab. For the\nrest, we use Keras’s backend. We train our model on data\nsimulated from the Li´enard-type differential equation [15]. The\nresult is a time series t,x(t). We compute the ﬁrst derivative\nby shifting x by a step of one and subtracting x from it i.e.,\ndx\ndt = xt+1 − xt. The 2nd derivative is computed similarly\nfrom dx\ndt. We transform our time series into supervised learning\n(X,Y ) where X = (x(t− p),...x(t− 1)) is the input and\nY = (x(t),dx\ndt(t),d2x\ndt2 (t)) is the output and p is the lookback\n(in our case p=10). We feed data to our network composed of\nthree LSTM layers with 50 neurons each and a Dense layer\nwith 3 neurons (3 outputs). The feedback mechanism has the\nstandard data loss: RMSE(ypredicted,yreal) and a physical\nloss ensuring the differential equation (as in Equation 1) is\nsatisﬁed. The total loss is expressed as Ltotal = Ldata +\n0.2Lphysical.\nWe save the network state after the training. For real\ndata, we follow the same procedure by loading the state of\nthe network that we saved from the pre-training when we\ninitialized our model before training it.\nV. E XPERIMENTAL RESULTS\nA. Preliminary data analysis\nTable I displays the statistical speciﬁcations\nof our datasets including stationarity (using\nKwiatkowski–Phillips–Schmidt–Shi test), nonlinearity (using\nTer¨asvirta’s neural network test), long-term dependency (using\nHurst-Exponent), and evidence of chaos (using Lyapunov\nExponent test) as described in [38]. Our datasets exhibit strong\nnonlinearity, long-term dependency, chaotic behavior, and\nnon-stationarity (except for the El Ni ˜no dataset’s stationary\nstructure).\nB. Baseline models\nFor comparative analysis, we considered the following\ncompetitive forecasting models: Long-Short Term Memory\n(LSTM) [17]; Feed-Forward Neural Networks [19]; Convo-\nlutional Neural Networks (1D CNN) [18], and Reservoir\nComputing (ESN) [20]. In the experiments, we ﬁt an LSTM\nwith 3 hidden layers and 50 neurons, an RC with a reservoir\nsize of 500, an FFNN with one hidden layer and 50 neurons,\na 1D CNN with 64 ﬁlters and a kernel size of 2, and a\ntimestep of 3 followed by a MaxPooling and Flatten layers\nto a fully connected layer with 50 neurons. Our network has\nthe same architecture as the LSTM used for comparison, with\nthe addition of physical regularization and prior knowledge\nfrom pre-training. All the methods are extensively evaluated\non different train/test splits scenarios to assess the performance\nof the different methods under different data sizes.\nC. Results\n1) El Ni ˜no dataset: On the El Ni ˜no dataset, we can see\nthat the RMSE of our method is lower than that of 4 other\nforecasting models. The lower the RMSE, the better we\nconsider our prediction. As for MAE, our network gets the\nhighest score for training/test sizes up to 60%, being a close\nsecond with 80% training and 20% test (0.336 vs. 0.333\nMAE). Our proposed model has lower physical inconsistency\nthan all other prediction models, especially when trained on\nlow data sizes. For higher data sizes, our network is close to\nLSTM as classical approaches can capture complex dynamics\nwhen fed with enough data points.\n2) San Juan Dengue: Our proposed network manages to\nget better physical inconsistency with higher training sizes on\nSan Juan Dengue datasets. This could be explained by the\nfact that this system exhibits extreme events globally but not\nlocally. Therefore, we removed the prior training (on simulated\ndata) and trained the network solely on this dataset. Having\nprior knowledge of different dynamics further degrades the\naccuracy of the model.\n3) Bjørnøya daily precipitation: The data exhibits chaos\nwhen the total number of observations is high enough (15k\nobservations in this case as opposed to 1.5k for other datasets).\nTraditional deep learning methods can capture the complex\npatterns and dynamics in data when fed with enough data\npoints which can be further validated when we look at the\nphysical inconsistency values where our network falls short\ncompared to Reservoir Computing. Our network scores best\nRMSE for training sizes up to 60% and 2nd place when the\ntraining size is 80%. For MAE, we get 1st rank when training\nsizes are respectively 20%,60% and 80%.\nD. Comparison with baselines\nTable II depicts the experimental results and comparison\nwith several baseline models. We further apply multiple com-\nparisons with the best (MCB) test to check the statistical sig-\nniﬁcance of the comparative performance [39]. Finally, based\non the results obtained in the previous section, we plot the\naverage ranking of the models on all datasets and all training:\ntest sizes based on MAE and RMSE. From ﬁgures 2 and 3, we\ncan conclude that our proposed KDL method outperforms all\nother models in terms of RMSE and MAE. All the code and\nmaterial are made available at https://github.com/Zelabid/KdL\nto support the principle of reproducible research.\nVI. C ONCLUSION AND DISCUSSION\nIn this paper, we proposed a knowledge-based deep learning\nframework combining physical and prior transferable knowl-\nedge from synthetic and real data to forecast extreme events.\nThe proposed model takes a hybrid approach, learning partially\nfrom the time series, their physics, and the knowledge gained\nin prior training. We built our model under the hypothesis that\nchaotic systems exhibit extreme events and can be modeled\nusing a Li ´enard-type system. This assumption holds strongly\nas our network ranks ﬁrst among all other forecasting models\nconsidered in the comparison. It is interesting to consider the\nTABLE I: Preliminary data analysis\nData Name Frequency Observations Start End Min-Max Value Behaviour\nEl Ni˜no Dataset [15] Weekly 1634 January,3rd 1999 April,21st 2021 18.3 - 29.2 Stationary, Non-linear, Chaotic, Long-term dependent\nSan Juan Dengue cases [36] Weekly 1197 April,23rd 1990 April,21st 2013 0 - 461 Non-stationary, Non-linear, Chaotic, Long-term dependent\nBjørnøya Precipitation [37] Daily 15320 June,16th 1980 June,16th 2022 0.0 - 42.1 Non-stationary, Non-linear, Chaotic, Long-term dependent\nTABLE II: Comparison of baseline models: LSTM, FFNN, CNN, ESN with our model KDL on three real-world datasets: El\nNi˜no, San Juan dengue, and Bjørnøya precipitation on different training sizes: 20%, 40%, 60%, and 80%.\nDataset Network/Loss LSTM [17] FFNN [19] CNN [18] Reservoir Computing [20] KDL (proposed)\nTrain : Test splitRMSE MAE PIC RMSE MAE PIC RMSE MAE PIC RMSE MAE PIC RMSE MAE PIC\nEl Ni˜no\n20% : 80% 0.433 0.342 283.340 2.444 2.108 1713.237 0.523 0.413 339.859 1.399 0.925 733.396 0.428 0.338 276.654\n40% : 60% 0.418 0.327 268.309 1.948 1.677 1349.331 0.438 0.378 313.898 2.889 1.685 1176.900 0.416 0.327 268.777\n60% : 40% 0.433 0.343 285.274 2.068 1.780 1456.752 0.452 0.350 293.451 0.694 0.503 395.430 0.431 0.341 283.476\n80% : 20% 0.427 0.333 280.754 2.157 1.846 1540.997 0.490 0.394 336.259 0.562 0.439 359.607 0.427 0.336 283.362\nSan Juan\nDengue\n20% : 80% 15.081 9.214 130027 38.995 33.393 252054.10917.037 9.536 156565.04720.722 13.763 182934.59414.333 8.537 133578\n40% : 60% 11.402 7.456 89123 34.753 25.631 231226.23414.152 9.218 127899.24212.031 8.140 89863.445 11.328 7.352 90754\n60% : 40% 13.131 8.565 136547 38.039 24.461 330925.90617.266 10.259 184430.34413.289 8.779 129067.08612.894 8.358 128159\n80% : 20% 15.562 10.803 235978 49.334 30.185 609710.93819.739 12.755 278096.34415.420 10.762 235443.34415.416 10.721 231362\nBjørnøya\nRainfall\n20% : 80% 2.474 1.328 51.775 2.506 1.348 51.608 3.179 1.712 88.626 2.474 1.351 50.932 2.473 1.322 51.760\n40% : 60% 2.507 1.380 53.719 2.543 1.448 53.562 3.071 1.687 69.999 2.510 1.383 52.821 2.506 1.403 53.705\n60% : 40% 2.428 1.418 47.696 2.457 1.456 47.450 2.923 1.527 65.802 2.428 1.389 46.876 2.427 1.383 47.671\n80% : 20% 2.561 1.369 63.420 2.612 1.498 63.328 3.167 1.583 120.577 2.563 1.930 62.666 2.562 1.368 63.485\nFig. 2: Comparison with baseline models based on RMSE.\nKDL scores an average rank of 1.12, the highest rank among\nbaseline models, followed by LSTM, ESN, CNN, and FFNN.\nFig. 3: Comparison with baseline models based on MAE.\nKDL scores an average rank of 1.29, the highest rank among\nbaseline models, followed by LSTM, ESN, CNN, and FFNN.\nproposed KDL approach with multi-step forecasting for future\nwork. Generalizing the framework to other relevant forecasting\nproblems can also be regarded as future scope of this study.\nACKNOWLEDGMENT\nThe support of TotalEnergies is fully acknowledged. Za-\nkaria ELabid (PhD Student) and Abdenour Hadid (Professor,\nIndustry Chair at SCAI Center of Abu Dhabi) are funded by\nTotalEnergies collaboration agreement with Sorbonne Univer-\nsity Abu Dhabi.\nREFERENCES\n[1] P. R. Vlachas, W. Byeon, Z. Y . Wan, T. P. Sapsis,\nand P. Koumoutsakos, “Data-driven forecasting of high-\ndimensional chaotic systems with long short-term mem-\nory networks,” Proceedings of the Royal Society A:\nMathematical, Physical and Engineering Sciences , vol.\n474, no. 2213, p. 20170844, 2018.\n[2] C. Klos, Y . F. K. Kossio, S. Goedeke, A. Gilra, and R.-\nM. Memmesheimer, “Dynamical learning of dynamics,”\nPhysical Review Letters, vol. 125, no. 8, p. 088103, 2020.\n[3] M. Tabor, Chaos and integrability in nonlinear dynamics:\nan introduction. Wiley-Interscience, 1989.\n[4] S. Wiggins, S. Wiggins, and M. Golubitsky, Introduc-\ntion to applied nonlinear dynamical systems and chaos .\nSpringer, 2003, vol. 2, no. 3.\n[5] M. Lakshmanan and S. Rajaseekar, Nonlinear dynamics:\nintegrability, chaos and patterns . Springer Science &\nBusiness Media, 2012.\n[6] M. Raissi, P. Perdikaris, and G. E. Karniadakis, “Physics-\ninformed neural networks: A deep learning framework\nfor solving forward and inverse problems involving non-\nlinear partial differential equations,” Journal of Compu-\ntational physics, vol. 378, pp. 686–707, 2019.\n[7] L. Yang, X. Meng, and G. E. Karniadakis, “B-pinns:\nBayesian physics-informed neural networks for forward\nand inverse pde problems with noisy data,” Journal of\nComputational Physics, vol. 425, p. 109913, 2021.\n[8] T. Shinbrot, C. Grebogi, J. Wisdom, and J. A. Yorke,\n“Chaos in a double pendulum,” American Journal of\nPhysics, vol. 60, no. 6, pp. 491–499, 1992.\n[9] B. Sivakumar, “Chaos theory in geophysics: past, present\nand future,” Chaos, Solitons & Fractals , vol. 19, no. 2,\npp. 441–462, 2004.\n[10] A. Trevisan and L. Palatella, “Chaos and weather fore-\ncasting: the role of the unstable subspace in predictability\nand state estimation problems,” International Journal of\nBifurcation and Chaos , 2011.\n[11] G. Chen and T. Ueta, Chaos in circuits and systems .\nWorld Scientiﬁc, 2002, vol. 11.\n[12] R. J. Field et al., Chaos in chemistry and biochemistry .\nWorld Scientiﬁc, 1993.\n[13] G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris,\nS. Wang, and L. Yang, “Physics-informed machine learn-\ning,” Nature Reviews Physics , vol. 3, no. 6, 2021.\n[14] K. Kashinath, M. Mustafa, A. Albert, J. Wu, C. Jiang,\nS. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chat-\ntopadhyay, A. Singh et al. , “Physics-informed machine\nlearning: case studies for weather and climate mod-\nelling,” Philosophical Transactions of the Royal Society\nA, vol. 379, no. 2194, p. 20200093, 2021.\n[15] A. Ray, T. Chakraborty, and D. Ghosh, “Optimized\nensemble deep learning framework for scalable forecast-\ning of dynamics containing extreme events,” Chaos: An\nInterdisciplinary Journal of Nonlinear Science , vol. 31,\nno. 11, p. 111105, 2021.\n[16] V . G. Satorras, E. Hoogeboom, and M. Welling, “E\n(n) equivariant graph neural networks,” in International\nconference on machine learning . PMLR, 2021.\n[17] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation , vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[18] L. Li, K. Ota, and M. Dong, “Everything is image: Cnn-\nbased short-term electrical load forecasting for smart\ngrid.” IEEE, 2017, pp. 344–351.\n[19] Z. Tang and P. A. Fishwick, “Feedforward neural nets\nas models for time series forecasting,” ORSA journal on\ncomputing, vol. 5, no. 4, pp. 374–385, 1993.\n[20] C. Gallicchio, A. Micheli, and L. Pedrelli, “Deep reser-\nvoir computing: A critical experimental analysis,” Neu-\nrocomputing, vol. 268, pp. 87–99, 2017.\n[21] S. Vaidyanathan and C. V olos,Advances and applications\nin chaotic systems . Springer, 2016, vol. 636.\n[22] J. L. Martin, S. C. McCutcheon, and R. W. Schottman,\nHydrodynamics and transport for water quality model-\ning. CRC press, 2018.\n[23] C. Moore, “Unpredictability and undecidability in dy-\nnamical systems,” Physical Review Letters , 1990.\n[24] Y . Che and C. Cheng, “Uncertainty quantiﬁcation in\nstability analysis of chaotic systems with discrete delays,”\nChaos, Solitons & Fractals , 2018.\n[25] A. Daw, A. Karpatne, W. Watkins, J. Read, and V . Ku-\nmar, “Physics-guided neural networks (pgnn): An ap-\nplication in lake temperature modeling,” arXiv preprint\narXiv:1710.11431, 2017.\n[26] S. Cai, Z. Wang, F. Fuest, Y . J. Jeon, C. Gray, and G. E.\nKarniadakis, “Flow over an espresso cup: inferring 3-\nd velocity and pressure ﬁelds from tomographic back-\nground oriented schlieren via physics-informed neural\nnetworks,” Journal of Fluid Mechanics , vol. 915, 2021.\n[27] T. Beucler, S. Rasp, M. Pritchard, and P. Gen-\ntine, “Achieving conservation of energy in neural net-\nwork emulators for climate modeling,” arXiv preprint\narXiv:1906.06622, 2019.\n[28] V . Chandrasekar, M. Senthilvelan, and M. Lakshmanan,\n“Unusual li´enard-type nonlinear oscillator,” Physical Re-\nview E, vol. 72, no. 6, p. 066203, 2005.\n[29] J. Sugie and X. Zhao, “Qualitative behavior of solutions\nof li ´enard-type systems with state-dependent impulses,”\nNonlinear Analysis: Real World Applications , vol. 67, p.\n103634, 2022.\n[30] S. L. Kingston, K. Thamilmaran, P. Pal, U. Feudel,\nand S. K. Dana, “Extreme events in the forced li ´enard\nsystem,” Physical Review E , vol. 96, no. 5, 2017.\n[31] S. Hochreiter and J. Schmidhuber, “Long short-term\nmemory,” Neural computation , vol. 9, no. 8, pp. 1735–\n1780, 1997.\n[32] P. R. Vlachas, W. Byeon, Z. Y . Wan, T. P. Sapsis,\nand P. Koumoutsakos, “Data-driven forecasting of high-\ndimensional chaotic systems with long short-term mem-\nory networks,” Proceedings of the Royal Society A:\nMathematical, Physical and Engineering Sciences , vol.\n474, no. 2213, p. 20170844, 2018.\n[33] L. Torrey and J. Shavlik, “Transfer learning,” in Hand-\nbook of research on machine learning applications and\ntrends: algorithms, methods, and techniques. IGI global,\n2010, pp. 242–264.\n[34] J. C. Butcher, “On the implementation of implicit runge-\nkutta methods,” BIT Numerical Mathematics , vol. 16,\nno. 3, pp. 237–240, 1976.\n[35] A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M.\nSiskind, “Automatic differentiation in machine learn-\ning: a survey,” Journal of Marchine Learning Research ,\nvol. 18, pp. 1–43, 2018.\n[36] T. Chakraborty, S. Chattopadhyay, and I. Ghosh, “Fore-\ncasting dengue epidemics using a hybrid methodology,”\nPhysica A: Statistical Mechanics and its Applications ,\nvol. 527, p. 121266, 2019.\n[37] “Norwegian Center for Climate Services,” Accessed\nApril 2022. [Online]. Available: https://seklima.met.no/\nobservations/\n[38] T. Chakraborty, I. Ghosh, T. Mahajan, and T. Arora,\n“Nowcasting of covid-19 conﬁrmed cases: Foundations,\ntrends, and challenges,” Modeling, Control and Drug\nDevelopment for COVID-19 Outbreak Prevention , pp.\n1023–1064, 2022.\n[39] M. Panja, T. Chakraborty, U. Kumar, and N. Liu,\n“Epicasting: An ensemble wavelet neural network\n(ewnet) for forecasting epidemics,” arXiv preprint\narXiv:2206.10696, 2022."
}