{
  "title": "LLMs in Automated Essay Evaluation: A Case Study",
  "url": "https://openalex.org/W4398184732",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2303249755",
      "name": "Milan Kostic",
      "affiliations": [
        "Università di Camerino"
      ]
    },
    {
      "id": "https://openalex.org/A293162892",
      "name": "Hans Friedrich Witschel",
      "affiliations": [
        "FHNW University of Applied Sciences and Arts"
      ]
    },
    {
      "id": "https://openalex.org/A2036851807",
      "name": "Knut Hinkelmann",
      "affiliations": [
        "FHNW University of Applied Sciences and Arts"
      ]
    },
    {
      "id": "https://openalex.org/A5051534862",
      "name": "Maja Spahic-Bogdanovic",
      "affiliations": [
        "FHNW University of Applied Sciences and Arts"
      ]
    },
    {
      "id": "https://openalex.org/A2303249755",
      "name": "Milan Kostic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A293162892",
      "name": "Hans Friedrich Witschel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2036851807",
      "name": "Knut Hinkelmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5051534862",
      "name": "Maja Spahic-Bogdanovic",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034730645",
    "https://openalex.org/W3035009426",
    "https://openalex.org/W2154541934",
    "https://openalex.org/W2025719462",
    "https://openalex.org/W4387323365",
    "https://openalex.org/W2569115912",
    "https://openalex.org/W2253332623",
    "https://openalex.org/W1964030308",
    "https://openalex.org/W4396511191",
    "https://openalex.org/W2170020655",
    "https://openalex.org/W3087889967",
    "https://openalex.org/W2833690790",
    "https://openalex.org/W2964413085",
    "https://openalex.org/W3201077663",
    "https://openalex.org/W2955427418",
    "https://openalex.org/W2007097081"
  ],
  "abstract": "This study delves into the application of large language models (LLMs), such as ChatGPT-4, for the automated evaluation of student essays, with a focus on a case study conducted at the Swiss Institute of Business Administration. It explores the effectiveness of LLMs in assessing German-language student transfer assignments, and contrasts their performance with traditional evaluations by human lecturers. The primary findings highlight the challenges faced by LLMs in terms of accurately grading complex texts according to predefined categories and providing detailed feedback. This research illuminates the gap between the capabilities of LLMs and the nuanced requirements of student essay evaluation. The conclusion emphasizes the necessity for ongoing research and development in the area of LLM technology to improve the accuracy, reliability, and consistency of automated essay assessments in educational contexts.",
  "full_text": "LLMs in Automated Essay Evaluation: A Case Study \nMilan Kostic1, Hans Friedrich Witschel2, Knut Hinkelmann1, 2, Maja Spahic-Bogdanovic1, 2 \n1University of Camerino (UNICAM) \n2FHNW University of Applied Sciences and Arts Northwestern Switzerland \nmilan.kostic@unicam.it, {hansfriedrich.witschel, knut.hinkelmann, maja.spahic}@fhnw.ch \n \n \n \nAbstract \nThis study delves into the application of Large Language \nModels (LLMs), such as ChatGPT-4, for the automated eval-\nuation of student essays, focusing on a case study conducted \nat the Swiss Institute of Business Administration (SIB). It ex-\nplores the effective ness of LLMs in assessing German -lan-\nguage student transfer assignments, contrasting their perfor-\nmance with traditional human lecturer evaluations. The pri-\nmary findings highlight LLMs ’ challenges in accurately \ngrading complex texts according to predefined c riteria and \nproviding detailed feedback. This research illuminates the \ngap between LLM capabilities and the nuanced requirements \nof student essay evaluation. The conclusion emphasizes the \nnecessity for ongoing research and development in LLM \ntechnology to improve automated essay assessments ’ accu-\nracy, reliability, and consistency in educational contexts. \nIntroduction  \nThe ability of students to demonstrate their intellectual de-\nvelopment in a specific field by writing represents a critical \ncomponent of the academic process, as highlighted by Hay-\nland (2013). Writing essays provides a platform for assess-\nment by lecturers a nd motivates students to engage more \ndeeply with the subject matter, thereby promoting cognitive \nskills such as analysis and synthesis (Zupanc and Bosnić, \n2018). Considering essay assessment’s time-consuming and \ncostly nature , Page (1966) presented the “Project Essay \nGrade” concept  for automatically grading students’ essays  \non the English language or literature. Such systems aim to \nincrease assessment efficiency, consistency, and objectivity \nwhile minimizing time and resources. \n Various systems are available to extract and evaluate var-\nious text attributes using algorithms. These systems use \nNLP algorithms and regression models to assess writing \nstyle or content quality (Ifenthaler, 2023). Most of these au-\ntomatic evaluation systems  require a corpus of essays al-\nready graded by humans. Based on these human evaluations, \nthe system learns and can assess new essays without human \n \nCopyright © 2024, Association for the Advancement of Artificial \nIntelligence (www.aaai.org). All rights reserved. \nintervention. However, various systems ’ test quality, accu-\nracy, or correctness are often not publicly accessible or ex-\namined in comprehensive empirical studies (Wilson and Ro-\ndrigues, 2020). \n Ramesh and Sanampudi (2021) conducted a detailed lit-\nerature analysis. They found that the challenge of automati-\ncally evaluating longer texts, such as essays, has yet to be \nfully mastered. The significant problem is that existing sys-\ntems have difficulty processing long texts and providing un-\nderstandable evaluations to both students and lecturers. \n The use of Natural Language Processing with Large Lan-\nguage Models (LLMs) is a rapidly growing research field in \nArtificial Intelligence (Strasser, 2023). It covers a wide \nrange of applications, including automated essay evaluation. \nLLMs like Google Bard or  ChatGPT can process complex \nsentences and establish relationships between text elements, \nincluding the user ’s intent. Such pre -trained LLMs are an \nadvantage because users do not need extensive program-\nming knowledge and require minimal or no training data  to \nachieve satisfactory results. Users can also provide textual \ninstructions and examples to the model to initiate a desired \ninteraction. \n In this paper, we first examine the development of auto-\nmatic essay evaluation systems before discussing potential \nideas for using LLMs to evaluate students’ transfer assign-\nments written in German automatically . Transfer assign-\nments are a pivotal evaluation tool at the Swiss Institute of \nBusiness Administration (SIB) to assess students ’ ongoing \nlearning paths. These assignments, which resemble essays \nin their format, frequently require students to engage with \nreal-world professional scenarios encountered wi thin their \nemployment. We investigated OpenAI’s ChatGPT-4 (GPT-\n4) ability to evaluate student transfer assignments according \nto predefined assessment criteria and aspects and generate \ncorresponding feedback. Initially, we analyze the con-\nsistency of the lecturers when evaluating transfer assign-\nments before tasking GPT-4 with the same. This experiment \n \nAAAI Spring Symposium Series (SSS-24)\n143\nused four transfer assignments written in German and of var-\nying quality from the SIB. \n This paper reviews state-of-the-art literature in Section 2, \npresents a practical use case in Section 3, proposes innova-\ntive strategies to improve assessment consistency in Section \n4, and concludes with key findings and implications in Sec-\ntion 5. \nRelated Work \nPage (1966) introduced the Project Essay Grading concept, \ncatalyzing the development of computer systems capable of \nevaluating essays without human intervention. This ad-\nvancement necessitated a development in the terminology of \nthese systems, leading to the interchangeable use of various \nterms. Initially, Page (1966, 1968) described the process as \n“Analyzing Students ’ Essays by Computers ” and “Com-\nputer Grading of Essays.” Subsequently, the field broadened \nto include terms like “Automated Essay Scoring ” (AES) \n(Attali and Burstein, 2006; Ke and Ng, 2019), “Automated \nEssay Grading ” (AEG) (Valenti, Neri, and Cucchiarelli, \n2003), and “Automated Writing Evaluation” (AWE) (Beig-\nman Klebanov and Madnani, 2020). These terminological \nchanges mirror the advancements in automated assessment \nsystems. Contemporary systems aim to design their func-\ntionality to provide detailed and transparent assessments. \nThis approach encompa sses delivering holistic evaluation \nresults, such as grades or scores, complemented with feed-\nback for improvement. Zupanc and Bosnić (2017) particu-\nlarly underscore this aspect in their discussion of ‘Auto-\nmated Essay Evaluation ’ (AEE), illuminating the shift in \nthese systems towards a more nuanced approach. \n In their analysis, Zupanc and Bosnić (2015) compared 21 \nAEE systems based on various criteria, including methodol-\nogy, main focus, feedback application, required essays for \ntraining, prediction model, rank, and average accuracy. \nTheir findings revealed that  while these systems broadly \ntackle similar tasks, they often employ distinct methodolo-\ngies for extracting attributes and constructing their models. \nNatural Language Processing (NLP) is the predominant \nmethod in this comparison. However, the AEE systems a re \ncapable of not only identifying syntactic errors but also \nproviding holistic feedback. Moreover, they currently lack \nto offer meaningful and content-specific feedback. \n Further analysis by Zupan and Bosnić (2018) indicated a \ngrowing adoption of these systems linked to their enhanced \nreliability. High -stakes assessments such as the Graduate \nRecord Examination (GRE), the Test of English as a Foreign \nLanguage (TOEFL), and the Graduate Management Admis-\nsion Test (GMAT) increasingly employ these systems. \nThese assessments require precise and reliable essay grad-\ning, a challenge met through the cooperation of automated \nsystems and human graders. \n Ramesh and Sanampudi  (2021) performed a systematic \nliterature review, including publications from 2010 to 2020, \nexamining AEE systems that process data in English. They \nidentified 62 publications, indicating that most automated \ngrading systems leverage NLP methods for grade p redic-\ntion. These predictions rely on training with corpora of hu-\nman-rated essays, enabling the systems to grade autono-\nmously and process large quantities of essays efficiently. \nThey identified several limitations of AEE systems, includ-\ning the absence of as sessments based on content relevance, \ncohesion, and coherence, a lack of domain knowledge-based \nevaluation through machine learning models, the inability of \nNLP libraries to process words with multiple meanings, and \na lack of focus on consistency and completeness in the eval-\nuation process. \n The utilization of this technology, however, is not limited \nto English. It encompasses a range of languages, including \nArabic, Bahasa Malay, Basque, Chinese, Finnish, French, \nGerman, Hebrew, Japanese, Malaysian, Spanish, and Swe-\ndish (Hussein, Hassan, and Nassef, 2019). The advancement \nof automatic essay grading tools in non -English languages \nfaces challenges, primarily due to the complexity of devel-\noping NLP tools for each language and the global predomi-\nnance of English in research. \n Evaluating the efficacy of Automated Essay Grading Sys-\ntems remains a complex task , especially sinc e agreement \namong human evaluators can be r elatively low when grad-\ning a given essay or assignment. Williamson, Xi, and Breyer \n(2012) advocate for a reliable evaluation method that com-\npares the grading outcomes of human graders with those of \nautomated systems and then calculates the interrater agree-\nment. Whe n human and automated grading results align \nclosely, especially within a predefined threshold, the system \ncan be considered adequate and within acceptable levels of \nprediction accuracy. Ramesh and Sanampudi (2021) have \nidentified that in this domain, researchers predominantly uti-\nlize three metrics for evaluation: the quadratic weighted \nKappa coefficient, the Pearson Correlation Coefficient, and \nthe Mean Absolute Error, which are necessary for accurately \nmeasuring the performance of these systems. \n Ramesh and Sanampudi (2021) and Ifenthaler (2023) \nhave highlighted the limitations of AEE systems, including \nchallenges in providing adequate feedback and capturing \nlong-distance dependencies. In contrast, LLMs are gaining \npopularity due to their capabilit y to handle diverse tasks \nwithout specific training on certain datasets. Masikisiki, \nMarivate, and Hlophe (2023) reported that this increasing \nprevalence has sparked interest in their potential educational \napplications, particularly for assessment purposes. \n144\nUse Case \nThe SI B uses so -called “transfer assignments” to test stu-\ndents’ practical application of theoretical business \nknowledge. These assignments necessitate that students \ndemonstrate their comprehension and application skills in \nreal-life contexts. The assignments, typically between 9 and \n15 pages, are methodically graded using an analytical rubric. \nThis rubric is designed to capture a multifaceted view of stu-\ndent performance and consists of six criteria. These criteria, \ndefined by specific guiding questions, include Company De-\nscription, Stakeholder Analysis, Environmental Analysis, \nProposed Measures, Organization and Structure, and Formal \nRequirements. Each criterion consists of various evaluation \naspects, each allowing students to earn different point totals. \nThere are 16 evaluation aspects, with a maximum possible \nscore of 60 points. Lecturers also provide constructive writ-\nten feedback on each criterion, highlighting areas for student \nimprovement. Each lecturer is responsible for grading their \nstudents’ assignments to ensure consistency and fairness in \nassessment. If neither the SIB quality control department \nnor the students raised any concerns regarding the provided \nevaluation results, this suggests their overall acceptability. \nWorking Alone, Together Method \n“Working Alone, Together ” is a collaborative method for \ncreative thinking or problem-solving (Bruns, 2013). All par-\nticipants tackle the same challenge in this method by writing \ndown or illustrating their ideas. In the initial solo phase, par-\nticipants individually develop their solutions, which they \nshare in the subsequent group session. The strength of this \nmethod lies in preserving and valuing everyone’s creativity, \nopinion, and viewpoint, ensuring they are not overshadowed \nor left unexpressed in the group setting, and minimizing mu-\ntual influence among participants. \n In the workshop, we utilized the “Working Alone, To-\ngether” method to assess the consistency of lecturers ’ eval-\nuation of transfer assignments in General Business Admin-\nistration. The evaluation included the grading of each aspect \nand the writing of feedback. Four  assignments, previously \nevaluated by the author of this paper and varying in quality, \nwere selected for evaluation. Neither the SIB quality control \ndepartment nor the students raised any concerns about these \nassessments, implying their general accep tability. While \naware that these papers had been previously evaluated, the \nthree participating lecturers were not informed of the origi-\nnal evaluation outcomes nor who had initially evaluated \nthem. \nConsistency Evaluation \nDuring the workshop, we analyzed the consistency of eval-\nuations of three lecturers by comparing their grades on four \ntransfer assignments (TA1 to TA4) with the original grades.  \n \nTable 1: Consistency Assessment Results. \nThe lecturers re -assessed the transfer assignments. Table 1 \npresents individual evaluations, including grades from both \nlecturers and original grades. To quantify each lecturer’s de-\nviation, we calculated the Mean Absolute Deviation (MAD) \nfrom the original grades for each evaluation. The column la-\nbeled ‘MAD per Lecturer ’ displays the deviations, while \n‘MAD per TA ’ shows the average deviation across all lec-\nturers for each assignment. Although the sample size is \nsmall, w e also calculated the Pearson Correlation Coeffi-\ncient (PCC) to measure the strength of the linear relationship \nbetween the lecturers’ evaluations and the original grades. \n The analysis indicates an increasing MAD value for each \nsubsequent assignment, suggesting growing evaluation var-\niance. The PCC values show a positive correlation between \nlecturer evaluations and original grades, with lecturer 1 hav-\ning the strongest correla tion (PCC = 0.7832) and lecturer 3 \nhaving the weakest (PCC = 0.5617). \n The sequence effect, where evaluating one assignment in-\nfluences the perception of subsequent ones, may also be a \nfactor, mainly if an early assignment is exceptionally strong \nor weak (Attali, 2011). Despite conducting reflections after \neach evaluation in the workshop to identify differences and \nminimize deviations, we observed no apparent learning ef-\nfect among the lecturers. Ironically, although these reflec-\ntion phases aim to improve evaluation consistency, we noted \na paradoxical decrease in consistency fr om TA1 to TA4.  \nThe lecturers cited increasing fatigue and loss of concentra-\ntion as the reason for the large discrepancy in scores. \n The consistency of PCC values shows that lecturers’ eval-\nuations correlate positively with the original grades. The \nsubjective evaluation methodology and the lecturers’ subtle, \nindividual evaluation styles, which are not immediately ap-\nparent in the analysis, may be causing the increase in MAD. \nAlthough the sample size was limited, the workshop pro-\nvided insights into grading consistency across different lec-\nturers. A higher level of grading consistency was observed \nspecifically for TA1 to TA3. However, to det ermine \nwhether TA4 is an outlier, a larger sample size would be \nnecessary. The workshop highlighted that , despite applying \n145\nanalytical rubrics, the challenge remains to maintain stand-\nardized assessment practices. This challenge is particularly \ntrue in an environment characterized by diverse teaching \nmethods and the individual perspectives of lecturers. Con-\nfronting the complexities of evaluation, we sought state -of-\nthe-art solutions to enhance evaluation consistency. \nLLMs for Consistency Improvement \nWe chose GPT-4 for its multimodal capabilities to process \ntext and image input without requiring programming skills  \nto evaluate its effectiveness in evaluating transfer assign-\nments. Three experiments were conducted in the German \nlanguage using TA2. \n First, we uploaded three documents to GPT -4, including \nthe requirements for the Transfer Assignments in General \nBusiness Administration, the SIB Guide for Written Assign-\nments, the TA2 in PDF format, and the analytical evaluation \nrubric. On this basis, GPT-4 was instructed to evaluate TA2, \nwhich was graded low by all lecturers and had a low MAD \ndeviation of 5.33. On the first attempt, GPT-4 provided min-\nimal feedback but did not fully meet the evaluation  rubric, \nscoring the TA2 with 52 points, approximately 87% of the \npossible points. \n In the second test, we replaced the Excel rubric with a \nPDF document detailing how to evaluate. GPT -4 graded \nTA2 with 50 out of 60 points. Again, the feedback for im-\nprovement indicated that the evaluation did not fully adhere \nto the provided assessment rubric. \n On the third attempt, all the information necessary for \nevaluation was included in the prompt, followed by the text \nof the TA2 itself. GPT-4 graded the paper with the maxi-\nmum possible points this time, giving feedback that TA2 \nfully met all evaluation aspects. \n Based on our experiment’s results,  ChatGPT-4 may not \nbe the most suitable tool for evaluating transfer assignments. \nThe feedback and grading provided by GPT -4 were incor-\nrect, suggesting that further investigation is needed to deter-\nmine its effectiveness. The results differed significantly \nfrom the lecturers’ evaluation, indicating that there may be \nsome limitations to the current approach. \nFuture Ideas \nWhile initial testing of GPT-4 has revealed its susceptibility \nto hallucinations, it is essential to investigate the potential \napplications of GPT -4 and other LLMs such as Google \nBard, Google Gemini, and Meta ’s Llama 2. The focus \nshould also be identifying prompts that can effectively as-\nsess the diverse aspects of transfer assignments, including \ninterpreting and processing visual elements such as figures \nand tables. \n More research is needed to establish the efficiency of dif-\nferent NLP techniques in assessing particular aspects and in-\nvestigate alternative ways to generate feedback. This can in-\nvolve analyzing the content of transfer assignments and in-\ncorporating previous ly evaluated transfer assignments into \nindependent LLMs. \n Another inquiry is the feasibility of developing a pipeline \ncomprising multiple evaluation methods, with each transfer \nassignment undergoing these methods to address all relevant \nevaluation aspects. Investigating a range of evaluation meth-\nods for the six criteria and the 16 specific evaluation aspects \ncan enhance the precision and granularity of the results. \nTransfer assignments would be subjected to various evalua-\ntion stages designed for distinct criteria or aspects to achieve \na comprehensive assessment, much like traditional lecturer \nevaluations. \n Moreover, it is essential to incorporate the practical expe-\nriences of lecturers who evaluate these transfer assignments \ninto the system. Their expertise and insights should be inte-\ngral to the evaluation process , implying that the expertise \nand insights of lecturers should be incorporated into training \nof LLMs. It is essential to distinguish which evaluation as-\npects are suitable for LLM assessment and which require al-\nternative methods. \nConclusion \nIn conclusion, exploring automated essay evaluation sys-\ntems, mainly using LLMs like ChatGPT-4, presents a trans-\nformative opportunity in academic assessment. This posi-\ntion paper has delved into the historical and current land-\nscapes of AEE systems, highlighting their evolution from \nsimple grading systems to more complex and nuanced ap-\nproaches like NLP and LLMs. Through our case study at the \nSIB, we have demonstrated the potential and challenges of \nemploying LLMs, like GPT-4, in assessing student transfer \nassignments written in German. \n As evidenced by our “Working Alone, Together” method, \nthere is a need for more standardized and objective assess-\nment tools. While LLMs offer promising text processing \nand understanding capabilities, our experiments revealed \ntheir limitations in accurately evaluating complex academic \ntexts according t o predefined criteria. These findings high-\nlight the gap between the current abilities of LLMs and the \nnuanced requirements of academic essay evaluation. \n It is essential to continue research in this field, exploring \nvarious LLMs and their potential for more sophisticated and \nreliable essay evaluation. Developing an autonomous, tai-\nlored LLM, trained on a corpus of pre-assessed assignments, \ncould pave the way for more accurate and consistent grading \nsystems, streamline the assessment process, and enhance the \neducational experience by providing students with detailed, \nconstructive feedback. \n146\nReferences \nAttali, Y.; and Burstein, J. 2006. Automated Essay Scoring \nWith e-rater® V.2. The Journal of Technology, Learning, \nand Assessment 4(3). \nAttali, Y. 2011. Sequential Effects in Essay Ratings. Educa-\ntional and Psychological Measurement 71(1): 68 -79. \ndoi.org/10.1177/0013164410387344. \nBeigman Klebanov, B.; and Madnani, N. 2020. Automated \nEvaluation of Writing – 50 Years and Counting. In Proceed-\nings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics. Association for Computational Lin-\nguistics. 10.18653/v1/2020.acl-main.697. \nBruns, H. C. 2013. Working Alone Together: Coordination \nin Collaboration Across Domains of Expertise. Academy of \nManagement Journal 56(1): 62 -83. \ndoi.org/10.5465/amj.2010.0756. \nHayland, K. 2013. Writing in the university: education, \nknowledge and reputation. Language Teaching 46(1): 53-\n70. 10.1017/S0261444811000036 \nHussein, M. A.; Hassan, H.; and Nassef, M. 2019. Auto-\nmated language essay scoring systems: a literature review. \nPeerJ Computer Science. 5:e208. doi.org/ 10.7717/peerj -\ncs.208. \nIfenthaler, D. 2023. Automated essay grading systems. In \nHandbook of open, distance and digital education, edited by \nO. Zawacki-Richter and I. Jung, 1-15. Singapore: Springer. \nKe, Z.; and Ng, V. 2019. Automated Essay Scoring: A Sur-\nvey of the State of the Art. In Proceedings of the Twenty -\nEighth International Joint Conference on Artificial Intelli-\ngence. International Joint Conferences on Artificial Intelli-\ngence Organization. doi.org/10.24963/ijcai.2019/879. \nMasikisiki, B.; Marivate, V.; and Hlophe, Y. 2023. Investi-\ngating the Efficacy of Large Language Models in Reflective \nAssessment Methods through Chain of Thoughts Prompt-\ning. arXiv preprint. arXiv:2310.00272 [cs.CL]. Ithaca, NY: \nCornell University Library. \nPage, E. B. 1966. The Imminence of... Grading Essays by \nComputer. The Phi Delta Kappan 47(5): 238-234. \nPage, E. B. 1968. The Use of the Computer in Analyzing \nStudent Essay. International Review of Education 14(2): \n210-225. \nRamesh, D.; and Sanampudi, S. K. 2021. An automatic es-\nsay scoring system: a systematic literature review. Artificial \nIntelligence Review 55: 2495 -2527. \ndoi.org/10.1007/s10462-021-10068-2. \nStrasser, A. 2023. On pitfalls (and advantages) of sophisti-\ncated large language models. arXiv preprint. arXiv: \n2303.17511 [cs.CY]. Ithaca, NY: Cornell University Li-\nbrary. \nValenti, S.; Neri, F.; and Cucchiarelli, A. 2003. An Over-\nview of Current Research on Automated Essay Grading. \nJournal of International Technology Education 2. \ndoi.org/10.28945/331. \nWilliamson, D.; Xi, X.; and Breyer, F. 2012. A Framework \nfor Evaluation and Use of Automated Scoring. Educational \nMeasurements: Issues and Practice 31(1): 2 -13. \n10.1111/j.1745-3992.2011.00223.x. \nWilson, J.; and Rodrigues, J. 2020. Classification accuracy \nand efficiency of writing screening using automated essay \nscoring. Journal of School Psychology 82: 123-140. \ndoi.org/10.1016/j.jsp.2020.08.008. \nZupanc, K; and Bosni ć, Z. 2015. Advances in the Field of \nAutomated Essay Evaluation. Informatica 39(4): 383-395. \nZupanc, K.; and Bosnić, Z. 2017. Automated essay evalua-\ntion with semantic analysis. Knowledge-Based Systems 120: \n118-132. doi.org/10.1016/j.knosys.2017.01.006. \nZupanc, K.; and Bosnić, Z. 2018. Increasing accuracy of au-\ntomated essay grading by grouping similar graders. In Pro-\nceedings of the 8th International Conference on Web Intel-\nligence, Mining and Semantics - WIMS 18. New York: As-\nsociation for Computing Machinery. \ndoi.org/10.1145/3227609.3227645. \n147",
  "topic": "Grading (engineering)",
  "concepts": [
    {
      "name": "Grading (engineering)",
      "score": 0.6678215861320496
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6464977264404297
    },
    {
      "name": "German",
      "score": 0.6241774559020996
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.4265784025192261
    },
    {
      "name": "Psychology",
      "score": 0.3269254267215729
    },
    {
      "name": "Political science",
      "score": 0.3217109441757202
    },
    {
      "name": "Computer science",
      "score": 0.2488732933998108
    },
    {
      "name": "Engineering",
      "score": 0.19183135032653809
    },
    {
      "name": "Linguistics",
      "score": 0.17301809787750244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.11594212055206299
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I112859197",
      "name": "Università di Camerino",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I2972652528",
      "name": "FHNW University of Applied Sciences and Arts",
      "country": "CH"
    }
  ]
}