{
    "title": "Earthquake transformer—an attentive deep-learning model for simultaneous earthquake detection and phase picking",
    "url": "https://openalex.org/W3047855151",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2805409373",
            "name": "S Mostafa Mousavi",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A699407392",
            "name": "William L. Ellsworth",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2230757880",
            "name": "Weiqiang Zhu",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4212146493",
            "name": "Lindsay Y. Chuang",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2089789678",
            "name": "Gregory C. Beroza",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2805409373",
            "name": "S Mostafa Mousavi",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A699407392",
            "name": "William L. Ellsworth",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2230757880",
            "name": "Weiqiang Zhu",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4212146493",
            "name": "Lindsay Y. Chuang",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2089789678",
            "name": "Gregory C. Beroza",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2954286821",
        "https://openalex.org/W2910087333",
        "https://openalex.org/W2912815540",
        "https://openalex.org/W2937917695",
        "https://openalex.org/W2942977592",
        "https://openalex.org/W2895546528",
        "https://openalex.org/W2798828763",
        "https://openalex.org/W2798961812",
        "https://openalex.org/W2528961483",
        "https://openalex.org/W2102919512",
        "https://openalex.org/W2980395728",
        "https://openalex.org/W6713134421",
        "https://openalex.org/W2166423844",
        "https://openalex.org/W2050123244",
        "https://openalex.org/W2605668928",
        "https://openalex.org/W1504877043",
        "https://openalex.org/W2099857446",
        "https://openalex.org/W2044291679",
        "https://openalex.org/W2461698768",
        "https://openalex.org/W2163379050",
        "https://openalex.org/W2065566528",
        "https://openalex.org/W2559842663",
        "https://openalex.org/W2560061849",
        "https://openalex.org/W2992786058",
        "https://openalex.org/W2514408638",
        "https://openalex.org/W2945306713",
        "https://openalex.org/W2762410434",
        "https://openalex.org/W2781891981",
        "https://openalex.org/W2969262604",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W6632455782",
        "https://openalex.org/W423095831",
        "https://openalex.org/W2530876040",
        "https://openalex.org/W2797187289",
        "https://openalex.org/W2529337537",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4234552385",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964089206",
        "https://openalex.org/W2327501763",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2726264694",
        "https://openalex.org/W2962901607",
        "https://openalex.org/W2963187786",
        "https://openalex.org/W6682137061",
        "https://openalex.org/W3121383166",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2951527505",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2794417179",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W3102794316",
        "https://openalex.org/W2964059111",
        "https://openalex.org/W3021034800",
        "https://openalex.org/W2011301426"
    ],
    "abstract": null,
    "full_text": "ARTICLE\nEarthquake transformer— an attentive deep-\nlearning model for simultaneous earthquake\ndetection and phase picking\nS. Mostafa Mousavi 1✉, William L. Ellsworth 1, Weiqiang Zhu 1, Lindsay Y. Chuang2 & Gregory C. Beroza1\nEarthquake signal detection and seismic phase picking are challenging tasks in the processing\nof noisy data and the monitoring of microearthquakes. Here we present a global deep-\nlearning model for simultaneous earthquake detection and phase picking. Performing these\ntwo related tasks in tandem improves model performance in each individual task by com-\nbining information in phases and in the full waveform of earthquake signals by using a\nhierarchical attention mechanism. We show that our model outperforms previous deep-\nlearning and traditional phase-picking and detection algorithms. Applying our model to\n5 weeks of continuous data recorded during 2000 Tottori earthquakes in Japan, we were able\nto detect and locate two times more earthquakes using only a portion (less than 1/3) of\nseismic stations. Our model picks P and S phases with precision close to manual picks by\nhuman analysts; however, its high efﬁciency and higher sensitivity can result in detecting and\ncharacterizing more and smaller events.\nhttps://doi.org/10.1038/s41467-020-17591-w OPEN\n1 Geophysics Department, Stanford University, 397 Panama Mall, Stanford, CA 94305-2215, USA.2 School of Earth and Atmospheric Sciences, Georgia\nInstitute of Technology, Atlanta, GA 30332, USA.✉email: mmousavi@stanford.edu\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 1\n1234567890():,;\nD\neep learning is a widely applied and effective method for a\nbroad range of applications1. Earthquake monitoring has\na growing need for more efﬁcient and robust tools for\nprocessing of increasingly large data volumes, is conceptually\nstraightforward, and has a large quantity of available labeled data,\nwhich make earthquake detection and phase picking attractive\ntargets for the new wave of machine learning applications in\nseismology. To date, earthquake signal detection and phase-\npicking form the largest portion of this relatively young sub-\nﬁeld\n2–10. Despite the differences in approaches and results, most\nof these studiesﬁnd important advantages to deep-learning-based\nmethods compared with traditional approaches11,12.\nEarthquake signal detection and phase picking are challenging\nproblems in earthquake monitoring. Detection refers to identiﬁ-\ncation of earthquake signals among a wide variety of non-\nearthquake signals and noise recorded by a seismic sensor. Phase\npicking is the measurement of arrival times of distinct seismic\nphases (P-wave and S-wave phases) within an earthquake signal\nthat are used to estimate the location of an earthquake. Although\nthese two tasks share some similarities, their objectives are not\nquite the same. Minimizing the false negative and false positive\nrates are the main goals in detection; however, in phase picking\nthe focus is on increasing the temporal accuracy of arrival-time\npicks. This is due to the extreme sensitivity of earthquake location\nestimates to earthquake arrival time measurements - 0.01 second\nof error in determining P-wave arrivals can translate to tens of\nmeters of error in location. Although both detection and picking\ncan be viewed as identifying distinct variations in time-series\ndata, phase picking is a local problem compared to detection,\nwhich uses a more global view of the full waveform and consists\nof information from multiple seismic phases including scattered\nwaves. Because of this, previous machine-learning studies have\napproached these tasks individually using separate networks;\nhowever, these tasks are closely related to each other. In practice,\nanalysts ﬁrst look at the entire waveform on multiple stations to\nidentify consistent elements of an earthquake signal (e.g. P, S,\ncoda and surface waves) with a speciﬁc ordering (P-wave always\narrives before S-wave, higher frequency body waves always pre-\ncede dispersive surface waves etc.) to determine whether or not a\nsignal is from an earthquake. Then they focus on each phase to\npick the arrival times precisely. This practice indicates the\ninterconnection of these two tasks and the importance of con-\ntextual information in earthquake signal modeling.\nDeep-learning detection/picking models work by learning\ngeneral characteristics of earthquake waveforms and seismic\nphases from high-level representations. Here we test the\nhypothesis that better representations obtained by incorporating\nthe contextual information in earthquake waveforms will result in\nbetter models. Our expectation is that not all parts of a seismic\nsignal are equally relevant for a speciﬁc classiﬁcation task. Hence,\nit is beneﬁcial to determine the relevant sections for modeling the\ninteraction of local (narrow windows around speci ﬁc phase\narrivals) and global (full waveform) seismic features. We achieve\nthis by incorporating an attention mechanism13 into our network.\nAttention mechanisms in Neural Networks are inspired by\nhuman visual attention. Humans focus on a certain region of an\nimage with high resolution while perceiving the surrounding\nimage at low resolution and then adjusting the focal point over\ntime. Our model emulates this through two levels of attention\nmechanism, one at the global level for identifying an earthquake\nsignal in the input time series, and one at the local level for\nidentifying different seismic phases within that earthquake signal.\nWe introduce a new deep-learing model (EQTransformer1) for\nthe simultaneous detection of earthquake signals and pickingﬁrst\nP and S phases on single-station data recorded at local epicentral\ndistances (<300 km) based on attention mechanism. Here we\npresent our approach and compare its performance with multiple\ndeep-learning and traditional pickers and detectors. Our trained\nmodel is applied on 5 weeks of continuous waveforms recorded in\nJapan. The events we detect are located to demonstrate the gen-\neralization of the model to other regions and its ability to improve\nearthquake source characterization.\nResults\nNetwork architecture. Our neural network has a multi-task\nstructure consisting of one very-deep encoder and three separate\ndecoders composed of 1D convolutions, bi-directional and uni-\ndirectional long-short-term memories (LSTM), Network-in-Net-\nwork, residual connections, feed-forward layers, transformer, and\nself-attentive layers (Fig. 1). More details are provided in the\nmethod section. The encoder consumes the seismic signals in the\ntime domain and generates a high-level representation and con-\ntextual information on their temporal dependencies. Decoders\nthen use this information to map the high-level features to three\nsequences of probabilities associated with: existence of an earth-\nquake signal, P-phase, and S-phase, for each time point.\nIn self-attentive models the amount of memory grows with\nrespect to the sequence length; hence, we add a down-sampling\nsection composed of convolutional and max-pooling layers to the\nfront of the encoder. These down-sampled features are trans-\nformed to high-level representations through a series of residual\nconvolution and LSTM blocks. A global attention section at the\nend of the encoder aims at directing the attention of the network\nto the parts associated with the earthquake signal. These high-\nlevel features are then directly mapped to a vector of probabilities\nrepresenting the existence of an earthquake signal (detection)\nusing one decoder branch. Two other decoder branches are\nassociated with the P-phase and the S-phase respectively in which\nan LSTM/local attention unit is placed at the beginning. This local\nattention will further direct the attention of the network into local\nfeatures within the earthquake waveform that are associated with\nindividual seismic phases. Residual connections within each block\nand techniques such as network-in-networks help to expand the\ndepth of the network while keeping the error rate and training\nspeed manageable. As a result, our very deep network with 56\nlayers has only about 372 K trainable parameters. The network\narchitecture design is based on domain expertise. Optimization\nand hyperparameter selection are based on experiments on a\nlarge number of prototype networks.\nData and labeling . We used STanford EArthquake Dataset\n(STEAD)\n13 to train the network. STEAD is a large-scale global\ndataset of labeled earthquake and non-earthquake signals. Here\nwe used 1 M earthquake and 300 K noise waveforms (including\nboth ambient and cultural noise) recorded by seismic stations at\nepicentral distances up to 300 km. Earthquake waveforms are\nassociated with about 450 K earthquakes with a diverse geo-\ngraphical distribution (Fig.2). The majority of these earthquakes\nare smaller than M 2.5 and have been recorded within 100 km\nfrom the epicenter. A full description of properties of the dataset\ncan be found in13. Although STEAD contains earthquake wave-\nforms from a variety of geographical regions and tectonic settings,\nit does not have any earthquake seismograms from Japan. We\nsplit the data into training (85%), validation (5%), and test (10%)\nsets randomly. Waveforms are 1 minute long with a sampling rate\nof 100 Hz and are causally band-passedﬁltered from 1.0–45.0 Hz.\nA box-shaped label is used as ground truth for the detection. In\nthis binary vector, corresponding samples from the P arrival to\nthe S arrival+ 1.4 × (S - P time) are set to 1 and the rest to 0. We\ntested three different forms of: box, Gaussian, and triangular, to\nlabel phase arrivals. Triangular labeling resulted in a lower loss\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n2 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications\nand higher F-score during our hyperparameter selection proce-\ndure and is used for theﬁnal model. In this form, probabilities of\nP and S are set to 1 at theﬁrst arriving P and S wave and linearly\ndecrease to 0 within 20 samples before and 20 samples after each\nphase arrival.\nTraining. For both convolutional and LSTM units, all the weight\nand ﬁlter matrices were initialized with a Xavier normal initiali-\nzer14 and bias vectors set to zeros. We used ADAM 15 with\nvarying learning rates for optimization while the learning rate\nvaried during training. The model took O(89) hours to complete\nConv 1D, 8 kr 11, MaxPooling/2\nConv 1D, 16 kr 9, MaxPooling/2\nConv 1D, 16 kr 7, MaxPooling/2\nConv 1D, 32 kr 7, MaxPooling/2\nConv 1D, 32 kr 5, MaxPooling/2\nConv 1D, 64 kr 5, MaxPooling/2\nConv 1D, 64 kr 3, MaxPooling/2\nRes CNN, 64 kr 3\nRes CNN, 64 kr 3\nRes CNN, 64 kr 3\nRes CNN, 64 kr 2\nRes CNN, 64 kr 2\n2 Upsampling, Conv 1D, 96kr3\n2 Upsampling, Conv 1D, 96kr5\n2 Upsampling, Conv 1D, 32kr5\n2 Upsampling, Conv 1D, 32kr7\n2 Upsampling, Conv 1D, 16kr7\n2 Upsampling, Conv 1D, 16kr9\n2 Upsampling, Conv 1D, 8kr11\nSigmoid layer\n2 Upsampling, Conv 1D, 96kr3\n2 Upsampling, Conv 1D, 96kr5\n2 Upsampling, Conv 1D, 32kr5\n2 Upsampling, Conv 1D, 32kr7\n2 Upsampling, Conv 1D, 16kr7\n2 Upsampling, Conv 1D, 16kr9\n2 Upsampling, Conv 1D, 8kr11\nSigmoid layer\n2 Upsampling, Conv 1D, 96kr3\n2 Upsampling, Conv 1D, 96kr5\n2 Upsampling, Conv 1D, 32kr5\n2 Upsampling, Conv 1D, 32kr7\n2 Upsampling, Conv 1D, 16kr7\n2 Upsampling, Conv 1D, 16kr9\n2 Upsampling, Conv 1D, 8kr11\nSigmoid layer\nDetection\nP\n1\nP\n1\nt\nP\n1\nt t\nPicking P-phase Picking S-phase\nBi-LSTM / NiN\nBi-LSTM / NiN\nLSTM\nLSTM LSTM\nTransformer—Global attention\nTransformer—Global attention\nLocal attention Local attention IIIII\nI\nFig. 1 Network architecture.Our network architecture. Full details of each block are provided in the method section. The convolutional layers read as\n(number of kernels) kr (kernel size).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 3\nthe training using 4 parallel Tesla-V100 GPUs under the ten-\nsorﬂow framework16. Training stopped when validation loss did\nnot improve for 12 consecutive epochs.The data was augmented\nby adding a secondary earthquake signal into the empty part of\nthe trace, adding a random level of Gaussian noise into earth-\nquake waveform, randomly shifting the event within the trace -\nthrough array rotation, randomly adding gaps (zeroing a short\ntime span) in noise waveforms, and randomly dropping one or\ntwo channels (zeroing values of one or two channels) with 0.3,\n0.5, 0.99, 0.2 and 0.3 probability respectively. Half of the data in\neach batch are augmented versions of the waveforms in the other\nhalf. Data augmentation and normalization (by standard devia-\ntion) are done simultaneously during the training on 24 CPUs in\nparallel. We used a dropout rate of 0.1 for all dropout layers both\nat training and test time.\nExploring the network’s attention. The attention weights deﬁne\nhow much of each input state should be considered for predicting\neach output and can be interpreted as a vector of importance\nweights. By explicitly visualizing these attention weights we can\nsee on what parts of the input sequence the neural network has\nlearned to focus.\nFigure 3 presents the output of each of these attention layers\n(summation of hidden states at all other time steps, weighted by\ntheir scoring) for one speciﬁc event from the evaluation set. We\ncan see that the network has learned to focus on different parts of\nthe waveform at different attention levels. This highlights the\nmost useful parts of the input waveform for each task. The shorter\npath through the detection decoder and its higher loss (due to\nlonger length of label) naturally force the network toﬁrst learn to\ndistinguish the earthquake signal within a time series. We can see\nthis from the learning curves as well (Supplementary Fig. 2). This\nmimics a seismic analyst’s decision-making workﬂow. The second\ntransformer (I in Fig.1), at the end of encoder section, mainly\npasses the information corresponding to the earthquake signal to\nthe subsequent decoders. This means that the encoder learns to\nselect what parts of the signal holds the most important\ninformation for detection and phase picking. This information\nis directly used by the detection decoder to predict the existence\nof an earthquake signal in the time series. The local attention\nlayers at the beginning of P and S decoders further focus on\nsmaller sections, within the earthquake waveform, to make their\npredictions. The alignment scores are normalized and can be\nthought of as probability distributions. So we can interpret the\nhierarchical attention mechanisms in our network as conditional\nprobabilities: P(earthquakesignal∣inputwaveform) = encoder\n(inputwaveform), and P(P_phase∣inputwaveform) = P_decoder(P\n(earthquakesignal∣inputwaveform)).\nResults and comparison with other methods. We used more\nthan 113 k test waveforms (both earthquake and noise examples)\nto evaluate and to compare the detection and picking perfor-\nmance of EQTransformer with other deep-learning and tradi-\ntional methods. Deep-learning models used here for the\ncomparisons are pre-trained models based on different training\nsets and all are applied to a common test set from STEAD. The\ntest set data contains 1-min long 3C-waveforms. All the tests are\nperformed without additionalﬁltering of the test data. Figure4\nillustrates the network predictions for 4 representative samples\nfrom the test set (Fig. 4a–d). The model works very well for\nearthquakes with different waveform shapes. The model is able to\nretain a global view for the detection while picking distinct arrival\ntimes with high temporal resolution. This can be seen clearly\nfrom the example in Fig.4b, where two strong and apparently\nseparate waves are detected as parts of a single event rather than\ntwo individual events. The very deep structure of the network\nmakes it less sensitive to the noise level and it works well for small\nevents with a high background noise (Fig.4c, d). Moreover, the\nprovided uncertainties can be useful to identify unreliable pre-\ndictions even when the output probabilities are high (Fig.4c).\nWe also applied the model to continuous data. The only\nreprocessing steps that need to be done prior to the test/\nprediction are: ﬁlling the gaps, removing the trend, band-pass\nﬁltering, and re-sampling the data to 100 Hz. Augmentations are\napplied only during the training process. After pre-processing, the\ncontinuous data can be sliced into 1-min windows (preferentially\nwith some overlap). The model can be applied on a single or a\nbatch of these 1-min slices. The normalization is done during\nfeeding the data to the model. Figure4e–h presents the results of\napplication of the model to continuous data recorded in\nRidgecrest, California and Tottori, Japan.\nFig. 2 The training and test dataset.Geographic distribution of station locations recording 300 k noise and 1 M earthquake seismograms in STanford\nEArthquake Dataset (STEAD) used in this study.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n4 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications\nThe augmentations used during the training process were\nessential in the performance of theﬁnal model. P arrival times are\nclustered at theﬁrst quarter of the windows in the training data\nand each trace only contains 1 event. However, Fig.4e, f shows\nthat regardless of these characteristics in the training set, the\nmodel works well when more than one event exists in a 1-min\nwindow and at various time points. The model can detect/pick\nevents that occur on the edges (Fig.4e) as long as 0.2 s of P and S\nexist within the window. Augmentations prevent the model from\nproducing false positives at abrupt changes dueﬁlling the gaps in\ncontinuous data (Fig. 3e). Our model works for single channel\ndata (Fig. 3h) or when other channels are broken or dominated\nby noise (Fig.3g). More examples of the model performance on a\nvariety of cases are provided in the supplementary materials\n(Supplementary Fig. 3–6).\nWe present the detection performance on the test set with a\nconfusion matrix (SupplementaryFig. 7). Our method resulted in\nonly 1 false positive with 0 false negatives (no missing events) out of\n113 K test samples using a threshold value of 0.5 (Supplementary\nFig. 8). To compare the performances, we applied three deep-\nlearning (DetNet5,Y e w s4,a n dC R E D7) detectors and one traditional\n(STA/LTA11) detector to the same test set (Table1). We should\nacknowledge that there is a level of tuning involved in each of these\napproaches (traditional and deep-learning detectors/pickers), and\nthat the performance can vary based on this tuning. Our proposed\nmodel outperforms the other methods in terms of F1-score. CRED\nalso contains both convolutional and recurrent units and was\ntrained on the same data set (STEAD); however, its performance did\nnot reach that of EQTransformer. This points to the beneﬁcial\neffects of the incorporated attention mechanism and the use of a\ndeeper network for earthquake signal detection. DetNet was trained\non a much smaller dataset compare to Yews, but it has a better\nperformance; however, neitherD e t N e tn o rY e w sr e a c ht h eS T A /\nLTA results in terms of F-score, and STA/LTA requires no training.\nWe now compare the picking performance with ﬁve\ndeep-learning (PhaseNet 8,G P D10,P p k N e t5,Y e w s4,P i c k N e t2)\n(Supplementary Fig. 9) and three traditional (Kurtosis17, Filter-\nPicker18, and AIC19) (Supplementary Fig. 10) auto pickers. We\ndid not ﬁnd a well-documented code or trained model for other\ndeep-learning pickers mentioned in section 2. These are pre-\ntrained models based on datasets of different sizes and from\ndifferent regions to evaluate their generalization. The list of these\ntraining sets are given in Tables2 and 3 for P and S picks. We\nassess the performance of each picker using 7 scores (standard\ndeviation of error, mean error, precision, recall, F1-score, mean\nabsolute error, and mean absolute percentage error). A pick was\nconsidered as a true positive when its absolute distance from the\nground truth was less than 0.5 second. EQTransformer increases\nthe F-scores of both P and S picking. The improvement in P-wave\npicks are more signiﬁcant than for S-wave picks. This may be due\nthe fact that picking S-waves is more difﬁcult and prone to more\nerrors, which can lead to higher labeling error in the training set.\nThe error distributions for some of the deep-learning pickers are\nnot uniform and cluster at sporadic times perhaps due to their\nmoving-window scheme. All of these models (GPD\n10, PpkNet5,\nand Yews4) use wider labels compared to the other models\n(PhaseNet8, PickNet 2, and EQTransformer). However, it is\ndifﬁcult to narrow down the exact reason behind their non-\nnormal error distributions.\nApplication to other regions . STEAD, the dataset used for\ntraining of our model, does not contain any waveform data from\nJapan. This makes Japan an ideal place to test the performance\nand generalization of our model. We select the aftershock region\nof the 2000 Mw 6.6 western Tottori earthquake for this test. Our\ndetector/phase-picker model was applied to continuous data of 18\nHiNet stations from 6 October to 17 November 2000. These are a\nportion of stations (57) originally used for studying this sequence\nby the Japan Meteorological Agency (JMA). The prediction\nmodule in EQTransformer code outputs the results when at least\none phase (P or S) with a probability above a speciﬁed threshold\nvalues exists over a time period with high probability of being an\n0\n–1\n0\nNormalized\namplitude\n1\n10 20 30\nDetection\n40 50\n0\n–1\n0\nNormalized\namplitude\n1\n10 20 30\nP wave\n40 50\n0\n–1\n0\nNormalized\namplitude\n1\n10 20 30\nS wave\nTime s\n40 50\nb\nc\na\nFig. 3 Attention weights.Input waveform overlain by contextual information - output of the attention layers fora transformer (I in Fig1), b local attention\nfor P-phase (II in Fig 1), andc the local attention for S-phase (III in Fig 1).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 5\n10,000\nMOD.BK_20141110123806_EV\nSNZO.IU_20011013124731_EV\nRDT.AV_20180104064826_EVDOM.SN_20170216151721_EV\nTrace Name: AKSH_NN_HH_2000-10-16T06::38:18.000000Z Trace Name: CA03_GS_HH_2019-07-08T22::37:39.964900Z\nTrace Name: WNM_Cl_EH_2019-07-02T05::52:06.000000ZTrace Name: SV02_ZY_HH_2019-07-12T01::45:42.005000Z\n0\n10,000\n–10,000\n0\n–100,000\n100,000\n0\n–200,000\n0\n–200,000\n05000\n–5000\n1\n0\n–250\n0\n250\n–250\n0\n250\n–250\n1\n0\n0\n250\n0\n500\n–500\n1000\n0\n0\n500\n–500\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n500\n5\n–5\n0\n0\n0\n5000\n–5000\n1.0\n0.8\n0.6\n0.4\nProbability\nAmplitude\ncounts Amplitude\ncounts\nAmplitude\ncounts\n0.00\n0.05\n–0.05\nAmplitude\ncounts\n0.00\n0.05\n–0.05\nAmplitude\ncounts\nAmplitude\ncounts\nAmplitude\ncounts\nAmplitude\ncounts\nAmplitude\ncount\nAmplitude\ncount\nAmplitude\ncount\n–50\n0\nAmplitude\ncount\n–50\n0\nAmplitude\ncount\n–50\n0\nAmplitude\ncount\n–25,000\n25,000\n0\nAmplitude\ncounts\nAmplitude\ncount\nAmplitude\ncount\nAmplitude\ncount\nAmplitude\ncount\n10,000\n–10,000\n0\nAmplitude\ncounts\n10,000\n–10,000\n0\nAmplitude\ncounts\nAmplitude\ncount\n0.2\n0.0\n1.0\n0.8\n0.6\n0.4\nProbability0.2\n0.0\n0\n100\n–100\nAmplitude\ncounts\n0\nProbability\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nProbability\nProbability\n1\n0\nProbability\nProbability\n1\n0\nProbability\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000\nSample\nSample\n4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000\nSample\n4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000\nEarthquake P_arrival S_arrival\nEarthquake P_arrival S_arrival Earthquake P_arrival S_arrival\nEarthquake P_arrival S_arrival\nZ\nPicked P\nPicked S\nZ\nPicked P\nPicked S\nN\nPicked P\nPicked S\nE\nPicked P\nPicked S\n3000\nSample Sample\n4000 5000 6000\nEqTransformer\n0.1.0\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000\nSample\n4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000\nSample\n4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000\nSample\n4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\n0 1000 2000 3000 4000 5000 6000\nEqTransformer\n0.1.0\nZ\nPicked P\nPicked S\nN\nPicked P\nPicked S\nE\nPicked P\nPicked S\nZ\nPicked P\nPicked S\nN\nPicked P\nPicked S\nE\nPicked P\nPicked S\nEqTransformer\n0.1.0\nEqTransformer\n0.1.0\na b\nc d\ne f\ng h\nFig. 4 Test results.Four representative waveforms (a–d) from the test set and four waveforms from applying the model to continuous data in Tottori, Japan (e)\nand Redgcrest, California (f–h) presenting performance of the model on different types of data. Each waveform is 60 seconds long with 100 samples per second\nband-bassﬁltered from 1-45 Hz. Each panel shows three channel waveforms on top and output predictions of the model for earthquake signal detection, P-arrival,\nand S-arrival at the bottom. Ina to d, the vertical color-coded lines onﬁrst two channels are manual arrival time picks from catalogs.a is an event with the local\nmagnitude of 2.2 recorded in 55 km distance from the epicenter,b is an event with the local magnitude of 4.3 recorded 173 km away from the epicenter,c is an\nevent with the local magnitude of 0.1 recorded 38 km away from the epicenter, andd is an event with the local magnitude of 2.0 recorded 110 km away from the\nepicenter. The output probabilities are presented as distributions that can represent variations or model uncertainties.e to h are detected events after applying the\nmodel to continuous data, representing the performance of the model when more than one event exists in a 1-minute window (e and f), when data contains gaps\nﬁlled by zeros (e), when an event occurs near the edge (e), when two channels are broken or noisy (g), or when only one component data exists (h).\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n6 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications\nearthquake. Here we used threshold values of 0.5, 0.3, and 0.3 for\ndetection, P-picking, and S-picking respectively. A batch size of\n500 and 30% overlapping is used during the pre-processing. We\nassociated phase picks to individual events based on detection\ntimes. Hypoinverse20 and HypoDD21 are used to locate and\nrelocate the associated events. Both travel time differences and\ncross-correlation were used for the relocation.\nWe detected and located 21,092 events within this time period\n(Fig. 5). This is more than a 2 fold increase in the number of\nevents compared to Fukuyama et al.22 who relocated 8521 events\nduring the same time period with hand-picked phases provided\nby the JMA. Our catalog includes almost all of the events reported\nby JMA. We also note that our results were obtained using only a\nsubset of the stations that were used by Fukuyama et al.22. About\n15 % of the associated events did not end up in theﬁnal catalog;\nhowever, this could be due to our simplistic association approach,\nand it is hard to assign them as false detections.\nWe used a local magnitude relationship 23 calibrated using\nreported magnitudes by JMA to estimated magnitudes of\nrelocated events. The majority of newly detected and located\nevents in our catalog are smaller earthquakes — with noisier\nwaveforms- compared to those previously reported by JMA\n(Fig. 6a). We estimate the magnitudes of completeness (Mc) for\nJMA and our catalog as 1.82 and 1.50 respectively using the\nmaximum curvature method24. While the frequency-magnitude\ndistribution result (Fig. 6a) indicates that our deep-learning\napproach is effective in detecting and characterizing up to 20\ntimes smaller microearthquakes, other factors such as better\nnetwork coverage and smaller station spacing are required to\ndecrease the overall magnitude of completeness25,26.\nIn total, JMA’s analysts picked 279,104 P and S arrival times on\n57 stations, while EQTransformer was able to pick 401,566 P and\nS arrival time on 18 of those stations (due to unavailability of data\nfor other stations). To compare the manual picks by JMA with\nTable 1 Detection performance.\nModel Pr Re F1 Training data Training size Ref .\nEQTransformer 1.0 1.0 1.0 Global 1.2M This study\nCRED 1.0 0.96 0.98 Global 1.2M 7\nDetNet 1.0 0.89 0.94 China 30K 5\nYews 0.84 0.85 0.85 Taiwan 1.4M 4\nSTA/LTA 0.91 1.0 0.95 —— 11\nPr, Re, and F1 are precision, recall, and F1-score respectively. EQTransformer and CRED have been trained on STEAD dataset while DetNet and Yews results are based on pre-trained models on different\ndatasets. Recursive STA/LTA algorithm is used here.\nBold values represent the best performance.\nTable 2 P-phase picking.\nModel μσ Pr Re F1 MAE MAPE Training data Training size Ref .\nEQTransformer 0.00 0.03 0.99 0.99 0.99 0.01 0.00 Global 1.2M This study\nPhaseNet −0.02 0.08 0.96 0.96 0.96 0.07 0.01 North California 780K 8\nGPD 0.03 0.10 0.81 0.80 0.81 0.08 0.01 South California 4.5M 10\nPickNet 0.00 0.09 0.81 0.49 0.61 0.07 0.02 Japan 740K 2\nPpkNet −0.01 0.15 0.90 0.90 0.90 0.10 1.90 Japan 30K 5\nYews 0.07 0.13 0.54 0.72 0.61 0.09 0.02 Taiwan 1.4M 4\nKurtosis −0.03 0.09 0.94 0.79 0.86 0.08 0.01 —— 17\nFilterPicker −0.01 0.08 0.95 0.82 0.88 0.14 0.02 —— 18\nAIC −0.04 0.09 0.92 0.83 0.87 0.09 0.01 —— 19\nμ and σ are mean and standard deviation of errors (ground truth— prediction) in seconds respectively. Pr, Re, and F1 are precision, recall, and F1-score respectively. MAEand MAPE are mean absolute\nerror and mean absolute percent error respectively. Note Yews and PpkNet models used here are trained based on different datasets mentioned in the related work section.\nBold values represent the best performance.\nTable 3 S-phase picking.\nModel μσ Pr Re F1 MAE MAPE Training data Training size Ref .\nEQTransformer 0.00 0.11 0.99 0.96 0.98 0.01 0.00 Global 1.2M This Study\nPhaseNet −0.02 0.11 0.96 0.93 0.94 0.09 0.01 North California 780K 8\nGPD 0.03 0.14 0.81 0.83 0.82 0.10 0.01 South California 4.5M 10\nPickNet 0.08 0.17 0.75 0.75 0.75 0.10 0.03 Japan 740K 2\nPpkNet 0.02 0.15 1.00 0.91 0.95 0.10 1.85 Japan 30K 5\nYews −0.02 0.13 0.83 0.55 0.66 0.11 0.01 Taiwan 1.4M 4\nKurtosis −0.10 0.13 0.89 0.39 0.55 0.11 0.01 —— 17\nFilterPicker −0.05 0.13 0.61 0.41 0.49 0.10 0.01 —— 18\nAIC −0.07 0.15 0.87 0.51 0.64 0.12 0.02 —— 19\nμ and σ are mean and standard deviation of errors (ground truth— prediction) in seconds respectively. Pr, Re, and F1 are precision, recall, and F1-score respectively. MAEand MAPE are mean absolute\nerror and mean absolute percent error respectively.\nBold values represent the best performance.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 7\nour automatic picks we used about 42,000 picks on the common\nstations and calculated the arrival time differences. The distribu-\ntions of these arrival time differences between the manual and\ndeep-learning picks for P and S waves are shown in Fig.6b. The\nstandard deviation of differences between picks are around\n0.08 second with mean absolute error of around 0.06 second or\n6 samples. Results are slightly better for S picks. The mean error is\nonly 1 sample (0.01 s).\nDiscussion\nThe better performance of the proposed method for phase\npicking could be due to several factors (e.g., quality and quantity\nof training set, architecture design, attention mechanism, depth of\nthe network, the augmentations used during training process,\netc). The attention mechanism helps to incorporate global and\nlocal scale features within the full waveform. A deeper network\nmight result in more discriminatory power through learning of a\nmore nonlinear mapping function.\nBased on the test set results for our picker, errors seem to\ncorrelate with noise level (Supplementary Fig. 11). A similar\ncorrelation is seen between the variations in the predictionsand\nbackground noise level (Supplementary Fig. 12). We did notﬁnd\na clear correlation between the estimated epistemic uncertainties\n(variations in the output probabilities) and picking errors. Alea-\ntory uncertainties might provide better estimates for picking\nconﬁdence intervals; however, such estimation of aleatory\nuncertainty for classiﬁcation tasks is difﬁcult\n27. Even so, knowl-\nedge of epistemic uncertainties and variability in the output\nprobabilities can be useful to reduce the false positive rate.\nSupplementary Fig. 13 presents examples of cultural noise\nrecorded in Western Texas resulting in a false positive. The\nimpulsive nature and frequency range of these arrivals makes it\nhard to differentiate them from an earthquake wave especially\nwhen a short window around the arrival is used. This can result in\npredicting a high probability for P or S picks. However, detection\nprobabilities based on longer windows exhibit a higher variations/\nuncertainties that can be used to eliminate the false defections.\nIncluding a large variety of anthropogenic and atmospheric noise\ninto a training set would be an effective way to reduce such false\npositives; however, reliable labeling of such noise is itself a chal-\nlenging task. Incorporating the spectral characteristics of the\nwaveforms during the training process\n7 might be another solution.\nPicking P waves tends to be more uncertain for waveforms\nrecorded at larger epicentral distances (Supplementary Fig. 12).\nThese higher uncertainties could be due to having fewer long-\ndistance waveforms in the training set and the fact that P waves\ncan be more difﬁcult to pick when theﬁrst arrival is emergent or\nis the diving wave Pn. The 1.0 Hz high-passﬁltering of the data\ncan also contribute to difﬁculty in picking the initial onset. As\nexpected, we observe higher uncertainties in picking smaller\nevents (Supplementary Fig. 12). We also note that lower predic-\ntion probabilities exhibit a higher uncertainty level and that the\nmodel outputs lower probabilities for P-wave picks with lower\nSNR, larger event-station distance, or smaller magnitude. Such\ntendencies are not as strong for the S-picks (Supplementary\nFig. 14).\nThe geographical location and the size of training data do not\nseem to be the main factor controlling performance. PhaseNet\nhas very good performance even thought it was trained on data\nab\ndc\n8521 Relocated events\n35.2°N\n133.2°E 133.4°E\nkm\n1050\n133.2°E 133.4°E 133.6°E\n35°N\n35.2°N\n35.4°N\n35°N\n21092 Relocated events\nkm\n1050\nFig. 5 Tottori Earthquakes.Seismicity of Tottori region between 6 October and 17 November 2000.a relocated events in Fukuyama et al.22 using manual\nphase picks by JMA.b relocated events using the automatic phase picker (EQTransformer) of this study.c Distribution of 57 seismic stations used by JMA\nand Fukuyama et al.22 d distribution of 18 stations used in our study to detect and locate earthquakes in Tottori region.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n8 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications\nonly from the Northern California. This indicates that deep-\nlearning models trained by data set in speciﬁc region can gen-\neralize well to other regions and that deep-learning pickers for\nmost purposes can be used off the shelf without retraining.\nPpkNet5 that was trained by 30 K waveforms resulted in higher F-\nscores for both P and S waves compared to the other models that\nwere built using much larger training sets. This suggests other\nfactors such as the network type (e.g., recurrent vs convolutional),\ntraining process (e.g. the use of augmentation), or/and quality of a\ntraining set can play a more important role than the size of the\ntraining set.\nThe precision of picking seems to be inﬂuenced more by the\nlabeling and training procedure. For instance, the sporadic error\ndistributions for P-picks in the Yews 4, GPD10, and PpkNet5\nresults may be due to their training procedure that render them\nsensitive to arrival times clustered at particular time points.\nCompared to the traditional pickers, deep-learning-based meth-\nods perform better for noisier waveforms - especially for the S\nwaves (Fig. 7).\nPerforming comparative analyses on models with different\nproperties is a very challenging task. Different models have\nadopted different labeling approaches and incorporated different\nnetwork designs. This results in different sets of hyperparameters\nthat can affect the model performance extensively. Quality of a\ntraining set and the training procedure are other important fac-\ntors that are hard to quantify and measure their inﬂuence. On the\nother hand, setting up a fair environment for the comparison and\nutilizing a reliable and independent benchmark are important for\na more unbiased assessment. Despite all these deﬁciencies, we\nhope to initiate such efforts and encourage our colleagues in the\nseismological community to pursue more rigorous testing and\ncomparative analyses to learn from and build on previous\nattempts.\nTraditional pickers do relatively well in the accuracy of arrival-\ntime picking while their main disadvantage is generally lower\nrecall and poorer performance in picking S phases (Tables2 and\n3). Non-symmetric error distributions of traditional pickers\n(Supplementary Figs. 9 and 10) are primarily due to skew\nintroduced by their systematic delay in picking the arrival times,\nwhich is more signiﬁcant for S waves; however, their comparable\nperformance to some of the deep-learning models indicates their\neffectiveness even though they do not require training. We also\nnote that the traditional pickers are not necessarily faster. For\ninstance, on a machine with a 2.7 GHz Intel Core i7 processor\nand 16 GB of memory it takes 62 hr and 12 min, 3 hr and 25 min,\nand 31 hr and 18 min for Kurtosis, FilterPicker, and AIC pickers\n(based on the python implementation in PhasePApy\n28) respec-\ntively to pick the entire test set, while EQTransformerﬁnished the\ndetection/picking in 2 hr and 28 min (on the same machine).\nOur applications of EQTransformer to Japanese data indicates\nhigh generalization and accuracy of the model. The precision of\narrival time picks by EQTransformer are comparable to manual\npicks, and its higher sensitivity results in more than twice the\nnumber of detected events. The newly detected events are not\n103\n102\nFrequency\nFrequency\n101\n100\n5000 MAE = 0.06 s\n/afii9839 = 0.01 s\n/afii9846 = 0.08 s\n4000\n3000\n2000\n1000\n0\n–0.4 –0.2\ntJMA – tEqT  S\n0.0 0.2 0.4\n–2 –1 0 1\nMagnitude\n234 5\nEQTransformer\nJMA\nP picks\n5000\n6000\nMAE = 0.05 s\n/afii9839 = –0.01 s\n/afii9846 = 0.07 s4000\n3000\n2000\n1000\n0\n–0.4 –0.2\nt\nJMA – tEqT  S\n0.0 0.2 0.4\nS picks\na\nb\nFig. 6 Distributions of frequency magnitude of earthquakes and picking errors. afrequency-magnitude distributions of located events in JMA catalog\nand relocated events in our catalog (EQTransformer). Magnitudes for all events have been estimated using a local magnitude scale.b distributions of\narrival-time differences (in second) between P (left) and S (right) picks by JMAʼs analysts and EQTransformer.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 9\nlimited to the two main faults active in the Tottori sequence, and\ninclude sparse seismicity under the easternﬂank of Mt. Daisen.\nThis result was attained using only a portion (less than 1/3) of the\nseismic stations, with relatively large station spacing, and a simple\nassociation method h. Using more advanced phase association\nalgorithms (e.g., Glass3 29) would likely further increase the\nnumber of events. The effectiveness of the method together with\nits high efﬁciency (the processing time for 1 month of continuous\ndata at 1 station is 23 minutes on a single CPU without uncer-\ntainty estimation) highlight the potential of the proposed method\nfor improving the earthquake monitoring.\nIn this paper, we present a multi-task network for simultaneous\nearthquake detection and phase picking based on a hierarchical\nattentive model. Our network consists of one deep encoder and\nthree separate decoders. Two levels of self-attention (global and\nlocal) are embedded into the network in a hierarchical structure\nthat helps the neural network capture and exploit dependencies\nbetween local and global features within an earthquake waveform.\nOur model has several distinctive characteristics: (1) it is theﬁrst\nhierarchical-attentive model of earthquake signal; (2) with 56\nactivation layers, it is the deepest network that has been trained\nfor seismic signal processing; (3) it has a multi-task architecture\nthat simultaneously performs the detection and phase picking\nwhile modeling the dependency of these tasks on each other\nthrough a hierarchical structure; (4) in addition to the prediction\nprobabilities, it provides output variations based on Bayesian\ninference; (5) it is theﬁrst model trained using a globally dis-\ntributed training set of 1.2 M local earthquake observations.\nMethods\nRelated work. Perol et al.30 used a network of 8 convolutional and one fully\nconnected layers to detect and cluster events based on three component waveforms.\nThey trained their network using ~2700 earthquake and ~700,00 noise waveform\nrecorded in Oklahoma and tested it on 209 events and ~131,000 noise waveform\nfrom the same region. They mainly compared their approach with similarity search\nmethods and concluded that a deep neural network can achieve superior perfor-\nmance in less computational time. Wu et al.\n31 applied a densely connected network\nof 7 fully convolutional layers to detect laboratory earthquakes (1000 samples) of\ndifferent sizes. Ross et al.10 trained a network of 4 convolutional and 2 fully\nconnected layers using 4.5 Million seismograms recorded in Southern California to\ndetect short windows of P-waves, S-waves, and noise. They applied the trained\nnetwork to 24 h of continuous data recorded at a single station in Bombay Beach,\nCalifornia and a single event recorded by multiple stations in Japan and showed\nthat deep neural networks are capable of detecting events with different waveform\nshapes than those used for the training without sacriﬁcing detection sensitivity.\nRoss et al.\n9 adopted a similar approach (3 convolutional and 2 fully connected\nlayers) for picking P arrival times. Zhu and Beroza8 used U-Net, a fully convolu-\ntional encoder-decoder network with skip connections, for an end-to-end picking\nof P and S phases. They trained their network using 780 K seismograms and tested\nit using 78 K seismograms recorded in Northern California. Mousavi et al.\n7 pro-\nposed a residual network of convolutional, bidirectional Long Short Term Memory\nunits, and fully connected layers for detecting earthquake signals in the time-\nfrequency domain. They used 500 K 3-component records of noise and tectonic\nearthquakes from North California for training the network and tested the per-\nformance of theﬁnal model by applying it to semi-synthetic data and one month of\ncontinuous seismograms recorded during a sequence of induced seismicity in\nCentral Arkansas. This study showed deep-learning-based models can generalize\nwell to seismic events with substantially different characteristics that are recorded\nin different tectonic settings and still achieve high precision even in the presence of\nhigh background noise levels with low computational cost. Pardo et al.\n6 also used\n~774 K seismograms form Northern California to train their two-stage phase\npicker. They used a convolutional network for a rough segmentaion of phasesﬁrst,\nand then in a second stage performed a regression to pick the arrival times. Zhou\net al.\n5 (~136 K augmented P and S waveforms) and Zhu et al.4 (~30 K) used seismic\ndata from the 2007 Wenchuan aftershock sequence in Sichuan, China to train\ndeep-leaning -based detectors and pickers. While Zhou et al.5 used two separate\nnetworks of an 8-layer convolutional networks and a two-layers of bidirectional\nGated Recurrent Units for detection and picking respectively, Zhu et al.4 used the\nsame network (11 convolutional and 1 fully connected layers) in a recursive\nmanner for both detection and at the cost of larger computational time. Dokht\net al.3 trained two separate networks each consisting of 4 convolutional and 2 fully\nconnected layers for detection and a rough estimation of P and S arrival times in\nthe time-frequency domain. They used ~162 K waveforms recorded in Western\nCanada for training. Wang et al.2 built two separate models based on modiﬁcation\nof VGG-16 network and ~740 K seismograms recorded in Japan for picking P and\nS arrival times respectively. Their models work for short time windows that are\nroughly centered around the S phase arrival. This centering is done using the\ntheoretical arrival times, which in practice are unknown without information about\nthe earthquake locations.\nThese studies not only differ in network architecture and overall approach, they\nalso employ different data pre-processing, augmentation techniques, use datasets of\ndifferent sizes, magnitude range, epicentral distances, event types, noise levels,\ngeographical locations, and report the results using different matrices (e.g.\naccuracy, precision, recall, F1-score, average precision, hit rates, absolute error,\npicking error, etc.) that make it difﬁcult to determine the relative performance,\nstrengths, and weaknesses of each method. This prevents the community from\nadopting and building on the most effective approach. This is due in part to the\nlack of a standard benchmark dataset with high quality labels to facilitate rigorous\ncomparisons. The data set used in this study (STanford EArthquake Dateset\n13)i sa\ncandidate standard benchmark for developing and comparing detection and phase\npicking algorithms for local earthquakes.\nNetwork design. Seismic signals are sequential time series consisting of different\nlocal (individual seismic phases) and more global (e.g. packages of body and sur-\nface waves and scattered waves) features. Hence, it is useful to retain the complex\ninteraction between the local and global dependencies in an end-to-end deep\nlearning model of seismic signals. Traditionally, recurrent neural networks have\nbeen used for such sequence modeling; however, relatively long duration seismic\nsignals require some down sampling prior to the recurrent layers to manage the\ncomputational complexity. Hence, a combination of recurrent and convolutional\nlayers has been shown to be an effective architecture for sequential modeling of\nseismic signals\n7. Building upon our previous work7, we introduce a multi-task\nnetwork of recurrent and convolutional layers that incorporates attention\nmechanism as well. Attention mechanism is a method of encoding sequence data in\nwhich elements within a sequence will be highlighted or down-weighted based on\ntheir importance or irrelevance to a task\n32–35. The overall structure of our network\nincludes one encoder and three separate decoders. The encoder consumes the\nseismic signal in the time domain and generates a high-level representation and\n8000\n7000\n6000\n5000\n4000 Frequency3000\n2000\n1000\n0\n02 0 4 0\nSNR db\nP-picks\nEQTransformer\nPhaseNet\nGDP\nKurtosis\nFilterBank\nAIC\n60 80 100\n6000\n5000\n4000\nFrequency\n3000\n2000\n1000\n0\n02 0 4 0\nSNR db\nS-picks\nEQTransformer\nPhaseNet\nGDP\nKurtosis\nFilterBank\nAIC\n60 80 100\nb\na\nFig. 7 Phase picking performance as a function of noise level.P( a) and S\n(b) phase picks as a function of signal-to-noise ratio (SNR) for three deep-\nlearning and three traditional pickers.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n10 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications\ncontextual information on their temporal dependencies. Decoders then use this\ninformation to map the high-level features to three sequences of probabilities\nassociated with: existence of an earthquake signal, a P-phase, and an S-phase\nrespectively.\nVery deep encoder. Several studies36–38 have shown in end-to-end learning from\nraw waveforms that employing deeper networks can be beneﬁcial for having more\nexpressive power, better generalization, and more robustness to noise in the\nwaveform. We build a very deep encoder, which is known to be important in\nperformance of a sequence-to-sequence model with attention.\nIn self-attentive models the amount of memory grows with the sequence length.\nHence, we add a down sampling section composed of convolutional and max-\npooling layers to the front end of the encoder. The encoder follows with several\nblocks of residual convolution layers and recurrent blocks including network-in-\nnetwork connections.\nConvolutional layers exploit local structure and provide the model with better\ntemporal invariance, thus typically yielding a better generalization. To be able to\nextend the depth of the network without degradation we use blocks of\nconvolutional layers with residual connections\n39 as depicted in39 (Supplementary\nFig. 15).\nLong-short term memory (LSTM)40 are speciﬁc types of recurrent neural\nnetworks commonly used for modeling longer sequences. The main element in a\nLSTM unit is a memory cell. At each time step, an LSTM unit receives an input,\noutputs a hidden state, and updates the memory cell based on a gate mechanism.\nHere we expand the bidirectional LSTM blocks by including Network-in-\nNetwork\n41 modules in each block to help increase the network’s depth without\nincreasing the number of learnable parameters (Supplementary Fig. 16). LSTM\nlayers prior to self-attentional layers, have been shown to be necessary for\nincorporating positional information\n42–44.\nAttention mechanism. We represent the output of an LSTM layer byH ¼f ht g2\nRn ´ dh as a sequence of vector elements (a high level representation of the original\ninput signal), where,n is the sequence length anddh is the dimension of the\nrepresentation. We calculate the self (internal) attention as follows45,46:\net;t0 ¼ σðWT\n2 ½tan hðWT\n1 ht þ WT\n1 ht0 þ b1Þ/C138 þ b2Þ ; ð1Þ\nαt;t0 ¼ expðet;t0 ÞP\nt0 expðet;t0 Þ ; ð2Þ\nct ¼\nXdh\nt0 ¼1 αt;t0 :ht0 ; ð3Þ\nwhere ht and ht0 are hidden state representations at time stepst and t0 respectively.\nW and b are weights matrices and bias vectors respectively.σ is the element-wise\nsigmoid function. αt;t0 are scalar scores (also called alignment) indicating pairwise\nsimilarities between the elements of the sequence. The attentive hidden state\nrepresentation, ct, at time stept is given by summation of hidden states at all other\ntime steps, ht0 , weighted by their similarities to the current hidden state,αt;t0 .\nVector ct 2 Rdh is a sequence of context-aware (with respect to surrounding ele-\nments) encoding that deﬁnes how much attention will be given to the features at\neach time step based on their neighborhood context. This will be incorporated into\na downstream task as an additional contextual information to direct the focus to\nthe important parts of the sequence and discard the irrelevant parts.\nWe adopt the residual attention blocks introduced in the Transformer47,48.W e\nreplace the multi-head scaled-dot product attention by above single-head additive\nattention (Supplementary Fig. 17). The feed-forward layer consists of two linear\ntransformations with a ReLU activation in between,\nFFðxÞ¼ maxð0; xW1 þ b1ÞW2 þ b2, intended to introduce additional\nnonlinearities.\nOur goal is to implement two levels of attention mechanisms in a hierarchical\nstructure46,49,50 at both the earthquake full waveform, and individual phase levels.\nA logical way to do this is to perform attention mechanisms at two levels with\ndifferent temporal resolutionsuch as: applying the detection attention at the high\nlevel representation at the end of the encoder and the phase attention at the end of\nassociated decoders where higher temporal resolutions are available. WithO(n2. d)\ncomplexity of self-attention, however, this is not computationally feasible for the\nlong duration time series (6000 samples) used here. Hence, we applied attention\nmechanisms with both global and local attentions at the bottleneck. The attention\nblock at the end of the encoder performs global attention, by attending to all the\npositions in the sequence, to learn to identify the earthquake signals within the\ninput time series. The shortened path from this layer to the detection encoder and\nthe naturally higher detection loss make this learning easier.\nAttention blocks at the beginning of phase-picker decoders perform additional\nlocal attention by attending only to a small subset of the sequence\n45— aiming to\nsharpen the focus to individual seismic phases within the earthquake waveform.\nOne LSTM layer with 16 units is applied before theﬁrst attention block at each\nlevel to provide position information42,51.\nUncertainty estimation. Model uncertainty is important in applied deep-learning,\nand for seismic monitoring; however, none of the previous deep-learning model for\nearthquake detection/phase picking provides a probabilistic output with a measure\nof model uncertainty. The predictive probabilities provided by these models are not\nequivalent to model conﬁdence. A model can be uncertain in its predictions even\nwith a high softmax output52.\nIn deep learning, the model uncertainty is usually estimated by inferring\ndistributions over the network weights. Due to the computational complexity, this\nis done by approximating the model posterior using inference techniques. Gal and\nGhahramani\n52 showed that dropout53, a technique commonly used to prevent\noverﬁtting, can be used for approximating Bayesian inference over the network’s\nweights. This is done by using the dropout at test time to impose a Bernoulli\ndistribution over the network’s weights. This is equivalent to Monte Carlo\nsampling from the posterior distribution over models. We implement a dropout\nafter every layer of our neural network and use it during both training and\nprediction.\nData availability\nSTanford EArthquake Dataset (STEAD) used for the training, validation, and test is\navailable at:https://github.com/smousavi05/STEAD. The continuous data for the Totorri\nregion was downloaded from HiNet (http://www.hinet.bosai.go.jp/about_data/?\nLANG=en). Maps andﬁgures in this paper were generated using the Generic Mapping\nTools and Matplotlib54. Catalog of all events detected and relocated in our study are\nprovided in the supplementary materials. Source data are provided with this paper.\nCode availability\nOur source code and model are available athttps://github.com/smousavi05/\nEQTransformer and can be used to apply the model to continuous data or reproduce\nresults presented in the paper.\nReceived: 11 March 2020; Accepted: 26 June 2020;\nReferences\n1. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning.Nature 521, 436 (2015).\n2. Wang, J., Xiao, Z., Liu, C., Zhao, D. & Yao, Z. Deep-learning for picking\nseismic arrival times.J. Geophys. Res. 124, 6612–6624 (2019).\n3. Dokht, R. M., Kao, H., Visser, R. & Smith, B. Seismic event and phase\ndetection using time-frequency representation and convolutional neural\nnetworks. Seismol. Res. Lett.90, 481–490 (2019).\n4. Zhu, L. et al. Deep learning for seismic phase detection and picking in the\naftershock zone of 2008 Mw7. 9 Wenchuan Earthquake.Phys. Earth Planet.\nInteriors. 293, 106261 (2019).\n5. Zhou, Y., Yue, H., Kong, Q. & Zhou, S. Hybrid event detection and phase-\npicking algorithm using convolutional and recurrent neural networks.Seismol.\nRes. Lett. 90, 1079–1087 (2019).\n6. Pardo, E., Gar ﬁas, C. & Malpica, N., Seismic phase picking using\nconvolutional networks. IEEE Trans. Geosci. Remote Sensing57, 7086–7092\n(2019).\n7. Mousavi, S. M., Zhu, W., Sheng, Y. & Beroza, G. C. CRED: a deep residual\nnetwork of convolutional and recurrent units for earthquake signal detection.\nSci. Rep. 9, 10267 (2019).\n8. Zhu, W. & Beroza, G. C. PhaseNet: a deep-neural-network-based seismic\narrival-time picking method.Geophys. J. Int.216, 261–273 (2018).\n9. Ross, Z. E., Meier, M.-A. & Hauksson, E. P wave arrival picking andﬁrst-\nmotion polarity determination with deep learning.J. Geophys. Res.123,\n5120–5129 (2018).\n10. Ross, Z. E., Meier, M.-A., Hauksson, E. & Heaton, T. H. Generalized seismic\nphase detection with deep learning.Bull. Seismol. Soc. Am.108, 2894–2901\n(2018).\n11. Allen, R. V. Automatic earthquake recognition and timing from single traces.\nBull. Seismol. Soc. Am.68, 1521–1532 (1978).\n12. Gibbons, S. J. & Ringdal, F. The detection of low magnitude seismic events\nusing array-based waveform correlation.Geophys. J. Int.165, 149–166\n(2006).\n13. Mousavi, S. M., Sheng, Y., Zhu, W. & Beroza, G. C. InSTanford EArthquake\nDataset (STEAD): A Global Data Set of Seismic Signals for AI(IEEE, 2019).\n14. Glorot, X. & Bengio, Y. Understanding the difﬁculty of training deep\nfeedforward neural networks.J. Mach. Learn. Res.9, 249–256 (2010).\n15. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint\nat https://arxiv.org/pdf/1412.6980.pdf (2014).\n16. Abadi, M. et al. Tensorﬂow: a system for large-scale machine learning. In12th\nSymposium on Operating Systems Design and Implementation(16), 265–283\n(Savannah, 2016).\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications 11\n17. Saragiotis, C. D., Hadjileontiadis, L. J. & Panas, S. M. PAI-S/K: a robust\nautomatic seismic P phase arrival identiﬁcation scheme. IEEE Trans. Geosci.\nRemote Sens. 40, 1395–1404 (2002).\n18. Lomax, A., Satriano, C. & Vassallo, M. Automatic picker developments and\noptimization: FilterPicker— A robust, broadband picker for real-time seismic\nmonitoring and earthquake early warning.Seismol. Res. Lett.83, 531–540\n(2012).\n19. Maeda, N. A method for reading and checking phase times in autoprocessing\nsystem of seismic wave data.Zisin 38, 365–379 (1985).\n20. Klein, F. W. Userʼs Guide to HYPOINVERSE-2000, a Fortran Program to\nSolve for Earthquake Locations and Magnitudes 4/2002 version. USGS, Open\nFile Report 02-171 Version, 1, 123. (2002).\n21. Waldhauser, F. & Ellsworth, W. L. A double-difference earthquake location\nalgorithm: method and application to the northern Hayward fault, California.\nBull. Seismol. Soc. Am.90, 1353–1368 (2000).\n22. Fukuyama, E., Ellsworth, W. L., Waldhauser, F. & Kubo, A. Detailed fault\nstructure of the 2000 western Tottori, Japan, earthquake sequence.Bull.\nSeismological Soc. Am.93, 1468–1478 (2003).\n23. Bakun, W. H. & Joyner, W. B. The ML scale in central California.Bull.\nSeismological Soc. Am.74, 1827–1843 (1984).\n24. Wiemer, S. & Wyss, M. Minimum magnitude of completeness in earthquake\ncatalogs: Examples from Alaska, the western United States, and Japan.Bull.\nSeismol. Soc. Am.90, 859–869 (2000).\n25. Mousavi, S. M. Comment on“Recent developments of the Middle East\ncatalog” by Zare et al.J. Seismol. 21, 257–268 (2017).\n26. Mousavi, S. M. Mapping seismic moment and b-value within the continental-\ncollision orogenic-belt region of the Iranian Plateau.J. Geodynamics 103,\n26–41 (2017).\n27. Mousavi, S. M. & Beroza, G. C. Bayesian-Deep-Learning Estimation of\nEarthquake Location From Single-Station Observations.IEEE. Trans. Geosci.\nRemote. Sens. https://doi.org/10.1109/TGRS.2020.2988770 (2020).\n28. Chen, C. & Holland, A. A. PhasePApy: a robust pure Python package for\nautomatic identiﬁcation of seismic phases.Seismol. Res. Lett.87, 1384–1396\n(2016).\n29. Yeck, W. L. et al. Glass3: a standalone multiscale seismic detection associator.\nBull. Seismol. Soc. Am.109\n, 1469–1478 (2019).\n30. Perol, T., Gharbi, M. & Denolle, M. Convolutional neural network for\nearthquake detection and location.Sci. Adv. 4, e1700578 (2018).\n31. Wu, Y. et al. DeepDetect: a cascaded region-based densely connected network\nfor seismic event detection.IEEE Trans. Geosci. Remote Sens.57,6 2–75\n(2018).\n32. Hu, D. An introductory survey on attention mechanisms in NLP problems.\nIn Proceedings of SAI Intelligent Systems Conference, pp. 432–448 (Springer,\nCham, 2019).\n33. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by jointly\nlearning to align and translate. Preprint athttps://arxiv.org/abs/1409.0473\n(2014).\n34. Xu, K. et al. Show, attend and tell: neural image caption generation with visual\nattention. In International Conference on Machine Learning, 2048–2057\n(2015).\n35. Hermann, K. M. et al. Teaching machines to read and comprehend. In\nAdvances in Neural Information Processing Systems, 1693–1701 (2015).\n36. Zhang, Y., Chan, W. & Jaitly, N., Very deep convolutional networks for end-\nto-end speech recognition. In2017 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 4845–4849 (2017).\n37. Tan, T. et al. Adaptive very deep convolutional residual network for noise\nrobust speech recognition.IEEE/ACM Trans. Audio Speech Lang. Process.26,\n1393–1405 (2018).\n38. Dai, W., Dai, C., Qu, S., Li, J. & Das, S. Very deep convolutional neural\nnetworks for raw waveforms. In2017 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 421–425 (2017).\n39. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image\nrecognition. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 770–778 (2016).\n40. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural Comput.9,\n1735–1780 (1997).\n41. Lin, M., Chen, Q. & Yan, S. Network in network. Preprint athttps://arxiv.org/\nabs/1312.4400 (2013).\n42. Sperber, M., Niehues, J., Neubig, G., Stüker, S. & Waibel, A. Self-attentional\nacoustic models. Preprint athttps://arxiv.org/abs/1803.09519 (2018).\n43. Chan, W., Jaitly, N., Le, Q. & Vinyals, O. Listen, attend and spell: a neural\nnetwork for large vocabulary conversational speech recognition. In2016 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP),\n4960–4964 (2016).\n44. Yang, Z., He, X., Gao, J., Deng, L. & Smola, A. Stacked attention networks for\nimage question answering. InProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition,2 1–29 (IEEE, 2016).\n45. Luong, M.-T., Pham, H. & Manning, C. D. Effective approaches to attention-\nbased neural machine translation. Preprint athttps://arxiv.org/abs/1508.04025\n(2015).\n46. Yang, Z., et al. Hierarchical attention networks for document classiﬁcation. In\nProceedings of the 2016 conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies,\n1480–1489 (2016).\n47. Vaswani, A., et al. Attention is all you need. InAdvances in Neural\nInformation Processing Systems, 5998–6008 (2017).\n48. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep\nbidirectional transformers for language understanding. Preprint athttps://\narxiv.org/abs/1810.04805 (2018).\n49. Ji, J. et al. A nested attention neural hybrid model for grammatical error\ncorrection. Preprint athttps://arxiv.org/abs/1707.02026 (2017).\n5 0 . Y u ,L . ,B a n s a l ,M .&B e r g ,T .L .H i e r a r c h ically-attentive RNN for album summari-\nzation and storytelling. Preprint athttps://arxiv.org/abs/1708.02977(2017).\n51. Mnih, V., Heess, N., Graves, A. et al. Recurrent models of visual attention. In\nAdvances in Neural Information Processing Systems, 2204–2212 (2014).\n52. Gal, Y. & Ghahramani, Z. Dropout as a bayesian approximation: representing\nmodel uncertainty in deep learning. InInternational Conference on Machine\nLearning, 1050–1059 (2016).\n53. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R.\nDropout: a simple way to prevent neural networks from overﬁtting. J. Mach.\nLearn. Res. 15, 1929–1958 (2014).\n54. Hunter, J. D. Matplotlib: a 2D graphics environment.https://doi.org/10.1109/\nMCSE.2007.55Comput.Sci (2007).\nAcknowledgements\nWe would like to thank Yijian Zhou for testing PpkNet models on our test set. Eiichi\nFukuyama, Kaz Imanishi, Shin Aoi, Takanori Matsuzawa helped us to acquire HiNet\ndata for the Tottori sequence. S.M.M. was supported by the Stanford Center for Induced\nand Triggered Seismicity (SCITS) and G.C.B. was supported by AFRL under contract\nnumber FA9453-19-C-0073.\nAuthor contributions\nS.M.M. designed the study, implemented the method, performed the tests and conducted\nthe analyses. W.L.E. designed the Tottori test and helped with the interpretation of\nanalyses results. W.Z. applied the PhaseNet on test set. L.C. applied Yews on the test\nset and performed some analyses. G.C.B. lead the project, and reviewed the manuscript.\nS.M.M. and G.C.B. wrote the manuscript. All authors discussed extensively the results.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationis available for this paper athttps://doi.org/10.1038/s41467-\n020-17591-w.\nCorrespondence and requests for materials should be addressed to S.M.M.\nPeer review informationNature Communications thanks William Yeck, Alberto\nMichelini and Christopher Johnson for their contribution to the peer review of this work.\nPeer reviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2020\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-17591-w\n12 NATURE COMMUNICATIONS|         (2020) 11:3952 | https://doi.org/10.1038/s41467-020-17591-w | www.nature.com/naturecommunications"
}