{
  "title": "Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA",
  "url": "https://openalex.org/W4389523927",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2259269324",
      "name": "Cheol Ryu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2551745056",
      "name": "Seolhwa Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2762895375",
      "name": "Subeen Pang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2578207332",
      "name": "Chanyeol Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2172081929",
      "name": "Hojun Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5104211450",
      "name": "Myeonggee Min",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4382260331",
      "name": "Jy-yong Sohn",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4284689799",
    "https://openalex.org/W2970728484",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4378770815",
    "https://openalex.org/W4296557505",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W4385849309",
    "https://openalex.org/W4385571412",
    "https://openalex.org/W4281257301",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W4393161188",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W2154652894"
  ],
  "abstract": "While large language models (LLMs) have demonstrated significant capabilities in text generation, their utilization in areas requiring domain-specific expertise, such as law, must be approached cautiously. This caution is warranted due to the inherent challenges associated with LLM-generated texts, including the potential presence of factual errors. Motivated by this issue, we propose Eval-RAG, a new evaluation method for LLM-generated texts. Unlike existing methods, Eval-RAG evaluates the validity of generated texts based on the related document that are collected by the retriever. In other words, Eval-RAG adopts the idea of retrieval augmented generation (RAG) for the purpose of evaluation. Our experimental results on Korean Legal Question-Answering (QA) tasks show that conventional LLM-based evaluation methods can be better aligned with Lawyers' evaluations, by combining with Eval-RAG. In addition, our qualitative analysis show that Eval-RAG successfully finds the factual errors in LLM-generated texts, while existing evaluation methods cannot.",
  "full_text": "Proceedings of the Natural Legal Language Processing Workshop 2023, pages 132–137\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nRetrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA\nCheol Ryu Seolhwa Lee Subeen Pang Chanyeol Choi\nLinq Labs\n{cheol.ryu,seolhwa.lee,subeen.pang,jacob.choi}@getlinq.com\nHojun Choi Myeonggee Min\nLaw&Good\n{hojun.choi,mgmin}@lawandgood.com\nJy-yong Sohn∗\nYonsei University\njysohn1108@yonsei.ac.kr\nAbstract\nWhile large language models (LLMs) have\ndemonstrated significant capabilities in text\ngeneration, their utilization in areas requiring\ndomain-specific expertise, such as law, must\nbe approached cautiously. This caution is war-\nranted due to the inherent challenges associ-\nated with LLM-generated texts, including the\npotential presence of factual errors. Motivated\nby this issue, we propose Eval-RAG, a new\nevaluation method for LLM-generated texts.\nUnlike existing methods, Eval-RAG evaluates\nthe validity of generated texts based on the re-\nlated document that are collected by the re-\ntriever. In other words, Eval-RAG adopts the\nidea of retrieval augmented generation (RAG)\nfor the purpose of evaluation. Our experimental\nresults on Korean Legal Question-Answering\n(QA) tasks show that conventional LLM-based\nevaluation methods can be better aligned with\nLawyers’ evaluations, by combining with Eval-\nRAG. In addition, our qualitative analysis show\nthat Eval-RAG successfully finds the factual\nerrors in LLM-generated texts, while existing\nevaluation methods cannot.\n1 Introduction\nRecent advances of large language models (LLMs)\nare remarkable, in various natural language gen-\nerative tasks such as translation or summariza-\ntion. However, LLMs are prone to generating\nhallucinated text (Ji et al., 2023), i.e., the texts\ngenerated by LLMs are sometimes having fac-\ntual errors, which implies the unreliability of the\nLLM-generated text. Consequently, it is necessary\nto develop a proper system of evaluating LLM-\ngenerated text, for using LLMs in real-world appli-\ncations.\nVarious methods are proposed for evaluating nat-\nural language generation (NLG) (Papineni et al.,\n2002; Lin, 2004; Zhang et al., 2020). However, it\nis known that such classical evaluation methods\n∗ Corresponding author\nEvaluator EQuery Q\nAnswer A\nLLM\nEval-RAG\nRetriever R Score  \nof the \ninput \n(Q, A)\nQuestion-answer pairs\nLegal provisions\nLegal cases\nDocuments\nRelevant \nDocument d\nd\nNLG\n(Q, A)\nQ\nFigure 1: The proposed Eval-RAG method for Legal\nQA task. Suppose an LLM generates answer A for the\nquery Q. Eval-RAG is a framework for evaluating the\nquality of the answer, using retriever R and evaluator E.\nFirst, R retrieves the document d relevant to the query\nQ. Second, E evaluates the score of (Q, A) based on\nthe contents of the retrieved document d.\nhave room for improvement compared with the hu-\nman evaluation, especially for evaluating question\nanswering (QA) tasks (Liu et al., 2023).\nTo address this issue of classical methods, recent\napproaches considered evaluating LLM-generated\ntexts by another LLM (Chan et al., 2023; Liu et al.,\n2023; Wang et al., 2023b). Although the LLM-\nbased evaluation methods outperforms classical\nmethods, they are suffering from innate issues of\nLLMs, such as the lack of training data on spe-\ncialized domains and the inability to reflect recent\ndata.\nMain Contributions In this paper, we propose\nEval-RAG, a retrieval-based method evaluating\nLLM-generated texts. Eval-RAG first retrieves the\ndocument relevant to the generation task, and then\nevaluates the texts based on the relevant docu-\nment. This method can be considered as applying\nretrieval-augmented generation (RAG) concept on\nthe existing LLM-based evaluation systems. Our\nexperimental results on Korean legal QA tasks\nshow that Eval-RAG outperforms existing evalua-\ntion methods, in terms of higher correlation with\nthe human evaluation. Our qualitative analysis also\nshows that Eval-RAG overcomes the limitation of\nexisting LLM-based evaluation methods which can-\nnot judge the hallucination of LLMs.\n132\nQuery  \nQ\nRetriever R\nSet of question-document pairs\n(q1, d1), (q2, d2), /uni22EF, (qN, dN)\nSentence embedding space\nDocuments  \ncorresponding to the \nclosest questions  \ndi\nqi\nConvert query & questions \ninto embedding vectors\nQq3\nq2q1\nq6\nq4\nq5\nTop-k d1\nd2\nd6\nFigure 2: The pipeline of retriever.\n2 Method\nHere we introduce the proposed Eval-RAG frame-\nwork depicted in Fig. 1, which has two main com-\nponents: the retriever R and the evaluator E. Once\nan LLM generates answer A for the query Q, Eval-\nRAG evaluates whether answer A is proper or not\nfor the query Q. First, the retriever R finds legal\ndocuments d that are relevant to the query Q. Sec-\nond, the evaluator E evaluates the validity of an-\nswer based on the related document d.\nRetriever Fig. 2 illustrates how retriever R re-\ntrieves the documents d related with the query Q.\nThis process contains three steps: data collection,\nsentence embedding, and query processing. In Step\n1, we collect three different types of publicly avail-\nable legal documents: (1) legal question-answer\npairs, (2) legal provisions and (3) legal cases (prece-\ndents). Since our focus is given to QA tasks, we\nconvert these documents into (question, document\ncontaining the answer) format, denoted by (q, d),\nin the following manner. Given the legal question-\nanswer (q, a) pairs, we extract related document\nd from the answer a. For other documents d (le-\ngal provisions and legal cases), we use LLMs to\ngenerate relevant questions q. Table 1 shows the\nexamples of question-document pair(q, d) for each\ndata source, which are translated from the Korean\nusing DeepL1. In Step 2, the question-document\npairs {(qi, di)}N\ni=1 collected in step 1 are stored\nin a vector database (Johnson et al., 2019) by us-\ning a sentence embedding model, which converts\na sentence q into an n-dimensional vector. In Step\n3, given query Q and the set of questions {qi}N\ni=1\ncollected in step 2, the retriever R first finds the\nquestion qi that is most similar to Q (in the sen-\ntence embedding space) and returns the relevant\ndocument di.\n1http://deepl.com/\nEvaluator The quality of answer A to a given\nquery Q is evaluated based on the relevant doc-\nument d, as shown in Fig. 1. Recall that exist-\ning LLM-based evaluators (Chan et al., 2023; Liu\net al., 2023; Wang et al., 2023b) ask LLMs for\nthe evaluation, where the prompts include Q and\nA, along with the evaluation criteria and methods\nfor evaluation. The evaluator proposed by us is\nbuilt on top of these methods, by adding the rele-\nvant document d (delivered from retrieverR) in the\nprompt. This allows E to evaluate based on the rel-\nevant documents. Note that our method (evaluating\nLLMs based on related documents) is motivated by\nthe RAG methods (which generates texts with the\naid of retrieved documents), and thus we call our\nevaluation method as Eval-RAG. Table 2 provides\nan example of how an associated document d is\nadded to the prompt of the existing method called\nFairEval (Wang et al., 2023b). It is worth noting\nthat Eval-RAG can be combined with any existing\nLLM-based evaluation methods.\n3 Experiments\nWe tested the performances of Eval-RAG and base-\nlines for Korean legal QA tasks, specifically fo-\ncused on the questions related to divorce.\nExperimental Setup The divorce-related QA\npairs used for the retriever are collected from Ko-\nrea Legal Aid Corporation2, while the legal pro-\nvisions and cases mentioned in the QA pairs are\ngathered from Korea Legislation Research Insti-\ntute3. The number of QA pairs, provisions, and\ncases used in the experiment were 287, 84, and\n240, respectively. we used GPT-4 to generate rel-\nevant questions for each provision and case. The\nquery set (n = 100) used in the experiment was\nselected from actual client consultation cases at the\nlaw firm Law&Good4. For each query, we prepared\nthree types of answers: an actual answer from a le-\ngal professional (Lawyer), an answer generated by\nassigning the role of a lawyer to ChatGPT ( gpt),\nand an answer generated with the inclusion of rel-\nevant documents (rag). We used OpenAI’s ‘text-\nembedding-ada-002’ sentence embedding model\nto store questions in vector database.\n2https://www.klac.or.kr\n3https://www.klri.re.kr\n4https://www.lawandgood.com\n133\nq: Does divorce evidence have an expiration date?\na: Yes, there is an expiration date for divorce evidence. Article 841 of the Civil Act of Republic of Korea stipulates that\ndivorce evidence is valid for six months from the date the spouse becomes aware of the evidence and two years from the\ndate of the grounds for filing.\nd: Article 841 of the Civil Act (Extinction of Right to Apply for Divorce due to Unchastity) The spouse may not file for\na divorce when the spouse has given a prior consent or an ex post facto forgiveness, or when six months have passed\nsince the date the spouse becomes aware of the event or two years have passed since the date of the event.\nq: I want to know what I can or should do with the items etc. that I gave to and received from the was-to-be-spouse as\nengagement gifts in case of a disengagement.\nprovision: Article 806 of the Civil Code (Termination of Engagement and Right to Claim Damages) Upon termination\nof an engagement, either party may claim compensation for damages from the negligent other party.\nd: Article 806 of the Civil Act (Dissolution of Matrimonial Engagement and Claims for Damages) Upon the termination\nof an engagement, a party may claim compensation for damages from the negligent other party.\nq: I would like to raise a claim for division of property on divorce upon the severance pay of the other party/spouse as\nwell. Is this possible?\ncase: Case No. 2013MU2250 (Main Issues and Holdings) Whether the future retirement benefits are subject to the\ndivision of property on divorce when one of the divorcees is still employed at the time of divorce and has not yet\nretired(affirmative) and the scope of such division.\nd: Case No. 2013MU2250 (Summary of Decision) Because it is required to work for a certain period of time to receive\nretirement benefits, as far as the cooperation of the spouse is recognized to have contributed to such work, the retirement\nbenefits may be subject to a division of property.\nTable 1: Examples of (question, relevant document) pair, denoted by (q, d), for three types of data sources: (1) legal\nquestion-answer pair (q, a), (2) legal provision, and (3) legal case.\nYou are a helpful and precise assistant for checking the quality of the answer. Please evaluate the answer based on stated\nrelevant law of South Korea.\n[Question]{question}\n[The Start of Assistant 1’s Answer] {answer_1} [The End of Assistant 1’s Answer]\n[The Start of Assistant 2’s Answer] {answer_2} [The End of Assistant 2’s Answer]\n[The Start of Relevant Law of South Korea]{document}[The End of Relevant Law of South Korea]\nTable 2: The prompt of Eval-RAG based on FairEval (Wang et al., 2023b). Black sentences are originally designed\nby FairEval, while blue sentences are designed by Eval-RAG.\nMethods & Performance MetricsWe compare\nthe performances of different methods: (1) the lat-\nest LLM-based evaluators, ChatEval (Chan et al.,\n2023) and FairEval (Wang et al., 2023b), (2) these\nlatest evaluators combined with Eval-RAG, and (3)\nthe human grading evaluated by the lawyers, where\nall scores are scaled with minimum score 1 and the\nmaximum score 10. Due to the limit of the avail-\nable prompt length of the evaluator, the retriever\nreturns only one relevant document of the question\nclosest to the query. The human grading for the\nthree types of answers for each query is considered\nas the benchmark, and we measured the correlation\nbetween this benchmark and each method, using\nPearson, Spearman, and Kendall correlation coeffi-\ncients.\nResult Table 3 compares different evaluation\nmethods, in terms of the correlation between the\nscore of each method and the human grading. One\ncan confirm that existing evaluators (FairEval and\nChatEval) can be improved when combined with\nEval-RAG, for both GPT-3.5 and GPT-4.\nTable 4 provides qualitative comparison of eval-\nuation methods, for a specific query Q. The table\non the top shows query Q, the answer A generated\nby GPT and RAG (denoted by (gpt) and (rag)),\nand the related document d. The bottom table com-\npares the evaluation of(gpt) and (rag) for different\nevaluation methods: Lawyer (human evaluation),\nChatEval and ChatEval combined with Eval-RAG.\nHere, (gpt) and (rag) have opposing views on\nthe divorce-related capital gains tax. Note that\nthe document d and the lawyer’s evaluation indi-\ncate that (rag) is correct and (gpt) is incorrect.\n134\nTable 3: Correlation between the score of each evalu-\nation method and the human grading. FairEval-RAG\nmeans FairEval combined with Eval-RAG.\nEvaluator Pearson Spearman Kendall\nFairEval (GPT-3.5) 0.0432 0.0515 0.0457\nFairEval-RAG (GPT-3.5) 0.1156 0.1264 0.1108\nFairEval (GPT-4) 0.4972 0.4999 0.4299\nFairEval-RAG (GPT-4) 0.5923 0.5841 0.4991\nChatEval (GPT-3.5) 0.1091 0.0896 0.0768\nChatEval-RAG (GPT-3.5) 0.0905 0.0816 0.0735\nChatEval (GPT-4) 0.4467 0.4541 0.4115\nChatEval-RAG (GPT-4) 0.5103 0.5129 0.4618\nHowever, ChatEval’s evaluation, based on LLM’s\nknowledge, fails to recognize that (gpt) is incor-\nrect. In contrast, Eval-RAG, referencing the nec-\nessary document d for evaluation, assesses (rag)\nas the more appropriate answer. This observation\nis consistent across both the GPT-3.5 and GPT-4\nmodels. More specifically, only the evaluation of\nChatEval-RAG (GPT-4) accurately states that(gpt)\nis incorrect based on document d.\n4 Related Work\nQA and LLM Question answering (QA) is a task\nof providing answers to a given question, which is\nexplored in last decades (Rogers et al., 2023; Zhu\net al., 2021; Bolotova et al., 2022). Recently, there\nare studies on QA for specific domains such as\nlaw (Ravichander et al., 2019; Wang et al., 2023a;\nLouis et al., 2023), and studies on long-form ques-\ntion answering (LFQA) which involves searching\nexternal documents relevant to a given query, and\nthen generating paragraph-length answers (Krishna\net al., 2021; Wang et al., 2022; Xu et al., 2023).\nThe area of QA is greatly advanced using large lan-\nguage models (LLMs) (OpenAI, 2023). However,\nLLMs have several limitations, such as the genera-\ntion of hallucinated text (Ji et al., 2023). One way\nof overcoming the limitations of LLMs is to us the\nRetrieval Augmented Generation (RAG) method,\nwhich integrates the retrieval module (which re-\ntrieves information from some data source) and\nthe text generation module (which generates texts\nbased on the retrieved information) in a unified\nframework (Lewis et al., 2020). RAG methods\nhave recently demonstrated strong performance in\nQA (Mialon et al., 2023; Lazaridou et al., 2022;\nKhattab et al., 2023).\nNLG Metrics Various types of metrics were de-\nveloped to measure the quality of natural language\ngeneration (NLG). Bilingual Evaluation Under-\nstudy (BLEU) (Papineni et al., 2002) is a widely\nused metric for evaluating machine translation\nsystems. It measures the similarity between the\nmachine-generated output and the reference trans-\nlation based on n-gram precision. Recall-Oriented\nUnderstudy for Gisting Evaluation (ROUGE) (Lin,\n2004) is a family of metrics for evaluating text sum-\nmarization systems. ROUGE measures the similar-\nity between the machine-generated summary and\nthe reference summary based on n-gram overlap\nand sentence-level overlap. BLEU (Papineni et al.,\n2002) and ROUGE (Lin, 2004) are also used for\nQA evaluation, but are not suitable because they\nare designed as translations and summaries, respec-\ntively. BERTScore (Zhang et al., 2020) is a evalu-\nation metric for similarity between two sentences,\nby using pre-trained BERT models. BERTScore\nis shown to outperform other metrics in QA tasks.\nHowever, BERTscore is not reliable when the texts\ncontain domain-specific knowledge due to the limi-\ntations of BERT model (Chalkidis et al., 2020).\nLLM evaluation Recently, evaluating generated\ntexts using LLMs is proposed (Fu et al., 2023; Liu\net al., 2023; Wang et al., 2023b; Chan et al., 2023).\nG-Eval (Liu et al., 2023) proposes probability-\nweighting schemes to calibrate the evaluation score\nfrom LLM. FairEval (Wang et al., 2023b) proposes\na new evaluation methodology, motivated by the\nobservation that the evaluation results can vary de-\npending on the order of inputs in the prompt. Chat-\nEval (Chan et al., 2023) proposes a novel approach\nwhere multiple agents engage in a discussion to\nevaluate the generated text. Our paper is built on top\nof these LLM-based evaluations, and outperforms\nthese baselines by using retrieval-based-evaluation.\n5 Conclusion\nIn this paper, we proposed Eval-RAG, a new\nmethod of evaluating LLM-generated texts, with\nthe aid of retrieved documents that are related with\nthe target task. Our experiments on Korean Legal\nQA tasks show that existing LLM-based evalua-\ntions can be better aligned with ground-truth hu-\nman evaluation, when combined with Eval-RAG.\n6 Ethical considerations\nThe example we tested the performance might in-\nclude sensitive case information of clients. How-\never, it is essential to emphasize that this ethical\nconcern is alleviated, as the samples in question\nhave been withheld from public disclosure.\n135\nQ: We would like to know whether the transfer of real estate ownership from one party to the other through a division\nof property on divorce is subject to the capital gains tax.\n(gpt): The transfer of real estate ownership from one party to the other in a divorce constitutes a transfer of property\nfor valuable consideration. Capital gains tax is a tax incurred under the Income Tax Act when properties such as real\nestate, etc. are transferred. However, the transfer of real estate resulting from the division of property in divorce may be\nexempted from capital gains tax under Article 9 of the Income Tax Act.\n(rag): According to the Supreme Court precedent Case No. 2002DU6422, the division of property constitutes a partition\nof the article jointly owned and the legal principles on a partition of the article jointly owned shall be applied. The\npartition of the article jointly owned is understood to be a change in the form of ownership by concentrating the\nco-owned share which was exercised in proportion to the co-owned share in the article jointly owned into an ownership\nof a specific part of the article and making it subsist only on that specific part of the article. For this reason, the division\nof property in a divorce is deemed to be a transfer of property for valuable consideration and thereby is not subject to\ncapital gains tax.\nd: Case No. 96NU14401 (Summary of Decision) A division of property in a divorce by agreement to liquidate the\nde facto marital property is substantially a partition of the article jointly owned in light of its legal nature, the object\nand scope of such division. Therefore, legal principles on a partition of the article jointly owned shall be apply mutatis\nmutandis, and the transfer of property by the result of such division is not a transfer of property for valuable consideration\nwhich is subject to capital gains tax.\nLawyer :The score of (gpt) = 4, the score of (rag) = 8\nChatEval (GPT-3.5) :After carefully reviewing the responses of both AI assistants, I found that (gpt) provided a more\naccurate and detailed answer. (gpt) correctly stated that transferring ownership of a property during divorce proceedings\nis considered a transfer of assets and may be subject to transfer income tax. Moreover, (gpt) provided the specific\nconditions that must be met for the transfer to be exempt from transfer income tax. In contrast,(rag) incorrectly\nstated that transferring ownership of a property during divorce proceedings is not considered a transfer of assets and\ntherefore not subject to transfer income tax. Therefore, the score of (gpt) = 8, The score of (rag) = 5.\nChatEval-RAG (GPT-3.5) :Based on the stated relevant law of South Korea, (rag)’s answer is more accurate and\nrelevant to the question. The law clearly states that the division of property in divorce is not considered a transfer of\nownership and therefore not subject to transfer income tax. (rag) correctly cites a relevant court case to support this.\n(gpt)’s answer is also helpful, but it does not provide as much detail on the specific law and court case. Therefore, the\nscore of (gpt) = 7, the score of (rag) = 9.\nChatEval (GPT-4) :(gpt) provided a detailed and accurate response, explaining the conditions under which the transfer\nof property due to divorce can be exempt from transfer income tax. This is in line with the Transfer Income Tax Law\nin South Korea. (rag), while citing a specific case, seems to have misunderstood the nature of property division in a\ndivorce and its tax implications. Therefore, the score of (gpt) = 9, the score of (rag) = 6.\nChatEval-RAG (GPT-4) :Based on the stated relevant law of South Korea, Case 2002DU6422, the division of property\nduring a divorce does not constitute a paid transfer of assets. (rag) correctly pointed this out and provided an accurate\ninterpretation of the law. On the other hand, (gpt) incorrectly statedthat the division of property during a divorce\nconstitutes a paid transfer of assets and could be subject to capital gains tax. However, (gpt) did provide detailed\ninformation about the conditions for exemption from capital gains tax, which could be useful information for the user.\nThe score of (gpt) = 6, the score of (rag) = 9.\nTable 4: (Top): An example query Q, the answers generated by GPT and RAG, and related document d. (Bottom):\nEvaluation of generated texts by Lawyer, ChatEval (Chan et al., 2023) and ChatEval-RAG, a short-hand notation for\ncombining ChatEval with Eval-RAG. The evaluation with Eval-RAG aligns better with the evaluation of Lawyer.\nReferences\nValeriia Bolotova, Vladislav Blinov, Falk Scholer,\nW. Bruce Croft, and Mark Sanderson. 2022. A non-\nfactoid question-answering taxonomy. In Proceed-\nings of the 45th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, page 1196–1207. Association for Comput-\ning Machinery.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 2898–\n2904. Association for Computational Linguistics.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.\n2023. Chateval: Towards better llm-based evaluators\nthrough multi-agent debate.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. GPTScore: Evaluate as you desire.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\n136\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2023. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive NLP.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus,\nFabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems, volume 33,\npages 9459–9474. Curran Associates, Inc.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNLG evaluation using GPT-4 with better human\nalignment.\nAntoine Louis, Gijs van Dijck, and Gerasimos Spanakis.\n2023. Interpretable long-form legal question answer-\ning with retrieval-augmented large language models.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey.\nOpenAI. 2023. GPT-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting on Association for Compu-\ntational Linguistics, page 311–318. Association for\nComputational Linguistics.\nAbhilasha Ravichander, Alan W Black, Shomir Wilson,\nThomas Norton, and Norman Sadeh. 2019. Question\nanswering for privacy policies: Combining compu-\ntational and legal perspectives. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4947–4958. Association\nfor Computational Linguistics.\nAnna Rogers, Matt Gardner, and Isabelle Augenstein.\n2023. QA dataset explosion: A taxonomy of NLP\nresources for question answering and reading com-\nprehension. ACM Comput. Surv., 55(10).\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,\nBinghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. 2023a. Large language models are not\nfair evaluators.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023b. Large language models are not fair evalua-\ntors.\nShufan Wang, Fangyuan Xu, Laure Thompson, Eunsol\nChoi, and Mohit Iyyer. 2022. Modeling exemplifi-\ncation in long-form question answering via retrieval.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2079–2092. Association for Computational\nLinguistics.\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol\nChoi. 2023. A critical evaluation of evaluations for\nlong-form question answering.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. BERTScore: Eval-\nuating text generation with BERT.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering.\n137",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6606106162071228
    },
    {
      "name": "Natural language processing",
      "score": 0.49387580156326294
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48130929470062256
    },
    {
      "name": "Information retrieval",
      "score": 0.41060441732406616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40185147523880005
    },
    {
      "name": "Mathematics",
      "score": 0.07252001762390137
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    }
  ],
  "cited_by": 9
}