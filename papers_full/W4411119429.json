{
  "title": "Prompt Compression for Large Language Models: A Survey",
  "url": "https://openalex.org/W4411119429",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2138784754",
      "name": "Zongqian Li",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2147175456",
      "name": "Yinhong Liu",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2481891025",
      "name": "Yixuan Su",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2098750953",
      "name": "Nigel Collier",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4398796143"
  ],
  "abstract": null,
  "full_text": "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 7182–7195\nApril 29 - May 4, 2025 ©2025 Association for Computational Linguistics\nPrompt Compression for Large Language Models: A Survey\nZongqian Li*, Yinhong Liu*, Yixuan Su, Nigel Collier\nUniversity of Cambridge\n{zl510, yl535, ys484, nhc30}@cam.ac.uk\nAbstract\nLeveraging large language models (LLMs) for\ncomplex natural language tasks typically re-\nquires long-form prompts to convey detailed\nrequirements and information, which results in\nincreased memory usage and inference costs.\nTo mitigate these challenges, multiple efficient\nmethods have been proposed, with prompt\ncompression gaining significant research in-\nterest. This survey provides an overview of\nprompt compression techniques, categorized\ninto hard prompt methods and soft prompt\nmethods. First, the technical approaches of\nthese methods are compared, followed by an\nexploration of various ways to understand their\nmechanisms, including the perspectives of at-\ntention optimization, Parameter-Efficient Fine-\nTuning (PEFT), modality integration, and new\nsynthetic language. We also examine the down-\nstream adaptations of various prompt compres-\nsion techniques. Finally, the limitations of cur-\nrent prompt compression methods are analyzed,\nand several future directions are outlined, such\nas optimizing the compression encoder, com-\nbining hard and soft prompts methods, and\nleveraging insights from multimodality. 1\n1 Introduction\nAs task complexity increases, prompts become\nlonger for LLMs to accommodate more detailed re-\nquirements, contextual information, and in-context\nlearning (ICL) examples. Lengthy prompts reduce\ninference speed, increase memory costs, and nega-\ntively impact user experience. Current methods for\nimproving LLM efficiency can be broadly catego-\nrized into model-centric and prompt-centric meth-\nods (Wan et al., 2024). Model-centric approaches,\nsuch as parameter pruning (Ma et al., 2023) and\nquantization (Dettmers et al., 2022), focus on opti-\nmizing the model itself. In contrast, prompt-centric\n*Co-first authors.\n1https://github.com/ZongqianLi/\nPrompt-Compression-Survey\nFigure 1: Illustrative examples of prompt compres-\nsion methods. Hard prompt methods remove low-\ninformation tokens or paraphrase for conciseness. Soft\nprompt methods compress text into a smaller number\nof special tokens, < cn >. The grids below visualize\nattentions, where the y-axis represents the sequence of\ntokens, and the x-axis shows the tokens they attend to.\n(Bottom left) Original prompt: Each token attends to\nall previous tokens. (Bottom middle) Hard prompt (fil-\ntering): Each token cannot attend to previous deleted\ntokens (Di). (Bottom right) Soft prompt (whole): After\nthe compression token (Ci) attends to all prior input to-\nkens (Ii), subsequent output tokens (Oi) cannot attend\nto tokens before the compression token.\nmethods, including prompt compression (Li et al.,\n2023) and prompt design (Shin et al., 2020), aim\nto improve the efficiency of LLM by lowering the\ncomplexity of input. Prompt-centric methods typ-\nically introduce minimal or no changes to the pa-\nrameters of the LLM, allowing them to be used in\na modular way. Thus, these methods, especially\n7182\nfor prompt compression as shown in Figure 1, have\ngained increasing attention. However, the opti-\nmal architectures and underlying mechanisms of\nprompt compression remain unclear, highlighting\nan important area for further investigation.\nThis survey aims to introduce LLM prompt com-\npression to people with prior knowledge of atten-\ntion mechanisms, transformers, and LLMs. Unlike\nprevious surveys on general efficient prompting\n(Chang et al., 2024), this paper specifically focuses\non prompt compression. The preliminary knowl-\nedge is introduced in Section 2, covering prompt-\ning and efficiency. This is followed by a detailed\ndiscussion of hard prompt methods in Section 3\nand soft prompt methods in Section 4. The key\nelements driving the evolution of prompt compres-\nsion models are examined, and insights into various\nmethods are provided. Their downstream adapta-\ntions are discussed in Section 5 as well. Finally,\nSection 6 analyzes the challenges of current prompt\ncompression techniques and proposes several po-\ntential solutions. An overview of prompt compres-\nsion methods and their downstream adaptations is\nshown in Figure 2.\nThis survey highlights that:\n• In addition to the exploration of existing prompt\ncompression methods, we provide our interpreta-\ntion of their mechanisms: filtering or paraphras-\ning in hard prompt methods, and attention mod-\nification, PEFT, modality integration, or a new\nsynthetic language in soft prompt methods.\n• We identify challenges in current methods, in-\ncluding fine-tuning problems such as overfitting\nand performance reduction, long compression\ntime, and the need for comparison with attention\noptimization techniques.\n• We propose several future directions to solve\ncurrent challenges, including optimizing the\ncompression encoder, combining hard and soft\nprompts, and integrating insights from multi-\nmodality.\n2 Preliminary\nPrompts are important for LLMs, broadly classi-\nfied into hard and soft prompts (Liu et al., 2023b).\nThey serve as input instructions to guide LLMs in\nperforming tasks such as summarization, classifica-\ntion, translation, and question answering without\nthe need for fine-tuning (Vatsal and Dubey, 2024).\nThe flexibility and effectiveness of prompts lever-\nage the generalization abilities of LLMs, influenced\nby elements such as wording, in-context examples,\nclarity, and accuracy, making prompt design an\nimportant area (Schulhoff et al., 2024).\nPrompt structures in this survey consist of two\nmain components: an instruction or context and\nan input or question, depending on the task. The\ninstruction, input, output format is commonly used\nin instruction fine-tuning datasets such as Alpaca\n(Taori et al., 2023) and PwC (Ge et al., 2024) for\ntasks such as content creation and language transla-\ntion. The context, question, answer format is preva-\nlent in QA and reading comprehension datasets\nsuch as SQuAD (Rajpurkar et al., 2016), HotpotQA\n(Yang et al., 2018), and RACE (Lai et al., 2017).\nIn few-shot scenarios, several demonstrations with\nthese formats are prepended at the beginning of\nthe prompt, which serves as additional instruc-\ntion or context to guide the behavior of the LLM.\nPrompts can vary significantly in length, with dif-\nferent prompt compression methods targeting some\nor all of these components.\nHard prompts are natural language prompts\nmade up of tokens from the vocabulary set of\nthe LLM, corresponding to specific words or sub-\nwords (Sennrich et al., 2016). These prompts can\nbe generated by either humans or models. While\nnatural language prompts are interpretable and pro-\nvide transparency, their inherent ambiguity can\nmake it difficult to fully capture intent in a con-\ncise manner. This limitation reduces the utility\nof hard prompts in diverse contexts and scenar-\nios. Additionally, creating effective and precise\nhard prompts requires considerable human effort,\nand can involve training a model to refine or op-\ntimize these prompts. Furthermore, variations in\nhard prompts can lead to differences in LLM per-\nformance for the same task.\nSoft prompts are trainable, continuous vectors\nthat share the same dimensions as token embed-\ndings in the dictionary of the LLM (Zhao et al.,\n2023). Different from hard prompts, the vectors in\nsoft prompts are trained to convey nuanced mean-\nings that cannot be captured by discrete tokens in\nthe predefined vocabulary. When tuned on diverse\ndatasets, soft prompts are expected to help the LLM\nperform well across different tasks. However, as\nthe dataset size grows, the computational resources\nneeded increase as well. Additionally, soft prompts\nare less explainable by humans compared to hard\nprompts, as their tokens are not directly readable.\nPrompt compression aims to reduce the length\nof prompts, thereby improving the efficiency of pro-\ncessing LLM inputs (Wan et al., 2024). There are\n7183\nFigure 2: Tree overview of prompt compression methods and their downstream adaptions. For downstream\nadaptations, compression methods not belonging to specific categories can be classified into general QA.\ntwo primary approaches to prompt compression:\nremoving unnecessary or low-information content\n(hard prompt methods) and learning continuous\nprompt vectors of the prompt information in the\nembedding space (soft prompt methods) (Chang\net al., 2024). Hard prompt methods act as a form\nof filtering, still using natural language tokens, al-\nthough the resulting prompts may be less fluent\nand grammatically correct, and can potentially gen-\neralize to LLMs with different embeddings. Soft\nprompt methods, on the other hand, employ encod-\ning to convert prompt information into continuous\nprompt vectors, resulting in special embeddings\nthat cannot be understood by humans. Different\nfrom KV compression, prompt compression only\nmanipulates on the input tokens, but not changes\nthe KV values for the input tokens during encod-\ning. Thus, the compression ratio is calculated by\nthe number of original input tokens divided by the\nnumber of compression tokens. The evaluation\nof prompt compression methods typically follows\ntwo approaches: comparing their performance un-\nder the same compression ratio, or examining their\nachievable compression ratios while maintaining\ncomparable performance levels.\n3 Hard Prompt Methods\nHard prompt methods remove unnecessary tokens\nfrom the original prompt while maintaining the use\nof natural language words or sub-words (Reynolds\nand McDonell, 2021). These methods are partic-\nularly useful for LLMs that only accept natural\nlanguage inputs, such as close API models, rather\nthan word embeddings. There are three represen-\ntative hard prompt methods: SelectiveContext (Li\net al., 2023) and LLMLingua (Jiang et al., 2023)\nfor filtering, and Nano-Capsulator (Chuang et al.,\n2024) for paraphrasing, as shown in Figure 3.\nSelectiveContext identifies and deletes redun-\ndant or less informative parts of an input prompt by\nquantifying the informativeness of lexical units us-\ning self-information (Li et al., 2023). To maintain\ntext coherence, the syntactic parsing capabilities\nof SpaCy2 are used to group individual tokens into\nnoun phrases based on dependency parsing. Se-\nlectiveContext does not rely on external models\nor additional parameters, and can be applied to\nany model architecture. However, there are two\nmain disadvantages: (1) It relies on accurate phrase\nboundary detection using SpaCy. (2) Currently,\nthere are no methods for merging verb phrases.\n2https://spacy.io/api/pipeline-functions/\n#merge_noun_chunks\n7184\nFigure 3: Architectures for various prompt compression models by hard prompt methods. For SelectiveContext and\nLLMLingua, the bottom language models filter the prompt tokens without modifying them, serving as selection\nmechanisms. In Nano-Capsulator, the bottom LLM generates a paraphrased version of the input prompt which then\nserves as input for the LLM above. \"SLM\" means \"small language model\". \"Close LLM\" refers to closed-source\nlanguage models that only accept natural language inputs through API calls.\nLLMLingua employs a smaller language model,\nsuch as GPT-2 (Radford et al., 2019), to calculate\nthe self-information of content, similar to Selec-\ntiveContext, and removes redundant tokens from\nthe natural language prompt before it is fed to the\nLLM (Jiang et al., 2023). LLMLingua operates on\nprompts structured as {Instruction, Input (Demon-\nstrations), Question}, initially selecting key demon-\nstrations based on self-information. It then applies\ntoken-level filtering across the prompt, allowing for\nbreaking words into sub-word units and avoiding\nnoun phrase merging by SpaCy. For key elements\nsuch as numbers and units, LLMLingua incorpo-\nrates token preservation algorithms that prioritize\nthese elements within instructions and questions.\nAchieving compression ratios up to 20x, the com-\npression process of LLMLingua is managed by\nthe smaller external language model, allowing it\nto work with close LLMs. However, there are two\nlimitations: (1) The smaller language model re-\nquires additional memory and may use a different\ntokenizer than the larger LLM. (2) Since not all\nprompts contain a large portion of in-context exam-\nples, it is important to differentiate between prompt\ncompression and in-context example selection.\nNano-Capsulator summarizes the original\nprompt into a concise natural language version,\nwhich is then input into the LLM (Chuang et al.,\n2024). This process removes irrelevant informa-\ntion and makes the prompt into fluent sentences.\nThe compression model, a fine-tuned Vicuna-7B,\noperates independently of the LLM. Different from\nstandard summarization, Nano-Capsulator includes\na semantic preservation loss to retain key meanings\nimportant for downstream tasks and a reward func-\ntion to optimize the utility of the prompt for the\nLLM. This targeted approach yields better task per-\nformance by enhancing semantic relevance. How-\never, the memory cost of the compression model\nis not negligible. Additionally, the compression\nprocess is akin to a pre-generation step, requiring\nmore computational resources due to the need for\nadditional inference rather than simple encoding.\nIn addition to the methods mentioned above,\ngeneral hard prompt methods include LongLLM-\nLingua, which has a longer compression window\nthan LLMLingua by applying document reorder-\ning and subsequence recovery (Jiang et al., 2024a),\nand AdaComp, an adaptive compression method\nthat dynamically selects relevant documents based\non query difficulty and retrieval quality (Zhang\net al., 2024b). These general methods have been\nfurther improved in different ways. For example,\nLLMLingua-2 uses data distillation to create a com-\npressed dataset and trains a classifier to retain essen-\ntial tokens (Pan et al., 2024). Both PCRL (Jung and\nKim, 2024) and TACO-RL (Shandilya et al., 2024)\napply RL for token selection: PCRL is model-\nagnostic, while TACO-RL is task-specific. CPC\n(Liskavets et al., 2024) and TCRA-LLM (Liu et al.,\n2023a) reduce tokens by leveraging embeddings:\nCPC ranks sentence relevance with context-aware\nembeddings, while TCRA-LLM uses embeddings\nfor summarization and semantic compression.\n4 Soft Prompt Methods\n4.1 Architectures\nSoft prompt compression methods typically con-\nsist of two main components: an encoder that\ncompresses the prompts into a shorter sequence\nof continuous special tokens and a decoder that\nprocesses the compressed prompts to generate cor-\nresponding responses. As these models improve,\nthey show better generalization abilities, require\nfewer additional trainable parameters in the origi-\nnal LLMs, allow for longer prompt lengths to be\n7185\nFigure 4: Architectures for various prompt compression models by soft prompt methods. Tokens with diagonal\nstripes represent the output tokens processed by the language models. Different from hard prompt methods, the\nbottom LLMs in soft prompt methods process the input tokens, and their outputs (tokens with diagonal stripes)\nserve as input for the LLMs above.\ncompressed, and achieve higher compression ra-\ntios. Several common architectures for these com-\npression models are illustrated in Figure 4, includ-\ning contrastive conditioning (CC) (Wingate et al.,\n2022), gist tokens (GIST) (Mu et al., 2024), Au-\ntoCompressor (Chevalier et al., 2023), in-context\nautoencoder (ICAE) (Ge et al., 2024), 500xCom-\npressor (Li et al., 2024), xRAG (Cheng et al., 2024),\nand UniICL (Gao et al., 2024)\nCC is a decoder-only method that trains a shorter\nsoft prompt to approximate the output distribution\nof a natural language prompt by minimizing the\nKullback-Leibler (KL) divergence across token se-\nquences, thereby aligning with desired responses\n(Wingate et al., 2022). The output distributions\nare estimated through repeated sequence sampling,\nwith both the natural and soft prompts serving as\nconditional inputs for token generation. By apply-\ning various contrastive contexts, soft prompts can\nbe trained to produce specific attributes, such as\nenhanced sentiment, allowing for effective content\ncontrol. However, each soft prompt is uniquely\ntrained for a specific natural language prompt,\nwhich limits the generalization capabilities of CC,\nas new prompts require retraining from scratch.\nGIST modifies the attention mechanism of the\nLLM (Mu et al., 2024). Compression tokens, a\nseries of a new extended vocabulary token initial-\nized by specific trained values, are appended after\nthe original prompt tokens. While these tokens\ncan attend to the original prompt, newly gener-\nated tokens can only attend to the compression to-\nkens, enforcing a separation in attention. This setup\ncan be viewed as an encoder-decoder architecture:\nan LLM fine-tuned on Alpaca+ functions as the\nencoder, compressing the original prompt into a\nsmaller set of KV values. These KV values then\nserve as input to the decoder (the same fine-tuned\nLLM), which generates corresponding responses.\nDifferent from CC, GIST can compress unseen\nprompts without additional fine-tuning, achieving a\ncompression ratio of up to 26x, though it is limited\nby the maximum compressible prompt length used\nin its fine-tuning dataset. Additionally, the com-\npression tokens cannot be used with the original\nuntuned LLM, restricting its broader applicability.\nAutoCompressor shares a similar architecture\nwith GIST and can handle long-context prompt\ncompression up to 30,720 tokens (Chevalier et al.,\n2023). The whole process is recursive, with the\noriginal prompt divided into several sub-prompts.\nIn each iteration, the sub-prompt is compressed into\na small set of continuous prompt vectors, which are\nthen passed to the next iteration along with a new\nsub-prompt for further compression. This contin-\nues until all the encoded embeddings are collected,\nrepresenting the information from the entire origi-\nnal prompt. While AutoCompressor increases the\nmaximum prompt length that can be compressed,\nthe training process is time-consuming, and the\ncompression tokens cannot be used by the original\nuntuned LLM.\nICAE increases the compression length and uses\nthe frozen LLM as the decoder (Ge et al., 2024). It\ncompresses long, information-rich contexts into a\nsmall number of continuous prompt vectors, which\nare then used for QA. Unlike GIST, which com-\npresses low-information texts of around 30 tokens\n7186\nfocused on questions and instructions, ICAE can\nhandle detailed contexts. The question itself re-\nmains uncompressed, and the answer is generated\nbased on the compressed context and the uncom-\npressed question. Since the decoder is frozen, the\ncompression tokens can be used directly with the\noriginal LLM without fine-tuning. ICAE com-\npresses up to 512 tokens into 32, 64, or 128 continu-\nous vectors, achieving compression ratios between\n4x and 16x. By concatenating multiple groups of\nencoded embeddings, ICAE can handle up to 5,120\ntokens. However, its compression ratio decreases\ncompared to GIST. In addition, ICAE is trained\nand tested on the Pile dataset (Gao et al., 2020b),\nwhich may overlap with the training corpus of the\nLLM, raising concerns about potential data leakage\nand the possibility of retrieving answers from the\nmemory of the LLM.\n500xCompressor explores prompt compression\nat high compression ratios while maintaining the\ncompression length limit of ICAE (Li et al., 2024).\nSimilar to ICAE, 500xCompressor employs train-\nable LoRA parameters (Gao et al., 2020a) in the\nencoder, while keeping the original LLM frozen\nin the decoder. However, different from ICAE,\n500xCompressor feeds the decoder with the KV\npairs of the compression tokens rather than the\ncontinous vectors themselves. These KV pairs\nkeep more detailed information than embedding\nvectors, especially under high compression ratios.\n500xCompressor uses a small number of tokens\n(1-16) to compress longer sequences (up to 480\ntokens), achieving compression ratios from 6x to\n480x, while retaining more than 60-70% of the\ncapabilities of the uncompressed prompts. More-\nover, the test set ArxivQA, generated from arXiv\nabstracts from January to April 2024, ensures eval-\nuation on strictly unseen data, mitigating poten-\ntial data leakage. Though 500xCompressor col-\nlects the KV pairs for the compression tokens, it\ndoes not modify them, thus 500xCompressor is a\nprompt compression method not a KV compression\nmethod.\nxRAG uses a frozen embedding model as the en-\ncoder, with only an adapter, positioned between the\nencoder and the decoder LLM, containing trainable\nparameters (Cheng et al., 2024). Although primar-\nily designed for Retrieval-Augmented Generation\n(RAG) (Lewis et al., 2020), the task of xRAG re-\nmains a QA task, making it applicable as a general\nprompt compression method. xRAG proves that\ncurrent embedding models can compress informa-\ntion into a single token for QA tasks. While several\nembedding models were tested in the original pa-\nper, the final choice was SFR-Embedding-Mistral\n(Meng et al., 2024), which is still based on the\nLLM and requires substantial memory. As a result,\nxRAG needs to load two LLMs and a projector,\nwhereas ICAE and 500xCompressor only need to\nload a single LLM and a set of LoRA parameters.\nUniICL focuses on compressing the demonstra-\ntions component of the input prompt, leaving the\nquery unchanged (Gao et al., 2024). The only train-\nable component in UniICL is a projector placed\nbetween the encoder and decoder. Unlike previous\nsoft prompt methods, both the encoder and decoder\nin UniICL are frozen and utilize the same LLM,\nreducing gradient computation during training and\nconserving memory for loading the LLM. In ICL,\nthe quality and relevance of the examples influence\nmodel performance. The encoded continuous vec-\ntors in UniICL can be considered as embeddings\nfor various in-context examples, eliminating the\nneed for additional embedding processes during\nin-context example selection.\nBesides the seven methods introduced above,\nseveral other similar approaches exist, such as CO-\nCOM (Rau et al., 2024), LLoCO (Tan et al., 2024),\nand QGC (Cao et al., 2024). COCOM and LLoCO\nuse fine-tuned encoder-decoder setups, while QGC\nemploys a frozen decoder. Both COCOM and\nLLoCO are designed for the task of RAG: CO-\nCOM compresses multiple documents into groups\nof context embeddings and inputs them together\ninto the decoder, while LLoCO stores and retrieves\nspecific LoRA parameters for the decoder to adapt\nto particular text types and tasks. Different architec-\ntures possess unique characteristics and advantages\n(Jha et al., 2024), warranting a comprehensive com-\nparative analysis.\n4.2 Insights\nSoft prompt methods for prompt compression can\nbe understood from several perspectives, includ-\ning attention mechanism optimization, analogies\nwith prompt and prefix tuning, encoding natural\nlanguage into a new modality, and viewing com-\npressed prompts as a new language for LLMs.\nAttention mechanism. In the standard self-\nattention of transformer, each newly generated to-\nken attends to all previous tokens, which increases\ncomputational costs along with the sequence length.\nSoft prompt compression reduces the input length\nin two stages: first, a small number of special to-\n7187\nkens attend to the full input, storing key informa-\ntion in these tokens; second, new tokens are gener-\nated based solely on the encoded continuous vec-\ntors. This process blocks the need for the full input\nduring generation, limiting attention to the encoded\ncontinuous vectors and reducing computation. This\ncan be seen as a form of attention optimization,\nthough it differs in that the KV pairs generated in\nthe first stage are not identical to those in the origi-\nnal LLM (as LoRA parameters are added). There\nare other attention optimization methods as well,\nincluding sliding window attention (Beltagy et al.,\n2020) and sparse attention (Child et al., 2019). In\nsliding window attention, for example, all input to-\nkens are kept, however, each token can only attend\nto a limited number of preceding tokens.\nPrompt and prefix tuning. Prompt tuning\n(Lester et al., 2021) and prefix tuning (Li and Liang,\n2021) are PEFT methods for language models. In\nprompt tuning, a set of trainable embeddings is\nadded to the beginning of the input. In prefix tun-\ning, both the added embeddings and their corre-\nsponding KV pairs in each layer of the language\nmodel are trainable as well. While these methods\nare useful for fine-tuning models for specific tasks,\ntheir significance has decreased with the appear-\nance of LLMs capable of handling downstream\ntasks through simple prompting. ICAE and related\nmethods are similar to prompt tuning. Instead of\nmanually training the embeddings, these embed-\ndings are generated by the encoder (a fine-tuned\nLLM) through compressing the natural language\ninput. Each natural language prompt is encoded\ninto a unique set of embeddings, with different\nprompts corresponding to different sets. This pro-\ncess provides a zero-shot way of determining the\nparameters typically trained in prompt tuning. In\ncontrast, 500xCompressor is more akin to prefix\ntuning. Here, the inputs for the decoder are not\nembeddings, but KV pairs, which are determined\nby the fine-tuned LLM encoder. These KV pairs\ncontain more parameters than embeddings and are\nassumed to store richer details (Li and Liang, 2021).\n500xCompressor proves that KV pairs can com-\npress more detailed information at high compres-\nsion ratios and outperform embeddings in terms of\ndata retention (Li et al., 2024). Since both embed-\ndings and KV pairs are generated simultaneously\nby the encoder, they offer similar improvements in\nLLM efficiency, including higher inference speed\nand reduced computational costs.\nModality integration. Compressed texts can\nbe considered a new modality, similar to vision\nand speech. In vision-language models, for ex-\nample, an image encoder converts an image into\na list of embeddings, which are then used by an\nLLM decoder for downstream tasks such as im-\nage captioning or QA (Zhang et al., 2024a). This\narchitecture parallels that of prompt compression\nmodels, where an encoder compresses natural lan-\nguage into special embeddings and an LLM de-\ncoder utilizes these continuous vectors. The train-\ning processes for both are similar as well: image\nencoders are paired with decoders trained to predict\nmissing parts of an image and align with textual\ndescriptions, while prompt compression encoders\nare coupled with LLM decoders trained to regener-\nate original texts from compressed prompt vectors.\nThus, encoded continuous vectors from text can be\nseen as a new, rich-information modality derived\nfrom natural language. However, it is important\nto note that texts contain more specific details and\nhave a higher information density than images. As\na result, text compression requires greater precision\nthan image encoding, and the potential for infor-\nmation loss during prompt compression is more\nsignificant than during image encoding.\nNew synthetic language. A new language can\nbe defined by three key characteristics: (1) it can\nencode information and convert thoughts, concepts,\nor data into embeddings; (2) this encoded informa-\ntion can be transmitted between entities; and (3)\nthe entity can dynamically adjust its interpretation\nor output based on the input it receives, a process\nknown as adaptive evaluation (Mansilla, 2004). In\nthe context of prompt compression, the encoded\nembeddings represent natural language texts as con-\ntinuous vectors, which can be understood by LLMs.\nThese continuous vectors can be saved, transferred\nbetween different LLMs, and facilitate knowledge\ntransfer. Moreover, when the encoded information\nchanges, LLMs can dynamically adjust their re-\nsponses based on the new input. Therefore, the\nencoded embeddings generated by prompt com-\npression models can be considered a new, more\nefficient language for LLMs. Work is ongoing in\nthe definition and evaluation of this emerging LLM-\nspecific language (Guo et al., 2020).\n5 Downstream Adaptions\nPrompt compression has a wide range of applica-\ntions in domains such as general QA, RAG (Cheng\net al., 2024; Xu et al., 2024a; Rau et al., 2024; Yoon\n7188\net al., 2024; Jung et al., 2024; Zhang et al., 2024b;\nTan et al., 2024; Liu et al., 2023a), ICL (Gao et al.,\n2024), role playing (Ge et al., 2023), agent-based\n(Jiang et al., 2024c; Xu et al., 2024b), and interdisci-\nplinary tasks (Shen et al., 2024; Teehan et al., 2024).\nIn general QA, prompt compression methods are\ngenerally used to compress instructions (Mu et al.,\n2024) or contexts (Li et al., 2024) to perform var-\nious instruction-following tasks (Ge et al., 2024).\nIn RAG, xRAG exemplifies a method that answers\nquestions based on text embeddings encoded from\nretrieved documents rather than processing the en-\ntire text (Cheng et al., 2024). For ICL, UniICL is\nan example method that compresses in-context ex-\namples to a smaller number of embedding tokens,\nhelping with the selection of relevant examples\n(Gao et al., 2024). In agent, API documentation\ncan be compressed to enable more efficient tool use\n(Jiang et al., 2024c).\n6 Challenges and Future Work\n6.1 Current Challenges\nCurrent prompt compression methods face several\nchallenges, including information loss, reduced\nmodel capability, and marginal improvements in\nefficiency, due to fine-tuning limitations and ineffi-\ncient compression processes.\nFinetuning problems. Although some soft\nprompt methods, such as ICAE, 500xCompressor,\nxRAG, and UniICL, do not need to fine-tune the\ndecoder LLM, the input soft prompt functions sim-\nilarly to prompt tuning or prefix tuning for the de-\ncoder, leading to problems related to fine-tuning.\nPrevious work has shown that fine-tuning base mod-\nels can result in forgetting, overfitting, and model\ndrift, which can decrease the generalization perfor-\nmance of the base LLM (Gururangan et al., 2020).\nTo avoid these problems, prompt compression mod-\nels must be trained on large, diverse datasets, which\nis computationally expensive. Furthermore, the\ntrained encoders are specific to the corresponding\ndecoder LLMs, meaning that when the LLM is\nupdated, for example, from LLaMA-2 (Touvron\net al., 2023) to LLaMA-3 (Dubey et al., 2024), the\nencoder must be re-trained. Hard prompts face\nchallenges as well: filtered hard prompts may de-\ncrease grammatical correctness and provide an un-\nfamiliar input distribution to the LLM, potentially\ninfluencing its performance.\nLimited efficiency improvements. Although\nprompt length can be reduced by dozens or even\nhundreds of times, the time required for compres-\nsion and the memory needed to store the compres-\nsor remain under-optimized. Current encoders in\nsoft prompt methods can be finetuned using ei-\nther full-parameter training or PEFT methods, such\nas LoRA. Fully fine-tuned LLMs are the most\nresource-expensive, while the additional parame-\nters in LoRA-based encoders are modular, allow-\ning for separate loading. However, larger encoders\nresult in longer compression times. Hardware vari-\nability can lead to fluctuations in time for the same\ntask as well (Hennessy and Patterson, 2011), intro-\nducing large standard deviations and relative errors,\nmaking it essential to perform multiple evaluations\nfor accuracy. Moreover, if the encoder and de-\ncoder are of equal size in soft prompt methods, the\ncomputations required for prompt compression are\nnearly the same or even slightly higher than those\nfor encoding the original prompt. As a result, effi-\nciency gains are only realized during the generation\nof new tokens. For tasks with short outputs, these\nimprovements may not be substantial. Hard prompt\nmethods also face issues: additional models may\nbe needed to determine which tokens to delete, and\nthe filtered prompts still need to be re-encoded by\nthe LLM, further influencing efficiency.\nComparison with attention optimization\nmethods. As discussed in Section 4.2, soft prompt\nmethods can be regarded as special attention mod-\nifications. However, current prompt compression\nmethods have not been compared to traditional at-\ntention optimization methods, such as sliding win-\ndow attention (Beltagy et al., 2020) and sparse at-\ntention (Child et al., 2019). Unlike soft prompt\nmethods, attention modifications do not need an\nencoder model, which eliminates additional mem-\nory costs (Ren et al., 2021). Additionally, in soft\nprompt methods, the input and generated tokens\nrely on different attention mechanisms, whereas at-\ntention optimization methods apply the same mech-\nanism to both the input and the generated tokens,\nresulting in greater stability and scalability (Tay\net al., 2022). Therefore, it is important to deter-\nmine the compression ratio when both methods\nhave equivalent computational efficiency and com-\npare prompt compression methods with attention\nmodification methods.\n6.2 Future Directions\nTo address current challenges and improve prompt\ncompression methods, several future directions are\nproposed to reduce information loss while increas-\n7189\ning compression ratio and speed.\nEncoder Optimization. In current soft prompt\nmethods, encoders are similar in size to decoders.\nFor instance, if a decoder has 8 billion parameters,\nthe encoder may add tens of millions of trainable\nparameters on top of that. As a result, the time\nused for compression is comparable to the time it\ntakes for the original LLM to process inputs, mean-\ning that soft prompt methods only improve effi-\nciency during the generation of new tokens. In the-\nory, large encoders that are similar to the decoders\ncan compress information well from the original\ntext into continuous vectors for the LLM. However,\nsmaller, well-trained models such as BERT, which\nhave fewer parameters (as least 10 times smaller\nthan LLMs), are capable of encoding semantic in-\nformation effectively (Devlin et al., 2019). This\nwill substantially increase compression speed. Be-\nsides LoRA, other PEFT methods, such as QLoRA\n(Dettmers et al., 2023), DoRA (yang Liu et al.,\n2024), and MoRA (Jiang et al., 2024b) are worth\ntrying for fine-tuning the compression encoder as\nwell.\nCombination of hard and soft prompts. Hard\nprompt methods increase information density by\nfiltering out unnecessary tokens. Soft prompt meth-\nods represent the original text using a small number\nof special tokens. Since hard and soft prompt meth-\nods operate through orthogonal mechanisms, their\ncombination can further enhance compression ra-\ntios. However, when combining these methods,\ntheir compression times add up sequentially, lead-\ning to increased overall processing time for input.\nInsights from multimodality. As discussed in\nSection 4.2, soft prompt methods can be under-\nstood as a form of modality integration between\nnatural language and compressed language. This\nopens the possibility of applying insights from mul-\ntimodality to benefit prompt compression models.\nThere are mainly two ways to encode images into\nembeddings: self-attention and cross-attention (Jin\net al., 2024). In self-attention, images and query\nvectors are input to the image encoder together,\nwhereas in cross-attention, only query vectors are\ninput to the encoder, and they attend to external im-\nage embeddings in each layer. Current soft prompt\nmethods rely on self-attention to transfer informa-\ntion from natural language prompts to compres-\nsion tokens, however, cross-attention remains un-\nexplored. Trying other multimodal architectures,\nsuch as those using cross-attention, may offer new\nways to leverage compression tokens. In image-\ntext integration, image encoders are trained to align\nimage embeddings with natural language embed-\ndings, a process that can be adapted for prompt\ncompression models (Radford et al., 2021).\n7 Conclusions\nThis survey provides a comprehensive overview\nof the previous prompt compression methods,\nfrom the perspectives of hard and soft prompt ap-\nproaches. In addition to discussing the technical\napproaches of these models, different perspectives\non understanding the compression process and their\napplications are explored. Furthermore, we also\ndiscuss the challenges of the existing prompt com-\npression methods and suggest the potential future\ndevelopment directions. We hope our survey of-\nfers a comprehensive overview of existing methods,\nproviding deeper insights into their motivations.\nWe also aim for our suggested future directions to\ninspire the community and support future develop-\nments in the field.\nLimitations\nThis paper focuses specifically on prompt compres-\nsion and does not provide an overview of all effi-\ncient prompting methods or other efficiency-related\nLLM techniques. Rather than addressing a broad\ntopic, it offers a detailed explanation and insights\ninto the specific area of prompt compression. It\nshould be noted that the use of any prompt com-\npression methods should follow their guidelines\nand copyright restrictions.\nEthics Statement\nNo ethical approval was required for this study. No\nethical concerns are present.\nAvailability Statement\nThe list of papers and updates related to this\nsurvey are uploaded to the open-source com-\nmunity at https://github.com/ZongqianLi/Prompt-\nCompression-Survey.\nReferences\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nZhiwei Cao, Qian Cao, Yu Lu, Ningxin Peng, Luyang\nHuang, Shanbo Cheng, and Jinsong Su. 2024. Retain-\ning key information under high compression ratios:\n7190\nQuery-guided compressor for LLMs. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12685–12695, Bangkok, Thailand. Association\nfor Computational Linguistics.\nKaiyan Chang, Songcheng Xu, Chenglong Wang,\nYingfeng Luo, Tong Xiao, and Jingbo Zhu. 2024.\nEfficient prompting methods for large language mod-\nels: A survey. Preprint, arXiv:2404.01077.\nXin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-\nQing Chen, Furu Wei, Huishuai Zhang, and Dongyan\nZhao. 2024. xrag: Extreme context compression\nfor retrieval-augmented generation with one token.\nPreprint, arXiv:2405.13792.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and\nDanqi Chen. 2023. Adapting language models to\ncompress contexts. In The 2023 Conference on Em-\npirical Methods in Natural Language Processing.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nYu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang,\nZirui Liu, Xun Chen, and Xia Hu. 2024. Learning\nto compress prompt in natural language formats. In\nProceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(Volume 1: Long Papers), pages 7756–7767, Mexico\nCity, Mexico. Association for Computational Lin-\nguistics.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Gpt3.int8(): 8-bit matrix multi-\nplication for transformers at scale. In Advances in\nNeural Information Processing Systems, volume 35,\npages 30318–30332. Curran Associates, Inc.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. In Advances in Neural Information\nProcessing Systems, volume 36, pages 10088–10115.\nCurran Associates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, ..., and Zhiwei\nZhao. 2024. The llama 3 herd of models. Preprint,\narXiv:2407.21783.\nJun Gao, Ziqiang Cao, and Wenjie Li. 2024. Unify-\ning demonstration selection and compression for in-\ncontext learning. Preprint, arXiv:2405.17062.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020a. The pile: An\n800gb dataset of diverse text for language modeling.\nPreprint, arXiv:2101.00027.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020b.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTao Ge, Hu Jing, Li Dong, Shaoguang Mao, Yan Xia,\nXun Wang, Si-Qing Chen, and Furu Wei. 2023. Ex-\ntensible prompts for language models on zero-shot\nlanguage style customization. In Advances in Neural\nInformation Processing Systems, volume 36, pages\n35576–35591. Curran Associates, Inc.\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen,\nand Furu Wei. 2024. In-context autoencoder for con-\ntext compression in a large language model. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nShangmin Guo, Yi Ren, Agnieszka Słowik, and Kory\nMathewson. 2020. Inductive bias and language ex-\npressivity in emergent communication. 4th NeurIPS\nWorkshop on Emergent Communication.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nJohn L Hennessy and David A Patterson. 2011. Com-\nputer architecture: a quantitative approach. Elsevier.\nSiddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Kurt\nKeutzer, and Amir Gholami. 2024. Characterizing\nprompt compression methods for long context infer-\nence. In Workshop on Efficient Systems for Founda-\ntion Models II @ ICML2024.\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing\nYang, and Lili Qiu. 2023. LLMLingua: Compressing\nprompts for accelerated inference of large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 13358–13376, Singapore. Association for\nComputational Linguistics.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dong-\nsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\n2024a. LongLLMLingua: Accelerating and enhanc-\ning LLMs in long context scenarios via prompt com-\npression. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1658–1677, Bangkok,\nThailand. Association for Computational Linguistics.\n7191\nTing Jiang, Shaohan Huang, Shengyue Luo, Zihan\nZhang, Haizhen Huang, Furu Wei, Weiwei Deng,\nFeng Sun, Qi Zhang, Deqing Wang, et al. 2024b.\nMora: High-rank updating for parameter-efficient\nfine-tuning. arXiv preprint arXiv:2405.12130.\nYichen Jiang, Marco Vecchio, Mohit Bansal, and An-\nders Johannsen. 2024c. Hierarchical and dynamic\nprompt compression for efficient zero-shot API usage.\nIn Findings of the Association for Computational Lin-\nguistics: EACL 2024, pages 2162–2174, St. Julian’s,\nMalta. Association for Computational Linguistics.\nYizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai\nWu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin\nTan, Zhenye Gan, Yabiao Wang, Chengjie Wang,\nand Lizhuang Ma. 2024. Efficient multimodal\nlarge language models: A survey. Preprint,\narXiv:2405.10739.\nDongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou,\nand Muhao Chen. 2024. Familiarity-aware evi-\ndence compression for retrieval augmented gener-\nation. Preprint, arXiv:2409.12468.\nHoyoun Jung and Kyung-Joong Kim. 2024. Discrete\nprompt compression with reinforcement learning.\nIEEE Access, 12:72578–72587.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785–\n794, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nYucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin.\n2023. Compressing context to enhance inference ef-\nficiency of large language models. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6342–6353, Singa-\npore. Association for Computational Linguistics.\nZongqian Li, Yixuan Su, and Nigel Collier. 2024.\n500xcompressor: Generalized prompt compres-\nsion for large language models. Preprint,\narXiv:2408.03094.\nBarys Liskavets, Maxim Ushakov, Shuvendu Roy,\nMark Klibanov, Ali Etemad, and Shane Luke. 2024.\nPrompt compression with context-aware sentence en-\ncoding for fast and improved llm inference. Preprint,\narXiv:2409.01227.\nJunyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and\nYiming Qian. 2023a. TCRA-LLM: Token compres-\nsion retrieval augmented large language model for\ninference cost reduction. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2023,\npages 9796–9810, Singapore. Association for Com-\nputational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9).\nXinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.\nLlm-pruner: On the structural pruning of large lan-\nguage models. In Advances in Neural Information\nProcessing Systems, volume 36, pages 21702–21720.\nCurran Associates, Inc.\nPaloma Ubeda Mansilla. 2004. Foundations of lan-\nguage (brain, meaning, grammar, evolution), by ray\njackendoff. Ibérica: Revista de la Asociación Eu-\nropea de Lenguas para Fines Específicos (AELFE),\n(7):150–152.\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming\nXiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfr-\nembedding-mistral:enhance text retrieval with trans-\nfer learning. Salesforce AI Research Blog.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2024.\nLearning to compress prompts with gist tokens. In\nProceedings of the 37th International Conference on\nNeural Information Processing Systems, NIPS ’23,\nRed Hook, NY , USA. Curran Associates Inc.\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin\nXia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor\nRühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao,\nLili Qiu, and Dongmei Zhang. 2024. LLMLingua-\n2: Data distillation for efficient and faithful task-\nagnostic prompt compression. In Findings of the\nAssociation for Computational Linguistics ACL 2024,\npages 963–981, Bangkok, Thailand and virtual meet-\ning. Association for Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\n7192\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research, pages\n8748–8763. PMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nDavid Rau, Shuai Wang, Hervé Déjean, and Stéphane\nClinchant. 2024. Context embeddings for ef-\nficient answer generation in rag. Preprint,\narXiv:2407.09252.\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang,\nJure Leskovec, Dale Schuurmans, and Bo Dai. 2021.\nCombiner: Full attention transformer with sparse\ncomputation cost. In Advances in Neural Information\nProcessing Systems, volume 34, pages 22470–22482.\nCurran Associates, Inc.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Com-\nputing Systems, CHI EA ’21, New York, NY , USA.\nAssociation for Computing Machinery.\nSander Schulhoff, Michael Ilie, Nishant Balepur, Kon-\nstantine Kahadze, Amanda Liu, Chenglei Si, Yin-\nheng Li, Aayush Gupta, HyoJung Han, Sevien Schul-\nhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara,\nDayeon Ki, Sweta Agrawal, Chau Pham, Gerson\nKroiz, Feileen Li, Hudson Tao, Ashay Srivastava,\nHevander Da Costa, Saloni Gupta, Megan L. Rogers,\nInna Goncearenco, Giuseppe Sarli, Igor Galynker,\nDenis Peskoff, Marine Carpuat, Jules White, Shya-\nmal Anadkat, Alexander Hoyle, and Philip Resnik.\n2024. The prompt report: A systematic survey of\nprompting techniques. Preprint, arXiv:2406.06608.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nShivam Shandilya, Menglin Xia, Supriyo Ghosh,\nHuiqiang Jiang, Jue Zhang, Qianhui Wu, and Vic-\ntor Rühle. 2024. Taco-rl: Task aware prompt com-\npression optimization with reinforcement learning.\nPreprint, arXiv:2409.13035.\nJunhong Shen, Neil Tenenholtz, James Brian Hall,\nDavid Alvarez-Melis, and Nicolo Fusi. 2024. Tag-\nllm: Repurposing general-purpose llms for special-\nized domains. arXiv preprint arXiv:2402.05140.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nSijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tian-\njun Zhang, Kurt Keutzer, Joseph E. Gonzalez, and\nRaluca Ada Popa. 2024. Lloco: Learning long con-\ntexts offline. Preprint, arXiv:2404.07979.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComput. Surv., 55(6).\nRyan Teehan, Brenden Lake, and Mengye Ren. 2024.\nCoLLEGe: Concept embedding generation for large\nlanguage models. In First Conference on Language\nModeling.\nEric Todd, Millicent Li, Arnab Sen Sharma, Aaron\nMueller, Byron C Wallace, and David Bau. 2024.\nFunction vectors in large language models. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nShubham Vatsal and Harsh Dubey. 2024. A sur-\nvey of prompt engineering methods in large lan-\nguage models for different nlp tasks. Preprint,\narXiv:2407.12994.\nZhongwei Wan, Xin Wang, Che Liu, Samiul Alam,\nYu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan,\nYi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and\nMi Zhang. 2024. Efficient large language models: A\nsurvey. Transactions on Machine Learning Research.\nSurvey Certification.\nDavid Wingate, Mohammad Shoeybi, and Taylor\nSorensen. 2022. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5621–5634, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\n7193\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2024a. RE-\nCOMP: Improving retrieval-augmented LMs with\ncontext compression and selective augmentation. In\nThe Twelfth International Conference on Learning\nRepresentations.\nYang Xu, Yunlong Feng, Honglin Mu, Yutai Hou, Yi-\ntong Li, Xinghao Wang, Wanjun Zhong, Zhongyang\nLi, Dandan Tu, Qingfu Zhu, Min Zhang, and Wanxi-\nang Che. 2024b. Concise and precise context com-\npression for tool-using language models. Preprint,\narXiv:2407.02043.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nShih yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo\nMolchanov, Yu-Chiang Frank Wang, Kwang-Ting\nCheng, and Min-Hung Chen. 2024. DoRA: Weight-\ndecomposed low-rank adaptation. In Forty-first In-\nternational Conference on Machine Learning.\nChanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Min-\nbyul Jeong, and Jaewoo Kang. 2024. Compact: Com-\npressing retrieved documents actively for question\nanswering. Preprint, arXiv:2407.09014.\nJingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu.\n2024a. Vision-language models for vision tasks: A\nsurvey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 46(8):5625–5644.\nQianchi Zhang, Hainan Zhang, Liang Pang, Hongwei\nZheng, and Zhiming Zheng. 2024b. Adacomp: Ex-\ntractive context compression with adaptive predic-\ntor for retrieval-augmented large language models.\nPreprint, arXiv:2409.01579.\nWenbo Zhao, Arpit Gupta, Tagyoung Chung, and Jing\nHuang. 2023. SPC: Soft prompt construction for\ncross domain generalization. In Proceedings of the\n8th Workshop on Representation Learning for NLP\n(RepL4NLP 2023), pages 118–130, Toronto, Canada.\nAssociation for Computational Linguistics.\nA Appendix\nTo provide readers with a way to locate reference\npapers on prompt compression methods, Figure 2\nhas been translated into Table 1. Data examples\nfor prompt structures in Section 2 are presented in\nTable 2.\nThis paper was refined with ChatGPT.\n7194\nHard Prompt Methods\nFiltering:\nGeneral: SelectiveContext (Li et al., 2023), LLMLingua (Jiang et al., 2023), LongLLMLingua (Jiang\net al., 2024a), AdaComp (Zhang et al., 2024b)\nDistillation Enhanced: LLMLingua-2 (Pan et al., 2024)\nRL Enhanced: TACO-RL (Shandilya et al., 2024), PCRL (Jung and Kim, 2024)\nEmbedding Enhanced: CPC (Liskavets et al., 2024), TCRA-LLM (Liu et al., 2023a)\nParaphrasing:\nNano-Capsulator (Chuang et al., 2024), CompAct (Yoon et al., 2024), FA VICOMP (Jung et al., 2024)\nSoft Prompt Methods\nDecoder Only:\nNot Finetuned: CC (Wingate et al., 2022)\nFinetuned: GIST (Mu et al., 2024), AutoCompressor (Chevalier et al., 2023)\nEncoder-decoder:\nBoth Finetuned: COCOM (Rau et al., 2024), LLoCO (Tan et al., 2024)\nFinetuned Encoder: ICAE (Ge et al., 2024), 500xCompressor (Li et al., 2024), QGC (Cao et al.,\n2024)\nEmbedding Encoder: xRAG (Cheng et al., 2024)\nProjector: UniICL (Gao et al., 2024)\nApplications:\nRAG:\nRAG: xRAG (Cheng et al., 2024), RECOMP (Xu et al., 2024a), COCOM (Rau et al., 2024),\nCompAct (Yoon et al., 2024), FA VICOMP (Jung et al., 2024), AdaComp (Zhang et al., 2024b),\nLLoCO (Tan et al., 2024), TCRA-LLM (Liu et al., 2023a)\nAgents: HD-Gist (Jiang et al., 2024c), (Xu et al., 2024b)\nDomain-specific tasks: Tag-llm (Shen et al., 2024), CoLLEGe (Teehan et al., 2024)\nOthers: ICL (Gao et al., 2024), Role Playing (Ge et al., 2023), Functions (Todd et al., 2024)\nTable 1: Summary of methods and applications in prompt compression.\n{Instruction, Input, Output}\nExample in Alpaca:\nInstruction: Evaluate this sentence for spelling and grammar mistakes\nInput: He finnished his meal and left the resturant\nOutput: There are two spelling errors in the sentence. The corrected sentence should be: \"He finished\nhis meal and left the restaurant.\"\n{Context, Question, Answer}\nExample in SQuAD:\nContext: Super Bowl 50 was an American football game to determine the champion of the National\nFootball League (NFL) for the 2015 season. The American Football Conference (AFC) champion\nDenver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers\n24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi’s\nStadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl,\nthe league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as\ntemporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under\nwhich the game would have been known as \"Super Bowl L\"), so that the logo could prominently\nfeature the Arabic numerals 50.\nQuestion: Where did Super Bowl 50 take place?\nAnswer: Santa Clara, California\nTable 2: Examples for the QA items with formats of {Instruction, Input, Output} and {Context, Question, Answer}.\n7195",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6843351125717163
    },
    {
      "name": "Compression (physics)",
      "score": 0.5316434502601624
    },
    {
      "name": "Data compression",
      "score": 0.4491271674633026
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19950011372566223
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ]
}