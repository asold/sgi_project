{
  "title": "TSTNN: Two-stage Transformer based Neural Network for Speech Enhancement in the Time Domain",
  "url": "https://openalex.org/W3138320152",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1902378672",
      "name": "Wang Kai",
      "affiliations": []
    },
    {
      "id": null,
      "name": "He, Bengbeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223408500",
      "name": "Zhu Wei-ping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2921144622",
    "https://openalex.org/W3015199127",
    "https://openalex.org/W2600556233",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2802304149",
    "https://openalex.org/W2141998673",
    "https://openalex.org/W2603567530",
    "https://openalex.org/W2963103134",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2906042495",
    "https://openalex.org/W3016129867",
    "https://openalex.org/W1983108229",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W2889134433",
    "https://openalex.org/W2403555780",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2937484199",
    "https://openalex.org/W2144404214",
    "https://openalex.org/W2943895317",
    "https://openalex.org/W2774389566",
    "https://openalex.org/W3036367741",
    "https://openalex.org/W3015197852",
    "https://openalex.org/W2902132730",
    "https://openalex.org/W1495679096",
    "https://openalex.org/W3045904949",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2094721231"
  ],
  "abstract": "In this paper, we propose a transformer-based architecture, called two-stage transformer neural network (TSTNN) for end-to-end speech denoising in the time domain. The proposed model is composed of an encoder, a two-stage transformer module (TSTM), a masking module and a decoder. The encoder maps input noisy speech into feature representation. The TSTM exploits four stacked two-stage transformer blocks to efficiently extract local and global information from the encoder output stage by stage. The masking module creates a mask which will be multiplied with the encoder output. Finally, the decoder uses the masked encoder feature to reconstruct the enhanced speech. Experimental results on the benchmark dataset show that the TSTNN outperforms most state-of-the-art models in time or frequency domain while having significantly lower model complexity.",
  "full_text": " \n Â© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.  \nTSTNN: TWO-STAGE TRANSFORMER BASED NEURAL NETWORK FOR SPEECH ENHANCEMENT IN THE TIME DOMAIN  Kai Wang, Bengbeng He, Wei-Ping Zhu  Department of Electrical and Computer Engineering, Concordia University, Montreal, Canada   ABSTRACT  In this paper, we propose a transformer-based architecture, called two-stage transformer neural network (TSTNN) for end-to-end speech denoising in the time domain. The proposed model is com-posed of an encoder, a two-stage transformer module (TSTM), a masking module and a decoder. The encoder maps input noisy speech into feature representation. The TSTM exploits four stacked two-stage transformer blocks to efficiently extract local and global information from the encoder output stage by stage. The masking module creates a mask which will be multiplied with the encoder output. Finally, the decoder uses the masked encoder feature to re-construct the enhanced speech. Experimental results on the bench-mark dataset show that the TSTNN outperforms most state-of-the-art models in time or frequency domain while having significantly lower model complexity. Index Termsâ€” time domain, two-stage transformer, local and global information, lower model complexity, speech enhancement  1. INTRODUCTION  Speech enhancement as an indispensable front-end task in many speech processing related applications, such as automatic speech recognition, hearing aid, telecommunication and so on, has been widely studied over the past decades, especially in recent years when deep learning emerges as a powerful tool to develop various data-driven approaches for solving traditional estimation problems with or without supervision. Most of current deep learning architectures for speech enhance-ment are implemented in time-frequency (T-F) domain, such as con-volutional neural network (CNN) and recurrent neural network (RNN). By using short-time Fourier transform (STFT), those meth-ods usually treat the spectral magnitude as training target. The phase of noisy speech is utilized along with the enhanced speech magni-tude to reconstruct the time-domain signal with inverse short-time Fourier transform (iSTFT). Despite some inspiring results achieved [1-4], there are still two main limitations in T-F domain methods. First, Fourier transform operation is an additional overhead which hinders fast speech denoising. Second, the noisy phase information is usually ignored during denoising process. However, the phase in-formation has been proved important for enhancing the speech qual-ity [5]. Some studies have considered both magnitude and phase simultaneously during the training stage to achieve better enhance-ment results [6].  Recently, various works have directly estimated the clean waveform from noisy raw data in time domain [7-11]. Many re-searchers have investigated encoder-decoder frameworks based on the CNN or RNN. For modeling long-range sequence like speech, CNN requires more convolutional layers to enlarge receptive field. \nThe dilated convolutional neural network has been proposed for pro-cessing the long-term temporal sequence [12]. Additionally, RNN such as long short-term memory (LSTM) and gated recurrent units (GRU), are commonly used in modeling long-term sequence with order information. However, the drawback of RNN based models is that they cannot perform parallel processing, thus leading to high computation complexity. Although some improvements can be achieved by adding temporal convolutional network (TCN) blocks [10] or LSTM layers between encoder and decoder for further ex-tracting high-level features and enlarging receptive fields [11], the contextual information of speech is often ignored, which restricts the denoising performance. Fortunately, transformer neural network can resolve the long-dependency problem effectively and operate well in parallel, showing good performance on many natural language processing tasks [13]. In [14], the authors proposed a transformer-based network for speech enhancement while it has relatively large model size.  Inspired by the capability of the transformer in sequence mod-eling, and the effectiveness of the dual-path network for extracting contextual information [15], we here propose a two-stage trans-former neural network (TSTNN) for end-to-end monaural speech enhancement in the time domain. The proposed model incorporates the TSTM between the encoder and decoder to learn both local and global contextual information of long-range speech sequences. Our extensive experiments on benchmark dataset show that the TSTNN outperforms the state-of-the-art methods in terms of most evaluation criteria while incurs relatively light model complexity.  2. TWO-STAGE TRANSFORMER  In this section, we first present an improved version of the general transformer structure, based on which we propose a two-stage trans-former block for extracting local and global contextual information of speech feature.  2.1. Improved transformer  The general transformer structure consists of encoder and decoder networks [13]. In our model, we only use the encoder part since the input mixtures and output enhanced sequences have the same length in speech denoising. The original transformer encoder is comprised of three important modules: positional encoding, multi-head atten-tion and position-wise feed-forward network. Different from that, we remove the positional encoding part since it is not suitable for acoustic sequence. Inspired by the effectiveness of RNNs in tracking order information, the first fully connected layer of feed-forward network is replaced with a GRU layer to learn the positional infor-mation [16, 17], as shown in Fig. 1. In multi-head attention block, first, the input (ğ‘‹) is mapped  \n \n  \n Fig. 1: Improved transformer  with different, learnable linear transformation â„ times to get queries (ğ‘„), keys (ğ¾) and values (ğ‘‰) representation, respectively, as de-scribed in Eq. (1). Then, the dot product of the query with all keys is computed, followed by division of a constant. After applying the softmax function, the weights on the values are obtained. The atten-tion of each head is the dot product of the weights and values as shown in Eq. (2). The attentions of all heads are concatenated and linearly projected again to obtain the final output in Eq. (3), which is followed by layer normalization and residual connection of input ğ‘‹ as given by Eq. (4).  ğ‘„!=ğ‘‹ğ‘Š!\" ,  ğ¾!=ğ‘‹ğ‘Š!# , ğ‘‰!=ğ‘‹ğ‘Š!$           (1) â„ğ‘’ğ‘ğ‘‘!=ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„!,ğ¾!,ğ‘‰!)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(\"!#!\"âˆš&)ğ‘‰!     (2) ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘(ğ‘„,ğ¾,ğ‘‰)=ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(â„ğ‘’ğ‘ğ‘‘',â€¦,â„ğ‘’ğ‘ğ‘‘()ğ‘Š)  (3)                   ğ‘€ğ‘–ğ‘‘=ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘œğ‘Ÿğ‘š(ğ‘‹+ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–â„ğ‘’ğ‘ğ‘‘)            (4) where ğ‘‹\tâˆˆâ„*Ã—& is the input with sequences of length ğ‘™ and dimen-sion ğ‘‘, ğ‘–=1,2,â€¦â„ and ğ‘„!,ğ¾!,ğ‘‰!âˆˆâ„*Ã—&/( are the mapped que-ries, keys and values respectively, ğ‘Š!\",ğ‘Š!#,ğ‘Š!$\tâˆˆâ„&Ã—&/( denote the ğ‘–th linear transformation matrix for queries, keys and values, re-spectively.  ğ‘Š)âˆˆâ„&Ã—&is the linear transformation matrix and â„ is the number of parallel attention layers which is set as 4 in our model. Then, the output from multi-head attention block is processed by feed-forward network to obtain the final output of improved transformer encoder, where residual connections and layer normali-zation [18] are add as well. The procedures are defined as follows:  ğ¹ğ¹ğ‘(ğ‘€ğ‘–ğ‘‘)=ğ‘…ğ‘’ğ¿ğ‘ˆ(ğºğ‘…ğ‘ˆ(ğ‘€ğ‘–ğ‘‘))ğ‘Š'+ğ‘'        (5) ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘œğ‘Ÿğ‘š(ğ‘€ğ‘–ğ‘‘+ğ¹ğ¹ğ‘)\t           (6) where ğ¹ğ¹ğ‘(âˆ™) denotes the output of the position-wise feed-forward network, and  ğ‘Š'âˆˆâ„&##Ã—&, ğ‘'âˆˆâ„& , and ğ‘‘--=4\tÃ—\tğ‘‘.  2.2. Two-stage transformer block  We propose a two-stage transformer block based on the improved transformer. As shown in Fig. 2, it has a local transformer and a global transformer, which extracts local and global contextual infor-mation, respectively. More specifically, the input is 3-D tensor ([ğ¶,ğ‘,ğ¹]), and the local transformer is first applied to individual chunks to parallelly process local information, which performs on the last dimension  ğ¹ of input tensor. Then the global transformer is used for fusing the information of output from local transformer to learn global dependency, which implements on the dimension ğ‘ of tensor. Besides, each transformer is followed by the group normali-zation operation and utilizes residual connection. \n  Fig. 2: Two-stage transformer block  3. PROPOSED MODEL  In this section, we propose a two-stage transformer based neural net-work (TSTNN) for speech enhancement. As shown in Fig. 3, the new model consists of segmentation stage, encoder, two-stage trans-former module, masking module, decoder and overlap-add stage.  3.1. Segmentation and overlap-add  The segmentation stage splits raw mixture ğ‘‹âˆˆâ„'Ã—. into frames of length ğ¹ with hop size\tğ». Then all the frames are stacked to form a 3-D tensor ğ¼\tâˆˆâ„'Ã—/Ã—0 as the input of encoder.  Here ğ¿ is the length of input mixture, and ğ‘ denotes the number of frames as given by: ğ‘=âŒˆ(ğ¿âˆ’ğ¹)/(ğ¹âˆ’ğ»)+1âŒ‰                            (7) The overlap-add method is used as the inverse operation of seg-mentation, which is used for recovering enhanced waveform.  3.2. Encoder  The encoder uses two convolutional layers of which the first one is increasing the number of channels to 64 using convolution with fil-ter of size (1, 1) and the second one halves the dimension of frame size using filter of size (1, 3) with a stride of (1, 2), where a dilated-dense block [19] with four dilation convolution layers is incorpo-rated between them. All convolutional layers are followed by layer normalization and PReLU nonlinearity [20].   3.3. Two-stage transformer module (TSTM)  The TSTM consists of four stacked two-stage transformer blocks. Before feeding the output from encoder into TSTM, we halve the channel dimension using convolution with a kernel of size (1, 1) fol-lowed by the PReLU nonlinearity, which reduces the computational complexity of the following transformer network. Next, feature rep-resentation is processed by TSTM to learn local and global contex-tual features.  3.4. Masking module   Masking network makes use of the output features from TSTM to obtain the mask for denoising. The output from TSTM is doubled along the channel dimension with PReLU nonlinearity and convo-lution for matching the output of encoder. Then it passes through a two-path 2-D convolution and nonlinearity operation, with the out-puts being multiplied together as the input for 2-D convolution and ReLU to get the mask. The final masked encoder feature is obtained by the element-wise multiplication between the mask and the output of the encoder. \n\n \n  \n Fig. 3: Two-stage transformer neural network (TSTNN) (Note: F, N and C denote frame size, the number of frames and channel, respectively)  3.5. Decoder  In the decoder, a dilated dense block and a sub-pixel convolution [21] are applied to reconstruct the masked encoder feature into en-hanced speech feature. Then the 2-D convolution with filter size of (1, 1) recovers the channel dimension of the enhanced speech feature into 1 and produces enhanced speech waveform by overlap-add method.  3.6. Loss function  Our loss function combines both time domain and time-frequency domain losses. The loss in time-frequency domain can supervise the model to learn more information, leading to higher speech intelligi-bility and perceptual quality [19], which is defined as:   \t\t\t\t\t\t\t\t\t\t\t\t\tğ‘™ğ‘œğ‘ ğ‘ 0='10âˆ‘âˆ‘[(|ğ‘‹2(ğ‘¡,ğ‘“)|+|ğ‘‹!(ğ‘¡,ğ‘“)|)âˆ’03'-4513'645\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[\\ğ‘‹]2(ğ‘¡,ğ‘“)\\+\\ğ‘‹]!(ğ‘¡,ğ‘“)\\^]                                   (8) where ğ‘‹ and ğ‘‹] denote the spectrogram of clean waveform and the spectrogram of enhanced waveform. ğ‘Ÿ and ğ‘– are the real and imagi-nary parts of the complex variable. ğ‘‡ and ğ¹ are the number of time frames and the number of frequency bins, respectively. The time-domain loss is based on the mean square error (MSE) between the denoised speech and clean speech, which is defined as: ğ‘™ğ‘œğ‘ ğ‘ 1='/âˆ‘(ğ‘¥!âˆ’ğ‘¥`!)7/3'!45                         (9) where ğ‘¥ and ğ‘¥` are the sample of the clean speech and the denoised speech, respectively, and ğ‘ denotes the number of samples.       The final loss function combines these two types of losses men-tioned above, as follows: ğ‘™ğ‘œğ‘ ğ‘ =\tğ›¼âˆ—ğ‘™ğ‘œğ‘ ğ‘ 0+(1âˆ’ğ›¼)ğ‘™ğ‘œğ‘ ğ‘ 1          (10) where ğ›¼ is a tunable parameter and set as 0.2 in our work.  4. EXPERIMENTS  4.1. Datasets  We evaluate our proposed model on a standard speech dataset from [22], which is selected from Voice Bank corpus [23], including 11572 utterances of 28 speakers (14 female and 14 male) for training set and 824 utterances of 2 speakers (one male and one female) for testing set. The noisy speech is generated with 10 types of noises (8 \nfrom DEMAND dataset [24] and 2 artificially generated) at SNRs of 15 dB, 10 dB, 5 dB and 0 dB for training, and with 5 types of unseen noises at SNRs of 17.5 dB, 12.5 dB, 7.5 dB and 2.5 dB for testing.  4.2. Experimental setup  All the utterances are resampled to 16 kHz. Each frame has a size of 512 samples (32ms) with an overlap of 256 samples (16ms). We randomly slice segment of 4 seconds from an utterance if it is larger than 4 seconds. Within a batch, the smaller utterances are zero-pad-ded to match the size of largest utterance.  In the training stage, we train our model for 100 epochs and optimize it by Adam. We use the gradient clipping with maximum L2-norm of 5 to avoid gradient explosion. For learning rate, we use the dynamic strategies during the training stage [13]. More specifi-cally, we first linearly increase the learning rate within ğ‘›ğ‘¢ğ‘š_ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ğ‘ \ttraining steps, and then decay it by 0.98 for every two epochs as follows: ğ‘™ğ‘Ÿ=\tğ‘˜'âˆ™ğ‘‘89&:*35.<âˆ™ğ‘›âˆ™ğ‘›ğ‘¢ğ‘š_ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ğ‘ 3'.<,\t\t\t\t\tğ‘›â‰¤ğ‘›ğ‘¢ğ‘š_ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ğ‘  \tğ‘™ğ‘Ÿ=\tğ‘˜7âˆ™0.98=:>9?(7@,\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tğ‘›>ğ‘›ğ‘¢ğ‘š_ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ğ‘  where ğ‘› is the number of steps, and\tğ‘˜'\t, \tğ‘˜7 are tunable parameters. In our experiments, we set\tğ‘˜'=0.2,\tğ‘˜7=4ğ‘’3B and ğ‘›ğ‘¢ğ‘š_ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ğ‘ \t = 4000. Finally, ğ‘‘89&:* denotes the feature size of the input of transformer which is set as 64 in our paper.    4.3. Evaluation metrics   We evaluate the proposed speech enhancement model on several ob-jective measures (Table 1): perceptual evaluation of speech quality (PESQ) [25] with a score range from -0.5 to 4.5; Short-time objec-tive intelligibility (STOI) [26] with a score range from 0 to 1. We also adopt subjective mean opinion scores (MOSs) [27] including CSIG for signal distortion, CBAK for noise distortion evaluation and COVL for overall quality evaluation. All MOSs range from 1 to 5; Segmental signal-to-noise ratio (SSNR) [28] with a value range from -10 to 35 is also used.   4.4. Comparison with other time-domain methods  Our proposed model is compared with several other waveform-based methods. As seen from Table 1, TSTNN outperforms, in terms\n\n \n  \nTable 1: Evaluation results of our proposed model compared with other methods on the same dataset [22].  Six objective metrics and the number of trainable parameters are considered. (Higher scores are better except parameter size, â€˜Fâ€™ denotes frequency and â€˜Tâ€™ is time)  Model Domain PESQ STOI (%) CSIG CBAK COVL SSNR Para.(Million) Noisy - 1.97 91 3.34 2.44 2.63 1.73 - Wiener - 2.22 - 3.23 2.68 2.67 5.07 - SEGAN, 2017[8] T 2.16 93 3.48 2.94 2.80 7.73 97.47 Wavenet, 2018[9] T - - 3.62 3.23 2.98 - - CNN-GAN, 2018[5] F 2.34 93 3.55 2.95 2.92 - - Wave U-Net, 2018[7] T 2.40 - 3.52 3.24 2.96 9.97 10.0 MMSE-GAN, 2018[2] F 2.53 93 3.80 3.12 3.14 - - MetricGAN, 2019[3] F 2.86 - 3.99 3.18 3.42 - - DCUnet-16, 2019[6] F 2.93 - 4.10 3.77 3.52 14.44 2.3 DEMUCS (small), 2020[11] T 2.93 95 4.22 3.25 3.52 - 18.9 DEMUCS (large), 2020[11] T 3.07 95 4.31 3.4 3.63 - 33.5 TSTNN (ours) T 2.96 95 4.33 3.53 3.67 9.70 0.92 of PESQ score, most existing waveform-based methods and achieves comparable performance with only about 36 times fewer of parameters to DEMUCS with large model configuration (Fig. 4). For STOI value, TSTNN achieves the best score (95%) among all the existing time-domain models. Besides, TSTNN achieves best score in three MOSs evaluation compared with existing time-do-main models.  \n Fig. 4: Comparison in terms of PESQ and model size among time-domain methods  4.5. Comparison with T-F methods    Table 1 also shows that TSTNN outperforms all T-F methods in most evaluation metrics, especially the PESQ. Moreover, our TSTNN has 0.92 million parameters, which is 2.5 times fewer than DCUNet-16 (2.3 million parameters), achieving extremely low model complexity compared with all other T-F models in Table 1.   4.6. The influence of two-stage transformer block    In our model, the two-stage transformer block is designed to extract the features of input speech. In order to further demonstrate the ef-fectiveness of our proposed transformer block, we also designed an-other architecture for comparison. In this architecture, we use two transformer blocks rather than four blocks in the TSTNN, and we increase the number of encoder layers, which is used as the main feature extractor. Correspondingly, the number of decoder layers is increased to match the encoder layers. In this comparison architec-ture, we set 4 encoder layers while only one encoder layer in TSTNN. The configuration of each encoder and decoder in the comparison   \nmodel is the same as the counterparts of TSTNN. Table 2: TSTNN vs. Comparison model.   \n   Table 3: Parameters of TSTNN and comparison model. Model TSTNN Comparison Para.(million) 0.92 2.37  From Tables 2 and 3, we can see that TSTNN has better scores in all evaluation metrics with 2.6 times fewer parameters than the comparison model. It shows that the transformer block is more effi-cient than encoder layers in terms of feature extraction. The reason for the remarkable performance could be the properties of our two-stage transformer that it not only works well on long-range sequence but also extracts both local and global contextual information, which outperforms most current architectures.       5. CONCLUSION  In this study, we proposed a two-stage transformer neural network for monaural speech enhancement in the time domain, which effi-ciently extracts both local and global contextual information for long-range speech sequences. Experimental results showed that our model outperforms most of the state-of-the-art methods for most evaluation metrics. Furthermore, our proposed model has much fewer trainable parameters compared with other current models.  6. REFERENCES  [1] S.-W. Fu, T.-y. Hu, Y. Tsao, and X. Lu, â€œComplex spectro-gram enhancement by convolutional neural network with multi-metrics learning,â€ in 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2017, pp. 1â€“6  [2] M. H. Soni, N. Shah, and H. A. Patil, â€œTime-Frequency Masking-Based Speech Enhancement Using Generative Ad-versarial Network,â€ in 2018 IEEE International Conference \nModel                   PESQ  STOI(%)  CSIG  CBAK  COVL  SSNR  TSTNN                2.96      95            4.33      3.53      3.67      9.70 Comparison         2.87      95            4.19      3.47      3.54      9.65 \n \n  \non Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5039â€“5043.  [3] S.-W. Fu et al., â€œMetricgan: Generative adversarial networks based black-box metric scores optimization for speech en-hancement,â€ in ICML, 2019.  [4] N. Shah, H. A. Patil, and M. H. Soni, â€œTime-Frequency Mask-based Speech Enhancement using Convolutional Gen-erative Adversarial Network,â€ in Proceedings, APSIPA An-nual Summit and Conference, 2018, vol. 2018, pp. 12â€“15.  [5] N. Takahashi, P. Agrawal, N. Goswami, and Y. Mitsufuji, â€œPhaseNet: Discretized Phase Modeling with Deep Neural Networks for Audio Source Separation,â€ in Proc. Interspeech 2018, 2018, pp. 2713â€“2717.  [6] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee, â€œPhase-aware Speech Enhancement with Deep Complex U-Net,â€ Mar. 2019.  [7] C. Macartney and T. Weyde, â€œImproved speech enhancement with the wave-u-net,â€ arXiv preprint arXiv:1811.11307, 2018.  [8] Santiago Pascual, Antonio Bonafonte, and Joan Serra, â€œSegan: Speech enhancement generative adversarial network,â€ arXiv preprint arXiv:1703.09452, 2017.  [9] D. Rethage, J. Pons, and X. Serra, â€œA Wavenet for Speech De-noising,â€ in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5069â€“5073.  [10] A. Pandey and D. Wang, â€œTCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain,â€ in ICASSP, 2019, pp. 6875â€“6879  [11] Defossez A, Synnaeve G, Adi Y, â€œReal Time Speech Enhance-ment in the Waveform Domainâ€. arXiv preprint arXiv:2006.12847, 2020.  [12] Yu F, Koltun V, â€œMulti-scale context aggregation by dilated convolutions.â€ arXiv preprint arXiv:1511.07122, 2015.  [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in Advances in neural information processing systems, 2017, pp. 5998â€“6008.  [14] J. Kim, M. El-Khamy, and J. Lee, â€œT-gsa: Transformer with gaussian-weighted self-attention for speech enhancement,â€ in ICASSP 2020-2020 IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6649â€“6653.  [15] Y. Luo, Z. Chen, and T. Yoshioka, â€œDual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation,â€ in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 46â€“50.   \n[16] M. Sperber, J. Niehues, G. Neubig, S. Stuker, and A. Waibel, â€œSelf-attentional acoustic models,â€ Proc. Interspeech 2018, pp. 3723â€“3727, 2018.  [17] J Chen, Q Mao, D. Liu â€œDual-Path Transformer Network: Di-rect Context-Aware Modeling for End-to-End Monaural Speech Separationâ€. arXiv preprint arXiv:2007.13975, 2020.  [18] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer normalization,â€ arXiv preprint arXiv:1607.06450, 2016.  [19] Pandey, A., & Wang, D. â€œDensely Connected Neural Network with Dilated Convolutions for Real-Time Speech Enhancement in The Time Domain.â€ In ICASSP 2020-2020 IEEE Interna-tional Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6629-6633.  [20] K. He, X. Zhang, S. Ren, and J. Sun, â€œDelving deep into recti-fiers: Surpassing human-level performance on imagenet classi-fication,â€ in IEEE International Conference on Computer Vi-sion, 2015, pp. 1026â€“1034.  [21] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang, â€œReal-time single image and video super-resolution using an efficient sub-pixel convo-lutional neural network,â€ in IEEE conference on computer vi-sion and pattern recognition, 2016, pp. 1874â€“1883.  [22] Valentini-Botinhao C, Wang X, Takaki S, et al. â€œInvestigating RNN-based speech enhancement methods for noise-robust Text-to-Speech.â€ In SSW. 2016: 146-152.  [23] Veaux C, Yamagishi J, King S. â€œThe voice bank corpus: De-sign, collection and data analysis of a large regional accent speech database.â€ In 2013 International Conference Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation(O-COCOSDA/CASLRE). IEEE, 2013: 1-4.  [24] Joachim Thiemann, Nobutaka Ito, and Emmanuel Vincent, â€œThe diverse environments multi-channel acoustic noise data-base: A database of multichannel environ- mental noise record-ings,â€ The Journal of the Acoustical Society of America, vol. 133, no. 5, pp. 3591â€“3591, 2013.  [25] Loizou P C. Speech enhancement: theory and practice. CRC press, 2013.  [26] C. H. Taal et al., â€œAn algorithm for intelligibility prediction of timeâ€“frequency weighted noisy speech,â€ IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125â€“2136, 2011.  [27] Hu Y, Loizou P C. â€œEvaluation of objective quality measures for speech enhancement.â€ Audio, Speech, and Language Pro-cessing, IEEE Transactions on, 2008, 16(1): 229-238.  [28] Hansen J H L, Pellom B L. â€œAn effective quality evaluation protocol for speech enhancement algorithms.â€ in Fifth interna-tional conference on spoken language processing. 1998.  ",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.8120739459991455
    },
    {
      "name": "Computer science",
      "score": 0.7176053524017334
    },
    {
      "name": "Transformer",
      "score": 0.7132369875907898
    },
    {
      "name": "Speech recognition",
      "score": 0.5128093361854553
    },
    {
      "name": "Artificial neural network",
      "score": 0.4539494514465332
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4482099711894989
    },
    {
      "name": "Time domain",
      "score": 0.4132775664329529
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3813853859901428
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36767593026161194
    },
    {
      "name": "Voltage",
      "score": 0.12832847237586975
    },
    {
      "name": "Computer vision",
      "score": 0.12712734937667847
    },
    {
      "name": "Engineering",
      "score": 0.1262500286102295
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Encoder"
}