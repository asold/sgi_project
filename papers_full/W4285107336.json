{
  "title": "Prompt-free and Efficient Few-shot Learning with Language Models",
  "url": "https://openalex.org/W4285107336",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2516060474",
      "name": "Rabeeh Karimi Mahabadi",
      "affiliations": [
        "Idiap Research Institute",
        null
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A1980977304",
      "name": "James Henderson",
      "affiliations": [
        "Idiap Research Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2266316079",
      "name": "Lambert Mathias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2211355259",
      "name": "Marzieh Saeidi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130709233",
      "name": "Veselin Stoyanov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116785513",
      "name": "Majid Yazdani",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W1579035156",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3120074043",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3113151582",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3096580779",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W4287026929",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2370924594",
    "https://openalex.org/W2962945654",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3173617765"
  ],
  "abstract": "Rabeeh Karimi Mahabadi, Luke Zettlemoyer, James Henderson, Lambert Mathias, Marzieh Saeidi, Veselin Stoyanov, Majid Yazdani. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3638 - 3652\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nPERFECT : Prompt-free and Efficient Few-shot Learning with Language Models\nRabeeh Karimi Mahabadi1,3,4 Luke Zettlemoyer1,2 James Henderson4\nMarzieh Saeidi1 Lambert Mathias1 Veselin Stoyanov1 Majid Yazdani1\n1Meta AI 2University of Washington 3EPFL 4Idiap Research Institute\n{rabeeh.karimi, james.henderson}@idiap.ch\nlsz@cs.washington.edu\n{marzieh,mathiasl,ves,myazdani}@fb.com\nAbstract\nCurrent methods for few-shot fine-tuning of\npretrained masked language models (PLMs)\nrequire carefully engineered prompts and\nverbalizers for each new task to convert examples\ninto a cloze-format that the PLM can score. In\nthis work, we proposePERFECT , a simple and\nefficient method for few-shot fine-tuning of\nPLMs without relying on any such handcrafting,\nwhich is highly effective given as few as 32\ndata points. PERFECT makes two key design\nchoices: First, we show that manually engineered\ntask prompts can be replaced withtask-specific\nadaptersthat enable sample-efficient fine-tuning\nand reduce memory and storage costs by roughly\nfactors of 5 and 100, respectively. Second, instead\nof using handcrafted verbalizers, we learn new\nmulti-token label embeddingsduring fine-tuning,\nwhich are not tied to the model vocabulary and\nwhich allow us to avoid complex auto-regressive\ndecoding. These embeddings are not only\nlearnable from limited data but also enable nearly\n100x faster training and inference. Experiments\non a wide range of few shot NLP tasks demon-\nstrate that PERFECT , while being simple and\nefficient, also outperforms existing state-of-the-\nart few-shot learning methods. Our code is\npublicly available athttps://github.com/\nfacebookresearch/perfect.git.\n1 Introduction\nRecent methods for few-shot language model\ntuning obtain impressive performance but require\ncareful engineering of prompts and verbalizers to\nconvert inputs to a cloze-format (Taylor, 1953) that\ncan be scored with pre-trained language models\n(PLMs) (Radford et al., 2018; Radford et al.; Brown\net al., 2020; Schick and Schütze, 2021a,b). For\nexample, as Figure 1 shows, a sentiment classifier can\nbe designed by inserting the input textx in a prompt\ntemplate“x It was [MASK]” whereverbalizers(e.g.,\n‘great’ and ‘terrible’) are substituted for the[MASK]\nto score target task labels (‘positive’ or ‘negative’).\nIn this paper, we show that such engineering is\n[CLS] The restaurant had excellent foods. It was [MASK] [SEP]\nPretrained Language Model\nInput Pattern\nMLM Head\n terrible\ngreat\nVerbalizers\npositive\nnegative\nLabels\nFigure 1: Existing few-shot fine-tuning methods require\nmanual engineering to reduce new tasks to masked lan-\nguage modeling.PERFECT does not rely on any handcraft-\ning, removing both patterns and verbalizers (see Figure 3).\nnot needed for few-shot learning and instead can\nbe replaced with simple methods for data-efficient\nfine-tuning with as few as 32 end-task examples.\nMore specifically, we propose PERFECT , a\nPrompt-free and Efficient paRadigm for FEw-shot\nCloze-based fine-Tuning. To remove handcrafted\npatterns,PERFECT uses task-specific adapter layers\n(Houlsby et al., 2019; Pfeiffer et al., 2020) (§3.1).\nFreezing the underlying PLM with millions or billions\nof parameters (Liu et al., 2019; Raffel et al., 2020),\nand only tuning adapters with very few new param-\neters saves on memory and storage costs (§4.2), while\nallowing very sample-efficient tuning (§4). It also\nstabilizes the training by increasing the worst-case\nperformance and decreasing variance across the\nchoice of examples in the few shot training sets (§4.3).\nTo remove handcrafted verbalizers (with variable\ntoken lengths), we introduce a new multi-token\nfixed-length classifier schemethat learns task label\nembeddings which are independent from the language\nmodel vocabulary during fine-tuning (§3.2). We\nshow (§4) that this approach is sample efficient\nand outperforms carefully engineered verbalizers\nfrom random initialization(§4). It also allows us\nto avoid previously used expensive auto-regressive\ndecoding schemes (Schick and Schütze, 2021b), by\nleveraging prototypical networks (Snell et al., 2017)\nover multiple tokens. Overall, these changes enable\nup to 100x faster learning and inference (§4.2).\n3638\nPERFECT has several advantages: It avoids\nengineering patterns and verbalizers for each new\ntask, which can be cumbersome. Recent work has\nshown that even some intentionally irrelevant or\nmisleading prompts can perform as well as more\ninterpretable ones (Webson and Pavlick, 2021).\nUnlike the zero-shot or extreme few-shot case, where\nprompting might be essential, we argue in this paper\nthat all you need is tens of training examples to avoid\nthese challenges by adoptingPERFECT or a similar\ndata-efficient learning method. Experiments on a\nwide variety of NLP tasks demonstrate thatPERFECT\noutperforms state-of-the-art prompt-based methods\nwhile being significantly more efficient in inference\nand training time, storage, and memory usage (§4.2).\nTo the best of our knowledge, we are the first to\npropose a few-shot learning method using the MLM\nobjective in PLMs that provide state-of-the-art results\nwhile removing all per-task manual engineering.\n2 Background\nProblem formulation: We consider a general\nproblem of fine-tuning language models in a few-shot\nsetting, on a small training set withK unique classes\nand N examples per class, such that the total number\nof examples is|D|=N×K. Let D=∪K\nk=1Dk be the\ngiven training set, whereDk ={(xi\nk,yi\nk)}N\ni=1 shows\nthe set of examples labeled with classk and yi\nk ∈Y\nis the corresponding label, where |Y| = K. We\nadditionally assume access to a development set with\nthe same size as the training data. Note that larger val-\nidation sets can grant a substantial advantage (Perez\net al., 2021), and thus it is important to use a limited\nvalidation size to be in line with the goal of few-shot\nlearning. Unless specified otherwise, in this work, we\nuse 16 training examples (N = 16) and a validation\nset with 16 examples, for a total of 32-shot learning.\n2.1 Adapters\nRecent work has shown that fine-tuningall param-\neters of PLMs with a large number of parameters\nin low-resource datasets can lead to a sub-optimal\nsolution (Peters et al., 2019; Dodge et al., 2020). As\nshown in Figure 2, Rebuffi et al. (2018) and Houlsby\net al. (2019) suggest an efficient alternative, by\ninserting small task-specific modules calledadapters\nwithin layers of a PLMs. They then only train the\nnewly added adapters and layer normalization, while\nfixing the remaining parameters of a PLM.\nEach layer of a transformer model is composed\nof two primary modules: a) an attention block,\nFeed forward down\nprojection\nNonlinearity\nAdapter Layer\nMulti-head attention\nAdapter\n+ \nTransformer Layer\nLayer norm\nFeed forward\nAdapter\n+ \nLayer norm\nFeed forward  \nup projection\n+ \nFigure 2: Left: Adapter integration in a PLM. Right: An\nadapter architecture. Adapters are usually inserted after the\nfeed-forward and self-attention modules. During training,\nwe only optimize the green components\nand b) a feed-forward block, where both modules\nare followed by a skip connection. As depicted in\nFigure 2, adapters are normally inserted after each\nof these blocks before the skip connection.\nAdapters are bottleneck architectures. By keeping\ninput and output dimensions the same, they introduce\nno additional architectural changes. Each adapter,\nA(.) ∈ RH, consists of a down-projection,D(.) ∈\nRH×B, a non-linearity, such as GeLU (Hendrycks and\nGimpel, 2016), and an up-projectionU(.) ∈RB×H,\nwhere H is the dimension of input hidden statesx,\nand B is the bottleneck size. Formally defined as:\nA(x)= U(GeLU(D(x)))+x, (1)\n2.2 Prompt-based Fine-tuning\nStandard Fine-tuning: In standard fine-tuning\nwith PLMs (Devlin et al., 2019), first a special[CLS]\ntoken is appended to the inputx, and then the PLM\nmaps it to a sequence of hidden representations\nh = (h1,..., hS) with hi ∈ RH, where H is the\nhidden dimension, andS is the maximum sequence\nlength. Then, a classifier,softmax(WT h[CLS]), using\nthe embedding of the classification token (h[CLS]),\nis trained end-to-end for each downstream task. The\nmain drawback of this approach is the discrepancy\nbetween the pre-training and fine-tuning phases since\nPLMs have been trained topredict mask tokensin a\nmasked language modeling task (Devlin et al., 2019).\nPrompt-based tuning: To address this discrepancy,\nprompt-based fine-tuning (Schick and Schütze,\n3639\n2021a,b; Gao et al., 2021) formulates tasks in a cloze-\nformat (Taylor, 1953). This way, the model can predict\ntargets with a masked language modeling (MLM)\nobjective. For example, as shown in Figure 1, for a\nsentiment classification task, inputs are converted to:\nxprompt = [CLS]x . It was| {z }\npattern\n[MASK]. [SEP]\nThen, the PLM determines which verbalizer(e.g.,\n‘great’ and ‘terrible’) is the most likely substitute for\nthe mask in thexprompt. This subsequently determines\nthe score of targets (‘positive’ or ‘negative’). In detail:\nTraining strategy: Let M:Y →Vbe a mapping\nfrom target labels to individual words in a PLM’s\nvocabulary. We refer to this mapping asverbalizers.\nThen the input is converted to xprompt = T (x) by\nappending apatternand a mask tokento x so that it\nhas the format of a masked language modeling input.\nThen, the classification task is converted to a MLM\nobjective (Tam et al., 2021; Schick and Schütze,\n2021a), and the PLM computes the probability of the\nlabely as:\np(y|x)= p([MASK]=M(y)|xprompt)\n=\nexp(WT\nM(y)h[MASK])\nP\nv′∈Vexp(WT\nv′ h[MASK]), (2)\nwhereh[MASK] is the last hidden representation of the\nmask, and Wv shows the output embedding of the\nPLM for each verbalizerv∈V. For many tasks, ver-\nbalizers have multiple tokens. Schick and Schütze\n(2021b) extended (2) to multiple mask tokens by\nadding the maximum number of mask tokens M\nneeded to express the outputs (verbalizers) for a task.\nIn that case, Schick and Schütze (2021b) computes\nthe probability of each class as the summation of the\nlog probabilities of each token in the corresponding\nverbalizer, and then they add a hinge loss to ensure a\nmargin between the correct verbalizer and the incor-\nrect ones.\nInference strategy: During inference, the model\nneeds to select which verbalizer to use in the given\ncontext. Schick and Schütze (2021b) predicts the\nverbalizer tokens in an autoregressive fashion. They\nfirst trim the number of mask tokens fromM to each\ncandidate verbalizer’s token length and compute the\nprobability of each mask token. They then choose\nthe predicted token with the highest probability and\nreplace the corresponding mask token. Conditioning\nMASK1CLS\nMulti-head Attention\nAdapter\n+ \nPLM Layer\nLayer norm\nFeed forward\n+ \nLayer norm\nEmbedding Layer\nSEPTOK1\nAdapter\nMASK \nEmbedding1\nHinge Loss\nDesired \nLabels\nEstimated \nLabels\nTOKN MASKM\nMASK \nEmbeddingM\nWM\nLabel Embedding\nW1\nInput Masks \nFigure 3: We remove handcrafted patterns and verbalizers.\nWe replace patterns using task-specific adapters and design\nlabel embeddings for the classes. We only train the green\nblocks (the label embeddings, adapters, and layer norms).\non this new token, the probabilities of the remaining\nmask positions are recomputed. They repeat this\nautoregressive decoding until they fill all mask\npositions. This inference strategy is very slow, as the\nnumber of forward passes increases with the number\nof classes and the number of verbalizer’s tokens.\nThis formulation obtained impressive few-shot\nperformance with PLMs. However, the success of this\napproach heavily relies on engineering handcrafted\npatternsand verbalizers. Coming up with suitable\nverbalizers and patterns can be difficult (Mishra et al.,\n2022b,a). Additionally, the performance is sensitive to\nthe wording of patterns (Zhao et al., 2021; Perez et al.,\n2021; Schick and Schütze, 2021a; Jiang et al., 2020) or\nto the chosen verbalizers (Webson and Pavlick, 2021).\nIn addition, handcrafted verbalizers cause problems\nfor efficient training: a) they require updating the\nPLM embedding layer, causing large memory\noverhead; b) fine-tuning PLMs also requires a very\nsmall learning rate (usually 10−5), which slows\ndown tuning the parameters of the verbalizers;\nc) modeling verbalizers as one of the tokens of\n3640\nthe PLM vocabulary (perhaps unintentionally)\nimpacts the input representation during tuning; d)\nverbalizers have variable token lengths, complicating\nthe implementation in a vectorized format, thereby\nmaking it challenging to efficiently fine-tune PLMs.\n3 Method\nWe proposePERFECT , a verbalizer and pattern free\nfew-shot learning method. We designPERFECT to\nbe close to the pre-training phase, similar to the PET\nfamily of models (Schick and Schütze, 2021b; Gao\net al., 2021), while replacing handcrafted patterns and\nverbalizers with new components that are designed\nto describe the task and learn the labels. As shown\nin Figure 3, we first convert each inputxinput to its\nmasked language modeling (MLM) input containing\nM mask tokens[MASK] 1 with no added patterns,\ndenoted as xmasked = T\n′\n(xinput).2 PERFECT then\ntrains a classifier per-token and optimizes the average\nmulti-class hinge loss over each mask position.\nThree main components play a role in the success\nof PERFECT : a) a pattern-free task description, where\nwe use task-specific adapters to efficiently tell the\nmodel about the given task, replacing previously\nmanually engineered patterns (§3.1), b) multi-token\nlabel-embedding as an efficient mechanism to learn\nthe label representations, removing manually designed\nverbalizers (§3.2). c) an efficient inference strategy\nbuilding on top of the idea of prototypical networks\n(Snell et al., 2017) (§3.4), which replaces prior\niterative autoregressive decoding methods (Schick\nand Schütze, 2021b).\nAs shown in Figure 3, we fix the underlying PLM\nmodel and only optimize the new parameters that\nwe add (green boxes). This includes the task-specific\nadapters to adapt the representations for a given task\nand the multi-token label representations. We detail\neach of these components below.\n3.1 Pattern-Free Task Description\nWe use task-specific adapter layers to provide\nthe model with learned, implicit task descriptions.\nAdapters additionally bring multiple other benefits:\na) fine-tuning all weights of PLMs with millions or\nbillions of parameters is sample-inefficient, and can\nbe unstable in low-resource settings (Dodge et al.,\n1We discuss the general case with inserting multiple masks;\nfor some datasets this improves performance (§4.3.1).\n2We insert mask tokens after the input string in single-\nsentence benchmarks, and after the first sentence in the case\nof sentence-pair datasets and encode both sentences as a single\ninput, which we found to perform the best (Appendix C).\n2020); adapters allow sample-efficient fine-tuning, by\nkeeping the underlying PLM fixed, b) adapters reduce\nthe storage and memory footprints (§4.2), c) they\nalso increase stability and performance (§4), making\nthem an excellent choice for few-shot fine-tuning.\nTo our knowledge, this is the first approach for using\ntask-specific adaptersto effectively and efficiently\nremove patterns in few-shot learning. Experimental\nresults in §4 show its effectiveness compared to\nhandcrafted patterns and soft prompts (Li and Liang,\n2021; Lester et al., 2021).\n3.2 Multi-Token Label Embeddings\nWe freeze the weights of the PLM’s embedding\nlayer and introduce a separate label embedding\nL∈RK×M×H, which is a multi-token label represen-\ntation whereM is the number of tokens representing\neach label,K indicates the number of classes,H is\nthe input hidden dimension. Using a fixed number of\ntokensM for each label, versus variable-token length\nverbalizers used in prior work (Schick and Schütze,\n2021a,b) substantially simplifies the implementation\nand accelerates the training (§4.2).\n3.3 Training PERFECT\nAs shown in Figure 3, we optimize label embeddings\nso that the PLM predicts the correct label, and\noptimize adapters to adapt the PLM for the given task.\nFor label embeddings,PERFECT trains a classifier\nper token and optimizes the average multi-class\nhinge loss over all mask positions. Given xmasked,\nlet h[MASK] i be the embedding of itsi-th mask token\nfrom the last layer of the PLM encoder. Additionally,\nlet f(.) :RH → RK be a per-token classifier that\ncomputes the predictions by multiplying the mask\ntoken embedding with its corresponding label\nembedding. Formally defined as:\nti =f(h[MASK] i)= LT\ni h[MASK] i,\nwhere Li ∈ RK×H shows the label embedding for\nthe i-th mask position. Then, for each mask position,\nwe optimize a multi-class hinge loss between their\nscoresti and labels. Formally defined as:\nL(x,y,i)=\nPK\nk=1,k̸=ymax(0,m−tiy+tik)\nK ,\nwheretik shows thek-th element ofti, representing\nthe score corresponding to class k, and m is the\nmargin, which we fix to the default value ofm = 1.\nThen, the final loss is computed by averaging the loss\n3641\nover all mask tokens and training samples:\nL= 1\nM|D|\nX\n(x,y)∈D\nMX\ni=1\nL(x,y,i) (3)\n3.4 Inference with PERFECT\nDuring evaluation, instead of relying on the prior\niterative autoregressive decoding schemes (Schick\nand Schütze, 2021b), we classify a query point by\nfinding the nearest class prototype to the mask token\nembeddings:\ny=argmax\ny∈Y\nmax\ni∈{1,...,M}\n\u0010\nexp−d(hq\ni ,ciy)\n\u0011\n, (4)\nwhered is squared euclidean distance,3 hq\ni indicates\nthe embedding of the i-th mask position for the\nquery sample q, and ciy ∈ RD is the prototype\nrepresentation of thei-th mask token with class label\ny, i.e., the mean embedding ofi-th mask position in\nall training samples with labely:\nciy = 1\n|Dy|\nX\nb∈Dy\nhb\ni, (5)\nwherehb\ni shows the embedding ofi-th mask position\nfor training sampleb, and Dy is the training instances\nwith classy. This strategy closely follows prototypical\nnetworks (Snell et al., 2017), but applied across\nmultiple tokens. We choose this form of inference\nbecause prototypical networks are known to be\nsample efficient and robust (Snell et al., 2017),\nand because it substantially speeds up evaluation\ncompared to prior methods (§4.2).\n4 Experiments\nWe conduct extensive experiments on a variety of\nNLP datasets to evaluate the performance ofPERFECT\nand compare it with state-of-the-art few-shot learning.\nDatasets: We consider 7 tasks and 12 datasets: 1)\nthe sentiment analysis datasets SST-2 (Socher et al.,\n2013), SST-5 (Socher et al., 2013), MR (Pang and\nLee, 2005), and CR (Hu and Liu, 2004), 2) the\nsubjectivity classification dataset SUBJ (Pang and\nLee, 2004), 3) the question classification dataset\nTREC (V oorhees and Tice, 2000), 4) the natural\nlanguage inference datasets CB (De Marneffe et al.,\n2019) and RTE (Wang et al., 2019a), 5) the question\nanswering dataset QNLI (Rajpurkar et al., 2016), 6)\nthe word sense disambiguation dataset WiC (Pilehvar\n3We also tried with cosine similarity but found a slight\nimprovement with squared Euclidean distance (Snell et al., 2017).\nand Camacho-Collados, 2019), 7) the paraphrase\ndetection datasets MRPC (Dolan and Brockett, 2005)\nand QQP .4 See datasets statistics in Appendix A.\nFor MR, CR, SST-5, SUBJ, and TREC, we test on\nthe original test sets, while for other datasets, since test\nsets are not publicly available, we test on the original\nvalidation set. We sample 16 instances per label from\nthe training set to form training and validation sets.\nBaselines We compare with the state-of-the-art\nfew-shot learning of PET and fine-tuning:\nPET (Schick and Schütze, 2021a,b) is the state-\nof-the-art few-shot learning method that employs\ncarefully crafted verbalizers and patterns. We report\nthe best (PET -best) and average (PET-average) results\namong all patterns and verbalizers.5\nFINETUNE The standard fine-tuning (Devlin et al.,\n2019), with adding a classifier on top of the [CLS]\ntoken and fine-tuning all parameters.\nOur method We study the performance of\nPERFECT and perform an extensive ablation study\nto show the effectiveness of our design choices:\nPERFECT -rand We randomly initialize the label\nembeddingL from a normal distributionN(0,σ) with\nσ = 10−4 (chosen based on validation performance,\nsee Appendix D)without relying on any handcrafted\npatterns and verbalizers. As an ablation, we study\nthe following two variants:\nPERFECT -init We initialize the label embedding\nwith the token embeddings of manually designed\nverbalizers in the PLM’s vocabulary to study the\nimpact of engineered verbalizers.\nprompt+mteTo compare the impact of adapters\nversus soft prompt-tuning for few-shot learning, we\nappend trainable continuous prompt embeddings to\nthe input (Lester et al., 2021). Then we only tune the\nsoft prompt and multi-token label embeddings (mte).\nbitfit+mte Following Cai et al. (2020) and Rav-\nfogel et al. (2021), we tune biases as an alternative\nto adapters. We additionally tune multi-token label\nembeddings.\nLogan IV et al. (2021)Following Logan IV et al.\n(2021), we remove patterns and tune the biases in the\nPET.\nExperimental details: We use the RoBERTa large\nmodel (Liu et al., 2019) (355M parameters) as the un-\nderlying PLM for all methods. We use the Hugging-\nFace PyTorch implementation (Wolf et al., 2020). For\n4https://quoradata.quora.com/\n5For a controlled study, we use the MLM variant shown in\n(2), which has been shown to perform the best (Tam et al., 2021).\n3642\nMethod SST-2 CR MR SST-5 Subj TREC Avg\nSingle-Sentence Benchmarks\nFINETUNE 81.4/70.0/4.0 80.1/72.9/4.1 77.7/66.8/4.6 39.2/34.3/2.5 90.2/84.1/1.8 87.6/75.8/3.7 76.0/67.3/3.4\nPET-Average 89.7/81.0/2.4 88.4/68.8/3.0 85.9/79.0/2.1 45.9/40.3/2.4 88.1/79.6/2.4 85.0/70.6/4.5 80.5/69.9/2.8\nPET-Best 89.1/81.0/2.6 88.8/85.8/1.9 86.4/82.0/1.6 46.0/41.2/2.4 88.7/84.6/1.8 85.8/70.6/4.4 80.8/74.2/2.4\nLogan IV et al. (2021)89.8/84.1/1.7 89.9/87.2/1.1 84.9/76.2/3.2 45.7/41.6/2.3 81.8/73.5/4.0 84.7/81.8/1.6 79.5/74.1/2.3\nPERFECT -rand 90.7/88.2/1.2 90.0/85.5/1.4 86.3/81.4/1.6 42.7/35.1/2.9 89.1/82.8/2.1 90.6/81.6/3.2 81.6/75.8/2.1\nAblation\nPERFECT -init 90.9/87.6/1.5 89.7/87.4/1.2 85.4/75.8/3.3 42.8/35.9/3.5 87.6/81.6/2.8 90.4/86.6/1.8 81.1/75.8/2.4\nprompt+mte 70.6/56.0/8.3 71.0/55.8/8.2 66.6/49.6/7.3 32.2/26.5/3.2 82.7/69.6/3.9 79.6/66.8/6.5 67.1/54.0/6.2\nbitfit+mte 89.5/81.7/3.0 90.1/87.8/1.0 85.6/80.5/1.9 42.3/36.8/3.3 89.1/82.4/2.4 90.4/85.0/1.4 81.2/75.7/2.2\nMethod CB RTE QNLI MRPC QQP WiC Avg\nSentence-Pair Benchmarks\nFINETUNE 72.9/67.9/2.5 56.8/50.2/3.5 62.7/51.4/7.0 70.1/62.7/4.7 65.0/59.8/3.6 52.4/46.1/3.7 63.3/56.4/4.2\nPET-Average 86.9/73.2/5.1 60.1/49.5/4.7 66.5/55.7/6.2 62.1/38.2/6.8 63.4/44.7/7.9 51.0/46.1/2.6 65.0/51.2/5.6\nPET-Best 90.0/78.6/3.9 62.3/51.3/4.5 70.5/57.9/6.4 63.4/49.3/6.5 70.7/55.2/5.8 51.6/47.2/2.3 68.1/56.6/4.9\nLogan IV et al. (2021)91.0/87.5/2.7 64.4/58.5/3.9 71.2/66.5/2.6 63.9/53.7/5.3 70.4/62.7/3.4 52.4/48.4/1.8 68.9/62.9/3.3\nPERFECT -rand 90.3/83.9/3.5 60.4/53.1/4.7 74.1/60.3/4.6 67.8/54.7/5.7 71.2/64.2/3.5 53.8/47.0/3.0 69.6/60.5/4.2\nAblation\nPERFECT -init 87.9/75.0/4.9 60.7/52.7/4.5 72.8/56.7/6.8 65.9/56.6/6.0 71.1/65.6/3.5 51.7/46.6/2.8 68.4/58.9/4.8\nprompt+mte 73.0/62.5/6.1 56.9/50.7/4.1 55.4/50.2/4.6 60.0/51.5/5.8 54.3/46.2/5.6 51.3/46.7/2.8 58.5/51.3/4.8\nbitfit+mte 89.6/82.1/4.3 61.3/53.8/5.2 70.6/51.9/5.9 68.5/57.4/5.1 69.4/63.0/3.9 52.9/47.8/2.7 68.7/59.3/4.5\nTable 1: Performance of all methods on single-sentence and sentence-pair benchmarks. We report average/worst-case\naccuracy/standard deviation. PERFECT obtains the state-of-the-art results. Bold fonts indicate the best results.\nthe baselines, we used the carefully manually designed\npatterns and verbalizers in Gao et al. (2021), Min et al.\n(2021), and Schick and Schütze (2021b) (usually 5\ndifferent options per datasets; see Appendix B).\nWe evaluate all methods using 5 different random\nsamples to create the training/validation sets and 4\ndifferent random seeds for training. Therefore, for\nPET-average, we report the results on 20 x 5 (number\nof patterns and verbalizers) = 100 runs, while for\nPET-best and our method, we report the results over\n20 runs. The variance in few-shot learning methods is\nusually high (Perez et al., 2021; Zhao et al., 2021; Lu\net al., 2021). Therefore, we report average, worst-case\nperformance, and standard deviation across all runs,\nwhere the last two values can be important for\nrisk-sensitive applications (Asri et al., 2016).\n4.1 Experimental Results\nTable 1 shows the performance of all methods.\nPERFECT obtains state-of-the-art results, improving\nthe performance compared toPET-average by +1.1\nand +4.6 points for single-sentence and sentence-pair\ndatasets respectively. It even outperformsPET-best,\nwhere we report the best performance ofPET across\nmultiple manually engineered patterns and verbalizers.\nMoreover, PERFECT generally improves the mini-\nmum performance and reduces standard deviation\nsubstantially. Finally,PERFECT is also significantly\nmore efficient: reducing the training and inference\ntime, memory usage, and storage costs (see §4.2).\nPET-best improves the results overPET-average\nshowing thatPET is unstable to the choice of patterns\nand verbalizers; this difference is more severe for\nsentence-pair benchmarks. This might be because the\nposition of the mask highly impacts the results, and\nthe patterns used for sentence-pair datasets in Schick\nand Schütze (2021b) exploits this variation by putting\nthe mask in multiple locations (see Appendix B).\nRemoving patterns and tuning biases in Logan IV\net al. (2021) is not expressive enough and performs\nsubstantially worse than PERFECT on average.\nAs an ablation, even if we initialize the label\n3643\nMetric PET P ERFECT ∆%\nTrained params (M) 355.413.28 -99.08%\nPeak memory (GB) 20.93 16.34 -21.93%\nTraining time (min) 23.42 0.65 -97.22%\n+ PET in batch 0.94 0.65 -30.85%\nInference time (min) 9.57 0.31 -96.76%\nTable 2: Percentage of trained parameters, average peak\nmemory, training, and inference time.∆% is the relative\ndifference with respect to PET. Lower is better.\nembedding with handcrafted verbalizers in PER-\nFECT-init, it consistently obtains lower performance,\ndemonstrating thatPERFECT is able to obtain state-of-\nthe-art performance with learning frompure random\ninitialization. We argue that initializing randomly\nclose to zero (with low varianceσ =10−4), as done\nin our case, slightly improves performance, which\nperhaps is not satisfied when initializing from the\nmanually engineered verbalizers (see Appendix D).\nAs a second ablation, when learning patterns with\noptimizing soft prompts in prompt+mte, we observe\nhigh sensitivity to learning rate, as also confirmed\nin Li and Liang (2021) and Mahabadi et al. (2021a).\nWe experimented with multiple learning rates but\nperformance consistently lags behindPERFECT -rand.\nThis can be explained by the low flexibility of such\nmethods as all the information regarding specifying\npatterns needs to be contained in the prefixes. As a\nresult, the method only allows limited interaction with\nthe rest of the model parameters, and obtaining good\nperformance requires very large models (Lester et al.,\n2021). In addition, increasing the sequence length\nleads to memory overhead (Mahabadi et al., 2021a),\nand the number of prompt tokens is capped by the\nnumber of tokens that can fit in the maximum input\nlength, which can be a limitation for tasks requiring\nlarge contexts.\nAs a third ablation, tuning biases with optimizing\nsoft prompts in bitfit+mte obtains lower performance\ncompared toPERFECT , showing that adapters are a\nbetter alternative compared to tuning biases to learn\ntask descriptions for few-shot learning.\nWe include more ablation results on design choices\nof PERFECT in Appendix E.\n4.2 Efficiency Evaluation\nIn this section, we compare the efficiency ofPERFECT\nwith the state-of-the-art few-shot learning method,\nPET. To this end, we train all methods for ten epochs\non the 500-sampled QNLI dataset. We select the\nlargest batch size for each method that fits a fixed\nbudget of the GPU memory (40 GB).\nDue to the auto-regressive inference strategy of\nPET (Schick and Schütze, 2021b), all prior work\nimplemented it with a batch size of 1 (Perez et al.,\n2021; Schick and Schütze, 2021b; Tam et al., 2021).\nAdditionally, since PET deals with verbalizers of\nvariable lengths, it is hard to implement their training\nphase in batch mode. We specifically choose QNLI\nto have verbalizers of the same length and enable\nbatching for comparison purposes (referred to as\nPET in batch). However, verbalizers are still not of\nfixed-length for most other tasks, and this speed-up\ndoes not apply generally to PET.\nIn Table 2, for each method we report the\npercentage of trained parameters, memory usage,\ntraining time, and inference time.PERFECT reduces\nthe number of trained parameters, and therefore the\nstorage requirement, by 99.08%. It additionally re-\nduces the memory requirement by 21.93% compared\nto PET. PERFECT speeds up training substantially, by\n97.22% relative to the originalPET’s implementation,\nand 30.85% to our implementation of PET. This is\nbecause adapter-based tuning saves on memory and\nallows training with larger batch sizes. In addition,\nPERFECT is significantly faster during inference time\n(96.76% less inference time relative to PET).\nNote that although prompt+mte and bitfit+mte can\nalso reduce the storage costs, by having 0.02M and\n0.32 M trainable parameters respectively, they are not\nexpressive enough to learn task descriptions, and their\nperformance substantially lags behindPERFECT (see\nTable 1).\nOverall, given the size of PLMs with millions\nand billions of parameters (Liu et al., 2019; Raffel\net al., 2020), efficient few-shot learning methods are\nof paramount importance for practical applications.\nPERFECT not only outperforms the state-of-the-art in\nterms of accuracy and generally improves the stability\n(Table 1), but also is significantly more efficient in\nruntime, storage, and memory.\n4.3 Analysis\nCan task-specific adapters replace manually\nengineered patterns? PERFECT is a pattern-free\napproach and employs adapters to provide the PLMs\nwith task descriptions implicitly. In this section, we\nstudy the contribution of replacing manual patterns\nwith adapters in isolation without considering our\nother contributions in representing labels, training,\nand inference. In PET (Schick and Schütze, 2021a,b),\n3644\nDataset PET-Average Pattern-Free\nSST-2 89.7/81.0/2.4 90.5/87.8/1.2\nCR 88.4/68.8/3.0 89.8/87.0/1.4\nMR 85.9/79.0/2.1 86.4/83.0/1.8\nSST-5 45.9/40.3/2.4 44.8/40.0/2.4\nSUBJ 88.1/79.6/2.4 85.3/74.7/3.8\nTREC 85.0/70.6/4.5 87.9/84.6/1.8\nCB 86.9/73.2/5.1 93.0/89.3/1.9\nRTE 60.1/49.5/4.7 63.7/56.3/4.1\nQNLI 66.5/55.7/6.2 71.3/65.8/2.5\nMRPC 62.1/38.2/6.8 66.0/54.4/5.6\nQQP 63.4/44.7/7.9 71.8/64.3/3.7\nWiC 51.0/46.1/2.6 53.7/50.3/2.0\nAvg 72.8/60.6/4.2 75.4/69.8/2.7\nTable 3: Average performance ofPET with five different\npatterns vs.Pattern-Freethat replaces handcrafted patterns\nwith task-specific adapters. We report the average/worst-\ncase performance/and the standard deviation.\nwe replace the handcrafted patterns with task-specific\nadapters (Pattern-Free) while keeping the verbalizers\nand the training and inference intact6 and train it\nwith a similar setup as in §4. Table 3 shows the\nresults. While PET is very sensitive to the choice of\nprompts, adapters provide an efficient alternative to\nlearn patterns robustly by improving the performance\n(average and worst-case) and reducing the standard\ndeviation. This finding demonstrates that task-specific\nadapters can effectively replace manually engineered\nprompts. Additionally, they also save on the training\nbudget by at least 1/number of patterns(normally\n1/5) by not requiring running the method for different\nchoices of patterns, and by freezing most parameters,\nthis saves on memory and offers additional speed-up.\n4.3.1 Ablation Study\nImpact of Removing Adapters To study the\nimpact of adapters in learning patterns, we remove\nadapters, while keeping the label embedding.\nHandcrafted patterns are not included and we\ntune all parameters of the model. Table 4 shows\nthe results. Adding adapters for learning patterns\ncontributes to the performance by improving the\naverage performance, and making the model robust by\nimproving the minimum performance and reducing\nthe standard deviation. This is because training PLMs\nwith millions of parameters is sample-inefficient\nand unstable on resource-limited datasets (Dodge\n6Since we don’t have patterns, in the case of multiple sets of\nverbalizers, we use the first set of verbalizers as a random choice.\nDataset PERFECT -Adapters\nSST-2 90.7/88.2/1.2 88.2/81.9/2.3\nCR 90.0/85.5/1.4 89.2/83.1/1.7\nMR 86.3/81.4/1.6 82.5/78.2/2.5\nSST-5 42.7/35.1/2.9 40.6/33.6/3.3\nSUBJ 89.1/82.8/2.1 89.7/85.0/1.9\nTREC 90.6/81.6/3.2 89.8/74.2/4.3\nCB 90.3/83.9/3.5 89.6 /83.9/2.8\nRTE 60.4/53.1/4.7 61.7/53.8 /5.1\nQNLI 74.1/60.3/4.6 73.2/56.3/5.8\nMRPC 67.8/54.7/5.7 68.0/54.2/6.1\nQQP 71.2/64.2/3.5 71.0/62.0/3.7\nWiC 53.8/47.0/3.0 52.5/46.9/3.0\nAvg 75.6/68.1/3.1 74.7/66.1/3.5\nTable 4: Performance ofPERFECT w/o adapters,-Adapters.\nWe report the average performance/worst-case perfor-\nmance/and the standard deviation.\net al., 2020; Zhang et al., 2020; Mosbach et al., 2021).\nHowever, by using adapters, we substantially reduce\nthe number of trainable parameters, allowing the\nmodel to be better tuned in a few-shot setting.\nImpact of the number of masksIn Table 1, to\ncompare our design with PET in isolation, we fixed\nthe number of mask tokens as the maximum number\ninserted by PET. In table 5, we study the impact of\nvarying the number of inserted mask tokens for a\nrandom selection of six tasks. For most tasks, having\ntwo mask tokens performs the best, while for MR and\nRTE, having one, and for MRPC, inserting ten masks\nimproves the results substantially. The number of\nrequired masks might be correlated with the difficulty\nof the task. PERFECT is designed to be general,\nenabling having multiple mask tokens.\n5 Related Work\nAdapter Layers: Mahabadi et al. (2021b) and\nÜstün et al. (2020) proposed to generate adapters’\nweights using hypernetworks (Ha et al., 2017), where\nMahabadi et al. (2021b) proposed to share a small\nhypernetwork to generate conditional adapter weights\nefficiently for each transformer layer and task. Ma-\nhabadi et al. (2021a) proposed compacter layers by\nbuilding on top of ideas of parameterized hyper-\ncomplex layers (Zhang et al., 2021) and low-rank\nmethods (Li et al., 2018; Aghajanyan et al., 2021), as\nan efficient fine-tuning method for PLMs. We are\nthe first to employ adapters to replace handcrafted\npatterns for few-shot learning.\n3645\nDatasets 1 2 5 10\nCR 90.1 90.2 89.0 87.8\nMR 86.9 86.1 85.4 85.6\nMRPC 67.4 68.2 70.1 72.3\nQNLI 73.7 73.9 73.0 65.1\nRTE 60.0 57.3 56.2 56.0\nTREC 90.0 90.9 88.9 88.8\nAvg 78.0 77.8 77.1 75.9\nTable 5: Test performance for the varying number of mask\ntokens. Bold fonts indicate the best results in each row.\nFew-shot Learning with PLMs:Le Scao and Rush\n(2021) showed that prompting provides substantial im-\nprovements compared to fine-tuning, especially in\nlow-resource settings. Subsequently, researchers con-\ntinuously tried to address the challenges of manually\nengineered patterns and verbalizers: a) Learning the\npatterns in a continuous space (Li and Liang, 2021;\nQin and Eisner, 2021; Lester et al., 2021), while freez-\ning PLM for efficiency, has the problem that, in most\ncases, such an approach only works with very large\nscale PLMs (Lester et al., 2021), and lags behind full\nfine-tuning in a general setting, while being ineffi-\ncient and not as effective compared to adapters (Ma-\nhabadi et al., 2021a). b) Optimizing patterns in a\ndiscrete space (Shin et al., 2020; Jiang et al., 2020;\nGao et al., 2021) has the problem that such methods\nare computationally costly. c) Automatically find-\ning verbalizers in a discrete way (Schick et al., 2020;\nSchick and Schütze, 2021a) is computationally ex-\npensive and does not perform as well as manually\ndesigned ones. d) Removing manually designed pat-\nterns (Logan IV et al., 2021) substantially lags behind\nthe expert-designed ones. Our proposed method,PER-\nFECT, does not rely on any handcrafted patterns and\nverbalizers.\n6 Conclusion\nWe proposedPERFECT , a simple and efficient method\nfor few-shot learning with pre-trained language\nmodels without relying on handcrafted patterns\nand verbalizers. PERFECT employs task-specific\nadapters to learn task descriptions implicitly, replacing\nprevious handcrafted patterns, and a continuous\nmulti-token label embedding to represent the output\nclasses. Through extensive experiments over 12 NLP\nbenchmarks, we demonstrate thatPERFECT , despite\nbeing far simpler and more efficient than recent\nfew-shot learning methods, produces state-of-the-art\nresults. Overall, the simplicity and effectiveness of\nPERFECT make it a promising approach for few-shot\nlearning with PLMs.\nAcknowledgements\nThe authors would like to thank Sebastian Ruder\nand Marius Mosbach for their comments on drafts\nof this paper. This research was partly supported by\nthe Swiss National Science Foundation under grant\nnumber 200021_178862.\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.\n2021. Intrinsic dimensionality explains the effectiveness\nof language model fine-tuning. InACL.\nHiba Asri, Hajar Mousannif, Hassan Al Moatassime,\nand Thomas Noel. 2016. Using machine learning\nalgorithms for breast cancer risk prediction and\ndiagnosis. Procedia Computer Science.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and\nDanilo Giampiccolo. 2006. The second pascal recognis-\ning textual entailment challenge.Second PASCAL Chal-\nlenges Workshop on Recognising Textual Entailment.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\nGiampiccolo, and Bernardo Magnini. 2009. The fifth\npascal recognizing textual entailment challenge. InTAC.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. 2020. Language models are few-shot\nlearners. InNeurIPS.\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han. 2020.\nTinytl: Reduce memory, not parameters for efficient\non-device learning. InNeurIPS.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005.\nThe pascal recognising textual entailment challenge. In\nMachine Learning Challenges Workshop.\nMarie-Catherine De Marneffe, Mandy Simons, and\nJudith Tonhauser. 2019. The commitmentbank:\nInvestigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of deep\nbidirectional transformers for language understanding.\nIn NAACL.\n3646\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight\ninitializations, data orders, and early stopping.arXiv\npreprint arXiv:2002.06305.\nWilliam B Dolan and Chris Brockett. 2005. Automatically\nconstructing a corpus of sentential paraphrases. InIWP.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making\npre-trained language models better few-shot learners.\nIn ACL.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing tex-\ntual entailment challenge. InACL-PASCAL Workshop\non Textual Entailment and Paraphrasing.\nDavid Ha, Andrew Dai, and Quoc V . Le. 2017. Hypernet-\nworks. In ICLR.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian error\nlinear units (gelus).arXiv preprint arXiv:1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. InICML.\nMinqing Hu and Bing Liu. 2004. Mining and summarizing\ncustomer reviews. InSIGKDD.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? InTACL.\nTeven Le Scao and Alexander M Rush. 2021. How many\ndata points is a prompt worth? InNAACL.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. InEMNLP.\nQuentin Lhoest, Albert Villanova del Moral, Patrick\nvon Platen, Thomas Wolf, Mario Šaško, Y acine\nJernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil,\nMariama Drame, Julien Chaumond, Julien Plu, Joe\nDavison, Simon Brandeis, Victor Sanh, Teven Le\nScao, Kevin Canwen Xu, Nicolas Patry, Steven Liu,\nAngelina McMillan-Major, Philipp Schmid, Sylvain\nGugger, Nathan Raw, Sylvain Lesage, Anton Lozhkov,\nMatthew Carrigan, Théo Matussière, Leandro von\nWerra, Lysandre Debut, Stas Bekman, and Clément\nDelangue. 2021a. huggingface/datasets: 1.15.1.\nQuentin Lhoest, Albert Villanova del Moral, Y acine Jernite,\nAbhishek Thakur, Patrick von Platen, Suraj Patil,\nJulien Chaumond, Mariama Drame, Julien Plu, Lewis\nTunstall, Joe Davison, Mario Šaško, Gunjan Chhablani,\nBhavitvya Malik, Simon Brandeis, Teven Le Scao,\nVictor Sanh, Canwen Xu, Nicolas Patry, Angelina\nMcMillan-Major, Philipp Schmid, Sylvain Gugger,\nClément Delangue, Théo Matussière, Lysandre Debut,\nStas Bekman, Pierric Cistac, Thibault Goehringer,\nVictor Mustar, François Lagunas, Alexander Rush, and\nThomas Wolf. 2021b. Datasets: A community library\nfor natural language processing. InEMNLP.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason\nY osinski. 2018. Measuring the intrinsic dimension of\nobjective landscapes. InICLR.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. InACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nRobert L Logan IV , Ivana Balaževi´c, Eric Wallace, Fabio\nPetroni, Sameer Singh, and Sebastian Riedel. 2021.\nCutting down on prompts and parameters: Simple\nfew-shot learning with language models.arXiv preprint\narXiv:2106.13353.\nY ao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nRabeeh Karimi Mahabadi, James Henderson, and\nSebastian Ruder. 2021a. Compacter: Efficient low-rank\nhypercomplex adapter layers. InNeurIPS.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021b. Parameter-\nefficient multi-task fine-tuning for transformers via\nshared hypernetworks. InACL.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. InCommunications of the ACM.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. 2021. Noisy channel language model\nprompting for few-shot text classification. arXiv\npreprint arXiv:2108.04106.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Y ejin\nChoi, and Hannaneh Hajishirzi. 2022a. Reframing\ninstructional prompts to gptk’s language. InFindings\nof ACL.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Han-\nnaneh Hajishirzi. 2022b. Cross-task generalization via\nnatural language crowdsourcing instructions. InACL.\nMarius Mosbach, Maksym Andriushchenko, and Dietrich\nKlakow. 2021. On the stability of fine-tuning bert:\nMisconceptions, explanations, and strong baselines. In\nICLR.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nsentiment analysis using subjectivity summarization\nbased on minimum cuts. InACL.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. InACL.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In\nNeurIPS.\n3647\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. InRepL4NLP.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rück´le, Cho\nKyunghyun, and Iryna Gurevych. 2021. AdapterFusion:\nNon-destructive task composition for transfer learning.\nIn EACL.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli ´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020. Adapterhub: A\nframework for adapting transformers. In EMNLP:\nSystem Demonstrations.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluating\ncontext-sensitive meaning representations. InNAACL.\nGuanghui Qin and Jason Eisner. 2021. Learning how to ask:\nQuerying lms with mixtures of soft prompts. InNAACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. 2018. Improving language understanding\nby generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Y anqi Zhou, Wei\nLi, and Peter J Liu. 2020. Exploring the limits of transfer\nlearning with a unified text-to-text transformer.JMLR.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. InEMNLP.\nShauli Ravfogel, Elad Ben-Zaken, and Y oav Goldberg.\n2021. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked languagemodels.\narXiv:2106.10199.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nV edaldi. 2018. Efficient parametrization of multi-\ndomain deep neural networks. InCVPR.\nTimo Schick, Helmut Schmid, and Hinrich Schütze. 2020.\nAutomatically identifying words that can serve as labels\nfor few-shot text classification. InCOLING.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. InEACL.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just size\nthat matters: Small language models are also few-shot\nlearners. InNAACL.\nKarin Kipper Schuler. 2005. V erbnet: A broad-coverage,\ncomprehensive verb lexicon.PhD Thesis.\nTaylor Shin, Y asaman Razeghi, Robert L Logan IV , Eric\nWallace, and Sameer Singh. 2020. Eliciting knowledge\nfrom language models using automatically generated\nprompts. InEMNLP.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017. Pro-\ntotypical networks for few-shot learning. InNeurIPS.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang,\nChristopher D Manning, Andrew Y Ng, and Christopher\nPotts. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. InEMNLP.\nDerek Tam, Rakesh R Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training.arXiv preprint\narXiv:2103.11955.\nWilson L Taylor. 1953. “cloze procedure”: A new tool for\nmeasuring readability.Journalism quarterly.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and Gertjan\nvan Noord. 2020. Udapter: Language adaptation for\ntruly universal dependency parsing. InEMNLP.\nEllen M V oorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. InSIGIR.\nAlex Wang, Y ada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019a. Superglue: a stickier\nbenchmark for general-purpose language understanding\nsystems. InNeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. 2019b. GLUE: A\nmulti-task benchmark and analysis platform for natural\nlanguage understanding. InICLR.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-based\nmodels really understand the meaning of their prompts?\narXiv preprint arXiv:2109.01247.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Y acine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In\nEMNLP: System Demonstrations.\nAston Zhang, Yi Tay, SHUAI Zhang, Alvin Chan,\nAnh Tuan Luu, Siu Hui, and Jie Fu. 2021. Beyond fully-\nconnected layers with quaternions: Parameterization\nof hypercomplex multiplications with 1/n parameters.\nIn ICLR.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Y oav Artzi. 2020. Revisiting few-sample\nbert fine-tuning. InICLR.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improving\nfew-shot performance of language models.ICML.\n3648\nDataset Task #Train #Test K\nSingle-Sentence Benchmarks\nMR Sentiment analysis 8662 2000 2\nCR Sentiment analysis 1774 2000 2\nSST-2 Sentiment analysis 6920 872 2\nSST-5 Sentiment analysis 8544 2210 5\nSUBJ Subjectivity classification 8000 2000 2\nTREC Question classification 5452 500 6\nSentence-Pair Benchmarks\nCB Natural language inference 250 56 3\nRTE Natural language inference 2490 277 2\nWiC Word sense disambiguation 5428 638 2\nMRPC Paraphrase detection 3668 408 2\nQNLI Question answering 104743 5463 2\nQQP Paraphrase detection 363846 40430 2\nTable 6: Statistics of datasets used in this work. We sample\nN×|Y| instances (with multiple seeds) from the original\ntraining set to form the few-shot training and validation\nsets. The test column shows the size of the test set.\nA Experimental Details\nDatasets Table 6 shows the stastistics of the\ndatasets used. We download SST-2, MR, CR, SST-5,\nand SUBJ from Gao et al. (2021), while the rest of\nthe datasets are downloaded from the HuggingFace\nDatasets library (Lhoest et al., 2021b,a). RTE, CB,\nWiC datasets are from SuperGLUE benchmark (Wang\net al., 2019a), while QQP , MRPC and QNLI are from\nGLUE benchmark (Wang et al., 2019b) with Creative\nCommons license (CC BY 4.0). RTE (Wang et al.,\n2019a) is a combination of data from RTE1 (Dagan\net al., 2005), RTE2 (Bar-Haim et al., 2006), RTE3 (Gi-\nampiccolo et al., 2007), and RTE5 (Bentivogli et al.,\n2009). For WiC (Pilehvar and Camacho-Collados,\n2019) sentences are selected from V erbNet (Schuler,\n2005), WordNet (Miller, 1995), and Wiktionary.\nComputing infrastructure We run all the exper-\niments on oneNVIDIA A100 with 40G of memory.\nTraining hyper-parametersWe set the maximum\nsequence length based on the recommended values\nin the HuggingFace repository (Wolf et al., 2020)\nand prior work (Min et al., 2021; Schick and Schütze,\n2021b), i.e., we set it to 256 for SUBJ, CR, CB, RTE,\nand WiC, and 128 for other datasets. For all methods,\nwe use a batch size of 32. ForFINETUNE and PET,\nwe use the default learning rate of10−5, while for\nour method, as required by adapter-based methods\n(Mahabadi et al., 2021a), we set the learning rate to\na higher value of 10−4.7 Through all experiments,\nwe fix the adapter bottleneck size to 64. Following\nPfeiffer et al. (2021), we experimented with keeping\none of the adapters in each layer for better training\nefficiency and found keeping the adapter after the\nfeed-forward module in each layer to perform the best.\nFor tuning label embedding, we use the learning rate\nof {10−1,10−2,10−3,10−4,10−5} and choose the\none obtaining the highest validation performance. For\nPERFECT -prompt, we tune the continuous prompt\nfor learning rate of {10−1,10−2,10−3}.8Following\nLester et al. (2021), for PERFECT -prompt, we set\nthe number of prompt tokens to 20, and initialize\nthem with a random subset of the top 5000 token’s\nembedding of the PLM. We train all methods for\n6000 steps. Based on our results, this is sufficient to\nallow the models to converge. We save a checkpoint\nevery 100 steps for all methods and report the results\nfor the hyper-parameters performing the best on the\nvalidation set for each task.\nB Choice of Patterns and Verbalizers\nFor SST-2, MR, CR, SST-5, and TREC, we used\n4 different patterns and verbalizers from Gao et al.\n(2021). For CB, WiC, RTE datasets, we used the\ndesigned patterns and verbalizers in Schick and\nSchütze (2021b). For QQP , MRPC, and QNLI, we\nwrote the patterns and verbalizers inspired by the ones\nin Schick and Schütze (2021b). The used patterns\nand verbalizers are as follows:\n• For sentiment analysis tasks (MR, CR, SST-2,\nSST-5), given a sentences:\ns A <MASK> one.\ns It was <MASK>.\ns All in all <MASK>.\ns A <MASK> piece.\nwith \"great\" as a verbalizer for positive, \"terrible\"\nfor negative. In case of SST-5 with five labels,\nwe expand it to \"great\", \"good\", \"okay\", \"bad\",\nand \"terrible\".\n7We have also tried to tune the baselines with the learning\nrate of10−4 but it performed worst.\n8We also tried tuning prompts with learning rates of\n{10−4,10−5} but it performed worst, as also observed in prior\nwork (Mahabadi et al., 2021a; Min et al., 2021).\n3649\n• For SUBJ, given a sentences:\ns This is <MASK>.\ns It’s all <MASK>.\ns It’s <MASK>.\ns Is it <MASK>?\nwith \"subjective\" and \"objective\" as verbalizers.\n• For TREC, given a question q, the task is to\nclassify the type of it:\nq <MASK>:\nq Q:<MASK>:\nq why<MASK>?\nq Answer: <MASK>.\nwith \"Description\", \"Entity\", \"Expression\",\n\"Human\", \"Location\", \"Number\" as verbalizers\nfor question types of \"Description\", \"Entity\",\n\"Abbreviation\", \"Human\", \"Location\", and\n\"Numeric\".\n• For entailment task (RTE) given a premise p\nand hypothesish:\n\"h\" ? | <MASK>, \"p\"\nh? | <MASK>,p\n\"h\" ? | <MASK>.p\nwith \"Y es\" as a verbalizer for entailment, \"No\"\nfor contradiction.\np question:h True or False? answer: <MASK>\nwith \"true\" as a verbalizer for entailment, \"false\"\nfor contradiction.\n• For entailment task (CB) given a premisep and\na hypothesish:\n\"h\" ? | <MASK>, \"p\"\nh? | <MASK>,p\n\"h\" ? | <MASK>.p\nwith \"Y es\" as a verbalizer for entailment, \"No\"\nfor contradiction, \"Maybe\" for neutral.\np question: h true, false or neither? answer:\n<MASK>\nwith \"true\" as a verbalizer for entailment, \"false\"\nfor contradiction, \"neither\" for neutral.\n• For QNLI, given a sentences and questionq:\ns. Question:q? Answer: <MASK>.\nwith \"Y es\" or \"true\" as verbalizers for entailment\nand \"No\" or \"false\" for not entailment.\ns. Based on the previous sentence,q? <MASK>.\nwith \"Y es\" or \"true\" as verbalizers for entailment\nand \"No\" or \"false\" for not entailment.\nBased on the following sentence,q?<MASK>.s\nwith \"Y es\" and \"No\" as verbalizers for\nentailment and not entailment respectively.\n• For QQP, given two questionsq1 and q2:\nDo q1 and q2 have the same meaning?<MASK>.\nwith \"Y es\" or \"true\" as verbalizers for duplicate\nand \"No\" or \"false\" for not duplicate.\nq1. Based on the previous question, q2?\n<MASK>.\nwith \"Y es\" or \"true\" as verbalizers for duplicate\nand \"No\" or \"false\" for not duplicate.\nBased on the following question,q1?<MASK>.q2\nwith \"Y es\" and \"No\" as verbalizers for duplicate\nand not duplicate respectively.\n3650\n• For MRPC, given two sentencess1 and s2:\nDo s1 and s2 have the same meaning?<MASK>.\nwith \"Y es\" or \"true\" as verbalizers for equivalent\nand \"No\" or \"false\" for not equivalent.\ns1. Based on the previous sentence, s2?\n<MASK>.\nwith \"Y es\" or \"true\" as verbalizers for equivalent\nand \"No\" or \"false\" for not equivalent.\nBased on the following sentence,\ns1?<MASK>.s2\nwith \"Y es\" and \"No\" as verbalizers for equivalent\nand not equivalent respectively.\n• For WiC, given two sentencess1 and s2 and a\nword w, the task is to classify whetherw is used\nin the same sense.\n\"s1\" / \"s2\". Similar sense of \"w\"? <MASK>.\ns1 s2 Does w have the same meaning in both\nsentences? <MASK>\nWith \"No\" and \"Y es\" as verbalizers for False,\nand True.\nw . Sense (1) (a) \"s1\" (<MASK>) \"s2\"\nWith \"2\" and \"b\" as verbalizers for False, and\nTrue.\nC Impact of the Position\nof Masks in Sentence-pair Datasets\nWe evaluate the impact of the position of mask tokens\nin sentence-pair benchmarks. Given two sentencess1\nand s2, we consider the following four locations for\ninserting mask tokens, where in the case of encoding\nas two sentences, input parts to the encoder are\nseparated with|:\n1. s1 s2 <MASK>\n2. s1 <MASK>s2\n3. s1 | <MASK>s2\n4. s1 | s2<MASK>\nDatasets 1 2 3 4\nCB 89.8 91.6 88.9 86.5\nRTE 69.1 69.1 64.5 65.3\nQNLI 72.0 83.3 77.7 73.1\nMRPC 71.6 69.5 66.4 72.0\nQQP 79.2 82.8 72.5 70.2\nWiC 60.3 59.5 60.2 59.5\nAvg 73.7 76.0 71.7 71.1\nTable 7: V alidation performance for sentence-pair\nbenchmarks for different locations of mask tokens. Bold\nfonts indicate the best results in each row.\nDatasets 10−2 10−3 10−4 10−5\nCB 90.0 /82.5 92.2/85.0 91.6/87.5 91.6/87.5\nMRPC 69.8 /56.2 70.8/56.2 69.5/56.2 70.8/56.2\nQNLI 83.3 /71.9 82.7/71.9 83.3/71.9 83.1/68.8\nQQP 82.8 /78.1 82.7/75.0 82.8/75.0 83.0/75.0\nRTE 69.8 /62.5 69.2/59.4 69.1/62.5 68.3/62.5\nWiC 62.2 /50.0 59.7/46.9 59.5/53.1 58.9/50.0\nAvg 76.3 /66.9 76.2/65.7 76.0/67.7 76.0/66.7\nTotal Avg 71.6 71.0 71.8 71.3\nTable 8: V alidation performance for different values of\nσ. We show mean performance/worst-case performance\nacross 20 runs. The last row shows the average of mean\nperformance/worst-case performance.\nTable 7 shows how the position of masks impact\nthe results. As demonstrated, pattern 2, inserting\nmask tokens between the two sentences and encoding\nboth as a single sentence obtains the highest\nvalidation performance. We use this choice in all the\nexperiments when removing handcrafted patterns.\nD Impact of Initialization\nWe initialize the label embedding matrix with random\ninitialization from a normal distributionN(0,σ). In\ntable 8, we show the development results for different\nvalues ofσ. We choose theσ obtaining the highest\nperformance on average over average and worst case\nperformance, i.e.,σ=10−4.\nE Ablation Results\nTo study the impact of different design choices in\nPERFECT , we considered the following experiments:\n• -Hinge Loss:In this variant, we replace the\nhinge loss with multi-class cross entropy loss.\n3651\nDataset PERFECT -Hinge Loss +Label Emb -Prototypical\nSST-2 90.7/88.2/1.2 90.0/85.9/1.7 90.6/87.6/1.1 90.4/85.2/1.6\nCR 90.0/85.5/1.4 90.1/88.6/0.9 89.7/86.6/1.4 89.9/86.8/1.4\nMR 86.3/81.4/1.6 85.2/78.6/2.4 85.8/82.4/1.4 85.7/78.0/2.0\nSST-5 42.7/35.1/2.9 43.3/36.8/3.1 41.8/37.1/2.5 41.2/35.9/2.4\nSUBJ 89.1/82.8/2.1 89.4/83.1/2.2 90.0/86.0/1.8 89.7/86.0/1.8\nTREC 90.6/81.6/3.2 89.9/76.8/4.2 89.7/71.6/6.1 89.6/76.2/4.9\nCB 90.3/83.9/3.5 89.2/80.4/4.8 89.6/82.1/3.6 89.3/80.4/3.9\nRTE 60.4/53.1/4.7 60.7/54.5/4.0 58.6/50.9/4.0 58.5/50.9/4.5\nQNLI 74.1/60.3/4.6 72.9/64.4/3.9 74.9/66.7/3.6 74.7/67.5/3.5\nMRPC 67.8/54.7/5.7 67.0/49.8/5.5 68.1/56.9/4.8 68.1/56.9/4.8\nQQP 71.2/64.2/3.5 69.9/63.0/4.1 70.3/62.2/4.0 70.2/62.2/4.0\nWiC 53.8/47.0/3.0 53.7/46.7/3.3 53.6/50.2/2.4 53.6/50.0/2.6\nAvg 75.6/68.1/3.1 75.1/67.4/3.3 75.2/68.4/3.1 75.1/68.0/3.1\nTable 9: Ablation results on the impact of different design choices inPERFECT . We report the average performance/worst-\ncase performance/and the standard deviation.\n• +Label Emb:We use the trained label em-\nbeddings during the inference, substituting the\ncomputed prototypes in (5).\n• -Prototypical:Instead of using prototypical\nnetworks, during inference, we use the same\nobjective as training, i.e., (4).\nResults are shown in Table 9. Experimental results\ndemonstrate thatPERFECT obtains the best results\non average. Using multi-class cross-entropy instead\nof hinge loss, obtains substantially lower minimum\nperformance (67.4 versus 68.1), demonstrating that\ntraining with hinge loss makes the model more\nstable. Using the trained label embeddings (+Label\nEmb) obtains very close results toPERFECT (slightly\nworse on average and slightly better on the minimum\nperformance). Using the similar objective as training\nwith replacing prototypical networks (-Prototypical),\nobtains lower performance on average (75.1 versus\n75.6). These results confirm the design choices for\nPERFECT .\n3652",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5391054749488831
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5000278949737549
    },
    {
      "name": "Computational linguistics",
      "score": 0.4763638973236084
    },
    {
      "name": "Linguistics",
      "score": 0.46692535281181335
    },
    {
      "name": "Natural language processing",
      "score": 0.4453085660934448
    },
    {
      "name": "Association (psychology)",
      "score": 0.4445374608039856
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4406852722167969
    },
    {
      "name": "Cognitive science",
      "score": 0.3780423700809479
    },
    {
      "name": "Philosophy",
      "score": 0.3061760663986206
    },
    {
      "name": "Psychology",
      "score": 0.22974038124084473
    },
    {
      "name": "Epistemology",
      "score": 0.22630652785301208
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}