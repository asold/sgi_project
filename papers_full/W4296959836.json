{
  "title": "SuperFormer: Volumetric Transformer Architectures for MRI Super-Resolution",
  "url": "https://openalex.org/W4296959836",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4382766031",
      "name": "Forigua, Cristhian",
      "affiliations": [
        "Universidad de Los Andes"
      ]
    },
    {
      "id": "https://openalex.org/A2108199541",
      "name": "Escobar, María",
      "affiliations": [
        "Universidad de Los Andes"
      ]
    },
    {
      "id": "https://openalex.org/A2746341112",
      "name": "Arbelaez, Pablo",
      "affiliations": [
        "Universidad de Los Andes"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2964297772",
    "https://openalex.org/W2782945719",
    "https://openalex.org/W6601849091",
    "https://openalex.org/W2912226037",
    "https://openalex.org/W3002349559",
    "https://openalex.org/W3202624106",
    "https://openalex.org/W2997222478",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W3204700807",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2906473494",
    "https://openalex.org/W3096319150",
    "https://openalex.org/W2186925649",
    "https://openalex.org/W2807184855",
    "https://openalex.org/W4312428231",
    "https://openalex.org/W2024729467",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3020887200",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W3204255739",
    "https://openalex.org/W3204650100",
    "https://openalex.org/W3098848838"
  ],
  "abstract": null,
  "full_text": "SuperFormer: Volumetric Transformer\nArchitectures for MRI Super-Resolution\nCristhian Forigua1[0000−0003−4472−1144], Maria Escobar1[0000−0002−5880−762X],\nand Pablo Arbelaez1[0000−0001−5244−2407]\nCenter for Research and Formation in Artificial Intelligence, Universidad de los\nAndes, Bogotá, Colombia\n{cd.forigua, mc.escobar11, pa.arbelaez}@uniandes.edu.co\nAbstract. This paper presents a novel framework for processing volu-\nmetric medical information using Visual Transformers (ViTs). First, We\nextend the state-of-the-art Swin Transformer model to the 3D medical\ndomain. Second, we propose a new approach for processing volumetric\ninformation and encoding position in ViTs for 3D applications. We in-\nstantiate the proposed framework and present SuperFormer, a volumet-\nric transformer-based approach for Magnetic Resonance Imaging (MRI)\nSuper-Resolution. Our method leverages the 3D information of the MRI\ndomain and uses a local self-attention mechanism with a 3D relative\npositional encoding to recover anatomical details. In addition, our ap-\nproach takes advantage of multi-domain information from volume and\nfeature domains and fuses them to reconstruct the High-Resolution MRI.\nWe perform an extensive validation on the Human Connectome Project\ndataset and demonstrate the superiority of volumetric transformers over\n3D CNN-based methods. Our code and pretrained models are available\nat https://github.com/BCV-Uniandes/SuperFormer\nKeywords: MRI Reconstruction· Super-Resolution · Visual transform-\ners.\n1 Introduction\nHigh Resolution (HR) Magnetic Resonance Imaging (MRI) contains detailed\nanatomical structures that are crucial for accurate analysis and diagnosis of\nseveral diseases. This modality is commonly used in specialized medical centers.\nStill, its broader adoption is hindered by costly equipment and long scan times,\nresulting in small spatial coverage and low Signal-to-Noise Ratio (SNR) [3].\nIn contrast, Low Resolution (LR) imaging requires lower acquisition time, less\nstorage space, and less sophisticated scanners [17]. Nevertheless, LR medical\nimages suffer from artifacts of patient’s motion and lack detailed anatomical\nstructures. Therefore, volumetric image Super-Resolution (SR) is a promising\nframework for generating HR MRIs by mapping them from a LR input while\nkeeping the advantages of the LR acquisition.\nThe human Connectome Project (HCP) dataset [20] has been widely used to\nstudy the medical imaging SR framework since it provides HR volumetric MRI\narXiv:2406.03359v1  [eess.IV]  5 Jun 2024\n2 C. Forigua et al.\nFig. 1.Overviewofourmethod.SuperFormerencodesfeaturesandvolumeembeddings\nfor deep feature extraction through volumetric transformers and combines the multi-\ndomain representations to reconstruct the super-resolved volume.\nfrom 1,113 subjects [2,3,22]. There are two different approaches that tackle the\nproblem of medical imaging SR. On one hand, 2D oriented methods interpret\nthe 3D information as a set of slices without any volumetric interaction [16,18].\nOn the other hand, 3D-based approaches exploit the inherent connections of\nthe volumetric information [6,7,9,15]. For instance, the seminal works mDCSRN\n[2] and MRDG48 [22] propose 3D Generative Adversarial Networks (GANs) to\nsuper-resolve MRI volumes with high perceptual quality on the HCP dataset.\nHowever, there is a lack of a standardized evaluation framework for 3D medical\nimage SR, and the proposed methods are not publicly available, limiting the\nprogress in this area.\nRecently, the computer vision community has experienced the revolution of\nVision Transformers (ViT)s [5] for several computer vision tasks [1,4,14]. Unlike\nCNNs, which have restricted receptive fields, ViTs encode visual representations\nfrom a sequence of 2D patches and leverage self-attention mechanisms for captur-\ning long-range global information [10]. Since ViTs learn stronger feature repre-\nsentations, they are commonly used as feature encoders for further downstream\ntasks. Liu et al. proposed Shifted windows (Swin) Transformer, a 2D trans-\nformer backbone that uses non-overlapping windows to compute self-attention\nat multi-scale modeling. Since its introduction, Swin Transformers have achieved\nstate-of-the-artresultsinseveralcomputervisiontasks[14].Althoughtransform-\ners are being adopted in the medical imaging domain for problems such as 3D\nmedical segmentation [10,11,19,25,26], their potential remains unexplored on the\n3D medical SR framework. Fenget al.[8] proposed Task Transformer Network,\na transformer-based method for MRI reconstruction and SR. However, this 2D-\noriented approach does not fully leverage the continuous information in the 3D\nSuperFormer 3\ndomain since they process volumes on a slice by slice basis, ignoring the inherent\nvolumetric information of MRIs.\nInthispaper,wepresentSuperFormer,avolumetricapproachforMRISuper-\nResolution based on 3D visual transformers. Inspired by the success of ViTs for\nimage restoration tasks [12], we create a volumetric adaptation for medical imag-\ning. This approach is the first to use ViTs in the 3D domain for medical imaging\nSuper-Resolution to the best of our knowledge. Figure 1 shows an overview of\nour method. We leverage the 3D and the multi-domain information from the vol-\nume and feature embeddings. Moreover, our approach uses a local self-attention\nmechanism with a 3D relative position encoding to generate SR volumes by vol-\numetric processing. We perform an extensive validation and demonstrate the su-\nperiority of volumetric transformers over 3D CNN-based methods. Additionally,\nour results indicate that using multi-domain embeddings improves the perfor-\nmance of volumetric transformers compared to single-domain approaches. Our\nmain contributions can be summarized as follows:\n1. We propose a 3D generalization of the Swin Transformer framework to serve\nas a general-purpose backbone for medical tasks on volumetric data.\n2. We introduce a new approach for processing volumetric information and\nencoding position in ViTs for 3D frameworks, increasing the transformer’s\nreceptive field and ensuring a volumetric understanding.\n3. We present SuperFormer, a novel volumetric visual transformer for MRI\nSuper-Resolution. Our method leverages the 3D and multi-domain informa-\ntion from volume and feature embeddings to reconstruct HR MRIs using a\nlocal self-attention mechanism.\nFurthermore, we provide a medical SR toolbox to promote further research in 3D\nMRI Super-Resolution. Our toolbox includes the extension of one transformer-\nbased and two CNN methods into the 3D framework. Our toolbox, source code,\ndataset division, and pre-trained models are publicly available.\n2 Method\nWe propose SuperFormer, a transformer-based network that generates HR MRI\nvolumes by mapping them from a LR volume to the high-resolution space. Su-\nperFormer leverages the 3D information of medical images and multi-domain\ninformation by using volumetric transformers that analyze the image and fea-\nture domains of the input volume. Then, SuperFormer processes both domains\ntogether to reconstruct the HR output. Fig. 1 shows an overview of the proposed\nmethod.\nVolumetric Transformer ProcessingTransformer architectures for natural\nimages operate on a sequence of 1D input embeddings extracted from the 2D\ndomain. In the 3D domain, we need to compute these embeddings from volu-\nmetric information. Given an input volumeX ∈ RH×W×D×C, we first extract\n4 C. Forigua et al.\n3D tokens as non-overlapping patches with a resolution of(H′, W′, D′) from X.\nEach token has a dimension ofH′ × W′ × D′ × C. Subsequently, we project the\n3D tokens into aCemb-dimensional space, which remains constant throughout\nthe transformer layers via an embedding layer. Then, we flatten the embedded\nrepresentations to obtain the one-dimensional embeddings that the transformers\nprocess. Under this configuration, the transformers can operate on 1D embed-\ndings that encode the 3D information and provide a volumetric perception of the\ninput volume, have a larger receptive field depending on the patch resolution,\nand have fewer computational costs and hardware constraints.\n2.1 Feature Embedding\nTo compute the feature representation of the input volume, SuperFormer first\nextracts 3D shallow features directly from the input. Then, the feature patch\nembedding layer extracts 3D tokens from these features and projects them for\nfurther volumetric transformer processing. Since the shallow features encode\nlow-frequency information of the input volume, the Feature Embedding ensures\nthe transformers consider the low frequencies for recovering the lost anatomical\ndetails.\n3D Shallow Feature ExtractionGiven a LR volumeVLQ ∈ RH×W×D×Cin ,\nwhere H, W, D and Cin are the image height, width, depth and input channel\nnumber, respectively, we use a3×3×3 convolutional layer to extract the shallow\nfeatures F0 ∈ RH×W×D×Cemb , where Cemb is the embedding dimension. By\nputting this layer at an early stage of processing, we achieve better results and\nmore stable optimization [24], while mapping the input channel dimension to a\nhigher dimensional space for volumetric processing.\n2.2 Volume Embedding\nThe intensity information in the 3D domain contains relevant information about\npatternsandanatomicalstructuresthatarevolumetricallyorganizedintheMRI.\nThus, encoding the volume domain provides a clear idea of the structures we\nwant to recover. SuperFormer encodes volume representations by processing the\ninput directly in the volume domain. We use a volumetric patch embedding layer\nthat computes the 3D tokens directly from the input and projects them into the\nCemb-dimensional space.\n2.3 3D Deep Feature Extraction\nSuperFormer processes the feature and volume embedding representations by\nextracting 3D deep features for each domain. To compute these deep represen-\ntations, SuperFormer employsK 3D Residual Swin Transformer blocks (RSTB)\nand a final3 × 3 × 3 convolutional layer. Each RSTB is a residual block that\nconsists ofL 3D Swin transformer layers (STL), a3 × 3 × 3 convolution layer,\nand a residual connection.\nSuperFormer 5\nFig. 2.3D Shifted window for computing self-attention in the Deep Feature Extraction\nfor 2×2×2 3D token and 8×8×8 window size.\n3D Swin Transformer layer.In contrast to the standard multi-head self-\nattentionoftheoriginalvisualtransformer[21],wemodelvolumetricinteractions\nbetween tokens as three-dimensional local self-attention with a 3D shifted win-\ndow mechanism [14]. Then, we partition the input volume into non-overlapping\nwindows to compute the local self-attention, as shown in Fig. 2. Precisely, at\nSTL n, we compute windows of sizeM ×M ×M to split the input 3D token into\n[H′\nM ] × [W′\nM ] × [D′\nM ] regions. Then, at STLn + 1, we shift the previous windows\nby M\n2 voxels along each dimension. Inspired by [14,19], we adopt a 3D cyclic-\nshifting for the continuous computation of the shifted partitions. Moreover, for\na feature in a local window we compute thequery (Q), key (K) and value (V)\nmatrices as\nQ = XPQ, K = XPK, V = XPV (1)\nwhere X is the local window feature,PQ, PK and PV are the shared projection\nmatrices between all windows. We then compute the self-attention as\nAttention(Q, K, V) =Softmax ( QKT\n√\nd + B )V. (2)\nwhere B is a relative position encoding for learning the locations of the 3D\ntokens regarding the input volume. Unlike most ViTs-based models for 3D image\nprocessing [10,19], which use absolute position embeddings [21], we adapt the\n2D absolute position bias to the 3D framework. This positional encoding has\nbeen proved to lead to better results in the natural image domain [12,14]. The\nwhole STL configuration, as shown in Fig. 1, consists of the standard or shifted\nlocal attention mechanism, normalization layers, and a Multi-Layer Perceptron\n(MLP).\n2.4 HQ Volume Reconstruction.\nSuperFormer learns a deep representation from the shallow features and volume\ndomains to reconstruct the SR volume. First, we combine the information of\n6 C. Forigua et al.\nthe two domains by averaging the deep features and aggregating the shallow\nfeatures with a residual connection. Since deep features differ in size because of\nthe volumetric processing, we employ an up-sampling layer to match the dimen-\nsions. Intuitively, the shallow features convey low-frequency information, while\nthe deep features concentrate on recovering the high frequencies. The residual\nconnection transfers the low frequencies directly into the reconstruction stage to\nrecover high frequencies. Specifically, we reconstruct the HR volume by a3×3×3\nand 1 × 1 × 1 convolutional layers with a LeakyReLU activation function.\n3 Experimental setup\nDataset We use the Human Connectome Project (HCP) dataset [20] to train\nand test our method, a large publicly accessible brain structural MRI dataset\ncontaining high spatial resolution 3D T1W images from 1,113 healthy subjects.\nAll the images were acquired via Siemens 3T platform using a 32-channel head\ncoil on multiple centers and come in high spatial resolution as 0.7 mm isotropic\nwith a matrix size of 320x320x256. Ground truth annotations come from these\nhigh-quality images, which provide detailed anatomical structures. We used the\nsame data distribution as in [2,22]; 780 for training, 111 for validation, 111 for\nevaluation, and 111 for testing. We calculate three evaluation metrics for quan-\ntitatively measuring the similarity between HR and SR volumes: Peak Signal\nto Noise Ratio (PSNR), Normalized Root Mean Squared Error (NRMSE), and\nsubject-wise average Structural Similarity Index (SSIM).\nLow-Resolution Volumes GenerationSince we need a paired dataset of LR\nand HR images to train and test SR models, we generate the LR images from the\nHR ones by following the approach used in [3]. First, we convert the HR volumes\ninto k-space by applying the Fast Fourier Transform (FFT). Then, we degrade\nthe resolution by truncating the outer part of the 3D k-space by a factor of 2x2.\nFinally, we convert back to the image domain by applying the inverse FFT and\nlinear interpolation to the original HR volume size. This approach mimics the\nactual acquisition process of LR and HR MRIs since it does not perform any\ndimensional change but undersamples the frequency k-space [2].\n3.1 Implementation Details\nWe implement SuperFormer in PyTorch and train the model on a workstation\nwith 4 Nvidia RTX 8000 GPUs during 55k iterations during four days. We use an\nADAM optimizer with a learning rate of2e−4 to minimize the L1 loss function\nwith a batch size of 4. For the ablation studies, we use 252 as embedding di-\nmension, six RSTBs, each with six STLs and six attention heads to compute the\nlocal self-attention. In SuperFormer’s configuration, we use three RSTBs instead\nof six because of hardware constraints. In addition, we use a patch resolution of\n2 × 2 × 2 for volumetric processing.\nSuperFormer 7\nTable 1.Comparison of our method against the state-of-the-art methods on the test\nsplit. SuperFormer outperforms 3D CNN and transformer-based approaches.\n#Params PSNR ↑ SSIM ↑ NRMSE ↓\n2D SwinIR [12] 22.5M 31,4285±3,5496 0,8297±0,0303 0,2002±0,0761\n3D EDSR 41.2M 31,0217±3,1195 0,9147±0,0314 0,2050±0,0636\n3D RRDBNet 115M 31,3059±3,3876 0,9355±0,0207 0,2006±0,0692\nSuperFormer 20M 32,4742±2,9847 0,9059±0,0271 0,1747±0,0635\n3.2 Results\nComparison with the State-of-the-artWe evaluate SuperFormer against\ntwo CNN-based methods: EDSR [13], and RRDBNet [23]. Since these methods\nare originally designed for 2D SR, we extend them for the 3D domain. In addi-\ntion, we compare our method against the 2D transformer-based SwinIR [12] to\ndemonstrate the advantages of leveraging volumetric information. We can not\ndirectly compare our approach against mDCSRN [2] nor MRDG48 [22] because\nthere is no publicly available code for these methods and their experimental\nframework is different.\nTable 1 shows the results in our test split of the HCP dataset. First, by\ncomparing SuperFormer and SwinIR against 3D EDSR and 3D RRDBNet, we\nfind that using a transformer-based approach is greatly beneficial for the task of\nSR. In fact, SuperFormer outperforms both 3D CNN methods in the PSNR and\nNRMSE metrics. Moreover, using a 2D transformer like SwinIR still produces\nhigher results than the 3D CNN counterparts. These findings validate the supe-\nriority of transformers for deep feature extraction in SR. Second, the comparison\nbetween SuperFormer and SwinIR empirically demonstrates that leveraging 3D\ninformation through volumetric transformers improves the performance of MRI\nSR. This finding is consistent with our intuition since the volumetric transformer\ninterprets a three-dimensional vision of the anatomical structures and acquires a\nbroader context for generating a more coherent SR output. Furthermore, Super-\nformer performs significantly better (p-value<0.05) and has fewer parameters.\nFig 3 shows qualitative results that support the findings of Table 1. In par-\nticular, the improvement in perceptual quality is noticeable when comparing the\n2D SwinIR baseline against our implementation of volumetric transformers. The\nimages super-resolved with volumetric transformers have sharper structures in\ncomparison with their 2D SwinIR counterpart.\nAblation Study Our ablation studies include a thorough analysis of Super-\nFormer’s embedding configuration. We compare our method against three dif-\nferent configurations:SR-Features when we compute the deep features only from\nthe shallow features embedding,SR-Volume when we extract them just from the\nvolume embedding, andSR-Avg when we take the average between the volume\nand feature embeddings to compute the deep representations.\nTable 2 shows the results of our final model and the ablation experiments\nfor the validation set of the HCP dataset. These results empirically validate\n8 C. Forigua et al.\nFig. 3.Qualitative comparison of our method against CNN and 2D transformer-based\nmethods on the axial, coronal and sagittal anatomical axes.\nthe relevance of using multi-domain embeddings in our final method. If we use\nthe embedding representation of only one domain, regardless of which one, the\nPSNR decreases by 1 decibel. Additionally, we explore how our approach merges\nthe two embedding representations. The results demonstrate that averaging the\nextracted deep features brings an improvement across all metrics compared to\naveraging the embedding representations. These ablation experiments show that\nSuperFormer’s embedding configuration is beneficial for 3D MRI SR.\nTable 2.Ablation experiments for SuperFormer’s branch configuration on the HCP\nvalidation set. We report the results of our final method and ablation experiments.\nPSNR ↑ SSIM ↑ NRMSE ↓\nSR-Volume 31,5354±2,145 0,8644±0,015 0,1829±0,035\nSR-Features 31,5195±3,5209 0,9085±0,344 0,1906±0,0659\nSR-Avg 32,0557±3,1081 0,8950±0,0294 0,1777±0,0570\nSuperFormer 32,5164±3.028 0,9059±0.027 0,1668±0.046\n4 Conclusion\nIn this work, we present SuperFormer, a method for SR that leverages the 3D\ninformation of MRIs by using volumetric transformers. To the best of our knowl-\nedge, SuperFormer is the first method for this task that implements 3D visual\ntransformers and uses a volumetric local self-attention mechanism. We exper-\nimentally demonstrate that leveraging the 3D information through volumetric\ntransformers and multi-domain embeddings leads to better results compared to\nstate-of-the-art approaches. Furthermore, our generalizable framework will push\nthe envelope further in the development of volumetric Visual transformers for\nmedical imaging.\nSuperFormer 9\nReferences\n1. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European conference on computer\nvision (ECCV). pp. 213–229. Springer (2020)\n2. Chen, Y., Shi, F., Christodoulou, A.G., Xie, Y., Zhou, Z., Li, D.: Efficient and\naccurate mri super-resolution using a generative adversarial network and 3d multi-\nlevel densely connected network. In: International Conference on Medical Image\nComputing and Computer-Assisted Intervention. pp. 91–99. Springer (2018)\n3. Chen, Y., Xie, Y., Zhou, Z., Shi, F., Christodoulou, A.G., Li, D.: Brain\nmri super resolution using 3d deep densely connected neural networks. 2018\nIEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) (Apr\n2018). https://doi.org/10.1109/isbi.2018.8363679, http://dx.doi.org/10.\n1109/ISBI.2018.8363679\n4. Cheng, B., Choudhuri, A., Misra, I., Kirillov, A., Girdhar, R., Schwing, A.G.:\nMask2former for video instance segmentation. arXiv preprint arXiv:2112.10764\n(2021)\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n6. Du, J., Wang, L., Gholipour, A., He, Z., Jia, Y.: Accelerated super-resolution mr\nimage reconstruction via a 3d densely connected deep convolutional neural net-\nwork. In: 2018 IEEE International Conference on Bioinformatics and Biomedicine\n(BIBM). pp. 349–355 (2018).https://doi.org/10.1109/BIBM.2018.8621073\n7. Du, J., Wang, L., Liu, Y., Zhou, Z., He, Z., Jia, Y.: Brain mri super-resolution using\n3d dilated convolutional encoder–decoder network. IEEE Access8, 18938–18950\n(2020). https://doi.org/10.1109/ACCESS.2020.2968395\n8. Feng, C.M., Yan, Y., Fu, H., Chen, L., Xu, Y.: Task transformer network for joint\nmri reconstruction and super-resolution. In: International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention. pp. 307–317. Springer\n(2021)\n9. Georgescu, M.I., Ionescu, R.T., Verga, N.: Convolutional neural networks with\nintermediate loss for 3d super-resolution of ct and mri scans. IEEE Access 8,\n49112–49124 (2020). https://doi.org/10.1109/access.2020.2980266, http://\ndx.doi.org/10.1109/ACCESS.2020.2980266\n10. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,\nRoth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In:\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision. pp. 574–584 (2022)\n11. Karimi, D., Vasylechko, S.D., Gholipour, A.: Convolution-free medical image seg-\nmentationusingtransformers.In:InternationalConferenceonMedicalImageCom-\nputing and Computer-Assisted Intervention. pp. 78–88. Springer (2021)\n12. Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., Timofte, R.: Swinir: Image\nrestorationusingswintransformer.In:ProceedingsoftheIEEE/CVFInternational\nConference on Computer Vision. pp. 1833–1844 (2021)\n13. Lim,B.,Son,S.,Kim,H.,Nah,S.,MuLee,K.:Enhanceddeepresidualnetworksfor\nsingle image super-resolution. In: Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition workshops. pp. 136–144 (2017)\n10 C. Forigua et al.\n14. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer:Hierarchicalvisiontransformerusingshiftedwindows.In:Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 10012–10022\n(2021)\n15. Pham, C.H., Tor-Díez, C., Meunier, H., Bednarek, N., Fablet, R., Passat, N.,\nRousseau, F.: Multiscale brain mri super-resolution using deep 3d convolutional\nnetworks. Computerized Medical Imaging and Graphics 77, 101647 (2019).\nhttps://doi.org/https://doi.org/10.1016/j.compmedimag.2019.101647,\nhttps://www.sciencedirect.com/science/article/pii/S0895611118304105\n16. Qiu, D., Zheng, L., Zhu, J., Huang, D.: Multiple improved residual net-\nworks for medical image super-resolution. Future Generation Computer Sys-\ntems 116, 200–208 (2021). https://doi.org/https://doi.org/10.1016/j.\nfuture.2020.11.001, https://www.sciencedirect.com/science/article/pii/\nS0167739X20330259\n17. Sarracanie, M., LaPierre, C.D., Salameh, N., Waddington, D.E., Witzel, T., Rosen,\nM.S.: Low-cost high-performance mri. Scientific Reports 5(1) (2015). https://\ndoi.org/10.1038/srep15177\n18. Shi, J., Li, Z., Ying, S., Wang, C., Liu, Q., Zhang, Q., Yan, P.: Mr image super-\nresolution via wide residual networks with fixed skip connection. IEEE Journal of\nBiomedical and Health Informatics23(3), 1129–1140 (2019). https://doi.org/\n10.1109/JBHI.2018.2843819\n19. Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V.,\nHatamizadeh, A.: Self-supervised pre-training of swin transformers for 3d medi-\ncal image analysis. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 20730–20740 (2022)\n20. Van Essen, D.C., Smith, S.M., Barch, D.M., Behrens, T.E., Yacoub, E., Ugurbil,\nK.: The wu-minn human connectome project: An overview. NeuroImage80, 62–79\n(2013). https://doi.org/10.1016/j.neuroimage.2013.05.041\n21. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\nŁ., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems30 (2017)\n22. Wang, J., Chen, Y., Wu, Y., Shi, J., Gee, J.: Enhanced generative adversarial net-\nwork for 3d brain mri super-resolution. In: Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision. pp. 3627–3636 (2020)\n23. Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., Change Loy, C.:\nEsrgan: Enhanced super-resolution generative adversarial networks. In: Proceed-\nings of the European conference on computer vision (ECCV) workshops. pp. 0–0\n(2018)\n24. Xiao, T., Singh, M., Mintun, E., Darrell, T., Dollár, P., Girshick, R.: Early convo-\nlutions help transformers see better. Advances in Neural Information Processing\nSystems 34, 30392–30400 (2021)\n25. Xie, Y., Zhang, J., Shen, C., Xia, Y.: Cotr: Efficiently bridging cnn and transformer\nfor 3d medical image segmentation. In: International Conference on Medical Image\nComputing and Computer-assisted Intervention. pp. 171–180. Springer (2021)\n26. Zhang, Y., Higashita, R., Fu, H., Xu, Y., Zhang, Y., Liu, H., Zhang, J., Liu, J.: A\nmulti-branch hybrid transformer network for corneal endothelial cell segmentation.\nIn: International Conference on Medical Image Computing and Computer-Assisted\nIntervention. pp. 99–108. Springer (2021)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8484840393066406
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5584918260574341
    },
    {
      "name": "Transformer",
      "score": 0.5389135479927063
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5305358171463013
    },
    {
      "name": "Computer vision",
      "score": 0.46004369854927063
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3266216516494751
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162096671",
      "name": "Universidad de Los Andes",
      "country": "CO"
    }
  ]
}