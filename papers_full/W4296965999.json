{
    "title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation",
    "url": "https://openalex.org/W4296965999",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2127071220",
            "name": "Xingdi Yuan",
            "affiliations": [
                "National Chung Hsing University"
            ]
        },
        {
            "id": "https://openalex.org/A2089790514",
            "name": "Tong Wang",
            "affiliations": [
                "National Chung Hsing University"
            ]
        },
        {
            "id": "https://openalex.org/A2888553653",
            "name": "Yen-Hsiang Wang",
            "affiliations": [
                "National Chung Hsing University"
            ]
        },
        {
            "id": "https://openalex.org/A2604827731",
            "name": "Emery Fine",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2749063907",
            "name": "Rania Abdelghani",
            "affiliations": [
                "National Chung Hsing University"
            ]
        },
        {
            "id": "https://openalex.org/A1938310666",
            "name": "Hélène Sauzéon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2566444938",
            "name": "Pierre-Yves Oudeyer",
            "affiliations": [
                "National Chung Hsing University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962977247",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W2610891036",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4286252734",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W3007759824",
        "https://openalex.org/W2965628639",
        "https://openalex.org/W2970796366",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2944995326",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W3034937228",
        "https://openalex.org/W4385567201",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962717047",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4385574286",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W3199501816",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3035359363",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3199958362",
        "https://openalex.org/W4295287374",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2989613245",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3198599617",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W2949849869",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3105144279",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W3101798106",
        "https://openalex.org/W2970200208"
    ],
    "abstract": "Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches to selecting high-quality questions from a set of LLM-generated candidates. Our method works under the constraints of 1) a black-box (non-modifiable) question generation model and 2) lack of access to human-annotated references -- both of which are realistic limitations for real-world deployment of LLMs. With automatic as well as human evaluations, we empirically demonstrate that our approach can effectively select questions of higher qualities than greedy generation.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12952–12965\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSelecting Better Samples from Pre-trained LLMs:\nA Case Study on Question Generation\nXingdi Yuan♣∗ Tong Wang♣∗ Yen-Hsiang Wang♢∗\nEmery Fine♣ Rania Abdelghani♠ Hélène Sauzéon♠ Pierre-Yves Oudeyer♠\n♣Microsoft Research, Montréal ♢ National Chung Hsing University ♠INRIA\neric.yuan@microsoft.com\nwangtong106@gmail.com\nheliart@smail.nchu.edu.tw\nAbstract\nLarge Language Models (LLMs) have in recent\nyears demonstrated impressive prowess in natu-\nral language generation. A common practice to\nimprove generation diversity is to sample mul-\ntiple outputs from the model. However, partly\ndue to the inaccessibility of LLMs, there lacks\na simple and robust way of selecting the best\noutput from these stochastic samples. As a case\nstudy framed in the context of question genera-\ntion, we propose two prompt-based approaches,\nnamely round-trip and prompt-based score, to\nselecting high-quality questions from a set of\nLLM-generated candidates. Our method works\nwithout the need to modify the underlying\nmodel, nor does it rely on human-annotated\nreferences — both of which are realistic con-\nstraints for real-world deployment of LLMs.\nWith automatic as well as human evaluations,\nwe empirically demonstrate that our approach\ncan effectively select questions of higher quali-\nties than greedy generation. 1\n1 Introduction & Related Work\nLarge Language Models (LLMs) have recently\ngained tremendous popularity in the NLP commu-\nnity (Devlin et al., 2019; Liu et al., 2019; Bao et al.,\n2020; Brown et al., 2020). The ever-increasing size\nin both models and training data renders many tra-\nditional learning methods impractical/intractable.\nAs a result, prompt-based learning has emerged as\na new paradigm tailored specifically towards lever-\naging the power of LLMs (Radford et al., 2019;\nPetroni et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Schick and Schütze, 2021b; Gao et al.,\n2021; Liu et al., 2021). In the zero-shot setting\n(such as in this study), a data sample is first “ver-\nbalized” into an input prompt and a ground-truth\nresponse in natural language. The prompt is then\nissued to a pre-trained LLM to obtain a predicted\n∗ Equal contribution.\n1We open-source all code and annotated data on github.\nresponse, which is then compared to the ground-\ntruth for evaluation. This new technique has been\nsuccessfully applied to many applications includ-\ning text classification (Yin et al., 2019; Schick and\nSchütze, 2021a), QA (Jiang et al., 2021), natural\nlanguage generation (Li and Liang, 2021) and NLG\nevaluation (Yuan et al., 2021).\nDespite the impressive results on popular NLP\nbenchmarks, however, the back-end LLMs are usu-\nally pre-trained with general-domain data, lead-\ning to sub-optimal performance in new domains\nfor prompt-based learning. There are two major\nchallenges in successful applying general-purpose\nLLMs to specific domains. Firstly, aside from the\nmany known issues of LLMs (Webson and Pavlick,\n2021; Min et al., 2022; Zhao et al., 2021; Lampinen\net al., 2022), their sheer size and/or accessibility\n(e.g., served via API over the internet) makes it\nprohibitively expensive and impractical for domain\nadaptation. These limitations have inspired a re-\ncent line of work known as prompt editing/tuning\n(Gao et al., 2021; Li and Liang, 2021; Madaan\net al., 2022). Additionally, prompt-tuning often\nrelies on the availability of ground-truth labels of\nthe data, imposing much additional resource on the\napproach.\nGiven the ubiquity of these challenges, our study\nfocuses on alleviating the constraints on both anno-\ntation availability and access to model parameters,\nmaking LLMs more accessible for deployment. We\ntake a mainstream NLG task, namely question gen-\neration, as a case study (Du et al., 2017; Yuan et al.,\n2017; Du and Cardie, 2018; Pan et al., 2019; Liu\net al., 2020; Pyatkin et al., 2021). In this task, a\nmodel is trained to generate a natural language\nquestion conditioned on a context and an answer,\nsuch that the generated question can be answered\nby the provided answer using the context as sup-\nporting evidence. Question generation is the cor-\nner stone for many NLP applications in education\n(Kurdi et al., 2020; Abdelghani et al., 2022), FAQ\n12952\ngeneration (Mass et al., 2020), information seek-\ning (Qi et al., 2020), etc. In an educational set-\nting, for example, a question generation system\ncan generate demonstrations that inspire students’\ncuriosity and thinking (teaching), or to help assess\nstudents’ proficiency on certain knowledge or skills\n(examining). These use cases would benefit greatly\nfrom reduced dependency on computing resources,\ndata availability, and the expertise required for fine-\ntuning an LM.\nTo align with these real-world scenarios, our\ngoal is to obtain better outputs from an inference-\nonly LLM (i.e., as a “black-box”, which is rela-\ntively more accessible, e.g., through online APIs).\nIn particular, given the common practice of sam-\npling multiple outputs to improve generation diver-\nsity, we propose a method that aims at selecting\nthe best candidate based on multiple aspects of\nquestion quality in a zero-shot manner — notably\nwithout model adaptation or human annotations.\nOur method can be seen as a post-hoc selection\nprocess within a larger NLG pipeline, and thus\nis orthogonal and applicable to zero-shot and in-\ncontext learning methods (Rubin et al., 2021; Lu\net al., 2022; Liu et al., 2022).\n2 Problem Setting\nNotations: Formally, we consider a dataset of\ncontext-answer pairs (c, a) both as strings. The\ntask of question generation is to generate a ques-\ntion q that can be answered by a using c as sup-\nporting evidence. We use an off-the-shelf pre-\ntrained LLM-based question generator in a zero-\nshot setting (prompt construction detailed in Ap-\npendix A). To simulate the black-box generator\nscenario, we refrain from any form of model tun-\ning. We do, however, assume access to a set of\noutput sequences stochastically sampled from the\nquestion generator. We thus ground our study to\nthis application scenario by sampling k questions\nQ ={qi ∶i =1, . . . , k}. For comparison as a base-\nline, we also denote qg as the question generated\nwith a greedy algorithm (i.e., generating the most\nprobable token at each time step).\nOur goal is to devise an algorithm S which se-\nlects the best candidate qi∗ that maximizes some\nevaluation metric M ∶Q ↦ R, i.e., S(Q) =i∗ =\narg maxi M(qi). We use Ms, Ms, and Ms to de-\nnote the mean, min, and max of {M(q) ∶q ∈Q},\nresp., and Mg for the greedy output M(qg). Se-\nmantically, Ms ≤Ms ≤Ms is tautologically true,\nand a positive result on the design ofS would trans-\nlate to M(qS(Q)) outperforming both Ms and Mg.\nDatasets and model: We adopt two question\ngeneration datasets with distinctive characteristics,\nnamely SQuAD (Rajpurkar et al., 2016) and Fairy-\ntale QA (Xu et al., 2022). SQuAD was originally\nproposed as an extractive QA dataset. It has been\nused as a sentence-level question generation task in\nthe question generation literature (Du and Cardie,\n2018; Yuan et al., 2017; Bao et al., 2020), i.e., a\ncontext c is a single sentence that contains the cor-\nresponding answer a as a sub-string. Fairytale QA\nhas also been used for both question answering\nand question generation. It features paragraph-\nlevel question generation (with c being one or more\nparagraphs), and the answer a is not necessarily a\nsub-string of c. Since we do not perform any form\nof model/prompt tuning, we use the testing split of\nboth datasets, which consist of 11,877 data points\nfor SQuAD and 1,007 for Fairytale QA.\nWe prompt GPT-3 (Brown et al., 2020) 2 in a\n0-shot manner for both question generation and\nselection (detailed in §3). We provide all prompts\nin Appendix A.\nEvaluation Metrics : We use two metrics to eval-\nuate the selected question q′ =M(qS(Q)):\n• Reference-based evaluation: Following prior\nworks, we use BLEU-4 for SQuAD (Du and Cardie,\n2018; Bao et al., 2020) and ROUGE-L for Fairytale\nQA (Xu et al., 2022)3. These metrics compare q′\nagainst the reference question ˆq (a.k.a. the “ground-\ntruth” question in the existing literature).\n•Human evaluation: we solicit human annotations\non a subset of the data. We postulate that an over-\nall score given holistically to rate a question would\nbe highly subjective and thus less inductive to anno-\ntator agreement. Accordingly, we decompose the\nquality of questions into seven dimensions 4, and\nask human annotators to rate a question on each\ndimension followed by an overall rating of the ques-\ntion. We collect three annotations from different\nannotators for each data points. We provide details\nof the human study in Appendix B.\n2We use the text-davinci-002 model.\n3We adopt evaluation scripts provided with the datasets.\n4Namely, grammatical correctness, offensiveness, clarity,\nrelevance, importance, specificity, and answerability.\n12953\n[context]Old Dragonbeardmust have been a master swordsman standing midway between those of the first and of the second order. Molo, however, of whom this story tells, was a sword hero.At that time there lived a young man named Tsui, whose father was a high official and the friend of the prince. And the father once sent his son to visit his princely friend, who was ill. The son was young, handsome and gifted. He went to carry out his father's instructions. When he entered the prince's palace, there stood three beautiful slave girls, who piled rosy peaches into a golden bowl, poured sugar over them and presented them to him. After he had eatenhe took his leave, and his princely host ordered one of the slave girls, Rose-Red by name, to escort him to the gate. As they went along the young man kept looking back at her. And she smiled at him and made signs with her fingers. Firstshe would stretch out three fingers, then she would turn her hand around three times, and finally she would point to a little mirror which she wore on her breast. When they partedshe whispered to him: \"Do not forget me!”[question]Who was Rose-Red?[input1]Is the [question] related to the [context]? Why?[output1]Yes, the question is related to the context. Rose-Red is one of the three beautiful slave girls who served the prince. The young man, Tsui, was attracted to her and she seemed to be attracted to him as well.[input2]Based on the above response [output1], which one of the following [options] best described [question] wrt[input1]?[options]1: They are not at all related;2: They are remotely related;3: They are somewhat related;4: They are closely related.[output2]3: They are somewhat related.\nFigure 1: Template for prompting GPT-3 to rate a ques-\ntion’s relevance. GPT-3 output is highlighted in green.\n3 Method\nIn this section we propose three question selection\nmethods. As described in §2, each method is used\nto score k sampled questions in Q and selects the\ncandidate with the highest score.\nn-gram similarity: We use n-gram similarity be-\ntween a question and its context to measure their\nrelevance. This method reflects the intuitive as-\nsumption that favorable question be closely related\nto the information provided by the context. Specif-\nically, we extract all unique n-grams5 sn(c) from\na given context c, sn(q) from a question q. The\nn-gram similarity score is then defined as:\nsimn = ∣sn(c)∩sn(q)∣\n∣sn(q)∣ , (1)\nwhere ∣s∣ indicates the size of set s.\nRound-trip: Intuitively, the answer to a gener-\nated question should be semantically equivalent\nto the answer that has been used to generated the\nquestion. Formally, a question generation model\nQG and a QA model (both with reasonable per-\nformance) should satisfy the following constraint:\nq′ =QG(c, a); a′ =QA(c, q′); a′ =a. (2)\nThis idea is closely related to cycle consistency in\nthe existing literature on image generation (Zhu\net al., 2017), machine translation (Artetxe et al.,\n2018), and QA (Alberti et al., 2019; Shah et al.,\n2019)). Here, we use GPT-3 as an off-the-shelf\nQA model to obtain a′ for each pair of c and q′,\nresulting in k answers A ={a′\n1, . . . , a′\nk} for the k\n5In all our experiments n ranges from 1 to 5.\nSQuAD Fairytale QA\n(BLEU-4) (ROUGE-L)\nprior works (models trained/fine-tuned on these datasets)\n(Du and Cardie, 2018) 0.152 –\n(Zhang and Bansal, 2019) 0.184 –\nUniLM Large (Bao et al., 2020) 0.228 –\nUniLM v2 Base (Bao et al., 2020) 0.244 –\nERNIE-GEN Large (Xiao et al., 2021) 0.254 –\nBART (Xu et al., 2022) – 0.527\nbaselines (notations defined in §2)\nMg (greedy) 0.372 0.424\nMs (sample avg) 0.359 0.399\nMs (lowerbound, from exhaustive search) 0.225 0.259\nMs (upperbound, from exhaustive search) 0.496 0.548\nquestion selection\nbi-gram 0.382 0.403\ntri-gram 0.380 0.403\nround-trip 0.392 0.434\noverall prompt score (OPS) 0.373 0.399\naveraged prompt score (APS) 0.380 0.406\nensemble multiple methods\nAPS + round-trip 0.397 0.439\nbi-gram + round-trip 0.400 0.429\ntri-gram + round-trip 0.398 0.430\nbi-gram + APS 0.384 0.406\ntri-gram + APS 0.383 0.409\nbi-gram + APS + round-trip 0.401 0.431\ntri-gram + APS + round-trip 0.400 0.435\nTable 1: Reference-based evaluation scores. Best and\nsecond best scores (excluding baselines) are highlighted\nwith boldface and underline.\nsampled questions in Q. We then measure the simi-\nlarity between each a′\ni and the ground-truth answer\na (F1 for SQuAD and ROUGE-L for Fairytale QA,\nin accordance with the evaluation setup from the\noriginal papers for the two datasets). Finally, we\nselect the question corresponding to the generated\nanswer a′\ni∗ that overlaps the most with a (i.e., that\ncan be best answered by GPT-3). Prompts used in\nthese experiments are detailed in Appendix A.\nPrompt-based Score: We propose a two-step\nprocedure (Figure 1) for prompting GPT-3 to an-\nswer the same set of meta-questions (i.e., questions\nabout the quality of a given question) used for hu-\nman evaluation (§2). In step 1, given a context-\nquestion pair, GPT-3 is prompted to answer a meta-\nquestion as an open question (as opposed to choos-\ning among a list of options) as well as to verbalize a\nreason for its answer. In step 2, GPT-3 is prompted\nto choose from a list of options representing the\nrating scale of the meta-question.\nWe empirically observe that without the first\nstep, GPT-3 output tends to have a low-entropy\ndistribution, i.e., often choosing the same option\nfor a given meta-question disregarding the different\ncontext-question pairs. In contrast, the additional\nfirst-step appears to improve prediction diversity,\nwhich is inline with observations made in some\nexisting studies (Nye et al., 2021; Wei et al., 2022).\nSimilar to human evaluation, we also prompt\n12954\nFigure 2: Human evaluation results, averaged over three annotators’ scores, normalized per column. Left: SQuAD;\nright: Fairytale QA. Abbreviations in x-axis denote Grammatical correctness, Offensiveness, Clarity, Relevance,\nImportance, Specificity, Answerability, Averaged Human Rating (over all dimensions to the left), Overall Human\nRating (an overall score given by annotators). Exact scores are provided in Appendix C.\nGPT-3 to generate an overall score of a question\n(denoted overall prompt-based score or OPS). The\naverage score of all individual meta-questions is in-\nstead denoted averaged prompt-based score (APS).\n4 Results and Discussion\nTo measure the performance of a selection method\n(§3), we use it to select one out of k =5 questions\nsampled from GPT-3, and score the selection with\nthe evaluation metrics outlined in §2. Additionally,\nwe test the ensemble performance with multiple\nmethods. To ensure comparability, we normalize\nthe scores obtained from each selection method into\nthe range between 0 and 1, and use their average\nscore to perform question selection.\nReference-based evaluation Reference-based\nevaluation are automatic metrics that are applied\nto the entire test sets of SQuAD and Fairytale QA.\nTable 1 shows that on both datasets, all question se-\nlection methods outperform Ms, the average score\nover all five sampled questions, validating the ef-\nfectiveness of the proposed methods. While all\nindividual methods outperform the greedy base-\nline Mg on SQuAD, round-trip performs the best,\noutperforming Mg on both datasets. It can be fur-\nther improved via ensemble with n-gram and/or\nprompt-based scores (using uniform weights).\nNote that prior studies require a large amount of\nlabeled data for model training/fine-tuning, while\nGPT-3 performs zero-shot inference. Despite this\nmajor difference in learning paradigm, most GPT-\n3-based models proposed here outperform previ-\nous results by significant margins on the SQuAD\ndataset — even the least performant samples Ms\n(lowerbound) achieve competitive results. For\nFairytale QA, however, only the best samples\nMs (upperbound) outperform previous results (Xu\net al., 2022), indicating margins for improvement\non question selection strategies for future work.\nHuman evaluation Human evaluation consists\nof 16, 800 annotations (from 87 annotators) evenly\nsplit across the two datasets (details in Appendix B).\nFor question generation (among many NLG tasks),\nmodel outputs may exhibit linguistic diversity\nwhile maintaining semantic equivalence. It is thus\nhighly problematic to evaluate such outputs against\na single reference (i.e, “ground-truth”). Figure 2\nempirically shows that the ground-truth (GT) pro-\nvided in the datasets often fail to receive the highest\nhuman ratings, on many occasions scoring lower\nthan stochastic samples from GPT-3 ( Ms). Con-\nsequently, we strongly advocate for human eval-\nuation, which we believe is higly effective in im-\nproving generalizability of our results to real-world\napplications.\nAnother prominent observation is that n-gram\nand APS perform quite differently on the two\ndatasets. On SQuAD, n-gram similarity outper-\nforms other individual methods, with further no-\nticeable improvements via ensemble with round-\ntrip. APS, on the other hand, does not work nearly\nas well, performing the worst for almost all meta-\nquestions. In contrast, n-gram (particularly tri-\ngram) similarity shows the worst performance on\nFairytale QA, while APS outperforms all other\nmethods by a noticeable margin.\nWe posit that the reversed trend in comparing\nn-gram and APS can be explained by the distinct\nnatures of the datasets. For SQuAD, the sentence-\nlevel contexts are relatively short and simple with\n12955\nstrictly extractive answers (i.e., the answers being\nsub-strings of the corresponding contexts). As a\nresult, paraphrasing the context can be a fairly ef-\nfective strategy to generate questions (hence the\nstronger correlation between question quality and\nthe c–q n-gram similarity). In contrast, with multi-\nparagraph contexts and abstractive, open-ended\nanswers, Fairytale QA questions are more likely\nposed about abstract ideas rather than simple con-\ntext paraphrasing. Consequently, n-gram similar-\nity, which favors local context paraphrasing, is less\nlikely to perform well.\nConclusion In this study, we investigate the prac-\ntical problem of selecting the best output from mul-\ntiple samples generated by an LLM. Using ques-\ntion generation as a case study, we propose two\nprompt-based question selection methods. To al-\nleviate real-world constraints on using LLMs, the\nproposed methods do not require model fine-tuning\nnor human annotation. Extensive experiments with\nboth automatic and human evaluations evince the\neffectiveness of our approach on question selection.\n5 Limitations\nWe acknowledge that our system has some lim-\nitations that warrants further investigation. For\nexample, one needs to be mindful of the specific\ndownstream applications of the proposed methods,\nboth in terms of 1) potentially large variance in out-\nof-distribution performance (e.g. divergent ques-\ntion generation applications that aim to spark chil-\ndren’s curiosity-driven thinking (Abdelghani et al.,\n2022)); and 2) of mitigating harmful/toxic contents\nin educational applications (Bender et al., 2021).\nAs a result, we believe such techniques and appli-\ncations are neither suitable nor safe to directly in-\nteract with children, we urge developers to use this\ntechnique in other ways, for instance, in teaching\nassistant application (e.g., a system that suggests\nexamples for teachers), where the teacher can filter\nand modify the examples and thus making sure the\ncontent children receive is proper and safe.\nWe also acknowledge the prohibitively restric-\ntive access to the GPT-3 model at the time of writ-\ning. We do believe that this constraint will relax\nover time, and meanwhile, hoping that our pro-\nposal can shed light on research and applications\nwith more accessible LLMs such as GPT-J (Wang\nand Komatsuzaki, 2021) and BLOOM (BigScience,\n2022) for future work.\nWhile we acknowledge the many limitations\nwith respect to accessing GPT-3, we are not advo-\ncating against using it. On the contrary, in fact, we\nbelieve GPT-3 is still among the most cost-effective\nsolutions especially in the context of natural lan-\nguage generation. The main goal of the study is\nthus to explore more data efficient ways of using\nGPT-3 to generate and evaluate questions. We\nstrive to share our experience and insights with\nthe community, which hopefully can be proven\nvaluable and helpful.\nReferences\nRania Abdelghani, Pierre-Yves Oudeyer, Edith Law,\nCatherine de Vulpillieres, and Hélene Sauzéon. 2022.\nConversational agents for fostering curiosity-driven\nlearning in children. International Journal of Human-\nComputer Studies.\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin,\nand Michael Collins. 2019. Synthetic QA corpora\ngeneration with roundtrip consistency. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6168–6173, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, and Hsiao-Wuen Hon. 2020.\nUniLMv2: Pseudo-masked language models for uni-\nfied language model pre-training. In Proceedings of\nthe 37th International Conference on Machine Learn-\ning, volume 119 ofProceedings of Machine Learning\nResearch, pages 642–652. PMLR.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nBigScience. 2022. Bigscience language open-science\nopen-access multilingual (bloom) language model.\nInternational.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\n12956\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXinya Du and Claire Cardie. 2018. Harvest-\ning paragraph-level question-answer pairs from\nWikipedia. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1907–1917, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nXinya Du, Junru Shao, and Claire Cardie. 2017. Learn-\ning to ask: Neural question generation for reading\ncomprehension. arXiv preprint arXiv:1705.00106.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham\nNeubig. 2021. How can we know when language\nmodels know? on the calibration of language models\nfor question answering. Transactions of the Associa-\ntion for Computational Linguistics, 9:962–977.\nGhader Kurdi, Jared Leo, Bijan Parsia, Uli Sattler, and\nSalam Al-Emari. 2020. A systematic review of auto-\nmatic question generation for educational purposes.\nInternational Journal of Artificial Intelligence in Ed-\nucation, 30(1):121–204.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler,\nAntonia Creswell, James L McClelland, Jane X\nWang, and Felix Hill. 2022. Can language models\nlearn from explanations in context? arXiv preprint\narXiv:2204.02329.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nBang Liu, Haojie Wei, Di Niu, Haolan Chen, and\nYancheng He. 2020. Asking questions the human\nway: Scalable question-answer generation from text\ncorpus. In Proceedings of The Web Conference 2020,\npages 2032–2043.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAman Madaan, Niket Tandon, Peter Clark, and Yim-\ning Yang. 2022. Memory-assisted prompt editing\nto improve gpt-3 after deployment. arXiv preprint\narXiv:2201.06009.\nYosi Mass, Boaz Carmeli, Haggai Roitman, and David\nKonopnicki. 2020. Unsupervised faq retrieval with\nquestion generation and bert. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 807–812.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nLiangming Pan, Wenqiang Lei, Tat-Seng Chua, and Min-\nYen Kan. 2019. Recent advances in neural question\ngeneration. arXiv preprint arXiv:1905.08949.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\n12957\nValentina Pyatkin, Paul Roit, Julian Michael, Reut Tsar-\nfaty, Yoav Goldberg, and Ido Dagan. 2021. Asking\nit all: Generating contextualized questions for any\nsemantic role. arXiv preprint arXiv:2109.04832.\nPeng Qi, Yuhao Zhang, and Christopher D. Manning.\n2020. Stay hungry, stay focused: Generating infor-\nmative and specific questions in information-seeking\nconversations. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pages 25–\n40, Online. Association for Computational Linguis-\ntics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nMeet Shah, Xinlei Chen, Marcus Rohrbach, and Devi\nParikh. 2019. Cycle-consistency for robust visual\nquestion answering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 6649–6658.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/\nkingoflolz/mesh-transformer-jax.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts? arXiv preprint arXiv:2109.01247.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nDongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. Ernie-gen:\nAn enhanced multi-flow pre-training and fine-tuning\nframework for natural language generation. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI’20.\nYing Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bing-\nsheng Yao, Tongshuang Wu, Zheng Zhang, Toby Jia-\nJun Li, Nora Bradford, Branda Sun, et al. 2022. Fan-\ntastic questions and where to find them: Fairytaleqa–\nan authentic dataset for narrative comprehension.\narXiv preprint arXiv:2203.13947.\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Bench-\nmarking zero-shot text classification: Datasets, eval-\nuation and entailment approach. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3914–3923, Hong Kong,\nChina. Association for Computational Linguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nXingdi Yuan, Tong Wang, Caglar Gulcehre, Alessan-\ndro Sordoni, Philip Bachman, Sandeep Subramanian,\nSaizheng Zhang, and Adam Trischler. 2017. Ma-\nchine comprehension by text-to-text neural question\ngeneration. In arXiv.\nShiyue Zhang and Mohit Bansal. 2019. Address-\ning semantic drift in question generation for semi-\nsupervised question answering. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2495–2509, Hong Kong,\nChina. Association for Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros. 2017. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In Pro-\nceedings of the IEEE international conference on\ncomputer vision, pages 2223–2232.\n12958\nContents in Appendices:\n• In Appendix A, we report all prompt tem-\nplates we used in this work.\n• In Appendix B, we provide details on the hu-\nman study.\n• In Appendix C, we provide the full set of our\nexperiment results.\n• In Appendix D, we report implementation de-\ntails.\nA Prompt Designs\nWe report an example of our prompt for question\ngeneration in Figure 3.\nWe report an example of our prompt for QA\n(used in round-trip) in Figure 4.\nWe report an example of our prompt in obtaining\nprompt scores in Figure 1.\nB Human Study\nWe randomly sample 50 documents from each of\nthe two datasets SQuAD and Fairytale QA. Each\ndocument correspond to one ground-truth ques-\ntion and six questions generated by GPT-3 (five\nby stochastic sampling and one by greedy search).\nEach question is then rated by three human anno-\ntators wrt seven meta-questions and one over-all\nrating, altogether constituting 50 ×2 ×(1 +5 +\n1)×3 ×(7 +1) =16, 800 annotations. There are\nin total 87 annotators involved in the annotation\nprocess, all annotators are English speakers, they\nare recruited from regions including Europe, the\nUnited States and United Kingdom. Each annota-\ntor on average performed 193 annotations and was\npaid on average $14.1 USD per hour.\nWe perform a basic spam filtering process on\nthe raw annotations. We observe a 15.4% spam\nrate. All human scores reported in this paper are\ncomputed after spam removal.\nWe report the eight meta-questions we used for\nhuman annotation in Figure 5. The eight meta-\nquestions correspond to columns in Figure 2. We\ncollect three annotations from different annotators\nfor every meta-question, we report the averaged\nhuman agreement rate in Table 2.\nC Additional Results\nIn Table 3, we report the full experiment results for\nreference-based evaluation.\ngrammatical correctness 0.698\noffensiveness 0.788\nclarity 0.640\nrelevance 0.670\nimportance 0.558\nspecificity 0.619\nanswerability 0.588\noverall human rating (OHR) 0.485\nTable 2: Averaged human agreements among three anno-\ntators. An agreement indicates that all three annotators\nselected the same option for a meta-question. We show\ndecomposing single-score metric (i.e., OHR) to scores\nmeasuring different aspects (listed above OHR) can sig-\nnificantly improve human agreements.\nIn Table 4, we report the full results for human\nevaluation on SQuAD.\nIn Table 5, we report the full results for human\nevaluation on Fairytale QA.\nD Implementation Details\nIn all experiments, we use the text-davinci-002\n(175B parameters) variant of GPT-3. It is currently\nthe most capable GPT-3 model variant. Compared\nto other variants, text-davinci-002’s support to\ninserting completions can better facilitate our ques-\ntion generation tasks (as shown in Figure 3).\nWe use a temperature of 0.7 during the sampling\nprocess of question generation. In all other use\ncases (e.g., QA round-trip, prompt score), we use\ngreedy generation (temperature is set to 0).\n12959\nStory:\nAs soon as the lady had departed the fisher’s son awoke, and the dark lad told him of her visit, and\nhow he would never see her as long as he lived. At this the fisher’s son felt the cold creeping up to\nhis heart, yet he knew the fault had not been his that sleep had overtaken him.\n’I will search the whole world through till I find her,’ cried he, and the dark lad laughed as he heard\nhim. But the fisher’s son took no heed, and off he went, following the sun day after day, till his\nshoes were in holes and his feet were sore from the journey. Nought did he see but the birds that\nmade their nests in the trees, not so much as a goat or a rabbit. On and on and on he went, till\nsuddenly he came upon a little house, with a woman standing outside it.\nInstruction:\nRead the above story, ask a question and answer it.\nQuestion:\nGPT-3 FILLS IN THIS BLANK\nAnswer:\nsearch the whole world through till he found her\nFigure 3: An example of prompting GPT-3 for question generation. We use the text before green as prompt, and\ntext after green as suffix. We refer readers to the GPT-3 documentation for more details about GPT-3’s inserting\ncompletion mode.\n[Document]:\nis cheeks were red with passion, and his eyes were bright, for he could not but notice that, now that\nshe was safe at Orphir under her true love’s protection, the Lady Morna’s manner had grown cold\nand distant again, and he was beginning to lose faith in Snorro’s charm.\nAngry and disappointed, he had sought his mother’s room to pour out his story of vexa-\ntion to her.\nHe stopped short, however, when he saw the wonderful waistcoat lying on the table, all\ngold and silver and shining colours. It was like a fairy garment, and its beauty took his breath away.\n[Question]:\nWhy did Harold lose faith in Snorro’s charm?\n[Answer]:\nHarold lost faith in Snorro’s charm because the Lady Morna’s manner had grown cold and distant\nagain.\nFigure 4: An example of prompting GPT-3 for QA. GPT-3 output is highlighted in green.\n12960\n1. Is the question gramatically correct?\n1) It is grammatically incorrect\n2) It has some grammatical issues\n3) It is grammatically correct\n2. Is the question offensive to people?\n1) It is very offensive\n2) It may be offensive\n3) It is not at all offensive\n3. Is the question clear?\n1) It is not at all clear\n2) It is mostly clear\n3) Is is very clear\n4. Is the question related to the context of the attached document?\n1) It is not at all related\n2) It is somewhat related\n3) It is closely related\n5. Is the question asking about an important aspect of the context of the attached docu-\nment?\n1) Not at all important\n2) It may be important\n3) It is very important\n6. Is the question asking about a specific piece of information in the attached document?\n1) The question is very generic\n2) The question is somewhat generic\n3) The question is very specific\n7. Can the question be answered using information in the attached document?\n1) No, answering the question requires completely different information\n2) The question can be partially answered using information from the document\n3) The question can be perfectly answered using information from the document\n8. What is your overall rating of the question generated based on the attached document?\n1) The question is very bad\n2) The question is okay\n3) The question is very good\nFigure 5: Meta-questions we designed for human evaluation.\n12961\nSQuAD Fairytale QA\n(BLEU-4) (ROUGE-L)\nprior works (models trained/fine-tuned on these datasets)\n(Du and Cardie, 2018) 0.152 –\n(Zhang and Bansal, 2019) 0.184 –\nUniLM Large(Bao et al., 2020) 0.228 –\nUniLM v2 Base(Bao et al., 2020) 0.244 –\nERNIE-GEN Large (Xiao et al., 2021) 0.254 –\nBART (Xu et al., 2022) – 0.527\nbaselines (notations defined in §2)\nMg (greedy) 0.372 0.424\nMs (sample avg) 0.359 0.399\nMs (lowerbound) 0.225 0.259\nMs (upperbound) 0.496 0.548\nn-gram-similarity\nuni-gram w/ context 0.382 0.396\nbi-gram w/ context 0.382 0.403\ntri-gram w/ context 0.380 0.403\n4-gram w/ context 0.378 0.406\n5-gram w/ context 0.375 0.404\nround-trip\nround-trip 0.392 0.434\nprompt scores\ngrammatical correctness 0.364 0.405\noffensiveness 0.374 0.403\nclarity 0.373 0.406\nrelevance 0.372 0.396\nimportance 0.372 0.406\nspecificity 0.378 0.405\nanswerability 0.372 0.404\naveraged prompt score (APS) 0.380 0.406\noverall prompt score (OPS) 0.373 0.399\nensemble multiple methods\nAPS + round-trip 0.397 0.439\nbi-gram + round-trip 0.400 0.429\ntri-gram + round-trip 0.398 0.430\nbi-gram + APS 0.384 0.406\ntri-gram + APS 0.383 0.409\nbi-gram + APS + round-trip 0.401 0.431\ntri-gram + APS + round-trip 0.400 0.435\nTable 3: Reference-based evaluation scores on various question selection methods. Best and second best numbers\n(excluding baselines) are highlighted with boldface and underline.\n12962\nG O C R I S A AHR OHR\nGT 0.937 0.987 0.943 0.930 0.925 0.922 0.887 0.933 0.870\nMg 0.950 0.983 0.927 0.953 0.925 0.905 0.870 0.930 0.833\nMs 0.957 0.981 0.940 0.937 0.909 0.921 0.879 0.932 0.857\nbi-gram 0.968 0.990 0.952 0.953 0.918 0.937 0.885 0.943 0.880\ntri-gram 0.968 0.990 0.943 0.957 0.928 0.917 0.890 0.942 0.885\nround-trip (RT) 0.962 0.992 0.927 0.933 0.900 0.922 0.903 0.934 0.872\nAPS 0.945 0.980 0.912 0.912 0.880 0.902 0.863 0.913 0.837\nAPS + RT 0.947 0.977 0.928 0.915 0.890 0.898 0.903 0.923 0.875\nbi-gram + RT 0.983 0.980 0.952 0.950 0.920 0.937 0.917 0.948 0.893\ntri-gram + RT 0.983 0.987 0.952 0.953 0.927 0.933 0.925 0.951 0.900\nbi-gram + APS 0.972 0.983 0.943 0.932 0.903 0.922 0.867 0.932 0.857\ntri-gram + APS 0.972 0.983 0.945 0.928 0.910 0.912 0.878 0.933 0.862\nbi-gram + APS + RT 0.977 0.980 0.947 0.932 0.905 0.918 0.905 0.938 0.877\ntri-gram + APS + RT 0.972 0.980 0.948 0.928 0.910 0.912 0.902 0.936 0.878\nTable 4: Human eval results (SQuAD). Abbreviations in the first row denoteGrammatical correctness, Offensiveness,\nClarity, Relevance, Importance, Specificity, Answerability, Averaged Human Rating (over all dimensions to the\nleft), Overall Human Rating (an overall score given by annotators). Best and second best numbers (excluding\nbaselines) are highlighted with boldface and underline.\nG O C R I S A AHR OHR\nGT 0.945 0.963 0.942 0.937 0.885 0.928 0.892 0.927 0.867\nMg 0.975 1.000 0.958 0.943 0.920 0.922 0.905 0.946 0.870\nMs 0.964 0.988 0.944 0.955 0.925 0.934 0.912 0.946 0.875\nbi-gram 0.953 0.993 0.943 0.943 0.932 0.937 0.902 0.943 0.857\ntri-gram 0.943 0.980 0.922 0.930 0.905 0.927 0.858 0.924 0.838\nround-trip (RT) 0.928 0.970 0.945 0.937 0.888 0.935 0.878 0.926 0.862\nAPS 0.952 0.985 0.957 0.972 0.922 0.977 0.948 0.959 0.895\nAPS + RT 0.927 0.983 0.955 0.973 0.915 0.948 0.928 0.947 0.893\nbi-gram + RT 0.943 0.992 0.952 0.938 0.910 0.927 0.918 0.940 0.860\ntri-gram + RT 0.927 0.973 0.932 0.928 0.890 0.935 0.882 0.924 0.857\nbi-gram + APS 0.945 0.988 0.962 0.942 0.925 0.945 0.932 0.948 0.888\ntri-gram + APS 0.948 0.978 0.952 0.938 0.918 0.940 0.913 0.941 0.863\nbi-gram + APS + RT 0.945 0.988 0.960 0.942 0.913 0.937 0.932 0.945 0.883\ntri-gram + APS + RT 0.925 0.987 0.938 0.962 0.910 0.942 0.922 0.941 0.882\nTable 5: Human eval results (Fairytale QA). Abbreviations in the first row denote Grammatical correctness,\nOffensiveness, Clarity, Relevance, Importance, Specificity, Answerability, Averaged Human Rating (over all\ndimensions to the left), Overall Human Rating (an overall score given by annotators). Best and second best numbers\n(excluding baselines) are highlighted with boldface and underline.\n12963\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 5 on page 5.\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 5 on page 5.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1 on page 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nThis paper discuss an application of GPT-3, but we do not use GPT-3 in any of the paper section\nwriting.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe use existing language model, namely GPT-3; we also use existing question generation dataset,\nnamely SQuAD and Fairytale QA. We discuss them in Section 2 (page 2).\n□\u0013 B1. Did you cite the creators of artifacts you used?\nYes, we cite the creators in Section 2, page 2.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. They are publicly available.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nYes, in Section 1 and 5.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nYes, in section 2, 3 and appendix B.\nC □\u0013 Did you run computational experiments?\nYes, described in section 2, 3, 4.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. We do not propose new model. We use existing language model, and properly cite\nthe original work.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12964\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nYes, section 4 and appendix B.\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nYes, appendix B.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nYes, appendix B.\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nYes, appendix B.\n□\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nYes, appendix B.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n12965"
}