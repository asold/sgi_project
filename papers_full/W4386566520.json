{
  "title": "Scalable Prompt Generation for Semi-supervised Learning with Language Models",
  "url": "https://openalex.org/W4386566520",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101972658",
      "name": "Yuhang Zhou",
      "affiliations": [
        "Amazon (United States)",
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5032620055",
      "name": "Suraj Maharjan",
      "affiliations": [
        "Amazon (United States)",
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A5000748332",
      "name": "Beiye Liu",
      "affiliations": [
        "Amazon (United States)",
        "University of Maryland, College Park"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W3096580779",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4285286749",
    "https://openalex.org/W4224909620",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3196642073",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4309444617",
    "https://openalex.org/W4297808394"
  ],
  "abstract": "Prompt-based learning methods in semi-supervised learning (SSL) settings have been shown to be effective on multiple natural language understanding (NLU) datasets and tasks in the literature. However, manually designing multiple prompts and verbalizers requires domain knowledge and human effort, making it difficult and expensive to scale across different datasets. In this paper, we propose two methods to automatically design multiple prompts and integrate automatic verbalizer in SSL settings without sacrificing performance. The first method uses various demonstration examples with learnable continuous prompt tokens to create diverse prompt models. The second method uses a varying number of soft prompt tokens to encourage language models to learn different prompts. For the verbalizer, we use the prototypical verbalizer to replace the manual one. In summary, we obtained the best average accuracy of 71.5% (a relative improvement of 0.99% over even the previous state-of-the-art SSL method with manual prompts and verbalizers) in different few-shot learning settings.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 770–781\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nScalable Prompt Generation for Semi-supervised Learning with Language\nModels\nYuhang Zhou*\nUniversity of Maryland\nCollege Park, MD\ntonyzhou@umd.edu\nSuraj Maharjan*\nAmazon\nSeattle, W A\nmhjsuraj@amazon.com\nBeiye Liu\nAmazon\nNew York, NY\nbeiyeliu@amazon.com\nAbstract\nPrompt-based learning methods in semi-\nsupervised learning (SSL) settings have been\nshown to be effective on multiple natural lan-\nguage understanding (NLU) datasets and tasks\nin the literature. However, manually designing\nmultiple prompts and verbalizers requires do-\nmain knowledge and human effort, making it\ndifficult and expensive to scale across different\ndatasets. In this paper, we propose two meth-\nods to automatically design multiple prompts\nand integrate automatic verbalizer in SSL set-\ntings without sacrificing performance. The first\nmethod uses various demonstration examples\nwith learnable continuous prompt tokens to cre-\nate diverse prompt models. The second method\nuses a varying number of soft prompt tokens\nto encourage language models to learn differ-\nent prompts. For the verbalizer, we use the\nprototypical verbalizer to replace the manual\none. In summary, we obtained the best average\naccuracy of 73.2% (a relative improvement of\n2.52% over even the previous state-of-the-art\nSSL method with manual prompts and verbal-\nizers) in different few-shot learning settings.\n1 Introduction\nPre-training large language models with huge\namounts of text corpora in masked language mod-\neling tasks and then fine-tuning the pre-trained lan-\nguage model (PLM) on downstream tasks have\nshown superior performance in many natural lan-\nguage processing tasks. However, the discrep-\nancy between the pretraining task (masked lan-\nguage modeling objective) and the downstream\nfine-tuning task (task without MASK token) could\nlead to unexpected behaviors. Recently, there\nhas been growing research interest in the area of\nprompt-tuning, where any NLU task is transformed\ninto a cloze task to mimic the pre-training objective\nof a large masked language model (Kumar et al.,\n*Equal contribution. This work was done during Yuhang’s\ninternship at Amazon, Alexa AI.\n2016; McCann et al., 2018; Radford et al., 2018).\nPrompt-based learning transforms an input x into\nx′ using a prompt function. It makes use of the\nvast amount of acquired knowledge of PLMs to\npredict a distribution of tokens at the masked posi-\ntion. The verbalizer then maps the predicted tokens\nto classes. The main advantage of this approach is\nthat this method works well in a few-shot learning\nenvironment (Schick and Sch ¨utze, 2021). How-\never, the main disadvantage of this method is the\nlimitation posed by the prompt and verbalizer func-\ntions, which require human knowledge to carefully\ncraft them. Such handcrafting work is expensive\nand not scalable with the increase in the variety of\ntasks and datasets. For example, in Alexa, there\nare thousands of domains and manually designing\nprompts and verbalizer for intent classification for\neach of them according to the dataset content de-\nmand human expertise, which is time consuming\nand not applicable. It is essential to reduce the\nhuman efforts in the process of prompt generation.\nPrompt-based learning requires finding the right\ntokens in the prompts that align with the task re-\nquirement and dataset content. However, since the\nobjective of these prompt tokens is only for the\nlanguage models to perform the task at hand, it is\nnot necessary for them to be a sequence of words\nthat humans can understand.\nContinuous prompt-based learning alleviates the\nneed for human intervention to determine prompt\ntokens. Instead, it automates the prompt design pro-\ncess. In the literature, there are mainly two meth-\nods: i) automatically search for discrete prompt\ntext tokens (Shin et al., 2020a) ii) automatically\nlearn numerical prompt embeddings (Lester et al.,\n2021; Li and Liang, 2021; Liu et al., 2021c,b; Ham-\nbardzumyan et al., 2021). The main difference be-\ntween these two approaches is that the first searches\nfor actual discrete tokens from the language model\nvocabulary, whereas the second method directly\nlearns the embeddings for prompt tokens, which\n770\nmay not be human comprehensible. Similarly, au-\ntomatic selection of label words (Shin et al., 2020a;\nSchick et al., 2020a; Gao et al., 2021), soft ver-\nbalizer (Hambardzumyan et al., 2021; Liu et al.,\n2021b), and prototypical verbalizer (Cui et al.,\n2022) are the methods proposed to eliminate the\ntedious process of manually defining verbalizer\nmapping functions.\nMost of these continuous prompt and automatic\nverbalizer methods focus on supervised learning\n(SL) settings but ignore their generalization un-\nder semi-supervised learning (SSL) settings. The\nprevious state-of-the-art (SoTA) SSL method with\nvarious manual prompts and verbalizers has shown\nsuperiority over SL language models with a sin-\ngle manual prompt (Schick and Sch¨utze, 2021). In\nthis SSL pipeline, we normally train several labeler\nmodels with different manual prompts to capture\ndiverse information from the limited training data\nand make use of them to annotate a huge amount\nof unlabeled data. Having to design several manual\nprompts and verbalizer models for SSL settings\nand applying them across multiple datasets and\ntasks will exacerbate the scalability and cost prob-\nlem. In this paper, we tackle the problem posed by\nmanual prompt and verbalizer design and propose\nautomatic methods to fully automate the design of\ndiverse prompts and verbalizers in SSL settings.\nOur main contributions are as follows.\n• We propose methods to generate various\nprompts by adding multiple demonstration ex-\namples with continuous prompt tokens for use\nin SSL settings.\n• To the best of our knowledge, we are the first\nto completely eliminate human involvement\nin designing multiple prompts and verbalizers\nin SSL settings and obtain similar and even\nbetter performance than the SoTA methods\nwith manual prompts and verbalizers.\n• We empirically show that using the automatic\nverbalizer with manual prompts can achieve\na similar performance to manual verbalizers’\nperformance in the SSL pipeline.\n2 Methodology\nOur overall prompt-based SSL workflow follows\nPattern-exploiting Training (PET) semi-supervised\nlearning setting (Schick and Sch¨utze, 2021). PET\nfirst transforms the input sequence x to a cloze\nquestion containing a single MASK token. Next, it\nuses PLM to fill in the value of the MASK token\nand applies verbalizers to map the output tokens\nto the class labels y ∈Y . They devise a semi-\nsupervised framework to produce soft labels on\na large amount of unlabeled data, which are later\nused to train a final supervised classifier F. They\nreport strong performance over other supervised\nprompt-tuning methods and other semi-supervised\napproaches without prompts across multiple NLU\ntasks. Before this paper, the PET approach was the\nstate-of-the-art (SoTA) framework that integrates\nthe prompt-tuning method into the SSL pipeline.\nThe PET method fine-tunes multiple PLMs with\ndifferent prompts. It introduces diversity in the\nprompts by manually designing several prompts\nusing domain and task knowledge. Similarly, it\nuses human expertise to design verbalizer map-\npings for each of the datasets based on the knowl-\nedge of the tasks. Here, we use continuous and\nautomatic prompts and verbalizers, thus eliminat-\ning the need for human involvement in designing\nmanual prompts and verbalizers.\n2.1 Overall Pipeline\nFigure 1 shows the overall pipeline of our pro-\nposed methods. Unlike the original PET pipeline\nwith manual prompts and verbalizers, we use a\nprompt generation function to generate multiple\nautomatic prompts. Each PLM with automatic\nprompts serves as a labeler model. We train each\nof these prompts + automatic verbalizer models\nwith a labeled dataset T in few-shot settings. With\nan input sequence xt ∈T and the given label yt,\nwe first use the prompt function P to transform xt\ninto a sequence P(xt) with a MASK token. The\nverbalizer then maps the predicted word probability\nat the masked position to the label probability. For\neach PLM m, the predicted probability pm(yt|xt)\nis defined as\npm(yt|xt) = exp m(yt|xt)∑\ny′∈Y exp m(y′|xt) (1)\nwhere m(y|x) is the raw score of PLM m in the\nmasked position. After obtaining the probability,\nwe minimize the cross-entropy loss Lc between\npm(y|x) and y.\nWe apply trained labeler models to each sentence\nxd ∈D in the unlabeled datasetDand get the prob-\nability pm(yd|xd) for each trained model. We then\ntake the average of these probabilities from each\n771\nFigure 1: Semi-Supervised Learning (SSL) Training. Multiple diverse prompt-based learning models are trained\non labeled data to soft label huge amounts of unlabeled data. The soft labels serve as ground truth to train the\nfinal classifier. P0, P1, . . .are continuous prompt tokens and Demo A, DemoB, . . .are demonstration examples\nrandomly sampled from the training data.\ntrained model m as the ground-truth probability,\npt(yd|xd) = 1\nZ\n∑\nm∈M\npm(yd|xd)\nwhere Z is the total number of trained PLMs with\ndifferent automatic prompts. Eventually, we fine-\ntune a final pre-trained language model F with a\nstandard sequence classification head. We use the\nKullback-Leibler (KL) divergence as our loss func-\ntion. Given pt(yd|xd) and the predicted probability\nˆp(yd|xd) of the final classifier F, the divergence\nloss Ldiv for this input is:\nLdiv(xd) =\n∑\ny′∈Y\npt(y′|xd) log\n(pt(y′|xd)\nˆp(y′|xd)\n)\n(2)\nThe final classifier F is then applied to the test set\nto obtain the results.\nSchick and Sch ¨utze (2021) introduce diversity\nin their SSL pipeline by training several models\nwith different manual prompts and applying them\nto softly label a large number of unlabeled datasets.\nThe diversity between manual prompts brings con-\nsistent improvements. We observe that diverse\nknowledge learned by the language model is mostly\nintroduced by the prompts rather than manual ver-\nbalizers, since in most datasets, they prepare only\none manual verbalizer but multiple prompts for ex-\nperimentation. Thus, we propose replacing manual\nprompts with multiple automatic prompts and us-\ning the same automatic verbalizer for all labeler\nmodels.\n2.2 Continuous Prompt Design\nSeveral researchers have proposed methods to auto-\nmate the prompt design process (Liu et al., 2021c;\nLi and Liang, 2021; Lester et al., 2021). In most of\nthese methods, they insert the continuous trainable\nprompt tokens into the input sentence and learn\nthe token embeddings during the training process.\nHowever, existing continuous prompt-based learn-\ning methods do not consider their application in\nthe PET pipeline, which requires training several\nlabeler models (Schick and Sch¨utze, 2021), in or-\nder to learn diverse knowledge from the datasets.\nTherefore, most methods do not define strategies\nto compose multiple continuous prompts. We pro-\npose two scalable solutions to introduce different\nvariables in the design of continuous prompt la-\nbeler models (various demonstration examples or\nvarying numbers of continuous prompt tokens). We\nexpect that with these diverse continuous prompts,\ntrained language models can fully learn different\naspects of knowledge from the training dataset.\n2.2.1 Scalable Prompt Generation\nInspired by the P-tuning (Liu et al., 2021c) method,\nwe insert multiple continuous prompt tokens pn\ninto the input sentence x, transforming it into\n772\n[x][p0, p1, . . . , pn][MASK].. Different from the\noriginal P-tuning method, we invent two scalable\ndesigns to make it suitable for the prompt-based\nSSL pipeline.\nAdd Demonstration Examples: In this method,\nwe add different demonstration examples to con-\nstruct diverse prompts. This is similar to the prompt\naugmentation method, in which one chooses to\nadd additional answered prompts to demonstrate\nwhat kind of answer the language model should\nproduce for the MASK token (Liu et al., 2021a).\nThese additional answered prompts are called the\ndemonstration example [demo]. To reduce the\ndiscrepancy between the demonstration examples\nand the input sentences, we also add a fixed num-\nber of continuous prompt tokens p between the\ndemonstration sentence and its true label. Thus,\ngiven the labeled input xd and its correspond-\ning ground-truth label yd from the labeled train-\ning dataset, we construct the demonstration exam-\nple as [demo] = [xd][p0, p1, . . . , pn][yd], where\np0, p1, . . . , pn are continuous prompt tokens.\nAfter composing the demonstration examples\n[demo], given a training input from the labeled\ndataset xt = (si, s2, . . . , sk) ∈ Tand label yt,\nwhere si, s2, . . . , sk are input tokens for the PLM\nm, the prompt template functionP1(xt) is formally\ndefined as\nP1(xt)1 = [demo1][xt][p0, . . . , pn][MASK]\n. . .\nP1(xt)k = [demok][xt][p0, . . . , pn][MASK]\n(3)\nWe create multiple prompts by adding different\ndemonstration examples with exactly n continuous\nsoft tokens with the input sentence. Demonstration\nexamples are randomly sampled from the labeled\ndatasets. For longer input sentences, we first trun-\ncate the length of [demo] to fit the PLM require-\nment. Our intuition is that different demonstration\nexamples will introduce the diversity necessary for\nSSL experimentation.\nVary Soft Token Numbers: In this method, we\nvary the number of continuous prompt tokens be-\ntween different labeler models. In other words, this\nprompt function P2(xt) with input sentence xt is\ndefined as\nP2(xt)1 = [xt][p0, p1, . . . , pn1 ][MASK]\n. . .\nP2(xt)k = [xt][p0, p1, . . . , pnk ][MASK]\n(4)\nand each of the labeler models uses different n1\nto nk number(s) of continuous prompt tokens\np. Here, we do not prepend the demonstration\nexample. Our intuition is that given different\nnumbers of continuous prompt tokens, the opti-\nmized learned continuous prompts may also be\ndifferent. For example, for AG’s News dataset\n(Zhang et al., 2015a) about news topics, the opti-\nmized prompts with two continuous prompt tokens\ncould be: [[x][News : ][MASK]], while optimized\nprompts with three continuous prompt tokens could\nbe: [[x][the category is][MASK]]. We expect that\nvarying the number of continuous prompt tokens\nwill have a similar impact to manually constructing\ndifferent prompts.\n2.2.2 Reparameterization Block\nLi and Liang (2021) and Liu et al. (2021c) empiri-\ncally show that directly updating the parameters in\ncontinuous prompts leads to unstable optimization.\nHence, we first feed prompt embeddings through\na reparameterization block rather than directly\nfeeding them into the PLM. Our reparametriza-\ntion block uses a bidirectional LSTM (Hochreiter\nand Schmidhuber, 1997) network with a two-layer\nReLU activated multilayer perceptron (MLP) (Liu\net al., 2021c; Li and Liang, 2021).\nWe denote the random initialized tokens as p′\ni\nand the real input embeddings, which are fed into\nthe PLM, as pi. The pi are the output of the bidi-\nrectional LSTM network and the MLP as,\npi = MLP([LSTM(p′\n0:i), LSTM(p′\ni:n)])\nwhere pi is also the soft token used in Equations 3\nand 4.We learn the optimized continuous prompt\ntokens ˆp0:n during the training process. With the\ndownstream cross-entropy loss Lc, we can differ-\nentially optimize the continuous prompts by:\nˆp0:n = argmin\np\nLc(pm(x|y), y) (5)\n2.3 Automatic Verbalizers\nThere are several automatic verbalizer methods that\neliminate the need for human intervention and ex-\npertise to build mapping functions. We experiment\nwith three types of automatic verbalizers: i) soft\nverbalizer (Hambardzumyan et al., 2021), ii) proto-\ntypical verbalizer (Cui et al., 2022), and iii) search-\nbased verbalizer (Schick et al., 2020b).\nCui et al. (2022) experimentally show the su-\nperiority of the prototypical verbalizer in a super-\nvised learning environment. However, they did not\n773\nconduct such experiments for SSL settings. Our\nexperiment with the SSL PET method (details in\nSection 3.5) with different automatic verbalizers\nshowed that the prototypical verbalizer performed\nbetter than the soft verbalizer and the search-based\nverbalizer on multiple datasets. Thus, we choose\nto use the prototypical verbalizer as a replacement\nfor the manual verbalizer.\nWith the optimized embedding of the MASK\ntoken from PLM m and the ground-truth labels y,\nthe prototypical verbalizer learns the prototype vec-\ntors for each class using contrastive learning (Oord\net al., 2018). The prototypical verbalizer first ini-\ntializes a prototype embedding for each class label\nand then uses the embedding of the MASK token as\nthe instance embedding. It uses instance-instance\nloss Lins to maximize intra-class similarity and\nminimize inter-class similarity. Similarly, it uses\ninstance-prototype loss Lproto to maximize the sim-\nilarity between the prototype and instances belong-\ning to the same class and minimize the similarity of\ninstances belonging to other classes. The probabil-\nity distribution of the MASK token for each class\nis calculated by the cosine similarity between the\ninstance embedding and each optimized prototype\nembedding. For inference, it assigns the class of\nthe prototype vector to the instance with the high-\nest probability score, which is computed by taking\nthe similarity scores of the instance vector with the\nprototype vectors and normalizing them.\n2.4 Training and Inference Strategy\nAll model parameters to be optimized are randomly\ninitialized. As mentioned in Section 2.2.2 and\n2.3, we update the parameters in the continuous\nprompts and PLMs with the loss Lc and optimize\nthe parameters in the verbalizers with the loss Lins\nand Lproto. Instead of summing all losses together,\nour training strategy is to first freeze the param-\neters in the prototypical verbalizer and then train\nthe parameters in the reparameterization block and\nthe PLM together with the cross-entropy loss Lc.\nThen we freeze the learned parameters and train\nthe parameters in the prototypical verbalizers with\ninstance-instance loss Lins and instance-prototype\nloss Lproto. After training all labeler models and\nobtaining the class probability on the unlabeled\ndataset, we use Ldiv to fine-tune the final language\nmodel classifier. During inference, we do not rely\non any prompt-based labeler models and directly\nuse the final fine-tuned language modelF to predict\non the test dataset.\n3 Experiments\nTo verify the effectiveness of our framework, we\nconduct multiple semi-supervised learning experi-\nments with several strong baseline frameworks on\nthe commonly-used NLU benchmarks.\n3.1 Dataset Collection\nWe experiment with five different datasets 1:\nAG’s News (Zhang et al., 2015a), Yahoo An-\nswers (Zhang et al., 2015b), MNLI (MultiNLI,\nMulti-Genre Natural Language Inference, Williams\net al. (2018)), RTE (Recognizing Textual Entail-\nment, Dagan et al. (2006)) and CB (Commitment-\nBank, de Marneffe et al. (2019)). AG’s News\nand Yahoo answers are topic classification (TC)\ndatasets, while MNLI, RTE, and CB are natural\nlanguage inference (NLI) datasets. In Table 1, we\nprovide the number of distinct classes, the unla-\nbeled dataset size used for SSL, and the test size\nfor all five datasets. Details about the design of\nprompts and verbalizers can be found in Appendix\nA.\nDataset Task #Class #Unlabeled #Test\nAG’s News TC 4 40,000 7,600\nYahoo TC 10 100,000 60,000\nCB NLI 3 30,000 56\nRTE NLI 2 20,000 277\nMNLI NLI 3 30,000 9,815\nTable 1: Data statistics. TC= Topic Classification, NLI=\nNatural Language Inference\nWe perform multiple experiments in few-shot\nsettings for all datasets. For few-shot experi-\nments, we use 1, 5, 10, 20 examples per class for\nall datasets except for CB and RTE, where we ex-\nperiment with 32 examples to align with earlier\nresearch work (Schick and Sch¨utze, 2021). We re-\nport the average accuracy for the evaluation across\nthree runs of each experiment with three different\nrandom seeds.\n3.2 Proposed Models\nDemo+Soft Tokens PET: The first method is to\nreplace the manual verbalizer with the prototypical\nverbalizer and manual prompts with demonstration\nexamples and continuous prompt tokens.\n1We downloaded these datasets using the script pro-\nvided by OpenPrompt https://github.com/thunlp/\nOpenPrompt\n774\nVary Soft Tokens PET: The second method is to\nintroduce diversity by varying the number of con-\ntinuous prompt tokens, and we use the prototypical\nverbalizer across multiple labeler models.\n3.3 Models for Comparison\nWe design several strong baseline experiments in\naddition to our proposed models and also perform\nan ablation study to show the superiority of our\nproposed models in multiple NLU tasks.\n3.3.1 Baseline Models\nFine-tune: This is a supervised method, where we\ndirectly fine-tune the RoBERTa-large PLM with\ntraining examples in different few-shot settings. In\nthis method, we do not leverage the unlabeled data.\nPrototypical Verbalizer PET : This is a semi-\nsupervised learning method similar to Schick and\nSch¨utze (2021), but we replace the manual verbal-\nizer with the prototypical verbalizer and keep the\nmanual prompts. Experiments with this setup will\nshow the benefits of applying automatic verbalizer\nin the PET framework.\nManual PET: This is a semi-supervised learning\nmethod from Schick and Sch¨utze (2021). Our main\ngoal is to show that, with our proposed method,\nwe can achieve similar or better results than this\nmanual method.\nThere are other SSL methods that rely on data\naugmentation without prompt tuning, such as UDA\n(Xie et al., 2020) and MixText (Chen et al., 2020).\nSince their performance is consistently worse than\nthe Manual PET model across multiple datasets\n(Schick and Sch¨utze, 2021), we do not choose these\nmodels for comparison in this work.\n3.3.2 Model Intervention for Ablation Study\nFixed Soft Tokens PET : This semi-supervised\nlearning method is similar to our second proposed\nmethod, where we vary the number of continuous\ntokens to create multiple prompts. However, here\nwe keep the number of continuous tokens fixed and\ndo not add demonstration examples as well. This\nexperiment will help us to understand the impor-\ntance of diversity introduced by varying continuous\ntokens in prompt design.\nDemo+Soft in SL : This is a supervised method,\nwhere we use a prompt template to transform the\ninput by adding a randomly selected demonstra-\ntion example from the training data and a fixed\nnumber of continuous prompt tokens to the input,\nand we use the prototypical verbalizer for classi-\nfication. We use RoBERTa-large for PLM. With\nthis experiment, we try to understand the power\nof semi-supervised learning methods with multiple\nprompts over supervised training.\n3.4 Implementation Details\nWe use the RoBERTa-Large model (Liu et al.,\n2019) as our PLM for all of our experiments. We\nuse AdamW as our optimizer with a learning rate\nof 1e−5 and a weight decay of 0.01 with linear\nscheduler, batch size of 2, and trained for 5 epochs.\nThe reparameterization block contains 2-layer bidi-\nrectional LSTM and 2 linear layers with ReLU acti-\nvation function. The hidden dimension of the linear\nlayer and LSTM layer is 768, as well as the hidden\ndimension of Roberta-Large. We train the parame-\nters in the reparameterization block and the PLM\ntogether. For the prototypical verbalizer, we base\nour implementation on the Pytorch2, Huggingface\ntransformer3, and OpenPrompt4 frameworks (Ding\net al., 2021). The number of continuous prompt\ntokens is consistent 5. For our Vary Soft Tokens\nPET, we prepare 5 prompts for each dataset and the\nnumber of soft tokens in each prompt ranges from\n1 to 5.\n3.5 Results of Multiple Automatic Verbalizers\nDatasets SSL PET\n# instancesSoftVerb SearchVerb ProtoVerb\nAG’s News 10 49.4 80.5 77.2\nYahoo 10 11.8 34.0 51.9\nCB 32 88.7 73.2 85.7\nRTE 32 48.2 50.2 52.8\nMNLI 10 39.0 37.0 50.0\nTable 2: Average accuracy on different datasets by\nreplacing manual verbalizers with automatic verbalizers\nin the PET SSL setup. For CB and RTE, we use 32\ntraining examples, whereas for other datasets, we use\n10 training examples to train labeler models. The best\nperformance is marked in bold.\nTo understand which automatic verbalizer is a\nbetter replacement for manual verbalizer, we first\nexperiment with three automatic verbalizers: soft\nverbalizer (Hambardzumyan et al., 2021; Liu et al.,\n2021c,b), search verbalizer (Gao et al., 2021; Shin\net al., 2020a; Schick et al., 2020a), and prototyp-\nical verbalizer (Cui et al., 2022). For all of these\n2https://pytorch.org/\n3https://huggingface.co/\n4https://github.com/thunlp/OpenPrompt\n775\nexperiments, we apply experimental setups sim-\nilar to PET paper, but only replace the manual\nverbalizer with the automatic verbalizer (Schick\nand Sch ¨utze, 2021). Table 2 shows the average\naccuracy over three runs with three different seeds\non different datasets with these verbalizers. From\nTable 2, the prototypical verbalizer shows better\nperformance than other verbalizers for three (Ya-\nhoo, RTE, and MNLI) out of five datasets. The\nsearch verbalizer and soft verbalizer models per-\nform better than the prototypical verbalizer model\nonly on one dataset each. Since the prototypical\nverbalizer performs better than other verbalizers in\nmajority of the datasets, we decided to use this as\nour automatic verbalizer.\n3.6 Comparison with Manual PET\nWith the prototypical verbalizer as our automatic\nverbalizer, we then experiment with our proposed\nmethods for automatic prompt design. Table 3\nshows our results on different datasets and tasks in\nthe few-shot setting. Table 3 shows that by only\nreplacing the manual verbalizer with the prototyp-\nical verbalizer (column Protoverb) and keeping\nother aspects of the experiment the same as the\nPET method, we can achieve slightly lower per-\nformance ( 70.1 average accuracy) compared to\nManual PET (71.4 average accuracy) (Schick and\nSch¨utze, 2021). This shows that to eliminate hu-\nman involvement in designing verbalizers, we can\nsimply replace the manual verbalizer with the pro-\ntotypical verbalizer with only a little performance\nsacrifice.\nFor our next set of experiments, we replace man-\nual prompts with our proposed method, automati-\ncally creating multiple prompts. The first method\n(Demo+Soft Tokens PET), which adds randomly\nsampled demonstration examples from training\ndata with a fixed number of trainable continuous\nprompt tokens with input, achieves better perfor-\nmance than Manual PET method. The next method\n(Vary Soft PET), in which we vary the number of\ncontinuous trainable tokens, also achieves better\nperformance than Manual PET method. For topic\nclassification tasks, under multiple few-shot set-\ntings, the average accuracy of Demo+Soft and Vary\nSoft PET are 77.0 and 77.3, respectively, while\nthe average accuracy of Manual PET method is\n77.1. Similarly, for NLI datasets under different\nfew-shot settings, the average accuracy of our Vary\nSoft PET method is 69.6 and Demo+Soft Tokens\nPET method is 70.7. Both of these results are bet-\nter than Manual PET method (67.7). Furthermore,\nacross all these datasets, Demo+Soft Tokens PET\nand Vary Soft PET achieve an average performance\nof 73.2 and 72.6, respectively. These results are\nbetter than Manual PET (71.4) method. This exper-\niment shows that it is possible to completely elimi-\nnate human involvement and expertise in designing\nprompts and verbalizers for the SSL pipeline with\neven better performance.\nWe also observe that for the case of one-shot\nexperiments with MNLI dataset, Demo + Soft PET\nmethod obtains an accuracy of36.1, which is much\nworse than other prompt baseline models. This\nmay be due to randomly sampled[demo] examples,\nas previous studies have shown that the choice of\nexamples in the few-shot setting can result in high-\nvariance performance (Lu et al., 2021). In future\nwork, we can utilize sentence embeddings to make\nintelligent decisions while selecting demonstration\nexamples.\n3.7 Ablation Study\n3.7.1 Impact of Semi-supervised Learning\nWe compare our proposed methods with super-\nvised learning methods: fine-tuning and prompt-\nbased tuning methods (Demo+Soft in SL). All\nsemi-supervised learning methods perform signif-\nicantly better than supervised learning methods.\nTraditional fine-tuning methods perform the worst\n(45.1 average accuracy) on different datasets and\ntasks. Demo+Soft in SL method is similar to our\nproposed Demo+Soft Tokens PET method but does\nnot make use of unlabeled data. Demo+Soft in SL\nperforms better than the fine-tuning method and\nachieves an average accuracy of 68.7 on multiple\ndatasets and tasks in different few-shot settings.\nBoth of the supervised learning methods perform\nworse than any SSL prompting model, indicating\nthe necessity of the SSL pipeline in NLU tasks.\n3.7.2 Impact of Diversity in the Prompts\nIn order to understand the effect of introducing di-\nversity through multiple prompts in SSL, we devise\nanother experiment, where we use the SSL setup\nbut use only one prompt labeler model (not adding\na demonstration example but using trainable soft to-\nkens) to label unlabeled data. We name this method\nas Fixed Soft Tokens PET. Table 3 shows that in\nmost comparisons (13/14), our proposed Vary Soft\nPET or Demo+Soft PET method achieves better\nperformance. When comparing with the Fixed Soft\n776\nSemi Supervised Learning PET Supervised\nDataset # TrainingDemo+Soft Vary Soft Fixed Soft Protoverb ManualFine-Tune Demo+Soft\nTopic Classification\nAG’s News 1 83.5 81.3 82.8 80.0 80.7 25.7 62.2\nAG’s News 5 87.6 88.0 87.3 87.3 87.8 32.6 84.9\nAG’s News 10 88.3 88.3 86.5 88.7 88.8 58.3 87.2\nAG’s News 20 88.8 89.3 88.9 89.2 89.2 86.1 88.0\nYahoo 1 61.1 62.9 59.6 62.0 62.3 10.7 55.6\nYahoo 5 67.4 67.9 67.1 67.8 68.0 12.1 65.2\nYahoo 10 68.9 69.5 69.1 70.0 69.5 37.8 67.0\nYahoo 20 70.7 71.0 70.4 70.9 70.7 66.7 66.5\nTC Avg - 77.0 77.3 76.5 77.0 77.1 41.2 72.1\nNatural Language Inference\nMNLI 1 36.1 51.7 52.7 44.2 44.8 34.3 35.1\nMNLI 5 51.2 58.1 57.7 55.3 55.2 33.5 46.9\nMNLI 10 60.4 57.8 58.4 62.3 60.5 34.3 54.4\nMNLI 20 64.0 64.7 60.5 69.6 68.6 35.0 41.9\nCB 32 88.7 88.1 88.7 85.7 86.9 60.7 87.6\nRTE 32 70.4 62.5 62.6 52.8 58.8 48.1 67.4\nNLI Avg - 70.7 69.6 69.5 65.5 67.7 47.7 66.5\nOverall Avg - 73.2 72.6 72.3 70.1 71.4 45.1 68.7\nTable 3: Few-shot experiment results (average accuracy) on different datasets with our proposed methods in PET\nSSL setup. For CB and RTE, we use32 training examples, whereas for other datasets we use{1, 5, 10, 20}randomly\nselected examples per class for few-shot learning experiments. The best performance is marked in bold. Note that to\nreport the average results for NLI task, we first average over the MNLI results under different few-shot settings, and\nthen average over the three NLI datasets to give each task equal weight. The overall average results are computed\nfollowing a similar approach, giving each dataset an equal weight.\nPET, our proposed Demo+Soft PET shows an im-\nprovement of average accuracy from 72.3 to 73.2\n(p <0.05 by paired t test) (Hsu and Lachenbruch,\n2014). Moreover, both Demo+Soft and Vary Soft\nPET methods obtain better average performance\nthan the Fixed Soft Tokens PET in NLI and topic\nclassification tasks. These results show the impor-\ntance of diversity introduced by multiple prompt\nlabeler models.\n4 Related Work\n4.1 Language Model Prompting\nCui et al. (2021) authors fine-tuned the pre-trained\ngenerative language model, BART, with a prede-\nfined template (candidate span is a entity type\nentity) for NER classification. Wang et al. (2021)\nproposed Entailment as Few-shot Learner (EFT)\nmethod, which transforms classification tasks into\nnatural language textual entailment tasks and then\nfine-tunes the LM. The transformation also makes\nit easy to leverage unsupervised contrastive data\naugmentation methods to add pairwise examples\nto the limited annotated data. This setting further\nshowed an average 2.7% improvement in 15 dif-\nferent NLP tasks. In addition to using the prompts\nfor supervised learning, PET is the SoTA method\nto adapt the manual prompts along with semi-\nsupervised learning to obtain strong performance\nacross multiple NLU tasks. (Schick and Sch ¨utze,\n2021).\n4.2 Automatic Prompts and Verbalizers\nShin et al. (2020a) used a gradient-guided search\nto find the discrete tokens for prompts based on\ntask accuracy, initialize tokens, and then fine-tune\nthe LM. For automatic label token selection, they\nfirst train a logistic regression classifier from the\ncontextualized embedding of the MASK token and\nthen predict the score from MLM’s output word\nembeddings. They select the top-k highest scoring\nwords for each label. They showed better perfor-\nmance over manual prompting methods for sen-\ntiment classification and textual entailment tasks.\nSimilarly, instead of using a gradient-guided search\nfor prompt tokens, Li and Liang (2021) and Lester\net al. (2021) attached prefix vectors and learned\nthe embeddings for prefix vectors by keeping the\nLM model parameters frozen. Liu et al. (2021c)\nproposed P-tuning, which replaces the input em-\nbeddings of pre-trained language models with its\ndifferentiable output embeddings, using the pat-\n777\ntern based on human design. Liu et al. (2021b)\noptimized and adapted the Prefix Tuning model\nfor NLU. Vu et al. (2021) proposed to learn soft\nprompt embeddings from one or more source tasks\nand then transfer them to initialize the prompts for\nthe target task. In addition, they also proposed an\nefficient retrieval approach to find task embeddings\nand predict the most transfarable source tasks for a\ngiven novel target task.\nSeveral automatic verbalizers, such as search-\nbased verbalizers, soft verbalizers, and prototypi-\ncal verbalizers, have been proposed to automate the\ndesign of the verbalizer mapping function. Search-\nbased verbalizers aim to find the appropriate tokens\nto replace human selection (Schick et al., 2020a;\nShin et al., 2020b; Gao et al., 2020). Both soft ver-\nbalizers and prototypical verbalizers learn trainable\nclass or prototyope embeddings during the train-\ning process (Cui et al., 2022; Zhang et al., 2021;\nHambardzumyan et al., 2021).\nMahabadi et al. (2022) proposed a prompt-free\nmethod (PERFECT) to train the language model,\nwhich does not rely on manual commands and ver-\nbalizers. PERFECT reported performance similar\nto that of PET (Schick and Sch ¨utze, 2021) in a\nfew-shot setting. However, they used a supervised\nlearning setup and compared their results with the\nsingle labeler model with one prompt rather than\nthe results from the final classifier. Here, we use a\nsimilar SSL setting to Schick and Sch¨utze (2021)\nand report the results of the final classifier.\n5 Conclusions\nIn this paper, we are able to successfully use auto-\nmatic prompts and verbalizers in semi-supervised\nlearning settings. We show that our proposed au-\ntomatic prompt generation methods with proto-\ntypical verbalizer can eliminate human engineer-\ning in prompt-based SSL setup and achieve simi-\nlar or better performance than the SoTA Manual\nPET method. Our methods have the added ad-\nvantage of being scalable with multiple tasks and\ndatasets. We also empirically verify the power of\nsemi-supervised learning methods, which take ad-\nvantage of large amounts of unlabeled data, over\nsupervised methods.\nIn the next steps, we plan to investigate whether\nwe would be able to achieve similar performance\nby freezing PLMs’ parameters and only tuning ver-\nbalizer and prompt parameters. This setup will save\na tremendous amount of space by making it easy\nto share and reuse PLMs. Moreover, we plan to ex-\nplore ways to combine the two proposed methods\nDemo+Soft PET and Vary Soft PET, which would\ntake advantage of both methods.\n6 Limitations\nAlthough we experiment with multiple NLU tasks\nand datasets, these datasets are only in the En-\nglish language. Prompt-based learning relies on\nlarge language models, which have acquired knowl-\nedge through pre-training on huge corpora. With\nlow-resource languages, it might be difficult to get\nPLMs trained on a huge corpus, which might make\nit hard to reproduce performance similar to the En-\nglish corpus. The fine-tuning and inference of PLM\nrequires multiple large GPUs, which might not be\naccessible to everyone.\nAcknowledgments\nWe would like to thank the anonymous reviewers\nas well as Wei Ai, Paiheng Xu, Akram Almatarky,\nJangwon Kim, Morteza Ziyadi, and Giannis Kara-\nmanolakis for reviewing the paper and for provid-\ning helpful comments and suggestions.\nReferences\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mixtext:\nLinguistically-informed interpolation of hidden space\nfor semi-supervised text classification. arXiv preprint\narXiv:2004.12239.\nGanqu Cui, Shengding Hu, Ning Ding, Longtao Huang,\nand Zhiyuan Liu. 2022. Prototypical verbalizer for\nprompt-based few-shot tuning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7014–7024, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.\n2021. Template-based named entity recognition us-\ning BART. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n1835–1845, Online. Association for Computational\nLinguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classification,\nand Recognising Tectual Entailment, pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nNing Ding, Shengding Hu, Weilin Zhao, Yulin Chen,\nZhiyuan Liu, Hai-Tao Zheng, and Maosong Sun.\n2021. Openprompt: An open-source framework for\nprompt-learning. arXiv preprint arXiv:2111.01998.\n778\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversarial\nReProgramming. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4921–4933, Online. Association for\nComputational Linguistics.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nHenry Hsu and Peter A Lachenbruch. 2014. Paired t\ntest. Wiley StatsRef: statistics reference online.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,\nJames Bradbury, Ishaan Gulrajani, Victor Zhong, Ro-\nmain Paulus, and Richard Socher. 2016. Ask me\nanything: Dynamic memory networks for natural lan-\nguage processing. In International conference on\nmachine learning, pages 1378–1387. PMLR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021b. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR, abs/2110.07602.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021c. Gpt\nunderstands, too. arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nRabeeh Karimi Mahabadi, Luke Zettlemoyer, James\nHenderson, Marzieh Saeidi, Lambert Mathias,\nVeselin Stoyanov, and Majid Yazdani. 2022. Per-\nfect: Prompt-free and efficient few-shot learning with\nlanguage models. arXiv preprint arXiv:2204.01172.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nProceedings of Sinn und Bedeutung, 23(2):107–124.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language\ndecathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nTimo Schick, Helmut Schmid, and Hinrich Sch ¨utze.\n2020a. Automatically identifying words that can\nserve as labels for few-shot text classification. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 5569–5578,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nTimo Schick, Helmut Schmid, and Hinrich Sch ¨utze.\n2020b. Automatically identifying words that can\nserve as labels for few-shot text classification. arXiv\npreprint arXiv:2010.13641.\nTimo Schick and Hinrich Sch ¨utze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nTimo Schick and Hinrich Sch ¨utze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020a. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceedings\nof the 2020 Conference on Empirical Methods in\n779\nNatural Language Processing (EMNLP), pages 4222–\n4235, Online. Association for Computational Lin-\nguistics.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020b. Auto-\nprompt: Eliciting knowledge from language models\nwith automatically generated prompts. arXiv preprint\narXiv:2010.15980.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and\nDaniel Cer. 2021. Spot: Better frozen model adap-\ntation through soft prompt transfer. arXiv preprint\narXiv:2110.07904.\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao,\nand Hao Ma. 2021. Entailment as few-shot learner.\narXiv preprint arXiv:2104.14690.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. Advances in Neural\nInformation Processing Systems, 33:6256–6268.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\nZhen Bi, Chuanqi Tan, Fei Huang, and Huajun\nChen. 2021. Differentiable prompt makes pre-trained\nlanguage models better few-shot learners. arXiv\npreprint arXiv:2108.13161.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015a.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015b.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nA Prompts and Verbalizers\nA.1 Manual Prompts and Manual Verbalizers\nWe use the same manual prompts and manual ver-\nbalizers for our baseline experiment as used by\nSchick and Sch¨utze (2021, 2020).\nAG’s News is a news topic classification dataset\nwith four classes. We use the manual verbalizer that\nmaps class 1-4 to “World”, “Sports”, “Business”\nand “Technology”. For the input sentence x =\n(a, b), where a is the news headline and b is the\nbody of the news text, we use the manual prompts\nbelow:\nP1(x) =[MASK] : [a] [b]\nP2(x) =[MASK] - [a] [b]\nP3(x) = [a] ([MASK]) [b]\nP4(x) = [a] [b] ([MASK])\nP5(x) =[MASK] News: [a] [b]\nP6(x) =Category : [MASK] [a] [b]\nYahoo Questions is another dataset for topic\nclassification with ten classes. We use the same\nmanual prompts as AG’s News, but define the man-\nual verbalizer for the Yahoo dataset, which maps\nthe classes 1-10 to “Society”, “Science”, “Health”,\n“Education”, “Computer”, “Sports”, “Business”,\n“Entertainment”, “Relationship” and “Politics”.\nMNLI is the dataset for textual entailment tasks,\nconsisting of text pairs x = (a, b). We define\ntwo manual verbalizer pairs v1 and v2. v1 ver-\nbalizer maps class 0-2 to “Wrong”, “Right” and\n“Maybe”. v2 verbalizer maps class 0-2 to “No”,\n“Yes”, “Maybe”. We use the following manual\nprompts:\nP1(x) = “[a]” ? ||[MASK], “[b]”\nP2(x) = [a] ? ||[MASK], [b]\nRTE and CB are datasets for textual entailment\ntasks. We use v1 as the manual verbalizer similar to\nMNLI task. We use the following manual prompts:\nP1(x) = “[a]” ? ||[MASK], “[b]”\nP2(x) = [a] ? ||[MASK], [b]\nP3(x) = [a] ? ||[MASK]. [b]\nP4(x) = “[a]” ? ||[MASK]. “[b]”\nA.2 Continuous Prompts\nFor our proposed models: Demo+Soft and Vary\nSoft models, we apply continuous prompts and\nautomatic verbalizers to ensure that the prompt-\ntuning SSL method can be scaled across multiple\ndatasets. From previous works, we find that few\nanchor tokens help to improve the performance of\nNLU tasks (Liu et al., 2021c), so we design two dif-\nferent continuous prompts dependant on the nature\nof NLU tasks. For the continuous prompt for AG’s\nNews and Yahhoo Questions (text classification\ntask), our design is:\nP(x) = [a] [b] Category: [p0, p1, . . . , pn] [MASK]\n780\nFor continuous prompt for MNLI, CB and RTE\n(NLI tasks), our design is:\nP(x) = [a] [b] ? [p0, p1, . . . , pn] answer : [MASK]\nThe construction of continuous prompts also follow\nthe design of the P-tuning paper (Liu et al., 2021c).\nRather than designing multiple manual prompts for\ndifferent datasets, we can use our proposed meth-\nods to automate this process. This reduces human\nefforts and costs when we scale across multiple\ndatasets and tasks.\n781",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8904018402099609
    },
    {
      "name": "Scalability",
      "score": 0.684910774230957
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5920063257217407
    },
    {
      "name": "Machine learning",
      "score": 0.5143429040908813
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4596220552921295
    },
    {
      "name": "Natural language",
      "score": 0.4369047284126282
    },
    {
      "name": "Natural language understanding",
      "score": 0.43272486329078674
    },
    {
      "name": "Language model",
      "score": 0.41895318031311035
    },
    {
      "name": "Natural language processing",
      "score": 0.398263156414032
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}