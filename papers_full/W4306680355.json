{
  "title": "Transformer-Based Value Function Decomposition for Cooperative Multi-Agent Reinforcement Learning in StarCraft",
  "url": "https://openalex.org/W4306680355",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2151427468",
      "name": "Muhammad Junaid Khan",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2225692531",
      "name": "Syed Hammad Ahmed",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2041536743",
      "name": "Gita Sukthankar",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2151427468",
      "name": "Muhammad Junaid Khan",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2225692531",
      "name": "Syed Hammad Ahmed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2041536743",
      "name": "Gita Sukthankar",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6737849119",
    "https://openalex.org/W2768629321",
    "https://openalex.org/W2121092017",
    "https://openalex.org/W3205068159",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W3104860527",
    "https://openalex.org/W2031748952",
    "https://openalex.org/W3094456479",
    "https://openalex.org/W2794643322",
    "https://openalex.org/W2136202932",
    "https://openalex.org/W2946606218",
    "https://openalex.org/W2807741983",
    "https://openalex.org/W1641379095",
    "https://openalex.org/W2749807327",
    "https://openalex.org/W6781750019",
    "https://openalex.org/W2173564293",
    "https://openalex.org/W6681222830",
    "https://openalex.org/W6800388583",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W2144445939",
    "https://openalex.org/W3194203264",
    "https://openalex.org/W4288594419",
    "https://openalex.org/W4221140674",
    "https://openalex.org/W4287755265",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295598622",
    "https://openalex.org/W4226151089",
    "https://openalex.org/W2949963774",
    "https://openalex.org/W4226237817",
    "https://openalex.org/W3046288222",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W3004640943",
    "https://openalex.org/W2617547828",
    "https://openalex.org/W4299335686",
    "https://openalex.org/W2951799221"
  ],
  "abstract": "The StarCraft II Multi-Agent Challenge (SMAC) was created to be a challenging benchmark problem for cooperative multi-agent reinforcement learning (MARL). SMAC focuses exclusively on the problem of StarCraft micromanagement and assumes that each unit is controlled individually by a learning agent that acts independently and only possesses local information; centralized training is assumed to occur with decentralized execution (CTDE). To perform well in SMAC, MARL algorithms must handle the dual problems of multi-agent credit assignment and joint action evaluation. This paper introduces a new architecture TransMix, a transformer-based joint action-value mixing network which we show to be efficient and scalable as compared to the other state-of-the-art cooperative MARL solutions. TransMix leverages the ability of transformers to learn a richer mixing function for combining the agents' individual value functions. It achieves comparable performance to previous work on easy SMAC scenarios and outperforms other techniques on hard scenarios, as well as scenarios that are corrupted with Gaussian noise to simulate fog of war.",
  "full_text": "Transformer-Based Value Function Decomposition for Cooperative Multi-Agent\nReinforcement Learning in StarCraft\nMuhammad Junaid Khan, Syed Hammad Ahmed, Gita Sukthankar\nDepartment of Computer Science\nUniversity of Central Florida\nOrlando, FL US\n{junaid k, hammad.ahmed}@knights.ucf.edu, gitars@eecs.ucf.edu\nAbstract\nThe StarCraft II Multi-Agent Challenge (SMAC) was cre-\nated to be a challenging benchmark problem for cooperative\nmulti-agent reinforcement learning (MARL). SMAC focuses\nexclusively on the problem of StarCraft micromanagement\nand assumes that each unit is controlled individually by a\nlearning agent that acts independently and only possesses lo-\ncal information; centralized training is assumed to occur with\ndecentralized execution (CTDE). To perform well in SMAC,\nMARL algorithms must handle the dual problems of multi-\nagent credit assignment and joint action evaluation.\nThis paper introduces a new architecture TransMix, a\ntransformer-based joint action-value mixing network which\nwe show to be efficient and scalable as compared to the\nother state-of-the-art cooperative MARL solutions. TransMix\nleverages the ability of transformers to learn a richer mixing\nfunction for combining the agents’ individual value functions.\nIt achieves comparable performance to previous work on easy\nSMAC scenarios and outperforms other techniques on hard\nscenarios, as well as scenarios that are corrupted with Gaus-\nsian noise to simulate fog of war.\nIntroduction\nStarCraft poses many exciting research challenges for AI\nagents (Onta˜n´on et al. 2013), stemming from both the myr-\niad macromanagement (strategic gameplay choices related\nto production and expansion) and micromanagement (tac-\ntics related to movement and targeting) tasks that occur dur-\ning full gameplay. Even though humans typically play Star-\nCraft in a centralized way in which the single human player\ncontrols all the units, the StarCraft II Multi-agent Challenge\ntestbed (Samvelyan et al. 2019) treats engagements as a co-\noperative multi-agent problem in which each AI agent con-\ntrols a single unit and has limited visibility. SMAC pro-\nvides a set of battle scenarios as benchmarks for cooperative\nmulti-agent reinforcement learning (MARL), unlike PySC2\n(Vinyals et al. 2017) which is designed for a single learn-\ning agent controlling all the units. During execution, each\nSMAC agent conditions its decisions on the partially observ-\nable limited area of view; enemy units do not have the op-\nportunity to adapt and are controlled by the built-in heuristic\ncontroller. Scenarios include the initial positions, count, and\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ntypes of units, as well as terrain feature information. Some\nof the SMAC scenarios are designed to encourage learning\nagents to acquire a micro-trick such as kiting or focusing\nfire. A scenario can be symmetric or asymmetric, based on\nthe unit counts of allies and enemies, and homogeneous or\nheterogeneous according to the unit types of each side.\nMulti-agent reinforcement learning is significantly more\nchallenging than single agent tasks due to the problem\nof credit assignment: it’s difficult for agents to determine\nwhether it was its own action selection that yielded rewards\nor another agent’s choices. A na ¨ıve solution is to concate-\nnate each agent’s individual state-action spaces and treat it\nas a single-agent problem (Gupta, Egorov, and Kochender-\nfer 2017). Both learning and execution are centralized and\na single joint reward is learnt which cannot be decomposed\ninto independent agents’ contributions. The primary concern\nwith this approach is the curse of dimensionality, as it cre-\nates a drastic growth in the total number of unique global\nstates resulting in exponential space and time complexity.\nWith a decentralized paradigm, agents independently\ncondition only on their local observations without inter-\nagent communication and without a joint action-value func-\ntion (Tan 1993). Due to its decentralized nature, parallelism\ncan be exploited, leading to faster learning but with no con-\nvergence guarantees. These limitations can be addressed\nby centralizing the learning and decentralizing each agent’s\nexecution — known as Centralized Training with Decen-\ntralized Execution (CTDE) (Oliehoek, Spaan, and Vlassis\n2011).\nThe release of the SMAC environment has spurred a burst\nof innovation in multi-agent value learning algorithms, such\nas QMIX (Rashid et al. 2018), QPLEX (Wang et al. 2021),\nQTran (Son et al. 2019), and Qatten (Yang et al. 2020). A\nkey distinction between these algorithms is the credit as-\nsignment process which factorizes joint action-values into\nindividual action-values for every agent. To do this many\nMARL techniques make limiting assumptions about the\nvalue function to facilitate the learning process. Adhering\nto the IGM (Individual-Global-Max) principle (Son et al.\n2019) avoids incompatible agent policies: an action selected\nusing the joint action-value function should be equivalent to\nthe greedy action selections of individual agents.\nProceedings of the Eighteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment\n(AIIDE 2022)\n113\nThis paper introduces a transformer-based mixing ap-\nproach, TransMix1, for joint action-value learning in coop-\nerative MARL. TransMix is able to extract global as well\nas local contextual interaction amongst individual agent Q-\nvalues, agent histories, and global state information, and can\ndeal with longer time horizons which helps it achieve better\nperformance on the hard SMAC scenarios, as well as sce-\nnarios that are corrupted with Gaussian noise. The next sec-\ntion presents background on the mathematical notation used\nthroughout the rest of the paper.\nBackground\nDecentralized Partially Observable MDP (Dec-\nPOMDP): MARL tasks are typically modeled as a\nDec-POMDP (Oliehoek, Spaan, and Vlassis 2011), de-\nscribed by the tuple M = <N, S, A, P, r, Ω, O, n, γ>\nwhere i ∈ N = {1, 2, 3, ...} is the set of agents while s ∈ S\nrepresents the true or global state in the environment. For\nevery time step, agent i ∈ N selects an action ai ∈ A ≡ An\nbased on state s. When the joint action a is executed, a\njoint reward r(s, a) is received, resulting in a transition to\nthe next state s′ based on a transition probability function\nP(s\n′\n|s, a) with a discount factor γ[0, 1).\nTo make it a partially observable process, each agent i\nreceives an observation oi ∈ Ω based on the observation\nprobability function O(oi|s, ai). In addition, not only does\neach agent maintain an action-observation history function\nτi ∈ τ ≡ (Ω × A∗), it conditions its stochastic policy\nπi(ai|τi) on this history as well. The agents seek to achieve a\njoint policy Π that maximizes the joint value functionVπ(s)\nas well as joint action-value function Qπ(s, a).\nDeep Q-Learning: Q-learning algorithms maximize\nthe action value function Q∗(s, a) = r(s, a) +\nγEs′[max\n′\na Q∗(s\n′\n, a\n′\n)]. In deep Q-learning, this function is\nlearned using a deep neural network with parametersθ. This\nidea of deep Q-networks was first introduced by Mnih et al.\n(2015) who used replay memory to store transition tuples of\nthe form (s, a, r, s\n′\n), where r is the reward of taking action\na resulting in a state transition from s to s\n′\n. The network\nparameters, θ, are optimized by sampling batches from this\nreplay memory in order to eliminate correlations in the tuple\nsequence.\nStandard DQN-based approaches utilize the temporal dif-\nference (TD) loss to optimize the network, given by the\nequation\nL(θ) =\nbX\ni=1\n[(yDQN\ni − Q(s, a; θ)2] (1)\nwhere yDQN\ni represents the target network which is updated\nat regular intervals (rather than at each iteration) andb is the\nbatch size sampled from the replay buffer.\nIn order to deal with the problem of partial observability,\na recurrent version of the Q-network (DQRN) (Hausknecht\n1Code is available at: https://github.com/junaiddk/transmix\nand Stone 2015), has been used where a recurrent neural net-\nwork (RNN) based agent learns temporal information after\neach state transition. In this case, the replay memory stores\nthe joint action-observation history tuple(τ, a, r, τ\n′\n) and the\nQ-value is calculated using Q(τ, a; θ) instead of Q(s, a; θ).\nTraining and Execution: Recent MARL value factor-\nization techniques benefit from the usage of a Centralized\nTraining and Decentralized Execution (CTDE) paradigm\n(Oliehoek, Spaan, and Vlassis 2011). Tan (1993) demon-\nstrated that by sharing individual observations and poli-\ncies, independent agents learn a cooperative task substan-\ntially quicker at the cost of communication and space\noverhead. Without cooperation, the independent Q-learners\nmay not converge even if exhaustive exploration is as-\nsumed. The Centralized Training with Decentralized Execu-\ntion paradigm addressed the issues with independent learn-\ning agents (Russell and Zimdars 2003; Tan 1993) and fully-\ncentralized learning approaches (Foerster et al. 2018; Gupta,\nEgorov, and Kochenderfer 2017) by avoiding misleading\nagent reward assignments, and eliminating the need for com-\nbined action and observation spaces.\nIn CTDE, agents are trained in a central setting where\neach agent has access to global state information. At train-\ning time, each agent aims to maximize its own action-value\nfunction which leads to the maximization of the team’s joint\naction-value function. At execution time each agent selects\nits action based on its own learned action-value functions.\nThe Individual-Global Max (IGM) concept was introduced\nby Son et al. (2019) who state that the optimal joint actions\nof agents are dependent upon optimal actions of individual\nagents i.e.,\nargmaxaQtot(τ, a) =\n\n\nargmaxa1 Q1(τ1, a1)\nargmaxa2 Q2(τ2, a2)\n..\n.\nargmaxaN QN (τN , aN )\n\n (2)\nValue decomposition networks (Sunehag et al. 2018) cal-\nculate Qtot by summing up the individual Qi of each agents\nwhile QMIX (Rashid et al. 2018) applies a monotonicity\nconstraint. These simplifications to the factorization process\nfacilitate learning but are unable to represent some classes\nof joint action-value functions. Our research attempts to ad-\ndress this problem through the usage of a more complex\ntransformer-based mixing function.\nRelated Work\nPlaying a full game of StarCraft requires creating an AI\nthat can rapidly make both strategic (macromanagement)\nand tactical (micromanagement) decisions against oppo-\nnents shrouded by fog of war on a large changing map\n(see Onta˜n´on et al. (2013) for a comprehensive overview of\nStarCraft research challenges). Much of the recent research\non reinforcement learning in StarCraft II has been con-\nducted on micromanagement scenarios executed using either\nthe StarCraft II Learning Environment (SC2LE) (Vinyals\n114\net al. 2017) or the StarCraft II Multi-agent Challenge\ntestbed (Samvelyan et al. 2019), which is built on top of\nSC2LE. SMAC models battles as being executed by mul-\ntiple cooperative agents. Rather than tackling the problem\nof learning policies for an entire game which requires sig-\nnificant computational power (e.g., such as what was used\nfor DeepMind’s AlphaStar system (Vinyals et al. 2019)),\nSMAC evaluates the ability of cooperative agents to learn\nmicromanagement policies for a single battle. The PyMARL\nframework was also released as part of SMAC to facili-\ntate the development and performance comparisons of newly\nproposed RL algorithms with other popular implementa-\ntions (Sunehag et al. 2018; Rashid et al. 2018, 2020; Son\net al. 2019; Wang et al. 2021). The next section provides an\noverview of recent work on RL for SMAC.\nMARL in StarCraft\nValue decomposition networks (VDN) (Sunehag et al. 2018)\nintroduced the seminal idea of learning a unified reward\nvalue and factorizing it into agent-wise reward values; the\nalgorithm assumes that the joint value function can be addi-\ntively decomposed into individual value functions for each\nagent. Since VDN only considers linear value functions,\nit does not perform well for complex benchmarks like the\nSMAC scenarios that have many agent interdependencies\n(Samvelyan et al. 2019). Also unlike later MARL algo-\nrithms, VDN does not leverage global state information.\nIn QMIX, Rashid et al. (2018), improved on VDN by in-\ncorporating global state information during training and sup-\nporting a broader range of non-linear value functions. The\nQMIX architecture includes a mixing network conditioned\non the state information which improves the representational\ncomplexity of the value functions. The monotonicity of the\njoint value function is ensured through the usage of positive\nweights and individual agent networks conditioned on lo-\ncal observations only. However QMIX’s loss function tries\nto minimize loss across all joint actions for every state; this\ncan result in incorrect argmax selections when decomposing\nnon-monotonic functions. Weighted QMIX (Rashid et al.\n2020) fixes some issues with the original architecture by as-\nsigning weights to the projected joint action-values instead\nof using equal weights as was done in the original QMIX pa-\nper; these weight assignments can either be done centrally or\noptimistically.\nSon et al. (2019) identified some limitations with the\nusage of additivity (VDN) and monotonicity (QMIX) con-\nstraints. They emphasize that an effective factorization of\noptimal joint action-values must instead satisfy the IGM\n(Individual-Global-Max) property such that the best joint\naction-value is the aggregation of individual agents’ optimal\naction-values. Their algorithm QTRAN uses an additional\nstate-value network to compete a scalar value representing\nthe state. This value helps minimize the difference in the\ntotal joint reward and the sum of each agent’s individual re-\nwards, and therefore supports a variety of non-monotonic\njoint action-value functions. QPLEX (Wang et al. 2021) en-\nsures compliance to the IGM principle by introducing an\nadvantage-based function realized with a duplex dueling ar-\nchitecture (Wang et al. 2016); it is one of the best performing\nSMAC algorithms. In this paper, we benchmark our work\nagainst both QMIX and QPLEX.\nAs the number of agents in a multi-agent problem in-\ncreases, the role of context assumes greater importance since\nthe agent must consider the states of other agents, but only\na small subset of the other agents are relevant to the task\non hand. To overcome this problem, Yang et al. (2020)\nproposed incorporating a multi-headed attention module\ninto the mixing network used for joint Q-value estimation.\nUnlike convolutional and recurrent architectures, attention\nmechanisms can model far-flung dependencies in input and\noutput sequences (Vaswani et al. 2017). Instead of propos-\ning enhancements to the mixing of agent network structures,\nHao et al. (2022) sought to scale up to larger multi-agent\nproblems by improving input sample efficiency. They real-\nized that in a system with a small set of cooperating agents,\nsampling all permutations of their ordered representations is\nredundant, hence the training sample number can be reduced\nby forcing agent-ordering to be permutation invariant.\nOur work leverages the transformer model which relies\nexclusively on attention to learn data dependencies; also it\ncan be designed to support permutation invariance. Khan,\nHassan, and Sukthankar (2021) demonstrated promising re-\nsults on the usage of transformer networks for macroman-\nagement task prediction in StarCraft II; however, their work\nwas done on a static dataset using supervised learning.\nUnfortunately the basic transformer architecture has been\nshown to be unstable with the shifting RL objective func-\ntion (Parisotto et al. 2020) so embedding transformers into\nan online RL agent requires careful training. Fine-tuning\nan offline-learnt transformer with online RL training can\nimprove both overall agent performance and sample effi-\nciency (Zheng, Zhang, and Grover 2022).\nMethod\nThis paper introduces our transformer-based value decom-\nposition and mixing network, TransMix. Our transformer\ndesign is inspired by the approach of Fastformer (Wu et al.\n2021). Each agent is represented by a GRU-based DQRN\nnetwork. At every time step, agents receive observations\noi\nt ∈ Ω and previous action ai\nt−1 ∈ A. Based on oi\nt and ai\nt−1\nthe agent network estimates the individual Qi(τi, ai) where\nτ is the action-observation history and selects the next ac-\ntions for each agent by following an ϵ-greedy policy.\nThe transformer encoder consists of 2 to 6 transformer\nlayers. The number of transformer layers or depth is based\non the complexity of the task at hand. Once a batch of data\nis ready, it is fed to the transformer which ingests individual\nQi, action-observation histories ht\ni and global states St.\nWe calculate the query vector, Q, from the global states\nby applying linear transformation and self-attention. This\noperation ensures that the most important states get atten-\ntion and contribute more towards the global context in learn-\ning the Qtot. Similarly, we apply a separate linear trans-\nformation to individual Qi and the resultant is then multi-\n115\nFigure 1: Complete architecture of TransMix. (a) is the transformer encoder. (b) is the overall architecture of the TransMix. (c)\nis the GRU-based agent network. A stack of 2 to 6 transformer encoder layers are used based on the complexity of the task.\nplied element-wise with Q vector which gives a global con-\ntext aware key vector between St and Qi. Next, we apply\nself-attention to extract the most relevant information in the\nform of global K vector. This K vector is then element-wise\nmultiplied with transformed ht\ni that goes through further lin-\near transformation to produce global context awareV vector.\nLastly, the transformed states and V vectors are aggregated\ntogether to estimate the Qtot. The complete TransMix archi-\ntecture is presented in Fig. 1.\nFollowing the design guidelines of Fastformer, we also\nmake use of additive self-attention for calculating K and Q\nvectors rather than the standard self-attention proposed by\nthe vanilla transformer design (Vaswani et al. 2017). The\nbenefits of additive self-attention are many fold. Firstly this\nattention mechanism is rather light weight. Second, it re-\nduces the total complexity of transformer encoder. Finally,\nit improves the inference time of the transformer as well.\nThe matrices K, V, and Q ∈ RN×d, where N ∈R512 is\nthe sequence length or the embedded dimension of the trans-\nformation layers and d ∈ R2048 is the hidden dimension. We\nalso utilize 4 attention heads for our TransMix network. The\nlinear transformations with sequence length or hidden di-\nmension R512 ensure that we can provide a fixed length in-\nput to transformer layers which helps to keep the complexity\nand computation under control.\nWe train TransMix in online data collection settings in an\nend-to-end fashion. The network is optimized with an Adam\noptimizer and standard TD loss (Equation 1). The learning\nrate is set to 0.001 for most of the SMAC scenarios with\nβ1 = 0.9 and β2 = 0.999, and batch size is set to 96. We\nalso utilize a skip connection that consists of concatenated\nQi and ht\ni which is passed through a bottleneck linear trans-\nformation. This bottleneck skip connection provides train-\ning stability and helps the network converge while reduc-\ning the dimensionality of the concatenated matrices. Unlike\nmany other methods, our approach does not use hypernet-\nwork generated weights extracted from global states. This\nmakes our approach less dependent on global states com-\npared to others.\nResults\nWe evaluate TransMix on both easy and hard scenarios from\nthe SMAC (Samvelyan et al. 2019) challenge benchmark. In\nSMAC, the focus is on micromanagement tasks in SC2 such\nas unit battles. During the battles, we train our RL agents in\nan online fashion while the opponents are controlled by the\nbuilt-in AI. A battle is considered as “win” if the ally unit\nkills an opponent unit. On the other hand a “loss” occurs if\nthe opponent kills an ally unit or the maximum number of\nepisodes are reached. When an RL agent damages the oppo-\nnent, a reward is received proportional to damage done. Sim-\nilarly, a reward of 10 is received for killing an opponent’s\nagent; winning the battle accumulates a reward of 200. The\ndetails of SMAC scenarios are provided in Table 3.\nTraining and Evaluation\nWe follow the same evaluation metric proposed by Rashid\net al. (2018). For each SMAC map, the experiment is re-\n116\nMaps Difficulty TransMix QMIX QPLEX\n2m vs 1z Easy 99. 7 95.3 100\n3m Easy 100 96.9 100\n8m Easy 100 97.7 100\n2s3z Easy 100 88.25 100\n1c3s5z Easy 97.62 93.37 97.2\nMMM Easy 100 95.3 96.9\nbane vs bane Easy 100 89.21 98.47\n3s vs 5z Hard 95.91 88.7 99.1\n3s5z Hard 96.68 88.3 95.9\n5m vs 6m Hard 77.41 69.2 73.4\n8m vs 9m Hard 96.88 92.2 87.5\n2c vs 64zg Hard 92.62 84.38 91.2\n10m vs 11m Hard 91.77 89.2 90.12\nTable 1: Median win rate on SMAC maps. We train our network for 2M training timesteps on each map. TransMix outperforms\nQMIX and QPLEX on most hard SMAC scenarios and ties on easy scenarios.\npeated 5 times. We train the model for 2M timesteps on all\nthe scenarios with a replay buffer capacity of 5000 episodes;\nlinear ϵ annealing from 1.0 to 0.05 is performed over 50k\nsteps for easy maps while for the hard maps the range varies\nfrom 250k to 500k steps. After every 10k timesteps, we\npause the training and test our method for 20 test episodes in\na decentralized fashion. Moreover, after every 200 episodes,\nwe update our target network parameters.\nThe main results are summarized in Table 1. Since our\napproach is based on transformer, which is highly scalable,\nwe leverage the built-in parallel episode runner environment\nfor training and testing which substantially reduces the train-\ning time for TransMix. We benchmark our work against both\nQMIX (Rashid et al. 2018) and QPLEX (Wang et al. 2021)\nwhich provide a good illustration of the performance of our\nmethod. QMIX is one of the oldest methods to be success-\nful at SMAC, whereas QPLEX is a recent top performer\nthat incorporates all the value function decomposition im-\nprovements proposed by earlier authors. Our method always\noutperforms QMIX and performs better than QPLEX in the\nmajority of the hard scenarios.\nEffects of Noisy States\nThis paper also examines whether the usage of the trans-\nformer makes the MARL agents more robust to fog of\nwar by injecting noise into the global state (Weber and\nMateas 2009). For these experiments, we add Gaussian\nnoise, N(0, 0.05), to global states and train our model,\nQMix and QPlex from scratch. We follow the same training\napproach as discussed earlier and record the win rate. We\nobserve that our method is less prone to noisy states com-\npared to QMIX and QPlex, and that the performance drop\nof our method is less significant compared to others on the\nsame maps without noisy states. These results are reported\nin Fig. 2 and Table 3. As the complexity of the map grows,\nthe performance drop becomes more significant.\nMaps\nTransMix QMIX QPlex\n3m 99.82 95.31 97.41\n8m 96.87 93.75 93.75\n2s3z 93.75 81.67 89.15\n1c3s5z 93.75 78 83.88\n3s5z 84.18 65.76 72.62\nTable 2: Noisy global state evaluation. TransMix conclu-\nsively outperforms QMIX and QPlex when the global states\nare corrupted by noise.\nDiscussion\nThe key to our method, TransMix, is the ability to learn the\ncomplex global and local contextual interaction amongst in-\ndividual agents’ Q-values, Qi, the agents’ histories, ht\ni, and\nthe global states, St. The transformer correctly learns the\ncontext surrounding the correct choice of action, while re-\nmaining robust to noisy global states.\nAnother aspect of our method is that the transformers are\nby design permutation invariant and thus do not depend on\nordering. For instance, QMIX has to maintain the individual\nvalue function order which we do not require at all. While\nTransMix achieved a better test win rate on hard scenarios\nand comparable performance on easy scenarios, like other\nSMAC benchmarks, it performs poorly on super hard sce-\nnarios.\nThe policies learnt by our method are very interesting.\nFor instance, for the MMM map, the approach is aggressive.\nEach side has a heterogeneous team of 1 medivac, 2 mau-\nraders, and 7 marines. The medivac agent remains behind\nall other agents while the marauders take the lead since they\nhave heavy armors, and marines cover up the marauders.\n117\nFigure 2: Win rate comparison between TransMix (labeled as TMix), QPlex, and QMIX. The top row represents win rate with\nregular global states while bottom row represents win rate with noisy global states.\nMap Ally Unit Enemy Unit\n2m vs 1z 2 Marines 1 Zealot\n3m 3 Marines 3 Marines\n8m 8 Marines 8 Marines\n2s3z 2 Stalkers and\n3 Zealots\n2 Stalkers and\n3 Zealots\n1c3s5z 1 Colossi, 3\nStalkers and 5\nZealots\n1 Colossi, 3\nStalkers and 5\nZealots\nMMM 1 Medivac, 2\nMarauders and\n7 Marines\n1 Medivac, 2\nMarauders and\n7 Marines\nbane vs bane 20 Zerglings\nand 4\nBanelings\n20 Zerglings\nand 4\nBanelings\n3s vs 5z 3 Stalkers 5 Zealots\n3s5z 3 Stalkers and\n5 Zealots\n3 Stalkers and\n5 Zealots\n5m vs 6m 5 Marines 6 Marines\n8m vs 9m 8 Marines 9 Marines\n2c vs 64zg 2 Colossi 64 Zerglings\n10m vs 11m 10 Marines 11 Marines\nTable 3: Details of SMAC Scenarios\nThis demonstrates that the transformer is good at learning\nthe appropriate role mapping for different types of units.\nConclusion\nThis paper introduces TransMix, a value decomposition and\nmixing network for cooperative MARL tasks. TransMix\nuses a stack of transformer encoder layers trained in an end-\nto-end way, learning in a centralized fashion while executing\nthe learned policies in a totally decentralized fashion. Our\nmethod is capable of representing non-linear value decom-\nposition functions while maintaining consistency.\nResults show that our method always outperforms QMIX\nand bests QPLEX on the majority of the hard StarCraft\nII Multi-agent Challenge scenarios. Furthermore, TransMix\ncan still achieve good performance when the global states\nare perturbed with Gaussian noise, unlike QMIX. In future\nwork we seek to improve the performance on super hard sce-\nnarios by improving the exploration policy.\nAcknowledgments\nResearch was partially sponsored by the Army Research Of-\nfice and was accomplished under Cooperative Agreement\nNumber W911NF-21-2-0103. The views and conclusions\ncontained in this document are those of the authors and\nshould not be interpreted as representing the official policies,\neither expressed or implied, of the Army Research Office or\nthe U.S. Government. The U.S. Government is authorized to\nreproduce and distribute reprints for Government purposes\nnotwithstanding any copyright notation herein.\n118\nReferences\nFoerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and\nWhiteson, S. 2018. Counterfactual multi-agent policy gra-\ndients. In Proceedings of the AAAI Conference on Artificial\nIntelligence.\nGupta, J. K.; Egorov, M.; and Kochenderfer, M. 2017. Co-\noperative Multi-agent Control Using Deep Reinforcement\nLearning. In Sukthankar, G.; and Rodriguez-Aguilar, J. A.,\neds., Autonomous Agents and Multiagent Systems, 66–83.\nCham: Springer International Publishing.\nHao, X.; Wang, W.; Mao, H.; Yang, Y .; Li, D.; Zheng, Y .;\nWang, Z.; and Hao, J. 2022. API: Boosting Multi-Agent Re-\ninforcement Learning via Agent-Permutation-Invariant Net-\nworks. arXiv preprint arXiv:2203.05285.\nHausknecht, M. J.; and Stone, P. 2015. Deep Recurrent Q-\nLearning for Partially Observable MDPs. In AAAI Fall Sym-\nposia.\nKhan, M. J.; Hassan, S.; and Sukthankar, G. 2021. Leverag-\ning Transformers for StarCraft Macromanagement Predic-\ntion. In IEEE International Conference on Machine Learn-\ning and Applications (ICMLA), 1229–1234.\nMnih, V .; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness,\nJ.; Bellemare, M. G.; Graves, A.; Riedmiller, M. A.; Fidje-\nland, A.; Ostrovski, G.; Petersen, S.; Beattie, C.; Sadik, A.;\nAntonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg,\nS.; and Hassabis, D. 2015. Human-level control through\ndeep reinforcement learning. Nature, 518: 529–533.\nOliehoek, F. A.; Spaan, M. T. J.; and Vlassis, N. 2011. Opti-\nmal and Approximate Q-value Functions for Decentralized\nPOMDPs. CoRR, abs/1111.0062.\nOnta˜n´on, S.; Synnaeve, G.; Uriarte, A.; Richoux, F.;\nChurchill, D.; and Preuss, M. 2013. A Survey of Real-Time\nStrategy Game AI Research and Competition in StarCraft.\nIEEE Transactions onComputational Intelligence and AI in\nGames, 5(4): 293–311.\nParisotto, E.; Song, F.; Rae, J.; Pascanu, R.; Gulcehre, C.;\nJayakumar, S.; Jaderberg, M.; Kaufman, R. L.; Clark, A.;\nNoury, S.; et al. 2020. Stabilizing transformers for rein-\nforcement learning. InInternational Conference on Machine\nLearning, 7487–7498. PMLR.\nRashid, T.; Farquhar, G.; Peng, B.; and Whiteson, S. 2020.\nWeighted QMIX: Expanding monotonic value function\nfactorisation for deep multi-agent reinforcement learning.\nAdvances in Neural Information Processing Systems, 33:\n10199–10210.\nRashid, T.; Samvelyan, M.; de Witt, C. S.; Farquhar, G.; Fo-\nerster, J. N.; and Whiteson, S. 2018. QMIX: Monotonic\nValue Function Factorisation for Deep Multi-Agent Rein-\nforcement Learning. CoRR, abs/1803.11485.\nRussell, S. J.; and Zimdars, A. 2003. Q-decomposition for\nreinforcement learning agents. In Proceedings of the 20th\nInternational Conference on Machine Learning (ICML-03),\n656–663.\nSamvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;\nNardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Foer-\nster, J.; and Whiteson, S. 2019. The StarCraft multi-agent\nchallenge. arXiv preprint arXiv:1902.04043.\nSon, K.; Kim, D.; Kang, W. J.; Hostallero, D. E.; and Yi, Y .\n2019. QTRAN: Learning to factorize with transformation\nfor cooperative multi-agent reinforcement learning. In In-\nternational Conference on Machine Learning, 5887–5896.\nPMLR.\nSunehag, P.; Lever, G.; Gruslys, A.; Czarnecki, W. M.; Zam-\nbaldi, V .; Jaderberg, M.; Lanctot, M.; Sonnerat, N.; Leibo,\nJ. Z.; Tuyls, K.; and Graepel, T. 2018. Value-Decomposition\nNetworks For Cooperative Multi-Agent Learning Based On\nTeam Reward. In Proceedings of the International Con-\nference on Autonomous Agents and MultiAgent Systems ,\n2085–2087.\nTan, M. 1993. Multi-agent reinforcement learning: Indepen-\ndent vs. cooperative agents. In Proceedings of the Interna-\ntional Conference on Machine Learning, 330–337.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need.Advances in Neural Information Pro-\ncessing Systems, 30.\nVinyals, O.; Babuschkin, I.; Czarnecki, W. M.; Mathieu, M.;\nDudzik, A.; Chung, J.; Choi, D. H.; Powell, R.; Ewalds,\nT.; Georgiev, P.; et al. 2019. Grandmaster level in Star-\nCraft II using multi-agent reinforcement learning. Nature,\n575(7782): 350–354.\nVinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhn-\nevets, A. S.; Yeo, M.; Makhzani, A.; K ¨uttler, H.; Agapiou,\nJ. P.; Schrittwieser, J.; Quan, J.; Gaffney, S.; Petersen, S.;\nSimonyan, K.; Schaul, T.; van Hasselt, H.; Silver, D.; Lilli-\ncrap, T. P.; Calderone, K.; Keet, P.; Brunasso, A.; Lawrence,\nD.; Ekermo, A.; Repp, J.; and Tsing, R. 2017. StarCraft\nII: A New Challenge for Reinforcement Learning. CoRR,\nabs/1708.04782.\nWang, J.; Ren, Z.; Liu, T.; Yu, Y .; and Zhang, C. 2021.\nQPLEX: Duplex Dueling Multi-Agent Q-Learning. In In-\nternational Conference on Learning Representations, ICLR.\nWang, Z.; Schaul, T.; Hessel, M.; Hasselt, H.; Lanctot, M.;\nand Freitas, N. 2016. Dueling network architectures for deep\nreinforcement learning. In International Conference on Ma-\nchine Learning, 1995–2003. PMLR.\nWeber, B. G.; and Mateas, M. 2009. A Data Mining Ap-\nproach to Strategy Prediction. In Proceedings of the In-\nternational Conference on Computational Intelligence and\nGames, 140–147. IEEE Press. ISBN 9781424448142.\nWu, C.; Wu, F.; Qi, T.; and Huang, Y . 2021. Fast-\nformer: Additive Attention Can Be All You Need. ArXiv,\nabs/2108.09084.\nYang, Y .; Hao, J.; Liao, B.; Shao, K.; Chen, G.; Liu, W.;\nand Tang, H. 2020. Qatten: A general framework for co-\noperative multiagent reinforcement learning. arXiv preprint\narXiv:2002.03939.\nZheng, Q.; Zhang, A.; and Grover, A. 2022. Online Decision\nTransformer. arXiv preprint arXiv:2202.05607.\n119",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8442781567573547
    },
    {
      "name": "Computer science",
      "score": 0.6871414184570312
    },
    {
      "name": "Transformer",
      "score": 0.624108076095581
    },
    {
      "name": "Scalability",
      "score": 0.5887854695320129
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4695388078689575
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.46263664960861206
    },
    {
      "name": "Architecture",
      "score": 0.4113907516002655
    },
    {
      "name": "Computer engineering",
      "score": 0.38600146770477295
    },
    {
      "name": "Machine learning",
      "score": 0.3372753858566284
    },
    {
      "name": "Engineering",
      "score": 0.21367457509040833
    },
    {
      "name": "Electrical engineering",
      "score": 0.08778637647628784
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106165777",
      "name": "University of Central Florida",
      "country": "US"
    }
  ],
  "cited_by": 14
}