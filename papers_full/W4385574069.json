{
  "title": "HEGEL: Hypergraph Transformer for Long Document Summarization",
  "url": "https://openalex.org/W4385574069",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2122442786",
      "name": "Haopeng Zhang",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2097346119",
      "name": "Xiao Liu",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2103988011",
      "name": "Jiawei Zhang",
      "affiliations": [
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3198534342",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2998559045",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W4287798538",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W3101233295",
    "https://openalex.org/W2161068821",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4230597592",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W3158986179",
    "https://openalex.org/W3117801652",
    "https://openalex.org/W2970263339",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W3034353423",
    "https://openalex.org/W2964144561",
    "https://openalex.org/W2962972512",
    "https://openalex.org/W4284883198",
    "https://openalex.org/W3104433120",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W3106229813",
    "https://openalex.org/W3192144552",
    "https://openalex.org/W3109468313",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4285182336",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2952138241",
    "https://openalex.org/W2081527784",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W2148374900",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2950342809",
    "https://openalex.org/W4205897796"
  ],
  "abstract": "Extractive summarization for long documents is challenging due to the extended structured input context. The long-distance sentence dependency hinders cross-sentence relations modeling, the critical step of extractive summarization. This paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure. We validate HEGEL by conducting extensive experiments on two benchmark datasets, and experimental results demonstrate the effectiveness and efficiency of HEGEL.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10167–10176\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nHEGEL: Hypergraph Transformer for Long Document Summarization\nHaopeng Zhang, Xiao Liu, Jiawei Zhang\nIFM Lab, Department of Computer Science, University of California, Davis, CA, USA\nhaopeng,xiao,jiawei@ifmlab.org\nAbstract\nExtractive summarization for long documents\nis challenging due to the extended structured\ninput context. The long-distance sentence de-\npendency hinders cross-sentence relations mod-\neling, the critical step of extractive summa-\nrization. This paper proposes HEGEL , a hy-\npergraph neural network for long document\nsummarization by capturing high-order cross-\nsentence relations. HEGEL updates and learns\neffective sentence representations with hyper-\ngraph transformer layers and fuses different\ntypes of sentence dependencies, including la-\ntent topics, keywords coreference, and section\nstructure. We validate HEGEL by conduct-\ning extensive experiments on two benchmark\ndatasets, and experimental results demonstrate\nthe effectiveness and efficiency of HEGEL .\n1 Introduction\nExtractive summarization aims to generate a\nshorter version of a document while preserving the\nmost salient information by directly extracting rel-\nevant sentences from the original document. With\nrecent advances in neural networks and large pre-\ntrained language models (Devlin et al., 2018; Lewis\net al., 2019), researchers have achieved promis-\ning results in news summarization (around 650\nwords/document) (Nallapati et al., 2016a; Cheng\nand Lapata, 2016; See et al., 2017; Zhang et al.,\n2022; Narayan et al., 2018; Liu and Lapata, 2019).\nHowever, these models struggle when applied to\nlong documents like scientific papers. The input\nlength of a scientific paper can range from 2000 to\n7,000 words, and the expected summary (abstract)\nis more than 200 words compared to 40 words in\nnews headlines.\nScientific paper extractive summarization is\nhighly challenging due to the long structured in-\nput. The extended context hinders sequential mod-\nels like RNN from capturing sentence-level long-\ndistance dependency and cross-sentence relations,\nFigure 1: An illustration of modeling cross-sentence re-\nlations from section structure, latent topic, and keyword\ncoreference perspectives.\nwhich are essential for extractive summarization.\nIn addition, the quadratic computation complexity\nof attention with respect to the input tokens length\nmakes Transformer (Vaswani et al., 2017) based\nmodels not applicable. Moreover, long documents\ntypically cover diverse topics and have richer struc-\ntural information than short news, which is difficult\nfor sequential models to capture.\nAs a result, researchers have turned to graph\nneural network (GNN) approaches to model cross-\nsentence relations. They generally represent a doc-\nument with a sentence-level graph and turn extrac-\ntive summarization into a node classification prob-\nlem. These work construct graph from document\nin different manners, such as inter-sentence cosine\nsimilarity graph in (Erkan and Radev, 2004; Dong\net al., 2020), Rhetorical Structure Theory (RST)\ntree relation graph in (Xu et al., 2019), approximate\ndiscourse graph in (Yasunaga et al., 2017), topic-\nsentence graph in (Cui and Hu, 2021) and word-\ndocument heterogeneous graph in (Wang et al.,\n2020). However, the usability of these approaches\n10167\nis limited by the following two aspects: (1) These\nmethods only model the pairwise interaction be-\ntween sentences, while sentence interactions could\nbe triadic, tetradic, or of a higher-order in natu-\nral language (Ding et al., 2020). How to capture\nhigh-order cross-sentence relations for extractive\nsummarization is still an open question. (2) These\ngraph-based approaches rely on either semantic\nor discourses structure cross-sentence relation but\nare incapable of fusing sentence interactions from\ndifferent perspectives. Sentences within a docu-\nment could have various types of interactions, such\nas embedding similarity, keywords coreference,\ntopical modeling from the semantic perspective,\nand section or rhetorical structure from the dis-\ncourse perspective. Capturing multi-type cross-\nsentence relations could benefit sentence repre-\nsentation learning and sentence salience modeling.\nFigure 1 is an illustration showing different types of\nsentence interactions provide different connectiv-\nity for document graph construction, which covers\nboth local and global context information.\nTo address the above issues, we propose HEGEL\n(HypErGraph transformer for Extractive Long doc-\nument summarization), a graph-based model de-\nsigned for summarizing long documents with rich\ndiscourse information. To better model high-order\ncross-sentence relations, we represent a document\nas a hypergraph, a generalization of graph struc-\nture, in which an edge can join any number of ver-\ntices. We then introduce three types of hyperedges\nthat model sentence relations from different per-\nspectives, including section structure, latent topic,\nand keywords coreference, respectively. We also\npropose hypergraph transformer layers to update\nand learn effective sentence embeddings on hyper-\ngraphs. We validate HEGEL by conducting exten-\nsive experiments and analyses on two benchmark\ndatasets, and experimental results demonstrate the\neffectiveness and efficiency of HEGEL . We high-\nlight our contributions as follows:\n(i) We propose a hypergraph neural model,\nHEGEL , for long document summarization. To\nthe best of our knowledge, we are the first to\nmodel high-order cross-sentence relations with hy-\npergraphs for extractive document summarization.\n(ii) We propose three types of hyperedges (sec-\ntion, topic, and keyword) that capture sentence de-\npendency from different perspectives. Hypergraph\ntransformer layers are then designed to update and\nlearn effective sentence representations by message\npassing on the hypergraph.\n(iii) We validate HEGEL on two benchmarked\ndatasets (arXiv and PubMed), and the experimental\nresults demonstrate its effectiveness over state-of-\nthe-art baselines. We also conduct ablation studies\nand qualitative analysis to investigate the model\nperformance further.\n2 Related Works\n2.1 Scientific Paper Summarization\nWith the promising progress on short news summa-\nrization, research interest in long-form documents\nlike academic papers has arisen. Cohan et al. (2018)\nproposed benchmark datasets ArXiv and PubMed,\nand employed pointer generator network with hi-\nerarchical encoder and discourse-aware decoder.\nXiao and Carenini (2019) proposed an encoder-\ndecoder model by incorporating global and local\ncontexts. Ju et al. (2021) introduced an unsuper-\nvised extractive approach to summarize long sci-\nentific documents based on the Information Bottle-\nneck principle. Dong et al. (2020) came up with\nan unsupervised ranking model by incorporating\nhierarchical graph representation and asymmetri-\ncal positional cues. Recently, Ruan et al. (2022)\nproposed to apply pre-trained language model with\nhierarchical structure information.\n2.2 Graph based summarization\nGraph-based models have been exploited for ex-\ntractive summarization to capture cross-sentence\ndependencies. Unsupervised graph summarization\nmethods rely on graph connectivity to score and\nrank sentences (Radev et al., 2004; Zheng and La-\npata, 2019; Dong et al., 2020). Researchers also\nexplore supervised graph neural networks for sum-\nmarization. Yasunaga et al. (2017) applied Graph\nConvolutional Network (GCN) on the approximate\ndiscourse graph. Xu et al. (2019) proposed to apply\nGCN on structural discourse graphs based on RST\ntrees and coreference mentions. Cui et al. (2020)\nleveraged topical information by building topic-\nsentence graphs. Recently, Wang et al. (2020) pro-\nposed to construct word-document heterogeneous\ngraphs and use word nodes as the intermediary be-\ntween sentences. Jing et al. (2021) proposed to\nuse multiplex graph to consider different sentence\nrelations. Our paper follows this line of work on\ndeveloping novel graph neural networks for sin-\ngle document extractive summarization. The main\ndifference is that we construct a hypergraph from\n10168\n(a)\n (b)\nFigure 2: (a) The overall architecture of HEGEL . (b) Two-phase message passing in hypergraph transformer layer\na document that could capture high-order cross-\nsentence relations instead of pairwise relations, and\nfuse different types of sentence dependencies, in-\ncluding section structure, latent topics, and key-\nwords coreference.\n3 Method\nIn this section, we introduce HEGEL in great detail.\nWe first present how to construct a hypergraph for\na given long document. After encoding sentences\ninto contextualized representations, we extract their\nsection, latent topic, and keyword coreference re-\nlations and fuse them into a hypergraph. Then,\nour hypergraph transformer layer will update and\nlearn sentence representations according to the hy-\npergraph. Finally, HEGEL will score the salience\nof sentences based on the updated sentence repre-\nsentations to determine if the sentence should be\nincluded in the summary. The overall architecture\nof our model is shown in Figure 2(a).\n3.1 Document as a Hypergraph\nA hypergraph is defined as a graph G = (V,E),\nwhere V = {v1,...,v n}represents the set of\nnodes, and E = {e1,...,e m}represents the set\nof hyperedges in the graph. Here each hyperedge\ne connects two or more nodes (i.e., σ(e) ≥ 2).\nSpecifically, we use the notations v∈eand v /∈e\nto denote node v is connected to hyperedge eor\nnot in the graph G, respectively. The topological\nstructure of hypergraph can also be represented by\nits incidence matrix A ∈Rn×m:\nAij =\n{ 1, if vi ∈ej\n0, if vi /∈ej\n(1)\nGiven a document D = {s1,s2,...,s n}, each\nsentence si is represented by a corresponding node\nvi ∈V. A Hyperedge ej will be created if a sub-\nset of nodes Vj ⊂V share common semantic or\nstructural information.\n3.1.1 Node Representation\nWe first adopt sentence-BERT (Reimers and\nGurevych, 2019) as sentence encoder to embed\nthe semantic meanings of sentences as X =\n{x1,x2,..., xn}. Note that the sentence-BERT is\nonly used for initial sentence embedding, but not\nupdated in HEGEL .\nTo preserve the sequential information, we also\nadd positional encoding following Transformer\n(Vaswani et al., 2017). We adopt the hierarchi-\ncal position embedding (Ruan et al., 2022), where\nposition of each sentence si can be represented as\ntwo parts: the section index of the sentence psec\ni ,\nand the sentence index in its corresponding section\npsen\ni . The hierarchical position embedding (HPE)\nof sentence si can be calculated as:\nHPE(si) =γ1PE(psec\ni ) +γ2PE(psen\ni ), (2)\nwhere γ1,γ2 are two hyperparameters to adjust the\nscale of positional encoding and PE(·) refers to the\nposition encoding function:\n10169\nPE(pos,2i) = sin(pos/100002i/dmodel),\n(3)\nPE(pos,2i+ 1) = cos(pos/100002i/dmodel).\n(4)\nThen we can get the initial input node representa-\ntions H0 = {h0\n1,h0\n2,..., h0\nn}, with vector h0\ni de-\nfined as:\nh0\ni = xi + HPE(si) (5)\n3.1.2 Hyperedge Construction\nTo effectively model multi-type cross-sentence re-\nlations in a long context, we propose the following\nthree hyperedges. These hyperedges could capture\nhigh-order context information via the multi-node\nconnection and model both local and global con-\ntext through document structures from different\nperspectives.\nSection Hyperedges: Scientific papers mostly\nfollow a standard discourse structure describing the\nproblem, methodology, experiments/results, and fi-\nnally conclusions, so sentences within the same sec-\ntion tend to have the same semantic focus (Suppe,\n1998). To capture the local sequential context, we\nbuild section hyperedges that consider each section\nas a hyperedge that connects all the sentences in\nthis section. Section hyperedges could also address\nthe incidence matrix sparsity issue and ensure all\nnodes of the graph are connected by at least one\nhyperedge. Assume a document has q sections,\nsection hyperedge esec\nj for the j-th section can be\nrepresented formally in its corresponding incidence\nmatrix Asec ∈Rn×q as:\nAsec\nij =\n{ 1, if si ∈esec\nj\n0, if si /∈esec\nj\n(6)\nwhere Asec\nij denotes whether the i-th sentence is in\nthe j-th section.\nTopic Hyperedges: Topical information has\nbeen demonstrated to be effective in capturing im-\nportant content (Cui et al., 2020). To leverage top-\nical information of the document, we first apply\nthe Latent Dirichlet Allocation (LDA) model (Blei\net al., 2003) to extract the latent topic relationships\nbetween sentences and then construct the topic hy-\nperedge. In addition, topic hyperedges could ad-\ndress the long-distance dependency problem by\ncapturing global topical information of the doc-\nument. After extracting p topics from LDA, we\nconstruct pcorresponding topic hyperedges etopic\nj ,\nrepresented by the entry Atopic\nij in the incidence\nmatrix Atopic ∈Rn×p as:\nAtopic\nij =\n{\n1, if si ∈etopic\nj\n0, if si /∈etopic\nj\n(7)\nwhere Atopic\nij denotes whether the i-th sentence be-\nlongs to the j-th latent topic.\nKeyword Hyperedges: Previous work finds that\nkeywords compose the main body of the sentence,\nwhich are regarded as the indicators for impor-\ntant sentence selection (Wang and Cardie, 2013;\nLi et al., 2020). Keywords in the original sentence\nprovide significant clues for the main points of the\nsentence. To utilize keyword information, we first\nextract keywords for academic papers with Key-\nBERT (Grootendorst, 2020) and construct keyword\nhyperedges to link the sentences that contain the\nsame keyword regardless of their sequential dis-\ntance. Like topic hyperedges, keyword hyperedges\nalso capture global context relations and thus, ad-\ndress the long-distance dependency problem. After\nextracting k keywords for a document, we con-\nstruct k corresponding keyword hyperedges ekw\nj ,\nrepresented in the incidence matrix Akw ∈Rn×k\nas:\nAkw\nij =\n{ 1, if si ∈ekw\nj\n0, if si /∈ekw\nj , (8)\nwhere si ∈ekw\nj means the i-th sentence contains\nthe j-th keyword.\nWe finally fuse the three hyperedges by con-\ncatenation ∥and get the overall incidence matrix\nA ∈Rn×m as:\nA = Asec∥Atopic∥Akw, (9)\nwhere dimension m= q+ p+ k\nThe initial input node representations H0 =\n{h0\n1,h0\n2,..., h0\nn}and the overall hyperedge inci-\ndence matrix A will be fed into hypergraph trans-\nformer layers to learn effective sentence embed-\ndings.\n3.2 Hypergraph Transformer Layer\nThe self-attention mechanism in Transformer\n(Vaswani et al., 2017) has demonstrated its effec-\ntiveness for learning text representation and graph\nrepresentations (Veliˇckovi´c et al., 2017; Ying et al.,\n2021; Ding et al., 2020; Zhang and Zhang, 2020;\n10170\nZhang et al., 2020). To model cross-sentence rela-\ntions and learn effective sentence (node) represen-\ntations in hypergraphs, we propose the Hypergraph\nTransformer Layer as in Figure 2(b).\n3.2.1 Hypergraph Attention\nGiven node representations H0 = {h0\n1,h0\n2,..., h0\nn}\nand hyperedge incidence matrix A ∈Rn×m, a l-\nlayer hypergraph transformer computes hypergraph\nattention (HGA) and updates node representations\nH in an iterative manner as shown in Algorithm 1.\nSpecifically, in each iteration, we first obtain all\nmhyperedge representations {gl\n1,gl\n2,..., gl\nm}as:\ngl\nj = LeakyReLU\n\n∑\nvk∈ej\nαjkWhhl−1\nk\n\n, (10)\nαjk = exp\n(\nwT\nahuk\n)\n∑\nvp∈ej exp\n(\nwT\nahup\n),\nuk = LeakyReLU\n(\nWhhl−1\nk\n)\n,\n(11)\nwhere the superscript l denotes the model layer,\nmatrices Wh,wah are trainable weights and αjk is\nthe attention weight of node vk in hyperedge ej.\nThe second step is to update node representa-\ntions Hl−1 based on the updataed hyperedge repre-\nsentations {gl\n1,gl\n2,..., gl\nm}by:\nhl\ni = LeakyReLU\n(∑\nvi∈ek\nβijWegl\nk\n)\n, (12)\nβki = exp\n(\nwT\naezk\n)\n∑\nvi∈eq exp (wTaezi),\nzk = LeakyReLU\n([\nWegl\nk∥Whhl−1\ni\n])\n,\n(13)\nwhere hl\ni is the representation of node vi, We,wae\nare trainable weights, and βki is the attention\nweight of hyperedge ek that connects node vi. ∥\nhere is the concatenation operation. In this way,\ninformation of different granularities and types can\nbe fully exploited through the hypergraph attention\nmessage passing processes.\nMulti-Head Hypergraph Attention As in Trans-\nformer, we also extend hypergraph attention (HGA)\ninto multi-head hypergraph attention (MH-HGA)\nto expand the model’s representation subspaces,\nrepresented as:\nMH-HGA(H,A) =σ(WO∥h\ni=1headi),\nheadi = HGAi(H,A), (14)\nwhere HGA(·) denotes hypergraph attention, σ\nis the activation function, WO is the multi-head\nweight, and ∥denotes concatenation.\n3.2.2 Hypergraph Transformer\nAfter obtaining the multi-head attention, we also\nintroduce the feed-forward blocks (FFN) with resid-\nual connection and layer normalization (LN) like\nin Transformer. We formally characterize the Hy-\npergraph Transformer layer as below:\nH′(l) = LN(MH-HGA(Hl−1,A) +Hl−1)\nHl = LN(FFN(H′(l)) +H′(l) (15)\nAlgorithm 1: MH-HGAhead(H,A)\ninput : node representation Hl−1 ∈Rn×d,\nincidence matrix A ∈Rn×m\noutput :updated representation Hl ∈Rn×d\n1 for head= 1,2,...,h do\n// update hyperedges from nodes\n2 for j = 1,2,...,m do\n3 for node vk ∈ej do\n4 compute attention αjk with Eq. 11;\n5 update hyperedge representation gl\nj\nwith Eq. 10;\n6 end\n7 end\n// update node representations\n8 for i= 1,2,...,n do\n9 for hyperedge that vi ∈ek do\n10 compute attention βki with Eq. 13;\nupdate node representation hl\ni with\nEq. 12;\n11 end\n12 end\n13 end\n3.3 Training Objective\nAfter passing L hypergraph transformer layers,\nwe obtain the final sentence node representations\nHL = {hL\n1 ,hL\n2 ,..., hL\nn}. We then add a multi-\nlayer perceptron(MLP) followed by a sigmoid acti-\nvation function indicating the confidence score for\nselecting each sentence. Formally, the predicted\nconfidence score ˆyi for sentence si is:\nzi = LeakyReLU(Wp1hL\ni ),\nˆyi = sigmoid(Wp2zi), (16)\n10171\nArxiv PubMed\n# train 201,427 112,291\n# validation 6,431 6,402\n# test 6,436 6,449\navg. document length 4,938 3,016\navg. summary length 203 220\nTable 1: Statistics of PubMed and Arxiv datasets.\nwhere Wp1,Wp2 are trainable parameters.\nCompared with the sentence ground truth label\nyi, we train HEGEL in an end-to-end manner and\noptimize with binary cross-entropy loss as:\nL= − 1\nN ·Nd\nN∑\nd=1\nNd∑\ni=1\n(yi log ˆyi + (1−yi) log (1−ˆyi)),\n(17)\nwhere N denotes the number of training instances\nin the training set, and Nd denotes the number of\nsentences in the document.\n4 Experiment\nThis section presents experimental details on\ntwo benchmarked academic paper summarization\ndatasets. We compare our proposed model with\nstate-of-the-art baselines and conduct detailed anal-\nysis to validate the effectiveness of HEGEL .\n4.1 Experiment Setup\nDatsasets Scientific papers are an example of\nlong documents with section discourse structure.\nHere we validate HEGEL on two benchmark sci-\nentific paper summarization datasets: ArXiv and\nPubMed (Cohan et al., 2018). PubMed contains\nacademic papers from the biomedical domain,\nwhile arXiv contains papers from different scien-\ntific domains. We use the original train, validation,\nand testing splits as in (Cohan et al., 2018). The\ndetailed statistics of datasets are shown in Table 1.\nCompared Baselines We perform a system-\natic comparison with state-of-the-art baseline ap-\nproaches as follows:\n• Unsupervised methods: LEAD that selects\nthe first few sentences as summary; graph-\nbased methods LexRank (Erkan and Radev,\n2004), PACSUM (Zheng and Lapata, 2019),\nand HIPORANK (Dong et al., 2020).\n• Neural extractive models: encoder-decoder\nbased model Cheng&Lapata (Cheng and La-\npata, 2016) and SummaRuNNer (Nallapati\net al., 2016a); local and global context model\nExtSum-LG (Xiao and Carenini, 2019) and\nits variant RdLoss/MMR (Xiao and Carenini,\n2020); transformer-based models SentCLF,\nSentPTR (Subramanian et al., 2019), and\nHiStruct+ (Ruan et al., 2022).\n• Neural abstractive models: pointer network\nPGN (See et al., 2017), hierarchical attention\nmodel DiscourseAware (Cohan et al., 2018),\ntransformer-based model TLM-I+E (Subra-\nmanian et al., 2019), and divide-and-conquer\nmethod DANGER (Gidiotis et al., 2020).\n4.2 Implementation Details\nWe use pre-trained sentence-BERT (Reimers and\nGurevych, 2019) checkpoint all-mpnet-base-v2 as\nthe encoder for initial sentence representations.\nThe embedding dimension is 768, and the input\nlayer dimension is 1024. In our experiment, we\nstack two layers of hypergraph transformer, and\neach has 8 attention heads with a hidden dimension\nof 128. The output layer’s hidden dimension is set\nto 4096. We generate at most 100 topics for each\ndocument and filter out the topic and keyword hy-\nperedges that connect less than 5 sentence nodes\nor greater than 25 sentence nodes. For position\nencodings, we set the rescale weights γ1 and γ2 to\n0.001.\nThe model is optimized with Adam optimizer\n(Loshchilov and Hutter, 2017) with a learning rate\nof 0.0001 and a dropout rate of 0.3. We train the\nmodel on an RTX A6000 GPU for 20 epochs and\nvalidate after each epoch using ROUGE-1 F-score\nto choose checkpoints. Early stopping is employed\nto select the best model with the patience of 3.\nFollowing the standard-setting, we use ROUGE\nF-scores (Lin and Hovy, 2003) for performance\nevaluation. Specifically, ROUGE-1/2 scores mea-\nsure summary informativeness, and the ROUGE-L\nscore measures summary fluency. Following prior\nwork (Nallapati et al., 2016b), we construct ex-\ntractive ground truth (ORACLE) by greedily op-\ntimizing the ROUGE score on the gold-standard\nabstracts for extractive summary labeling.\n4.3 Experiment Results\nThe performance of HEGEL and baseline methods\non arXiv and Pubmed datasets are shown in Ta-\nble 2. The first block lists the extractive ground\ntruth ORACLE and the unsupervised methods. The\nsecond block includes recent extractive summariza-\ntion models, and the third contains state-of-the-art\nabstractive methods.\nThe LEAD method has limited performance\non scientific paper summarization compared to\n10172\nModels PubMed ArXiv\nROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L\nORACLE 55.05 27.48 49.11 53.88 23.05 46.54\nLEAD 35.63 12.28 25.17 33.66 8.94 22.19\nLexRank (2004) 39.19 13.89 34.59 33.85 10.73 28.99\nPACSUM (2019) 39.79 14.00 36.09 38.57 10.93 34.33\nHIPORANK (2021) 43.58 17.00 39.31 39.34 12.56 34.89\nCheng&Lapata (2016) 43.89 18.53 30.17 42.24 15.97 27.88\nSummaRuNNer (2016) 43.89 18.78 30.36 42.81 16.52 28.23\nExtSum-LG (2019) 44.85 19.70 31.43 43.62 17.36 29.14\nSentCLF (2020) 45.01 19.91 41.16 34.01 8.71 30.41\nSentPTR (2020) 43.30 17.92 39.47 42.32 15.63 38.06\nExtSum-LG + RdLoss (2021) 45.30 20.42 40.95 44.01 17.79 39.09\nExtSum-LG + MMR (2021) 45.39 20.37 40.99 43.87 17.50 38.97\nHiStruct+ (2022) 46.59 20.39 42.11 45.22 17.67 40.16\nPGN (2017) 35.86 10.22 29.69 32.06 9.04 25.16\nDiscourseAware (2018) 38.93 15.37 35.21 35.80 11.05 31.80\nTLM-I+E (2020) 42.13 16.27 39.21 41.62 14.69 38.03\nDANCER-LSTM (2020) 44.09 17.69 40.27 41.87 15.92 37.61\nDANCER-RUM (2020) 43.98 17.65 40.25 42.70 16.54 38.44\nHEGEL (ours) 47.13 21.00 42.18 46.41 18.17 39.89\nTable 2: Experimental Results on PubMed and Arxiv datasets.\nits strong performance on short news summariza-\ntion like CNN/Daily Mail (Hermann et al., 2015)\nand New York Times (Sandhaus, 2008). The phe-\nnomenon indicates that academic paper has less po-\nsitional bias than news articles, and the ground truth\nsentence distributes more evenly. For graph-based\nunsupervised baselines, HIPORANK (Dong et al.,\n2020) achieves state-of-the-art performance that\ncould even compete with some supervised methods.\nThis demonstrates the significance of incorporat-\ning discourse structural information when model-\ning cross-sentence relations for long documents.\nIn general, neural extractive methods perform bet-\nter than abstractive methods due to the extended\ncontext. Among extractive baselines, transformer-\nbased methods like SentPTR and HiStruct+ show\nsubstantial performance gain, demonstrating the ef-\nfectiveness of the attention mechanism. HiStruct+\nachieves strong performance by injecting inherent\nhierarchical structures into large pre-trained lan-\nguage models Longformer. In contrast, our model\nHEGEL only relies on hypergraph transformer lay-\ners for sentence representation learning and re-\nquires no pre-trained knowledge.\nAs shown in Table 2, HEGEL outperforms state-\nof-the-art extractive and abstractive baselines on\nboth datasets. The supreme performance of HEGEL\nshows hypergraphs’ capability of modeling high-\norder cross-sentence relations and the importance\nof fusing both semantic and structural information.\nWe conduct an extensive ablation study and perfor-\nmance analysis next.\nModel ROUGE-1 ROUGE-2 ROUGE-L\nfull HEGEL 47.13 21.00 42.18\nw/o Position 46.86 20.05 41.91\nw/o Keyword 46.92 20.71 42.03\nw/o Topic 46.35 20.30 41.48\nw/o Section 45.63 19.30 40.71\nTable 3: Ablation study results on PubMed dataset.\n5 Analysis\n5.1 Ablation Study\nWe first analyze the influence of different compo-\nnents of HEGEL . Table 3 shows the experimental\nresults of removing hyperedges and the hierarchi-\ncal position encoding of HEGEL on the PubMed\ndataset. As shown in the second row, removing the\nhierarchical position embedding hurts the model\nperformance, which indicates the importance of\ninjecting sequential order information. Regarding\nhyperedges (row 3-5), we can see that all three\ntypes of hyperedges (section, keyword, and topic)\nhelp boost the overall model performance. Specifi-\ncally, the performance drops most when the section\nhyperedges are removed. The hypergraph becomes\nsparse and hurts its connectivity. This indicates\nthat the section hyperedges, which contain local\ncontext information, play an essential role in the in-\nformation aggregation process. Note that although\nwe only discuss three types of hyperedges (sec-\ntion, keyword, and topic) in this work, it is easy\nto extend our model with hyperedges from other\nperspectives like syntactic for future work.\n10173\n5.2 Hyperedge Analysis\nFigure 3: Average attention distribution over\nthree types of hyperedges on PubMed dataset.\nWe also explore the hyperedge pattern to under-\nstand the performance ofHEGEL further. As shown\nin Figure 3, we have the most topic hyperedges on\naverage, and section hyperedges have the largest\ndegree (number of connected nodes). In terms of\ncross attention over the predicted sentence nodes,\nHEGEL pays more than half of the attention to sec-\ntion hyperedges and pays least to keywords edges.\nThe results are consistent with the earlier ablation\nstudy that local section context information plays a\nmore critical role in long document summarization.\nFigure 4: Visualization of sentence nodes em-\nbeddings for 100 documents in PubMed test\nset.\n5.3 Embedding Analysis\nTo explore the sentence embedding learned by\nHEGEL , we show a visualization of the output sen-\ntence node embedding from the last hypergraph\ntransformer layer. We employ T-SNE (van der\nMaaten and Hinton, 2008) and reduce each node’s\ndimension to 2, as shown in Figure 4. The orange\ndots represent the ground truth sentences, and the\nblue dots are the non-ground truth sentences. We\ncan see some clustering effects of the ground truth\nnodes, which also tend to appear in the bottom left\nzone of the plot. The results indicate that HEGEL\nlearns effective sentence embeddings as indicators\nfor salient sentence selection.\n5.4 Case Study\nHere we also provide an example output summary\nfrom HEGEL in Table 4. We could see that the\nselected sentences span a long distance in the origi-\nnal document, but are triadically related according\nto the latent topic and keyword coreference. As\na result, HEGEL effectively captures high-order\ncross-sentence relations through multi-type hyper-\nedges and selects these salient sentences according\nto learned high-order representation.\n[Method] Phylogenetic analyses of partial middle east\nrespiratory syndrome coronavirus genomic sequences for\nviruses detected in dromedaries imported from oman to\nunited arab emirates, may 2015. (Section 1)\n[Information] Additional information regarding 2 persons\nwith asymptomatic merscov infection and other persons\ntested in the study. (Section 2)\n[Information] Our findings provide further evidence\nthat asymptomatic human infections can be caused by\nzoonotic transmission. (Section 2)\n[Method] Merscov genomic sequences determined in this\nstudy are similar to those of viruses detected in 2015 in\npatients in saudi arabia and south korea with hospital -\nacquired infections. (Section 3)\n[Information] The infected dromedaries were imported\nfrom oman , which suggests that viruses from this clade\nare circulating on the arabian peninsula. (Section 4)\nTable 4: An example output summary ofHEGEL . Topics\nare marked in orange, key words are marked in green,\nand sections are marked in blue.\n6 Conclusion\nThis paper presents HEGEL for long document sum-\nmarization. HEGEL represents a document as a\nhypergraph to address the long dependency issue\nand captures higher-order cross-sentence relations\nthrough multi-type hyperedges. The strong perfor-\nmance of HEGEL demonstrates the importance of\nmodeling high-order sentence interactions and fus-\ning semantic and structural information for future\nresearch in long document extractive summariza-\ntion.\n10174\nLimitations\nDespite the strong performance of HEGEL , its\ndesign still has the following limitations. First,\nHEGEL relies on existing keyword and topic mod-\nels to pre-process the document and construct hy-\npergraphs. In addition, we only explore academic\npaper datasets as a typical example for long docu-\nment summarization.\nThe above limitations may raise concerns about\nthe model’s performance. However, HEGEL is an\nend-to-end model, so the pre-process steps do not\nadd the model computation complexity. Indeed,\nHEGEL relies on hyperedge for cross-sentence at-\ntention, so it is parameter-efficient and uses 50%\nless parameters than heterogeneous graph model\n(Wang et al., 2020) and 90% less parameters than\nLongformer-base (Beltagy et al., 2020). On the\nother hand, our experimental design follows a se-\nries of previous long document summarization\nwork (Xiao and Carenini, 2019, 2020; Subrama-\nnian et al., 2019; Ruan et al., 2022; Dong et al.,\n2020; Cohan et al., 2018) on benchmark datasets\nArXiv and PubMed. These two new datasets con-\ntain much longer documents, richer discourse struc-\nture than all the news datasets and are therefore\nideal test-beds for long document summarization.\nAcknowledgements\nThis work is supported by NSF through grants IIS-\n1763365 and IIS-2106972. We thank the anony-\nmous reviewers for the helpful feedback.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993–1022.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. arXiv\npreprint arXiv:1603.07252.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents.\narXiv preprint arXiv:1804.05685.\nPeng Cui and Le Hu. 2021. Topic-guided abstractive\nmulti-document summarization. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021, pages 1463–1472, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nPeng Cui, Le Hu, and Yuanchao Liu. 2020. Enhancing\nextractive text summarization with topic-aware graph\nneural networks. arXiv preprint arXiv:2010.06253.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nKaize Ding, Jianling Wang, Jundong Li, Dingcheng Li,\nand Huan Liu. 2020. Be more with less: Hypergraph\nattention networks for inductive text classification.\narXiv preprint arXiv:2011.00387.\nYue Dong, Andrei Mircea, and Jackie CK Cheung.\n2020. Discourse-aware unsupervised summariza-\ntion of long scientific documents. arXiv preprint\narXiv:2005.00513.\nGünes Erkan and Dragomir R Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text sum-\nmarization. Journal of artificial intelligence research,\n22:457–479.\nAlexios Gidiotis, Stefanos Stefanidis, and Grigorios\nTsoumakas. 2020. AUTH @ CLSciSumm 20, Lay-\nSumm 20, LongSumm 20. In Proceedings of the\nFirst Workshop on Scholarly Document Processing,\npages 251–260, Online. Association for Computa-\ntional Linguistics.\nMaarten Grootendorst. 2020. Keybert: Minimal key-\nword extraction with bert.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nBaoyu Jing, Zeyu You, Tao Yang, Wei Fan, and Hang-\nhang Tong. 2021. Multiplex graph neural network\nfor extractive text summarization. arXiv preprint\narXiv:2108.12870.\nJiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan\nDu, and Shirui Pan. 2021. Leveraging information\nbottleneck for scientific document summarization.\narXiv preprint arXiv:2110.01280.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nHaoran Li, Junnan Zhu, Jiajun Zhang, Chengqing Zong,\nand Xiaodong He. 2020. Keywords-guided abstrac-\ntive sentence summarization. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8196–8203.\n10175\nChin-Yew Lin and Eduard Hovy. 2003. Automatic\nevaluation of summaries using n-gram co-occurrence\nstatistics. In Proceedings of the 2003 Human Lan-\nguage Technology Conference of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 150–157.\nYang Liu and Mirella Lapata. 2019. Text summa-\nrization with pretrained encoders. arXiv preprint\narXiv:1908.08345.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2016a.\nSummarunner: A recurrent neural network based\nsequence model for extractive summarization of doc-\numents. arXiv preprint arXiv:1611.04230.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing\nXiang, et al. 2016b. Abstractive text summarization\nusing sequence-to-sequence rnns and beyond. arXiv\npreprint arXiv:1602.06023.\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\n2018. Ranking sentences for extractive summariza-\ntion with reinforcement learning. arXiv preprint\narXiv:1802.08636.\nDragomir R Radev, Hongyan Jing, Małgorzata Sty´s, and\nDaniel Tam. 2004. Centroid-based summarization\nof multiple documents. Information Processing &\nManagement, 40(6):919–938.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nQian Ruan, Malte Ostendorff, and Georg Rehm. 2022.\nHistruct+: Improving extractive text summariza-\ntion with hierarchical structure information. arXiv\npreprint arXiv:2203.09629.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia,\n6(12):e26752.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint\narXiv:1704.04368.\nSandeep Subramanian, Raymond Li, Jonathan Pi-\nlault, and Christopher Pal. 2019. On extractive\nand abstractive neural document summarization\nwith transformer language models. arXiv preprint\narXiv:1909.03186.\nFrederick Suppe. 1998. The structure of a scientific\npaper. Philosophy of Science, 65(3):381–405.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of Machine\nLearning Research, 9(86):2579–2605.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. arXiv preprint\narXiv:1710.10903.\nDanqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu,\nand Xuanjing Huang. 2020. Heterogeneous graph\nneural networks for extractive document summariza-\ntion. arXiv preprint arXiv:2004.12393.\nLu Wang and Claire Cardie. 2013. Domain-independent\nabstract generation for focused meeting summariza-\ntion. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1395–1405.\nWen Xiao and Giuseppe Carenini. 2019. Extrac-\ntive summarization of long documents by com-\nbining global and local context. arXiv preprint\narXiv:1909.08089.\nWen Xiao and Giuseppe Carenini. 2020. Systematically\nexploring redundancy reduction in summarizing long\ndocuments. arXiv preprint arXiv:2012.00052.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing\nLiu. 2019. Discourse-aware neural extractive\nmodel for text summarization. arXiv preprint\narXiv:1910.14142.\nMichihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush\nPareek, Krishnan Srinivasan, and Dragomir Radev.\n2017. Graph-based neural multi-document summa-\nrization. arXiv preprint arXiv:1706.06681.\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin\nZheng, Guolin Ke, Di He, Yanming Shen, and Tie-\nYan Liu. 2021. Do transformers really perform badly\nfor graph representation? Advances in Neural Infor-\nmation Processing Systems, 34.\nHaopeng Zhang, Semih Yavuz, Wojciech Kry ´sci´nski,\nKazuma Hashimoto, and Yingbo Zhou. 2022. Im-\nproving the faithfulness of abstractive summarization\nvia entity coverage control. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022,\npages 528–535.\nHaopeng Zhang and Jiawei Zhang. 2020. Text graph\ntransformer for document classification. In Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP).\nJiawei Zhang, Haopeng Zhang, Li Sun, and Congying\nXia. 2020. Graph-bert: Only attention is needed\nfor learning graph representations. arXiv preprint\narXiv:2001.05140.\nHao Zheng and Mirella Lapata. 2019. Sentence central-\nity revisited for unsupervised summarization. arXiv\npreprint arXiv:1906.03508.\n10176",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8489694595336914
    },
    {
      "name": "Sentence",
      "score": 0.7695172429084778
    },
    {
      "name": "Computer science",
      "score": 0.7558971643447876
    },
    {
      "name": "Coreference",
      "score": 0.6403059959411621
    },
    {
      "name": "Hegelianism",
      "score": 0.635543704032898
    },
    {
      "name": "Transformer",
      "score": 0.5707162022590637
    },
    {
      "name": "Hypergraph",
      "score": 0.5627939701080322
    },
    {
      "name": "Natural language processing",
      "score": 0.5568262934684753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.542656660079956
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4533289670944214
    },
    {
      "name": "Dependency (UML)",
      "score": 0.42268916964530945
    },
    {
      "name": "Mathematics",
      "score": 0.14832445979118347
    },
    {
      "name": "Resolution (logic)",
      "score": 0.10715922713279724
    },
    {
      "name": "Engineering",
      "score": 0.07980582118034363
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Discrete mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ]
}