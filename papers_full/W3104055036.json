{
  "title": "Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training",
  "url": "https://openalex.org/W3104055036",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2132756360",
      "name": "Hai Ye",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2242520679",
      "name": "Qingyu Tan",
      "affiliations": [
        "National University of Singapore",
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2740518062",
      "name": "Ruidan He",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    },
    {
      "id": "https://openalex.org/A2126452540",
      "name": "Juntao Li",
      "affiliations": [
        "Soochow University"
      ]
    },
    {
      "id": "https://openalex.org/A2146810117",
      "name": "Hwee Tou Ng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (Cayman Islands)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2981630749",
    "https://openalex.org/W2985406498",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2151752537",
    "https://openalex.org/W2963912690",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2995852213",
    "https://openalex.org/W2997636389",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3030236966",
    "https://openalex.org/W2889774592",
    "https://openalex.org/W2162651021",
    "https://openalex.org/W2895281799",
    "https://openalex.org/W3034565015",
    "https://openalex.org/W2963729324",
    "https://openalex.org/W3137695714",
    "https://openalex.org/W2951970475",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W2945328857",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2171068337",
    "https://openalex.org/W2995607862",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2594718649",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2978426779",
    "https://openalex.org/W2952289666",
    "https://openalex.org/W2212660284",
    "https://openalex.org/W2963308086",
    "https://openalex.org/W2963096987",
    "https://openalex.org/W2261310161",
    "https://openalex.org/W2530816535",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W92894758",
    "https://openalex.org/W2971292190",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2952621248",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1593532658",
    "https://openalex.org/W2478454054",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2987154291",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2153353890"
  ],
  "abstract": "Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7386‚Äì7399,\nNovember 16‚Äì20, 2020.c‚Éù2020 Association for Computational Linguistics\n7386\nFeature Adaptation of Pre-Trained Language Models\nacross Languages and Domains with Robust Self-Training\nHai Ye1 Qingyu Tan‚àó1,2 Ruidan He2 Juntao Li3\nHwee Tou Ng1 Lidong Bing2\n1Department of Computer Science, National University of Singapore\n2DAMO Academy, Alibaba Group\n3School of Computer Science and Technology, Soochow University\n{yeh,nght}@comp.nus.edu.sg\n{qingyu.tan,ruidan.he,l.bing}@alibaba-inc.com\nljt@suda.edu.cn\nAbstract\nAdapting pre-trained language models\n(PrLMs) (e.g., BERT) to new domains has\ngained much attention recently. Instead of\nÔ¨Åne-tuning PrLMs as done in most previous\nwork, we investigate how to adapt the features\nof PrLMs to new domains without Ô¨Åne-tuning.\nWe explore unsupervised domain adaptation\n(UDA) in this paper. With the features from\nPrLMs, we adapt the models trained with\nlabeled data from the source domain to the un-\nlabeled target domain. Self-training is widely\nused for UDA, and it predicts pseudo labels on\nthe target domain data for training. However,\nthe predicted pseudo labels inevitably include\nnoise, which will negatively affect training\na robust model. To improve the robustness\nof self-training, in this paper we present\nclass-aware feature self-distillation (CFd) to\nlearn discriminative features from PrLMs, in\nwhich PrLM features are self-distilled into a\nfeature adaptation module and the features\nfrom the same class are more tightly clustered.\nWe further extend CFd to a cross-language\nsetting, in which language discrepancy is\nstudied. Experiments on two monolingual\nand multilingual Amazon review datasets\nshow that CFd can consistently improve the\nperformance of self-training in cross-domain\nand cross-language settings.\n1 Introduction\nPre-trained language models (PrLMs) such as\nBERT (Devlin et al., 2019) and its variants (Liu\net al., 2019c; Yang et al., 2019) have shown signif-\nicant success for various downstream NLP tasks.\nHowever, these deep neural networks are sensitive\nto different cross-domain distributions (Quionero-\nCandela et al., 2009) and their effectiveness will\nbe much weakened in such a scenario. How to\n‚àóQingyu Tan is under the Joint PhD Program between\nAlibaba and National University of Singapore.\nadapt PrLMs to new domains is important. Unlike\nthe most recent work that Ô¨Åne-tunes PrLMs on the\nunlabeled data from the new domains (Han and\nEisenstein, 2019; Gururangan et al., 2020), we are\ninterested in how to adapt the PrLM features with-\nout Ô¨Åne-tuning. To investigate this, we speciÔ¨Åcally\nstudy unsupervised domain adaptation (UDA) of\nPrLMs, in which we adapt the models trained with\nsource labeled data to the unlabeled target domain\nbased on the features from PrLMs.\nSelf-training has been proven to be effective in\nUDA (Saito et al., 2017), which uses the model\ntrained with source labeled data to predict pseudo\nlabels on the unlabeled target set for model training.\nUnlike the methods of adversarial learning (Ganin\net al., 2016; Chen et al., 2018) and Maximum Mean\nDiscrepancy (MMD) (Gretton et al., 2012) that\nlearn domain-invariant features for domain align-\nment, self-training aims to learn discriminative fea-\ntures over the target domain, since simply matching\ndomain distributions cannot make accurate predic-\ntions on the target after adaptation (Lee et al., 2019;\nSaito et al., 2017). To learn discriminative features\nfor the target, self-training needs to retain a model‚Äôs\nhigh-conÔ¨Ådence predictions on the target domain\nwhich are considered correct for training. Meth-\nods like ensemble learning (Zou et al., 2019; Ge\net al., 2020; Saito et al., 2017) which adopt mul-\ntiple models to jointly make decisions on pseudo-\nlabel selections have been introduced to achieve\nthis goal. Though these methods can substantially\nreduce wrong predictions on the target, there will\nstill be noisy labels in the pseudo-label set, with\nnegative effects on training a robust model, since\ndeep neural networks with their high capacity can\neasily Ô¨Åt to corrupted labels (Arpit et al., 2017).\nIn our work, to improve the robustness of self-\ntraining, we propose to jointly learn discrimina-\ntive features from the PrLM on the target do-\nmain to alleviate the negative effects caused by\n7387\nnoisy labels. We introduce class-aware feature self-\ndistillation (CFd) to achieve this goal (¬ß4.2). The\nfeatures from PrLMs have been proven to be highly\ndiscriminative for downstream tasks, so we propose\nto distill this kind of features to a feature adaptation\nmodule (FAM) to make FAM capable of extract-\ning discriminative features ( ¬ß4.2.1). Inspired by\nrecent work on representation learning (van den\nOord et al., 2018; Hjelm et al., 2019), we introduce\nmutual information (MI) maximization for feature\nself-distillation (Fd). We maximize the MI between\nthe features from the PrLM and the FAM to make\nthe two kinds of features more dependent. Since Fd\ncan only distill features from the PrLM, it ignores\nthe cluster information of data points which can\nalso improve feature discriminativeness (Chapelle\nand Zien, 2005; Lee et al., 2019). Hence, for the\nfeatures output by FAM, if the corresponding data\npoints belong to the same class, we further mini-\nmize their feature distance to make the cluster more\ncohesive, so that different classes will be more sep-\narable. To retain high-conÔ¨Ådence predictions, we\nre-rank the predicted candidates and balance the\nnumbers of samples in different classes (¬ß4.1).\nWe use XLM-R (Conneau et al., 2019) as the\nPrLM which is trained on over 100 languages. We\nalso extend our method to cross-language, as well\nas cross-language and cross-domain settings us-\ning XLM-R, since it has already mapped different\nlanguages into a common feature space. We experi-\nment with two monolingual and multilingual Ama-\nzon review datasets for sentiment classiÔ¨Åcation:\nMonoAmazon for cross-domain and MultiAmazon\nfor cross-language experiments. We demonstrate\nthat self-training can be consistently improved by\nCFd in all settings (¬ß5.3). Further empirical results\nindicate that the improvements come from learning\nlower errors of ideal joint hypothesis (¬ß4.3,5.4).\n2 Related Work\nAdaptation of PrLMs. Recently, signiÔ¨Åcant im-\nprovements on multiple NLP tasks have been en-\nabled by pre-trained language models (PrLMs) (De-\nvlin et al., 2019; Yang et al., 2019; Liu et al., 2019c;\nHoward and Ruder, 2018; Peters et al., 2018). To\nenhance their performance on new domains, much\nwork has been done to adapt PrLMs. Two main\nadaptation settings have been studied. The Ô¨Årst\nis the same as what we study in this work: the\nPrLM provides the features based on which do-\nmain adaptation is conducted (Han and Eisenstein,\n2019; Cao et al., 2019; Logeswaran et al., 2019; Ma\net al., 2019; Li et al., 2020). In the second setting,\nthe corpus for pre-training a language model has\nlarge domain discrepancy with the target domain,\nso in this scenario, we need the target unlabeled\ndata to Ô¨Åne-tune the PrLM after which we train\na task-speciÔ¨Åc model (Gururangan et al., 2020).\nFor example, Lee et al. (2020) and Alsentzer et al.\n(2019) transfer PrLMs into biomedical and clinical\ndomains. Instead of Ô¨Åne-tuning PrLMs with unla-\nbeled data from the new domain as in most previ-\nous work (Rietzler et al., 2019; Han and Eisenstein,\n2019; Gururangan et al., 2020), we are interested\nin the feature-based approach (Devlin et al., 2019;\nPeters et al., 2019) to adapt PrLMs, which does not\nÔ¨Åne-tune PrLMs. The feature-based approach is\nmuch faster, easier, and more memory-efÔ¨Åcient for\ntraining than the Ô¨Åne-tuning-based method, since\nit does not have to update the parameters of the\nPrLMs which are usually massive especially the\nnewly released GPT-3 (Brown et al., 2020).\nDomain Adaptation. To perform domain adap-\ntation, previous work mainly focuses on how to\nminimize the domain discrepancy and how to learn\ndiscriminative features on the target domain (Ben-\nDavid et al., 2010). Kernelized methods, e.g.,\nMMD (Gretton et al., 2012; Long et al., 2015),\nand adversarial learning (Ganin et al., 2016; Chen\net al., 2018) are commonly used to learn domain-\ninvariant features. To learn discriminative features\nfor DA, self-training is widely explored (Saito et al.,\n2017; Ge et al., 2020; Zou et al., 2019, 2018; He\net al., 2018). To retain high-conÔ¨Ådence predictions\nfor self-training, ensemble methods like tri-training\n(Saito et al., 2017), mutual learning (Ge et al., 2020)\nand dual information maximization (Ye et al., 2019)\nhave been introduced. However, the pseudo-label\nset will still have noisy labels which will nega-\ntively affect model training (Arpit et al., 2017;\nZhang et al., 2017). Other methods on learning\ndiscriminative features include feature reconstruc-\ntion (Ghifary et al., 2016), semi-supervised learn-\ning (Laine and Aila, 2017), and virtual adversar-\nial training (Lee et al., 2019). Based on cluster\nassumption (Chapelle and Zien, 2005) and the re-\nlationship between decision boundary and feature\nrepresentations, Lee et al. (2019) explore class in-\nformation to learn discriminative features. Class\ninformation is also studied in distant supervision\nlearning for relation extraction (Ye et al., 2017).\nIn NLP, early work explores domain-invariant and\n7388\nùë•Embedding layer 0Transformer\nTransformer\nTransformerTransformerùëìùëì\nùëì\nùê¥ùë°ùë°\n\"‚Ñé!\nùëß! ùëßùëêùëôùëéùë†ùë†ùëñùëìùëñùëíùëü\nùêøùëéùë†ùë°ùëÅùëôùëéùë¶ùëíùëüùë†\nùêπùê¥ùëÄùëÉùëüùêøùëÄ\nFigure 1: Illustration of our model architecture which\nincludes a pre-trained language model, a feature adap-\ntation module, and a classiÔ¨Åer.\ndomain-speciÔ¨Åc words to reduce domain discrep-\nancy (Blitzer et al., 2007; Pan et al., 2010; He et al.,\n2011).\n3 Preliminary\nIn this section, we introduce the problem deÔ¨Ånition\nand the model architecture based on which we build\nour domain adaptation algorithm presented in the\nnext section.\n3.1 Unsupervised Domain Adaptation\nIn order to improve the feature adaptability of pre-\ntrained transformers cross domains, we study unsu-\npervised domain adaptation of pre-trained language\nmodels where we train models with labeled data\nand unlabeled data from the source and target do-\nmain respectively. We use the features from PrLMs\nto perform domain adaptation. Labeled data from\nthe source domain are deÔ¨Åned as S = {Xs,Ys}, in\nwhich every sample xs ‚ààXs has a label ys ‚ààYs.\nThe unlabeled data from the target domain are\nT = {Xt}. In this work, we comprehensively\nstudy domain adaptation in cross-domain and cross-\nlanguage settings, based on the features from the\nmulti-lingual PrLM where we adopt XLM-R (Con-\nneau et al., 2019) for evaluation. By using XLM-R,\ndifferent languages can be mapped into a common\nfeature space. In this work, we evaluate our method\non the task of sentiment classiÔ¨Åcation using two\ndatasets.\n3.2 Model Architecture\nAs presented in Figure 1, our model consists of\na pre-trained language model (PrLM), a feature\nadaptation module (FAM), and a classiÔ¨Åer.\n3.2.1 Pre-trained Language Model\nFollowing BERT (Devlin et al., 2019), most PrLMs\nconsist of an embedding layer and several trans-\nformer layers. Suppose a PrLM has L+ 1 lay-\ners, layer 0 is the embedding layer, and layer\nL is the last layer. Given an input sentence\nx = [ w1,w2,¬∑¬∑¬∑ ,w|x|], the embedding layer of\nthe PrLM will encode x as:\nh0 = Embedding(x) (1)\nwhere h0 = [ h1\n0,h2\n0,¬∑¬∑¬∑ ,h|x|\n0 ]. After obtaining\nthe embeddings of the input sentence, we compute\nthe features of the sentence from the transformer\nblocks of PrLM. In layer l, we compute the trans-\nformer feature as:\nhl = Transformerl(hl‚àí1) (2)\nwhere hl = [ h1\nl,h2\nl,¬∑¬∑¬∑ ,h|x|\nl ] and l ‚àà {1,2,\n¬∑¬∑¬∑ ,L}. Using all the |x|features will incur much\nmemory space. After experiments, we take the\naverage of hl as:\n¬Øhl = 1\n|x|\n|x|‚àë\ni=1\nhi\nl (3)\nand ¬Øhl will be fed into the FAM.\n3.2.2 Feature Adaptation Module\nTo transfer the knowledge from the source to the\ntarget domain, the features from PrLMs should be\nmore transferable. Previous work points out that\nthe PrLM features from the intermediate layers are\nmore transferable than the upper-layer features, and\nthe upper-layer features are more discriminative for\nclassiÔ¨Åcation (Hao et al., 2019; Peters et al., 2018;\nLiu et al., 2019b). By making a trade-off between\nspeed and model performance, we combine the last\nN-layer features from the PrLM for domain adapta-\ntion, which is called the multi-layer representation\nof the PrLM.\nOur FAM consists of a feed-forward neural net-\nwork (followed by a tanh activation function) and\nan attention mechanism. We map ¬Øhl from layer l\ninto zl with the feed-forward neural network:\nzl = f(¬Øhl) (4)\nMulti-layer Representation. Since feature effec-\ntiveness differs from layer to layer, we use an at-\ntention mechanism (Luong et al., 2015) to learn to\nweight the features from the last N layers. We get\n7389\nthe multi-layer representation z of the PrLM as:\nz = E(x; Œ∏) =\nL‚àë\ni=L‚àíN+1\nŒ±izi\nŒ±i = etanh(Wattzi)\n‚àëL\nj=L‚àíN+1 etanh(Wattzj)\n(5)\nin which Watt is a matrix of trainable parameters.\nInspired by Berthelot et al. (2019), we want the\nmodel to focus more on the higher-weighted layers,\nso we further calculate the attention weight as:\nŒ±i = Œ±1/œÑ\ni\n‚àëL\nj=L‚àíN+1 Œ±1/œÑ\nj\n(6)\nwhere Œ∏ is a set of learnable parameters that in-\ncludes the parameters from the feed-forward neural\nnetwork and the attention mechanism.\n3.2.3 ClassiÔ¨Åer\nAfter obtaining the multi-layer representationz, we\ntrain a classiÔ¨Åer with the source domain labeled set\nS. We deÔ¨Åne the loss function for the task-speciÔ¨Åc\nclassiÔ¨Åer as:\nLS\npred = 1\n|S|\n‚àë\n‚ü®x,y‚ü©‚ààS\nl\n(\ng(E(x; Œ∏); œÜ),y\n)\n(7)\nwhere gis a classiÔ¨Åer that takes in the features out\nof E, and g is parameterized by œÜ. l is the loss\nfunction which is cross-entropy loss in our work.\n4 Class-aware Feature Self-distillation\nfor Domain Adaptation\nIn this section, we introduce our method for domain\nadaptation. Our domain adaptation loss function\ntakes the form of:\nL= LS\npred + LT‚Ä≤\npred + LCFd (8)\nin which LS\npred is for learning a task-speciÔ¨Åc clas-\nsiÔ¨Åer with the source labeled set S(Eq. 7), LT‚Ä≤\npred\nis the self-training loss trained with the pseudo-\nlabel set T‚Ä≤ (¬ß4.1), and LCFd is to enhance the\nrobustness of self-training by learning discrimina-\ntive features from the PrLM ( ¬ß4.2), which is the\nmain algorithm for domain adaptation in this work.\n4.1 Self-training for Adaptation\nWe build our adaptation model based on self-\ntraining, which predicts pseudo labels on unlabeled\ntarget data. The predicted pseudo labels will be\nused for model training. In the training process, we\npredict pseudo labels on all the target samples in T.\nTo retain high-conÔ¨Ådence predictions from T, we\nùë•Embedding layer\nTransformer\nTransformerTransformer\nFAM\n\"‚Ñé!\nAve\n\"‚Ñé!\nÃÖùë• ùëßùëëùëñùë†ùë°ùëñùëôùëô\nùêøùëéùë†ùë°ùëÅùëôùëéùë¶ùëíùëüùë†\nFigure 2: Illustration of feature self-distillation. We\ntake the sum of the lastN-layer features for distillation.\nintroduce a simple but effective method calledrank-\ndiversify to build the pseudo-label set T‚Ä≤, which is\na subset of T:\nRank. We calculate the entropy loss for every\nsample in T, speciÔ¨Åcally:\ng(z) = Softmax\n(\ng(z)\n)\nLe(z) = ‚àí\n‚àë\ng(z)T log g(z)\n(9)\nin which z is the multi-layer feature and g is the\nclassiÔ¨Åer in Eq. 7. A lower entropy loss indicates\na higher conÔ¨Ådence of the model for the pseudo\nlabel. Then we use the entropy loss to re-rank\nT. However, after re-ranking, some classes may\nhave too many samples in the top K candidates,\nwhich will bias model training, so we also need to\ndiversify the pseudo labels in the top Klist.\nDiversify. We classify the samples into differ-\nent classes with pseudo labels, and re-rank them\nwith entropy loss in ascending order in every class.\nSamples are selected following the order from ev-\nery class in turn until Ksamples are selected.\nWith the retained pseudo-label set T‚Ä≤, we have\nthe loss function for training as:\nLT‚Ä≤\npred = Œ± 1\n|T‚Ä≤|\n‚àë\n‚ü®x,y‚ü©‚ààT‚Ä≤\nl\n(\ng(E(x; Œ∏); œÜ),y\n)\n(10)\nin which Œ±is a hyper-parameter which will increase\ngradually in the training process.\n4.2 Robust Self-training by Discriminative\nFeature Learning\nTo alleviate the negative effects caused by the noisy\nlabels in the pseudo-label set T‚Ä≤, we propose to\nlearn discriminative features from the PrLM.\n4.2.1 Feature Self-distillation\nTo maintain the discriminative power of PrLM\nfeatures, we propose to self-distill the PrLM fea-\ntures into the newly added feature adaptation mod-\nule (FAM). Similar to traditional knowledge distil-\n7390\nlation (Hinton et al., 2015), feature distillation in\nour work is to make the FAM (student) also capable\nof generating discriminative features for adaptation\nas the PrLM (teacher) does. Since the source do-\nmain already has the labeled data, there is no need\nfor self-distillation on the source domain, and we\napply feature self-distillation (Fd) to the target do-\nmain. Inspired by recent work on representation\nlearning (van den Oord et al., 2018; Hjelm et al.,\n2019; Tian et al., 2020), we propose to use mutual\ninformation (MI) maximization for Fd.\nMI for Feature Self-distillation. MI measures\nhow different two random variables are. Maximiz-\ning the MI between them can reduce their differ-\nence. By maximizing the MI between the features\nfrom PrLM and FAM, we can make the two fea-\ntures more similar. We are interested in distilling\nthe PrLM features into the multi-layer represen-\ntation z. We can distill the feature ¬Øhl from any\nlayer linto z. However, only distilling one-layer\nfeature of the PrLM may neglect the information\nfrom other layers, so we use the sum of the last\nN-layer features for distillation1:\n¬Øx =\nL‚àë\ni=L‚àíN+1\n¬Øhi (11)\nThe distillation process is illustrated in Figure 2.\nThen we maximize the MI I(z,¬Øx). We need to Ô¨Ånd\nits lower bound for maximization, since it is hard\nto directly estimate mutual information. Follow-\ning van den Oord et al. (2018), we also use Noise\nContrastive Estimation (NCE) to infer the lower\nbound as:\nI(z,¬Øx) ‚â•Jfeat\nNCE(z,¬Øx) (12)\nTo estimate the NCE loss, we need a negative sam-\nple set in which the PrLM features are randomly\nsampled for the current z. Given a negative sample\nset ¬ØXneg = {¬Øxneg\ni }|¬ØXneg|\ni=1 , we estimate Jfeat\nNCE as:\nJfeat\nNCE = f(z,¬Øx) ‚àí 1\n|¬ØXneg|\n‚àë\n¬Øxneg\ni ‚àà¬ØXneg\nf(z,¬Øxneg\ni )\n(13)\nf(z,¬Øx‚àó) is the similarity function, deÔ¨Åned as:\nf(z,¬Øx‚àó) = cos\n(\ninf(z),¬Øx‚àó)\n(14)\nwhere ¬Øx‚àó‚àà{¬Øx}‚à™ ¬ØXneg; inf(¬∑) is a trainable feed-\nforward neural network followed by the tanh ac-\ntivation, which is to resize the dimension of z to\nbe equal to ¬Øx‚àó. To obtain the negative sample set,\n1Based on Eq.14, taking the sum or average of the last\nN-layer features will have the same effect.\nwe select one negative ¬Øx by randomly shufÔ¨Çing the\nbatch of features which the negative ¬Øx is in, and\nthis process is repeated |¬ØXneg|times.\n4.2.2 Class Information\nFeature distillation can only maintain the discrimi-\nnative power of PrLM features but ignores the class\ninformation present in class labels. To explore the\nclass information, when performing feature self-\ndistillation, we further introduce an intra-class loss\nto minimize the feature distance under the same\nclass. By giving the pseudo-label set T‚Ä≤and the\nsource labeled set S, we group the multi-layer fea-\ntures out of the FAM into different classes. For\nevery class c, we calculate the center feature as zc.\nWe deÔ¨Åne the intra-class loss as follows:\nLintra class =\n‚àë\nc‚ààC\n‚àë\nzi‚ààSc‚à™T‚Ä≤c\n‚à•zi ‚àízc‚à•2 (15)\nwhere Cis the set of classes. The center feature zc\nfor class c‚ààCis calculated as:\nzc = 1\n|Sc ‚à™T‚Ä≤c|\n‚àë\nzj‚ààSc‚à™T‚Ä≤c\nzj (16)\nBefore training for an epoch, the center features\nwill be calculated and Ô¨Åxed during training. After\none epoch of training, the center features will be\nupdated. After the above analysis, our Ô¨Ånal CFd\nloss becomes:\nLCFd = LT\nFd + LS,T‚Ä≤\nC = ‚àí\n‚àë\nx‚ààT\nJfeat\nNCE\n(\nE(x; Œ∏),¬Øx\n)\n+ Œª\n‚àë\n‚ü®x,y‚ü©‚ààS‚à™T‚Ä≤\nLintra class\n(17)\nwhere Œªis a hyper-parameter which controls the\ncontribution of LS,T‚Ä≤\nC .\n4.3 Analysis\nWe provide a theoretical understanding for why\nCFd can enhance self-training based on the domain\nadaptation theory from Ben-David et al. (2010).\nTheorem 1. (Ben-David et al., 2010) Let Hbe the\nhypothesis space. With the generalization error Œ¥s\nand Œ¥t of a classiÔ¨Åer G‚ààH on the source Sand\ntarget T, we have:\nŒ¥t(G) ‚â§Œ¥s(G) + dH‚àÜH(S,T) + œµ (18)\nin which dH‚àÜHmeasures the domain discrepancy\nand is deÔ¨Åned as:\ndH‚àÜH(S,T) = sup\nh,h‚Ä≤‚ààH\n‚èê‚èêEx‚ààS[h(x) Ã∏= h‚Ä≤(x)]\n‚àíEx‚ààT[h(x) Ã∏= h‚Ä≤(x)]\n‚èê‚èê\n(19)\n7391\nMonoAmazonE‚ÜíBK BT‚ÜíBK M‚ÜíBK BK‚ÜíE BT‚ÜíE M‚ÜíE BK ‚ÜíBT E‚ÜíBT M‚ÜíBT BK‚ÜíM E‚ÜíM BT ‚ÜíM Ave.\nDAS 67.12 66.53 70.31 58.73 66.14 55.78 51.30 60.76 50.66 55.98 59.06 60.50 60.24\nxlmr-tuning 70.030.2 69.940.9 70.710.8 61.270.5 68.490.4 63.521.0 66.271.3 69.811.2 68.320.6 61.692.5 59.221.1 61.751.9 65.92\nxlmr-1 64.70 64.26 68.64 53.21 66.39 55.67 57.88 70.10 55.20 61.05 63.92 65.60 63.52\nxlmr-10 70.58 0.3 69.960.6 71.100.5 59.800.3 70.880.3 64.640.7 63.930.9 72.480.5 65.060.9 65.790.4 67.780.4 63.491.0 67.12\nKL 70.91 0.7 71.120.3 72.100.3 65.610.1 70.300.5 66.850.4 67.690.7 72.680.2 70.360.3 67.660.7 66.461.1 68.561.1 69.19\nMMD 71.91 0.7 73.580.6 70.480.8 69.370.6 71.270.5 65.920.9 71.710.5 72.810.5 69.300.5 69.240.5 65.871.0 69.141.0 70.05\nAdv 71.28 0.5 69.531.0 72.390.2 61.200.6 69.980.4 66.470.2 63.911.3 72.840.3 70.470.1 66.530.7 67.650.4 64.471.6 68.06\np 70.90 0.4 71.380.8 72.180.9 64.001.2 70.410.5 67.010.3 67.480.4 71.670.5 70.710.3 67.160.6 67.921.1 69.770.2 69.21\np+CFd 75.25 0.5 74.700.5 75.080.6 70.190.2 72.000.3 68.960.3 71.630.4 73.730.5 70.050.4 70.860.3 69.800.7 70.460.4 71.89\nTable 1: The cross-domain classiÔ¨Åcation accuracy (%) results on MonoAmazon. Models are evaluated by 5 random\nruns except xlmr-tuning which is run for 3 times. We report the mean and standard deviation results. Best task\nperformance is boldfaced. Results of DAS are taken from He et al. (2018).\nDATA train (S) valid (S) test (T) unlabeled (T) |C|\nMonoAmazon 5,000 1,000 6,000 6,000 3\nMultiAmazon 2,000 2,000 2,000 8,000 2\nTable 2: The data splits for the experiments. |C|is the\nnumber of classes. ( ¬∑) denotes the domain which the\ndata comes from.\nand œµis the error of the ideal joint hypothesis which\nis deÔ¨Åned as:\nœµ= Œ¥s(h‚àó) + Œ¥t(h‚àó) (20)\nwhere h‚àó= arg minh‚ààHŒ¥s(h) + Œ¥t(h).\nFrom Ineq. 18, the performance of domain adap-\ntation is bounded by the generalization error on\nthe source domain, domain discrepancy, and the\nerror of the ideal joint hypothesis (joint error). Self-\ntraining aims to learn a low joint error by learning\ndiscriminative features on the target domain, so that\nthe adaptation performance can be improved (Saito\net al., 2017). Our proposed CFd enhances the ro-\nbustness of self-training by self-distilling the PrLM\nfeatures and exploring the class information. In\nthis way, the joint error can be further reduced\ncompared to self-training (Fig. 3). Besides, by op-\ntimizing the intra-class loss, dH‚àÜHin Ineq. 18 can\nbe reduced since under the same class, the feature\ndistance of samples from both the source and target\ndomain is minimized (Fig. 4).\n5 Experiments\n5.1 Datasets\nWe use two Amazon review datasets for evaluation.\nOne is monolingual and the other is multilingual.\nMonoAmazon. This dataset consists of English re-\nviews from He et al. (2018) and has four domains:\nBook (BK), Electronics (E), Beauty (BT), and Mu-\nsic (M). Each domain has 2,000 positive, 2,000\nnegative, and 2,000 neutral reviews.\nMultiAmazon. This is a multilingual review\ndataset (Prettenhofer and Stein, 2010) in English,\nGerman, French, and Japanese. For every language,\nthere are three domains: Book, Dvd, and Music.\nEach domain has 2,000 reviews for training and\n2,000 for test, with 1,000 positive and 1,000 nega-\ntive reviews in each set. 6,000 additional reviews\nform the unlabeled set for each domain. The source\ndomains are only selected from the English corpus.\nTable 2 shows the data split. To construct the\nunlabeled set for the target domain, we use reviews\nfrom the test set as the unlabeled data in MonoAma-\nzon following He et al. (2018). For MultiAmazon,\nreviews from the training set and original unlabeled\nset both from the target domain are combined.\nWe also evaluate our model on the benchmark\ndataset of (Blitzer et al., 2007). The results are\npresented in Appendix B.\n5.2 Experimental Setup\nModel Settings. To enable cross-language trans-\nfer, we use XLM-R2 (Conneau et al., 2019) which\nhas 25 layers as the pre-trained language model.\nThe dimension of its token embeddings is 1024\nwhich is mapped into 256 by the FAM. Based on\none transfer result, the last 10-layer features are\nused in FAM. Œªfor intra-class loss is set as 1 and 2\nfor MonoAmazon and MultiAmazon respectively.\nWe set the size of negative sample set as 10 and we\nperform Fd training only in the target domain.œÑfor\nattention mechanism in Eq. 6 is set as 0.3. In the\ntraining process, we gradually increase the num-\nber of retained pseudo labels for self-training, in\nwhich we increase the number by 100 for MonoA-\nmazon and 300 for MultiAmazon every epoch. Œ±\nfor LT‚Ä≤\npred is the linear and quadratic function of\nepoch for MonoAmazon and MultiAmazon respec-\ntively. More details of the experimental settings are\nin Appendix A.\n2https://github.com/pytorch/fairseq/\ntree/master/examples/xlmr\n7392\nGerman French JapaneseMultiAmazonBook Dvd Music Ave. Book Dvd Music Ave. Book Dvd Music Ave.\nCross-language\nxlmr-tuning 91.030.3 88.020.6 90.130.2 89.73 92.120.5 91.170.3 89.580.8 90.96 87.520.5 87.120.4 88.520.7 87.72\nxlmr-1 73.69 69.86 87.34 76.96 91.26 91.13 88.37 90.25 70.96 71.20 87.07 76.41\nxlmr-10 93.15 0.8 89.591.2 92.260.6 91.67 93.790.4 93.280.4 92.230.6 93.10 87.131.1 88.630.1 88.050.5 87.94\nKL 93.990.4 91.120.4 93.890.2 93.00 93.910.1 93.310.3 92.390.2 93.20 88.600.1 88.820.2 88.120.2 88.51\nMMD 93.97 0.1 90.770.8 93.530.4 92.76 93.480.2 93.210.2 92.670.2 93.12 89.170.1 89.220.1 88.540.4 88.98\nAdv 93.27 0.4 89.780.6 92.530.6 91.86 93.700.4 93.030.4 92.280.3 93.00 88.220.8 88.680.1 88.340.2 88.41\np 92.99 1.0 89.330.6 93.820.3 92.05 93.810.1 93.000.2 92.500.2 93.10 88.680.3 88.860.1 88.390.1 88.64\np+CFd 93.950.2 91.690.3 93.890.2 93.18 94.250.2 93.790.1 93.390.1 93.81 89.410.2 88.680.1 89.540.3 89.21\nCross-language and Cross-domain\nCLDFA 83.95 83.14 79.02 82.04 83.37 82.56 83.31 83.08 77.36 80.52 76.46 78.11\nMAN-MoE 82.40 78.80 77.15 79.45 81.10 84.25 80.90 82.08 62.78 69.10 72.60 68.16\nxlmr-tuning 90.84 88.48 89.75 89.69 90.29 90.54 89.65 90.16 85.90 86.02 87.85 86.59\nxlmr-1 74.10 77.16 66.52 72.59 87.95 88.00 88.15 88.03 76.46 75.20 65.93 72.53\nxlmr-10 91.00 85.95 92.48 89.81 90.17 90.29 92.66 91.04 85.67 85.69 87.89 86.41\nKL 93.24 90.39 93.00 92.21 91.98 92.53 92.81 92.44 86.65 88.21 88.61 87.82\nMMD 93.44 90.50 92.58 92.17 92.70 92.53 93.07 92.77 87.75 88.25 88.73 88.24\nAdv 92.76 88.77 92.80 91.44 91.58 91.70 92.64 91.97 86.88 88.11 88.03 87.67\np 93.11 88.43 92.84 91.46 92.09 92.41 92.52 92.34 87.10 88.22 88.57 87.96\np+CFd 94.29 90.73 93.62 92.88 93.10 92.81 93.62 93.18 88.93 89.00 89.41 89.11\nTable 3: The classiÔ¨Åcation accuracy (%) results on MultiAmazon. Models are evaluated by 5 random runs except\nxlmr-tuning which is run for 3 times. Results of CLDFA and MAN-MoE are taken from Xu and Yang (2017) and\nChen et al. (2019) respectively. More detailed transfer results are included in Appendix D.\nBaselines. Since we are interested in adapting fea-\ntures of PrLMs without tuning, we mainly set up\nthe baselines that use the features from XLM-R\nby freezing XLM-R. Trained on the source do-\nmain, xlmr-1 directly tests on the target without\ndomain adaptation and it only uses the last-layer\nfeatures of XLM-R. xlmr-10 is the same as xlmr-1,\nexcept that it uses the multi-layer representation of\nXLM-R with last 10-layer features. KL (Zhuang\net al., 2015) uses the balanced Kullback-Leibler\ndivergence loss to decrease the domain discrep-\nancy for domain adaptation. MMD adopts the\nMaximum Mean Discrepancy loss (Gretton et al.,\n2012) in which Gaussian Kernel is implemented.\nAdv (Ganin et al., 2016; Chen et al., 2018) adver-\nsarially trains a domain classiÔ¨Åer to learn domain-\ninvariant features by reversing the gradients from\nthe domain classiÔ¨Åer following Ganin et al. (2016).\np is our self-training method introduced in ¬ß4.1.\np+CFd is our full model that uses CFd to enhance\nthe robustness of self-training. DAS (He et al.,\n2018) uses semi-supervised learning. CLDFA (Xu\nand Yang, 2017) is a cross-lingual baseline which\nuses cross-lingual resources. MAN-MoE (Chen\net al., 2019) studies multi-lingual transfer which\nhas multiple languages in the source domain. MoE\nlearns to focus on more transferable source do-\nmains for adaptation. xlmr-10, KL, MMD, Adv,\np, and p+CFd are all based on the multi-layer rep-\nresentations with last 10-layer features. For KL,\nMMD, and Adv, to minimize domain discrepancy,\nwe use an unlabeled set of the same size in the\nsource domain as the target domain.\nxlmr-tuning3 Ô¨Årst Ô¨Åne-tunes XLM-R with\nsource labeled data using the representation from\nthe Ô¨Ånal layer [CLS] and being fed to the classi-\nÔ¨Åer (Devlin et al., 2019), then tests on the target.\nBy setting up this baseline, we want to see how\nwell the feature-based approach works.\nMore detailed baseline settings can be found in\nAppendix A.3.\n5.3 Main Results\nWe conduct experiments in cross-domain (CD),\ncross-language (CL), and both cross-language and\ncross-domain (CLCD) settings. Results of CD are\nevaluated on MonoAmazon (Table 1) and results\nof CL and CLCD are on MultiAmazon (Table 3).\nFor CL, English is set as the source language. The\ndomains in the source and target languages are\nthe same, i.e., When German&book is the target,\nthe source will be English&book. For CLCD, the\nsources are also only from English. For example,\nwhen the target is German&book, the source lan-\nguage is English and the source domain is dvd\nor music, in which two sources are set up: En-\nglish&dvd and English&music, and the two adap-\ntation results are averaged for German&book.\n3Because of the limited computing resources, we cannot\nÔ¨Åne-tune XLM-R with unlabeled target data using LM loss.\n7393\nMETHOD E‚ÜíBK BT‚ÜíBK M‚ÜíBK BK‚ÜíE BT‚ÜíE M‚ÜíE BK‚ÜíBT E‚ÜíBT M‚ÜíBT BK‚ÜíM E‚ÜíM BT‚ÜíM Ave.\nxlmr-10 70.41 67.80 70.83 56.47 70.65 64.74 61.30 71.57 65.38 63.33 67.69 64.47 65.16\np 70.90 71.38 72.18 64.00 70.41 67.01 67.48 71.67 70.71 67.16 67.92 69.77 69.21\np+CFd 75.25 74.70 75.08 70.19 72.00 68.96 71.63 73.73 70.05 70.86 69.80 70.46 71.89\np+C (w/o Fd) 73.16 73.59 74.80 68.72 71.11 68.15 69.80 74.02 71.03 66.78 69.22 68.93 70.78\np+Fd (w/o C) 71.61 71.10 72.39 67.14 71.23 67.38 69.41 73.04 70.80 68.84 68.14 68.97 70.00\nCFd (w/o p) 70.08 72.37 71.30 66.72 70.57 64.21 68.32 72.38 69.27 68.23 66.12 68.37 69.00\nFd (w/o p+C) 68.16 69.55 70.18 66.59 71.02 63.92 69.18 72.10 67.77 69.73 65.71 66.13 68.34\nTable 4: The classiÔ¨Åcation accuracy (%) results of p+CFd and its ablations on MonoAmazon.\nWe have the following Ô¨Åndings from Table 1 and\n3 based on the overall average scores. xlmr-10\nvs. xlmr-tuning: xlmr-10 is slightly better than\nxlmr-tuning which demonstrates the effectiveness\nof the feature-based approach. xlmr-1 vs. xlmr-10:\nxlmr-10 is much better than xlmr-1 which means\nour multi-layer representation of XLM-R is much\nmore transferable than the last-layer feature. xlmr-\n10 vs. p: p is consistently better than xlmr-10\nwhich shows our self-training method is effective.\np vs. p+CFd: After using CFd, p can be con-\nsistently improved and p+CFd achieves the best\nperformance among all the methods, which shows\nthe effectiveness of CFd.\n5.4 Further Analysis\nAblation Study. We conduct the ablation exper-\niments to see the contributions of feature self-\ndistillation (Fd) and class information (C), which\nare evaluated on MonoAmazon based on last 10-\nlayer features. By ablating p+CFd, we have four\nbaselines of p+C (w/o Fd), p+Fd (w/o C), CFd (w/o\np) and Fd (w/o p+C). From the results in Table 4,\np+Fd and p+C perform worse than p+CFd but still\nbetter than p, so feature self-distillation and class\ninformation both contribute to the improvements\nof p. Also, by removing the effects of p, CFd and\nFd substantially outperform xlmr-10, which means\nCFd and Fd are both effective for domain adapta-\ntion, independent of the self-training method.\nJoint errors. Here we study why CFd can en-\nhance self-training and provide empirical results to\ndemonstrate the theoretical understanding in ¬ß4.3.\nBy testing on MonoAmazon based on last 10-layer\nfeatures, Figure 3 presents the joint error results.\nFor example, to Ô¨Ånd h‚àóin Eq. 20 for baseline p,\nfollowing Liu et al. (2019a), we train a classiÔ¨Åer\nusing the combined source and target labeled data\nbased on the Ô¨Åxed FAM trained by p. We note that\np+Fd and p+C can achieve lower joint errors com-\npared to p, and p+CFd has the best performance,\nwhich is consistent with our analysis in ¬ß4.3.\nBK->M BK->BT BK->E\n48\n50\n52\n54\n56\n58\n60Joint error (%)\np\np+Fd\np+C\np+CFd\nFigure 3: The errors of ideal joint hypothesis tested on\nMonoAmazon.\nMETHODM‚ÜíBKBK‚ÜíEM‚ÜíEBK‚ÜíBTM‚ÜíBTBK‚ÜíM Ave.\nSuper 77.54 74.56 74.08 75.66 75.48 72.88 75.03\nFd 76.34 72.44 72.44 74.35 73.15 74.12 73.81\nTable 5: The classiÔ¨Åcation accuracy (%) results of in-\ndomain test evaluated on MonoAmazon.\nEffects of Feature Self-distillation. We conduct\nan in-domain test to verify that Fd learns discrim-\ninative features from the PrLM. We build a sen-\ntiment classiÔ¨Åcation model with in-domain data\nbased on the last 10-layer features. From the same\ndomain in MonoAmazon, we select 4,000 labeled\npairs for training, 1,000 for validation, and 1,000\nfor test. We Ô¨Årst pre-train the FAM by Fd using\nthe entire 6,000 raw texts, then we freeze FAM\nand train a classiÔ¨Åer with the training data with\nfeatures out of FAM. We compare the results with\nthe baseline that directly trains the FAM and classi-\nÔ¨Åer with training set (Super). From the results in\nTable 5, the performances of Fd are very close to\nSuper, showing that the features out of FAM after\nFd training are discriminative.\nEffects of Class Information. Table 6 presents\nthe average intra-class loss in the training process.\nBy exploring class information, the intra-class loss\ncan be dramatically minimized and accordingly the\ntransfer performances are improved.\nA-distance. As an indicator of domain discrep-\nBK‚ÜíM p p+C p+Fd p+CFd\nLintraclass 966.38 11.00 327.72 12.17\nAcc. (%) 66.59 68.88 70.16 70.95\nTable 6: Effects of class information tested on MonoA-\nmazon with last 10-layer features.\n7394\nBK->M BK->BT BK->E\n0.2\n0.5\n0.8\n1.0\n1.2\n1.5\n1.8\n2.0A-distance\np\np+Fd\np+C\np+CFd\nFigure 4: The A-distance tested on MonoAmazon.\nMETHOD One layer last-10 last-20\nA VE ATT A VE ATT\nBK‚ÜíM 69.51 69.20 70.07 66.62 69.31\nBK‚ÜíBT 69.27 67.62 69.34 64.16 69.02\nBK‚ÜíE 66.35 64.62 66.71 62.90 67.08\nTable 7: Study of our attention mechanism based on Fd\nbaseline and tested on MonoAmazon.\nancy, following Saito et al. (2017), we calculate the\nA-distance based on the last 10-layer features out\nof FAM trained by method of p or others, and train\na classiÔ¨Åer to classify the source and target domain\ndata. dAis equal to 2(1 ‚àíŒ¥) and Œ¥is the domain\nclassiÔ¨Åcation error. From Figure 4, p+C and p+CFd\nhave much smaller A-distance, which means that\nthe intra-class loss reduces the domain discrepancy.\np+Fd has larger A-distance, probably because Fd\nlearns domain-speciÔ¨Åc information from the target\nso the domain distance becomes larger.\nEffects of Attention Mechanism. We further\nshow whether combining the intermediate-layer\nfeatures can enhance adaptation. In Table 7, one\nlayer means only using one-layer features for trans-\nfer and the results are obtained by using the fea-\nture from the most transferable layer. We intro-\nduce the attention mechanism to combine the last\nN-layer features. We demonstrate that using last\n10-layer features with attention can achieve better\nperformances. A VE that averages the lastN-layer\nfeatures cannot improve the performance, since it\nlacks the ability to focus more on effective features.\nWe also study how the size of negative sample\nset affects feature distillation and the effects of\nsharpen on attention mechanism. The analysis is\nincluded in Appendix C.\n6 Conclusion\nIn this paper, we study how to adapt the features\nfrom the pre-trained language models without tun-\ning. We speciÔ¨Åcally study unsupervised domain\nadaptation of PrLMs, where we transfer the models\ntrained in labeled source domain to the unlabeled\ntarget domain based on PrLM features. We build\nour adaptation method based on self-training. To\nenhance the robustness of self-training, we present\nthe method of class-aware feature self-distillation\nto learn discriminative features. Experiments on\nsentiment analysis in cross-language and cross-\ndomain settings demonstrate the effectiveness of\nour method.\nAcknowledgments\nThis work was supported by Alibaba Group\nthrough the Alibaba Innovative Research (AIR)\nProgram.\nReferences\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly available clinical\nBERT embeddings. In Proceedings of the 2nd Clin-\nical Natural Language Processing Workshop.\nDevansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas,\nDavid Krueger, Emmanuel Bengio, Maxinder S.\nKanwal, Tegan Maharaj, Asja Fischer, Aaron C.\nCourville, Yoshua Bengio, and Simon Lacoste-\nJulien. 2017. A closer look at memorization in deep\nnetworks. In Proceedings of ICML.\nShai Ben-David, John Blitzer, Koby Crammer, Alex\nKulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. 2010. A theory of learning from different\ndomains. Machine Learning.\nDavid Berthelot, Nicholas Carlini, Ian J. Goodfellow,\nNicolas Papernot, Avital Oliver, and Colin Raffel.\n2019. Mixmatch: A holistic approach to semi-\nsupervised learning. In Proceedings of NeurIPS.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiÔ¨Åcation. In\nProceedings of ACL.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nYu Cao, Meng Fang, Baosheng Yu, and Joey Tianyi\nZhou. 2019. Unsupervised domain adaptation on\nreading comprehension. CoRR, abs/1911.06137.\nOlivier Chapelle and Alexander Zien. 2005. Semi-\nsupervised classiÔ¨Åcation by low density separation.\nIn Proceedings of AISTATS.\nXilun Chen, Ahmed Hassan Awadallah, Hany Hassan,\nWei Wang, and Claire Cardie. 2019. Multi-source\ncross-lingual model transfer: Learning what to share.\nIn Proceedings of ACL.\n7395\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,\nand Kilian Weinberger. 2018. Adversarial deep av-\neraging networks for cross-lingual sentiment classi-\nÔ¨Åcation. TACL.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm¬¥an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, Franc ¬∏ois Lavi-\nolette, Mario Marchand, and Victor S. Lempitsky.\n2016. Domain-adversarial training of neural net-\nworks. Journal of Machine Learning Research.\nYixiao Ge, Dapeng Chen, and Hongsheng Li. 2020.\nMutual mean-teaching: Pseudo label reÔ¨Ånery for\nunsupervised domain adaptation on person re-\nidentiÔ¨Åcation. In Proceedings of ICLR.\nMuhammad Ghifary, W. Bastiaan Kleijn, Mengjie\nZhang, David Balduzzi, and Wen Li. 2016. Deep\nreconstruction-classiÔ¨Åcation networks for unsuper-\nvised domain adaptation. In Proceedings of ECCV.\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,\nBernhard Sch¬®olkopf, and Alexander J. Smola. 2012.\nA kernel two-sample test. Journal of Machine\nLearning Research.\nSuchin Gururangan, Ana Marasovic, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks.\nCoRR, abs/2004.10964.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nEMNLP-IJCNLP.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of EMNLP-IJCNLP.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018. Adaptive semi-supervised learn-\ning for cross-domain sentiment classiÔ¨Åcation. In\nProceedings of EMNLP.\nYulan He, Chenghua Lin, and Harith Alani. 2011.\nAutomatically extracting polarity-bearing topics for\ncross-domain sentiment classiÔ¨Åcation. In Proceed-\nings of ACL.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nR. Devon Hjelm, Alex Fedorov, Samuel Lavoie-\nMarchildon, Karan Grewal, Philip Bachman, Adam\nTrischler, and Yoshua Bengio. 2019. Learning deep\nrepresentations by mutual information estimation\nand maximization. In Proceedings of ICLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model Ô¨Åne-tuning for text classiÔ¨Åcation. In\nProceedings of ACL.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nSamuli Laine and Timo Aila. 2017. Temporal ensem-\nbling for semi-supervised learning. In Proceedings\nof ICLR.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nSeungmin Lee, Dongwan Kim, Namil Kim, and Seong-\nGyun Jeong. 2019. Drop to adapt: Learning discrim-\ninative features for unsupervised domain adaptation.\nIn Proceedings of ICCV.\nJuntao Li, Ruidan He, Hai Ye, Hwee Tou Ng, Lidong\nBing, and Rui Yan. 2020. Unsupervised domain\nadaptation of a pretrained cross-lingual language\nmodel. In Proceedings of IJCAI-PRICAI.\nHong Liu, Mingsheng Long, Jianmin Wang, and\nMichael I. Jordan. 2019a. Transferable adversarial\ntraining: A general approach to adapting deep clas-\nsiÔ¨Åers. In Proceedings of ICML.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019b. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of NAACL-HLT.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In Proceedings of ACL.\nMingsheng Long, Yue Cao, Jianmin Wang, and\nMichael I. Jordan. 2015. Learning transferable fea-\ntures with deep adaptation networks. InProceedings\nof ICML.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of\nEMNLP.\n7396\nXiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallap-\nati, and Bing Xiang. 2019. Domain adaptation with\nBERT-based domain classiÔ¨Åcation and data selec-\ntion. In Proceedings of the 2nd Workshop on Deep\nLearning Approaches for Low-Resource NLP.\nA¬®aron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. CoRR, abs/1807.03748.\nSinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang\nYang, and Zheng Chen. 2010. Cross-domain senti-\nment classiÔ¨Åcation via spectral feature alignment. In\nProceedings of WWW.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of NAACL-HLT.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? Adapting pretrained\nrepresentations to diverse tasks. Proceedings of the\n4th Workshop on Representation Learning for NLP.\nPeter Prettenhofer and Benno Stein. 2010. Cross-\nlanguage text classiÔ¨Åcation using structural corre-\nspondence learning. In Proceedings of ACL.\nJoaquin Quionero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D Lawrence. 2009. Dataset\nShift in Machine Learning. The MIT Press.\nAlexander Rietzler, Sebastian Stabinger, Paul Opitz,\nand Stefan Engl. 2019. Adapt or get left behind:\nDomain adaptation through BERT language model\nÔ¨Ånetuning for aspect-target sentiment classiÔ¨Åcation.\nCoRR, abs/1908.11860.\nKuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.\n2017. Asymmetric tri-training for unsupervised do-\nmain adaptation. In Proceedings of ICML.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.\nContrastive representation distillation. In Proceed-\nings of ICLR.\nRuochen Xu and Yiming Yang. 2017. Cross-lingual\ndistillation for text classiÔ¨Åcation. In Proceedings of\nACL.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V . Le.\n2019. XLNet: generalized autoregressive pretrain-\ning for language understanding. In Proceedings of\nNeurIPS.\nHai Ye, Wenhan Chao, Zhunchen Luo, and Zhoujun Li.\n2017. Jointly extracting relations with class ties via\neffective deep ranking. In Proceedings of ACL.\nHai Ye, Wenjie Li, and Lu Wang. 2019. Jointly learn-\ning semantic parser and natural language generator\nvia dual information maximization. In Proceedings\nof ACL.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2017. Understand-\ning deep learning requires rethinking generalization.\nIn Proceedings of ICLR.\nFuzhen Zhuang, Xiaohu Cheng, Ping Luo, Sinno Jialin\nPan, and Qing He. 2015. Supervised representation\nlearning: Transfer learning with deep autoencoders.\nIn Proceedings of IJCAI.\nYang Zou, Zhiding Yu, B. V . K. Vijaya Kumar, and\nJinsong Wang. 2018. Unsupervised domain adap-\ntation for semantic segmentation via class-balanced\nself-training. In Proceedings of ECCV.\nYang Zou, Zhiding Yu, Xiaofeng Liu, B. V . K. Vijaya\nKumar, and Jinsong Wang. 2019. ConÔ¨Ådence regu-\nlarized self-training. In Proceedings of ICCV.\nA Experimental Settings\nA.1 Datasets\nWe obtain the datasets from He et al. (2018) which\ncan be downloaded online 4. Then we follow He\net al. (2018) to pre-process the datasets which only\ninvolves splitting the data into training, validation,\nand test sets.\nA.2 Model ConÔ¨Åguration\nFor MonoAmazon, the learning rate is 0.0001, and\nthe batch size is 50 for classiÔ¨Åer training and MI\nlearning. We run 35 times for each baseline except\nxlmr-1 and xlmr-10 which are run 20 times and\nthe batch size is 100. In epoch 0, we set to retain\nthe top 950 high-conÔ¨Ådence predictions for self-\ntraining and we increase the number of retained\ndata by 100 every epoch. Œªfor Fd training is 1.\nFor MultiAmazon and Benchmark, the learning\nrate is 0.0005. The batch size for classiÔ¨Åer learn-\ning is 50 and for MI training is 200. The training\nepoch is 20. Œªfor MultiAmazon and Benchmark\nis 2. In epoch 0, we set to retain the top 1000\nhigh-conÔ¨Ådence predictions for self-training. We\nincrease by 150 retained samples every epoch for\nBenchmark, and by 300 for MultiAmazon.\nŒ±for LT‚Ä≤\npred is the linear function of epoch for\nMonoAmazon, and the quadratic function for Mul-\ntiAmazon and Benchmark. Adam (Kingma and Ba,\n2015) is used for model training. In the training\nprocess, if the validation performance does not im-\nprove after 10 consecutive epochs, the learning rate\nwill be halved.\nFor all the datasets, the size of negative sample\nset is set as 10. œÑ for attention mechanism is set as\n0.3, tuned from {0.1, 0.3, 0.5, 0.8, 1.0}.\n4https://github.com/ruidan/DAS\n7397\nparameter MonoAmazon MultiAmazon Benchmark\nlearning rate 0.0001 0.0005 0.0005\nŒª 1 2 2\nŒ± linear function of epoch quadratic function of epoch quadratic function of epoch\nMax epoch 35 20 20\nSize of negative sample set 10 10 10\nTable 8: Hyper-parameter settings for main experiments.\nDATA train (S) valid (S) test (T) unlabeled (T) |C|\nBenchmark 1,600 400 400 6,000 2\nTable 9: The data split for training, validation, test,\nand unlabeled set on Benchmark. |C|is the number\nof classes.\nA.3 Settings for Baselines\nKL. The KL-divergence loss (Zhuang et al., 2015)\nis deÔ¨Åned as:\nKL = DKL(Œæs||Œæt) + DKL(Œæt||Œæs) (21)\nwhere\nŒæ‚Ä≤\ns = 1\nn\nn‚àë\ni=1\nzi\ns Œæs = softmax(Œæ‚Ä≤\ns)\nŒæ‚Ä≤\nt = 1\nn\nn‚àë\ni=1\nzi\nt Œæt = softmax(Œæ‚Ä≤\nt)\n(22)\nin which n is the batch size. We set the weight\nof KL loss as 500, tuned from {100, 500, 1000,\n5000}.\nMMD. We use the Gaussian kernel to implement\nthe MMD loss (Gretton et al., 2012). The kernel\nnumber is 5. The weight for MMD loss is set to 1,\ntuned from {1, 0.1, 0.5}\nAdv. We follow Ganin et al. (2016) to reverse\nthe gradients from the domain classiÔ¨Åer. We set\nthe learning rate for Adv to be the same as the\nbaselines, but set the weight for domain classiÔ¨Åer\nas 0.01, tuned from {1, 0.1, 0.01, 0.001}.\nxlmr-tuning. The Ô¨Åne-tuning baseline uses the\nÔ¨Årst [CLS] token as the document representation.\nThe learning rate is 1e-5 and the batch size for\ngradient update is 32. The Ô¨Åne-tuning models gen-\nerally overÔ¨Åt the training data in 5 epochs.\nB Results on Benchmark\nBenchmark. This is a benchmark dataset for do-\nmain adaptation (Blitzer et al., 2007), whose re-\nviews are also in English. Four domains are in-\ncluded: Book (B), DVDs (D), Electronics (E), and\nKitchen (K). Each domain has 1,000 positive and\n1,000 negative reviews. Following He et al. (2018),\nthere are 4,000 unlabeled reviews for each domain.\nTable 9 summarizes the data split when training on\nBenchmark. The unlabeled set is the combination\nof the training set and the original unlabeled set.\nTable 10 shows the results on Benchmark.\nC Further Analysis\nSize of Negative Sample Set. We study how the\nsize of negative sample set will affect Fd training.\nThe results are shown in Fig. 5. The method used\nis xlmr-10+Fd. We Ô¨Ånd that using a size that is too\nsmall or too big is not a good strategy for Fd learn-\ning. Size of 10 is a good option for Fd learning.\nEffects of Sharpen on Attention Mechanism. In\nFig. 6, we show the effects of sharpen mechanism\nin our attention method which demonstrates that\nwhen not using sharpen (œÑ is ‚àû), the performance\nwill drop and œÑ set as 0.3 is a good option for our\nattention method.\nD Full Results on MultiAmazon\nTable 11 shows the full results on MultiAmazon.\n7398\nBenchmarkD‚ÜíB K ‚ÜíB E ‚ÜíB B ‚ÜíD K ‚ÜíD E ‚ÜíD B ‚ÜíK D ‚ÜíK E ‚ÜíK B ‚ÜíE D ‚ÜíE K ‚ÜíE Ave.\nAsyTri 73.20 72.50 73.20 80.70 74.90 72.90 82.50 82.50 86.90 79.80 77.00 84.60 78.39\nDAS 82.05 80.05 80.00 82.75 81.40 80.15 82.25 81.50 84.85 81.15 81.55 85.80 81.96\nxlmr-1 88.50 78.45 82.50 85.25 80.55 81.80 84.50 81.15 88.45 81.25 79.35 90.05 83.48\nxlmr-10 91.30 1.0 87.951.0 87.950.3 87.900.5 87.050.6 86.850.4 90.451.0 87.551.5 92.300.7 88.900.5 89.051.7 91.600.3 89.07\nKL 91.500.8 88.950.6 88.050.5 87.200.6 87.850.5 87.300.6 90.001.0 91.150.3 92.700.4 89.700.6 90.650.2 91.351.0 89.70\nMMD 91.75 0.5 88.651.1 87.550.9 87.050.7 86.450.3 86.500.6 90.050.3 90.700.5 92.300.3 90.150.3 91.500.6 91.650.7 89.53\nAdv 91.40 0.8 88.100.4 88.150.4 87.701.0 87.350.8 86.650.3 90.650.5 87.551.5 92.250.2 89.250.5 89.801.3 91.600.6 89.20\np 91.40 0.3 89.500.4 88.200.6 87.400.3 87.150.3 87.050.9 90.000.6 87.551.7 92.600.3 88.850.2 89.651.9 91.850.4 89.27\np+CFd 91.50 0.4 89.750.8 88.650.4 87.650.1 87.800.4 88.200.4 92.450.6 92.450.2 93.600.5 91.300.2 91.550.3 92.600.5 90.63\nTable 10: The cross-domain classiÔ¨Åcation accuracy (%) results on Benchmark. Models are evaluated by 5 random\nruns. We report the mean and standard deviation results. The best task performance is boldfaced. Results of DAS\nand AsyTri are taken from He et al. (2018) and Saito et al. (2017) respectively. AsyTri (Saito et al., 2017) is a\nself-training baseline with tri-training.\n0 1 10 20 30 50\nNum. of negative samples\n58\n60\n62\n64\n66\n68\n70Acc.(%)\n BK->M\nBK->BT\nBK->E\nFigure 5: The effects of the negative sample set size for feature self-distillation. Method is xlmr-10+Fd which is\nevaluated on MonoAmazon.\n0.1 0.3 0.5 0.8 1\n64\n66\n68\n70\n72Acc.(%)\nBK->M\nBK->BT\nBK->E\nFigure 6: Effects of sharpen on MonoAmazon with method of xlmr-10+Fd.\n7399\nEnglish‚ÜíGerman\nS book dvd music book dvd music book dvd music\nT book book book dvd dvd dvd music music music Ave.\nxlmr-tuning 91.030.3 91.030.5 90.650.3 88.470.4 88.020.6 88.480.3 89.750.4 89.750.7 90.130.2 89.70\nxlmr-1 73.69 62.08 86.12 68.03 69.86 86.28 66.4 66.63 87.34 74.05\nxlmr-10 93.15 0.8 93.790.6 88.201.4 87.221.1 89.591.2 84.681.3 92.331.5 92.630.5 92.260.6 90.43\nKL 93.990.4 93.990.1 92.490.2 90.810.4 91.120.4 89.960.3 93.130.1 92.870.4 93.890.2 92.47\nMMD 93.97 0.1 93.810.4 93.070.1 90.890.3 90.770.8 90.100.2 92.920.1 92.230.5 93.530.4 92.37\nAdv 93.27 0.4 94.110.6 91.410.3 90.391.2 89.780.6 87.140.4 92.990.2 92.610.4 92.530.6 91.58\np 92.99 1.0 93.890.5 92.330.1 87.831.5 89.330.6 89.030.6 92.970.3 92.700.3 93.820.3 91.65\np+CFd 93.950.2 94.830.1 93.740.2 91.030.1 91.690.3 90.420.4 93.590.3 93.650.3 93.890.2 92.98\nEnglish‚ÜíFrench\nS book dvd music book dvd music book dvd music\nT book book book dvd dvd dvd music music music Ave.\nxlmr-tuning 92.120.5 90.700.3 89.880.9 90.700.4 91.170.3 90.380.5 90.170.5 89.130.5 89.580.8 90.43\nxlmr-1 91.26 89.44 86.46 89.33 91.13 86.67 87.18 89.11 88.37 88.77\nxlmr-10 93.79 0.4 92.670.7 87.670.8 93.210.2 93.280.4 87.371.8 92.860.4 92.450.5 92.230.6 91.73\nKL 93.91 0.1 93.590.2 90.370.2 92.960.3 93.310.3 92.090.2 92.510.7 93.110.1 92.390.2 92.69\nMMD 93.48 0.2 93.550.2 91.850.6 92.850.2 93.210.2 92.210.2 93.340.4 92.800.6 92.670.2 92.88\nAdv 93.70 0.4 93.420.3 89.730.7 93.140.5 93.030.4 90.260.6 92.430.6 92.850.3 92.280.3 92.32\np 93.81 0.1 93.570.2 90.610.5 93.140.3 93.000.2 91.680.3 92.240.7 92.800.3 92.500.2 92.59\np+CFd 94.25 0.2 93.400.3 92.800.2 93.100.4 93.790.1 92.510.1 93.330.6 93.910.2 93.390.1 93.39\nEnglish‚ÜíJapanese\nS book dvd music book dvd music book dvd music\nT book book book dvd dvd dvd music music music Ave.\nxlmr-tuning 87.520.5 85.900.6 85.900.4 86.130.3 87.120.4 85.900.4 88.180.2 87.520.4 88.520.7 86.96\nxlmr-1 70.96 68.18 84.73 64.96 71.2 85.43 61.81 70.04 87.07 73.82\nxlmr-10 87.13 1.1 87.520.6 83.811.6 87.881.0 88.630.1 83.492.3 88.940.3 86.831.4 88.050.5 86.92\nKL 88.60 0.1 87.530.4 85.760.5 88.880.3 88.820.2 87.530.2 88.800.4 88.410.2 88.120.2 88.05\nMMD 89.17 0.1 88.200.1 87.290.2 88.800.3 89.220.1 87.690.5 89.230.3 88.230.5 88.540.4 88.49\nAdv 88.22 0.8 87.720.3 86.040.5 88.640.5 88.680.1 87.570.4 88.171.3 87.890.3 88.340.2 87.92\np 88.68 0.3 87.950.2 86.250.5 88.770.2 88.860.1 87.670.2 88.890.3 88.250.3 88.390.1 88.19\np+CFd 89.41 0.2 88.780.2 89.080.1 88.770.5 88.680.1 89.220.3 89.830.2 88.980.2 89.540.3 89.14\nTable 11: Full classiÔ¨Åcation accuracy (%) results on MultiAmazon. Models are evaluated by 5 random runs except\nxlmr-tuning which is run for 3 times to save time. We report the mean and standard deviation results. The best task\nperformance is boldfaced.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8419556617736816
    },
    {
      "name": "Discriminative model",
      "score": 0.7502950429916382
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7326357364654541
    },
    {
      "name": "Domain adaptation",
      "score": 0.6709340810775757
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6367453932762146
    },
    {
      "name": "Training set",
      "score": 0.5491735935211182
    },
    {
      "name": "Language model",
      "score": 0.528312623500824
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5235647559165955
    },
    {
      "name": "Machine learning",
      "score": 0.48693740367889404
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4746609032154083
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4540844261646271
    },
    {
      "name": "Hidden Markov model",
      "score": 0.4348791539669037
    },
    {
      "name": "Natural language processing",
      "score": 0.3839239180088043
    },
    {
      "name": "Speech recognition",
      "score": 0.3319552540779114
    },
    {
      "name": "Classifier (UML)",
      "score": 0.1971343755722046
    },
    {
      "name": "Mathematics",
      "score": 0.06463417410850525
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I185940356",
      "name": "Soochow University",
      "country": "TW"
    }
  ]
}