{
  "title": "Demystifying Prompts in Language Models via Perplexity Estimation",
  "url": "https://openalex.org/W4389520255",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2671828835",
      "name": "Hila Gonen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3121774788",
      "name": "Srini Iyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2516232129",
      "name": "Terra Blevins",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166589550",
      "name": "Noah Smith",
      "affiliations": [
        "Allen Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A334758317",
      "name": "Luke Zettlemoyer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3198963017",
    "https://openalex.org/W2963001247",
    "https://openalex.org/W4308245305",
    "https://openalex.org/W4287891033",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4389518888",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281784180",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4288804650",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2891575196",
    "https://openalex.org/W2784720878",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4389519967",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W2993818995",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W3174770825"
  ],
  "abstract": "Language models can be prompted to perform a wide variety of tasks with zero- and few-shot in-context learning. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens. In this paper, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is predicted by the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt, the better it is able to perform the task, when considering reasonable prompts that are related to it. As part of our analysis, we also devise a method to automatically extend a small seed set of manually written prompts by paraphrasing with GPT3 and backtranslation. This larger set allows us to verify that perplexity is a strong predictor of the success of a prompt and we show that the lowest perplexity prompts are consistently effective.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10136–10148\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDemystifying Prompts in Language Models via Perplexity Estimation\nHila Gonen1,2 Srini Iyer2 Terra Blevins1 Noah A. Smith1,3 Luke Zettlemoyer1,2\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Meta AI Research 3Allen Institute for Artificial Intelligence\nhilagnn@gmail.com\nsviyer@meta.com\n{blvns,nasmith,lsz}@cs.washington.edu\nAbstract\nLanguage models can be prompted to perform a\nwide variety of tasks with zero- and few-shot in-\ncontext learning. However, performance varies\nsignificantly with the choice of prompt, and we\ndo not yet understand why this happens. In this\npaper, we analyze the factors that contribute\nto this variance and establish a new empirical\nhypothesis: the performance of a prompt is\npredicted by the extent to which the model is\nfamiliar with the language it contains. Over a\nwide range of tasks, we show that the lower the\nperplexity of the prompt, the better it is able\nto perform the task, when considering reason-\nable prompts that are related to it. As part of\nour analysis, we also devise a method to auto-\nmatically extend a small seed set of manually\nwritten prompts by paraphrasing with GPT3\nand backtranslation. This larger set allows us\nto verify that perplexity is a strong predictor\nof the success of a prompt and we show that\nthe lowest perplexity prompts are consistently\neffective.\n1 Introduction\nLanguage models can be prompted to perform\na wide range of zero- and few-shot learning\ntasks (Brown et al., 2020; Schick and Schütze,\n2020). However, there is significant variance in the\nperformance of seemingly similar prompts (Chen\net al., 2022): for AG News (Zhang et al., 2015), we\nfind an over 30 point accuracy gap between differ-\nent manually curated prompts (see Table 1) on OPT\n175B (Zhang et al., 2022). Despite efforts to im-\nprove prompt engineering (Shin et al., 2020; Li and\nLiang, 2021; Gao et al., 2021), it is still challenging\nto develop high-quality prompts for new tasks, and\nlittle is known about why this phenomenon occurs.\nWe are interested in understanding what makes\nsome prompts better than others, and using this un-\nderstanding to create better prompts for given tasks\nand models. We hypothesize that the lower the per-\nplexity of a prompt is, the better its performance\nFigure 1: Accuracy vs. perplexity for the AG News\ndataset with OPT 175B. The x axis is in log scale. Each\npoint stands for a different prompt.\non the task will be, when considering reasonable\nprompts that are related to the task. This is based\non the intuition that the more frequently the prompt\n(or very similar phrases) appears in the training\ndata, the more the model is familiar with it and\nis able to perform the described task. We refrain\nfrom using the training data directly as it is often\nunavailable, expensive to search due to its size, and\nhard to use for approximate matching of similar\nprompts. Instead, we focus on the perplexity of the\nprompt as a proxy for its occurrences in the data.\nTo enable more complete analysis, we automati-\ncally expand the set of manually created prompts\nfor the task by paraphrasing, resulting in a much\nlarger and diverse set of prompts. We focus on\nprompts in English that reasonably describe the\ntask for two reasons: (a) our main motivation is to\nunderstand what lies under the variance of perfor-\nmance in this type of prompt; (b) we aim to devise\na useful method for creating prompts that are con-\nsistently effective, that could be easily adopted and\ninterpreted by future, potentially non-expert users.\nWe show empirically that our hypothesis holds\nacross a diverse set of tasks (including classifi-\ncation and word prediction), models, and model\n10136\nsizes, providing us some insights about the under-\nlying mechanism of prompting (see Figure 1). As\na result, we devise a method, SPELL (Selecting\nPrompts by Estimating LM Likelihood), for cre-\nating prompts in an informed manner. We show\nthat using SPELL to choose prompts results in less\nvariability in performance as well as in accuracy\ngains (1.8 accuracy points with OPT and 2.3 accu-\nracy points with Bloom on average). Importantly,\nour method does not require labels at all, only a\nsmall sample of inputs for the task.\nOur contributions can be summarized as follows:\n(a) we formalize the notion that better familiarity\nof the model with the prompt correlates with better\nperformance (Section 2); (b) we automatically elab-\norate a given set of seed prompts using paraphras-\ning (Section 3); (c) we establish experimentally the\nhypothesis that lower perplexity of the prompt cor-\nrelates well with better performance (Section 5);\n(d) we devise a method to create a more consistent\nset of prompts, that also improve results even with\nno labels for the task (Section 7).\n2 Why are prompts not all created equal?\nDespite the popularity of prompting as a method\nfor using language models (Shin et al., 2020; Li\nand Liang, 2021; Gao et al., 2021), the cause for\nthe different behavior of various prompts remains\nunclear so far. Table 1 shows four example prompts\nfor a news topic classification task (AG News) and\ntheir respective accuracies when used to prompt\nOPT 175B (Zhang et al., 2022). The accuracy gap\nbetween the different prompts is not trivial, and it\nis not possible to predict from the prompts alone.\nPrompt Accuracy\nWhat is this piece of news regarding? 40.9\nWhat is this article about? 52.4\nWhat is the best way to describe this article?68.2\nWhat is the most accurate label for this news article?71.2\nTable 1: Example prompts for the task AG News (news\nclassification) that vary considerably in accuracy.\nWe propose that the more frequently a prompt\nappears in some variation in the data, the better\nit works for the task. The intuition behind this\nis that a sequence that is more expected by the\nmodel is more likely to aid the model to extract\nthe relevant information. However, this premise is\nhard to measure accurately: most language models\nuse huge amounts of training data (e.g., OPT uses\na corpus of roughly 180B tokens, and Bloom uses\nroughly 366B tokens), and in addition, this training\ndata is not always publicly available (e.g., GPT3;\nBrown et al. 2020). Our initial attempts to estimate\nexact-match occurrences of prompts in the data\nresulted in very sparse counts, which led us to look\nfor a softer formalization.1\nInstead of considering the training data directly,\nwe propose to focus on theperplexity of the prompt\nas a proxy for its occurrences in some form in\nthe data – essentially indicating to what extent the\nmodel expects this prompt. This perplexity-based\nframing helps to avoid the challenge of exact match\nin the data, and takes into account variations of the\nprompt that the model is also exposed to and might\nbe influenced by. In addition, it helps overcome the\nchallenges mentioned above as it requires neither\naccess to the pretraining data (which is not always\npublicly available for LMs) nor matching over huge\namounts of text.\nHypothesis: lower perplexity correlates with bet-\nter performance. We hypothesize that on aver-\nage, lower-perplexity prompts perform better. We\nare interested in establishing this hypothesis by\nexperimentally showing a significant negative cor-\nrelation between the perplexity of the prompt and\nits performance on the task, across a diverse set of\ntasks and models.\nWe define the perplexity of the prompt as the\nperplexity of the full prompt sequence, including\nthe input itself, and without the label, averaged\nover 1,000 examples (see Section 4 for details).\nThe input is a part of the prompt in the case of\nthe word prediction tasks by design (e.g., “The\nopposite of the word good is”). Inclusion of the\ntask input as part of the prompt for classification\ntasks as well is intentional: we want to ground the\nprompt to the task (without the input, we are testing\nthe hypothesis that lower perplexity prompts across\nall tasks work better on every task). The label is\nnot considered a part of the prompt and is not taken\ninto consideration when computing the prompt. In\npractice, this also results in a huge advantage of our\nmethod, SPELL (Section 7), which aims to find\nbetter prompts—it does not require any labels.\nFor performance measures, we use the log-\nlikelihood score assigned by the model to the cor-\nrect label given that prompt. We choose this metric\n1We experimented with the task of AG News (see Sec-\ntion 4.1), and looked for all of its prompts (using exact match)\nin the OPT training data. Indeed, only 9/108 of the prompts\nappear in the training data. Such sparse counts do not allow\nfor any useful or reliable analysis of prompt behaviour.\n10137\nover accuracy as it gives a more fine-grained dis-\ntinction between prompts and because accuracy can\nbe unstable, as explained in more detail in Section 4.\nFor classification tasks, we also report correlation\nwith accuracy, which is the main evaluation metric\nfor this type of task.\n3 Automatic Expansion of Seed Prompts\nWe are interested in expanding our pool of prompts\nin order to: (a) have a more diverse set of prompts,\nmaking it more likely to find a better prompt for\nour task, and (b) support better analysis to validate\nour prompt quality hypothesis. In this section, we\ndescribe our method for automatically expanding\na seed set of manually created prompts using para-\nphrasing.\nStep 0: Creating a seed set of manually-written\nprompts We first write/collect a small set of hu-\nman written prompts that describe the task. For\nclassification tasks we assume that the input ap-\npears before the prompt, with no choices appearing\nas part of the prompt (to help in smooth paraphras-\ning of the prompt itself).\nStep 1: Paraphrasing with GPT3 We use the\ntext-davinci-002 version of GPT3 (Brown et al.,\n2020) to generate paraphrases for each of the man-\nual prompts in our seed set. We prompt it with\na meta-prompt for paraphrasing to generate varia-\ntions of one of our seed prompts. An example of\nsuch a meta-prompt is: Write a paraphrase for the\nfollowing sentence: <seed prompt> Paraphrase:.\nThe 7 meta-prompts used in this step are listed in\nTable 2.\nWe choose GPT3 as our paraphrasing model\nbecause of its well-documented generation abilities.\nThis is also to ensure that there is a separation\nbetween the model we use to create the prompts and\nthe models we use to rank them (OPT and Bloom,\nsee Section 4 for details), to avoid confounding the\nexperimental setup.\nStep 2: Paraphrasing using backtranslation\nOur second step takes as input the paraphrases from\nGPT3 (in addition to the seed set of prompts) and\ntranslates them into different languages and back\ninto English to get additional prompt paraphrases\n(Wieting et al., 2017). We use a set of 8 languages\navailable in the NLLB translation model (Costa-\njussà et al., 2022) that are relatively high resource\nand close to English,2 to reduce the risk of noise.\nSince we aim to get about 100 prompts per task,\nwe add 8 additional languages3 in the case where\nthe basic 8 languages yielded too few alternatives.\nFor word prediction tasks, we use the sequence of\nthe created prompt up to the index of the label, not\nincluding the label, for example: The word “dog”\nin French is “. Depending on the task, we enforce\nthe existence of specific words (e.g., the name of\nthe language, and the source word, in word-level\ntranslation) or enforce the prompt to be a question.\nExamples and Statistics Table 4 lists all 4 man-\nually created prompts we use for the AG News\ntask (news classification), alongside a few sampled\nprompts created automatically using our method.\nAs was typically the case, we are able to get\nprompts that are rather different in phrasing and\nstructure from those included in the seed set.\nThe statistics of the prompts in the manually\ncreated seed set (Step 0) as well as the prompts after\nStep 1 and Step 2 for each task (see Section 4.1 for\ndetails about the tasks) are detailed in Table 3.\n4 Experimental Setup\n4.1 Models, Tasks and Datasets\nWe study four auto-regressive models: OPT (Zhang\net al., 2022) of different sizes (1.3B, 30B, 175B\nparameters), all trained mainly on English, 4 and\nBloom (176B parameters; Luccioni et al. 2022),\nwhich is trained on 46 natural languages and 13\nprogramming languages. We experiment with two\ntypes of tasks: word prediction tasks and classifica-\ntion tasks, as detailed below.\nWord Prediction Tasks The first task in this cat-\negory is word-level translation. Given a source\nword in English and a target language, we expect\nthe model to predict the correct translation. For this\ntask we use NorthEuraLex5 (Dellert et al., 2019),\na lexical database providing translations of 1016\nwords into 107 languages. We experiment with\n9 languages that use the Latin script. For Bloom,\nwe use 5 additional languages that do not use the\n2Danish, German, Italian, French, Dutch, Portuguese,\nSwedish, Spanish.\n3Norwegian, Romanian, Catalan, Turkish, Ukrainian, Pol-\nish, Russian, Arabic.\n4As stated in the paper, the training corpora were previ-\nously collected or filtered to contain predominantly English\ntext, but a small amount of non-English data is still present\nwithin the corpus via CommonCrawl.\n5http://northeuralex.org/\n10138\nMeta prompts\nWrite a paraphrase for the following sentence: <seed-prompt> Paraphrase:\n<seed-prompt> Paraphrase:\nWrite a likely paraphrase of the text: <seed-prompt> Paraphrase:\nWrite a sentence similar to the following one: <seed-prompt> Paraphrase:\nParaphrase the following sentence: <seed-prompt> Paraphrase:\nWrite a variation of this sentence: <seed-prompt>\nHow would you say the following sentence in a different way? <seed-prompt>\nTable 2: Meta prompts used in Step 1 of our method for paraphrasing using GPT3.\nTask # Step 0 # Step 1 # Step 2\nWord-Level Translation12 59 118\nAntonyms 12 85 176\nGLUE Cola 4 27 144\nNewspop 13 43 119\nAG News 4 23 108\nIMDB 10 45 178\nDBpedia 8 23 103\nEmotion 4 14 94\nTweet Offensive 5 41 119\nTable 3: Number of prompts for the different tasks:\nprompts after step 0 (creating prompts manually),\nprompts after step 1 (GPT3 paraphrasing), and prompts\nafter step 2 (backtranslation).\nLatin script (since Bloom is multilingual). Note\nthat only 5 of the languages we experiment with\nare officially covered by Bloom.6\nWe also consider antonym prediction where,\ngiven a word, the model is expected to predict its\nantonym. For this task, we use data from Kaggle,7\nwhich is based on WordNet (Miller, 1995). We\nchoose 1,000 word pairs at random.\nClassification Tasks We choose classification\ntasks from Huggingface Datasets,8 with an attempt\nto have a set of diverse tasks that use relatively short\ninputs, with some prompts available in Prompt-\nSource (Bach et al., 2022):9 (a) GLUE Cola (gram-\nmaticality; Warstadt et al. 2018); (b) Newspop\n(news classification; Moniz and Torgo 2018); (c)\nAG News (news classification; Zhang et al. 2015);\n(d) IMDB (movie review classification; Maas et al.\n2011); (e) DBpedia (topic classification; Lehmann\net al. 2015); (f) Emotion (classification to emo-\n6Basque, French, Portuguese, Spanish, and Arabic.\n7https://www.kaggle.com/datasets/duketemon/\nantonyms-wordnet\n8https://huggingface.co/docs/datasets/index\n9https://github.com/bigscience-workshop/\npromptsource\ntions; Saravia et al. 2018); (g) Tweet Offensive\n(classification to offensive vs. not offensive tweets;\nBarbieri et al. 2020). We use 1,000 random exam-\nples from each dataset.\nThe full set of manual prompts is listed in Sec-\ntion A in the Appendix. In these tasks, the prompt\nfollows the input, and at the end of each prompt\nwe add the choices of classes (i.e., we provide the\npossible labels explicitly in the prompt by listing\nthe possible answers as defined by the dataset it-\nself.): “Choices: X, Y, Z. Answer:”as we find it\nhelps in terms of accuracy. Defining the label space\nlikely helps in our zero-shot setting because there\nare no previous demonstrations from which the\nmodel can learn the possible classes. Additionally,\nadding class options to the prompt helps to reduce\nthe effect of the surface form competition (Holtz-\nman et al., 2021). The option of generating the\nanswer and comparing it with the gold label was\nnot reasonable here, since we cannot expect the\nmodel to generate the exact label as the first choice\noften enough.\n4.2 Implementation Details\nIn all experiments we evaluate zero-shot perfor-\nmance. To avoid noise when computing perplexity,\nwe instantiate the prompts with 1,000 examples of\nthe dataset, compute the perplexity of the prompt\nwith each example, and calculate the average across\nall instantiated prompts.\nTo estimate the performance of the prompt, we\nlook at two measures: (a) the language model\nscore (log probability) of the correct label, aver-\naged across 1,000 examples; (b) the accuracy on\nthe task, computed over the 1,000 examples. To\ncompute accuracy, for each example we score all\nclasses and choose the highest ranking class as the\nprediction of the model. The score of a label of\nmultiple tokens is defined by the sum of the token\n10139\nAll Manually Created Prompts Examples of Similar Automatically Created Prompts\nWhat label best describes this news article? What’s the most accurate label for this news article?\nWhat is this piece of news regarding? What does this piece of news concern?\nWhich newspaper section would this article likely appear in?In what section of the newspaper could this article be published?\nWhat topic is this news article about? What category does this article fall into?\nTable 4: Prompts for the task AG News (news classification): the manually created prompts and a sample of\nautomatically created prompts using our method.\nModel Task Perplexity-score corr.Perplexity-acc corr.Avg Acc Acc 50%\nPearson Spearman Pearson Spearman\nOPT 175B\nAntonyms **-0.41 **-0.53 – – – –\nGLUE Cola -0.15 -0.14 -0.04 -0.02 47.7 57.1\nNewspop *-0.24 **-0.26 *-0.20 -0.18 66.4 72.9\nAG News **-0.63 **-0.68 **-0.77 **-0.81 57.5 68.7\nIMDB **0.35 **0.40 0.14 *0.20 86.2 91.0\nDBpedia **-0.50 **-0.44 **-0.51 **-0.42 46.7 55.2\nEmotion -0.14 -0.19 **-0.30 **-0.32 16.4 23.0\nTweet Offensive *-0.19 0.07 0.18 *0.23 51.3 55.8\nBloom 176B\nAntonyms **-0.37 **-0.23 – – – –\nGLUE Cola 0.07 0.11 **-0.25 **-0.26 55.5 65.6\nNewspop **-0.50 **-0.42 **-0.59 **-0.51 78.9 87.8\nAG News **-0.62 **-0.54 **-0.44 **-0.44 50.3 59.4\nIMDB 0.04 0.09 -0.08 -0.14 89.3 92.2\nDBpedia **-0.47 *-0.27 **-0.35 *-0.21 27.2 33.4\nEmotion **-0.33 **-0.42 **-0.48 **-0.55 29.3 31.7\nTweet Offensive 0.14 *0.24 *-0.20 -0.03 41.6 46.2\nOPT 30B\nAntonyms **-0.54 **-0.70 – – – –\nGLUE Cola -0.05 0.03 -0.13 0.02 32.2 35.5\nNewspop *-0.23 *-0.25 *-0.18 -0.12 60.3 66.6\nAG News **-0.66 **-0.71 **-0.81 **-0.80 49.3 60.7\nIMDB -0.06 *0.17 0.04 **0.22 81.6 86.1\nDBpedia **-0.41 **-0.34 *-0.21 *-0.25 35.9 42.4\nEmotion 0.00 -0.03 0.18 0.13 12.3 16.2\nTweet Offensive **-0.44 **-0.39 -0.11 -0.05 54.6 60.2\nOPT 1.3B\nAntonyms **-0.45 **-0.53 – – – –\nGLUE Cola **-0.39 **-0.36 -0.09 *-0.19 60.3 65.9\nNewspop **0.33 *0.21 -0.07 -0.07 37.6 40.3\nAG News **-0.33 **-0.29 **-0.56 **-0.49 31.9 37.6\nIMDB -0.11 -0.07 **0.24 **0.22 86.0 89.1\nDBpedia -0.16 -0.14 -0.02 -0.01 8.7 9.2\nEmotion 0.08 0.08 **-0.29 **-0.30 7.0 9.1\nTweet Offensive **-0.42 **-0.35 **-0.50 **-0.38 58.6 62.6\nTable 5: Correlation results for the different tasks, with OPT (different sizes) and Bloom. Correlations withp <0.05\nare marked with *. Correlations with p <0.00625 (according to Bonferroni correction for multiple hypotheses) are\nmarked with **. Dark and light blue colored cells stand for negative correlations < −0.2 and > −0.2, respectively.\nDark and light orange colored cells stand for positive correlations > 0.2 and < 0.2, respectively. Average accuracy\nacross all prompts and average accuracy of best 50% prompts are also reported for reference (Avg Acc and Acc\n50%, respectively).\nscores.\nFor the word prediction tasks we only report\nscores, since accuracy in general is less stable, suf-\nfers more from the surface form competition (Holtz-\nman et al., 2021), and is usually quite low for these\ntasks in our setting (the chances the model will gen-\nerate an exact match of the label are low). Hence,\nthe score of the correct label gives a better estimate\nof the actual performance of the model.\n10140\nLang OPT 175B Bloom 176B\nPearson Spear. Pearson Spear.\nita -0.44 -0.57 -0.37 -0.63\nspa -0.47 -0.61 -0.51 -0.66\ncat -0.47 -0.58 -0.24 -0.31\nfra -0.48 -0.57 -0.48 -0.64\ndeu -0.44 -0.60 -0.46 -0.65\nfin -0.44 -0.62 -0.34 -0.56\npor -0.45 -0.62 -0.46 -0.61\neus -0.47 -0.61 -0.45 -0.61\ntur -0.44 -0.62 -0.33 -0.62\njpn – – -0.33 -0.26\narb – – -0.36 -0.47\nrus – – -0.54 -0.69\nkor – – -0.42 -0.58\nell – – -0.40 -0.51\nTable 6: Correlation results for word-level translation,\nwith OPT 175B and Bloom 176B. All correlations are\nstatistically significant also according to Bonferroni cor-\nrection for multiple hypotheses for OPT (p <0.0055).\nSame for Bloom ( p < 0.00357), except for Catalan\n(Pearson) and Japanese (Spearman).\n5 Results\nClassification Tasks and Antonym Prediction\nTable 5 depicts the Pearson and Spearman corre-\nlation results on the classification tasks and the\nantonym task, with both OPT 175B and Bloom\n(two upper blocks). We see that most correlations\nare negative and statistically significant, as we ex-\npect. This validates our hypothesis and shows that\nin the majority of tasks we indeed get a strong cor-\nrelation between low perplexity of the prompt and\nbetter performance on the task.10 For each task we\nalso report the average accuracy.\nWord-Level Translation The results of the word-\nlevel translation task are reported in Table 6. Here\nthe correlations are extremely consistent across all\nlanguages and across models, with statistical sig-\nnificance for all languages except for Catalan and\nJapanese (in Bloom).\nResults across Different Model Sizes We repeat\nthe same experiment with the OPT models of sizes\n1.3B and 30B, to investigate whether these corre-\nlations are also consistent across model sizes or\nwhether this is a phenomenon we should expect\nonly in large language models. Table 5 (two lower\nblocks) shows these results for all classification\ntasks and antonym prediction. We do see that in\n10Repeating the experiments with the length of the prompt\ninstead of perplexity yields weak positive correlations, almost\nall of which are not statistically significant.\ngeneral the trend appears to be the same in the\nsmaller models as well; however, the correlations\nseem to be slightly weaker. We hypothesize that\nthis might be due to the overall lower performance\nof these smaller models, making the performance\nresults we use for correlation less stable and reli-\nable. For word-level translation, however, all corre-\nlations with the 30B and 1.3B models are similar\nto those with the 175B model, and are all statisti-\ncally significant (also after Bonferroni correction\nfor multiple hypotheses).\n6 Analysis\nNext, we further explore the observed relationship\nbetween model perplexity and prompt performance.\nDespite the consistently high correlation between\nthese two factors, the structure of this relationship\nvaries across tasks (Section 6.1). Additionally, we\nfind that the automatically added prompts are high-\nquality and not a significant source of noise (Sec-\ntion 6.2), and that the best prompts selected by our\napproach vary across models (Section 6.3).\n6.1 Visualizing the Relationship between\nPerplexity and Performance\nTo visualize the correlations we get between the per-\nplexity and the performance of the prompts across\nthe different settings, we plot a few examples for\ndifferent tasks and languages. Figures 1 and 2 show\nsome of the results for selected tasks, as detailed\nin the captions. The negative trend of the corre-\nlation is clearly visible in all plots. Interestingly,\nthe structure of the plots for word-level translation\nare very similar across all the language pairs, sug-\ngesting that prompts get consistent perplexity and\nperformance across languages (possibly at different\nscales). Indeed, the intersection of the 10 lowest\nperplexity prompts between any two different lan-\nguages is 8.6 and 8.4 on average (for OPT 175B\nand Bloom, respectively), which is extremely high.\nThis is not very surprising since we know that the\nonly differences between the prompts in the differ-\nent languages are the names of the target languages\n(e.g., The word for “dog” in French is “). Addition-\nally, the intersection of 10 prompts with the highest\nlabel score between any two different languages is\n7 and 6.5 on average (for OPT 175B and Bloom,\nrespectively).\nA notable finding that appears in the word-level\ntranslation plots is the clear separation between\nprompts that include or do not include quotation\n10141\nmarks for the label (usually aligns with whether\nthe prompt uses quotation marks for the source\nword) – three example prompts appear on the plot.\nPrompts with quotation marks for the words tend to\nhave both lower perplexity and better performance,\nconsistently. We further analyze the results for\nOPT 175B within clusters (with/without quotations\nmarks). In the cluster with quotation marks, we\nget negative correlations (in the range of –0.28 to\n–0.38) that are statistically significant for almost all\nlanguages. The correlations within the other cluster\nare weaker and less significant (this is expected\ngiven the overall lower performance of that cluster).\nFigure 2: Score of correct label vs. perplexity for the\nword-level translation task in French with OPT 175B.\nThe x axis is in log scale. The blue points stand for\nprompts with quotation marks for the words, while the\nyellow points are of prompts without quotation marks.\n6.2 Effect of Noisy Prompts\nWe expect our automatic method for expand-\ning the set of prompts to also introduce some\nnoise. Though our focus is on the lower perplexity\nprompts, since we want to benefit from this anal-\nysis and be able to devise a method for creating\nbetter prompts, we do want to make sure that this\npotential noise is not the cause for the strong cor-\nrelations we get. In other words, one might claim\nthat some noisy prompts have particularly high per-\nplexity and also perform badly, thus, supporting\nour hypothesis in an undesirable and uncontrolled\nmanner.\nWe turn to inspect the 10% highest perplex-\nity prompts in the different tasks and find subjec-\ntively that they are not noisy, and are usually valid\nprompts for the tasks. The 5 highest perplexity\nprompts for the GLUE Cola task are listed in Ta-\nble 7 as an example.\nprompt ppl\nIs this example correct English usage? 25.79\nIs this example using English correctly? 25.46\nIs this example correct English? 25.33\nIs this the example in correct English? 25.00\nIs English in this example correct? 24.90\nTable 7: Example of the 5 highest perplexity prompts\nfor GLUE Cola, using OPT 175B.\nTask Lang Before filtering After filtering\nPearson SpearmanPearson Spearman\nAG News - -0.63 -0.68 -0.62 -0.54\nWLT\nita -0.44 -0.58 -0.44 -0.57\nspa -0.47 -0.61 -0.47 -0.61\ncat -0.45 -0.57 -0.47 -0.58\nfra -0.47 -0.57 -0.48 -0.57\ndeu -0.43 -0.60 -0.44 -0.60\nfin -0.41 -0.60 -0.44 -0.62\npor -0.43 -0.61 -0.45 -0.62\neus -0.45 -0.60 -0.47 -0.61\ntur -0.43 -0.61 -0.44 -0.62\nTable 8: Correlations before and after filtering out noisy\nprompts, with AG News and Word-Level Translation\n(WLT).\nAs a sanity check, we choose two tasks: word-\nlevel translation and AG News, manually filter out\nthe noisy prompts, and compute the correlations\nagain. The annotation is done by external anno-\ntators (NLP researchers) that were presented with\nthe tasks and asked to label whether the prompt is\nreasonable to use for the task. The new correlations\nwith OPT 175B are reported in Table 8. We find\nthat all correlations remain strong and statistically\nsignificant when noise is manually removed from\nthe analysis. We get the same trends with Bloom\nas well.\n6.3 Best Performing Prompts\nTable 9 lists the 5 lowest perplexity prompts for the\ntask of antonym prediction, as an example. Similar\nlists for the rest of the tasks are listed in Section B\nin the Appendix.\nA closer look at the lowest perplexity prompts\nreveals that the intersection of 10 lowest perplex-\nprompt ppl\nThe following two words are antonyms: “good” and “ 10.24\nThe antonym of the word “good” is “ 10.32\nThe word that has the opposite meaning of the word “good” is “ 10.43\nThe word “good” is the antithesis of the word “ 10.85\nThe word “good” is the opposite of the word “ 11.15\nTable 9: Lowest perplexity prompts for the antonym\nprediction task, using OPT 175B.\n10142\nity prompts between OPT 175B and Bloom is 7.1\non average, across the classification tasks. When\nlooking at the 10 highest accuracy prompts across\nmodels we get an average intersection of 3.1 across\nthe classification tasks.\n7 SPELL: Selecting Prompts by\nEstimating LM Likelihood\nThe primary contribution of this work is the analy-\nsis of the relationship between prompt perplexity\nand downstream task performance (Section 5). As\none potential application of our findings, we also\npresent a new method, SPELL , for generating and\nselecting consistently effective prompts.\nAssuming a fixed computational budget for find-\ning effective prompts for a given task, and that the\nsearch space might be quite large, we devise the\nfollowing straightforward procedure:\n1. Obtain a small set of manually created\nprompts for the task.\n2. Expand the set of prompts with automatic\nparaphrasing using a LM (e.g., GPT3) and\nbacktranslation (see Section 3).\n3. Rank the list of prompts by perplexity (aver-\naged on a representative sample of task inputs,\ne.g., 1,000).\n4. Choose the k (e.g., 3) lowest perplexity\nprompts.\nUsing this algorithm, we show empirically that\nit is best to prioritize experimenting with the lowest\nperplexity prompts, as they are more stable (exhibit\nless variation in performance) and perform better\nthan manual prompts on average. This method\nalso does not require any labels for the task, and is\napplicable to any task, also by non-experts, given\nexample inputs only.\n7.1 Empirical Validation of SPELL\nTo show the effectiveness of our method, we report\nthe results we get usingSPELL across the different\ntasks. In Table 10 we report the average accuracy\nwith the manual prompts compared to the average\naccuracy with the 3 lowest-perplexity prompts, for\nboth OPT 175B and Bloom. Indeed, in most cases,\nthe average accuracy using the 3 lowest perplex-\nity prompts outperforms the average accuracy of\nthe manual prompts, with an average of 1.8 accu-\nracy points across tasks with OPT and 2.3 accuracy\nOPT Bloom\nTask low-ppl manual∆ low-ppl manual∆\nGLUE Cola 51.7 48.5 3.1 64.5 60.9 3.6\nNewspop 80.6 70.4 10.2 90.0 80.0 10.0\nAG News 68.4 61.9 6.5 51.0 63.5 -12.5\nIMDB 90.4 88.9 1.4 91.3 88.8 2.5\nDBpedia 46.0 51.7 -5.7 31.2 30.2 1.0\nEmotion 21.6 22.6 -1.1 35.8 32.1 3.6\nTweet Offensive48.4 50.6 -2.3 48.6 40.8 7.8\nTable 10: The average accuracy with the manual\nprompts (manual) compared to the average accuracy\nwith the 3 lowest-perplexity prompts (low-ppl), for both\nOPT 175B and Bloom, across tasks.\npoints with Bloom, demonstrating the effectiveness\nof our method.\nThe variability in accuracy of the 3 lowest per-\nplexity prompts is also much lower than that of\nthe manually created prompts: with OPT 175B,\nthe average standard deviation within the 3 lowest\nperplexity prompts (across tasks) is 5.07, vs. 6.86\nfor the manual prompts, and with Bloom the gap is\nmuch bigger, with an average of 2.6 for the 3 lowest\nperplexity prompts vs. 7.47 for the manual ones.11\nThis further shows that SPELL is more stable and\nreliable compared to using an arbitrary set of man-\nually created prompts. SPELL sets the stage for\nfurther development in this direction, and serves\nas an initial indication of the benefits of involving\nperplexity estimation in the process of generating\neffective prompts.\n8 Related Work\nRelation between performance and training\ndata Previous work looking directly into the rela-\ntion between the training data and the performance\nis limited. Razeghi et al. (2022) study numeric\ndeduction tasks, and examine the correlations be-\ntween the model performance on specific test in-\nstances and the frequency of terms from those in-\nstances in the pretraining data. They find that the\nmodels are more accurate on instances whose terms\nare more prevalent in the training data. Addition-\nally, Han and Tsvetkov (2022) propose a method to\neffectively identify a very small subset of pretrain-\ning data that directly supports the model in perform-\ning a specific task. Elazar et al. (2022) use causal\ninference to measure the effect of pretraining data\nstatistics on factual knowledge performance, and\n11We also calculate the standard deviation when using the\nsame amount of low-perplexity prompts as in the manual\nprompts set for each task and get averages of 6.32 and 3.78\nfor OPT 175B and Bloom, respectively.\n10143\nKandpal et al. (2022) show correlational and causal\nrelationships between accuracy and relevant docu-\nment count (from training data) for QA datasets.\nPrompt tuning and analysis There is a very rich\nline of work trying to find prompts automatically.\nShin et al. (2020) present an automated method to\ncreate discrete prompts for a diverse set of tasks,\nbased on a gradient-guided search, and they demon-\nstrate their method on masked LMs. Other work\nalso focuses on discrete prompts, aiming to im-\nprove zero-shot performance (Gao et al., 2021;\nLe Scao and Rush, 2021; Deng et al., 2022; Shi\net al., 2022), or trains continuous prompts (Li and\nLiang, 2021; Lester et al., 2021; Qin and Eisner,\n2021).\nOn top of works that suggest a variety of meth-\nods for creating better prompts, some work also\nanalyzes those prompts to try and get some insights\nabout them: Khashabi et al. (2022a) find that model\nperformance is highly sensitive to small changes\nin wordings and Khashabi et al. (2022b) point to\na surprising disconnect between continuous and\ndiscrete prompts.\n9 Conclusion\nWe investigate the phenomenon where some\nprompts perform better than others despite appear-\ning similar to the human users of LMs. Specifically,\nwe hypothesize that the perplexity of a prompt un-\nder a given LM is closely tied to its task perfor-\nmance. We test this theory on a large number of\ntasks and autoregressive LMs, and the resulting\ncorrelation study validates our hypothesis. Further\nanalysis of this relationship demonstrates that the\nbest prompts differ across models, highlighting the\nimportance of model-specific analysis, and that the\nunderlying structure of the relationship between\nperplexity and performance varies across tasks.\nIn light of these findings, we then propose\na method, SPELL , to help users find well-\nperforming prompts for new tasks. Empirical\nvalidation of the proposed procedure shows that\nSPELL generates effective prompts with low vari-\nability in performance, and produces small gains\nof 1.8 (2.3) accuracy points with OPT (Bloom)\nover manual prompts. We therefore conclude that\nSPELL provides a general and interpretable ap-\nproach for applying LMs to new tasks while requir-\ning minimal human effort, and no labels.\nLimitations\nSearching for human-readable prompts We\nlimit our search space to human-readable prompts\nthat are fluent and accurately describe the task\nat hand, as we are primarily motivated in under-\nstanding why some relevant prompts work better\nthan others. We do this by using manually cre-\nated prompts and their automatically created para-\nphrases. Our findings may not hold when the possi-\nble prompt space is expanded to include any token\nsequence; we leave this direction to future work.\nGenerality of our analysis and of the SPELL\nmethod We perform our analysis on and build\nour method around specific models, namely OPT\nand Bloom. Additionally, our study is limited to the\nspecific tasks we experiment with and to English. It\nis possible that our analysis andSPELL method do\nnot generalize to other pretrained models or tasks;\nhowever, we consider models of various sizes and\nfrom different sources, and a wide range of tasks\nto mitigate this risk.\nAcknowledgements\nWe thank Alisa Liu and Orevaoghene Ahia for their\nhelp in annotating noisy prompts. We also thank\nthe reviewers for their valuable comments on the\npaper.\nReferences\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V . Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nFrancesco Barbieri, Jose Camacho-Collados, Luis\nEspinosa-Anke, and Leonardo Neves. 2020. TweetE-\nval:Unified Benchmark and Comparative Evaluation\nfor Tweet Classification. In Proceedings of Findings\nof EMNLP.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\n10144\nYanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown,\nand He He. 2022. On the relation between sensitivity\nand accuracy in in-context learning. arXiv e-prints,\npages arXiv–2209.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nJohannes Dellert, Thora Daneyko, Alla Münch, Alina\nLadygina, Armin Buch, Natalie Clarius, Ilja Grigor-\njew, Mohamed Balabel, Hizniye Isabella Boga, Za-\nlina Baysarova, et al. 2019. Northeuralex: a wide-\ncoverage lexical database of northern eurasia. Lan-\nguage Resources and Evaluation, pages 1–29.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\narXiv preprint arXiv:2205.12548.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Amir\nFeder, Abhilasha Ravichander, Marius Mosbach,\nYonatan Belinkov, Hinrich Schütze, and Yoav Gold-\nberg. 2022. Measuring causal effects of data statis-\ntics on language model’sfactual’predictions. arXiv\npreprint arXiv:2207.14251.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830.\nXiaochuang Han and Yulia Tsvetkov. 2022. Orca: In-\nterpreting prompted language models via locating\nsupporting data evidence in the ocean of pretraining\ndata. arXiv preprint arXiv:2205.12600.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing,\npages 7038–7051.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge. arXiv\npreprint arXiv:2211.08411.\nDaniel Khashabi, Chitta Baral, Yejin Choi, and Han-\nnaneh Hajishirzi. 2022a. Reframing instructional\nprompts to gptk’s language. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 589–612.\nDaniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. 2022b. Prompt wayward-\nness: The curious case of discretized interpretation\nof continuous prompts. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies. Association for Com-\nputational Linguistics.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web, 6(2):167–195.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-\nLaure Ligozat. 2022. Estimating the carbon footprint\nof bloom, a 176b parameter language model. arXiv\npreprint arXiv:2211.02001.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nN. Moniz and L. Torgo. 2018. Multi-source social feed-\nback of online news feeds. ArXiv, abs/1801.07055.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint\narXiv:2202.07206.\n10145\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3687–3697, Brussels, Belgium. Association\nfor Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman,\nYulia Tsvetkov, and Luke Zettlemoyer. 2022. Toward\nhuman readable prompt tuning: Kubrick’s the shining\nis a good movie, and a good prompt too? arXiv\npreprint arXiv:2212.10539.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nJohn Wieting, Jonathan Mallinson, and Kevin Gimpel.\n2017. Learning paraphrastic sentence embeddings\nfrom back-translated bitext. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 274–285.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\nA Manually Created Prompts\nTable 11 lists the manually created prompts we use\nfor the different tasks. We manually add, remove\nand edit prompts for some of these tasks, to make\nthem fit for our setting. For example, the following\nprompt for AG News, taken from Promptsource,\ndoes not fit our setting: Would you recommend the\nfollowing article to a politician, an athlete, busi-\nness executive, or a scientist?\nB Lowest Perplexity Prompts\nTable 12 lists the 5 lowest perplexity prompts for\neach task, using OPT 175B.\n10146\nTask Manual Prompts\nAntonyms\nThe antonym of the word “good” is “\nThe opposite meaning of the word “good” is “\n“Good” is the opposite of “\n“Good” is the negation of “\nThe following are opposites of each other: “good” and “\nThe word “good” contradicts the word “\nThe antonym of the word good is\nThe opposite meaning of the word good is\nGood is the opposite of\nGood is the negation of\nThe following are opposites of each other: good and\nThe word good contradicts the word\nGLUE Cola\nDoes the this sentence make sense and use correct English?\nIs this example grammatically correct and sensible?\nDoes this sentence make sense and is it grammatically correct?\nDoes this example use correct English?\nNewspop\nWhat is the article about?\nWhat is this news about?\nWhat is the topic of this news piece?\nWhat does this article discuss?\nWhat is the topic of this sentence?\nWhat category does the article belong to?\nPick one category for this news piece.\nPick the category that fits the text.\nThe article refers to which category?\nWhat topic does the article belong to?\nWhat category fits this article?\nWhat topic does this news piece belong to?\nChoose the correct category for this article.\nAG News\nWhat label best describes this news article?\nWhat is this piece of news regarding?\nWhich newspaper section would this article likely appear in?\nWhat topic is this news article about?\nIMDB\nThis movie review expresses what sentiment?\nDid the reviewer find this movie good or bad?\nIs this review positive or negative?\nHow does the viewer feel about the movie?\nWhat sentiment does the writer express for the movie?\nWhat sentiment is expressed for the movie?\nWhat is the sentiment expressed in this text?\nDid the reviewer enjoy the movie?\nWhat is the sentiment expressed by the reviewer for the movie?\nHow does the reviewer feel about the movie?\nDBpedia\nWhat category does the paragraph belong to?\nPick one category for the text.\nPick the category that fits the text.\nThe text refers to which category?\nWhat category does the title belong to?\nWhat category fits this text?\nWhat topic does this text belong to?\nChoose the correct category for the text.\nEmotion\nWhat is the emotion expressed in this message?\nWhat emotion does this message express?\nHow will you feel about the message?\nWhat emotion does the writer express for the message?\nTweet Offensive\nIs this tweet offensive?\nCan the tweet be removed for being offensive?\nIs the author’s tweet offensive?\nTask: Identify if the tweet or text is offensive.\nIs this an offensive tweet?\nWord-Level Translation\nThe translation of the word “dog” to French is “\nThe translation of the word dog to French is\nThe word “dog” in French is “\n“dog” (In French: “\nTranslate the word dog into French:\nThe translation of dog to French is\n“dog” (French: “\nThe word dog in French is\nTranslate the word “dog” into French: “\ndog (In French:\ndog (French:\nThe translation of “dog” to French is “\nTable 11: The set of manually created prompts for each task.\n10147\nTask Lowest Perplexity Prompts Perplexity\nAntonyms\nThe following two words are antonyms: “good” and “ 10.24\nThe antonym of the word “good” is “ 10.32\nThe word that has the opposite meaning of the word “good” is “ 10.43\nThe word “good” is the antithesis of the word “ 10.85\nThe word “good” is the opposite of the word “ 11.15\nGLUE Cola\nIs this an example of the proper use of the English language? 11.63\nDoes the sentence make sense and does it follow the rules of grammar? 11.76\nIs this sentence an example of the correct use of the English language? 12.10\nDoes this sentence make sense and is it grammatically correct? 12.15\nIs this sentence grammatically correct and does it make sense? 12.68\nNewspop\nWhat is the main subject of the article? 10.01\nWhat is the main topic of the article? 10.01\nWhat is the subject matter of the article? 10.17\nWhat is the subject of the article? 10.21\nWhat is the main idea of this article? 10.21\nAG News\nIn what section of the newspaper would you expect to find this article? 7.51\nIn which section of the newspaper would you expect to find this article? 7.52\nIn which section of the newspaper would this article be most likely to appear? 7.60\nIn what section of the newspaper do you expect to find this article? 7.80\nIn what section of the newspaper would this article most likely appear? 7.87\nIMDB\nWhat is the opinion of the review? Is it positive or negative? 7.19\nIs this a positive or negative review? 7.31\nWhat do you think of the movie? 7.33\nWhat do you think of the film? 7.35\nIs that a positive or a negative? 7.35\nDBpedia\nWhat is the category to which the text refers? 8.99\nWhat is the subject of the text? 9.15\nWhat category does the title belong to? 9.18\nWhich category does the text refer to? 9.19\nWhat is the subject of this text? 9.20\nEmotion\nHow do you feel when you hear this message? 12.72\nWhat is the writer’s emotional reaction to this news? 13.18\nWhat is the emotion expressed in this message? 13.20\nHow does this message make you feel? 13.32\nHow do you feel about this message? 13.50\nTweet Offensive\nIf someone said this to you, would you be offended? 13.00\nIf someone said that to you, would you be offended? 13.10\nWould you be offended if someone said that to you? 13.73\nWould it offend you if someone said that to you? 14.79\nIf someone told you that, would you be offended? 14.93\nWord-Level Translation\nThe word for “dog” in French is “ 7.73\nThe French word for “dog” is “ 8.16\nThe French translation of the word “dog” is “ 8.24\nThe translation of the word “dog” in French is “ 8.35\nThe translation of the word “dog” into French is “ 8.91\nTable 12: The 5 lowest perplexity prompts for each task, using OPT 175B.\n10148",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9968765377998352
    },
    {
      "name": "Computer science",
      "score": 0.8432126045227051
    },
    {
      "name": "Language model",
      "score": 0.6763049364089966
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6508232355117798
    },
    {
      "name": "Variance (accounting)",
      "score": 0.6457152962684631
    },
    {
      "name": "Task (project management)",
      "score": 0.615993320941925
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6158865690231323
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5916778445243835
    },
    {
      "name": "Natural language processing",
      "score": 0.5508822798728943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5405269265174866
    },
    {
      "name": "Machine learning",
      "score": 0.4541347622871399
    },
    {
      "name": "Training set",
      "score": 0.4102838635444641
    },
    {
      "name": "Programming language",
      "score": 0.06842419505119324
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ],
  "cited_by": 81
}