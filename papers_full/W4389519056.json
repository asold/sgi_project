{
  "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
  "url": "https://openalex.org/W4389519056",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4287858569",
      "name": "Alessandro Stolfo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2279044859",
      "name": "Yonatan Belinkov",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4386554921",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W3176514068",
    "https://openalex.org/W4229494842",
    "https://openalex.org/W4389520380",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4287855115",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4327811009",
    "https://openalex.org/W4308023630",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2792641098",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4310926773",
    "https://openalex.org/W4378508793",
    "https://openalex.org/W3134095442",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4368304611",
    "https://openalex.org/W4327526719",
    "https://openalex.org/W3010694149",
    "https://openalex.org/W4297412003",
    "https://openalex.org/W4378976798",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4285283606",
    "https://openalex.org/W4385573636",
    "https://openalex.org/W4379928343",
    "https://openalex.org/W4316135772",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4220848927",
    "https://openalex.org/W3176751053",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4285662474",
    "https://openalex.org/W3201339301",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4287124808",
    "https://openalex.org/W4225405251",
    "https://openalex.org/W1951216520"
  ],
  "abstract": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7035–7052\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Mechanistic Interpretation of Arithmetic Reasoning in Language Models\nusing Causal Mediation Analysis\nAlessandro Stolfo\nETH Zürich\nstolfoa@ethz.ch\nYonatan Belinkov\nTechnion – IIT, Israel\nbelinkov@technion.ac.il\nMrinmaya Sachan\nETH Zürich\nmsachan@ethz.ch\nAbstract\nMathematical reasoning in large language mod-\nels (LMs) has garnered significant attention\nin recent work, but there is a limited under-\nstanding of how these models process and store\ninformation related to arithmetic tasks within\ntheir architecture. In order to improve our un-\nderstanding of this aspect of language mod-\nels, we present a mechanistic interpretation\nof Transformer-based LMs on arithmetic ques-\ntions using a causal mediation analysis frame-\nwork. By intervening on the activations of spe-\ncific model components and measuring the re-\nsulting changes in predicted probabilities, we\nidentify the subset of parameters responsible\nfor specific predictions. This provides insights\ninto how information related to arithmetic is\nprocessed by LMs. Our experimental results\nindicate that LMs process the input by trans-\nmitting the information relevant to the query\nfrom mid-sequence early layers to the final to-\nken using the attention mechanism. Then, this\ninformation is processed by a set of MLP mod-\nules, which generate result-related information\nthat is incorporated into the residual stream. To\nassess the specificity of the observed activation\ndynamics, we compare the effects of different\nmodel components on arithmetic queries with\nother tasks, including number retrieval from\nprompts and factual knowledge questions.1\n1 Introduction\nMathematical reasoning with Transformer-based\nmodels (Vaswani et al., 2017) is challenging as\nit requires an understanding of the quantities and\nthe mathematical concepts involved. While large\nlanguage models (LMs) have recently achieved\nimpressive performance on a set of math-based\ntasks (Wei et al., 2022a; Chowdhery et al., 2022;\nOpenAI, 2023), their behavior has been shown\nto be inconsistent and context-dependent (Bubeck\net al., 2023). Recent literature shows a multitude\n1Our code and data is available at https://github.com/\nalestolfo/lm-arithmetic.\nWhat is 15 times 12 ?\n180MLP ModuleAttention\nA: Operands & \nOperator\nProcessing \nB: Information Transfer \nC: Result\nGeneration \nFigure 1: Visualization of our findings. We trace\nthe flow of numerical information within Transformer-\nbased LMs: given an input query, the model processes\nthe representations of numbers and operators with early\nlayers (A). Then, the relevant information is conveyed\nby the attention mechanism to the end of the input se-\nquence (B). Here, it is processed by late MLP modules,\nwhich output result-related information into the residual\nstream (C).\nof works proposing methods to improve the perfor-\nmance of large LMs on math benchmark datasets\nthrough enhanced pre-training (Spokoyny et al.,\n2022; Lewkowycz et al., 2022; Liu and Low, 2023)\nor specific prompting techniques (Wei et al., 2022b;\nKojima et al., 2022; Yang et al., 2023, inter alia).\nHowever, there is a limited understanding of the\ninner workings of these models and how they store\nand process information to correctly perform math-\nbased tasks. Insights into the mechanics behind\nLMs’ reasoning are key to improvements such as\ninference-time correction of the model’s behavior\n(Li et al., 2023) and safer deployment. Therefore,\nresearch in this direction is critical for the develop-\nment of more faithful and accurate next-generation\nLM-based reasoning systems.\nIn this paper, we present a set of analyses aimed\n7035\nat mechanistically interpreting LMs on the task of\nanswering simple arithmetic questions (e.g.,“What\nis the product of 11 and 17?” ). In particular, we\nhypothesize that the computations involved in rea-\nsoning about such arithmetic problems are carried\nout by a specific subset of the network. Then, we\ntest this hypothesis by adopting a causal mediation\nanalysis framework (Vig et al., 2020; Meng et al.,\n2022), where the model is seen as a causal graph\ngoing from inputs to outputs, and the model compo-\nnents (e.g., neurons or layers) are seen as mediators\n(Pearl, 2001). Within this framework, we assess\nthe impact of a mediator on the observed output\nbehavior by conducting controlled interventions on\nthe activations of specific subsets of the model and\nexamining the resulting changes in the probabilities\nassigned to different numerical predictions.\nThrough this experimental procedure, we track\nthe flow of information within the model and iden-\ntify the model components that encode information\nabout the result of arithmetic queries. Our findings\nshow that the model processes the input by convey-\ning information about the operator and the operands\nfrom mid-sequence early layers to the final token\nusing attention. At this location, the information is\nprocessed by a set of MLP modules, which output\nresult-related information into the residual stream\n(shown in Figure 1). We verify this finding for bi-\nand tri-variate arithmetic queries across four pre-\ntrained language models with different sizes: 2.8B,\n6B, and 7B parameters. Finally, we compare the\neffect of different model components on answer-\ning arithmetic questions to two additional tasks:\na synthetic task that involves retrieving a number\nfrom the prompt and answering questions related\nto factual knowledge. This comparison validates\nthe specificity of the activation dynamics observed\non arithmetic queries.\n2 Related Work\nMechanistic Interpretability. The objective of\nmechanistic interpretability is to reverse engineer\nmodel computation into components, aiming to\ndiscover, comprehend, and validate the algorithms\n(called circuits in certain works) implemented by\nthe model weights (Räuker et al., 2023). Early\nwork in this area analyzed the activation values of\nsingle neurons when generating text using LSTMs\n(Karpathy et al., 2015). A multitude of studies\nhave later focused on interpreting weights and inter-\nmediate representations in neural networks (Olah\net al., 2017, 2018, 2020; V oss et al., 2021; Goh\net al., 2021) and on how information is processed\nby Transformer-based (Vaswani et al., 2017) lan-\nguage models (Geva et al., 2021, 2022, 2023; Ols-\nson et al., 2022; Nanda et al., 2023). Although\nnot strictly mechanistic, other recent studies have\nanalyzed the hidden representations and behavior\nof inner components of large LMs (Belrose et al.,\n2023; Gurnee et al., 2023; Bills et al., 2023).\nCausality-based Interpretability. Causal medi-\nation analysis is an important tool that is used to ef-\nfectively attribute the causal effect of mediators on\nan outcome variable (Pearl, 2001). This paradigm\nwas applied to investigate LMs by Vig et al. (2020),\nwho proposed a framework based on causal medi-\nation analysis to investigate gender bias. Variants\nof this approach were later applied to mechanis-\ntically interpret the inner workings of pre-trained\nLMs on other tasks such as subject-verb agreement\n(Finlayson et al., 2021), natural language inference\n(Geiger et al., 2021), indirect object identification\n(Wang et al., 2022), and to study their retention of\nfactual knowledge (Meng et al., 2022).\nMath and Arithmetic Reasoning. A growing\nbody of work has proposed methods to analyze the\nperformance and robustness of large LMs on tasks\ninvolving mathematical reasoning (Pal and Baral,\n2021; Pi˛ ekos et al., 2021; Razeghi et al., 2022;\nCobbe et al., 2021; Mishra et al., 2022). In this\narea, Stolfo et al. (2023) use a causally-grounded\napproach to quantify the robustness of large LMs.\nHowever, the proposed formulation is limited to\nbehavioral investigation with no insights into the\nmodels’ inner mechanisms. To the best of our\nknowledge, our study represents the first attempt\nto connect the area of mechanistic interpretability\nto the investigation of the mathematical reasoning\nabilities in Transformer-based LMs.\n3 Methodology\n3.1 Background and Task\nWe denote an autoregressive language model as G:\nX→P . The model operates over a vocabulary V\nand takes a token sequence x= [x1,...,x T ] ∈X,\nwhere each xi ∈V. Ggenerates a probability dis-\ntribution P ∈P : R|V |→[0,1] that predicts possi-\nble next tokens following the sequence x. In this\nwork, we study decoder-only Transformer-based\nmodels (Vaswani et al., 2017). Specifically, we\nfocus on models that represent a slight variation of\n7036\nthe standard GPT paradigm, as they utilize parallel\nattention (Wang and Komatsuzaki, 2021) and rotary\npositional encodings (Su et al., 2022). The internal\ncomputation of the model’s hidden states h(l)\nt at\nposition t ∈{1,...,T }of the input sequence is\ncarried out as follows:\nh(l)\nt = h(l−1)\nt + a(l)\nt + m(l)\nt (1)\na(l)\nt = A(l)\n(\nh(l−1)\n1 ,...,h (l−1)\nt\n)\nm(l)\nt = W(l)\nproj σ\n(\nW(l)\nfc h(l−1)\nt\n)\n=: MLP(l)(h(l−1)\nt ),\nwhere at layer l, σ is the sigmoid nonlinearity,\nW(l)\nfc and W(l)\nproj are two matrices that parameter-\nize the multilayer perceptron (MLP) of the Trans-\nformer block and A(l) is the attention mechanism.2\nWe consider the task of computing the result\nof arithmetic operations. Each arithmetic query\nconsists of a list of operandsN = (n1,n2,... ) and\na function fO representing the application of a set\nof arithmetic operators (+,−,×,÷). We denote\nas r= fO(N) the result obtained by applying the\noperators to the operands. Each query is rendered\nas a natural language question through a prompt\np(N,fO) ∈X such as “How much is n1 plus n2?”\n(in this case, fO(n1,n2) = n1 + n2). The prompt\nis then fed to the language model to produce a\nprobability distribution P over V. Our aim is to\ninvestigate whether certain hidden state variables\nare more important than others during the process\nof computing the result r.\n3.2 Experimental Procedure\nWe see the modelGas a causal graph (Pearl, 2009),\nframing internal model components, such as spe-\ncific neurons, as mediators positioned along the\ncausal path connecting model inputs and outputs.\nFollowing a causal mediation analysis procedure,\nwe then quantify the contribution of particular\nmodel components by intervening on their acti-\nvation values and measuring the change in the\nmodel’s output. Previous work has isolated the\neffect of every single neuron within a model (Vig\net al., 2020; Finlayson et al., 2021). However, this\napproach becomes impractical for models with bil-\nlions of parameters. Therefore, for our main exper-\niments, the elements that we consider as variables\nalong the causal path described by the model are\n2For brevity, layer normalization (Ba et al., 2016) is omit-\nted as it is not essential for our analysis.\nWhat is 23 plus 98?p2 =\n…\nWhat is 189 plus 42?p1 =\n…\nSet Value of \nActivations\nModel Prediction:\n231\nClean Run\nPrediction: 121\nPost-Intervention\nPrediction: 231\nAfter Intervention\nMLP MLP\nMLP\nMLP\nMLP\nMLP\nA\nA\nA\nA\nA\nA\nFigure 2: By intervening on the activation values of\nspecific components within a language model and com-\nputing the corresponding effects, we identify the subset\nof parameters responsible for specific predictions.\nthe outputs of the MLP(l) and A(l) functions at\neach token t, i.e., m(l)\nt and a(l)\nt .\nTo quantify the importance of modules MLP(l)\nand A(l) in mediating the model’s predictions at\nposition t, we use the following procedure.\n1. Given fO, we sample two sets of operands\nN, N′, and we obtain r = fO(N) and r′ =\nfO(N′). Then, two input questions with only\nthe operands differing, p1 = p(N,fO) and\np2 = p(N′,fO), are passed through the model.\n2. During the forward pass with input p1, we store\nthe activation values ¯m(l)\nt := MLP(l)(h(l−1)\nt ),\nand ¯a(l)\nt := A(l)(h(l−1)\n1 ,...,h (l−1)\nt ) .\n3. We perform an additional forward pass using\np2, but this time we intervene on components\nMLP(l) and A(l) at position t, setting their ac-\ntivation values to ¯m(l)\nt , and ¯a(l)\nt , respectively.\nThis process is illustrated in Figure 2.\n4. We measure the causal effect of the intervention\non variables m(l)\nt and a(l)\nt on the model’s predic-\ntion by computing the change in the probability\nvalues assigned to the results rand r′.\n7037\nMore specifically, we compute the indirect ef-\nfect (IE) of a specific mediating component by\nquantifying its contribution in skewing P towards\nthe correct result. Consider a generic activation\nvariable z∈{m(1)\n1 ,...,m (L)\nt ,a(1)\n1 ,...,a (L)\nt }. We\ndenote the model’s output probability following an\nintervention on zas P∗\nz. Then, we compute the IE\nas:\nIE(z) = 1\n2\n[P∗\nz(r) −P(r)\nP(r) + P(r′) −P∗\nz(r′)\nP∗z(r′)\n]\n(2)\nwhere the two terms in the sum represent the rela-\ntive change in the probability assigned by the model\nto rand r′, caused by the intervention performed.\nThe larger the measured IE, the larger the contri-\nbution of component zin shifting probability mass\nfrom the clean-run result r′to result rcorrespond-\ning to the alternative input p1.3\nWe additionally measure the mediation effect\nof each component with respect to the operation\nfO. We achieve this by fixing the operands and\nchanging the operator across the two input prompts.\nMore formally, in step 1, we sample a list of\noperands N and two operators fO and f′\nO. Then,\nwe generate two prompts p1 = p(N,fO) and\np2 = p(N,f ′\nO) (e.g., “What is the sum of 11 and 7?”\nand “What is the product of 11 and 7?”). Finally,\nwe carry out the procedure in steps 2–4.\n3.3 Experimental Setup\nWe present the results of our analyses in the main\npaper for GPT-J (Wang and Komatsuzaki, 2021),\na 6B-parameter pre-trained LM (Gao et al., 2020).\nAdditionally, we validate our findings on Pythia\n2.8B (Biderman et al., 2023), LLaMA 7B (Touvron\net al., 2023), and Goat, a version of LLaMA fine-\ntuned on arithmetic tasks (Liu and Low, 2023).\nWe report the detailed results for these models in\nAppendix J.\nIn our experiments, we focus on two- and three-\noperand arithmetic problems. Similar to previous\nwork (Razeghi et al., 2022; Karpas et al., 2022),\nfor single-operator two-operand queries, we use\na set of six diverse templates representing a ques-\ntion involving each of the four arithmetic operators.\nFor the three-operand queries, we use one template\nfor each of the 29 possible two-operator combina-\ntions. Details about the templates are reported in\n3As an alternative metric to quantify the IE, we experiment\nusing the difference in log probabilities (Appendix I). The\nresults obtained with the two metrics show consistency and\nlead to the same conclusions.\nAppendix A. In the bi-variate case, for each of the\nfour operators fO ∈{+,−,×,÷}and for each of\nthe templates, we generate 50 pairs of prompts by\nsampling two pairs of operands (n1,n2) ∈S2 and\n(n′\n1,n′\n2) ∈S2, where S⊂ V ∩N. For the operand-\nrelated experiment, we sample (n1,n2) and a sec-\nond operation f′\nO. In both cases, we ensure that the\nresult rfalls within S.4 In the three-operand case,\nwe generate 15 pairs of prompts for each of the 29\ntemplates, following the same procedure. In order\nto ensure that the model achieves a meaningful task\nperformance, we use a two-shot prompt in which\nwe include two exemplars of question-answer for\nthe same operation that is being queried. We report\nthe accuracy results in Appendix B.\n4 Causal Effects on Arithmetic Queries\nOur analyses address the following question:\nQ1 What are the components of the model that\nmediate predictions involving arithmetic com-\nputations?\nWe address this question by first studying the flow\nof information throughout the model by measur-\ning the effect of each component (MLP and atten-\ntion block) at each point of the input sequence for\ntwo-operand queries (§4.1). Then, we distinguish\nbetween model components that carry information\nabout the result and about the operands of the arith-\nmetic computations (§4.2 and §4.3). Finally, we\nconsider queries involving three operands (§4.4)\nand present a measure to quantify the changes in\ninformation flow (§4.5).\n4.1 Tracing the Information Flow\nWe measure the indirect effect of each MLP and\nattention block at different positions along the in-\nput sequence. The output of these modules can be\nseen as new information being incorporated into\nthe residual stream. This new information can be\nproduced at any point of the sequence and then con-\nveyed to the end of the sequence for the prediction\nof the next token. By studying the IE at different\nlocations within the model, we can identify the\nmodules that generate new information relevant to\nthe model’s prediction. The results are reported in\nFigures 3a and 3b for MLP and attention, respec-\ntively.\n4Unless otherwise specified, we use S= {1, 2, . . . ,300},\nas larger integers get split into multiple tokens by the tokenizer.\n7038\n(e)\n(f)\n(g)\n(h)\n(a) (b)\n(c) (d)\nFigure 3: Indirect effect (IE) measured within GPT-J. Figures (a) and (b) illustrate the flow of information related to\nboth the operands and the result of the queries, while the effect displayed in Figures (c) and (d) is related to the\noperands only (the result is kept unchanged). Figures (e–h) show a re-scaled visualization of the effects at the last\ntoken for each of the four heatmaps (a–d). The difference in the effect registered for the MLPs at layers 15–25\nbetween figures (a) and (c) illustrates the role of these components in producing result-related information.\nOur analysis reveals four primary activation sites:\nthe MLP module situated at the first layer corre-\nsponding to the tokens of the two operands; the\nintermediate attention blocks at the last token of\nthe sequence; and the MLP modules in the middle-\nto-late layers, also located at the last token of the\nsequence. It is expected to observe a high effect\nfor the first MLPs associated with the tokens that\nvary (i.e., the operands), as such modules are likely\nto affect the representation of the tokens, which\nis subsequently used for the next token prediction.\nOn the other hand, of particular interest is the high\neffect detected at the attention modules in layers\n11–18 and in the MLPs around layer 20.\nAs for the flow of information tied to the opera-\ntor, the activations display a parallel pattern: high\neffect is registered at early MLPs associated with\nthe operator tokens and at the same last-token MLP\nand attention locations. We report the visualization\nof the operator-related results in Appendix C.\nA possible explanation of the model’s behavior\non this task is that the attention mechanism facil-\nitates the propagation of operand- and operator-\nrelated information from the first layers early in\nthe sequence to the last token. Here, this infor-\nmation is processed by the MLP modules, which\nincorporate the information about the result of the\ncomputation in the residual stream. This hypoth-\nesis aligns with the existing theory that attributes\nthe responsibility of moving and extracting infor-\nmation within Transformer-based models to the\nattention mechanism (Elhage et al., 2021; Geva\net al., 2023), while the feed-forward layers are as-\nsociated with performing computations, retrieving\nfacts and information (Geva et al., 2022; Din et al.,\n2023; Meng et al., 2022). We test the validity of\nthis hypothesis in the following section.\n4.2 Operand- and Result-related Effects\nOur objective is to verify whether the contribu-\ntion to the model’s prediction of each component\nmeasured in Figures 3a and 3b is due to ( 1) the\ncomponent representing information related to the\noperands, or (2) the component encoding informa-\ntion about the result of the computation. To this\nend, we formulate a variant of our previous experi-\nmental procedure. In particular, we condition the\nsampling of the second pair of operands (n′\n1,n′\n2)\non the constraint r = r′. That is, we generate the\ntwo input questions p1 and p2, such that their re-\nsult is the same (e.g., “What is the sum of 25 and\n7?” and “ What is the sum of 14 and 18? ”). In\ncase number (1), we would expect a component to\nhave high IE both in the result-varying setting and\nwhen r= r′, as the operands are modified in both\nscenarios. In case ( 2), we expect a subset of the\nmodel to have a large effect when the operands are\nsampled without constraints but a low effect for the\nfixed-result setting.\nWe report the results in Figure 3c and 3d. By\n7039\ncomparing Figures 3a and 3c, two notable obser-\nvations emerge. First, the high effect in the early\nlayers corresponding to the operand tokens is ob-\nserved in both the result-preserving and the result-\nvarying scenarios. Second, the last-token mid-late\nMLPs that lead to a high effect on the model’s\nprediction following a result change, dramatically\ndecrease their effect on the model’s output in the\nresult-preserving setting, as described in scenario\n(2). These observations point to the conclusion that\nthe MLP blocks around layer 20 incorporate result-\nrelevant information. As for the contribution of the\nattention mechanism (Figures 3b and 3d), we do\nnot observe a substantial difference in the layers\nwith the highest IE between the two settings, which\naligns this scenario to the description of case ( 1).\nThese results are consistent with our hypothesis\nthat operand-related information is transferred by\nthe attention mechanism to the end of the sequence\nand then processed by the MLPs to obtain the result\nof the computation.\n4.3 Zooming in on the Last Token\nIn Figures 3e–3h, we show a re-scaled version of\nthe IE measurements for the layers at the end of\nthe input sequence. While the large difference in\nmagnitude was already evident in the previously\nconsidered visualizations, in Figures 3e and 3f we\nnotice that the MLPs with the highest effect in\nthe two settings differ: the main contribution to\nthe model’s output when the results are not fixed\nis given by layers 19 and 20, while in the result-\npreserving setting the effect is largest at layers 14-\n18. For the attention (Figures 3g and 3h), we do\nnot observe a significant change in the shape of\nthe curve describing the IE across different layers,\nwith layer 13 producing the largest contribution.\nWe interpret this as additional evidence indicating\nthat the last-token MLPs at layers 19-20 encode\ninformation about r, while the attention modules\ncarry information related to the operands.\n4.4 Three-operand Queries & Fine-tuning\nWe extend our analyses by including three-operand\narithmetic queries such as “What is the difference\nbetween n1 and the ratio betweenn2 and n3?”. An-\nswering correctly this type of questions represents\na challenging task for pre-trained language models,\nand we observe poor accuracy (below 10%) with\nGPT-J. Thus, we opt for fine-tuning the model on\na small set of three-operand queries. The model\nthat we consider for this analysis is Pythia 2.8B,\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE of the MLPs in Pythia 2.8B\n10 1\n102\n105\n108\n1011\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE of the MLPs in Pythia 2.8B (FT)\n10 1\n102\n105\n108\n1011\nFigure 4: Indirect effect (IE) on three-operand queries\nfor different MLP modules in Pythia 2.8B before and\nafter fine-tuning. The effect produced by the last-token\nmid-late MLP activation site emerges with fine-tuning.\nResults for the attention are reported in Appendix J.\nas its smaller size allows for less computationally\ndemanding training than the 6B-parameter GPT-J.\nAfter fine-tuning, the model attains an accuracy of\n∼40%. We provide the details about the training\nprocedure in Appendix F.\nWe carry out the experimental procedure as in\nSection 4.1. In particular, we compare the informa-\ntion flow in the MLPs of the model before and after\nfine-tuning (Figure 4). In the non-fine-tuned ver-\nsion of the model, the only relevant activation site,\nbesides the early layers at the operand tokens, is the\nvery last layer at the last token. In the fine-tuned\nmodel, on the other hand, we notice the emergence\nof the mid-late MLP activation site that was previ-\nously observed in the two-operand setting.\n4.5 Quantifying the Change of the\nInformation Flow\nDenote the set of MLPs in the model by M. We\ndefine the relative importance (RI) of a specific\nsubset M∗⊆M of MLP modules as\nRI(M∗) =\n∑\nm∈M∗ log(IE(m) + 1)∑\nm∈Mlog(IE(m) + 1) . (3)\nIn order to quantitatively show the difference in the\nactivation sites observed in Figure 3, we compute\nthe RI measure for the set\nMlate\n−1 = {m(⌊L/2⌋)\n−1 ,m(⌊L/2⌋+1)\n−1 ,...,m (L)\n−1 },\n7040\n|N| Model RI(Mlate\n−1) RI(Mlate\n−1)\nResult Fixed\n2\nGPT-J 40.2% 4.4%\nPythia 2.8B 43.2% 5.8%\nLLaMA 7B 36.1% 7.5%\nGoat 33.5% 7.4%\nGPT-J (Words) 27.8% 4.5%\n3 Pythia 2.8B 13.5% 6.7%\nPythia 2.8B (FT) 24.7% 13.6%\nTable 1: Relative importance (RI) measurements for\nthe last-token late MLP activation site. The decrease\nin the RI observed when fixing the result of the two\npairs of operands used for the interventions quantita-\ntively confirms the role of this subset of the model in\nincorporating result-related information.\nwhere the subscript −1 indicates the last token of\nthe input sequence and Lis the number of layers\nin the model. This quantity represents the rela-\ntive contribution of the mid-late last-token MLPs\ncompared to all the MLP blocks in the model.\nFor the two-operand setting, we carry out the\nexperimental procedure described in Section 3 for\nthree additional models: Pythia 2.8B, LLaMA 7B,\nand Goat.5 Furthermore, we repeat the analyses\non GPT-J using a different number representation:\ninstead of Arabic numbers (e.g., the token 2), we\nrepresent quantities using numeral words (e.g., the\ntoken two). For the three-operand setting, we re-\nport the results for Pythia 2.8B before and after\nfine-tuning. We measure the effects using both\nrandomly sampled and result-preserving operand\npairs, comparing the RI measure in the two settings.\nThe results (Table 1) exhibit consistency across all\nthese four additional experiments. These quantita-\ntive measurements further highlight the influence\nof last-token late MLP modules on the prediction\nof r. We provide in Appendix J the heatmap illus-\ntrations of the effects for these additional studies.\n5 Causal Effects on Different Tasks\nIn order to understand whether the patterns in the\neffect of the model components that we observed\nso far are specific to arithmetic queries, we com-\npare our observations on arithmetic queries to two\ndifferent tasks: the retrieval of a number from the\nprompt (§5.1), and the prediction of factual knowl-\n5The LLaMA tokenizer considers each digit as an inde-\npendent token in the vocabulary. This makes it problematic\nto compare the probability value assigned by the model to\nmulti-digit numbers. Therefore, we restrict the set of possible\nresults to the set of single-digit numbers.\n0 5 10 15 20 25\nLayer\nEarly\neq 1st Tok\neq Mid Toks\neq Last Tok\nLate\nLast\nIE of the MLP Modules (Number Retrieval)\n10 1\n100\n101\nFigure 5: Indirect effect measured on the MLPs of GPT-\nJ for predictions on the number retrieval task.\nedge (§5.2). With this additional set of experiments,\nwe aim to answer the question:\nQ2 Are the activation patterns observed so far spe-\ncific to the arithmetic setting?\n5.1 Information Flow on Number Retrieval\nWe consider a simple synthetic task involving nu-\nmerical predictions. We construct a set of templates\nof the form “Paul has n1 e1 and n2 e2. How many\neq does Paul have?”, where n1, n2 are two ran-\ndomly sampled numbers, e1 and e2 are two entity\nnames sampled at random,6 and eq ∈{e1,e2}. In\nthis case, the two input prompts p1 and p2 differ\nsolely in the value of eq. To provide the correct\nanswer to a query, the model has simply to retrieve\nthe correct number from the prompt. With this task,\nwe aim to analyze the model’s behavior in a setting\ninvolving numerical predictions but not requiring\nany kind of arithmetic computation.\nWe report the indirect effect measured for the\nMLPs modules of GPT-J in Figure 5. In this setting,\nwe observe an unsurprising high-effect activation\nsite corresponding to the tokens of the entity eq\nand a lower-effect site at the end of the input in\nlayers 14–20. The latter site appears in the set of\nthe model components that were shown to be ac-\ntive on arithmetic queries. However, computing\nthe relative importance of the late MLPs on this\ntask shows that this second activation site is respon-\nsible for only RI(Mlate\n−1 ) = 8 .7% of the overall\nlog IE. The low RI, compared to the higher values\nmeasured on arithmetic queries, suggests that the\nfunction of the last-token late MLPs is not dictated\nby the numerical type of prediction, but rather by\ntheir involvement in processing the input informa-\ntion. This finding is aligned with our theory that\n6We sample entities from a list containing names of ani-\nmals, fruits, office tools, and other everyday items and objects.\n7041\n0 5 10 15 20 25\nLayer\nEarly\nSubj First T ok\nOther Subj T oks\nSubj Last T ok\nLate\nLast\nIE of the MLP Modules (Factual Q.)\n10 1\n101\n103\n105\nFigure 6: Indirect effect measured on the MLPs of GPT-\nJ for predictions to factual queries.\nsees Mlate\n−1 as the location where information about\nris included in the residual stream.\n5.2 Information Flow on Factual Predictions\nWe carry out our experimental procedure using\ndata from the LAMA benchmark (Petroni et al.,\n2019), which consists of natural language tem-\nplates representing knowledge-base relations, such\nas “[subject] is the capital of [object]”. By in-\nstantiating a template with a specific subject (e.g.,\n“Paris”), we prompt the model to predict the cor-\nrect object ( “France”). Similar to our approach\nwith arithmetic questions, we create pairs of factual\nqueries that differ solely in the subject. In partic-\nular, we sample pairs of entities from the set of\nentities compatible for a given relation (e.g., cities\nfor the relation “is the capital of”). Details about\nthe data used for this procedure are provided in\nAppendix H. We then measure the indirect effect\nfollowing the formulation in Equation 2, where the\ncorrect object corresponds to the correct numerical\noutcome in the arithmetic scenario.\nFrom the results (Figure 6), we notice that a main\nactivation site emerges in early layers at the tokens\ncorresponding to the subject of the query. These\nfindings are consistent with previous works (Meng\net al., 2022; Geva et al., 2023), which hypothe-\nsize that language models store and retrieve factual\nassociations in early MLPs located at the subject\ntokens. We compute the RI metric for the late MLP\nmodules, which quantitatively validates the contri-\nbution of the early MLP activation site by attaining\na low value of RI(Mlate\n−1 ) = 4 .2%. The large IE\nobserved at mid-sequence early MLPs represents\na difference in the information flow with respect\nto the arithmetic scenario, where the modules with\nthe highest influence on the model’s prediction are\nlocated at the end of the sequence. This differ-\nence serves as additional evidence highlighting the\nAr W NR F\nAr W NR F\n100% 50% 23% 10%\n50% 100% 22% 9%\n23% 22% 100% 9%\n10% 9% 9% 100%\n25%\n50%\n75%\n100%\nFigure 7: Overlap ratio in the top 400 neurons with the\nlargest effect on predicting answers to factual queries\ninvolving Arabic Numerals (Ar) and numeral words\n(W), number retrieval (NR), and factual knowledge (F).\nThe results are obtained with GPT-J.\nspecificity of the model’s activation patterns when\nanswering arithmetic queries.\n5.3 Neuron-level Interventions\nThe experimental results in Sections 5.1 and 5.2\nshowed a quantitative difference in the contribu-\ntions of last-token mid-late MLPs between arith-\nmetic queries and two tasks that do not involve\narithmetic computation. Now, we investigate\nwhether the components active within Mlate\n−1 on\nthe different types of tasks are different. We carry\nout a finer-grained analysis in which we consider\nindependently each neuron in an MLP module (i.e.,\neach dimension in the output vector of the func-\ntion MLP(l)) at a specific layer l. In particular,\nfollowing the same procedure as for layer-level ex-\nperiments, we intervene on each neuron by setting\nits activation to the value it would take if the input\nquery contained different operands (or a different\nentity). We then compute the corresponding indi-\nrect effect as in Eq. 2. We carry out this procedure\nfor arithmetic queries using Arabic numerals (Ar)\nand numeral words (W), for the number retrieval\ntask (NR), and for factual knowledge queries (F).7\nWe rank the neurons according to the average effect\nmeasured for each of these four settings and com-\npute the overlap in the top 400 neurons (roughly\n10%, as GPT-J has a hidden dimension of 4096).\nWe carry out this procedure for layerl= 19, as it\nexhibits the largest IE within Mlate\n−1 on all the tasks\nconsidered. The heatmap in Figure 7 illustrates\nthe results. We observe a consistent overlap (50%)\nbetween the top neurons active for the arithmetic\nqueries using Arabic and word-based representa-\n7To have the same result space for all the arithmetic queries\n(Ar and NW) and for the number retrieval task, we restrict the\nset Sto {1, . . . ,20}(or the corresponding numeral words).\n7042\ntions. Interestingly, the size of the neuron overlap\nbetween arithmetic queries and number retrieval is\nconsiderably lower (22% and 23%), even though\nboth tasks involve the prediction of numerical quan-\ntities. Finally, the overlaps between the top neurons\nfor the arithmetic operations and the factual pre-\ndictions (between 9% and 10%) are not larger than\nfor random rankings: the expected overlap ratio\nbetween the top 400 indices in two random rank-\nings of size 4096 is 9.8% (Antverg and Belinkov,\n2022). These results support the hypothesis that\nthe model’s circuits responsible for different kinds\nof prediction, though possibly relying on similar\nsubsets of layers, are distinct. However, it is im-\nportant to note that this measurement does not take\ninto account the magnitude of the effect.\n6 Conclusion\nWe proposed the use of causal mediation analy-\nsis to mechanistically investigate how LMs pro-\ncess information related to arithmetic. Through\ncontrolled interventions on specific subsets of the\nmodel, we assessed the impact of these mediators\non the model’s predictions.\nWe posited that models produce predictions to\narithmetic queries by conveying the math-relevant\ninformation from the mid-sequence early layers\nto the last token, where this information is then\nprocessed by late MLP modules. We carried out a\ncausality-grounded experimental procedure on four\ndifferent Transformer-based LMs, and we provided\nempirical evidence supporting our hypothesis. Fur-\nthermore, we showed that the information flow we\nobserved in our experiments is specific to arith-\nmetic queries, compared to two other tasks that do\nnot involve arithmetic computation.\nOur findings suggest potential avenues for re-\nsearch into model pruning and more targeted\ntraining/fine-tuning by concentrating on specific\nmodel components associated with certain queries\nor computations. Moreover, our results offer in-\nsights that may guide further studies into using\nLMs’ hidden representations to correct the model’s\nbehavior on math-based tasks at inference time (Li\net al., 2023) and to estimate the probability of the\nmodel’s predictions to be true (Burns et al., 2023).\nLimitations\nThe scope of our work is investigating arithmetic\nreasoning and we experiment with the four funda-\nmental arithmetic operators. Addition, subtraction,\nmultiplication, and division form the cornerstone\nof arithmetic calculations and serve as the basis\nfor a wide range of mathematical computations.\nThus, exploring their mechanisms in language mod-\nels provides a starting point to explore more com-\nplex forms of mathematical processing. Studying\na broader set of mathematical operators represents\nan interesting avenue for further investigation.\nOur work focuses on synthetically-generated\nqueries that are derived from natural language de-\nscriptions of the four basic arithmetic operators. To\nbroaden the scope, future research can expand the\nanalysis of model activations to encompass math-\nbased queries described in real-life settings, such\nas math word problems.\nFinally, a limitation of our work concerns the\nanalysis of different attention heads. In our ex-\nperiments, we consider the output of an attention\nmodule as a whole. Future research could focus\non identifying the specific heads that are responsi-\nble for forwarding particular types of information\nin order to offer a more detailed understanding of\ntheir individual contributions.\nAcknowledgments\nAS is supported by armasuisse Science and Tech-\nnology through a CYD Doctoral Fellowship. YB\nis supported by an AI Alignment grant from Open\nPhilanthropy, the Israel Science Foundation (grant\nNo. 448/20), and an Azrieli Foundation Early Ca-\nreer Faculty Fellowship. MS acknowledges sup-\nport from the Swiss National Science Foundation\n(Project No. 197155), a Responsible AI grant by\nthe Haslerstiftung, and an ETH Grant (ETH-19\n21-1). We are grateful to Vilém Zouhar and Neel\nNanda for the insightful discussions.\nReferences\nOmer Antverg and Yonatan Belinkov. 2022. On the\npitfalls of analyzing individual neurons in language\nmodels. In International Conference on Learning\nRepresentations.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\narXiv preprint arXiv:2303.08112.\n7043\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nSteven Bills, Nick Cammarata, Dan Moss-\ning, Henk Tillman, Leo Gao, Gabriel Goh,\nIlya Sutskever, Jan Leike, Jeff Wu, and\nWilliam Saunders. 2023. Language mod-\nels can explain neurons in language models.\nhttps://openaipublic.blob.core.windows.\nnet/neuron-explainer/paper/index.html.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Stein-\nhardt. 2023. Discovering latent knowledge in lan-\nguage models without supervision. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nAlexander Yom Din, Taelin Karidi, Leshem Choshen,\nand Mor Geva. 2023. Jump to conclusions: Short-\nCutting transformers with linear transformations.\narXiv preprint arXiv:2303.09435.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal analysis of syntactic\nagreement mechanisms in neural language models.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1828–1843, Online. Association for\nComputational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAtticus Geiger, Hanson Lu, Thomas Icard, and Christo-\npher Potts. 2021. Causal abstractions of neural net-\nworks. Advances in Neural Information Processing\nSystems, 34:9574–9586.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual asso-\nciations in auto-regressive language models. arXiv\npreprint arXiv:2304.14767.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nGabriel Goh, Nick Cammarata, Chelsea V oss, Shan\nCarter, Michael Petrov, Ludwig Schubert, Alec Rad-\nford, and Chris Olah. 2021. Multimodal neurons in\nartificial neural networks. Distill, 6(3):e30.\nWes Gurnee, Neel Nanda, Matthew Pauly, Kather-\nine Harvey, Dmitrii Troitskii, and Dimitris Bert-\nsimas. 2023. Finding neurons in a haystack:\nCase studies with sparse probing. arXiv preprint\narXiv:2305.01610.\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak\nLenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\nBata, Yoav Levine, Kevin Leyton-Brown, et al. 2022.\nMRKL systems: A modular, neuro-symbolic archi-\ntecture that combines large language models, exter-\nnal knowledge sources and discrete reasoning. arXiv\npreprint arXiv:2205.00445.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. InAdvances in\nNeural Information Processing Systems, volume 35,\npages 22199–22213. Curran Associates, Inc.\n7044\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models.\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\nPfister, and Martin Wattenberg. 2023. Inference-time\nintervention: Eliciting truthful answers from a lan-\nguage model.\nTiedong Liu and Bryan Kian Hsiang Low. 2023. Goat:\nFine-tuned LLaMA outperforms GPT-4 on arithmetic\ntasks. arXiv preprint arXiv:2305.14201.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 35:17359–17372.\nSwaroop Mishra, Arindam Mitra, Neeraj Varshney,\nBhavdeep Sachdeva, Peter Clark, Chitta Baral, and\nAshwin Kalyan. 2022. NumGLUE: A suite of funda-\nmental yet challenging mathematical reasoning tasks.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3505–3523, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNeel Nanda, Lawrence Chan, Tom Liberum, Jess Smith,\nand Jacob Steinhardt. 2023. Progress measures for\ngrokking via mechanistic interpretability. arXiv\npreprint arXiv:2301.05217.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel\nGoh, Michael Petrov, and Shan Carter. 2020.\nZoom in: An introduction to circuits. Distill.\nHttps://distill.pub/2020/circuits/zoom-in.\nChris Olah, Alexander Mordvintsev, and Ludwig Schu-\nbert. 2017. Feature visualization. Distill, 2(11):e7.\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan\nCarter, Ludwig Schubert, Katherine Ye, and Alexan-\nder Mordvintsev. 2018. The building blocks of inter-\npretability. Distill, 3(3):e10.\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas\nJoseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,\nDanny Hernandez, Scott Johnston, Andy Jones, Jack-\nson Kernion, Liane Lovitt, Kamal Ndousse, Dario\nAmodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. 2022. In-context\nlearning and induction heads. Transformer Circuits\nThread.\nOpenAI. 2023. GPT-4 technical report.\nKuntal Kumar Pal and Chitta Baral. 2021. Investigating\nnumeracy learning ability of a text-to-text transfer\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3095–3101,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJudea Pearl. 2001. Direct and indirect effects. In UAI\n’01: Proceedings of the 17th Conference in Uncer-\ntainty in Artificial Intelligence, University of Wash-\nington, Seattle, Washington, USA, August 2-5, 2001,\npages 411–420. Morgan Kaufmann.\nJudea Pearl. 2009. Causality. Cambridge University\nPress.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nPiotr Pi˛ ekos, Mateusz Malinowski, and Henryk\nMichalewski. 2021. Measuring and improving\nBERT’s mathematical abilities by predicting the or-\nder of reasoning. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 383–394, Online. Association\nfor Computational Linguistics.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 840–854, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nTilman Räuker, Anson Ho, Stephen Casper, and Dylan\nHadfield-Menell. 2023. Toward transparent AI: A\nsurvey on interpreting the inner structures of deep\nneural networks.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\nDaniel Spokoyny, Ivan Lee, Zhao Jin, and Taylor Berg-\nKirkpatrick. 2022. Masked measurement prediction:\nLearning to jointly predict quantities and units from\ntextual context. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 17–\n29, Seattle, United States. Association for Computa-\ntional Linguistics.\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Schoelkopf, and Mrinmaya Sachan. 2023. A\ncausal framework to quantify the robustness of mathe-\nmatical reasoning with language models. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 545–561, Toronto, Canada. Association\nfor Computational Linguistics.\n7045\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2022. Roformer: En-\nhanced transformer with rotary position embedding.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Investigating gender bias in language\nmodels using causal mediation analysis. Advances in\nNeural Information Processing Systems, 33:12388–\n12401.\nChelsea V oss, Nick Cammarata, Gabriel Goh, Michael\nPetrov, Ludwig Schubert, Ben Egan, Swee Kiat Lim,\nand Chris Olah. 2021. Visualizing weights. Distill,\n6(2):e00024–007.\nBen Wang and Aran Komatsuzaki. 2021. GPT-\nJ-6B: A 6 billion parameter autoregressive lan-\nguage model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022. Inter-\npretability in the wild: A circuit for indirect object\nidentification in GPT-2 small. In NeurIPS ML Safety\nWorkshop.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2023.\nLarge language models as optimizers. arXiv preprint\narXiv:2309.03409.\n7046\n0 5 10 15 20 25\nLayer\nEarly\nOperator\nOperands\nLate\nLast\nIE of the MLP Modules (Operator Information)\n10 1\n100\n101\n102\n103\n104\n0 5 10 15 20 25\nLayer\nEarly\nOperator\nOperands\nLate\nLast\nIE of the Attention (Operator Information)\n10 1\n100\n101\n102\n103\n104\nFigure 8: Indirect effect (IE) measured in GPT-J when\nvarying the word describing the operator involved in the\ninput query. Similar to the operands case, we observe\na high contribution produced by middle-to-late MLP\nmodules at the end of the input sequence.\nA Prompt Templates\nIn Tables 2 and 3, we report the question tem-\nplates from Karpas et al. (2022), which we used as\nprompts for the model for two- and three-operand\nqueries, respectively. For three-operand queries,\nwe use one query template for each of the 29 possi-\nble two-operation combinations.\nB Performance of the Models\nIn Table 4, we report the accuracy of the models\non the arithmetic queries that we use for our analy-\nses. The higher accuracy obtained using numeral\nwords is likely given by the smaller set of possi-\nble solutions considered (we used S = {“one”,\n“two”, ..., “twenty”}, as the numeral words corre-\nsponding to larger numbers get split into multiple\ntokens by the tokenizer). The accuracy of GPT-J\non the factual queries from the LAMA benchmark\nis 65.0% (we constrain the vocabulary to the set of\nall possible objects for all the relations considered).\nOn the synthetic number retrieval task, GPT-J’s\naccuracy is 86.7%.\nC Flow of Operator-related Information\nThe measurements of the indirect effect for each\nmodel component when fixing the operand and\nvarying the operator in the two input prompts p1\nand p2 reveal how the model processes the informa-\ntion related to the operator. We report in Figure 8\n0\n200\n400\n600IE\nAddition\n0\n1000\n2000\n3000IE\nSubtraction\n0 10 20\nLayer\n0\n100\n200\n300IE\nMultiplication\n0 10 20\nLayer\n0\n2\n4\n6IE\nDivision\nFigure 9: Indirect effect of the MLPs at the last token\nin each layer in GPT-J, for each of the four arithmetic\noperators. We observe a peak in the effect at layer 19\nfor all four types of operation.\nthe heatmap visualizations of these results for two-\noperand queries. Similar to the operand-related\ninformation, we observe a high effect in three acti-\nvation locations: early MLP blocks corresponding\nto the operand tokens; middle-to-early attention\nmodules at the last token; and middle-to-late MLP\nmodules at the last token. These results align with\nour hypothesis that arithmetic-related information\nis transferred to the end of the sequence by the\nattention mechanism, where it is then processed\nby late MLP layers. In this setting, we measure\nRI(Mlate\n−1 ) =31.4%.\nD Effects for Each Operator\nFor each of the four operators, we report the indi-\nrect effect measured for the last-token MLP mod-\nules in GPT-J in Figure 9. The results for each of\nthe four operators show a common spike in the ef-\nfect at layers 19-20. This indicates the presence of a\nspecific part of the model relevant to the numerical\npredictions of the bi-variate arithmetic questions,\nirrespective of the operator involved. We also no-\ntice a difference in the magnitude of the effects,\nwhich is linked to the capability of the model to\ncorrectly answer the query.\nE Changes in the Model’s Prediction\nWe measured the influence of the model compo-\nnents in terms of probability changes. Now, we\nstudy the dynamics of the actual model predictions.\nIn particular, considering the scenario in which\nr = r′, we verify whether the intervention leads\nto a change in the model’s prediction. That is, we\n7047\nType addition subtraction\n1 Q: How much is n1 plus n2? A: Q: How much is n1 minus n2? A:\n2 Q: What is n1 plus n2? A: Q: What is n1 minus n2? A:\n3 Q: What is the result of n1 plus n2? A: Q: What is the result of n1 minus n2?\n3 Q: What is the sum of n1 and n2? A: Q: What is the difference between A: n1 and n2? A:\n5 The sum of n1 and n2 is The difference between n1 and n2 is\n6 n1 + n2 = n1 - n2 =\nmultiplication division\n1 Q: How much is n1 times n2? A: Q: How much is n1 over n2? A:\n2 Q: What is n1 times n2? A: Q: What is n1 over n2? A:\n3 Q: What is the result of n1 times n2? A: Q: What is the result of n1 over n2? A:\n4 Q: What is the product of n1 and n2? A: Q: What is the ratio between n1 and n2? A:\n5 The product of n1 and n2 is The ratio of n1 and n2 is\n6 n1 * n2 = n1 / n2 =\nTable 2: Question templates for two-operand arithmetic queries.\nFormula Template\n(n1+n2)*n3 Sum n1 and n2 and multiply by n3\nn1+n2*n3 What is the sum of n1 and the product of n2 and n3?\n(n1-n2)*n3 What is the product of n1 minus n2 and n3?\nn1/(n2/n3) How much is n1 divided by the ratio between n2 and n3?\nn1-n2*n3 What is the difference between n1 and the product of n2 and n3?\nn1*(n2-n3) How much is n1 times the difference between n2 and n3?\nTable 3: Examples of templates of three-operand queries. For the full list, we refer to Karpas et al. (2022).\nModel Operation Accuracy (%)\nGPT-J\n+ 69.3\n− 78.0\n× 82.8\n÷ 40.8\nOverall 67.8\nGPT-J (Numeral Words)\n+ 95.5\n− 86.7\n× 83.3\n÷ 59.7\nOverall 81.3\nPythia 2.8B\n+ 57.4\n− 77.5\n× 64.7\n÷ 40.2\nOverall 59.9\nLLaMA\n+ 100.0\n− 99.8\n× 100.0\n÷ 88.7\nOverall 97.2\nGoat\n+ 100.0\n− 100.0\n× 91.4\n÷ 54.0\nOverall 85.6\nPythia 2.8B (3 Operands) Overall 0.9\nPythia 2.8B Fine-tuned Overall 39.7(3 Operands)\nTable 4: Accuracy of the models analyzed in the paper\non various types of arithmetic queries.\ncompute\n1{arg max\nx∈S\nP∗\nz(x) ̸= arg max\nx∈S\nP(x)}, (4)\ndistinguishing between desired\n(arg maxx∈SP∗(x) = r) and undesired\n(arg maxx∈SP(x) = r) changes. The re-\nsults reported in Figure 10 show an increase in\nthe desired change in prediction at layers 19-20,\nwhile the undesired change in prediction is higher\nfor layers 14-17. This means that interventions on\nthe MLPs at layers 19-20 are more likely to lead\nto a correct adjustment of the prediction, while\nthe opposite is true for earlier layers (14-15 in\nparticular). This finding is consistent with our\nprevious observations and we see this as additional\nevidence highlighting the influence of the MLPs at\nlayers 19-20 on the prediction of r.\nF Fine-tuning Details\nWe fine-tune Pythia 2.8B on three-operand queries.\nWe train the model for 2 epochs on a set of queries\nobtained by sampling 1000 triples of operands\nfor each of the 29 templates. We use Adafactor\n(Shazeer and Stern, 2018) a learning rate of 10−5,\nlinearly decaying, and a batch size of 8. We make\nsure that there is no overlap between the set of\n7048\n0 5 10 15 20 25\nLayer\n0.000\n0.025\n0.050\n0.075% of Prediction Change\nDesired Change in Prediction\n0 5 10 15 20 25\nLayer\n0.00\n0.05\n0.10% of Prediction Change\nUndesired Change in Prediction\nFigure 10: Desired (wrong to correct) and undesired\n(correct to wrong) change in the prediction induced by\nthe intervention on the MLPs in GPT-J. The layers at\nwhich the two types of prediction change peak corre-\nspond to the layers with the largest corresponding IE.\n0 5 10 15 20 25\nLayer\nEarly\neq 1st Tok\neq Mid Toks\neq Last Tok\nLate\nLast\nIE of the Attention (Number Retrieval)\n10 1\n100\n101\nFigure 11: Indirect effect (IE) measured for the attention\nmodules in GPT-J on the task of number retrieval.\nqueries used for training and the set used for the\ncomputation of the indirect effect.\nG Computing Infrastructure\nThe experiments for all models are carried out us-\ning a single Nvidia A100 GPU with 80GB of mem-\nory. The computation of the indirect effect across\nthe whole model for a single type of component\n(attention or MLP) took ∼15 hours for GPT-J and\n∼6 hours for Pythia (using 50 examples for each\ntwo-operand template) and ∼7 hours for LLaMA\nand Goat (using 20 examples for each two-operand\ntemplate). For the fine-tuning of Pythia 2.8B, we\nused a single Nvidia A100 GPU with 80GB of\nmemory. The training procedure took ∼1 hour. Ex-\nperiment tracking was carried out using Weights &\nBiases.8\n8http://wandb.ai\n0 5 10 15 20 25\nLayer\nEarly\nSubj First T ok\nOther Subj T oks\nSubj Last T ok\nLate\nLast\nIE of the Attention (Factual Q.)\n10 1\n101\n103\n105\nFigure 12: Indirect effect (IE) measured for the attention\nmodules in GPT-J on factual knowledge queries.\nH Factual Knowledge Data\nFor the experiments involving the prediction of fac-\ntual knowledge, we use the following six relations\nfrom the T-REx subset of the LAMA benchmark\n(Petroni et al., 2019):\n1. “[subject] is the capital of [object]”\n2. “[subject] was born in [object]”\n3. “[subject] died in [object]”\n4. “The native language of [subject] is [object]”\n5. “[subject] is a subclass of [object]”\n6. “The capital of [subject] is [object]”.\nWe sample pairs of subject entities from the set\nof entities compatible for a given relation (e.g.,\ncities for the relation “is the capital of”). For each\nrelation, we sample 100 pairs of subject entities.\nI Log Probability to Quantify the IE\nIn order to validate whether the measurements of\nthe indirect effect are specific to the metric that we\ndescribe in Equation 2, we quantify the IE using\nthe absolute difference in the log of the probability\nvalues assigned by the model to the results rand\nr′. More formally, we compute\nIEalt(z) =\n{\n∆′+ ∆ if r̸= r′\n|∆| otherwise , (5)\nwhere\n∆′= logP∗\nz(r) −log P(r) (6)\n∆ = logP(r′) −log P∗\nz(r′). (7)\nThe results are reported in Figure 13. The activa-\ntion sites that we observe are the same as reported\nin Section 4.1: first-layer MLP at the operand to-\nkens and last-token MLP and attention modules.\n7049\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIEalt of the MLP Modules\n0\n2\n4\n6\n8\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIEalt of the Attention\n0\n2\n4\n6\n8\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIEalt of the MLP Modules (Result Fixed)\n0\n2\n4\n6\n8\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIEalt of the Attention (Result Fixed)\n0\n2\n4\n6\n8\nFigure 13: Indirect effect measured using the difference in the log probability as described in Equation 5 (IEalt).\nThe results are obtained with GPT-J on two-operand arithmetic queries.\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE (MLPs) in Pythia (FT) (Res. Fixed)\n10 1\n102\n105\n108\n1011\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE (Attn.) in Pythia (FT) (Res. Fixed)\n10 1\n102\n105\n108\n1011\nFigure 14: Indirect effect (IE) measured for the attention modules in the fine-tuned version of Pythia 2.8B on\nthree-operand arithmetic queries, when r= r′.\nJ Additional Information Flow\nVisualizations\nWe include the IE measurements for the attention\nmodules of GPT-J on the number retrieval task\n(Figure 11) and on the factual knowledge queries\n(Figure 12), and for Pythia 2.8B on three-operand\narithmetic queries before and after fine-tuning (Fig-\nure 15). Additionally, we report the heatmap visu-\nalizations of the indirect effect measured for the fol-\nlowing models: Pythia 2.8B (Figure 16), LLaMA\n7B (Figure 17), Goat (Figure 18), and GPT-J using\nword numerals (Figure 19). Finally, we visualize\nin Figure 14 the IE of MLPs and attention modules\nfor the fine-tuned Pythia 2.8B in the fixed-result\ncase.\n7050\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE of the Att. in Pythia 2.8B\n10 1\n101\n103\n105\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nBetween Operands 1\n2nd Operand\nBetween Operands 2\n3rd Operand\nLate\nLast\nIE of the Att. in Pythia 2.8B (FT)\n10 1\n101\n103\n105\nFigure 15: Indirect effect (IE) measured for the attention modules in Pythia 2.8B on three-operand arithmetic\nqueries, before and after fine-tuning.\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs\n10 1\n100\n101\n102\n103\n104\n105\n106\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention\n10 1\n101\n103\n105\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs (Result Fixed)\n10 1\n100\n101\n102\n103\n104\n105\n106\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention (Result Fixed)\n10 1\n101\n103\n105\nFigure 16: Indirect effect (IE) measured in the MLP and attention modules of Pythia 2.8B on two-operand arithmetic\nqueries.\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs\n10 1\n100\n101\n102\n103\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention\n10 1\n100\n101\n102\n103\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs (Result Fixed)\n10 1\n100\n101\n102\n103\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention (Result Fixed)\n10 1\n100\n101\n102\n103\nFigure 17: Indirect effect (IE) measured in the MLP and attention modules of LLaMA 7B on two-operand arithmetic\nqueries.\n7051\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs\n10 1\n101\n103\n105\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention\n10 1\n101\n103\n105\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLPs (Result Fixed)\n10 1\n101\n103\n105\n0 5 10 15 20 25 30\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention (Result Fixed)\n10 1\n101\n103\n105\nFigure 18: Indirect effect (IE) measured in the MLP and attention modules of Goat on two-operand arithmetic\nqueries.\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLP Modules\n10 1\n100\n101\n102\n103\n104\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention\n10 1\n100\n101\n102\n103\n104\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the MLP Modules (Result Fixed)\n10 1\n100\n101\n102\n103\n104\n0 5 10 15 20 25\nLayer\nEarly\n1st Operand\nMiddle\n2nd Operand\nLate\nLast\nIE of the Attention (Result Fixed)\n10 1\n100\n101\n102\n103\n104\nFigure 19: Indirect effect (IE) measured in the MLP and attention modules of GPT-J on two-operand arithmetic\nqueries, using numeral words to represent quantities.\n7052",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7529622912406921
    },
    {
      "name": "Mediation",
      "score": 0.4556690454483032
    },
    {
      "name": "Security token",
      "score": 0.45345446467399597
    },
    {
      "name": "Causal reasoning",
      "score": 0.4492361843585968
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44254592061042786
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.4326363503932953
    },
    {
      "name": "Theoretical computer science",
      "score": 0.42718833684921265
    },
    {
      "name": "Natural language processing",
      "score": 0.42716753482818604
    },
    {
      "name": "Process (computing)",
      "score": 0.42188137769699097
    },
    {
      "name": "Language model",
      "score": 0.4203072786331177
    },
    {
      "name": "Causal model",
      "score": 0.4164385497570038
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38042622804641724
    },
    {
      "name": "Arithmetic",
      "score": 0.3640190660953522
    },
    {
      "name": "Cognition",
      "score": 0.2750393748283386
    },
    {
      "name": "Programming language",
      "score": 0.18990078568458557
    },
    {
      "name": "Mathematics",
      "score": 0.1558920443058014
    },
    {
      "name": "Psychology",
      "score": 0.09673961997032166
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}