{
  "title": "Text Augmentation-Based Model for Emotion Recognition Using Transformers",
  "url": "https://openalex.org/W4387436025",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5063400442",
      "name": "Fida Mohammad",
      "affiliations": [
        "University of Haripur"
      ]
    },
    {
      "id": "https://openalex.org/A5102969693",
      "name": "Mukhtaj Khan",
      "affiliations": [
        "University of Haripur"
      ]
    },
    {
      "id": "https://openalex.org/A5026828646",
      "name": "Safdar Nawaz Khan Marwat",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5024663222",
      "name": "Naveed Jan",
      "affiliations": [
        "Northern University"
      ]
    },
    {
      "id": "https://openalex.org/A5015714141",
      "name": "Neelam Gohar",
      "affiliations": [
        "Shaheed Benazir Bhutto Women University"
      ]
    },
    {
      "id": "https://openalex.org/A5025580598",
      "name": "Muhammad Bilal",
      "affiliations": [
        "Northern University"
      ]
    },
    {
      "id": "https://openalex.org/A5078354205",
      "name": "Amal Al‐Rasheed",
      "affiliations": [
        "Princess Nourah bint Abdulrahman University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W7075385858",
    "https://openalex.org/W2032254851",
    "https://openalex.org/W6755541679",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6792047633",
    "https://openalex.org/W6767220426",
    "https://openalex.org/W6755960745",
    "https://openalex.org/W6796158530",
    "https://openalex.org/W3169255552",
    "https://openalex.org/W6750532223",
    "https://openalex.org/W6730529904",
    "https://openalex.org/W3038471032",
    "https://openalex.org/W6779511434",
    "https://openalex.org/W2996849360",
    "https://openalex.org/W6766310171",
    "https://openalex.org/W6788300874",
    "https://openalex.org/W6788444043",
    "https://openalex.org/W3202775307",
    "https://openalex.org/W6785065469",
    "https://openalex.org/W6748956627",
    "https://openalex.org/W2980748755",
    "https://openalex.org/W2164330572",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3173590174",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W6666019651",
    "https://openalex.org/W4226407676",
    "https://openalex.org/W4296990086",
    "https://openalex.org/W6747059868",
    "https://openalex.org/W2563786098",
    "https://openalex.org/W6755230293",
    "https://openalex.org/W2965417721",
    "https://openalex.org/W6805781511",
    "https://openalex.org/W3094440766",
    "https://openalex.org/W6790907927",
    "https://openalex.org/W2900511962",
    "https://openalex.org/W2907255073",
    "https://openalex.org/W6806216142",
    "https://openalex.org/W6747414080",
    "https://openalex.org/W6730189690",
    "https://openalex.org/W6762200848",
    "https://openalex.org/W3035020961",
    "https://openalex.org/W3134966150",
    "https://openalex.org/W2062227835",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2965453734",
    "https://openalex.org/W4206221224",
    "https://openalex.org/W3117369308",
    "https://openalex.org/W2560026539",
    "https://openalex.org/W2963647655",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W2985882473",
    "https://openalex.org/W3133916721",
    "https://openalex.org/W3099056802",
    "https://openalex.org/W4289329331",
    "https://openalex.org/W2798357113",
    "https://openalex.org/W2895259358",
    "https://openalex.org/W2782730293",
    "https://openalex.org/W3115382905",
    "https://openalex.org/W2771541038",
    "https://openalex.org/W3176399185",
    "https://openalex.org/W3132988742",
    "https://openalex.org/W2942649361",
    "https://openalex.org/W3100498948",
    "https://openalex.org/W4205918042",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Emotion Recognition in Conversations (ERC) is fundamental in creating emotionally intelligent machines. Graph-Based Network (GBN) models have gained popularity in detecting conversational contexts for ERC tasks. However, their limited ability to collect and acquire contextual information hinders their effectiveness. We propose a Text Augmentation-based computational model for recognizing emotions using transformers (TA-MERT) to address this. The proposed model uses the Multimodal Emotion Lines Dataset (MELD), which ensures a balanced representation for recognizing human emotions. The model used text augmentation techniques to produce more training data, improving the proposed model's accuracy. Transformer encoders train the deep neural network (DNN) model, especially Bidirectional Encoder (BE) representations that capture both forward and backward contextual information. This integration improves the accuracy and robustness of the proposed model. Furthermore, we present a method for balancing the training dataset by creating enhanced samples from the original dataset. By balancing the dataset across all emotion categories, we can lessen the adverse effects of data imbalance on the accuracy of the proposed model. Experimental results on the MELD dataset show that TA-MERT outperforms earlier methods, achieving a weighted F1 score of 62.60% and an accuracy of 64.36%. Overall, the proposed TA-MERT model solves the GBN models' weaknesses in obtaining contextual data for ERC. TA-MERT model recognizes human emotions more accurately by employing text augmentation and transformer-based encoding. The balanced dataset and the additional training samples also enhance its resilience. These findings highlight the significance of transformer-based approaches for special emotion recognition in conversations.",
  "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided the\noriginal work is properly cited.\nechT PressScience\nDOI:10.32604/cmc.2023.040202\nARTICLE\nText Augmentation-Based Model for Emotion Recognition Using\nTransformers\nFida Mohammad1,*,M u k h t a jK h a n1, Safdar Nawaz Khan Marwat2, Naveed Jan3,N e e l a mG o h a r4,\nMuhammad Bilal3 and Amal Al-Rasheed5\n1DepartmentofComputerScience,TheUniversityofHaripur,Haripur,22620,Pakistan\n2DepartmentofComputerSystemsEngineering,FacultyofElectricalandComputerEngineering,UniversityofEngineeringand\nTechnologyPeshawar,Peshawar,25120,Pakistan\n3DepartmentofElectronicsEngineeringTechnology,UniversityofTechnology,Nowshera,24100,Pakistan\n4DepartmentofComputerScience,ShaheedBenazirBhuttoWomenUniversity,Peshawar,25000,Pakistan\n5Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman\nUniversity,Riyadh,11671,SaudiArabia\n*CorrespondingAuthor:FidaMohammad.Email:fidamuhammad120@gmail.com\nReceived:08March2023 Accepted:04July2023 Published:08October2023\nABSTRACT\nEmotionRecognitioninConversations(ERC)isfundamentalincreatingemotionallyintelligentmachines.Graph-\nBasedNetwork(GBN)modelshavegainedpopularityindetectingconversationalcontextsforERCtasks.However,\ntheir limited ability to collect and acquire contextual information hinders their effectiveness. We propose a Text\nAugmentation-based computational model for recognizing emotions using transformers (TA-MERT) to address\nthis. The proposed model uses the Multimodal Emotion Lines Dataset (MELD), which ensures a balanced repre-\nsentationforrecognizinghumanemotions.Themodelusedtextaugmentationtechniquestoproducemoretraining\ndata, improving the proposed model’s accuracy. Transformer encoders train the deep neural network (DNN)\nmodel, especially Bidirectional Encoder (BE) representations that capture both forward and backward contextual\ninformation. This integration improves the accuracy and robustness of the proposed model. Furthermore, we\npresent a method for balancing the training dataset by creating enhanced samples from the original dataset. By\nbalancing the dataset across all emotion categories, we can lessen the adverse effects of data imbalance on the\naccuracy of the proposed model. Experimental results on the MELD dataset show that TA-MERT outperforms\nearlier methods, achieving a weighted F1 score of 62.60% and an accuracy of 64.36%. Overall, the proposed\nTA-MERT model solves the GBN models’ weaknesses in obtaining contextual data for ERC. TA-MERT model\nrecognizes human emotions more accurately by employing text augmentation and transformer-based encoding.\nThe balanced dataset and the additional training samples also enhance its resilience. These findings highlight the\nsignificanceoftransformer-basedapproachesforspecialemotionrecognitioninconversations.\nKEYWORDS\nEmotionrecognitioninconversation;graph-basednetwork;textaugmentation-basedmodel;multimodalemotion\nlinesdataset;bidirectionalencoderrepresentationfortransformer\n3524 CMC, 2023, vol.76, no.3\n1 Introduction\nEmotion Recognition in Conversation (ERC) is a subfield of Emotion Recognition (ER) that\nfocuses on extracting human emotions from dialogues or discussions involving two or more inter-\nlocutors. Emotion understanding is crucial to developing humanoid Artificial Intelligence (AI)\nsystems. With the widespread availability of conversational data on platforms like LinkedIn, Twitter,\nReddit, Y ouTube, Facebook, and E-commerce sites, the research focus has shifted towards emotion\ndetection and recognition in conversations using Natural Language Processing (NLP) techniques.\nEmotion recognition in text-based conversations has gained significant attention due to its potential\nfor sentiment analysis and opinion mining in openly accessible conversational data. While the\ndomains of speech and facial emotion recognition have made notable advancements, text-based\nemotion identification remains an area that requires further exploration and research [1]. Recognizing\nand understanding human emotions conveyed through Text is becoming increasingly important in\ncomputational linguistics, given its practical implications and applications [2].\nOur study addresses explicitly recognizing human emotions in conversations using the MELD\ndataset (Multimodal et al. [3]). The MELD dataset is a multimodal resource incorporating Text,\naudio, and visual modalities. However, for our proposed model, we focus exclusively on the textual\ncomponent of the dataset. By leveraging the rich textual information in conversations, we aim to\nadvance the field of text-based emotion recognition.\nA transformer-based approach transforms the text-based input into a numerical representation\nsuitable for machine learning and deep learning models. Specifically, we utilize the Bi-directional\nEncoder Representations from Transformers (BERT) [4] model, a pre-trained language model capable\nof encoding the contextual information of a text. BERT converts input utterances into input IDs and\nattention mask vectors, which work as inputs to our deep neural network (DNN) model for training.\nEmotion recognition in textual conversations has gained considerable attention in recent years\ndue to its wide range of applications across various domains, such as opinion mining, healthcare,\npsychology, robotics, human-computer interaction, and IoT-based systems. The ability to accurately\ndetect and understand human emotions from Text plays a crucial role in enhancing communication,\npersonalization, and user experience. However, existing approaches face several challenges that hinder\ntheir accuracy and effectiveness.\nOne significant challenge in emotion recognition is the presence of imbalanced datasets used for\ntraining and evaluation. Imbalanced datasets, where the distribution of emotion classes is uneven,\ncan lead to biased models that perform well on majority classes but struggle with minority classes.\nThis limitation affects the overall performance and generalizability of the models. Additionally, many\nprevious studies in this field have relied on non-contextual word embeddings, which may limit their\nability to capture the nuanced meanings and contextual information in textual conversations.\nMoreover, despite the advancements in deep learning, deep neural networks (DNNs) have yet\nto be extensively explored for text-based emotion recognition. DNNs have demonstrated remarkable\ncapabilities in capturing complex patterns and relationships in data, making them suitable for\nmodeling the intricate nature of human emotions. By leveraging the power of DNNs, there is an\nopportunity to improve the accuracy and performance of emotion recognition models in textual\nconversations.\nTo address these limitations, we propose a novel text augmentation-based model for emotion\nrecognition using transformers. Our research aims to enhance the accuracy and effectiveness of\nemotion recognition models by addressing the challenges of dataset imbalance and leveraging the\nCMC, 2023, vol.76, no.3 3525\ncapabilities of contextualized word embeddings and deep neural networks. By incorporating text\naugmentation techniques and balancing the dataset, we aim to mitigate the impact of class imbalance\nand enable more accurate modeling of emotions in textual conversations.\nThe transformer-based approach, specifically the Bi-directional Encoder Representations from\nTransformers (BERT), captures the contextual information in textual data. BERT provides a robust\nframework for representing words in their surrounding context, allowing for a more nuanced and\naccurate understanding of a text. Furthermore, by employing deep neural networks for emotion\nclassification, we can leverage their ability to capture complex relationships and patterns in the data,\nleading to improved emotion recognition performance.\nThis paper presents the details of our proposed text augmentation-based model for emotion\nrecognition. We evaluate the performance of our model using various performance metrics, including\naccuracy, weighted F1 score, recall, and precision, to assess its effectiveness compared to existing\napproaches. We demonstrate our model’s superior performance and improved accuracy through\nextensive experiments and analysis, highlighting its novelty and advancement in text-based emotion\nrecognition. Fig. 1shows the emotion recognition in conversation example from the MELD dataset.\nFigure 1:Emotion recognition in conversation example from MELD dataset\n2 Literature Review\nIn this section, we critically examine several methods or models of emotion recognition in\nconversation, word embeddings, and text data augmentation for emotion classification. In the relevant\nliterature, several models have been proposed to recognize or classify human emotions in conversation,\nand they achieved different accuracy and weighted F1-score.\nChoi et al. [5] proposed a Residual-based Graph Convolutional Network (RGCN) computational\nmodel for predicting human emotions in conversion—the proposed model employed intra-utterance\nfeature extraction on ResNet. Inter-utterance feature extraction on Graph Convolutional Network\n3526 CMC, 2023, vol.76, no.3\n(GCN) is used to extract features. The author proposed a new loss method. The proposed model\napplied pre-trained Global Vector (GloVe) word embeddings with 300 dimensions, using an Adam\noptimizer for optimization. The proposed model is compared with bidirectional contextual long\nshort-term memory (bc-LSTM), Dialogue Graph Convolutional Network (DialogueGCN), Dialogue\nRecurrent Neural Network (DialogueRNN), and Knowledge Enriched Transformer (KET). The\nexperimental outcome yielded that the RGCN model accomplished a weighted F1 score of 55.98%\non the MELD dataset.\nGhosal et al. [6] proposed a DialogueGCN computational model to classify human emotions in a\nconversation using the benchmark dataset. The proposed model employed two encoders, i.e., sequen-\ntial context and speaker-level encoders, to encode the input features. To optimize hyperparameters, the\nproposed model applied pre-trained GloVe word embeddings with 300 dimensions and grid search.\nThe proposed model acquired a weighted F1 score of 58.10% and 59.46% accuracy on the MELD\ndataset, according to the results of their experiments.\nMajumder et al. [7] suggested a DialogueRNN computation model which detects human emotion\nin conversation. The proposed method employed a CNN to extract the textual features. The Dialogue\nRNN network gets trained at utterance with emotion labels. The author used Adam optimizer to\ntrain the network and grid search for hyperparameters optimization. The proposed model achieved a\nweighted F1 score of 57.03% and 59.54% accuracy.\nHu et al. [8] proposed a Dialogue CRN computational model to recognize human emotion in\nconversation. The proposed model applied pre-trained GloVe word embedding with 300 dimensions.\nDialogue CRN model included three phases. In the first phase, the model generated the context\nrepresentation of utterance at the situation and speaker level. In the second phase, it developed\nmultiple turns for reasoning to collect and incorporate emotional information iteratively. The last\nphase, emotion recognition, predicts or classifies the emotion. A weighted F1 score was utilized on the\nMELD dataset to evaluate model performance. The suggested DialogueCRN model had a weighted\nF1 score of 58.39% and 60.73% accuracy.\nZhong et al. [9] proposed the KET model. This model uses external knowledge of common sense\nbased on the attention mechanism. The external knowledge works on the emotion lexicon [10]a n d\nConceptNet [11]. GloVe word embedding with 300 dimensions vectorizes the textual data into a\nnumeric representation. Adam optimizer is helpful for network optimization. The proposed model\ngains a 58.18% weighted F1 score on the MELD dataset.\nXing et al. [12] proposed Adapted Dynamics Memory Network (ADMN) computational model to\npredict human emotion in conversation. ADMN model used a global recurrent neural network (RNN)\nto get inter-speaker impact. The proposed method separately modeled two things, i.e., self and inter-\nspeaker impact, and produced learning for the current utterance. MELD dataset is a multimodality\ndataset, i.e., it contains textual, audio, and visual data. The A-DMN model trained on textual, visual,\nand audio data; therefore, they used a convolutional neural network (CNN) for the extraction of\ntextual features, Open Smile (OS) for the extraction of audio features, and a 3D-CNN for the extraction\nof visual features. The proposed model achieved a 60.45% weighted F1 score on the MELD dataset.\nWang et al. [13] used an LSTM-based encoder to train the model. Features were extracted using\na CNN. The proposed model used GloVe word embeddings with 300 dimensions and an Adam\noptimizer for model optimization. The model achieved a weighted F1 score of 58.36% on the MELD\ndataset.\nCMC, 2023, vol.76, no.3 3527\nYe h e t a l . [14] introduced a dialogical emotion decoding (DED) method, which analyses a dialogue\nas a series and sequentially decodes the emotion states of each phrase over time using a particular\nrecognition engine. Combining emotional effects from intra and inter-speakers in a conversation trains\nthe decoder. For decoding, the proposed method used CNN and RNN. The model used a Distance-\nDependent Chinese Restaurant Procedure (DDCRP) for emotion assignment. It is a clustering method\nused in image segmentation, text modeling, and speaker identification. The proposed method achieved\n43.6% weighted accuracy on the MELD dataset and 69.0% on the Interactive Emotional Dyadic\nMotion capture (IEMOCAP) dataset.\nJiao et al. [15] introduced Attention Gated Hierarchical Memory Network (AGHMN) method\nfor emotion categorization. Conversational real-time emotion recognition is vital to construct an\nexpressively intelligent method. The model was tested, trained, and a detailed analysis on two\nemotion discussion datasets, MELD and IEMOCAP . The convolutional neural network assists in\nfeature extraction. Firstly, text data was converted into numerical vector form, and word2Vec word\nembeddings were applied. Secondly, the numerical form of the data is passed to the decoder, a\nhierarchical memory network with two levels, a lower one and an upper one. The lower level is the\nutterance reader, and the upper one is the fusion layer. Thirdly, attention weight is calculated, and\nthe output is stored in the memory bank. Finally, a SoftMax activation function is applied for the\nclassification of emotions. Adam optimizer is used for model optimization. On the IEMOCAP dataset,\nthe proposed model received a 63.5% weighted F1 score, unlike the MELD dataset, which received a\n58.1% weighted F1 score.\nZhang et al. [16] developed the ConGCN model, focusing on emotion recognition in multi-speaker\ndiscussions rather than two regular speaker chats. The author proposed a conversational graph-\nbased CNN. In this method, each utterance and speaker are represented as nodes. Speaker-sensitive\ndependence is represented by an undirected edge between an utterance node and its speaker node. In\ncontrast, context-sensitive reliance is represented by an undirected edge between two utterance nodes\nfrom the same dialogue. The entire corpus of conversational data is portrayed as a sizable composite\nnetwork, and the emotion detection problem is rephrased as a problem of classifying the nodes in\nthe graph. Firstly, to generate input, the proposed model used GloVe word embeddings with 300\ndimensions, and an Adam optimizer was used to train the model. Secondly, the convolutional layer\ncontains three filters, and then a max pooling layer is applied to find the pooled features. Finally, the\npooled features are passed to the bidirectional long-short-term-memory layer to classify the emotions.\nThe proposed model achieved a 59.4% weighted F1 score using MELD multimodal (Text and audio)\nand a 57.4% weighted F1 score on the MELD unimodal.\nSheng et al. [17] developed SumAggGIN, a two-stage network that integrates local dependence\nreasoning over nearby utterances with topic-related emotional expression inference in a global-to-\nlocal manner. The SumAggGIN model used GloVe word embeddings with 300 dimensions and an\nAdam optimizer for model optimization. SumAggGIN employed three sizes of convolution filters:\nthree, four, and five, each with 50 feature maps. Then, the output of the convolution layer is passed\nto the max-pooling layer and ReLU activation function. Finally, these activation results are combined\nand input into a 150-dimensional layer for classification. For local feature extraction, bidirectional\nLSTM is applied. SumAggGIN achieved a 58.45% weighted F1 score on the MELD dataset.\nFor emotion classification in conversation, an iterative emotion interaction network was devel-\noped by Lu et al. [18]. This network directly represents the emotional interaction between utterances\nwith the help of an iterative improvement technique. The authors used an utterance encoder to\ncollect speech representations and initially predicted all utterance emotions. Then, using an emotion\n3528 CMC, 2023, vol.76, no.3\ninteraction-based context encoder, they combined the initial prediction and the utterances to provide\nan updated emotion prediction. Finally, they iteratively update the emotions using the iterative\nimprovement method. The proposed model has three main modules; an emotion encoder which\nencodes the emotions; a bidirectional gated recurrent unit; and an emotion classifier which includes a\nSoftMax activation function to classify the emotions. The model used GloVe word embeddings with\n300 dimensions and obtained a 60.72% weighted F1 score.\nLi et al. [19] proposed bidirectional emotion recurrent unit to classify emotions. In the first step,\nthe proposed method converts utterance into a vector, and then the vector is passed to the CNN\nto extract features. Then the output of CNN is passed to the Max-Pooling layer with the ReLU\nactivation function. Finally, a SoftMax activation function is used to classify the utterance emotion.\nThe proposed method obtained 60.9% accuracy on the MELD dataset (textual modality).\nIshiwatari et al. [20] developed relational position encodings that offer sequential information to\nRelational Graph Attention Networks (RGAT) that reflect the relational graph structure. As a result,\nboth the speaker dependency and the sequential information may be captured by our RGAT model.\nThe proposed model framework involves three modules of contextual utterance embedding. Fine-tune\nthe BERT model for the speaker dependency module graph-based neural network, and finally, the\nclassifier is used to classify the emotion. The proposed RGAT method obtained a weighted F1 score\nof 60.91% on the MELD dataset (textual modality).\nHu et al. [21] proposed a Multimodal Dynamic Fusion Network (MM-DFN) to classify emotions\nin multimodal conversation. The proposed method used multimodalities, i.e., Text, audio, and visual,\nto predict the emotions label. For textual feature extraction, they applied a bidirectional gated\nrecurrent unit. MM-DFN model achieved 62.49% accuracy and 59.46% weighted F1 score on the\nMELD dataset using multimodality (Text, audio, and video).\nRecognizing human emotion based on textual conversations has several applications in various\ndomains, such as opinion mining, healthcare, psychology, robotics, human-computer interaction, and\nIoT-based system. Several researchers have proposed different computational models for emotion\ndetection in conversation, achieving good accuracy. However, their datasets were imbalanced, which\naffected the performance and accuracy of the models. Few researchers have addressed text augmen-\ntation in the published literature. Most of the study uses non-contextual word embedding, affecting\nthe previous model’s accuracy. To the author’s knowledge, the researchers have yet to use deep neural\nnetworks for emotion recognition. In this work, we propose a text augmentation-based model for\nemotion recognition to increase accuracy using a balanced dataset.\nThe objective of this study is listed below:\n• The text augmentation technique is applied to the MELD dataset to improve ERC accuracy.\n• The dataset is balanced to overcome the impact of the imbalance dataset.\n• Transformer-based BERT model is applied to contextualized word embedding.\n• DNN is applied for emotions classification.\n• The performance of the suggested model is assessed using various performance metrics.\nBased on the objectives mentioned above, the proposed model performance is better than the\npreviously published techniques in terms of accuracy, weighted F1 score, recall, and other evaluation\nmetrics, which define the novelty of the proposed work.\nThe rest of the paper is organized as follows:Section 3describes the methodology. Results and\nDiscussion are inSection 4,a n dSection 5gives the study’s conclusion.\nCMC, 2023, vol.76, no.3 3529\n3 The Proposed Model\nThis section explains the detail of benchmark datasets, text data augmentation methods, word\nembeddings, transformers, and classification.Fig. 2illustrates the block diagram of the proposed\nmodel. The proposed research begins with a technical assessment of the available computer models\nfor classifying emotions in a conversation. Many researchers have worked to classify human emotions\nin a conversation by applying machine learning and deep learning techniques to achieve good accuracy.\nHowever, there are still possible ways and techniques to achieve better results. Secondly, the proposed\nstudy selects a valid MELD dataset focusing on textual data. Thirdly, MELD is an imbalanced\ndataset; therefore, different text augmentation techniques are applied to resolve the problem. Fourthly,\nword embedding techniques are applied to convert Text into vectors while keeping the contextual\ninformation of a sentence. BERT transformer is used for this purpose. Fifthly, the BERT model for\nclassification is applied to classify emotions. Finally, the evaluation matrices, such as confusion metrics,\nare calculated to evaluate the proposed model.\nFigure 2:Proposed model block diagram\n3.1 Dataset\nThis study uses MELD [1] as a benchmark dataset, the extended version of the Emotion Line\n[22] dataset. The MELD dataset is a multimodal dataset that includes Text, video, and auditory data\nfrom the friend’s television series. The proposed method only considers the textual data. The MELD\ndataset has 1400 conversations and 13000 utterances. Each utterance in the MELD dataset has been\nassigned 1 of the seven emotions: fear, surprise, joy, sadness, neutral, disgust, and anger.Fig. 3shows\nthe distribution of the MELD dataset.\n3.2 Text Data Augmentation\nIn the classification task, the class imbalance problem is one of the key issues which can cause bias,\nand classifiers are biased toward the majority class [23]. For a long time, the problem of imbalanced\ndata has been a focused area of research, and several solutions have been developed [24]. The MELD\ndataset class distribution is shown inFig. 3. The neutral class has 4710 instances, the majority group\nand the disgusting class has 271 records, a minority group. So, the MELD dataset is imbalanced\n(contains a class imbalance problem). Data augmentation techniques can be employed to resolve the\nMELD dataset imbalanced problem. The following data augmentation techniques are used to resolve\nthe imbalanced dataset problem.\n3530 CMC, 2023, vol.76, no.3\n• Back Translation Methods,\n• Easy Data Augmentation Methods,\n• NLPAlbumentation Methods,\n• NLPAug Library.\nFigure 3:MELD dataset class distribution\nTo balance the MELD dataset, we substitute a word with its synonym (synonym replacement)\nthrough word embeddings to achieve a term with similar sense but dissimilar words in a sentence,\nreplacing words with their synonyms. The synonym substitution is based on the Bert-base-uncased\npre-trained contextual word embedding technique to find the synonym for the selected word. The\naugmented data is defined byEq. (1). This is done by applying the Data Augmentation function to\nthe original data.\nx\n′\ni = DataAugmentation(xi) (1)\nwhere the original training data sentences are denoted by the variablesxi, the augmented datax′\ni is\ncreated by changing synonyms in the original training data sentences. However, the label in the original\ntraining data’y\ni and the augmented datay′\ni label is identical toyi.\n3.3 Transformer and Word Embedding\nThe transformer technique was introduced by a group of Google researchers [25]. RNNs and\nCNNs were the most commonly utilized NLP algorithms at that time. The capabilities of the\ntransformer technique are primarily improved due to the application [26] of RNNs and CNNs models\nbecause these models do not require data in the sequence’s forms. These models have the capabilities\nto process the data in any order. As the transformer technique processes data in any order, it enables\nthe models to train on larger datasets that were previously inconceivable. Thus, pre-trained models like\nBERT could train on vast amounts of language data before release.\nWhen classifying text data, the first step is to create a data representation (numerical data) that\nmay be used to train machine learning or deep learning model. Because machines cannot understand\nnatural language or Text in the same way humans can, the text data must be characterized in actual\nCMC, 2023, vol.76, no.3 3531\nvalues, with syntactically and semantically equivalent meaning word vectors lying close to each other\nin vector space and mathematical relationships drawn between them. This method is known as word\nembeddings. There are two main classes of word embeddings: contextual and non-contextual. Context\nembedding models may create unique word embeddings for a word that capture its position in a\nsentence, making them context-dependent. Transformer-based models are commonly used to produce\nthese embeddings. The embeddings are obtained by feeding the entire Text to a model that has already\nbeen trained (pre-trained model). It is worth noting that there is a collection of words here, but it does\nnot include contextual embeddings. The word embeddings are created for each word by the other words\nin a sentence. The context of a sentence refers to those different words. The attention process used\nby the transformer-based models looks at the relationship between a word’s neighbors (surrounding\nwords). Consequently, given a word, embeddings are generated dynamically from a pre-trained or\nfine-tuned model. Thus, we apply transformer-based contextual embeddings.\nBecause of the transformer, BERT is better at understanding context and ambiguity in language.\nEach word in the phrase is examined by all the other words rather than separately by the transformer.\nUnlike the previous language processing method known as word embedding, earlier models such as\nword2vec and GloVe would create a vector for each word that only represented one dimension, or\na slice, of that word’s meaning. BERT uses a masked language modeling technique to stop the word\nin focus from “seeing itself” or having a fixed meaning independent of its context. The hidden word\nmust then be recognized solely by its context. Instead of having a predetermined identity, their context\ndefines words in BERT. Transformers use the encoder and decoder architecture.\nThe encoder block employs the self-attention method to provide contextual information from\nthe entire phrase to each token. Any token may have multiple semantics or functions depending on\nthe tokens around it. As a result, the self-attention mechanism uses eight parallel attention heads to\nallow the model to tap into a variety of embedding subspaces. A linear layer, ReLU, and another\nlinear layer process each embedding vector individually with identical weights in the position-wise\nfeed-forward network (FFN). As a result, the FFN layer transforms each embedding vector, which\nincludes contextual information from multi-head attention. Assuming “x” is an embedding vector, the\nFFN can be formulated asEq. (2).\nf (x) = g (xw\n1 + b1) w2 + b2 (2)\nwhere x is an embedding vector, the FFN includes a ReLU activation functiong, weightsw1 and w2,\nand biased termsb1 and b2. w1 increases the dimensionality ofx from 512 to 2048, whilew2 decreases\nit back to 512. The weights in the FFN are consistent within the same layer. Residual connections,\nexpressed asEq. (3), are used in the encoder block, employing element-wise additions.\nx + sublayer(x) (3)\nThe encoder blocks in the transformer utilize multi-head attention and FFN as sublayers.\nResidual connections pass the prior embeddings to the next layer. The encoder blocks enrich the\nembedding vectors with information from multi-head self-attention and FFN computations. Each\nresidual connection is followed by layer normalization, as depicted inEq. (4). Layer normalization\noperates on each embedding vector individually to mitigate the impact of covariant shift and ensure\nstable and efficient training.\nNormalizLayer(x + sublayer(x)) (4)\n3532 CMC, 2023, vol.76, no.3\nThe transformer architecture consists of six stacked encoder blocks. The decoder utilizes the\noutputs from the final encoder block as input features. These input features are enriched embeddings\nthrough multi-head attention, FFN, residual connections, and layer normalization.\n3.4 Deep Neural Network\nThis study applies a deep neural network (DNN) as a classification engine. The deep neural\nnetwork is a subfield of AI and directly learns features from the data before making decisions. In\nnumerous fields, including speech recognition [27–30], image processing [31–34], natural language\nprocessing [35,36], and bioengineering [37], deep learning algorithms have demonstrated that they\nare the most effective and exceptional machine learning algorithms. Additionally, several studies\ndemonstrated that deep learning algorithms outperform traditional machine learning methods when\napplied to various complex learning problems [38,39]. Due to its impressive performance in various\ndomains, we used the DNN model as a classifier for emotion recognition in conversation. The DNN\nmodel comprises an input, output, and several hidden layers. The input layer is the first layer of the\nmodel through which the data is fed to the model. The output layer is the last layer of the model, and\nit generates the model output. Several hidden layers in the middle are involved in learning the model\nthrough the learning procedure. The performance of a DNN model is influenced by the number of\nhidden layers and other configurations through hyperparameters. In general, a network configured\nwith many hidden layers can produce better performance [40]; however, severe issues like overfitting,\ncomputation costs, and model complexity may arise [41].\nThe DNN model is configured with two input layers (one for kids and another for attention\nmasks), two hidden layers, and one output layer, as illustrated inFig. 4. Each layer is set up with a\ndifferent number of neurons. First, the input layer receives the input ids and attention mask vectors\nproduced by the word embeddings. The input from these layers is then passed to the hidden layer, which\nhas 512 neurons and a rectified linear activation unit (ReLU) activation function. The output of the\nhidden layers is then passed to the output layer, which contains seven neurons with SoftMax activation\nfunctions to do multi-class classification. The output layer will generate a probability distribution\nvector with the probabilities for each emotion class. In the probability distribution vector emotion, a\nclass with the highest probability will be considered a predicted emotion. Adam optimizer is used to\noptimize the weights of the model. The BERT model is a fine-tuning of the augmented dataset. The\nDNN can be expressed as:\nZ\n1 = w1 × (input ids, attention mask) × b1 (5)\nZ2 = BERT(w2 × Z1 × b2) (6)\nZ3 = relu(w3 × Z2 × b3) (7)\nZ4 = SoftMax(Z3) (8)\nwhere Z1 represents the first layer output after applying the weighted sum of the word input_ids and\nattention_mask (attention mask), using the weight matrixw1 and bias vectorb1, Z2 represents the\noutput of the second layer, where we apply the BERT model on the weighted sum ofZ1, using the\nweight matrixw2 and bias vectorb2. Z3 represents the intermediate output after applying the rectified\nlinear activation function (relu) to the weighted sum ofZ2, using the second layer’s weight matrixw3\nand bias vectorb3. Z4 represents the final output after applying the softmax activation function toZ3,\nwhich represents the predicted emotion class probabilities.\nCMC, 2023, vol.76, no.3 3533\nFigure 4:Proposed model architecture\n3.5 Performance Evaluation\nIt is essential to measure how well the classification model predicts the desired outcome when\nconstructing and optimizing it. Before being used in a real-world environment, the performance of\na newly developed classifier based on machine learning algorithms can be evaluated using specific\nprocesses [42]. We can only know whether a learning model is helpful if it is evaluated. Various types\nof performance evaluation metrics for evaluating the performance of a machine learning model have\nbeen proposed in the literature for evaluating the performance of a machine learning model [43]. In this\nstudy, we have considered the following metrics to evaluate the performance of the proposed model.\nSpecificity = no of true negative\nno of true negative+ no of false positive= TN\nTN + FP (9)\nRecall = no of true positive\nno of true positive+ no of false negative= TP\nTP + FN (10)\nAccuracy = no of correct prediction\ntotal number of predictions= TP + TN\nTP + TN + FP + FN (11)\nprecision = True Positive\nTotal predicted positive= TP\nTP + FP (12)\nF1 score = 2 × (precision ∗ recall)\n(precision + recall) (13)\nIn the case of a multi-class problem, the parameters of the confusion matrix can be calculated as\nTrue Positive (TP), False Positive (FP), True Negative (TN), and False Negative (FN). The TP value is\none in which the actual and predicted values are equal (same) and can be formulated asEq. (14)where\n“i” is a reference to each class and “c” is the value in the cell where the number of row and column is\nequal for example c11.\ntp\ni = cii (14)\n3534 CMC, 2023, vol.76, no.3\nThe FP value for a class is the total of the values in the corresponding column, except the True\nPositive value. FP can be formulated asEq. (15).\nfpi =\nn∑\nk=1\ncki − tpi (15)\nTN is the sum of the column and row values, excluding the class values for which the values are\nbeing computed, which makes up the TN value for a given class. TN formulates with the help of\nEq. (16).\ntn\ni =\nn∑\nk=1\nn∑\nj=1\nckj − tpi − fpi − fni (16)\nThe FN value for a class is the sum of the values of the related rows, except the true positive value.\nFN can be formulated asEq. (17).\nfni =\nn∑\nk=1\ncik − tpi (17)\n3.6 Experimental Setup\nIn this section, firstly, the experimental environment is described. Secondly, the methodology to\nbalance the benchmark dataset is presented.\n3.6.1 System Specification\nGoogle Colab GPU is used to perform the experiments. We used Keras, TensorFlow, Transform-\ners, Matplotlib, nlpaug, pickle, pandas, NumPy, and sklearn libraries to implement the proposed\nmodel.\n3.6.2 MELD Dataset and Augmentation\nFor dataset balancing, one method is to drop some of the samples, called downsampling, and\nanother is to add additional samples to some of the emotion classes that make all the emotion\nclass samples equal in number, called augmentation. Both downsampling and augmentation methods\nbalance the dataset.\n1. MELD Training Dataset\nAfter augmentation, the following are the statistics of the training dataset.\n• MELD: The original MELD training dataset.\n• MELD\nN4710: Augmentation concerning the total number of training samples of neutral\nemotion class which are 4710, shown inTable 1.\n• MELDJ1743: Augmentation and downsampling concerning the total no of training samples\nof joy emotion class which are 1743, shown inTable 1.\n• MELDA1109: Augmentation and downsampling concerning the total no of training samples\nof anger emotion class which are 1109, shown inTable 1.\n• MELDAvg1427: Augmentation and downsampling concerning the average of all emotion\nclasses training samples, which are 1427, shown inTable 1.\nCMC, 2023, vol.76, no.3 3535\nTable 1: Augmentation and downsampling of MELD train dataset\nEmotions label Total number of training samples\nMELD MELD N4710 MELDJ1743 MELDA1109 MELDAvg1427\nNeutral 4710 4710 1743 1109 1427\nSurprise 1205 4710 1743 1109 1427\nFear 268 4710 1743 1109 1427\nSadness 683 4710 1743 1109 1427\nJoy 1743 4710 1743 1109 1427\nDisgust 271 4710 1743 1109 1427\nAnger 1109 4710 1743 1109 1427\nOverall 9989 32970 12201 7763 9989\n2. MELD Test and Validation Dataset\nAfter augmentation, the following are the statistics of test and validation datasets:\n• MELDT: The original MELD test dataset.\n• MELDV: The original MELD validation dataset.\n• MELDTN1256: Augmentation concerning the total number of testing samples of neutral\nemotion class which are 1256, shown inTable 2.\n• MELDTJ402: Augmentation and downsampling concerning the total no of testing samples of\njoy emotion class which are 402, shown inTable 2.\n• MELDTS281: Augmentation and downsampling concerning the total no of testing samples of\nsurprise emotion class, which is 281, shown inTable 2.\n• MELDVN470: Augmentation concerning the total number of validation samples of the neutral\nemotion class, which is 470, shown inTable 3.\n• MELDVJ163: Augmentation and downsampling concerning the total no of validation samples\nof joy emotion class which are 163, shown inTable 3.\n• MELDVS150: Augmentation and downsampling concerning the total no of validation samples\nof surprise emotion class, which are 150, shown inTable 3.\n• MELDV20: 20 percent of the training dataset.\nTable 2: Augmentation and downsampling of the MELD test dataset\nEmotions label Total number of testing samples\nMELDT MELDTN1256 MELDTJ402 MELDTS281\nNeutral 1256 1256 402 281\nSurprise 281 1256 402 281\nFear 50 1256 402 281\nSadness 208 1256 402 281\nJoy 402 1256 402 281\nDisgust 68 1256 402 281\n(Continued)\n3536 CMC, 2023, vol.76, no.3\nTable 2 (continued)\nEmotions label Total number of testing samples\nMELDT MELDTN1256 MELDTJ402 MELDTS281\nAnger 345 1256 402 281\nOverall 2610 8792 2814 1967\nTable 3: Augmentation and downsampling of the MELD validation dataset\nEmotions label Total number of validation samples\nMELDV MELDVN470 MELDVJ163 MELDVS150\nNeutral 470 470 163 150\nSurprise 150 470 163 150\nFear 40 470 163 150\nSadness 111 470 163 150\nJoy 163 470 163 150\nDisgust 22 470 163 150\nAnger 153 470 163 150\nOverall 1109 3290 1141 1050\n3.6.3 Configuration Parameters\nIn Deep learning models, several configuration parameters require configuring. The values\nof these parameters significantly affect how well a learning model performs. The configuration\nparameters for the model configuration are listed inTable 4, along with descriptions of each. Unlike\nthe regular parameters, the user specifies configuration parameters during model setup. Deciding what\nvalues to specify for a learning model configuration parameter for a given dataset might be challenging.\nTo discover the optimal configuration parameters for the suggested computational model, we tweaked\nthe configuration parameters by altering their values. We created a model for each augmentation,\nadjusted its configuration parameters, and assessed its output before archiving the data. The set of\nconfiguration parameters that produces the best results among all models is chosen as the optimum\nparameter set for the classifier. We consider the essential parameters during configuration parameter\noptimization, such as learning rate, batch size, and number of epochs.Table 5shows the optimal\nconfiguration parameters found for the proposed model.\nTable 4: Hyperparameters and their explanation\nS. No. Parameter Explanation\n1 Learning rate The learning rate defines how fast a network updates its weights. If the\nlearning rate is lower, converging will take longer. If the learning rate is\nhigh, it will overshoot the minima. Therefore, we train our model on\ndifferent learning rates and get the optimum one.\n(Continued)\nCMC, 2023, vol.76, no.3 3537\nTable 4 (continued)\nS. No. Parameter Explanation\n2 No of epochs One epoch means when all the data is passed through the network,\ncompleting both forward and backward passes.\n3 Batch size We divide our complete dataset into batches. Batch size means the total\nno of training samples appearing in a batch. We choose batch sizes 1, 4,\n8, 16, and 32 for our model.\n4 Activation\nfunction\nWe add an activation function in our artificial neural network to learn\nthe complex patterns in the data. There are several activation functions.\nReLU and SoftMax activation functions are used for the proposed\nmodel.\n5 Optimizer Optimizers are used to fine-tune a model’s parameters. The primary role\nof an optimizer is to modify model weights to maximize or minimize the\nloss function. The loss function is used to measure how well the model\nperforms.\n6 Decay The decay learning rate is a training method for advanced artificial\nneural networks. It begins by training the network with a high learning\nrate and gradually reduces it until local minima are obtained. Decay\nhelps in both optimization and generalization.\nTable 5: Optimal hyperparameters with a configuration value\nS. No. Parameter Value\n1 Word embedding BERTbase-cased model\n2 Shuffle train dataset By 2000\n3 Shuffle validation dataset By 100\n4B a t c h s i z e 1 6\n5 learning rate 1e-5\n6 Decay rate 1e-6\n7 Loss function Categorical_Cross_Entropy\n8 Optimizer Adam\n9 Epochs 3\n10 Activation function ReLU and SoftMax\n4 Results and Discussion\n4.1 Experiments\nThis section discusses and analyzes the experimental results with different text augmentation\ntechniques.\n3538 CMC, 2023, vol.76, no.3\n4.1.1 Experiment-1\nThe proposed model is trained and validated using the original MELD dataset in the first\nexperiment. The MELD dataset contained training instances (9989), the validating instances MELDV\n(1109), and test instances MELDT (2610). The BERT model transformed the MELD text utterance\ninto a discrete vector.Fig. 5 shows that the validation loss is increasing over each epoch while\nincreasing the number of epochs; the training accuracy of the proposed model is also increasing from\n60.64% to 88.32% while the validation accuracy decreases from 60.87% to 56.97%. This behavior of\nthe model shows that the model is overfitting.\nFigure 5:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-1: (a) error loss of the proposed model; (b) accuracy of the proposed model\n4.1.2 Experiment-2\nIn this experiment, the performance of the proposed model is analyzed by an augmented MLED\ndataset considering neutral class instances. The neutral class has 4710 instances, which is more than the\ninstances of other classes. Hence, we have increased the number of instances of other classes through\naugmentation to balance the dataset.Fig. 6shows that increasing the number of epochs decreases\nthe training error loss while the validation error loss increases. Moreover, the training accuracy of the\nmodel is increasing from 52.29% to 83.21%, and validation accuracy is decreasing 57.44% to 54.37%\nwith increasing the number of epochs. The results show that the model is overfitting.\n4.1.3 Experiment-3\nIn this experiment, the performance of the proposed model is analyzed by an augmented MLED\ndataset considering the joy class instances. The joy class has 1743 instances which are second the\nbiggest in several instances. Hence, we have decreased the number of instances of the neutral class and\nincreased the number of instances of the five remaining classes to balance the dataset. The result of\nthis experiment is illustrated inFig. 7, which shows that the training error loss decreases by increasing\nthe number of epochs. In contrast, the validation error loss increases, and the training accuracy of\nthe model increases from 43.91% to 68.54%, while the validation accuracy decreases from 58.70% to\n57.25%. With increasing the number of epochs, the results show that the model is overfitting.\nCMC, 2023, vol.76, no.3 3539\nFigure 6:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-2: (a) error loss of the proposed model; (b) accuracy of the proposed model\nFigure 7:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-3: (a) error loss of the proposed model; (b) accuracy of the proposed model\n4.1.4 Experiment-4\nIn the fourth experiment, the model is trained by feeding the augmented MELDA1109 train, testing\nMELDT (2610), and validating MELDV (1109) dataset. All the other parameters remain the same as\nin the third experiment. The training and validation loss depicted inFig. 8shows that the training\naccuracy increases over each epoch from 43.99% to 67.55%, while the validation accuracy decreases\nfrom 56.70% to 54.62%.\n4.1.5 Experiment-5\nIn the fifth experiment, the model is trained by feeding the augmented MELDAvg1427 train, testing\nMELDT (2610), and validating MELDV (1109) datasets. All the other parameters are the same as\nin the third experiment. The training and validation loss depicted inFig. 9shows that the training\naccuracy increases over each epoch from 43.28% to 67.19%, while the validation accuracy decreases\nfrom 58.88% to 57.79%.\n3540 CMC, 2023, vol.76, no.3\nFigure 8:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-4: (a) error loss of the proposed model; (b) accuracy of the proposed model\nFigure 9:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-5: (a) error loss of the proposed model; (b) accuracy of the proposed model\n4.1.6 Experiment-6\nThe proposed model is trained in the sixth experiment by feeding the augmented MELDJ1743 train,\ntesting MELDTJ402 (2610), and validating the MELDVN470 (1109) datasets. All the other parameters\nare the same as in the third experiment.Fig. 10shows that the training accuracy increases over each\nepoch from 44.90% to 68.84% for three epochs, while the validation accuracy decreases from 42.47%\nto 41.40%.\n4.1.7 Experiment-7\nIn the seventh experiment, we train the model by feeding the augmented MELDN4710 train and\ntesting the MELDT datasets. All the other parameters are the same as in the third experiment.Fig. 11\nshows that the model training is the best fit because the training accuracy increases from 45.39% to\n72.57% over each epoch. In comparison, the validation accuracy also increases over each epoch from\n53.05% to 69.33%.\nCMC, 2023, vol.76, no.3 3541\nFigure 10:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-6: (a) error loss of the proposed model; (b) accuracy of the proposed model\nFigure 11:Error loss and accuracy of the proposed model on both training and validation datasets in\nExperiment-7: (a) error loss of the proposed model; (b) accuracy of the proposed model\nAll seven experiments’ training, validation, and testing accuracy are shown inTable 6.\nTable 6: Impact of data augmentation on the accuracy of the proposed model\nExperiment Dataset Accuracy Weighted\nF1 scoreTrain Val Test Training Validation Testing\nE1 MELD MELD V MELDT 88.32% 56.97% 52.91% 53.78%\nE2 MELDN4710 MELDV MELDT 83.21% 54.37% 62.83% 59.45%\nE3 MELDJ1743 MELDV MELDT 68.54% 57.25% 58.08% 59.00%\nE4 MELDA1109 MELDV MELDT 67.55% 54.62% 57.50% 58.93%\nE5 MELDAvg1427 MELDV MELDT 67.19% 58.79% 59.84% 60.28%\nE6 MELDJ1743 MELDVN470 MELDTJ402 68.44% 41.40% 49.73% 49.23%\nE7 MELDN4710 MELDV20 MELDT 72.57% 69.33% 63.10% 61.55%\n3542 CMC, 2023, vol.76, no.3\n4.2 Parameter Tuning\nIn this section, the impact of configuration parameters on the performance of the proposed model\nis analyzed. We have considered two influential parameters for the tuning purpose to improve the\nperformance of the proposed model. These parameters are learning rate and batch size. It is to be\nnoted that for parameter tuning, we have considered Experiment-7.\n4.2.1 Learning Rate\nThe learning rate is a significant factor in machine learning because it controls the size of the\nmodel’s steps during each iteration. The amount by which weights are updated during the model\ntraining phase is known as the step size. The learning rate values range between 0.0 and 1.0. A large\nvalue can quickly train the model, but it may overshoot or ignore some of the best characteristics of\nthe features being used during the model training. A small learning rate value may cause overfitting\nand take longer to train the model. The impact of various learning rate values on the proposed model’s\nperformance is presented inTable 7.\nTable 7: Impact of learning rate on accuracy and F1 score\nLearning rate Training acc (%) Validation acc (%) Testing acc (%) F1 score (%)\n0.1 14.43 14.44 48.12 31.26\n0.01 14.07 14.40 13.21 3\n0.001 13.85 14.32 7.0 1.0\n0.0001 14.42 14.15 1.0 0.0\n0.00001 72.57 69.33 63.10 61.55\n0.000001 46.75% 48.09% 63.10% 59.74%\nTable 7shows that there is a high variation in both training and testing accuracies when the values\nof the learning rate are altered. For example, the model generated training and validation accuracies of\n14.07% and 14.40%, respectively, on the learning rate 0.1. In contrast, the model yielded 72.57% and\n69.33% training and testing accuracies, respectively, using a learning rate 0.0001. These results show\nthat the learning rate substantially impacts the outcome of the proposed model.\n4.2.2 Batch Size\nBatch size means the total no of training samples appearing in a batch. For the proposed model,\nwe choose the 8, 16, 32, and 64 batch sizes to find an optimal batch size. The impact of batch size on\naccuracy and F1 score is shown inTable 8.\nTable 8: Impact of batch size on accuracy and F1 score\nBatch size Training acc (%) Validation acc (%) Testing acc (%) F1 score (%)\n8 76.84 69.85 59.50 59.32\n16 72.57 69.33 63.10 61.55\n32 66.77 64.58 63.06 62.12\nCMC, 2023, vol.76, no.3 3543\nIt can be observed fromTable 8that the proposed model generated different outcomes with\nvarying batch sizes. For example, the model produced 76.84% and 69.85% training and validation\naccuracies using batch size 8, whereas the model obtained 66.77% and 64.58% training and validation\naccuracies, respectively, using batch size 32. The variations in accuracies show that batch size has a\nhigh impact on the performance of the proposed model.\n4.2.3 Epochs\nThe impact of epochs on the performance of the proposed model is presented inFig. 12.F r o m\nFig. 12a, we can observe that the error loss continuously decreases when increasing the number of\nepochs. Similarly, inFig. 12b, the proposed model’s accuracy increases when the number of epochs\nincreases. These results yielded that the number of epochs impacts the performance of the proposed\nmodel. Table 9shows the performance of the proposed model with two epochs.\nFigure 12:Error loss and accuracy of the proposed model on both training and validation dataset: (a)\nerror loss of the proposed model; (b) accuracy of the proposed model\nTable 9: Performance of the proposed model with two epochs\nEmotion classes Precision (%) Recall (%) F1 score (%) Support\nAnger 55 37 44 345\nDisgust 30 25 27 68\nFear 19 22 20 50\nJoy 57 59 58 402\nNeutral 73 86 79 1256\nSadness 41 22 28 208\nSurprise 61 56 59 281\nAccuracy 64 2610\nMacro average 48 44 45 2610\nWeighted average 62 64 63 2610\n3544 CMC, 2023, vol.76, no.3\n4.3 Comparison of Proposed Model with Existing Models\nThis section compares the proposed model’s performance with the existing models. The compar-\nison is made with nine currently published models from the literature review, which are: RGCN [5],\nDialogueGCN [6], DialogueRNN [7], DialogueCRN [8], CESTa [13], AGHMN [15], A-DMN [12],\nBiERU [19], MM-DFN [21]. The comparison is made regarding the accuracy and weighted F1 score, as\nshown inTable 10. The proposed model significantly improves on the current state-of-the-art accuracy\nand F1 score models. For example, considering the accuracy of the proposed model of 63.10%, the\nsecond-highest accuracy, 62.49%, was achieved by the MM-DFN [21], and the third-highest accuracy,\n60.90%, was achieved by BiERU [19]. Regarding the F1 score, the proposed model scored 61.55%, and\nthe A-DMN [12] achieved 60.45%. These results confirmed that the proposed model improves on the\nexisting models and can recognize emotion with better accuracy and F1 score, illustrated inTable 10.\nTable 10: Comparison of accuracy and F1 score of the proposed model and existing models\nS. No. Methods Accuracy % Weighted F1 score %\n1 RGCN [ 5] – 55.98\n2 DialogueGCN [ 6] 59.46% 58.10\n3 DialogueRNN [ 7] 59.54 57.03\n4 DialogueCRN [ 8] 60.73 58.39\n5C E S T a [ 13] – 58.36\n6 AGHMN [ 15] – 58.10\n7 A-DMN [ 12] – 60.45\n8B i E R U [ 19] 60.90 –\n9 MM-DFN [ 21] 62.49 59.46\n10 Proposed model 64.36 62.60\nNote: ∗∗ The missing values (−) for the attributes show that the model(s) did not consider\nthat attributes in the original paper.\n5 Conclusions\nThis research study presented a computational model for recognizing human emotion in con-\nversation. In this research, we applied text data augmentation on the MELD dataset to balance the\ndataset. For balancing the dataset, sentences are generated from the MELD dataset utterance without\ndisturbing the meaning of the utterance. In this research, firstly, we employed text augmentation\ntechniques to balance the imbalanced dataset. Secondly, word embedding was employed, and BERT\nmodels were used as feature extraction techniques to construct the feature vector. Finally, an emotion\nrecognition system was developed based on the DNN model. The outcome of the proposed emotion\nrecognition system was rigorously assessed using various measurement metrics, including accuracy,\nweighted F1 score, recall, precision, and confusion matrix. The performance of the proposed model\nwas examined on different learning rates and batch sizes. The results demonstrate that the proposed\nmodel achieved the highest accuracy, 64.36%, and the maximum F1 score, 62.60%. Moreover, the\nproposed model performs significantly better at predicting human emotion in conversation when\ncompared with existing approaches. The proposed model can be used as a helpful tool for identifying\nhuman emotion and may have applications in many fields, including psychology in healthcare, student\ndissatisfaction in education, robotics, automated client support systems, and opinion mining.\nCMC, 2023, vol.76, no.3 3545\nAcknowledgement: The authors sincerely appreciate the support from Princess Nourah bint Abdulrah-\nman University Researchers, Riyadh, Saudi Arabia for providing all the necessary technical assistance.\nFunding Statement:Princess Nourah bint Abdulrahman University Researchers Supporting Project\nNumber (PNURSP2023R235), Princess Nourah bint Abdulrahman University, Riyadh,\nSaudi Arabia.\nAuthor Contributions:The authors confirm contribution to the paper as follows: study conception and\ndesign: Fida Mohammad, Mukhtaj khan; data collection: Safdar Nawaz Khan Marwat; analysis and\ninterpretation of results: Neelam Gohar, Naveed Jan, Muhammad Bilal; draft manuscript preparation:\nFida Muhammad, Amal Al-Rasheed. All authors reviewed the results and approved the final version\nof the manuscript.\nAvailability of Data and Materials:The data will be shared up on request.\nConflicts of Interest:The authors declare no conflicts of interest to report regarding the present study.\nReferences\n[1] N. Sebe, I. Cohen, T. Gevers, T. Huang Nicu Sebe and T. S. Huang, “Multimodal approaches for emotion\nrecognition: A survey,”Internet Imaging, vol. 5670, no. 17, pp. 56–67, 2005.\n[2] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kolliaset al.,“Emotion recognition in human-\ncomputer interaction,”IEEE Signal Processing Magazine, vol. 18, no. 1, pp. 32–80, 2001.\n[3] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambriaet al., “MELD: A multimodal multi-party\ndataset for emotion recognition in conversations,” inthe ACL Conf. on Natural Language Processing,\nFlorence, Italy, pp. 527–536, 2019.\n[4] J. Devlin, M. W . Chang, K. Lee, K. T. Google and A. I. Language, “BERT: Pre-training of deep\nbidirectional transformers for language understanding,” inComputational Linguistics Conf. on Human\nLanguage Technologies, Minneapolis, Minnesota, USA, pp. 4171–4186, 2019.\n[5] Y . J. Choi, Y . W . Lee and B. G. Kim, “Residual-based graph convolutional network for emotion recognition\nin the conversation for smart internet of things,”Big Data, vol. 9, no. 4, pp. 279–288, 2021.\n[6] D. Ghosal, N. Majumder, S. Poria, N. Chhaya and A. Gelbukh, “DialogueGCN: A graph convolutional\nneural network for emotion recognition in conversation,” inThe EMNLP-IJCNLP Conf. on Empirical\nMethods in Natural Language Processing, Hong Kong, China, pp. 154–164, 2019.\n[7] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukhet al.,“DialogueRNN: An attentive RNN\nfor emotion detection in conversations,” inThe AAAI Conf. on Artificial Intelligence, Honolulu, Hawaii,\nUSA, pp. 6818–6825, 2019.\n[8] D. Hu, L. Wei and X. Huai, “DialogueCRN: Contextual reasoning networks for emotion recognition in\nconversations,”in The ACL-IJCNLP Conf. on Natural Language Processing, Bangkok, Thailand, pp. 7042–\n7052, 2021.\n[9] P . Zhong, D. Wang and C. Miao, “DialogueCRN: Contextual reasoning networks for emotion recognition\nin conversations,” inThe EMNLP-IJCNLP Conf. on Empirical Methods in Natural Language Processing,\nHong Kong, China, pp. 154–164, 2019.\n[10] S. M. Mohammad, “Obtaining reliable human ratings of valence, arousal, and dominance for 20,000\nEnglish words,” inThe ACL Conf. on Economics and Natural Language Processing, Melbourne, Australia,\npp. 174–184, 2018.\n[11] R. Speer, J. Chin and C. Havasi, “ConceptNet 5.5: An open multilingual graph of general knowledge,” in\nThe AAAI Conf. on Artificial Intelligence, San Francisco, California, USA, pp. 4444–4451, 2017.\n[12] S. Xing, S. Mai and H. Hu, “Adapted dynamic memory network for emotion recognition in conversation,”\nIEEE Transactions on Affective Computing, vol. 13, no. 3, pp. 1426–1439, 2022.\n3546 CMC, 2023, vol.76, no.3\n[13] Y . Wang, J. Zhang, J. Ma, S. Wang and J. Xiao, “Contextualized emotion recognition in conversation as\nsequence tagging,” inThe ACL SIGDIAL Conf. on Discourse and Dialogue, 1st Virtual Meeting, pp. 186–\n195, 2020.\n[14] S. L. Y eh, Y . S. Lin and C. C. Lee, “A dialogical emotion decoder for speech motion recognition in spoken\ndialog,” inThe ICASSP , IEEE Conf. on Acoustics, Speech, and Signal Processing, Barcelona, Spain, pp.\n6479–6483, 2020.\n[15] W . Jiao, M. R. Lyu and I. King, “Real-time emotion recognition via attention-gated hierarchical memory\nnetwork,” inProc. of the AAAI Conf. on Artificial Intelligence, vol. 34, no. 5, pp. 8002–8009, 2020.\n[16] D. Zhang, L. Wu, C. Sun, S. Li, Q. Zhuet al.,“Modeling both context and speaker-sensitive dependence for\nemotion detection in multi-speaker conversations,” inThe IJCAI Conf. on Artificial Intelligence, Macao,\nChina, pp. 5415–5421, 2019.\n[17] D. Sheng, D. Wang, Y . Shen, H. Zheng and H. Liu, “Summarize before aggregate: A global-to-local\nheterogeneous graph inference network for conversational emotion recognition,” inThe ACL Conf. on\nComputational Linguistics, Barcelona, Spain, pp. 4153–4163, 2020.\n[18] X. Lu, Y . Zhao, Y . Wu, Y . Tian, H. Chenet al.,“An iterative emotion interaction network for emotion\nrecognition in conversations,” inThe ACL Conf. on Computational Linguistics, Barcelona, Spain, pp. 4078–\n4088, 2020.\n[19] W . Li, W . Shao, S. Ji and E. Cambria, “BiERU: Bidirectional emotional recurrent unit for conversational\nsentiment analysis,”Neurocomputing, vol. 467, no. 5, pp. 73–82, 2022.\n[20] T. Ishiwatari, Y . Yasuda, T. Miyazaki and J. Goto, “Relation-aware graph attention networks with relational\nposition encodings for emotion recognition in conversations,” inThe ACL Conf. on Empirical Methods in\nNatural Language Processing, Barcelona, Spain, pp. 7360–7370, 2020.\n[21] D. Hu, X. Hou, L. Wei, L. Jiang and Y . Mo, “Multimodal dynamic fusion network for emotion recognition\nin conversations,” inThe IEEE Conf. on Acoustics, Speech, and Signal Processing, Sydney, Australia, pp.\n7037–7041, 2022.\n[22] S. Y . Chen, C. C. Hsu, C. C. Kuo, T. H. K. Huang and L. W . Ku, “EmotionLines: An emotion corpus of\nmulti-party conversations,” inProc. of the 11th Int. Conf. on Language Resources and Evaluation LREC,\n2018, Miyazaki, Japan, pp. 1597–1601, 2018.\n[23] C. Padurariu and M. E. Breaban, “Dealing with data imbalance in text classification,”Procedia Computer\nScience, vol. 159, no. 7, pp. 736–745, 2019.\n[24] V . Lopez, A. Fernandez, S. Garcia, V . Palade and F . Herrera, “An insight into classification with imbalanced\ndata: Empirical results and current trends on using data intrinsic characteristics,”Information Sciences,v o l .\n250, no. 2–3, pp. 113–141, 2013.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Joneset al.,“Attention is all you need,” inAdvances\nin Neural Information Processing Systems 30 NeurIPS, Long Beach, CA, USA, 2017.\n[26] K. Muhammad, A. Ullah, A. S. Imran, M. Sajjad, M. S. Kiranet al.,“Human action recognition using\nattention-based LSTM network with dilated CNN features,”Future Generation Computer Systems,v o l .\n125, no. 3, pp. 820–830, 2021.\n[27] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamedet al.,“Deep neural networks for acoustic modeling\nin speech recognition: The shared views of four research groups,”IEEE Signal Processing Magazine,v o l .\n29, no. 6, pp. 82–97, 2012.\n[28] G. E. Dahl, T. N. Sainath and G. E. Hinton, “Improving deep neural networks for LVCSR using rectified\nlinear units and dropout,” inThe IEEE Conf. on Acoustics, Speech and Signal Processing, Vancouver,\nCanada, pp. 8609–8613, 2013.\n[29] M. B. Kamal, A. A. Khan, F . A. Khan, M. A. Shahid, C. Wechtaisonget al.,“An innovative approach\nutilizing binary-view transformer for speech recognition task,”Computers, Materials & Continua, vol. 72,\nno. 3, pp. 5547–5562, 2022.\n[30] S. Kumar, M. A. Haq, A. Jain, C. A. Jason, N. R. Moparthiet al.,“Multilayer neural network based speech\nemotion recognition for smart assistance,”Computers, Materials & Continua, vol. 74, no. 1, pp. 1523–1540,\n2023.\nCMC, 2023, vol.76, no.3 3547\n[31] P . Sharma and A. Singh, “Era of deep neural networks: A review,” inProc. of the 8th Int. Conf. on\nComputing, Communication and Networking Technologies, Delhi, India, pp. 1–5, 2017.\n[32] S. Bosse, D. Maniry, K. R. Muller, T. Wiegand and W . Samek, “Deep neural networks for no-reference\nand full-reference image quality assessment,”IEEE Transactions on Image Processing, vol. 27, no. 1, pp.\n206–219, 2018.\n[33] M. Komar, P . Yakobchuk, V . Golovko, V . Dorosh and A. Sachenko, “Deep neural network for image\nrecognition based on the Caffe framework,” inThe IEEE Conf. on Data Stream Mining & Processing,\nLviv, Ukraine, pp. 102–106, 2018.\n[34] H. Wu, Q. Liu and X. Liu, “A review on deep learning approaches to image classification and object\nsegmentation,” Computers, Materials & Continua, vol. 60, no. 2, pp. 575–597, 2019.\n[35] Y . Zhou, “Natural language processing with improved deep learning neural networks,”Scientific Program-\nming, vol. 2022, pp. 1–8, 2022.\n[36] I. Nirmal, A. Khamis, M. Hassan, W . Hu and X. Zhu, “Deep learning for radio-based human sensing:\nRecent advances and future directions,”IEEE Communications Surveys & Tutorials, vol. 23, no. 2, pp. 995–\n1019, 2021.\n[37] A. Bizzego, G. Gabrieli and G. Esposito, “Deep neural networks and transfer learning on a multivariate\nphysiological signal dataset,”Bioengineering, vol. 8, no. 3, pp. 35, 2021.\n[38] I. Nusrat and S. B. Jang, “A comparison of regularization techniques in deep neural networks,”Symmetry,\nvol. 10, no. 11, pp. 648, 2018.\n[39] C. Xu, D. Chai, J. He, X. Zhang and S. Duan, “InnoHAR: A deep neural network for complex human\nactivity recognition,”IEEE Access, vol. 7, pp. 9893–9902, 2019.\n[40] S. Han, R. F . Zhang, L. Shi, R. Richie, H. Liuet al., “Classifying social determinants of health from\nunstructured electronic health records using deep learning-based natural language processing,”Journal of\nBiomedical Informatics, vol. 127, no. 6, pp. 103984, 2022.\n[41] I. Bilbao and J. Bilbao, “Overfitting problem and the over-training in the era of data: Particularly for\nartificial neural networks,” inThe IEEE Conf. on Intelligent Computing and Information Systems,K r a k o w ,\nPoland, pp. 173–177, 2017.\n[42] V . Silva, “Real-time emotions recognition system,” inProc. of the Int. Congress on Ultra-Modern Telecom-\nmunications and Control Systems and Workshops (ICUMT), Lisbon, Portugal, pp. 201–206, 2016.\n[43] C. Halimu, A. Kasem and S. H. S. Newaz, “Empirical comparison of area under ROC curve (AUC) and\nMathew correlation coefficient (MCC) for evaluating machine learning algorithms on imbalanced datasets\nfor binary classification,” inThe ACM Conf. on Machine Learning and Soft Computing,N e wY o r k ,U S A ,\npp. 1–6, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8183982372283936
    },
    {
      "name": "Transformer",
      "score": 0.6736855506896973
    },
    {
      "name": "Encoder",
      "score": 0.6026650667190552
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5494243502616882
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5455429553985596
    },
    {
      "name": "Machine learning",
      "score": 0.5117068290710449
    },
    {
      "name": "Artificial neural network",
      "score": 0.4495019018650055
    },
    {
      "name": "Speech recognition",
      "score": 0.3416590988636017
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801349156",
      "name": "University of Haripur",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I4210112276",
      "name": "Northern University",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I112819271",
      "name": "Shaheed Benazir Bhutto Women University",
      "country": "PK"
    },
    {
      "id": "https://openalex.org/I106778892",
      "name": "Princess Nourah bint Abdulrahman University",
      "country": "SA"
    }
  ]
}