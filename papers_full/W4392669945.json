{
  "title": "Smoothing Entailment Graphs with Language Models",
  "url": "https://openalex.org/W4392669945",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2254838831",
      "name": "Nick McKenna",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103815823",
      "name": "Tianyi Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097308019",
      "name": "Mark Johnson",
      "affiliations": [
        "Macquarie University"
      ]
    },
    {
      "id": "https://openalex.org/A2028447033",
      "name": "Mark Steedman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4226139377",
    "https://openalex.org/W3172798735",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3155618984",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W3119389531",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W2966024226",
    "https://openalex.org/W3186504814",
    "https://openalex.org/W2919611234",
    "https://openalex.org/W2736889448",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W4205686796",
    "https://openalex.org/W2059799772",
    "https://openalex.org/W2122702954",
    "https://openalex.org/W2017087592",
    "https://openalex.org/W2953213518",
    "https://openalex.org/W2000255081",
    "https://openalex.org/W2144211451",
    "https://openalex.org/W65111197",
    "https://openalex.org/W4242626930",
    "https://openalex.org/W2133413932",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3212160013",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W3156587088",
    "https://openalex.org/W1964838558",
    "https://openalex.org/W4294198906",
    "https://openalex.org/W4206510579",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W4285306964",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2509913944",
    "https://openalex.org/W4385573401"
  ],
  "abstract": "Nick McKenna, Tianyi Li, Mark Johnson, Mark Steedman. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 551–563\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n551\nSmoothing Entailment Graphs with Language Models\nNick McKenna† Tianyi Li† Mark Johnson‡ Mark Steedman†\n†University of Edinburgh ‡Macquarie University\nnick.mckenna@ed.ac.uk tianyi.li@ed.ac.uk\nmark.johnson@mq.edu.au steedman@inf.ed.ac.uk\nAbstract\nThe diversity and Zipfian frequency distribu-\ntion of natural language predicates in corpora\nleads to sparsity in Entailment Graphs (EGs)\nbuilt by Open Relation Extraction (ORE). EGs\nare computationally efficient and explainable\nmodels of natural language inference, but as\nsymbolic models, they fail if a novel premise\nor hypothesis vertex is missing at test-time. We\npresent theory and methodology for overcom-\ning such sparsity in symbolic models. First,\nwe introduce a theory of optimal smoothing\nof EGs by constructing transitive chains. We\nthen demonstrate an efficient, open-domain,\nand unsupervised smoothing method using an\noff-the-shelf Language Model to find approx-\nimations of missing premise predicates. This\nimproves recall by 25.1 and 16.3 percentage\npoints on two difficult directional entailment\ndatasets, while raising average precision and\nmaintaining model explainability. Further, in a\nQA task we show that EG smoothing is most\nuseful for answering questions with lesser sup-\nporting text, where missing premise predicates\nare more costly. Finally, controlled experi-\nments with WordNet confirm our theory and\nshow that hypothesis smoothing is difficult, but\npossible in principle.1\n1 Introduction\nAn Entailment Graph (EG) is a learned structure\nfor making natural language inferences of the form\n[premise] entails [hypothesis], such as “if Arsenal\ndefeated Man United, then Arsenal played Man\nUnited.” An EG consists of a set of vertices (typed\nnatural language predicates), and a set of directed\nedges constituting entailments between predicates.\nThey are constructed in an unsupervised manner\nusing the Distributional Inclusion Hypothesis (Gef-\nfet and Dagan, 2005): a representation is generated\nfor each predicate based on its distribution with\narguments in a training corpus, and representation\n1Code available at github.com/nighttime/EntGraph\nFigure 1: The question cannot be answered because a\npredicate in the text isn’t in the Entailment Graph. An\nLM embeds the predicate so a nearest neighbor in the\nEG can be found, completing the directional inference.\nsubsumption is used for learning directional en-\ntailments between predicates. A directional infer-\nence is stricter than paraphrase or similarity, in that\nit is true only in one direction, but not both, e.g.\nDEFEAT ⊨ PLAY but PLAY ⊭ DEFEAT (where ⊨\nmeans “entails”). Directional inferences are diffi-\ncult to learn, but crucial to language understanding.\nEGs are useful in tasks like Knowledge Graph\nlink prediction (Hosseini et al., 2019, 2021) and\nquestion answering from text (Lewis and Steedman,\n2013; McKenna et al., 2021). EG learning is unsu-\npervised: building them only requires a parser and\nentity linker for a new language domain (Li et al.,\n552\n2022b). EGs are relatively very data- and compute-\nefficient, requiring less than two days to train on\n2GB of unlabeled text using a single GPU (Hos-\nseini et al., 2021). Further, EGs are editable and\nalso explainable, because decisions can be traced\nback to distinct sentences on a task.\nHowever, EGs suffer from two kinds of sparsity.\nOne is edge sparsity, when two predicates are not\nobserved with co-occurring entities, so cannot be\nconnected together. Recent work improves on EG\nconnectivity (Berant et al., 2015; Hosseini, 2021;\nChen et al., 2022) but to our knowledge we are\nthe first to acknowledge vertex sparsity, arising\nwhen a predicate is not seen at all in training. EGs\nare structures of symbols, so they cannot handle\nmissing queries: in an inference task, if either the\npremise or hypothesis predicate is not in training,\nno entailment edge can be learned. In fact, many\nEG demonstrations achieve just 50% of task recall.\nPredicates occur in a Zipfian frequency distribu-\ntion with an unbounded tail of rare predicates, so\nit’s impractical to scale up the learning of predi-\ncate symbols by reading larger corpora. There will\nvirtually always be predicates missing at test-time.\nModern Language Models combine representa-\ntions of subword tokens to solve a similar issue\n(Peters et al., 2018; Devlin et al., 2019), and recent\nscaling of LMs has lead to breakthrough perfor-\nmance on many tasks (Hoffmann et al., 2022; Wei\net al., 2022), offering relief to sparsity problems via\ntechniques like in-context learning (Brown et al.,\n2020). However, as LMs scale in size and compute\nthey bring new problems: they require balloon-\ning GPU resources to train or run; or are costly\nto query via API; and centralizing models under\nprivate companies opens challenges of data privacy.\nWe are thus motivated to research lower-compute\nand more data-efficient methods which run on the\nscale of a single GPU.\nWe are the first to define vertex sparsity and ap-\nproach the problem by applying a small, pretrained\nLM to improve an existing EG using the benefits of\nmodern embeddings. We offer four contributions:\n(1) A theory for optimal smoothing of symbolic\ninference models such as EGs by constructing tran-\nsitive chains, accounting for a distinction between\npremise and hypothesis.\n(2) A low-compute method for unsupervised\nsmoothing of EG vertices using LM embeddings to\nfind approximations of missing predicates (see Fig-\nure 1). Applied to premises, we improve recall by\n16.3 and 25.1 percentage points on Levy/Holt and\nANT entailment datasets while raising precision.\n(3) On a QA task we show LM premise smooth-\ning is most helpful when there is less supporting\ncontext and missing a predicate is more costly.\n(4) Finally, in controlled experiments with Word-\nNet relations we confirm the behavior of the LM\nfor premise smoothing and show that hypothesis\nsmoothing is possible, but more difficult.\n2 Background\nResearch on unsupervised Entailment Graph induc-\ntion has mainly oriented toward edges: overcoming\nedge sparsity using graph properties like transitiv-\nity (Berant et al., 2015; Hosseini et al., 2018; Chen\net al., 2022), incorporating contextual or extralin-\nguistic information to improve edge precision (Hos-\nseini et al., 2021; Guillou et al., 2020), and research\ninto the underlying theory of the Distributional\nInclusion Hypothesis (Kartsaklis and Sadrzadeh,\n2016; McKenna et al., 2021). However, none of\nthese address vertex sparsity.\nWe leverage sub-symbolic encoding by an LM\nusing WordPieces (Devlin et al., 2019) in this work\nas a means of smoothing, to generalize beyond a\nfixed vocabulary of predicates. Our most direct\ncomparison is with Schmitt and Schütze (2021)\nwho apply contemporary prompting techniques\nwith the computationally tractable RoBERTa (Liu\net al., 2019) to learn open-domain predicate en-\ntailment. They finetune on premise-hypothesis\npairs and labels from the development split of the\nLevy/Holt NLI dataset (Holt, 2018), used in our ex-\nperiments. They use templates like “[hypothesis],\nbecause [premise]” which are encoded by the LM,\nthen classified true/false. They report high scores\non datasets, but Li et al. (2022a) have shown that de-\nspite excelling at paraphrase detection, rather than\nlearning directional inference (e.g. BUY ⊨ OWN\nand OWN ⊭ BUY ), this technique picks up dataset\nartifacts spuriously correlated with the labels in\nLevy/Holt. In contrast, our approach combines the\nstrengths of each: open-domain encoding using a\ncomputationally tractable LM with the directional\ninference capability of an EG.\n3 Theory of Smoothing\nWe first present a theory for optimal smoothing of\na symbolic EG which overcomes the problem of\nvertex sparsity. We definesmoothing as the approx-\nimation of missing predicates using those in the\n553\nexisting predicate vocabulary, in reference to ear-\nlier work in smoothing n-gram Language Models\n(Chen and Goodman, 1996). We next discuss the\ntheoretical intuition behind applying an LM as an\nopen-domain smoother.\n3.1 Directionality by Transitive Chaining\nWe argue that it is most important when modifying\nEG predictions by smoothing to maintain the EG’s\nstrong directional inference capability. Our theory\nmaintains directionality by constructing transitive\nchains, importantly distinguishing the role of the\nproposition as premise or hypothesis. We distin-\nguish ways to P-smooth premises and H-smooth\nhypotheses.\nWe start with a query entailment relation Q :\np ⊨ h, with unknown truth value to be verified\nby a model which is missingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissingmissing entries for at least\np or h. We specify smoothing as the process of\ngenerating a new relation Qs suitable for the model\nby identifying a replacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacementreplacement predicate p′ and/or h′\nwithin the model’s vocabulary. We claim that to\nmaintain directional precision, this must be done\nby identifying a p′ (or h′) related to p (or h) such\nthat a transitive chain is constructed, as in the cases\nbelow. By this transitivity, confirmation of Qs is\nleveraged to confirm Q.\n1. Generalize Missing P. Identify a more general\npremise p′ in the EG such that p ⊨ p′. This\nyields a new Qs : p′ ⊨ h.\n(Q) “ a obliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliteratedobliterated b” ⊨ “a played b”\n⊨\n(Qs) “ a beatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeatbeat b” ⊨ “a played b”\np ⊨ p′ is known, so if the EG confirms p′ ⊨ h,\nthen p ⊨ h is confirmed by transitivity.\n2. Specialize Missing H. Identify a more special-\nized hypothesis h′ in the EG such that h′ ⊨ h.\nThis yields a new Qs : p ⊨ h′.\n(Q) “ a bought b” ⊨ “a shopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped forshopped for b”\n⊨\n(Qs) “ a bought b” ⊨ “a paid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid forpaid for b”\nIf the EG confirms p ⊨ h′, then also knowing\nh′ ⊨ h confirms p ⊨ h by transitivity.\n3. Generalize P and Specialize H.If missing both\np and h, combine methods: identify new p′ and\nh′ as above, yielding a new Qs : p′ ⊨ h′.\nKnowing p ⊨ p′ and h′ ⊨ h, if a model confirms\np′ ⊨ h′, then p ⊨ h is confirmed by transitivity.\nOf course, the success of this smoothing depends\non being able to find p′ such that p ⊨ p′, and h′\nsuch that h′ ⊨ h. However, when an additional\ninference is found, it is likely to be correct, aiding\nmodel precision. By definition we cannot use the\nEG for this, and we turn to Language Models to\nidentify replacement predicates.\n3.2 LM Embeddings and Specificity\nWe assume that p′ and h′ are respectively among\nthe nearest neighbors of p and h in the embed-\nding space of the LM, and in this paper propose\na method to leverage LM embeddings in an unsu-\npervised way to find them. As defined later in §4,\nwe first embed all EG predicates, then at test-time\nwe embed the target query predicate and search\nfor the K nearest neighbors to the target in embed-\nding space. We predict that doing so for a premise\npredicate will build a transitive chain satisfying the\nconditions of §3.1. We identify two factors which,\ncombined, lead to predictions that are likely more\nsemantically general than the target, which enables\nP-smoothing, but not H-smoothing:\n(A) The LM training objective. Li et al. (2020)\nshow that the masked language modeling objective\nin BERT induces a particular structure in its la-\ntent embedding space: on average, corpus-frequent\nwords are embedded near the origin and infrequent\nones further out. This is because of statistical\nlearning, which biases LMs toward high frequency\nwords since they are trained on a corpus to pre-\ndict the most probable tokens. This objective leads\nLSTM-based LMs to produce a beneficially Zip-\nfian frequency distribution of words (Takahashi and\nTanaka-Ishii, 2017), and similar biases are evident\nin Transformers for generation like GPT-2 and XL-\nNet (Shwartz and Choi, 2020).\n(B) The natural anti-correlation of word fre-\nquency with specificity in text. Probabilistically,\nthe more frequent a word, the lower its “seman-\ntic content” (in other words, the less specific it is).\nCaraballo and Charniak (1999) show this for nouns,\nand this assumption is even used in the “IDF” com-\nponent of TF-IDF (Spärck Jones, 1972).\nThese factors imply that embedding a vocabu-\nlary of EG predicates using an LM will result in\na space densely populated toward the origin by\ncorpus-frequent predicates. KNN-search starting\nfrom a target predicate embedding will likely return\nneighbors toward this dense origin, thus selecting\nmore corpus-frequent, semantically general words.\n554\nWe illustrate further in §3.3.\nThis effect has even been studied elsewhere: in\nMachine Translation, frequency bias causes a quan-\ntified semantic generalizing effect from transla-\ntion input to output (Vanmassenhove et al., 2021),\ndubbed “Machine Translationese” due to the artifi-\ncially non-specific tone.\n3.3 The Specificity Taxonomy\nTo help show the relation between frequency and\ngenerality and characterize the source of vertex\nsparsity, we illustrate a hierarchical taxonomy of\npredicates ordered by specificity, following from\nthe theories of natural categories and prototype\ninstances (Rosch and Mervis, 1975; Rosch et al.,\n1976). We place very general predicate categories\nat the top of this taxonomy such as “act” and\n“move,” with concrete subcategories beneath, and\nhighly specific ones at the bottom, like “innoculate”\nand “perambulate.” Rosch et al define their middle\n“basic level categories” for nouns, containing ev-\neryday concepts like “dog” and “table,” which are\nlearned early by humans and are used most com-\nmonly among all categories, even by adults (Mervis\net al., 1976). We assume an analogous basic level\nin a predicate taxonomy, too, in Figure 2.\nFigure 2: The specificity taxonomy. The basic level\ncontains “everyday” predicates. Above becomes more\ngeneral, and below becomes more concrete and specific.\nUsage frequency decreases away from the basic level.\nThere are few general categories at the top and\nmany specific ones at the bottom (e.g., consider\nthe many ways to “move,” e.g. “walk,” “sprint,”\n“lunge”). However, since basic level categories are\nthe most frequently used, moving either up or down\nin the taxonomy accompanies a decrease in usage\nfrequency. Above the basic level, predicates are\nfewer and more abstract, and can be infelicitous in\ndaily use (e.g. calling a cat a “mammal” in Rosch’s\ncase or predicates like “actuate” in ours). Below,\npredicates are highly specialized for specific con-\ntexts, so there are many more of them, and they are\nlower-frequency (e.g. “elongate,” “defenestrate”).\nThis is a major source of vertex sparsity.\nThis asymmetry encourages P-smoothing us-\ning an LM (and foreshadows its failure at H-\nsmoothing). A predicate z is likely to be missing\nfrom an EG if it is corpus-infrequent, thus likely\nspecific. Randomly sampling another EG predicate\nz′ neighboring z in embedding space, but sampled\nproportional to observed frequencies, is likely to\nreturn a predicate of higher frequency, toward the\nbasic level, which is usually higher in the specificity\ntaxonomy. Thus given z, a frequency-proportional\nsample z′ is likely to be more general thanz, usable\nfor P-smoothing to construct a transitive chain.\n4 Experimental Methods\nIn this work we consider Entailment Graphs of\ntyped binary predicates. An EG is defined as G =\n(V, E), consisting of a set of vertices V of natural\nlanguage predicates (with argument types in the set\nT ), and directed edges E indicating entailments.\nBinary predicates in V have two argument slots\nlabeled with their types. For example, the pred-\nicate TRAVEL .TO(:person, :location) ∈ V , and\nthe types :person, :location ∈ T. An exam-\nple entailment is TRAVEL .TO(:person, :location)\n⊨ ARRIVE .AT(:person, :location) ∈ E.\nOur smoothing method may be applied to any\nexisting EG. In this work we show the comple-\nmentary benefits of vertex-smoothing with existing\nmethods in improving edge sparsity by comparing\ntwo related baseline models, described in §5. These\nEGs are learned from the same set of vertices, but\nare constructed differently so have different edges.\nThe FIGER type system is used for these experi-\nments (Ling and Weld, 2012), where|T |= 49, and\nthese models typically have up to|T |2 = 492 typed\nsubgraphs g ∈ G. Typing disambiguates senses of\nthe same predicate, which improves precision of\ninferences, an observation in NLP tracing back to\nYarowsky (1993). For example, RUN(:person, :or-\nganization) which is learned in the typed subgraph\ng(person-organization) has a different meaning and en-\ntailments than RUN(:person, :software).\n4.1 Nearest Neighbors Search\nOur method assumes that existing EGs contain\nenough predicates already present in the graph to\nenable discovery of suitable replacements for an\n555\nx : (join.1,join.2)#person#organization\n⇒“person join organization”\nx : (give.2,give.to.2)#medicine#person\n⇒“give medicine to person”\nx : (export.1,export.to.2)#location_1#location_2\n⇒“location_1 export to location_2”\nTable 1: A typed predicate x is converted to a sentence\n(shown) and encoded with an LM by L(x). The final\noutput is the average overpredicate WordPiece vectors.\nunseen target predicate, using an LM. For exam-\nple, in the sports domain, the EG may be missing\na rare predicate OBLITERATE but contain similar\npredicates BEAT and DEFEAT which can be found\nas close neighbors in Language Model embedding\nspace. These nearby predicates are expected to\nhave similar semantics (and entailments) to the\nunseen target predicate, and will thus be suitable\nreplacements. See Figure 1 for an illustration.\nWe define the smoothed retrieval function S,\nwhich replaces the typical method for retrieving\na target predicate vertex x from a typed subgraph\ng(t) = (V (t), E(t)), with typing t ∈ {T × T }.\nAhead of test-time, for each typed subgraph g(t)\nwe encode the EG predicate vertices V (t) as a ma-\ntrix V(t). For each predicatev(t)\ni ∈ V (t), we encode\nv(t)\ni = L(v(t)\ni ), a row vector v(t)\ni ∈ V(t).\nAt test-time we encode a corresponding vector\nfor the target predicate x, x = L(x). Then S re-\ntrieves the K-nearest neighbors of x in g(t):\nS(x, g(t), K) =\n{v(t)\ni | v(t)\ni ∈ V (t), if v(t)\ni ∈ KNN(x, V(t), K)}\nL(·) is a function which encodes a typed natural\nlanguage predicate using a pretrained LM. First,\na short sentence is constructed from the predicate\nusing the types as generic arguments, and then\nthe sentence is encoded by the LM (see Table 1\nfor examples). We extract the representations of\nWordPieces corresponding to the predicate, and\naverage them into the resulting predicate vector.\nIn our experiments we use RoBERTa (Liu et al.,\n2019) for encoding, a well-tested, off-the-shelf LM\nof tractable size for running on a single GPU, which\nhas pretrained on 160GB of unlabeled text.\nFor the KNN search metric we use Euclidean\nDistance (L 2 norm) from the target vector x to\nvectors in V(t). We precompute a BallTree using\nscikit-learn (Pedregosa et al., 2011) which spatially\n“The audience applauded the comedian” ⊨ “The audi-\nence observed the comedian”\n“The audience observed the comedian”⊭ “The audience\napplauded the comedian”\n“The laptop satisfied the criteria” ⊨ “The laptop was\nassessed against the criteria”\n“The laptop was assessed against the criteria” ⊭ “The\nlaptop satisfied the criteria”\nTable 2: Example queries, ANT (dev) directional subset.\norganizes the EG vectors to speed up search from\nlinear in the number of vertices |V (t)| to log |V (t)|.\n4.2 Datasets\nWe demonstrate our smoothing method on two ex-\nplicitly directional datasets, which test both direc-\ntions of predicate inference, creating a 50% posi-\ntive/50% negative class balance.\nLevy/Holt. This dataset (Holt, 2018; Levy and\nDagan, 2016) has been explored thoroughly in pre-\nvious work (Hosseini, 2021; Guillou et al., 2021; Li\net al., 2022b; Chen et al., 2022). Importantly, it in-\ncludes inverses for all queries, allowing systematic\ninvestigation of directionality, although it contains\na high proportion of paraphrases and selection bias\nartifacts that can be picked up by finetuning in\nsupervised models (Li et al., 2022a). We test on\nthe 1,784 questions forming the purely directional\nsubset, which is more challenging.\nANT. This is a new, high-quality dataset improv-\ning on Levy/Holt, which tests predicate entailment\nin the general domain (Guillou and Bijl de Vroe,\n2023). It was created by expert annotation of entail-\nment relations between clusters of predicate para-\nphrases, expanded automatically using WordNet\nand other dictionary resources into thousands of\ntest questions of the format “given [premise], is [hy-\npothesis] true?” We test on the directional subset\nof 2,930 questions.\nSee Table 2 for dataset examples. Each comes\npreprocessed with argument types from CoreNLP\n(Manning et al., 2014; Finkel et al., 2005), roughly\naligning with EG FIGER types. We use the MoN-\nTEE system (Bijl de Vroe et al., 2021) to ex-\ntract CCG-parsed and typed predicate relations (x)\nshown in Table 1, which are used as queries to\nEntailment Graphs.\n4.3 Models\nWe smooth two recent Entailment Graphs which\npreviously scored highly amongst unsupervised\nmodels on the full Levy/Holt dataset. Importantly,\n556\nFigure 3: Experiment 1: Smoothing the CTX EG with\nan LM on the ANT dataset. P-smoothing improves\nrecall and precision, whereas H-smoothing is detrimen-\ntal. We try different K ∈ {2, 3, 4} and show the best\nKprem = 4and Khyp = 2.\nthey are constructed from the same set of predicate\nvertices but have different edges, so we can observe\nhow vertex- and edge-improvements combine.\nGBL. The EG of Hosseini et al. (2018), which\nintroduces a “globalizing” graph-based method to\nimprove the edges after “local” EG learning.\nCTX. The state-of-the-art contextualized EG of\nHosseini et al. (2021), which improves over GBL\nedges by augmenting local learning with a contex-\ntual link-prediction objective, before globalizing.\nGBL-P / GBL-H and CTX-P / CTX-H. We ap-\nply an LM separately for both P- and H-smoothing\non GBL and CTX. As described earlier, we use\nthe RoBERTa LM (Liu et al., 2019) to produce\nembeddings for smoothing the EG.\nS&S. The finetuned RoBERTa model of Schmitt\nand Schütze (2021) (discussed in §2). We insert\neach premise/hypothesis pair into their 5 prompt\ntemplates, and take the maximum entailment score\nas the model prediction for the pair. Li et al. (2022a)\nfind that this model has overfit to artifacts present\nin Levy/Holt, so we compare with it on a different\nquestion answering task in §6.\n5 Experiment 1: Entailment Detection\nWe run two experiments on both Levy/Holt and\nANT. (1) We apply our unsupervised smoothing\nto augment the Premise of each test entailment,\ngenerating K new target premise predicates. Sepa-\nrately, (2) we smooth the Hypothesis of each test\nentailment the same way. For both we try different\nvalues of the hyperparameter K ∈ {2, 3, 4}.\nPlots for model performances are shown in Fig-\nure 3, in which we compare P-smoothing vs. H-\nsmoothing of the CTX graph using Kprem = 4and\nKhyp = 2, chosen for producing the best AUC n\nANT Levy/Holt\nModel AUCn AP AUC n AP\nGBL 3.79 58.36 3.01 55.82\nGBL-PK=4 13.91 64.71 9.95 60.70\nGBL-HK=2 1.41 52.57 1.09 52.05\nCTX 15.44 65.66 9.40 60.19\nCTX-PK=4 25.86 67.47 13.45 60.80\nCTX-HK=2 9.94 58.52 8.33 57.97\nTable 3: Experiment 1: P- and H-smoothing, compared\nto unsmoothed models. P-Smoothing with an LM im-\nproves AUCnorm (AUCn) and Average Precision (AP)\nin both CTX and GBL models.\n(see Appendix A for all results). In Appendix B\nwe also show P-smoothing in particular of the CTX\ngraph vs. the GBL graph. For all models (best K\nselected) on both datasets we show summary statis-\ntics in Table 3, including normalized area under\nthe precision-recall curve (AUCn) and average pre-\ncision (AP) across the recall range. A sample of\nmodel outputs is shown in Table 4.\nLi et al. (2022a) introduce AUC n, a fair way\nto compare models which may achieve different\nmaximum recalls. It computes only the area under\nthe precision-recall curve above the random-guess\nbaseline for the dataset, so it is highly discerning\ncompared to AUC, which can inflate performance\nwhen there is a high random baseline. In our case,\nthe high 50% random baseline means that AUCn\nscores are systematically much smaller than AUC.\nAs predicted, our method of selecting nearest-\nneighbors of a target predicate in an EG using their\nLM embedding distance has different behavior for\nP-smoothing than H-smoothing. We observe that\nP-smoothing with an LM is very beneficial to both\nthe recall and precision of both Entailment Graphs\nit is applied to, with a slight advantage in AUC n\nto higher values of K. When applied to the SOTA\nmodel CTX on the ANT dataset, our smoothing\nmethod increases maximum recall by 25.1 absolute\npercentage points (pp) to 74.3% while increasing\naverage precision from 65.66% to 67.47%. On\nLevy/Holt we increase maximum recall by 16.3\nabsolute pp to 62.7% while slightly raising average\nprecision. However, H-smoothing with the LM is\nhighly detrimental: despite improving recall, av-\nerage precision on ANT is cut to 58.52%, and the\nlowest confidence predictions are at random chance\n(50% precision).\nWe also note that P-smoothing greatly improves\nrecall and precision when applied to both GBL\n557\nPredicate Missing\nfrom EG\nNearest Neighbors\nby Embedding Dist.\nDISCREDIT (:person, :thing) PROBE , ACCUSE\nCRACK .UP.AT(:person,\n:written_work)\nMAKE .JOKE .AT,\nYELL .AT\nMINIMIZE (:organization, :thing) SOFTEN , EVADE\nREBUKE (:person, :person) OPPOSE , REMIND\nTable 4: Experiment 1: Sample of CTX outputs on ANT.\nA target PREDICATE (type1, type2) that is missing from\nCTX is closest in LM embedding space to K=2 CTX\npredicates, which are more semantically general.\nand CTX graphs. This shows the complementary\nnature of improving vertex sparsity with improving\nedge sparsity in EGs: these techniques improve\ndifferent aspects, which can be applied together.\nSince effects are similar for both EGs, from now on\nwe show results only for CTX, and report additional\nresults for the weaker GBL in Appendix B.\n6 Experiment 2: Question Answering\nWe now experiment with LM smoothing in appli-\ncation on an applied task. We test on the Boolean\nOpen QA task, BoOQA (Li et al., 2022a), in which\nmodels answer true/false questions about entities\nmentioned in news articles from multiple sources.\nBoOQA questions are chosen to be adversarial to\nsimple similarity baselines, and EGs have proven\nuseful by using directional reasoning.\n6.1 Boolean Open-Domain QA\nBoOQA is a task over open domain news articles,\nwith questions formed by extracting triples of (en-\ntity, relation, entity), in the format “is it true that\n<triple>?” Context statements are other triples\nsourced from the articles concerning the same ques-\ntion entities, and the task is to compare each context\nstatement with the question itself. If any context\nstatement entails the question by means of its rela-\ntion, the question can be labeled “true,” otherwise\n“false.” BoOQA also contains false questions de-\nrived from true ones, so models must decide care-\nfully what is supported by evidence and what isn’t.\nWe address vertex sparsity in a natural setting,\nso we relax the original entity restriction of Li\net al. (2022a): instead of sampling questions about\nfrequently-mentioned entities (which always have\nmany context statements to decide from), we in-\ncrease the challenge by sampling from the natural\ndistribution of entities, regardless of popularity.\nContext Size CTX CTX-P CTX-H S&S\n[2, 5) 20.05 20.66 19.07 17.00\n[5, 10) 29.13 29.17 29.01 23.05\n[10, 15) 32.32 32.31 32.25 24.98\n15+ 36.58 36.57 36.51 26.13\nAll Questions 21.26 21.74 20.64 16.99\nTable 5: Experiment 2: Effect of P- and H-smoothing\nvs. baseline CTX and S&S across context sizes (AUCn\nis reported). P-smoothing is useful on CTX when fewer\ncontext statements are available.\n6.2 Results Across Context Sizes\nResults corroborate the earlier tests: P-smoothing\nimproves AUCn from 21.26% to 21.74% over all\nquestions, while H-smoothing worsens to 20.64%\n(as in §4, AUCn is systematically lower than AUC).\nWe also outperform Schmitt and Schütze (2021),\nour most direct competition which uses a tractable-\nsize LM. Despite facility to encode any predicate,\nit lacks directional precision useful for this task.\nTo demonstrate when smoothing an EG is help-\nful, we further analyze the effect on different con-\ntext size bands. For each question, we count the\nnumber of context sentences available to answer it;\nquestions are bucketed into bands of [2, 5), [5, 10),\n[10, 15), 15+. From the overall dataset we sample\napproximately 55,000 questions per context size\nband (see Appendix C for exact counts). On each\nband we compare an unsmoothed model with P-\nsmoothing and H-smoothing, and we report results\nin Table 5.\nThe benefit of P-smoothing is greatest in the\nlowest band f <5, and diminishes in higher bands.\nThis is because in the lower bands there are fewer\ncontext statements which may be used to answer\nthe question, increasing difficulty. Here the EGs are\nmore prone to sparsity, because missing even a few\ncontext predicates devastates its chance to answer\nthe question. In fact, the proportion of questions\nfor which all context relations are missing from the\nEG is 1.5% for f >15, but 32.7% for f <5.\n7 Experiment 3: P and H with WordNet\nLM P-smoothing works well, but not H-smoothing.\nWe now show controlled experiments using Word-\nNet relations (Fellbaum, 1998) to confirm this is\ndue to semantic generalization (in line with our the-\nory in §3.1). We show by constructing a transitive\nchain using WordNet hyponyms that Hypothesis\nsmoothing is possible in principle, without claim-\ning that it provides a practical alternative to an LM.\n558\nFigure 4: Experiment 3: Comparing WordNet relations used in smoothing P(remise), H(ypothesis), and P+H, with\nCTX graphs on the ANT dataset. Hypernyms are useful for P-smoothing, and hyponyms less so for H-smoothing.\n7.1 Controlled Search with WordNet\nWe re-run the §4 experiment by smoothing the CTX\nmodel on the ANT dataset (GBL in Appendix B).\nHowever, the target premise or hypothesis is now\napproximated without the LM. Instead, we generate\nreplacements using two WordNet relations.2\nIn this test, we choose specific WordNet lexical\nrelations as instances of entailment, then generate\nsmoothing predictions from the WN database. The\nhyponymy relation is used for specialization and\nhypernymy for generalization, and these are com-\npared for both P- and H-smoothing.\nTo illustrate, if smoothing by specializing,\ngiven a predicate “receive from,” we retrieve\nWN hypernyms like “inherit from.” We do\nthis by querying WN for relations of the pred-\nicate head word. We use results from the first\nword sense to replace the query word. E.g.,\nfrom (receive.2,receive.from.2) the WN\nquery hyponym(“receive”) ⇒ “inherit” generates\n(inherit.2,inherit.from.2).\n7.2 Results\nResults are shown in Figure 4. Importantly, from\nthese plots a switch in performance is observed be-\ntween the application of hypernyms and hyponyms\nwhen used for P- and H-smoothing on CTX (sim-\nilar results for GBL, see Appendix B). It is clear\nthat generalizing the premise using hypernyms is\nhighly effective in terms of recall and precision, but\nspecializing with hyponyms is extremely damaging\nto precision. For the hypothesis, the reverse is true:\ngeneralizing with hypernyms worsens performance,\nbut specializing with hyponyms can lead to some\nperformance gains (when used with P-smoothing,\n2WN was partly used in ANT’s construction, so this result\nexplains the LM effect, rather than offering a practical model.\nsee discussion below). We also tested Levy/Holt\nand see a similar trend.\nThese results nearly replicate the behavior of the\nLM-smoother in §4, verifying that nearest neighbor\nsearch in LM embedding space has a semantically\ngeneralizing effect suitable for P-smoothing. Ta-\nble 4 shows examples of generalized predictions.\nFinally, we note P-smoothing with WordNet per-\nforms similarly to the LM in this “laboratory” set-\nting (see Appendix D), but an LM smoother is still\npreferable due to being fully automatic and open-\ndomain, handling new words, misspellings, etc.\n7.3 Discussion\nWe note two phenomena of interest. (1) For both\nCTX and GBL, performance is boosted in the low-\nrecall/high-precision range when using both opti-\nmal smoothers (Phyper + Hhypo), higher than us-\ning either smoother individually. (2) Additionally,\nHhypo is the better H smoother tested, though it\nappears unreliable on its own withoutP smoothing:\nHhypo is not useful for smoothing CTX, but it does\nimprove the weaker GBL, see Appendix B.\nBoth of these phenomena are likely related to\ndata frequency. Generalized hypernyms such as\nBEAT and USE are quite common in training data,\nand therefore have more learned edges in the EG\nwith high quality edge weights. However, special-\nized hyponyms like ELONGATE can be extremely\nsparse in training data, leading to poorer learned\nrepresentations and fewer edges. Phenomenon (1)\nshows that using a frequently-occurring smoothed\npremise of high quality yields better odds of finding\nan edge to a smoothed hypothesis, leading to some\nperformance gains over either smoother individu-\nally. Phenomenon (2) suggests that H-smoothing\nmay be naturally more difficult than P-smoothing,\nand less stable due to sparsity of hyponyms (spe-\n559\ncializations) in corpora. If a hypothesish is missing\nfrom the EG (meaning it wasn’t seen in training)\nthen deriving a candidate for replacement h′ spe-\ncialized from h will also be unlikely to occur in\ntraining, thus even if found in the EG it may have\nfew or poorly learned edges. Though it can be\nbeneficial to precision, natural data sparsity makes\nH-smoothing fundamentally harder.\n8 Conclusion\nWe introduce a theory for optimal smoothing of\na symbolic model of language inference like an\nEntailment Graph, which solves the problem of\nvertex sparsity in EGs by constructing transitive in-\nference chains. Further, we show an unsupervised,\nopen-domain method of P-smoothing by approx-\nimating premises missing from an EG using Lan-\nguage Model embeddings, which improves both\nrecall and precision on two difficult directional en-\ntailment datasets. We also test the method on a\nQA task, where we show the most benefit in diffi-\ncult scenarios where limited context information is\navailable. Our method is low-compute, combining\nan existing EG with a pretrained LM of tractable\nsize for a single GPU, and it improves over two low-\ncompute baselines: a SOTA EG and a finetuned\nRoBERTa-based prompting model.\nWe also demonstrate our theory of optimal\nsmoothing by directing the search for predictions\nusing WordNet relations, without an LM. Our ex-\nperiments replicate the behavior of the LM-based\nsmoother, offering an explanation for why LM em-\nbeddings are useful for P-smoothing, but not H-\nsmoothing, in terms of the semantic generalizing\neffect when searching a neighborhood in embed-\nding space.\nLimitations\nIn this work we present a simple “graph smoothing”\nmethod which leverages the natural structure in LM\nembedding space to find approximations of predi-\ncates missing from the EG, a major source of error.\nNearest neighbors search within LM embedding\nspace is biased toward returning predicates that are\nmore semantically general, which is helpful for\nP-smoothing.\nHowever, generalizing is detrimental to H-\nsmoothing, which requires specialization. While\nwe show a proof of specialization and empirical\nevidence using WordNet, solving H-smoothing in\nan open domain using an unsupervised model such\nas a Language Model is left open in this work. It\nis likely that H-smoothing is a more difficult task\nthan P-smoothing due to natural data sparsity as dis-\ncussed in the paper. If a hypothesis is missing from\nthe EG, it is likely to be a corpus-infrequent predi-\ncate, and specializing it will yield other predicates\nof low frequency, yielding poor odds of recovery.\nFurther, using a sub-symbolic LM encoder theo-\nretically enables inference using any premise pred-\nicate, but we are still restricted to choosing approx-\nimations from the predicate vocabulary of the EG.\nIf the vocabulary is not suitable e.g. for a new tar-\nget genre/domain, Hosseini et al. (2021) show that\nEG learning may be scaled up easily, which may\nprovide a sufficiently scoped vocabulary for any\napplication, but exploration is left to future work.\nFinally, our work is demonstrated only on the\nEnglish language. However, we expect this method\nshould succeed with arbitrary natural languages. Li\net al. (2022b) demonstrate that learning Entailment\nGraphs in Chinese can be done using the same\nprocess as English, and our technique leverages a\nsimple fundamental property of Language Models\nstemming from the natural Zipfian distribution of\npredicates in corpora, across languages.\nEthical Considerations\nThis work is designed to extend the capabilities\nof Entailment Graphs, which are general-purpose\nstructures of meaning postulates. These can be\napplied most readily to question answering appli-\ncations, but they can also be used for other NLU or\nNLI tasks. As an unsupervised, corpus-based learn-\ning algorithm, we believe that EGs could be sus-\nceptible to learning biases in human beliefs present\nin corpora, but this algorithm is most sensitive to\nwidely repeated statements, which may be easier to\ndetect in data cleaning than uncommon statements.\nWe believe there is no immediate risk in basic ques-\ntion answering when using EGs that are trained\non published news articles, as shown in this work,\nbecause the training data is professionally edited to\na standard. However, models for general language\nunderstanding like an EG may be used for many\npurposes beyond this.\nAcknowledgements\nThis research was supported by ERC Advanced Fel-\nlowship GA 742137 SEMANTAX, the University\nof Edinburgh Huawei Laboratory, and a Google\nFaculty Award.\n560\nReferences\nJonathan Berant, Noga Alon, Ido Dagan, and Jacob\nGoldberger. 2015. Efficient global learning of entail-\nment graphs. Computational Linguistics, 41(2):221–\n263.\nSander Bijl de Vroe, Liane Guillou, Miloš Stanojevi´c,\nNick McKenna, and Mark Steedman. 2021. Modality\nand negation in event extraction. In Proceedings of\nthe 4th Workshop on Challenges and Applications of\nAutomated Extraction of Socio-political Events from\nText (CASE 2021), pages 31–42, Online. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSharon A. Caraballo and Eugene Charniak. 1999. De-\ntermining the specificity of nouns from text. In 1999\nJoint SIGDAT Conference on Empirical Methods in\nNatural Language Processing and Very Large Cor-\npora.\nStanley F. Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In 34th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 310–318,\nSanta Cruz, California, USA. Association for Com-\nputational Linguistics.\nZhibin Chen, Yansong Feng, and Dongyan Zhao. 2022.\nEntailment graph learning with textual entailment\nand soft transitivity. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 5899–\n5910, Dublin, Ireland. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nChristiane Fellbaum. 1998. WordNet: An Electronic\nLexical Database. Bradford Books.\nJenny Rose Finkel, Trond Grenager, and Christopher\nManning. 2005. Incorporating non-local information\ninto information extraction systems by gibbs sam-\npling. In Proceedings of the 43rd Annual Meeting on\nAssociation for Computational Linguistics, ACL ’05,\npage 363–370, USA. Association for Computational\nLinguistics.\nMaayan Geffet and Ido Dagan. 2005. The distribu-\ntional inclusion hypotheses and lexical entailment.\nIn Proceedings of the 43rd Annual Meeting of the\nAssociation for Computational Linguistics (ACL’05),\npages 107–114, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nLiane Guillou and Sander Bijl de Vroe. 2023. Ant\ndataset.\nLiane Guillou, Sander Bijl de Vroe, Mohammad Javad\nHosseini, Mark Johnson, and Mark Steedman. 2020.\nIncorporating temporal information in entailment\ngraph mining. In Proceedings of the Graph-\nbased Methods for Natural Language Processing\n(TextGraphs), pages 60–71, Barcelona, Spain (On-\nline). Association for Computational Linguistics.\nLiane Guillou, Sander Bijl de Vroe, Mark Johnson, and\nMark Steedman. 2021. Blindness to modality helps\nentailment graph mining. In Proceedings of the Sec-\nond Workshop on Insights from Negative Results in\nNLP, pages 110–116, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, Thomas Hennigan, Eric\nNoland, Katherine Millican, George van den Driess-\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKarén Simonyan, Erich Elsen, Oriol Vinyals, Jack\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn Advances in Neural Information Processing Sys-\ntems, volume 35, pages 30016–30030. Curran Asso-\nciates, Inc.\nXavier Holt. 2018. Probabilistic models of relational\nimplication. Master’s thesis, Macquarie University.\nMohammad Javad Hosseini. 2021. Unsupervised Learn-\ning of Relational Entailment Graphs from Text. Ph.D.\nthesis, University of Edinburgh.\nMohammad Javad Hosseini, Nathanael Chambers, Siva\nReddy, Xavier R. Holt, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2018. Learning typed en-\ntailment graphs with global soft constraints. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:703–717.\nMohammad Javad Hosseini, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2019. Duality of link pre-\ndiction and entailment graph induction. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4736–4746, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\n561\nMohammad Javad Hosseini, Shay B. Cohen, Mark John-\nson, and Mark Steedman. 2021. Open-domain con-\ntextual link prediction and its complementarity with\nentailment graphs. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n2790–2802, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nDimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2016.\nDistributional inclusion hypothesis for tensor-based\ncomposition. In Proceedings of COLING 2016, the\n26th International Conference on Computational Lin-\nguistics: Technical Papers, pages 2849–2860, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nOmer Levy and Ido Dagan. 2016. Annotating rela-\ntion inference in context via question answering. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 249–255, Berlin, Germany. As-\nsociation for Computational Linguistics.\nMike Lewis and Mark Steedman. 2013. Combined\ndistributional and logical semantics. Transactions of\nthe Association for Computational Linguistics, 1:179–\n192.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nTianyi Li, Mohammad Javad Hosseini, Sabine Weber,\nand Mark Steedman. 2022a. Language models are\npoor learners of directional inference. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2022, pages 903–921, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nTianyi Li, Sabine Weber, Mohammad Javad Hosseini,\nLiane Guillou, and Mark Steedman. 2022b. Cross-\nlingual inference with a Chinese entailment graph.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 1214–1233, Dublin,\nIreland. Association for Computational Linguistics.\nXiao Ling and Daniel S. Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of the Twenty-Sixth\nAAAI Conference on Artificial Intelligence, AAAI’12,\npage 94–100. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Association for Compu-\ntational Linguistics (ACL) System Demonstrations,\npages 55–60.\nNick McKenna, Liane Guillou, Mohammad Javad Hos-\nseini, Sander Bijl de Vroe, Mark Johnson, and Mark\nSteedman. 2021. Multivalent entailment graphs for\nquestion answering. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 10758–10768, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nCarolyn B Mervis, Jack Catlin, and Eleanor Rosch.\n1976. Relationships among goodness-of-example,\ncategory norms, and word frequency. Bulletin of the\npsychonomic society, 7(3):283–284.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nEleanor Rosch and Carolyn B Mervis. 1975. Family\nresemblances: Studies in the internal structure of\ncategories. Cognitive Psychology, 7(4):573–605.\nEleanor Rosch, Carolyn B Mervis, Wayne D Gray,\nDavid M Johnson, and Penny Boyes-Braem. 1976.\nBasic objects in natural categories. Cognitive Psy-\nchology, 8(3):382–439.\nMartin Schmitt and Hinrich Schütze. 2021. Language\nmodels for lexical inference in context. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 1267–1280, Online. Association\nfor Computational Linguistics.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6863–6870, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nKaren Spärck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of documentation.\nShuntaro Takahashi and Kumiko Tanaka-Ishii. 2017.\nDo neural nets learn statistical laws behind natural\nlanguage? PLOS ONE, 12(12):1–17.\nEva Vanmassenhove, Dimitar Shterionov, and Matthew\nGwilliam. 2021. Machine translationese: Effects of\nalgorithmic bias on linguistic complexity in machine\n562\ntranslation. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 2203–2213,\nOnline. Association for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nDavid Yarowsky. 1993. One sense per collocation.\nIn Human Language Technology: Proceedings of\na Workshop Held at Plainsboro, New Jersey, March\n21-24, 1993.\nA Hyperparameter Search\nIn §5 we test three values for hyperparameters\nKprem and Khyp, each from choices {2, 3, 4}. Fig-\nure 5 shows all smoothing combinations. We select\nKprem = 4and Khyp = 2in the main experiments\ndue to having the highest AUCn values for P- and\nH-smoothing, respectively. We highlight a few\ntrends. (1) higher Kprem appears better (most no-\ntably, Kprem = 4yields slightly better recall than\nKprem = 2), though it has diminishing returns.\n(2) lower Khyp is better, because H-smoothing us-\ning an LM is actively harmful (Khyp = 0, an un-\nsmoothed EG, would “perform” better in practice!).\nFigure 5: Experiment 1: LM smoothing on the ANT\ndataset. Comparison of P- and H-smoothing CTX\nwith different Kprem and Khyp from choices {2, 3, 4}.\nHigher values of K are shown more darkly.\nB The GBL Entailment Graph\nWe test the older GBL graph (Hosseini et al., 2018)\non the ANT dataset. Results confirm findings on\nthe newer CTX (Hosseini et al., 2021). Figure 6\nshows results for the experiment in §4 but compar-\ning P-smoothing with LM predictions for the CTX\nand GBL graphs. We note that base CTX performs\nmuch better than GBL, and that P-smoothing with\nan LM improves both GBL and CTX.\nFigure 7 shows results for the experiment in §7\nof smoothing an EG using WordNet relations, but\nwe now show smoothing the older GBL graph. We\nobserve similar results as with CTX: there is notice-\nable improvement over the base EG when smooth-\ning either premises with hypernyms, hypotheses\nwith hyponyms (stronger than when applied to\nCTX), or both combined.\nFigure 6: Experiment 1: LM smoothing on the ANT\ndataset. Comparison of P-smoothing GBL and CTX\nwith optimal K=4.\nC BoOQA Context Size Bands\nIn the QA task a model must try to draw an in-\nference from any context statement (premises) to\ninfer the validity of the question (hypothesis). Any\nmodel is less likely to find an entailment when there\nare few premises, but symbolic EGs are especially\nprone because missing premises means even fewer\nchances to find an entailment. From the original\ndataset, we sample approximately 55,000 questions\nfor each context size band, including 55,000 ques-\ntions from the natural distribution, with no con-\ntext limitation (“All Questions”). Sample sizes are\nshown in Table 6.\n[2, 5) 56,390\n[5, 10) 56,425\n[10, 15) 54,778\n15+ 54,926\nAll Questions 56,494\nTable 6: Experiment 2: Sample sizes for context bands\non the QA task.\n563\nFigure 7: Experiment 3: WordNet relations used to smooth P(remise), H(ypothesis), and P+H, with the Entailment\nGraph GBL on the ANT dataset. Hypernyms are useful for P-smoothing, and hyponyms for H-smoothing.\nD P-Smoothing: LM vs. WordNet\nIn Figure 8 we show a comparison of P-smoothing\nbetween the LM (CTX-PLM AUCn = 25.86) and\nWordNet (CTX-Phyper AUCn = 27.39) on the\nANT dataset. We note that although WordNet per-\nforms within about 1.5% of the LM smoother in\nthis “laboratory” experiment, we believe the LM-\nsmoother is preferable in use, because it is fully\nautomatic to learn and apply, and because it en-\ncodes an open domain of predicates, which may\ninclude new words, misspellings, etc. that WordNet\ncannot handle.\nFigure 8: Comparison of P-smoothing methods on ANT:\nLM-based smoother performs similarly to WordNet hy-\npernym relations on the Entailment Graph CTX.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6608352661132812
    },
    {
      "name": "Computational linguistics",
      "score": 0.6198940277099609
    },
    {
      "name": "Natural language processing",
      "score": 0.562773585319519
    },
    {
      "name": "Smoothing",
      "score": 0.5168845653533936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4598371386528015
    },
    {
      "name": "Joint (building)",
      "score": 0.4286084473133087
    },
    {
      "name": "Linguistics",
      "score": 0.4262532591819763
    },
    {
      "name": "Philosophy",
      "score": 0.11817744374275208
    },
    {
      "name": "Engineering",
      "score": 0.08655557036399841
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I99043593",
      "name": "Macquarie University",
      "country": "AU"
    }
  ],
  "cited_by": 4
}