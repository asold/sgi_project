{
    "title": "CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models",
    "url": "https://openalex.org/W4409348007",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5108815041",
            "name": "Dongfang Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5066749075",
            "name": "Zetian Sun",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5024039226",
            "name": "Xinshuo Hu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5083079672",
            "name": "Baotian Hu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5108856873",
            "name": "Min Zhang",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6746266179",
        "https://openalex.org/W6728634397",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6840426214",
        "https://openalex.org/W6779944756",
        "https://openalex.org/W3017160304",
        "https://openalex.org/W6852735266",
        "https://openalex.org/W6860578347",
        "https://openalex.org/W4384392961",
        "https://openalex.org/W6779857854",
        "https://openalex.org/W4396881792",
        "https://openalex.org/W6758711939",
        "https://openalex.org/W4389471205",
        "https://openalex.org/W6720711607",
        "https://openalex.org/W1682403713",
        "https://openalex.org/W4361290857",
        "https://openalex.org/W6851812595",
        "https://openalex.org/W4309877123",
        "https://openalex.org/W2046702882",
        "https://openalex.org/W4383046915",
        "https://openalex.org/W2803552920",
        "https://openalex.org/W2804175194",
        "https://openalex.org/W2581624817",
        "https://openalex.org/W4387838079",
        "https://openalex.org/W4395687448",
        "https://openalex.org/W4387151262",
        "https://openalex.org/W6748729045",
        "https://openalex.org/W3155806510",
        "https://openalex.org/W4392616726",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4285597685",
        "https://openalex.org/W4319049883",
        "https://openalex.org/W4391673048",
        "https://openalex.org/W6732742072",
        "https://openalex.org/W4304192526",
        "https://openalex.org/W4221155125",
        "https://openalex.org/W6730822450",
        "https://openalex.org/W6771301040",
        "https://openalex.org/W4377865177",
        "https://openalex.org/W4399554362",
        "https://openalex.org/W6838157354"
    ],
    "abstract": "Large Language Models (LLMs) need to adapt to the continuous changes in data, tasks, and user preferences. Due to their massive size and the high costs associated with training, LLMs are not suitable for frequent retraining. However, updates are necessary to keep them in sync with rapidly evolving human knowledge. To address these challenges, this paper proposes the Compression Memory Training (CMT) method, an efficient and effective online adaptation framework for LLMs that features robust knowledge retention capabilities. Inspired by human memory mechanisms, CMT compresses and extracts information from new documents to be stored in a memory bank. When answering to queries related to these new documents, the model aggregates these document memories from the memory bank to better answer user questions. The parameters of the LLM itself do not change during training and inference, reducing the risk of catastrophic forgetting. To enhance the encoding, retrieval, and aggregation of memory, we further propose three new general and flexible techniques, including memory-aware objective, self-matching and top-k aggregation. Extensive experiments conducted on three continual learning datasets (i.e., StreamingQA, SQuAD and ArchivalQA) demonstrate that the proposed method improves model adaptability and robustness across multiple base LLMs (e.g., +4.07 EM &amp; +4.19 F1 in StreamingQA with Llama-2-7b).",
    "full_text": "CMT: A Memory Compression Method for Continual Knowledge Learning of\nLarge Language Models\nDongfang Li, Zetian Sun, Xinshuo Hu, Baotian Hu, Min Zhang\nHarbin Institute of Technology (Shenzhen)\ncrazyofapple@gmail.com\nAbstract\nLarge Language Models (LLMs) need to adapt to the continu-\nous changes in data, tasks, and user preferences. Due to their\nmassive size and the high costs associated with training, LLMs\nare not suitable for frequent retraining. However, updates are\nnecessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes\nthe Compression Memory Training (CMT) method, an efﬁ-\ncient and effective online adaptation framework for LLMs that\nfeatures robust knowledge retention capabilities. Inspired by\nhuman memory mechanisms, CMT compresses and extracts\ninformation from new documents to be stored in a memory\nbank. When answering to queries related to these new docu-\nments, the model aggregates these document memories from\nthe memory bank to better answer user questions. The parame-\nters of the LLM itself do not change during training and infer-\nence, reducing the risk of catastrophic forgetting. To enhance\nthe encoding, retrieval, and aggregation of memory, we further\npropose three new general and ﬂexible techniques, including\nmemory-aware objective, self-matching and top-k aggregation.\nExtensive experiments conducted on three continual learning\ndatasets (i.e., StreamingQA, SQuAD and ArchivalQA) demon-\nstrate that the proposed method improves model adaptability\nand robustness across multiple base LLMs (e.g., +4.07 EM &\n+4.19 F1 in StreamingQA with Llama-2-7b).\nIntroduction\nLarge language models (LLMs) have become the core of\nnatural language processing (NLP) (Touvron et al. 2023a;\nOpenAI 2023). The current challenge is how these LLMs\nadapt to rapidly changing world knowledge, especially in the\ncontext of increasing new data and growing model complex-\nity (Shi et al. 2024). Typically, LLMs are trained on static and\npre-deﬁned datasets. For example, the Llama-3.1 model is an\nopen-source large language model by Meta, with a training\ndataset over 15 trillion tokens (Llama Team 2024). However,\nin practical applications, language usage habits, information\ncontent, and user needs are all dynamically changing (Wu\net al. 2024). On the other hand, once training is complete,\nthe model becomes ﬁxed, and the cost and computational de-\nmands of retraining or incremental pre-training is extremely\nhigh. For example, the GPT-3 model has 174.6 billion pa-\nrameters, and retraining it once requires approximately 3640\nCopyright © 2025, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n/g58/g82/g85/g78/g76/g81/g74\n/g48/g72/g80/g82/g85/g92\n/g47/g82/g81/g74/g16/g87/g72/g85/g80/g3\n/g48/g72/g80/g82/g85/g92\n/g53/g72/g87/g85/g76/g72/g89/g68/g79\n/g56/g83/g71/g68/g87/g72/g71\n/g46/g81/g82/g90/g79/g72/g71/g74/g72\n/g40/g81/g70/g82/g71/g76/g81/g74\n/g21/g19/g20/g23\n/g44/g81/g87/g72/g74/g85/g68/g87/g72/g3/g49/g72/g90/g3/g46/g81/g82/g90/g79/g72/g71/g74/g72/g3/g76/g81/g87/g82/g3\n/g47/g68/g85/g74/g72/g3/g47/g68/g81/g74/g88/g68/g74/g72/g3/g48/g82/g71/g72/g79/g86/g3\n/g21/g19/g21/g23\n/g54/g87/g82/g85/g68/g74/g72\nFigure 1: In general, memory can be divided into three stages:\n(1) encoding involves reorganizing and transforming external\ninformation; (2) storage entails hierarchically categorizing\nand preserving information in long-term memory; (3) re-\ntrieval extracts information from long-term memory.\nPF-days of computing power (i.e., performing 10 quadrillion\ncalculations per second for 3640 days) (Brown et al. 2020).\nTherefore, how to adapt downstream tasks effectively and\nefﬁciently with updating the model with the new knowledge\nwhile retaining the existing knowledge has become an impor-\ntant and urgent topic.\nTo address these challenges, existing continual learning\nmethods dynamically update new incremental knowledge\nthrough techniques such as data replay and incremental task\nparameters while balancing the generalization of new and\nold knowledge (Schwarz et al. 2018; Riemer et al. 2018; Shi\nand Wang 2024). However, data replay and incrementally\nadding task parameters bring non-negligible computational\noverhead. Additionally, methods like model editing only pro-\nvide patches to the model, resulting in poor generalization\nability and even causing collapse (Yao et al. 2023). While\nit is possible to perform thousands of edits simultaneously,\nscalability is low when updating a large amount of knowledge\nin large models, and catastrophic forgetting can occur. On the\nother hand, memory is the foundation of human intelligence\nwhile humans use memory to achieve continual learning. As\nshown in Figure 1, memory impact on intellectual activi-\nties such as learning, abstraction, association, and reasoning\nin the human brain spans three stages: (1) Encoding: It in-\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n24413\nvolves reorganizing and transforming external information.\nThe efﬁciency of learning depends on the strategies used for\nmemory encoding (Pyc and Rawson 2010). Strategies such as\nmulti-channel encoding and contextual association can signif-\nicantly enhance learning outcomes. (2) Storage: Information\nis stored hierarchically and categorized in long-term mem-\nory. Retaining learned content makes subsequent learning\nmore efﬁcient (Bjork 1994). (3) Retrieval: It involves extract-\ning and aggregating information from long-term memory.\nIt consolidates memory storage, stimulates meta-cognitive\nabilities, and promotes reasoning, abstraction, and associa-\ntion (Karpicke and Roediger III 2008). Hence, how to draw\non human memory mechanisms to the continual learning\nprocess has become an interesting and possible direction for\nadapting to changing world knowledge of LLMs.\nTo this end, we introduce the Compression Memory\nTraining (CMT) method that encodes knowledge extracted\nfrom new documents into a dynamic memory bank within\nits latent space, serving as long-term memory for subsequent\nretrieval and aggregation. The core idea is to freeze the param-\neters of the LLM itself and construct a memory-based module\nthat learns to automatically encode and collect relevant infor-\nmation. Speciﬁcally, we ﬁrst utilize an instantiable compres-\nsor to compress information from new documents into com-\npact representations, which are cached to maximize the per-\nformance of the LLM on unseen tasks. Different from Tack\net al. (2024), this representation is generated through mem-\nory tokens with decoder-only model representing compressed\nknowledge, resulting in a memory bank that is less redundant\nthan traditional knowledge bases in retrieval-based methods\nor contexts in prompting compression methods. Thus, during\nonline adaptation, each document stream instance is stored\nin the memory bank. It allows contexts to be pre-computed\nofﬂine once and reducing the LLM’s computational costs at\ninference. Next, we learn to aggregate representations (i.e.,\nmemory) in the feature space into a single representation\nbased on the given query, which is then mapped into cached\nkey-value pairs within each transformer layer of the LLM.\nTo ensure the effectiveness and scalability of CMT, we fur-\nther propose three training and inference techniques corre-\nsponding to the encoding, retrieval and aggregation stages of\nmemory respectively: (1) memory-aware objective; (2) self-\nmatching; and (3) top-k aggregation. The evaluation of CMT\nfocuses on several key aspects: (1) Integration of new knowl-\nedge. The model’s performance is assessed with downstream\nQA tasks, where CMT demonstrates substantial improve-\nments over existing methods, indicating the superiority of\nsupplementing LLMs with CMT; (2) Knowledge retention.\nCMT is evaluated on knowledge retention experiments un-\nder scenarios with different numbers of adapted documents,\nshowcasing its ability to recall knowledge; (3) Robustness.\nWe use the proportion of unrelated documents as a measure\nto test the model’s performance in the presence of irrele-\nvant interference. The results show that CMT outperforms\ncompetitive baselines, demonstrating superior robustness.\nOur contributions are summarized as follows:\n• We introduce CMT that incorporates an integrated mem-\nory bank within the latent space to address the challenges\nof continual learning of LLMs.\n• To utilize encoded memory more efﬁciently, we further\npropose three effective training and inference strategies.\n• CMT demonstrates competitive performance across three\nbenchmarks and knowledge retention settings, showcas-\ning its versatility, effectiveness, and robustness.\nRelated Work\nMemory-Augmented Models Memory-augmented mod-\nels are not a new concept. Early memory networks introduced\ncomputational methods to store contextual information in lim-\nited space, thereby enhancing inference efﬁciency (Weston,\nChopra, and Bordes 2015; Sukhbaatar et al. 2015; Ba et al.\n2016). Following this, Memory Transformer (Burtsev and\nSapunov 2020) and RMT (Bulatov, Kuratov, and Burtsev\n2022) proposed adding memory tokens when reading con-\ntexts. However, expanding memory and incorporating infor-\nmation without disrupting the model’s original capabilities re-\nmains a long-term challenge (Khandelwal et al. 2019; Zhong,\nLei, and Chen 2022; Modarressi et al. 2023; Moro et al. 2023;\nZhong et al. 2023; Wang et al. 2023; Yang et al. 2024). Recent\nresearch has also focused on compressing prompts to enhance\nLLM inference efﬁciency (Wingate, Shoeybi, and Sorensen\n2022; Snell, Klein, and Zhong 2022; Phang et al. 2023).\nFor instance, AutoCompressor (Chevalier et al. 2023) and\nICAE (Ge et al. 2024) propose auto-encoding methods for\ncompressing contexts into soft embeddings. Gisting (Mu, Li,\nand Goodman 2024) introduces learnable tokens to compress\ncontext information within attention hidden states. Moreover,\nseveral improvements to transformers have demonstrated the\nbeneﬁts of equipping LLMs with external, controllable mem-\nory (e.g., MemoryLLM) (Kim et al. 2023; He et al. 2024;\nWang et al. 2024b). However, these methods have not yet\nbeen applied to continual knowledge learning for existing\nLLMs as they typically require training from scratch and rely\non inﬂexible and non-reusable implementations.\nContinual Learning of LLMs Continual learning aims to\nintegrate LLMs into dynamic data distributions, task struc-\ntures, and user preferences without signiﬁcantly degrading\nperformance in learned domains (Zheng et al. 2024; Shi et al.\n2024). This involves sequentially training models on a series\nof tasks with the goal of maintaining performance across all\ntasks (Kirkpatrick et al. 2017; Li and Hoiem 2017; Riemer\net al. 2018; Buzzega et al. 2020). During training, models\noften have limited or no access to previous data, making\nit challenging to retain past knowledge since optimization\nconstraints from previous data are absent during current-task\nlearning (Li and Hoiem 2017; Buzzega et al. 2020; Smith et al.\n2023; Shi and Wang 2024). This challenge, known as catas-\ntrophic forgetting(McCloskey and Cohen 1989), has been a\ncentral focus since its inception. Over the years, researchers\nhave explored various techniques to mitigate forgetting in\nmodels. These include replay-based methods (Schwarz et al.\n2018; Riemer et al. 2018; Shi and Wang 2024), parameter\nregularization (Kirkpatrick et al. 2017; Ritter, Botev, and\nBarber 2018; Aljundi et al. 2018; Sprechmann et al. 2018),\nand model architecture expansion (Wang et al. 2022). Re-\ncently, in the context of continual learning for LLMs, the\n24414\n/g38/g82/g81/g87/g76/g81/g88/g68/g79/g3/g47/g72/g68/g85/g81/g76/g81/g74/g3/g68/g81/g71/g3/g46/g81/g82/g90/g79/g72/g71/g74/g72/g3/g53/g72/g87/g72/g81/g87/g76/g82/g81/g3/g82/g81/g3/g50/g81/g79/g76/g81/g72/g3/g36/g71/g68/g83/g87/g68/g87/g76/g82/g81\n/g56/g83/g71/g68/g87/g72\n /g56/g83/g71/g68/g87/g72\n/g39/g82/g90/g81/g86/g87/g85/g72/g68/g80/g3/g52/g36/g3/g40/g89/g68/g79/g88/g68/g87/g76/g82/g81\n/g58/g75/g76/g70/g75/g3/g41/g85/g72/g81/g70/g75/g80/g68/g81/g3/g90/g68/g86/g3/g87/g75/g72/g3\n/g75/g72/g68/g71/g3/g82/g73/g3/g51/g68/g85/g76/g86/g3/g50/g79/g92/g80/g83/g76/g70/g86/g3\n/g21/g19/g21/g23/g34\n/g48/g72/g80/g82/g85/g92/g3ℳ/g2296\nℳ/g2869\n/g17/g17/g17\n/g17\n/g17\nℳ/g2870\nℳ/g3041\n/g55/g85/g68/g81/g86/g73/g82/g85/g80/g72/g85/g3/g47/g68/g92/g72/g85/g3/g2298/g3039\n/g47/g68/g92/g72/g85/g3/g3ℒ/g33971\n/g47/g68/g92/g72/g85/g3/g3ℒ\n/g1837/g1848/g2870\n/g1837/g1848/g2869\n/g1837/g1848/g2871\n/g1837/g1848/g3013 /g17/g17/g17\n/g1837/g1848/g3039\n∗\n /g2298/g3039/g2879/g2869\n/g90/g76/g87/g75/g3/g83/g72/g73/g87/g3/g80/g82/g71/g88/g79/g72\n/g40/g81/g71/g16/g87/g82/g16/g72/g81/g71/g3/g47/g72/g68/g85/g81/g76/g81/g74\n/g73/g82/g85/g3/g55/g68/g86/g78/g3/g55/g68/g85/g74/g72/g87\n/g4668/g2169/g2778/g4669 /g4668/g2169/g2779/g4669\n/g4668/g2169|/g2270|/g4669\n/g2296∈/g2270\n/g38/g82/g80/g83/g85/g72/g86/g86/g82/g85\n/g38/g85/g82/g86/g86/g36/g87/g87/g81\n/g52 /g4668/g2169/g2191/g4669\n/g48/g72/g80/g82/g85/g92/g3\nℳ∗\n/g4668/g2202/g2197/g2198/g2193 /g2169|/g2270|/g4669\n/g41/g76/g79/g87/g72/g85\n/g56/g83/g71/g68/g87/g72\nFigure 2: Illustration of compression memory training method. During the adaptation process, each document to be learned will\nbe compressed into the dense vector by a compressor, and these vectors {M|D|} will be aggregated through the cross attention\nmechanism and sent to LLMs together with the question for answer output. The training goal is the accuracy of the downstream\ntask answer. In the online adaptation stage, all documents will be compressed into vectors and then ﬁltered and aggregated.\nchallenge has shifted from storage efﬁciency to computa-\ntional efﬁciency (Song et al. 2023; Wang et al. 2024a). In this\npaper, we focus on integrating memory systems into contin-\nual learning, enabling encoding, retrieval, and aggregation of\nknowledge without the need for expensive retraining.\nMethod\nTask Formulation\nWe consider the scenario where an outdated model, fθ,i s\nupdated using an online stream of recent documents, Dtest =\n{xi}. This process produces an updated model,fθ+△ θ, which\nis then evaluated against a set of queries, Qtest = {qi}, with\ncorresponding labels, Ytest = {yi}. Each queryqi and its label\nyi are derived from a distribution related to the corresponding\ndocument xi: qi,yi ∼ p(qi,yi | xi). For instance, qi could be\na question about some information in document xi, with yi\nbeing the answer provided by the document. A key constraint\nis that during the update process using Dtest, we do not have\naccess to Qtest. Thus, our methodology for updating fθ must\nbe general rather than query-speciﬁc. To make this problem\ntractable, we assume the availability of an additional corpus\nof documents Dtrain and corresponding query samples Qtrain\nand labels Ytrain generated by a similar process to Qtest,Ytest.\nThis training set enables learning the types of queries that\nmay be of interest, guiding us on how to update our model\nto optimize performance on test queries while minimizing\ndisruption to its prior knowledge and behaviors. We deﬁne\nthe adaptation process involvingDtrain, Qtrain, and Ytrain as the\nlearning phase, while the process involvingDtest is referred to\nas the online adaptation phase. We next describe the method\nthat adjusts the learning phase to more efﬁciently update our\nbase model on the test stream of documents Dtest.\nCMT: Compression Memory Training\nOur goal is to efﬁciently adapt given LLMs to unseen knowl-\nedge while retaining previously learned knowledge, whether\nfrom the original pre-training stage or updates from docu-\nments in a stream of new data. To achieve this, we designed\na learning method called Compression Memory Training\nshown in Figure 2. During the online adaptation phase, it\nonly requires a single forward pass to compress the docu-\nments knowledge into memory, which avoids the cost of\ngradient computation. Here, we ﬁrst introduce the general\nprocess of the method.\nFirst, each document d in the document set D is com-\npressed into condensed vectors M through the compressor\nΘ. Then, these dense vectors corresponding to each document\nare further aggregated. The aggregated vectors are further\nmapped and input into the LLM Φ to be adapted in the form\nof cached key-value pairs. The LLM Φ to be adapted freeze\ntheir parameters during both the training and online adap-\ntation phases, reducing the risk of catastrophic forgetting\nduring continuous updating. The parameters to be learned\ninclude memory encoding, storage, and mapping parts. The\nentire network is trained during the learning phase, with the\nlearning objective being to better answer Qtrain.\nCompression Memory We introduce the method of com-\npressing documents D into condensed vectors. It aims to\ntransform lengthy documents into concise, compact repre-\nsentations while striving to maintain the core semantics and\nintegrity of the original knowledge. We deﬁne a document\nd ∈ D as w =( w1,w2,...,w n,c1,c2,...,c k), where wi\nmeans the i-th token of document d, cj means the j-th soft\nvirtual token adhere to this document, and nis the number of\nactual tokens in document d. Let e(·)represent the word em-\nbedding lookup in the LLM and m(·)represent the learnable\nembeddings of soft tokens c1,c2,...,c k. A document com-\npressor model Θ utilizes the document embeddings e(w)=\n(e(w1),e(w2),..., e(wn)) and the soft token embeddings\nesoft(c)=( esoft(c1),esoft(c2),..., esoft(ck))to produce\ncompact representations M =( m1,m2,..., mk) ∈ Rk×d\nof the document d, where k is the length of the compressed\ndocument and k ≪ n. The condensed vectors M can replace\nthe original context and be combined with other prompt em-\nbeddings e(p)=( e(p1),..., e(pl)) for input to an LLM Φ.\nThe output y =( y1,...,y m) remains faithful to the content\n24415\nof the original context w. As illustrated in Figure 2, inspired\nby Ge et al. (2024), the compressor can be instantiated as a\nseries of cross-attention layers, pre-trained decoder models,\nand encoder-decoder models with a set of learnable soft to-\nkens, termed condensed tokens. Here, the compressor utilizes\ndocument tokens and condensed tokens as inputs, leveraging\na causal Transformer decoder to compress the document in-\nformation into condensed vectors. We leave the application\nof these vector across different LLMs for future work.\nMemory Aggregation Given the memory bank of com-\npressed documents D represented as {Mi}|D|\ni=1, we aim to\nlearn how to select most relevant information in the form of\na transformation M∗ ∈ Rk×d for a given input qi. There\nare two feasible methods: (1) Retrieve one or multiple mem-\nory units and map them into the LLM space. For example,\nxRAG (Cheng et al. 2024) addresses context aggregation\nfrom a multi-modal fusion perspective. It introduces a modal-\nity projector trained to directly project retrieved dense vectors\ninto the LLM representation space. However, this approach\nhas the risk of selecting incorrect memory units and requires\na pre-training phase to learn how to resolve relationships\namong different memories. (2) Linearly interpolate multi-\nple memory units, aggregate them by weights into a single\nmemory unit, and map it into the LLM space. For exam-\nple, Sukhbaatar et al. (2015) computes the weighted sum of\nthe memory bank as the representative vector of the memory.\nThe advantage of this method is that it can leverage ideas like\nattention mechanisms, model soups (Wortsman et al. 2022)\nand the mixture-of-experts method (Shazeer et al. 2017) to\nﬁlter and aggregate different memory units, maintaining per-\nmutation invariance of the memory units. However, such\nmethods do not consider the relative position information of\nsoft tokens within memory units during aggregation. More-\nover, as we discussed in the task deﬁnition, the source of\naccurate information for answering question\nQi is mostly\nrelated to the i-th document. Therefore, similar to Tack et al.\n(2024), we select M∗ using cross-attention blocks (Vaswani\net al. 2017; Kim et al. 2019; Xu et al. 2020), with the set\naggregation network ψ:\nM∗ = ψ\n(\nΘ(qi),{Mi}|D|\ni=1\n)\n(1)\nHere, for reasons of efﬁciency and consistency in the repre-\nsentation space, we use a document compressor Θ to com-\npress the input qi (e.g., user query). Using an additional\nquestion encoder is left for future work. As the vanilla cross-\nattention mechanism suffers from capturing the relative posi-\ntional relationships among soft tokens within the document d.\nIt implies that swapping any two tokens in the memory results\nin an identical condensed vector. Hence, in the aggregation\nprocess, we apply RoPE (Su et al. 2024) to represent the rela-\ntive positional relations within the soft tokens. We only per-\nform position embedding operations on query and key. And\nwe allocate positional embeddings as if placing the soft to-\nkens subsequent to the context tokens. The RoPE embeddings\nRi and Rj manifests the relative positional relationships\nthrough the inner product between Qpos = {qi} := Θ(qi)\nand Kpos = {ki} := {Mk}K\nk=1:\n(Riq)T(Rjk)= qTRT\ni Rjk = qTRj−ik (2)\nIn this way, each soft token can recognize the relative posi-\ntions relations of both intra- and inter-document soft tokens.\nAlignment for LLM After obtaining M∗ ∈ Rk×d,w ed o\nnot intend to use the memory as an embedding layer input\nto the LLM Φ, as this does not fully leverage the memory to\npromote the association. Therefore, we design a network π\nto map the original memory representation into cached key-\nvalue pairs (with the number being the actual tokens count).\nThe purpose of π is to perform self-attention on the memory\ntokens and use multiple multi-layer perceptrons to transform\nthe memory tokens’ features into actual tokens’ features.\nSpeciﬁcally, the module handles actual tokens by repeating\nmemory tokens and recombines the processed features into\nthe ﬁnal output. Here, the actual tokens are deﬁned as the\nnew virtual tokens in each layer multiplied by the number of\nlayers, then multiplied by 2 (i.e., key and value).\nTraining Objective To train the memory embeddings\nesoft, the compressor Θ, the aggregation networks ψ and\nalignment module π, we optimize both networks end-to-end\nusing the loss functionL, which is the negative log-likelihood\nof the given label y:\nL =m i n\nDtrain,Qtrain,Ytrain\n1\nN\nN∑\ni=1\nL\n(\nLMθ(qi;π(M∗)),yi\n)\n(3)\nwhere N is the number of the training queries and labels in\nQtrain. Note that we do not update the static LLM θ to avoid\nthe risk of catastrophic forgetting by overwriting important\nparameters. It is important to train the model using the cross-\nentropy loss of the ﬁnal QA task. We experimented with\ndocument auto-encoding pretraining tasks (Ge et al. 2024),\nwith dividing it into two stages or using multi-task learning,\nbut neither approach resulted in signiﬁcant improvements.\nOnline Adaptation After training the entire network on a\ngiven training corpus Dtrain, Qtrain, and Ytrain, we introduce\nan online adaptation phase. Formally, the CMT processes\na stream of test documents\nDtest that are sequentially fed\nto the LLM. Considering that the task query qi\ntest ∈ Qtest is\nunavailable during the adaptation process, we ﬁrst store the\ncondensed representation of the document in the memory\nM = {Θ(di\ntest)}|Dtest|\ni=0 and later use the aggregation network\nto predict the modulation to adapt the LLM:\nˆyi\ntest = LLMθ(qi\ntest;π(M∗)) (4)\nwhere M∗ = ψ\n(\nΘ(qi\ntest),M\n)\n.\nEffective Learning Strategy\nMemory-Aware Conditional Objective To enhance the\nmodel’s utilization of memory, we propose a training objec-\ntive of contrastive ensemble between the logits. It enables the\nmodel to account for external knowledge which may not be\naligned with the model’s training data. Speciﬁcally, we adopt\nthe vanilla logits from LLM as lθ for the prior knowledge\nlθ(yi | qi). We demote this knowledge from the model’s\noriginal output distribution via lθ(yi|M∗,qi)\nlθ(yi|qi) . We choose this\nformulation because it also represents the pointwise mutual\n24416\ninformation between the external knowledge from the docu-\nment set M∗ conditioned on qi. Optionally, one can adjust\nthe original distribution by lθ(yi|M∗,qi)\nlθ(yi|M−,qi) , where M− repre-\nsents an explicit knowledge one wants to demote from (e.g., a\nset of other unrelated documents). We interpolate this demo-\ntion lθ(yi|M∗,qi)\nlθ(yi|qi) and the original output logitslθ(yi | M∗,q)\nvia a product-of-experts weighted by α. We sample yi from\nthe reweighted distribution:\nyi ∝ lθ(yi | M∗,qi)\n(lθ(yi | M∗,q)\nlθ(yi | qi)\n)α\n(5)\nFurther, we normalize it across all possible yi:\nyi ∼ (1+ α)lθ(yi | M∗,qi)−αlθ(yi | qi) (6)\nwhere larger α means more weight on our adjustment (we\nset it to 0.5) and α =0 reduces to standard negative log-\nlikelihood. If we identify an external knowledge M∗ con-\nditionally independent to the generation, lθ(yi | M∗,qi)=\nlθ(yi | qi), even a non-zero α would not have an impact on\nthe original output distribution.\nKnowledge Transfer with Self-Matching Recall that\nduring the aggregation process, we calculate the attention\nweights between the query and the memory bank. These\nweights represent the contribution of each document to an-\nswering the current question to some extent. Therefore, this\ninspires us to leverage the nature of the task to capture signif-\nicant features of memory during continual learning. Note that\nthe speciﬁc vector\nq ∈ Rd for each query has the same dimen-\nsion as the memory unit mi ∈ Rd. Then, we calculate the\ncosine similarity between the query embedding and the mem-\nory vectors for the current ith document, as the document\nmatching score α[: i]=c o s (qi,M [: i]). The additional\ntraining objective for updating the ith document is to maxi-\nmize the cosine similarity between mi and the corresponding\nquery embedding q. To prevent the model from collapsing all\ndocuments into similar vectors, we add a uniformity term to\npenalize excessive similarity between document embeddings,\npromoting diversity by encouraging larger pairwise distances\nbetween different memory vectors.\nInference with Top-k Aggregation Additionally, we em-\nploy a ﬁltering mechanism to handle a large memory bank\nduring downstream task inference. Let nand |D|be the num-\nber of output tokens for each context and the number of\nmemory units, respectively. Then, the memory usage of\nt\ncross-attention layers in the memory aggregation becomes\nt·O (|D|n2). It indicates that the memory cost of the aggre-\ngation process scales linearly with the size of the memory.\nUnlike previous work (Tack et al. 2024) that reduces memory\nconsumption using hierarchical modulation aggregation, here\nwe use a simple but effective top-k ﬁltering method. Speciﬁ-\ncally, for a given memory bank withk units, we ﬁrst compute\nthe similarity between them and query, then sort by similarity\nand ﬁlter according to the window size used during training.\nHence, the memory bank window size seen by the model in\nthe aggregation process during training and testing is consis-\ntent, which requiring no additional training or modiﬁcations\nto the primary training objective. It also reduces the impact\nof overﬁtting and noise, and yields better results. Similar\nobservations are reported in retrieval-augmented generation\nworks (Qin et al. 2023; Cuconasu et al. 2024).\nExperiments\nWe conduct extensive experiments on these Question An-\nswering (QA) benchmark datasets to answer the following\nResearch Questions (RQs):\n• RQ1: How does our model contribute to QA accuracy\ncompared with other state-of-the-art methods?\n• RQ2: How effective are the key components in our model,\nsuch as the self-matching of memory vectors?\n• RQ3: Can our model demonstrate robustness against\nknowledge interference from irrelevant documents?\n• RQ4: How does our model perform in terms of the forget-\nting and plasticity dynamics within the document stream?\nStreamingQA The StreamingQA dataset (Li ˇska et al.\n2022) features both human-written and language model-\ngenerated questions. These questions are sourced from En-\nglish WMT news articles published between 2007 and 2020.\nEach question is linked to a complete WMT news article,\nwith an average length of about 500 tokens per article. For\nlearning purposes, question-article pairs from post-2018 pub-\nlications are used, resulting in 21k training questions, 1.7k\nvalidation questions, and 5k test questions.\nSQuAD The Stanford Question Answering Dataset\n(SQuAD) (Rajpurkar et al. 2016) includes questions created\nby humans based on Wikipedia articles. The answer to each\nquestion is a text span from a speciﬁc paragraph within the\narticle. Typically, paragraphs are around 150 tokens. We uti-\nlize the validation set of SQuAD as our test set and divide\nthe training set into four additional splits. It results in 39.9k\ntraining questions, 5.6k validation questions, and 10.6k test\nquestions. Additionally, we use 8.6k training documents, 1.2k\nvalidation documents, and 2.1k test documents.\nArchivalQA The ArchivalQA dataset (Wang, Jatowt, and\nYoshikawa 2022) contains questions generated automatically\nfrom the New York Times Annotated Corpus (Sandhaus,\nEvan 2008). Each answer is a text span within an article,\nwith questions paired to paragraphs from NYT articles. We\nsplit the validation set of ArchivalQA into ﬁve segments\nfor our study. This setup provides us with 21.7k training\nquestions, 5.3k validation questions, and 8.7k test questions.\nFor documents, we utilize 12.8k training documents, 3.0k\nvalidation documents, and 5.0k test documents.\nExperiment Settings We conducted extensive experiments\nusing 4 LLMs as backbones, including the GPT-2 series\n(Radford et al. 2018) and the Llama-2 series (Touvron et al.\n2023b). The model parameters are 82M, 774M, 1.5B, and\n7B, respectively. Larger models, such as the 70B, were not\nincluded due to insufﬁcient computational resources for train-\ning. For the compressor, unlike previous work, we chose\na larger decoder model, Llama-2-7b, and used Parameter-\nEfﬁcient Fine-Tuning (PEFT) with a rank of 6 and a LoRA\nalpha set to 32, employing a different encoding method as\n24417\nDatasets Method DistilGPT2 GPT2-Large GPT2-XL Llama-2\nEM F1 EM F1 EM F1 EM F1\nStreamingQA\nUniform 1.62 3.76 4.74 7.00 5.11 7.48 12.43 13.54\nSalient Spans 1.44 4.67 4.86 8.54 5.40 9.42 13.33 18.97\nCaMeLS 1.62 5.79 5.35 10.60 6.55 11.67 - -\nMAC 5.59 10.18 7.25 13.31 8.99 15.38 14.29 21.79\nCMT (ours) 6.43 12.32 7.32 13.43 9.61 16.48 18.36 25.98\nSQuAD\nUniform 1.24 2.54 3.64 4.97 6.10 6.78 13.25 17.01\nSalient Spans 1.03 2.47 4.03 6.48 4.55 6.74 13.74 18.66\nCaMeLS 1.47 3.08 4.97 8.63 6.70 10.15 - -\nMAC 2.01 6.85 6.43 11.42 7.10 12.55 15.07 21.14\nCMT (ours) 3.12 7.59 7.15 12.45 9.81 12.85 19.54 25.50\nArchivalQA\nUniform 4.86 4.08 7.66 8.71 8.61 10.78 18.53 21.35\nSalient Spans 4.52 3.76 9.75 11.19 11.81 14.11 18.97 22.75\nCaMeLS 4.62 6.19 9.92 12.41 13.87 15.74 - -\nMAC 7.55 10.58 11.84 15.26 14.01 17.12 20.12 23.90\nCMT (ours) 8.15 11.03 12.28 16.12 14.55 18.01 21.73 25.40\nTable 1: Performance comparisons of online adaptation with CMT are presented. We report the Exact Match (EM) and F1 scores\nafter adapting the LLMs on a stream of documents and subsequently conducting downstream QA for test. We use the average of\n3 random seeds and baseline results are from the corresponding papers. The boldfaced means the best results for this dataset.\nwell. We performed 1,000 steps of pre-training using English\nWikipedia with auto-encoder tasks, as additional steps did not\nyield consistent improvements. The number of soft tokens\nused is 24. Additionally, we found that while increasing the\nnumber of tokens beyond 24 initially led to marginal gains,\nthe performance plateaued or slightly decreased due to over-\nﬁtting and increased computational overhead. We evaluated\nonline adaptation performance on a test dataset composed of\ndocuments and QA pairs. Following Tack et al. (2024), we\nadapted the LLMs using 1,665 documents and then assessed\nits performance post-adaptation. To test the model’s general\nunderstanding, QA pairs were sampled from these documents.\nEach document contains up to 1,024 tokens. Cross-attention\ninvolves 4 blocks. The batch sizes for updates and validation\nare 8 and 16 respectively, with gradient accumulation over 4\nsteps. The optimizer is AdamW, and training will run for 50\nepochs with validation every 250 steps. The learning rate is\nset to 1e−6 with a warmup ratio of 0.01 and a constant-with-\nwarmup schedule. During training, we sample the document\nmemory in the same batch and at inference k is equal to\ntraining batch size. We run on the NVIDIA A100 80G GPUs.\nBaseline We include the online ﬁne-tuning baselines intro-\nduced in Tack et al. (2024), including Uniform, Salient Spans,\nCaMeLS and MAC. The uniform baseline uses uniform token\nweighting kearning documents and involves additional ﬁne-\ntuning for question answering after adaptation. Salient Spans\nassigns uniform weights to tokens in salient spans (Guu et al.\n2020) and no weights to other tokens. CaMeLS leverages\nthe output of a token-weighting language model (i.e., meta-\nlearned to predict important tokens to maximize the perfor-\nmance of the adapted LLM). Memory of Amortized Contexts\n(MAC) is an efﬁcient online learning framework that uses the\nmodulation to integrate new document knowledge.\nModel Comparison (RQ1)\nTable 1 illustrates the performance of CMT in online adapta-\ntion compared to other baselines. CMT consistently outper-\nforms these baseline methods across all datasets and models,\ndemonstrating its superior capabilities in continual learning\nand knowledge retention on online adaption. These advan-\ntages are particularly evident in larger models, indicating\nthat CMT effectively scales with model size and complexity.\nFor instance, in the StreamingQA dataset, CMT consistently\nsurpasses other methods for all model variants. Speciﬁcally,\nwith DistilGPT2, CMT achieves an EM score of 6.43 and\nan F1 score of 12.32, outperforming the next best method,\nMAC, which scores 5.59 (EM) and 10.18 (F1). This perfor-\nmance gap widens with larger models, with CMT achieving\nthe highest scores on Llama-2 (EM: 18.36, F1: 25.98). This\ndemonstrates CMT’s superior ability to incorporate and re-\ntain new knowledge in real-time. Furthermore, CMT is efﬁ-\ncient in terms of memory usage and adaptation time. Unlike\nCaMeLS, CMT does not require gradient computation for\nupdates. Furthermore, CMT reduces the proportion of train-\nable parameters by 5.4 times and inference time due to top-k\naggregation compared to MAC, facilitating easier scalability\nwith larger document corpora and model sizes.\nAblation Study (RQ2)\nThis section presents an ablation study to assess the impact of\nvarious components of the CMT on its performance. Table 2\nsummarizes the results across three datasets. We evaluate the\nfull CMT model (1) and three variants: CMT without the\nMemory-Aware Objective (2), CMT without Self-Matching\n(3), and CMT without Top-k Aggregation (4). For example,\nin the StreamingQA dataset, the full CMT model achieves an\nEM score of 18.36 and an F1 score of 25.98, outperforming\n24418\n# Method StreamingQA SQuAD ArchivalQA\nEM F1 EM F1 EM F1\n(1) CMT 18.36 25.98 19.54 25.50 21.73 25.40\n(2) w/o Memory-Aware Objective 18.54 23.71 15.38 22.77 20.89 24.18\n(3) w/o Self-Matching 17.87 22.54 17.97 23.40 22.43 25.68\n(4) w/o Top-k Aggregation 16.43 20.13 18.35 24.12 21.09 23.99\nTable 2: Results of ablation study where the best results are boldfaced and the second-best results are underlined.\nFigure 3: Knowledge retention analysis underLlama-2-7b\ntrained on StreamingQA dataset.\nall other variants. The removal of the Memory-Aware Ob-\njective (variant 2) slightly increases the EM score to 18.54\nbut decreases the\nF1 score to 23.71. The absence of Self-\nMatching (variant 3) results in lower scores (EM: 17.87, F1:\n22.54), indicating the importance of this component. The\nvariant without Top-k Aggregation (4) shows the lowest per-\nformance (EM: 16.43,F1: 20.13), highlighting its critical role\nin CMT. The ablation study reveals the relative importance\nof each CMT component. The Memory-Aware Objective and\nTop-k Aggregation are crucial for maximizing performance\nacross most datasets. The Top-k aggregation also helps by\nfocusing on relevant memory units, reducing inference la-\ntency by 18%. Self-Matching, while generally beneﬁcial, can\nsometimes be omitted without severe performance degrada-\ntion, as seen in the ArchivalQA results. However, the full\nCMT model consistently provides the best or near-best per-\nformance, validating the integrated approach.\nKnowledge Retention (RQ3)\nFollowing Tack et al. (2024), we evaluate retention ratio de-\ntermined by the decline in theF1 score of the initially adapted\n200 documents during online adaption. As shown in Figure 3,\nCMT and CaMeLS lead in performance, followed by MAC,\nSalient, and ﬁnally the Uniform method. The plot indicates\nthat all methods reduce from an increased number of online\nadaptation documents, as shown by the downward trends in\nboth F1 score and retention rate. However, the rate of im-\nprovement varies among the methods. Methods like CMT\nand CaMeLS show a higher scalability factor, indicating that\nthey are better suited for environments where the volume of\nFigure 4: Performance of robustness analysis experiments.\nWe use synthetic unrelated documents to test the impact of\nirrelevant interference brought by memory integration.\nadaptation data is substantial. The gap between the highest\n(CMT) and the lowest performing method (Uniform) widens\nas the number of documents increases, highlighting the im-\nportance of choosing a more efﬁcient method for large-scale\nonline adaptation tasks.\nRobustness Analysis (RQ4)\nWe make use of irrelevant synthetic data, which is obtained\nby generating text that is irrelevant to the current document\nusing gpt-4o. As shown in Figure 4, the performance of\nthe Uniform shows a signiﬁcant decline as the proportion\nof irrelevant documents increases. Initially maintaining a\nhigh relative F1 score, the performance deteriorates sharply\nbeyond the 20%, indicating a high sensitivity to irrelevant\ndata. CaMeLS exhibits a more robust performance compared\nto the Uniform method. However, a noticeable performance\ndrop is still observed beyond the 60% threshold. The MAC\nmethod demonstrates a relatively stable performance across\ndifferent proportions of irrelevant documents. While there\nis a gradual decline in the\nF1 score, it is less pronounced\ncompared to the Uniform and CaMeLS methods, highlighting\nMAC’s effectiveness in handling irrelevant data. Among the\nfour methods, CMT shows the best performance stability.\nThe relative change in F1 score remains minimal even as the\nproportion of irrelevant documents approaches 100%.\nConclusion\nWe propose a continual learning method for LLMs named\nCMT including the memory bank in a latent space as the\nmodel’s updatable knowledge parameters. CMT can update\nthe memory with new knowledge, enabling effective knowl-\nedge integration and slow forgetting of prior knowledge.\n24419\nAcknowledgements\nWe thank reviewers and ACs for their comments. This work is\nsupported by the Natural Science Foundation of China (Grant\nNo. 62406088) and the Guangdong Basic and Applied Basic\nResearch Foundation (2023A1515110078) and the Natural\nScience Foundation of China (Grant No. 62376067). Baotian\nHu is the corresponding author.\nReferences\nAljundi, R.; Babiloni, F.; Elhoseiny, M.; Rohrbach, M.; and\nTuytelaars, T. 2018. Memory aware synapses: Learning what\n(not) to forget. In ECCV.\nBa, J.; Hinton, G.; Mnih, V .; Leibo, J. Z.; and Ionescu, C.\n2016. Using fast weights to attend to the recent past. In\nICONIP.\nBjork, R. A. 1994. Memory and Metamemory Considerations\nin the. Metacognition: Knowing about Knowing.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.;\nSutskever, I.; and Amodei, D. 2020. Language Models are\nFew-Shot Learners. In NeurIPS.\nBulatov, A.; Kuratov, Y .; and Burtsev, M. S. 2022. Recurrent\nMemory Transformer. In NeurIPS.\nBurtsev, M. S.; and Sapunov, G. V . 2020. Memory Trans-\nformer. CoRR.\nBuzzega, P.; Boschini, M.; Porrello, A.; Abati, D.; and Calder-\nara, S. 2020. Dark experience for general continual learning:\na strong, simple baseline. NeurIPS.\nCheng, X.; Wang, X.; Zhang, X.; Ge, T.; Chen, S.; Wei, F.;\nZhang, H.; and Zhao, D. 2024. xRAG: Extreme Context\nCompression for Retrieval-augmented Generation with One\nToken. CoRR.\nChevalier, A.; Wettig, A.; Ajith, A.; and Chen, D. 2023.\nAdapting Language Models to Compress Contexts. In\nEMNLP.\nCuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Campag-\nnano, C.; Maarek, Y .; Tonellotto, N.; and Silvestri, F. 2024.\nThe Power of Noise: Redeﬁning Retrieval for RAG Systems.\nIn SIGIR.\nGe, T.; Jing, H.; Wang, L.; Wang, X.; Chen, S.-Q.; and Wei,\nF. 2024. In-context Autoencoder for Context Compression in\na Large Language Model. In ICLR.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020.\nRetrieval Augmented Language Model Pre-Training. In\nICML.\nHe, Z.; Qin, Z.; Prakriya, N.; Sun, Y .; and Cong, J. 2024.\nHMT: Hierarchical Memory Transformer for Long Context\nLanguage Processing. CoRR.\nKarpicke, J. D.; and Roediger III, H. L. 2008. The critical\nimportance of retrieval for learning. science.\nKhandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.;\nand Lewis, M. 2019. Generalization through memoriza-\ntion: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172.\nKim, H.; Mnih, A.; Schwarz, J.; Garnelo, M.; Eslami, S.\nM. A.; Rosenbaum, D.; Vinyals, O.; and Teh, Y . W. 2019.\nAttentive Neural Processes. In ICLR.\nKim, J.; Yeom, J.; Yun, S.; and Song, H. O. 2023. Com-\npressed Context Memory For Online Language Model Inter-\naction. CoRR.\nKirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Des-\njardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.;\nGrabska-Barwinska, A.; et al. 2017. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national\nacademy of sciences.\nLi, Z.; and Hoiem, D. 2017. Learning without forgetting.\nIEEE transactions on pattern analysis and machine intelli-\ngence.\nLiˇska, A.; Koˇcisk´y, T.; Gribovskaya, E.; Terzi, T.; Sezener, E.;\nAgrawal, D.; de Masson d’Autume, C.; Scholtes, T.; Zaheer,\nM.; Young, S.; Gilsenan-McMahon, E.; Austin, S.; Blunsom,\nP.; and Lazaridou, A. 2022. StreamingQA: A Benchmark\nfor Adaptation to New Knowledge over Time in Question\nAnswering Models. arXiv:2205.11388.\nLlama Team, A. . M. 2024. The Llama 3 Herd of Models.\nMcCloskey, M.; and Cohen, N. J. 1989. Catastrophic Inter-\nference in Connectionist Networks: The Sequential Learning\nProblem.\nModarressi, A.; Imani, A.; Fayyaz, M.; and Sch ¨utze, H.\n2023. RET-LLM: Towards a General Read-Write Memory for\nLarge Language Models. arXiv preprint arXiv:2305.14322.\nMoro, G.; Ragazzi, L.; Valgimigli, L.; Frisoni, G.; Sartori,\nC.; and Marﬁa, G. 2023. Efﬁcient Memory-Enhanced Trans-\nformer for Long-Document Summarization in Low-Resource\nRegimes. Sensors.\nMu, J.; Li, X.; and Goodman, N. 2024. Learning to compress\nprompts with gist tokens. NeurIPS.\nOpenAI. 2023. GPT-4 Technical Report. CoRR.\nPhang, J.; Mao, Y .; He, P.; and Chen, W. 2023. Hypertun-\ning: Toward adapting large language models without back-\npropagation. In ICML.\nPyc, M. A.; and Rawson, K. A. 2010. Why testing improves\nmemory: Mediator effectiveness hypothesis. Science.\nQin, Z.; Jagerman, R.; Hui, K.; Zhuang, H.; Wu, J.; Shen, J.;\nLiu, T.; Liu, J.; Metzler, D.; Wang, X.; and Bendersky, M.\n2023. Large Language Models are Effective Text Rankers\nwith Pairwise Ranking Prompting. CoRR.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by generative\npre-training. In preprint.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. arXiv:1606.05250.\nRiemer, M.; Cases, I.; Ajemian, R.; Liu, M.; Rish, I.; Tu, Y .;\nand Tesauro, G. 2018. Learning to learn without forgetting\n24420\nby maximizing transfer and minimizing interference. arXiv\npreprint arXiv:1810.11910.\nRitter, H.; Botev, A.; and Barber, D. 2018. Online struc-\ntured laplace approximations for overcoming catastrophic\nforgetting. NeurIPS.\nSandhaus, Evan. 2008. The New York Times Annotated\nCorpus.\nSchwarz, J.; Czarnecki, W.; Luketina, J.; Grabska-Barwinska,\nA.; Teh, Y . W.; Pascanu, R.; and Hadsell, R. 2018. Progress\n& compress: A scalable framework for continual learning. In\nICML.\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q. V .;\nHinton, G. E.; and Dean, J. 2017. Outrageously Large Neural\nNetworks: The Sparsely-Gated Mixture-of-Experts Layer. In\nICLR.\nShi, H.; and Wang, H. 2024. A Uniﬁed Approach to Domain\nIncremental Learning with Memory: Theory and Algorithm.\nNeurIPS.\nShi, H.; Xu, Z.; Wang, H.; Qin, W.; Wang, W.; Wang, Y .;\nand Wang, H. 2024. Continual Learning of Large Language\nModels: A Comprehensive Survey. CoRR.\nSmith, J. S.; Tian, J.; Halbe, S.; Hsu, Y .-C.; and Kira, Z.\n2023. A Closer Look at Rehearsal-Free Continual Learning.\narXiv:2203.17269.\nSnell, C.; Klein, D.; and Zhong, R. 2022. Learning by distill-\ning context. arXiv preprint arXiv:2209.15189.\nSong, C.; Han, X.; Zeng, Z.; Li, K.; Chen, C.; Liu, Z.; Sun,\nM.; and Yang, T. 2023. ConPET: Continual Parameter-\nEfﬁcient Tuning for Large Language Models. CoRR.\nSprechmann, P.; Jayakumar, S. M.; Rae, J. W.; Pritzel, A.;\nBadia, A. P.; Uria, B.; Vinyals, O.; Hassabis, D.; Pascanu, R.;\nand Blundell, C. 2018. Memory-based Parameter Adaptation.\nIn ICLR.\nSu, J.; Ahmed, M. H. M.; Lu, Y .; Pan, S.; Bo, W.; and Liu, Y .\n2024. RoFormer: Enhanced transformer with Rotary Position\nEmbedding. Neurocomputing.\nSukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-end\nmemory networks. NeurIPS.\nTack, J.; Kim, J.; Mitchell, E.; Shin, J.; Teh, Y . W.; and\nSchwarz, J. R. 2024. Online Adaptation of Language Models\nwith a Memory of Amortized Contexts. CoRR.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Canton-Ferrer, C.; Chen, M.; Cu-\ncurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller,\nB.; Gao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hos-\nseini, S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa,\nM.; Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poulton,\nA.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva,\nR.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang, B.; Tay-\nlor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.;\nZhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,\nA.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023a. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models. CoRR.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023b. Llama 2: Open foundation and ﬁne-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nis All you Need. In NeurIPS.\nWang, J.; Jatowt, A.; and Yoshikawa, M. 2022. ArchivalQA:\nA Large-scale Benchmark Dataset for Open Domain\nQuestion Answering over Historical News Collections.\narXiv:2109.03438.\nWang, L.; Zhang, X.; Li, Q.; Zhu, J.; and Zhong, Y . 2022.\nCoSCL: Cooperation of Small Continual Learners is Stronger\nThan a Big One. In ECCV.\nWang, L.; Zhang, X.; Su, H.; and Zhu, J. 2024a. A Compre-\nhensive Survey of Continual Learning: Theory, Method and\nApplication. IEEE Trans. Pattern Anal. Mach. Intell.\nWang, W.; Dong, L.; Cheng, H.; Liu, X.; Yan, X.; Gao, J.;\nand Wei, F. 2023. Augmenting Language Models with Long-\nTerm Memory. arXiv preprint arXiv:2306.07174.\nWang, Y .; Chen, X.; Shang, J.; and McAuley, J. J. 2024b.\nMEMORYLLM: Towards Self-Updatable Large Language\nModels. CoRR.\nWeston, J.; Chopra, S.; and Bordes, A. 2015. Memory net-\nworks. In ICLR.\nWingate, D.; Shoeybi, M.; and Sorensen, T. 2022. Prompt\nCompression and Contrastive Conditioning for Controllabil-\nity and Toxicity Reduction in Language Models. In EMNLP\nFindings.\nWortsman, M.; Ilharco, G.; Gadre, S. Y .; Roelofs, R.; Lopes,\nR. G.; Morcos, A. S.; Namkoong, H.; Farhadi, A.; Carmon,\nY .; Kornblith, S.; and Schmidt, L. 2022. Model soups: averag-\ning weights of multiple ﬁne-tuned models improves accuracy\nwithout increasing inference time. In ICML.\nWu, T.; Luo, L.; Li, Y .; Pan, S.; Vu, T.; and Haffari, G. 2024.\nContinual Learning for Large Language Models: A Survey.\nCoRR.\nXu, J.; Ton, J.; Kim, H.; Kosiorek, A. R.; and Teh, Y . W. 2020.\nMetaFun: Meta-Learning with Iterative Functional Updates.\nIn ICML.\nYang, H.; Lin, Z.; Wang, W.; Wu, H.; Li, Z.; Tang, B.; Wei,\nW.; Wang, J.; Tang, Z.; Song, S.; et al. 2024. Memory3:\nLanguage Modeling with Explicit Memory. arXiv preprint\narXiv:2407.01178.\nYao, Y .; Wang, P.; Tian, B.; Cheng, S.; Li, Z.; Deng, S.; Chen,\nH.; and Zhang, N. 2023. Editing Large Language Models:\nProblems, Methods, and Opportunities. In EMNLP.\nZheng, J.; Qiu, S.; Shi, C.; and Ma, Q. 2024. Towards Life-\nlong Learning of Large Language Models: A Survey. CoRR.\nZhong, W.; Guo, L.; Gao, Q.; and Wang, Y . 2023. Memory-\nBank: Enhancing Large Language Models with Long-Term\nMemory. arXiv preprint arXiv:2305.10250.\nZhong, Z.; Lei, T.; and Chen, D. 2022. Training Language\nModels with Memory Augmentation. In EMNLP.\n24421"
}