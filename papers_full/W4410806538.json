{
    "title": "Valsci: an open-source, self-hostable literature review utility for automated large-batch scientific claim verification using large language models",
    "url": "https://openalex.org/W4410806538",
    "year": 2025,
    "authors": [
        {
            "id": null,
            "name": "Brice Edelman",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2216851533",
            "name": "Jeffrey Skolnick",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": null,
            "name": "Brice Edelman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2216851533",
            "name": "Jeffrey Skolnick",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4391591686",
        "https://openalex.org/W3092311467",
        "https://openalex.org/W2950782805",
        "https://openalex.org/W4402855429",
        "https://openalex.org/W2128438887",
        "https://openalex.org/W4404534210",
        "https://openalex.org/W4318146754",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4288026847",
        "https://openalex.org/W3185077430",
        "https://openalex.org/W2901939506",
        "https://openalex.org/W2487443737",
        "https://openalex.org/W4287801874",
        "https://openalex.org/W4307177867",
        "https://openalex.org/W2911293880",
        "https://openalex.org/W3123726009"
    ],
    "abstract": "By providing an open and transparent platform for large-batch literature verification, Valsci substantially lowers the barrier to comprehensive evidence-based reviews and fosters a more reproducible research ecosystem.",
    "full_text": "Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you \nmodified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of \nit. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise \nin a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http:// creat iveco mmons. org/ licen ses/ by- nc- nd/4. 0/.\nSOFTWARE\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140  \nhttps://doi.org/10.1186/s12859-025-06159-4\nBMC Bioinformatics\nValsci: an open-source, self-hostable \nliterature review utility for automated \nlarge-batch scientific claim verification using \nlarge language models\nBrice Edelman1 and Jeffrey Skolnick1* \nAbstract \nBackground: The exponential growth of scientific publications poses a formidable \nchallenge for researchers seeking to validate emerging hypotheses or synthesize exist-\ning evidence. In this paper, we introduce Valsci, an open-source, self-hostable utility \nthat automates large-batch scientific claim verification using any OpenAI-compatible \nlarge language model. Valsci unites retrieval-augmented generation with structured \nbibliometric scoring and chain-of-thought prompting, enabling users to efficiently \nsearch, evaluate, and summarize evidence from the Semantic Scholar database \nand other academic sources. Unlike conventional standalone LLMs, which often suffer \nfrom hallucinations and unreliable citations, Valsci grounds its analyses in verifiable \npublished findings. A guided prompt-flow approach is employed to generate query \nexpansions, retrieve relevant excerpts, and synthesize coherent, evidence-based \nreports. \nResults: Preliminary evaluations across claims from the SciFact benchmark dataset \nreveal that Valsci significantly outperforms base GPT-4o outputs in citation hallucina-\ntion rate while maintaining a low misclassification rate. The system is highly scalable, \nprocessing hundreds of claims per hour through asynchronous parallelization.\nConclusions: By providing an open and transparent platform for large-batch literature \nverification, Valsci substantially lowers the barrier to comprehensive evidence-based \nreviews and fosters a more reproducible research ecosystem.\nKeywords: Claim verification, Large language models, Retrieval-augmented \ngeneration, Bibliometric scoring, Chain-of-Thought, Open-source, Batch processing, \nBioinformatics\nBackground\nThe exponential increase in scientific literature in recent years has both democratized \nknowledge and introduced formidable bottlenecks in evidence-based research [12]. \nResearchers, particularly in fields such as bioinformatics and biomedical sciences, \nroutinely confront the challenge of rapidly verifying new hypotheses or large sets of \n*Correspondence:   \nskolnick@gatech.edu\n1 Georgia Tech Center \nfor the Study of Systems Biology, \nAtlanta, GA, USA\nPage 2 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nmodel-generated predictions. Classical approaches to literature review require manu -\nally searching, extracting, and synthesizing evidence, and can be slow, labor-intensive, \nand prone to human error [2]. The urgency of generating accurate, real-time insights has \nthus sparked intense interest in automating, scaling, and streamlining the claim verifica -\ntion process.\nScientific claim verification—the specific task of assessing the accuracy and validity \nof statements based on evidence extracted from existing literature—is central to this \nprocess of literature review, and therefore to advancing knowledge while safeguard -\ning research quality and reproducibility. Its importance lies in preventing the spread of \nmisinformation, enhancing trust in published findings, and enabling rapid, evidence-\ninformed decision-making in research and clinical settings.\nThe rise of AI‑assisted literature analysis & current gaps\nRecent advancements in large language models (LLMs) have transformed various \ndomains of natural language processing (NLP), from machine translation to summari -\nzation. Yet, when deployed as stand-alone tools for scientific claim verification, LLMs \noften struggle with hallucinations—fabricating data or citing non-existent sources—\nthereby undermining their utility as reliable evidence synthesizers [6]. Many commercial \nplatforms, such as Scite [10], Elicit, and Consensus, have integrated NLP-based capa -\nbilities to expedite literature retrieval and summarization. However, these proprietary \nsolutions typically operate as “black boxes” and impose financial or usage constraints, \nmaking them suboptimal for large-scale or highly specialized academic applications.\nExisting open-source tools have made important strides toward transparent, repro -\nducible literature analysis. For instance, some employ retrieval-augmented generation \n(RAG) to ensure that LLM outputs are grounded in texts scraped from academic reposi -\ntories [8]. Others provide user-friendly environments to rank or cluster abstracts based \non relevance to predefined queries. Yet, most of these frameworks either lack end-to-\nend pipelines—requiring users to manually feed abstracts—or cannot efficiently handle \nlarge numbers of claims concurrently. As a result, researchers aiming for high-through -\nput verification continue to face a patchwork of partial solutions.\nValsci: an end‑to‑end solution\nIn response to these limitations, we present Valsci, an open-source and fully self-hosta -\nble utility for automated scientific claim verification at scale. The source code for Valsci \nis distributed under the June 29, 2007 Version 3 GNU General Public License. The sys -\ntem is built around four core principles:\n• Retrieval-Augmented Generation (RAG): Valsci seamlessly integrates with the \nSemantic Scholar database [7, 9] to fetch relevant abstracts and full texts, ensuring \nits output remains anchored in verifiable sources rather than the LLM’s parametric \nmemory.\n• Structured Bibliometric Scoring: In addition to relevance, Valsci incorporates an \nauthor’s H-index [5], citation counts, and estimated journal impact into an overall \nevidence score, providing a more nuanced view of source credibility.\nPage 3 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \n• Guided Prompt Flow and Chain-of-Thought (CoT): Valsci uses specialized prompts \nto not only refine search queries but also systematically organize retrieved evidence \ninto comprehensive and transparent verification reports. This approach mitigates the \nrisk of hallucinations by requiring the LLM to cite and assess real, validated excerpts.\n• High-Throughput, Asynchronous Execution: Designed for large-batch tasks, Valsci \nemploys asynchronous parallelization in Python to manage network requests, token \nusage, and inference calls concurrently. With typical resource settings, the system \ncan process hundreds of claims per hour—far outpacing both manual human review-\ners and single-threaded open-source tools.\nContributions and significance\nThis work aims to advance automated literature verification by offering a robust, trans -\nparent, and scalable alternative to proprietary solutions. We show that Valsci not only \nlowers the misclassification rate for scientific claims compared to direct outputs from \nbase language models, but also successfully eliminates citation hallucinations. Addi -\ntionally, it offers a speed increase of over thirty times under reasonable API rate limits \ncompared to manual review by undergraduate researchers. By coupling an open-source, \nmodular design with flexible integration options for LLM backends, Valsci empow -\ners researchers to streamline large-scale systematic reviews, meta-analyses, and high-\nthroughput validation of computational predictions.\nRelated work\nThe growing complexity and volume of scientific literature have driven the develop -\nment of AI-assisted tools for claim verification, literature review, and citation analysis. \nWhile many systems now integrate large language models (LLMs) with bibliographic \ndatabases, existing approaches remain constrained in scalability, cost, transparency, and \nautomation. Valsci differs by being fully open-source, self-hostable, and designed for \nhigh-throughput, large-batch claim verification. It combines retrieval-augmented gen -\neration (RAG) with structured bibliometric scoring and guided chain-of-thought (CoT) \nreasoning [15], offering an end-to-end, adaptable alternative to proprietary and aca -\ndemic solutions.\nIn the NLP community, claim verification is often formalized as a classification or \nnatural language inference (NLI) task, in which claims are evaluated against retrieved \nevidence, producing judgments such as supported, refuted, or inconclusive [3, 11]. State-\nof-the-art solutions typically integrate retrieval systems with neural inference models; \nhowever, comprehensive and scalable end-to-end open-source implementations remain \nlimited.\nClosed-Source Tools One of the earliest AI-driven citation analysis tools, Scite, clas -\nsifies citations as supporting, contradicting, or mentioning a referenced claim from a \nparticular academic work. By extracting citation context, it provides a useful measure \nof how a study is discussed in the literature. However, Scite does not actively retrieve or \nanalyze new sources; rather, it depends on existing citation networks. This approach lim-\nits its utility for claims that require comprehensive evidence synthesis from the broader \nscientific corpus. Unlike Scite, Valsci dynamically searches for relevant literature, \nPage 4 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nextracts supporting text, and evaluates the claim through structured analysis, ensuring \nthat verification is based on the strongest available evidence rather than pre-existing \ncitation relationships.\nClosed-source LLM-assisted literature review tools such as Elicit and Consensus aim \nto automate evidence retrieval and synthesis but remain black-box systems with unclear \nweighting of retrieved sources. Elicit allows users to search for relevant literature and \nextract key findings but imposes constraints on how many papers can be processed, with \na “Pro” plan allowing only 1,200 papers per year. Consensus, built on the same Semantic \nScholar database as Valsci, offers a “Pro Analysis” feature that provides structured evi -\ndence summaries, though the exact methodology behind its synthesis remains opaque. \nBoth tools are subscription-based, limiting accessibility and transparency. Valsci, by con-\ntrast, is both open-source and allows unrestricted batch processing at no cost, making it \nmore suitable for large-scale systematic reviews and high-volume research workflows.\nOpen-Source Tools Unlike these proprietary systems, open-source solutions such as \nLitLLM and LLAssist provide more transparent AI-assisted literature analysis. LitLLM \napplies retrieval-augmented generation (RAG) to ensure that LLM-generated summa -\nries are grounded in retrieved texts, reducing the risk of hallucination [1]. However, it \nrequires users to manually provide abstracts for input, rather than performing active lit -\nerature searches. LLAssist takes a batch of abstracts and ranks them by relevance to a \npredefined research question but does not automatically search for or retrieve relevant \npapers [4].\nOther open-source solutions include systems such as MultiVerS [14], which focus on \ndocument-level fact-checking in controlled scientific contexts and rely on specialized \nmodel training (i.e. resource-intensive, domain-specific fine-tuning) for tasks like anno -\ntating SciFact claims. Consequently, these systems are less generalizable to arbitrary \ndomains and lack the end-to-end automation needed for large-scale claim verification \nacross diverse scientific fields.\nThese tools serve as useful components for systematic review workflows but do not \nprovide a fully automated end-to-end pipeline. Valsci extends beyond retrieval and rank-\ning by autonomously handling search, evaluation, bibliometric scoring, and final synthe -\nsis into structured claim verification reports.\nFocus on Scalability A critical distinction between Valsci and these alternatives is \nits ability to process large batches of claims efficiently. Existing retrieval-based systems, \nwhether open-source or commercial, generally handle claims sequentially or within \nlimited pre-configured batch sizes. Valsci, leveraging asynchronous parallelization of \nnetwork requests, can simultaneously process as many claims as the throughput of the \nconfigured LLM providing endpoint supports. This scalability is particularly valuable for \nresearch fields requiring high-throughput verification, such as bioinformatics and clini -\ncal guideline assessments, where researchers often need to verify thousands of claims \nderived from computational models.\nWhile existing AI-driven literature tools have made notable strides, none provide an \nopen, self-hostable, end-to-end claim verification system that integrates retrieval, evalu -\nation, bibliometric scoring, and structured synthesis into a single pipeline. Valsci fills this \ngap by offering an adaptable and scalable solution that is not only more cost-effective but \nalso ensures transparency and reproducibility in AI-assisted scientific verification.\nPage 5 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nImplementation\nThe claim verification process begins with query generation, where an LLM is \nemployed to transform a claim into multiple well-structured search queries opti -\nmized for literature retrieval. Instead of relying on simple keyword extraction, Valsci \nemploys a strategic query expansion prompt designed to maximize the likelihood that \nrelevant literature entries are captured by the search. This involves incorporating syn -\nonyms, alternative phrasing, and mechanistic decompositions to ensure a diverse set \nof relevant papers is retrieved. Additionally, the system deliberately constructs que -\nries that target both supporting and refuting evidence, ensuring that the retrieved lit -\nerature provides a balanced assessment of the claim. Queries are tailored to match the \nsearch conventions of Semantic Scholar and other academic search engines, improv -\ning retrieval efficiency across multiple disciplines. The prompt used to perform this \nexpansion is available in Appendix A. The information flow through Valsci’s modular \ndata processing structure is depicted in Fig. 1  below.\nOnce queries are generated, Valsci executes them against the free Semantic Scholar \nSearch API to obtain a corpus of potentially relevant papers. The response from the \nAPI includes the unique identifier of academic works in the Semantic Scholar Data -\nbase, which is locally hosted within Valsci for fast retrieval without additional API \ncalls. Valsci includes a utility allowing the user to download the full Semantic Scholar \ndatabase as part of the system setup. The pattern of leveraging the Semantic Scholar \nAPI for relevance search in combination with Valsci’s indexing utility for fast retrieval \nof texts from the local database is what makes effective retrieval augmented genera -\ntion (RAG) against such a large corpus of data possible.\nFig. 1 How information flows through Valsci’s modular data processing structure. Information goes through \nthe stages of retrieval (blue), analysis (orange), and synthesis (green). For each claim, up to N-times-M papers \nmay be evaluated\nPage 6 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nTo maximize access to full-text sources, the system first attempts to retrieve the work \nfrom the local Semantic Scholar Open Research Corpus (S2ORC), which contains full-\ntext versions of millions of academic papers. By prioritizing full-text retrieval, Valsci \nincreases the likelihood of retrieving detailed excerpts rather than relying solely on \nabstracts. However, to simultaneously ensure that the breadth of literature coverage is \nnot unnecessarily limited, abstracts are still used as a fallback if the full text is not avail -\nable. The retrieval engine employs deduplication techniques through an LLM call to \nprevent redundant papers from being analyzed multiple times. With default settings of \nfive generated queries and a cap of up to five papers retrieved per query, the standard \nretrieval process typically yields between 10 and 20 papers per claim, although this con -\nfiguration is adjustable based on computational resources and research requirements.\nFollowing retrieval, each paper undergoes content extraction and relevance analysis \nto identify key excerpts that pertain to the claim under investigation. Using a structured \nLLM prompt, Valsci isolates directly relevant statements, mechanistic explanations, \nand empirical findings that either support or contradict the claim. These excerpts are \nextracted verbatim, preserving critical context, including statistical findings and meth -\nodological details. The verbatim excerpts are also paired with an explanation to provide \nrelevant context or rationale for the excerpt’s inclusion in the final analysis. The rel -\nevance of each excerpt is then assessed using a confidence score ranging from 0 to 1, \nensuring that only evidence that could reasonably be considered potentially relevant is \nconsidered in later synthesis stages. If a paper is determined to have no relevance, it is \nclassified separately and excluded from final synthesis.\nTo further refine the claim verification process, Valsci implements an optional evi -\ndence-scoring mechanism that ranks papers based on their scientific credibility and \nimpact. Unlike simple relevance-based ranking, this scoring process incorporates bib -\nliometric indicators such as the H-index of the first and last authors, the citation count \nof the paper, and an LLM-derived estimate of the journal’s impact. The author impact \nmetric considers the expertise of the researchers involved, while citation count provides \na measure of how influential the study has been in the field. The journal impact metric \nis assessed using an LLM prompt aiming to recognize venue prestige based on historical \npublication standards and citation patterns in the model’s parametric knowledge, which \nsimplifies implementation by removing the need for integration with an external data -\nbase. These three factors are weighted to produce a final evidence score, which deter -\nmines the relative importance of each paper in the final assessment. The weight given to \neach bibliometric factor can be adjusted through the interface when submitting a claim \nfor verification. Users who prefer not to use any bibliometric weighting can disable this \nfeature altogether.\nOnce literature retrieval, relevance assessment, and evidence scoring are complete, \nValsci synthesizes the results into a structured report using a guided CoT LLM prompt. \nThis final synthesis process is designed to mimic expert-level reasoning by systemati -\ncally structuring the evaluation across multiple dimensions. The report begins with a \nsummary of supporting evidence, where the strongest corroborating papers and their \nkey excerpts are presented. This is followed by a discussion of contradictory evidence, \nensuring that conflicting findings are critically examined. The report then includes a \nmechanistic evaluation, exploring biological, chemical, or theoretical pathways that may \nPage 7 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nexplain or refute the claim. Finally, the balance of evidence is assessed, and a claim verac-\nity rating is assigned based on the strength and consistency of the findings.\nThe final claim rating follows an ordinal scale: Contradicted, Likely False, Mixed \nEvidence, Likely True, Highly Supported, or, if no relevant works were found, No Evi -\ndence. This rating system provides an interpretable and standardized measure of claim \nvalidity, allowing researchers to quickly determine the degree of empirical support for \na given statement. For large-scale research applications, Valsci allows batch downloads \nof processed claim reports, enabling streamlined verification workflows across multiple \nprojects.\nThe entire system is implemented in Python. The system’s processing module runs as \na polling loop to monitor for submitted jobs, coordinate LLM calls while staying within \nrate limits, and track the status of claim verification tasks during intermediate stages \nof analysis. API calls are performed asynchronously and only gathered when necessary \nusing Python’s asyncio library. By default, the system specifies a 0.0 temperature set -\nting for all LLM calls without any other hyperparameter adjustments. The frontend is \na graphical web interface, which can be used for submitting batches, reviewing results, \nand exporting files. Screenshots of the interface can be found in Appendix B. It runs \ncomfortably on consumer hardware for users who integrate with cloud-based frontier \nLLM providers or other hosted LLM services on their network. Network operations, \nLLM queries, and file I/O are executed concurrently, enabling large-scale claim veri -\nfication with minimal latency. The request management system dynamically regulates \nLLM usage, allowing users to tune their implementation to the specific rate limits of the \nLLM provider they are leveraging. Valsci is designed to be highly scalable and adaptable, \nsupporting integration with cloud-hosted LLM APIs as well as locally deployed open-\nsource models such as LLaMA, Deepseek-R1, and Mistral for privacy-focused research \nenvironments.\nResults\nTo assess Valsci’s performance in large-batch scientific claim verification, we conducted \nexperiments across three key evaluation dimensions: hallucination reduction, process -\ning speed and efficiency, and misclassifications. We used two popular base large lan -\nguage models, GPT-4o (2024-11-20 version) and GPT-4o-mini (2024-07-18 version), \nand compared the results using these models standalone versus as the backend of the \nValsci claim verification system. Processing speed was additionally evaluated against a \nhuman undergraduate researcher performing manual claim verification.\nOur evaluation methodology focuses on quantifying improvements in evidence-based \nclaim validation and computational efficiency. Experiments with both models were con -\nducted using claims extracted from the SciFact dataset, a benchmark of scientific claims \nannotated with evidence from literature, developed by the Allen Institute for AI [13]. \nThe SciFact dataset comprises claims labeled with supporting or contradicting evidence, \nwhich are critical for verifying Valsci’s outputs against ground truth annotations. Specif -\nically, we map SciFact claims labeled as “Supported” as true and “Contradicted” as false \nfor our experimentation. To ensure accurate performance assessment, we excluded unla-\nbeled claims (that is, claims with neither the “Supported” nor “Contradicted” label pre -\nsent) from the dataset, focusing solely on those with verifiable evidence. The selection \nPage 8 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nof approximately 500 claims was a deliberate choice to balance the need for a sufficient \nsample size to draw robust conclusions with the computational resource intensiveness \nof running repeated benchmarks. Importantly, our approach was developed and tested \nexclusively on this labeled subset, and no tweaking or adjustments were made based on \nthe remaining, unlabeled portions of the SciFact dataset.\nOur tests were run using a Microsoft Azure cloud-hosted Standard D2ads v6 (2 vcpus, \n8 GiB memory) virtual machine running Debian 12.\nHallucination rate reduction\nA major limitation of generative LLMs is their propensity for hallucination—generating \nplausible yet false citations, particularly in the absence of strong retrieval mechanisms. \nValsci effectively eliminates hallucinations by retrieving real excerpts from Semantic \nScholar into the context window, then starting the guided CoT by evaluating the avail -\nable evidence from them before conducting the rest of its analysis. In a sample of 10 \nclaims containing 211 citations, every work cited by Valsci was confirmed to be extant.\nThe base models, on the other hand, were asked to provide citations with the same for-\nmat (paper title, lead authors, and a URL) as Valsci as part of their output. We scored the \ngenerated citations as “full hallucinations” if they appeared to be completely fabricated \nand as “partial hallucinations” if there were errors, but searching the internet for the \ntitle and author still yielded a similar, extant paper. Partial hallucinations were assigned \nhalf-credit in determining the hallucination rate. The comparison of citation verification \nbetween the baseline GPT-4o and Valsci is shown in Table 1 below.\nIn a sample of 30 papers, the GPT-4o model failed to provide a single paper without a \nsignificant error. Only three papers were scored as “partial” hallucinations. Our sample \nfrom the GPT-4o-mini model also did not include any accurate citations – the smaller \nmodel more commonly defaulted to using obvious placeholder names and URLs (such \nas “Jane Doe” and www. examp le. com) rather than the plausible-seeming but incorrect \ncitations created by the larger model.\nProcessing speed and efficiency\nTable 2, shown below, presents the average processing rate for evaluation of scientific \nclaims and generation of structured claim verification reports by Valsci compared to an \nundergraduate researcher with more than an academic semester of experience working \nin a research setting and conducting similar work.\nValsci processes approximately 144 claims per hour under rate limits set to use \nno more than 10 requests and 25,000 tokens in a rolling five-second window. Even \nwith these limits, the processing is approximately 36 × faster than an undergraduate \nTable 1 Evaluation of citations provided by each literature review method in a review of ten claims\nBold indicates the best scores for the citation identification task\nMethod Verified citations Partial hallucinations Full \nhallucinations\nGPT-4o-mini 0 0 30\nGPT-4o (baseline) 0 3 27\nValsci (using GPT-4o) 211 0 0\nPage 9 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nresearcher manually verifying claims. The undergraduate researcher was allowed to use \npublicly available academic databases and search engines but was not permitted to use \ngenerative AI for this evaluation.\nWe emphasize that Valsci’s rate limits can be configured to maximally utilize the \nresources available in the environment of the user’s deployment—the limiting step for \nprocessing speed is the throughput of the LLM endpoint (local- or cloud-deployed), \navailable to the end researcher.\nPrecision, recall, F1, and uncertainty rate under various conditions\nTo evaluate whether accuracy is preserved during the claim verification task, we com -\npute precision, recall, F1-score, and uncertainty rate. For clarity, we define the counts \nused in these calculations based on the system’s classifications as follows:\n• C: The number of claims correctly classified as true or false.\n• I: The number of claims incorrectly classified as true or false.\n• U: The number of claims where the system abstained from making a definitive clas -\nsification (i.e., uncertain).\nUsing these counts, we calculate the following metrics:\n• Precision: The ratio of correctly classified claims to the total number of claims where \nthe system made a definitive classification:\n• C\nC +I\n• Recall: The ratio of correctly classified claims to the total number of claims, including \nthose where the system was uncertain:\n• C\nC +I+U\n• F1 Score: The harmonic mean of precision and recall, providing a balanced measure \nof both:\n• 2 × Precision×Recall\nPrecision+Recall\n• Uncertainty Rate: The proportion of claims where the system abstained from making \na definitive classification:\n• U\nC +I+U\nThese metrics provide a comprehensive evaluation of the system’s performance. Addi -\ntionally, we include columns specifying the maximum number of citations that Valsci \nwas configured to review and whether bibliometric indicators were incorporated into \nthe scoring.\nTable 2 Rate of scientific claim evaluation by Valsci integrated with GPT-4o versus an \nundergraduate researcher\nMethod Claims \nprocessed \nper hour\nValsci (using GPT-4o) 144\nUndergraduate Researcher 4\nPage 10 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nThe maximum number of citations represents the product of the number of queries \ngenerated by Valsci and the number of papers reviewed per query. Specifically, we con -\nfigured Valsci for three scenarios: 3 queries with 3 papers each, 5 queries with 5 papers \neach, and 7 queries with 7 papers each.\nDue to resource constraints, experiments varying configuration parameters and per -\nforming bibliometric ablation were conducted exclusively using the GPT-4o-mini model, \nchosen for its significantly lower computational cost compared to the larger GPT-4o \nmodel.\nTable 3 below presents a detailed comparison of precision, recall, F1-score, and uncer -\ntainty rate across these varying conditions.\nThe raw results from each configuration used for calculating the above scores are \navailable in the supplementary data hosted on Zenodo and in the “Valsci_Supplementa -\nryData_2025_03_27.zip” additional file.\nOverall, these findings indicate that Valsci consistently outperforms stand-alone LLM \nbaselines. As shown in Table  3, using Valsci with GPT-4o yields both an increased F1 \nscore (0.761 vs. 0.706) and a markedly lower uncertainty rate (0.267 vs. 0.360) compared \nto GPT-4o alone. A similar trend holds for GPT-4o-mini: integrating the model into \nValsci results in a substantial jump in F1 (from 0.597 to 0.720) and a decrease in uncer -\ntainty (from 0.412 to 0.345).\nMoreover, when varying Valsci’s retrieval settings (i.e., the product of the number of \nqueries and papers per query), higher citation coverage improves both recall and F1 \nwhile reducing the rate of uncertain (abstained) classifications. For instance, increasing \nthe maximum citation count from 9 to 49 yields respective gains in F1 from 0.655 to \n0.724 and a reduction in the uncertainty rate from 0.404 to 0.331. Disabling bibliometric \nindicators has a small but noticeable impact on performance: the system’s F1 dips from \n0.720 to 0.704, suggesting that weighting evidence by author impact and journal prestige \ncan further refine final claim assessments.\nIn order to further validate the system on a different underlying dataset, an additional \nmanual quality review was performed on Valsci’s assessment of twenty hand-curated \ntrue claims extracted from publicly available Cochrane Reviews. Our inspection of \nValsci’s outputs on these claims shows reasonable analysis and chains of thought – we \nTable 3 Comparison of precision, recall, F1, and uncertainty rate across various models and system \nconfigurations\nHigher is better for Precision, Recall, and F1. Lower is better for Uncertainty Rate\n*Failure to annotate after maximum of 3 retries is counted as “Uncertain” in stats calculations. Annotation failures by the \nstandalone GPT-4o-mini occurred when the LLM became stuck generating repeated nonsense tokens up to the response \nlength limit, which is a known issue with many small LLMs\nBold indicates the best scores for the citation identification task\nSystem Max citations Bibliometric \nindicators\nPrecision Recall F1 Uncertainty Rate\nGPT-4o No limit Yes 0.907 0.578 0.706 0.360\nValsci (using GPT-4o) 25 Yes 0.900 0.659 0.761 0.267\nGPT-4o-mini* No limit Yes 0.807 0.473 0.597 0.412\nValsci (using GPT-4o-mini) 9 Yes 0.877 0.523 0.655 0.404\nValsci (using GPT-4o-mini) 25 Yes 0.909 0.596 0.720 0.345\nValsci (using GPT-4o-mini) 49 Yes 0.902 0.604 0.724 0.331\nValsci (using GPT-4o-mini) 25 No 0.886 0.584 0.704 0.341\nPage 11 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \ninclude the full, unadjusted output from Valsci for these claims in the supplementary \ndata hosted on Zenodo and in the “Valsci_SupplementaryData_2025_03_27.zip” addi -\ntional file. Table 4, which includes evaluation statistics for this batch of claims, is shown \nbelow:\nOf the twenty claims, Valsci was sufficiently confident to classify sixteen, and it was \ncorrect in those classifications.\nDiscussion\nThe results presented herein demonstrate that Valsci successfully meets its primary \nobjectives of providing an open-source, high-throughput, and verifiable framework for \nscientific claim verification. Critically, its integration of retrieval-augmented generation \n(RAG) and bibliometric scoring methods offers distinct advantages over both standalone \nlarge language models (LLMs) and existing proprietary solutions. This discussion exam -\nines how these findings extend the current state of literature verification tools, identifies \nlimitations, and outlines opportunities for future work.\nRetrieval‑augmented framework and accuracy\nA key driver of Valsci’s superior performance compared to the baseline GPT-4o model \nis its retrieval augmentation system. Rather than relying solely on parametric knowledge \nembedded within an LLM, Valsci always grounds its outputs in actual scientific liter -\nature. By employing guided query expansion, Valsci systematically seeks out relevant, \ndiverse evidence. This is particularly beneficial for specialized research fields with idi -\nosyncratic terminology, thereby broadening the scope of retrieved articles and augment-\ning recall. The consistent accuracy at very high throughput is striking and underscores \nhow modern AI can not only match but occasionally exceed human capabilities in rapid \nliterature review tasks.\nMoreover, the guided chain-of-thought (CoT) synthesis approach allows Valsci to \nstructurally integrate bibliometric indicators, context from full-text excerpts, and mech -\nanistic insights into a coherent verification report. This layered reasoning contrasts with \ncommon single-step LLM responses, providing better transparency and interpretability. \nThe multi-stage pipeline—encompassing query generation, excerpt extraction, biblio -\nmetric weighting, and final structured reporting—mirrors human expert workflows, but \nat far greater scale and throughput.\nHallucination mitigation and bibliometric scoring\nBy validating each reference against a genuine retrieval from the Semantic Scholar data -\nbase and employing deduplication, Valsci ensures that spurious citations do not per -\nmeate final results. The inclusion of an evidence-scoring mechanism, which integrates \nauthor-level impact (H-index), citation count, and LLM-estimated journal prestige, also \nTable 4 Precision, recall, F1, and uncertainty rate for Valsci on a set of hand-curated claims sourced \nfrom publicly available Cochrane Reviews\nSystem Max Citations Bibliometric Indicators Precision Recall F1 Uncertainty Rate\nValsci (using GPT-4o) 25 Yes 1.000 0.800 0.889 0.200\nPage 12 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nintroduces a beneficial quality-control dimension. This approach not only ranks refer -\nences by credibility but inherently promotes verifiability. Papers from higher-impact \njournals or authors with substantial contributions in the domain intuitively hold greater \nweight in the final assessment, potentially reducing the influence of marginal or errone -\nous studies.\nHowever, bibliometric indicators also come with caveats that merit further attention. \nCitation counts and H-index values can inflate due to general popularity of certain top -\nics or self-citations, and journal-based impact metrics may overlook important findings \npublished in less prominent venues. In fast-moving fields or interdisciplinary research \nareas, relying too heavily on bibliometrics can inadvertently sideline novel, high-value \nstudies. Thus, while the inclusion of bibliometric scores bolsters validity, Valsci’s ulti -\nmate goal of thoroughly capturing supporting and contradicting evidence demands that \nthese measures be employed judiciously. Fine-tuning the weights assigned to each met -\nric or incorporating alternative credibility heuristics could further optimize this aspect \nof the pipeline.\nScalability and throughput\nValsci’s asynchronous parallelization yields a considerable throughput advantage for \nbatch verification tasks. In experiments with rate limits set to use no more than 10 \nrequests and 25,000 tokens in a rolling five-second window, Valsci processed claims at \na rate approximately 36 times faster than undergraduate researchers, a significant boost \nfor time-sensitive fields such as bioinformatics and clinical guideline updates. Further -\nmore, as discussed earlier, these rate limits can be adjusted to take maximal advantage \nof the LLM inference resources available in the end user’s environment. This efficiency \nstems from orchestrating multiple processes (e.g., search requests, LLM inference, file \nI/O) concurrently. The capacity for researchers to integrate local or cloud-hosted Ope -\nnAI-compatible LLMs further enhances Valsci’s adaptability, offering a cost-effective, \nflexible alternative to subscription-based platforms.\nLimitations\nDespite its strengths, Valsci has several limitations. First, although it benefits from the \nexpansive Semantic Scholar database and the S2ORC corpus, coverage gaps remain for \nproprietary or paywalled journals, leading to possible incomplete evidence retrieval. \nLocal storage of the Semantic Scholar database also requires significant hard disk \nspace—almost two terabytes—and it must be updated periodically using the bundled \nutility to ensure the latest indexed works are available to the system. Moreover, while \nValsci explicitly searches for both supporting and refuting sources, highly specialized \nclaims with limited prior studies may yield sparse or no corroboration, complicating \nthe assignment of a final veracity rating. Researchers should interpret “No Evidence” or \n“Mixed Evidence” findings cautiously, recognizing the dependence on existing literature \navailability and retrieval completeness.\nSecond, Valsci relies on frontier LLMs for query generation, relevance scoring, and \nfinal CoT synthesis. While substituting local or alternative LLM models can mitigate \ncost and privacy issues, performance may vary significantly among different LLMs, \nPage 13 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nparticularly for specialized domains or non-English literature. Ensuring that the under -\nlying LLM is robust and domain-tuned may be important to mitigate these concerns.\nLastly, interpretability and accountability remain important concerns. Although Valsci \nprovides structured CoT outputs, discerning the rationale behind final veracity ratings \nmay still be non-trivial for complex claims. Valsci’s interpretive chain-of-thought itself \nis only as transparent as the language model’s competence and accuracy in its articula -\ntion of intermediate reasoning. Advances in the interpretability and alignment scoring of \nbase LLMs will allow Valsci users to better understand their outputs as well.\nFuture directions\nLooking ahead, several avenues could refine and extend Valsci’s capabilities:\n• Enhanced Bibliometric Models: Incorporating additional metrics such as altmet -\nrics, domain-specific citation databases, or network-based analyses could yield more \nnuanced evidence weighting. Analysis of the LLM-derived journal impact score and \ncomparison to methods using bibliographic databases and additional author disam -\nbiguation techniques would be helpful for optimizing the system.\n• Domain Specialization of Backend Models: Domain-tuned LLMs, especially in bio -\nmedical fields, may improve query generation and excerpt extraction by leveraging \nspecialized vocabularies and knowledge representations.\n• More Sources of Evidence: Integrating data from clinical trials, chemical structures, \nor biological pathway databases could expand Valsci’s claim verification beyond tex -\ntual information from Semantic Scholar’s databases.\n• User-Friendly Interface Improvements: While Valsci has a graphical web interface \nbuilt-in, improvements to the design and usability could encourage broader adoption \nby non-technical researchers.\n• Explainable AI Integration: Although the system’s reasoning for assigning a rating \nto a claim can be traced through the chain-of-thought included in the final report, \nadvancements in explainable AI and mechanistic interpretability could be leveraged \nto further improve the user’s confidence in the outputs.\n• Additional Evaluations: The system was tested using popular closed-source models \noffered through Azure due to the computational resources available to our team, but \nfurther testing with different models and datasets could be insightful. It could also be \ninteresting to evaluate the processing speed of a human with access to standard AI \ntools relative to Valsci’s fully automated process or the human reviewer without AI \naccess.\nConclusion\nThis work introduces Valsci, an open-source, self-hostable utility that effectively bridges \nthe gap between large language model capabilities and rigorous literature-based verifica-\ntion. By coupling retrieval-augmented generation with structured bibliometric scoring \nand guided chain-of-thought synthesis, Valsci excels in accurately assessing the verac -\nity of scientific claims at scale. Experimental comparisons indicate that it not only dra -\nmatically reduces hallucinations relative to raw LLM outputs but also maintains high \nPage 14 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \naccuracy while dramatically increasing verification speed relative to a human conducting \nthe same task.\nThe system’s capacity to operate asynchronously on large batches of claims empow -\ners researchers to make full use of their resources to efficiently validate extensive sets \nof computational predictions, systematic review topics, or clinical guidelines. Its open \ndesign and modular architecture support custom LLM backends and evolving bib -\nliometric metrics, providing both flexibility and transparency absent from proprietary \nalternatives. Although issues such as database coverage gaps, variability in LLM perfor -\nmance, and interpretability challenges persist, Valsci offers a valuable foundation for fur-\nther innovation in automated scientific validation.\nAppendix A: Prompts\nSearch query generation (system prompt):\nYou are an expert at converting scientific claims into strategic literature search queries. \nSpecifically, your queries will be used to search the Semantic Scholar database. Your goal \nis to generate queries that will comprehensively evaluate both supporting and contra -\ndicting evidence for a given claim.\nGuidelines for Query Generation:\n1  Identify core concepts and their relationships in the claim\n2  Include field-specific terminology, common synonyms, and alternative phrasings\n3 Decompose complex claims into testable components\n4 Use only plain text search queries, no boolean operators or special syntax\n5 Break hyphenated terms into separate words (e.g. \"drug-resistant\"—> \"drug resist -\nant\")\n6 Balance specificity with recall—avoid overly narrow or broad queries\n7 Consider both direct evidence and mechanistic studies\n8 Account for competing hypotheses and alternative explanations\nSearch Strategy:\n– Generate queries for direct evidence testing the claim\n– Include queries for underlying mechanisms and pathways\n– Consider related phenomena that could provide indirect evidence\n– Look for potential confounding factors or methodological challenges\n– Search for systematic reviews and meta-analyses when applicable\n–  The queries should be sufficiently diverse to capture as much relevant information as \npossible and avoid overlap\nExample Approach:\nFor \"Metformin increases lifespan\", you could consider:\n• Direct evidence: clinical studies, epidemiological data\n• Mechanisms: AMPK pathway, insulin sensitivity, mitochondrial function\n• Related outcomes: mortality, age-related diseases, biomarkers of aging\n• Potential confounds: diabetes status, age, concurrent medications\nPage 15 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nwhen generating your queries.\nOutput Format:\nSearch query generation (user prompt):\nGenerate {num_queries} strategic search queries to evaluate this scientific claim:\n\"{claim_text}\".\nRequirements:\n– Each query should be precisely formulated for Semantic Scholar database searching\n– Include a mix of specific and broader search strategies\n– Consider both direct evidence and mechanistic studies\n– Account for different research methodologies and study types\n– Use only plain text search queries, no boolean operators or special syntax\n– Break hyphenated terms into separate words (e.g. \"drug-resistant\"—> \"drug resist -\nant\")\nReturn results as a JSON object with ’explanations’ and ’queries’ arrays.\nPaper analysis (system prompt):\nYou are an expert at analyzing scientific papers and evaluating their relevance to spe -\ncific claims through both direct evidence and mechanistic pathways.\nGuidelines for Analysis:\n1.  Evaluate direct evidence that supports or refutes the claim\n2. Identify mechanistic evidence that strengthens or weakens the claim’s plausibility\n3. Examine methodology, results, and conclusions with careful attention to detail\nPage 16 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n4. Extract verbatim quotes with complete scientific context in which they are found\n5. Consider study limitations and their impact on evidence quality\n6. Assess both statistical and practical significance of findings\n7. Note experimental conditions that may affect generalizability\nGuidelines for Quote Extraction:\n1. Include complete sentences or paragraphs that capture full context.\n2. Maintain exact spelling, punctuation, and formatting.\nReturn a JSON object with:\nPaper analysis (user prompt):\nAnalyze this paper content for both direct and mechanistic evidence related to the fol-\nlowing claim:\nClaim: {claim_text}.\nPaper content:\n{cleaned_content}.\nTasks :\n1. Determine if this paper provides relevant evidence for or against the claim\n2. Extract complete, verbatim sentences or paragraphs that:\n– Support or refute the claim\n– Describe relevant mechanisms\n– Provide essential context for understanding the evidence\n3. For each sentence or paragraph:\nPage 17 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \n– Explain how it relates to the claim\n– Note whether it’s direct evidence or mechanistic\n– Include any limitations or caveats\n4. If relevance < 0.1, provide a detailed explanation why\nRemember:\n– Include complete sentences and surrounding context\n– Maintain exact wording, including statistical details\nCalculate venue impact (system prompt):\nYou are an expert in academic publishing and research venues.\nEstimate the impact/prestige of an academic venue on a scale of 0–10.\nConsider factors like:\n– Venue reputation in the field\n– Publication standards and peer review\n– Typical citation rates\n– Publisher reputation\nReturn only json object with a single key \"score\" and a number between 0 and 10.\nCalculate venue impact (user prompt):\nRate the academic impact and prestige of this venue:\nVenue: {paper_journal}.\nReturn only json object with a single key \"score\" and a number between 0 and 10, \nwhere:\n0–2: Low impact or predatory venues.\n3–5: Legitimate but lower impact venues.\n6–8: Well-respected, mainstream venues.\n9–10: Top venues in the field.\nSynthesize report (system prompt):\nYou are an expert scientific reviewer specializing in evaluating the plausibility of sci -\nentific claims based on evidence from academic papers and your expert knowledge. \nYour task is to synthesize a detailed evaluation of the claim and assign a final plausibility \nrating based on both your scientific knowledge and the evidence provided in the paper \nexcerpts you receive.\nThe final rating you assign should be one of the following:\n– Contradicted: Strong evidence refutes the claim.\n– Likely False: Evidence suggests the claim is unlikely but not definitively refuted.\n– Mixed Evidence: It is not clear whether the supporting or contradicting evidence is \nstronger.\n– Likely True: The claim is supported by reasonable evidence, though it may not be \ndefinitive.\n– Highly Supported: The claim is strongly supported by compelling and consistent evi -\ndence.\nPage 18 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n– No Evidence: There is no evidence to support or refute the claim in the provided papers.\nWhen formulating your evaluation, consider the following aspects:\n– Supporting Evidence: Summarize the most robust evidence that supports the claim. Be \nspecific, referencing the findings of relevant papers and their implications.\n– Caveats or Contradictions: Identify any limitations, contradictory findings, or alterna-\ntive interpretations that might challenge the claim.\n– Analysis: Based on your expertise, analyze the systems and structures relevant to the \nclaim for any deeper relationships, mechanisms, or second-order implications that \nmight be relevant.\n– Assessment: Assess the balance of evidence, explaining which side is more compelling \nand why. Contextualize caveats but avoid undue hedging; consider the overall weight of \nthe evidence like an expert would.\n– Rating Assignment: Choose a single category from the list above that best reflects the \noverall strength of evidence for the claim. Assign this rating based on the preponder-\nance of evidence, contextualizing caveats without allowing minor exceptions to over-\nshadow the dominant trend.\nUse the following process to review the claim and evidence and conduct your analysis:\nFirst, under the explanationEssay attribute, write your thoughts as an essay with distinct \nparagraphs for:\n– Supporting evidence.\n– Caveats or contradictory evidence.\n– Analysis of potential underlying mechanisms, deeper relationships, or second-order \nimplications.\n– An explanation of which rating seems most appropriate based on the relative strength \nof the evidence.\nOnce you’ve written the essay, read over it and analyze your logic one more time for any \nflaws or inconsistencies. If you find any, revise your rating and explanation accordingly in \nthe finalReasoning attribute. Otherwise, you can reaffirm your rating and explanation in the \nfinalReasoning attribute. This string can be as long as you need to conduct a rigorous final \nanalysis.\nLastly, assign the final rating from the list above in the claimRating attribute.\nYou will receive the text of the claim and excerpts from academic papers that could sup-\nport or refute the claim. Craft your evaluation, then provide a JSON response in the follow-\ning format:\nPage 19 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nSynthesize report (user prompt):\nEvaluate the following claim based on the provided evidence and counter-evidence \nfrom scientific papers:\nClaim: {claim_text}.\nEvidence:\n{paper_summaries_text}.\nAppendix B: Screenshots\nFigure 2 shown below, is a screenshot of Valsci’s landing page.\nFigure 3 shown below, is a screenshot illustrating the configuration options available to \nthe user when submitting a claim processing job.\nFigure 4 shown below, is a screenshot of the progress page allowing the user to moni -\ntor the system as it runs.\nFigure 5, shown below, is a screenshot showing the “Batch Results” page.\nFigure 6 shown below, is a screenshot of an individual report.\nPage 20 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nFig. 2 A screenshot of the Valsci landing page\nPage 21 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nFig. 3 A screenshot of Valsci’s job submission configuration options\nPage 22 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nFig. 4 A screenshot of Valsci’s progress monitoring screen\nPage 23 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \nFig. 5 A screenshot of the Valsci Batch Results screen\nFig. 6 A screenshot of the top of a Valsci report\nPage 24 of 25Edelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \nAbbreviations\nLLM  Large Language Model\nRAG   Retrieval-Augmented Generation\nCoT  Chain-of-Thought\nAPI  Application Programming Interface\nS2ORC  Semantic Scholar Open Research Corpus\nGPL  General Public License\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 025- 06159-4.\nAdditional file1 (ZIP 6000 kb)\nAcknowledgements\nWe thank Jessica Forness for proofreading the manuscript. We also thank our undergraduate researcher, Shaya Farah-\nmand, for his manual literature review assessment.\nAuthor contributions\nBE and JS collaborated on the ideation and design of the Valsci system. BE programmed the system and provided bench-\nmarking and data analysis. JS provided expertise to solve technical issues and guide the development effort. BE wrote \nthe text of the manuscript and JS provided review and revisions.\nFunding\nThis research was supported in part by grant GMR35-118039 of the Division of General Medical Sciences of the National \nInstitutes of Health.\nAvailability of data and materials\nThe source code is available in the GitHub repository, https:// github. com/ brice e98/ Valsci A snapshot of the source code \nat the time of submission and the datasets generated and analyzed during the current study are available in the Zenodo \nrepository, https:// zenodo. org/ recor ds/ 15098 570. This data is also included as the additional file “Valsci_Supplementary-\nData_2025_03_27.zip” .\nDeclarations\nConflict of interest\nThe authors declare no competing interests.\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nReceived: 26 February 2025   Accepted: 7 May 2025\nReferences\n 1. Agarwal S, Laradji IH, Charlin L, Pal C. LitLLM: a toolkit for scientific literature review (No. arXiv: 2402. 01788). 2024. \narXiv. https:// doi. org/ 10. 48550/ arXiv. 2402. 01788.\n 2. Haddaway NR, Bethel A, Dicks LV, Koricheva J, Macura B, Petrokofsky G, Pullin AS, Savilaakso S, Stewart GB. Eight \nproblems with literature reviews and how to fix them. Nat Ecol Evol. 2020;4(12):1582–9. https:// doi. org/ 10. 1038/ \ns41559- 020- 01295-x.\n 3. Hanselowski A, PVS A, Schiller B, Caspelherr F, Chaudhuri D, Meyer CM, Gurevych I. A Retrospective Analysis of the Fake \nNews Challenge Stance Detection Task (No. arXiv: 1806. 05180). 2018. arXiv. https:// doi. org/ 10. 48550/ arXiv. 1806. 05180.\n 4. Haryanto CY. LLAssist: simple tools for automating literature review using large language models (No. arXiv: 2407. \n13993). 2024. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2407. 13993.\n 5. Hirsch JE. An index to quantify an individual’s scientific research output. Proc Natl Acad Sci USA. \n2005;102(46):16569–72. https:// doi. org/ 10. 1073/ pnas. 05076 55102.\n 6. Huang L, Yu W, Ma W, Zhong W, Feng Z, Wang H, Chen Q, Peng W, Feng X, Qin B, Liu T. A survey on hallucination in \nlarge language models: principles, taxonomy, challenges, and open questions. ACM Trans Inf Syst. 2025;43(2):42:1-\n42:55. https:// doi. org/ 10. 1145/ 37031 55.\n 7. Kinney R, Anastasiades C, Authur R, Beltagy I, Bragg J, Buraczynski A, Cachola I, Candra S, Chandrasekhar Y, Cohan \nA, Crawford M, Downey D, Dunkelberger J, Etzioni O, Evans R, Feldman S, Gorney J, Graham D, Hu F, Weld D.S. The \nSemantic Scholar Open Data Platform (No. arXiv: 2301. 10140). 2023. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2301. 10140.\n 8. Lewis P , Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, Küttler H, Lewis M, Yih W, Rocktäschel T, Riedel S, Kiela D. \nRetrieval-augmented generation for knowledge-intensive NLP tasks (No. arXiv: 2005. 11401). 2021. arXiv. https:// doi. \norg/ 10. 48550/ arXiv. 2005. 11401.\nPage 25 of 25\nEdelman and Skolnick  BMC Bioinformatics          (2025) 26:140 \n \n 9. Lo K, Wang LL, Neumann M, Kinney R, Weld DS. S2ORC: the semantic scholar open research corpus (No. arXiv: 1911. \n02782). 2020. arXiv. https:// doi. org/ 10. 48550/ arXiv. 1911. 02782.\n 10. Nicholson JM, Mordaunt M, Lopez P , Uppala A, Rosati D, Rodrigues NP , Grabitz P , Rife SC. scite: a smart citation index \nthat displays the context of citations and classifies their intent using deep learning. Quant Sci Stud. 2021;2(3):882–\n98. https:// doi. org/ 10. 1162/ qss_a_ 00146.\n 11. Nie Y, Chen H, Bansal M. Combining fact extraction and verification with neural semantic matching networks (No. \narXiv: 1811. 07039). 2018. arXiv. https:// doi. org/ 10. 48550/ arXiv. 1811. 07039.\n 12. Pan RK, Petersen AM, Pammolli F, Fortunato S. The memory of science: Inflation, myopia, and the knowledge net-\nwork. J Informet. 2018;12(3):656–78. https:// doi. org/ 10. 1016/j. joi. 2018. 06. 005.\n 13. Wadden D, Lin S, Lo K, Wang LL, Zuylen van M, Cohan A, Hajishirzi H. Fact or Fiction: Verifying Scientific Claims (No. \narXiv: 2004. 14974). 2020. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2004. 14974.\n 14. Wadden D, Lo K, Wang LL, Cohan A, Beltagy I, Hajishirzi H. MultiVerS: Improving scientific claim verification with \nweak supervision and full-document context (No. arXiv: 2112. 01640). 2022. arXiv. https:// doi. org/ 10. 48550/ arXiv. \n2112. 01640\n 15. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi EH, Le QV, Zhou D.: Chain-of-thought prompting elicits \nreasoning in large language models. In: Proceedings of the 36th International Conference on neural information \nprocessing systems, 2022; 24824–24837.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}