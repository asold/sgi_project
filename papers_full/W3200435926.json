{
  "title": "Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results",
  "url": "https://openalex.org/W3200435926",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5074637993",
      "name": "Ian Magnusson",
      "affiliations": [
        "Northeastern University",
        "Smart Information Flow Technologies (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5100856844",
      "name": "Scott Friedman",
      "affiliations": [
        "Smart Information Flow Technologies (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6601808413",
    "https://openalex.org/W2971182981",
    "https://openalex.org/W2962903510",
    "https://openalex.org/W3186034730",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W3197214707",
    "https://openalex.org/W3090302425",
    "https://openalex.org/W3020786614",
    "https://openalex.org/W2806882588",
    "https://openalex.org/W3035372073",
    "https://openalex.org/W3170447327",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W2964222246"
  ],
  "abstract": "Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as nodes and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4651–4658\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4651\nExtracting Fine-Grained Knowledge Graphs of Scientiﬁc Claims:\nDataset and Transformer-Based Results\nIan H. Magnusson♠♥ and Scott E. Friedman♠\n♠SIFT, Minneapolis, MN, USA\n♥Northeastern University, Boston, MA, USA\nmagnusson.i@northeastern.edu\nfriedman@sift.net\nAbstract\nRecent transformer-based approaches demon-\nstrate promising results on relational scientiﬁc\ninformation extraction. Existing datasets fo-\ncus on high-level description of how research\nis carried out. Instead we focus on the sub-\ntleties of how experimental associations are\npresented by building SciClaim, a dataset of\nscientiﬁc claims drawn from Social and Be-\nhavior Science (SBS), PubMed, and CORD-\n19 papers. Our novel graph annotation schema\nincorporates not only coarse-grained entity\nspans as nodes and relations as edges between\nthem, but also ﬁne-grained attributes that mod-\nify entities and their relations, for a total of\n12,738 labels in the corpus. By including\nmore label types and more than twice the la-\nbel density of previous datasets, SciClaim cap-\ntures causal, comparative, predictive, statis-\ntical, and proportional associations over ex-\nperimental variables along with their qualiﬁ-\ncations, subtypes, and evidence. We extend\nwork in transformer-based joint entity and rela-\ntion extraction to effectively infer our schema,\nshowing the promise of ﬁne-grained knowl-\nedge graphs in scientiﬁc claims and beyond.\n1 Introduction\nUsing relations as edges to connect nodes consist-\ning of extracted entity mention spans produces ex-\npressive and unambiguous knowledge graphs from\nunstructured text. This approach has been applied\nto diverse domains from moral reasoning in so-\ncial media (Friedman et al., 2021b) to qualitative\nstructure in ethnographic texts (Friedman et al.,\n2021a), and is particularly useful for reasoning\nabout scientiﬁc claims, where several experimental\nvariables in a sentence may have differing rela-\ntions. Scientiﬁc information extraction datasets\nsuch as SciERC (Luan et al., 2018) use relations\nfor labeling general scientiﬁc language. Utilizing\nthe advances of SciBERT (Beltagy et al., 2019) in\nscientiﬁc language modeling, SpERT (Eberts and\nText: Levels of social support for medical staff were significantly associated with self - efficacy and sleep quality and negatively associated with the degree of anxiety and stress .\nLevels of social support\nFactor\nself - efficacy\nFactor\nq+\nsleep quality\nFactorq+\nthe degree of anxiety\nFactor\nq-\nstress\nFactor\nq-for medical staff\nQualifier\nsignificantly\nMagnitude\nassociated with\nAssociation (Correlation, Sign+) arg0\narg1\narg1\nnegatively\nMagnitude\nassociated with\nAssociation (Correlation, Sign-)\narg0\narg1\narg1\nInput: “Levels of social support for medical staﬀ were signiﬁcantly \nassociated with self-eﬃcacy and sleep quality and negatively \nassociated with the degree of anxiety and stress.”\nFigure 1: SciClaim knowledge graph with entities\n(nodes), relations (edges), and attributes (parentheti-\ncals) connecting an independent variable via arg0 to\ndistinct correlations with dependent variables via arg1.\nUlges, 2020)—a transformer-based joint entity and\nrelation extraction model—advanced the state of\nthe art on SciERC.\nTo extend relational scientiﬁc information ex-\ntraction to speciﬁcally target scientiﬁc claims, we\nannotate SciClaim, 1 a dataset of 12,738 anno-\ntations on 901 sentences from expert identiﬁed\nclaims in Social and Behavior Science (SBS) pa-\npers (Alipourfard et al., 2021), detected causal lan-\nguage in PubMed papers (Yu et al., 2019), and\nclaims and causal language heuristically identiﬁed\nfrom CORD-19 abstracts (Wang et al., 2020).\nFor annotation, we developed a novel graph\nschema that reiﬁes claimed associations as entity\nspans with ﬁne-grained attributes and extracts fac-\ntors as additional entities connected with relations\nto one or more associations in which they are in-\nvolved. In Figure 1, two association entities re-\nlate two pairs of dependent factors to an indepen-\ndent factor, while attributes and additional rela-\ntions delimit the scope and qualitative proportion-\nalities of the claim. Inspired by semantic role la-\nbeling, attributes modify associations and the roles\nof their arguments, allowing us to represent claims\nof causal, comparative, predictive, statistical, and\nproportional associations along with their qualiﬁ-\n1Dataset available at https://github.com/siftech/SciClaim.\n4652\nText: We predicted that the subliminal prime would , under specifiable conditions , increase the accessibility of the pertinent negative outcome and thereby increase its perceived likelihood of occurr ence .\npredicted\nEpistemic\nthe subliminal prime\nFactor\nthe accessibility of the pertinent negative outcome\nFactor\nq+\nwould\nMagnitude\nunder specifiable conditions\nQualifier\nincrease\nAssociation (Causation, Sign+)\narg0\narg1\nits perceived likelihood of occurrence\nFactor\nq+\nincrease\nAssociation (Causation, Sign+) arg0\narg1\nInput: “We predicted that the subliminal prime would, under speciﬁable conditions, increase the accessibility \nof the pertinent negative outcome and thereby increase its perceived likelihood of occurrence.”\nFigure 2: This SciClaim graph captures the chaining together of associations and uncovers a mediating factor in\nthe qualitative proportionality (q+) between the \"subliminal prime\" and \"perceived likelihood of occurrence.\"\ncations, subtypes, and evidence.\nWe adapt SpERT to model this additional multi-\nlabel attribute task and demonstrate that extrac-\ntion of our highly expressive knowledge graphs is\nwithin reach of present methods.\n2 Related Work\nMany previous datasets for relational scientiﬁc in-\nformation extraction—such as SemEval 2017 task\n10 and 2018 task 7, SciERC, and SciREX (Au-\ngenstein et al., 2017; Gábor et al., 2018; Luan\net al., 2018; Jain et al., 2020)—have annotated\ncorpora from NLP, computer science, or similar\nengineering-oriented ﬁelds. As such their annota-\ntion schemas have emphasized the description of\nhow research was carried out, by extracting cate-\ngories of entities such as methods, tasks, metrics,\nand datasets as well as relations mostly describing\ntheir intrinsic properties such as uses, composition,\nand hyponymy. Two of these datasets (Luan et al.,\n2018; Gábor et al., 2018) contain associative rela-\ntions that directly link entities being compared or\nproducing a result. Our work extends further in\nthis direction by examining not only which enti-\nties are associated, but also how the presentation\nof the associations is nuanced by the assertion of\nﬁne-grained attributes such as causality or propor-\ntionality.\nSciClaim provides the largest number of ﬁne-\ngrained label types among comparable datasets.\nTable 1 shows SciClaim’s remarkable label den-\nsities per word. SciClaim also contains 81.88%\nas many total labels as SciERC and more total la-\nbels than SemEval 2017 task 10 and 2018 task\n7. On the other hand, SciREX utilizes distant su-\npervision from an existing knowledge base and\nnoisy automatic labeling trained on SciERC to pro-\nvide an order of magnitude more labels and anno-\ntate complete documents. This is one example of\nhow smaller yet more densely and directly labeled\ndatasets like SciERC and SciClaim can enable and\ncompliment larger, higher-level corpora.\nMeanwhile, our dataset also focuses on scien-\ntiﬁc claims. Some previous work identiﬁes claims\nwithin scientiﬁc texts (Wadden et al., 2020; Gel-\nman et al., 2021), but does not extract the relations\nand factors within the claims themselves. Other\nrecent symbolic semantic NLP systems do model\nrelational representations of scientiﬁc claims (e.g.,\nFriedman et al., 2017), but these approaches rely\non rule-based engines with hand tuning, which re-\nquire NLP experts to maintain and adapt to new\ndomains. Instead we modify SpERT (Eberts and\nUlges, 2020), a transformer-based method that has\nbeen shown to effectively extract relational scien-\ntiﬁc information on SciERC (Luan et al., 2018).\nWe extend this model to accommodate our ad-\nditional multi-label attributes and apply it to our\nclaim graph extraction task.\n3 Approach\n3.1 Problem Deﬁnitions\nSciClaim deﬁnes the multi-attribute knowledge\ngraph extraction task as follows: for a sentence\nSof ntokens s1,...,s n, and sets of entity types Te,\nattribute types Ta, and relation types Tr, predict the\nset of entities ⟨sj,sk,t ∈Te⟩∈E ranging from\ntokens sj to sk, where 1 ≤j ≤k ≤n, the set of\nrelations over entities ⟨ehead ∈E ,etail ∈E ,t ∈\nTr⟩∈R where ehead ̸= etail, and the set of at-\ntributes over entities ⟨e ∈E,t ∈Ta⟩∈A . This\ndeﬁnes a directed multi-graph without self-cycles,\nwhere each unique span can be represented by at\n4653\nEntities Relations Attributes/Corefs Total Labels\nDataset Words Count Per Word Count Per Word Count Per Word Count Per Word\nSciREX 2512806 157680 6.27% 9198 0.37% - - 166878 06.64%\nSemEval2017 84010 9946 11.84% 672 0.79% - - 10618 12.64%\nSemEval2018 58144 7483 12.87% 1595 2.74% - - 9078 15.61%\nSciERC 65334 8089 12.38% 4716 7.21% 2752 4.21% 15557 23.81%\nSciClaim 20070 5548 27.64% 5346 26.64% 1844 9.19% 12738 63.47%\nTable 1: Our SciClaim dataset contains the highest label densities per word and comparable label counts to other\nscientiﬁc information extraction datasets except SciREX, which uses distant supervision and noisy automatic la-\nbeling. Our dataset contains ﬁne-grained attributes as additional labels, while SciERC contains coreference links.\nmost one entity node with zero to |Ta|attributes.\n3.2 Dataset Construction\nTo create SciClaim, two NLP researchers annotated\n901 sentences from several sources: 336 from pa-\npers in Social and Behavior Science (SBS) with ex-\npert identiﬁed claims (Alipourfard et al., 2021), 411\nﬁltered for causal language in PubMed papers (Yu\net al., 2019), 135 containing claims and causal lan-\nguage identiﬁed from CORD-19 abstracts (Wang\net al., 2020) with heuristic keywords, and 19 man-\nual perturbations included only in training data.\nTo aid in the labeling of these densely anno-\ntated sentences, we iteratively trained on already\ncollected data and utilized the predictions of the\npartially trained model on new training examples as\nsuggestions in our labeling interface. We disabled\nthese model suggestions on our 100 example test\nset to ensure that this did not bias our evaluation.\nDue to the dense and potentially overlapping\nspan annotations, small decisions about what to-\nkens to include in a span frequently inﬂuence the\nspan boundaries of several other entities in a sen-\ntence. However, most of these decisions have neg-\nligible impact on the meaningfulness of the anno-\ntation (e.g. the decision to include a determiner\nin span), rendering exact match agreement ineffec-\ntive. Instead to promote consistency and domain\nrelevance we employed iterative schema design\nsessions in consultation with a subject matter ex-\npert in reproducibility of SBS experiments and a\nprocess of consensus, schema re-development, and\nre-annotation on 250 examples where annotators\noverlapped.\nTable 1 contrasts SciClaim’s label counts and\ndensity with the other relational scientiﬁc informa-\ntion extraction datasets discussed in Section 2, and\nprecise counts for each label type are provided in\nTable 3. Further details are in Appendix A.\n3.3 Graph Schema\nThe SciClaim graph schema is designed to cap-\nture associations between factors (e.g., causation,\ncomparison, prediction, statistics, proportionality),\nmonotonicity constraints across factors, epistemic\nstatus, subtypes, and high-level qualiﬁers.\nText: Compared to control group , the isolated species from T2DM group had higher proteinase activity .\ncontrol group\nFactor\nthe isolated species from T2DM group\nFactor\nproteinase activity\nFactor\nq+\nhigher\nAssociation (Comparison, Sign+)\ncomp_to\narg0\narg1\nInput: “Compared to control group, the isolated species \nfrom T2DM group had higher proteinase activity.”\nFigure 3: Comparison attributes modify arguments to\naccount for a (sometimes implicit) frame of reference.\nEntities are labeled text spans. The same ex-\nact span cannot correspond to more than one en-\ntity type, but two entity spans can overlap. Enti-\nties comprise the nodes of SciClaim graphs upon\nwhich attributes and relations are asserted. Our\nschema includes six entity types: Factors are vari-\nables that are tested or asserted within a claim (e.g.,\n“sleep quality” in Figure 1). Associations are ex-\nplicit phrases associating one or more factors (e.g.,\n“higher” Figure 3). Magnitudes are modiﬁers of\nan association indicating its likelihood, strength,\nor direction (e.g., “signiﬁcantly” and “negatively”\nin Figure 1). Evidence is an explicit mention of\na study, theory, or methodology supporting an as-\nsociation (e.g., “ our SEIR model ”). Epistemics\nexpress the belief status of an association, often\nindicating whether something is hypothesized, as-\nsumed, or observed (e.g., “predicted” in Figure 2).\nQualiﬁers constrain the applicability or scope of\nan assertion (e.g., “for medical staff ” in Figure 1).\nAttributes are multi-label ﬁne-grained annota-\ntions (visualized in parentheses), where zero or\nmore may apply to any given entity. Our schema\nincludes the following attributes, all of which ap-\nply solely to Association entities: Causation ex-\n4654\npresses cause-and-effect over its constituent factors\n(e.g., both “increase” spans in Figure 2). Correla-\ntion expresses interdependence over its constituent\nfactors (e.g., both “associated with” spans in Fig-\nure 1). Comparison expresses an association with\na frame of reference (as in the “higher” statement\nof Figure 3). Sign+ and Sign- expresses high/low\nor increased/decreased factor value (e.g., “ corre-\nlates more closely with ” or “ shortened” respec-\ntively). Test expresses statistical measurements\n(e.g., “ANOVA”). Indicates expresses a predictive\nrelationship (e.g., “prognostic factors for”).\nRelations are directed edges between labeled\nentities in SciClaim graphs. They are critical for\nexpressing what-goes-with-what over the set of en-\ntities. Note that in Figures 1 and 2 the unlabeled\narrows are all modiﬁer relations, left blank to avoid\nclutter. We encode six relations: arg0 relates an\nassociation to its cause, antecedent, subject, or inde-\npendent variable (e.g., “levels of social support” in\nFigure 1). arg1 relates an association to its result or\ndependent variable (e.g., “self-efﬁcacy” and “stress”\nin Figure 1). comp_to is an explicit frame of ref-\nerence in a comparative association (e.g., “control\ngroup” in Figure 3). subtype relates a head en-\ntity to a subtype tail (e.g., “stillbirth” as a subtype\nof “pregnancy outcome”). modiﬁer relates asso-\nciations to qualiﬁers, magnitudes, epistemics, and\nevidence (e.g., all unlabeled arrows in Figure 1 and\nFigure 2). q+ and q- indicate positive and nega-\ntive qualitative proportionality, respectively, where\nincreasing the head factor increases or decreases\nthe tail factor, respectively (e.g., “levels of social\nsupport” is q+ to “sleep quality” and q- to “stress”\nin Figure 1).\n3.4 Model Architecture\nIn order to model the additional multi-label task\nin SciClaim, we extend SpERT (Eberts and Ulges,\n2020) with an attribute classiﬁer. SpERT provides\ncomponents (Figure 4 a–c) for joint entity and rela-\ntion extraction and permits the overlapping spans in\nour data. These classiﬁers utilize a span represen-\ntation that combines the SciBERT (Beltagy et al.,\n2019) contextual embeddings of all tokens in the\nspan through maxpooling, along with a context rep-\nresentation and learned width embedding. SpERT\nclassiﬁes entities ﬁrst and only infers relations on\npairs of identiﬁed entities.\nInstead of maxpool we adopt an attention-based\nspan representation (Figure 4 e) inspired by Lee\nSciBERT (ﬁne-tuned)\nentity\nclassiﬁer\n(a) entity\nclassiﬁcation\n(d) attribute classiﬁcation\n(b) span \nﬁltering\n(c) relation\nclassiﬁcation\nwidth\nembeddings\n1\n2\n3\n4\n5\n…\n(entitiy) (entitiy) (no entitiy)\nx\n[CLS]\nSpERT\n(Eberts and Ulges, 2020)\nNew \nAddition\nattribute\nclassiﬁer\nrelation\nclassiﬁer\nCandidate Span Context Span\nKey\nmaxpool[CLS] width\nembed\nα = softmax( Hw + b ) \nattention attention attention\nĥ = HTα\n(e) attention-based spans\nFigure 4: Our extension of SpERT components (a,\nb, and c) with multi-label attributes (d) and attention-\nbased entity span representations (e).\net al. (2017). This produces scalars αi,t for each\nSciBERT token vector ht in a span iusing learned\nparameters w and b:\nαi,t = exp(w ·ht + b)\n∑END (i)\nk=START (1) exp(w ·hk + b)\n(1)\nThese attention weights are used to make a span\nrepresentation ˆhi with the following weighted sum:\nˆhi =\nEND (i)∑\nt=START (1)\nαi,tht (2)\nWe use the same cascaded inference strategy\nand input the span representations of identiﬁed en-\ntities xa to an attribute classiﬁer (Figure 4 d) with\nweights Wa and bias ba. A pointwise sigmoid\nσ yields seperate conﬁdence scores ˆ ya for each\nattribute:\nˆ ya = σ(Waxa + ba) (3)\nWe train the attribute classiﬁer with a binary cross\nentropy loss La summed with the SpERT entity\nand relation losses, Le and Lr, for a joint loss:\nL= Le + Lr + La (4)\n4 Evaluation\nIn Table 2 we report micro performance metrics on\nthe SciClaim test set averaged over 5 runs.\nIn addition to the modiﬁed SpERT (detailed in\nSection 3.4), we also test a variant attrs-as-ents\n4655\nEntities Attributes Relations\nData Model P R F1 P R F1 P R F1\nSciERC SpERT 70.87 69.79 70.33 - - - 53.40 48.54 50.84\nSciClaim SpERT-attrs-as-ents 90.13 88.63 89.37 92.35 82.13 86.94 77.59 74.34 75.92\nSciClaim SpERT-modiﬁed 89.81 87.87 88.83 91.89 82.62 87.01 76.43 73.72 75.05\nTable 2: Micro Precision, Recall, and F1 averaged over 5 runs on SciClaim with SciERC for comparison.\nLabel P R F1 S\nEntities\nfactor 91.28 89.97 90.62 2756\nevidence 88.80 93.33 90.96 230\nepistemic 91.21 72.17 80.52 299\nassociation 92.45 88.20 90.27 1290\nmagnitude 87.71 88.38 88.02 613\nqualiﬁer 75.86 78.33 77.02 360\nAttributes\ncausation 38.15 60.00 46.20 342\ncomparison 86.69 80.00 83.19 329\nindicates 84.79 76.25 80.20 84\nsign+ 97.27 88.31 92.58 542\nsign- 91.97 72.86 81.28 202\ncorrelation 98.42 84.41 90.88 320\nRelations\narg0 79.53 75.03 77.19 1325\narg1 79.92 77.57 78.71 1384\ncomp_to 65.86 60.00 62.78 187\nmodiﬁer 77.71 76.21 76.95 1582\nsubtype 40.00 33.33 36.00 156\nq+ 65.53 67.61 66.50 504\nq- 70.70 53.00 60.09 208\nTable 3: High Precision, Recall, and F1 across most\ntypes relative to total Support in SciClaim, using\nSpERT-modiﬁed averaged over 5 runs.\nwhere all attribute labels on an entity span are col-\nlapsed into a single combined annotation, allowing\nunmodiﬁed SpERT to process attributes. Precisely,\nwe collapse Te entity types with all combinations of\nTa attribute types into {Te ×\n(Ta\nk\n)\n: 0≤k≤|Ta|}\nmulti-class entity labels. We hypothesized that the\ncombinatorially larger number of labels required\nby attrs-as-ents would lower performance on rarely\noccurring combinations. Surprisingly the variants\nget almost identical results, suggesting that—at\nleast for our data—a single layer classiﬁer can in-\nfer the attributes of a span simultaneously just as\nwell as doing so independently. We tested other\nmodel variants that also produced changes∼1% F1\nand thus are relegated to Appendix B.\nTo our knowledge no previous models exists that\ncan run directly on all three tasks in our dataset due\nto the presence of both overlapped entity spans and\nmulti-label attributes. For comparison we include\nSpERT’s state-of-the-art performance on SciERC,\nthe dataset closest to ours in terms of label density.\nThe high performance of our adapted SpERT on\nSciClaim demonstrates the practicality of extract-\ning our novel graph schema with present methods\ndespite its ﬁne-grained approach.\nThe per-class evaluations for our main model are\nreported in Table 3. With few exceptions perfor-\nmance is good, and generally follows support for\nthe label in the dataset. The Causation attribute\nmetrics may be inﬂuenced by noise from anoma-\nlously low representation in the test set (only 5\ninstances compared to 59 instances of Correlation).\nLikewise the Test attribute unfortunately does not\nappear in the test set at all, but receives valida-\ntion F1 of 95.95% despite only appearing 25 times\nin the corpus. Another outlier, the subtype rela-\ntion, is particularly challenging, especially with\nits low rate of occurrence, due to it being one of\nthe few relation types occurring directly between\nfactors rather than mediated through a reiﬁed as-\nsociation span. The q+/q- relations are likewise\nexpressed as direct links between factors. Although\nthese require complex reasoning about the quali-\ntative proportionalities of factors (e.g., Figure 2),\nthey nonetheless receive promising results. The\nattributes Sign+/Sign- serve a similar role and pro-\nvide partial redundancy for q+/q- labels, allowing\ndownstream reasoning to back off to these less pre-\ncise, more robust attributes when the qualitative\nproportionalities are not extracted.\n5 Conclusion\nPrevious scientiﬁc information extraction crafts\nuseful high-level representation of papers, going\nas far as document level relations spanning thou-\nsands of words in Jain et al. (2020). Complimen-\ntary to these efforts, we propose ﬁne-grained and\ndensely annotated scientiﬁc information extraction\nthat captures not just what is said but how it is pre-\nsented and argued. SciClaim applies this approach\nto associative claims and demonstrates that existing\nmodels such as SpERT (Eberts and Ulges, 2020)\ncan be modiﬁed to accurately extract ﬁne-grained\nknowledge graphs ripe for downstream reasoning.\n4656\nAcknowledgements\nThis material is based upon work supported by\nthe Defense Advanced Research Projects Agency\n(DARPA) and Army Research Ofﬁce (ARO) un-\nder Contract No. W911NF-20-C-0002. Any opin-\nions, ﬁndings and conclusions or recommenda-\ntions expressed in this material are those of the\nauthor(s) and do not necessarily reﬂect the views of\nthe Defense Advanced Research Projects Agency\n(DARPA) and Army Research Ofﬁce (ARO). We\nthank the reviewers for their helpful feedback.\nReferences\nNazanin Alipourfard, Beatrix Arendt, Daniel M Ben-\njamin, Noam Benkler, Michael M Bishop, Mark\nBurstein, Martin Bush, James Caverlee, Yiling Chen,\nChae Clark, and et al. 2021. Systematizing conﬁ-\ndence in open research and evidence (score).\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel,\nLakshmi Vikraman, and Andrew McCallum. 2017.\nSemEval 2017 task 10: ScienceIE - extracting\nkeyphrases and relations from scientiﬁc publica-\ntions. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 546–555, Vancouver, Canada. Association for\nComputational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nMarkus Eberts and Adrian Ulges. 2020. Span-based\njoint entity and relation extraction with transformer\npre-training. 24th European Conference on Artiﬁ-\ncial Intelligence.\nScott Friedman, Mark Burstein, David McDonald,\nAlex Plotnick, Laurel Bobrow, Rusty Bobrow, Brent\nCochran, and J Pustejovsky. 2017. Learning by read-\ning: Extending and localizing against a model. Ad-\nvances in Cognitive Systems, 5:77–96.\nScott E. Friedman, Ian H. Magnusson, and Sonja M.\nSchmer-Galunder. 2021a. Extracting qualitative\ncausal structure with transformer-based nlp. In\nQR2021 @ IJCAI.\nScott E. Friedman, Ian H. Magnusson, Sonja M.\nSchmer-Galunder, Ruta Wheelock, Jeremy Gottlieb,\nPooja Patel, and Christopher Miller. 2021b. To-\nward Transformer-Based NLP for Extracting Psy-\nchosocial Indicators of Moral Disengagement. In\nAnnual Meeting of the Cognitive Science Community\n(CogSci).\nKata Gábor, Davide Buscaldi, Anne-Kathrin Schu-\nmann, Behrang QasemiZadeh, Haïfa Zargayouna,\nand Thierry Charnois. 2018. SemEval-2018 task\n7: Semantic relation extraction and classiﬁcation in\nscientiﬁc papers. In Proceedings of The 12th Inter-\nnational Workshop on Semantic Evaluation , pages\n679–688, New Orleans, Louisiana. Association for\nComputational Linguistics.\nBen Gelman, Chae Clark, Scott Friedman, Ugur Kuter,\nand James Gentile. 2021. Toward a robust method\nfor understanding the reproducibility and replicabil-\nity of research. AAAI Workshop on Scientiﬁc Docu-\nment Understanding.\nSarthak Jain, Madeleine van Zuylen, Hannaneh Ha-\njishirzi, and Iz Beltagy. 2020. SciREX: A chal-\nlenge dataset for document-level information extrac-\ntion. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 7506–7516, Online. Association for Compu-\ntational Linguistics.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 188–197, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3219–3232, Brussels, Bel-\ngium. Association for Computational Linguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verify-\ning scientiﬁc claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7534–7550, On-\nline. Association for Computational Linguistics.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Doug Burdick, Dar-\nrin Eide, Kathryn Funk, Yannis Katsis, Rodney\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\nPaul Mooney, Dewey Murdick, Devvret Rishi, Jerry\nSheehan, Zhihong Shen, Brandon Stilson, Alex\nWade, Kuansan Wang, Nancy Xin Ru Wang, Chris\nWilhelm, Boya Xie, Douglas Raymond, Daniel S.\nWeld, Oren Etzioni, and Sebastian Kohlmeier. 2020.\nCord-19: The covid-19 open research dataset.\nBei Yu, Yingya Li, and Jun Wang. 2019. Detecting\ncausal language use in science ﬁndings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4664–4674, Hong\nKong, China. Association for Computational Lin-\nguistics.\n4657\nEntities Attributes Relations\nModel Avg Val F1 P R F1 P R F1 P R F1\nSpERT-attrs-as-ents 80.45 90.13 88.63 89.37 92.35 82.13 86.94 77.59 74.34 75.92\nSpERT-modiﬁed 80.66 89.81 87.87 88.83 91.89 82.62 87.01 76.43 73.72 75.05\nSpERT-modiﬁed-maxpool 80.22 90.32 88.54 89.42 92.00 80.90 86.09 76.11 75.92 75.99\nSpERT-modiﬁed-unﬁltered 79.99 89.28 88.03 88.64 91.65 80.74 85.84 75.62 73.98 74.78\nTable 4: Micro Precision, Recall, and F1 averaged over 5 runs on the SciClaim test set as well as F1 averaged over\nthe 3 tasks on 5 runs of SciClaim validation data (Avg Val F1).\nA Claims Dataset\nOur English language dataset SciClaim consists of\n901 examples sentences divided into a training set\nof 721 sentences, a validation set of 80 sentences,\nand a test set of 100 sentences. The training and\nvalidation data contain examples that were labeled\nfrom corrected suggestions from a partially trained\nmodel, while the test set was labeled from scratch\nwithout any model suggestions as a starting point.\nOur data from CORD-19 (Wang et al., 2020) is\nsampled with the following keywords as a heuristic\nidentiﬁcation of claims and causal language simi-\nlar to our expert identiﬁed data from PubMed and\nSocial and Behavioral Science (SBS) papers: as-\nsociated with, reduce, increase, leads to, led to,\nour result, greater, less, more, cause, demonstrate,\nshow, improve.\n200 of our sentences (100 from PubMed and\n100 from SBS) were selected to intentionally mini-\nmize the likelihood of claims and causal language.\nThis includes sentences that discuss factors and\nother entities present in our schema but either do\nnot contain associations or frame associations in\nunusual ways such as rhetorical questions. We\nintend for this data to encourage robustness that\nmaintains correct labels for partial graph extrac-\ntions rather than simply hallucinating associations\nin all sentences. We employ the following heuris-\ntics to identify this data: We sample 100 PubMed\nsentences from Yu et al. (2019) that are identiﬁed\nas having low causal content. We sample 50 ti-\ntles from SBS paper present in Alipourfard et al.\n(2021), as titles contain factors but rarely contain\nexplicit associations and may be present in input\ndata from automatically extracted text from PDFs.\nFinally we sample 50 ﬁrst lines of SBS papers from\nAlipourfard et al. (2021), as these lines frequently\nintroduce topics or rhetorical questions which ei-\nther lack associations or present highly hypotheti-\ncal associations unlike those in our main corpus.\nEach ﬁltered data source was sampled chrono-\nlogically.\nWe utilized the following procedure for label-\ning: The annotators undertook extensive, iterative\nschema design sessions in consultation with a sub-\nject matter expert in reproducibility of SBS exper-\niments. After the schema was settled on pilot ex-\namples, a lead annotator established the annotation\nstandards on several hundred examples through\na process of relabeling and retraining the sugges-\ntion model. Once the suggestion model became\neffective, the lead annotator and model suggestions\nguided the other annotator in adopting the annota-\ntion standards. The lead annotator reviewed and\ncorrected the 250 overlapping examples in a con-\nsensus process with the other annotator.\nB Variants and Hyperparameters\nB.1 Variants\nWe experiment with several variants none of which\nsubstantially outperformed the others. SpERT-\nmodiﬁed-maxpool contains our modiﬁcations but\nsimply uses SpERT’s original maxpooling span\nrepresentation instead of the attention-based repre-\nsentations inspired by Lee et al. (2017). SpERT-\nmodiﬁed-unﬁltered forgoes cascading inferences\nand instead classiﬁes all possible spans for at-\ntributes. Full test and averaged validation results\nare presented in Table 4.\nB.2 Hyperparameters\nThe following hyperparameters and settings were\nselected using manual tuning of 10-fold cross val-\nidation on the training set and optimizing for av-\nerage micro-f1 performance over all 3 tasks: lan-\nguage model SciBERT uncased, mini batch size\n8, epochs 40, optimizer AdamW, linear schedul-\ning, warm up 0.05, learning rate 5e-5, learning rate\nwarm up 0.1, weight decay 0.01, max grad norm\n1.0, size embedding dimension 25, dropout prob-\nability 0.1, maximum span size 20, attribute ﬁlter\nthreshold 0.55, relation ﬁlter threshold 0.4.\n4658\nWe ran 32 trials on 5 RTX 2080 ti GPUs, where\neach trial takes roughly 20 minutes. Our model\ncontains 110 million parameters.\nWe explored the following hyperparameter\nbounds: language model ∈{BERT, SciBERT,\nSpanBERT, SciBERT tuned on SciERC }, epochs\n∈{20, 40, 80}, batch size ∈{4, 8, 16}, learning\nrate ∈{1e-5, 5e-5, 1e-4}, scheduling ∈{linear,\ncyclic }, warm up ∈{0.0, 0.05, 0.1, 0.15. 0.2,\n0.25, 0.3}, attribute ﬁlter threshold ∈{0.4, 0.5,\n0.55, 0.6}, relation ﬁlter threshold ∈{0.35, 0.4,\n0.5, 0.6}. The remaining settings we inherit from\nSpERT as initial experimentation on early datasets\nrevealed little impact.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7523926496505737
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.593241810798645
    },
    {
      "name": "Knowledge graph",
      "score": 0.5893939733505249
    },
    {
      "name": "Relationship extraction",
      "score": 0.4721158742904663
    },
    {
      "name": "Knowledge extraction",
      "score": 0.4600841999053955
    },
    {
      "name": "Transformer",
      "score": 0.45993900299072266
    },
    {
      "name": "Annotation",
      "score": 0.44343990087509155
    },
    {
      "name": "Information retrieval",
      "score": 0.41741734743118286
    },
    {
      "name": "Data science",
      "score": 0.4034690856933594
    },
    {
      "name": "Data mining",
      "score": 0.3941825330257416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34290021657943726
    },
    {
      "name": "Information extraction",
      "score": 0.3408347964286804
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210103432",
      "name": "Smart Information Flow Technologies (United States)",
      "country": "US"
    }
  ]
}