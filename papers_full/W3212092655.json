{
  "title": "Unsupervised Multi-View Post-OCR Error Correction With Language Models",
  "url": "https://openalex.org/W3212092655",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5111378101",
      "name": "Harsh K. Gupta",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A5001075207",
      "name": "Luciano Del Corro",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A5010181302",
      "name": "Samuel Broscheit",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A5023996418",
      "name": "Johannes Hoffart",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A5082296086",
      "name": "Eliot Brenner",
      "affiliations": [
        "University of Mannheim"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798485145",
    "https://openalex.org/W2122410182",
    "https://openalex.org/W1771665122",
    "https://openalex.org/W2756610685",
    "https://openalex.org/W3012957048",
    "https://openalex.org/W2807055569",
    "https://openalex.org/W3046591412",
    "https://openalex.org/W3003484198",
    "https://openalex.org/W1975679321",
    "https://openalex.org/W3102783139",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2741223475",
    "https://openalex.org/W2786672397",
    "https://openalex.org/W4256120941",
    "https://openalex.org/W2002006695",
    "https://openalex.org/W2130200371",
    "https://openalex.org/W2063828767",
    "https://openalex.org/W4237342689",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W2069189382",
    "https://openalex.org/W4312290170",
    "https://openalex.org/W2623293810",
    "https://openalex.org/W3157745755"
  ],
  "abstract": "We investigate post-OCR correction in a setting where we have access to different OCR views of the same document. The goal of this study is to understand if a pretrained language model (LM) can be used in an unsupervised way to reconcile the different OCR views such that their combination contains fewer errors than each individual view. This approach is motivated by scenarios in which unconstrained text generation for error correction is too risky. We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view. We also show the importance of domain adaptation for post-OCR correction on out-of-domain documents.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8647–8652\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n8647\nUnsupervised Multi-View Post-OCR Error Correction\nWith Language Models\nHarsh Gupta† Luciano Del Corro† Samuel Broscheit‡∗ Johannes Hoffart† Eliot Brenner†\n†Goldman Sachs\n{Harsh.Gupta, Luciano.DelCorro, Johannes.Hoffart, Eliot.Brenner} @ gs.com\n‡University of Mannheim\nbroscheit@informatik.uni-mannheim.de\nAbstract\nWe investigate post-OCR correction in a set-\nting where we have access to different OCR\nviews of the same document. The goal of this\nstudy is to understand if a pretrained language\nmodel (LM) can be used in an unsupervised\nway to reconcile the different OCR views such\nthat their combination contains fewer errors\nthan each individual view. This approach is\nmotivated by scenarios in which unconstrained\ntext generation for error correction is too risky.\nWe evaluated different pretrained LMs on two\ndatasets and found signiﬁcant gains in realistic\nscenarios with up to 15% WER improvement\nover the best OCR view. We also show the im-\nportance of domain adaptation for post-OCR\ncorrection on out-of-domain documents.\n1 Introduction\nDigital scans of printed paper are still one of the\nmain sources of digitized text across industries, li-\nbraries and governmental organizations. Scanned\ndocuments need to be processed by Optical Char-\nacter Recognition (OCR) systems in order to be\nconsumed by natural language processing (NLP)\npipelines. Unfortunately, OCR errors are perva-\nsive and input noise can severely hinder down-\nstream NLP applications, e.g., search (Stein et al.,\n2012), neural machine translation (Belinkov and\nBisk, 2018) or NLU in general (Kumar et al., 2020).\nPrior work used text generation techniques or\nredundancy in similar passages for OCR error cor-\nrection, which is not appropriate in cases of low\ncorpus redundancy or weak document contextual\ninformation. For example, this may pose a risk\nin sensitive documents in the legal or ﬁnancial do-\nmain, where documents tend to be templatized and\nspeciﬁc information, such as legal entities or num-\nbers (e.g., interest rates or amounts), are speciﬁc\nto single documents and cannot be safely inferred.\nOther prior work uses multiple OCR views of a\n∗ work done during an internship at Goldman Sachs\nOCR1: Total 0 ft suppl ies : 23 .64\nOCR2: Total 0 % suppl tea .3 .64\nReconciled: Total 0 % suppl ies : 23 .64\nFigure 1: Example ICDAR dataset. Output from two\npopular OCR systems (OCR 1 and OCR2) and the rec-\nonciled version generated by our approach.\ndocument and reconciles them in a supervised way,\nrequiring expensive and difﬁcult to acquire training\ndata.\nTherefore, we investigated post-OCR correction\nwith multiple OCR views on the same document\nin an unsupervised way. A key assumption of this\nwork is that different OCR views make mistakes in\ndifferent parts of the input document. To create a\nbetter OCR view from multiple OCR inputs we use\na language model (LM) to pick the most probable\nreconciliation. See Figure 1 for an example. The\nkey question of this study is if it is possible to\nuse a LM to reconcile the OCR views such that\ntheir combination contains fewer errors than the\nindividual views. The advantage of the proposed\napproach is that (i) it does not require supervision,\nand that (ii) it is well suited for risky scenarios.\nWe explore two different settings: (i) using an\noff-the-shelf pre-trained LM (i.e., GPT/2 family\nand a n-gram model), and (ii) domain adaptation of\nthe LM. We evaluated our approach on two datasets\nthat we adapted for our experiments 1. The RE-\nTAS dataset (Yalniz and Manmatha, 2011), consist-\ning of 20 English books with a total of 100 OCR\nviews, and the more challenging ICDAR Scanned\nReceipts dataset (Huang et al., 2019), with 625\nscanned receipts. Our results indicate that the pro-\nposed unsupervised approach is able to improve\nover the individual OCR systems. On RETAS we\nmeasured a 8% gain over the best OCR view and\non ICDAR a gain of 15% over the best OCR view.\nWe also found that domain adaptation is crucial for\n1https://github.com/HarshGupta11/ocr_correction_reﬁner\n8648\ndocuments from domains distinct from the LM’s\ntraining data, as we can show that domain adapta-\ntion improved the error rates on critical numerical\ndata.\n2 Related Work\nMulti-Input post-OCR correction.Using ensem-\nble methods and voting schemas across multiple\ninputs are well established strategies for post-OCR\nerror processing (Lopresti and Zhou, 1997; Yama-\nzoe et al., 2011; Lund et al., 2013; Xu and Smith,\n2017; Dong and Smith). The inputs may come\nfrom different views of the same document (Lo-\npresti and Zhou, 1997; Lund et al., 2013) or from\ncorpus redundancy using similar passages on a\nlarge corpora (Dong and Smith; Xu and Smith,\n2017). Here we follow the ﬁrst approach because\ncorpus redundancy may be risky in some settings.\nFor instance, in low variance documents, such as\nﬁnancial documents) the important information is\ndocument speciﬁc (e.g., amounts, interest rates,\nentities, etc.); extracting those values from other\nsimilar passages would lead to risky errors in the\ncrucial document bits.\nInput reconciliation.Earlier single-system multi-\ninput approaches were relying on multiple scans\nof the same documents (Lopresti and Zhou, 1997),\nor alterations to the original image to force (Lund\net al., 2013) alternative OCR outcomes. The recon-\nciliation was performed either by voting schemas\nor direct supervision. Even those systems relying\non document redundancy used supervision (Schulz\nand Kuhn, 2017; Dong and Smith) with manual\nor automatically generated training data. Unlike\nprevious methods we are the ﬁrst to use large LMs\nto reconcile the inputs in an unsupervised way. Our\nwork is in this aspect similar to (Xu and Smith,\n2017) as they used a character-based ngram LM in\naddition to the majority voting to decide among the\nsimilar passages, however they also rely on similar\npassages to generate redundancy.\nExplicit correction. Text generation is a stan-\ndard technique in post-OCR correction (Xu and\nSmith, 2017; Schulz and Kuhn, 2017; Amrhein and\nClematide, 2018; Richter et al., 2018; Dong and\nSmith; Lyu et al., 2021). Neural approaches, for\ninstance (Amrhein and Clematide, 2018; Dong and\nSmith; Nguyen et al., 2020; Lyu et al., 2021) use\nan encoder decoder architecture which takes the\nOCR’ed text as input and generates the corrected\nversion. In our scenario these strategies can lead\nto severe problems. For instance, in the case of\nnumerical values, contextualized LMs will proba-\nbly be able to generate a numerical value, but this\nvalue will be arbitrary in the context of the speciﬁc\ndocument (e.g., 3,50%, 5%, etc.). An unreadable\nnumber is in this case better than a wrongly read-\nable one as the error will be more easily visible for\nboth humans and information extraction systems.\n3 Background\n3.1 Post-OCR error correction\nPost-OCR error correction is the task of correcting\nthe errors generated during the OCR process. It\ninvolves two main challenges: (i) the detection of\nthe errors in text, and (ii) the correction of those\nerrors. In general, the ﬁrst challenge is non-trivial.\nFor example, if there is only a single OCR input\nto the correction module, OCR errors might not be\nobvious corruptions (e.g., OCR2 in Figure 1) which\ncan result in text at least superﬁcially readable, so\neven the location of the error is not obvious.\nBy using multiple OCR inputs of the same docu-\nment this challenge can be sidestepped. Thus, in-\nstead of solving error detection we only have to ad-\ndress the much more specialized problem of deter-\nmining the differences between multiple versions\nor “views\". Spotting the differences is an estab-\nlished problem, with multiple techniques available,\nsuch as Yalniz and Manmatha (2011).\n3.2 Language Models\nIn general, alanguage model (LM) may refer to any\nparameterized method of assigning a probability to\na sequence t1,...,t k of tokens:\np(t1,t2,...,t k) =\nk∏\nn=1\np(tn|t1 ...t n−1)\nIn what follows we use a normalized form of\nthe probability known as perplexity, deﬁned as\nthe inverse of the k-th root of the probability of\nthe sequence. Note that we can distinguish two\ntypes of LMs, n-gram and neural, which are distin-\nguished by whether they estimate the probability\nfactor p(tk|t1 ...t k−1) by simple statistical meth-\nods or by a neural network. For the analysis in this\nwork we use 3 models of the GPT (Radford and\nNarasimhan, 2018; Radford et al., 2019) family,\nplus a 3-gram model trained on Wikipedia.\n8649\n4 Proposed Approach\nOur approach focuses on a setting with multiple\nOCR views where the challenge is to pick the best\nsegments of each view where the aligned OCR in-\nputs differ. Given two OCR views of the same\ndocument our approach consists of the following\nsteps: (i) align the OCR outputs and spot differ-\nences, and then (ii) score the different choices of\nthose differences to pick the best solution.\nStep 1: Spot differences. Suppose we have\ntwo sequences S1 = c1,1,...,c 1,n and S2 =\nc2,1,...,c 2,n, where ci,j is the j-th character of se-\nquence i. Further we denote di,j as the j-th chunk\nin sequence iwhich is not present in the other se-\nquence, while ei is a chunk shared by both inputs.\nFinding the longest common subsequence be-\ntween S1 and S2 is equivalent to ﬁnding the\nshortest edit script (Myers, 1986). Denote the\nshortest edit script by Diﬀ(S1,S2). For exam-\nple, suppose that it takes the form Diﬀ(S1,S2) =\ne1[d1,1,d2,1]e2[d1,2,d2,2]e3. [d1,j,d2,j] indicates\nthat to transform S1 into S2, one must delete d1,j\nand insert d2,j. In the example the number of dif-\nfering chunks length(Diﬀ(S1,S2)) = 2.\nEach difference yields a binary choice to either\npick d1,j or d2,j. All possible outputs that can be\nproduced from the pair of inputs (S1 and S2) is the\nset of all root-leaf paths in a binary tree.\nStep 2: Score interpretations.We score all of the\npossible root-leaf paths with an LM’s perplexity\nand take the argmin. If followed naively, this leads\nto an exponential computational complexity in the\nheight of the tree. Therefore we adopt beam search\nwith beam width β(Russell and Norvig, 2002).\n5 Experiments\nThe goal of the experiments is to understand if the\nproposed approach is able to generate a combined\nOCR view which contains fewer errors than the\nindividual inputs.\n5.1 Datasets\nWe have the following requirements: (i) We need\nmultiple OCR views of the same document and (ii)\na ground truth for evaluation. As there were no\npublic datasets that meet those requirements we\nadapted the RETAS dataset (Yalniz and Manmatha,\n2011) and the more challenging ICDAR 2019 Com-\npetition on Scanned Receipt OCR (Huang et al.,\n2019). Other more standard datasets for post-OCR\ncorrection (Chiron et al., 2017; Rigaud et al., 2019)\nwere designed for text correction approaches, and\nwere not suitable for our setup.\nRETAS Originally created for text alignment. We\nused 16 English books with 96 OCR views in total.\nThe ground truth for each book comes from the\nGutenberg Project and since the OCR views are\ngenerated from different editions of the book there\nare alignment miss matches that we solved in the\nfollowing way: (i) we divided the ground truth in\nchunks of 200 characters, (ii) we discarded those\nchunks for which the aligned OCR views have a\nlength discrepancy of more than 10%, (iii) we re-\nmoved chunks across views with ground truths dis-\ncrepancies, and (iv) we removed chunks that were\nnot present across views. In total we discarded\naround 40% of the data. We made a 60:20:20 split\nfor domain adaptation, validation and test respec-\ntively. The datasets were split per book to avoid\nleakage into the test set. We use 9 books for domain\nadaptation, 3 for validation and 4 for testing.\nICDAR 2019 Scanned Receipts.It contains 625\nreceipt images. Each image is annotated with text\nbounding boxes and the transcript of each text. We\nextracted a total of 18,228 lines, out of which we\nhave perfect alignments between ground truth and\nthe two OCR systems for 15,905. The rest were\ndiscarded. To generate the different OCR views,\nwe processed the images with two popular OCR\nsystems and discarded those images not processed\nby either of the systems, resulting in 533 receipts.\nWe used 319 for domain adaptation, 107 for vali-\ndation, and 107 for testing, randomly sampled in\n5-fold cross validation.\n5.2 Experimental Setup\nModels. We use three autoregressive models: GPT,\nGPT2 and GPT2XL, and trained a 3-gram model on\nWikipedia as baseline. One tunable hyperparameter\nis the maximum number of tokens in the preﬁx and\nsufﬁx around the difference. We tuned this on the\nvalidation sets and used 50 characters on each side\nof the OCR difference for both datasets. For the\nbeam search we use beam search with β = 6.\nDomain adaptation.A typical use of neural LMs\nis to ﬁne-tune them in a NLP task. This often in-\nvolves two stages: unsupervised pre-training for\ndomain adaptation and supervised task ﬁne-tuning.\nFor OCR correction, we do not want to assume\nthere is any supervision, because that would require\na corpus of OCR’ed text and manually transcribed\ntext in the domain of interest. However, it can\n8650\nDataset b w GPT2GPT2-DAGPT2XL\nRETAS 3.74 7.36 3.43 3.50 3.45\nICDAR 201946.82102.2245.99 40.81 45.08\nTable 1: WER of each OCR view per Dataset, Best\n(b) and Worst (w), best WER LMs, with and without\ndomain adaptation (DA).\nModel Best+2nd Best+Worst Random\nb w b w b w\nwithout domain adaption\n3-gram -1% 49% -13% 43% -32% 33%\nGPT 5% 52% -6% 46% -14% 42%\nGPT2 8% 53% -4% 47% -10% 44%\nGPT2XL 8% 53% -4% 47% -10% 44%\ndomain adaptation\nGPT 3% 51% -8% 45% -21% 39%\nGPT2 6% 52% -2% 48% -11% 44%\nTable 2: RETAS. WER relative improvement over\nbest OCR view (b) and worst view (w). Combining\nBest+2nd, Best+Worst and iteratively combining all\nOCR views in random order (Random).\nbe realistic, depending on the domain, to expect a\nhigh quality in-domain text corpus. For example,\nthere might be a corpus of already electronically\navailable documents. Therefore we compare off-\nthe-shelf LMs — which were pre-trained only on\na standard web corpus — and LMs that we fur-\nther adapted on high-quality in-domain text. We\nuse 60% of the data in each case for the domain\nadaptation step.\nReported settings.For the RETAS dataset we gen-\nerated 3 settings, as each book can contain more\nthan one OCR view: (i) combining the two best\nviews, (ii) the best and the worst, and (iii) iteratively\ncombining all in a random order. For ICDAR we\nonly have 2 views. We analyze three scenarios: (i)\nnumerical data, (ii) non-numerical data, and (iii)\nfull data. We report the WER absolute results in\nTab. 1, and the improvements achieved by our sys-\ntem on RETAS and ICDAR in Tab. 2 and Tab. 3\nrespectively. The numbers indicate the improve-\nment over both the best (b) possible input view and\nover the worst (w) input view.\n5.3 Results\nRETAS. Tab. 2 shows that without domain adap-\ntation, when the two best views are combined\n(Best+2nd), results are positive with an improve-\nment of up to 8% with respect to the best view. For\nBest+Worst, the results deteriorate with respect to\nthe best view, but always improve with respect to\nModel Numeric Non-Numeric All\nb w b w b w\nwithout domain adaptation\n3-gram -23% 43% -21% 46% -23% 44%\nGPT -15% 47% -24% 45% -18% 46%\nGPT2 5% 56% -5% 53% 2% 55%\nGPT2XL 6% 57% -2% 54% 4% 56%\nwith domain adaptation\nGPT -2% 53% 4% 57% 0% 54%\nGPT2 15% 61% 8% 59% 13% 60%\nTable 3: ICDAR. WER relative improvement over best\nOCR view (b) and worst view (w). Results for only\nNumeric characters, Non-numeric characters and All.\nthe worst. The results for Random, in which all 96\navailable views are iteratively combined in a ran-\ndom order, the results are worse than Best+Worst.\nThis shows that this method would not automati-\ncally yield a good result without some prior selec-\ntion of good OCR systems.\nDomain adaptation did not improve here, and\nthere is even a general and slight deterioration of\nthe results. This is likely caused by the fact that as\nthe model was pre-trained on the same domain, it\noverﬁts some books without speciﬁc domain gains.\nICDAR 2019.This dataset is particularly challeng-\ning due to the quality of the images which leads\nto noisy OCR views. Also, unlike the text on RE-\nTAS, the context is organized in a receipt layout\nstructure, which does not necessarily ﬁt the autore-\ngressive generation assumption of the LMs. This\ndataset is signiﬁcantly more difﬁcult for OCR sys-\ntems, i.e., compared to the RETAS dataset Tab. 1\nshows an absolute WER more than 12 times larger\nfor the best OCR and almost 14 times larger for the\nworst OCR views. This means that the gains of any\ncorrection are very impactful.\nTable 3 shows that only the GPT2 model with\ndomain adaptation signiﬁcantly outperforms the\nrest over the best OCR view for numeric data. Non-\nnumerical data seems to be more challenging with\nonly domain adaptation settings generating an im-\nprovement, we conjecture that this is due to se-\nquences of symbolic characters that are common\nin the receipts. As in the RETAS dataset GPT2\nachieves the best performance\n5.4 Error Analysis\nOur method tends to make mistakes when the errors\noccur at the very beginning of the sequence, prop-\nagating them as the sequence advances. Looking\n8651\nat longer sequences via beam search has mitigated\nthis to a certain extent (although the computational\ncost limits the lookahead). We believe that this is\ncaused by the auto-regressive nature of the GPT\nmodels, which suggests that it might make sense to\nexplore the use of bidirectional models. This would\nimply a set of challenges outside the scope of this\nwork, such as a mechanism to score the sequence\nor a way to deal with OCR views with a different\nnumber of tokens.\nThe proposed approach also tends to fail when\nthe OCR quality of one of the underlying systems\nis poor. This can be seen in Table 2: the experi-\nments with random OCR views perform worse than\nthe setting with two views. Such an issue is also\npresent in the ICDAR dataset with just two views:\nwhenever one of the views has signiﬁcantly infe-\nrior quality, the reconciliation can be worse than\nthe best view. This happens in around 8% of the\ntest sequences and indicates that the quality of the\nunderlying OCRs is still relevant. Thus, a way to\nestimate the quality of the OCR views in advance\nwould be beneﬁcial.\n6 Conclusions and Future Work\nWe presented an approach for post-OCR correc-\ntions in an unsupervised way, relying on multiple\nOCR inputs and LMs for reconciliation. Our re-\nsults show that the approach consistently improves\nover the single best input. We also show that in a\ndataset with a different domain with respect to the\npretraining data, a domain adaptation step is able to\nsigniﬁcantly improve the performance. Questions\nthat are not addressed in this study and which are\nopen for future work are: (i) how can one deliber-\nately generate different OCR views in our setting;\n(ii) if there is also an unsupervised way to pick\ngood OCR views, because as the Random setting\non RETAS shows that just randomly merging views\ndoes not automatically yield the best possible re-\nsult; (iii) whether it is possible to use a bidirectional\nLM; (iv) whether it is possible to assess in advance\nthe quality of the underlying OCRs.\nReferences\nChantal Amrhein and Simon Clematide. 2018. Super-\nvised OCR error detection and correction using sta-\ntistical and neural machine translation methods. J.\nLang. Technol. Comput. Linguistics.\nYonatan Belinkov and Yonatan Bisk. 2018. Synthetic\nand natural noise both break neural machine transla-\ntion. In Proceedings of ICLR.\nGuillaume Chiron, Antoine Doucet, Mickaël Coustaty,\nand Jean-Philippe Moreux. 2017. ICDAR2017 com-\npetition on post-ocr text correction. In Proceedings\nof ICDAR.\nRui Dong and David Smith. Multi-input attention for\nunsupervised OCR correction. In Proceedings of\nACL, pages 2363–2372.\nZheng Huang, Kai Chen, Jianhua He, Xiang Bai, Di-\nmosthenis Karatzas, Shijian Lu, and C. V . Jawahar.\n2019. ICDAR2019 competition on scanned receipt\nOCR and information extraction. In Proceedings of\nICDAR, pages 1516–1520.\nA. Kumar, Piyush Makhija, and Anuj Gupta. 2020.\nNoisy text data: Achilles’ heel of bert. In W-\nNUT@EMNLP.\nD. Lopresti and J. Zhou. 1997. Using consensus se-\nquence voting to correct ocr errors.Computer Vision\nand Image Understanding, 67:39–47.\nWilliam B. Lund, Douglas J. Kennard, and Eric K.\nRingger. 2013. Combining multiple thresholding bi-\nnarization values to improve OCR output. In Pro-\nceedings of DRR.\nLijun Lyu, Maria Koutraki, Martin Krickl, and Besnik\nFetahu. 2021. Neural OCR post-hoc correction of\nhistorical corpora. CoRR.\nEugene W. Myers. 1986. An O(ND) difference algo-\nrithm and its variations. Algorithmica, 1(2):251–\n266.\nThi Tuyet Hai Nguyen, Adam Jatowt, Nhu-Van\nNguyen, Mickael Coustaty, and Antoine Doucet.\n2020. Neural machine translation with bert for post-\nocr error detection and correction. In Proceedings\nof JCDL, page 333–336.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nCaitlin Richter, Matthew Wickes, Deniz Beser, and\nMitch Marcus. 2018. Low-resource post processing\nof noisy OCR output for historical corpus digitisa-\ntion. In Proceedings of LREC.\nChristophe Rigaud, Antoine Doucet, Mickaël Cous-\ntaty, and Jean-Philippe Moreux. 2019. ICDAR 2019\ncompetition on post-ocr text correction. In Proceed-\nings of ICDAR, pages 1588–1593.\nStuart Russell and Peter Norvig. 2002. Artiﬁcial intel-\nligence: a modern approach. Pearson.\n8652\nSarah Schulz and Jonas Kuhn. 2017. Multi-modular\ndomain-tailored OCR post-correction. In Proceed-\nings of EMNLP, pages 2716–2726.\nBenno Stein, Dennis Hoppe, and Tim Gollub. 2012.\nThe impact of spelling errors on patent search. In\nProceedings of EACL, page 570–579.\nShaobin Xu and David Smith. 2017. Retrieving and\ncombining repeated passages to improve OCR. In\nProceedings of JCDL, pages 1–4.\nIsmet Zeki Yalniz and Raghavan Manmatha. 2011. A\nfast alignment scheme for automatic OCR evalua-\ntion of books. In Proceedings of ICDAR, pages 754–\n758.\nTakafumi Yamazoe, Minoru Etoh, Takeshi Yoshimura,\nand Kousuke Tsujino. 2011. Hypothesis preser-\nvation approach to scene text recognition with\nweighted ﬁnite-state transducer. In Proceedings of\nICDAR, pages 359–363.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8542182445526123
    },
    {
      "name": "Error detection and correction",
      "score": 0.6896204948425293
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6591182947158813
    },
    {
      "name": "Domain adaptation",
      "score": 0.6266330480575562
    },
    {
      "name": "Optical character recognition",
      "score": 0.6244590282440186
    },
    {
      "name": "Language model",
      "score": 0.6081402897834778
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5682239532470703
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.566893994808197
    },
    {
      "name": "Speech recognition",
      "score": 0.5394834280014038
    },
    {
      "name": "Natural language processing",
      "score": 0.5262657403945923
    },
    {
      "name": "Error analysis",
      "score": 0.4774623215198517
    },
    {
      "name": "Machine learning",
      "score": 0.3510342240333557
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34586650133132935
    },
    {
      "name": "Image (mathematics)",
      "score": 0.16970640420913696
    },
    {
      "name": "Algorithm",
      "score": 0.14094328880310059
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ]
}