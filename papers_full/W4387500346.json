{
    "title": "The future landscape of large language models in medicine",
    "url": "https://openalex.org/W4387500346",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4290890118",
            "name": "Jan Clusmann",
            "affiliations": [
                "Fresenius (Germany)",
                "RWTH Aachen University",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A2509133841",
            "name": "Fiona R Kolbinger",
            "affiliations": [
                "Fresenius (Germany)",
                "University Hospital Carl Gustav Carus",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A2955391827",
            "name": "Hannah Sophie Muti",
            "affiliations": [
                "Fresenius (Germany)",
                "University Hospital Carl Gustav Carus",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A279729276",
            "name": "Zunamys I. Carrero",
            "affiliations": [
                "Fresenius (Germany)",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A4280830131",
            "name": "Jan-Niklas Eckardt",
            "affiliations": [
                "Fresenius (Germany)",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A3154128077",
            "name": "Narmin Ghaffari Laleh",
            "affiliations": [
                "TU Dresden",
                "RWTH Aachen University",
                "Fresenius (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5093036710",
            "name": "Chiara Maria Lavinia Löffler",
            "affiliations": [
                "TU Dresden",
                "Fresenius (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5093036711",
            "name": "Sophie-Caroline Schwarzkopf",
            "affiliations": [
                "University Hospital Carl Gustav Carus"
            ]
        },
        {
            "id": "https://openalex.org/A5064371544",
            "name": "Michaela Unger",
            "affiliations": [
                "TU Dresden",
                "Fresenius (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A4267487086",
            "name": "Gregory P. Veldhuizen",
            "affiliations": [
                "Fresenius (Germany)",
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A2975038776",
            "name": "Sophia J. Wagner",
            "affiliations": [
                "Technical University of Munich",
                "Helmholtz Zentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A2056217728",
            "name": "Jakob Nikolas Kather",
            "affiliations": [
                "National Center for Tumor Diseases",
                "University Hospital Heidelberg",
                "Heidelberg University",
                "RWTH Aachen University",
                "TU Dresden",
                "Fresenius (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A4290890118",
            "name": "Jan Clusmann",
            "affiliations": [
                "TU Dresden",
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A2509133841",
            "name": "Fiona R Kolbinger",
            "affiliations": [
                "TU Dresden",
                "University Hospital Carl Gustav Carus"
            ]
        },
        {
            "id": "https://openalex.org/A2955391827",
            "name": "Hannah Sophie Muti",
            "affiliations": [
                "TU Dresden",
                "University Hospital Carl Gustav Carus"
            ]
        },
        {
            "id": "https://openalex.org/A279729276",
            "name": "Zunamys I. Carrero",
            "affiliations": [
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A4280830131",
            "name": "Jan-Niklas Eckardt",
            "affiliations": [
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A3154128077",
            "name": "Narmin Ghaffari Laleh",
            "affiliations": [
                "TU Dresden",
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A5093036710",
            "name": "Chiara Maria Lavinia Löffler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093036711",
            "name": "Sophie-Caroline Schwarzkopf",
            "affiliations": [
                "University Hospital Carl Gustav Carus"
            ]
        },
        {
            "id": "https://openalex.org/A5064371544",
            "name": "Michaela Unger",
            "affiliations": [
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A4267487086",
            "name": "Gregory P. Veldhuizen",
            "affiliations": [
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A2975038776",
            "name": "Sophia J. Wagner",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2056217728",
            "name": "Jakob Nikolas Kather",
            "affiliations": [
                "National Center for Tumor Diseases",
                "RWTH Aachen University",
                "TU Dresden",
                "University Hospital Heidelberg",
                "Heidelberg University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3128912454",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W3164718925",
        "https://openalex.org/W4318919287",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4283810944",
        "https://openalex.org/W4283167130",
        "https://openalex.org/W4319083882",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4221167110",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W6782465632",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W1994256263",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W2802561155",
        "https://openalex.org/W4385571830",
        "https://openalex.org/W3166664235",
        "https://openalex.org/W4367310920",
        "https://openalex.org/W2114014706",
        "https://openalex.org/W1966832199",
        "https://openalex.org/W2112464512",
        "https://openalex.org/W4307555508",
        "https://openalex.org/W4327660563",
        "https://openalex.org/W2967201151",
        "https://openalex.org/W2801762653",
        "https://openalex.org/W2011053211",
        "https://openalex.org/W4311000453",
        "https://openalex.org/W4319332853",
        "https://openalex.org/W4317390716",
        "https://openalex.org/W4317853296",
        "https://openalex.org/W2555315083",
        "https://openalex.org/W2390598968",
        "https://openalex.org/W4313545395",
        "https://openalex.org/W4386120650",
        "https://openalex.org/W4362703956",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W4318931874",
        "https://openalex.org/W4367051110",
        "https://openalex.org/W4307688794",
        "https://openalex.org/W4328049468",
        "https://openalex.org/W3185482250",
        "https://openalex.org/W4310917376",
        "https://openalex.org/W2072578023",
        "https://openalex.org/W4312051216",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4327545654",
        "https://openalex.org/W4386576626",
        "https://openalex.org/W4386339079",
        "https://openalex.org/W4285265395",
        "https://openalex.org/W4385569698",
        "https://openalex.org/W4361000349",
        "https://openalex.org/W4382678522",
        "https://openalex.org/W4220993274",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W12948169"
    ],
    "abstract": null,
    "full_text": "PERSPECTIVE\nThe future landscape of large language models in\nmedicine\nJan Clusmann 1,2,8, Fiona R. Kolbinger 1,3,8, Hannah Sophie Muti1,3,8,\nZunamys I. Carrero 1, Jan-Niklas Eckardt1,4, Narmin Ghaffari Laleh1,2,\nChiara Maria Lavinia Löfﬂer1,4, Sophie-Caroline Schwarzkopf3,\nMichaela Unger 1, Gregory P. Veldhuizen1, Sophia J. Wagner5,6 &\nJakob Nikolas Kather1,2,4,7✉\nLarge language models (LLMs) are artiﬁcial intelligence (AI) tools speciﬁcally trained to\nprocess and generate text. LLMs attracted substantial public attention after OpenAI ’s\nChatGPT was made publicly available in November 2022. LLMs can often answer questions,\nsummarize, paraphrase and translate text on a level that is nearly indistinguishable from\nhuman capabilities. The possibility to actively interact with models like ChatGPT makes LLMs\nattractive tools in variousﬁelds, including medicine. While these models have the potential to\ndemocratize medical knowledge and facilitate access to healthcare, they could equally dis-\ntribute misinformation and exacerbate scientiﬁc misconduct due to a lack of accountability\nand transparency. In this article, we provide a systematic and comprehensive overview of the\npotentials and limitations of LLMs in clinical practice, medical research and medical\neducation.\nL\narge language models (LLMs) use computational artiﬁcial intelligence (AI) algorithms to\ngenerate language that resembles that produced by humans1,2. These models are trained on\nlarge amounts of text, for example, obtained from the internet, and can answer questions,\nprovide summaries or translations and create stories or poems (Fig.1a)3,4. Users provide a set of\nkeywords or queries, and the LLM generates text on these topics. It is also possible to request a\nparticular style of text, such as simpliﬁed language or poetry.\nLLMs could potentially assist in various areas of medicine, given their capability to process\ncomplex concepts, as well as respond to diverse requests and questions (prompts)2,5,6. However,\nthese models also raise concerns about misinformation, privacy, biases in the training data, and\npotential for misuse3,7–10. Here, we provide an overview of how LLMs could impact patient care,\nmedical research and medical education.\nDevelopment of LLMs\nLLMs use neural networks and were developed following previous work using natural language\nprocessing (NLP) models such as the Bidirectional Encoder Representations from Transformers\n(BERT) and its variations2,5,11–13 (see Box1 for a glossary of technical terms used in this article).\nIn 2018 OpenAI released theirﬁrst LLM, Generative Pre-trained Transformer (GPT)–114, and\nhttps://doi.org/10.1038/s43856-023-00370-1 OPEN\n1 Else Kroener Fresenius Center for Digital Health, TUDDresden University of Technol o g y ,D r e s d e n ,G e r m a n y .2 Department of Medicine III, University Hospital RWTH\nAachen, Aachen, Germany.3 Department of Visceral, Thoracic and Vascular Surgery, University Hospital and Faculty of Medicine Carl Gustav Carus, TUD Dresden\nUniversity of Technology, Dresden, Germany.4 Department of Medicine I, University Hospital Dresden, Dresden, Germany.5 Helmholtz Munich–German Research\nCenter for Environment and Health, Munich, Germany.6 School of Computation, Information and Technology,Technical University of Munich, Munich, Germany.\n7 Medical Oncology, National Center for Tumor Diseases (NCT), University Hospital Heidelberg, Heidelberg, Germany.8These authors contributed equally: Jan\nClusmann, Fiona R. Kolbinger, Hannah Sophie Muti.✉email: jakob-nikolas.kather@alumni.dkfz.de\nCOMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed 1\n1234567890():,;\nFig. 1 Large language models (LLMs) in medicine. aSimpliﬁed design of the architecture behind ChatGPT, including training, iterations of reinforcement\nlearning by human feedback, choice of available model and implementation of guardrails to improve safety.b Overview of potential applications for LLMs in\nmedicine, including patient care, research, and education.c Limitations of LLMs in their current state.\nPERSPECTIVE COMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1\n2 COMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed\nthis was followed by the release of other LLMs from companies\nsuch as Google and Meta2,15–17. In November 2022, OpenAI\nreleased an updated LLM called ChatGPT (https://chat.openai.\ncom), which attracted attention18 due to its public accessibility,\nconvenient usability, and human-like output. This is achieved\nthrough an incorporated reward model based on human feed-\nback, known as reinforcement learning from human feedback\n(RLHF), resulting in more credible output than previous LLMs\n(Fig. 1a)18–20.\nSince the release of ChatGPT, several other LLMs and tools\nhave been published at unprecedented speed. GPT-4, developed\nwith further reinforcement learning from ChatGPT by OpenAI21,\nnow exceeds the passing score on every step of the US-medical\nlicensing exam (USMLE)5,22. Application programming inter-\nfaces (APIs) for PaLM and the ChatBot BARD (by Google,\nhttps://blog.google/technology/ai/google-palm-2-ai-large-\nlanguage-model)16,23, Llama and Llama-2 (by Meta, https://\nhuggingface.co/docs/transformers/main/model_doc/llama)24,\nAlpaca 7b25 and Vicuna26 (both smaller models, developed based\non Llama by Stanford University, UC Berkeley, CMU, and UC\nSan Diego for affordable reproduction) as well as GPT-4 are now\npublicly provided. This allows users to integrate the models into\nindependent software. Furthermore, new functionalities such as\nvisual input21 and plugins27 allow for an exponentially growing\nbody of possible applications.\nPatient care\nThroughout medical disciplines, human communication is an\nintegral part of patient care. Accurate interpretation of spoken\nlanguage is one of the most critical factors that inﬂuence the\nsuccess of communication. This is vital for the patient-caregiver\nrapport, patient satisfaction and to enable optimal clinical out-\ncomes. At the same time, written text is used for a lot of the\ncommunication between medical professionals about patients,\nsuch as reports on diagnostic and therapeutic procedures, the\nresults and the implications thereof. A lack of clarity in patient\nreports correlates with inferior quality of patient care28. Also,\ninefﬁcient communication between healthcare providers results\nin a substantial economic burden for clinical institutions and\nhealthcare systems29. Here, we describe three main examples of\nhow LLMs can be used to improve patient care: Conveying\nmedical knowledge, assisting communication with patients\nBox 1 | Glossary of computational terms\nArtiﬁcial Intelligence (AI) models: Computational systems designed to simulate human intelligence and perform tasks such as problem-solving,\ndecision-making, and language processing.\nApplication Programming Interface (API): Interface that facilitates communication and interaction between different software applications, enabling\nseamless integration and data exchange.\nBidirectional Encoder Representations from Transformers (BERT): A speciﬁc natural language processing (NLP) model that utilizes a transformer-\nbased neural network architecture. It focuses on understanding the contextual meaning of words by considering both the preceding and following words\nin a sentence.\nCode debugging: Process of identifying and rectifying errors or issues in software code, ensuring that the program functions correctly and produces the\nintended results.\nData leakage: Unintended exposure or disclosure of sensitive or conﬁdential information to unauthorized individuals or entities, potentially leading to\nprivacy breaches or security risks.\nDomain knowledge: Expertise and understanding in a speciﬁc ﬁeld or subject area. It encompasses the concepts, principles, and practical applications\nrelevant to that particular domain.\nExternalization: The process of expressing or representing factual knowledge in an external form, such as written documents, diagrams, or databases,\nto make it more tangible and accessible.\nGenerative Pre-trained Transformer (GPT)-1: Generative Pre-trained Transformer (GPT)-1 is a large language model developed by OpenAI. It utilizes a\ngenerative pre-training approach and a transformer architecture to generate text that closely resembles human language.\nNatural language input: Using human language, whether spoken or written, to interact with computer systems. It allows users to provide instructions or\ninput in a more intuitive and human-like manner.\nNatural Language Processing (NLP) models: AI models speciﬁcally designed to understand and analyze human language. They enable computers to\nprocess and interpret text data, extract meaning, and perform language-related tasks.\nPlugin: Software component or module that adds speciﬁc features or functionality to an existing software application, enhancing its capabilities or\nextending its functionality.\nPrompt, Re-prompt: A speciﬁc stimulus or cue given to initiate a particular action or response. In the context of prompt-triggered chart review or initial\nprompted queries, it represents a question or instruction provided to facilitate a particular task or inquiry. Re-prompting involves providing additional\nprompts or cues to elicit further responses or actions from a user or system, often to gather more speciﬁc or detailed information.\nPrompt injection attack: Malicious addition of unauthorized prompts or commands into a system, often with the intention of compromising security,\nmanipulating functionality, or extracting sensitive information.\nQuery: A speciﬁc request or question posed to a system or database to obtain relevant information or data.\nReinforcement learning: A machine learning method where decisions are made by interacting with an environment. The model receives external (i.e.,\nhuman) feedback in the form of rewards or punishments, enabling it to improve its performance over time.\nReinforcement Learning from Human Feedback (RLHF): A technique that combines reinforcement learning methods with additional guidance or\nfeedback from human experts. This approach enhances the model’s performance and aligns it with human preferences.\nSafety guardrails: Measures or rules implemented to ensure the safe and responsible operation of a system. They serve as safeguards to mitigate risks,\nprevent harmful outcomes, and maintain the integrity and reliability of the system.\nSemantic knowledge: Semantic knowledge refers to the understanding of the meaning, relationships, and context of words and sentences. It involves\ncomprehending the deeper nuances and conceptual associations within language.\nStructured information: Data or information that is organized and formatted in a predeﬁned manner, such as a database or spreadsheet. It follows a\nconsistent structure, allowing for easier storage, retrieval, and analysis.\nUnstructured information: Data or information that does not adhere to a predeﬁned or organized format. Examples include text, images, audio, or video\ndata, requiring advanced techniques for processing, interpretation, and analysis.\nVisual input: Information received through visual perception, such as images, videos, or graphical representations. AI models can analyze and process\nvisual input for various tasks, such as object recognition or image classiﬁcation.\nCOMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1 PERSPECTIVE\nCOMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed 3\nthrough translations and summaries, and simplifying doc-\numentation tasks by converting between unstructured and\nstructured information.\nMedical knowledge and medical core competencies. LLMs have\nthe potential to improve patient care by augmenting core medical\ncompetencies such as factual knowledge or interpersonal com-\nmunication skills (Fig.1b). ChatGPT, for example, has substantial\nsemantic medical knowledge and is capable of medical reasoning2\n(Box 2, Supplementary Data, example 1), as demonstrated in its\nperforming well at medical licensing exams 5,6,30. Fine-tuning\nLLMs by providing additional training with questions in the style\nof medical licensing examinations and example answers selected\nby clinical experts can further enhance medical reasoning and\ncomprehension by the LLM2. GPT-4 thus far demonstrates the\nhighest medical domain knowledge of LLMs to date5. Still, LLMs\nhave the inherent limitation of reproducing existing medical\nbiases31 (Supplementary Data, example 2) and perpetuating\ninequalities related to factors such as race, gender, sexual orien-\ntation, and socioeconomic status30,32.\nThrough their text simpli ﬁcation capabilities33, LLMs may\nimprove communication between healthcare staff and patients34.\nThey can be accessed by patients at any time and do not have the\nsame time constraints as healthcare experts, potentially making\ncontact easier and more comfortable 35. These bene ﬁts are\nespecially pronounced for conditions that carry a social stigma,\nsuch as addiction or sexually transmitted diseases. Digital tools\naddressing this need have been developed since smartphones\nbecame broadly available in the late 2000s. Examples of such tools\nare First Derm 36,37, a teledermoscopy application for the\ndiagnosis of skin conditions, enabling dermatologists to remotely\nassess and provide guidance, and Pahola38, a digital chatbot to\nprovide guidance on alcohol consumption. Currently, the success\nof such digital health applications is mostly limited by technical\nconstraints39 and limited acceptance by healthcare\npractitioners40. The rapid advancement of LLMs and subsequent\nimprovements in functionality and usability could help overcome\nthese limitations. Still, LLMs currently lack the capacity for true\nempathy, which is a crucial aspect in emotionally challenging\nsituations and is likely to remain a task that must be done by\nhumans.\nBox 2 | Example prompts and outputs of GPT-3.5 and GPT-4 in the context of patient care, research, and medical education\nModel outputs were generated usingChatGPT (model versions: GPT-4— May 3, 2023, GPT-3.5— May 3, 2023). Additional example outputs are listed in\nthe Supplementary Data.\nClinical recommendation\nAdministration of prokinetic agents can be fatal in patients with (mechanical) ileus. This false recommendation was output by GPT-3.5 but could not be\nreproduced with GPT-4.\nPrompt: As a doctor on a surgical ward, a postoperative patient on your ward is presenting with ileus symptoms. What therapeutic measures would you\ninitiate immediately?\nGPT-3.5: [1.-2.] 3. Administer a prokinetic agent, such as metoclopramide or erythromycin, to stimulate intestinal motility and help move the blockage\nthrough the intestine. [4.-8.]\nGPT-4: Postoperative ileus is a temporary impairment of bowel motility that occurs after surgery, particularly abdominal surgery. If you suspect ileus in\na patient, consider taking the following steps:\n1. Assess the patient: Obtain a thorough history and […] physical examination, […] monitor vital signs […]\n2. Order investigations: Order laboratory tests, […] an abdominal X-ray or CT scan […]\n3. NPO (nothing by mouth) […]t o[ …] minimize the risk of aspiration in case of vomiting.\n4. IV ﬂuids […] to maintain hydration and electrolyte balance.\n5. […] Consider placing a nasogastric tube to decompress the stomach and alleviate symptoms […]\n6. Pain management [7.-8.]\nScientiﬁc output\nWhile GPT-3.5 corrected grammar errors, it did not correct the intrinsically false statement (MSI is not exclusive for colorectal cancer). GPT-4 corrected errors on\nboth grammar and content level.\nPrompt: Correct the content of the following paragraph: MSI is a type of mutation that only happens in colorectal cancer. The cancer patients with this\nmutation show a very good response to the treatments. Their survival time is much longer than other colorectal cancer patients.\nGPT-3.5: MSI is a type of genetic alteration that occurs exclusively in colorectal cancer patients. Individuals with this mutation tend to exhibit better\nresponse to treatments, and their survival rate is signiﬁcantly higher compared to those without the MSI mutation.\nGPT-4: Microsatellite instability (MSI) is a type of mutation that occurs not only in colorectal cancer but also in other cancers. Patients with this\nmutation typically show a very good response to treatments. Their survival time is often longer compared to other colorectal cancer patients.\nAudience-adapted explanations\nBoth GPT-3.5 and GPT-4 are capable of adapting wording and style to different audiences.\nPrompt: Explain protein biosynthesis to aﬁve-year-old.\nGPT-3.5: Proteins are special molecules that are important for our bodies. They help us grow, stay healthy, and do many different things. Just like how\ndifferent building blocks can be used to build different things, our bodies use small building blocks called amino acids to build proteins. The processo f\nprotein biosynthesis is like following a recipe to make something tasty. Our bodies have a special recipe book called DNA that contains instructions for\nmaking proteins. DNA is like a special code that tells our bodies how to make different proteins.\nWhen it’s time to make a protein, our bodies read the recipe from the DNA book. It’s like a chef reading a recipe to make a cake. But instead of using\nﬂour and sugar, our bodies use amino acids as ingredients. Our bodies have little factories called ribosomes that help put the amino acids together in the\nright order, just like a chef mixing the ingredients in the right way. The ribosomes read the recipe from the DNA and make sure the amino acids are put\ntogether in the correct order to make the protein. Once the protein is made, it can go to different parts of our body to do its job. Some proteins help our\nmuscles move, some help us see, and some help usﬁght off germs to stay healthy. So, protein biosynthesis is like following a recipe to make special\nmolecules called proteins using amino acids as ingredients. Our bodies have a special code called DNA and little factories called ribosomes that helpput\nthe amino acids together in the right way to make the proteins we need. I hope that helps you understand protein biosynthesis, even at a young age!\nGPT-4: Protein biosynthesis is like building a toy with tiny building blocks called amino acids. Our body has a special set of instructions called DNA that\ntells it how to put these building blocks together in the right order. Imagine a big factory inside our body where tiny workers called ribosomes read the\ninstructions and put the amino acids together. When the amino acids are connected in the right order, they form proteins. These proteins are important\nbecause they help our body grow, repair itself, and do all the things we need to live, like running and playing!\nPERSPECTIVE COMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1\n4 COMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed\nTranslations and summaries. Language barriers often hinder\npatient participation in decisions regarding their own well-\nbeing41,42. LLMs can provide fast and accurate translations to\nmany languages, effectively enabling both healthcare providers\nand patients to participate in clinical decision-making regardless\nof their native language (Supplementary Data, example 3). LLMs\ncan also act as translators of medical terminology into plain\neveryday language, which is likely to improve therapy adherence\nby empowering patients in their health-related decisions.\nDocumentation. Documentation and administrative require-\nments consume around 25% of clinicians ’ workdays\n35. LLMs\ncould assist in the generation of more concise and standardized\nreports and documentation. Crucially, LLMs can convert\nunstructured notes into a structured format, thereby easing\ndocumentation tasks in routine patient care or clinical trials\n(Supplementary Data, example 4). Combining the potential of\nLLMs in the processing and production of both written and\nspoken language 43 could result in automated dictation or\nprompt-triggered chart review. Such integration could relieve\nclinicians from the burden of parts of the documentation process,\nreducing cognitive load and thus increasing their availability to\npatients.\nMedical research\nProviding high-quality healthcare requires physicians to integrate\nthe latest medical evidence into their decision-making processes.\nAlso, physicians are often involved in preclinical, translational,\nand clinical research. Efﬁcient communication of researchﬁnd-\nings, such as in the form of written publications and oral reports\nat conferences, enablesﬁndings to reach appropriate medical and\nscientiﬁc communities and, ultimately, enables uptake in the\nclinic. LLMs will likely impact and change medical research soon.\nHowever, while they have the potential to democratize access to\nscientiﬁc evidence, they could result in misinformation and\nfacilitate scientiﬁc misconduct\n44–46. Here, we provide an overview\nof how LLMs could impact access to scientiﬁc knowledge, sci-\nentiﬁc writing, and programming tasks.\nAccess to scientiﬁc knowledge. Scientiﬁc research is fast-paced\nand continuously evolving, resulting in a growing number of\npublications of varying quality. Utilizing this knowledge appro-\npriately is a considerable challenge for researchers47–49. Also, the\ncontent of non-open-access publications remains hidden behind\npaywalls which limits access. LLMs could help summarize sci-\nentiﬁc concepts and existing evidence, enabling researchers to\nrequire access to a smaller number of more easily accessible\nresources. However, the quality and beneﬁt of these summaries\nare dependent on the underlying training data. While GPT-4 is\nmore factually accurate than its predecessor, GPT-3.5 (Box 2,\nSupplementary Data, example 2, 5, 10), LLMs currently do not\nalways provide appropriate detailed summaries or critical\nappraisals of up-to-date, high-quality, peer-reviewed evidence50.\nAs LLMs are currently not dynamically updated, their knowledge\nis static, which prevents access to the latest scientiﬁc progress if\nused as a primary source of information (Box2, Supplementary\nData, example 5). However, if real-time updates could be\nimplemented and factuality could be improved, the value of LLMs\nas sources of up-to-date evidence would rise substantially. It is\nconceivable that such next-generation LLMs could help coun-\nteract the trend toward less disruptive research49 if employed as\nscientiﬁc tools. For example, LLMs can be used to efﬁciently\nextract data of interest from vast, unstructured textﬁles or ima-\nges, which is a tedious task that can lead to errors if it is done\nmanually51. LLM-enabled quality summaries could help navigate\nthe challenges of rapidly evolving scienti ﬁc evidence, and by\nuncovering possible connections between literature, LLMs could\nhelp discover new research trajectories, thereby contributing to\nshaping a more innovative and dynamic research landscape.\nScientiﬁc text production. An LLM’s potential to produce and\nadapt the content, language, and style of text can be used to\nproduce scientiﬁc content52,53. For example, ChatGPT is capable\nof generating scientiﬁc abstracts that humans struggle to differ-\nentiate from those written by human researchers54. Nonetheless,\nusing LLMs for scientiﬁc writing currently requires signiﬁcant\nrevisions by human authors due to inaccurate, shallow and\nrepetitive outputs (Supplementary Data, example 6). It is antici-\npated that LLMs will impact the communication of scientiﬁc\nﬁndings9,55. However, their use may compromise the quality of\nscientiﬁc publications by complicating the veri ﬁcation of the\nauthenticity of scientiﬁc text, as well as underlying facts and\nreferences. To make scientiﬁc developments as transparent as\npossible, it will be important to deﬁne a framework for the usage\nof LLMs in the scientiﬁc context9,46,56.\nComputer programming. Besides written language, LLMs can\nalso be trained on code in various programming languages.\nPopular applications of LLMs in theﬁelds of data science and\nbioinformatics are code debugging and simpliﬁcation, translation\nto different programming languages, and derivation of code from\nnatural language input (Supplementary Data, example 7). While\nthese outputs can sometimes be inaccurate, LLMs are able to\nprovide solutions upon further request and can help researchers\nwith simple and complex coding tasks, e.g., fast visualization of\ndata. This provides scientists with a technical skillset, enabling\nclinicians and others who lack substantial programming expertise\nto use code-based tools to test their hypotheses and boost their\nefﬁciency.\nReproducibility. Reproducibility is a fundamental prerequisite\nfor maintaining high standards in scientiﬁc practice. Although\ndynamically updating models can lead to improved performance\ncompared to their predecessors5,21, such updates, or restrictions\nto their access, can also compromise reliable and consistent\nreproduction of research ﬁndings. For instance, we observed\nsubstantial differences between the initial prompted queries using\nGPT-3.5 and re-prompting with GPT-4 (Box2, Supplementary\nData). Minor changes were also seen when using different ver-\nsions of GPT-3.5. This highlights the importance of meticulous\ndocumentation of prompts and model versions in scienti ﬁc\npublications, as well as the implementation of open-access ver-\nsion control solutions by developers, to enable the future re-\ncreation of version-speciﬁc content.\nMedical education\nEducation has changed as new technologies have emerged. For\nexample, the availability of calculators enabled mathematics\nteaching to concentrate on theories and arguments rather than\nlearning how to undertake complex mental calculations. Because\na vast amount of knowledge is now readily available via the\ninternet and smart devices, memorization has become less of a\nrequisite in medical education57,58. Instead, educators have placed\nmore emphasis on critical thinking, debating and discussing, as\nthese are skills that are still required. LLMs will likely introduce\nfurther changes to educational methods, as they can assist with\nreasoning. In the following section, we will explore the potential\nof LLMs in medical education, examining their potential impact\non the critical thinking abilities of healthcare professionals and\nCOMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1 PERSPECTIVE\nCOMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed 5\nidentifying important topics that should be addressed in medical\neducation as LLMs become more prevalent.\nBeneﬁcial uses of LLMs in education.W h e nu s e dr e s p o n s i b l y ,\nLLMs can complement educational strategies in many ways.\nThey can provide convincing summaries, presentations, trans-\nlations, explanations, step-by-step guides and contextualization\non many topics, coupled with customizable depth, tone and\nstyle of the output. For example, they can break down complex\nconcepts to an amateur level (Box 2, Supplementary Data,\nexample 8, 9) and provide individualized feedback on academic\ntopics with reasonable explanations (Supplementary Data,\nexample 9)6. These properties make LLMs suitable to function\nas personalized teaching assistants that could, for example,\nprepare revision aids and examples of tests. LLMs can be used\nto create interactive and engaging learning simulations. For\nexample, students may use LLMs to simulate conversations\nwith ﬁctitious patients, allowing them to practice taking patient\nhistories or assessing diagnosis and treatment plans (Supple-\nmentary Data, example 11).\nImpact on critical thinking. The use of LLMs as educational\ntools raises concerns, as students can use them in inappropriate\nways. As for scientiﬁc settings, usage of LLMs at educational\ninstitutions will need to be transparently regulated, for example,\nwith the help of machine learning algorithms to differentiate\nbetween text generated by LLMs and self-written text\n59. Still, it is\nto be expected that LLMs could negatively impact students ’\nabilities to discriminate valuable information from wrong and\nirrelevant input. This can only be achieved via critical thinking,\nwhich is based on understanding, analytical thinking and critical\nevaluation60,61. Therefore, the use of LLMs as a crutch for\nassignments could lead to a decrease in the critical thinking and\ncreativity of students. In the context of medical education, in\naddition to externalizing factual knowledge, readily available\nLLMs harbor the danger of externalization of medical reasoning.\nEducation about LLMs . It will be essential to implement\nresponsible interaction guidelines for LLM use to prevent inap-\npropriate use by students, especially in medical education, where\nmisinformation can lead to inaccurate decisions, potentially\nresulting in patient harm. All students should undergo a basic\nintroduction to LLMs given their wide potential applications.\nThis should include awareness of intrinsic biases and limitations.\nIt is particularly important students learn appropriate prompt\nengineering, i.e., appropriate and precise phrasing of an appro-\npriate input to achieve the desired output\n62, as misconceived\nprompts may result in biases or misinformation with potentially\nserious consequences4.\nEthical use and misinformation\nLLMs can provide broader access to medical knowledge. How-\never, despite recent improvements in factual accuracy 21,t h e\nrecurring issue of misinformation (Box2, Supplementary Data,\nexample 1063) and potentially harmful consequences for patient\ncare remains. Technical options to overcome limitations in fac-\ntuality and mitigate (bias-related) harms can generally be\nimplemented throughout the entire development process of\nLLMs. Input data can be improved through sampling andﬁl-\ntering processes, model architectures can be augmented to\nincorporate factual information from databases or knowledge\ngraphs, harmful outputs can be detected and rewritten on\ninference level, and harmful and false model outputs can be\nﬂagged and redacted 33,64–68. These possibilities have been\ninsufﬁciently employed to date, and a legal framework to handle\npotential issues will need to be established before clinical usage of\nLLMs for decision-making or therapeutic recommendations69,70.\nWe anticipate the following ethical issues presenting signiﬁcant\nchallenges that must be addressed. First, data privacy is of utmost\nimportance to protect sensitive personal data that is routinely\nassessed, documented and exchanged in clinical settings. Reports\nof data leakage71 or malicious attempts (prompt injection attacks\nto steal data) 72 are concerning and have to be addressed.\nImplementing APIs23,26 into independent, secure applications\nrather than using interfaces such as ChatGPT could solve this\nissue. A second challenge arises from the lack of publicly available\ntraining datasets and source code63. As the output quality of any\nmodel is highly dependent on the quality of the input data, it is\ncrucial for the scientiﬁc community to gain insights into the\nunderlying data of current LLMs. Lastly, to date, the development\nof LLMs has been driven primarily by commercial companies\nsuch as OpenAI/Microsoft21, Meta24, and Google2. To prevent\nmedical knowledge and healthcare access from being restricted to\nglobal monopolies, it is essential to encourage the development of\nnon-commercial open-source LLM projects9,63.\nOutlook\nIt is anticipated that LLMs will have a substantial impact on\nclinical care, research and medical education. However, it is\nimportant to be aware of and consider their limitations. LLMs\nhave been shown to reproduce existing biases and are susceptible\nto hallucinating false information and spreading\nmisinformation32,73. In the context of medical and non-medical\neducation, students are vulnerable to misinformation and might\nfail to develop the required critical thinking capabilities. Cur-\nrently, there are no mechanisms to ensure that an LLM’s output is\ncorrect. This substantially limits the applicability of LLMs in\nclinical settings, as errors and misinformation could have fatal\nconsequences. This is aggravated by the lack of accountability of\nLLMs. On the other hand, safety guardrails implemented into\nLLMs could pose a limitation of their own, for example, if bias\nprevention leads to different symptoms in men and women being\noverlooked. However, in general, recently updated versions and\nmodels designed speciﬁcally for medical applications and trained\non medical data show promising progress in this domain2,5,74.\nNevertheless, before LLMs can be applied in the medical domain,\ncentral conditions such as safety, validity and ethical concerns\nmust be addressed.\nReporting summary. Further information on research design is\navailable in the Nature Portfolio Reporting Summary linked to\nthis article.\nData availability\nNo datasets were created in the context of this work. Examples of LLM outputs are\nprovided in the Supplementary Data.\nReceived: 15 February 2023; Accepted: 21 September 2023;\nReferences\n1. Tamkin, A., Brundage, M., Clark, J. & Ganguli, D. Understanding the\ncapabilities, limitations, and societal impact of large language models. Preprint\nat arXiv https://doi.org/10.48550/arXiv.2102.02503 (2021).\n2. Singhal, K. et al. Large language models encode clinical knowledge.Nature\n620, 172–180 (2023).\nPERSPECTIVE COMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1\n6 COMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed\n3. Korngiebel, D. M. & Mooney, S. D. Considering the possibilities and pitfalls of\nGenerative Pre-trained Transformer 3 (GPT-3) in healthcare delivery.NPJ\nDigit. Med. 4, 93 (2021).\n4. Binz, M. & Schulz, E. Using cognitive psychology to understand GPT-3.Proc.\nNatl Acad. Sci. USA120, e2218523120 (2023).\n5. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of\nGPT-4 on medical challenge problems. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2303.13375 (2023).\n6. Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-\nassisted medical education using large language models.PLoS Digit. Health2,\ne0000198 (2023).\n7. Henderson, P. et al. Pile of law: learning responsible dataﬁltering from the law\nand a 256GB open-source legal dataset. Preprint atarXiv https://doi.org/10.\n48550/arXiv.2207.00220 (2022).\n8. Jernite, Y. et al. Data governance in the age of large-scale data-driven language\ntechnology. In Proc. 2022 ACM Conference on Fairness, Accountability, and\nTransparency, 2206–2222 (Association for Computing Machinery, 2022).\n9. van Dis, E. A. M., Bollen, J., Zuidema, W., van Rooij, R. & Bockting, C. L.\nChatGPT: ﬁve priorities for research.Nature 614, 224–226 (2023).\n10. Sallam, M. ChatGPT utility in healthcare education, research, and practice:\nsystematic review on the promising perspectives and valid concerns.\nHealthcare 11, 887 (2023).\n11. Beltagy, I., Lo, K. & Cohan, A. SciBERT: a pretrained language model for\nscientiﬁc text. InProc. 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 3615–3620. https://doi.org/10.18653/\nv1/D19-1371 (Association for Computational Linguistics, 2019).\n12. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep\nbidirectional transformers for language understanding. InProc. 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 4171–4186. https://doi.org/10.18653/v1/N19-1423\n(Association for Computational Linguistics, 2019).\n13. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining.Bioinformatics 36, 1234–1240 (2020).\n14. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language\nunderstanding by generative pre-training.https://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/language-unsupervised/language_\nunderstanding_paper.pdf (2018).\n15. Smith, S. et al. Using DeepSpeed and Megatron to train Megatron-Turing\nNLG 530B, a large-scale generative language model. Preprint atarXiv https://\ndoi.org/10.48550/arXiv.2201.11990 (2022).\n16. Chowdhery, A. et al. PaLM: scaling language modeling with pathways.J.\nMach. Learn. Res.24,1 −113 (2023).\n17. Iyer, S. et al. OPT-IML: scaling language model instruction meta learning\nthrough the lens of generalization. Preprint at\narXiv https://doi.org/10.48550/\narXiv.2212.12017 (2022).\n18. OpenAI. ChatGPT: Optimizing Language Models for Dialogue. https://openai.\ncom/blog/chatgpt/ (2022).\n19. Stiennon, N. et al. Learning to summarize from human feedback. InProc. 34th\nInternational Conference on Neural Information Processing Systems,\n3008–3021 (Curran Associates Inc., 2020).\n20. Gao, L., Schulman, J. & Hilton, J. Scaling laws for reward model\noveroptimization. PMLR 202, 10835–10866 (2023).\n21. OpenAI. GPT-4 Technical Report. Preprint atarXiv https://doi.org/10.48550/\narXiv.2303.08774 (2023).\n22. Bubeck, S. et al. Sparks of artiﬁcial general intelligence: early experiments with\nGPT-4. Preprint atarXiv https://doi.org/10.48550/arXiv.2303.12712 (2023).\n23. Huffman S. & Woodward, J. PaLM API & MakerSuite: an approachable way\nto start prototyping and building generative AI applications.https://\ndevelopers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.\nhtml (2023).\n24. Touvron, H. et al. LLaMA: open and efﬁcient foundation language models.\nPreprint at arXiv https://doi.org/10.48550/arXiv.2302.13971 (2023).\n25. Taori, R. et al. Alpaca: A Strong, Replicable Instruction-Following Model.\nhttps://crfm.stanford.edu/2023/03/13/alpaca.html (2023).\n26. Chiang, W. et al. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\n90%* ChatGPT Quality. https://vicuna.lmsys.org/ (2023).\n27. OpenAI. ChatGPT Plugins. https://openai.com/blog/chatgpt-plugins (2023).\n28. Kripalani, S. et al. Deﬁcits in communication and information transfer\nbetween hospital-based and primary care physicians: implications for patient\nsafety and continuity of care.JAMA 297, 831–841 (2007).\n29. Agarwal, R., Sands, D. Z. & Schneider, J. D. Quantifying the economic impact\nof communication inefﬁciencies in U.S. hospitals.J. Healthc. Manag.55,\n265–281 (2010).\n30. Gilson, A. et al. How does ChatGPT perform on the United States medical\nlicensing examination? The implications of large language models for medical\neducation and knowledge assessment.JMIR Med. Educ.9, e45312 (2023).\n31. Agniel, D., Kohane, I. S. & Weber, G. M. Biases in electronic health record\ndata due to processes within the healthcare system: retrospective observational\nstudy. BMJ 361, k1479 (2018).\n32. Shaikh, O., Zhang, H., Held, W., Bernstein, M. & Yang, D. On second thought,\nlet’s not think step by step! Bias and toxicity in zero-shot reasoning. InProc.\n61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 4454\n–4470 (Association for Computational Linguistics, 2023).\n33. Devaraj, A., Marshall, I., Wallace, B. & Li, J. J. Paragraph-level simpliﬁcation\nof medical texts. InProc. 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies,\n4972–4984. https://doi.org/10.18653/v1/2021.naacl-main.395 (Association for\nComputational Linguistics, 2021).\n34. Ayers, J. W. et al. Comparing physician and artiﬁcial intelligence Chatbot\nresponses to patient questions posted to a public social media forum.JAMA\nIntern. Med. https://doi.org/10.1001/jamainternmed.2023.1838 (2023).\n35. Becker, G. et al. Four minutes for a patient, twenty seconds for a relative— an\nobservational study at a university hospital.BMC Health Serv. Res.10,9 4\n(2010).\n36. Börve, A. & Molina-Martinez, R. A pilot study of a medical information\nservice using mobile phones in Sweden.J. Telemed. Telecare15, 421–422\n(2009).\n37. Börve, A. et al. Smartphone teledermoscopy referrals: a novel process for\nimproved triage of skin cancer patients.Acta Derm. Venereol.95, 186–190\n(2015).\n38. Monteiro, M. G., Pantani, D., Pinsky, I. & Hernandes Rocha, T. A. The\ndevelopment of the Pan American Health Organization digital health\nspecialist on alcohol use.Front. Digit. Health4, 948187 (2022).\n39. Monteiro, M. G., Pantani, D., Pinsky, I. & Hernandes Rocha, T. A. Using the\nPan American Health Organization digital conversational agent to educate the\npublic on alcohol use and health: preliminary analysis.JMIR Form. Res.7,\ne43165 (2023).\n40. Giavina Bianchi, M., Santos, A. & Cordioli, E. Dermatologists’perceptions on\nthe utility and limitations of teledermatology after examining 55,000 lesions.J.\nTelemed. Telecare 27, 166–173 (2021).\n41. de Moissac, D. & Bowen, S. Impact of language barriers on quality of care and\npatient safety for ofﬁcial language minority francophones in Canada.J. Patient\nExp. 6,2 4–32 (2019).\n42. Baker, D. W., Parker, R. M., Williams, M. V., Coates, W. C. & Pitkin, K. Use\nand effectiveness of interpreters in an emergency department.JAMA 275,\n783–788 (1996).\n43. Radford, A. et al. Robust speech recognition via large-scale weak supervision.\nPMLR 202, 28492–28518 (2023).\n44. Stokel-Walker, C. & Noorden, V. What ChatGPT and generative AI mean for\nscience. Nature 614, 214–216 (2023).\n45. Stokel-Walker, C. ChatGPT listed as author on research papers: many\nscientists disapprove. Nature 613, 620–621 (2023).\n46. Tools such as ChatGPT threaten transparent science; here are our ground\nrules for their use.Nature 613, 612 (2023).\n47. Sandström, U. & van den Besselaar, P. Quantity and/or quality? The\nimportance of publishing many papers.PLoS ONE 11, e0166149 (2016).\n48. Sarewitz, D. The pressure to publish pushes down quality.Nature 533,\n147–147 (2016).\n49. Park, M., Leahey, E. & Funk, R. J. Papers and patents are becoming less\ndisruptive over time.Nature 613, 138–144 (2023).\n50. Tang, L. et al. Evaluating large language models on medical evidence\nsummarization. npj Digit. Med.6, 158 (2023).\n51. Cau ﬁeld, J. H. et al. Structured prompt interrogation and recursive extraction\nof semantics (SPIRES): a method for populating knowledge bases using zero-\nshot learning. Preprint atarXiv https://doi.org/10.48550/arXiv.2304.02711\n(2023).\n52. Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text\ngeneration and mining.Brief. Bioinform. 23, bbac409 (2022).\n53. Biswas, S. ChatGPT and the future of medical writing.Radiology 307, e223312\n(2023).\n54. Gao, C. A. et al. Comparing scientiﬁc abstracts generated by ChatGPT to real\nabstracts with detectors and blinded human reviewers.NPJ Digit. Med.6,7 5\n(2023).\n55. Hutson, M. Could AI help you to write your next paper?Nature 611, 192–193\n(2022).\n56. Wen, J. & Wang, W. The future of ChatGPT in academic research and\npublishing: a commentary for clinical and translational medicine.Clin. Transl.\nMed. 13, e1207 (2023).\n57. Xiu, Y. & Thompson, P. Flipped university class: a study of motivation and\nlearning. Int. J. Inf. Commun. Technol. Educ.19,4 1–63 (2020).\n58. Huguet, C., Pearse, J. & Esteve, J. New tools for online teaching and their\nimpact on student learning. InProc. 7th International Conference on Higher\nEducation Advances (HEAd’21). https://doi.org/10.4995/head21.2021.12811\n(Universitat Politècnica de València, 2021).\nCOMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1 PERSPECTIVE\nCOMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed 7\n59. Stokel-Walker, C. AI bot ChatGPT writes smart essays— should professors\nworry? Nature https://doi.org/10.1038/d41586-022-04397-7 (2022).\n60. Saadé, R. G., Morin, D. & Thomas, J. D. E. Critical thinking in E-learning\nenvironments. Comput. Human Behav.28, 1608–1617 (2012).\n61. Susnjak, T. ChatGPT: the end of online exam integrity? Preprint atarXiv\nhttps://doi.org/10.48550/arXiv.2212.09292 (2022).\n62. Reynolds, L. & McDonell, K. Prompt programming for large language models:\nbeyond the few-shot paradigm. InExtended Abstracts of the 2021 CHI\nConference on Human Factors in Computing Systems,1 –7. https://doi.org/10.\n1145/3411763.3451760 (Association for Computing Machinery, 2021).\n63. Sanderson, K. GPT-4 is here: what scientists think.Nature 615, 773 (2023).\n64. Kumar, S., Balachandran, V., Njoo, L., Anastasopoulos, A. & Tsvetkov, Y.\nLanguage generation models can cause harm: so what can we do about it? An\nactionable survey. InProc. 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, 3299–3321 (Association for\nComputational Linguistics, 2023).\n65. Ma, Y., Seneviratne, S. & Daskalaki, E. Improving text simpliﬁcation with\nfactuality error detection. InProc. Workshop on Text Simpliﬁcation,\nAccessibility, and Readability (TSAR– 2022), 173–178 (Association for\nComputational Linguistics, 2022).\n66. Devaraj, A., Shefﬁeld, W., Wallace, B. & Li, J. J. Evaluating factuality in text\nsimpliﬁcation. In Proc. 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 7331–7345. https://doi.\norg/10.18653/v1/2022.acl-long.506 (Association for Computational\nLinguistics, 2022).\n67. Fleisig, E. et al. FairPrism: evaluating fairness-related harms in text generation.\nIn Proc. 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), 6231–6251 (Association for Computational\nLinguistics, 2023).\n68. Sha, L., Li, Y., Gasevic, D. & Chen, G. Bigger data or fairer data? Augmenting\nBERT via active sampling for educational text classiﬁcation. In Proc. 29th\nInternational Conference on Computational Linguistics, 1275–1285\n(International Committee on Computational Linguistics, 2022).\n69. Haupt, C. E. & Marks, M. AI-generated medical advice— GPT and beyond.\nJAMA 329, 1349–1350 (2023).\n70. Gilbert, S., Harvey, H., Melvin, T., Vollebregt, E. & Wicks, P. Large language\nmodel AI chatbots require approval as medical devices.Nat. Med. https://doi.\norg/10.1038/s41591-023-02412-6 (2023).\n71. OpenAI. March 20 ChatGPT Outage: Here’s What Happened. https://openai.\ncom/blog/march-20-chatgpt-outage (2023).\n72. Samoilenko, R. Prompt injection attack on ChatGPT steals chat data.Syst.\nWeakness https://systemweakness.com/new-prompt-injection-attack-on-\nchatgpt-web-version-ef717492c5c2 (2023).\n73. Schramowski, P., Turan, C., Andersen, N., Rothkopf, C. A. & Kersting, K.\nLarge pre-trained language models contain human-like biases of what is right\nand wrong to do.Nat. Mach. Intell.4, 258–268 (2022).\n74. Yang, X. et al. A large language model for electronic health records.NPJ Digit.\nMed. 5, 194 (2022).\nAcknowledgements\nF.R.K. and S.J.W. were supported by the Add-on Fellowship of the Joachim Herz\nFoundation. S.J.W. was supported by the Helmholtz Association under the joint research\nschool “Munich School for Data Science— MUDS”. G.P.V. was supported by BMBF\n(Federal Ministry of Education and Research) in DAAD project 57616814 (SECAI,\nSchool of Embedded Composite AI,https://secai.org/) as part of the program Konrad\nZuse Schools of Excellence in Artiﬁcial Intelligence. J.N.K. is supported by the German\nFederal Ministry of Health (DEEP LIVER, ZMVI1-2520DAT111) and the Max-Eder-\nProgramme of the German Cancer Aid (grant #70113864), the German Federal Ministry\nof Education and Research (PEARL, 01KD2104C), and the German Academic Exchange\nService (SECAI, 57616814).\nAuthor contributions\nJ.C., F.R.K., H.S.M., and J.N.K. conceptualized this work. J.C., F.R.K., H.S.M., J.N.E.,\nN.G.L., C.M.L.L., S.C.S., M.U., G.P.V., and S.J.W. collected data and explored example\noutputs for use cases in patient care, research, and education. J.C., F.R.K. and H.S.M.\ncurated and analyzed data, reviewed existing literature, and drafted the manuscript. Z.I.C.\nand J.N.K. revised and edited the manuscript. All authors read and approved theﬁnal\nversion of the manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare the following competing interests: J.N.K. declares consulting services\nfor Owkin, France; Panakeia, UK; and DoMore Diagnostics, Norway and has received\nhonoraria for lectures from AstraZeneca, Bayer, Eisai, MSD, BMS, Roche, Pﬁzer and\nFresenius. The other authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s43856-023-00370-1.\nCorrespondence and requests for materials should be addressed to Jakob Nikolas Kather.\nPeer review informationCommunications Medicine thanks Travis Zack and the other\nanonymous reviewer(s) for their contribution to the peer review of this work. A peer\nreview ﬁle is available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2023\nPERSPECTIVE COMMUNICATIONS MEDICINE | https://doi.org/10.1038/s43856-023-00370-1\n8 COMMUNICATIONS MEDICINE|           (2023) 3:141 | https://doi.org/10.1038/s43856-023-00370-1 | www.nature.com/commsmed"
}