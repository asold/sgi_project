{
  "title": "Deepfake Detection Scheme Based on Vision Transformer and Distillation",
  "url": "https://openalex.org/W3141468933",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3139007072",
      "name": "Heo Young Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2332551150",
      "name": "Choi Young Ju",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293333280",
      "name": "Lee, Young-Woon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293333281",
      "name": "Kim Byung-Gyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911424785",
    "https://openalex.org/W2984700035",
    "https://openalex.org/W2963684180",
    "https://openalex.org/W2904573504",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W2598581049",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3036806226",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3035446294",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W1799366690",
    "https://openalex.org/W2739748921",
    "https://openalex.org/W2912336782",
    "https://openalex.org/W3037967553",
    "https://openalex.org/W2995516027",
    "https://openalex.org/W3095986900",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3148140980",
    "https://openalex.org/W3099319035",
    "https://openalex.org/W2980459401",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W3036576316",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3034713808",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963616706",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2982058372",
    "https://openalex.org/W2766527293",
    "https://openalex.org/W2963767194",
    "https://openalex.org/W2981523288",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3045755384",
    "https://openalex.org/W2898877033",
    "https://openalex.org/W2790871512"
  ],
  "abstract": "Deepfake is the manipulated video made with a generative deep learning technique such as Generative Adversarial Networks (GANs) or Auto Encoder that anyone can utilize. Recently, with the increase of Deepfake videos, some classifiers consisting of the convolutional neural network that can distinguish fake videos as well as deepfake datasets have been actively created. However, the previous studies based on the CNN structure have the problem of not only overfitting, but also considerable misjudging fake video as real ones. In this paper, we propose a Vision Transformer model with distillation methodology for detecting fake videos. We design that a CNN features and patch-based positioning model learns to interact with all positions to find the artifact region for solving false negative problem. Through comparative analysis on Deepfake Detection (DFDC) Dataset, we verify that the proposed scheme with patch embedding as input outperforms the state-of-the-art using the combined CNN features. Without ensemble technique, our model obtains 0.978 of AUC and 91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1 score on the same condition.",
  "full_text": "Deepfake Detection Scheme Based on Vision Transformer and Distillation\nYoung-Jin Heo, Young-Ju Choi, Byung-Gyu Kim\nSookmyung Women’s University\nIVPL\n{yj.heo, yj.choi, bg.kim}@ivpl.sookmyung.ac.kr\nYoung-Woon Lee\nSunmoon University\nIVPL\nyw.lee@ivpl.sookmyung.ac.kr\nAbstract\nDeepfake is the manipulated video made with a gener-\native deep learning technique such as Generative Adver-\nsarial Networks (GANs) or Auto Encoder that anyone can\nutilize. Recently, with the increase of Deepfake videos,\nsome classiﬁers consisting of the convolutional neural net-\nwork that can distinguish fake videos as well as deepfake\ndatasets have been actively created. However, the previ-\nous studies based on the CNN structure have the problem\nof not only overﬁtting, but also considerable misjudging\nfake video as real ones. In this paper, we propose a Vision\nTransformer model with distillation methodology for detect-\ning fake videos. We design that a CNN features and patch-\nbased positioning model learns to interact with all positions\nto ﬁnd the artifact region for solving false negative prob-\nlem. Through comparative analysis on Deepfake Detection\n(DFDC) Dataset, we verify that the proposed scheme with\npatch embedding as input outperforms the state-of-the-art\nusing the combined CNN features. Without ensemble tech-\nnique, our model obtains 0.978 of AUC and 91.9 of f1 score,\nwhile previous SOTA model yields 0.972 of AUC and 90.6\nof f1 score on the same condition.\n1. Introduction\nDeepfake is a combination of ‘Deep learning’ and ‘fake’,\nwhich refers to the technique of changing a source person\non a target video. This technology makes the source person\nseem to be doing the actions or saying things of the target\nperson by superimposing the source person as a target per-\nson as a deep learning model.\nHowever, cases of abuse such as fake news and revenge\nporn have emerged as a social issue. Because it is a tech-\nnology that can confuse people, some technologies and\ndatasets for detecting fake videos have been studied in re-\nsponse. Fake videos can be detected by locating and detect-\ning artiﬁcial parts within frames or between frames. Net-\nworks that search for factitious parts in a frame are mainly\ncomposed of the CNN by considering spatial characteris-\ntics. In the DFDC full dataset [13] released by Facebook\nAI, EfﬁcientNetwork-b7 [45] became the SOTA model. In\nthis paper, we ﬁnd fake videos using these spatial character-\nistics.\nThe Transformer model has currently been actively re-\nsearched in the ﬁeld of NLP and Computer VIsion [50]. In\nBERT [12], only the encoder part of Trasnformer was used\nto produce state-of-the-art models on GLUE, MultiNLI,\nSQuAD v1.1 and SQuAD v2.0. Another model, GPT [6],\n[43], [42], uses only the transformer’s decoder and boasts\nhigh performance in the ﬁeld of document generation.\nThe ﬁrst vision transformer [15] by the Google, origi-\nnally has been used in NLP task. It contributed in clas-\nsiﬁcation task. Another Transformer network called DeiT\n[48] by facebook used a distillation methodology and gave\nhigher accuracy than VIT (Vision Transformer) even fewer\ndatasets. From this observation, we add a distill token to\nthe VIT and consider patch embedding and CNN features\ntogether for the model input to create a more generalized\nmodel for deepfake detection. Based on this, we can expect\nthat this model is more robust on deepfake detection. This\npaper is organised as follows: In Section 2, we describe the\nrelated works. The proposed scheme is explained in Section\n3. Section 4 will give experimental results and analysis. Fi-\nnally, we will make a concluding remark in Section 5.\n2. Related Work\n2.1. Face synthesis\nImage sysnthesis network are divided into two ap-\nproaches: Generative Adversarial Netowrk (GAN) [17] and\nVariational AutoEncoder (V AE) [29]. The GAN networks\nhave more application models than V AE. Especially cGAN\n[35], WGAN [5], WGAN-GP [51], PGGAN [25], DCGAN\n[41], DiscoGAN [28], CycleGAN [53], StarGAN [9], and\nStyleGAN [26] are well-known. Currently, StyleGAN-V2\n[27] is a leader in facial synthesis as well as many synthetic\nprograms being shared as open source anyone can use. The\nﬁrst network to produce deepfake videos is the V AE. As\nsuggested by deepfake autoencoder (DFAE), source images\narXiv:2104.01353v1  [cs.CV]  3 Apr 2021\nwhich are the faces to be synthesized to the target are trained\nby each two V AE models and then switched the last decoder\npart that identiﬁes who it is. As a result, images are changed\nfrom the face of the target video to the face of the source\nvideo.\nAccording to [39], this approach applies to deepfake pro-\ngram such as DeepFaceLab [1], DFaker [3], and DeepFaketf\n[2]. Face Swapping GAN (FSGAN) is subject agnostic\nface swapping and reenactment without requiring training\non those faces [40]. They used a face mixing network to\nblend the two faces seamlessly while preserving the target\nskin color and lighting conditions.\n2.2. Deepfake detection\nMost conﬁgured models in deepfake detection is based\non the CNN structure. There are two approaches for dis-\ncriminating deepfake videos. One is to exploit unnatural\nspatial properties within one frame of video as an image\nunit, and the other is to exploit temporal properties to ﬁnd\nunnaturalness between video frames.\nMontserrat detects the space-time awkwardness by\nputting the frames of the video into the efﬁcientnet and each\nfeature into the Gated Recurrent Unit (GRU) [36]. Sim-\nilarly, G ¨uera use CNN to extract frame-level features and\ntrain a RNN that learns to classify fake videos [19]. In ad-\ndition, Unlike previous studies using CNN and RNN net-\nworks to ﬁnd out spatio-temporal properties, de Lima [11]\nuse 3DCNN to detect them at once. They use I3D [8],\nR3D [21], MC3 [49] because of higher performing network\narchitectures. As the method of optical ﬂow based cnn,\nAmerini proposed a optical ﬂow ﬁeld to exploit possible\ninter-frame dissimilarities [4].\nTo detect the spatial of manipulated in the face, Li\n[31] use CNN model such as VGG16 [46], ResNet50,\nResNet101, and ResNet152 [22]. Nguyen proposed a cap-\nsule network that can detect various kinds of deepfakes\n[38]. they use the features pretrained by VGG16 and sug-\ngested Capsule-Forensics architecture. Classical classiﬁca-\ntion method using the SVM was proposed by Yang [52] and\nGuarnera used K-nearest neighbors and linear discriminant\nanalysis [18].\n3. Proposed Deepfake Detection Algorithm\nIn this section, we describe the Vision Transformer ar-\nchitecture for deepfake detection. Our baseline follows Vi-\nsion Transformer network with adding distillation token.\nInput sequences are combined patch embedding with the\nCNN features. The entire network is shown in Figure 1. We\nintroduce the role and characteristic of Vision Transformer\nfor deepfake detection in Section 3.1, Speciﬁcally, we il-\nlustrate that how the input is consisted in Section 3.2 and\ndistillation method with teacher network in Section 3.3.\n3.1. Base network architecture\nWe overview the Vision Transformer [15] and recognize\nits effectiveness in the ﬁeld of deepfake detection. Trans-\nformer was originally used for NLP task but recently, there\nare many attempts to apply on image modeling [16], [37],\n[34]. The Vision Transformer only has encoder similar to\nBERT, which has position information and embedding se-\nquences.\nIn [15], before the multi head self attention layers, im-\nage x∈R(H×W×C) split into patch xp ∈R(H/P×W/P ×E)\nby learnable embedding, where (H,W) is resolution of the\nimage, C is channel, P is patch size and E is the num-\nber of embedding feature. All patches are ﬂattened by\nlinear projection and add to the position embedding equal\nin (H/P ×W/P). The Transformer encoder consists of\nmultiheaded self attention (MSA) and multi layer percep-\ntron (MLP). MLP contains two layers with a Gaussian Er-\nror Linear Unit (GELU) non-linearity [15]. We follow the\nMSA, MLP function as encoder to discriminate robustly\nfake video detection.\nSequences of feature vectors include all part of the im-\nage. In addition, an encoder refer to all of the sequences\nof split patches. The previous CNN structure employed at-\ntention only activated part of face and could not refer to\nother distant position. However, input sequences depend on\nglobal information and this point can reduce the overﬁtting\nproblem in transformer. Also, we ﬁnd out interesting result\nthat transformer makes relatively fair classiﬁcation of real\nand fake videos, rather than skewed to either side, unlike\nthe previous CNN model.\n3.2. Combination of patch embedding and CNN fea-\ntures\nWe introduce the input tokens before feeding to encoder,\nunlike the original oneself from input vectors of vision\ntransformer. We deﬁne Zp = (x1\npE,x2\npE,··· ,xN\np E) and\nZf = f(x) = (x1\nf ,x2\nf ,··· ,xM\nf ), where xp is a patch, E\nis a learnable embedding, N is exponential the number of\nsplit patches, M is the number of CNN features and f(·) is\na CNN model. Thus, Zf means feature of CNN model and\nwe use f as EfﬁcientNet.\nThese features are combined as Zp ⊕Zf and applied\nglobal pooling. Min Lin suggested that the global average\npooling was more interpretable between feature maps and\ncategories [33]. We representZp⊕f = globalpooling(Zp ⊕\nZf ) as input vectors, by N + M to N(vectors number).\nAs a result, we consider not only the main part features of\nface, but also all parts correlation. By using thie approach,\nwe can get better AUC and f1 score than only using patch\nembedding or CNN features.\nimg\npatch tokens\nEﬃcient \nnet\nself-a/g425en/g415on\nFFN\nself-a/g425en/g415on\nFFN\nself-a/g425en/g415on\nFFN\nself-a/g425en/g415on\nFFN\nclass \ntoken\ndis/g415lla/g415on \ntoken Global Pooling\nFigure 1. The proposed overall deepfake detection network. The image split into patches and pass EfﬁcientNet [47]. We got (Batch, N,\nembedding features), (Batch, M, embedding features) respectively. These tokens are concatenated, through global pooling, and fed to\ntransformer encoder. Encoder consist of Multi-Self Attention (MSA) and 2 layers of GeLU function. We add distillation token which is\ntrained by the teacher network.\n3.3. Distillation method and teacher network\nWe add class token and distillation token to input\nZp ⊕Zf , so we deﬁne ﬁnal input Z0 = [xclass; ZP ⊕\nZf ; xdistillation] +Epos, where xclass and xdistillation is\ntoken for training by label and teacher network, Epos is the\nlearnable position embedding. Finally, we can deﬁne a the\nset train loss as:\nLfake = λLBCE (Zcfake ,y)+\n(1 −λ)LBCE (Zdfake ,σ(Ztfake )), (1)\nLreal = λLBCE (Zcreal ,y)+\n(1 −λ)LBCE (Zdreal ,σ(Ztreal )), (2)\nLtrain = (Lfake + Lreal)/2, (3)\nwhere (Ztfake , Ztreal ) are the logits of the teacher model\nfor fake prediction and real prediction, (Zdfake , Zdreal ) and\n(Zcfake , Zcreal ) are the logits of the distillation tokens and\nthe class tokens for fake prediction and real prediction. We\nset λby 1\n2 , binary cross entropy (LBCE ) on the labels yand\nσthe sigma function.\nIn [48] by Facebook AI, a distillation method has the\neffect of preventing overﬁtting by expanding the range of\nweights of labels. Also, when teacher network was the CNN\nmodel, transformer produced the best result than other mod-\nels.\nTherefore, we choose the teacher network as Efﬁcient-\nNet which is the state of the art model on the DFDC dataset\nin deepfake detection. Each class token and distillation to-\nken represents the probability that the video is fake. The\ndistillation tokens are used instead of class token when test-\ning. It outperforms class token on test dataset.\n4. Experiments\nIn this section, we describe the dataset and detail of pa-\nrameter. Also, we compare to the SOTA model, represent-\ning condition of performance measurements. We will ex-\nplain why we use the DFDC dataset in Section 4.1. Also,\nwe describe the parameter setting and conﬁguration envi-\nronments required in the training process in Section 4.2, and\nanalyze the experimental results in Section 4.3 compared to\nthe SOTA model.\n4.1. DFDC Dataset\nIn kaggle contest 1, they opened DFDC Preview dataset\n[14] and later, facebook AI opened full DFDC dataset [13].\nThe DFDC dataset is the largest and public-available deep-\nfake dataset, including about 100,000 total videos produced\nby generative adversarial network (GAN).\nIn [13], the face swap datasets are divided into three\ngenerations. First-generation datasets such as DF-TIMIT\n[30], UADFV [52], and FaceForensics++DF (FF++DF)\n[44] have 104 106 frames and up to 5000 videos. Second-\ngeneration datasets are Celeb-DF [32] and DFDC Preview\n[14]. DFDC full dataset is third-generation and has 128,154\n1https://www.kaggle.com/c/deepfake-detection-challenge\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n 0.6\n 0.65\n 0  5  10  15  20  25  30  35  40\nfake loss\nepoch\nEfficientNet-b7\nOur model\n(a)\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0  5  10  15  20  25  30  35  40\nreal loss\nepoch\nEfficientNet-b7\nOur model\n(b)\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 0.45\n 0.5\n 0.55\n 0.6\n 0.65\n 0  5  10  15  20  25  30  35  40\nreal loss\nepoch\naverage SOTA\naverage our model\n(c)\nFigure 2. The results of loss in validation DFDC dataset: (a) repre-\nsents the loss for fake video, (b) represents the loss for real video,\nand (c) represents the average loss. The green plot means our\nmodel’s loss and the purple plot means the SOTA model’s loss.\nThere is little difference in loss of real video, but much difference\nin fake video.\ntotal videos and 104,500 unique fake videos.\nFor these reasons, we choose the largest deepfake dataset\nand compare the performance with the state-of-the-art\n(SOTA) on the DFDC full dataset. In the analysis of Dol-\nhansky [13], the submitted best model has 0.734 AUC in\nthe private test set. Also, the higher average precision of\nsubmitted models (such as [45], [20], [10], [24], [23]) in\nDFDC dataset, the better performance in real video [13].\nTherefore, if the performance is good in the DFDC dataset,\nthe results can be generalized in real video.\n4.2. Training Detail\nPre-processing : We used a face detector as the Multi\ntask Cascaded Convolutional Networks (MTCNN) and crop\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Precision\nNo Skill\nOur model\nSOTA\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nNo Skill\nOur model\nSOTA\nFigure 3. ROC-AUC curve. Orange is the curve of our model and\ngreen is the curve of SOTA model. Our model has larger area than\nthe compared SOTA.\nall the frames to 384x384. We augmented our training\ndata using Albumentations [7]. In addition, we cutout and\ndropout the part of the image simliar to [45]. Our pre-\nprocessing procedure was based on Selim[45].\nTraining : Our patch size for the embedding features is\n32 and the embedding dimension is 1024. We initialize our\ntransformer and Efﬁcientnet-b7 model by the pre-trained\nmodel. We set the transformer 16-heads and 24-layers\nwhich is identical to the large VIT default model. Also,\nour teacher network used a pre-trained one by Efﬁcientnet-\nB7 [45] on the DFDC dataset. We used a distillation token\nwhen testing.\nParameter : training and test were done on a V100-GPU\nmachine with a batch size of 12 for training. We used an\nSGD optimizer with an initial learning rate of 0.01 and a dif-\nferent learning rate reduction policy which is a step-based\nmethod. The training epoch is 40, batches per epoch are\n2500 and it takes 2 days on single V100-GPU.\nFor classiﬁcation, we propagate logit values backward\nfrom the transformer model using the binary cross entropy.\nWe tested on the public available DFDC 5000 test dataset.\nThe f1 score was measured for comparison with the SOTA\nmodel with the 0.55 of threshold value.\n4.3. Performance analysis\nWe compare our model to the state-of-the-art model [45].\nWe trained our model by train dataset and chose the model\nweights with the lowest loss in the validation set. In Figure\n2, we compare the validation loss with the SOTA model [45]\nReal:0\nFake:1\nPredicted label\nReal:0\nFake:1 True label\n2388 112\n335 2165\nConfusion matrix\nReal:0\nFake:1\nPredicted label\nReal:0\nFake:1 True label\n2278 222\n187 2313\nConfusion matrix\nFigure 4. We set the threshold 0.55 which is probability of fake\nvideos. Top-right, top-left, bottom-right, bottom-left means false-\npositive, true-negative, true-positive, false-negative in order. Con-\nfusion Matrix of the left side is previous SOTA model’s prediction\nand the right side is our results. We can see that our model predict\nclearly on fake videos.\non real and fake video respectively. This graph shows that\nour model is a more robust classiﬁer on fake videos. Also,\nReal loss is similar, but the overall average loss was lower.\nThe test loss is deﬁned as:\nLogLoss = −1\nn\nn∑\ni=1\n[yilog(ˆyi + (1−yi)log(1 −ˆyi)], (4)\nwhere nis the number of videos being predicted, ˆyi is the\npredicted probability of the video being FAKE,yi is 1 if the\nvideo is FAKE, 0 if REAL. We getˆyi by distillation tokens.\nIn addition, our model’s ROC-AUC curve (0.978) has a\nlarger area than the SOTA model (0.972) in Figure 3. It\nrepresents the proposed classiﬁer is a more robust than the\nSOTA model on fake videos, because the precision is higher\nthan the SOTA model as the recall is close to 1.\nTo verify robust classiﬁcation, the confusion matrix was\nobtained by setting the threshold of 0.55, which represents\nthe probability of fake video. It represents the predicted\nnumber of video according to each label in Figure 4. Each\nmodel’s result of false negative was 335 and 187. Thus, we\ncan see that the proposed model is robust in fake videos and\nthe f1 score was 91.9, which was higher than 90.6 of the\nSOTA model.\nTo ﬁgure out the correlation of predictions between the\nSOTA model and our model, we scatter all the fake video\nprediction probabilities for 5000 videos. In Figure 5, when\nthe x-axis is the value predicted by our model and the y-\naxis is the value predicted by SOTA, the correlated data are\nconcentrated on (0,0) and (1,1). We noted that, in the part\nthat our model predicts as 1 (predicted as fake video), the\nprobability values of the SOTA model are evenly spread.\nThis condition describes that our model can predict a fake\nvideo more clearly.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5. Correlation of prediction values between our model\nand SOTA. The x-axis is the predicted probability of our model\nwhether the video is fake or not, and the y-axis is the predicted\nvalue of the SOTA model. We note that our model classiﬁes fake\nvideos clearly but the SOTA model is not conﬁdent about many\nfake videos.\n5. Conclusion\nIn this paper, we have proposed a robust Vision Trans-\nformer model for Deepfake detection. The proposed\nscheme was combined vision transformer and EfﬁcientNet\nin the patch embedding level and distillation technique.\nWe demonstrated the efﬁciency of the robust Vision Trans-\nformer model compared to the previous SOTA model, Efﬁ-\ncientNet, which consisted of a CNN network. We observed\n0.972 of AUC for the SOTA model and 0.978 for our model\nunder conditions of the same environment without ensem-\nble. For f1 score, the proposed scheme gave the better per-\nformance as 91.9 while the STOA model achieved 90.6 in\nsame threshold condition threshold of (0.55).\nReferences\n[1] Deepfacelab. Available at https://github.\ncom/iperov/DeepFaceLab.\n[2] Deepfake tf: Deepfake based on tensorﬂow. Avail-\nable at https://github.com/StromWine/\nDeepFake_tf.\n[3] Dfaker. Available at https://github.com/\ndfaker/df.\n[4] Irene Amerini, Leonardo Galteri, Roberto Caldelli,\nand Alberto Del Bimbo. Deepfake video detection\nthrough optical ﬂow based cnn. In Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion Workshops, pages 0–0, 2019.\n[5] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.\nWasserstein generative adversarial networks. In Inter-\nnational conference on machine learning, pages 214–\n223. PMLR, 2017.\n[6] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\n[7] Alexander Buslaev, Vladimir I Iglovikov, Eugene\nKhvedchenya, Alex Parinov, Mikhail Druzhinin, and\nAlexandr A Kalinin. Albumentations: fast and ﬂexible\nimage augmentations. Information, 11(2):125, 2020.\n[8] Joao Carreira and Andrew Zisserman. Quo vadis,\naction recognition? a new model and the kinet-\nics dataset. In proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages\n6299–6308, 2017.\n[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-\nWoo Ha, Sunghun Kim, and Jaegul Choo. Stargan:\nUniﬁed generative adversarial networks for multi-\ndomain image-to-image translation. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 8789–8797, 2018.\n[10] Azat Davletshin. https:\n//github.com/NTech-Lab/\ndeepfake-detection-challenge.\n[11] Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake\nKarwoski, and Annet George. Deepfake detection\nusing spatiotemporal convolutional networks. arXiv\npreprint arXiv:2006.14749, 2020.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[13] Brian Dolhansky, Joanna Bitton, Ben Pﬂaum, Jikuo\nLu, Russ Howes, Menglin Wang, and Cristian Can-\nton Ferrer. The deepfake detection challenge dataset.\narXiv preprint arXiv:2006.07397, 2020.\n[14] Brian Dolhansky, Russ Howes, Ben Pﬂaum, Nicole\nBaram, and Cristian Canton Ferrer. The deepfake\ndetection challenge (dfdc) preview dataset. arXiv\npreprint arXiv:1910.08854, 2019.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[16] Rohit Girdhar, Joao Carreira, Carl Doersch, and An-\ndrew Zisserman. Video action transformer network.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 244–253,\n2019.\n[17] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial\nnetworks. arXiv preprint arXiv:1406.2661, 2014.\n[18] Luca Guarnera, Oliver Giudice, and Sebastiano Bat-\ntiato. Deepfake detection by analyzing convolutional\ntraces. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition Work-\nshops, pages 666–667, 2020.\n[19] David G ¨uera and Edward J Delp. Deepfake video de-\ntection using recurrent neural networks. In 2018 15th\nIEEE International Conference on Advanced Video\nand Signal Based Surveillance (AVSS) , pages 1–6.\nIEEE, 2018.\n[20] Hao Cui Hanqing Zhao and Wenbo Zhou. https:\n//github.com/cuihaoleo/kaggle-dfdc.\n[21] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh.\nLearning spatio-temporal features with 3d residual\nnetworks for action recognition. In Proceedings of\nthe IEEE International Conference on Computer Vi-\nsion Workshops, pages 3154–3160, 2017.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition.\nIn Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n[23] James Howard and Ian Pan. https://github.\ncom/jphdotam/DFDC/.\n[24] Zhenfei Yin Zheng Fang Guojun Yin Siyu\nChen Ning Ning Jing Shao, Huafeng Shi and\nYu Liu. https://github.com/Siyu-C/\nRobustForensics.\n[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko\nLehtinen. Progressive growing of gans for im-\nproved quality, stability, and variation. arXiv preprint\narXiv:1710.10196, 2017.\n[26] Tero Karras, Samuli Laine, and Timo Aila. A style-\nbased generator architecture for generative adversar-\nial networks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 4401–4410, 2019.\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of stylegan. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8110–8119, 2020.\n[28] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim,\nJung Kwon Lee, and Jiwon Kim. Learning to\ndiscover cross-domain relations with generative\nadversarial networks. In International Conference on\nMachine Learning, pages 1857–1865. PMLR, 2017.\n[29] Diederik P Kingma and Max Welling. Auto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114 ,\n2013.\n[30] Pavel Korshunov and S ´ebastien Marcel. Deepfakes: a\nnew threat to face recognition? assessment and detec-\ntion. arXiv preprint arXiv:1812.08685, 2018.\n[31] Yuezun Li and Siwei Lyu. Exposing deepfake videos\nby detecting face warping artifacts. arXiv preprint\narXiv:1811.00656, 2018.\n[32] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and\nSiwei Lyu. Celeb-df: A large-scale challenging\ndataset for deepfake forensics. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 3207–3216, 2020.\n[33] Min Lin, Qiang Chen, and Shuicheng Yan. Network\nin network. arXiv preprint arXiv:1312.4400, 2013.\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030 ,\n2021.\n[35] Mehdi Mirza and Simon Osindero. Condi-\ntional generative adversarial nets. arXiv preprint\narXiv:1411.1784, 2014.\n[36] Daniel Mas Montserrat, Hanxiang Hao, Sri K Yarla-\ngadda, Sriram Baireddy, Ruiting Shao, J´anos Horv´ath,\nEmily Bartusiak, Justin Yang, David Guera, Fengqing\nZhu, et al. Deepfakes detection with automatic face\nweighting. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\nWorkshops, pages 668–669, 2020.\n[37] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan\nAsselmann. Video transformer network. arXiv\npreprint arXiv:2102.00719, 2021.\n[38] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen.\nUse of a capsule network to detect fake images and\nvideos. arXiv preprint arXiv:1910.12467, 2019.\n[39] Thanh Thi Nguyen, Cuong M Nguyen, Dung Tien\nNguyen, Duc Thanh Nguyen, and Saeid Nahavandi.\nDeep learning for deepfakes creation and detection: A\nsurvey. arXiv preprint arXiv:1909.11573, 2019.\n[40] Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan:\nSubject agnostic face swapping and reenactment. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 7184–7193, 2019.\n[41] Alec Radford, Luke Metz, and Soumith Chintala. Un-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks. arXiv preprint\narXiv:1511.06434, 2015.\n[42] Alec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. Improving language understand-\ning by generative pre-training. 2018.\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners. OpenAI blog,\n1(8):9, 2019.\n[44] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva,\nChristian Riess, Justus Thies, and Matthias Nießner.\nFaceforensics++: Learning to detect manipulated fa-\ncial images. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1–11,\n2019.\n[45] Selim Seferbekov. https://github.com/\nselimsef/dfdc_deepfake_challenge.\n[46] Karen Simonyan and Andrew Zisserman. Very deep\nconvolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556, 2014.\n[47] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages\n6105–6114. PMLR, 2019.\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efﬁcient image transformers\n& distillation through attention. arXiv preprint\narXiv:2012.12877, 2020.\n[49] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray,\nYann LeCun, and Manohar Paluri. A closer look\nat spatiotemporal convolutions for action recognition.\nIn Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition , pages 6450–6459,\n2018.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need.\narXiv preprint arXiv:1706.03762, 2017.\n[51] Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, and\nLiqiang Wang. Improving the improved training of\nwasserstein gans: A consistency term and its dual ef-\nfect. arXiv preprint arXiv:1803.01541, 2018.\n[52] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep\nfakes using inconsistent head poses. In ICASSP 2019-\n2019 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 8261–\n8265. IEEE, 2019.\n[53] Jun-Yan Zhu, Taesung Park, Phillip Isola, and\nAlexei A Efros. Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks. In\nProceedings of the IEEE international conference on\ncomputer vision, pages 2223–2232, 2017.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8118928670883179
    },
    {
      "name": "Overfitting",
      "score": 0.7871489524841309
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7093108296394348
    },
    {
      "name": "Transformer",
      "score": 0.6499410271644592
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5467435717582703
    },
    {
      "name": "Machine learning",
      "score": 0.5362529158592224
    },
    {
      "name": "Deep learning",
      "score": 0.48500892519950867
    },
    {
      "name": "Encoder",
      "score": 0.47590693831443787
    },
    {
      "name": "Generative adversarial network",
      "score": 0.4604414701461792
    },
    {
      "name": "Embedding",
      "score": 0.44938698410987854
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44164949655532837
    },
    {
      "name": "Generative grammar",
      "score": 0.4230969548225403
    },
    {
      "name": "Artificial neural network",
      "score": 0.2470439076423645
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 41
}