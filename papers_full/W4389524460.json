{
  "title": "Biomedical Parallel Sentence Retrieval Using Large Language Models",
  "url": "https://openalex.org/W4389524460",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093458310",
      "name": "Sheema Firdous",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108669064",
      "name": "Sadaf Abdul Rauf",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W7072217406",
    "https://openalex.org/W14574270",
    "https://openalex.org/W2876799257",
    "https://openalex.org/W2905749056",
    "https://openalex.org/W7075682760",
    "https://openalex.org/W6782179003",
    "https://openalex.org/W2902363747",
    "https://openalex.org/W2758685863",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2767176684",
    "https://openalex.org/W2902713373",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2890026292",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2575814162",
    "https://openalex.org/W6815833287",
    "https://openalex.org/W3088331602",
    "https://openalex.org/W6750200984",
    "https://openalex.org/W2141068210",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W3119385107",
    "https://openalex.org/W2101096097",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6761843422",
    "https://openalex.org/W3016134300",
    "https://openalex.org/W2965538726",
    "https://openalex.org/W4288284086",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3103152812",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W4299574851",
    "https://openalex.org/W3049491733",
    "https://openalex.org/W2970618241",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W3089276103",
    "https://openalex.org/W2798931235",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962735107",
    "https://openalex.org/W2567571499",
    "https://openalex.org/W2798389157",
    "https://openalex.org/W2250681176",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4386566675",
    "https://openalex.org/W4281715010",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1973152633",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3038033387",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2886198413",
    "https://openalex.org/W3098341425",
    "https://openalex.org/W2759183967",
    "https://openalex.org/W2963366389"
  ],
  "abstract": "We have explored the effect of in domain knowledge during parallel sentence filtering from in domain corpora. Models built with sentences mined from in domain corpora without domain knowledge performed poorly, whereas model performance improved by more than 2.3 BLEU points on average with further domain centric filtering. We have used Large Language Models for selecting similar and domain aligned sentences. Our experiments show the importance of inclusion of domain knowledge in sentence selection methodologies even if the initial comparable corpora are in domain.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 263–270\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n263\nBiomedical Parallel Sentence Retrieval using Large Language Models\nSheema Firdous\nFatima Jinnah Women University,\nPakistan\nsheemafirdous400@gmail.com\nSadaf Abdul Rauf\nFatima Jinnah Women University, Pakistan\nUniv. Paris-Saclay, CNRS, LIMSI France\nsadaf.abdulrauf@gmail.com\nAbstract\nWe have explored the effect of in domain\nknowledge during parallel sentence filtering\nfrom in domain corpora. Models built with sen-\ntences mined from in domain corpora without\ndomain knowledge performed poorly, whereas\nmodel performance improved by more than\n2.3 BLEU points on average with further do-\nmain centric filtering. We have used Large Lan-\nguage Models for selecting similar and domain\naligned sentences. Our experiments show the\nimportance of inclusion of domain knowledge\nin sentence selection methodologies even if the\ninitial comparable corpora are in domain.\n1 Introduction\nThis paper describes FJWU’s submission to the\nbiomedical translation task. This year the focus\nof our research was domain specific parallel cor-\npus mining from Wikipedia using Large Language\nModels, we explored the potential of the mined\nsentences using two sentence selection schemes.\nNeural Machine Translation (NMT) (Kalchbrenner\nand Blunsom, 2013; Sutskever et al., 2014; Bah-\ndanau et al., 2015; Cho et al., 2014) has witnessed\ngreat success over the years (Vaswani et al., 2017;\nZhang and Zong, 2020). NMT systems train on par-\nallel corpora to produce translations that capture\nlanguage intricacies and context with enormous\nprecision as compared to the previous counterpart\nStatistical Machine Translation (SMT) systems.\nMachine translation in the biomedical domain is\nbecoming increasingly important due to the critical\nnature of medical scientific texts. The majority of\nthese texts are published in English, and the goal of\nBiomedical Machine Translation is to make them\naccessible in multiple languages. However, this is\na complex undertaking due to the extensive nature\nof this field and the vast and diverse vocabulary it\nencompasses. This vocabulary includes specialized\nterms and non-lexical forms (such as dates and\nbiomedical entities) that pose unique challenges.\nConsequently, the quality of machine translation\noutput fluctuates depending on the availability of\nbiomedical resources tailored to each target lan-\nguage.\nAvailability of parallel corpora in reasonable\namounts has greatly enhanced the performance of\nNMT systems, especially for the high-resource lan-\nguages (Bojar et al., 2018). However, its efficacy\nremains sub optimal for low-resource languages\nand domain-specific contexts (Zoph et al., 2016;\nKoehn and Knowles, 2017; Lample et al., 2018;\nChu and Wang, 2020). Performance of NMT sys-\ntem degrades as soon as the application domain\ndeviates from training domain. Domain adaptation\n(Freitag and Al-Onaizan, 2016), transfer learning\n(Zoph et al., 2016; Khan et al., 2018; Abdul Rauf\net al., 2020), model fusion (Gulcehre et al., 2015),\nback translation (Sennrich et al., 2015; Ul Haq\net al., 2020), fine-tuning (Dakwale and Monz, 2017;\nHuck et al., 2018), data augmentation (Fadaee et al.,\n2017), data selective training (Van Der Wees et al.,\n2017; Knowles and Koehn, 2018), decoding strate-\ngies (Park et al., 2020), zero-shot translation (John-\nson et al., 2017) are some of the techniques used\nto address this issue. We will be focusing on do-\nmain adaptation using data augmentation and fine\ntuning.\nFor this years submission we explore the poten-\ntial of Large-scale Language Models for extract-\ning parallel sentences from Wikipedia 1. French-\nEnglish parallel articles are scraped as detailed\nin Section 4. For learning sentence embeddings\nof scraped bilingual data, rather than training en-\ncoders from scratch, we leverage the potential of\nLLM in parallel sentence extraction from our bilin-\ngual scraped articles. We used LEALLA-Large, a\nlightweight system developed by (Mao and Nak-\nagawa, 2023) to compute the language-agnostic\nlow-dimensional sentence embeddings for each\n1An online multilingual encyclopedia https://en.\nwikipedia.org/wiki/Main_Page\n264\nsentence in the English and French parallel arti-\ncles. Potential parallel sentences are filtered based\non the similarity scores. These sentence are then\nfurther domain filtered by comparing the closeness\nwith Medline Titles embeddings computed using\nTransformers MiniLM. Our experiments show the\nimportance of inclusion of domain knowledge in\nsentence selection methodologies even if the ini-\ntial comparable corpora are in domain. Our main\ncontributions include:\n• Presenting a methodology for domain inclu-\nsion in sentence retrieval tasks by using capa-\nbilities of Large Language Models\n• Highlighting the importance of inculcation\nof in domain knowledge in sentence retrieval\ntasks even when the data source is in domain\n• Release of the mined parallel corpora to the\nresearch community2\nThe paper is structured as follows: Section 2\npresents a brief overview of background and re-\nlated work, Section 3,4 elaborates the data collec-\ntion pipeline, Section 5 outline the NMT experi-\nments and results, followed by the conclusion of\nthis study.\n2 Related Work\nRecent work on parallel sentence extraction has fo-\ncused on lightweight end-to-end word-level and\nsentence-level embedding methods (Guo et al.,\n2018; Artetxe and Schwenk, 2018; Yang et al.,\n2019a). These embedding-based approaches have\ngained success (Grégoire and Langlais, 2017;\nBouamor and Sajjad, 2018; Schwenk, 2018) as\nthese systems outperformed the large-distributed\ncomputationally intensive systems (Uszkoreit et al.,\n2010; Abdul-Rauf and Schwenk, 2009) used to\nmine parallel documents. Bilingual sentence em-\nbeddings, learned from dual-encoder models, have\nalso been used effectively for parallel corpus min-\ning (Guo et al., 2018). Cross-lingual embeddings\nencode bilingual texts into a single unified vector\nspace allowing nearest-neighbor search can be used\nto find potential translation candidates. These em-\nbedding approaches produce noisy matches that re-\nquire a re-scoring step in order to obtain a clean par-\nallel sentence retrieval as addressed by (Yang et al.,\n2https://github.com/sabdul111/\nBiomedical-Parallel-Corpus\n2019a) who explored using a bi-directional dual\nencoder with additive margin softmax (Wang et al.,\n2018) which results in state-of-the-art performance\nfor sentence filtering. Multilingual sentence em-\nbedding approaches (Artetxe and Schwenk, 2018;\nChidambaram et al., 2018) also show promising\nresults.\nSince language-specific models often demand\nextensive amounts of labeled data for training\nand can be limited by their language-specific\nparameters, language-agnostic sentence embed-\nding(Artetxe and Schwenk, 2019; Yang et al.,\n2019b; Reimers and Gurevych, 2020; Feng et al.,\n2020; Mao et al., 2022) align multiple languages in\na shared embedding space, facilitating parallel sen-\ntence alignment that extracts parallel sentences for\ntraining translation systems. Among them, LaBSE\n(Feng et al., 2020) achieved state-of-the-art perfor-\nmance on various bi-text retrieval. The problem\nof inference speed and computation overhead of\nlarge language models was addressed by (Mao and\nNakagawa, 2023) who proposed Learning Leight-\nWeight Language-agnostic Sentence Embeddings\n(LEALLA) with Knowledge Distillation (Kim and\nRush, 2016). They reported significant reduction\nin computation overhead and inference speed by\nproviding language-agnostic low-dimensional sen-\ntence embeddings. We also use LEALLA in the\nsecond phase of our pipeline for parallel sentence\nalignemnent.\n3 Wikipedia as a potential resource for\nbiomedical data\nOur primary objective was to collect a compre-\nhensive dataset from the biomedical domain, we\nexplored Wikipedia’s key biological categories and\nselected those having a substantial volume of arti-\ncles. A brief overview of the selected subdomains\nis given below:\n1. Biodbs 3 refers to biological databases and\ncontains links of a variety of biological\ndatabases.\n2. Genome Reference Consortium is an inter-\nnational collaboration dedicated to creating\nand maintaining the most accurate and up-to-\ndate Human Genome 4 reference sequence.\n3https://en.wikipedia.org/wiki/List_of_\nbiological_databases\n4https://en.wikipedia.org/wiki/Human_genome\n265\nDomain Scraped URLs Scraped Articles Parallel Articles Unique Articles\nFrench English French English\nBiodbs 39.4K 77.3K 39.3K 68.7K 39.3K 1.2K\nHuman Genome 25.9K 59.1K 25.9K 49K 25.9K 25.9K\nHealth BioMed 42.8K 122.5K 42.8K 92.5K 42.8K 14.7K\nNCBI 64.2K 133.8K 64K 133.6K 64K 51.2K\nPubmed 62.9K 134.5K 62.9K 117.4K 62.9K 22.4K\nTotal 235.2K 527.2K 234.9K 461.2K 234.9K 115.4K\nTable 1: Scraped Data per subdomain\n3. National Institute of Biomedical Imaging\nand Bio engineering plays a central role in\nadvancing biomedical engineering research\nand provides a wealth of data and resources in\nthe domain of Health Biomedical Engineering\n5.\n4. The National Center for Biotechnology In-\nformation (NCBI) 6 is a U.S. government\nagency that provides an extensive collection\nof biomedical and genomic resources.\n5. PubMed 7 is a widely used online database\nmaintained by the National Library of\nMedicine (NLM) which provides access to\na vast collection of biomedical literature.\n4 Parallel Corpus Mining\nThis section presents an overview of our parallel\ndata creation pipeline. Wikipedia has been exten-\nsively used as a data resource for corpus devel-\nopment (Chu et al., 2014; Tufi¸ s et al., 2013; Ste-\nfanescu et al., 2012; Karimi et al., 2018; Aghae-\nbrahimian, 2018; Schwenk et al., 2019). We also\nused Wikipedia’s inter language links to mine po-\ntential parallel sentences by exploring the potential\nof Large language models for filtering the closet\ncandidates. Our data preparation pipeline involves\nthree main steps; 1) Domain specific web scraping,\n2) Candidate sentence scoring and filtering and 3)\nDomain adapted filtering.\nParallel article scrapping To extract the bilin-\ngual data we used Wikipedia’s Interwiki8 (also\nknown as inter language links) property (Adafre\n5https://en.wikipedia.org/wiki/Biomedical_\nengineering#Hospital_and_medical_devices\n6https://en.wikipedia.org/wiki/National_\nCenter_for_Biotechnology_Information\n7https://en.wikipedia.org/wiki/PubMed\n8The Interwiki property links the articles across various\nlanguage editions of Wikipedia.\nand De Rijke, 2006; Otero and López, 2010; Chu\net al., 2014; Aghaebrahimian, 2018). English\nWikipedia has consistently held the distinction of\npossessing the highest article count among all lan-\nguage editions of Wikipedia. As of August 2023,\nthere are 6,696,0719 articles in English containing\nover 4.3 billion words.\nWe maximized recall in our article selection pro-\ncedure by choosing English as the base language\nsince it provided wider coverage of topics. Thus,\nfor each unique English article, the corresponding\nFrench article (if found) was scrapped. We named\nthe scrapped articles using the title of the English\nversion, distinguishing them with .en for English\nand .fr for French files. At this stage, we had to re-\ntrieve the parallel articles since many of the English\narticles did not have the corresponding French arti-\ncles (see Table 1). For parallel article retrieval, we\ncompiled a list of all French articles and used this\nlist to retrieve parallel English articles which re-\nsulted in our parallel French-English articles. The\nsubdomains (see section § 3) had many overlapping\narticles which were removed and unique articles\nfrom each subdomain were selected.\nTable 1 shows the amount of URLs, articles, paral-\nlel articles and the corresponding unique articles.\nAt this stage we have unique parallel articles from\neach subdomain.\nParallel sentence filtering We used a\nlightweight pre-trained large language model\nLEALLA-Large (Mao and Nakagawa, 2023)\nwhich computes sentence embedding of 256\ndimensions by distilling knowledge from LaBSE\n(Feng et al., 2020). It can be used to mine potential\nparallel sentences by finding the nearest neighbour\nof each source sentence in the target side according\nto cosine similarity, and filtering those below a\nthreshold.\n9https://en.wikipedia.org/wiki/Wikipedia:\nSize_of_Wikipedia\n266\nDomain Parallel Sentences\nThreshold 90 Threshold 85 Threshold 80\nBiodbs 1,188 3,240 4,944\nHuman Genome 25,975 19,849 62,499\nHealth BioMed 14,677 41,555 66,008\nNCBI 65,591 198,692 328,621\nPubmed 16,853 46,273 72,741\nTotal 124,284 309,609 534,813\nTable 2: Parallel Sentences from the unique articles based on similarity threshold computed using LEALLA.\nParallel Sentences BioFiltered Parallel Sentences\nThreshold 20 Threshold 10 Threshold 0\nThreshold 90 3,602 16,861 47,964\nThreshold 85 15,286 64,888 169,215\nThreshold 80 23,727 101,845 275,063\nTotal 42,615 183,594 492,242\nTable 3: Bio-Filtered: Parallel sentences from Table 2 selected based on their proximity with Medline titles using\nMiniLM.\nLEALLA Embedding vector is computed for\neach sentence in the French and English article.\nThus for each French(source) sentence we have\nN potential matching sentences, where N is the\nnumber of sentences in English(target) article. The\ndot-product is then used to compute the similar-\nity between each source and N target candidate\nsentences. The top 10 candidate sentences are re-\ntrieved for each sentence. At this stage we have a\nsorted list of potential parallel sentences from each\nsubdomain.\nIt is important to note that these are potential bio\nmed domain sentences since these are mined from\nin-domain articles. We focus on both precision and\nrecall at this stage. Our sentence retrieval is recall\noriented, given that English articles were roughly\ndouble the French articles, thus using French sen-\ntence as prompt to retrieve the matching English\nsentences promised a wider search space. For final\nparallel corpus creations we selected the sentences\non similarity threshold. We report three thresh-\nolds (thresholds 80, 85, and 90) to retrieve parallel\nsentences from the retrieved top-10 sentence pairs.\nWe are working on lower threshold sentences. A\nhigher threshold indicates a greater degree of par-\nallelism between the sentences. Table 2 shows the\nnumber of parallel sentences retrieved using differ-\nent thresholds for each subdomain. We call these\nLLMfilter sentences for reference.\nIn domain filtering We did a second level selec-\ntion from the LLMfilter parallel sentences extracted\nin the previous step. Even though these sentences\ncome from bio-medical articles and are in-domain\nbut our hypothesis is that there will be many\nsentences that may categorize as general domain.\nOur second filter is to ensure collection of purely\nbiomedical sentences. For this we select Medline\ntitles (Jimeno Yepes et al., 2017) as biomedical\nrepresentative dataset since titles contain the main\ndomain terminologies. An embedding was gener-\nated for Medline Titles using sentence transformers\nparaphrase-multilingual-MiniLM-L12-v210\nwhich was then used to remove the out-domain\nsentences, striving to retain an optimal amount of\nin-domain sentences (pertaining to the biomedical\ndomain). Dot product of each sentence with the\nMedline titles embedding was used to compute\nthe similarity score(ranging from -1 to 1). We\nselected thresholds 20, 10, and 0 which correspond\nto 0.2, 0.1, and 0.0 respectively in the similarity\nscore. Table 3 shows the number of sentences per\nthreshold, we call these Biofilter sentences for\nreference.\nPost-processing involved the removal of excep-\ntionally short sentences, special characters, and sen-\ntences in languages other than the intended source\nand target languages. Duplicated and identical sen-\ntences were also removed from both English and\nFrench sides.\n5 Translation performance on retrieved\nsentences\nWe used Transformer base (Vaswani et al., 2017)\narchitecture provided by Fairseq (Ott et al., 2019)\n10https://huggingface.co/sentence-\ntransformers/paraphrase-multilingual-MiniLM-L12-v2\n267\nWMT20 testset\nModel Fine-tuning Model\nName LLMfilter Name Biofilter\nB1 - 19.52\nS1 B1 =>t90 18.12 SB1 20.29\nS2 B1 =>t85 18.41 SB2 20.29\nS3 B1 =>t80 18.54 SB3 20.58\nS4 B1 =>t90-t85-t80 18.78 SB4 21.11\nB2 - 38.71\nL5 B2 =>t90 19.69 LB1 21.81\nL6 B2 =>t85 20.57 LB2 21.88\nL7 B2 =>t80 20.62 LB3 22.07\nL8 B2 =>t90-t85-t80 20.36 LB4 22.43\nTable 4: BLEU scores on fine tuned datasets. B1 and B2 denote the baselines. B1 is trained on the biomedical texts\nprovided by the WMT’23 organizers, while B2 is a big model trained on general domain and biomed data.\nas transformer_iwslt_en_de. The ReLU activa-\ntion function was used in all encoder and decoder\nlayers. We optimize with Adam (Kingma and Ba,\n2015), set up with a maximum learning rate of\n0.0005 and an inverse square root decay schedule,\nas well as 4000 warmup updates.\nAll corpora were segmented into subword units\nusing Sentence Piece (Kudo and Richardson, 2018)\nwith a vocabulary of 32K units. We share the de-\ncoder input and output embedding matrices. Mod-\nels are trained with mixed precision and a batch\nsize of 4096 tokens on a single GPU. Systems were\ntrained until convergence based on the BLEU score\non the development sets. Evaluation was performed\nusing SacreBleu (Post, 2018). Scores are chosen\nbased on the best score on the development set\n(Medline 18, 19), and the corresponding scores for\nthat checkpoint are reported on Medline 20 test set.\nFor fine-tuned systems, the process starts with\nmodels trained to convergence, based on BLEU\nscore on dev sets. Training then resumes using a se-\nlected portion of the training corpus using the same\nparameters and criterion as for the base systems.\nBaseline We trained a smaller model B1 on the\nbiomedical texts provided by the WMT’23 or-\nganizers: Edp, Medline abstracts and titles (Ji-\nmeno Yepes et al., 2017), Scielo (Neves et al., 2016)\nand the Ufal Medical corpus11 consisting of Cesta,\nEcdc, Emea (OpenSubtitles), PatTR Medical and\nSubtitles. We used a bigger model B2 by (Xu et al.,\n2021) trained on WMT14 general domain corpus\nand WMT and supplementary biomed data includ-\ning B1 data.\n11https://ufal.mff.cuni.cz/ufal_medical_corpus\n5.1 Results and Discussion\nTable 4 presents the results using the two data selec-\ntion methods. LLMfilter column shows the BLEU\nscores on Medline 20 testset for sentences filtered\nbased on the sentence similarity score, whereas\nBiofilter are the sentences which were selected\nfrom the LLMfilter based on their closeness with\nthe Biomedical Medline titles. Both filters used\nLLMs for computing similarity as detailed in sec-\ntion 4.\nB1 represents a smaller baseline model trained\non all biomed data provided by WMT organizers\nhaving a BLEU score of 19.52. This was further\nfine-tuned using each threshold dataset i.e. thresh-\nold 90, 85, and 80 (represented by t90, t85, and t80\nrespectively in 4), and finally with a concatenation\nof the 3 thresholds. Concatenation refers to the\nunion of t90, t85, and t80. We did this to upsample\nthe higher quality corpora (i.e. t90) to analyze the\nimpact on MT. Evidently, none of the LLMfilter\nsentences improved the initial bio med baseline.\nThe Biofilter sentences on the other hand helped\nimprove the scores even when a small amount is\nadded e.g. for t90 and the scores improved con-\nsistently with the increase in the number of sen-\ntences with SB4 yielding an increase of 1.59 BLEU\npoints from the baseline. For the larger baseline\nB2, though none of the filtering schemes help im-\nprove the initial high score but still the supremacy\nof Biofilter sentences over LLMfilter is evident.\nArguably, both LLMfilter and Biofilter contain\nin-domain sentences as these have been selected\nfrom biomedical articles. The models built using\nthe same thresholds for the two schemes have a\ndifference of more than 2 BLEU points on average\n268\nwith Biofilter systems being superior. Our results\ndemonstrate the importance of inculcation of in-\ndomain knowledge in sentence retrieval tasks even\nif the data source is in-domain as there are many\nsentences that do not pertain specifically to the\ndomain and affect the results of domain-centered\ntranslation.\n6 Conclusion\nIn this study, we explored the potential of large lan-\nguage models for parallel sentence extraction from\ndomain-adapted bilingual corpus extracted from\nWikipedia. On our dataset, we experimented with\ntwo data selection schemes and assessed the NMT\nperformance for the biomedical domain. Our find-\nings demonstrate that merely web-mining from in-\ndomain corpus is not sufficient to improve domain-\nspecific NMT performance but there is also a need\nfor further filtering out out-domain sentences to\nimprove the domain-specific NMT systems. Lever-\naging large language models to extract in-domain\nparallel sentences resulted in improved NMT per-\nformance by outperforming the baseline with 2\nBLEU points.\nAcknowledgments\nThis study is funded by the National Research\nProgram for Universities (NRPU) by Higher\nEducation Commission of Pakistan (5469/Pun-\njab/NRPU/R&D/HEC/2016).\nReferences\nAbdul Rauf, S., Rosales Núñez, J. C., Pham, M. Q., and\nYvon, F. (2020). Limsi @ wmt 2020. In Proceed-\nings of the Fifth Conference on Machine Translation,\npages 803–812, Online. Association for Computa-\ntional Linguistics.\nAbdul-Rauf, S. and Schwenk, H. (2009). On the use of\ncomparable corpora to improve SMT performance.\nIn Proceedings of the 12th Conference of the Euro-\npean Chapter of the ACL (EACL 2009), pages 16–23,\nAthens, Greece. Association for Computational Lin-\nguistics.\nAdafre, S. F. and De Rijke, M. (2006). Finding similar\nsentences across multiple languages in wikipedia. In\nProceedings of the Workshop on NEW TEXT Wikis\nand blogs and other dynamic text sources.\nAghaebrahimian, A. (2018). Deep neural networks at\nthe service of multilingual parallel sentence extrac-\ntion. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1372–\n1383, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nArtetxe, M. and Schwenk, H. (2018). Margin-based\nparallel corpus mining with multilingual sentence\nembeddings. arXiv preprint arXiv:1811.01136.\nArtetxe, M. and Schwenk, H. (2019). Massively mul-\ntilingual sentence embeddings for zero-shot cross-\nlingual transfer and beyond. Transactions of the As-\nsociation for Computational Linguistics, 7:597–610.\nBahdanau, D., Cho, K., and Bengio, Y . (2015). Neural\nmachine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473.\nBojar, O., Federmann, C., Fishel, M., Graham, Y ., Had-\ndow, B., Huck, M., Koehn, P., and Monz, C. (2018).\nFindings of the 2018 conference on machine transla-\ntion (WMT18). In Proceedings of the Third Confer-\nence on Machine Translation: Shared Task Papers,\npages 272–303, Belgium, Brussels. Association for\nComputational Linguistics.\nBouamor, H. and Sajjad, H. (2018). Parallel sentence\nextraction from comparable corpora using multilin-\ngual sentence embeddings. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC), Miyazaki, Japan ,\npages 7–12.\nChidambaram, M., Yang, Y ., Cer, D., Yuan, S.,\nSung, Y .-H., Strope, B., and Kurzweil, R. (2018).\nLearning cross-lingual sentence representations via\na multi-task dual-encoder model. arXiv preprint\narXiv:1810.12836.\nCho, K., Van Merriënboer, B., Bahdanau, D., and Ben-\ngio, Y . (2014). On the properties of neural machine\ntranslation: Encoder-decoder approaches. arXiv\npreprint arXiv:1409.1259.\nChu, C., Nakazawa, T., and Kurohashi, S. (2014). Con-\nstructing a chinese—japanese parallel corpus from\nwikipedia. In LREC, pages 642–647.\nChu, C. and Wang, R. (2020). A survey of domain\nadaptation for machine translation. Journal of infor-\nmation processing, 28:413–426.\nDakwale, P. and Monz, C. (2017). Fine-tuning for\nneural machine translation with limited degradation\nacross in-and out-of-domain data. In Proceedings of\nMachine Translation Summit XVI: Research Track,\npages 156–169.\nFadaee, M., Bisazza, A., and Monz, C. (2017). Data\naugmentation for low-resource neural machine trans-\nlation. arXiv preprint arXiv:1705.00440.\nFeng, F., Yang, Y ., Cer, D., Arivazhagan, N., and Wang,\nW. (2020). Language-agnostic bert sentence embed-\nding. arXiv preprint arXiv:2007.01852.\nFreitag, M. and Al-Onaizan, Y . (2016). Fast domain\nadaptation for neural machine translation. arXiv\npreprint arXiv:1612.06897.\n269\nGrégoire, F. and Langlais, P. (2017). A deep neural net-\nwork approach to parallel sentence extraction. arXiv\npreprint arXiv:1709.09783.\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L.,\nLin, H.-C., Bougares, F., Schwenk, H., and Bengio, Y .\n(2015). On using monolingual corpora in neural ma-\nchine translation. arXiv preprint arXiv:1503.03535.\nGuo, M., Shen, Q., Yang, Y ., Ge, H., Cer, D., Abrego,\nG. H., Stevens, K., Constant, N., Sung, Y .-H., Strope,\nB., et al. (2018). Effective parallel corpus mining\nusing bilingual sentence embeddings. arXiv preprint\narXiv:1807.11906.\nHuck, M., Stojanovski, D., Hangya, V ., and Fraser, A.\n(2018). Lmu munich’s neural machine translation\nsystems at wmt 2018. In Proceedings of the Third\nConference on Machine Translation: Shared Task\nPapers, pages 648–654.\nJimeno Yepes, A., Névéol, A., Neves, M., Verspoor, K.,\nBojar, O., Boyer, A., Grozea, C., Haddow, B., Kittner,\nM., Lichtblau, Y ., Pecina, P., Roller, R., Rosa, R., Siu,\nA., Thomas, P., and Trescher, S. (2017). Findings of\nthe WMT 2017 biomedical translation shared task.\nIn Proceedings of the Second Conference on Machine\nTranslation, pages 234–247, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nJohnson, M., Schuster, M., Le, Q. V ., Krikun, M., Wu,\nY ., Chen, Z., Thorat, N., Viégas, F., Wattenberg, M.,\nCorrado, G., et al. (2017). Google’s multilingual\nneural machine translation system: Enabling zero-\nshot translation. Transactions of the Association for\nComputational Linguistics, 5:339–351.\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent\ncontinuous translation models. In Proceedings of\nthe 2013 conference on empirical methods in natural\nlanguage processing, pages 1700–1709.\nKarimi, A., Ansari, E., and Sadeghi Bigham, B. (2018).\nExtracting an English-Persian parallel corpus from\ncomparable corpora. In Proceedings of the Eleventh\nInternational Conference on Language Resources\nand Evaluation (LREC 2018), Miyazaki, Japan. Eu-\nropean Language Resources Association (ELRA).\nKhan, A., Panda, S., Xu, J., and Flokas, L. (2018).\nHunter nmt system for wmt18 biomedical translation\ntask: Transfer learning in neural machine translation.\nIn Proceedings of the Third Conference on Machine\nTranslation: Shared Task Papers, pages 655–661.\nKim, Y . and Rush, A. M. (2016). Sequence-level knowl-\nedge distillation. arXiv preprint arXiv:1606.07947.\nKingma, D. P. and Ba, J. (2015). Adam: A method for\nstochastic optimization. In Bengio, Y . and LeCun, Y .,\neditors, 3rd International Conference on Learning\nRepresentations, ICLR 2015, San Diego, CA, USA,\nMay 7-9, 2015, Conference Track Proceedings.\nKnowles, R. and Koehn, P. (2018). Context and copying\nin neural machine translation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3034–3041, Brussels,\nBelgium. Association for Computational Linguistics.\nKoehn, P. and Knowles, R. (2017). Six challenges\nfor neural machine translation. arXiv preprint\narXiv:1706.03872.\nKudo, T. and Richardson, J. (2018). SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nLample, G., Ott, M., Conneau, A., Denoyer, L., and\nRanzato, M. (2018). Phrase-based & neural un-\nsupervised machine translation. arXiv preprint\narXiv:1804.07755.\nMao, Z., Chu, C., and Kurohashi, S. (2022). Ems:\nefficient and effective massively multilingual sen-\ntence representation learning. arXiv preprint\narXiv:2205.15744.\nMao, Z. and Nakagawa, T. (2023). Lealla: Learn-\ning lightweight language-agnostic sentence embed-\ndings with knowledge distillation. arXiv preprint\narXiv:2302.08387.\nNeves, M., Yepes, A. J., and Névéol, A. (2016). The\nScielo Corpus: a parallel corpus of scientific publica-\ntions for biomedicine. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation (LREC’16), pages 2942–2948, Portorož,\nSlovenia. European Language Resources Association\n(ELRA).\nOtero, P. G. and López, I. G. (2010). Wikipedia as\nmultilingual source of comparable corpora. In Pro-\nceedings of the 3rd Workshop on Building and Using\nComparable Corpora, LREC, pages 21–25.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,\nN., Grangier, D., and Auli, M. (2019). fairseq: A\nfast, extensible toolkit for sequence modeling. arXiv\npreprint arXiv:1904.01038.\nPark, C., Yang, Y ., Park, K., and Lim, H. (2020). Decod-\ning strategies for improving low-resource machine\ntranslation. Electronics, 9(10):1562.\nPost, M. (2018). A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nReimers, N. and Gurevych, I. (2020). Making monolin-\ngual sentence embeddings multilingual using knowl-\nedge distillation. arXiv preprint arXiv:2004.09813.\n270\nSchwenk, H. (2018). Filtering and mining parallel\ndata in a joint multilingual space. arXiv preprint\narXiv:1805.09822.\nSchwenk, H., Chaudhary, V ., Sun, S., Gong, H., and\nGuzmán, F. (2019). Wikimatrix: Mining 135m paral-\nlel sentences in 1620 language pairs from wikipedia.\narXiv preprint arXiv:1907.05791.\nSennrich, R., Haddow, B., and Birch, A. (2015). Im-\nproving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709.\nStefanescu, D., Ion, R., and Hunsicker, S. (2012). Hy-\nbrid parallel sentence mining from comparable cor-\npora. In Proceedings of the 16th annual conference\nof the European association for machine translation,\npages 137–144.\nSutskever, I., Vinyals, O., and Le, Q. V . (2014). Se-\nquence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nTufi¸ s, D., Ion, R., Dumitrescu, ¸ S. D., and Stefanescu,\nD. (2013). Wikipedia as an smt training corpus. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing RANLP\n2013, pages 702–709.\nUl Haq, S., Abdul Rauf, S., Shaukat, A., and Saeed,\nA. (2020). Document level NMT of low-resource\nlanguages with backtranslation. In Proceedings of\nthe Fifth Conference on Machine Translation, pages\n442–446, Online. Association for Computational Lin-\nguistics.\nUszkoreit, J., Ponte, J., Popat, A., and Dubiner, M.\n(2010). Large scale parallel document mining for\nmachine translation.\nVan Der Wees, M., Bisazza, A., and Monz, C. (2017).\nDynamic data selection for neural machine transla-\ntion. arXiv preprint arXiv:1708.00712.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,\nJones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin,\nI. (2017). Attention is all you need. Advances in\nneural information processing systems, 30.\nWang, F., Cheng, J., Liu, W., and Liu, H. (2018). Ad-\nditive margin softmax for face verification. IEEE\nSignal Processing Letters, 25(7):926–930.\nXu, J., Pham, M. Q., Abdul Rauf, S., and Yvon, F.\n(2021). LISN @ WMT 2021. In Proceedings of\nthe Sixth Conference on Machine Translation, pages\n232–242, Online. Association for Computational Lin-\nguistics.\nYang, Y ., Abrego, G. H., Yuan, S., Guo, M., Shen, Q.,\nCer, D., Sung, Y .-H., Strope, B., and Kurzweil, R.\n(2019a). Improving multilingual sentence embed-\nding using bi-directional dual encoder with additive\nmargin softmax. arXiv preprint arXiv:1902.08564.\nYang, Y ., Cer, D., Ahmad, A., Guo, M., Law, J., Con-\nstant, N., Abrego, G. H., Yuan, S., Tar, C., Sung,\nY .-H., et al. (2019b). Multilingual universal sen-\ntence encoder for semantic retrieval. arXiv preprint\narXiv:1907.04307.\nZhang, J. and Zong, C. (2020). Neural machine transla-\ntion: Challenges, progress and future. Science China\nTechnological Sciences, 63(10):2028–2050.\nZoph, B., Yuret, D., May, J., and Knight, K. (2016).\nTransfer learning for low-resource neural machine\ntranslation. arXiv preprint arXiv:1604.02201.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8494006395339966
    },
    {
      "name": "Sentence",
      "score": 0.791642427444458
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7909292578697205
    },
    {
      "name": "Natural language processing",
      "score": 0.7009409666061401
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6727510690689087
    },
    {
      "name": "Language model",
      "score": 0.5599279403686523
    },
    {
      "name": "Domain knowledge",
      "score": 0.5415037870407104
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5354222655296326
    },
    {
      "name": "Parallel corpora",
      "score": 0.47031915187835693
    },
    {
      "name": "Domain model",
      "score": 0.41316959261894226
    },
    {
      "name": "Machine translation",
      "score": 0.17561963200569153
    },
    {
      "name": "Mathematics",
      "score": 0.06642484664916992
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210115485",
      "name": "Laboratoire d'Informatique pour la Mécanique et les Sciences de l'Ingénieur",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I277688954",
      "name": "Université Paris-Saclay",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I6292670",
      "name": "Fatima Jinnah Women University",
      "country": "PK"
    }
  ]
}