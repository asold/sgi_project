{
    "title": "Methods to integrate a language model with semantic information for a word prediction component",
    "url": "https://openalex.org/W1916806189",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Wandmacher, Tonio",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288671355",
            "name": "Antoine, Jean-Yves",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W104527757",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W1593045043",
        "https://openalex.org/W1797288984",
        "https://openalex.org/W2039534649",
        "https://openalex.org/W1665921526",
        "https://openalex.org/W180232814",
        "https://openalex.org/W2065905229",
        "https://openalex.org/W1996903695",
        "https://openalex.org/W85598974",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W2026430321",
        "https://openalex.org/W2159637323",
        "https://openalex.org/W166966826",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W2143975169",
        "https://openalex.org/W1528085921"
    ],
    "abstract": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.",
    "full_text": "Methods to integrate a language model with semantic  \ninformation for a word prediction component \nTonio Wandmacher \nLaboratoire d’Informatique (LI) \nUniversité François Rabelais de Tours \n3 place Jean-Jaurès, 41000 Blois, France \ntonio.wandmacher@ \nuniv-tours.fr \nJean-Yves Antoine \nLaboratoire d’Informatique (LI) \nUniversité François Rabelais de Tours \n3 place Jean-Jaurès, 41000 Blois, France \njean-yves.antoine@ \nuniv-tours.fr \n \nAbstract \nMost current word prediction systems make \nuse of n-gram language models (LM) to es- \ntimate the probability of the following word \nin a phrase. In the past years there have \nbeen many attempts to enrich such lan- \nguage models with further syntactic or se- \nmantic information. We want to explore the \npredictive powers of Latent Semantic \nAnalysis (LSA), a method that has been \nshown to provide reliable information on \nlong-distance semantic dependencies be- \ntween words in a context. We present and \nevaluate here several methods that integrate \nLSA-based information with a standard \nlanguage model: a semantic cache , partial \nreranking , and different forms of interpola- \ntion. We found that all methods show sig- \nnificant improvements, compared to the 4-\ngram baseline, and most of them to a sim- \nple cache model as well. \n1 Introduction: NLP for AAC systems \nAugmented and Alternative Communication \n(AAC) is a field of research which concerns natural \nlanguage processing as well as human-machine \ninteraction, and which aims at restoring the com- \nmunicative abilities of disabled people with severe \nspeech and motion impairments. These people can \nbe for instance cerebrally and physically handi- \ncapped persons or they suffer from a locked-in \nsyndrome due to a cerebral apoplexy. Whatever the \ndisease or impairment considered, oral communica- \ntion is impossible for these persons who have in \naddition serious difficulties to control physically \ntheir environment. In particular, they are not able to \nuse standard input devices of a computer. Most of \nthe time, they can only handle a single switch de- \nvice. As a result, communicating with an AAC sys- \ntem consists of typing messages by means of a vir- \ntual table of symbols (words, letters or icons) \nwhere the user successively selects the desired \nitems. \nBasically, an AAC system, such as FASTY  \n(Trost et al. 2005) or S\nIBYLLE  (Schadle et al, 2004), \nconsists of four components. At first, one finds a \nphysical input interface connected to the computer. \nThis device is adapted to the motion capacities of \nthe user. When the latter must be restricted to a \nsingle switch (eye glimpse or breath detector, for \ninstance), the control of the environment is reduced \nto a mere Yes/No command.  \nSecondly, a virtual keyboard is displayed on \nscreen. It allows the user to select successively the \nsymbols that compose the intended message. In \nS\nIBYLLE , key selection is achieved by pointing let- \nters through a linear scan procedure: a cursor suc-\ncessively highlights each key of the keyboard.  \nThe last two components are a text editor (to \nwrite e-mails or other documents) and a speech \nsynthesis module, which is used in case of spoken \ncommunication. The latest version of S\nIBYLLE  \nworks for French and German, and it is usable with \nany Windows™  application (text editor, web \nbrowser, mailer...), which means that the use of a \nspecific editor is no longer necessary.  \nThe main weakness of AAC systems results from \nthe slowness of message composition. On average, \ndisabled people cannot type more than 1 to 5 words \nper minute; moreover, this task is very tiring. The \nuse of NLP techniques to improve AAC systems is \ntherefore of first importance. \n \n \n \nFigure 1: User interface of the SIBYLLE  AAC system  \n \n \nTwo complementary approaches are possible to \nspeed up communication. The first one aims at \nminimizing the duration of each item selection. \nConsidering a linear scan procedure, one could for \ninstance dynamically reorganize the keyboard in \norder to present the most probable symbols at first. \nThe second strategy tries to minimize the number \nof keystrokes to be made. Here, the system tries to \npredict the words which are likely to occur just af- \nter those already typed. The predicted word is then \neither directly displayed after the end of the in- \nserted text (a method referred to as “word comple- \ntion”, cf. Boissière and Dours, 1996), or a list of N-\nbest (typically 3 to 7) predictions is provided on the \nvirtual keyboard. When one of these predictions \ncorresponds to the intended word, it can be selected \nby the user. As can be seen in figure 1, the interface \nof the S\nIBYLLE  system presents such a list of most \nprobable words to the user. \nSeveral approaches can be used to carry out \nword prediction. Most of the commercial AAC sys- \ntems make only use of a simple lexicon: in this ap-\nproach, the context is not considered. \nOn the other hand, stochastic language models \ncan provide a list of word suggestions, depending \non the n-1 (typically n = 3 or 4) last inserted words. \nIt is obvious that such a model cannot take into ac- \ncount long-distance dependencies. There have been \nattempts to integrate part-of-speech information \n(Fazly and Hirst, 2003) or more complex syntactic \nmodels (Schadle et al, 2004) to achieve a better \nprediction. In this paper, we will nevertheless limit \nour study to a standard 4-gram model as a baseline \nto make our results comparable. Our main aim is \nhere to investigate the use of long-distance seman-\ntic dependencies to dynamically adapt the predic- \ntion to the current semantic context of communica- \ntion. Similar work has been done by Li and Hirst \n(2005) and Matiasek and Baroni (2003), who ex- \nploit Pointwise Mutual Information  (PMI; Church \nand Hanks, 1989). Trnka et al. (2005) dynamically \ninterpolate a high number of topic-oriented models \nin order to adapt their predictions to the current \ntopic of the text or conversation. \nClassically, word predictors are evaluated by an \nobjective metric called Keystroke Saving Rate  \n(ksr ): \n100 1 ⋅\n\n\n\n\n −=\na\np\nn\nk\nkksr  (1) \n \nwith k\np, ka being the number of keystrokes \nneeded on the input device when typing a message \nwith ( k\np) and without prediction ( ka = number of \ncharacters in the text that has been entered, n = \nlength of the prediction list, usually n = 5). As \nTrost et al. (2005) and Trnka et al. (2005), we as-\nsume that one additional keystroke is required for \nthe selection of a word from the list and that a \nspace is automatically inserted afterwards. Note \nalso that words, which have already occurred in the \nlist, will not reappear after the next character has \nbeen inserted.  \nThe perplexity measure, which is frequently \nused to assess statistical language models, proved \nto be less accurate in this context. We still present \nperplexities as well in order to provide comparative \nresults. \n2 Language modeling and semantics \n2.1 Statistical Language Models \nFor about 10 to 15 years statistical language model- \ning has had a remarkable success in various NLP \ndomains, for instance in speech recognition, ma- \nchine translation, Part-of-Speech tagging, but also \nin word prediction systems. N-gram based lan- \nguage models (LM) estimate the probability of oc- \ncurrence for a word, given a string of n-1 preceding \nwords. However, computers have only recently \nbecome powerful enough to estimate probabilities \non a reasonable  amount of training data. More- \nover, the larger n gets, the more important the prob- \nlem of combinatorial explosion for the probability \nestimation becomes. A reasonable trade-off be- \ntween performance and number of estimated events \nseems therefore to be an n of 3 to 5, including so- \nphisticated techniques in order to estimate the \nprobability of unseen events (smoothing methods). \nWhereas n-gram-like language models are al- \nready performing rather well in many applications, \ntheir capacities are also very limited in that they \ncannot exploit any deeper linguistic structure. \nLong-distance syntactic relationships are neglected \nas well as semantic or thematic constraints. \nIn the past 15 years many attempts have been \nmade to enrich language models with more com- \nplex syntactic and semantic models, with varying \nsuccess (cf. (Rosenfeld, 1996), (Goodman, 2002) \nor in a word prediction task: (Fazly and Hirst, \n2003), (Schadle, 2004), (Li and Hirst, 2005)). We \nwant to explore here an approach based on Latent \nSemantic Analysis  (Deerwester et al, 1990). \n2.2 Latent Semantic Analysis \nSeveral works have suggested the use of Latent \nSemantic Analysis (LSA) in order to integrate se- \nmantic similarity to a language model (cf. Belle- \ngarda, 1997; Coccaro and Jurafsky, 1998). LSA \nmodels semantic similarity based on co-occurrence \ndistributions of words, and it has shown to be help- \nful in a variety of NLP tasks, but also in the domain \nof cognitive modeling (Landauer et al, 1997). \nLSA is able to relate coherent contexts to spe- \ncific content words, and it is good at predicting the \noccurrence of a content word in the presence of \nother thematically related terms. However, since it \ndoes not take word order into account (“bag-of-\nwords” model) it is very poor at predicting their \nactual position within the sentence, and it is com-\npletely useless for the prediction of function words. \nTherefore, some attempts have been made to inte- \ngrate the information coming from an LSA-based \nmodel with standard language models of the n-\ngram type.  \nIn the LSA model (Deerwester et al, 1990) a \nword w\ni is represented as a high-dimensional vec- \ntor, derived by Singular Value Decomposition \n(SVD) from a term × document (or a term × term) \nco-occurrence matrix of a training corpus. In this \nframework, a context or history h (= w\n1, ... , w m ) \ncan be represented by the sum of the (already nor- \nmalized) vectors corresponding to the words it con-\ntains (Landauer et al. 1997):  \n \n∑\n=\n=\nm\ni\niwh\n1\nrr\n (2) \n \nThis vector reflects the meaning of the preceding \n(already typed) section, and it has the same dimen-\nsionality as the term vectors. It can thus be com- \npared to the term vectors by well-known similarity \nmeasures (scalar product, cosine).  \n2.3 Transforming LSA similarities into prob- \nabilities \nWe make the assumption that an utterance or a \ntext to be entered is usually semantically cohesive. \nWe then expect all word vectors to be close to the \ncurrent context vector, whose corresponding words \nbelong to the semantic field of the context. This \nforms the basis for a simple probabilistic model of \nLSA: After calculating the cosine similarity for \neach word vector \niwr  with the vector h\nr\n of the cur- \nrent context, we could use the normalized similari-\nties as probability values. This probability distribu- \ntion however is usually rather flat (i.e. the dynamic \nrange is low). For this reason a contrasting (or tem- \nperature) factor γ is normally applied (cf. Coccaro \nand Jurafsky, 1998), which raises the cosine to \nsome power ( γ is normally between 3 and 8). After \nnormalization we obtain a probability distribution \nwhich can be used for prediction purposes. It is \ncalculated as follows: \n \n( )\n( )∑ −\n−=\nk\nγ\nk\nγ\ni\niLSA \nhhw\nhhwhwP\n)(cos ),cos( \n)(cos ),cos( )(\nmin \nmin \nrrr\nrrr\n  (3) \n \nw i is a word in the vocabulary, h is the current con-\ntext (history) iwr and h\nr\nare their corresponding vec- \ntors in the LSA space; cos min (h\nr\n) returns the lowest \ncosine value measured for h\nr\n). The denominator \nthen normalizes each similarity value to ensure that \n∑ =\nn\nk kLSA \nhwP 1),( . \nLet us illustrate the capacities of this model by \ngiving a short example from the French version of \nour own LSA predictor: \n \nContext : “Mon père était professeur en mathématiques \net je pense que  ” \n (“My dad has been a professor in mathemat- \nics and I think that ”) \n \nRank Word P \n1. professeur (‘professor’) 0.0117 \n2. mathématiques  (“mathematics”) 0.0109 \n3. enseigné  (participle of ‘taught’) 0.0083 \n4. enseignait  (‘taught’) 0.0053 \n5. mathematicien  (‘mathematician’) 0.0049 \n6. père  (‘father’) 0.0046 \n7. mathématique  (‘mathematics’) 0.0045 \n8. grand-père  (‘grand-father’) 0.0043 \n9. sciences  (‘sciences’) 0.0036 \n10. enseignant  (‘teacher’) 0.0032 \nExample 1: Most probable words returned by the \nLSA model for the given context. \n \nAs can be seen in example 1, all ten predicted \nwords are semantically related to the context, they \nshould therefore be given a high probability of oc-\ncurrence. However, this example also shows the \ndrawbacks of the LSA model: it totally neglects the \npresence of function words as well as the syntactic \nstructure of the current phrase. We therefore need \nto find an appropriate way to integrate the informa- \ntion coming from a standard n-gram model and the \nLSA approach. \n2.4 Density as a confidence measure \nMeasuring relation quality in an LSA space, \nWandmacher (2005) pointed out that the reliability \nof LSA relations varies strongly between terms. He \nalso showed that the entropy of a term does not \ncorrelate with relation quality (i.e. number of se-\nmantically related terms in an LSA-generated term \ncluster), but he found a medium correlation ( Pear- \nson  coeff. = 0.56) between the number of semanti- \ncally related terms and the average cosine similar-\nity of the m  nearest neighbors (density). The closer \nthe nearest neighbors of a term vector are, the more \nprobable it is to find semantically related terms for \nthe given word. In turn, terms having a high density \nare more likely to be semantically related to a given \ncontext (i.e. their specificity is higher). \nWe define the density of a term w\ni as follows: \n \n  ∑\n=\n⋅=\nm\nj\nijiim wNN wmwD\n1\n)) (,cos( 1)( rr  (4) \n \nIn the following we will use this measure (with \nm =100) as a confidence metric to estimate the reli- \nability of a word being predicted by the LSA com- \nponent, since it showed to give slightly better re-\nsults in our experiments than the entropy measure.  \n3 Integrating semantic information \nIn the following we present several different meth-\nods to integrate semantic information as it is pro-\nvided by an LSA model into a standard LM. \n3.1 Semantic cache model \nCache (or recency promotion) models have shown \nto bring slight but constant gains in language mod-\neling (Kuhn and De Mori, 1990). The underlying \nidea is that words that have already occurred in a \ntext are more likely to occur another time. There- \nfore their probability is raised by a constant or ex- \nponentially decaying factor, depending on the posi-\ntion of the element in the cache. The idea of a de-\ncaying cache function is that the probability of re- \noccurrence depends on the cosine similarity of the \nword in the cache and the word to be predicted. \nThe highest probability of reoccurrence is usually \nafter 15 to 20 words. \nSimilar to Clarkson and Robinson (1997), we im- \nplemented an exponentially decaying cache of \nlength l (usually between 100 and 1000), using the \nfollowing decay function for a word w i and its posi- \ntion p in the cache. \n \n2)( 5 , 0\n),(\n\n\n\n\n −−\n=\nσ\nµp\nid epwf  (5) \n \nσ = µ/3 if p < µ and  σ = l/3 if p ≥ µ. The func- \ntion returns 0 if w i is not in the cache, and it is 1 if \np = µ. A typical graph for (5) can be seen in figure \n(2). \n \n \nFigure 2: Decay function with µ=20 and l=300. \n \nWe extend this model by calculating for each ele- \nment having occurred in the context its m  nearest \nLSA neighbors ( \n),( θwNN occ m\nr , using cosine simi- \nlarity), if their cosine lies above a threshold θ, and \nadd them to the cache as well, right after the word \nthat has occurred in the text (“ Bring your friends ”-\nstrategy). The size of the cache is adapted accord-\ningly (for \nµ, σ and  l ), depending on the number of \nneighbors added. This results in the following \ncache function: \n \n),(),()(\n1 cos pwfwwfβwP id\nl\ni\ni\nocc icache ∑ ⋅⋅=    (6) \n \nwith l = size of the cache. \nβ is a constant  con- \ntrolling the influence of the component (usually β ≈ \n0.1/l); w i\nocc  is a word that has already recently oc- \ncurred in the context and is therefore added as a \nstandard cache element, whereas w\ni is a nearest \nneighbor to w i\nocc . fcos (w i\nocc , w i) returns the cosine \nsimilarity between \ni\nocc wr  and iwr , with cos (\ni\nocc wr , iwr ) \n> θ (Rem: w i with cos (\ni\nocc wr , iwr ) ≤ θ have not been \nadded to the cache).  Since cos ( iwr , iwr )=1, terms \nhaving actually occurred before will be given full \nweight, whereas all w\ni being only nearest LSA \nneighbors to w i\nocc  will receive a weight correspond- \ning to their cosine similarity with w i\nocc  , which is \nless than 1 (but larger than θ). \nfd(w i,p) is the decay factor for the current posi- \ntion p of w i in the cache, calculated as shown in \nequation (5).  \n3.2 Partial reranking \nThe underlying idea of partial reranking is to re- \ngard only the best n candidates from the basic lan- \nguage model for the semantic model in order to \nprevent the LSA model from making totally im- \nplausible (i.e. improbable) predictions. Words be- \ning improbable for a given context will be disre- \ngarded as well as words that do not occur in the \nsemantic model (e.g. function words), because LSA \nis not able to give correct estimates for this group \nof words (here the base probability remains un- \nchanged). \nFor the best n candidates their semantic probability \nis calculated and each of these words is assigned an \nadditional value, after a fraction of its base prob- \nability has been subtracted ( jackpot  strategy). \nFor a given context h we calculate the ordered set \nB\nEST n(h) = < w 1, …  , w n>, so that P(w 1|h) ≥ \nP(w 2|h) ≥… ≥P(w n|h) \nFor each w i in BEST n(h) we then calculate its \nreranking probability as follows: \n \n)), (()(),cos( )( iniiiRR whBest IwDhwβwP ⋅⋅⋅=\nrr  (7) \n \nβ is a weighting constant controlling the overall \ninfluence of the reranking process, cos ( iwr , iwr ) re- \nturns the cosine of the word’s vector and the cur- \nrent context vector, D (w\ni) gives the confidence \nmeasure of w i and I is an indicator function being \n1, iff w i ∈ B EST (h), and 0 otherwise.  \n3.3 Standard interpolation \nInterpolation is the standard way to integrate in- \nformation from heterogeneous resources. While for \na linear combination we simply add the weighted \nprobabilities of two (or more) models, geometric \ninterpolation multiplies the probabilities, which are \nweighted by an exponential coefficient (0 ≤λ\n1≤1): \n \nLinear Interpolation (LI): \n \n)()1 ()()( ' 11 isibi wPλwPλwP ⋅−+⋅=    (8) \n \nGeometric Interpolation (GI): \n \n∑ =\n−\n−\n⋅\n⋅= n\nj\nλ\njs\nλ\njb\nλ\nis\nλ\nib\ni\nwPwP\nwPwPwP\n1\n)11 (1\n)11 (1\n)()(\n)()()( '     (9) \n \nThe main difference between the two methods is \nthat the latter takes the agreement of two models \ninto account. Only if each of the single models as-\nsigns a high probability to a given event will the \ncombined probability be assigned a high value. If \none of the models assigns a high probability and \nthe other does not the resulting probability will be \nlower. \n3.4 Confidence-weighted interpolation \nWhereas in standard settings the coefficients are \nstable for all probabilities, some approaches use \nconfidence-weighted coefficients that are adapted \nfor each probability. In order to integrate n-gram \nand LSA probabilities, Coccaro and Jurafsky \n(1998) proposed an entropy-related confidence \nmeasure for the LSA component, based on the ob- \nservation that words that occur in many different \ncontexts (i.e. have a high entropy), cannot well be \npredicted by LSA. We use here a density-based \nmeasure (cf. section 2.2), because we found it more \nreliable than entropy in preliminary tests. For inter- \npolation purposes we calculate the coefficient of \nthe LSA component as follows: \n \n)( ii wDβλ ⋅= , iff D (w i) > 0; 0 otherwise (10) \n \nwith β being a weighting constant to control the \ninfluence of the LSA predictor. For all experi- \nments, we set \nβ to 0.4 (i.e. 0 ≤ λi ≤ 0.4), which \nproved to be optimal in pre-tests. \n4 Results \nWe calculated our baseline n-gram model on a 44 \nmillion word corpus from the French daily Le \nMonde  (1998-1999). Using the SRI  toolkit (Stol- \ncke, 2002) \n1 we computed a 4-gram LM over a con- \ntrolled 141,000 word vocabulary, using modified \nKneser-Ney  discounting (Goodman, 2001), and we \napplied Stolcke pruning (Stolcke, 1998) to reduce \nthe model to a manageable size ( θ = 10 -7). \n                                                 \n1 SRI Toolkit: www.speech.sri.com.  \nThe LSA space was calculated on a 100 million \nword corpus from Le Monde  (1996 – 2002). Using \nthe Infomap  toolkit 2, we generated a term × term \nco-occurrence matrix for an 80,000 word vocabu- \nlary (matrix size = 80,000 × 3,000), stopwords \nwere excluded. After several pre-tests, we set the \nsize of the co-occurrence window to ±100. The ma- \ntrix was then reduced by singular value decomposi- \ntion to 150 columns, so that each word in the vo- \ncabulary was represented by a vector of 150 di- \nmensions, which was normalized to speed up simi- \nlarity calculations (the scalar product of two nor-\nmalized vectors equals the cosine of their angle).  \nOur test corpus consisted of 8 sections from the \nFrench newspaper Humanité , (January 1999, from \n5,378 to 8,750 words each), summing up to 58,457 \nwords. We then calculated for each test set the key- \nstroke saving rate based on a 5-word list ( ksr \n5) and \nperplexity for the following settings 3: \n1. 4-gram LM only (baseline) \n2. 4-gram + decaying cache ( l = 400) \n3. 4-gram + LSA using linear interpolation \nwith λ\nLSA  = 0.11 (LI). \n4. 4-gram + LSA using geometric interpola- \ntion, with λ\nLSA  = 0.07 (GI). \n5. 4-gram + LSA using linear interpolation \nand (density-based) confidence weighting \n(CWLI). \n6. 4-gram + LSA using geometric interpola- \ntion and (density-based) confidence \nweighting (CWGI). \n7. 4-gram + partial reranking ( n = 1000, \nβ = \n0.001) \n8. 4-gram + decaying semantic cache  \n(l = 4000; m = 10; θ = 0.4, \nβ = 0.0001)  \nFigures 3 and 4 display the overall results in terms \nof ksr  and perplexity.  \n                                                 \n2 Infomap Project: http://infomap-nlp.sourceforge.net/  \n3 All parameter settings presented here are based on results of \nextended empirical pre-tests. We used held-out development \ndata sets that have randomly been chosen from the Humanité  \ncorpus.(8k to 10k words each). The parameters being pre- \nsented here were optimal for our test sets. For reasons of sim- \nplicity we did not use automatic optimization techniques such \nas the EM algorithm (cf. Jelinek, 1990). \n \n \nFigure 3: Results ( ksr 5) for all methods tested. \n \n \n  \nFigure 4: Results (perplexity) for all methods \ntested. \n \nUsing the results of our 8 samples, we performed \npaired t tests for every method with the baseline as \nwell as with the cache model. All gains for ksr  \nturned out to be highly significant (sig. level < \n0.001), and apart from the results for CWLI, all \nperplexity reductions were significant as well (sig. \nlevel < 0.007), with respect to the cache results. We \ncan therefore conclude that, with exception of \nCWLI, all methods tested have a beneficial effect, \neven when compared to a simple cache model. The \nhighest gain in ksr  (with respect to the baseline) \nwas obtained for the confidence-weighted geo-\nmetric interpolation method (CWGI; +1.05%), the \nhighest perplexity reduction was measured for GI \nas well as for CWGI (-9.3% for both). All other \nmethods (apart from IWLI) gave rather similar re- \nsults (+0.6 to +0.8% in ksr, and -6.8% to -7.7% in \nperplexity). \nWe also calculated for all samples the correla- \ntion between ksr  and perplexity. We measured a \nPearson  coefficient of -0.683 (Sig. level < 0.0001).  \nAt first glance, these results may not seem over- \nwhelming, but we have to take into account that \nour ksr  baseline of 57.9% is already rather high, \nand at such a level, additional gains become hard to \nachieve (cf. Lesher et al, 2002). \nThe fact that CWLI performed worse than even \nsimple LI was not expected, but it can be explained \nby an inherent property of linear interpolation: If \none of the models to be interpolated overestimates \nthe probability for a word, the other cannot com- \npensate for it (even if it gives correct estimates), \nand the resulting probability will be too high. In \nour case, this happens when a word receives a high \nconfidence value; its probability will then be over- \nestimated by the LSA component. \n5 Conclusion and further work \nAdapting a statistical language model with seman- \ntic information, stemming from a distributional \nanalysis like LSA, has shown to be a non-trivial \nproblem. Considering the task of word prediction \nin an AAC system, we tested different methods to \nintegrate an n-gram LM with LSA: A semantic \ncache model, a partial reranking approach, and \nsome variants of interpolation. \nWe evaluated the methods using two different \nmeasures, the keystroke saving rate ( ksr ) and per- \nplexity, and we found significant gains for all \nmethods incorporating LSA information, compared \nto the baseline. In terms of ksr  the most successful \nmethod was confidence-weighted geometric inter- \npolation (CWGI; +1.05% in ksr ); for perplexity, \nthe greatest reduction was obtained for standard as \nwell as for confidence-weighted geometric interpo- \nlation (-9.3% for both). Partial reranking and the \nsemantic cache gave very similar results, despite \ntheir rather different underlying approach.  \nWe could not provide here a comparison with \nother models that make use of distributional infor-\nmation, like the trigger approach by Rosenfeld \n(1996), Matiasek and Baroni (2003) or the model \npresented by Li and Hirst (2005), based on Point- \nwise Mutual Information (PMI). A comparison of \nthese similarities with LSA remains to be done.  \nFinally, an AAC system has not only the func- \ntion of simple text entering but also of providing \ncognitive support to its user, whose communicative \nabilities might be totally depending on it. There- \nfore, she or he might feel a strong improvement of \nthe system, if it can provide semantically plausible \npredictions, even though the actual gain in ksr  \nmight be modest or even slightly decreasing. For \nthis reason we will perform an extended qualitative \nanalysis of the presented methods with persons \nwho use our AAC system S\nIBYLLE .  This is one of \nthe main aims of the recently started ESAC_IMC  \nproject. It is conducted at the Functional Reeduca- \ntion and Rehabilitation Centre  of Kerpape, Brit- \ntany, where SIBYLLE  is already used by 20 children \nsuffering from traumatisms of the motor cortex. \nThey appreciate the system not only for communi- \ncation but also for language learning purposes. \nMoreover, we intend to make the word predictor \nof S\nIBYLLE  publicly available ( AFM Voltaire  pro- \nject ) in the not-too-distant future.  \nAcknowledgements \nThis research is partially founded by the UFA \n(Université Franco-Allemande) and the French \nfoundations APRETREIMC ( ESAC_IMC  project) \nand AFM (VOLTAIRE project). We also want to \nthank the developers of the SRI  and the  Infomap  \ntoolkits for making their programs available. \nReferences \nBellegarda, J. (1997): “A Latent Semantic Analysis \nFramework for Large-Span Language Modeling”, \nProceedings of the Eurospeech 97, Rhodes, Greece. \nBoissière Ph. and Dours D. (1996).  “VITIPI : Versatile \ninterpretation of text input by persons with impair- \nments”. Proceedings ICCHP'1996.  Linz, Austria. \nChurch, K. and Hanks, P. (1989). “Word association \nnorms, mutual information and lexicography”. Pro- \nceedings of ACL , pp. 76-83. \nClarkson, P. R. and Robinson, A.J. (1997). “Language \nModel Adaptation using Mixtures and an Exponen- \ntially Decaying Cache”, in Proc. of the IEEE \nICASSP-97 , Munich. \nCoccaro, N. and Jurafsky, D. (1998). “Towards better \nintegration of semantic predictors in statistical lan- \nguage modeling”, Proc. of the ICSLP-98 , Sydney. \nDeerwester, S. C., Dumais, S., Landauer, T., Furnas, G. \nand Harshman, R. (1990). “Indexing by Latent Se- \nmantic Analysis”, JASIS  41(6), pp. 391-407. \nFazly, A. and Hirst, G. (2003). “Testing the efficacy of \npart-of-speech information in word completion”, \nProceedings of  the Workshop on Language Modeling \nfor Text Entry Methods  on EACL , Budapest. \nGoodman, J. (2001): “A Bit of Progress in Language \nModeling”, Extended Version Microsoft Research \nTechnical Report MSR-TR-2001-72. \nJelinek, F. (1990): “Self-organized Language Models for \nSpeech Recognition”, In: A. Waibel and K.-F. Lee \n(eds.), Readings in Speech Recognition , Morgan \nKaufman Publishers, pp. 450-506. \nKuhn, R. and De Mori, R. (1990). “A Cache-Based \nNatural Language Model for Speech Reproduction”, \nIEEE Transactions on Pattern Analysis and Machine \nIntelligence , 12 (6), pp. 570-583. \nLandauer, T. K., Laham, D., Rehder, B. and Schreiner, \nM. E. (1997). “How well can passage meaning be de- \nrived without using word order? A comparison of \nLSA and humans”, Proceedings of the 19th annual \nmeeting of the Cognitive Science Society , pp. 412-\n417, Erlbaum Mawhwah, NJ. \nLesher, G. W., Moulton, B. J, Higginbotham, D.J. and \nAlsofrom, B. (2002). “Limits of human word predic- \ntion performance”, Proceedings of the CSUN 2002. \nLi, J., Hirst, G. (2005). “Semantic knowledge in a word \ncompletion task”, Proc. of the 7 \nth  Int. ACM Confer- \nence on Computers and Accessibility , Baltimore. \nMatiasek, H. and Baroni, M. (2003). “Exploiting long \ndistance collocational relations in predictive typing”, \nProceedings of the EACL-03 Workshop on Language \nModeling for Text Entry Methods , Budapest. \nRosenfeld, R. (1996). “A maximum entropy approach to \nadaptive statistical language modelling”, Computer \nSpeech and Language , 10 (1), pp. 187-228. \nSchadle I., Antoine J.-Y., Le Pévédic B., Poirier F. \n(2004).  “Sibyl - AAC system using NLP tech- \nniques”. Proc. ICCHP’2004 , Paris, France. LNCS \n3118 , Springer Verlag. \nStolcke, A. (1998): “Entropy-based pruning of backoff \nlanguage models”. Proc.s of the DARPA Broadcast \nNews Transcription and Understanding Workshop.  \nStolcke, A. (2002): “SRILM - An Extensible Language \nModeling Toolkit”, in Proc. of the Intl. Conference \non Spoken Language Processing , Denver, Colorado. \nTrnka, K., Yarrington, D., McCoy, K. F. and Penning- \nton, C. (2006): “Topic Modeling in Fringe Word Pre-\ndiction for AAC”, In Proceedings of the 2006 Inter- \nnational Conference on Intelligent User Interfaces , \npp. 276 – 278, Sydney, Australia. \nTrost, H., Matiasek, J. and Baroni, M. (2005): “The \nLanguage Component of the FASTY Text Prediction \nSystem”, Applied Artificial Intelligence , 19 (8), pp. \n743-781. \nWandmacher, T. (2005): “How semantic is Latent Se- \nmantic Analysis?”, in Proceedings of \nTALN/RECITAL 2005 , Dourdan, France, 6-10 june. \n "
}