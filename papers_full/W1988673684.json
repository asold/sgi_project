{
  "title": "Manual sorting of numerals in an inflective language for language modelling",
  "url": "https://openalex.org/W1988673684",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A5088522711",
      "name": "Gregor Donaj",
      "affiliations": [
        "University of Maribor"
      ]
    },
    {
      "id": "https://openalex.org/A5072645144",
      "name": "Zdravko Kačič",
      "affiliations": [
        "University of Maribor"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4402865735",
    "https://openalex.org/W1967325250",
    "https://openalex.org/W2107223151",
    "https://openalex.org/W6603142115",
    "https://openalex.org/W2078129921",
    "https://openalex.org/W2017244362",
    "https://openalex.org/W1492115724",
    "https://openalex.org/W2138079732",
    "https://openalex.org/W2134182172",
    "https://openalex.org/W1997705213",
    "https://openalex.org/W1997575393",
    "https://openalex.org/W2129914454",
    "https://openalex.org/W2076222873",
    "https://openalex.org/W35767841",
    "https://openalex.org/W76063220",
    "https://openalex.org/W1932968309"
  ],
  "abstract": "In speech recognition systems language models are used to estimate the probabilities of word sequences. In this paper special emphasis is given to numerals–words that express numbers. One reason for this is the fact that in a practical application a falsely recognized numeral can change important content information inside the sentence more than other types of errors. Standard $$n$$ -gram language models can sometimes assign very different probabilities to different numerals, according to their relative frequencies in training corpus. Based on the assumption that some different numbers are more equally likely to occur, than what a standard $$n$$ -gram language model estimates, this paper proposes several methods for sorting numerals into classes in an inflective language and language models based on these sorting techniques. We treat these classes as basic vocabulary units for the language model. We also expose the differences between the proposed language models and well known class-based language models. The presented approach is also transferable to other classes of words with similar properties, e.g. proper nouns. Results of experiments show that significant improvements are obtained on numeral-rich domains. Although numerals represent only a small portion of words in the test set, a relative reduction in word error rate of 1.4 % was achieved. Statistical significance tests were performed, which showed that these improvements are statistically significant. We also show that depending on the amount of numerals in a target domain the improvement in performance can grow up to 16 % relative.",
  "full_text": "Int J Speech Technol (2014) 17:281–289\nDOI 10.1007/s10772-014-9231-y\nManual sorting of numerals in an inﬂective language for language\nmodelling\nGregor Donaj · Zdravko Kaˇciˇc\nReceived: 10 September 2013 / Accepted: 2 March 2014 / Published online: 17 March 2014\n© The Author(s) 2014. This article is published with open access at Springerlink.com\nAbstract In speech recognition systems language models\nare used to estimate the probabilities of word sequences.\nIn this paper special emphasis is given to numerals–words\nthat express numbers. One reason for this is the fact that\nin a practical application a falsely recognized numeral can\nchange important content information inside the sentence\nmore than other types of errors. Standard n-gram language\nmodels can sometimes assign very different probabilities to\ndifferent numerals, according to their relative frequencies in\ntraining corpus. Based on the assumption that some differ-\nent numbers are more equally likely to occur, than what a\nstandard n-gram language model estimates, this paper pro-\nposes several methods for sorting numerals into classes in\nan inﬂective language and language models based on these\nsorting techniques. We treat these classes as basic vocabu-\nlary units for the language model. We also expose the dif-\nferences between the proposed language models and well\nknown class-based language models. The presented approach\nis also transferable to other classes of words with similar\nproperties, e.g. proper nouns. Results of experiments show\nthat signiﬁcant improvements are obtained on numeral-rich\ndomains. Although numerals represent only a small portion\nof words in the test set, a relative reduction in word error rate\nof 1.4 % was achieved. Statistical signiﬁcance tests were\nperformed, which showed that these improvements are sta-\ntistically signiﬁcant. We also show that depending on the\namount of numerals in a target domain the improvement in\nperformance can grow up to 16 % relative.\nG. Donaj ( B) · Z. Kaˇciˇc\nFaculty of Electrical Engineering and Computer Science,\nUniversity of Maribor, Maribor, Slovenia\ne-mail: gregor.donaj@um.si\nZ. Kaˇciˇc\ne-mail: zdravko.kacic@um.si\nKeywords Speech recognition · Language models ·\nNumerals · Manual sorting\n1 Introduction\nIn speech recognition language models are used to score\nhypotheses generated by the search algorithm ( Aubert 2002).\nThe language model estimates the likelihood that a sequence\nof words can occur in the given language. The most fre-\nquently used language models today are standard n-gram lan-\nguage models ( Rapp 2008 ). Those models statistically esti-\nmate the probability of a word given n − 1 proceeding words.\nFrom a language modelling point of view numerals are\nrather special words. Often a numeral in a sentence can be\nexchanged with another one and the sentence will still be\ngrammatically correct and both sentences will make sense\nfrom a semantic point of view. However a falsely recognized\nnumeral can result in a crucial change of the main message\nof the sentence, like recognizing the wrong result in news\nabout a sports event. The resulting new sentence will have a\ndifferent meaning, but on the language level both sentences\ncan not be distinguished by which one should be preferred\nby a speech recognition system. Even a person looking at\nboth hypotheses may not be able to decide which one could\nbe more probable.\nThere are of course exceptions like dates and times for\nexample. If a speech recognition system would return the\nhypothesis “on the 40th of January”, one would suspect a\nrecognition error. Also numerals presenting some small num-\nbers are an exception as they clearly do appear more often\nin a language and can be found in phrases, e.g. four cor-\nners, the seven seas, where we can not assume equal like-\nlihood of different numerals. Probabilities of a given word\nwith its context in a standard n-gram language models are\n123\n282 Int J Speech Technol (2014) 17:281–289\nestimated based on frequency counts. Due to the very high\nnumber of possible contexts in large vocabulary applications\nand the limited sizes of training corpora it is very unlikely that\ntwo words would have the same probability estimates given\nthe same context. However, there are some words, which\nseem reasonable to have the same probabilities. For exam-\nple, let us look at the sentences: “wait 35 minutes” and “wait\n45 minutes”. Both sentences are grammatically correct and\nmake the same amount of sense. We could say that both sen-\ntences are equally likely to appear in the given language. It is\nhowever very unlikely that both numerals will occur exactly\nthe same number of times in the context of the words wait\nand minutes in a corpus. Consequently the language model\nassigns different probabilities to the two sentences. If the cor-\npus is rather small, those differences can become even more\nevident especially since many different numerals exists.\nThere are countless numbers. Still, we need only a rela-\ntively small amount of different words for writing all possible\nnumerals: one, two, three, etc., 10, 20, 30, etc, 100, 1000, etc.\nIn some languages we need more words since the numbers\nbetween 11 and 99 are written as one word. In inﬂective lan-\nguages this number increases due to different word forms. As\nwe will show later in this paper the relative frequencies of\ndifferent numerals can signiﬁcantly differ due to data sparsity\nand the limited size of corpora.\nThe aim of this paper is to propose methods for a man-\nual sorting of numerals in an inﬂective language into classes,\nbased on the languages characteristics, and to propose a gen-\neralization of class-based language models for better per-\nformance on speech recognition in a numeral rich domain.\nErrors with numerals can be more critical to a user who reads\nhypothesises from a speech recognition system, since the\nsubstitution of numerals can change important information\nin the sentence while other errors can be less critical for cor-\nrect understanding of the message.\nIt is our aim in this work to model the probabilities of\nclasses of numerals instead of individual numerals. We there-\nfore present different methods of deﬁning such classes and\nevaluate the generated language models in large vocabulary\ncontinuous speech recognition (LVCSR). Our experiments\nare performed on a Broadcast News database, which is rather\ngeneral in speech characteristics. However, one can easily\nconsider to use speech recognition on more special domains\nlike ﬁnancial and sports news, which contain more numerals.\n1.1 Other word classes\nThe argument presented for numerals can be transferred to\nother word classes. One example are proper nouns. Like\nbefore let us look at two sentences: “I am speaking with\nJames” and “I am speaking with Jethro”. Again, both sen-\ntences make equally sense, but their language model score\nwill differ based on the frequencies of these two names.\nSince we are only trying to show a possible example we\nwill not look at population statistics and just assume that\nJames is a popular name and Jethro is a rather rare name.\nSuppose that we try to recognize the utterance “I am speak-\ning with Jethro” and that the search algorithm has considered\nboth of the above sentences as hypotheses. If the pronuncia-\ntion was clean and no other acoustic disorders were present\nthe hypotheses with “Jethro” should have a higher acoustic\nscore. However, if the rare name occurs in the training cor-\npus rarely enough the difference in language models score\ncould be higher and the speech recognizer will say “James”.\nA similar example could also be presented for geographical\nnames (e.g. Valencia and Palencia – two cities in Spain) and\nother proper nouns.\n1.2 Previous work\nPrevious work that was focused on modelling numerals was\ndone mostly in an application speciﬁc domain with small\nvocabularies, e.g. the recognition of digits or natural num-\nbers over telephone lines ( Kvale 1996 ; Kurian and Balakr-\nishnan 2009). In ( Ghanty et al. 2010 ) Bengaly numerals were\nconsidered in a isolated word recognition task. Sproat (2010)\nused expansion of numerals in text normalization for text-to-\nspeech synthesis. Taking into account the characteristics of\nRussian, which are similar to the characteristics of Slovene,\ngenerative and discriminative language models were pre-\nsented.\nOther similar work was considered with well known class-\nbased language models ( Whittaker and Woodland 2003 ).\nThose models sort vocabulary words into classes based on\ndifferent criteria. Usually all words in the vocabulary are\nsorted into classes. The models proposed here can also be\nseen as a special case of generalized class-based language\nmodels. However, we found no previous work that speciﬁ-\ncally concerned numerals in LVCSR in a general domain like\nBroadcast News.\nProper names are also a topic of interest in pronuncia-\ntion ( Reveil et al. 2012 ; Schlippe et al. 2014 ), which is also\nimportant for speech recognition.\nHuet et al. (2010) proposed a post-processing method\nof speech recognizer hypotheses, which includes morpho-\nsyntactic description tagging. In the post processing step con-\nsecutive numerals and consecutive proper names are grouped\ninto single cardinal and proper names tags. In their method a\nnumber, which is written as several words, can be treated as\none single token in the morpho-syntactic language model.\n1.3 Statistical tests\nWith numerals we address some of the words that appear in\nspeech recognition. Depending on a given speciﬁc domain\nthe rate of numerals can be either small or large. Therefore we\n123\nInt J Speech Technol (2014) 17:281–289 283\nexpect a corresponding small or large improvement in per-\nformance. Normally large improvements are not questioned\nwhether they are the results of an genuine improvement of\nthe recognition system or the happened by coincidence. To\nestimate if small improvements are signiﬁcant we perform\nstatistical signiﬁcance tests.\nMost statistical signiﬁcance test give us a result in form of\na p-value that is between 0 and 1. Without going into to much\ndetail, we can say that improvements are called statistically\nsigniﬁcant if the p-value lies below the signiﬁcance level α.\nUsually we select α = 0.05.\nThe need for statistical tests in speech recognition was\nearly recognized in Gillick and Cox (1989) where test for\nisolated word recognition were proposed. However, only few\npapers report signiﬁcance tests results in addition to bare per-\nformance results. A more recent example is in Bisani and\nNey (2004) where bootstrap resampling tests for LVCSR\nwere proposed. Another type of statistical test for natural\nlanguage applications is approximate randomization (AR)\n(Riezler and Maxwell 2005 ), widely used in machine trans-\nlation tasks. AR makes less assumptions about the test set\nthan bootstrap resampling. In Riezler and Maxwell (2005)\nit is shown that AR test are more conservative, since they\nusually give larger p-values. We decided to use AR tests to\nestimate the signiﬁcance of our results.\n1.4 Organization of the paper\nThis paper is organized as follows. Section 2 presents a few\nexamples of recognition errors and our motives for modelling\nnumerals. In Sect. 3 we present Slovenian numerals, their\nwriting rules and grammatical rules for matching. In Sect.\n4 we describe the proposed sorting and modelling methods\nas well as their possible application to other languages and\nother word classes. In Sect. 5 the experimental system used\nto evaluate the proposed methods is discussed. In Sect. 6 the\nresults of word error rates, a comparison with class-based\nlanguage models, and a more detailed analysis of our best\nmodel are given. The conclusion follows in Sect. 7.\n2 Recognition errors and numerals\nThe three types of errors in a continuous speech recognition\nsystem are substitution, deletion and insertion of words. The\nmost widely used metric for evaluation LVCSR systems is\nword error rate (WER), deﬁned by the equation:\nE = S + D + I\nN , (1)\nwhere S, D, I and N are the numbers of substitutions, dele-\ntions, insertions and total words respectively. WER is an\nobjective metric that weights all errors equally. However,\nFig. 1 Substitution error in speech recognition\nin a practical application different errors may have different\nimpact on the subjective perception of the performance of a\nLVCSR system. To clarify this thought, let us look at some\nexamples of possible recognition errors. Although this paper\ndescribes the recognition of numerals in Slovene, for better\nunderstanding these examples are in English.\nThe example in Fig. 1 shows a reference transcription\n(LAB) and a recognizer hypothesis (REC) with a substitution\nerror. If one reads the hypothesis, this sentence will seem to\nbe nonsense. A reader would probably also realize that the\nword wearing was substituted in the recognition process and\ntherefore might guess the actual spoken sentence.\nAn example of a deletion error is shown in Fig. 2. A reader\nwould probably also recognize the error in this example and\nunderstand the spoken sentence correctly from the recognizer\nhypothesis.\nFigure 3 shows a substitution example, where one numeral\nwas replaced by another one. Looking at both the reference\ntranscription and the recognizer hypothesis it is not possible\nto tell which one is correct, since both sentences are grammat-\nically correct and both make sense. A reader looking at the\nhypothesis would think that the sentence is correctly recog-\nnized.\nOne more frequent type of errors in Slovene and other\ninﬂectional languages is the false recognition of word end-\nings. Those endings deﬁne the grammatical role of a word\ninside the sentence. For example a word could be recognized\nin its plural form instead of singular. In most cases these\nerrors may not hinder one to understand the meaning of the\nsentence. However, they make the sentence harder to under-\nstand.\nFrom those examples we can conclude that substitution\nerrors of numerals can be more critical in a practical applica-\nFig. 2 Deletion error in speech recognition\nFig. 3 Substitution-of-numerals error in speech recognition\n123\n284 Int J Speech Technol (2014) 17:281–289\ntion than some other types of errors. If numerals in a language\nare written in several words, as this is the case with cardinal\nnumerals in Slovene, also deletion and insertion errors can\nhave a similar effect.\n3 Numerals in the slovenian language\nSlovene ( Toporišiˇc 2000 ) distinguishes cardinal (one, two,\nthree ...), ordinal (ﬁrst, second, third ...), disjunctive (single,\ndouble, triple ...), and multiplicative (once, twice, thrice ...)\nnumerals. In our work we primarily consider cardinal numer-\nals, which are the most frequent. We brieﬂy also consider\nordinal numerals.\n3.1 Inﬂection\nSlovene is an inﬂectional language. Inﬂectional words\n(nouns, adjectives, verbs and some adverbs) are inﬂected\naccording to their grammatical characteristics. The inﬂec-\ntion of Slovenian words is reﬂected by different endings in\nword forms. The endings of numerals change by grammatical\nnumber, gender, and case.\nThe numeral one varies according to its grammatical gen-\nder, case and number. The later can be singular or plural.\nThe numeral two exists only in dual form. All other cardi-\nnal numerals exist only in plural form. The numerals from 2\nto 4 vary in case and in the nominative case also in gender.\nTable 1 shows a short example of the use of the numeral three\nin nominative case. The cardinal numerals from ﬁve on vary\nonly in case and have 4 possible endings: -ih, -im, -imi, -0\n(empty ending). An example is given in Table 2.\nAll ordinal numerals vary with gender, case, and number.\nThis gives us a larger set of possible endings. Also because\nTable 1 The use of three in all three grammatical genders\nGender Slovene English\nMale Trije moški Three men\nFemale Tri ženske Three women\nNeutral Tri dekleta Three girls\nthose numerals are all written in one word this means that\nthere are far more words needed to represent those numbers.\nThese are the two reasons, why there is a much larger number\nof words needed to represent ordinal numbers, than cardinal\nnumbers.\n3.2 Writing rules\nIn Slovenian language the cardinal numerals from 1 to 99\nand the numerals 100, 200 ... 900 are written as one word.\nThose words do not compound with each other. Also the\nwords thousand, million, billion, etc. are written as isolated\nwords. For example the number 12,375 is written in four\nwords as: dvanajst tisoˇc tristo petinsedemdeset. Writing this\nnumeral in different cases will change the ending only on\nthe last word and all previous words stay in nominative case.\nThus, a language model shall assign small probabilities to\nword sequences of numeral words, which do not have this\nproperty.\nConsidering that most cardinal numerals have 4 different\nendings, we can estimate that we need about 400 different\nwords to write all numerals from 1 to 99. With additional\n36 words (4 different word forms for 100, 200, ... 900) we\ncan write all numerals up to 999. We can also add 4 different\nword forms for thousand, million, billion, etc. Thus, we need\nonly a total of approximately 450 different isolated words to\nwrite every cardinal numeral used in everyday speech.\nCardinal numerals on the other hand are always written\nas one word. For example the number 12,375th is written:\ndvanajsttisotisoˇctristopetinsedemdeseti. Almost all ordinal\nnumbers have 11 different endings. Since those numerals are\nalways written in one word, we would need approximately\n11,000 different words to write all ordinal numerals only\nfrom 1st to 999th.\n3.3 Grammatical matching\nOne important property of Slovenian language is that adjec-\ntives (including numerals) and nouns match in gender, case,\nand number. A language model, trained on a grammatical\ncorrect corpus, would assign small probabilities to pairs of\nTable 2 The use of ten in all six\ngrammatical cases Case Slovene English\nNominative (Mojih) deset prijateljev (My) ten friends\nGenitive (Ne vidim) desetih prijateljev (I don’t see) ten friends\nDative (Pošiljam) desetim prijateljem (I send to) ten friends\nAccusative (Vidim) deset prijateljev (I see) ten friends\nLocative (Govorim o) desetih prijateljih (I talk about) ten friends\nInstrumental (Družim se z) desetimi prijatelji (I hang out with) ten friends\n123\nInt J Speech Technol (2014) 17:281–289 285\nadjectives and nous, which do not match and higher proba-\nbilities to those that do match.\nNumerals from ﬁve on are however an exception. A noun,\nthat follows a numeral, matches it only in four cases. In nom-\ninative and accusative case the following noun is in genitive\nform. That can also be seen in Table 2.\nStill we can see that some combinations of numeral forms\nand noun forms are not grammatically correct. For example if\nthe numeral has the ending -imi, it can be only followed by a\nnoun in instrumental form. Thus the ending of a numeral can\ngive us hints of the possible forms of the next word. Therefore\nit seems not to be reasonable to group numerals with different\ngrammatical characteristics into the same class.\n4 Sorting methods for numerals\n4.1 The methods\nWe have tested different methods for manual sorting of\nnumerals, mostly based on writing rules and numeral word\nendings and therefore indirectly on their grammatical charac-\nteristics. The methods also differ in the number of numerals\nin a class. All together we tested 6 different models.\n Baseline model: We did not use any sorting of numerals.\nThey were treated like any other words. This is a standard\nword-based n-gram model.\n Writing rules with 4 classes (WR-4): We took cardinal\nnumerals and grouped them according to writing rules. We\nsplit the numerals in two ways: the nominative case from\nthe other cases and the numerals expressing the numbers\nfrom 1 to 99 from the others. Thus, we got 4 classes with\n447 numerals in total. This sorting was based on the writing\nrules and possible sequences of word expressing cardinal\nnumerals.\n Writing rules with 6 classes (WR-6): We split the classes\nobtained by method WR-4 further into classes with numer-\nals expressing numbers from 1 to 10 and classes with num-\nbers 11–99. We got 6 classes with 447 numerals in total.\nThis method was chosen on the assumption that smaller\nclasses give better results.\n Word endings with 4 classes (END-4): We included only\ncardinal numerals expressing the numbers from 11 to 99.\nEach of these numerals has only 4 possible endings. We\nseparated them into 4 classes according to their endings.\nThe classes included 89 numerals each. This technique was\nbased on the property of word matching in the Slovenian\nlanguage.\n Word endings with 12 classes (END-12): Again we took\nonly cardinal numerals and separated them according to\ntheir endings and to the numbers they express. We grouped\nnumerals expressing numbers from 5 to 10, from 11 to 99\nand the numbers 100, 200, ... 900. We got 12 classes with\n416 numerals total.\n Word endings with 12 classes and ordinal numerals (END-\n12+ORD): We took the classes from the previous method\nand added a 13th class with 10.879 ordinal numerals from\n11th to 999th. Ordinal numerals from 1st to 10th were not\nsorted.\n4.2 Language model probabilities and relation to\nclass-based models\nTo each class of numerals a label was assigned. A class acts\nas a single vocabulary entry when building language models.\nThus probabilities are calculated only for classes. For exam-\nple, take the sentence I saw ten things . A classical bigram\nlanguage model would assign it the probability:\nP(I saw ten things ) = P(I| < s >)\n· P(saw|I)\n· P(ten|saw)\n· P(things|ten)\n· P(< /s > |\nthings), (2)\nwhere <s> and </s> are start-of-sentence and end-of-\nsentence markers respectively.\nLet us say that in some of the proposed models the numeral\nten is in a class labeled <number_A>. The new models\nwould assign the probability:\nP(I saw ten things ) = P(Is a w< number _A >things)\n= P(I| < s>)\n· P(saw|I)\n· P(< number _A >|saw)\n· P(things|< number _A >)\n· P(< /s > |things), (3)\nThese models are similar to class based models, where\nthe probability of the word ten would be calculated as the\nproduct\nP(ten|saw) = P(< number _A >|saw) ·\nP(ten|< number _A >), (4)\nwhere the ﬁrst factor is the probability that a word from the\nclass < number _A > follows the word saw and the second\nis the probability that the word ten occurs given the occur-\nrence of the class < number _A >.\nThe proposed models do not assign different probabilities\nto different words in a class. In fact if we would describe\n123\n286 Int J Speech Technol (2014) 17:281–289\nthese models as class-based, we would say that each word in\na class has a probability of 1.\nA generalization of the standard class-based language\nmodel, which also includes the proposed model can be pre-\nsented with the equation\nP(wi |wi− 1) = P(ci |wi− 1) · P(wi |ci )α . (5)\nThe equation describes the bigram case, but it can be eas-\nily adapted for higher order models. The parameter α is a real\nnumber. In a standard class-based language model the value\nof α is 1. In the proposed model α is 0. The word wi is belongs\nto the class ci . One could say that we constructed a theoreti-\ncally faulty class-based language models since the sum over\nall words is not 1. Our assumption was that such a model is\nnot necessary the best, at least not, if it comes to numerals.\nHowever, our models uses numeral classes and other words\nas basic vocabulary unit. The sum of all probabilities over\nthose units remains 1.\nTo conclude theoretical aspects of proposed language\nmodel in terms of class-based models, our idea is to assign\nall numerals in the given class a probability of 1, once the\nprobability of the class is given. Hence these probabilities\nare independent from the class size. The distinction between\nnumerals from the same class is based only on the acoustical\nmodel score.\nFor a comparison we repeated all our experiments with\nclass-based language models. We used the same sorting and\ncorpus processing techniques. The difference was in the ﬁnal\nlanguage models. Here we took the class size into account.\nAll words in a class got the same probability depending on\nthe class size.\n4.3 Application to other languages\nSlovene shares many similarities with other Slavic languages\nlike Croatian, Serbian, Russian, Czech, Slovak, etc. How-\never, those languages do not share the same writing rules for\nnumerals. We give are few examples.\nSlovak has similar writing rules as Slovene. Numerals\nfrom 11 to 99 are written as one word, e.g. 21 = dvadsat-\njeden.\nCzech is a example of a Slavic language in which numerals\nlike 21, 22 ... 31, 32 ... 99 are written as two separate words,\ne.g. 21 = dvacet jedna. Similar rules are in Ukrainian.\nIn Croatian numbers from to 20 are written as one word.\nNumbers from 21 to 99, except 30, 40 etc., can be written\neither as one word (21 = dvadesetjedan)o ra st h r e ew o r d s\n(dvadeset i jedan). This is the same for cardinal and ordinal\nnumerals.\nThe formation of numerals from 20 to 99 is often very\nsimilar. For tens (20, 30 ... 90) The numerals is one single\ninﬂected word. For other numbers the ﬁrst word or ﬁrst part\npresents the tens. This is sometimes followed by the word\nand. The last part is the inﬂected form of numerals presenting\n1–9. The differences between languages are in the writing of\nthose parts: there are either written as one word or as separate\nwords. The writing rules for some non-slavic languages are\nalso similar, e.g. in English, where the words are connected\nby a hyphen.\nThe numerals in the mentioned languages are inﬂected by\ngender and case. Like in Slovene, inﬂections are reﬂected by\nchanges in the word ending. This if a well known feature of\nSlavic languages.\nAs the word formation rules for numerals in other Slavic\nand some non-Slavic languages are very much similar, the\nmethods based on writing rules can be easily used in other\nlanguages, where numerals are written as one word. With a\nfew additions those methods can also be used for languages\nwhere numerals are written as separate words.\nFor example in Croatian, where numbers 21, 22 ... 99 can\nbe written a separate words with the word and in the middle,\none could sort the words for writting numerals based on the\nfact the some of then can appear before the word and and\nare not inﬂected, while the other words can appear after the\nword and and are inﬂected.\nThe methods based on word endings are applicable to\nother languages with similar word inﬂection rules.\n4.4 Application to other word classes\nThe proposed idea of manual sorting of numerals can be\nadapted to make methods for the sorting of other word\nclasses.\nA speech recognition system may produce errors, when\ntrying to recognize a sentence in which the ﬁrst and last name\nof a person a stated and this combination does not occur in\nthe training corpora. With many possible combination of ﬁrst\nand last names, we can assume that many combination wont\nappear even in a very large corpus.\nA language model, which would use the proposed method\nof sorting with ﬁrst and last names as their classes, would\nassign the same or very similar probabilities to all combina-\ntions of ﬁrst and last names. Therefore it would permit the\ncorrect recognition of combinations of ﬁrst and last names,\nwhich are not in the corpora, as long as both names individ-\nually appear in the recognizers vocabulary.\nAs with small numerals presenting small numbers, we also\nhave combinations of names that are more likely to occur,\nnamely the names of well known people.\nA similar case are geographical names. Classes could be\nformed by grouping cities from a certain state then grouping\nthe name of states in the US, Brazil, Germany and provinces\nin Canada. Next we could form a class from the names of\ncountries, rivers, mountains, etc.\nSome common nouns could be also considered for the\nproposed methods, e.g. trees, fruits, vegetables, etc.\n123\nInt J Speech Technol (2014) 17:281–289 287\n5 Experimental system\nTo test the proposed models we performed tests on the\nSlovenian Broadcast News (BN) database ( Žgank et al.\n2005). The database is divided into three sets: the train set,\nwhich was used to train the acoustical models, the develop-\nment set, which was later used to optimize model weights\nand word insertion penalty, and the test set, used to evaluate\nperformance.\nThe used features for the acoustical models were the log-\nenergy and 12 mel-frequency cepspral coefﬁcients with their\nﬁrst and second derivate. Computation of the features was\ndone on 32 ms Hamming windows with 10 ms spacing.\nThe ﬁnal acoustical models were grapheme based triphone\ncross-word models, composed of 16 Gaussian mixture den-\nsities.\nLanguage models were trained on the FidaPLUS corpus\n(Arhar and Gorjanc 2007 ), which is the largest Slovenian\ncorpus available to us. The ﬁrst step in building the pro-\nposed models was to create partial dictionaries with the pro-\nposed classes of numerals. After this step we started the\nprocessing of the text corpus. Each word was checked if it\nequals a numeral from one of the deﬁned classes. If so it\nwas replaced by the label of the corresponding class. The\ndictionary was constructed from the 100.000 most frequent\nwords in the text corpus and the numerals. The processed\ncorpus was used to build bigram and trigram language mod-\nels. We applied Good-Turing smoothing and Katz back-\noff.\nFor a more detailed error analysis we used a freely avail-\nable part-of-speech tagger called Obeliks. The tagger was\ndeveloped at the Institute Jožef Stafan. We tagged reference\ntranscriptions of the test set as well as speech recognition\noutputs. The tagger identiﬁes word class and other morpho-\nsyntactical information of the words in its input sentences.\nThe possible word classes were deﬁned in the JOS project\n(Erjavec et al. 2010 ), which are different then in the formal\nSlovenian Grammar. In the Grammar, numerals are a sub-\nclass of Adjectives. In the JOS speciﬁcation, the numerals\nare an independent word class. The types of numerals in the\nJOS speciﬁcations are cardinal, ordinal, special (other) and\npronominal numerals ( one, other).\n6 Results\n6.1 Comparison of modeling methods\nWe tested all 6 models on the test set. The word error rates\nresults are given in Table 3. The results of the proposed mod-\nels are in the 2nd and 3rd column. The results for the corre-\nsponding class-based models are in the 4th in 5th column.\nThe best results with bigram models were achieved with\nthe methods END-4 and END-12, where we achieved an\nimprovement of 0.6 % relatively. With the trigram model we\nachieved the best results with the END-12 method, namely\na 1.4 % relative reduction of word error rate.\nThe methods WR-4 and WR-6 gave the highest WER. A\ncomparison between these two methods and methods END-\n4 and END-12 conﬁrms our assumption that smaller classes\ngive better results, although the difference is very small.\nMethods END-4 and END-12, which are based on endings\nof numerals, outperform methods WR-4 and WR-6, which\nare based on writing rules. Both methods END-4 and END-\n12 gave better results than the baseline model, while methods\nWR-4 and WR-6 did not outperform the baseline system.\nA comparison between the methods END-12 and END-\n12+ORD shows an increase in word error rate if ordinal\nnumerals are added to the modelling methods. We assume\nthis is a consequence of the large number of numerals in one\nclass. For a useful inclusion of ordinal numerals we would\nneed different criteria based on which we could do further\nsorting of ordinal numerals.\nTable 4 show signiﬁcance test results performed between\nthe baseline system and all other. With only a few exceptions\nall differences between results are statistically signiﬁcant at\nthe α = 0.05 signiﬁcance level. The fact that small differ-\nences are signiﬁcant is not surprising. The differences in the\nmodels are minimal for words other than numerals. Therefore\nmost of the recognition results are the same. Most differences\noccur at numerals.\nTable 3 Word Error Rates in % Method Proposed models Class-based models\nBigram model Trigram model Bigram model Trigram model\nBaseline 31,46 28,40 31,46 28,40\nWR-4 31,81 28,70 31,65 29,31\nWR-6 31,78 28,63 31,43 29,80\nEND-4 31,26 28,03 31,23 28,99\nEND-12 31,26 28,01 31,17 28,96\nEND-12+ORD 31,52 28,12 31,15 29,24\n123\n288 Int J Speech Technol (2014) 17:281–289\nTable 4 Signiﬁcance test\nresults: p-values Method Proposed models Class-based models\nBigram model Trigram model Bigram model Trigram model\nWR-4 0.015 0.039 0.133 < 0.001\nWR-6 0.020 0.105 0.851 < 0.001\nEND-4 0.019 <0.001 0.003 < 0.001\nEND-12 0.062 0.001 0.001 < 0.001\nEND-12+ORD 0.668 0.035 < 0.001 < 0.001\n6.2 Comparison with class-based language models\nA comparison with the results from the proposed models\nshows an interesting pattern. The comparison of word error\nrates of the bigram models shows that the class-based lan-\nguage models always outperform the proposed model. On\nthe other hand, a comparison of the results of trigram mod-\nels shows right the opposite. All of the proposed models\ngave better results than the corresponding class-based mod-\nels. We can observe that the proposed modeling technique\nperforms better with higher order models. It is also notable\nthat with class-based language models method END-12 gave\nthe best results. This indicates that the inclusion of larger\nclasses works better with class-based models.\nThe obtained results indicate the possibility of a general-\nization of class-based language models, deﬁned by Eq. ( 5),\nby allowing different values for the parameter α.\n6.3 Detailed analysis\nWe will present a more detailed analysis on the results\nobtained with the END-12 method. The test set con-\ntains 1898 segments with total 22743 words. We analysed\nit with a part-of-speech tagger. We identiﬁed 486 car-\ndinal, 129 ordinal, 119 pronominal, and 5 other numer-\nals.\nWe achieved the best results with the method END-12,\nwhere only cardinal numerals were modelled. Therefore we\nmade a more detailed analysis on errors of cardinal numer-\nals. In the baseline system 14 cardinal numerals were deleted,\n27 inserted, and 123 substituted with other words. The total\nerror count on cardinal numerals in the baseline system is\n164 (33.7 % of all numerals). In the END-12 system 14\ncardinal numerals were deleted, 10 inserted, and 73 substi-\ntuted with other words. In this system the total error count\non cardinal numerals is 97 (20.0 % of all numerals). This\nmakes relative error rate reduction of 41 % on cardinal numer-\nals.\nFurther we separated all sentences in the test set into 2\nsets. The ﬁrst set consists of all sentences containing at least\none modelled numeral and the second all other sentences.\nThe term modelled numeral refers to a numeral that was in\none of the classes used in method END-12. We checked the\nword error rates in those separated test sets and compared it\nwith the baseline model. In the set with numerals we got a\nword error rate reduction from 26.2 to 22.0 % (16 % relative\nimprovement) and in the set without numerals an increase in\nword error rate from 28.6 to 28.8 % (0.7 % relative worsen-\ning). Although these scenarios are not realistic they give us\na worst-case/best-case environment of what we can expect\nfrom the proposed models on domains with different amounts\nof numerals.\n7 Conclusion and further work\nIn this paper we presented the language modelling technique\nfor numerals. It is a variant of class-based language mod-\nels, that model only the classes itself and not words in it.\nAlthough the model is rather simple, it gives better recogni-\ntion results. Our models are based on grammatical knowledge\nof the Slovenian language. Similar methods could work with\nother inﬂectional languages like Czech, Russian or Polish,\nand other word classes.\nThe improvement in word error rate was small, but we\nshowed that in domains with more numerals, like ﬁnancial or\nsport news broadcast, we could expect much better results.\nMoreover, while the error rate on cardinal numerals in the\nbaseline system is much higher than the overall WER, the\nerror rate on cardinal numerals in our best performing pro-\nposed model is signiﬁcantly lower. The initially high error\nrate on cardinal numerals also conﬁrms our arguments that\nnumerals are harder to model than other words.\nWe also proposed a generalization of the class-based lan-\nguage model. Models, presented in this paper, and the well\nknown class-based models are special cases of it. Further\nwork in this area would include an optimization algorithm for\nchoosing optimal parameters for such models. The results of\nthis work also indicate that α should depend on the language\nmodel order.\nAcknowledgments This work was partly ﬁnancially supported by\nthe Slovenian Research Agency ARRS under contract number 1000-\n10-310131.\n123\nInt J Speech Technol (2014) 17:281–289 289\nOpen Access This article is distributed under the terms of the Creative\nCommons Attribution License which permits any use, distribution, and\nreproduction in any medium, provided the original author(s) and the\nsource are credited.\nReferences\nArhar, Š., & Gorjanc, V . (2007). Korpus FidaPLUS: Nova generacija\nslovenskega referenˇcnega korpusa. Jezik in slovstvo, 52, 95–110.\nAubert, X. L. (2002). An overview of decoding techniques for large\nvocabulary continuous speech recognition. Computer Speech & Lan-\nguage, 16, 89–114.\nBisani, M., & Ney, H. (2004). Bootstrap Estimates for Conﬁdence Inter-\nvals in ASR Performance Evaluation. ICASSP 2004 Proceedings (I-\n409-412). Montreal, Quebec, Canada: ICASSP.\nErjavec, T., Fišer, D., Krek, S., & Ledinek, N. (2010). The JOS\nLinguistically Tagged Corpus of Slovene. Seventh International\nConference on Language Resources and Evaluation Proceedings\n(pp. 1806–1809). Valletta, Malta: LREC.\nGhanty, S. K., Shaikh, S. H., & Chaki, N. (2010). On Recognition\nof Spoken Bengali Numerals. Computer Information Systems and\nIndustrial Management Applications (CISIM), 2010 International\nConference on (pp. 54–59).\nGillick, L., & Cox, S. J. (1989). Some Statistical Issues in the Compar-\nison of Speech Recognition Algorithms, ICASSP 1989 Proceedings\n(pp. 532–535). Glasgow, Scotland: ICASSP.\nHuet, S., Gravier, G., & Sebillot, P. (2010). Morpho-syntactic post-\nprocessing of N-best lists for improved French automatic speech\nrecognition. Computer Speech & Language, 24, 663–684.\nKvale, K. (1996). Norwegian numerals: a challenge to automatic speech\nrecognition. ICSLP 1996 Proceedings(pp. 2028–2031). Philadelphia\n:I C S L P .\nKurian, C., & Balakrishnan, K. (2009). Speech recognition of Malay-\nalam numbers. World Congress on Nature & Biologically Inspired\nComputing (pp. 1475–1479). Coimbatore, India: NaBIC.\nRapp, B. (2008) N-gram language models for Polish language. Basic\nconcepts and applications in automatic speech recognition systems.\nInternational Multiconference on Computer Science and Informa-\ntion Technology, Proceedings(pp. 321–324). Wisła, Poland.\nReveil, B., Martens, J.-P., & van den Heuvel, H. (2012). Improving\nproper name recognition by means of automatically per name recog-\nnition by means of learned pronunciation variants. Speech Commu-\nnication, 54, 321–340.\nRiezler, S., & Maxwell III, J.T. (2005). On Some Pitfails in Automatic\nEvaluation and Signiﬁcance testing fot MT. ACL05 Workshop on\nintrinsic and extrinsic evaluation measures for MT and/or summa-\nrization.\nSchlippe, T., Ochs, S., & Schultz, T. (2014). Web-based tools and meth-\nods for rapid pronunciation dictionary creation. Speech Communi-\ncation, 56, 101–118.\nSproat, R. (2010). Lightly supervised learning of text normalization:\nRussian number names. Spoken Language Technology Workshop\n(SLT) (pp. 436–441).\nToporišiˇc, J. (2000). Slovenska slovnica. Ljubljana: Založba Obzorja.\nWhittaker, E. W. D., & Woodland, P. C. (2003). Language modeling for\nRussian and English using words and classes. Computer Speech &\nLanguage, 17, 87–104.\nŽgank, A., Verdonik, D., Markuš, A.Z., & Kaˇ ciˇc, Z. (2005). BNSI\nSlovenian Broadcast News Database - speech and text corpus. 9th\nInternational conference on Speech communication technology pro-\nceedings (pp. 1537–1540). Lisboa, Portugal: INTERSPEECH.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8612480759620667
    },
    {
      "name": "Numeral system",
      "score": 0.8284822106361389
    },
    {
      "name": "Language model",
      "score": 0.6383171081542969
    },
    {
      "name": "Sorting",
      "score": 0.6090090274810791
    },
    {
      "name": "Natural language processing",
      "score": 0.6079695820808411
    },
    {
      "name": "Vocabulary",
      "score": 0.5581416487693787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.555938184261322
    },
    {
      "name": "n-gram",
      "score": 0.551939070224762
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5316539406776428
    },
    {
      "name": "Sentence",
      "score": 0.530742883682251
    },
    {
      "name": "Word (group theory)",
      "score": 0.490582138299942
    },
    {
      "name": "Cache language model",
      "score": 0.4588383734226227
    },
    {
      "name": "Word error rate",
      "score": 0.4434208273887634
    },
    {
      "name": "Speech recognition",
      "score": 0.34371617436408997
    },
    {
      "name": "Natural language",
      "score": 0.26431405544281006
    },
    {
      "name": "Linguistics",
      "score": 0.19044509530067444
    },
    {
      "name": "Universal Networking Language",
      "score": 0.15259531140327454
    },
    {
      "name": "Algorithm",
      "score": 0.14675185084342957
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Comprehension approach",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37696226",
      "name": "University of Maribor",
      "country": "SI"
    }
  ]
}