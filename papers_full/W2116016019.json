{
  "title": "A Unified Model of Structural Organization in Language and Music",
  "url": "https://openalex.org/W2116016019",
  "year": 2002,
  "authors": [
    {
      "id": "https://openalex.org/A5021400032",
      "name": "Rens Bod",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W191085059",
    "https://openalex.org/W6632206554",
    "https://openalex.org/W6681824180",
    "https://openalex.org/W2099832412",
    "https://openalex.org/W6680546146",
    "https://openalex.org/W2141380728",
    "https://openalex.org/W6672162848",
    "https://openalex.org/W6632145054",
    "https://openalex.org/W2110543508",
    "https://openalex.org/W6601010272",
    "https://openalex.org/W2132796688",
    "https://openalex.org/W7034716564",
    "https://openalex.org/W1992254746",
    "https://openalex.org/W6600613151",
    "https://openalex.org/W6845202251",
    "https://openalex.org/W6661832149",
    "https://openalex.org/W2061495533",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W6827768719",
    "https://openalex.org/W1917508706",
    "https://openalex.org/W6602901515",
    "https://openalex.org/W1976645892",
    "https://openalex.org/W2152324136",
    "https://openalex.org/W2037172343",
    "https://openalex.org/W2006697556",
    "https://openalex.org/W6664471178",
    "https://openalex.org/W6631963490",
    "https://openalex.org/W2017707809",
    "https://openalex.org/W6678648713",
    "https://openalex.org/W6648982606",
    "https://openalex.org/W2067694419",
    "https://openalex.org/W6754237513",
    "https://openalex.org/W1564723434",
    "https://openalex.org/W6633723149",
    "https://openalex.org/W6752253332",
    "https://openalex.org/W6699364410",
    "https://openalex.org/W4248681690",
    "https://openalex.org/W2127359522",
    "https://openalex.org/W2170716495",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W4239593869",
    "https://openalex.org/W4233559841",
    "https://openalex.org/W1994851566",
    "https://openalex.org/W1533169541",
    "https://openalex.org/W2116915753",
    "https://openalex.org/W2891979318",
    "https://openalex.org/W3019546484",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2092654472",
    "https://openalex.org/W2030402452",
    "https://openalex.org/W2148497595",
    "https://openalex.org/W1561949603",
    "https://openalex.org/W2137815234",
    "https://openalex.org/W4233735361",
    "https://openalex.org/W2131297983",
    "https://openalex.org/W2095063924",
    "https://openalex.org/W4232064687",
    "https://openalex.org/W2085710315",
    "https://openalex.org/W2087165009",
    "https://openalex.org/W2054658115",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W1938853666",
    "https://openalex.org/W2993383518",
    "https://openalex.org/W4206462942",
    "https://openalex.org/W2952110792",
    "https://openalex.org/W1844099549",
    "https://openalex.org/W1528946842",
    "https://openalex.org/W1638203394",
    "https://openalex.org/W2889255225",
    "https://openalex.org/W1535015163",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2044153609",
    "https://openalex.org/W3199946934",
    "https://openalex.org/W2104996750",
    "https://openalex.org/W71815502",
    "https://openalex.org/W2104399512",
    "https://openalex.org/W174472240",
    "https://openalex.org/W2624577962",
    "https://openalex.org/W4230960895",
    "https://openalex.org/W15058652",
    "https://openalex.org/W2317255752",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2017537431",
    "https://openalex.org/W1981654323",
    "https://openalex.org/W1539587067",
    "https://openalex.org/W3021452258",
    "https://openalex.org/W2726537443",
    "https://openalex.org/W2041404167",
    "https://openalex.org/W2111501447",
    "https://openalex.org/W2023723978",
    "https://openalex.org/W2343954916"
  ],
  "abstract": "Is there a general model that can predict the perceived phrase structure in language and music? While it is usually assumed that humans have separate faculties for language and music, this work focuses on the commonalities rather than on the differences between these modalities, aiming at finding a deeper 'faculty'. Our key idea is that the perceptual system strives for the simplest structure (the 'simplicity principle'), but in doing so it is biased by the likelihood of previous structures (the 'likelihood principle'). We present a series of data-oriented parsing (DOP) models that combine these two principles and that are tested on the Penn Treebank and the Essen Folksong Collection. Our experiments show that (1) a combination of the two principles outperforms the use of either of them, and (2) exactly the same model with the same parameter setting achieves maximum accuracy for both language and music. We argue that our results suggest an interesting parallel between linguistic and musical structuring.",
  "full_text": "Journal of Artificial Intelligence Research 17 (2002) 289-308                                  Submitted 6/02; published 10/02\n©2002  AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nA Unified Model of Structural Organization in\nLanguage and Music\nRens Bod    RENS@ILLC.UVA.NL\nInstitute for Logic, Language and Computation\nUniversity of Amsterdam, Nieuwe Achtergracht 166\n1018 WV Amsterdam, THE NETHERLANDS, and\nSchool of Computing, University of Leeds\nLS2 9JT Leeds, UK\nAbstract\n     Is there a general model that can predict the perceived phrase structure in language and\nmusic? While it is usually assumed that humans have separate faculties for language and\nmusic, this work focuses on the commonalities rather than on the differences between these\nmodalities, aiming at finding a deeper \"faculty\". Our key idea is that the perceptual system\nstrives for the simplest structure (the \"simplicity principle\"), but in doing so it is biased by the\nlikelihood of previous structures (the \"likelihood principle\"). We present a series of data-\noriented parsing (DOP) models that combine these two principles and that are tested on the\nPenn Treebank and the Essen Folksong Collection. Our experiments show that (1) a\ncombination of the two principles outperforms the use of either of them, and (2) exactly the\nsame model with the same parameter setting achieves maximum accuracy for both language\nand music. We argue that our results suggest an interesting parallel between linguistic and\nmusical structuring.\n1.  Introduction: The Problem of Structural Organization\nIt is widely accepted that the human cognitive system tends to organize perceptual information\ninto hierarchical descriptions that can be conveniently represented by tree structures. Tree\nstructures have been used to describe linguistic perception (e.g. Wundt, 1901; Chomsky,\n1965), musical perception (e.g. Longuet-Higgins, 1976; Lerdahl & Jackendoff, 1983) and\nvisual perception (e.g. Palmer, 1977; Marr, 1982). Yet, little attention has been paid to the\ncommonalities between these different forms of perception and to the question whether there\nexists a general, underlying mechanism that governs all perceptual organization. This paper\nstudies exactly that question: acknowledging the differences between the perceptual\nmodalities, is there a general model that can predict the perceived tree structure for sensory\ninput? In studying this question, we will use an empirical methodology: any model that we\nmight hypothesize will be tested against manually analyzed benchmarks such as the\nlinguistically annotated Penn Treebank (Marcus et al. 1993) and the musically annotated\nEssen Folksong Collection (Schaffrath, 1995). While we will argue for a general model of\nstructural organization in language, music and vision, we will carry out experiments only with\nlinguistic and musical benchmarks, since no benchmark of visual tree structures is currently\navailable, to the best of our knowledge.\n     Figure 1 gives three simple examples of linguistic, musical and visual information with their\ncorresponding tree structures printed below (these examples are resp. taken from Martin et al.\n1987, Lerdahl & Jackendoff, 1983, and Dastani, 1998).\nBOD\n290\nList  the  sales  of  products  in  1973\nV DT N P N P N\nNP PP PP\nNP\nNP\nS\nList the sales of products in  1973\n                 \nFigure 1: Examples of linguistic, musical and visual input with their tree structures\nThus, a tree structure describes how parts of the input combine into constituents and how\nthese constituents combine into a representation for the whole input. Note that the linguistic\ntree structure is labeled with syntactic categories, whereas the musical and visual tree\nstructures are unlabeled. This is because in language there are syntactic constraints on how\nwords can be combined into larger constituents (e.g. in English a determiner can be combined\nwith a noun only if it precedes that noun, which is expressed by the rule NP →  DT N), while\nin music (and to a lesser extent in vision) there are no such restrictions: in principle any note\nmay be combined with any other note.\n     Apart from these differences, there is also a fundamental commonality: the perceptual input\nundergoes a process of hierarchical structuring which is not found in the input itself. The main\nproblem is thus: how can we derive the perceived tree structure for a given input? That this\nproblem is not trivial may be illustrated by the fact that the inputs above can also be assigned\nthe following, alternative tree structures:\nList   the   sales   of   products   in   1973\nV DT N P N P N\nNP PP PP\nNP\nS\n           \nFigure 2: Alternative tree structures for the inputs in Figure 1\nThese alternative structures are possible in that they can be perceived. The linguistic tree\nstructure in Figure 1 corresponds to a meaning which is different from the tree in Figure 2.\nThe two musical tree structures correspond to different groupings into motifs. And the two\nvisual structures correspond to different visual Gestalts. But while the alternative tree\nstructures are all possible, they are not plausible: they do not correspond to the structures that\nare actually perceived by the human cognitive system.\n     The phenomenon that the same input may be assigned different structural organizations is\nknown as the ambiguity problem . This problem is one of the hardest problems in modeling\nhuman perception. Even in language, where a phrase-structure grammar may specify which\nwords can be combined into constituents, the ambiguity problem is notoriously hard (cf.\nManning & Schütze, 1999). Charniak (1997: 37) argues that many sentences from the Wall\nStreet Journal have more than one million different parse trees. The ambiguity problem for\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n291\nmusical input is even harder, since there are virtually no constraints on how notes may be\ncombined into constituents. Talking about rhythm perception in music, Longuet-Higgins and\nLee (1987) note that \"Any given sequence of note values is in principle infinitely ambiguous,\nbut this ambiguity is seldom apparent to the listener.\".\n     In the following Section, we will discuss two principles that have traditionally been proposed\nto solve ambiguity: the likelihood principle and the simplicity principle. In Section 3, we will\nargue for a new integration of the two principles within the data-oriented parsing framework.\nOur hypothesis is that the human cognitive system strives for the simplest structure generated\nby the shortest derivation, but that in doing so it is biased by the frequency of previously\nperceived structures. In Section 4, we go into the computational aspects of our model. In\nSection 5, we discuss the linguistic and musical test domains. Section 6 presents an empirical\ninvestigation and comparison of our model. Finally, in Section 7, we give a discussion of our\napproach and go into other combinations of simplicity and likelihood that have been proposed\nin the literature.\n2.   Two principles: Likelihood and Simplicity\nHow can we predict from the set of all possible tree structures the tree that is actually\nperceived by the human cognitive system? In the field of visual perception, two competing\nprinciples have traditionally been proposed to govern structural organization. The first, initiated\nby Helmholtz (1910), advocates the likelihood principle : perceptual input will be organized\ninto the most probable structure. The second, initiated by Wertheimer (1923) and developed by\nother Gestalt psychologists, advocates the simplicity principle : the perceptual system is\nviewed as finding the simplest rather than the most probable structure (see Chater, 1999, for\nan overview). These two principles have also been used in linguistic and musical structuring.\nIn the following, we briefly review these principles for each modality.\n2.1  Likelihood\nThe likelihood principle has been particularly influential in the field of natural language\nprocessing (see Manning and Schütze, 1999, for a review). In this field, the most appropriate\ntree structure of a sentence is assumed to be its most likely structure. The likelihood of a tree\nis computed from the probabilities of its parts (e.g. phrase-structure rules) which are in turn\nestimated from a large manually analyzed language corpus, i.e. a treebank . State-of-the-art\nprobabilistic parsers such as Collins (2000), Charniak (2000) and Bod (2001a) obtain around\n90% precision and recall on the Penn Wall Street Journal treebank (Marcus et al. 1993).\n     The likelihood principle has also been applied to musical perception, e.g. by Raphael (1999)\nand Bod (2001b/c). As in probabilistic natural language processing, the most probable musical\ntree structure can be computed from the probabilities of rules or fragments taken from a large\nannotated musical corpus. A musical benchmark which has been used by some models is the\nEssen Folksong Collection (Schaffrath, 1995).\n     Also in vision science, there is a huge interest in probabilistic models (e.g. Hoffman, 1998;\nKersten, 1999). Mumford (1999) has even seen fit to declare the Dawning of Stochasticity.\nUnfortunately, no visual treebanks are currently available.\n2.2  Simplicity\nThe simplicity principle has a long tradition in the field of visual perception psychology (e.g.\nRestle, 1970; Leeuwenberg, 1971; Simon, 1972; Buffart et al. 1983; van der Helm, 2000). In\nBOD\n292\nthis field, a visual pattern is formalized as a constituent structure by means of a visual coding\nlanguage based on primitive elements such as line segments and angles. Perception is\ndescribed as the process of selecting the simplest structure corresponding to the \"shortest\nencoding\" of a visual pattern.\n     The notion of simplicity has also been applied to music perception. Collard et al. (1981) use\nthe coding language of Leeuwenberg (1971) to predict the metrical structure for four preludes\nfrom Bach's Well-Tempered Clavier . More well-known in music perception is the theory\nproposed by Lerdahl and Jackendoff (1983). Their theory contains two kinds of rules: \"well-\nformedness rules\" and \"preference rules\". The role of well-formedness rules is to define the\nkinds of formal objects (grouping structures) the theory employs. What grouping structures a\nlistener actually hears, is then described by the preference rules which describe Gestalt-\npreferences of the kind identified by Wertheimer (1923), and which can therefore also be seen\nas an embodiment of the simplicity principle.\n     Notions of simplicity also exist in language processing. For example, Frazier (1978) can be\nviewed as arguing that the parser prefers the simplest structure containing minimal\nattachments. Bod (2000a) defines the simplest tree structure of a sentence as the structure\ngenerated by the smallest number of subtrees from a given treebank.\n3.  Combining Likelihood and Simplicity\nThe key idea of the current paper is that both principles play a role in perceptual organization,\nalbeit rather different ones: the simplicity principle as a general cognitive preference for\neconomy, and the likelihood principle as a probabilistic bias due to previous perceptual\nexperiences. Informally stated, our working hypothesis is that the human cognitive system\nstrives for the simplest structure generated by the shortest derivation, but that in doing so it is\nbiased by the frequency of previously perceived structures (some other combinations of\nsimplicity and likelihood will be discussed in Section 7). To formally instantiate our working\nhypothesis, we first need a model that defines the set of possible structures of an input. In this\npaper, we have chosen for a model that defines the set of phrase-structures for an input on\nthe basis of a treebank of previously analyzed input, and which is known as the Data-Oriented\nParsing or DOP model (see Bod, 1998; Collins & Duffy, 2002). DOP learns a grammar by\nextracting subtrees from a given treebank and combines these subtrees to analyze fresh input.\nWe have chosen DOP because (1) it uses subtrees of arbitrary size, thereby capturing non-\nlocal dependencies, and (2) it has obtained very competitive results on various benchmarks\n(Bod, 2001a/b; Collins & Duffy, 2002). In the following, we first review the DOP model and\ndiscuss the use of the likelihood and simplicity principles by this approach. Next, we show how\nthese two principles can be combined to instantiate our working hypothesis.\n3.1 Data-Oriented Parsing\nIn this Section, we illustrate the DOP model with a linguistic example (for a rigorous definition\nof DOP, the reader is referred to Bod, 1998). We will come back to some musical examples\nin Section 5. Suppose we are given the following extremely small linguistic treebank of two\ntrees for resp. she wanted the dress on the rack  and she saw the dog with the telescope\n(actual treebanks contain tens of thousands of trees, cf. Marcus et al. 1993):\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n293\n S\nNP\nshe\nVP\nVP\n V NP\nPP\n P NP\n S\nNP VP\n V\nwanted\nNP\nNP PP\nNP P\nshe\nthe dress\nthe rackon\nthe dog thesaw with telescope\nFigure 3: An example treebank\nThe DOP model can parse a new sentence , e.g. She saw the dress with the telescope , by\ncombining subtrees from this treebank by means of a substitution operation (indicated as ¡):\n S\nNP\nshe\nVP\nVP\n V NP\nPP\nsaw\nPP\n P NP\nthewith telescope\nNP\nthe dress\n° °  S\nNP\nshe\nVP\nVP\n V NP\nPP\n P NP\nthe thesaw with telescope\n=\ndress\nFigure 4: Parsing a sentence by combining subtrees from Figure 3\nThus the substitution operation combines two subtrees by substituting the second subtree on\nthe leftmost nonlexical leaf node of the first subtree (the result of which may be combined\nwith a third subtree, etc.). A combination of subtrees that results in a tree structure for the\nwhole sentence is called a derivation . Since there are many different subtrees, of various\nsizes, there are typically also many different derivations that produce, however, the same tree;\nfor instance:\n S\nNP\nshe\nVP\nVP PP\n P NP\nthewith telescope\nNP\nthe dress\n° =  S\nNP\nshe\nVP\nVP\n V NP\nPP\n P NP\nthe thesaw with telescopedress\n V NP\nsaw\nFigure 5: A different derivation which produces the same parse tree\nBOD\n294\nThe more interesting case occurs when there are different derivations that produce different\nparse trees. This happens when a sentence is ambiguous; for example, DOP also produces\nthe following alternative parse tree for She saw the dress with the telescope :\n S\nNP VP\n V NP\nNP PP\nshe\nthe dress\n V\nsaw\nPP\n P NP\nthewith telescope\n=  S\nNP VP\n V NP\nNP PP\nshe\nthe dress\nsaw\n P NP\nthewith telescope\n° °\nFigure 6: A different derivation which produces a different parse tree\n3.2 Likelihood-DOP\nIn Bod (1993), DOP is enriched with the likelihood principle to predict the perceived tree\nstructure from the set of possible structures. This model, which we will call Likelihood-DOP ,\ncomputes the most probable tree of an input from the occurrence-frequencies of the subtrees.\nThe probability of a subtree t, P(t), is computed as the number of occurrences of t, | t |, divided\nby the total number of occurrences of treebank-subtrees that have the same root label as t.\nLet r(t) return the root label of t. Then we may write:\nP(t)  = | t |\nΣ t': r(t')= r( t)  | t' |\nThe probability of a derivation t1¡...¡tn is computed by the product of the probabilities of its\nsubtrees ti:\nP(t1¡...¡tn)  =  Π i P(ti)\nAs we have seen, there may be different derivations that generate the same parse tree. The\nprobability of a parse tree T is thus the sum of the probabilities of its distinct derivations. Let\ntid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given\nby\nP(T)  =  Σ dΠ i P(tid)\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n295\nIn parsing a sentence s, we are only interested in the trees that can be assigned to s, which we\ndenote by Ts. The best parse tree, Tbest, according to Likelihood-DOP is then the tree which\nmaximizes the probability of Ts:\nTbest  =  arg    max P(Ts)\n              Ts\nThus Likelihood-DOP computes the probability of a tree as a sum of products, where each\nproduct corresponds to the probability of a certain derivation generating the tree. This\ndistinguishes Likelihood-DOP from most other statistical parsing models that identify exactly\none derivation for each parse tree and thus compute the probability of a tree by only one\nproduct of probabilities (e.g. Charniak, 1997; Collins, 1999; Eisner, 1997). Likelihood-DOP's\nprobability model allows for including counts of subtrees of a wide range of sizes: everything\nfrom counts of single-level rules to counts of entire trees.\n      Note that the subtree probabilities in Likelihood-DOP are directly estimated from their\nrelative frequencies in the treebank-trees. While the relative-frequency estimator obtains\ncompetitive results on several domains (Bonnema et al. 1997; Bod, 2001a; De Pauw, 2000), it\ndoes not maximize the likelihood of the training data (Johnson, 2002). This is because there\nmay be hidden derivations which the relative-frequency estimator cannot deal with. 1 There\nare estimation procedures that do take into account hidden derivations and that maximize the\nlikelihood of the training data. For example, Bod (2000b) presents a Likelihood-DOP model\nwhich estimates the subtree probabilities by a maximum likelihood re-estimation procedure\nbased on the expectation-maximization algorithm (Dempster et al. 1977). However, since the\nrelative frequency estimator has so far not been outperformed by any other estimator (see\nBod et al. 2002b), we will stick to the relative frequency estimator for the current paper.\n3.3 Simplicity-DOP\nLikelihood-DOP does not do justice to the preference humans display for the simplest\nstructure generated by the shortest derivation of an input. In Bod (2000a), the simplest tree\nstructure of an input is defined as the tree that can be constructed by the smallest number of\nsubtrees from a treebank. We will refer to this model as Simplicity-DOP. Instead of\nproducing the most probable parse tree for an input, Simplicity-DOP thus produces the parse\ntree generated by the shortest derivation consisting of the fewest treebank-subtrees,\nindependent  of the probabilities of these subtrees. We define the length of a derivation d,\nL(d), as the number of subtrees in d; thus if d = t1¡...¡tn then L(d) = n. Let dT be a derivation\nwhich results in parse tree T, then the best parse tree, Tbest, according to Simplicity-DOP is\nthe tree which is produced by a derivation of minimal length:\nTbest  =  arg    min L(dTs)\n             Ts\nAs in Section 3.2, Ts is a parse tree of a sentence s. For example, given the treebank in Figure\n3, the simplest parse tree for She saw the dress with the telescope  is given in Figure 5, since\n                                                \n1 Only if the subtrees are restricted to depth 1 does the relative frequency estimator coincide with the\nmaximum likelihood estimator. Such a depth-1 DOP model corresponds to a stochastic context-free\ngrammar. It is well-known that DOP models which allow subtrees of greater depth outperform depth-1\nDOP models (Bod, 1998; Collins & Duffy, 2002).\nBOD\n296\nthat parse tree can be generated by a derivation of only two treebank-subtrees, while the\nparse tree in Figure 6 (and any other parse tree) needs at least three treebank-subtrees to be\ngenerated.2\n     The shortest derivation may not be unique: it can happen that different parse trees of a\nsentence are generated by the same minimal number of treebank-subtrees (also the most\nprobable parse tree may not be unique, but this never happens in practice). In that case we\nwill back off to a frequency ordering of the subtrees. That is, all subtrees of each root label\nare assigned a rank according to their frequency in the treebank: the most frequent subtree (or\nsubtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.\nNext, the rank of each (shortest) derivation is computed as the sum of the ranks of the\nsubtrees involved. The derivation with the smallest sum, or highest rank, is taken as the final\nbest derivation producing the final best parse tree in Simplicity-DOP (see Bod, 2000a).\n     We performed one little adjustment to the rank of a subtree. This adjustment averages the\nrank of a subtree by the ranks of its own sub- subtrees. That is, instead of simply taking the\nrank of a subtree, we compute the rank of a subtree as the (arithmetic) mean of the ranks of\nall its sub- subtrees (including the subtree itself). The effect of this technique is that it\nredresses a very low-ranked subtree if it contains high-ranked sub-subtrees.\n     While Simplicity-DOP and Likelihood-DOP obtain rather similar parse accuracy on the\nWall Street Journal and the Essen Folksong Collection (in terms of precision/recall -- see\nSection 6), the best trees predicted by the two models do not quite match. This suggests that a\ncombined model, which does justice to both simplicity and likelihood, may boost the accuracy.\n3.4 Combining Likelihood-DOP and Simplicity-DOP: SL-DOP and LS-DOP\nThe underlying idea of combining likelihood and simplicity is that the human perceptual system\nsearches for the simplest tree structure (generated by the shortest derivation) but in doing so it\nis biased by the likelihood of the tree structure. That is, instead of selecting the simplest tree\nper se, our combined model selects the simplest tree from among the n likeliest trees, where n\nis our free parameter. There are of course other ways to combine simplicity and likelihood\nwithin the DOP framework. A straightforward alternative would be to select the most\nprobable tree from among the n simplest trees, suggesting that the perceptual system is\nsearching for the most probable structure only from among the simplest ones. We will refer to\nthe first combination of simplicity and likelihood (which selects the simplest among the n\nlikeliest trees) as Simplicity-Likelihood-DOP  or SL-DOP, and to the second combination\n(which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP  or LS-\nDOP. Note that for n=1, Simplicity-Likelihood-DOP is equal to Likelihood-DOP, since there is\nonly one most probable tree to select from, and Likelihood-Simplicity-DOP is equal to\nSimplicity-DOP, since there is only one simplest tree to select from. Moreover, if n gets large,\nSL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP. By\nvarying the parameter n, we will be able to compare Likelihood-DOP, Simplicity-DOP and\nseveral instantiations of SL-DOP and LS-DOP.\n                                                \n2 One might argue that a more straightforward metric of simplicity would return the parse tree with the\nsmallest number of nodes (rather than the smallest number of treebank-subtrees). But such a metric is\nknown to perform quite badly (see Manning & Schütze, 1999; Bod, 2000a).\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n297\n4.  Computational Issues\nBod (1993) showed how standard chart parsing techniques can be applied to Likelihood-DOP.\nEach treebank-subtree t is converted into a context-free rule r where the lefthand side of r\ncorresponds to the root label of t and the righthand side of r corresponds to the frontier labels\nof t. Indices link the rules to the original subtrees so as to maintain the subtree's internal\nstructure and probability. These rules are used to create a derivation forest for a sentence\n(using a chart parser -- see Charniak, 1993), and the most probable parse is computed by\nsampling a sufficiently large number of random derivations from the forest (\"Monte Carlo\ndisambiguation\", see Bod, 1998). While this technique has been successfully applied to parsing\nthe ATIS portion in the Penn Treebank (Marcus et al. 1993), it is extremely time consuming.\nThis is mainly because the number of random derivations that should be sampled to reliably\nestimate the most probable parse increases exponentially with the sentence length (see\nGoodman, 2002). It is therefore questionable whether Bod's sampling technique can be scaled\nto larger domains such as the Wall Street Journal (WSJ) portion in the Penn Treebank.\n     Goodman (1996) showed how Likelihood-DOP can be reduced to a compact stochastic\ncontext-free grammar (SCFG) which contains exactly eight SCFG rules for each node in the\ntraining set trees. Although Goodman's method does still not allow for an efficient computation\nof the most probable parse (in fact, the problem of computing the most probable parse in\nLikelihood-DOP is NP-hard -- see Sima'an, 1996), his method does allow for an efficient\ncomputation of the \"maximum constituents parse\", i.e. the parse tree that is most likely to have\nthe largest number of correct constituents. Unfortunately, Goodman's SCFG reduction method\nis only beneficial if indeed all subtrees are used, while maximum parse accuracy is usually\nobtained by restricting the subtrees. For example, Bod (2001a) shows that the \"optimal\"\nsubtree set achieving highest parse accuracy on the WSJ is obtained by restricting the\nmaximum number of words in each subtree to 12 and by restricting the maximum depth of\nunlexicalized subtrees to 6. Goodman (2002) shows that some subtree restrictions, such as\nsubtree depth, may be incorporated by his reduction method, but we have found no reduction\nmethod for our optimal subtree set.\n     In this paper we will therefore use Bod's subtree-to-rule conversion method for Likelihood-\nDOP, but we will not use Bod's Monte Carlo sampling technique from derivation forests, as\nthis turned out to be computationally prohibitive. Instead, we will use the well-known Viterbi\noptimization algorithm for chart parsing (cf. Charniak, 1993; Manning & Schütze, 1999) which\nallows for computing the k most probable derivations of an input in cubic time. Using this\nalgorithm, we will estimate the most probable parse tree of an input from the 10,000 most\nprobable derivations, summing up the probabilities of derivations that generate the same tree.\nAlthough this approach does not guarantee that the most probable parse tree is actually found,\nit is shown in Bod (2000a) to perform at least as well as the estimation of the most probable\nparse by Monte Carlo techniques on the ATIS corpus. Moreover, this approach is known to\nobtain significantly higher accuracy than selecting the parse tree generated by the single most\nprobable derivation (Bod, 1998; Goodman, 2002), which we will therefore not consider in this\npaper.\n     For Simplicity-DOP, we also first convert the treebank-subtrees into rewrite rules just as\nwith Likelihood-DOP. Next, the simplest tree, i.e. the shortest derivation, can be efficiently\ncomputed by Viterbi optimization in the same way as the most probable derivation, provided\nthat we assign all rules equal probabilities, in which case the shortest derivation is equal to the\nmost probable derivation. This can be seen as follows: if each rule has a probability p then the\nprobability of a derivation involving n rules is equal to pn, and since 0<p<1 the derivation with\nBOD\n298\nthe fewest rules has the greatest probability. In our experiments in Section 6, we give each\nrule a probability mass equal to 1/R, where R is the number of distinct rules derived by Bod's\nmethod. As mentioned in 3.3, the shortest derivation may not be unique. In that case we\ncompute all shortest derivations of an input and then apply our ranking scheme to these\nderivations. The ranks of the shortest derivations are computed by summing up the ranks of\nthe subtrees they involve. The shortest derivation with the smallest sum of subtree ranks is\ntaken to produce the best parse tree.\n     For SL-DOP and LS-DOP, we compute either n likeliest or n simplest trees by means of\nViterbi optimization. Next, we either select the simplest tree among the n likeliest ones (for\nSL-DOP) or the likeliest tree among the n simplest ones (for LS-DOP). In our experiments, n\nwill never be larger than 1,000.\n5.   The Test Domains\nAs our linguistic test domain we used the Wall Street Journal (WSJ) portion in the Penn\nTreebank (Marcus et al. 1993). This portion contains approx. 50,000 sentences that have been\nmanually annotated with the perceived linguistic tree structures using a predefined set of\nlexico-syntactic labels. Since the WSJ has been extensively used and described in the\nliterature (cf. Manning & Schütze, 1999; Charniak, 2000; Collins, 2000; Bod, 2001a), we will\nnot go into it any further here.\n     As our musical test domain we used the European folksongs in the Essen Folksong\nCollection (Schaffrath, 1995; Huron, 1996), which correspond to approx. 6,200 folksongs that\nhave been manually enriched with their perceived musical grouping structures. The Essen\nFolksong Collection has been previously used by Bod (2001b) and Temperley (2001) to test\ntheir musical parsers. The current paper presents the first experiments with Likelihood-DOP,\nSimplicity-DOP, SL-DOP and LS-DOP on this collection. The Essen folksongs are not\nrepresented by staff notation but are encoded by the Essen Associative Code (ESAC). The\npitch encodings in ESAC resemble \"solfege\": scale degree numbers are used to replace the\nmovable syllables \"do\", \"re\", \"mi\", etc. Thus 1 corresponds to \"do\", 2 corresponds to \"re\", etc.\nChromatic alterations are represented by adding either a \"#\" or a \"b\" after the number. The\nplus (\"+\") and minus (\"-\") signs are added before the number if a note falls resp. above or\nbelow the principle octave (thus -1, 1 and +1 refer al to \"do\", but on different octaves).\nDuration is represented by adding a period or an underscore after the number. A period (\".\")\nincreases duration by 50% and an underscore (\"_\") increases duration by 100% ; more than\none underscore may be added after each number. If a number has no duration indicator, its\nduration corresponds to the smallest value. Thus pitches in ESAC are encoded by integers\nfrom 1 to 7 possibly preceded or followed by symbols for octave, chromatic alteration and\nduration. Each pitch encoding is treated as an atomic symbol, which may be as simple as \"1\"\nor as complex as \"+2#_. \". A pause is represented by 0, possibly followed by duration\nindicators, and is also treated as an atomic symbol. No loudness or timbre indicators are used\nin ESAC.\n     Phrase boundaries are indicated by hard returns in ESAC. The phrases are unlabeled (cf.\nSection 1 of this paper). Yet to make the ESAC annotations readable for our DOP models,\nwe added three basic labels to the phrase structures: the label \"S\" to each whole song, the\nlabel \"P\" to each phrase, and the label \"N\" to each atomic symbol. In this way, we obtained\nconventional tree structures that could directly be employed by our DOP models to parse new\ninput. The use of the label \"N\" distinguishes our annotations from those in previous work (Bod,\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n299\n2001b/c) where we only used labels for song and phrase (\"S\" and \"P\"). The addition of \"N\"\nenhances the productivity and robustness of the musical parsing model, although it also leads\nto a much larger number of subtrees.\n     As an example, assume a very simple melody consisting of two phrases, (1 2) (2 3), then its\ntree structure is given in Figure 7.\nS\nP P\nN N N N\n1 2 2 3\nFigure 7: Example of a musical tree structure consisting of two phrases\nSubtrees that can be extracted from this tree structure include the following:\nS\nP P\nN N\n1\nP\nN N\n2 3\nN\n3\nFigure 8: Some subtrees that can be extracted from the tree in figure 7\nThus the first subtree indicates a phrase starting with a note 1, followed by exactly one other\n(unspecified) note, with the phrase itself followed by exactly one other (unspecified) phrase.\nSuch subtrees can be used to parse new musical input in the same way as has been explained\nfor linguistic parsing in Section 3.\n6.  Experimental Evaluation and Comparison\nTo evaluate our DOP models, we used the blind testing method which randomly divides a\ntreebank into a training set and a test set, where the strings from the test set are parsed by\nmeans of the subtrees from the training set. We applied the standard PARSEVAL metrics of\nprecision  and recall to compare a proposed parse tree P with the corresponding correct test\nset parse tree T as follows (cf. Black et al. 1991):\nPrecision =\n# correct constituents in P\n# constituents in P   \n# correct constituents in P\n# constituents in T\nRecall =\nA constituent in P is \"correct\" if there exists a constituent in T of the same label that spans the\nsame atomic symbols (i.e. words or notes). 3 Since precision and recall can obtain rather\n                                                \n3 The precision and recall scores were computed by using the \" evalb\" program (available via\nhttp://www.cs.nyu.edu/cs/projects/proteus/evalb/)\nBOD\n300\ndifferent results (see Bod, 2001b), they are often balanced by a single measure of\nperformance, known as the F-score (see Manning & Schütze, 1999):\nF-score =\n2 × Precision × Recall\nPrecision + Recall\nFor our experiments, we divided both treebanks (i.e. the WSJ and the Essen Folksong\nCollection) into 10 training/test set splits: 10% of the WSJ was used as test material each time\n(sentences ≤  40 words), while for the Essen Folksong Collection test sets of 1,000 folksongs\nwere used each time. For words in the test set that were unknown in the training set, we\nguessed their categories by using statistics on word-endings, hyphenation and capitalization\n(cf. Bod, 2001a); there were no unknown notes. As in previous work (Bod, 2001a), we limited\nthe maximum size of the subtrees to depth 14, and used random samples of 400,000 subtrees\nfor each depth > 1 and ≤  14.4 Next, we restricted the maximum number of atomic symbols in\neach subtree to 12 and the maximum depth of unlexicalized subtrees to 6. All subtrees were\nsmoothed by the technique described in Bod (1998: 85-94) based on simple Good-Turing\nestimation (Good, 1953).\n     Table 1 shows the mean F-scores obtained by SL-DOP and LS-DOP for language and\nmusic and for various values of n. Recall that for n=1, SL-DOP is equal to Likelihood-DOP\nwhile LS-DOP is equal to Simplicity-DOP.\nn\nSL-DOP LS-DOP\n(simplest among n likeliest) (likeliest among n  simplest)\nLanguage Music Language Music\n1 87.9% 86.0% 85.6% 84.3%\n5 89.3% 86.8% 86.1% 85.5%\n10 90.2% 87.2% 87.0% 85.7%\n11 90.2% 87.3% 87.0% 85.7%\n12 90.2% 87.3% 87.0% 85.7%\n13 90.2% 87.3% 87.0% 85.7%\n14 90.2% 87.2% 87.0% 85.7%\n15 90.2% 87.2% 87.0% 85.7%\n20 90.0% 86.9% 87.1% 85.7%\n50 88.7% 85.6% 87.4% 86.0%\n100 86.8% 84.3% 87.9% 86.0%\n1,000 85.6% 84.3% 87.9% 86.0%\nTable 1: F-scores obtained by SL-DOP and LS-DOP for language and music\n                                                \n4 These random subtree samples were not selected by first exhaustively computing the complete set of\nsubtrees (this was computationally prohibitive). Instead, for each particular depth > 1 we sampled\nsubtrees by randomly selecting a node in a random tree from the training set, after which we selected\nrandom expansions from that node until a subtree of the particular depth was obtained. We repeated\nthis procedure 400,000 times for each depth > 1 and ≤  14.\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n301\nThe Table shows that there is an increase in accuracy for both SL-DOP and LS-DOP if the\nvalue of n increases from 1 to 11. But while the accuracy of SL-DOP decreases after n=13\nand converges to Simplicity-DOP (i.e. LS-DOP at n=1), the accuracy of LS-DOP continues\nto increase and converges to Likelihood-DOP (i.e. SL-DOP at n=1). The highest accuracy is\nobtained by SL-DOP at 11 ≤  n ≤  13, for both language and music. Thus SL-DOP outperforms\nboth Likelihood-DOP and Simplicity-DOP, and the selection of the simplest structure out of\nthe top likeliest ones turns out to be a more promising model than the selection of the likeliest\nstructure out of the top simplest ones. According to paired t-testing, the accuracy\nimprovement of SL-DOP at n=11 over SL-DOP at n=1 (when it is equal Likelihood-DOP) is\nstatistically significant for both language (p<.0001) and music (p<.006).\n     It is surprising that SL-DOP reaches highest accuracy at such a small value for n. But it is\neven more surprising that exactly the same model (with the same parameter setting) obtains\nmaximum accuracy for both language and music. This model embodies the idea that the\nperceptual system strives for the simplest structure but in doing so it only searches among a\nfew most probable structures.\n     To compare our results for language with others, we also tested SL-DOP at n=11 on the\nnow standard division of the WSJ, which uses sections 2 to 21 for training (approx. 40,000\nsentences) and section 23 for testing (2416 sentences ≤  100 words) (see e.g. Manning &\nSchütze, 1999; Charniak, 2000; Collins, 2000). On this division, SL-DOP achieved an F-score\nof 90.7% while the best previous models obtained an F-score of 89.7% (Collins, 2000; Bod,\n2001a). In terms of error reduction, SL-DOP improves with 9.6% over these other models. It\nis common to also report the accuracy for sentences ≤  40 words on the WSJ, for which SL-\nDOP obtained an F-score of 91.8%.\n     Our musical results can be compared to Bod (2001b/c), who tested three probabilistic\nparsing models of increasing complexity on the same training/test set splits from the Essen\nFolksong Collection. The best results were obtained with a hybrid DOP- Markov parser:\n80.7% F-score. This is significantly worse than our best result of 87.3% obtained by SL-DOP\non the same splits from the Essen folksongs. This difference may be explained by the fact that\nthe hybrid DOP-Markov parser in Bod (2001b/c) only takes into account context from higher\nnodes in the tree and not from any sister nodes, while the DOP models presented in the\ncurrent paper take any subtree into account of (almost) arbitrary width and depth, thereby\ncovering a larger amount of musical context. Moreover, as mentioned in Section 5, the models\nin Bod (2001b/c) did not use the label \"N\" for notes; instead, a Markov approach was used to\nparse new sequences of notes.\n     It would also be interesting to compare our musical results to the melodic parser of\nTemperley (2001), who uses a system of preference rules similar to Lerdahl and Jackendoff\n(1983), and which is also evaluated on the Essen Folksong Collection. But while we have\ntested on several test sets of 1,000 randomly selected folksongs, Temperley used only one test\nset of 65 folksongs that was moreover cleaned up by eliminating folksongs with irregular\nmeter (Temperley, 2001: 74). It is therefore difficult to compare our results with Temperley's;\nyet, it is noteworthy that Temperley's parser correctly identified 75.5% of the phrase\nboundaries. Although this is lower than the 87.3% obtained by SL-DOP, Temperley's parser is\nnot \"trained\" on previously analyzed examples like our model (though we note that\nTemperley's results were obtained by tuning the optimal phrase length of his parser on the\naverage phrase length of the Essen Folksong Collection).\n     It should perhaps be mentioned that while parsing models trained on treebanks are widely\nused in natural language processing, they are still rather uncommon in musical processing.\nBOD\n302\nMost musical parsing models, including Temperley's, employ a rule-based approach where the\nparsing is based on a combination of low-level rules -- such as \"prefer phrase boundaries at\nlarge intervals\" -- and higher-level rules -- such as \"prefer phrase boundaries at changes of\nharmony\". The low-level rules are usually based on the well-known Gestalt principles of\nproximity and similarity ( Wertheimer, 1923), which prefer phrase boundaries at larger\nintervallic distances. However, in Bod (2001c) we have shown that the Gestalt principles\npredict incorrect phrase boundaries for a number of folksongs, and that higher-level\nphenomena cannot alleviate these incorrect predictions. These folksongs contain a phrase\nboundary which falls just before  or after a large pitch or time interval (which we have called\njump-phrases ) rather than at such intervals -- as would be predicted by the Gestalt principles.\nMoreover, other musical factors, such as melodic parallelism, meter and harmony, predict\nexactly the same incorrect phrase boundaries for these cases (see Bod, 2001b/c for details).\nWe have conjectured that such jump-phrases are inherently memory-based, reflecting idiom-\ndependent pitch contours (cf. Huron, 1996; Snyder, 2000), and that they can be best captured\nby a memory-based model that tries to mimic the musical experience of a listener from a\ncertain culture (Bod, 2001c).\n7.  Discussion and Conclusion\nWe have seen that our combination of simplicity and likelihood is quite rewarding for linguistic\nand musical structuring, suggesting an interesting parallel between the two modalities. Yet, one\nmay question whether a model which massively memorizes and re-uses previously perceived\nstructures has any cognitive plausibility. Although this question is only important if we want to\nclaim cognitive relevance for our model, there appears to be some evidence that people store\nvarious kinds of previously heard fragments, both in language ( Jurafsky, 2002) and music\n(Saffran et al. 2000). But do people store fragments of arbitrary  size, as proposed by DOP?\nIn his overview article, Jurafsky (2002) reports on a large body of psycholinguistic evidence\nshowing that people not only store lexical items and bigrams, but also frequent phrases and\neven whole sentences. For the case of sentences, people not only store idiomatic sentences,\nbut also \"regular\" high-frequency sentences. 5 Thus, at least for language there is some\nevidence that humans store fragments of arbitrary size provided that these fragments have a\ncertain minimal frequency. And this suggests that humans need not always parse new input by\nthe rules of a grammar, but that they can productively re-use previously analyzed fragments.\nYet, there is no evidence that people store all fragments they hear, as suggested by DOP.\nOnly high-frequency fragments seem to be memorized. However, if the human perceptual\nfaculty needs to learn which fragments will be stored, it will initially need to keep track of all\nfragments (with the possibility of forgetting them) otherwise frequencies can never\naccumulate. This results in a model which continuously and incrementally updates its fragment\nmemory given new input -- which is in correspondence with the DOP approach, and also with\nsome other approaches (cf. Daelemans, 1999; Scha et al. 1999; Spiro, 2002). While we\nacknowledge the importance of a rule-based system in acquiring a fragment memory, once a\nsubstantial memory is available it may be more efficient to construct a tree by means of\nalready parsed fragments than constructing it entirely by means of rules. For many cognitive\n                                                \n5 These results are derived from differences in reaction times in sentence recognition where only the\nfrequency of the (whole) test sentences is varied, while all other variables, such as lexical frequency,\nbigram frequency, plausibility, syntactic/semantic complexity, etc., are kept constant.\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n303\nactivities it is advantageous to store results, so that they can immediately be retrieved from\nmemory, rather than computing them each time from scratch. This has been shown, for\nexample, for manual reaches (Rosenbaum et al. 1992), arithmetic operations ( Rickard et al.\n1994), word formation (Baayen et al. 1997), to mention a few. And linguistic and musical\nparsing may be no exception to this.\n     It should be stressed that the experiments reported in this paper are limited in at least two\nrespects. First, our musical test domain is rather restricted. While a wide variety of linguistic\ntreebanks is currently available (see Manning & Schütze, 1999), the number of musical\ntreebanks is extremely limited. There is thus a need for larger and richer annotated musical\ncorpora covering broader domains. The development of such annotated corpora may be time-\nconsuming, but experience from natural language processing has shown that it is worth the\neffort, since corpus-based parsing systems dramatically outperform grammar-based parsing\nsystems. A second limitation of our experiments is that we have only evaluated the parse\nresults rather than the parse process . That is, we have only assessed how accurately our\nmodels can mimic the input-output behavior of a human annotator, without investigating the\nprocess by which an annotator arrived at the perceived structures. It is unlikely that humans\nprocess perceptual input by computing 10,000 most likely derivations using random samples of\n400,000 subtrees – as we did in the current paper. Yet, for many applications it suffices to\nknow the perceived structure rather than the process that led to that structure. And we have\nseen that our combination of simplicity and likelihood predicts the perceived structure with a\nhigh degree of accuracy.\n     There have been other proposals for integrating the principles of simplicity and likelihood in\nhuman perception (see Chater, 1999 for a review). Chater notes that in the context of\nInformation Theory (Shannon, 1948), the principles of simplicity and likelihood are identical. In\nthis context, the simplicity principle is interpreted as minimizing the expected length to encode\na message i, which is − log2 pi bits, and which leads to the same result as maximizing the\nprobability of i. If we used this information-theoretical definition of simplest structure in\nSimplicity-DOP, it would return the same structure as Likelihood-DOP, and no improved\nresults would be obtained by a combination of the two. On the other hand, by defining the\nsimplest structure as the one generated by the smallest number of subtrees, independent of\ntheir probabilities, we created a notion of simplicity which is provably different from the notion\nof most likely structure, and which, combined with Likelihood-DOP, obtained improved results.\n     Another integration of the two principles may be provided by the notion of Minimum\nDescription Length or MDL (cf. Rissanen, 1978). The MDL principle can be viewed as\npreferring the statistical model that allows for the shortest encoding of the training data. The\nrelevant encoding consists of two parts: the first part encodes the model of the data, and the\nsecond part encodes the data in terms of the model (in bit length). MDL is closely related to\nstochastic complexity (Rissanen, 1989) and Kolmogorov complexity (Li and Vitanyi, 1997),\nand has been used in natural language processing for estimating the parameters of a stochastic\ngrammar (e.g. Osborne, 1999). We will leave it as an open research question as to whether\nMDL can be successfully used for estimating the parameters of DOP's subtrees. However,\nsince MDL is known to give asymptotically the same results as maximum likelihood estimation\n(MLE) (Rissanen, 1989), its application to DOP may lead to an unproductive model. This is\nbecause the maximum likelihood estimator will assign the training set trees their empirical\nfrequencies, and assign 0 weight to all other trees (see Bonnema, 2002 for a proof). This\nwould result in a model which can only generate the training data and no other strings.\nJohnson (2002) argues that this may be an overlearning problem rather than a problem with\nBOD\n304\nMLE per se, and that standard methods, such as cross-validation or regularization, would seem\nin principle to be ways to avoid such overlearning. We will leave this issue to future\ninvestigation.\n     The idea of a general underlying model for language and music is not uncontroversial. In\nlinguistics it is usually assumed that humans have a separate language faculty, and Lerdahl and\nJackendoff (1983) have argued for a separate music faculty. This work does not propose that\nthese separate faculties do not exist, but wants to focus on the commonalities rather than on\nthe differences between these faculties, aiming at finding a deeper \"faculty\" which may hold\nfor perception in general. Our hypothesis is that the perceptual system strives for the simplest\nstructure but in doing so it only searches among the likeliest structures.\nAcknowledgements\nThanks to Aline Honingh, Remko Scha, Neta Spiro, Menno van Zaanen and three anonymous\nreviewers for their excellent comments. A preliminary version of this paper was presented as\na keynote talk at the LCG workshop (\"Learning Computational Grammars\", Tübingen, 2001).\nReferences\nBaayen, R. H., Dijkstra, T. & Schreuder, R. (1997). Singular and Plurals in Dutch: Evidence\nfor a Parallel Dual-Route Model. Journal of Memory and Language , 37, 94-117.\nBlack, E., Abney, S., Flickinger, D., Gnadiec, C., Grishman, R., Harrison, P., Hindle, D.,\nIngria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B.\n& Strzalkowski, T. (1991). A Procedure for Quantitatively Comparing the Syntactic\nCoverage of English, In Proceedings DARPA Speech and Natural Language\nWorkshop , Pacific Grove, Morgan Kaufmann.\nBod, R. (1993). Using an Annotated Language Corpus as a Virtual Stochastic Grammar. In\nProceedings AAAI-93 , Menlo Park, Ca.\nBod, R. (1998). Beyond Grammar: An Experience-Based Theory of Language . Stanford:\nCSLI Publications (Lecture notes number 88).\nBod, R. (2000a). Parsing with the Shortest Derivation. In Proceedings COLING-2000 ,\nSaarbrücken, Germany.\nBod, R. (2000b). Combining Semantic and Syntactic Structure for Language Modeling. In\nProceedings ICSLP-2000 , Beijing, China.\nBod, R. (2001a). What is the Minimal Set of Fragments that Achieves Maximal Parse\nAccuracy? In Proceedings ACL'2001 , Toulouse, France.\nBod, R. (2001b). A Memory-Based Model for Music Analysis. In Proceedings  International\nComputer Music Conference  (ICMC'2001), Havana, Cuba.\nBod, R. (2001c). Memory-Based Models of Melodic Analysis: Challenging the Gestalt\nPrinciples. Journal of New Music Research,  31(1), 26-36. ( available at\nhttp://staff.science.uva.nl/~rens/jnmr01.pdf)\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n305\nBod, R., Hay, J. & Jannedy, S. (Eds.) (2002a). Probabilistic Linguistics . Cambridge, The\nMIT Press. (in press)\nBod, R., Scha, R. & Sima'an, K. (Eds.) (2002b). Data-Oriented Parsing . Stanford, CSLI\nPublications. (in press)\nBonnema, R. (2002). Probability Models for DOP. In Bod et al. (2002b).\nBonnema, R., Bod, R. & Scha, R. (1997). A DOP Model for Semantic Interpretation, In\nProceedings ACL/EACL-97 , Madrid, Spain.\nBuffart, H., Leeuwenberg, E. & Restle, F. (1983). Analysis of Ambiguity in Visual Pattern\nCompletion. Journal of Experimental Psychology: Human Perception and\nPerformance . 9, 980-1000.\nCharniak, E. (1993). Statistical Language Learning , Cambridge, The MIT Press.\nCharniak, E. (1997). Statistical Techniques for Natural Language Parsing, AI Magazine ,\nWinter 1997, 32-43.\nCharniak, E. (2000). A Maximum-Entropy-Inspired Parser. In Proceedings ANLP-\nNAACL'2000, Seattle, Washington.\nChater, N. (1999). The Search for Simplicity: A Fundamental Cognitive Principle? The\nQuarterly Journal of Experimental Psychology , 52A(2), 273-302.\nChomsky, N. (1965). Aspects of the Theory of Syntax , Cambridge, The MIT Press.\nCollard, R., Vos, P. & Leeuwenberg, E. (1981). What Melody Tells about Metre in Music.\nZeitschrift für Psychologie . 189, 25-33.\nCollins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing , PhD-\nthesis, University of Pennsylvania, PA.\nCollins, M. (2000). Discriminative Reranking for Natural Language Parsing, In Proceedings\nICML-2000 , Stanford, Ca.\nCollins, M. & Duffy, N. (2002). New Ranking Algorithms for Parsing and Tagging: Kernels\nover Discrete Structures, and the Voted Perceptron. In Proceedings ACL'2002 ,\nPhiladelphia, PA.\nDaelemans, W. (1999). Introduction to Special Issue on Memory-Based Language\nProcessing. Journal of Experimental and Theoretical Artificial Intelligence  11(3),\n287-296.\nDastani, M. (1998). Languages of Perception . ILLC Dissertation Series 1998-05, University\nof Amsterdam.\nDempster, A., Laird, N. & Rubin, D. (1977). Maximum Likelihood from Incomplete Data via\nthe EM Algorithm, Journal of the Royal Statistical Society , 39, 1-38.\nBOD\n306\nDe Pauw, G. (2000). Aspects of Pattern-matching in Data-Oriented Parsing, In Proceedings\nCOLING-2000 , Saarbrücken, Germany.\nEisner, J. (1997). Bilexical Grammars and a Cubic-Time Probabilistic Parser, In Proceedings\nFifth International Workshop on Parsing Technologies , Boston, Mass.\nFrazier, L. (1978). On Comprehending Sentences: Syntactic Parsing Strategies . PhD.\nThesis, University of Connecticut.\nGood, I. (1953). The Population Frequencies of Species and the Estimation of Population\nParameters, Biometrika 40, 237-264.\nGoodman, J. (1996). Efficient Algorithms for Parsing the DOP Model, In  Proceedings\nEmpirical Methods in Natural Language Processing , Philadelphia, PA.\nGoodman, J. (2002). Efficient Parsing of DOP with PCFG-Reductions. In Bod et al. 2002b.\nvon Helmholtz, H. (1910). Treatise on Physiological Optics (Vol. 3), Dover, New York.\nHoffman, D. (1998). Visual Intelligence . New York, Norton & Company, Inc.\nHuron, D. (1996). The Melodic Arch in Western Folksongs. Computing in Musicology  10, 2-\n23.\nJohnson, M. (2002). The DOP Estimation Method is Biased and Inconsistent. Computational\nLinguistics, 28, 71-76.\nJurafsky, D. (2002). Probabilistic Modeling in Psycholinguistics: Comprehension and\nProduction. In Bod et al. 2002a. ( available at http://www.colorado.edu/ling/jurafsky/\nprob.ps)\nKersten, D. (1999). High-level vision as statistical inference. In Gazzaniga , S. (Ed.), The New\nCognitive Neurosciences , Cambridge, The MIT Press.\nLeeuwenberg, E. (1971). A Perceptual Coding Language for Perceptual and Auditory\nPatterns. American Journal of Psychology . 84, 307-349.\nLerdahl, F. & Jackendoff, R. (1983). A Generative Theory of Tonal Music . Cambridge, The\nMIT Press.\nLi, M. & Vitanyi, P. (1997). An Introduction to Kolmogorov Complexity and its\nApplications  (2nd ed.). New York, Springer.\nLonguet-Higgins, H. (1976). Perception of Melodies. Nature 263, 646-653.\nLonguet-Higgins, H. and Lee, C. (1987). The Rhythmic Interpretation of Monophonic Music.\nMental Processes: Studies in Cognitive Science , Cambridge, The MIT Press.\nManning, C. & Schütze, H. (1999). Foundations of Statistical Natural Language\nProcessing . Cambridge, The MIT Press.\nMarcus, M., Santorini, B., & Marcinkiewicz, M. (1993). Building a Large Annotated Corpus\nof English: the Penn Treebank, Computational Linguistics  19(2).\nA UNIFIED MODEL OF STRUCTURAL ORGANIZATION IN LANGUAGE AND MUSIC\n307\nMarr, D. (1982). Vision. San Francisco, Freeman.\nMartin, W., Church, K. & Patil, R. (1987). Preliminary Analysis of a Breadth-first Parsing\nAlgorithm: Theoretical and Experimental Results. In Bolc, L. (Ed.), Natural Language\nParsing Systems , Springer Verlag, Berlin.\nMumford, D. (1999). The dawning of the age of stochasticity. Based on a lecture at the\nAccademia Nazionale dei Lincei. ( available at http://www.dam.brown.edu/people/\nmumford/Papers/Dawning.ps)\nOsborne, M. (1999). Minimal description length-based induction of definite clause grammars\nfor noun phrase identification. In Proceedings EACL Workshop on Computational\nNatural Language Learning . Bergen, Norway.\nPalmer, S. (1977). Hierarchical Structure in Perceptual Representation. Cognitive\nPsychology , 9, 441-474.\nRaphael, C. (1999). Automatic Segmentation of Acoustic Musical Signals Using Hidden\nMarkov Models. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n21(4), 360-370.\nRestle, F. (1970). Theory of Serial Pattern Learning: Structural Trees. Psychological\nReview, 86, 1-24.\nRickard, T., Healy, A. & Bourne Jr., E. (1994). On the cognitive structure of basic arithmetic\nskills: Operation, order and symbol transfer effects. Journal of Experimental\nPsychology: Learning, Memory and Cognition , 20, 1139-1153.\nRissanen, J. (1978). Modeling by the shortest data description. Automatica, 14, 465-471.\nRissanen, J. (1989). Stochastic Complexity in Statistical Inquiry . Series in Computer\nScience - Volume 15. World Scientific, 1989.\nRosenbaum, D., Vaughan, J., Barnes, H. & Jorgensen, M. (1992). Time course of movement\nplanning: Selection of handgrips for object manipulation. Journal of Experimental\nPsychology: Learning, Memory and Cognition , 18, 1058-1073.\nSaffran, J., Loman, M. & Robertson, R. (2000). Infant Memory for Musical Experiences.\nCognition,  77, B16-23.\nScha, R., Bod, R. & Sima'an, K. (1999). Memory-Based Syntactic Analysis. Journal of\nExperimental and Theoretical Artificial Intelligence,  11(3), 409-440.\nSchaffrath, H. (1995). The Essen Folksong Collection in the Humdrum Kern Format. D.\nHuron (ed.). Menlo Park, CA: Center for Computer Assisted Research in the\nHumanities.\nShannon, C. (1948). A Mathematical Theory of Communication. Bell System Technical\nJournal . 27, 379-423, 623-656.\nSima'an, K. (1996). Computational Complexity of Probabilistic Disambiguation by means of\nTree Grammars. In Proceedings COLING-96 , Copenhagen, Denmark.\nBOD\n308\nSimon, H. (1972). Complexity and the Representation of Patterned Sequences as Symbols.\nPsychological Review . 79, 369-382.\nSnyder, B. (2000). Music and Memory . Cambridge, The MIT Press.\nSpiro, N. (2002). Combining Grammar-based and Memory-based Models of Perception of\nTime Signature and Phase. In Anagnostopoulou, C., Ferrand, M. & Smaill, A. (Eds.).\nMusic and Artificial Intelligence , Lecture Notes in Artificial Intelligence, Vol. 2445,\nSpringer-Verlag, 186-197.\nTemperley, D. (2001). The Cognition of Basic Musical Structures . Cambridge, The MIT\nPress.\nWertheimer, M. (1923). Untersuchungen zur Lehre von der Gestalt. Psychologische\nForschung 4, 301-350.\nWundt, W. (1901). Sprachgeschichte  und Sprachpsychologie . Engelmann, Leipzig.",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.7560569047927856
    },
    {
      "name": "Computer science",
      "score": 0.7376145124435425
    },
    {
      "name": "Parsing",
      "score": 0.6328152418136597
    },
    {
      "name": "Structuring",
      "score": 0.6285274028778076
    },
    {
      "name": "Key (lock)",
      "score": 0.5638184547424316
    },
    {
      "name": "Phrase",
      "score": 0.49726346135139465
    },
    {
      "name": "Natural language processing",
      "score": 0.4918479919433594
    },
    {
      "name": "Simplicity",
      "score": 0.4908791184425354
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4757149815559387
    },
    {
      "name": "Language model",
      "score": 0.45179933309555054
    },
    {
      "name": "Phrase structure rules",
      "score": 0.4390341639518738
    },
    {
      "name": "Language structure",
      "score": 0.41450071334838867
    },
    {
      "name": "Linguistics",
      "score": 0.3714500665664673
    },
    {
      "name": "Epistemology",
      "score": 0.130873441696167
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Generative grammar",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ]
}