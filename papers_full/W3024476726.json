{
  "title": "Many-to-Many Voice Transformer Network",
  "url": "https://openalex.org/W3024476726",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5001243214",
      "name": "Hirokazu Kameoka",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5000377034",
      "name": "Wen-Chin Huang",
      "affiliations": [
        "Nagoya University"
      ]
    },
    {
      "id": "https://openalex.org/A5106710403",
      "name": "Kou Tanaka",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5020693766",
      "name": "Takuhiro Kaneko",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5079710814",
      "name": "Nobukatsu Hojo",
      "affiliations": [
        "NTT (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A5078330211",
      "name": "Tomoki Toda",
      "affiliations": [
        "Nagoya University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2898847420",
    "https://openalex.org/W2161727827",
    "https://openalex.org/W2964243274",
    "https://openalex.org/W2963609956",
    "https://openalex.org/W2899882692",
    "https://openalex.org/W2076258817",
    "https://openalex.org/W2156142001",
    "https://openalex.org/W3099078140",
    "https://openalex.org/W2899877258",
    "https://openalex.org/W2157412983",
    "https://openalex.org/W2608338293",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2970006822",
    "https://openalex.org/W2946555236",
    "https://openalex.org/W2748654097",
    "https://openalex.org/W2800289214",
    "https://openalex.org/W1509691205",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2890983311",
    "https://openalex.org/W2972544500",
    "https://openalex.org/W2471520273",
    "https://openalex.org/W2395980997",
    "https://openalex.org/W2093450784",
    "https://openalex.org/W2120605154",
    "https://openalex.org/W2785608393",
    "https://openalex.org/W2788851830",
    "https://openalex.org/W2056852181",
    "https://openalex.org/W2953331651",
    "https://openalex.org/W2148846882",
    "https://openalex.org/W2901997113",
    "https://openalex.org/W2963691546",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2017425464",
    "https://openalex.org/W1965255698",
    "https://openalex.org/W2005438552",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2972970915",
    "https://openalex.org/W3034420534",
    "https://openalex.org/W2941094131",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2739735615",
    "https://openalex.org/W2963782041",
    "https://openalex.org/W95152782",
    "https://openalex.org/W2963975282",
    "https://openalex.org/W2142300631",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2796495654",
    "https://openalex.org/W2928664166",
    "https://openalex.org/W2517513811",
    "https://openalex.org/W2963685250",
    "https://openalex.org/W2123003832",
    "https://openalex.org/W2017742648",
    "https://openalex.org/W2963175743",
    "https://openalex.org/W2749651610",
    "https://openalex.org/W2964281804",
    "https://openalex.org/W2057609679",
    "https://openalex.org/W2949281321",
    "https://openalex.org/W3100696337",
    "https://openalex.org/W3015338123",
    "https://openalex.org/W2963808252",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2105160541",
    "https://openalex.org/W2902070858",
    "https://openalex.org/W3096567388",
    "https://openalex.org/W2947196194",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2972999331",
    "https://openalex.org/W2804998325",
    "https://openalex.org/W3047769339",
    "https://openalex.org/W2532494225"
  ],
  "abstract": "This paper proposes a voice conversion (VC) method based on a sequence-to-sequence (S2S) learning framework, which enables simultaneous conversion of the voice characteristics, pitch contour, and duration of input speech. We previously proposed an S2S-based VC method using a transformer network architecture called the voice transformer network (VTN). The original VTN was designed to learn only a mapping of speech feature sequences from one speaker to another. The main idea we propose is an extension of the original VTN that can simultaneously learn mappings among multiple speakers. This extension called the many-to-many VTN makes it able to fully use available training data collected from multiple speakers by capturing common latent features that can be shared across different speakers. It also allows us to introduce a training loss called the identity mapping loss to ensure that the input feature sequence will remain unchanged when the source and target speaker indices are the same. Using this particular loss for model training has been found to be extremely effective in improving the performance of the model at test time. We conducted speaker identity conversion experiments and found that our model obtained higher sound quality and speaker similarity than baseline methods. We also found that our model, with a slight modification to its architecture, could handle any-to-many conversion tasks reasonably well.",
  "full_text": "arXiv:2005.08445v4  [eess.AS]  6 Nov 2020\n1\nMany-to-Many V oice Transformer Network\nHirokazu Kameoka, W en-Chin Huang, Kou T anaka,\nT akuhiro Kaneko, Nobukatsu Hojo, and T omoki T oda\nAbstract—This paper proposes a voice conversion (VC) method\nbased on a sequence-to-sequence (S2S) learning framework,\nwhich enables simultaneous conversion of the voice charact eris-\ntics, pitch contour , and duration of input speech. W e previo usly\nproposed an S2S-based VC method using a transformer network\narchitecture called the voice transformer network (VTN). T he\noriginal VTN was designed to learn only a mapping of speech\nfeature sequences from one speaker to another . The main idea\nwe propose is an extension of the original VTN that can\nsimultaneously learn mappings among multiple speakers. Th is\nextension called the many-to-many VTN makes it able to fully\nuse available training data collected from multiple speake rs by\ncapturing common latent features that can be shared across\ndifferent speakers. It also allows us to introduce a trainin g\nloss called the identity mapping loss to ensure that the inpu t\nfeature sequence will remain unchanged when the source and\ntarget speaker indices are the same. Using this particular l oss\nfor model training has been found to be extremely effective\nin improving the performance of the model at test time. W e\nconducted speaker identity conversion experiments and fou nd\nthat our model obtained higher sound quality and speaker\nsimilarity than baseline methods. W e also found that our mod el,\nwith a slight modiﬁcation to its architecture, could handle any-\nto-many conversion tasks reasonably well.\nIndex T erms—V oice conversion (VC), sequence-to-sequence\nlearning, attention, transformer network, many-to-many V C.\nI. I N T RO D U CT IO N\nV oice conversion (VC) is a technique to modify the voice\ncharacteristic and speaking style of an input utterance wit hout\nchanging the linguistic content. Potential applications o f VC\ninclude speaker-identity modiﬁcation [1], speaking aids [ 2],\n[3], speech enhancement [4], [5], and accent conversion [6] .\nMany conventional VC methods use parallel utterances of\nsource and target speech to train acoustic models for featur e\nmapping. The training process typically consists of comput ing\nacoustic features from source and target utterances, using\ndynamic time warping (DTW) to align parallel utterances,\nand training the acoustic model that describes the mappings\nbetween the source features and the target features. Exampl es\nof acoustic models include Gaussian mixture models (GMMs)\n[7], [8], partial least square regression [9], dynamic kern el par-\ntial least square regression [10], frequency warping [11], non-\nnegative matrix factorization [12], group sparse represen tation\n[13], fully-connected deep neural networks (DNNs) [14], [1 5],\nand long-short term memory networks (LSTMs) [16], [17],\njust to name a few . Recently, attempts have also been made to\nH. Kameoka, K. T anaka, T . Kaneko and N. Hojo are with\nNTT Communication Science Laboratories, Nippon T elegraph and\nT elephone Corporation, Atsugi, Kanagawa, 243-0198 Japan ( e-mail:\nhirokazu.kameoka.uh@hco.ntt.co.jp) and W .-C. Huang and T . T oda are with\nNagoya University .\nThis work was supported by JSPS KAKENHI 17H01763 and JST CRES T\nGrant Number JPMJCR19A3, Japan.\nformulate non-parallel methods using deep generative mode ls.\nThese include methods based on variational autoencoders\n[18]–[22], generative adversarial networks [23], [24], an d\nﬂow-based models [25]. Interested readers are referred to\nan excellent review article [26] on the recent trends in VC\nresearch.\nAlthough most of the methods mentioned above are success-\nful in converting the local spectral features, they have dif ﬁculty\nin converting suprasegmental features that reﬂect long-te rm\ndependencies, such as the fundamental frequency ( F0) contour,\nduration and rhythm of the input speech. This is because\nacoustic models in these methods can only describe mappings\nbetween local features. However, since these suprasegmen-\ntal features are as important factors as local features that\ncharacterize speaker identities and speaking styles, it wo uld\nbe desirable if these features could also be converted more\nﬂexibly. Although some attempts have been made to achieve\nconversions of F0 and energy contours through their continu-\nous wavelet transform representations [17], [27]–[30], th ey do\nnot address conversions of duration and rhythm. One solutio n\nto overcome this limitation would be to adopt a sequence-to-\nsequence (seq2seq or S2S) model, as it has a powerful ability\nto learn mappings between sequential data of variable lengt hs\nby capturing and using long-range dependencies.\nS2S models [31], [32] have recently been applied with\nnotable success in many tasks such as machine translation,\nautomatic speech recognition (ASR) [32] and text-to-speec h\n(TTS) [33]–[38]. They are composed mainly of two elements:\nan encoder and decoder. The encoder encodes an input se-\nquence to its latent representation in the form of hidden sta te\nvectors whereas the decoder generates an output sequence\naccording to this latent representation. With the original S2S\nmodel, all input sequences are encoded into a single context\nvector of a ﬁxed dimension. One problem with this is that\nthe ability of the model to capture long-range dependencies\ncan be limited especially when input sequences are long. T o\novercome this limitation, a mechanism called attention [39 ]\nhas been introduced, which allows the network to learn where\nto pay attention in the input sequence when producing each\nitem in the output sequence.\nWhile recurrent neural networks (RNNs) have initially been\nused as the default option for designing the encoder and\ndecoder networks in S2S models, recent work has shown that\nconvolutional neural network (CNN)-based architectures a lso\nhave excellent potential for capturing long-term dependen cies\n[40]. Subsequently, yet another type of architecture calle d\nthe transformer has been proposed [41], which uses neither\nconvolution nor recurrent layers in its network, but only th e\nmechanism of attention. In particular, it uses multi-head s elf-\nattention layers to design the two networks. Self-attentio n is a\n2\ntype of attention mechanism, which offers an efﬁcient way to\nrelate different positions of a given sequence. The multi-h ead\nself-attention mechanism splits each vector in a sequence i nto\nsmaller parts and then computes the self-attention over the\nsequence of each part in parallel. Unlike RNNs, both these\narchitectures have the advantage that they are suitable for\nparallel computations using GPUs.\nSeveral VC methods based on S2S models have already\nbeen proposed [42]–[45]. W e also previously proposed VC\nmethods based on S2S models with RNN [46], CNN [47]\nand transformer architectures [48]. In our most recent work\n[48], we proposed a VC method based on a transformer\narchitecture, which we called the voice transformer networ k\n(VTN). Through this work, it transpired that the model train ed\nfrom scratch did not perform as expected when the amount\nof training data was limited. T o address this, we introduced\na TTS pretraining technique to provide a good initializatio n\nfor fast and sample-efﬁcient VC model training with the aid\nof text-speech pair data, thus reducing the parallel data si ze\nrequirement and training time.\nIn this paper, we propose several ideas to make the VTN\nperform well even without TTS pretraining. One limitation\nwith regular S2S models including the VTN is that they can\nonly learn a mapping from one speaker to another. When\nparallel utterances of multiple speakers are available, na ively\npreparing and training a different model independently for\neach speaker pair would be inefﬁcient, since the model for a\nparticular speaker pair fails to use the training data of the other\nspeakers for its training. T o fully utilize available train ing data\nin multiple speakers, the main idea we propose is extending\nthe VTN so that it can simultaneously learn mappings among\nmultiple speakers. W e call this extended version the many-\nto-many VTN. The idea of this many-to-many extension was\ntriggered by our previous studies on non-parallel VC method s\n[49] that showed the signiﬁcant superiority of a many-to-ma ny\nVC method [24] over its one-to-one counterpart [23]. This\nsuggests the possibility that in learning mappings between\na certain speaker pair the data of other speakers may also\nbe useful. However, the effect this extension would have on\nparallel VC tasks based on S2S models was nontrivial. In this\nregard, we consider the present contribution important. W e\nfurther propose several ideas, including the identity mapp ing\nloss, attention windowing, and settings needed to achieve a ny-\nto-many and real-time conversions. W e show that the perfor-\nmance improvement achieved by the many-to-many extension\nowes not only to the fact that it can leverage more training\ndata than the one-to-one counterpart but also to the identit y\nmapping loss, which is only available in the many-to-many\nversion. W e also show that the pre-layer normalization (Pre -\nLN) architecture, proposed in [50], [51], is effective in bo th\nthe original (one-to-one) and many-to-many VTNs.\nII. R E L AT E D WO RK\nSeveral VC methods based on S2S models have already\nbeen proposed, including the ones we proposed previously.\nRegular S2S models usually require large-scale parallel co r-\npora for training. However, collecting parallel utterance s is\noften costly and non-scalable. Therefore, in VC tasks using\nS2S models, one challenge is how to make them work with a\nlimited amount of training data.\nOne solution is to use text labels as auxiliary information\nfor model training. Miyoshi et al. proposed a VC model\ncombining an S2S model and acoustic models for ASR and\nTTS [42]. Zhang et al. proposed an S2S-based VC model\nguided by an ASR system [43]. Subsequently, Zhang et al.\nproposed a shared S2S model for TTS and VC tasks [44].\nRecently, Biadsy et al. proposed an end-to-end VC model\ncalled “Parrotron”, which uses a multitask learning strate gy\nto train the encoder and decoder along with an ASR model\n[45]. Our VTN [48] is another example, which relies on TTS\npretraining using text-speech pair data.\nThe proposed many-to-many VTN differs from the above\nmethods in that it does not rely on ASR or TTS models and\nrequires no text annotations for model training.\nIII. V O ICE TRA N S F O RM E R NE T WO RK\nA. F eature extraction and normalization\nFollowing our previous work [47], in this work, we choose\nto use the mel-cepstral coefﬁcients (MCCs) [52], log F0,\naperiodicity, and voiced/unvoiced indicator of speech as t he\nacoustic features to be converted. Once acoustic features h ave\nsuccessfully been converted, we can use either a convention al\nvocoder or one of the recently developed high-quality neura l\nvocoders [53]–[65] to generate the signals of converted spe ech.\nT o obtain an acoustic feature vector, we follow the same\nprocedure in [47]. Namely, we ﬁrst use WORLD [66] to\nanalyze the spectral envelope, log F0, coded aperiodicity,\nand voiced/unvoiced indicator within each time frame of a\nspeech utterance, then compute I MCCs from the extracted\nspectral envelope, and ﬁnally construct an acoustic featur e\nvector by stacking the MCCs, log F0, coded aperiodicity, and\nvoiced/unvoiced indicator. Each acoustic feature vector t hus\nconsists of I + 3 elements. At training time, we normalize\neach element xi,n (i = 1, . . . , I ) of the MCCs and log F0\nxI+1,n at frame n to xi,n ←(xi,n −µi)/σ i where i, µi, and\nσi denote the feature index, mean, and standard deviation of\nthe i-th feature within all the voiced segments of the training\nsamples of the same speaker.\nAs with our previous work [47], we found it useful to use\na similar trick introduced in previous work [67] to accelera te\nand stabilize training and inference. Namely, we divide the\nacoustic feature sequence obtained above into non-overlap ping\nsegments of equal length r and use the stack of the acoustic\nfeature vectors in each segment as a new feature vector so\nthat the new feature sequence becomes r times shorter than\nthe original feature sequence.\nB. Model\nW e hereafter use X(s) = [x(s)\n1 , . . . , x(s)\nNs ] ∈ RD×Ns and\nX(t) = [ x(t)\n1 , . . . , x(t)\nNt ] ∈ RD×Nt to denote the source\nand target speech feature sequences of non-aligned paralle l\nutterances, where Ns and Nt denote the lengths of the two\nsequences and D denotes the feature dimension. The VTN\n3\nFig. 1. Overall structure of VTN\n Fig. 2. Encoder layers with (a) Pre-LN\nand (b) Post-LN architectures\nFig. 3. Decoder layers with (a) Pre-LN and (b)\nPost-LN architectures\n[48] has an encoder-decoder structure with a transformer\narchitecture [41] that maps X(s) to X(t) (Fig. 1). The encoder\nis expected to extract contextual information from source\nspeech, and the decoder produces the target speech feature\nsequence according to the contextual information the encod er\nhas generated. Unlike RNN- and CNN-based S2S models,\nthe transformer model does not have any sense of the order\nof the elements in a sequence. Thus, the sinusoidal position\nencodings [41], denoted by PNs ∈RD×Ns and PNt ∈RD×Nt ,\nare ﬁrst added to the source and target feature sequences\nto make the model “aware” of the position at which an\neach element in the sequences is located. The source and\ntarget feature sequences are then passed through convoluti onal\nprenets, which we call the source and target prenets, before\nbeing fed into the encoder and decoder. The output ˜Y(s→t)\nfrom the decoder is ﬁnally passed through a convolutional\npostnet before producing the ﬁnal output Y(s→t). The two\nprenets and the postnet, each consisting of three convoluti on\nlayers, are used to capture and express the local dynamics\nin source and target speech and convert input sequences into\nsequences of the same lengths. In the following, we use\n˜X(s) ∈ Rd×Ns and ˜X(t) ∈ Rd×Nt to denote the outputs\nfrom the source and target prenets, respectively, where d is\nthe output channel number of each prenet.\n1) Encoder: The encoder takes ˜X(s) as the input and\nproduces a context vector sequence Z(s) = [z(s)\n1 , . . . , z(s)\nNs ] ∈\nRd×Ns . The encoder consists of L identical layers, each\nof which has self-attention (SA) and position-wise fully-\nconnected feed forward network (FFN) sub-layers. Residual\nconnections and layer normalizations are applied in additi on\nto the two sub-layers.\nMulti-head self-attention sub-layer: By using X ∈Rd×N\nand Y ∈Rd×N to denote the input and output sequences of\nlength N of an SA sub-layer, the process Y = SA(X), by\nwhich Y is produced, is given as\n[Q; K; V] =W1X ∈R3d×N , (1)\nwhere\n\n\n\nQ = [Q1; . . . ; QH ]\nK = [K1; . . . ; KH ]\nV = [V1; . . . ; VH ]\n, (2)\nAh = softmax\n(KT\nhQh\n√\nd\n)\n(h = 1, . . . , H ), (3)\nY = W2[V1A1; . . . ; VH AH ], (4)\nwhere W1 ∈R3d×d and W2 ∈Rd×d are learnable weight\nmatrices, softmax denotes a softmax operation performed\non the ﬁrst axis, H denotes the number of heads, and [; ]\ndenotes vertical concatenation of matrices (or vectors) wi th\ncompatible sizes. Intuitively, this process can be underst ood\nas follows. First, an input vector sequence is converted int o\nthree types of vector sequences with the same shape, which\ncan be metaphorically interpreted as the queries and the key -\nvalue pairs in a hash table. Each of the three vector sequence s\nis further split into H homogeneous vector sequences with\nthe same shape. By using the query and key pair, Eq. (3)\ncomputes a self-attention matrix, whose element measures\nhow contextually similar each pair of vectors is in the given\nsequence X. The splitting into H heads allows us to measure\nself-simiarity in terms of H different types of context. The n-\nth column of VhAh in Eq. (4) can be seen as a new feature\nvector given by activating the value vectors at all the posit ions\nthat are similar to the current position n in terms of context h\nand adding them together. Eq. (4) ﬁnally produces the output\nsequence Y after combining all these feature vector sequences\nusing learnable weights.\nPosition-wise feed forward network sub-layer: By using\nX ∈Rd×N and Y ∈Rd×N again to denote the input and\noutput sequences of length N of an FFN sub-layer, the process\n4\nY = FFN(X), by which Y is produced, is given as\nY = W4φ(W3X + B3) +B4, (5)\nwhere W3 ∈Rd′×d, W4 ∈Rd×d′\nare learnable weight ma-\ntrices, B3 = [b3, . . . , b3] ∈Rd′×N and B4 = [b4, . . . , b4] ∈\nRd×N are bias matrices, each consisting of identical learnable\ncolumn vectors, and φ denotes an elementwise nonlinear\nactivation function such as the rectiﬁed linear unit (ReLU)\nand gated linear unit (GLU) functions.\nLayer normalization sub-layers: Recent work has shown that\nthe location of the layer normalization in the transformer a rchi-\ntecture affects the speed and stability of the training proc ess as\nwell as the performance of the trained model [50], [51]. Whil e\nthe original transformer architecture places layer normal ization\nafter the SA and FFN sub-layers, the architectures presente d\nin the subsequent work [50], [51] are designed to place it\nbefore them, as illustrated in Fig. 2. T o distinguish betwee n\nthese two architectures, we refer to the former and latter as\npost-layer normalization (Post-LN) and Pre-LN architectu res,\nrespectively. W e show later how differently these architec tures\nactually performed in our experiments. Note that when we say\nwe apply layer normalization to an input vector sequence, sa y\nX = [x1, . . . , xN ], we mean applying layer normalization to\nall the vectors x1, . . . , xN treated as mini-batch samples.\nIf we use Xl and Xl+1 to denote the input and output of the\nl-th encoder layer (with the PreLN architecture), the proces s\nXl+1 = Encl(Xl) of the l-the layer is given by\nU = Xl + SA(LayerNorm1(Xl)), (6)\nXl+1 = U + FFN(LayerNorm2(U)), (7)\nwhere LayerNorm1 and LayerNorm2 denote different LN sub-\nlayers. As described above, each layer has learnable parame -\nters in the SA and FFN sub-layers and the two LN sub-layers.\nThe layer implemented as above is particularly attractive i n\nthat it is able to relate all the positions in the entire input\nsequence using only a single layer. This is in contrast to a\nregular convolution layer, which is only able to relate loca l\npositions near each position.\n2) Decoder: The decoder takes Z(s) and ˜X(t) as the in-\nputs and produces a converted feature sequence ˜Y(s→t) =\n[˜y(s→t)\n1 , . . . , ˜y(s→t)\nNt ] ∈ Rd×Nt . Similar to the encoder, the\ndecoder consists of L identical layers, each of which has SA\nand FFN sub-layers, residual connections, and layer normal -\nization sub-layers. In addition to these sub-layers, each l ayer\nhas a multi-head target-to-source attention (TSA) sub-lay er, as\nillustrated in Fig. 3, whose role is to ﬁnd which position in\nthe source feature sequence contextually corresponds to ea ch\nposition in the target feature sequence and convert the cont ext\nvector sequence according to the predicted corresponding\npositions.\nMulti-head target-to-source attention sub-layer: By using\nX ∈Rd×N and Y ∈Rd×N to denote the input and output\nsequences of length N of the TSA sub-layer, the process Y =\nTSA(X, Z), by which Y is produced, is given in the same\nmanner as the SA sub-layer with the only difference being\nthat the key and value pair (K, V) is computed using the\noutput Z from the encoder:\nQ = W5X, (8)\n[K; V] =W6Z, (9)\nwhere\n\n\n\nQ = [Q1; . . . ; QH ]\nK = [K1; . . . ; KH ]\nV = [V1; . . . ; VH ]\n, (10)\nAh = softmax\n(KT\nhQh\n√\nd\n)\n(h = 1, . . . , H ), (11)\nY = W7[V1A1; . . . ; VH AH ], (12)\nwhere W5 ∈ Rd×d, W6 ∈ R2d×d, and W7 ∈ Rd×d are\nlearnable weight matrices. Analogously to the SA sub-layer ,\nEq. (11) is used to compute a TSA matrix using the query and\nkey pair, where the (n, m )-th element indicates the similarity\nbetween the n-th and m-th frames of source and target speech.\nThe peak trajectory of Ah can thus be interpreted as a time\nwarping function that associates the frames of the source\nspeech with those of the target speech. The splitting into\nH heads allows us to measure the simiarity in terms of H\ndifferent types of context. VhAh in Eq. (12) can be thought of\nas a time-warped version of Vh in terms of context h. Eq. (12)\nﬁnally produces the output sequence Y after combining all\nthese time-warped feature sequences using learnable weigh ts.\nAll the other sub-layers are deﬁned in the same way as\nthe encoder. The overall structures of the decoder layers wi th\nthe PreLN and PostLN architectures are depicted in Fig. 3. If\nwe use Xl and Xl+1 to denote the input and output of the\nl-th decoder layer (with the PreLN architecture), the proces s\nXl+1 = Dec(Xl, Z) of the l-th layer is given by\nU1 = Xl + SA(LayerNorm1(Xl)), (13)\nU2 = U1 + TSA(LayerNorm2(U1), Z), (14)\nXl+1 = U2 + FFN(LayerNorm3(U2)). (15)\nNote that each layer has learnable parameters in the SA, FFN,\nand TSA sub-layers and the three LN sub-layers.\n3) Autoregressive structure: Since the target feature se-\nquence X(t) is of course not accessible at test time, we would\nwant to use a feature vector that the decoder has generated as\nthe input to the decoder for the next time step so that feature\nvectors can be generated one-by-one in a recursive manner.\nT o allow the model to behave in this manner, we must ﬁrst\ntake care that the decoder must not be allowed to use future\ninformation about the target feature vectors when producin g\nan output vector at each time step. This can be ensured by\nsimply constraining the convolution layers in the target pr enet\nto be causal and replacing Eq. (3) in all the SA sub-layers in\nthe decoder with\nAh = softmax\n(KT\nhQh\n√\nd + E\n)\n, (16)\nwhere E is a matrix whose (n, n ′)-th element is given by\nen,n ′ =\n{\n0 ( n ≤n′)\n−∞ (n > n ′) , (17)\nso that the predictions for position n can depend only on the\nknown outputs at positions less than n. Second, an all-zero\nvector is appended to the left end of X(t)\nX(t) ←[0, X(t)], (18)\n5\nso that the recursion always begins with the all-zero vector .\nThird, the output sequence Y(s→t) must correspond to a time-\nshifted version of X(t) so that at each time step the decoder\nwill be able to predict the target speech feature vector that is\nlikely to appear at the next time step. T o this end, we include\nan L1 loss\nLmain = 1\nM ∥[Y(s→t)]:, 1:M −[X(t)]:, 2:M+1∥1, (19)\nin the training loss to be minimized, where we have used the\ncolon operator : to specify the range of indices of the elements\nin a matrix we wish to extract. For ease of notation, we use :\nto represent all elements along an axis.\nC. Constraints on Attention Matrix\nSince the alignment path between parallel utterances must\nlie close to the diagonal, the diagonal region in the attenti on\nmatrices in each TSA sub-layer in the decoder should be\ndominant. By imposing such a restriction, the search space\nduring training can be signiﬁcantly reduced, thus signiﬁca ntly\nreducing the training effort. One way to force the attention\nmatrices to be diagonally dominant involves introducing a\ndiagonal attention loss (DAL) [37]:\nLdal = 1\nNMLH\n∑\nl\n∑\nh∥GNs×Nt ⊙Al,h ∥1, (20)\nwhere Al,h denotes the target-to-source attention matrix of the\nh-th head in the TSA sub-layer in the l-th decoder layer, ⊙\ndenotes an elementwise product, and GNs×Nt ∈RNs×Nt is a\nnon-negative weight matrix whose (n, m )-th element wn,m is\ndeﬁned as wn,m = 1−e−(n/N s−m/N t)2/ 2ν 2\n.\nD. T raining loss\nGiven examples of parallel utterances, the total training l oss\nfor the VTN to be minimized is given as\nL= EX(s), X(t) {Lmain + λdalLdal}, (21)\nwhere EX(s), X(t) {·} denotes the sample mean over all the\ntraining examples and λdal ≥0 is a regularization parameter,\nwhich weighs the importance of Ldal relative to Ldec.\nE. Conversion Algorithm\nAt test time, a source speech feature sequence X can be\nconverted via Algorithm 1. Note that in our model, the all-\nzero vector corresponds to the start-of-sequence token. As for\nthe end-of-sequence token, we intentionally did not includ e\nit in the source and target feature sequences. This is becaus e\nwe are assuming a situation where source speech features are\nconstantly coming in and the conversion is performed online .\nIn the following experiments, we set M to a sufﬁciently large\nnumber (speciﬁcally, twice the length N of the source feature\nsequence) and regarded the time m at which the attended time\npoint (i.e., the peak of the attention distribution) ﬁrst re ached\nN as the end-of-utterance.\nOnce Y has been obtained, we adjust the mean and variance\nof the generated feature sequence so that they match the\npretrained mean and variance of the feature vectors of the\ntarget speaker. W e can then generate a time-domain signal\nAlgorithm 1 Default conversion algorithm\nZ ←X, Y ←0\nfor l = 1 to L do\nZ ←Encl(Z)\nend for\nfor m = 1 to M do\nfor l = 1 to L do\nY ←Decl(Y, Z)\nend for\nY ←[0, Y]\nend for\nreturn Y\nFig. 4. Example of TSA matrices predicted using the original VTN with\nL = 4 and H = 4 trained from scratch (without pretraining). Graph of\ncolumn h and row l shows plot of Al,h.\nusing the WORLD vocoder or any recently developed neural\nvocoder.\nHowever, as Fig. 4 shows, it transpired that with the model\ntrained from scratch, the attended time point did not always\nmove forward monotonically and continuously at test time\nand occasionally made a sudden jump to a distant time point,\nresulting in some segments being skipped or repeated, even\nthough the DAL was considered in training. In our previous\nwork [48], we proposed to introduce pretraining techniques\nexploiting auxiliary text labels to improve the behavior an d\nperformance of the conversion algorithm, as mentioned earl ier.\nIn the next section, we present several ideas that can greatl y\nimprove the behavior of the VTN even without pretraining\nusing text labels.\nIV . M A N Y-TO -M A N Y VTN\nA. Many-to-Many Extension\nThe ﬁrst and main idea is a many-to-many extension of\nthe VTN, which uses a single model to enable mappings\namong multiple speakers by allowing the prenets, postnet,\nencoder, and decoder to take source and target speaker indic es\nas additional inputs. The overall structure of the many-to- many\nVTN is shown in Fig. 5.\nLet X(1), . . . , X(K) be examples of the acoustic feature\nsequences of different speakers reading the same sentence.\nGiven a single pair of parallel utterances X(k) and X(k′),\nwhere k and k′ denote the source and target speaker indices\n6\nFig. 5. Structure of many-to-many VTN.\n(integers), the source and target prenets take tuples (X(k), k )\nand (X(k′), k ′) as the inputs and produce modiﬁed feature\nsequences ˜X(k) and ˜X(k′), respectively. The encoder takes\na tuple ( ˜X(k), k ) as the input and produces a context vector\nsequence Z(k). The decoder takes ( ˜X(k′), Z(k), k ′) as the input\nand produces a converted feature sequence ˜Y(k→k′ ). The\npostnet takes ( ˜Y(k→k′ ), k ′) as the input and ﬁnally produces a\nmodiﬁed version Y(k→k′ ) of ˜Y(k→k′ ). Each of the networks\nincorporates the speaker index into its process by modifyin g\nthe input sequence, say X, via\nS = repeat(embed(k)), (22)\nX ←[X; S], (23)\nevery time before feeding X into the SA, FFN, or TSA sub-\nlayers, where embed denotes an operation that retrieves a\ncontinuous vector from an integer input and repeat denotes\nan operation that produces a vector sequence from an input\nvector by simply repeating it along the time axis. Again, we\nappend an all-zero vector to the left end of X(k′)\nX(k′) ←[0, X(k′)]. (24)\nThe loss functions to be minimized given this single trainin g\nexample are given as\nL(k,k ′ )\nmain = 1\nNk′ ∥[Y(k→k′ )]:, 1:Nk′ −[X(k′)]:, 2:Nk′ +1∥1, (25)\nL(k,k ′ )\ndal = 1\nNkNk′ HL\n∑\nh\n∑\nl∥GNk×Nk′ ⊙A(k,k ′ )\nl,h ∥1, (26)\nwhere A(k,k ′)\nl,h denotes the TSA matrix of the h-th head in the\nTSA sub-layer in the l-th decoder layer.\nWith the above model, we can also consider the case in\nwhich k = k′. Minimizing the sum of the above losses\nunder k = k′ encourages the model to let the input feature\nsequence X(k) remain unchanged when k and k′ indicate\nthe same speaker. W e call this loss the identity mapping\nloss (IML). The task of reconstructing an input sequence is\nFig. 6. Example of attention matrices predicted using many- to-many VTN\nwith L = 4 and H = 4 trained from scratch. Graph of column h and row l\nshows plot of Al,h.\nFig. 7. Example of attention matrices predicted using the wi ndowing\ntechnique based on original VTN with L = 4 and H = 4 trained from\nscratch. Graph of column h and row l shows plot of Al,h.\nrelatively easier than that of ﬁnding the mapping from an inp ut\nsequence to a different sequence. Using the IML as the traini ng\nobjective allows the model to concentrate on learning only t he\nautoregressive behaviour of the decoder. This can contribu te\nto making the model training easier. The total training loss\nincluding the IML thus becomes\nL=\n∑\nk,k ′̸=k\nEX(k), X(k′)\n{\nL(k,k ′ )\nall\n}\n+ λiml\n∑\nk\nEX(k)\n{\nL(k,k )\nall\n}\n,\nwhere L(k,k ′)\nall = L(k,k ′ )\nmain + λdalL(k,k ′ )\ndal , (27)\nwhere EX(k), X(k′) {·}and EX(k) {·}denote the sample means\nover all the training examples of parallel utterances of spe akers\nk and k′, and λiml ≥0 is a regularization parameter, which\nweighs the importance of the IML. The signiﬁcant effect of\nthe IML is shown later.\nFig. 6 shows examples of the TSA matrices predicted using\nthe many-to-many VTN from the same test samples used\nin Fig. 4. As these examples show , the predicted attention\nmatrices obtained with the many-to-many VTN exhibit more\nmonotonic and continuous trajectories than those with the\noriginal VTN, demonstrating the impact of the many-to-many\nextension.\n7\nB. Attention W indowing\nW e present another idea that can be used alone or combined\nwith the many-to-many extension to improve the original\nVTN. T o assist the attended point to move forward mono-\ntonically and continuously at test time, we propose to modif y\nAlgorithm 1 by adopting an idea inspired by the technique\ncalled windowing [32]. Speciﬁcally, we limit the paths through\nwhich the attended point is allowed to move by forcing the\nattentions to all the time points distant from the previous\nattended time point to zeros. W e assume the attended time\npoint to be the peak of the attention distribution, given as t he\nmean of all the TSA attention matrices in the TSA sub-layers\nin the decoder. This can be implemented by replacing Eq.\n(11) in the TSA sub-layer in each decoder layer l at the m′-th\niteration of the for-loop for m = 1, . . . , M in the conversion\nprocess with\nˆAl,h = softmax\n(KT\nhQh\n√\nd + F\n)\n(h = 1, . . . , H ), (28)\nwhere the (n, m )-th element fn,m of F is given by\nfn,m =\n\n\n\n\n\n−∞ (m = m′, n = 1, . . . , ˆn −N0)\n−∞ (m = m′, n = ˆn + N1, . . . , N )\n0 (otherwise)\n, (29)\nso that all the elements of the last column of the resulting ˆAl,h\nbecome zero except for the elements from row max(1, ˆn−N0)\nto row min(ˆn + N1, N ). Z denotes the ﬁnal output of the\nencoder, X and Y denote the outputs of the previous and\ncurrent sub-layers in the l-th decoder layer, and ˆn denotes the\nmaximum point of the attention distribution obtained at the\n(m′ −1)-th iteration:\nˆn =\n{\n1 ( m′ = 1)\nargmaxn\n1\nLH\n∑\nl\n∑\nh[ ˆAl,h ]:,m ′−1 (m′ ̸= 1). (30)\nNote that we set N0 and N1 at the nearest integers that\ncorrespond to 160 and 320 ms, respectively. Fig. 7 shows\nexamples of the TSA matrices obtained with this algorithm\nfrom the same test samples used in Fig. 4. As these examples\nshow , this algorithm was found to have a certain effect on\ngenerating reasonably monotonic and continuous attention tra-\njectories even without any modiﬁcations to the model struct ure\nof the original VTN.\nIt should be noted that we can also use the above algorithm\nas well as the algorithm presented in Subsection III-E for\nthe many-to-many VTN, simply by replacing Encl(Z) and\nDecl(Y, Z) with Encl(Z, k ) and Decl(Y, Z, k ′).\nC. Any-to-Many Conversion\nWith both the one-to-one and many-to-many VTNs, the\nsource speaker must be known and speciﬁed at both training\nand test time. However, in some cases we would need to\nhandle any-to-many VC tasks, namely, to convert the voice of\nan arbitrary speaker or an arbitrary speaking style that is n ot\nincluded in the training dataset. Any-to-many VC is attract ive\nin that it allows for input speech of unknown speakers withou t\nthe need for extra processing, such as model retraining and\nadaptation. It may also be useful as a voice normalization\npreprocessor for speaker-independent ASR. Another import ant\nadvantage of the many-to-many extension presented above\nis that it can be modiﬁed to handle any-to-many VC tasks\nby intentionally not allowing the source prenet and encoder\nto take the source speaker index k as inputs. Namely, with\nthis modiﬁed version, the output sequence of each layer in\nthese networks is directly passed to the next layer without\ngoing through Eqs. (22) and (23). W e show later how well\nthis modiﬁed version performs on an any-to-many VC task in\nwhich the source speaker is unseen in the training dataset.\nD. Real-T ime System Settings\nIt is important to be aware of real-time requirements when\nbuilding VC systems. T o let the VTN work in real-time,\nwe need to make two modiﬁcations. First, the source prenet\nand encoder must not use future information, as with the\ntarget prenet, decoder, and postnet during training. This c an\nbe implemented by constraining the convolution layers in th e\nsource prenet to be causal and replacing Eq. (3) with Eq. (16)\nfor all the sub-layers in the encoder. Second, since the spea king\nrate and rhythm of input speech cannot be changed at test time ,\nall the TSA matrices are simply set to identity matrices so th at\nthe speaking rate and rhythm will be kept unchanged.\nV . E X P E RIM E N T S\nA. Experimental Settings\nT o conﬁrm the effectiveness of the ideas proposed in Section\nIV, we conducted objective and subjective evaluation exper i-\nments. W e used the CMU Arctic database [68], which consists\nof recordings of four speakers, clb (female), bdl (male), sl t\n(female), and rms (male), reading the same 1,132 phonetical ly\nbalanced English utterances. W e used all these speakers for\ntraining and evaluation. Hence, there were a total of twelve\ndifferent combinations of source and target speakers. For e ach\nspeaker, the ﬁrst 1,000 and last 32 sentences of the 1,132\nsentences were used for training and evaluation, respectiv ely.\nAll the speech signals were sampled at 16 kHz. As already\ndetailed in Subsection III-A, for each utterance, the spect ral\nenvelope, log F0, coded aperiodicity, and voiced/unvoiced\ninformation were extracted every 8 ms using the WORLD\nanalyzer [66]. 28 mel-cepstral coefﬁcients (MCCs) were the n\nextracted from each spectral envelope using the Speech Pro-\ncessing T oolkit (SPTK) 1 . The reduction factor r was set to 3.\nThus, D = (28 + 3)×3 = 93.\nB. Network Architecture Details\nDropouts with rate 0.1 were applied to the input sequences\nbefore being fed into the source and target prenets and the\npostnet only at training time. For the nonlinear activation\nfunction φ in each FFN sub-layer, we chose to use the GLU\nfunction since it yielded slightly better performance than the\nReLU function. The two prenets and the postnet were each\ndesigned using three 1D dilated convolution layers with ker nel\nsize 5, each followed by a GLU activation function, where\n1 https://github.com/r9y9/pysptk\n8\nweight normalization [69] was applied to each layer. The\nchannel number d was set at 256 for the one-to-one VTN\nand 512 for the many-to-many VTN, respectively. The middle\nchannel number d′ of each FFN sub-layer was set at 512 for\nthe one-to-one VTN and 1024 for the many-to-many VTN,\nrespectively.\nC. Hyperparameter Settings\nλdal and λiml were set at 2000 and 1, respectively. ν was\nset at 0.3 for both the one-to-one and many-to-many VTNs.\nThe L1 norm ∥X∥1 used in Eqs. (19) and (25) were deﬁned\nas a weighted norm ∥X∥1 = ∑ N\nn=1\n1\nr\n∑ r\nj=1\n∑ 31\ni=1 γi|xij,n |,\nwhere x1j,n , . . . , x 28j,n , x29j,n , x30j,n and x31j,n denote the\nentries of X corresponding to the 28 MCCs, log F0, coded\naperiodicity and voiced/unvoiced indicator at time n, and the\nweights were set at γ1 = ··· = γ28 = 1\n28 , γ29 = 1\n10 ,\nγ30 = γ31 = 1\n50 , respectively. L and H were set to 4 and\n4 for the many-to-many version, and 6 and 1 for the one-\nto-one version, respectively. All the networks were traine d\nsimultaneously with random initialization. Adam optimiza tion\n[70] was used for model training where the mini-batch size\nwas 16 and 30,000 iterations were run. W e conﬁgured each\nmini-batch to consist of only utterances of the same source-\ntarget speaker pair. The learning rate and exponential deca y\nrate for the ﬁrst moment for Adam were set to 1. 0 ×10−4 and\n0.9 for the many-to-many version with the PreLN architectur e\nand to 5. 0×10−5 and 0.9 otherwise. All these hyperparameters\nwere tuned on a subset of the A TR Japanese speech database\n[71], which consisted of 503 phonetically balanced sentenc es\nuttered by two male and two female speakers.\nD. Objective P erformance Measures\nThe test dataset consisted of speech samples of each speaker\nreading the same sentences. Thus, the quality of a converted\nfeature sequence could be assessed by comparing it with the\nfeature sequence of the reference utterance.\n1) Mel-Cepstral Distortion: Given two mel-cepstra, we\ncan use the mel-cepstral distortion (MCD) to measure their\ndifference. W e used the average of the MCDs taken along the\nDTW path between converted and reference feature sequences\nas the objective performance measure for each test utteranc e.\nNote that a smaller MCD indicates better performance.\n2) Log F0 Correlation Coefﬁcient: T o evaluate the log\nF0 contour of converted speech, we used the correlation\ncoefﬁcient between the predicted and target log F0 contours\n[72] as the objective performance measure. In the experimen t,\nwe used the average of the correlation coefﬁcients taken ove r\nall the test utterances as the objective performance measur e\nfor log F0 prediction. Thus, the closer it is to 1, the better\nthe performance. W e call this measure the log F0 correlation\ncoefﬁcient (LFC).\n3) Local Duration Ratio: T o evaluate the speaking rate\nand rhythm of converted speech, we used the measure called\nthe local duration ratio (LDR) [47]. LDRs are deﬁned as the\nlocal slopes of the DTW path between converted and reference\nutterances. In the following, we use the mean absolute diffe r-\nence between the LDRs and 1 (in percentage) as the overall\nFig. 8. Architecture of ConvS2S-VC\nmeasure for the LDRs. Thus, the closer it is to zero, the bette r\nthe performance. For example, if the converted speech is 2\ntimes faster than the reference speech, the LDR will be 0.5\neverywhere, and so its mean absolute difference from 1 will\nbe 50%.\nE. Baseline Methods\n1) sprocket: W e chose the open-source VC method called\nsprocket [73] for comparison in our experiments. T o run this\nmethod, we used the source code provided by the author 2.\nNote that this method was used as a baseline in the V oice\nConversion Challenge (VCC) 2018 [74].\n2) RNN-S2S-VC and ConvS2S-VC: T o compare different\ntypes of network architectures, we also tested the RNN-base d\nS2S model [46], inspired by the architecture introduced in a n\nS2S model-based TTS system called T acotron [33], and the\nCNN-based model we presented previously [47]. W e refer to\nthese models as RNN-S2S-VC and ConvS2S-VC, respectively.\nRNN-S2S-VC: Although the original T acotron used mel-\nspectra as the acoustic features, the baseline method was\ndesigned to use the same acoustic features as our method.\nThe architecture was speciﬁcally designed as follows. The\nencoder consisted of a bottleneck fully connected prenet\nfollowed by a stack of 1 ×1 1D GLU convolutions and a bi-\ndirectional LSTM layer. The decoder was an autoregressive\ncontent-based attention network consisting of a bottlenec k\nfully connected prenet followed by a stateful LSTM layer\nproducing the attention query, which was then passed to a\nstack of 2 uni-directional residual LSTM layers, followed b y\na linear projection to generate the features.\nConvS2S-VC: The ConvS2S model we implemented for this\nexperiment consisted of source/target encoders and a decod er,\neach of which had eight 1D GLU dilated convolution layers\nwith kernel size of 5. W e used single-step single-head scale d\ndot-product attention to compute attention distributions from\n2 https://github.com/k2kobayashi/sprocket\n9\nT ABLE I\nPE R F O R M A N C E O F O N E -TO -O N E A N D M A N Y -TO -M A N Y V T N W I T H\nPO S T L N A N D PR E L N A R C H I T E C T U R E S W I T H A N D W I T H O U T WA .\nV ersions W A Measures\nMCD(dB) LFC LDR(%)\none-to-\none\nPostLN – 7.12 0.705 5.75\n✓ 6.96 0.734 5.45\nPreLN – 6.82 0.678 4.48\n✓ 6.66 0.703 3.90\nmany-to-\nmany\nPostLN – 6.47 0.751 3.99\n✓ 6.35 0.761 3.90\nPreLN – 6.28 0.759 4.16\n✓ 6.29 0.777 3.62\nT ABLE II\nPE R F O R M A N C E O F M A N Y -TO -M A N Y V T N T R A I N E D W I T H A N D W I T H O U T\nIM L\nV ersions IML Measures\nMCD(dB) LFC LDR(%)\nmany-to-\nmany\nPostLN – 6.94 0.644 4.12\n✓ 6.35 0.761 3.90\nPreLN – 6.57 0.650 4.12\n✓ 6.28 0.792 2.51\nthe outputs of the source/target encoders. The convolution s\nin the target encoder and decoder were constrained to be\ncausal, as with the target prenet and postnet in the one-to-o ne\nand many-to-many VTNs. A residual connection and weight\nnormalization were applied to each layer in the three networ ks.\nW e designed and implemented many-to-many extensions of\nthe RNN-based and CNN-based models to compare them with\nthe many-to-many VTN.\nF . Objective Evaluations\n1) Ablation Studies: W e conducted ablation studies to con-\nﬁrm the effectiveness of the many-to-many VTN, IML, and\nattention windowing (A W), and compared the performances\nobtained with the PostLN and PreLN architectures. It should\nbe noted that the models trained without the DAL were\nunsuccessful in producing recognizable speech, possibly d ue\nto the limited amount of training data. For this reason, we om it\nthe results obtained when λdal = 0.\nT ab. I lists the average MCDs, LFCs, and LDRs over\nthe test samples obtained with the one-to-one and many-to-\nmany VTNs with the PostLN and PreLN architectures with\nand without A W . Note that all the results for the many-to-\nmany version were obtained with the models trained using\nthe IML. W e observed that the effect of the many-to-many\nVTN was noticeable. Comparisons between with and without\nA W revealed that while the A W showed a certain effect in\nimproving the one-to-one version in terms of all the measure s,\nit was found to be only slightly effective for the many-to-\nmany version. This may imply that the prediction of attentio ns\nby the many-to-many version was already so successful that\nno correction by A W was necessary. As for the PostLN and\nPreLN architectures, the latter performed consistently be tter\nthan the former, especially for the one-to-one version.\nT ab. II shows the average MCDs, LFCs and LDRs over the\ntest samples obtained with the many-to-many VTN trained\nwith and without the IML. Note that all the results listed her e\nare obtained using A W . As these results indicate, the IML had\na signiﬁcant effect on performance improvements in terms of\nthe MCD and LFC measures.\n2) Comparisons with Baseline Methods: T ables III, IV\nand V show the MCDs, LFCs and LDRs obtained with\nthe proposed and baseline methods. It should be noted that\nsprocket was designed to only adjust the mean and variance\nof the log F0 contour of input speech and keep the rhythm\nunchanged. Hence, the performance gains over sprocket in\nterms of the LFC and LDR measures show how well the\ncompeting methods are able to predict the F0 contours and\nrhythms of target speech. As the results indicate, all the S2 S\nmodels performed better than sprocket in terms of the LFC\nand LDR measures, thus demonstrating the ability to properl y\nconvert the prosodic features in speech. They also performe d\nbetter than or comparably to sprocket in terms of the MCD\nmeasure. It is worth noting that the many-to-many extension\nwas found to be signiﬁcantly effective for all the architect ure\ntypes of the S2S models. It is interesting to compare the\nperformance of the many-to-many versions of RNN-S2S-\nVC, ConvS2S-VC and VTN. The many-to-many version of\nConvS2S-VC performed best in terms of the MCD and LFC\nmeasures whereas the many-to-many RNN-S2S-VC performed\nbest in terms of the LDR measure. This may indicate that the\nstrengths of S2S models can vary depending on the type of\narchitecture.\nAs mentioned earlier, one important advantage of the trans-\nformer architecture over its RNN counterpart is that it can b e\ntrained efﬁciently thanks to its parallelizable structure . In fact,\nwhile it took about 30 hours and 50 hours to train the one-to-\none and many-to-many versions of the RNN-S2S-VC model,\nit only took about 3 hours and 5 hours to train these two\nversions of the VTN under the current experimental settings .\nW e implemented all the methods in PyT orch and used a single\nT esla V100 GPU with a 32.0GB memory for training each\nmodel.\n3) P erformance of any-to-many VTN: W e evaluated the\nperformance of the any-to-many VTN described in Subsection\nIV -C under an open-set condition where the speaker of the\ntest utterances is unseen in the training data. W e used the\nutterances of speaker lnh (female) as the test input speech.\nThe results are listed in T ab. VI (a). For comparison, T ab. VI\n(b) lists the results of sprocket performed on the same speak er\npairs under a speaker-dependent closed-set condition. As t hese\nresults indicate, the any-to-many VTN still performed bett er\nthan sprocket, even though sprocket had an advantage in both\nthe training and test conditions.\n4) P erformance with Real-T ime System Setting: W e eval-\nuated the MCDs and LFCs obtained with the many-to-many\nVTN with the real-time system setting described in Subsecti on\nIV -D. The results are shown in T able VII. It is worth noting\nthat it performed only slightly worse than with the default\nsetting despite the restrictions related to the real-time s ystem\nsetting and performed still better than sprocket in terms of the\nMCD and LFC measures.\n5) Impact of training data size: T o evaluate the impact\nof the training data size, we compared the performance of\nthe one-to-one and many-to-many VTNs trained using 1,000,\n10\nT ABLE III\nM CD S (D B) O B TA I N E D W I T H BA S E L I N E A N D P RO P O S E D M E T H O D S\nSpeakers sprocket RNN-S2S ConvS2S VTN\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 6.98 6.80 6.94 6.99 6.40 6.83 6.64\nclb slt 6.34 6.28 6.24 6.48 5.74 6.21 5.97\nrms 6.84 6.41 6.33 6.47 5.88 6.49 6.23\nclb 6.44 6.33 6.20 6.61 5.79 6.17 6.03\nbdl slt 6.46 6.49 6.24 6.68 5.92 6.45 6.13\nrms 7.24 6.53 6.28 6.76 6.09 6.80 6.34\nclb 6.21 6.20 6.21 6.41 5.69 6.20 5.91\nslt bdl 6.80 7.06 7.18 7.16 6.33 7.20 6.77\nrms 6.87 6.40 6.44 6.76 5.97 6.73 6.26\nclb 6.43 6.36 6.26 6.38 5.88 6.63 5.94\nrms bdl 7.40 7.07 7.13 7.40 6.56 7.51 6.74\nslt 6.76 6.47 6.29 6.71 6.01 6.75 6.21\nAll pairs 6.73 6.50 6.39 6.74 6.03 6.66 6.29\nT ABLE IV\nL F CS O B TA I N E D W I T H BA S E L I N E A N D P RO P O S E D M E T H O D S\nSpeakers sprocket RNN-S2S ConvS2S VTN\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 0.643 0.822 0.851 0.792 0.848 0.791 0.790\nclb slt 0.790 0.811 0.837 0.846 0.891 0.787 0.835\nrms 0.556 0.749 0.796 0.719 0.808 0.643 0.747\nclb 0.642 0.738 0.809 0.752 0.828 0.785 0.767\nbdl slt 0.632 0.768 0.837 0.808 0.871 0.703 0.784\nrms 0.467 0.716 0.716 0.732 0.801 0.595 0.759\nclb 0.820 0.748 0.774 0.795 0.849 0.750 0.772\nslt bdl 0.663 0.766 0.813 0.729 0.835 0.704 0.775\nrms 0.611 0.713 0.786 0.712 0.747 0.660 0.730\nclb 0.632 0.785 0.815 0.809 0.829 0.632 0.717\nrms bdl 0.648 0.816 0.833 0.799 0.814 0.689 0.810\nslt 0.674 0.777 0.805 0.811 0.804 0.715 0.748\nAll pairs 0.653 0.775 0.808 0.785 0.826 0.703 0.777\nT ABLE V\nL D R D E V I AT I O N S (%) O B TA I N E D W I T H BA S E L I N E A N D P RO P O S E D M E T H O D S\nSpeakers sprocket RNN-S2S ConvS2S VTN\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 17.66 3.52 3.22 3.04 3.34 3.33 2.34\nclb slt 9.74 2.34 2.70 0.86 4.38 3.90 4.28\nrms 3.24 2.70 3.76 4.18 6.79 5.57 2.82\nclb 16.65 3.05 3.53 4.55 2.46 2.66 4.27\nbdl slt 4.58 4.18 4.21 6.10 3.52 3.77 3.04\nrms 15.20 2.90 2.21 3.78 3.42 2.69 3.00\nclb 9.25 2.89 3.41 4.23 2.32 3.74 3.29\nslt bdl 5.52 2.35 2.04 3.47 3.72 3.69 3.03\nrms 11.46 3.06 5.00 3.66 5.57 6.88 5.41\nclb 2.84 4.50 4.17 3.91 1.54 4.28 3.35\nrms bdl 17.76 4.68 3.19 3.41 4.15 3.31 3.40\nslt 11.95 4.74 3.74 3.61 4.48 4.28 4.27\nAll pairs 10.60 3.30 3.38 3.77 3.69 3.90 3.62\n500, and 250 utterances for each speaker, respectively. The se\nresults are shown in T ables VIII, IX, and X. W e can conﬁrm\nthat the performance of both versions degrades as expected i n\nterms of all the measures as the training data size decreases .\nMore importantly, we can see that the many-to-many VTN\ntrained using only 500 utterances for each speaker performe d\ncomparably or slightly better than the one-to-one VTN and\nsprocket trained using 1,000 utterances for each speaker. T his\nimplies the fact that in training the mapping between a certa in\nspeaker pair using 500 ×2 utterances, 500 ×2 utterances of\nthe remaining two speakers are as valuable as another 500 ×2\nutterances of that speaker pair, if leveraged efﬁciently.\nG. Subjective Listening T ests\nW e conducted subjective listening tests to compare the\nsound quality and speaker similarity of the converted speec h\nsamples obtained with the proposed and baseline methods. Fo r\nthese tests, we used 32 speech samples generated by each\nmethod for each source-target speaker pair.\nWith the sound quality test, we evaluated the mean opin-\nion score (MOS) for each speech sample. In this test, we\nincluded the speech samples synthesized in the same manner\n11\nFig. 9. Sound quality scores averaged across all speaker, in tra-gender, and inter-gender pairs, respectively (from le ft to right).\nFig. 10. Speaker similarity scores averaged across all spea ker, intra-gender, and inter-gender pairs, respectively ( from left to right).\nT ABLE VI\nPE R F O R M A N C E O F M A N Y -TO -M A N Y V T N W I T H A N Y -TO -M A N Y S E T T I N G\nU N D E R O P E N -S E T C O N D I T I O N A N D S P RO C K E T U N D E R C L O S E D -S E T\nC O N D I T I O N T E S T E D O N S A M E S A M P L E S\n(a) any-to-many VTN\nSpeaker pair Measures\nsource target MCD(dB) LFC LDR(%)\nlnh\nclb 6.49 0.690 2.18\nbdl 7.24 0.636 4.44\nslt 6.59 0.693 4.40\nrms 6.87 0.466 8.65\nAll pairs 6.71 0.630 4.41\n(b) sprocket\nSpeaker pair Measures\nsource target MCD(dB) LFC LDR(%)\nlnh\nclb 6.76 0.716 6.61\nbdl 8.26 0.523 13.38\nslt 6.62 0.771 5.72\nrms 7.22 0.480 4.87\nAll pairs 7.21 0.579 7.61\nT ABLE VII\nPE R F O R M A N C E O F M A N Y -TO -M A N Y V T N W I T H R E A L -T I M E S Y S T E M\nS E T T I N G\nSpeaker pair Measures\nsource target MCD(dB) LFC\nclb\nbdl 7.27 0.735\nslt 6.13 0.791\nrms 6.75 0.693\nbdl\nclb 6.36 0.685\nslt 6.61 0.715\nrms 6.61 0.660\nslt\nclb 6.12 0.743\nbdl 7.10 0.673\nrms 6.55 0.609\nrms\nclb 6.06 0.737\nbdl 7.22 0.612\nslt 6.60 0.730\nAll pairs 6.58 0.703\nas the proposed and baseline methods (namely, the WORLD\nsynthesizer) using the acoustic features directly extract ed\nfrom real speech samples. W e also included speech samples\nproduced using the one-to-one and many-to-many versions of\nRNN-S2S-VC, ConvS2S-VC, and VTN and sprocket in the\nstimuli. T wenty listeners participated in our listening te sts.\nEach listener was asked to evaluate the naturalness by selec ting\n5: Excellent, 4: Good, 3: Fair, 2: Poor, or 1: Bad for each\nutterance. The scores averaged across all, intra-gender, a nd\ninter-gender pairs are shown in Fig. 9. The one-to-one VTN\nperformed better than sprocket and the one-to-one versions\nof the other S2S models. W e also conﬁrmed that the many-\nto-many extension had a signiﬁcant effect in improving the\naudio quality of all the S2S models. It is worth noting that th e\nmany-to-many VTN performed better than all the competing\nmethods including the many-to-many version of ConvS2S-\nVC, even though the many-to-many version of ConvS2S-VC\nwas found to outperform the many-to-many VTN in terms of\nthe MCD and LFC measures through the objective evaluation\nexperiments, as reported earlier. According to the two-sid ed\nMann-Whitney test performed on the MOS scores of the\nmany-to-many VTN and each of the remaining methods, the\np-values for all the pairs except for the many-to-many VTN\nand many-to-many ConvS2S-VC pair were less than 0.05,\nindicating that the many-to-many VTN performed signiﬁcant ly\nbetter than all the competing methods except the many-to-\nmany ConvS2S-VC in terms of sound quality.\nWith the speaker similarity test, each listener was given\na converted speech sample and a real speech sample of the\ncorresponding target speaker and was asked to evaluate how\nlikely they are to be produced by the same speaker by selectin g\n5: Deﬁnitely, 4: Likely, 3: Fairly likely, 2: Not very likely , or 1:\nUnlikely. W e used converted speech samples generated by the\none-to-one and many-to-many versions of RNN-S2S-VC and\nConvS2S-VC, and sprocket for comparison as with the sound\nquality test. The scores averaged across all, intra-gender , and\ninter-gender pairs are shown in Fig. 10. The many-to-many\nversions of ConvS2S-VC and VTN performed comparably and\nslightly better than all other methods. According to the two -\nsided Mann-Whitney test, the many-to-many VTN was found\nto perform signiﬁcantly better than the one-to-one VTN, one -\nto-one ConvS2S-VC, one-to-one RNN-S2S-VC, and sprocket\nin terms of speaker similarity.\n12\nT ABLE VIII\nM CD S (D B) O B TA I N E D W I T H O N E -TO -O N E A N D M A N Y -TO -M A N Y V T N S U N D E R D I FFE R E N T T R A I N I N G DATA S I Z E C O N D I T I O N S\nSpeakers 1000 training utterances 500 training utterances 250 training utterances\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 6.83 6.64 7.04 7.03 7.49 7.55\nclb slt 6.21 5.97 6.44 6.27 6.79 6.79\nrms 6.49 6.23 6.72 6.56 7.22 7.02\nclb 6.17 6.03 6.53 6.41 7.37 6.80\nbdl slt 6.45 6.13 6.72 6.48 7.19 6.97\nrms 6.80 6.34 7.24 6.71 7.67 7.40\nclb 6.20 5.91 6.36 6.28 6.92 6.67\nslt bdl 7.20 6.77 7.21 7.24 7.76 7.82\nrms 6.73 6.26 6.94 6.73 7.67 7.24\nclb 6.63 5.94 6.86 6.40 7.36 6.97\nrms bdl 7.51 6.74 7.39 7.13 8.11 7.83\nslt 6.75 6.21 7.23 6.45 7.65 7.04\nAll pairs 6.66 6.29 6.89 6.64 7.47 7.06\nT ABLE IX\nL F CS O B TA I N E D W I T H O N E -TO -O N E A N D M A N Y -TO -M A N Y V T N S U N D E R D I FFE R E N T T R A I N I N G DATA S I Z E C O N D I T I O N S\nSpeakers 1000 training utterances 500 training utterances 250 training utterances\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 0.791 0.790 0.758 0.689 0.709 0.676\nclb slt 0.787 0.835 0.801 0.827 0.815 0.803\nrms 0.643 0.747 0.693 0.675 0.631 0.574\nclb 0.785 0.767 0.797 0.725 0.721 0.719\nbdl slt 0.703 0.784 0.756 0.818 0.759 0.769\nrms 0.595 0.759 0.647 0.724 0.573 0.503\nclb 0.750 0.772 0.804 0.771 0.746 0.720\nslt bdl 0.704 0.775 0.715 0.665 0.578 0.639\nrms 0.660 0.730 0.658 0.655 0.567 0.533\nclb 0.632 0.717 0.682 0.704 0.591 0.673\nrms bdl 0.689 0.810 0.754 0.628 0.637 0.513\nslt 0.715 0.748 0.630 0.765 0.666 0.73\nAll pairs 0.703 0.777 0.717 0.723 0.668 0.688\nT ABLE X\nL D R D E V I AT I O N S (%) O B TA I N E D W I T H O N E -TO -O N E A N D M A N Y -TO -M A N Y V T N S U N D E R D I FFE R E N T T R A I N I N G DATA S I Z E C O N D I T I O N S\nSpeakers 1000 training utterances 500 training utterances 250 training utterances\nsource target one-to-one many-to-many one-to-one many-to-many one-to-one many-to-many\nbdl 3.33 2.34 12.10 8.47 9.03 12.30\nclb slt 3.90 4.28 4.87 2.66 4.71 4.16\nrms 5.57 2.82 3.22 3.73 4.24 7.66\nclb 2.66 4.27 7.25 3.67 4.69 2.61\nbdl slt 3.77 3.04 8.19 3.34 8.29 3.06\nrms 2.69 3.00 9.44 5.61 8.12 1.80\nclb 3.74 3.29 3.05 3.01 3.37 3.36\nslt bdl 3.69 3.03 10.15 8.49 11.51 9.73\nrms 6.88 5.41 5.18 2.26 7.59 3.44\nclb 4.28 3.35 2.94 3.21 5.33 4.47\nrms bdl 3.31 3.40 10.10 10.65 11.99 13.88\nslt 4.28 4.27 6.94 3.41 5.57 3.82\nAll pairs 3.90 3.62 6.35 4.13 6.70 4.63\nAudio samples of the one-to-one and many-to-many VTNs\nare available on the web 3 .\nVI. C O N CL U S IO N\nW e proposed an extension of VTN, which provides the ﬂex-\nibility of handling many-to-many, any-to-many, and real-t ime\nVC tasks without relying on ASR models and text annotations.\nThrough ablation studies, we conﬁrmed the effectiveness of\neach of the proposed ideas. Objective and subjective evalua tion\n3 http://www .kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/vtn/index.html\nexperiments on a speaker identity conversion task showed\nthat the proposed method could perform better than baseline\nmethods.\nAlthough we used the WORLD vocoder for waveform\ngeneration in the above experiments, using a neural vocoder\ninstead could signiﬁcantly improve the quality of the conve rted\nspeech. Rather than simply performing feature mapping and\nthen using a neural vocoder to generate waveforms, we believ e\nthat further improvements could be made by integrating the\nVTN and a neural vocoder into a single model so that the\nwhole model can be trained end-to-end.\n13\nZero-shot VC is a task of converting input speech to the\nvoice or speaking style of an unseen speaker by looking at onl y\na few of his/her utterances [75]. Although in the many-to-ma ny\nVTN, the target voice or speaking style is speciﬁed via a targ et\nspeaker embedding vector, the embedding vector currently\nused for target speaker conditioning is nongeneralizable t o\nunseen speakers. However, as proposed in [75], replacing th e\nembedding vector with one used for speaker veriﬁcation [76]\nmay allow our model to handle zero-shot VC.\nRE F E RE N CE S\n[1] A. Kain and M. W . Macon, “Spectral voice conversion for te xt-to-speech\nsynthesis, ” in Proc. International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 1998, pp. 285–288.\n[2] A. B. Kain, J.-P . Hosom, X. Niu, J. P . van Santen, M. Fried- Oken, and\nJ. Staehely , “Improving the intelligibility of dysarthric speech, ” Speech\nCommunication, vol. 49, no. 9, pp. 743–759, 2007.\n[3] K. Nakamura, T . T oda, H. Saruwatari, and K. Shikano, “Spe aking-\naid systems using GMM-based voice conversion for electrola ryngeal\nspeech, ” Speech Communication , vol. 54, no. 1, pp. 134–146, 2012.\n[4] Z. Inanoglu and S. Y oung, “Data-driven emotion conversi on in spoken\nEnglish, ” Speech Communication , vol. 51, no. 3, pp. 268–283, 2009.\n[5] T . T oda, M. Nakagiri, and K. Shikano, “Statistical voice conversion\ntechniques for body-conducted unvoiced speech enhancemen t, ” IEEE\nTransactions on Audio, Speech, and Language Processing , vol. 20, no. 9,\npp. 2505–2517, 2012.\n[6] D. Felps, H. Bortfeld, and R. Gutierrez-Osuna, “Foreign accent conver-\nsion in computer assisted pronunciation training, ” Speech Communica-\ntion, vol. 51, no. 10, pp. 920–932, 2009.\n[7] Y . Stylianou, O. Capp´ e, and E. Moulines, “Continuous pr obabilistic\ntransform for voice conversion, ” IEEE Trans. SAP , vol. 6, no. 2, pp.\n131–142, 1998.\n[8] T . T oda, A. W . Black, and K. T okuda, “V oice conversion bas ed on\nmaximum-likelihood estimation of spectral parameter traj ectory , ” IEEE\nTransactions on Audio, Speech, and Language Processing , vol. 15, no. 8,\npp. 2222–2235, 2007.\n[9] E. Helander, T . V irtanen, J. Nurminen, and M. Gabbouj, “V oice conver-\nsion using partial least squares regression, ” IEEE Transactions on Audio,\nSpeech, and Language Processing , vol. 18, no. 5, pp. 912–921, 2010.\n[10] E. Helander, H. Silen, T . V irtanen, and M. Gabbouj, “V oi ce conversion\nusing dynamic kernel partial least squares regression, ” IEEE Transac-\ntions on Audio, Speech, and Language Processing , vol. 20, no. 3, pp.\n806–817, 2011.\n[11] X. Tian, S. W . Lee, Z. Wu, E. S. Chng, and H. Li, “ An exempla r-\nbased approach to frequency warping for voice conversion, ” IEEE/ACM\nTransactions on Audio, Speech, and Language Processing , vol. 25,\nno. 10, pp. 1863–1876, 2017.\n[12] R. T akashima, T . T akiguchi, and Y . Ariki, “Exemplar-ba sed voice\nconversion in noisy environment, ” in Proc. IEEE Spoken Language\nT echnology W orkshop (SLT) , 2012, pp. 313–317.\n[13] B. Sisman, H. Li, and K. C. T an, “Sparse representation o f phonetic fea-\ntures for voice conversion with and without parallel data, ” in Proc. IEEE\nAutomatic Speech Recognition and Understanding W orkshop ( ASRU),\n2017, pp. 677–684.\n[14] S. Desai, A. W . Black, B. Y egnanarayana, and K. Prahalla d, “Spectral\nmapping using artiﬁcial neural networks for voice conversi on, ” IEEE\nTransactions on Audio, Speech, and Language Processing , vol. 18, no. 5,\npp. 954–964, 2010.\n[15] S. H. Mohammadi and A. Kain, “V oice conversion using dee p neural\nnetworks with speaker-independent pre-training, ” in Proc. IEEE Spoken\nLanguage T echnology W orkshop (SLT) , 2014, pp. 19–23.\n[16] L. Sun, S. Kang, K. Li, and H. Meng, “V oice conversion usi ng deep\nbidirectional long short-term memory based recurrent neur al networks, ”\nin Proc. International Conference on Acoustics, Speech and Si gnal\nProcessing (ICASSP) , 2015, pp. 4869–4873.\n[17] H. Ming, D. Huang, L. Xie, J. Wu, M. Dong, and H. Li, “Deep\nbidirectional LSTM modeling of timbre and prosody for emoti onal voice\nconversion, ” in Proc. Annual Conference of the International Speech\nCommunication Association (Interspeech) , 2016, pp. 2453–2457.\n[18] C.-C. Hsu, H.-T . Hwang, Y .-C. Wu, Y . Tsao, and H.-M. W ang , “V oice\nconversion from non-parallel corpora using variational au to-encoder, ”\nin Proc. Asia-P aciﬁc Signal and Information Processing Assoc iation\nAnnual Summit and Conference (APSIP A ASC) , 2016, pp. 1–6.\n[19] ——, “V oice conversion from unaligned corpora using var iational au-\ntoencoding W asserstein generative adversarial networks, ” in Proc. An-\nnual Conference of the International Speech Communication Association\n(Interspeech), 2017, pp. 3364–3368.\n[20] Y . Saito, Y . Ijima, K. Nishida, and S. T akamichi, “Non-p arallel voice\nconversion using variational autoencoders conditioned by phonetic\nposteriorgrams and d-vectors, ” in Proc. International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2018, pp. 5274–\n5278.\n[21] H. Kameoka, T . Kaneko, K. T anaka, and N. Hojo, “ACV AE-VC :\nNon-parallel voice conversion with auxiliary classiﬁer va riational au-\ntoencoder, ” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 27, no. 9, pp. 1432–1443, 2019.\n[22] P . L. T obing, Y .-C. Wu, T . Hayashi, K. Kobayashi, and T . T oda,\n“Non-parallel voice conversion with cyclic variational au toencoder, ” in\nProc. Annual Conference of the International Speech Commun ication\nAssociation (Interspeech) , 2019, pp. 674–678.\n[23] T . Kaneko and H. Kameoka, “CycleGAN-VC: Non-parallel v oice con-\nversion using cycle-consistent adversarial networks, ” in Proc. European\nSignal Processing Conference (EUSIPCO) , 2018, pp. 2100–2104.\n[24] H. Kameoka, T . Kaneko, K. T anaka, and N. Hojo, “StarGAN- VC: Non-\nparallel many-to-many voice conversion using star generat ive adversarial\nnetworks, ” in Proc. IEEE Spoken Language T echnology W orkshop (SLT) ,\n2018, pp. 266–273.\n[25] J. Serr` a, S. Pascual, and C. Segura, “Blow: A single-sc ale hy-\nperconditioned ﬂow for non-parallel raw-audio voice conve rsion, ”\narXiv:1906.00794 [cs.LG] , Jun. 2019.\n[26] B. Sisman, J. Y amagishi, S. King, and H. Li, “ An overview of voice con-\nversion and its challenges: From statistical modeling to de ep learning, ”\narXiv:2008.03648 [eess.AS] , 2020.\n[27] G. Sanchez, H. Silen, J. Nurminen, and M. Gabbouj, “Hier archical mod-\neling of F0 contours for voice conversion, ” in Proc. Annual Conference\nof the International Speech Communication Association (In terspeech),\n2014, pp. 2318–2321.\n[28] H. Ming, D. Huang, L. Xie, J. Wu, M. Dong, and H. Li, “Exemp lar-\nbased sparse representation of timbre and prosody for voice conversion, ”\nin Proc. International Conference on Acoustics, Speech and Si gnal\nProcessing (ICASSP) , 2016, pp. 5175–5179.\n[29] Z. Luo, J. Chen, T . T akiguchi, and Y . Ariki, “Emotional v oice conversion\nwith adaptive scales F0 based on wavelet transform using lim ited amount\nof emotional data, ” in Proc. Annual Conference of the International\nSpeech Communication Association (Interspeech) , 2017, p. 3399.3403.\n[30] B. Sisman, M. Zhang, and H. Li, “Group sparse representa tion with\nW aveNet vocoder adaptation for spectrum and prosody conver sion, ”\nIEEE/ACM Transactions on Audio, Speech, and Language Proce ssing,\nvol. 27, no. 6, pp. 1085–1097, 2019.\n[31] I. Sutskever, O. V inyals, and Q. V . Le, “Sequence to sequ ence learning\nwith neural networks, ” in Adv . Neural Information Processing Systems\n(NIPS), 2014, pp. 3104–3112.\n[32] J. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben gio,\n“ Attention-based models for speech recognition, ” in Adv . Neural Infor-\nmation Processing Systems (NIPS) , 2015, pp. 577–585.\n[33] Y . W ang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. W eiss, N . Jaitly ,\nZ. Y ang, Y . Xiao, Z. Chen, S. Bengio, Q. Le, Y . Agiomyrgiannak is,\nR. Clark, and R. A. Saurous, “T acotron: T owards end-to-end s peech\nsynthesis, ” in Proc. Annual Conference of the International Speech\nCommunication Association (Interspeech) , 2017, pp. 4006–4010.\n[34] S. O. Arık, M. Chrzanowski, A. Coates, G. Diamos, A. Gibi ansky ,\nY . Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta, and\nM. Shoeybi, “Deep V oice: Real-time neural text-to-speech, ” in Proc.\nInternational Conference on Machine Learning (ICML) , 2017.\n[35] S. O. Arık, G. Diamos, A. Gibiansky , J. Miller, K. Peng, W . Ping,\nJ. Raiman, and Y . Zhou, “Deep V oice 2: Multi-speaker neural t ext-to-\nspeech, ” in Adv . Neural Information Processing Systems (NIPS) , 2017.\n[36] J. Sotelo, S. Mehri, K. Kumar, J. F . Santos, K. Kastner, A . Courville,\nand Y . Bengio, “Char2W av: End-to-end speech synthesis, ” in Proc.\nInternational Conference on Learning Representations (IC LR), 2017.\n[37] H. T achibana, K. Uenoyama, and S. Aihara, “Efﬁciently t rainable text-\nto-speech system based on deep convolutional networks with guided\nattention, ” in Proc. International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 2018, pp. 4784–4788.\n[38] W . Ping, K. Peng, A. Gibiansky , S. O. Arık, A. Kannan, S. N arang,\nJ. Raiman, and J. Miller, “Deep V oice 3: Scaling text-to-spe ech with\nconvolutional sequence learning, ” in Proc. International Conference on\nLearning Representations (ICLR) , 2018.\n[39] M.-T . Luong, H. Pham, and C. D. Manning, “Effective appr oaches to\nattention-based neural machine translation, ” in Proc. EMNLP , 2015.\n14\n[40] J. Gehring, M. Auli, D. Grangier, D. Y arats, and Y . N. Dau phin, “Con-\nvolutional sequence to sequence learning, ” arXiv:1705.03122 [cs.CL] ,\nMay 2017.\n[41] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jone s, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “ Attention is all you need, ” in Adv . Neural\nInformation Processing Systems (NIPS) , 2017.\n[42] H. Miyoshi, Y . Saito, S. T akamichi, and H. Saruwatari, “ V oice con-\nversion using sequence-to-sequence learning of context po sterior prob-\nabilities, ” in Proc. Annual Conference of the International Speech\nCommunication Association (Interspeech) , 2017, pp. 1268–1272.\n[43] J.-X. Zhang, Z.-H. Ling, L.-J. Liu, Y . Jiang, and L.-R. D ai, “Sequence-\nto-sequence acoustic modeling for voice conversion, ” arXiv:1810.06865\n[cs.SD], Oct. 2018.\n[44] M. Zhang, X. W ang, F . Fang, H. Li, and J. Y amagishi, “Join t training\nframework for text-to-speech and voice conversion using mu lti-source\nT acotron and W aveNet, ” in Proc. Annual Conference of the International\nSpeech Communication Association (Interspeech) , 2019, pp. 1298–1302.\n[45] F . Biadsy , R. J. W eiss, P . J. Moreno, D. Kanevsky , and Y . J ia, “Parrotron:\nAn end-to-end speech-to-speech conversion model and its ap plications\nto hearing-impaired speech and speech separation, ” in Proc. Annual\nConference of the International Speech Communication Asso ciation\n(Interspeech), 2019, pp. 4115–4119.\n[46] K. T anaka, H. Kameoka, T . Kaneko, and N. Hojo, “AttS2S-V C:\nSequence-to-sequence voice conversion with attention and context\npreservation mechanisms, ” in Proc. International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) , 2019, pp. 6805–6809.\n[47] H. Kameoka, K. T anaka, D. Kwa´ sny , T . Kaneko, and N. Hojo ,\n“ConvS2S-VC: Fully convolutional sequence-to-sequence v oice con-\nversion, ” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 28, pp. 1849–1863, 2020.\n[48] W .-C. Huang, T . Hayashi, Y .-C. Wu, H. Kameoka, and T . T od a,\n“V oice transformer network: Sequence-to-sequence voice c onversion\nusing transformer with text-to-speech pretraining, ” in Proc. Annual\nConference of the International Speech Communication Asso ciation\n(Interspeech), 2020.\n[49] H. Kameoka, T . Kaneko, K. T anaka, and N. Hojo, “Non-para llel\nvoice conversion with augmented classiﬁer star generative adversarial\nnetworks, ” arXiv:2008.12604 [eess.AS] , 2020.\n[50] Q. W ang, B. Li, T . Xiao, J. Zhu, C. Li, D. F . W ong, and L. S. C hao,\n“Learning deep transformer models for machine translation , ” in Proc.\nAnnual Meeting of the Association for Computational Lingui stics (ACL) ,\n2019, pp. 1810–1822.\n[51] R. Xiong, Y . Y ang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zh ang,\nY . Lan, L. W ang, and T .-Y . Liu, “On layer normalization in the\ntransformer architecture, ” in Proc. International Conference on Machine\nLearning (ICML) , 2020, pp. 503–512.\n[52] T . Fukada, K. T okuda, T . Kobayashi, and S. Imai, “ An adap tive algorithm\nfor mel-cepstral analysis of speech, ” in Proc. International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , 1992, pp. 137–\n140.\n[53] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. V iny als,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “ W aveNet:\nA generative model for raw audio, ” arXiv:1609.03499 [cs.SD] , Sep.\n2016.\n[54] A. T amamori, T . Hayashi, K. Kobayashi, K. T akeda, and T . T oda,\n“Speaker-dependent W aveNet vocoder, ” in Proc. Annual Conference\nof the International Speech Communication Association (In terspeech),\n2017, pp. 1118–1122.\n[55] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury , N. Cas agrande,\nE. Lockhart, F . Stimberg, A. van den Oord, S. Dieleman, and\nK. Kavukcuoglu, “Efﬁcient neural audio synthesis, ” arXiv:1802.08435\n[cs.SD], Feb. 2018.\n[56] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. So telo,\nA. Courville, and Y . Bengio, “SampleRNN: An unconditional e nd-to-\nend neural audio generation model, ” arXiv:1612.07837 [cs.SD] , Dec.\n2016.\n[57] Z. Jin, A. Finkelstein, G. J. Mysore, and J. Lu, “FFTNet: A real-time\nspeaker-dependent neural vocoder, ” in Proc. International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , 2018, pp. 2251–\n2255.\n[58] A. van den Oord, Y . Li, I. Babuschkin, K. Simonyan, O. V in yals,\nK. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo ,\nF . Stimberg, N. Casagrande, D. Grewe, S. Noury , S. Dieleman, E. Elsen,\nN. Kalchbrenner, H. Zen, A. Graves, H. King, T . W alters, D. Be lov , and\nD. Hassabis, “Parallel W aveNet: Fast high-ﬁdelity speech s ynthesis, ”\narXiv:1711.10433 [cs.LG] , Nov . 2017.\n[59] W . Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave g eneration in\nend-to-end text-to-speech, ” arXiv:1807.07281 [cs.CL] , Feb. 2019.\n[60] R. Prenger, R. V alle, and B. Catanzaro, “W aveGlow: A ﬂow -based\ngenerative network for speech synthesis, ” arXiv:1811.00002 [cs.SD] ,\nOct. 2018.\n[61] S. Kim, S. Lee, J. Song, and S. Y oon, “FloW aveNet: A gener ative ﬂow\nfor raw audio, ” arXiv:1811.02155 [cs.SD] , Nov . 2018.\n[62] X. W ang, S. T akaki, and J. Y amagishi, “Neural source-ﬁl ter-\nbased waveform model for statistical parametric speech syn thesis, ”\narXiv:1810.11946 [eess.AS] , Oct. 2018.\n[63] K. T anaka, T . Kaneko, N. Hojo, and H. Kameoka, “Syntheti c-to-\nnatural speech waveform conversion using cycle-consisten t adversarial\nnetworks, ” in Proc. IEEE Spoken Language T echnology W orkshop (SLT) ,\n2018, pp. 632–639.\n[64] K. Kumar, R. Kumar, T . de Boissiere, L. Gestin, W . Z. T eoh , J. Sotelo,\nA. de Brebisson, Y . Bengio, and A. Courville, “MelGAN: Gener ative\nadversarial networks for conditional waveform synthesis, ” in Adv . Neural\nInformation Processing Systems (NeurIPS) , 2019, pp. 14 910–14 921.\n[65] R. Y amamoto, E. Song, and J.-M. Kim, “Parallel W aveGAN: A fast\nwaveform generation model based on generative adversarial networks\nwith multi-resolution spectrogram, ” in Proc. International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , 2020, pp. 6199–\n6203.\n[66] M. Morise, F . Y okomori, and K. Ozawa, “WORLD: a vocoder- based\nhigh-quality speech synthesis system for real-time applic ations, ” IEICE\nTransactions on Information and Systems , vol. E99-D, no. 7, pp. 1877–\n1884, 2016.\n[67] Y . W ang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. W eiss, N . Jaitly ,\nZ. Y ang, Y . Xiao, Z. Chen, S. Bengio, Q. Le, Y . Agiomyrgiannak is,\nR. Clark, and R. A. Saurous, “T acotron: T owards end-to-end s peech\nsynthesis, ” in Proc. Annual Conference of the International Speech\nCommunication Association (Interspeech) , 2017, pp. 4006–4010.\n[68] J. Kominek and A. W . Black, “The CMU Arctic speech databa ses, ” in\nProc. ISCA Speech Synthesis W orkshop (SSW) , 2004, pp. 223–224.\n[69] T . Salimans and D. P . Kingma, “W eight normalization: A s imple\nreparameterization to accelerate training of deep neural n etworks, ” in\nAdv . Neural Information Processing Systems (NIPS) , 2016, pp. 901–909.\n[70] D. Kingma and J. Ba, “ Adam: A method for stochastic optim ization, ”\nin Proc. International Conference on Learning Representatio ns (ICLR) ,\n2015.\n[71] A. Kurematsu, K. T akeda, Y . Sagisaka, S. Katagiri, H. Ku wabara,\nand K. Shikano, “A TR Japanese speech database as a tool of spe ech\nrecognition and synthesis, ” Speech Communication , vol. 9, no. 4, pp.\n357–363, Aug. 1990.\n[72] D. J. Hermes, “Measuring the perceptual similarity of p itch contours, ”\nJ. Speech Lang. Hear . Res. , vol. 41, no. 1, pp. 73–82, 1998.\n[73] K. Kobayashi and T . T oda, “sprocket: Open-source voice conversion\nsoftware, ” in Proc. Odyssey , 2018, pp. 203–210.\n[74] J. Lorenzo-Trueba, J. Y amagishi, T . T oda, D. Saito, F . V illavicen-\ncio, T . Kinnunen, and Z. Ling, “The voice conversion challen ge\n2018: Promoting development of parallel and nonparallel me thods, ”\narXiv:1804.04262 [eess.AS] , Apr . 2018.\n[75] K. Qian, Y . Zhang, S. Chang, X. Y ang, and M. Hasegawa-Joh nson,\n“AutoVC: Zero-shot voice style transfer with only autoenco der loss, ”\nin Proc. International Conference on Machine Learning (ICML) , 2019,\npp. 5210–5219.\n[76] L. W an, Q. W ang, A. Papir, and I. L. Moreno, “Generalized end-to-\nend loss for speaker veriﬁcation, ” in Proc. International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) , 2018, pp. 4879–\n4883.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7767982482910156
    },
    {
      "name": "Computer science",
      "score": 0.7650330066680908
    },
    {
      "name": "Speech recognition",
      "score": 0.6147118210792542
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5351437330245972
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3770555853843689
    },
    {
      "name": "Engineering",
      "score": 0.09363585710525513
    },
    {
      "name": "Voltage",
      "score": 0.08812659978866577
    },
    {
      "name": "Linguistics",
      "score": 0.06199049949645996
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2251713219",
      "name": "NTT (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I60134161",
      "name": "Nagoya University",
      "country": "JP"
    }
  ],
  "cited_by": 9
}