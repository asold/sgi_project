{
  "title": "Automatic Assignment of Radiology Examination Protocols Using Pre-trained Language Models with Knowledge Distillation.",
  "url": "https://openalex.org/W3082684893",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2108645384",
      "name": "Wilson Lau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129669601",
      "name": "Laura Aaltonen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2014585847",
      "name": "Martin L. Gunn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5002548520",
      "name": "Meliha Yeti≈ügen",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2756043273",
    "https://openalex.org/W3027260829",
    "https://openalex.org/W2754703688",
    "https://openalex.org/W3015264156",
    "https://openalex.org/W2072379823",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2048661607",
    "https://openalex.org/W2422148244",
    "https://openalex.org/W3012070096",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2113124588",
    "https://openalex.org/W2032878939",
    "https://openalex.org/W2970474271",
    "https://openalex.org/W2766934786",
    "https://openalex.org/W2549775784",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2964222566",
    "https://openalex.org/W2083551746",
    "https://openalex.org/W2754799147",
    "https://openalex.org/W2963453233"
  ],
  "abstract": "Selecting radiology examination protocol is a repetitive, and time-consuming process. In this paper, we present a deep learning approach to automatically assign protocols to computed tomography examinations, by pre-training a domain-specific BERT model (BERT(rad)). To handle the high data imbalance across exam protocols, we used a knowledge distillation approach that up-sampled the minority classes through data augmentation. We compared classification performance of the described approach with n-gram models using Support Vector Machine (SVM), Gradient Boosting Machine (GBM), and Random Forest (RF) classifiers, as well as the BERT(base) model. SVM, GBM and RF achieved macro-averaged F1 scores of 0.45, 0.45, and 0.6 while BERT(base) and BERT(rad) achieved 0.61 and 0.63. Knowledge distillation boosted performance on the minority classes and achieved an F1 score of 0.66.",
  "full_text": null,
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.7187249660491943
    },
    {
      "name": "Random forest",
      "score": 0.7034899592399597
    },
    {
      "name": "Computer science",
      "score": 0.6983628273010254
    },
    {
      "name": "Support vector machine",
      "score": 0.6881674528121948
    },
    {
      "name": "Machine learning",
      "score": 0.6731333136558533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6624721884727478
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.6608434319496155
    },
    {
      "name": "Gradient boosting",
      "score": 0.6053719520568848
    },
    {
      "name": "Protocol (science)",
      "score": 0.5747978091239929
    },
    {
      "name": "Process (computing)",
      "score": 0.4534495174884796
    },
    {
      "name": "Macro",
      "score": 0.4395955801010132
    },
    {
      "name": "Natural language processing",
      "score": 0.3255741596221924
    },
    {
      "name": "Medicine",
      "score": 0.13198643922805786
    },
    {
      "name": "Pathology",
      "score": 0.10791394114494324
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}