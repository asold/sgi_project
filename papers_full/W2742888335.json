{
  "title": "Systematic searching for environmental evidence using multiple tools and sources",
  "url": "https://openalex.org/W2742888335",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2065872420",
      "name": "Barbara Livoreil",
      "affiliations": [
        "Fondation Pour la Recherche Sur la Biodiversité"
      ]
    },
    {
      "id": "https://openalex.org/A1821501360",
      "name": "Julie Glanville",
      "affiliations": [
        "Leeds and York Partnership NHS Foundation Trust",
        "University of York"
      ]
    },
    {
      "id": "https://openalex.org/A2237768312",
      "name": "Neal R Haddaway",
      "affiliations": [
        "Stockholm Environment Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2555266817",
      "name": "Helen Bayliss",
      "affiliations": [
        "Bangor University"
      ]
    },
    {
      "id": "https://openalex.org/A2110268768",
      "name": "Alison Bethel",
      "affiliations": [
        "University of Exeter"
      ]
    },
    {
      "id": "https://openalex.org/A2512740662",
      "name": "Frédérique Flamerie de Lachapelle",
      "affiliations": [
        "Sorbonne Université"
      ]
    },
    {
      "id": "https://openalex.org/A2187132021",
      "name": "Shannon Robalino",
      "affiliations": [
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A2068585884",
      "name": "Sini Savilaakso",
      "affiliations": [
        "Center for International Forestry Research"
      ]
    },
    {
      "id": "https://openalex.org/A1987952304",
      "name": "Wen Zhou",
      "affiliations": [
        "Center for International Forestry Research"
      ]
    },
    {
      "id": "https://openalex.org/A2563132375",
      "name": "Gill Petrokofsky",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2428209518",
      "name": "Geoff Frampton",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A2065872420",
      "name": "Barbara Livoreil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1821501360",
      "name": "Julie Glanville",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2237768312",
      "name": "Neal R Haddaway",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2555266817",
      "name": "Helen Bayliss",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110268768",
      "name": "Alison Bethel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2512740662",
      "name": "Frédérique Flamerie de Lachapelle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187132021",
      "name": "Shannon Robalino",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068585884",
      "name": "Sini Savilaakso",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1987952304",
      "name": "Wen Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2563132375",
      "name": "Gill Petrokofsky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2428209518",
      "name": "Geoff Frampton",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1895224385",
    "https://openalex.org/W2170629039",
    "https://openalex.org/W2510793068",
    "https://openalex.org/W1984183559",
    "https://openalex.org/W2140793310",
    "https://openalex.org/W2100684744",
    "https://openalex.org/W1568279238",
    "https://openalex.org/W1933648550",
    "https://openalex.org/W2179263580",
    "https://openalex.org/W2759190001",
    "https://openalex.org/W2094444890",
    "https://openalex.org/W2061959181",
    "https://openalex.org/W2149991176",
    "https://openalex.org/W1179295656",
    "https://openalex.org/W2205558186",
    "https://openalex.org/W2561629699",
    "https://openalex.org/W80391241",
    "https://openalex.org/W1898915641",
    "https://openalex.org/W2344290876",
    "https://openalex.org/W2154778977",
    "https://openalex.org/W122374328",
    "https://openalex.org/W4229492695",
    "https://openalex.org/W2050729455",
    "https://openalex.org/W2163507046",
    "https://openalex.org/W2150559305",
    "https://openalex.org/W1911705068",
    "https://openalex.org/W2343287595",
    "https://openalex.org/W2116277369",
    "https://openalex.org/W2160122277",
    "https://openalex.org/W4253314500",
    "https://openalex.org/W2046121962",
    "https://openalex.org/W2557346035",
    "https://openalex.org/W1850660615",
    "https://openalex.org/W1590137713",
    "https://openalex.org/W2145616864",
    "https://openalex.org/W2416320094",
    "https://openalex.org/W2155946253",
    "https://openalex.org/W2102842081",
    "https://openalex.org/W1994742944",
    "https://openalex.org/W1601914901",
    "https://openalex.org/W2143517884",
    "https://openalex.org/W4320164613",
    "https://openalex.org/W2238738947",
    "https://openalex.org/W983248824",
    "https://openalex.org/W1562463929",
    "https://openalex.org/W2051663551",
    "https://openalex.org/W1490605517",
    "https://openalex.org/W2109326402",
    "https://openalex.org/W1598602811",
    "https://openalex.org/W1496275067"
  ],
  "abstract": "&lt;br/&gt;Background:&lt;br/&gt;&lt;br/&gt;This paper provides guidance about how to plan, prepare, conduct, report, amend or update a systematic search. It aims to contribute to a new version of the Collaboration for Environmental Evidence (CEE) Guidelines for Systematic Reviews in Environmental Management, and the methods we describe are likely to be broadly applicable across a wider range of topics. In evidence synthesis, searches are expected to be repeatable, fit for purpose, with minimum biases, and to collate a maximum number of relevant articles. Failing to include relevant information in an evidence synthesis may lead to inaccurate or skewed conclusions and/or changes in conclusions as soon as the omitted information is added.&lt;br/&gt;&lt;br/&gt;Method:&lt;br/&gt;&lt;br/&gt;The paper takes into account similar documents produced by the Cochrane Collaboration and the Campbell Collaboration, including necessary adjustments for environmental policy and management, and the current version of the CEE Guidelines (version 4.2, 2013). Where possible this guidance is based on evidence from research, and in its absence on expert opinion and experience.&lt;br/&gt;&lt;br/&gt;Results:&lt;br/&gt;&lt;br/&gt;Here we aim to provide guidance on the optimal search structure as the basis on which any evidence synthesis should be built.&lt;br/&gt;&lt;br/&gt;Conclusion:&lt;br/&gt;&lt;br/&gt;It is aimed at all those who intend to conduct systematic evidence synthesis, including reviews and Ph.D. thesis.&lt;br/&gt;",
  "full_text": "Livoreil et al. Environ Evid  (2017) 6:23 \nDOI 10.1186/s13750-017-0099-6\nMETHODOLOGY\nSystematic searching for environmental \nevidence using multiple tools and sources\nBarbara Livoreil1* , Julie Glanville2, Neal R. Haddaway3, Helen Bayliss4†, Alison Bethel5†, \nFrédérique Flamerie de Lachapelle6†, Shannon Robalino7†, Sini Savilaakso8†, Wen Zhou8†, Gill Petrokofsky9† \nand Geoff Frampton10†\nAbstract \nBackground: This paper provides guidance about how to plan, prepare, conduct, report, amend or update a system-\natic search. It aims to contribute to a new version of the Collaboration for Environmental Evidence (CEE) Guidelines for \nSystematic Reviews in Environmental Management, and the methods we describe are likely to be broadly applicable \nacross a wider range of topics. In evidence synthesis, searches are expected to be repeatable, fit for purpose, with \nminimum biases, and to collate a maximum number of relevant articles. Failing to include relevant information in an \nevidence synthesis may lead to inaccurate or skewed conclusions and/or changes in conclusions as soon as the omit-\nted information is added.\nMethod: The paper takes into account similar documents produced by the Cochrane Collaboration and the Camp-\nbell Collaboration, including necessary adjustments for environmental policy and management, and the current ver-\nsion of the CEE Guidelines (version 4.2, 2013). Where possible this guidance is based on evidence from research, and \nin its absence on expert opinion and experience.\nResults: Here we aim to provide guidance on the optimal search structure as the basis on which any evidence syn-\nthesis should be built.\nConclusion: It is aimed at all those who intend to conduct systematic evidence synthesis, including reviews and \nPh.D. thesis.\nKeywords: Search strategy, Search string, Boolean operators, Evidence synthesis, Bibliographic sources, Literature \nreview, Systematic review, Systematic map, Grey literature\n© The Author(s) 2017. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, \nand indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/\npublicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.\nBackground\nIn a systematic review or systematic map (hereafter \nreferred to as “evidence synthesis”) searches are required \nto be transparent and reproducible and minimise biases. \nA key requirement of a project team engaged in evidence \nsynthesis is to try to gather a maximum of the avail -\nable relevant documented bibliographic evidence, herein \ncalled “ articles” , necessary to answer the review ques -\ntion. In this paper we use “article” to refer to any written \ndocument including scientific papers, abstracts, reports, \nbook chapters, other publications, thesis, or internet \npages, etc. Articles may contain more than one study \n(described observation or experience including methods \nand results) or the same study may be reported in more \nthan one article. In a systematic review or map, the unit \nof analysis (especially when conducting a meta-analysis) \nis the study.\nBiases (including those linked to the search itself) \nshould be minimized and/or highlighted as they may \naffect the outputs of the synthesis [7, 11, 21, 36].\nFailing to include relevant information in an evidence \nsynthesis could significantly affect and/or bias its find -\nings. This may also happen over time as new results are \npublished (see section about upgrading and amending).\nOpen Access\nEnvironmental Evidence\n*Correspondence:  barbara.livoreil@fondationbiodiversite.fr \n†Helen Bayliss, Alison Bethel, Frédérique Flamerie de Lachapelle, Shannon \nRobalino, Sini Savilaakso, Wen Zhou, Gill Petrokofsky and Geoff Frampton \ncontributed equally to this work\n1 Foundation for Research ON Biodiversity (FRB), Paris, France\nFull list of author information is available at the end of the article\nPage 2 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nIn practice, it is unlikely that absolutely all of the relevant \nliterature can be identified during an evidence synthesis \nsearch, for several reasons: (1) literature is often searched \nand examined only in those languages known to the pro\n-\nject team; (2) some articles may not be accessible due to \nrestricted access pay walls or confidentiality; (3) others lack \nabstract or have unhelpful titles, which make them diffi\n-\ncult to identify; (4) others may simply not be indexed in a \nsearchable database. Within these, searches conducted for \nevidence synthesis should be as comprehensive as possible, \nand they should be documented so they can be repeated \nand readers can appreciate their strengths and weaknesses. \nReporting any limitations to searches, such as unavoid\n-\nable gaps in coverage (e.g. lack of access to some literature) \nis an important part of the search process, to ensure that \nreaders’ have confidence in the review methods, to allow \nfor complementary searches when possible and to qualify \nthe interpretation of the evidence synthesis findings.\nIn this paper, we outline the steps necessary for plan\n-\nning, conducting and reporting of search activities within \nan evidence synthesis. We aim to contribute to a new \nversion of the Collaboration for Environmental Evidence \n(CEE) Guidelines for Systematic Reviews in Environmen\n-\ntal Management (current version 4.2, March 2013) by \nproviding in-depth information on good practice for this \nstep of evidence synthesis.\nSteps involved in a search are presented in chronologi\n-\ncal order, bearing in mind that some of the process may \nbe iterative. We also highlight the methods that enable \nthe project team to identify, minimise and report any \nrisks of bias that may affect the search and how this can \naffect the findings of an evidence synthesis.\nWe will use the following terminology: search terms \nencompasses individual or compound words used in a \nsearch to find relevant articles. A search string is a com\n-\nbination of comprises search terms combined using \nBoolean operators. Finally, a search strategy is the whole \nsearch methodology, including search terms, search \nstrings, the bibliographic sources searched, and enough \ninformation to ensure the reproducibility of the search. \nBibliographic sources (see “Identifying relevant sources \nof articles” for more details) capture any source of refer\n-\nences, including electronic bibliographic databases, those \nsources which would not be classified as databases (e.g. \nthe Internet via search engines), hand searched journals, \nand personal contacts.\nFlowchart of the steps of a search\nA step-by-step overview of the search process for evi -\ndence synthesis is illustrated in Fig.  1. The entire series \nof steps composing evidence synthesis has been provided \nelsewhere [7].\n1.2\nIdentifyings earcht erms\n1.3\nIdentifyingr elevant \nsources of articles\n1.1\nEstablishing a test-list\nQUESTION FORMULATION\nSEARCHING FOR EVIDENCE\nELIGIBILITY SCREENING\n2.1 \nPrioritizing\nbibliographic sources\n1. PLANNING THE SEARCH 2. CONDUCTING THE SEARCH\n1.5\nAdressing the need for grey\nliterature\n1.6 \nDeciding when to stop\n2.5\nSearching for grey literature\n2.3\nAssessing retrieval\nperformance\n2.4 \nRefining the results\n2.2 \nBuilding the search string\n1.7\nSubmitting the search strategy\nin the protocol for peer-review\n3. MANAGING REFERENCES\nAND REPORTING\n3.1\nKeepingt rack of the search\nand recording results\n1.4 \nChoosingb ibliographic\nmanagement software\n3.2 \nWriting the search report\n4. UPDATING AND \nAMENDING A SEARCH\n2.6 \nAdditional approaches\nFig. 1 Steps of a systematic search grouped into four blocs within the conduct of an evidence synthesis (vertical arrow). Numbers relate to sections \nin the text\nPage 3 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \nPreventing errors and biases\nConducting a rigorous evidence synthesis implies to try \nto minimise risks of errors and biases which may hap -\npen at all stages. Errors that can occur include during \nthe search include: missing search terms, unintentional \nmisspelling of search terms, errors in the search syntax \n(e.g. inappropriate use of Boolean operators, see “Build\n-\ning the search string ”) and inappropriate search terms. \nSuch problems may be minimised when the search term \nidentification process is conducted rigorously, and by \npeer-reviewing the search strategy, including within and \noutside the project team.\nBiases (systematic errors) in the search strategy may \naffect the search outcomes [46]. The methods used to \nminimize bias should be reported in the protocol and \nthe final review or map (see “Part 3”). Minimizing bias \nmay require (1) looking for evidence outside traditional \nacademic electronic bibliographic sources (e.g. grey lit\n-\nerature); (2) using multiple databases and search tools \nto reduce the possibility of bias in the retrieved results; \nand, (3) contacting organisations or individuals who may \nhave relevant material [2]. Some biases have been listed \nin Bayliss and Beyer [2] and a few of them are reported \nhere to be considered by project teams as appropriate: \nlanguage bias [46] means that studies with significant \nor ‘interesting’ results are more likely to be published in \nthe English language and easier to access to than results \npublished in other languages. The impact of this on syn\n-\nthesis outcomes is uncertain (e.g. [25, 37]) but the way to \nreduce the bias is to look beyond the English language lit-\nerature. Prevailing paradigm bias [2] suggests that stud -\nies relating to or supporting the prevailing paradigm or \ntopic (for example climate change) are more likely to be \npublished and hence discoverable. The ways to reduce \nthis bias is not to rely only on finding well known rele\n-\nvant studies. Temporal bias includes the risk that studies \nsupporting a hypothesis are more likely to be published \nfirst [2]. The results may not be supported by later studies \n[28]. Due to the culture of ‘the latest is best’ , older articles \nmay be overlooked and mis-interpretations perpetuated. \nThe ways to reduce this bias include searching older pub\n-\nlications, considering updating the search in the future, \nor test statistically whether this bias significantly affects \nthe results of studies. Publication bias [9, 23, 46] refers to \nasymmetry in the likelihood of publishing results: statis\n-\ntically significant results (positive results) are more likely \nto be accepted for publication than non-significant ones \n(negative results). This has been a source of major con\n-\ncern for systematic reviews and meta-analysis as it might \nlead to overestimating an effect/impact of an Interven\n-\ntion or Exposure on a Population (e.g. [16, 30, 40]). To \nminimise this bias, searches for studies reporting non-\nsignificant results (most probably found in grey literature \nand studies in languages other than English) should be \nconducted in all systematic reviews and maps [29]. Pos\n-\nsible sources of such results are the Journal of Negative \nResults in Ecology and Evolutionary Biology (http://jnr-\neeb.org/index.php/jnr) and the Journal of Non-Significant \nDifferences (https://cirt.gcu.edu/research/publication_\npresentation/gcujournals/nonsignificant). These journals \npublish studies that are scientifically rigorous but lack \nstatistical significance.\nRelationship between searching and scoping\nSearches occur at several points in evidence synthesis. \nFirst, an initial scoping search may be conducted when \npreparing the project. Scoping aims to quickly assess the \nquantity and type of articles that are relevant to the ques\n-\ntion. The scoping search is often conducted only using \none or two electronic bibliographic databases. The scop\n-\ning results may help to estimate the quantity and types \nof articles available, help to plan the human and other \nresources required (e.g. number of team members, librar\n-\nians, translators, statisticians, numbers of documents \nwhich need to be purchased, processed and extracted), \nand determine whether the evidence synthesis question \nshould be refined if resources are insufficient. Second, \nthe full search strategy is developed and presented within \nthe evidence synthesis protocol and possibly reviewed by \na third party. Third, the final search is then carried out \nto find relevant evidence. The current paper explains in \ndetail how to develop the full search strategy.\nStructuring the search with PICO/PECO elements\nAn evidence synthesis process starts with a question that \nis usually structured into “building blocks” (concepts or \nelements), some of which are then used to develop the \nsearch strategy. For the purpose of this paper the search \nstrategy will be illustrated based on PICO/PECO ele\n-\nments which are commonly used in CEE evidence syn -\nthesis (Table 1). Other elements and question structures \nexist and there are some variations in the abbreviations \nused to designate similar things (e.g. PIT, PO, SPIDER, \nSPICE, see review and examples in [11, 13, 24]). Some\n-\ntimes in CEE reviews SICO/SECO have been used \ninstead of PICO/PECO. This is because authors used \n‘subject’ rather than ‘population’ . There is a risk of con\n-\nfusion with the letter “S” being used to describe the Set -\ntings (or context) in the PICO/PECO semantic.\nIn any of these question structures it is possible to nar -\nrow the question (and the search) by adding additional \nsearch terms defining the Context or Setting of the ques\n-\ntion (e.g. “tropical” , “experimental” , or “pleistocene”). \nSearching for geographic location is not recommended \nbecause location names may be difficult to list or dupli\n-\ncate when the geographical range is broad. Geographical \nPage 4 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nelements (e.g. name of the country) may, instead, be more \nefficiently used as eligibility screening criteria [12].\nUse of multiple languages\nIdentifying which languages are most relevant for the \nsearch may depend on the topic of the evidence synthe\n-\nsis. There are two main challenges with languages for an \nevidence synthesis; translating search terms into various \nlanguages to capture as many relevant articles as possible, \nand then being able to select and use the paper when not \nwritten in a language spoken by the project team mem\n-\nbers. In many electronic bibliographic sources, articles \nwritten in languages other than English can be discovered \nusing English search terms. However, a large literature in \nlanguages other than English remains to be discovered in \nnational and regional databases, e.g. JICST for Japanese \nresearch. Searching is likely to require a range of languages \nwhen relevant articles are produced at the national level, \nas much of it will be published in the official language of \nthe nations [8]. Reporting the choice of language(s) in the \nprotocol and in the final synthesis report is important to \nenable repetition and updating when appropriate.\nHuman resources needed for searching\nEach evidence synthesis is conducted by a project team. \nIt may be composed of a project leader and associated \nexperts (thematic and methodological). Because of the \nsystematic aspect of the searching and the need to keep \ncareful track of the findings (see “Part 3”), projects teams \nshould, when possible, include librarians or information \nspecialists. Subject specialist librarians are conversant \nwith bibliographic sources, and are often very famil\n-\niar with the nuances of different transdisciplinary and \nsubject-specific resources [47]. They are aware of the \nbroad range of tools available for undertaking literature \nsearches and they are aware of recent improvements in \nthe range and use of those tools. They are also expert in \ncoverting research questions into search strategies. Such \nexperts can benefit in contributing to a project team \nsince their institutions may require demonstration of col\n-\nlaborative work [22].\nPart 1—planning the search\nThe first step in planning a search is to design a strategy \nto maximise the probability of identifying relevant arti\n-\ncles whilst minimizing the time spent doing so. There are \nseveral aspects of a search strategy detailed in this arti\n-\ncle. Planning may also include discussions about eligibil -\nity criteria for subsequent screening [12] as they are often \nlinked to search terms. Planning should also include dis\n-\ncussions about decision criteria defining when to stop the \nsearch as resource constraints (such as time, manpower, \nskills) may be a major reason to limit the search and \nshould be anticipated and explained in the protocol (see \n“Deciding when to stop”).\nEstablishing a test‑list\nA test-list is a set of articles that have been identified as \nrelevant to answer the question of the evidence synthe -\nsis (e.g. are within the scope and provide some evidence \nto answer the question). The test-list can be created by \nasking experts, researchers and stakeholders (i.e. any\n-\none who has an interest in the review question) for sug -\ngestions and by perusing existing reviews. The project \nteam should read the articles of the test-list to make \nsure they are relevant to the synthesis question. Estab\n-\nlishing a test-list is independent of the search itself and \nis used to help develop the search strategy and to assess \nthe performance of the search strategy. The performance \nof a search strategy should be reported, i.e. whether \nthe search strategy correctly retrieves relevant articles \nand whether all available relevant literature to answer \nthe evidence synthesis question is likely to have been \nidentified (see “ Assessing retrieval performance”). The \ntest-list may be presented in the protocol submitted for \npeer-review.\nThe test-list should ideally cover the range of authors, \njournals, and research projects within the scope of the \nquestion. In order to be an effective tool it needs to \nreflect the range of the evidence likely to be encountered \nin the review. The number of articles to include in the \ntest-list is a case-by-case decision and may also depend \non the breadth of the question. When using a very small \nTable 1 Elements of a reviewable PICO/PECO question, often structured as “does intervention (I) or exposure (E) applied \nto populations (P) produces outcome (O) [compared to comparator (C)]?”\nQuestion element Definition\nPopulation (of subjects) Statistical samples or populations of subject(s) (e.g. ecosystem, species, etc.), to which the intervention will be applied, or \nexposed to described conditions\nIntervention/exposure Policy, action or environmental variable impacting the populations or to which the subject populations are exposed\nComparator What the exposure or intervention are compared to. Either a control with no intervention/exposure or an alternative \nintervention or a counterfactual scenario\nOutcome Consequences of the intervention or exposure. All relevant variables that can be reliably measured\nPage 5 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \ntest-list, the project team may inappropriately conclude \nthat the search is effective whilst it is not. Using the test-\nlist may be an indicator for the project team to improve \nthe search strategy, or to help decide when to stop the \nsearch (see “Deciding when to stop”).\nIdentifying search terms\nA search string that is efficient at finding relevant articles \nmeans that a maximum of relevant papers will have been \nfound and the project team will not have to run the search \nagain during the course of the conduct of the evidence syn\n-\nthesis. Moreover, it may be re-used as such when amend-\ning or updating the search in the future, saving time and \nresources (see “Part 4”). Initial search terms can usually be \ngenerated from the question elements and by looking at \nthe articles in the test-list. However, authors of articles may \nnot always describe the full range of the PICO/PECO cri\n-\nteria in the few words available in the title and abstract. As \na consequence, building search strings from search terms \nrequires project teams to draw upon both their scientific \nexpertise, a certain degree of imagination, and an analysis \nof titles and abstracts to consider how authors might use \ndifferent terminologies to describe their research.\nReading the articles of the test-list as well as exist\n-\ning relevant reviews often helps to identify search \nterms describing the population, intervention/expo\n-\nsure, outcome(s), and the context of interest. Synonyms \ncan also be looked for in dictionaries. An advantage of \ninvolving librarians in the project team and among the \npeer-reviewers is that they bring their knowledge of spe\n-\ncialist thesauri to the creation of search term lists. For \nexample, for questions in agriculture, CAB Abstracts \nprovides a thesaurus whose terms are added to database \nrecords. The thesaurus terms can offer broad or narrow \nconcepts for the search term of interest, and can provide \nadditional ways to capture articles or to discover over\n-\nlooked words (http://www.cabi.org/cabthesaurus/). As \nwell as database thesauri that offer terms that can be used \nwithin individual databases, there are other thesauri that \nare independent of databases. For example, the Termino\n-\nlogical Resource for Plant Functional Diversity (http://\ntop-thesaurus.org/) offers terms for 700 plant charac\n-\nteristics, plant traits and environmental associations. \nExperts and stakeholders may suggest additional key\n-\nwords, for instance when an intervention is related to a \nspecial device (e.g. technical name of an engine, chemical \nnames of pollutants) or a population is very specific (e.g. \ntaxonomic names which have been changed over time, \ntechnical terminology of genetically-modified organ\n-\nisms). Other approaches can be used to identify search \nterms and facilitate eligibility screening (e.g. text-mining, \ncitation screening, cluster analysis and semantic analysis) \nand are likely to be helpful for CEE evidence synthesis.\nThe search terms identified using these various meth\n-\nods are presented as part of the draft evidence-synthesis \nprotocol so that additional terms may be suggested by \npeer-reviewers. Once the list is finalised in the published \nprotocol it should not be changed, unless justification is \nprovided in the final evidence-synthesis.\nIdentifying relevant sources of articles\nVarious sources of articles relevant to the question may \nexist. Understanding the coverage, the functions and limi\n-\ntations of information sources can be time-consuming, so \ninvolving a librarian or information specialist at this stage \nis highly recommended. We will use bibliography to refer \nto a list of articles generally described by authorship, title, \nyear of publication, place of publication, editor, and often, \nkeywords as well as, more recently, DOI identifiers. A \nbibliographic source allows these bibliographies to be cre\n-\nated by providing a search and retrieval interface. Much \nof the information today is likely to come from searches \nof electronic bibliographic sources, which are becoming \nincreasingly comprehensive with the passage of time as \nmore material is digitised (see “ Addressing the need for \ngrey literature” and “Searching for grey literature”). In this \npaper we use the term “electronic bibliographic source” in \nthe broad sense. It includes individual electronic biblio\n-\ngraphic sources (e.g. Biological Abstracts) as well as plat -\nforms that allow simultaneous searches of several sources \nof information (e.g. Web of Science or Google Scholar) \nor could be accessed through search engines (such as \nGoogle). Platforms are a way to access databases.\nCoverage and accessibility\nSeveral sources should be searched to ensure that as \nmany relevant articles as possible are identified [1, 15]. \nA decision needs to be made as to which sources would \nbe the most appropriate for the question. This mostly \ndepends on the disciplines addressed by the question (e.g. \nbiology, social sciences, other disciplines) and the iden\n-\ntification of sources that may provide the greatest quan -\ntity of relevant articles for a limited number of searches \nand their contribution in reducing the various biases \ndescribed earlier in the paper (see “Identifying relevant \nsources of articles”). The quantity of results given by an \nelectronic bibliographic source is NOT a good indicator \nof the relevance of the articles identified and thus should \nnot be a criterion to select or discard this source. Infor\n-\nmation about access to databases and articles (coverage) \ncan be obtained directly from the project team by sharing \nknowledge and experience, asking librarians and infor\n-\nmation experts and, if needed, stakeholders. Peer-review \nof the evidence synthesis protocol may also provide extra \nfeedback and information regarding the relevance of \nsearching in some other sources.\nPage 6 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nSome databases are open-access, such as Google \nScholar, whereas others require subscription such as Agri-\ncola (http://agricola.nal.usda.gov/). Therefore, access to \nelectronic bibliographic sources may depend on institu -\ntional library subscriptions, and so availability to project \nteams will vary across organisations. A diverse project \nteam from a range of institutions may therefore be ben\n-\neficial to ensure adequate breadth of search strategies. \nWhen the project team does not have access to all the rel\n-\nevant bibliographic sources, it should explain its approach \nand list the sources that were available but not searchable \nand acknowledge these limitations. This may include indi\n-\ncations as to how to further upgrade the evidence synthe-\nsis at a later stage.\nTypes of sources\nWe first present bibliographic sources which allow the \nuse of search strings, mostly illustrated from the envi\n-\nronmental sciences. An extensive list of searchable data -\nbases for the social sciences is available in Kugley et  al. \n[26]. Other sources and methods mentioned below (such \nas searches on Google) are complementary but cannot \nbe the core strategy of the search process of an evidence-\nsynthesis as they are less reproducible and transparent.\nBibliographic sources may vary in the search tools pro\n-\nvided by their platforms. Help pages give information on \nsearch capabilities and these should be read carefully. \nInvolving librarians who keep up-to-date with develop\n-\nments in information sources and platforms is likely to \nsave considerable time.\nElectronic bibliographic sources The platforms which \nprovide access to bibliographic information sources may \nvary according to: \n(A) Platform issues \n  • The syntax needed within search strings (see “Build\n-\ning the search string ”) and the complexity of search \nstrings that they will accept.\n  • Access: not all bibliographic sources are completely \naccessible. It depends on the subscriptions available \nto the project team members in their institutions. \nThe Web of Science platform, for example, contains \nseveral databases, and it is important to check and \ndocument which ones are accessible to the project \nteam via that platform.\n(B) Database issues \n  • Disciplines: subject-based bibliographic sources \n(CAB ebooks; applied life sciences, agriculture, envi\n-\nronment, veterinary sciences, applied economics, \nfood science and nutrition) versus multidisciplinary \nsources (Scopus, Web of Science);\n  • Geographical regions (e.g. Latin America, HAPI-\nHispanic American Periodicals Index, or Europe \nCORDIS). It may be necessary to search region-spe\n-\ncific bibliographic sources if the evidence-synthesis \nquestion has a regional focus [2];\n  • Document types: scientific papers, conference or \nproceedings, chapters, books, theses. Many univer\n-\nsity libraries hold digital copies of their theses, such \nas the EThOS British Library thesis database. Confer\n-\nence papers may be a source of unpublished results \nrelevant for the synthesis, and may be found through \nthe BIOSIS Citation index or the Conference Pro\n-\nceedings Citation Index (Thomson Reuters 2016, in \n[13]).\n  • Durations at the time of writing, in the Web of Sci\n-\nence Core Collection some articles may be accessible \nfrom 1900 although by no means all, in Scopus they \nmay date from 1960).\nPublishers’ databases The websites of individual com\n-\nmercial publishers may be valuable sources of evidence, \nsince they can also offer access to books, chapters of \nbooks, and other material (e.g. datasets). Using their \nrespective search tools and related help pages allows the \nretrieval of relevant articles based on search terms. For \nexample, Elsevier’s ScienceDirect and Wiley Interscience \nare publishers’ platforms that give access to their jour\n-\nnals, their tables of contents and (depending on licence) \nabstracts and the ability to download the article.\nWeb-based search engines Google is one example of a \nweb-based search engine that searches the Internet for \ncontent including articles, books, theses, reports and \ngrey literature (see “ Addressing the need for grey litera\n-\nture” and “Searching for grey literature”). It also provides \nits own search tools and help pages. Such resources are \ntypically not transparent (i.e. they order results using \nan unknown and often changing algorithm, [14]) and \nare restricted in their scope or in the number of results \nthat can be viewed by the user (Google Scholar). Google \nScholar has been shown not to be suitable as a standalone \nresource in systematic reviews but it remains a valuable \ntool for supplementing bibliographic searches [6 , 19] and \nto obtain full-text PDF of articles. BASE Bielefeld aca\n-\ndemic search engine (https://www.base-search.net) is \ndeveloped by the University of Bielefeld (Germany) and \ngives access to a wide range of information, including \nacademic articles, audio files, maps, theses, newspaper \narticles, and datasets. It lists sources of data and displays \ndetailed search results so that transparent reporting is \nfacilitated [35].\nPage 7 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \nFinding full‑text documents\nFull-text documents will be needed only when the find -\nings of the search have been screened for eligibility and \nretained based on their title and abstract, and need to \nbe screened at full-text (see [12]). Limitations to access \nto full-texts can be a source of bias in the synthesis, and \nfinding documents may be time-consuming as it may \ninvolve inter-library loans or direct contact with authors. \nDocuments can be obtained directly if (a) the articles \nare open-access, (b) the articles have been placed on \nan author’s personal webpage, or (c) are included in the \nproject team’ institutional subscriptions. Checking insti\n-\ntutional access when listing the sources of bibliography \nmay help the project team anticipate needs to get extra \nsupport.\nChoosing bibliographic management software\nSpecific reference management software may be used to \nextract the results of the search from the bibliographic \nsource onto a computer or in an online dedicated space \n(e.g. EndNote online). This can assist future removal of \nduplicates and eligibility screening [12]. Establishing an \nefficient workflow to collect, organize, store and share the \narticles retrieved by the searches should save the project \nteam’s time. Common reference management software \nincludes: EndNote and Reference Manager (subscrip\n-\ntion), or Zotero (open-source) and Mendeley (freeware). \nThe choice of software is likely to be influenced by availa\n-\nble resources and the familiarity of the project team with \nspecific software, and may require training. The choice of \nsoftware should ideally be made at the beginning of the \nproject, during the scoping, and is particularly important \nif the project team is dispersed across different locations, \nto ensure that access to references is facilitated at differ\n-\nent stages of the work.\nThe following elements may help when choosing bib -\nliographic management software:\n  • Ease of transferring references between different \nsoftware packages in case the project team members \ndo not have access to all packages;\n  • Ability to add extra metadata relevant to the evidence \nsynthesis (for instance coding around language, geo\n-\ngraphical location of results reported in each article) \nto assist with study identification or grouping for \nanalysis (including bibliometric analysis);\n  • Limitations that may pose a problem (e.g. EndNote \nonline is limited to 10,000 references);\n  • Possibility to retrieve full-texts, automatically or \nsemi-automatically;\n  • Limitations to the number of users of the software;\n  • Remote access to the software and/or results (to \nshare among team members);\n  • Options for storage (e.g. the Cloud) and associated \ncosts;\n  • Possibilities to create bibliographic lists according to \nthe style(s) required by the editor of the review (e.g. \ncite-as-you-write).\nThe functionality for exporting lists of bibliographic \nrecords varies across both electronic sources and the ref\n-\nerence management software used to store records. Some \nplatforms may require citations to be exported individu\n-\nally (e.g. Google Scholar) whereas others allow down -\nloading in batches (e.g. Web of Science). When the size \nof each batch is much smaller than the total number to \nbe exported (even if since 2017 Web of Science extended \ndownloads to batches of 5000 articles, searches may pro\n-\nduce thousands of records), exporting is made in a series \nof batches, which is a time-consuming process. Extract\n-\ning articles ordered by publication date rather than by \nrelevance (e.g. all articles published between 1950 and \n2000 in a first session, and the others later) may prevent \nerrors. In all cases, the project team needs to make sure \nall articles have been correctly retrieved (preferably with \ntheir abstracts). Some publishers ask that you contact \nthem if you wish to export large quantities of articles and \nthis may be worth considering. If there is no easy way to \naccess the full set of results, it is important to be trans\n-\nparent about the possible impact of this when reporting \nthe search.\nAddressing the need for grey literature\n“Grey literature” relates to documents that may be dif -\nficult to locate because they are not indexed in usual \nbibliographic sources. It has been defined as “manifold \ndocument types produced on all levels of government, \nacademics, business and industry in print and electronic \nformats that are protected by intellectual property rights, \nof sufficient quality to be collected and preserved by \nlibraries and institutional repositories, but not controlled \nby commercial publishers; i.e. where publishing is not the \nprimary activity of the producing body” (12th Int Conf \nOn Grey Lit. Prague 2010, but see [31]). Grey literature \nincludes reports, proceedings, theses and dissertations, \nnewsletters, technical notes, white papers, etc. (see list \non http://www.greynet.org/greysourceindex/document\n-\ntypes.html). This literature may not be as easily found \nby internet and bibliographic searches, and may need to \nbe identified by other means (e.g. asking experts) which \nmay be time-consuming and requires careful planning \n[41].\nSearches for grey literature might be included in evi\n-\ndence synthesis for two main reasons: (1) to try to min -\nimize possible publication bias (see “Submitting the \nsearch strategy in the protocol for peer-review ”; [23]), \nPage 8 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nwhere ‘positive’ (i.e. confirmative, statistically signifi -\ncant) results are more likely to be published in academic \njournals [29]; and (2) to include studies not intended for \nthe academic domain, such as practitioner reports and \nconsultancy documents which may nevertheless contain \nrelevant information such as details on study methods or \nresults not reported in journal articles often limited by \nword length.\nDeciding when to stop\nIf time and resources were unlimited, the project team \nshould be able to identify all published articles relevant \nto the evidence-synthesis question. In the real world this \nis rarely possible. Deciding when to stop a search should \nbe based on explicit criteria and it should be explained \nin the protocol or synthesis. Often, reaching the budget \nlimit (in terms of project team time) is the key reason \nfor stopping the search [41] but justification for stop\n-\nping should rely primarily on the acceptability of the \nperformance of the search for the project team. Search\n-\ning only one database is not considered as adequate [26]. \nObserving a high rate of article retrieval for the test-list \nshould not preclude the conduct additional searches \nin other sources to check whether new relevant papers \nare identified. Practically, when searching in electronic \nbibliographic sources, search terms and search strings \nare modified progressively, based on what is retrieved \nat each iteration, using the “test-list” as one indicator of \nperformance. When each additional unit of time spent \nin searching returns fewer relevant references, this may \nbe a good indication that it is time to stop the search [4]. \nStatistical techniques, such as capture-recapture and \nthe relative recall method, exist to guide decisions about \nwhen to stop searching, although to our knowledge they \nhave not been used in CEE evidence-synthesis to date \n(reviewed in [13]).\nFor web-searches (e.g. using Google) it is difficult to \nprovide specific guidance on how much searching effort \nis acceptable. In some evidence syntheses, authors have \nchosen a “first 50 hits” approach (hits meaning articles, \ne.g. [44]) or a ‘first 200 hits’ approach [34], but the CEE \ndoes not encourage such arbitrary cut-offs. What should \nbe reported is whether stopping the screening after the \nfirst 50 (or more) retrieved articles is justified by a decline \nin the relevance of new articles. As long as relevant arti\n-\ncles are being identified, the project team should ideally \nkeep on screening the list of results.\nSubmitting the search strategy in the protocol \nfor peer‑review\nPublishing the search strategy in the evidence synthe -\nsis protocol enables peer reviewers and stakeholders \nto provide input at an early stage and to detect missing \nelements (e.g. keywords, databases of important sources \nof grey literature), highlight possible misunderstandings, \nquestion the relevance of some options (scope, dates, \nvariety of outcomes, etc.), before the final search is con\n-\nducted. This step aims to ensure that the search will be \nof the best possible quality and relevance for the future \nusers of the synthesis. If the scope of the search needs \nto be restrained due to resource limitations, this is pre\n-\nsented to the readers before the review is conducted, and \nshould minimize misunderstanding and criticisms when \ndisclosing the results.\nPart 2—conducting the search\nOnce the search terms and strategy have been reviewed \nand agreed, the test-list is available as well as the list of \nsources, the project team can conduct the search by \nimplementing the whole search strategy, by building their \nsearch strings using the PICO or PECO structure, con\n-\nducting searches in the different sources and testing the \nperformance of the strategy.\nImplementing the search strategy is often a trade-off  \nbetween exhaustivity (or sensitivity) and precision (or \nrelevance, specificity) of the articles retrieved by the \nsearch string(s) [7, 21, 36]. Increasing the exhaustivity of \na search usually means that more non-relevant articles \nare retrieved (the precision is lowered), which may then \nincrease the time spent in assessing articles for relevance. \nDeveloping the optimal search strategy is often an itera\n-\ntive process where results obtained by using the search \nstring are assessed against the test-list and also assessed \nin terms of returning new studies not in the test list, and \nthe string subsequently amended by adding or remov\n-\ning keywords, changing the syntax, and/or using various \noperators, in order to obtain the best possible results. \nThis will be repeated across the various sources until the \nproject team finds the results acceptable. The steps for \nsearches in the bibliographic sources of indexed docu\n-\nments are detailed below.\nPrioritizing bibliographic sources\nGlanville et  al. [13] suggests that the project team \nshould start the search using the source where the larg\n-\nest number of relevant papers are likely to be found, \nand subsequent searches can be constructed with the \naim to complement these first results. Sources contain\n-\ning abstracts allow greater understanding of relevance \nand should be given priority. Combined with the use of \nthe test-list, ordering the use of sources may allow to \nfind the largest number of relevant articles early dur\n-\ning the search, which is useful when time and resources \nare limited. Searching the grey literature can be can be \nconducted in parallel with searches in sources of indexed \ndocuments.\nPage 9 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \nBuilding the search string\nThe list of search terms needs to be combined into search \nstrings that retrieve as many relevant results as possi -\nble (exhaustiveness) while also limiting the number of \nirrelevant results (precision). Search strings needs to be \ntailored to the search engine of each electronic biblio\n-\ngraphic source to be searched (e.g. [19]). To build up the \nstring, the team should rely on the syntax that is available \nin the help pages of the bibliographic sources, including \nthe use of Boolean operators, where applicable.\nElements of syntax\nThe search syntax is the set of options provided in the \ninterface of the bibliographic source to achieve searches. \nThe syntax options can usually be found in the help pages \nof the bibliographic source interface.\nTypical syntax features are listed below and will vary by \ninterface:\n  • Wildcards and truncation Symbols used within \nwords or at the end of the root of the word to sig\n-\nnal that the spelling may vary. Wildcards are useful \nwithin words to capture British and US spelling vari\n-\nants, for example ‘behavi?r’ in some interfaces will \nretrieve records containing ‘behaviour’ as well as \n‘behavior’ . As well as wildcards within words, many \ninterfaces offer truncation options at the end of word \nstems. Truncation can help with identifying words \nwith plural and various grammatical forms. For \nexample, ‘forest*’ in some bibliographic sources will \nretrieve records containing forest, forests, forestry, for\n-\nestal… Some options can also be further defined, for \nexample in the Ovid interface ‘forest$1’ can be used \nto restrict searches to words with no or one extra \ncharacter.\n  • Parentheses Are used, where provided, to group \nsearch terms together (e.g. a set of synonyms linked \nby a Boolean operator, see below) and they determine \nthe sequence in which search operations will be car\n-\nried out by the interface. Search string operations \nwithin parentheses are, typically, carried out before \nthose that are not enclosed within parentheses. In \ncomplex search strings, nesting of groups of search \nterms within different sets of parentheses may be \nhelpful, and the search operation is then performed \nfirst on the search terms that are within the inner\n-\nmost set of parentheses. In this sense, parentheses \nas used in search strings function in a similar way to \nthose used in mathematical calculations. For exam\n-\nple: (road*OR railway*) AND (killing OR mortality) \n(for more explanations about OR, see Boolean opera\n-\ntors below).\n  • Phrase searching Some database interfaces allow \nwords to be grouped and searched as phrases by \nusing, for example, double quotation marks. For \nexample, “organic farming” , “tropical forest” .\n  • Lemmatization Lemmatization involves the auto\n-\nmated reduction of words to their respective “lem -\nmas” (roots). For example, the lemma for the words \n“computation” and “computer” is the word “compute” . \nWhen using defense as a search term, it would also \nfind variants such as defence. Lemmatization can \nreduce or eliminate the need to use wildcards to \nretrieve plurals and variant spellings of a word, but \nit may also retrieve irrelevant variants (e.g. cite as a \nsearch term may retrieve articles with citing, cities, \ncited and citation, Web of Science helpfile). Web of \nScience automatically applies lemmatization rules \nto Topic and Title search queries. This facility is not \navailable in all interfaces.\nBoolean operators\nBoolean operators (AND, OR, NOT) specify logic func -\ntions. They are used to group search terms into blocks \naccording to the PICO or PECO elements, so that the \nsearch is structured and easy to understand, review and \namend, if necessary. AND and OR are at the core of the \nstructure of the search string. Using AND decreases the \nnumber of articles retrieved whilst using OR enlarges \nit, so combining these two operators will change the \nexhaustivity and precision of the search.\nOR is used to identify bibliographic articles in which \nat least one of the search terms is present. OR is used \nto combine terms within one of the PICO element, for \nexample all search terms related to the Population. Using \n“forest* OR woodland* OR mangrove*” will identify doc\n-\numents mentioning at least one of the three search terms.\nAND is used to narrow the search as it requires articles \nto include at least one search term from the lists given \non each side of the AND operator. Using AND identi -\nfies articles which contain, for example, both aa Popula -\ntion AND an Intervention (or Exposure) search term. \nFor instance, a search about a population of butterflies \nexposed to various toxic compounds and then observed \nfor the outcomes of interest can be structured as three \nsets of search terms combined with AND as follows \n[38]: “(lepidopter* OR butterfl* OR coleopter* OR beetl*) \nAND (toxi* OR cry* OR vip3* OR Bacillus thuringiensis* \nOR bt) AND (suscept* OR resist*)” . Truncating words at \n3 characters (e.g. cry* in this example) may find lots of \nirrelevant words and may not be recommended.\nNOT is used to exclude specified search terms or PICO \nelements from search results. However, it can have unan\n-\nticipated results and may exclude relevant records. For \nPage 10 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nthis reason, it should not usually be used in search strat -\negies for evidence synthesis. For example, searching for \n‘rural NOT urban’ will remove records with the word \n‘urban’ , but will also remove records which mention both \n‘rural’ AND ‘urban’ .\nProximity operators (e.g. SAME, NEAR, ADJ, depend\n-\ning on the source) can be used to constrain the search by \ndefining the number of words between the appearance \nof two search terms. For example, in the Ovid interface \n“pollinators adj4 decline*” will find records where the two \nsearch terms “pollinators” and “decline” are within four \nwords of each other. Proximity operators are more pre\n-\ncise than using AND, so may be helpful when a large vol -\nume of search results are being returned.\nAssessing retrieval performance\nChecking search results against the test-list can help to \nimprove a search strategy, using an iterative and com\n-\nparative process. If some articles in the test-list are not \nidentified by the search strategy, the project team should \nconsider why. Changing the search string (adding or \nremoving search terms for instance, or checking the com\n-\nbination of PICO/PECO elements being used) may help \nto find those articles. If any of the articles in the test-list \nare not indexed in the searched electronic bibliographic \nsources, additional bibliographic sources could be added \nto improve coverage. More generally, several sources will \nbe searched to ensure retrieval of all the papers of the \ntest-list (see above).\nThe project team should report the performance of the \nsearch strategy in the evidence synthesis report (e.g. as a \npercentage of the test-list finally retrieved by the search \nstrategy when applied in each electronic bibliographic \nsource, e.g. [19, 45]). A high percentage is one indica\n-\ntor that the search has been optimized and the conclu -\nsions of the review rely on a range of available relevant \narticles that reflect at least those provided by the test-list. \nA low percentage would indicate that the conclusion of \nthe review would be susceptible to change if other docu\n-\nments are added.\nRefining the results\nThe finalised search extracts a first pool of articles that \nis a mixture of relevant and irrelevant articles, because \nthe search, in trying to capture the maximum number \nof relevant papers, inevitably captures other articles \nthat do not attempt to answer the question. Screen\n-\ning the outputs of the search for eligibility will be done \nby examining the extracted papers at title, abstract \nand full-text [12]. If the volume of search results is too \nlarge to process within available resources, the project \nteam may consider using some tools provided by some \nelectronic databases (e.g. Web of Science) to refine \nthe results of the search by categories (e.g. discipline, \nresearch areas) in order to discard some irrelevant arti\n-\ncles prior to extracting the final pool of articles and thus \nlower the number of articles to be screened. There is a \nreal risk in using such tools, as removing articles based \non one irrelevant category may remove relevant papers \nthat also belong to another relevant category. This can \noccur because categories characterise the journal rather \nthan each article and because we are relying on the cat\n-\negories being applied consistently. As a consequence, \nusing refining tools provided by electronic bibliographic \nsources should be done with great caution and only tar\n-\nget categories that are strongly irrelevant for the ques -\ntion (e.g. excluding PHYSICS APPLIED, PERIPHERAL \nVASCULAR DISEASE or LIMNOLOGY in a search \nabout reintroduction or release of carnivores). Using \nthese tools on the results of a search should not change \nthe number of articles of the test list that have been suc\n-\ncessfully retrieved. The test-list is again an indicator of \nthe performance of the strategy when using such tools. If \nthe project team do decide to use such tools, they should \nreport all details of tools used to refine the outputs of \nthe search prior to screening in the evidence synthesis \nprotocol and discuss the limitations of the approach they \nhave used.\nSearching for grey literature\nMore and more documents are being indexed including \nthose in the grey literature [31]. Nevertheless, conducting \na search for grey literature requires time and the authors \nshould assess the need to include it or not in the synthe\n-\nsis [18]. Repeatability and susceptibility to bias should be \nassessed and reported as much as possible.\nBibliographic tools for grey literature\nThere are some databases or platforms which reference \ngrey literature. INIST (Institute for Scientific and Techni\n-\ncal Information, France) holds the European OpenSIGLE \nresource (opensigle.inist.fr), which provides access to all \nthe SIGLE records (System for Information on Grey Lit\n-\nerature), new data added by EAGLE members (the Euro -\npean Association for Grey Literature Exploitation) and \ninformation from Greynet. There are also some programs \nwhich can help to make web-based searches for grey lit\n-\nerature more transparent, a practice that is part of “scrap-\ning methods” [17]. Examples of sources available for grey \nliterature:\n  • BASE ( https://www.base-search.net) allows the \nselection of document types and provides the option \nto focus on unpublished material.\nPage 11 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \n  • Opengrey.eu provides access to more than 700,000 \nbibliographical references of grey literature produced \nin Europe.\n  • Zenodo is an open-access repository initially linked to \nEuropean projects. It welcomes research outputs from \nall over the world and all disciplines, including grey \nliterature. It allows search by keywords and includes \npublications, thesis, datasets, figures, posters, etc.\nExamples of sources providing access to theses and dis\n-\nsertations include: DART-Europe (free); Open Access \nTheses and Dissertations (free); ProQuest Dissertations \nand Theses (http://pqdtopen.proquest.com/, upon sub\n-\nscription); OAISTER; EThOS (British Library, free); \nWorldCat.org (free); OpenThesis.org (free, dissertations/\ntheses, but does include other types of publications). \nFurther resources can be found at http://www.ndltd.org/\nresources/find-etds. Individual universities frequently \nprovide access to their thesis collections.\nWebsites of organisations and professional networks\nMany organisations and professional networks make \ndocuments freely available through their web pages, and \nmany more contain lists of projects, datasets and refer\n-\nences. The list of organisations to be searched is depend -\nent upon both the subject of the evidence synthesis and \nany regional focus (see examples in [5, 27, 34, 45]). Many \nwebsites have a search facility but their functionality \ntends to be quite limited and must be taken into consid\n-\neration when planning for the time allocated to such task.\nExamples:\n  • TROPENBOS is a non-governmental agency cre -\nated in the Netherlands in 1986. It contributes to the \nestablishment of research programmes in tropical \nforestry and it has its own website with many docu\n-\nments, including proceedings of workshops, books \nand articles that contain useful datasets and refer\n-\nences. http://www.tropenbos.org.\n  • Databases such as ScienceResearch.com and \nAcademicInfo.net, contain links to hand-selected \nsites of relevance for a given topic or subject area \nand are particularly useful when searching for subject \nexperts or pertinent organisations, helping to focus \nthe searching process and ensure relevance.\nAsking authors, experts and the project team\nDirect contact with knowledge-holders and other stake -\nholders in networks and organisations may be very \ntime-consuming but may allow collection of very rel\n-\nevant articles [2, 43]. This can be especially useful to \nhelp access older or unpublished data sources, when \nthe research area is sensitive to controversy (e.g. GMO, \nFrampton, pers. comm.) or when resources are limited \n[10]. This may also help enable access to articles written \nin languages other than English.\nWorld‑wide web\nSearch engines (e.g. Google, Yahoo) cannot index the entire \nweb, and they differ widely in the order of their results. They \nall have their own algorithms favouring different criteria and \nboth retrieval and ranking of results may be affected by the \nlocation, the device used to search (mobile, desktops), the \nbusiness model of the search engine and commercial pur\n-\nposes. It is important to use more than one search engine to \nincrease chance to identify relevant papers. Google Scholar \nis often used to scope for existing relevant literature but it \ncannot be used as a standalone resource for evidence syn\n-\nthesis (see “Types of sources”; [6, 19]).\nAdditional approaches: hand‑searching, snowballing \nand citation searching\nHand-searching is a traditional (pre-digital) mode of \nsearching which involves looking at all items in a bib -\nliographic source rather than searching the publication \nusing search terms. Hand-searching can involve thor\n-\noughly reading the tables of contents of journals, meeting \nproceedings or books [13].\nSnowballing and citation searching (also referred to as \n‘pearl growing’ , ‘citation chasing’ , ‘footnote chasing’ , ‘refer\n-\nence scanning’ , ‘checking’ or ‘reference harvesting’) refer \nto methods where the reference lists contained within \narticles are used to identify other relevant articles [42]. \nCitation searching (or ‘reverse snowballing’) uses known \nrelevant articles to identify later publications which have \ncited those papers on the assumption that such publica\n-\ntions may be relevant for the review.\nUsing these methods depends on the resources available \nto the project team (access to sources, time). Hand-search-\ning is rarely at the core of the search strategy, but snowball-\ning and citation searching are frequently used (e.g. [32]). \nRecent developments in some bibliographic sources auto\n-\nmatically highlight and allow the user to link, to cited and \nrelated articles when viewing (e.g. when scanning Elsevier \njournals, or when downloading full-text PDF). This may \nbe difficult to handle as those references may or may not \nhave been found by the systematic approach using search \nstrings and may have to be reported as additional articles. \nThe use of those methods and their outputs should be \nreported in detail in the final evidence-synthesis.\nPart 3—managing references and reporting the \nsearch\nGood documenting, reporting and archiving of searches \nand their resulting articles may save a substantial amount \nof time and resource by reducing duplication of results \nPage 12 of 14Livoreil et al. Environ Evid  (2017) 6:23 \nand enabling the search be re-assessed or amended easily \n[21]. Good reporting ensures that any of the limitations \nof the search is explicit and hence allows assessment of \nany possible consequences of those limitations on the \nsynthesis’ findings. Good archiving enables the project \nteam to respond the queries about the search process \nefficiently. If a project team is asked why they did not \ninclude an article in their review, for example, proper \narchiving of the workflow will allow the team to check \nwhether the article was detected by the search, and if it \nwas, why it was discarded.\nGood documenting, reporting and archiving has two \nmain aspects: (1) the clear recording of the search strat\n-\negy and the results of all of the searches (records) and (2) \nthe way the search is reported in the evidence synthe\n-\nsis protocol and final report. Reporting standards keep \nimproving (see a comparative study in [33]) and many \nreporting checklists exist to help project teams [39], \nalthough none are available specifically for environmen\n-\ntal evidence-synthesis at the time of writing.\nKeeping track of the search strategy and recording results\nThe project team should document its search methodol -\nogy in order to be transparent and to be able to justify \ntheir use of a search term or the choice of resources. \nEnough detail should be provided to allow the search \nto be replicated including the name of the database, the \ninterface, the date of the search and the full search with \nall the search terms, which should be reported exactly \nas run [26]. The search history and number of articles \nretrieved by each search should be recorded in a log\n-\nbook or using screenshots and may be reported in the \nfinal evidence synthesis (e.g. as supplementary mate\n-\nrial). The number of articles retrieved and screened and \ndiscarded should be recorded in a PRISMA diagram and \nthis usually accompanies the reporting of the search and \neligibility screening stages within an evidence-synthesis \nreport (for an example of PRISMA see Frampton et al. \n[12]).\nFor internet searches, reviewers should record and \nreport the URL, the date of the search, the search strat\n-\negy used (search strings with all options making the \nsearch replicable), as well as the number of results of \nthe search, even if this may not be easily reproduc\n-\nible. Saving search results as HTML pages (possibly as \nscreenshots to allow archiving that can be perused later \neven if the webpage has changed in the meantime) pro\n-\nvides transparency for this type of search [20]. Record -\ning searches in citation formats (e.g. RIS files) make \nthem compatible with reference or review management \nsoftware and allow archiving for future use (Haddaway, \npers. comm.).\nReporting the final search strategy and findings\nAlthough the search strategy will have been listed in the \nprotocol, the searches as finally run should be reported \nin the final evidence synthesis report, possibly as addi\n-\ntional files or supplementary information, since the \nsearch as finally run may be different from the protocol. \nThe final synthesis reports the results and performance \nof the search. Minor amendments to the protocol (e.g. \nadding or removing search terms) should be reported \nin the final synthesis, but the search should not be sub\n-\nstantially changed once approved by reviewers (but see \n“Part 4”).\nCurrent details of what should be reported in the \nprotocol and the final evidence synthesis report are \ndescribed in the Guidelines for authors available at:\nhttp://environmentalevidencejournal.biomedcentral.\ncom/submission-guidelines.\nThe project team may report the details of each search \nstring and how it was developed (e.g. [5]) and whether \nthe strategy has been adjusted to the various databases \nconsulted (e.g. [19, 27]) or developed in several languages \n(e.g. [27]). Limitations of the search should be reported \nas much as possible, including the range of languages, \ntypes of documents, time-period covered by the search, \ndate of the search (e.g. [27, 45]), and any unexpected dif\n-\nficulty that impacted the search compared to what was \ndescribed in the protocol (e.g. end of access, [19]).\nPart 4—updating and amending searches\nFrom the moment a search is completed, new articles \nmay be published as research effort is dynamic. Updat\n-\ning or amending a search may be conducted by the same \nproject team that undertook the initial searches, but this \nis not always the case. Therefore, it is important that the \noriginal searches are well documented and, if possible, \nlibraries (e.g. EndNote databases) of retrieved articles \nare saved (and, if possible, reported or made available) to \nensure that new search results can be differentiated from \nprevious ones, as easily as possible.\nThere are two main reasons why a search needs to be \nchanged. The first may occur when the evidence synthe\n-\nsis extends over a long time period (for instance more \nthan 2  years) and the publication rate of relevant docu\n-\nments on the topic is high. In this case, the conclusions \nof the review may be out of date even before it is pub\n-\nlished. It is recommended that the search is rerun using \nthe same search strings [3] for the time period elapsed \nsubsequent to the end of the initial search. The second \ncase occurs when the evidence synthesis final report has \nbeen already published, and there is a need for revision \nbecause new results or developments have been pub\n-\nlished and need to be taken into account. In this case the \nPage 13 of 14\nLivoreil et al. Environ Evid  (2017) 6:23 \nsearch protocol should be checked to identify whether \nnew search terms need to be added or additional sources \nneed to be searched. Deciding whether a new protocol \nneeds to be published will depend on the extend of the \namendments and may be discussed with the Collabora\n-\ntion for Environmental Evidence.\nThere are a number of issues that need to be considered \nwhen updating a search:\n  • Do you have access to the original search strings, \nsources, and can you read these files (proper software \navailable)?\n  • Was the original search protocol adequate and appro\n-\npriate or does it need revising?\n  • Do you know when the initial search took place and \nwhich time boundaries were set up at that time? If \nnot, can you contact the authors to get those details?\n  • If relevant, do you have similar details regarding \nsearches in grey literature?\n  • Do you have access to the same sources of documents \n(e.g. database platforms), including institutional web\n-\nsites, subscriptions?\n  • Will the same languages be used?\nThen the revised (or original) strategy may be run [3]. \nAs with the original searches, it is important to docu -\nment clearly any updates to the searches, their dates, and \nany reasons for changes to the original searches, most \ntypically in an appendix. If the new search differs from \nthe initial one, a new protocol may need to be submitted \nbefore the amendment is conducted [3].\nAuthors’ contributions\nBL led the writing and conducted Skype exchanges with co-authors who all \nworked voluntarily and from a distance. GF, GP and BL as co-editors of the \nnew chapters of the CEE guidelines drafted the table of content. GF revised \nthe contents at key stages to ensure compatibility and consistency with \nother chapters of the CEE Guidelines for Systematic Review in Environmental \nEvidence currently under writing. All authors read and approved the final \nmanuscript.\nAuthor details\n1 Foundation for Research ON Biodiversity (FRB), Paris, France. 2 York Health \nEconomics Consortium, University of York, York, UK. 3 Mistra EviEM, Stockholm \nEnvironment Institute, Stockholm, Sweden. 4 Centre for Evidence-Based \nConservation, Bangor University, Bangor, UK. 5 Evidence Synthesis Team, Exeter \nMedical School, University of Exeter, Exeter, UK. 6 Université Pierre & Marie \nCurie, Paris, France. 7 Institute of Health and Society, Newcastle University, \nNewcastle, UK. 8 Centre for International Forestry, Bogor, Indonesia. 9 Biodiver-\nsity Institute, University of Oxford, Oxford, UK. 10 Southampton Health Technol-\nogy Assessments Centre, Faculty of Medicine, University of Southampton, \nSouthampton, UK. \nAcknowledgements\nBL sincerely thanks the co-authors for their involvement in a long endeavour \nbased on voluntary time. It has been a great experience to try to merge differ-\nent experiences and understanding of the challenges and tools of systematic \nsearches from different disciplines. We thank the Editor and anonymous \nreviewers for their constructive comments on the submitted manuscript. We \nthank Alison Specht (CESAB, France), for her valuable contribution in final edit-\ning of English language and improvement of clarity of this article.\nCompeting interests\nThe authors declare that they have no competing interests.\nFunding\nCEE provided financial support to Oxford Martin School, University of Oxford, \nto support a 2-day workshop to discuss and revise the manuscript among \nco-editors of the CEE guidelines (GF, GP , BL). CEE provided financial support for \nthe publication of this paper in Environmental Evidence.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.\nReceived: 20 January 2017   Accepted: 15 July 2017\nReferences\n 1. Avenell A, Handoll H, Grant A. Lessons for search strategies from a sys-\ntematic review, in The Cochrane Library, of nutritional supplementation \ntrials in patients after hip fracture. Am J Clin Nutr. 2001;73(3):505–10.\n 2. Bayliss HR, Beyer FR. Information retrieval for ecological syntheses. Res \nSynth Methods. 2015;6(2):136–48.\n 3. Bayliss HR, Haddaway NR, Eales J, Frampton GK, James KL. Updating and \namending systematic reviews and systematic maps in environmental \nmanagement. Environ Evid. 2016;5(1):20.\n 4. Booth A. How much searching is enough? Comprehensive versus \noptimal retrieval for technology assessments. Int J Technol Assess Health \nCare. 2010. doi:10.1017/s0266462310000966.\n 5. Bottrill M, Cheng S, Garside R, Wongbusarakum S, Roe D, Holland MB, \nEdmond J, Turner WR. What are the impacts of nature conservation \ninterventions on human well-being: a systematic map protocol. Environ \nEvid. 2014;3:16.\n 6. Bramer WM, Giustini D, Kramer BMR, Anderson PF. The comparative recall \nof Google Scholar versus PubMed in identical searches for biomedical \nsystematic review: a review of searches used in systematic reviews. Syst \nRev. 2013;2:115.\n 7. CEE (Collaboration for Environmental Evidence). Guidelines for systematic \nreview and evidence synthesis in environmental management. Version \n4.2. CEE; 2013.\n 8. Corlett RT. Trouble with the gray literature. Biotropica. 2011;43(1):3–5.\n 9. Dickersin K. Publication bias: recognizing the problem, understanding \nits origins and scope, and preventing harm. In: Rothstein HR, Sutton \nAJ, Borenstein M, editors. Publication bias in meta-analysis: prevention, \nassessment, and adjustments. London: Wiley; 2005. p. 11–3.\n 10. Doerr ED, Dorrough J, Davies MJ, Doerr VAJ, McIntyre S. Maximising \nthe value of systematic reviews in ecology when data or resources are \nlimited. Austral Ecol. 2015;40(1):1–11.\n 11. EFSA (European Food and Safety Authority). Application of systematic \nreview methodology to food and safety assessments to support decision \nmaking. EFSA J. 2010;8(6):1637.\n 12. Frampton GK, Livoreil B, Petrokofsky G. Eligibility screening in evidence \nsynthesis of environmental management topics. Environ Evid. 2017 (in \npress).\n 13. Glanville J. Searching bibliographic databases. In: Cooper HC, Hedges LV, \nValentine JC, editors. The handbook of research synthesis and meta-\nanalysis. 3rd ed. New York: Russell Sage Foundation; 2017.\n 14. Giustini D, Boulos MNK. Google Scholar is not enough to be used alone \nfor systematic reviews. Online J Public Health Inf. 2013;5(2):1–9.\n 15. Grindlay DJC, Brennan ML, Dean RS. Searching the veterinary literature: a \ncomparison of the coverage of veterinary journals by nine bibliographic \ndatabases. J Vet Med Educ. 2012;39(4):404–12.\n 16. Gurevitch J, Hedges LV. Statistical issues in ecological meta-analyses. Ecol-\nogy. 1999;80:1142–9.\nPage 14 of 14Livoreil et al. Environ Evid  (2017) 6:23 \n•  We accept pre-submission inquiries \n•  Our selector tool helps you to ﬁnd the most relevant journal\n•  We provide round the clock customer support \n•  Convenient online submission\n•  Thorough peer review\n•  Inclusion in PubMed and all major indexing services \n•  Maximum visibility for your research\nSubmit your manuscript at\nwww.biomedcentral.com/submit\nSubmit your next manuscript to BioMed Central \nand we will help you at every step:\n 17. Haddaway NR. The use of web-scraping software in searching for grey \nliterature. Grey J. 2015;11(3):186–90.\n 18. Haddaway NR, Bayliss HR. Shades of grey: two forms of grey literature \nimportant for reviews in conservation. Biol Conserv. 2015. doi:10.1016/j.\nbiocon.2015.08.018.\n 19. Haddaway NR, Collins AM, Coughlin D, Kirk S. The role of Google Scholar \nin evidence reviews and its applicability to grey literature searching. PLoS \nONE. 2015;10(9):e0138237.\n 20. Haddaway NR, Collins AM, Coughlin D, Kirk S. A rapid method to increase \ntransparency and efficiency in web-based searches. Environ Evid. \n2017;6:1. doi:10.1186/s13750-016-0079-2.\n 21. Higgins JP , Green S. Cochrane handbook for systematic reviews of inter-\nventions. Chichester: Wiley; 2011.\n 22. Holst R, Funk CJ. State of the art of expert searching: results of a Medical \nLibrary association survey. J Med Libr Assoc. 2005;93(1):45–52.\n 23. Hopewell S, McDonald S, Clarke MJ, Egger M. Grey literature in meta-\nanalyses of randomized trials of health care interventions. Cochrane \nDatabase Syst Rev. 2007. doi:10.1002/14651858.MR000010.pub3.\n 24. James KL, Randall NP , Haddaway NR. A methodology for systematic map-\nping in environmental sciences. Environ Evid. 2016;5:7.\n 25. Juni P , Holenstein F, Sterne J, Bartlett C, Egger M. Direction and impact \nof language bias of controlled trials: an empirical study. Int J Epidemiol. \n2002;31(1):115–23.\n 26. Kugley S, Wade A, Thomas J, Mahood Q, Klint-Jørgensen AM, Hammer-\nstrøm K, Sathe N. Searching for studies: a guide to information retrieval \nfor Campbell Systematic Reviews. Campbell Syst Rev. 2016 (Supplement \n1).\n 27. Land M, Granéli W, Grimwall A, Hoffmann CC, Mitsch WJ, Tonderski KS, \nVerhoeven JTA. How effective are created or restored freshwater wetlands \nfor nitrogen and phosphorus removal? A systematic review protocol. \nEnviron Evid. 2013;2:16.\n 28. Leimu R, Koricheva J. Cumulative meta-analysis: a new tool for detection \nof temporal trends and publication bias in ecology. Proc R Soc B Biol Sci. \n2004. doi:10.1098/rspb.2004.2828.\n 29. Leimu R, Koricheva J. What determines the citation frequency of ecologi-\ncal papers? Trends Ecol Evol. 2005;20(1):28–32.\n 30. Lortie CJ, Aarssen LW, Budden AE, Koricheva JK, Leimu R, Tregenza T. \nPublication bias and merit in ecology. Oikos. 2007;116:1247–53.\n 31. Mahood Q, van Eerd D, Irvin E. Searching for grey literature for systematic \nreviews: challenges and benefits. Res Synth Methods. 2014;3:221–34.\n 32. McKinnon MC, Cheng SH, Dupre S, Edmond J, Garside R, Glew L, Holland \nMB, Levine E, Masuda YJ, Miller DC, Oliveira I, Revenaz J, Roe D, Shamer S, \nWilkie D, Wongbusarakum S, Woodhouse E. What are the effects of nature \nconservation on human well-being? A systematic map of empirical \nevidence from developing countries. Environ Evid. 2016;5:8.\n 33. Mullins MM, DeLuca JB, Crepaz N, Lyles CM. Reporting quality of search \nmethods in systematic reviews of HIV behavioural interventions (2000–\n2010); are the searches clearly explained, systematic and reproducible? \nRes Synth Methods. 2014;5:116–30.\n 34. Ojanen M, Miller D, Zhou W, Mshale B, Mwangi E, Petrokovsky G. What are \nthe environmental impacts of property rights regimes in forests, fisheries \nand rangelands? A systematic review protocol. Environ Evid J. 2014;3:19.\n 35. Ortega JL. Academic search engines: a quantitative outlook. Oxford: \nChandos Publ; 2014.\n 36. Petticrew M, Roberts H. Systematic reviews in the social sciences. A \npractical guide. Oxford: Blackwell; 2006.\n 37. Pham B, Klassen TP , Lawson ML, Moher D. Language of publication \nrestrictions in systematic reviews gave different results depending on \nwhether the intervention was conventional or complementary. J Clin \nEpidemiol. 2005;58(8):769–76.\n 38. Priesnitz KU, Vaasen A, Gathmann A. Baseline susceptibility of different \nEuropean lepidopteran and coleopteran pests to Bt proteins expressed \nin Bt Maize: a systematic review. Environ Evid. 2016. doi:10.1186/\ns13750-016-0077-4.\n 39. Rader T, Mann M, Stansfield C, Cooper C, Sampson M. Methods for docu-\nmenting systematic review searches: a discussion of common issues. Res \nSynth Methods. 2014;5:98–115.\n 40. Rothstein HR, Sutton AJ, Borenstein M. Chapter 1. Publication bias in \nmeta-analysis. In: Rothstein HR, Sutton AJ, Borenstein M, editors. Publica-\ntion bias in meta-analysis—prevention, assessment and adjustments. \nLondon: Wiley; 2005. p. 2–7.\n 41. Saleh AA, Ratajeski MA, Bertolet M. Grey literature searching for health \nsciences systematic reviews: a prospective study of time spent and \nresources utilised. Evid Based Libr Inf Pract. 2014;9(3):28–50.\n 42. Sayers A. Tips and tricks in performing a systematic review. Br J Gen Pract. \n2007;57(542):759.\n 43. Schindler S, Livoreil B, Pinto IS, Araujo RM, Zulka KP , Pullin AS, Santamaria \nL, Kropik M, Fernandez-Mendez P , Wrbka T. The network BiodiversityK-\nnowledge in practice: insights from three trial assessments. Biodivers \nConserv. 2016;25(7):1301–18.\n 44. Smart JM, Burling D. Radiology and the Internet: a systematic review of \npatient information resources. Clin Radiol. 2001;56(11):867–70.\n 45. Söderström B, Hedlund K, Jackson LE, Kätterer T, Lugato E, Thomsen IK, \nJørgensen HB. What are the effects of agricultural management on soil \norganic carbon (SOC) stocks? Environ Evid. 2014;3:2.\n 46. Song F, Parekh S, Hooper L, Loke YK, Ryder J, Sutton AJ, Hing C, Kwok CS, \nPang C, Harvey I. Dissemination and publication of research findings: an \nupdated review of related biases. Health Technol Assess. 2010;14(8):iii, \nix–xi.\n 47. Zhang L, Sampson M, McGowan J. Reporting the role of expert searcher \nin cochrane reviews. Evid Based Libr Inf Pract. 2006;1(4):3–16.",
  "topic": "Systematic review",
  "concepts": [
    {
      "name": "Systematic review",
      "score": 0.6553347110748291
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.5291144251823425
    },
    {
      "name": "Expert opinion",
      "score": 0.4632467031478882
    },
    {
      "name": "Management science",
      "score": 0.42746999859809875
    },
    {
      "name": "Computer science",
      "score": 0.42543017864227295
    },
    {
      "name": "Evidence-based practice",
      "score": 0.421627402305603
    },
    {
      "name": "Data science",
      "score": 0.4105374813079834
    },
    {
      "name": "Political science",
      "score": 0.2510623037815094
    },
    {
      "name": "MEDLINE",
      "score": 0.1882881224155426
    },
    {
      "name": "Engineering",
      "score": 0.15289855003356934
    },
    {
      "name": "Geography",
      "score": 0.12292012572288513
    },
    {
      "name": "Medicine",
      "score": 0.09111905097961426
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Intensive care medicine",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}