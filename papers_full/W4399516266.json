{
    "title": "Transformer-based Language Models and Homomorphic Encryption: An Intersection with BERT-tiny",
    "url": "https://openalex.org/W4399516266",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3015476619",
            "name": "Lorenzo Rovida",
            "affiliations": [
                "University of Milano-Bicocca"
            ]
        },
        {
            "id": "https://openalex.org/A2059856784",
            "name": "Alberto Leporati",
            "affiliations": [
                "University of Milano-Bicocca"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4307823778",
        "https://openalex.org/W4226075590",
        "https://openalex.org/W2969350772",
        "https://openalex.org/W3202586005",
        "https://openalex.org/W3094696138",
        "https://openalex.org/W2768174108",
        "https://openalex.org/W3184112201",
        "https://openalex.org/W236632755",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W4287854684",
        "https://openalex.org/W3173128495",
        "https://openalex.org/W2163455955",
        "https://openalex.org/W2055580199",
        "https://openalex.org/W2007466965",
        "https://openalex.org/W2914630606"
    ],
    "abstract": "In recent years, emerging and improved Natural Language Processing (NLP) models, such as Bidirectional Encoder Representations from Transformers (BERT), have gained significant attention due to their performance on several natural language tasks. However, inappropriate focus is usually given to the critical problems of security and data privacy, since these models require access to plain data. To address these issues, we suggest a solution based on Fully Homomorphic Encryption (FHE), which allows for computations to be performed on encrypted data. In particular, we propose a FHE-based circuit that, by implementing the smallest existent BERT model, namely BERT-Tiny, enables the extraction of encrypted sentences representations and encrypted text classifications. Considering the nature and the depth of this circuit, we used the Cheon-Kim-Kim-Song (CKKS) scheme, along with the bootstrapping operation. We also propose to use precomputations for the Layer Normalization, in order to lighten computations. The experiments, which can be replicated using our open-source code, are conducted on the Stanford Sentiment Treebank (SST-2) dataset. They show that errors introduced by precomputed Layer Normalizaion, approximate FHE operations and polynomial approximations do not produce a significant performance loss.",
    "full_text": null
}