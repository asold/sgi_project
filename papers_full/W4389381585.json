{
  "title": "Human O-linked Glycosylation Site Prediction Using Pretrained Protein Language Model",
  "url": "https://openalex.org/W4389381585",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Subash Pakhrin",
      "affiliations": [
        "University of Houston - Downtown"
      ]
    },
    {
      "id": "https://openalex.org/A2116898080",
      "name": "Neha Chauhan",
      "affiliations": [
        "Wichita State University"
      ]
    },
    {
      "id": "https://openalex.org/A2096017297",
      "name": "Salman Khan",
      "affiliations": [
        "University of Houston - Downtown"
      ]
    },
    {
      "id": "https://openalex.org/A5093119302",
      "name": "Jamie Upadhyaya",
      "affiliations": [
        "University of Houston - Downtown"
      ]
    },
    {
      "id": "https://openalex.org/A2100370623",
      "name": "Charles Keller",
      "affiliations": [
        "University of Houston - Downtown"
      ]
    },
    {
      "id": null,
      "name": "Laurie Neuman",
      "affiliations": [
        "University of Houston - Downtown"
      ]
    },
    {
      "id": "https://openalex.org/A3135763032",
      "name": "Moriah Beck",
      "affiliations": [
        "Wichita State University"
      ]
    },
    {
      "id": "https://openalex.org/A2099107678",
      "name": "Eduardo Blanco",
      "affiliations": [
        "University of Arizona"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2077312152",
    "https://openalex.org/W2006787506",
    "https://openalex.org/W1963545514",
    "https://openalex.org/W2151120928",
    "https://openalex.org/W2035184699",
    "https://openalex.org/W1510000497",
    "https://openalex.org/W3127515140",
    "https://openalex.org/W2166720212",
    "https://openalex.org/W2950374603",
    "https://openalex.org/W2607268717",
    "https://openalex.org/W2164834993",
    "https://openalex.org/W2063768466",
    "https://openalex.org/W2107815233",
    "https://openalex.org/W2014509414",
    "https://openalex.org/W2103525038",
    "https://openalex.org/W2899867782",
    "https://openalex.org/W2922210059",
    "https://openalex.org/W2939599083",
    "https://openalex.org/W3216679652",
    "https://openalex.org/W2043338013",
    "https://openalex.org/W4361010297",
    "https://openalex.org/W2076746539",
    "https://openalex.org/W2528774686",
    "https://openalex.org/W3216341395",
    "https://openalex.org/W4381686770",
    "https://openalex.org/W3165545667",
    "https://openalex.org/W4220991280",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W4220978941",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W4315641887",
    "https://openalex.org/W4282984452",
    "https://openalex.org/W4377094818",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W4366083739",
    "https://openalex.org/W4225264859",
    "https://openalex.org/W4384498728",
    "https://openalex.org/W4281993476",
    "https://openalex.org/W2097606916",
    "https://openalex.org/W3045004532",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4205989901",
    "https://openalex.org/W4286500588",
    "https://openalex.org/W3197696221",
    "https://openalex.org/W2521200999",
    "https://openalex.org/W2397556949",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2949342052",
    "https://openalex.org/W2953008890",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W3165484394",
    "https://openalex.org/W4225318490",
    "https://openalex.org/W4206950245",
    "https://openalex.org/W4293713156",
    "https://openalex.org/W4285127790",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Abstract O- linked glycosylation of proteins is an essential post-translational modification process in Homo sapiens , where the attachment of a sugar moiety occurs at the oxygen atom of serine and/or threonine residues. This modification plays a pivotal role in various biological and cellular functions. While threonine or serine residues in a protein sequence are potential sites for O- linked glycosylation, not all threonine or serine residues are O- linked glycosylated. Furthermore, the modification is reversible. Hence, it is of vital importance to characterize if and when O- linked glycosylation occurs. We propose a multi-layer perceptron-based approach termed OglyPred-PLM which leverages the contextualized embeddings produced from the ProtT5-XL-UniRef50 protein language model that significantly improves the prediction performance of human O- linked glycosylation sites. OglyPred-PLM surpassed the performance of other indispensable O- linked glycosylation predictors on the independent benchmark dataset. This demonstrates that OglyPred-PLM is a powerful and unique computational tool to predict O- linked glycosylation sites in proteins and thus will accelerate the discovery of unknown O- linked glycosylation sites in proteins.",
  "full_text": "Page 1/20\nHuman O-linked Glycosylation Site Prediction Using\nPretrained Protein Language Model\nSubash Pakhrin  (  pakhrins@uhd.edu )\nUniversity of Houston-Downtown\nNeha Chauhan \nWichita State University\nSalman Khan \nUniversity of Houston-Downtown\nJamie Upadhyaya \nUniversity of Houston-Downtown\nCharles Keller \nUniversity of Houston-Downtown\nLaurie Neuman \nUniversity of Houston-Downtown\nMoriah Beck \nWichita State University\nEduardo Blanco \nUniversity of Arizona\nArticle\nKeywords:\nPosted Date: December 6th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3587524/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/20\nAbstract\nO-linked glycosylation of proteins is an essential post-translational modi\u0000cation process in Homo\nsapiens, where the attachment of a sugar moiety occurs at the oxygen atom of serine and/or threonine\nresidues. This modi\u0000cation plays a pivotal role in various biological and cellular functions. While\nthreonine or serine residues in a protein sequence are potential sites for O-linked glycosylation, not all\nthreonine or serine residues are O-linked glycosylated. Furthermore, the modi\u0000cation is reversible. Hence,\nit is of vital importance to characterize if and when O-linked glycosylation occurs. We propose a multi-\nlayer perceptron-based approach termed OglyPred-PLM which leverages the contextualized embeddings\nproduced from the ProtT5-XL-UniRef50 protein language model that signi\u0000cantly improves the prediction\nperformance of human O-linked glycosylation sites. OglyPred-PLM surpassed the performance of other\nindispensable O-linked glycosylation predictors on the independent benchmark dataset. This\ndemonstrates that OglyPred-PLM is a powerful and unique computational tool to predict O-linked\nglycosylation sites in proteins and thus will accelerate the discovery of unknown O-linked glycosylation\nsites in proteins.\nIntroduction\nO-linked glycosylation is an important post-translational modi\u0000cation (PTM) in humans, involving the\nattachment of glycans to threonine (T) or serine (S) residues of protein sequences1. O-linked\nglycosylation sites can affect protein structure and function, resulting in various physiological and\npathological outcomes, including protein dysfunction2 (namely, disruption in intercellular\ncommunication), cellular dysfunction, immune de\u0000ciencies3, hereditary disorders4, congenital disorders\nof glycosylation, and cancers5,6. Therefore, precise identi\u0000cation of O-linked glycosylation sites holds\nsigni\u0000cant remedial potential in humans.\nExperimental methods such as mass spectrometry7,8 are used to identify O-linked glycosylation sites.\nThrough this technique, 9,354 O-glycosylation sites have been identi\u0000ed9. Even though this type of\nexperiment is the most reliable method to identify O-linked glycosylation sites, there are several reasons\nthat limit e\u0000ciency. Notably, these methods can be intricate, time-consuming, labor-intensive, and\nexpensive. Therefore, the usage of statistical tools developed through machine learning (ML) and deep\nlearning (DL) methodologies can become the prominent solution for characterizing O-linked glycosylation\nsites.\nRecently there has been signi\u0000cant progress in ML and DL areas which has led to the creation of several\ncomputational tools that can predict O-linked glycosylation sites10. For example, NetOGlyc11 is a\nmultilayer perceptron (MLP) based tool built for mucin-type O-linked glycosylation prediction. This\napproach used the accessible surface area and amino acid composition for encoding the amino acid\nsequences12,13. The Oglyc14 approach relies on the binary pro\u0000le features and physicochemical\nproperties of protein peptides. This method leverages support vector machine (SVM) algorithms to\nPage 3/20\npredict O-glycosylation sites. EnsembleGly15 uses ensemble SVM to predict O-glycosylation sites that\nutilize physicochemical properties, evolutionary features and intuitive binary encoding scheme to encode\nthe protein peptide. The CKSAAP_OGlySite16 method uses SVM with the composition of k-spaced amino\nacids pairs (CKSAAP) feature encoding scheme to predict O-glycosylation sites. The GlycoPP17 tool\nutilized SVM to predict O-glycosylation sites where datasets were encoded with similar features used by\nEnsembleGly15 method. GlycoMine18 is a Random Forest (RF) based O-linked glycosylation site\nprediction and its datasets are numerically encoded with sequence and heterogeneous functional-based\nfeatures. Positive unlabeled (PU)19 learning technique is used in GlycoMine_PU20 to detect O-linked\nglycosylation sites. SPRINT-Gly21 is a MLP based method which detects human and mouse O-\nglycosylation sites. Here, the protein peptide (length = 5) is encoded in a sequence and predicted\nsecondary structural-based features. Captor22 is an O-glycosylation site prediction tool that uses the\nlatest experimentally characterized OGP dataset. These datasets were encoded with sequence and\nphysicochemical based features (AAindex)23. These features were merged and utilized for training the\nSVM classi\u0000er. Recently, Alkuhlani et al.24 developed TAPE PLM-based25 O-linked glycosylation sites\npredictor and used XGBoost to classify the O-glycosylation site in the proteins.\nThe majority of the methods mentioned above26–31 have their input features manually curated for\nprediction. Furthermore, the O-linked glycosylation prediction method from Alkuhlani et al.24 is the only\none that leveraged the bene\u0000ts of embeddings from large protein language models (TAPE25). However,\nthey have not extensively explored other complementary and recently developed PLMs. The O-linked\nglycosylation prediction method of Alkuhlani et al.24 still uses a window size of 31 around site of interest\nto extract the TAPE-based PLM features. Additionally, this method uses SVM based feature selection\napproaches which are indeed meticulous, yet also onerous. Moreover, a recent study32 evaluated the\nperformance of different protein language models (PLM) for protein representation comparing them\nagainst each other. Through this study, it has been concluded that ProtT5-XL-Uniref5033 (herein called\nProtT5) achieved the best performance in most of the proteomics tasks. However, ProtT5 has not yet\nbeen used for O-linked glycosylation site prediction. OglyPred-PLM utilizes high-quality experimentally\ncharacterized O-linked glycosylation data derived from the OGP database9, whose site of interest “S/T” is\nencoded by pre-trained PLM.\nRecent developments in the \u0000eld of natural language processing34 led to the creation of transformer-\nbased large language models that have been trained on extensive and diverse corpora of unlabeled data.\nA wide range of PLMs have been developed25,33,35,36 which are trained with an enormous number of\nunlabeled protein sequences that are available in UniProt databases37 and other resources. Elnaggar et\nal. have developed ProtT5-XL-UniRef5033 PLM trained with 2.5 billion protein sequences where protein\nsequences are considered as sentences and individual amino acids as words. The representative\nembeddings produced from these models have been used for numerous posterior tasks. The results\nindicate that the feature embeddings generated by self-supervised PLM effectively encapsulate crucial\ncharacteristics encompassing the evolutionary context of a sequence, physicochemical properties,\nPage 4/20\ncontact map, subcellular localization, distant interconnections in protein sequences, taxonomy, protein\nstructure and function38–43. Likewise, features derived from these transformer-based pretrained PLMs\nhave been proven to be effective in predicting a range of attributes, including intrinsic disorder sites44,\nsignal peptides45, N-linked glycosylation sites46, subcellular localization47, phosphorylation sites48,\nprotein structural featuers49, and binding residues41.\nIn this work, we developed a computation tool called OglyPred-PLM (O-linked glycosylation sites\nPrediction using pretrained Protein Language Model) that uses representative embeddings from a pre-\ntrained ProtT5 PLM to train the MLP model and eventually strengthen the prediction performance of O-\nlinked glycosylation sites. Similarly, we carry out a comprehensive comparison of the performance of\nProtT5 PLM embeddings with other quintessential PLMs i.e., ESM2 (3B parameters)35 and ANKH36\napproaches. The results demonstrate that the embeddings from ProtT5 are slightly better than other\nPLMs under consideration. Moreover, OglyPred-PLM was compared with the recent spectrum of O-linked\nglycosylation predictors. The experiments reveal that OglyPred-PLM was able to surpass the predictive\nperformance of essential predictors like SPRINT-Gly, Captor, and Alkuhlani et al. O-linked glycosylation\nmethods. OglyPred-PLM produces Matthew’s correlation coe\u0000cient (MCC), accuracy (ACC), sensitivity\n(SN), precision (PRE), and speci\u0000city (SP) of 0.614, 0.807, 0.817, 0.801 and 0.797 respectively on\nAlkuhlani et al. independent test dataset. Similarly, it outperformed indispensable SPRINT-Gly and Captor\nO-linked glycosylation predictors by a wide margin. Hence, OglyPred-PLM is an important computational\ntool that can be used to detect O-linked glycosylation sites and accelerate discovery. All programs and\ndata can be accessed via https://github.com/PakhrinLab/OglyPred-PLM\nResults and Discussion\nProtT5 PLM was used to generate contextualized embeddings (dimensionality: 1024) for each amino\nacid site by using the full length of OGP protein sequences as an input. Only “S/T” embeddings were sent\nas an input to the OglyPred-PLM to train and predict O-linked glycosylation sites. We utilized an\nexperimentally characterized OGP9 O-linked glycosylation dataset to train OglyPred-PLM. The psi-cd-hit50\ntool (sequence identity threshold of 30%) was applied to eliminate sequence redundancy from both the\nindependent testing and training datasets, mitigating the risk of over\u0000tting as well as maintaining\ndiversity. Furthermore, we conducted a strati\u0000ed 10-fold cross-validation grid search on the OGP training\ndataset to obtain optimal hyperparameters. Finally, we assessed the performance of the trained model\nusing the independent test dataset and conducted a comparative analysis with established methods.\nPerformance of models on The OGP Dataset\n10-fold Cross-Validation on the OGP Training Set with\nProtT5 Features\nPage 5/20\nTo optimize hyperparameters51, we conducted a strati\u0000ed 10-fold cross-validation (CV) on the OGP\ntraining dataset, and the results are presented in Table 1. Notably, the MLP architecture achieved superior\nresults, with a mean MCC, mean ACC, mean SN, mean SP, and mean PRE of 0.664 ± 0.024, 0.831 ± 0.012,\n0.833 ± 0.029, 0.830 ± 0.023, and 0.831 ± 0.016, respectively, for the 10-fold CV. The MLP architecture\nproduced better results than other ML and DL models, thus, we determined MLP as our primary model\nand termed it OglyPred-PLM.\n \nTable 1\nResults of the 10-fold CV on the OGP training dataset using various models when training dataset areencoded with ProtT5 PLM. The most noteworthy values in each column have been emphasized in bold.\nModels MCC ± 1 S.D. ACC ± 1 S.D. SN ± 1 S.D. SP ± 1 S.D. PRE ± 1 S.D.\nLR 0.623 ± 0.041 0.811 ± 0.020 0.814 ± 0.029 0.809 ± 0.014 0.809 ± 0.016\nXGBoost 0.632 ± 0.023 0.816 ± 0.011 0.804 ± 0.013 0.827 ± 0.027 0.824 ± 0.022\nRandom Forest 0.602 ± 0.020 0.799 ± 0.010 0.738 ± 0.019 0.859 ± 0.017 0.840 ± 0.015\nSVM 0.658 ± 0.017 0.829 ± 0.008 0.829 ± 0.015 0.829 ± 0.013 0.829 ± 0.011\nMLP 0.664 ± 0.024 0.831 ± 0.012 0.833 ± 0.029 0.830 ± 0.023 0.831 ± 0.016\n1D CNN 0.651 ± 0.022 0.825 ± 0.011 0.819 ± 0.029 0.831 ± 0.030 0.830 ± 0.021\n10-fold Cross-Validation on the OGP Training Set with Ankh\nFeatures\nWe further explored the utility of the Ankh pre-trained PLM36 via 10-fold CV (type = strati\u0000ed) on the OGP\ntraining dataset with Ankh PLM embeddings (Table 2). The MLP model used contextualized embeddings\nfrom Ankh PLM (feature vector length = 1,536) of the \"S/T\" token which produced impressive results with\nmean MCC, mean ACC, mean SN, mean SP, and mean PRE values of 0.658 ± 0.023, 0.828 ± 0.012, 0.831 ± \n0.032, 0.825 ± 0.036, and 0.828 ± 0.025, respectively. These large, pre-trained PLMs exhibit an increased\ncapacity to capture complex protein patterns, enhancing accuracy and generalization. However, 10-fold\ncross-validation with Ankh PLM slightly underperforms ProtT5 PLM embeddings. Hence, we chose pre-\ntrained ProtT5 PLM to encode the protein sequence.\n \nPage 6/20\nTable 2\nResult of the 10-fold CV on the OGP training dataset using various models when training dataset areencoded with Ankh PLM. The most noteworthy values in each column have been emphasized in bold.\nModels MCC ± 1 S.D. ACC ± 1 S.D. SN ± 1 S.D. SP ± 1 S.D. PRE ± 1 S.D.\nLR 0.614 ± 0.021 0.807 ± 0.010 0.791 ± 0.017 0.821 ± 0.010 0.816 ± 0.009\nXGBoost 0.617 ± 0.024 0.808 ± 0.012 0.784 ± 0.020 0.833 ± 0.011 0.824 ± 0.011\nRandom Forest 0.563 ± 0.027 0.778 ± 0.014 0.696 ± 0.017 0.860 ± 0.014 0.832 ± 0.016\nSVM 0.650 ± 0.035 0.825 ± 0.018 0.820 ± 0.018 0.830 ± 0.023 0.829 ± 0.021\nMLP 0.658 ± 0.023 0.828 ± 0.012 0.831 ± 0.032 0.825 ± 0.036 0.828 ± 0.025\n1D CNN 0.625 ± 0.021 0.812 ± 0.011 0.818 ± 0.029 0.806 ± 0.031 0.809 ± 0.020\n10-fold Cross-Validation on the OGP Training Set with ESM2\n(3B) Features\nTo encompass all PLM performance on O-linked glycosylation PTM prediction, the recently developed\nESM2 (3B parameters) PLM35 last layer’s (layer no. 36) contextualized embedding was also taken into\nconsideration. The feature vector size of ESM2 (3B) is 2,560 in length. The strati\u0000ed 10-fold CV results\nare presented in Table 3. Surprisingly, the MLP architecture produced better results than other ML and DL\nmethods across ProtT5, Ankh and ESM2 (3B) embeddings. The 10-fold CV result of ESM2 (3B) is slightly\nless than ProtT5 and ANKH PLM performance. Hence, we concluded that ProtT5 PLM’s embedding with\nMLP architecture is favorable for independent O-linked glycosylation PTM prediction purposes.\n \nTable 3\nResult of the 10-fold CV on the OGP training dataset using various models when training dataset areencoded with ESM2 (3B) PLM. The most noteworthy values in each column have been emphasized inbold.\nModels MCC ± 1 S.D. ACC ± 1 S.D. SN ± 1 S.D. SP ± 1 S.D. PRE ± 1 S.D.\nLR 0.614 ± 0.034 0.806 ± 0.017 0.816 ± 0.015 0.797 ± 0.027 0.801 ± 0.022\nXGBoost 0.577 ± 0.028 0.788 ± 0.014 0.766 ± 0.018 0.810 ± 0.022 0.802 ± 0.019\nRandom Forest 0.538 ± 0.028 0.763 ± 0.014 0.666 ± 0.026 0.861 ± 0.015 0.828 ± 0.016\nSVM 0.616 ± 0.024 0.808 ± 0.012 0.832 ± 0.017 0.783 ± 0.020 0.793 ± 0.015\nMLP 0.624 ± 0.037 0.811 ± 0.017 0.833 ± 0.029 0.788 ± 0.047 0.799 ± 0.032\n1D CNN 0.612 ± 0.025 0.805 ± 0.012 0.796 ± 0.025 0.814 ± 0.024 0.811 ± 0.017\nTesting on OGP Independent test Dataset with ProtT5\nFeatures\nPage 7/20\nTo assess the performance, we trained OglyPred-PLM on the overall OGP training (controlled) dataset and\nevaluated it with an independent (uncontrolled) OGP O-glycosylation test dataset. All O-linked sites (and\ntheir respective protein sequences) used in OglyPred-PLM training are absent in the independent test\ndataset. Therefore, the model cannot learn representations from any O-linked sites in the independent test\ndataset. The model achieved MCC, ACC, PRE, SN, and SP values of 0.613, 0.806, 0.817, 0.789, and 0.824\n(as shown in Table 4), respectively. The e\u0000ciency measures produced by OglyPred-PLM when the\nindependent test dataset was not under-sampled are shown in Supplementary Table S1. Furthermore,\nOglyPred-PLM classi\u0000ed 308 samples as true negative (TN), 295 as true positive (TP), 66 samples as\nfalse positive (FP) and 79 as false negative (FN). Notably, OglyPred-PLM exhibited the highest AUC and\nPrAUC (Figs. 1A and 1B) compared to other models, a\u0000rming its robustness for O-linked glycosylation\nsite prediction.\n \nTable 4\nPerformance metrics of various trained models on the OGP independenttest set with ProtT5 features. The most noteworthy values in eachcolumn have been emphasized in bold.\nModel MCC ACC SN SP AUC PRE\nLR 0.575 0.785 0.714 0.856 0.785 0.832\nXGBoost 0.576 0.785 0.712 0.858 0.785 0.834\nRandom Forest 0.566 0.775 0.660 0.890 0.775 0.858\nSVM 0.605 0.801 0.751 0.850 0.801 0.832\nMLP 0.613 0.806 0.789 0.824 0.806 0.817\n1D CNN 0.567 0.783 0.783 0.783 0.783 0.783\nTesting on OGP Independent Test Dataset with Ankh\nFeature\nThe performance of the trained OglyPred-PLM, whose protein sequences were encoded by Ankh PLM,\nwas evaluated with an OGP-independent test dataset. This trained model produced MCC, PRE, SN, SP, and\nACC of 0.604, 0.810, 0.789, 0.816, and 0.802 respectively. The model classi\u0000ed the independent test\ndataset (unseen) samples as 305 TN, 295 TP, 69 FP, and 79 FN. Table 5 shows independent test set\nresults of various ML and DL models, whose protein sequences were encoded by ANKH-PLM.\nTo acknowledge the effects of under-sampling, Supplementary Table S2 displays how various models,\nusing Ankh features, performed when the independent test dataset was not under-sampled.\n \nPage 8/20\nTable 5\nPerformance metrics of various models on the OGP unseen testdataset with Ankh features. The most noteworthy values in eachcolumn have been emphasized in bold.\nModels MCC ACC SN SP PRE\nLR 0.599 0.798 0.743 0.853 0.835\nXGBoost 0.568 0.781 0.706 0.856 0.830\nRandom Forest 0.532 0.754 0.604 0.904 0.863\nSVM 0.584 0.789 0.714 0.864 0.840\nMLP 0.604 0.802 0.789 0.816 0.810\n1D CNN 0.591 0.795 0.789 0.802 0.799\nTesting on OGP Independent Test Dataset with ESM2 (3B)\nFeature\nThe trained models with ESM2 (3B) PLM35 features were assessed with the independent test dataset.\nTable 6 elaborates on the performance of the model with the independent test dataset. All the ML and DL\nmodels faired similarly. However, the results from ESM2 (3B) embeddings were still less than ProtT5 and\nANKH PLM. The performance metrices, when the independent test dataset was not under sampled, are\nshown in Supplementary Table S3. Additionally, Supplementary Table S4 shows the confusion matrix\nproduced by OglyPred-PLM, also without under-sampling, using ProtT5, Ankh and ESM2 (3B) features.\nFurthermore, based upon MCC Table 7 elaborates that OglyPred-PLM trained with ProtT5 PLM feature\nrepresentation improves upon that of ANKH and ESM2 (3B) PLM.\n \nTable 6\nPerformance metrices of various models on the OGP unseentest dataset with ESM2 (3B) features. The most noteworthyvalues in each column have been emphasized in bold.\nModels MCC ACC SN SP PRE\nLR 0.586 0.793 0.791 0.795 0.794\nXGBoost 0.586 0.790 0.720 0.860 0.837\nRandom Forest 0.535 0.754 0.599 0.909 0.869\nSVM 0.600 0.799 0.770 0.829 0.818\nMLP 0.602 0.801 0.795 0.807 0.805\n1D CNN 0.598 0.799 0.783 0.815 0.809\nPage 9/20\n \nTable 7\nPrediction performance of OglyPred-PLM with ProtT5,ANKH and ESM2 (B) features on the unseen test dataset.The most noteworthy values in each column have beenemphasized in bold.\nPLM MCC ACC SN SP PRE\nProtT5 0.613 0.806 0.789 0.824 0.806\nAnkh 0.604 0.802 0.789 0.816 0.810\nESM2 (3B) 0.602 0.801 0.795 0.807 0.805\nVisualization Using t-distributed stochastic neighbor\nembedding Plot\nThe t-SNE52 method was used to discern the classi\u0000cation effectiveness of the embeddings from ProtT5\nPLM and the second to last fully connected layer of the OglyPred-PLM model. This method projects these\nfeatures into a two-dimensional space to identify class boundaries. The t-SNE's learning rate was set to a\nvalue of 50 to visualize the scatter plot from ProtT5 features and thirty-two-dimensional feature vectors\nobtained from the second to last fully connected layer of the trained OglyPred-PLM framework. This\nlearning rate was selected because it performed best among the range of learning rates considered,\nwhich was from 10 to 200 with a step size of 10. These plots were created using randomly selected set of\n9,770 data points, evenly split between 4,885 negative and 4,885 positive samples.\nO-linked glycosylated and non-O-linked glycosylated features were extracted from the ProtT5 PLM. Figure\n2 (A) illustrates clusters of positive and negative samples extracted from ProtT5 PLM; however, the class\nboundaries of the positive and negative samples remain indistinct. Figure 2 (B) presents the scatter or the\nt-SNE plot of the features produced from the second to last fully connected layer of the trained OglyPred-\nPLM framework, where the positive samples (orange points) and negative samples (blue points) are\ndistinctly clustered. Figure 2 (B) shows that the pre-trained per residue PLM feature embeddings from the\nsite of interest, along with the trained MLP framework, is capable of learning O-linked glycosylation\npatterns and thus can classify negative and positive samples in two-dimensional space.\nComparison of OglyPred-PLM with other O-linked Glycosylation Site Predictors\nTo assess the performance of OglyPred-PLM against other established predictors, we trained our model\non the OGP training dataset which excludes both the independent test dataset and SPRINT-Gly\nindependent test dataset. Moreover, to avoid possible bias, SPRINT-Gly21 used an under-sampling method\nto reduce the ratio of positive to negative samples to 1:3 for training. Hence, we also trained OglyPred-\nPLM with a 1:3 ratio. The trained model was evaluated without an under-sampled SPRINT-Gly\nindependent test dataset. Our model produced MCC, ACC, SN, SP, and PRE values of 0.297, 0.857, 0.873,\n0.856, and 0.124, respectively (as shown in Table 8). The results yielded by the model are better than the\nPage 10/20\nstructural and sequential feature-based SPRING-Gly method, making the detection of O-linked\nglycosylation sites more e\u0000cient. It should be noted that OglyPred-PLM has remarkable positive O-linked\nglycosylation detection capability (87.3%) compared to SPRINT-GLY (32.9%). Additionally, the confusion\nmatrix of the MLP architecture shows that the model detected 2,892 samples as TN and 69 as TP.\nHowever, it erroneously classi\u0000ed 484 samples as FP and 10 as FN. The results for the consensus-based\nmodel, GPP, GlycoMine, NetOGlyc and GlycoPP were adopted from SPRINT-Gly.\nMoreover, to compare our method with other quintessential predictors like OGP and Captor, we removed\nthe protein sequences from the OGP training dataset by excluding all O-linked and non-O-linked\nglycosylation sites that are present in the Captor independent test dataset and \u0000tted the model with the\nresulting training dataset. We examined the trained model with the Captor independent test dataset. Table\n9 reveals that the OglyPred-PLM results are better than Captor and OGP predictors (superior positive O-\nlinked glycosylation detection capability). OglyPred-PLM was successful at classifying 1,057 samples as\nTN and 261 samples as TP. However, it falsely classi\u0000ed 250 samples as FP and 79 samples as FN. It\nshould be noted that OglyPred-PLM, Captor and OGP methods were calibrated and examined with the\nsame experimentally veri\u0000ed OGP dataset.\n \nTable 8\nPrediction performance of OglyPred-PLM compared to other existing O-linkedglycosylation site predictors, evaluated on the SPRINT-Gly independent test set.The most noteworthy values in each column have been emphasized in bold.\nMethods MCC AUC ACC SN SP PRE\nOglyPred-PLM 0.297 0.865 0.857 0.873 0.856 0.124\nSPRINT-Gly 0.191 0.824 0.940 0.329 0.954 0.144\nConsensus-based model 0.042 0.523 0.227 0.223 0.216 0.018\nGPP 0.047 0.578 0.489 0.671 0.485 0.030\nGlycoMine 0.100 0.667 0.978 0.027 0.999 0.400\nNetOGlyc 0.128 0.749 0.633 0.785 0.629 0.047\nGlycoPP 0.061 0.538 0.973 0.038 0.994 0.136\n \nPage 11/20\nTable 9\nPrediction performance of OglyPred-PLM compared to other existing O-linked glycosylation site predictors on the Captor independent test set.The most noteworthy values in each column have been emphasized inbold.\nMethods MCC AUC ACC SN SP PRE\nOglyPred-PLM 0.504 0.788 0.800 0.768 0.809 0.511\nCaptor 0.278 0.801 0.795 0.619 0.798 -\nOGP 0.160 0.715 0.780 0.485 0.796 -\nFurthermore, to exhaustively compare OglyPred-PLM with PLM-based (TAPE) predictor we compared our\nresults with this recently developed Alkuhlani et al. O-linked glycosylation method. Again, OglyPred-PLM\nshows improved results over their PLM-based method. These results are presented in Table 10.\n \nTable 10\nPrediction performance of OglyPred-PLM compared to Alkuhlani et al.PLM-based (TAPE) O-linked glycosylation site predictor on the Alkuhlaniet al. independent test set. The most noteworthy values in each columnhave been emphasized in bold.\nMethods MCC AUC ACC SN SP PRE\nOglyPred-PLM 0.614 0.807 0.807 0.817 0.797 0.801\nAlkuhlani et al. 0.553 0.829 0.776 0.739 0.813 -\nConclusion\nIn this work, an MLP-based model is used to detect O-linked glycosylation sites in amino acid sequences\nof proteins. OglyPred-PLM utilizes an embedding from pre-trained PLM (ProtT5) to encode amino acid\nsequences. The uniqueness of this approach is the use of contextualized embedding of the site of\ninterest from a pre-trained PLM (ProtT5) and the use of a high-quality experimentally characterized O-\nlinked glycosylation data set. The unseen test dataset results and comparison with other approaches\nshow that OglyPred-PLM achieves more favorable performance over other approaches. Hence, it can be\nconcluded that OglyPred-PLM is a trustworthy human O-linked glycosylation site prediction tool that\nexceeds prior tools.\nThe experiments demonstrate that the improvement in the performance of the OglyPred-PLM is due to the\nfollowing three factors: (i) the use of site of interest (“S/T”) embeddings extracted from the ProtT5 PLM\ncontextualized \u0000le (ProtT5 PLM was fed with the entire protein .fasta \u0000le), (ii) training with a high-quality\nexperimentally veri\u0000ed OGP database and (iii) the use of the well-trained and generalized MLP\narchitecture. The t-SNE plot (Fig. 2 (B)) shows our trained model (OglyPred-PLM) can cluster O-linked and\nnon-O-linked glycosylation serine or threonine residues in two-dimensional space. For any protein\nPage 12/20\nsequence, these ProtT5 PLMs embeddings can be easily extracted hence OglyPred-PLM provides fast\nand reliable predictions of O-linked glycosylation sites.\nWe foresee three avenues of future research to improve the performance of methods that utilize PLMs for\nO-linked and N-linked glycosylation PTM site prediction: (i) combining PLM features with other\nphysicochemical features, (ii) using structural information predicted by AlphaFold253,54 or ESMFold55 to\nbuild models that utilize graph networks56 and (iii) taking representative negative O-linked glycosylation\nsites from proteins dwelling within the Golgi apparatus subcellular localization of human cells. Finally,\nwith the development of in\u0000uential PLMs in the future, the prediction performance of the methods that\nmake use of embeddings from these PLMs is more likely to improve.\nMethods\nData Set\nO-glycoprotein repository, known as OGP9, has the largest experimentally veri\u0000ed dataset for our research.\nIt contains more than 9,354 O-linked glycosylation sites. We used this repository to extract 1,474 human\nO-linked glycoproteins to train and evaluate our models. Four proteins from the OGP repository were\nomitted due to their absence in the UniProt database. We used a psi-cd-hit50 tool to remove homologous\nglycoproteins that share 30 percent sequence similarity or more resulting in a subset of 1,176\nglycoproteins. To avoid overestimation of performance, we made sure that no glycoproteins sequences\nfrom the independent test dataset were present in the training dataset as the PLM can learn its\nrepresentation for other sites from the same glycoproteins46. Moreover, we split 1,176 glycoproteins into\n1,059 training and 117 independent test glycoproteins. The annotated O-linked glycosylation sites from\nOGP database were de\u0000ned as positive sites, and any remaining S or T sites from same glycoproteins\nwere deemed as putative negative sites. Table 11 presents the total number of positive and negative sites\nin the training and independent test datasets. To mitigate any biases toward the negative majority class,\nwe balanced both the training and independent test dataset via under-sampling57.\n \nTable 11\nPositive and Negative O-linked Glycosylation Sites forTraining and Independent Testing.\nData set Number of Proteins Positive Negative\nTraining 1,059 4,885 114,144\nTest 117 374 11,466\nFeature Encoding\nAn important process while developing a ML/DL model for the prediction of protein O-linked\nglycosylation sites is representing the primary amino acids with \u0000xed size pertinent feature embeddings\nPage 13/20\nvia a suitable encoding scheme48,58. We employed a pretrained PLM to generate per-residue\ncontextualized embeddings for this investigation.\nPer-Residue Contextualized Embedding from ProtT5\nThe ProtT5-XL-UniRef50 PLM was used to generate a contextualized per-residue embedding for target\n(“S/T”) sites. ProtT5 is a self-supervised learning model based on the T5 architecture59. The self-\nsupervised model learns from the data itself, creating its own supervision signals without the need of\nexternal labels. Moreover, ProtT5 is trained with 2.5 billion unlabeled protein sequences from the BFD60,61\nand Uni-Ref5062 databases. The entire protein sequence was fed into the pre-trained ProtT5 PLM to\nproduce a contextualized per-residue feature embedding. Subsequently, site-of-interest (“S/T”)\nembeddings were extracted and fed into the MLP architecture.\nModel for Contextualized Embedding Obtained from ProtT5\nThe per-residue contextualized embedding produced from the last encoder layer of PLMs was used. The\nembeddings of the site of interest (“S/T”) were passed as an input to the MLP classi\u0000cation model.\nFurthermore, to select the appropriate embedding among ESM2 (3B parameters), ProtT5, and Ankh PLM,\nwe performed a 10-fold CV on the training dataset. The 10-fold CV exhibits (as shown in the results\nsection) that the ProtT5 PLM embeddings have superior protein representation compared to other PLMs\nconsidered. Figure 3 illustrates the pipeline used in this work. Moreover, ProtT5, ESM2 (3B), and ANKH\nPLM use rotary positional embeddings63 and can generalize sequences longer than 1022 amino acids.\nThe functionality of the OglyPred-PLM method is implemented by the MLP architecture which accepts\nProtT5 representations of target sites (1024-length feature vectors) as input. The classi\u0000ers were\nconstructed using Tensor\u0000ow64 platform. The MLP architecture consists of three fully connected hidden\nlayers with dimensions of 512, 256, and 32 respectively. Following each layer (except the \u0000nal one), the\nrecti\u0000ed linear unit (ReLU) activation function is used. To avoid over-\u0000tting, a dropout rate of 0.3 was\nused after each fully connected layer. Moreover, techniques to avoid over\u0000tting such as early stopping,\nModelCheckpoint and reduce learning rate on plateau were used. The output layer uses sigmoid\nactivation function with two neurons. This layer determines whether the given “S/T” amino acid is O-\nlinked glycosylated ( ≥  0.5) or non-O-linked glycosylated (< 0.5). A binary classi\u0000er was trained using the\nmini-batch training principle (with a batch size of 256) and Adam optimizer65 with a learning rate of\n0.001. Since the datasets were imbalanced, the training and independent test datasets were loaded using\nrandom under-sampling. The MLP binary classi\u0000er was validated by assessing performance on the\nvalidation set after every training epoch and the model that achieved the highest MCC was selected. The\ntrained model was then tested on the independent test (imbalanced or balanced) dataset, MCC and other\nbinary classi\u0000cation metrics were reported. The hyperparameters used in OglyPred-PLM are shown in\nSupplementary Table S5.\nDeclarations\nPage 14/20\nData availability\nThe developed tool, training and test data are available at https://github.com/PakhrinLab/OglyPred-PLM\nFunding\nThis work was supported by the faculty start-up fund provided to S.C.P. by U.H.D.\nAcknowledgment\nWe acknowledge the use of the BeoShock and Beocat High-Performance Computing resources located at\nWichita State University and Kansas State University. We appreciate the email discussions with Dr.\nCangzhi Jia regarding their generated Captor dataset. In addition, we would also like to acknowledge the\nOGPdataset, ProtT5-XL-UniRef50, ESM2 and Ankh Protein Language Model made freely available to\nresearchers. \nAuthor Contributions\nS.C.P., M.R.B., and E.B. conceived and designed the experiments; S.C.P., N.C., and S.K. performed all the\nexperiments and data analysis. J.U., L.N.N., and C.K. veri\u0000ed all the programs and models. S.C.P., N.C.,\nS.K., J.U., L.N.N., C.K., M.R.B., E.B. edited and revised the manuscript. All authors have read and agreed to\nthe published version of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nReferences\n1. Yang, X.-m. in Advanced Research on Computer Education, Simulation and Modeling. (eds Song Lin\n& Xiong Huang) 445–450 (Springer Berlin Heidelberg).\n2. Colley, K. J., Varki, A. & Kinoshita, T. in Essentials of Glycobiology (eds A. Varki et al.) 41–49 (2015).\n3. Wolfert, M. A. & Boons, G. J. Adaptive immune activation: glycosylation does matter. Nat Chem Biol\n9, 776–784, doi:10.1038/nchembio.1403 (2013).\n4. Boskovski, M. T. et al. The heterotaxy gene GALNT11 glycosylates Notch to orchestrate cilia type and\nlaterality. Nature 504, 456–459, doi:10.1038/nature12723 (2013).\n5. Chen, Y., Zhou, W., Wang, H. & Yuan, Z. Prediction of O-glycosylation sites based on multi-scale\ncomposition of amino acids and feature selection. Med Biol Eng Comput 53, 535–544,\ndoi:10.1007/s11517-015-1268-9 (2015).\n\u0000. Campos, D. et al. Probing the O-glycoproteome of gastric cancer cell lines for biomarker discovery.\nMol Cell Proteomics 14, 1616–1629, doi:10.1074/mcp.M114.046862 (2015).\nPage 15/20\n7. Agarwal, K. L., Kenner, G. W. & Sheppard, R. C. Feline gastrin. An example of peptide sequence\nanalysis by mass spectrometry. J Am Chem Soc 91, 3096–3097, doi:10.1021/ja01039a051 (1969).\n\u0000. Medzihradszky, K. F. Peptide sequence analysis. Methods Enzymol 402, 209–244,\ndoi:10.1016/S0076-6879(05)02007-0 (2005).\n9. Huang, J. et al. OGP: A Repository of Experimentally Characterized O-glycoproteins to Facilitate\nStudies on O-glycosylation. Genomics Proteomics Bioinformatics 19, 611–618,\ndoi:10.1016/j.gpb.2020.05.003 (2021).\n10. KC, D. B. Computational Methods for Predicting Post-Translational Modi\u0000cation Sites. (Springer US,\n2022).\n11. Julenius, K., Mølgaard, A., Gupta, R. & Brunak, S. Prediction, conservation analysis, and structural\ncharacterization of mammalian mucin-type O-glycosylation sites. Glycobiology 15, 153–164,\ndoi:10.1093/glycob/cwh151 (2005).\n12. Klausen, M. S. et al. NetSurfP-2.0: Improved prediction of protein structural features by integrated\ndeep learning. Proteins 87, 520–527, doi:10.1002/prot.25674 (2019).\n13. Heffernan, R., Yang, Y., Paliwal, K. & Zhou, Y. Capturing non-local interactions by long short-term\nmemory bidirectional recurrent neural networks for improving prediction of protein secondary\nstructure, backbone angles, contact numbers and solvent accessibility. Bioinformatics 33, 2842–\n2849, doi:10.1093/bioinformatics/btx218 (2017).\n14. Li, S., Liu, B., Zeng, R., Cai, Y. & Li, Y. Predicting O-glycosylation sites in mammalian proteins by using\nSVMs. Comput Biol Chem 30, 203–208, doi:10.1016/j.compbiolchem.2006.02.002 (2006).\n15. Caragea, C., Sinapov, J., Silvescu, A., Dobbs, D. & Honavar, V. Glycosylation site prediction using\nensembles of Support Vector Machine classi\u0000ers. BMC Bioinform. 8 (2007).\n1\u0000. Chen, Y. Z., Tang, Y. R., Sheng, Z. Y. & Zhang, Z. Prediction of mucin-type O-glycosylation sites in\nmammalian proteins using the composition of k-spaced amino acid pairs. BMC Bioinformatics 9,\n101, doi:10.1186/1471-2105-9-101 (2008).\n17. Chauhan, J. S., Bhat, A. H., Raghava, G. P. & Rao, A. GlycoPP: a webserver for prediction of N- and O-\nglycosites in prokaryotic protein sequences. PLoS One 7, e40155, doi:10.1371/journal.pone.0040155\n(2012).\n1\u0000. Li, F. et al. GlycoMine: a machine learning-based approach for predicting N-, C- and O-linked\nglycosylation in the human proteome. Bioinformatics 31, 1411–1419,\ndoi:10.1093/bioinformatics/btu852 (2015).\n19. Bekker, J. & Davis, J. Learning from positive and unlabeled data: a survey. Machine Learning 109,\n719–760, doi:10.1007/s10994-020-05877-5 (2020).\n20. Li, F., Zhang, Y., Purcell, A. W. W., Geoffrey I. Chou, Kuo-Chen Lithgow, Trevor, Li, C. & Song, J. Positive-\nunlabelled learning of glycosylation sites in thehuman proteome. BMC Bioinform. 20, 112 (2019).\n21. Taherzadeh, G., Dehzangi, A., Golchin, M., Zhou, Y. & Campbell, M. P. SPRINT-Gly: Predicting N- and O-\nlinked glycosylation sites of human and mouse proteins by using sequence and predicted structural\nproperties. Bioinformatics 4140–4146. (2019).\nPage 16/20\n22. Zhu, Y., Yin, S., Zheng, J., Shi, Y. & Jia, C. O-glycosylation site prediction for Homo sapiens by\ncombining properties and sequence features with support vector machine. J Bioinform Comput Biol\n20, 2150029, doi:10.1142/s0219720021500293 (2022).\n23. Kawashima, S. et al. AAindex: amino acid index database, progress report 2008. Nucleic Acids Res\n36, D202-205, doi:10.1093/nar/gkm998 (2008).\n24. Alkuhlani, A., Gad, W., Roushdy, M. & Salem, A.-B. Prediction Of O-Glycosylation Site Using Pre-\nTrained Language Model And Machine Learning. International Journal of Intelligent Computing and\nInformation Sciences 23, 41–52, doi:10.21608/ijicis.2023.160986.1218 (2023).\n25. Rao, R. B., Nicholas et al. in Adv Neural Inf Process Syst (2019).\n2\u0000. Hamby, S. E. & Hirst, J. D. Prediction of glycosylation sites using random forests. BMC\nBioinformatics 9, 500, doi:10.1186/1471-2105-9-500 (2008).\n27. Li, F. et al. GlycoMine(struct): a new bioinformatics tool for highly accurate mapping of the human N-\nlinked and O-linked glycoproteomes by incorporating structural features. Sci Rep 6, 34595,\ndoi:10.1038/srep34595 (2016).\n2\u0000. Pakhrin, S. C., Aoki-Kinoshita, K. F., Caragea, D. & Kc, D. B. DeepNGlyPred: A Deep Neural Network-\nBased Approach for Human N-Linked Glycosylation Site Prediction. Molecules 26, 7314,\ndoi:10.3390/molecules26237314 (2021).\n29. Dhakal, A., Gyawali, R., Wang, L. & Cheng, J. A large expert-curated cryo-EM image dataset for\nmachine learning protein particle picking. Scienti\u0000c Data 10, 392, doi:10.1038/s41597-023-02280-2\n(2023).\n30. Pakhrin, S. C., Pokharel, S., Saigo, H. & Kc, D. B. Deep Learning-Based Advances In Protein\nPosttranslational Modi\u0000cation Site and Protein Cleavage Prediction. 2022/06/14 edn, Vol. 2499\n(2022).\n31. Pakhrin, S. C., Shrestha, B., Adhikari, B. & Kc, D. B. Deep Learning-Based Advances in Protein\nStructure Prediction. Int J Mol Sci 22, doi:10.3390/ijms22115553 (2021).\n32. Unsal, S. et al. Learning functional properties of proteins with language models. Nature Machine\nIntelligence 4, 227–245, doi:10.1038/s42256-022-00457-9 (2022).\n33. Elnaggar, A. et al. ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised\nDeep Learning and High Performance Computing. IEEE Trans Pattern Anal Mach Intell PP,\ndoi:10.1109/TPAMI.2021.3095381 (2021).\n34. Vaswani, A. e. a. Attention is all you need. In Proceedings of 31st International Conference on Neural\nInformation Processing Systems (NIPS 2017) 1, 6000–6010 (2017).\n35. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model.\nScience 379, 1123–1130, doi:10.1126/science.ade2574 (2023).\n3\u0000. Elnaggar, A. et al. Ankh â¥: Optimized Protein Language Model Unlocks General-Purpose Modelling.\nbioRxiv, 2023.\n2001.2016.524265\n, doi:10.1101/2023.01.16.524265 (2023).\nPage 17/20\n37. UniProt, C. UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Res 49, D480-D489,\ndoi:10.1093/nar/gkaa1100 (2021).\n3\u0000. Pakhrin, S. C. Deep learning-based approaches for prediction of post-translational modi\u0000cation sites\nin proteins, Wichita State University, (2022).\n39. Weissenow, K., Heinzinger, M. & Rost, B. Protein language-model embeddings for fast, accurate, and\nalignment-free protein structure prediction. Structure 30, 1169–1177 e1164,\ndoi:10.1016/j.str.2022.05.001 (2022).\n40. Nallapareddy, V. et al. CATHe: Detection of remote homologues for CATH superfamilies using\nembeddings from protein language models. bioRxiv, doi:10.1101/2022.03.10.483805 (2022).\n41. Littmann, M., Heinzinger, M. & Dallago, C. Protein embeddings and deep learning predict binding\nresidues for various ligand classes. Scienti\u0000c reports 11, doi:https://doi.org/10.1038/s41598-021-\n03431-4 (2021).\n42. Zhang, S. et al. Applications of transformer-based language models in bioinformatics: a survey.\nBioinformatics Advances 3, doi:10.1093/bioadv/vbad001 (2023).\n43. Heinzinger, M. et al. Contrastive learning on protein embeddings enlightens midnight zone. NAR\nGenom Bioinform 4, lqac043, doi:10.1093/nargab/lqac043 (2022).\n44. Song, Y. et al. Fast and accurate protein intrinsic disorder prediction by using a pretrained language\nmodel. Brief Bioinform, doi:10.1093/bib/bbad173 (2023).\n45. Teufel, F. et al. SignalP 6.0 predicts all \u0000ve types of signal peptides using protein language models.\nNat Biotechnol 40, 1023–1025 doi:10.1038/s41587-021-01156-3 (2022).\n4\u0000. Pakhrin, S. C. et al. LMNglyPred: prediction of human N-linked glycosylation sites using embeddings\nfrom a pre-trained protein language model. Glycobiology, doi:10.1093/glycob/cwad033 (2023).\n47. Thumuluri, V., Almagro Armenteros, J. J., Johansen, A. R., Nielsen, H. & Winther, O. DeepLoc 2.0: multi-\nlabel subcellular localization prediction using protein language models. Nucleic Acids Res 50, W228-\nW234, doi:10.1093/nar/gkac278 (2022).\n4\u0000. Pakhrin, S. C. et al. LMPhosSite: A Deep Learning-Based Approach for General Protein\nPhosphorylation Site Prediction Using Embeddings from the Local Window Sequence and Pretrained\nProtein Language Model. J Proteome Res 22, 2548–2557, doi:10.1021/acs.jproteome.2c00667\n(2023).\n49. Høie, M. H. et al. NetSurfP-3.0: accurate and fast prediction of protein structural features by protein\nlanguage models and deep learning. Nucleic Acids Res 50, W510-W515, doi:10.1093/nar/gkac439\n(2022).\n50. Huang, Y., Niu, B., Gao, Y., Fu, L. & Li, W. CD-HIT Suite: a web server for clustering and comparing\nbiological sequences. Bioinformatics 26, 680–682, doi:10.1093/bioinformatics/btq003 (2010).\n51. Yang, L. & Shami, A. On hyperparameter optimization of machine learning algorithms: Theory and\npractice. Neurocomputing 415, 295–316, doi:10.1016/j.neucom.2020.07.061 (2020).\n52. Maaten, L. v. d. & Hinton, G. Visualizing Data using t-SNE. Mach. Learn. Res. 9, 2579–2605 (2008).\nPage 18/20\n53. Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589,\ndoi:10.1038/s41586-021-03819-2 (2021).\n54. Yuan, Q. et al. AlphaFold2-aware protein-DNA binding site prediction using graph transformer. Brief\nBioinform 23, doi:10.1093/bib/bbab564 (2022).\n55. Lin, Z. et al. Language models of protein sequences at the scale of evolution enable accurate\nstructure prediction. bioRxiv, 2022.2007.2020.500902, doi:10.1101/2022.07.20.500902 (2022).\n5\u0000. Yuan, Q., Chen, J., Zhao, H., Zhou, Y. & Yang, Y. Structure-aware protein-protein interaction site\nprediction using deep graph convolutional network. Bioinformatics 38, 125–132,\ndoi:10.1093/bioinformatics/btab643 (2021).\n57. Lemaitre, G., Nogueira, F. & Aridas, C. K. Imbalanced-learn: A Python Toolbox to Tackle the Curse of\nImbalanced Datasets in Machine Learning. J. Mach. Learn. Res. 18, 559–563 (2017).\n5\u0000. Y. Xu, Y.-X. D., J. Ding, Y.-H. Lei, L.-Y. Wu, N.-Y. Deng. iSuc-PseAAC: predicting lysine succinylation in\nproteins by incorporating peptide position-speci\u0000c propensity. Sci. Rep. 5, 10184 (2015).\n59. Raffel, C. et al. Exploring the limits of transfer learning with a uni\u0000ed text-to-text transformer. J.\nMach. Learn. Res. 21, 1–67 (2020).\n\u00000. Steinegger, M., Mirdita, M. & Soding, J. Protein-level assembly increases protein sequence recovery\nfrom metagenomic samples manyfold. Nat Methods 16, 603–606, doi:10.1038/s41592-019-0437-4\n(2019).\n\u00001. Steinegger, M. & Söding, J. Clustering huge protein sequence sets in linear time. Nature\nCommunications 9, 2542, doi:10.1038/s41467-018-04964-5 (2018).\n\u00002. Suzek, B. E., Wang, Y., Huang, H., McGarvey, P. B. & Wu, C. H. UniRef clusters: a comprehensive and\nscalable alternative for improving sequence similarity searches. Bioinformatics 31, 926–932,\ndoi:10.1093/bioinformatics/btu739 (2015).\n\u00003. Su, J. et al. ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING. arXiv\n(2022).\n\u00004. Abadi, M. et al. Tensor\u0000ow: A System for Large-Scale Machine Learning. 12th Symposium on\nOperating Systems Design and Implementation, 265–283 (2016).\n\u00005. Kingma, D. P. B., J. Adam: A Method for Stochastic Optimization. arXiv e-prints,\ndoi:https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K (2014).\n\u0000\u0000. Südhof, T. C. The cell biology of synapse formation. J Cell Biol 220, doi:10.1083/jcb.202103052\n(2021).\nFigures\nPage 19/20\nFigure 1\n (A) Comparisons of ROC curves of   OglyPred-PLM and other models on the OGP independent test\ndataset. Each   model’s area under the ROC curve is reported. (B) Comparisons of   precision-recall curves\nof OglyPred-PLM and other models on the OGP   independent test dataset. Each model’s area under the\nPrAUC curve is   reported.\nFigure 2\n t-SNE visualization  of the acquired features. (A) Features derived from the ProtT5 PLM, (B)   learned\nfeatures from the trained MLP model (OglyPred-PLM).\nPage 20/20\nFigure 3\nThe overall framework of OglyPred-PLM. Beads with letters represent protein sequences. The sky-colored\nrectangular box represents ProtT5 PLM. Green rectangular boxes are 1024 per residue embeddings\nproduced by ProtT5 PLM. Empty circles represent neurons. Each neuron is connected to other nodes via\nlinks like a biological axon synapse-dendrite connection66. A dropout of 0.3 means that 30 % of neurons\nare switched off randomly while training the MLP.\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nOglyPredPLMScienti\u0000cReportsSupplementary.docx",
  "topic": "Glycosylation",
  "concepts": [
    {
      "name": "Glycosylation",
      "score": 0.8599385023117065
    },
    {
      "name": "Threonine",
      "score": 0.7913886308670044
    },
    {
      "name": "Serine",
      "score": 0.6763458251953125
    },
    {
      "name": "Biochemistry",
      "score": 0.5372222661972046
    },
    {
      "name": "Glycan",
      "score": 0.4759399890899658
    },
    {
      "name": "Chemistry",
      "score": 0.41505953669548035
    },
    {
      "name": "Computational biology",
      "score": 0.4020419716835022
    },
    {
      "name": "Phosphorylation",
      "score": 0.32935529947280884
    },
    {
      "name": "Biology",
      "score": 0.32561376690864563
    },
    {
      "name": "Glycoprotein",
      "score": 0.26164698600769043
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16277215",
      "name": "University of Houston - Downtown",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I39587148",
      "name": "Wichita State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I138006243",
      "name": "University of Arizona",
      "country": "US"
    }
  ]
}