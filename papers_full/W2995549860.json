{
  "title": "From English To Foreign Languages: Transferring Pre-trained Language Models",
  "url": "https://openalex.org/W2995549860",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288218358",
      "name": "Tran, Ke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2148708890",
    "https://openalex.org/W2953109491",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W2963877297",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2561995736",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2982180741",
    "https://openalex.org/W2970803838",
    "https://openalex.org/W2891896107",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2786790428",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963247703",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W2963123301",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "Pre-trained models have demonstrated their effectiveness in many downstream natural language processing (NLP) tasks. The availability of multilingual pre-trained models enables zero-shot transfer of NLP tasks from high resource languages to low resource ones. However, recent research in improving pre-trained models focuses heavily on English. While it is possible to train the latest neural architectures for other languages from scratch, it is undesirable due to the required amount of compute. In this work, we tackle the problem of transferring an existing pre-trained model from English to other languages under a limited computational budget. With a single GPU, our approach can obtain a foreign BERT base model within a day and a foreign BERT large within two days. Furthermore, evaluating our models on six languages, we demonstrate that our models are better than multilingual BERT on two zero-shot tasks: natural language inference and dependency parsing.",
  "full_text": "From English to Foreign Languages:\nTransferring Pretrained Language Models\nKe Tran\nAmazon Alexa AI\ntrnke@amazon.com\nAbstract\nPretrained models have demonstrated their ef-\nfectiveness in many downstream natural lan-\nguage processing (NLP) tasks. The availabil-\nity of multilingual pretrained models enables\nzero-shot transfer of NLP tasks from high re-\nsource languages to low resource ones. How-\never, recent research in improving pretrained\nmodels focuses heavily on English. While it\nis possible to train the latest neural architec-\ntures for other languages from scratch, it is un-\ndesirable due to the required amount of com-\npute. In this work, we tackle the problem\nof transferring an existing pretrained model\nfrom English to other languages under a lim-\nited computational budget. With a single GPU,\nour approach can obtain a foreign BERT BASE\nmodel within a day (20 hours) and a foreign\nBERTLARGE within two days (46 hours). Fur-\nthermore, evaluating our models on six lan-\nguages, we demonstrate that our models are\nbetter than multilingual BERT on two zero-\nshot tasks: natural language inference and de-\npendency parsing. Our code is available at\nhttps://github.com/anonymized.\n1 Introduction\nPretrained models (Devlin et al., 2019; Peters et al.,\n2018) have received much of attention recently\nthanks to their impressive results in many down-\nstream NLP tasks. Additionally, multilingual pre-\ntrained models enable many NLP applications for\nother languages via zero-short cross-lingual trans-\nfer. Zero-shot cross-lingual transfer has shown\npromising results for rapidly building applications\nfor low resource languages. Wu and Dredze (2019)\nshow the potential of multilingual BERT (Devlin\net al., 2019) in zero-shot transfer for a large number\nof languages from different language families on\nﬁve NLP tasks, namely, natural language inference,\ndocument classiﬁcation, named entity recognition,\npart-of-speech tagging, and dependency parsing.\nAlthough multilingual models are an important\npiece for building up language technology in many\nlanguages, recent research on improving pretrained\nmodels puts much emphasis on English (Radford\net al., 2019; Dai et al., 2019; Yang et al., 2019).\nThe current state of affairs makes it difﬁcult to\ntranslate developments in pre-training from English\nto non-English languages. To our best knowledge,\nthere are only three publicly available multilingual\npretrained models to date:\n1. The multilingual BERT (mBERT) model (De-\nvlin et al., 2019) that supports 104 languages;\n2. Cross-lingual language model (XLM-R) 1\n(Lample and Conneau, 2019; Conneau et al.,\n2019) that supports 100 languages;\n3. Language Agnostic SEntence Representa-\ntions model (LASER)2 (Artetxe and Schwenk,\n2019) that supports 93 languages.\nAmong the three models, LASER is based on neu-\nral machine translation approach and strictly re-\nquires parallel data to train.\nA common practice to train a large-scale mul-\ntilingual model is to do so from scratch. But do\nmultilingual models always need to be trained from\nscratch? Can we transfer linguistic knowledge\nlearned by English pretrained models to other lan-\nguages? In this work, we develop a technique to\nrapidly transfer an existing pretrained model from\nEnglish to other languages. As the ﬁrst step, we\nfocus on building a bilingual language model (LM)\nof English and a target language. Starting from\na pretrained English LM, we learn the target lan-\nguage speciﬁc parameters (i.e., word embeddings),\nwhile keeping the encoder layers of the pretrained\n1https://github.com/facebookresearch/\nXLM\n2https://github.com/facebookresearch/\nLASER\narXiv:2002.07306v2  [cs.CL]  29 Apr 2020\nEnglish LM ﬁxed. We then ﬁne-tune both English\nand target model to obtain the bilingual LM. We ap-\nply our approach to autoencoding language models\nwith masked language model objective and show\nthe advantage of the proposed approach in zero-\nshot transfer. Our main contributions in this work\nare:\n•We propose a fast adaptation method for ob-\ntaining a bilingual BERTBASE of English and\na target language within a day using one Tesla\nV100 16GB GPU (§3).\n•We evaluate our bilingual LMs for six lan-\nguages on two zero-shot cross-lingual trans-\nfer tasks, namely natural language inference\n(XNL) (Conneau et al., 2018) and universal\ndependency parsing. We show that our models\noffer competitive performance or even better\nthat mBERT (§4).\n•We illustrate that our bilingual LMs can serve\nas an excellent feature extractor in supervised\ndependency parsing task (§5).\nConcurrent to our work, Artetxe et al. (2019)\ntransfer pretrained English model to other lan-\nguages by ﬁne-tuning only randomly initialized\ntarget word embeddings while keeping the Trans-\nformer encoder ﬁxed. Their approach is simpler\nthan ours but requires more compute (64 TPUv3\nchips) to achieve good results.\n2 Bilingual Pretrained LMs\nWe ﬁrst provide some background of pretrained\nlanguage models. Let Ee be English word-\nembeddings and enc(θ) be the Transformer\n(Vaswani et al., 2017) encoder with parameters\nθ. Let ewi denote the embedding of word wi (i.e.,\newi = Ee[w1]). We omit positional embeddings\nand bias for clarity. A pretrained LM typically\nperforms the following computations:\n(i) transform a sequence of input tokens to con-\ntextualized representations [cw1 ,..., cwn ] =\nenc(ew1 ,..., ewn ; θ)\n(ii) predict an output word yi at ith position\np(yi |cwi ) ∝exp(c⊤\nwi eyi )\nAutoencoding LM (Devlin et al., 2019) corrupts\nsome input tokens wi by replacing them with a spe-\ncial token [MASK]. It then predicts the original\ntokens yi = wi from the corrupted tokens. Au-\ntoregressive LM (Radford et al., 2019) predicts the\nnext token (yi = wi+1) given all the previous to-\nkens. The recently proposed XLNet model (Yang\net al., 2019) is an autoregressive LM that factor-\nizes output with all possible permutations, which\nshows empirical performance improvement over\nGPT-2 due to the ability to capture bidirectional\ncontext. Here we assume that the encoder performs\nnecessary masking with respect to each training\nobjective.\nGiven an English pretrained LM, we wish to\nlearn a bilingual LM for English and a given tar-\nget language ℓunder a limited computational re-\nsource budget. To quickly build a bilingual LM,\nwe directly adapt the English pre-traind model to\nthe target model. Our approach consists of two\nsteps. First, we initialize target language word-\nembeddings Eℓ in the English embedding space\nsuch that embeddings of a target word and its En-\nglish equivalents are close together ( §2.1). Next,\nwe construct a bilingual LM of Ee, Eℓ, and enc(θ)\nand ﬁne-tune all the parameters ( §2.2). Figure 1\nillustrates the two steps in our approach.\n[CLS] pipen'estCeci pas !youdoubleI[CLS]\nFine-tuning both foreign and English\nune\nShared Transformer Layers\n[CLS] Ceci n'est pas [MASK] pipe\nShared Transformer Layers\n[CLS] I double [MASK] you !\ndare\n2\nInitializing foreign embeddings\nEnglish embeddings Foreign embeddings\n1\nsparse word translation matrix\nFigure 1: Illustration of our two-step approach. In the\nﬁrst step, foreign embeddings are initialized in English\nspace (§2.1). In the second step, we joinly ﬁne-tune\nboth English and foreign models (§2.2).\n2.1 Initializing Target Embeddings\nOur approach to learn the initial foreign word em-\nbeddings Eℓ ∈ R|Vℓ|×d is based on the idea of\nmapping the trained English word embeddings\nEe ∈R|Ve|×d to Eℓ such that if a foreign word and\nan English word are similar in meaning then their\nembeddings are similar. We represent each foreign\nword embedding Eℓ[i] ∈Rd as a linear combina-\ntion of English word embeddings Ee[j] ∈Rd\nEℓ[i] =\n|Ve|∑\nj=1\nαijEe[j] = αiEe (1)\nwhere αi ∈ R|Ve| is a sparse vector and∑|Ve|\nj αij = 1.\nIn this step of initializing foreign embeddings,\nhaving a good estimation of αcould speed of the\nconvergence when tuning the foreign model and\nenable zero-shot transfer (§5). In the following, we\ndiscuss how to estimate αi ∀i ∈{1,2,..., |Vℓ|}\nunder two scenarios: (i) we have parallel data of\nEnglish-foreign, and (ii) we only rely on English\nand foreign monolingual data.\nLearning from Parallel Corpus Given an\nEnglish-foreign parallel corpus, we can estimate\nword translation probability p(ej |ℓi) for any\n(English-foreign) pair (ej,ℓi) using popular word-\nalignment (Brown et al., 1993) toolkits such as\nfast-align (Dyer et al., 2013). We then assign\nαij = p(ej |ℓi) (2)\nSince αi is estimated from word alignment, it is a\nsparse vector.\nLearning from Monolingual Corpus For low\nresource languages, parallel data may not be avail-\nable. In this case, we rely only on monolingual\ndata (e.g., Wikipedias). We estimate word trans-\nlation probabilities from word embeddings of the\ntwo languages. Word vectors of these languages\ncan be learned using fastText (Bojanowski et al.,\n2017) and then are aligned into a shared space\nwith English (Lample et al., 2018b; Joulin et al.,\n2018). Unlike learning contextualized representa-\ntions, learning word vectors is fast and computa-\ntionally cheap. Given the aligned vectors ¯Eℓ of\nforeign and ¯Ee of English, we calculate the word\ntranslation matrix A∈R|Vℓ|×|Ve|as\nA= sparsemax( ¯Eℓ ¯E⊤\ne ) (3)\nHere, we use sparsemax (Martins and Astudillo,\n2016) instead of softmax. sparsemax is a sparse\nversion of softmax and it puts zero probabilities\non most of the words in the English vocabulary\nexcept few English words that are similar to a given\nforeign word. This property is desirable in our\napproach since it leads to a better initialization of\nthe foreign word embeddings.\n2.2 Fine-tuning Bilingual LM\nWe create a bilingual LM by plugging foreign lan-\nguage speciﬁc parameters to the pretrained English\nLM (Figure 1). The new model has two separate\nword embedding layers (and output layers), one for\nEnglish and one for foreign language. The encoder\nlayer in between is shared. We then ﬁne-tune this\nmodel using English and foreign monolingual data.\nHere, we keep tuning the model on English to en-\nsure that it does not forget what it has learned in\nEnglish and that we can use the resulting model for\nzero-shot transfer (§3). In this step, the encoder is\nupdated so that in can learn syntactic aspects (i.e.,\nword order, morphological agreement) of the target\nlanguages.\n3 Zero-shot Experiments\nIn the scope of this work, we focus on transfer-\nring autoencoding LMs trained with masked lan-\nguage model objective. We choose BERT and\nRoBERTa (Liu et al., 2019) as the source models\nfor building our bilingual language models, named\nRAMEN3 for the ease of discussion. We imple-\nment our models on top of HuggingFace’s Trans-\nformer (Wolf et al., 2019). 4 For each pretrained\nmodel, we experiment with 12 layers (BERTBASE\nand RoBERTaBASE ) and 24 layers (BERTLARGE and\nRoBERTaLARGE ) variants. Using BERTBASE allows\nus to compare the results with mBERT model. Us-\ning BERTLARGE and RoBERTa allows us to inves-\ntigate whether the performance of the target LM\ncorrelates with the performance of the source pre-\ntrained model. RoBERTa is a recently published\nmodel that is similar to BERT architecturally but\nwith an improved training procedure. By training\nfor longer time, with bigger batches, on more data,\nand on longer sequences, RoBERTa matched or ex-\nceed previously published models including XLNet.\nWe include RoBERTa in our experiments to vali-\ndate the motivation of our work: with similar archi-\ntecture, does a stronger pretrained English model\nresult in a stronger bilingual LM?We evaluate our\nmodels on two cross-lingual zero-shot tasks: (1)\nCross-lingual Natural Language Inference (XNLI)\nand (2) dependency parsing.\n3The author likes ramen.\n4https://github.com/huggingface/\ntransformers\n3.1 Data\nWe evaluate our approach for six target languages:\nFrench (fr), Russian (ru), Arabic (ar), Chinese\n(zh), Hindi (hi), and Vietnamese (vi). These lan-\nguages belong to four different language families.\nFrench, Russian, and Hindi are Indo-European lan-\nguages, similar to English. Arabic, Chinese, and\nVietnamese belong to Afro-Asiatic, Sino-Tibetan,\nand Austro-Asiatic family respectively. The choice\nof the six languages also reﬂects different training\nconditions depending on the amount of monolin-\ngual data. French and Russian, and Arabic can\nbe regarded as high resource languages whereas\nHindi has far less data and can be considered as\nlow resource.\nFor experiments that use parallel data to initial-\nize foreign speciﬁc parameters, we use the same\ndatasets in the work of Lample and Conneau (2019).\nSpeciﬁcally, we use United Nations Parallel Cor-\npus (Ziemski et al., 2016) for en-ru, en-ar,\nen-zh, and en-fr. We collect en-hi paral-\nlel data from IIT Bombay corpus (Kunchukuttan\net al., 2018) and en-vi data from OpenSubtitles\n2018.5 For experiments that use only monolin-\ngual data to initialize foreign parameters, instead\nof training word-vectors from the scratch, we use\nthe pretrained word vectors 6 from fastText (Bo-\njanowski et al., 2017) to estimate word transla-\ntion probabilities (Eq. 3). We align these vectors\ninto a common space using orthogonal Procrustes\n(Artetxe et al., 2016; Lample et al., 2018b; Joulin\net al., 2018). We only use identical words be-\ntween the two languages as the supervised signal.\nWe use WikiExtractor7 to extract extract raw sen-\ntences from Wikipedias as monolingual data for\nﬁne-tuning target embeddings and bilingual LMs\n(§2.2). We do notlowercase or remove accents in\nour data preprocessing pipeline.\nWe tokenize English using the provided tok-\nenizer from pretrained models. For target lan-\nguages, we use fastBPE8 to learn 30,000 BPE codes\nand 50,000 codes when transferring from BERT\nand RoBERTa respectively. We truncate the BPE\nvocabulary of foreign languages to match the size\nof the English vocabulary in the source models.\nPrecisely, the size of foreign vocabulary is set to\n5http://opus.nlpl.eu\n6https://fasttext.cc/docs/en/\ncrawl-vectors.html\n7https://github.com/attardi/\nwikiextractor\n8https://github.com/glample/fastBPE\n32,000 when transferring from BERT and 50,000\nwhen transferring from RoBERTa.\nWe use XNLI dataset (Conneau et al., 2018)\nfor classiﬁcation task and Universal Dependen-\ncies v2.4 (Nivre et al., 2019) for dependency pars-\ning task. Since a language might have more\nthan one treebank in Universal Dependencies,\nwe use the following treebanks: en ewt (En-\nglish), fr gsd (French), ru syntagrus (Rus-\nsian), zh gsd (Chinese), vi vtb (Vietnamese),\nhi hdtb (Hindi), and ar padt (Arabic).\nRemark on BPE (Lample et al., 2018a) show\nthat sharing subwords between languages improves\nalignments between embedding spaces. Wu and\nDredze (2019) observe a strong correlation be-\ntween the percentage of overlapping subwords and\nmBERT’s performances for cross-lingual zero-shot\ntransfer. On the other hand, K et al., (2020) report\nin their control experiments that subword overlap-\nping has minimal contribution cross-lingual ability\nof BERT. Artetxe et al., (2019) conﬁrms in their\nstudy that a shared vocabulary is not necessary for\nmultilingual models. In our current approach, sub-\nwords between source and target are not shared. A\nsubword that is in both English and foreign vocab-\nulary has two different embeddings.\n3.2 Estimating translation probabilities\nSince pretrained models operate on subword level,\nwe need to estimate subword translation probabili-\nties. Therefore, we subsample 2M sentence pairs\nfrom each parallel corpus and tokenize the data\ninto subwords before running fast-align (Dyer et al.,\n2013).\nEstimating subword translation probabilities\nfrom aligned word vectors requires an additional\nprocessing step since the provided vectors from\nfastText are not at subword level.9 We use the fol-\nlowing approximation to obtain subword vectors:\nthe vector es of subword sis the weighted average\nof all the aligned word vectors ewi that have sas\nan subword\nes =\n∑\nwj: s∈wj\np(wj)\nns\newj (4)\nwhere p(wj) is the unigram probability of word\nwj estimated from monolingual data and ns =∑\nwj: s∈wj p(wj).\n9In our preliminary experiments, we learned the aligned\nsubword vectors but it results in poor performances.\nfr vi zh ru ar hi avg\n(Conneau et al., 2018) /check_minus67.7 66.4 65.8 65.4 64.8 64.1 65.7\n(Artetxe and Schwenk, 2019) /plus_square_o71.9 72.0 71.4 71.5 71.4 65.5 70.6\n(Lample and Conneau, 2019) (MLM)/check_minus76.5 72.1 71.9 73.1 68.5 65.7 71.3\n(Lample and Conneau, 2019) (MLM+TLM)/plus_square_o78.7 76.1 76.5 75.3 73.1 69.6 74.9\nmBERT (Wu and Dredze, 2019) /check_minus73.8 69.5 69.3 69.0 64.9 60.0 67.8\nRAMENBASE\n+ BERT /check_minus75.2 71.8 70.7 71.1 69.3 62.8 70.1\n/plus_square_o75.1 72.5 71.9 70.8 69.7 63.5 70.6\n+ RoBERTa /check_minus79.9 75.9 73.7 73.6 71.9 65.6 73.4\n/plus_square_o80.3 75.6 76.2 75.8 73.1 68.1 74.9\nRAMENLARGE\n+ BERT /check_minus78.1 74.8 74.5 73.7 70.8 64.5 72.7\n/plus_square_o78.0 75.1 71.3 74.0 71.8 66.1 72.7\n+ RoBERTa /check_minus81.3 76.2 76.3 75.6 73.5 64.5 74.6\n/plus_square_o81.0 76.2 76.8 75.0 72.9 68.2 75.0\nTable 1: Zero-shot classiﬁcation results on XNLI. /plus_square_oindicates parallel data is used. RAMEN only uses parallel data\nfor initialization. The best results are marked in bold.\nWe take the top 50,000 words in each aligned\nword-vectors to compute subword vectors.\nIn both cases, not all the words in the foreign vo-\ncabulary can be initialized from the English word-\nembeddings. Those words are initialized randomly\nfrom a Gaussian N(0,1/d2).\n3.3 Hyper-parameters\nIn all the experiments, we tune RAMENBASE for\n120,000 updates and RAMENLARGE for 300,000 up-\ndates. The sequence length is set to 256. For tuning\nbilingual LMs, we use a mini-batch size of 112 for\nRAMENBASE and 24 for RAMENLARGE where half\nof the batch are English sequences and the other\nhalf are foreign sequences. This strategy of balanc-\ning mini-batch has been used in multilingual neural\nmachine translation (Firat et al., 2016; Lee et al.,\n2017).\nWe optimizeRAMEN using Adam optimizer. We\nlinearly increase the learning rate from 10−7 to\n10−4 in the ﬁrst 4000 updates and then follow an\ninverse square root decay. When ﬁne-tuning RA-\nMEN on XNLI and UD, we use a mini-batch size\nof 32, Adam’s learning rate of10−5. The number\nof epochs are set to 4 and 50 for XNLI and UD\ntasks respectively.\nAll experiments are carried out on a single Tesla\nV100 16GB GPU. Each RAMENBASE model is\ntrained within a day and each RAMENLARGE is\ntrained within two days.10\n1022 and 46 GPU hours, to be precise. Learning alignment\nwith fast-align takes less than 2 hours and we do not account\nfor training time of fastText vectors.\n4 Results\nIn this section, we present the results of out models\nfor two zero-shot cross lingual transfer tasks: XNLI\nand universal dependency parsing.\n4.1 Cross-lingual Natural Language\nInference\nTable 1 shows the XNLI test accuracy. For ref-\nerence, we also include the scores from the pre-\nvious work, notably the state-of-the-art system\nXLM (Lample and Conneau, 2019). Before dis-\ncussing the results, we spell out that the fairest\ncomparison in this experiment is the comparison\nbetween mBERT and RAMENBASE +BERT trained\nwith monolingual only. We ﬁrst discuss the trans-\nfer results from BERT. Initialized from fastText\nvectors, RAMENBASE slightly outperforms mBERT\nby 2.6 points on average and widen the gap of\n4.4 points on Arabic. RAMENBASE gains extra\n0.3 points on average when initialized from par-\nallel data. With triple number of parameters,\nRAMENLARGE offers an additional boost in term\nof accuracy and initialization with parallel data\nconsistently improves the performance. It has been\nshown that BERTLARGE signiﬁcantly outperforms\nBERTBASE on eleven English NLP tasks (Devlin\net al., 2019), the strength of BERTLARGE also shows\nup when adapted to foreign languages.\nTransferring from RoBERTa leads to better\nzero-shot accuracies. With the same initializing\ncondition, RAMENBASE +RoBERTa outperforms\nRAMENBASE +BERT on average by 3.1 and 4.3\npoints when initializing from monolingual and par-\nallel data respectively. This result show that with\nsimilar number of parameters, our approach bene-\nﬁts from a better English pretrained model. When\ntransferring from RoBERTaLARGE , we obtain state-\nof-the-art results for ﬁve languages. It is worth\nmentioning that these models are still underﬁt and\nthey have potential to push the XNLI performance\nfurther.\nCurrently, RAMEN only uses parallel data to\ninitialize foreign embeddings. RAMEN can also\nexploit parallel data through translation objective\nproposed in XLM. We believe that by utilizing par-\nallel data during the ﬁne-tuning of RAMEN would\nbring additional beneﬁts for zero-shot tasks. We\nleave this exploration to future work. In summary,\nstarting from BERTBASE , our approach obtains com-\npetitive bilingual LMs with mBERT for zero-shot\nXNLI. Our approach shows the accuracy gains\nwhen adapting from a better pretrained model.\n4.2 Universal Dependency Parsing\nWe build on top of RAMEN a graph-based depen-\ndency parser (Dozat and Manning, 2016). For the\npurpose of evaluating the contextual representa-\ntions learned by our model, we do not use part-\nof-speech tags. Contextualized representations are\ndirectly fed into Deep-Biafﬁne layers to predict arc\nand label scores. Table 2 presents the Labeled At-\ntachment Scores (LAS) for zero-shot dependency\nparsing.\nfr vi zh ru ar hi avg\nmBERT /check_minus71.6 35.7 26.6 65.2 36.4 30.4 44.3\nRAMENBASE\n+ BERT /check_minus78.0 38.3 30.1 67.2 40.6 38.6 48.8\n/plus_square_o77.0 36.7 30.8 66.8 40.9 40.9 48.9\n+ RoBERTa/check_minus79.1 38.9 31.5 67.7 42.2 41.2 50.1\n/plus_square_o78.5 39.4 31.3 65.1 41.4 44.2 50.0\nRAMENLARGE\n+ BERT /check_minus78.2 39.5 30.5 65.6 45.3 44.0 50.5\n/plus_square_o78.9 38.4 30.5 66.3 43.4 44.1 50.3\n+ RoBERTa/check_minus79.7 40.0 32.265.7 44.1 44.6 51.1\n/plus_square_o79.1 39.2 30.5 65.4 43.8 46.8 50.8\nTable 2: LAS scores for zero-shot dependency parsing.\n/plus_square_oindicates parallel data is used for initialization. Punc-\ntuation are removed during the evaluation. The best re-\nsults are marked in bold.\nWe ﬁrst look at the fairest comparison be-\ntween mBERT and monolingually initialized\nRAMENBASE +BERT. The latter outperforms the for-\nmer on all the languages with the average of 4.5\nLAS point. We observe the largest gain of +8.2\nfor Hindi and +6.2 LAS for French. Arabic enjoys\n+4.2 LAS from our approach. With similar archi-\ntecture (12 or 24 layers) and initialization (using\nmonolingual or parallel data), RAMEN+RoBERTa\nperforms better than RAMEN+BERT for most of\nthe languages. Arabic and Hindi beneﬁt the most\nfrom bigger models. For the other four languages,\nRAMENLARGE renders a modest improvement over\nRAMENBASE .\n5 Analysis\nThroughout this section, we carry out the main anal-\nysis for the RAMENBASE +BERT model that only\nuse monolingual data.\n5.1 How does linguistic knowledge transfer\nhappen through each training stages?\nWe evaluate the performance of\nRAMEN+BERTBASE (initialized from mono-\nlingual data) at each 20K training steps. The\nresults are presented in Figure 2.\n20 40 60 80 100 120\nNumber of updates (x 1000)\nfr\nvi\nzh\nru\nar\nhi\nLanguage\n74.5 75.5 74.7 73.4 74.7 75.2\n70.4 71.1 72.4 71.3 71.7 71.8\n69.5 70.3 71.3 70.7 69.6 70.5\n68.2 70.3 68.5 69.6 68.0 71.1\n67.2 67.2 67.5 68.2 69.3 68.5\n60.6 61.8 61.7 62.2 62.2 62.8\nZero-shot XNLI\n20 40 60 80 100 120\nNumber of updates (x 1000)\nfr\nvi\nzh\nru\nar\nhi\nLanguage\n77.5 78.0 77.9 78.0 78.2 78.0\n36.6 37.2 37.1 37.5 37.6 38.3\n29.1 30.3 30.4 31.1 30.4 30.1\n62.0 66.0 66.1 66.1 67.0 67.2\n38.2 38.5 39.6 39.2 39.0 40.6\n36.2 36.2 38.3 38.8 38.8 38.6\nZero-shot Parsing\nFigure 2: Accuracy and LAS evaluated at each check-\npoints.\nWe observe the general trend that futher ﬁne-\ntuning RAMEN improves the performances of\ntwo zero-shot tasks. Just after 40,000 ﬁne-\ntuning updates—corresponding to seven training\nhours— RAMENBASE +BERT surpasses mBERT on\nboth XNLI (Table 1) and universal dependency\nparsing (Table 2). Interstingly, we ﬁnd that the\nEnglish encoder can quickly adapt to the foreign\nsyntax that has different word order (ar and hi)\nwith a little training data.\nLanguage similarities seem to have more impact\non transferring syntax than semantics. French en-\njoys 78.0 LAS for being closely related to English,\nwhereas Arabic and Hindi, SOV languages, mod-\nestly reach 38.2 and 36.2 points using the SVO\nencoder. Although Chinese has SVO order, it is\noften seen as head-ﬁnal while English is strong\nhead-initial. Perhaps, this explains the poor perfor-\nmance for Chinese.\n5.2 Impact of initialization\nInitializing foreign embeddings is the backbone\nof our approach. A good initialization leads to\nbetter zero-shot transfer results and enables fast\nadaptation. To verify the importance of a good\ninitialization, we train a RAMENBASE +BERT with\nforeign word-embeddings that are initialized ran-\ndomly from N(0,1/d2). For a fair comparison, we\nuse the same hyper-parameters in §3.3. Table 3\nshows the results of XNLI and UD parsing of ran-\ndom initialization. In comparison to the initial-\nization using aligned fastText vectors, random ini-\ntialization decreases the zero-shot performance of\nRAMENBASE by 10.3% for XNLI and 11.6 points\nfor UD parsing on average. We also see that zero-\nshot parsing of SOV languages (Arabic and Hindi)\nsuffers random initialization.\nfr vi zh ru ar hi avg\nXNLI\nrnd 66.9 67.1 65.7 59.1 50.8 48.5 59.7\nscr 71.1 58.0 62.8 63.8 62.1 47.5 60.8\nvec 75.2 71.8 70.5 71.1 68.5 62.8 70.0\nUD\nrnd 71.0 33.2 25.5 60.3 19.7 13.4 37.2\nscr 63.8 17.0 13.6 61.0 15.5 11.2 30.3\nvec 78.0 38.3 30.1 67.2 40.6 38.6 48.8\nTable 3: Comparison between random initialization\n(rnd) of language speciﬁc parameters and initializa-\ntion using aligned fastText vectors (vec) and bilingual\nBERT trained from scratch (scr) for 400 hours.\nTo highlight the efﬁciency of our transferring\napproach, we compare RAMENBASE +BERT with\na bilingual BERT (bBERT) trained from scratch\n(scr). For a fair comparision, we use the hyper-\nparameters described in section §3.3. All bBERT\nmodels are trained for 2,300,000 updates (400 GPU\nhours), which is more than sixteen times longer\nthan RAMEN. We emphasize two important ob-\nservations from Table 3. First, the randomly ini-\ntialized foreign embeddings RAMEN performs on\npar with bBERT on XNLI task and signiﬁcantly\nbetter than bBERT on UD parsing. This suggests\nthat a large amount of linguistic knowledge can\nbe transfered through reusing the pretrained BERT\nencoder. Secondly, with a careful initialization of\nforeign embeddings, our RAMENBASE +BERT out-\nshines bBERT with even just 20,000 updates (3.5\nGPU hours) as shown in Figure 2.\n5.3 Are contextual representations from\nRAMEN also good for supervised parsing?\nAll the RAMEN models are built from English and\ntuned on English for zero-shot cross-lingual tasks.\nIt is reasonable to expect RAMENs do well in those\ntasks as we have shown in our experiments.But are\nthey also a good feature extractor for supervised\ntasks? We offer a partial answer to this question by\nevaluating our model for supervised dependency\nparsing on UD datasets.\nfr vi zh ru ar hi avg\nmBERT 92.1 62.2 85.1 93.1 83.6 91.3 84.6\nRAMENBASE\n+ BERT 92.2 63.3 85.0 93.3 83.9 92.2 85.0\n+ RoBERTa 92.8 65.3 86.0 93.7 84.9 92.5 85.9\nRAMENLARGE\n+ BERT 92.8 64.7 86.2 93.9 84.9 92.0 85.7\n+ RoBERTa 93.0 66.4 87.3 94.0 85.3 92.8 86.5\nTable 4: Evaluation in supervised UD parsing. The\nscores are LAS.\nWe used train/dev/test splits provided in UD to\ntrain and evaluate our RAMEN-based parser. Ta-\nble 4 summarizes the results (LAS) of our super-\nvised parser. For a fair comparison, we choose\nmBERT as the baseline and all the RAMEN mod-\nels are initialized from aligned fastText vectors.\nWith the same architecture of 12 Transformer lay-\ners, RAMENBASE +BERT performs competitive to\nmBERT and outshines mBERT by +1.1 points for\nVietnamese. The best LAS results are obtained\nby RAMENLARGE +RoBERTa with 24 Transformer\nlayers. Overall, our results indicate the potential of\nusing contextual representations from RAMEN for\nsupervised tasks.\n6 Conclusions\nIn this work, we have presented a simple and ef-\nfective approach for rapidly building a bilingual\nLM under a limited computational budget. Us-\ning BERT as the starting point, we demonstrate\nthat our approach performs better than mBERT on\ntwo cross-lingual zero-shot sentence classiﬁcation\nand dependency parsing. We ﬁnd that the perfor-\nmance of our bilingual LM, RAMEN, correlates\nwith the performance of the original pretrained En-\nglish models. We also ﬁnd that RAMEN is also\na powerful feature extractor in supervised depen-\ndency parsing. Finally, we hope that our work\nsparks of interest in developing fast and effective\nmethods for transferring pretrained English models\nto other languages.\nReferences\nJoakim Nivre et al. 2019. Universal dependencies 2.4.\nLINDAT/CLARIN digital library at the Institute of\nFormal and Applied Linguistics ( ´UFAL), Faculty of\nMathematics and Physics, Charles University.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2016.\nLearning principled bilingual mappings of word em-\nbeddings while preserving monolingual invariance.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2289–2294, Austin, Texas. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the Cross-lingual Transferability of\nMonolingual Representations. arXiv e-prints, page\narXiv:1910.11856.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, and Robert L. Mercer. 1993. The math-\nematics of statistical machine translation: Parameter\nestimation. Computational Linguistics, 19(2):263–\n311.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTimothy Dozat and Christopher D. Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. In ICLR.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameter-\nization of IBM model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine trans-\nlation with a shared attention mechanism. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n866–875, San Diego, California. Association for\nComputational Linguistics.\nArmand Joulin, Piotr Bojanowski, Tomas Mikolov,\nHerv´e J ´egou, and Edouard Grave. 2018. Loss in\ntranslation: Learning bilingual word mapping with\na retrieval criterion. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2984, Brussels, Bel-\ngium. Association for Computational Linguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual {bert}: An empirical study. In International\nConference on Learning Representations.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC-2018), Miyazaki, Japan. Euro-\npean Languages Resources Association (ELRA).\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. NeurIPS.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a. Unsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations.\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018b.\nWord translation without parallel data. In Interna-\ntional Conference on Learning Representations.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017. Fully character-level neural machine trans-\nlation without explicit segmentation. Transactions\nof the Association for Computational Linguistics,\n5:365–378.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nAndr´e F. T. Martins and Ram ´on F. Astudillo. 2016.\nFrom softmax to sparsemax: A sparse model of at-\ntention and multi-label classiﬁcation. In Proceed-\nings of the 33rd International Conference on Inter-\nnational Conference on Machine Learning - Volume\n48, ICML’16, pages 1614–1623. JMLR.org.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. NeurIPS.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel cor-\npus v1.0. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC 2016), Paris, France. European Language Re-\nsources Association (ELRA).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8604449033737183
    },
    {
      "name": "Natural language processing",
      "score": 0.6897198557853699
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6696711778640747
    },
    {
      "name": "Inference",
      "score": 0.6207413077354431
    },
    {
      "name": "Parsing",
      "score": 0.6070541143417358
    },
    {
      "name": "Dependency (UML)",
      "score": 0.5467175841331482
    },
    {
      "name": "Dependency grammar",
      "score": 0.5403618216514587
    },
    {
      "name": "Language model",
      "score": 0.5171460509300232
    },
    {
      "name": "Foreign language",
      "score": 0.5056343078613281
    },
    {
      "name": "Transfer of learning",
      "score": 0.4232957661151886
    },
    {
      "name": "Scratch",
      "score": 0.4209245443344116
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4201764464378357
    },
    {
      "name": "Linguistics",
      "score": 0.20332971215248108
    },
    {
      "name": "Programming language",
      "score": 0.19762372970581055
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ]
}