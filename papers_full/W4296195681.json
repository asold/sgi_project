{
  "title": "Transformer Lesion Tracker",
  "url": "https://openalex.org/W4296195681",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2101050108",
      "name": "Wen Tang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107434326",
      "name": "Han Kang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2154133041",
      "name": "Haoyue Zhang",
      "affiliations": [
        "InferVision (China)",
        "Computational Diagnostics (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2742653759",
      "name": "Pengxin Yu",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2140006220",
      "name": "Corey W. Arnold",
      "affiliations": [
        "Computational Diagnostics (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2107842282",
      "name": "Rongguo Zhang",
      "affiliations": [
        "InferVision (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2101050108",
      "name": "Wen Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107434326",
      "name": "Han Kang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154133041",
      "name": "Haoyue Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742653759",
      "name": "Pengxin Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2140006220",
      "name": "Corey W. Arnold",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107842282",
      "name": "Rongguo Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787740020",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W3176549342",
    "https://openalex.org/W3091855899",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2964151353",
    "https://openalex.org/W2016974693",
    "https://openalex.org/W3158810081",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W3043500847",
    "https://openalex.org/W2507987841",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W3092210850",
    "https://openalex.org/W3108112547",
    "https://openalex.org/W2979736498",
    "https://openalex.org/W2220907510",
    "https://openalex.org/W3203379702",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3117686595",
    "https://openalex.org/W2970105478",
    "https://openalex.org/W2883683269",
    "https://openalex.org/W2775397671",
    "https://openalex.org/W2991213871",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3098269293"
  ],
  "abstract": null,
  "full_text": "Transformer Lesion Tracker\nWen Tang1, Han Kang1, Haoyue Zhang1,2, Pengxin Yu1, Corey W. Arnold 2,\nRongguo Zhang1(\u0000 )\nInferVision Medical Technology Co., Ltd., Beijing, China\nzrongguo@infervision.com,\nComputational Diagnostic Lab, UCLA, Los Angeles, USA\nAbstract. Evaluating lesion progression and treatment response via\nlongitudinal lesion tracking plays a critical role in clinical practice. Auto-\nmated approaches for this task are motivated by prohibitive labor costs\nand time consumption when lesion matching is done manually. Previous\nmethods typically lack the integration of local and global information. In\nthis work, we propose a transformer-based approach, termed Transformer\nLesion Tracker (TLT). Speciﬁcally, we design a Cross Attention-based\nTransformer (CAT) to capture and combine both global and local infor-\nmation to enhance feature extraction. We also develop a Registration-\nbased Anatomical Attention Module (RAAM) to introduce anatomical\ninformation to CAT so that it can focus on useful feature knowledge.\nA Sparse Selection Strategy (SSS) is presented for selecting features\nand reducing memory footprint in Transformer training. In addition,\nwe use a global regression to further improve model performance. We\nconduct experiments on a public dataset to show the superiority of our\nmethod and ﬁnd that our model performance has improved the average\nEuclidean center error by at least 14.3% (6mm vs. 7mm) compared with\nthe state-of-the-art (SOTA). Code is available at https://github.com/\nTangWen920812/TLT.\nKeywords: Transformer · Cross Attention · Registration.\n1 Introduction\nThe ability to accurately locate the location of follow-up lesions and subsequent\nquantitative assessment, referred to as ”lesion tracking,” is crucial to a variety of\nmedical applications, in particular, cancer management. In practice, physicians\nneed to spend signiﬁcant time and eﬀort to precisely match the same lesion across\ndiﬀerent time points. Thus, its investigation into a fully automated method of\nlesion tracking or lesion matching is highly desirable. However, compared with a\nlarge number of studies on lesion segmentation and detection [19, 22], there are\nvery few studies on lesion tracking [4, 11]. In the ﬁeld of natural images, there\nis a similar problem called target tracking or object tracking, for which several\ndeep learning-based methods have been proposed [3,12,23]. One of the simplest\nand most straightforward ideas is to apply these existing methods to lesion\ntracking tasks. However, lesion tracking is diﬀerent from the aforementioned\narXiv:2206.06252v1  [cs.CV]  13 Jun 2022\n2 Wen Tang et al.\nvisual tracking in a number of aspects: (1) Medical imaging data are mostly in\n3D format. (2) The lesion size varies at diﬀerent time points, such as increasing,\nshrinking, or stabilizing. (3) The appearance of the lesion may change during the\nfollow-up examination while its anatomical location remains unchanged. Thus,\nan eﬀective lesion tracker should account for the diﬀerences in the lesion itself and\nbe able to use anatomical information eﬀectively. However, existing registration-\nbased trackers [18,21] are not robust for small-sized lesions or heavily deformed\nlesions due to lack of sensitivity to local details, and Siamese networks [9, 12]\noverlook the information around the lesion. Cai et al. [4] ﬁrst provided an open-\nsource dataset for lesion tracking and designed Deep Lesion Tracker (DLT),\nwhich combines the advantages of both strategies and obtained a baseline on\nthis dataset. Although a large kernel size is extracted in cross correlation layers\nof DLT to encode the global image context, it is still susceptible to the inductive\nbias in convolution, leading to deviation in the aggregation of information around\nthe lesion.\nIn this work, we leverage Transformer architecture, inspired by TransT [7],\nto replace existing cross correlation, and propose a novel Transformer Lesion\nTracking framework (named TLT) using 3D features. To achieve our model, we\ndesign a Cross Attention-based Transformer (CAT) to capture global informa-\ntion. To better focus on useful features, we also introduce anatomical priors via\nthe proposed Registration-based Anatomical Attention Module (RAAM) into\nCAT. Meanwhile, considering the memory cost in training process, we present a\nSparse Selection Strategy (SSS) to extract the local eﬀective information from\nthe whole template image as input for CAT. In addition, we use a global re-\ngression as output to reduce the eﬀect of insuﬃcient multi-scale information\nand accelerate convergence. The experimental results show that the proposed\nmethod achieves better performance on the open-source dataset compared with\nthe state-of-the-art methods.\n2 Related Work\nRegistration-based Trackers.The anatomy presented in a patient’s medi-\ncal images at diﬀerent time points should be similar in the absence of surgery\nor similar treatment. Thus, lesion tracker should follow a spatial consistency\nthat the tissue or the structure around a lesion, and the organ in which the\nlesion is located will not change signiﬁcantly. Under this assumption, existing\nregistration methods, such as Voxelmorph [1], provide solutions for lesion track-\ning. Since registration algorithms [10,15] focus on alignment or optimization on\nglobal structures, registration-based lesion trackers achieve decent performance\nfor large-sized lesions or relatively stable lesions [18,21]. Still, due to the lack of\nsensitivity of registration algorithms to image details, these registration-based\nmethods obtain reduced performance when dealing with small-sized lesions or\nheavily deformed lesions. In this study, we treat image registration as an auxiliary\noperation, thereby improving model training eﬃciency as well as performance.\nSpeciﬁcally, we select the mask registered via an aﬃne registration method [15]\nTransformer Lesion Tracker 3\nas the prior attention and introduce it into the Transformer. The subsequent\nablation experiment results demonstrate the eﬀectiveness of this operation.\nSiamese Networks. In recent years, Siamese-based methods have been pop-\nular in the ﬁeld of visual object tracking. SiamFC [2], and its variants such as\nSiamRPN [13] and SiamRPN++ [12], are among the representative works. Sub-\nsequently, existing studies have demonstrated that lesion tracking could also be\ndone using Siamese-based methods. Gomariz et al. [9] and Liu et al. [14] ap-\nplied 2D Siamese networks for lesion tracking in ultrasound sequences. Whereas\nRafael-Palou et al. [17] performed 3D Siamese networks to track lung nodules on\nCT series. Cai et al. [4] followed SiamRPN++ to use 3D Siamese networks to con-\nduct universal lesion tracking in whole body CT images. These Siamese-based\nmethods mainly consist of two parts: a backbone network for feature extrac-\ntion and a correlation module to calculate the similarity between the template\npatch and the searching sub-region. However, such module is susceptible to the\ninductive bias of convolution operation and fails to fully utilize the global con-\ntext, leading to local optimum in the optimization process. Thus, we introduce\nan attention-based Transformer architecture to focus on the key object in the\nglobal feature space, while replacing the correlation part.\nTransformer-based Tracking.In recent years, Transformer architecture [24]\nhas taken over recurrent neural networks in natural language processing [8, 20],\nand has also had an impact on the status of convolutional neural networks in\ncomputer vision [6,16]. More recently, Chen et al. [7] proposed a target tracking\nmethod on natural images with Transformer architecture instead of the cross\ncorrelation layers and achieved SOTA results. However, several issues remain\nto be addressed when applying Transformer to lesion tracking on 3D medical\nimages. Speciﬁcally, to reduce memory cost and acquire features of diﬀerent\nsized lesion adaptively, we design a sparse selection strategy to extract irregular\npatches from template feature maps as input to Transformer. To introduce prior\nanatomical structure information to Transformer, we create a registration-based\nattention guidance for auxiliary model training.\n3 Method\nProblem Description.Same as object tracking [2], lesion tracking aims to ﬁnd\nits corresponding position in the searching image Is when given a lesion in the\ntemplate image It. Similar to [4], we simplify this task: given a lesion lin It with\nits known center ct and radius rt, we seek a mapping function Fto predict the\ncenter cs of l in Is.\nOverview. In this lesion tracking task, we deﬁne the baseline CT scan as the\ntemplate image It ∈RDt0×Ht0×Wt0 , and a corresponding follow-up CT scan as\nthe searching image Is ∈RDs0×Hs0×Ws0 . Dt0, Ht0 and Wt0 represent the depth,\nheight and width of the template image, respectively. And Ds0, Hs0 and Ws0\nare similarly deﬁned for searching image. The proposed lesion tracking network\n(TLT) mainly consists of three components, as shown in Fig. 1. The feature ex-\ntractor stacks 3D convolution and downsampling layers to eﬃciently represent\n4 Wen Tang et al.\nthe input volumes. The proposed sparse selection strategy is used for mem-\nory reducing and eﬃcient feature acquisition. Then, the cross attention-based\nTransformer (CAT) is used to fuse the features of the searching and the selected\ntemplate. In the CAT, a mask gained from the registration-based anatomical at-\ntention module (RAAM) is inserted to enhance fused features. Finally, the center\npredictor is responsible for getting the result from the output of Transformer.\n3.1 Feature Extractor and Sparse Selection Strategy\nIn the proposed network, a modiﬁed 3D ResNet18 with shared weights is em-\nployed as the feature extractor. Compared with the original one, we remove\nthe last stage of ResNet18, and take outputs of the fourth stage as ﬁnal out-\nputs. We also adjust the stride of the ﬁrst convolutional layer from 2 ×2 ×2\nto 1 ×1 ×1 to obtain a larger feature resolution. Considering the parame-\nter redundancy and overﬁtting in 3D networks, the number of feature chan-\nnels in each stage is reduced by half or more (see Fig. 1). Putting It and Is\nthrough the learnable feature extractor respectively, their own image features\nFt,ori ∈RC×Dt×Ht×Wt, Fs,ori ∈RC×Ds×Hs×Ws are obtained for subsequent pro-\ncesses, where Dt,Ht,Wt = Dt0\n8 ,Ht0\n8 ,Wt0\n8 , Ds,Hs,Ws = Ds0\n8 ,Hs0\n8 ,Ws0\n8 , C = 192.\nAs shown in Fig. 1(a), template-based feature mining via the proposedsparse\nselection strategy (SSS) precede the CTA to select features and to reduce mem-\nory cost. This is feasible because the location of the lesion in the template input\nIt is known, and we believe the features Ft,ori to have already incorporated lo-\ncal contextual information. The following are the details of the SSS ﬂow. Given\na lesion in the template image, we ﬁrst generate a three-dimensional Gaussian\nmap Gbased on the known center and radius of the lesion, which is formulated\nby:\nG(c,r) = exp(−\n∑\ni∈(x,y,z)(i−ci)2\n∑\ni∈(x,y,z)(2ri)2 ) (1)\nSpeciﬁcally, for the lesion l in It with its center ct and radius rt , the generated\nGaussian map Gt is Gt(ct,rt). Next, we resize Gt to the size of Ft,ori by trilinear\ninterpolation, and obtain the resized Gaussian map ˜Gt. Selecting a threshold Tr\nand using ˜Gt as a reference mask, we extract valid features Ft,sparse from the\nFt,ori as an input of the Transformer: Ft,sparse = Ft,ori(x,y,z |˜Gt(x,y,z ) >Tr )\n3.2 Cross Attention-based Transformer\nUnlike the similarity-based correlation module used in the previous Siamese-\nbased networks, we design a Cross Attention-based Transformer (CAT) to com-\nbine global and local context. Queries Q, keys K and values V are encoded\nfrom same source in Transformer [24]. But in CAT, to grab global context\nand blend multiple features of diﬀerent sizes, we adopt cross-attention (CA),\nin which K, V are stemmed from the same input while Q from another. In\nTLT, we put a CA on each of the template and searching path respectively (see\nTransformer Lesion Tracker 5\nV\nK\nQ\nAttentio\nn Map\nConcat\nAdd & Norm\nsoftmax\nWi\nQ\nWi\nK\nWi\nV\nTemplate \nGaussian \nMap:Gt\nSearching \nGaussian \nMap:Gs\nreshape\n* h\n32 64 128 192Template \nImage: It\nSearch \nImage: Is\nCA\nCA\nCA\nClassification\nRegression\nGlobal \nRegressionShared weights\n* N\n: 1x1x1 conv + reshape\nQ\nQ\nK,V\nK,V\nK,V\nQ\nMA\n: SSS\n32 64 128 192\nMATGt\n: MLP\nAffine transform\nT\nT : Transpose\n: Form all-ones-variable with same shape  \nRAAM\n(a)\n(b) (c)\nTr\nGt\nTr\nFt,ori Ft,sparse\nFs,ori\ndown-\nsample\n: Spatial position encoding\nFq\nFkv\nMA\nMA\nMA\nFt,reshape\nFs,reshape\nFt,reshape\nFs,CTA\nGs,reshape\nO\nFeature Extractor CAT Center Predictor\nT\nFig. 1.(a) Overall structure of the proposed network. (b) Cross attention in CAT. (c)\nStructure of RAAM\nFig 1(a)). In CA of the template line, K and V are encoded from the reshaped\nFs,ori : Fs,reshape ∈RDsHsWs×C and Q is encoded from the reshaped Ft,sparse\n: Ft,reshape ∈RL×C,L = len(Ft,sparse). While in CA of the searching line, K,\nV and Q have the opposite origins to those in the template one. In short, as\nshown in Fig. 1(b), Q is encoded from the features which need enhancement\n(Fq), and K, V are encoded from the other ( Fkv). We apply CA on each lines\nfor N(N = 3) times, and use another CA on searching line to obtain the ﬁnal\noutput features: Fs,CTA.\nWe further create a novel structure called Registration-based Anatomical\nAttention Module (RAAM) to calculate an anatomical information mask MA,\nwhose transpose is taken as MT\nA (see Fig. 1(c)). As described above, the anatom-\nical information is needed for lesion tracking. Thus, we create a matrix to pro-\nvide the anatomical information for each of template and searching side. For\ntemplate side, we assume all the voxels in Ft,reshape are of the same importance,\nand we build a matrix O ∈RL×1, in which all elements are 1. For searching\nside, we ﬁrst use an aﬃne registration method [15] to roughly align It and Is by\nsolving: TAﬀ = arg min||TAﬀ(It) −Is||1. We choose to use an aﬃne registration\nmethod instead a non-rigid one because the non-rigid registration is slow and\nprovides restriction to the attention that limit the model’s ability to learn for\nlocal variation and details. Then, we can obtain a registration-based Gaussian\nmap Gs = TAﬀ(Gt). Afterwards, we downsample Gs to the size of Fs,ori and re-\n6 Wen Tang et al.\nshape it to Gs,reshape ∈RDsHsWs×1, which is deﬁned as the matrix of searching\nside. Finally, MA can be calculated by the following formula:\nMA = O⊗GT\ns,reshape (2)\nwhere ⊗is matrix multiplication operation, and MA ∈ RL×DsHsWs. So the\nattention we use in CAT at each head (see Fig. 1(b)) can be deﬁne as following:\nAttentioni(Q,K,V ) = softmax((QWQ\ni )(KWK\ni )T\n√dk\n+ MA)(VW V\ni ), (3)\nwhere WQ\ni , WK\ni , WV\ni are parameter matrices, dk is the dimension of key, i ∈\n{1,...,h }is the index of head and h is the number of heads in multiple head\nattention.\n3.3 Center Predictor and Training Loss\nSimilar to the head of detection networks, our center predictor consists of a\nclassiﬁcation branch and a regression branch, where each branch is a multilayer\nperceptron (MLP). The classiﬁcation head is to classify if a voxel from the out-\nput is inside of a lesion, and the regression head is to regress the exact center\nposition. In detail, after inputting the features Fs,CTA, the predictor outputs the\nclassiﬁcation results ˆY ∈R1×DsHsWs and center coordinates ˆC ∈R3×DsHsWs.\nDuring training, we deﬁne the ground truth as a Gaussian map generated by\nEq. 1 with the target center cs and the corresponding radius rs. We downsample\nit to obtain the Gaussian label GL which matches the size of ˆY, and obtain label\nY = GL−min(GL)\nmax(GL)−min(GL) . L1 loss is used as the regression loss, which is formulated\nas:\nLr = ||ˆc−cs||1, ˆc=\n∑\nsoftmax( ˆY) ∗ˆC (4)\nwhere ˆc is the ﬁnal output of the center predictor, which we deﬁne as global\nregression. Meanwhile, a focal loss [4] is used as the classiﬁcation loss for auxiliary\ntraining:\nLc =\n∑\ni\n{\n(1 −ˆyi)αlog(ˆyi) if yi = 1\n(1 −yi)β(ˆyi)αlog(1 −ˆyi) otherwise (5)\nwhere yi and ˆyi are the i-th elements in Y and ˆY, respectively, and α= β = 2.\n4 Experiments and Experimental Results\n4.1 Dataset and Experiment Setup\nDataset. We validate our method on a public dataset, DLS [4], which consists\nof CT image pairs inherited from DeepLesion [27]. There are 3008, 403 and 480\nlesion pairs for training, validation,and testing in this dataset, respectively. Since\nTransformer Lesion Tracker 7\nTable 1.Lesion tracking comparison on Deep Lesion Tracking testing dataset. ∗ rep-\nresents the p value of paired t-test is smaller than 0.05.\nMethod CPM@\n10mm\nCPM@\nRadius\nMEDX\n(mm)\nMEDY\n(mm)\nMEDZ\n(mm)\nMED\n(mm)\nAﬃne [15] 48.33 65.21 4.1 ± 5.0 5.4 ± 5.6 7.1 ± 8.3 11.2 ± 9.9\nVoxelMorph [1] 49.90 65.59 4.6 ± 6.7 5.2 ± 7.9 6.6 ± 6.2 10.9 ± 10.9\nLENS-LesioGraph [25,28] 63.85 80.42 2.6 ± 4.6 2.7 ± 4.5 6.0 ± 8.6 8.0 ± 10.1\nVULD-LesionGraph [5,28] 64.69 76.56 3.5 ± 5.2 4.1 ± 5.8 6.1 ± 8.8 9.3 ± 10.9\nVULD-LesaNet [5,26] 65.00 77.81 3.5 ± 5.3 4.0 ± 5.7 6.0 ± 8.7 9.1 ± 10.8\nSiamRPN++ [12] 68.85 80.31 3.8 ± 4.8 3.8 ± 4.8 4.8 ± 7.5 8.3 ± 9.2\nLENS-LesaNet [25,26] 70.00 84.58 2.7 ± 4.8 2.6 ± 4.7 5.7 ± 8.6 7.8 ± 10.3\nDEEDS [10] 71.88 85.52 2.8 ± 3.7 3.1 ± 4.1 5.0 ± 6.8 7.4 ± 8.1\nDLT-Mix [4] 78.65 88.75 3.1 ± 4.4 3.1 ± 4.5 4.2 ± 7.6 7.1 ± 9.2\nDLT [4] 78.85 86.88 3.5 ± 5.6 2.9 ± 4.9 4.0 ± 6.1 7.0 ± 8.9\nTransT [7] 79.59 88.99 3.4 ± 5.9 5.4 ± 6.1 1.8 ± 2.2 7.6 ± 7.9\nTLT 87.37∗ 95.32∗ 3.0 ± 6.2 3.7 ± 5.2 1.7 ± 2.1 6.0 ± 7.7∗\nthe ground truth lesion center of all lesions in this dataset and the corresponding\nradius are deﬁned, we could mutually track within a lesion pair. Therefore, a\ntotal of 906 and 960 directed lesion pairs are used for evaluation in validation\nand testing sets, respectively.\nEvaluation Metrics.The center point matching (CPM) accuracy is selected to\nevaluate the performance of lesion matching. As deﬁned in [4], a match will be\ncounted correct when the Euclidean distance between ground truth and predicted\ncenters is smaller than a threshold (@10mm: min(10mm, rs), @Radius: rs). The\nmean Euclidean distacne (MED) in mm+/- standard deviation between ground\ntruth and predicted centers, and its projections in each direction (denoted as\nMEDX, MEDY and MEDZ, respectively) [4] are also used for model evaluation.\nImplementation Details. The proposed method is implemented using Py-\nTorch (v1.5.1). The network is optimized by Adam with initial learning rate\nof 0.0001 and trained for 300 epochs. The batch size is 4 and the number of\nparameters of the model is 5.98M. All CT volumes have been resampled to the\nisotropic resolution of 1mmbefore feeding into the network. This training setting\nis used in all deep learning-based methods selected for comparison. For the aﬃne\nregistration method [15] and DEEDS [10], following the setting of [4] and [10],\nall CT volumes are resampled to a isotropic resolution of 2 mm.\n4.2 Experimental Results and Discussion\nModel Comparison. We took TransT [7] as baseline, and selected DLT and\nother state-of-the-art approaches in [4] for comparison. Table 1 shows the quan-\ntitative results of these methods. Our method yields a CPM@10 mm of 87.37,\na CPM@Radius of 95.32, and a MED of 6 .0 ±7.7, which outperforms all the\ncompared methods in terms of both CPM and MED metrics. A paired t-test\nis used on CPM@10 mm, CPM@Radius and MED to perform statistical tests.\n8 Wen Tang et al.\nTable 2.Ablation study on each module and diﬀerent thresholds. ∗ represents the p\nvalue of paired t-test is smaller than 0.05.\nSSS RAAM Global\nRegressor\nCPM@\n10mm\nMED\n(mm)\nThreshold CPM@10mm MED(mm)\n0.9 83.57 6.80 ± 8.12\n0.8 83.99 6.65 ± 7.99\n79.59 7.58 ± 7.91 0.7 87.37∗ 5.98 ± 7.68∗\n/enc-33 84.78 6.76 ± 7.86 0.6 86.70 6.26 ± 7.88\n/enc-33 /enc-3386.58 6.30 ± 7.79 0.5 86.37 6.20 ± 7.83\n/enc-33 /enc-33 /enc-3387.37∗ 5.98 ± 7.68∗ 0.4 85.01 6.39 ± 7.98\nMoreover, we observe that transformer-based methods, TransT and our TLT,\nboth achieving large improvements in terms of MED Z compared with methods\nthat use convolution to compute similarity. This may be because the Transformer\nfocuses more on the information in the z-axis direction, which is also consistent\nwith physician cognition.\nAblation Study.To evaluate the eﬀectiveness of various conﬁgurations in our\nproposed method, we conduct ablation experiments from two aspects: module\nsetting and threshold setting. A paired t-test is also used for statistical tests. Ta-\nble 2 shows the experimental results. The results show that accuracy drops with\neach module change, which validates the competence of our proposed method.\nMeanwhile, it is observed that the threshold of 0.7 is much better than that of\nother thresholds. Therefore, we choose 0.7 as the thresholds Tr in our TLT.\nDiscussion. As we observe, in ablation study, the SSS module leads to the\nbiggest improvement. To verify this, we also perform ablation study with only\none single module removed, as shown in Table 1 in supplementary materials. This\nhappens when there are many small lesions in the dataset, such as lung nodules.\nIf these small lesions are cropped on the original image, due to downsampling,\nthe feature map will become very small, and in the last several downsampling\nprocesses will always become one voxel, which could lead to a decline in perfor-\nmance. The SSS solves this problem by selecting voxels on the last feature map.\nEven if only one voxel on the feature map is selected, this voxel can still obtain\nmore surrounding information in the networks than without SSS. Meanwhile,\nbased on our observations, we found that when the registration method failed,\nsometimes our model would fail as well. This is because we use registration\nto feed anatomical information to the transformer, and anatomical information\nhelps the transformer accelerate convergence, which forms a dependency. In addi-\ntion, when there are similar lesions in similar locations, such as two solid nodules\nat the edge of the right upper lung, and only a few layers diﬀerence in the z-axis\ndirection, the model will also be confused.\n5 Conclusion\nThis paper presents a novel Transformer-based framework for lesion tracking by\nleveraging both the anatomical prior and the cross image relevance. We fur-\nTransformer Lesion Tracker 9\nther introduce a global regression to integrate multi-scale information while\nusing sparse selection strategy to reduce memory consumption. TLT achieves\nthe state-of-the-art performance on DLT dataset, signiﬁcantly exceeding previ-\nous methods in lesion tacking accuracy. Future work includes multi-institutional\nvalidation and reader studies to examine the eﬃciency improvement for physi-\ncians in clinical setting.\nAcknowledgment. This work was funded by Science and Technology Innova-\ntion 2030-New Generation Artiﬁcial Intelligence Major Project (2021ZD0111104).\nReferences\n1. Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V.: An unsuper-\nvised learning model for deformable medical image registration. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition. pp. 9252–9260\n(2018)\n2. Bertinetto, L., Valmadre, J., Henriques, J.F., Vedaldi, A., Torr, P.H.: Fully-\nconvolutional siamese networks for object tracking. In: European conference on\ncomputer vision. pp. 850–865. Springer (2016)\n3. Bolme, D.S., Beveridge, J.R., Draper, B.A., Lui, Y.M.: Visual object tracking\nusing adaptive correlation ﬁlters. In: 2010 IEEE computer society conference on\ncomputer vision and pattern recognition. pp. 2544–2550. IEEE (2010)\n4. Cai, J., Tang, Y., Yan, K., Harrison, A.P., Xiao, J., Lin, G., Lu, L.: Deep lesion\ntracker: Monitoring lesions in 4d longitudinal imaging studies. In: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.\n15159–15169 (2021)\n5. Cai, J., Yan, K., Cheng, C.T., Xiao, J., Liao, C.H., Lu, L., Harrison, A.P.: Deep\nvolumetric universal lesion detection using light-weight pseudo 3d convolution and\nsurface point regression. In: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. pp. 3–13. Springer (2020)\n6. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European conference on computer\nvision. pp. 213–229. Springer (2020)\n7. Chen, X., Yan, B., Zhu, J., Wang, D., Yang, X., Lu, H.: Transformer tracking.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 8126–8135 (2021)\n8. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n9. Gomariz, A., Li, W., Ozkan, E., Tanner, C., Goksel, O.: Siamese networks with\nlocation prior for landmark tracking in liver ultrasound sequences. In: 2019 IEEE\n16th International Symposium on Biomedical Imaging (ISBI 2019). pp. 1757–1760.\nIEEE (2019)\n10. Heinrich, M.P., Jenkinson, M., Brady, M., Schnabel, J.A.: Mrf-based deformable\nregistration and ventilation estimation of lung ct. IEEE transactions on medical\nimaging 32(7), 1239–1248 (2013)\n11. Hering, A., Peisen, F., Amaral, T., Gatidis, S., Eigentler, T., Othman, A., Moltz,\nJ.H.: Whole-body soft-tissue lesion tracking and segmentation in longitudinal ct\n10 Wen Tang et al.\nimaging studies. In: Medical Imaging with Deep Learning. pp. 312–326. PMLR\n(2021)\n12. Li, B., Wu, W., Wang, Q., Zhang, F., Xing, J., Yan, J.S., et al.: Evolution of siamese\nvisual tracking with very deep networks. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, Long Beach, CA, USA. pp. 16–20\n(2019)\n13. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with\nsiamese region proposal network. In: Proceedings of the IEEE conference on com-\nputer vision and pattern recognition. pp. 8971–8980 (2018)\n14. Liu, F., Liu, D., Tian, J., Xie, X., Yang, X., Wang, K.: Cascaded one-shot de-\nformable convolutional neural networks: Developing a deep learning model for res-\npiratory motion estimation in ultrasound sequences. Medical Image Analysis 65,\n101793 (2020)\n15. Marstal, K., Berendsen, F., Staring, M., Klein, S.: Simpleelastix: A user-friendly,\nmulti-lingual library for medical image registration. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops. pp. 134–142\n(2016)\n16. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: International Conference on Machine Learning. pp. 4055–\n4064. PMLR (2018)\n17. Rafael-Palou, X., Aubanell, A., Bonavita, I., Ceresa, M., Piella, G., Ribas, V.,\nBallester, M.A.G.: Re-identiﬁcation and growth detection of pulmonary nodules\nwithout image registration using 3d siamese neural networks. Medical Image Anal-\nysis 67, 101823 (2021)\n18. Raju, A., Cheng, C.T., Huo, Y., Cai, J., Huang, J., Xiao, J., Lu, L., Liao, C., Har-\nrison, A.P.: Co-heterogeneous and adaptive segmentation from multi-source and\nmulti-phase ct imaging data: a study on pathological liver and lesion segmenta-\ntion. In: European Conference on Computer Vision. pp. 448–465. Springer (2020)\n19. Shao, Q., Gong, L., Ma, K., Liu, H., Zheng, Y.: Attentive ct lesion detection\nusing deep pyramid inference with multi-scale booster. In: International Conference\non Medical Image Computing and Computer-Assisted Intervention. pp. 301–309.\nSpringer (2019)\n20. Synnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sri-\nram, A., Liptchinsky, V., Collobert, R.: End-to-end asr: from supervised to semi-\nsupervised learning with modern architectures. arXiv preprint arXiv:1911.08460\n(2019)\n21. Tan, M., Li, Z., Qiu, Y., McMeekin, S.D., Thai, T.C., Ding, K., Moore, K.N., Liu,\nH., Zheng, B.: A new approach to evaluate drug treatment response of ovarian can-\ncer patients based on deformable image registration. IEEE transactions on medical\nimaging 35(1), 316–325 (2015)\n22. Tang, W., Kang, H., Cao, Y., Yu, P., Han, H., Zhang, R., Chen, K.: M-seam-nam:\nMulti-instance self-supervised equivalent attention mechanism with neighborhood\naﬃnity module for double weakly supervised segmentation of covid-19. In: Inter-\nnational Conference on Medical Image Computing and Computer-Assisted Inter-\nvention. pp. 262–272. Springer (2021)\n23. Teed, Z., Deng, J.: Raft: Recurrent all-pairs ﬁeld transforms for optical ﬂow. In:\nEuropean conference on computer vision. pp. 402–419. Springer (2020)\n24. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n/suppress L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\nTransformer Lesion Tracker 11\n25. Yan, K., Cai, J., Zheng, Y., Harrison, A.P., Jin, D., Tang, Y., Tang, Y., Huang, L.,\nXiao, J., Lu, L.: Learning from multiple datasets with heterogeneous and partial\nlabels for universal lesion detection in ct. IEEE Transactions on Medical Imaging\n40(10), 2759–2770 (2020)\n26. Yan, K., Peng, Y., Sandfort, V., Bagheri, M., Lu, Z., Summers, R.M.: Holistic\nand comprehensive annotation of clinically signiﬁcant ﬁndings on diverse ct im-\nages: learning from radiology reports and label ontology. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8523–\n8532 (2019)\n27. Yan, K., Wang, X., Lu, L., Summers, R.M.: Deeplesion: automated mining of large-\nscale lesion annotations and universal lesion detection with deep learning. Journal\nof medical imaging 5(3), 036501 (2018)\n28. Yan, K., Wang, X., Lu, L., Zhang, L., Harrison, A.P., Bagheri, M., Summers, R.M.:\nDeep lesion graphs in the wild: relationship learning and organization of signiﬁcant\nradiology image ﬁndings in a diverse large-scale lesion database. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition. pp. 9261–9270\n(2018)\nSupplementary Materials\nWen Tang1, Han Kang1, Haoyue Zhang2, Pengxin Yu1, Corey Arnold2,\nRongguo Zhang1(\u0000 )\nInferVision Medical Technology Co., Ltd.\nzrongguo@infervision.com,\nUniversity of California, Los Angeles\nTable 1.Ablation study with one single module removed\nSSS RAAM Global\nRegressor\nCPM@\n10mm\nMED\n(mm)\n/enc-33 /enc-3384.02 6.35 ± 7.95\n/enc-33 /enc-3385.11 6.31 ± 7.82\n/enc-33 /enc-3386.58 6.30 ± 7.79\n/enc-33 /enc-33 /enc-3387.37∗ 5.98 ± 7.68∗\nTable 2.Detailed stucture of our proposed method: TLT.\ntemplate image searching image\nconv 7 × 7 × 7, channel 32, stride 1.\nconv 3 × 3 × 3, channel 32, stride 1.\nResBlock :\n[\nconv 3 × 3 × 3, channel 64\nconv 3 × 3 × 3, channel 64\n]\n× 2\nResBlock :\n[\nconv 3 × 3 × 3, channel 128\nconv 3 × 3 × 3, channel 128\n]\n× 2\nResBlock :\n[\nconv 3 × 3 × 3, channel 192\nconv 3 × 3 × 3, channel 192\n]\n× 2\nSSS\nreshape\nCA :\n\n\nchannel 192, head 8\nQ:template features\nK,V:searching features\n\n × 3 CA :\n\n\nchannel 192, head 8\nQ:searching features\nK,V:template features\n\n × 3\nCA :\n\n\nchannel 192, head 8\nQ:searching features\nK,V:template features\n\n × 1\nMLP: classiﬁcation & regression\narXiv:2206.06252v1  [cs.CV]  13 Jun 2022\n2 Wen Tang et al.\nFig. 1.Visualization of the attention maps for some representative pairs. The images\nabove are from searching lines and the images below are from template lines. From\nleft to right: the original input, cross attention map of each level (1-3), the ﬁnal cross\nattention map before output.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6816545724868774
    },
    {
      "name": "Transformer",
      "score": 0.574805498123169
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5508367419242859
    },
    {
      "name": "Feature extraction",
      "score": 0.41359350085258484
    },
    {
      "name": "Machine learning",
      "score": 0.34945380687713623
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3477536141872406
    },
    {
      "name": "Engineering",
      "score": 0.1675269901752472
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210111607",
      "name": "InferVision (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210121124",
      "name": "Computational Diagnostics (United States)",
      "country": "US"
    }
  ],
  "cited_by": 12
}