{
    "title": "Context-Transformer: Tackling Object Confusion for Few-Shot Detection",
    "url": "https://openalex.org/W2997322677",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2147343871",
            "name": "Ze Yang",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2107245326",
            "name": "Yali Wang",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Shenzhen Institutes of Advanced Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2139474296",
            "name": "xianyu chen",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Shenzhen Institutes of Advanced Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2127038590",
            "name": "Jianzhuang Liu",
            "affiliations": [
                "Huawei Technologies (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A2106105882",
            "name": "Yu Qiao",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2147343871",
            "name": "Ze Yang",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2107245326",
            "name": "Yali Wang",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2139474296",
            "name": "xianyu chen",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2106105882",
            "name": "Yu Qiao",
            "affiliations": [
                "Shenzhen Institutes of Advanced Technology",
                "Institute of Art"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2288122362",
        "https://openalex.org/W6675342213",
        "https://openalex.org/W2606609115",
        "https://openalex.org/W6748931594",
        "https://openalex.org/W2604763608",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W6735463952",
        "https://openalex.org/W6677245018",
        "https://openalex.org/W6746975906",
        "https://openalex.org/W6756851690",
        "https://openalex.org/W2519284461",
        "https://openalex.org/W2194321275",
        "https://openalex.org/W6785652829",
        "https://openalex.org/W2125215748",
        "https://openalex.org/W6682933819",
        "https://openalex.org/W6756040250",
        "https://openalex.org/W1483870316",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2472819217",
        "https://openalex.org/W2747206364",
        "https://openalex.org/W6735236233",
        "https://openalex.org/W2418676188",
        "https://openalex.org/W2604260814",
        "https://openalex.org/W2214409633",
        "https://openalex.org/W6746034047",
        "https://openalex.org/W2739879705",
        "https://openalex.org/W6766092863",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2159564241",
        "https://openalex.org/W2963604034",
        "https://openalex.org/W4303633609",
        "https://openalex.org/W2115699968",
        "https://openalex.org/W2963603913",
        "https://openalex.org/W2805481182",
        "https://openalex.org/W3106250896",
        "https://openalex.org/W2788159735",
        "https://openalex.org/W2796346823",
        "https://openalex.org/W2962966271",
        "https://openalex.org/W2983156430",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963341924",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2601450892"
    ],
    "abstract": "Few-shot object detection is a challenging but realistic scenario, where only a few annotated training images are available for training detectors. A popular approach to handle this problem is transfer learning, i.e., fine-tuning a detector pretrained on a source-domain benchmark. However, such transferred detector often fails to recognize new objects in the target domain, due to low data diversity of training samples. To tackle this problem, we propose a novel Context-Transformer within a concise deep transfer framework. Specifically, Context-Transformer can effectively leverage source-domain object knowledge as guidance, and automatically exploit contexts from only a few training images in the target domain. Subsequently, it can adaptively integrate these relational clues to enhance the discriminative power of detector, in order to reduce object confusion in few-shot scenarios. Moreover, Context-Transformer is flexibly embedded in the popular SSD-style detectors, which makes it a plug-and-play module for end-to-end few-shot learning. Finally, we evaluate Context-Transformer on the challenging settings of few-shot detection and incremental few-shot detection. The experimental results show that, our framework outperforms the recent state-of-the-art approaches.",
    "full_text": "The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20)\nContext-Transformer: Tackling Object Confusion for Few-Shot Detection\nZe Yang,1∗ Yali Wang,1∗ Xianyu Chen,1 Jianzhuang Liu,2 Yu Qiao1,3†\n1ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIA T-SenseTime Joint Lab,\nShenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n2Huawei Noah’s Ark Lab\n3SIA T Branch, Shenzhen Institute of Artiﬁcial Intelligence and Robotics for Society\n{ze.yang, yali.wang, yu.qiao}@siat.ac.cn, xianyuchen1992@outlook.com, liu.jianzhuang@huawei.com\nAbstract\nFew-shot object detection is a challenging but realistic sce-\nnario, where only a few annotated training images are avail-\nable for training detectors. A popular approach to handle\nthis problem is transfer learning, i.e., ﬁne-tuning a detec-\ntor pretrained on a source-domain benchmark. However,\nsuch transferred detector often fails to recognize new ob-\njects in the target domain, due to low data diversity of\ntraining samples. To tackle this problem, we propose a\nnovel Context-Transformer within a concise deep transfer\nframework. Speciﬁcally, Context-Transformer can effectively\nleverage source-domain object knowledge as guidance, and\nautomatically exploit contexts from only a few training im-\nages in the target domain. Subsequently, it can adaptively\nintegrate these relational clues to enhance the discriminative\npower of detector, in order to reduce object confusion in few-\nshot scenarios. Moreover, Context-Transformer is ﬂexibly\nembedded in the popular SSD-style detectors, which makes\nit a plug-and-play module for end-to-end few-shot learning.\nFinally, we evaluate Context-Transformer on the challenging\nsettings of few-shot detection and incremental few-shot de-\ntection. The experimental results show that, our framework\noutperforms the recent state-of-the-art approaches.\n1 Introduction\nObject detection has been mainly promoted by deep learning\nframeworks (Ren et al. 2015; He et al. 2017; Redmon et al.\n2016; Liu et al. 2016). However, the impressive performance\nof these detectors heavily relies on large-scale benchmarks\nwith bounding box annotations, which are time-consuming\nor infeasible to obtain in practice. As a result, we often face\na real-world scenario, i.e., few-shot object detection, where\nthere are only a few annotated training images. In this case,\ndeep learning will deteriorate due to severe overﬁtting.\nA popular strategy is transfer learning, i.e., one can train\nan object detector with a large-scale benchmark in the source\ndomain, and then ﬁne-tune it with a few samples in the tar-\nget domain. By doing so, we observe an interesting and im-\n∗Ze Y ang and Y ali Wang contribute equally\n†Corresponding author.\nCopyright c⃝ 2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nportant phenomenon. For few-shot object detection, a trans-\nferred detector often performs well on localization while\nencounters difﬁculty in classiﬁcation, e.g., ahorse is well-\nlocalized but misclassiﬁed as adog in Fig. 1.\nThe main reason is that, an object detector uses bound-\ning box regressor (BBOX) for localization while ob-\nject+background classiﬁer (OBJ+BG) for classiﬁcation.\nBBOX is often category-irrelevant. Hence, we can use\nsource-domain BBOX as a reliable initialization of target-\ndomain BBOX. In this case, the detector can effectively\nlocalize new objects after ﬁne-tuning with a few training\nsamples in the target domain. On the contrary, OBJ+BG is\ncategory-speciﬁc. In other words, it has to be randomly ini-\ntialized for new categories in the target domain. However,\nonly a few training images are available in this domain. Such\nlow data diversity signiﬁcantly enlarges the training difﬁ-\nculty of classiﬁer, which leads to the key problem above,\ni.e., object confusion caused by annotation scarcity.\nTo address this problem, we propose a novel Context-\nTransformer. It can automatically exploit contexts from only\na few images on hand, and attentively integrate such dis-\ntinct clues to generalize detection. Our design is inspired by\n(Oliva and Torralba 2007) that, at an early age with little\nobject knowledge, humans can build the contextual associa-\ntions for visual recognition. In other words, under little su-\npervision scenarios, we will try to explore distinct clues in\nthe surroundings (which we refer to as contextual ﬁelds in\nthis paper), to clarify object confusion. For example, a few\nimages may be discriminative enough to distinguishhorse\nfrom dog, when we ﬁnd that these images contain important\ncontents such as a person sits on this animal, the scene is\nabout wild grassland, etc.\nTo mimic this capacity, we design Context-Transformer\nin a concise transfer framework. Speciﬁcally, it consists of\ntwo simple but effective submodules, i.e., afﬁnity discovery\nand context aggregation. For a target-domain image, afﬁn-\nity discovery ﬁrst constructs a set of contextual ﬁelds, ac-\ncording to default prior boxes (also called as anchor boxes)\nin the detector. Then, it adaptively exploits relations be-\ntween prior boxes and contextual ﬁelds. Finally, context\naggregation leverages such relations as guidance, and in-\ntegrates key contexts attentively into each prior box. As a\n12653\n \n Target Image (Few-Shot) \nContext-Transformer \nSource Detector \n(Fine-Tune) \nTarget Image (Few-Shot) \ndog \n       Localization: (√) \n       Classification: (X) \nhorse \n       Localization: (√) \n       Classification: (√) \nSource Detector \n(Fine-Tune) \nAffinity Discovery \nContext Aggregation \nPrior Box \n Contextual Fields  \n(a) Fine-Tune Source Detector \n (b) Fine-Tune Source Detector with Context-Transformer \nFigure 1: Our Motivation. Fine-tuning a pretrained detector is a popular approach for few-shot object detection. However, such\ntransferred detector often suffers from object confusion in the new target domain, e.g., ahorse is misclassiﬁed as adog, due to\nannotation scarcity. Alternatively, humans can effectively correct such few-shot confusions by further exploiting discriminative\ncontext clues from only a few images on hand. Inspired by this observation, we introduce a novel Context-Transformer to tackle\nobject confusion for few-shot object detection. More explanations can be found in Section 1 and 4.\nresult, Context-Transformer can generate a context-aware\nrepresentation for each prior box, which allows detector\nto distinguish few-shot confusion with discriminative con-\ntext clues. To our best knowledge, Context-Transformer\nis the ﬁrst work to investigate context for few-shot ob-\nject detection. Since it does not require excessive con-\ntextual assumptions on aspect ratios, locations and spatial\nscales, Context-Transformer can ﬂexibly capture diversi-\nﬁed and discriminative contexts to distinguish object con-\nfusion. More importantly, it leverages elaborative transfer\ninsights for few-shot detection. With guidance of source-\ndomain knowledge, Context-Transformer can effectively re-\nduce learning difﬁculty when exploiting contexts from few\nannotated images in the target domain. Additionally, we em-\nbed Context-Transformer into the popular SSD-style detec-\ntors. Such plug-and-play property makes it practical for few-\nshot detection. Finally, we conduct extensive experiments\non different few-shot settings, where our framework outper-\nforms the recent state-of-the-art approaches.\n2 Related Works\nOver the past years, we have witnessed the fast development\nof deep learning in object detection. In general, the deep de-\ntection frameworks are mainly categorized into two types,\ni.e., one-stage detectors (e.g., YOLO or SSD styles (Red-\nmon et al. 2016; Liu et al. 2016)) and two-stage detectors\n(e.g., R-CNN styles (Girshick et al. 2014; Girshick 2015;\nRen et al. 2015; He et al. 2017)). Even though both types\nhave achieved great successes in object detection, they heav-\nily depend on large-scale benchmarks with bounding boxes\nannotations. Collecting such fully-annotated datasets is of-\nten difﬁcult or labor-intensive for real-life applications.\nFew-Shot Object Detection. To alleviate this problem,\nweakly (Bilen and V edaldi 2016; Lai and Gong 2017;\nTang et al. 2017) or semi (Hoffman et al. 2014; Tang et al.\n2016) supervised detectors have been proposed. However,\nonly object labels are available in the weakly-supervised set-\nting, which restricts the detection performance. The semi-\nsupervised detectors often assume that, there is a moderate\namount of object box annotations, which can be still chal-\nlenging to obtain in practice. Subsequently, few-shot data\nassumption has been proposed in (Dong et al. 2018). How-\never, it relies on multi-model fusion with a complex training\nprocedure, which may reduce the efﬁciency of model de-\nployment for a new few-shot detection task. Recently, a fea-\nture reweighting approach has been introduced in a transfer\nlearning framework (Kang et al. 2019). Even though its sim-\nplicity is attractive, this approach requires object masks as\nextra inputs to train a meta-model of feature reweighting.\nMore importantly, most approaches may ignore object con-\nfusion caused by low data diversity. Alternatively, we pro-\npose a novel Context-Transformer to address this problem.\nObject Detection with Contexts. Modeling context has\nbeen a long-term challenge for object detection (Bell et\nal. 2016; Chen and Gupta 2017; Kantorov et al. 2016;\nMottaghi et al. 2014). The main reason is that, objects may\nhave various locations, scales, aspect ratios, and classes.\nIt is often difﬁcult to model such complex instance-level\nrelations by manual design. Recently, several works have\nbeen proposed to alleviate this difﬁculty, by automatically\nbuilding up object relations with non-local attention (Wang\net al. 2018; Hu et al. 2018). However, these approaches\nwould lead to unsatisfactory performance in few-shot detec-\ntion, without elaborative transfer insights. Alternatively, our\n12654\nContext-Transformer is built upon a concise transfer frame-\nwork, which can leverage source-domain object knowledge\nas guidance, and effectively exploit target-domain context\nfor few-shot generalization.\nFew-Shot Learning. Unlike deep learning models, hu-\nmans can learn new concepts with little supervision (Lake,\nSalakhutdinov, and Tenenbaum 2015). For this reason,\nfew-shot learning has been investigated by Bayesian pro-\ngram learning (Lake, Salakhutdinov, and Tenenbaum 2015),\nmemory machines (Graves, Wayne, and Danihelka 2014;\nSantoro et al. 2016), meta learning (Finn, Abbeel, and\nLevine 2017; Y oon et al. 2018), metric learning (Qi, Brown,\nand Lowe 2018; Snell, Swersky, and Zemel 2017; Vinyals et\nal. 2016), etc. However, these approaches are designed for\nthe standard classiﬁcation task. Hence, they may lack the\nadaptation capacity for few-shot object detection.\n3 Source Detection Transfer\nTo begin with, we formulate few-shot object detection in a\npractical transfer learning setting.First, we assume that, we\ncan access to a published detection benchmark withC\ns ob-\nject categories. It is used as large-scale dataset for model\npretraining in the source domain. Second, we aim at ad-\ndressing few-shot detection in the target domain. Speciﬁ-\ncally, this task consists of C\nt object categories. For each\ncategory, there are onlyN fully-annotated training images,\ne.g., N=5 for 5-shot case.Finally, we consider a challenging\ntransfer scenario, i.e., object categories are non-overlapped\nbetween source and target domains, for evaluating whether\nour framework can generalize well on new object categories.\nDetection Backbone. In this work, we choose the SSD-\nstyle detector (Liu et al. 2016; Liu, Huang, and others 2018)\nas backbone. One reason is that, multi-scale spatial receptive\nﬁelds in this architecture provide rich contexts. Additionally,\nits concise detection design promotes ﬂexibility of our trans-\nfer framework in practice. In particular, the SSD-style de-\ntector is a one-stage detection framework, which consists of\ndetection heads on K spatial scales. For each spatial scale,\nthe detection heads contain bounding box regressor (BBOX)\nand object+background classiﬁer (OBJ+BG).\nSource Detection Transfer . To generalize few-shot\nlearning in the target domain, we ﬁrst pretrain the SSD-\nstyle detector with large-scale benchmark in the source do-\nmain. In the following, we explain how to transfer source-\ndomain detection heads (i.e., BBOX and OBJ+BG), so that\none can leverage prior knowledge as much as possible to re-\nduce overﬁtting for few-shot object detection.\n(1) Source BBOX: Fine-Tuning. BBOX is used for local-\nization. As it is shared among different categories, source-\ndomain BBOX can be reused in the target domain. Fur-\nthermore, source-domain BBOX is pretrained with rich an-\nnotations in the large-scale dataset. Hence, ﬁne-tuning this\nBBOX is often reliable to localize new objects, even though\nwe only have a few training images in the target domain.\n(2) Source BG: Fine-Tuning. OBJ+BG is used for classi-\nﬁcation. In this work, we factorize OBJ+BG separately into\nOBJ and BG classiﬁers. The reason is that, BG is a binary\nclassiﬁer (object or background), i.e., it is shared among dif-\nferent object categories. In this case, the pretrained BG can\nbe reused in the target domain by ﬁne-tuning.\n(3) Source OBJ: Preserving. The last but the most chal-\nlenging head is OBJ, i.e., multi-object classiﬁer. Note that,\nobject categories in the target domain are non-overlapped\nwith those in the source domain. Traditionally, one should\nunload source-domain OBJ and add a new target-domain\nOBJ. However, adding new OBJ directly on top of high-\ndimensional feature would introduce a large number of\nrandomly-initialized parameters, especially for multi-scale\ndesign in SSD-style frameworks. As a result, it is often hard\nto train such new OBJ from scratch, when we have only a\nfew annotated images in the target domain. Alternatively,\nwe propose to preserve source-domain OBJ and add a new\ntarget-domain OBJ on top of it. The main reason is that, the\ndimensionality of prediction score in source-domain OBJ\nis often much smaller than the number of feature channels\nin convolutional layers. When adding a new target-domain\nOBJ on top of source-domain OBJ, we will introduce fewer\nextra parameters and therefore alleviate overﬁtting.\nContext-Transformer Between Source and Target\nOBJs. To some degree, preserving source-domain OBJ can\nreduce the training difﬁculty of target-domain OBJ. How-\never, simple source detection transfer is not enough to ad-\ndress the underlying problem of few-shot object detection,\ni.e., object confusion introduced by annotation scarcity in\nthe target domain. Hence, it is still necessary to further ex-\nploit target-domain knowledge effectively from only a few\nannotated training images. As mentioned in our introduc-\ntion, humans often leverage contexts as a discriminative clue\nto distinguish such few-shot confusion. Motivated by this,\nwe embed a novel Context-Transformer between source and\ntarget OBJs. It can automatically exploit contexts, with the\nguidance of source domain object knowledge from source\nOBJ. Then, it can integrate such relational clues to enhance\ntarget OBJ for few-shot detection.\n4 Context-Transformer\nIn this section, we introduce Context-Transformer for few-\nshot object detection. Speciﬁcally, it is a novel plug-and-\nplay module between source and target OBJs. We name it\nas Context-Transformer, because it consists of two submod-\nules to transform contexts, i.e., afﬁnity discovery and context\naggregation. The whole framework is shown in Fig. 2.\n4.1 Afﬁnity Discovery\nIn SSD-style detectors (Liu et al. 2016), prior boxes are de-\nfault anchor boxes with various aspect ratios. Since classiﬁ-\ncation is performed over the representations of these boxes,\nafﬁnity discovery ﬁrst constructs a set of contextual ﬁelds\nfor prior boxes. Subsequently, it exploits relations between\nprior boxes and contextual ﬁelds in a target-domain image,\nwith guidance of source-domain object knowledge.\nSource-Domain Object Knowledge of Prior Boxes.F o r\na target-domain image, we should ﬁrst ﬁnd reliable represen-\ntations of prior boxes, in order to perform afﬁnity discov-\nery under few-shot settings. Speciﬁcally, we feed a target-\ndomain image into the pretrained SSD-style detector, and\n12655\nFew-Shot \nImage (Target Domain) \nOBJ (Source) \nBG (Source) \nBBOX (Source) \nSource Detection Transfer (Source Domain)  \nOBJ (Target) \n horse \nFew-Shot Detection (Target Domain) \nPrior Box \nContextual Field \nContext-Transformer  \nFC:  g FC:  h \nFC:  f FC:  ߮ \nSpatial Max Pooling (Eq. 2) \nreshape reshape \nsoftmax \nAffinity Discovery (Eq. 3) \nContext Aggregation (Eq. 4-5) \n۾ ^ \n௞} ௞ୀଵǣ௄ \n௞} ௞ୀଵǣ௄ \n۾ \nۿ \nA L \nSource Score for Prior Boxes \nSource Score for Contextual Fields \nAffinity Matrix: Context-Aware Score \n for Prior Boxes: \nFigure 2: Few-Shot Detection with Context-Transformer. It is a plug-and-play module between source and target OBJs, based\non SSD-style detectors. It consists of afﬁnity discovery and context aggregation, which can effectively reduce object confusion\nin the few-shot target domain, by exploiting contexts in a concise transfer framework. More details can be found in Section 4.\nextract the score tensor from source OBJ (before softmax),\nP\nk ∈ RHk×Wk×(Mk×Cs),k =1 ,...,K, (1)\nwhere Pk(h,w,m, :) ∈ RCs is a source-domain score vec-\ntor, w.r.t., the prior box with them-th aspect ratio located at\n(h,w) of thek-th spatial scale. We would like to emphasize\nthat, the score of source-domain classiﬁer often provides\nrich semantic knowledge about target-domain object cate-\ngories (Tzeng et al. 2015; Yim et al. 2017). Hence,{P\nk}K\nk=1\nis a preferable representation of prior boxes for a target-\ndomain image. Relevant visualization can be found in our\nsupplementary material.\nContextual Field Construction via Pooling. After ob-\ntaining the representation of prior boxes, we construct a set\nof contextual ﬁelds for comparison. Ideally, we hope that\ncontextual ﬁelds are not constructed with excessive spatial\nassumptions and complicated operations, due to the fact that\nwe only have a few training images on hand. A naive strat-\negy is to use all prior boxes directly as contextual ﬁelds.\nHowever, there are approximately 10,000 prior boxes in the\nSSD-style architecture. Comparing each prior box with all\nothers would apparently introduce unnecessary learning dif-\nﬁculty for few-shot cases. Alternatively, humans often check\nsparse contextual ﬁelds, instead of paying attention to ev-\nery tiny detail in an image. Motivated by this observation,\nwe propose to perform spatial pooling (e.g., max pooling)\nover prior boxesP\nk. As a result, we obtain the score tensor\nQk ∈ RUk×Vk×(Mk×Cs) for a set of contextual ﬁelds,\nQk = SpatialPool(Pk),k =1 ,...,K, (2)\nwhere Uk ×Vk is the size of thek-th scale after pooling.\nAfﬁnity Discovery. To discover afﬁnity between prior\nboxes and contextual ﬁelds, we compare them according\nto their source-domain scores. For convenience, we reshape\nscore tensors P\n1:K and Q1:K respectively as matricesP ∈\nRDp×Cs and Q ∈ RDq×Cs , where each row of P (or Q)\nrefers to the source-domain score vector of a prior box (or a\ncontextual ﬁeld). Moreover,Dp = ∑ K\nk=1 Hk × Wk × Mk\nand Dq = ∑ K\nk=1 Uk × Vk × Mk are respectively the to-\ntal number of prior boxes and contextual ﬁelds in a target-\ndomain image. For simplicity, we choose the widely-used\ndot-product kernel to compare P and Q in the embedding\nspace. As a result, we obtain an afﬁnity matrixA ∈ R\nDp×Dq\nbetween prior boxes and contextual ﬁelds,\nA = f(P)×g(Q)⊤ , (3)\nwhere A(i,:) ∈ R1×Dq indicates the importance of all con-\ntextual ﬁelds, w.r.t., the i-th prior box. f(P) ∈ RDp×Cs\nand g(Q) ∈ RDq×Cs are embeddings for prior boxes and\ncontextual ﬁelds respectively, where f (or g) is a fully-\nconnected layer that is shared among prior boxes (or contex-\ntual ﬁelds). These layers can increase learning ﬂexibility of\nkernel computation. To sum up, afﬁnity discovery allows a\nprior box to identify its important contextual ﬁelds automati-\ncally from various aspect ratios, locations and spatial scales.\nSuch diversiﬁed relations provide discriminative clues to re-\nduce object confusion caused by annotation scarcity.\n4.2 Context Aggregation\nAfter ﬁnding afﬁnity between prior boxes and contextual\nﬁelds, we use it as a relational attention to integrate contexts\ninto the representation of each prior box.\nContext Aggregation. We ﬁrst add softmax on each\nrow of A. In this case, softmax(A(i,:)) becomes a gate\nvector that indicates how important each contextual ﬁeld is\nfor thei-th prior box. We use it to summarize all the contexts\nQ attentively,\nL(i,:) =softmax(A(i,:))×h(Q), (4)\nwhere L(i,:) is the weighted contextual vector for the i-\nth prior box (i=1,...,D\np). Additionally, h(Q) ∈ RDq×Cs\n12656\nrefers to a contextual embedding, where h is a fully-\nconnected layer to promote learning ﬂexibility. Finally, we\naggregate the weighted contextual matrixL ∈ RDp×Cs into\nthe original score matrix P, and obtain the context-aware\nscore matrix of prior boxesˆP ∈ RDp×Cs ,\nˆP = P +ϕ(L). (5)\nSimilarly, the embedding ϕ(L) ∈ RDp×Cs is constructed\nby a fully-connected layerϕ. Since ˆP is context-aware, we\nexpect that it can enhance the discriminative power of prior\nboxes to reduce object confusion in few-shot detection.\nTarget OBJ. Finally, we feedˆP into target-domain OBJ,\nˆY = softmax(ˆP ×Θ), (6)\nwhere ˆY ∈ R\nDp×Ct is the target-domain score matrix for\nclassiﬁcation. Note that, target OBJ is shared among dif-\nferent aspect ratios and spatial scales in our design, with\na common parameter matrix Θ ∈ R\nCs×Ct . One reason is\nthat, each prior box has combined with its vital contextual\nﬁelds of different aspect ratios and spatial scales, i.e., each\nrow of ˆP has become a multi-scale score vector. Hence, it\nis unnecessary to assign an exclusive OBJ on each individ-\nual scale. More importantly, target domain is few-shot. The\nshared OBJ can effectively reduce overﬁtting in this case.\nDiscussions. We further clarify the differences between\nrelated works and our Context-Transformer. (1) Few-\nShot Learners vs. Context-Transformer. Few-shot learn-\ners (Snell, Swersky, and Zemel 2017; Finn, Abbeel, and\nLevine 2017; Qi, Brown, and Lowe 2018) and our Context-\nTransformer follow the spirit of learning with little supervi-\nsion, in order to effectively generalize model based on few\ntraining samples. However, most few-shot learners are de-\nsigned for standard classiﬁcation tasks. Hence, they are of-\nten used as a general classiﬁer without taking any detec-\ntion insights into account. On the contrary, our Context-\nTransformer is a plug-and-play module for object detection.\nVia exploiting contexts in a concise transfer framework, it\ncan adaptively generalize source-domain detector to reduce\nobject confusion in the few-shot target domain. In fact, our\nexperiment shows that, one can ﬁne-tune the pretrained de-\ntector with Context-Transformer in the training phase, and\nﬂexibly unload it in the testing phase without much loss\nof generalization. All these facts make Context-Transformer\na preferable choice for few-shot detection. (2) Non-local\nTransformer vs. Context-Transformer. Non-local Trans-\nformer (Wang et al. 2018) and our Context-Transformer fol-\nlow the spirit of attention (V aswani et al. 2017) for mod-\neling relations. However, the following differences make\nour Context-Transformer a distinct module. First, Non-local\nTransformer does not take any few-shot insights into ac-\ncount. Hence, it would not be helpful to reduce training\ndifﬁculty with little supervision. Alternatively, our Context-\nTransformer leverages source knowledge as guidance to\nalleviate overﬁtting in few-shot cases. Second, Non-local\nTransformer is not particularly designed for object detec-\ntion. It is simply embedded between two convolution blocks\nin the standard CNN for space/spacetime modeling. Alter-\nnatively, our Context-Transformer is developed for few-shot\nobject detection. We elaborately embed it between source\nand target OBJs in a SSD-style detection framework, so\nthat it can tackle object confusions caused by annotation\nscarcity. Third, Non-local Transformer is a self-attention\nmodule, which aims at learning space/spacetime dependen-\ncies in general. Alternatively, our Context-Transformer is\nan attention module operated between prior boxes and con-\ntextual ﬁelds. It is used to automatically discover important\ncontextual ﬁelds for each prior box, and subsequently aggre-\ngate such afﬁnity to enhance OBJ for few-shot detection. In\nour experiments, we compare our Context-Transformer with\nthese related works to show effectiveness and advancement.\n5 Experiments\nTo evaluate our approach effectively, we adapt the popular\nbenchmarks as two challenging settings, i.e., few-shot object\ndetection, and incremental few-shot object detection. More\nresults can be found in our supplementary material.\n5.1 Few-Shot Object Detection\nData Settings . First, we set VOC07+12 as our target-\ndomain task. The few-shot training set consists ofN images\n(per category) that are randomly sampled from the original\ntrain/val set. Unless stated otherwise,N is 5 in our experi-\nments. Second, we choose a source-domain benchmark for\npretraining. To evaluate the performance of detecting novel\ncategories in the target domain, we remove 20 categories of\nCOCO that are overlapped with VOC, and use the rest 60\ncategories of COCO as source-domain data. Finally, we re-\nport the results on the ofﬁcial test set of VOC2007, by mean\naverage precision (mAP) at 0.5 IoU threshold.\nImplementation Details. We choose a recent SSD-style\ndetector (Liu, Huang, and others 2018) as basic architec-\nture, which is built upon 6 spatial scales (i.e., 38 × 38,\n19 × 19, 10 × 10, 5 × 5, 3 × 3, 1 × 1). For contex-\ntual ﬁled construction, we perform spatial max pooling on\nthe ﬁrst 4 scales of source-domain score tensors, where\nthe kernel sizes are 3, 2, 2, 2 and the stride is the same\nas the kernel size. The embedding functions in Context-\nTransformer are residual-style FC layers, where input and\noutput have the same number of channels. Finally, we im-\nplement our approach with PyTorch (Paszke et al. 2017),\nwhere all the experiments run on 4 TitanXp GPUs. For\npre-training in the source domain, we follow the details of\noriginal SSD-style detectors (Liu, Huang, and others 2018;\nLiu et al. 2016). For ﬁne-tuning in the target domain, we set\nthe implementation details where the batch size is 64, the op-\ntimization is SGD with momentum 0.9, the initial learning\nrate is 4 × 10\n−3 (decreased by 10 after 3k and 3.5k itera-\ntions), the weight decay is 5 × 10−4, the total number of\ntraining iterations is 4k.\nSource Detection Transfer. The key design in Source\nDetection Transfer is OBJ. To reduce object confusion in\nthe few-shot target domain, we propose to preserve source\nOBJ, and embed Context-Transformer between source and\ntarget OBJs. In Table 1, we evaluate the effectiveness\nof this design, by comparison with baseline (i.e., tradi-\ntional ﬁne-tuning with only target OBJ). First, our ap-\nproach outperforms baseline, by adding target OBJ on\n12657\nMethod OBJ (S) Context-Transformer OBJ (T) mAP\nBaseline × × √ 39.4\nOurs\n√ × √ 40.9\n× √ √ 41.5√ √ √ 43.8√ √ →× √ 43.4\nTable 1: Source Detection Transfer. Baseline: traditional\nﬁne-tuning with target-domain OBJ. √ →× : We ﬁne-\ntune the pretrained detector with Context-Transformer in the\ntraining phase, and then unload it in the testing phase.\nContext Construction mAP Embedding Layer mAP\nWithout 42.5 Without 42.2\nPool avg 43.5 FC no residual 43.0\nPool max 43.8 FC residual 43.8\nAfﬁnity Discovery mAP OBJ (Target) mAP\nEuclidean 43.5 Separate 41.4\nCosine 43.8 Share 43.8\nTable 2: Designs of Context-Transformer.\ntop of source OBJ. It shows that, preserving source OBJ\ncan alleviate overﬁtting for few-shot learning. Second,\nour approach outperforms baseline, by adding target OBJ\non top of Context-Transformer. It shows that, Context-\nTransformer can effectively reduce confusion by context\nlearning. Third, our approach achieves the best when we em-\nbed Context-Transformer between source and target OBJs.\nIn this case, Context-Transformer can sufﬁciently leverage\nsource-domain knowledge to enhance target OBJ. Note that,\nour design only introduces 15.6K extra parameters (i.e.,\nContext-Transformer: 14.4K, target OBJ: 1.2K), while base-\nline introduces 2,860K extra parameters by adding target\nOBJ directly on top of multi-scale convolution features. It\nshows that, our design is of high efﬁciency. Finally, we un-\nload Context-Transformer in the testing, after applying it in\nthe training. As expected, the performance drops marginally,\nindicating that Context-Transformer gradually generalizes\nfew-shot detector by learning contexts during training.\nDesigns of Context-Transformer. We investigate key\ndesigns of Context-Transformer in Table 2. (1) Context Con-\nstruction. First, the pooling cases are better. It shows that,\nwe do not need to put efforts on every tiny details in the\nimages. Pooling can effectively reduce the number of con-\ntextual ﬁelds, and consequently alleviate learning difﬁculty\nin comparison between prior boxes and contexts. Addition-\nally, max pooling is slightly better than average pooling.\nHence, we choose max pooling. (2) Embedding Functions.\nAs expected, Context-Transformer performs better with FC\nlayers, due to the improvement of learning ﬂexibility. Ad-\nditionally, the residual style is slightly better than the no-\nresidual case, since it can reduce the risk of random initial-\nization especially for few-shot learning. Hence, we choose\nthe residual-style FC layers. (3) Afﬁnity Discovery. We com-\npute afﬁnity by two popular similarity metric, i.e., Cosine\n(dot-product) and Euclidean distance. The results are com-\nparable, showing that Context-Transformer is robust to the\nmetric choice. For simplicity, we choose Cosine in our pa-\nper. (4) OBJ (Target). Sharing OBJ (Target) among spatial\nNo. of Shots(N) 1235 1 0 a l l\nBaseline 21.5 27.9 33.5 39.4 49.2 80.7\nOurs 27.0 30.6 36.8 43.8 51.4 81.5\nTable 3: Inﬂuence of Training Shots.\nTrials No. 12345 mean±std\nBaseline 41.7 43.1 37.9 43.5 38.2 (Baseline)\nOurs 43.7 46.5 41.5 45.7 41.1 40.4±2.0\nTrials No. 6789 1 0 mean±std\nBaseline 40.4 38.8 40.7 38.7 40.6 (Ours)\nOurs 44.7 41.9 43.4 42.4 42.3 43.3±1.8\nTable 4: Inﬂuence of Random Trials (5-Shot Case).\nSSD-Style Framework (Liu et al. 2016) (Liu et al. 2018)\nBaseline Ours Baseline Ours\nmAP 35.3 38.7 39.4 43.8\nTable 5: Inﬂuence of SSD-Style Framework.\nscales achieves better performance. This perfectly matches\nour insight in Section 4.2, i.e., each prior box has combined\nwith its key contextual ﬁelds of various spatial scales, after\nlearning with Context-Transformer. Hence, it is unnecessary\nto assign an exclusive OBJ (Target) for each scale separately.\nInﬂuence of Shot and Framework. First, the detection\nperformance tends to be improved as the number of train-\ning shots increases in Table 3. Interestingly, we ﬁnd that\nthe margin between our approach and baseline tends to de-\ncline gradually, when we have more training shots. This\nmatches our insight that, Context-Transformer is preferable\nto distinguish object confusion caused by low data diver-\nsity. When the number of training samples increases in the\ntarget domain, such few-shot confusion would be alleviated\nwith richer annotations. But still, Context-Transformer can\nmodel discriminative relations to boost detection in general.\nHence, our approach also outperforms baseline for all-shot\nsetting. Second, our approach exhibits high robustness to\nrandom trials (Table 4), where we run our approach on ex-\ntra 10 random trials for 5-shot case. The results show that\nour approach consistantly outperforms baseline. Finally, we\nbuild Context-Transformer upon two SSD-style frameworks\n(Liu et al. 2016) and (Liu, Huang, and others 2018). In Table\n5, our approach signiﬁcantly outperforms baseline. The re-\nsult is better on (Liu, Huang, and others 2018) due to multi-\nscale dilation.\nComparison with Related Learners . We compare\nContext-Transformer with popular few-shot learners (Snell,\nSwersky, and Zemel 2017; Qi, Brown, and Lowe 2018) and\nNon-local Transformer (Wang et al. 2018). We re-implement\nthese approaches in our transfer framework, where we re-\nplace Context-Transformer with these learners. More imple-\nmentation details can be found in our supplementary ma-\nterial. In Fig. 3, Context-Transformer outperforms Proto-\ntype (Snell, Swersky, and Zemel 2017) and Imprinted (Qi,\nBrown, and Lowe 2018), which are two well-known few-\nshot classiﬁers. It shows that, general methods may not\nbe sufﬁcient for few-shot detection. Furthermore, Context-\nTransformer outperforms Non-local (Wang et al. 2018). It\nshows that, it is preferable to discover afﬁnity between prior\n12658\nFigure 3: Comparison with Related Learners (Few-Shot Object Detection). We re-implement these learners in our transfer\nframework, where we replace our Context-Transformer by them.\nBefore Incremental Split1 Split2 Split3\nST ST ST\nS\n(All)\nShmelkov2017 67.2 - 68.3 - 69.3 -\nKang2019 69.7 - 72.0 - 70.8 -\nOurs 72.9 - 73.0 - 74.0 -\nAfter Incremental Split1 Split2 Split3\nST ST ST\nS+T\n(1Shot)\nShmelkov2017 52.5 23.9 54.0 19.2 54.6 21.4\nKang2019 66.4 14.8 68.2 15.7 65.9 19.2\nOurs 67.4 34.2 68.1 26.0 66.8 29.3\nS+T\n(5Shot)\nShmelkov2017 57.4 38.8 58.1 32.5 59.1 31.8\nKang2019 63.4 33.9 66.6 30.1 64.6 40.6\nOurs 67.3 44.2 67.4 36.3 67.4 40.8\nTable 6: Incremental Few-Shot Object Detection (mAP). S:\nsource-domain classes. T: target-domain classes. More de-\ntails can be found in Section 5.2.\n0.61 \n 0.14 \n0.10 \nHorse:0.83 \n0.27 \n 0.23 \nBaseline Ours  Top-3 Contextual Fields with Their Affinity Scores  \nCar:0.67 Train:0.84 \nDog:0.70 \n0.18 \nFigure 4: Context Afﬁnity. Context-Transformer can distin-\nguish a car from a train, when it ﬁnds that there is a family\nhouse (1st context) with windows (2nd context) and entrance\nstairs (3rd context). Similarly, it can distinguish ahorse from\na dog, when it ﬁnds that there is a person (1st context) on top\nof this animal (2nd and 3rd contexts).\nboxes and contextual ﬁelds to reduce object confusion, in-\nstead of self-attention among prior boxes.\n5.2 Incremental Few-Shot Object Detection\nIn practice, many applications refer to an incremental sce-\nnario (Kang et al. 2019), i.e., the proposed framework\nshould boost few-shot detection in a new target domain,\nwhile maintaining the detection performance in the previ-\nous source domain. Our transfer framework can be straight-\nforwardly extended to address it. Speciﬁcally, we add a\nresidual-style FC layer on top of the pretrained source-\ndomain OBJ, which allows us to construct a new source-\ndomain OBJ that can be more compatible with target-\ndomain OBJ. Then, we concatenate new source and target\nOBJs to classify objects in both domains. More implemen-\ntation details can be found in our supplementary material.\nBoat \nBoat \nBoat \nAero \nPerson Person \nPerson Bicyle \nBottle \nBottle \nBottle \nBottle \nBottle \nBus \nBus \nCar \nCar \nBus \nTrain \nBaseline Ours \nFigure 5: Detection Visualization (5-Shot Case).\n% of Localization (V) Classification (V) % of Localization (V) Classification (X)\n% of Localization (X) Classification (V) % of Localization (X) Classification (X)\n56 \n51 \n35 \n39 \n4 \n6 \n5 \n5 \n58 \n52 \n16 \n25 \n8 \n8 \n18 \n15 \n60 \n51 \n27 \n31 \n2 \n3 \n11 \n15 \n57 \n47 \n24 \n34 \n8 \n6 \n11 \n13 Baseline \nOurs \n(motorbike)  (aeroplane)  \n(bus) \nBaseline \nOurs \n(horse) \nFigure 6: Object Confusion (Top-4 Improvement). Context-\nTransformer can effectively reduce the correctly-localized\nbut wrongly-classiﬁed instances (e.g., baseline vs. ours: 34%\nvs. 24% for motorbike), and promote the correct detection\n(e.g., baseline vs. ours: 47% vs. 57% formotorbike).\nTo evaluate this incremental scenario, we follow the origi-\nnal data settings of (Kang et al. 2019). There are 3 data splits\nthat are built upon VOC07+12 (train/val) and VOC07 (test).\nFor each split, there are 15 source-domain classes (base)\nand 5 target-domain classes (novel). For each target-domain\nclass, there are onlyN-shot annotated bounding boxes. We\ncompare our approach with two recent incremental detection\napproaches, i.e., (Shmelkov, Schmid, and Alahari 2017) and\n(Kang et al. 2019) As shown in Table 6, our approach can\neffectively boost few-shot detection in the target domain as\nwell as maintain the performance in the source domain, and\nsigniﬁcantly outperform the state-of-the-art approaches.\n5.3 Visualization\nContext Afﬁnity. In Fig. 4, we show top-3 important con-\ntextual ﬁelds learned by our Context-Transformer. As we\ncan see, context afﬁnity can correct object confusion to boost\nfew-shot detection. Furthermore, the sum of top-3 afﬁnity\nscores is over 0.6. It illustrates that, Context-Transformer\ncan learn to exploit sparse contextual ﬁelds for a prior box,\ninstead of focusing on every tiny detail in the image.\nDetection Visualization. In Fig. 5, we show the detection\nperformance for 5-shot case. One can clearly see that, our\napproach correctly detects different objects, while vanilla\n12659\nﬁne-tuning introduces large object confusions.\nObject Confusion Analysis. In Fig. 6, we show top-4 im-\nproved categories by our approach. As expected, Context-\nTransformer can largely reduce object confusion and boost\nperformance in few-shot cases.\n6 Conclusion\nIn this work, we propose a Context-Transformer for few-\nshot object detection. By attentively exploiting multi-scale\ncontextual ﬁelds within a concise transfer framework, it can\neffectively distinguish object confusion caused by annota-\ntion scarcity. The extensive results demonstrate the effec-\ntiveness of our approach.\n7 Acknowledgements\nThis work is partially supported by the National Key\nResearch and Development Program of China (No.\n2016YFC1400704), and National Natural Science Foun-\ndation of China (61876176, U1713208), Shenzhen\nBasic Research Program (JCYJ20170818164704758,\nCXB201104220032A), the Joint Lab of CAS-HK, Shen-\nzhen Institute of Artiﬁcial Intelligence and Robotics for\nSociety.\nReferences\nBell, S.; Lawrence Zitnick, C.; Bala, K.; and Girshick, R. 2016.\nInside-outside net: Detecting objects in context with skip pooling\nand recurrent neural networks. InCVPR, 2874–2883.\nBilen, H., and V edaldi, A. 2016. Weakly supervised deep detection\nnetworks. In CVPR, 2846–2854.\nChen, X., and Gupta, A. 2017. Spatial memory for context reason-\ning in object detection. InICCV, 4086–4096.\nDong, X.; Zheng, L.; Ma, F.; Y ang, Y .; and Meng, D. 2018.\nFew-example object detection with model communication.IEEE-\nTPAMI 41(7):1641–1654.\nFinn, C.; Abbeel, P .; and Levine, S. 2017. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In ICML, 1126–\n1135. JMLR. org.\nGirshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014. Rich\nfeature hierarchies for accurate object detection and semantic seg-\nmentation. In CVPR, 580–587.\nGirshick, R. 2015. Fast r-cnn. InICCV, 1440–1448.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural turing\nmachines. arXiv preprint arXiv:1410.5401.\nHe, K.; Gkioxari, G.; Doll´ar, P .; and Girshick, R. 2017. Mask r-cnn.\nIn CVPR, 2961–2969.\nHoffman, J.; Guadarrama, S.; Tzeng, E. S.; Hu, R.; Donahue, J.;\nGirshick, R.; Darrell, T.; and Saenko, K. 2014. Lsda: Large scale\ndetection through adaptation. InNIPS, 3536–3544.\nHu, H.; Gu, J.; Zhang, Z.; Dai, J.; and Wei, Y . 2018. Relation\nnetworks for object detection. InCVPR, 3588–3597.\nKang, B.; Liu, Z.; Wang, X.; Y u, F.; Feng, J.; and Darrell, T. 2019.\nFew-shot object detection via feature reweighting. InICCV, 8420–\n8429.\nKantorov, V .; Oquab, M.; Cho, M.; and Laptev, I. 2016. Context-\nlocnet: Context-aware deep network models for weakly supervised\nlocalization. In ECCV, 350–365. Springer.\nLai, B., and Gong, X. 2017. Saliency guided end-to-end learn-\ning forweakly supervised object detection. InIJCAI, 2053–2059.\nAAAI Press.\nLake, B. M.; Salakhutdinov, R.; and Tenenbaum, J. B. 2015.\nHuman-level concept learning through probabilistic program in-\nduction. Science 350(6266):1332–1338.\nLiu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.-\nY .; and Berg, A. C. 2016. Ssd: Single shot multibox detector. In\nECCV, 21–37. Springer.\nLiu, S.; Huang, D.; et al. 2018. Receptive ﬁeld block net for accu-\nrate and fast object detection. InECCV, 385–400.\nMottaghi, R.; Chen, X.; Liu, X.; Cho, N.-G.; Lee, S.-W.; Fidler, S.;\nUrtasun, R.; and Y uille, A. 2014. The role of context for object\ndetection and semantic segmentation in the wild. InCVPR, 891–\n898.\nOliva, A., and Torralba, A. 2007. The role of context in object\nrecognition. Trends in cognitive sciences11(12):520–527.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Y ang, E.; DeVito,\nZ.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Auto-\nmatic differentiation in pytorch.\nQi, H.; Brown, M.; and Lowe, D. G. 2018. Low-shot learning with\nimprinted weights. InCVPR, 5822–5830.\nRedmon, J.; Divvala, S.; Girshick, R.; and Farhadi, A. 2016. Y ou\nonly look once: Uniﬁed, real-time object detection. InCVPR, 779–\n788.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal networks.\nIn NIPS, 91–99.\nSantoro, A.; Bartunov, S.; Botvinick, M.; Wierstra, D.; and Lil-\nlicrap, T. 2016. Meta-learning with memory-augmented neural\nnetworks. In ICML, 1842–1850.\nShmelkov, K.; Schmid, C.; and Alahari, K. 2017. Incremental\nlearning of object detectors without catastrophic forgetting. In\nICCV, 3400–3409.\nSnell, J.; Swersky, K.; and Zemel, R. 2017. Prototypical networks\nfor few-shot learning. InNIPS, 4077–4087.\nTang, Y .; Wang, J.; Gao, B.; Dellandr´ea, E.; Gaizauskas, R.; and\nChen, L. 2016. Large scale semi-supervised object detection using\nvisual and semantic knowledge transfer. InCVPR, 2119–2128.\nTang, P .; Wang, X.; Bai, X.; and Liu, W. 2017. Multiple instance\ndetection network with online instance classiﬁer reﬁnement. In\nCVPR, 2843–2851.\nTzeng, E.; Hoffman, J.; Darrell, T.; and Saenko, K. 2015. Simul-\ntaneous deep transfer across domains and tasks. InICCV, 4068–\n4076.\nV aswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is\nall you need. InNIPS, 5998–6008.\nVinyals, O.; Blundell, C.; Lillicrap, T.; Wierstra, D.; et al. 2016.\nMatching networks for one shot learning. InNIPS, 3630–3638.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-local\nneural networks. InCVPR, 7794–7803.\nYim, J.; Joo, D.; Bae, J.; and Kim, J. 2017. A gift from knowledge\ndistillation: Fast optimization, network minimization and transfer\nlearning. In CVPR, 4133–4141.\nY oon, J.; Kim, T.; Dia, O.; Kim, S.; Bengio, Y .; and Ahn, S. 2018.\nBayesian model-agnostic meta-learning. InNIPS, 7332–7342.\n12660"
}