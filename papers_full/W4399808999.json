{
    "title": "Large language models \"ad referendum\": How good are they at machine translation in the legal domain?",
    "url": "https://openalex.org/W4399808999",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Briva-Iglesias, Vicent",
            "affiliations": [
                "Dublin City University"
            ]
        },
        {
            "id": null,
            "name": "Dogru, Gokhan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Camargo, João Lucas Cavalheiro",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W7024321072",
        "https://openalex.org/W2997848917",
        "https://openalex.org/W4315482315",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2557854651",
        "https://openalex.org/W4236715212",
        "https://openalex.org/W2884074763",
        "https://openalex.org/W3200589108",
        "https://openalex.org/W2972978787",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2893330037",
        "https://openalex.org/W4330336443",
        "https://openalex.org/W3022857843",
        "https://openalex.org/W3170656445",
        "https://openalex.org/W4320711939",
        "https://openalex.org/W3207771108",
        "https://openalex.org/W4321472057",
        "https://openalex.org/W4320167623",
        "https://openalex.org/W4362707106",
        "https://openalex.org/W4288400169",
        "https://openalex.org/W3185683705",
        "https://openalex.org/W4386197070",
        "https://openalex.org/W3186081172",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W3106146701",
        "https://openalex.org/W2889641747",
        "https://openalex.org/W4367860087",
        "https://openalex.org/W4312166536",
        "https://openalex.org/W2947824095",
        "https://openalex.org/W4318903120",
        "https://openalex.org/W4384389802",
        "https://openalex.org/W4366697322",
        "https://openalex.org/W6894204664",
        "https://openalex.org/W4321125932",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4292355384",
        "https://openalex.org/W4311000453",
        "https://openalex.org/W3132134318",
        "https://openalex.org/W4287213495",
        "https://openalex.org/W3087597081",
        "https://openalex.org/W4297833460",
        "https://openalex.org/W2960479961",
        "https://openalex.org/W4321782661",
        "https://openalex.org/W3035252911",
        "https://openalex.org/W2801219566",
        "https://openalex.org/W4386428294",
        "https://openalex.org/W2149327368",
        "https://openalex.org/W4399250045",
        "https://openalex.org/W630532510",
        "https://openalex.org/W4310829037",
        "https://openalex.org/W2974269322",
        "https://openalex.org/W2981649176",
        "https://openalex.org/W4362702134",
        "https://openalex.org/W4327525501",
        "https://openalex.org/W2982300903",
        "https://openalex.org/W4318975084",
        "https://openalex.org/W4317547647",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W3013197936",
        "https://openalex.org/W4283824044",
        "https://openalex.org/W1820007071",
        "https://openalex.org/W565372808",
        "https://openalex.org/W3036677371",
        "https://openalex.org/W3105214104",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W2978113874",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W4380488562"
    ],
    "abstract": "This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a traditional neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation metrics (AEMs) and human evaluation (HE) by professional translators to assess translation ranking, fluency and adequacy. The results indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabilities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.",
    "full_text": "MonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLARGE LANGUAGE MODELS “AD REFERENDUM”: \nHOW GOOD ARE THEY AT MACHINE TRANSLATION \nIN THE LEGAL DOMAIN?\nVicent BriVa-iglesias\nvicent.brivaiglesias2@mail.dcu.ie  \nDublin City University\nJoão lucas caValheiro camargo\njoo.cavalheirocamargo2@mail.dcu.ie  \nDublin City University\ngokhan Dogru\ngokhan.dogru@uab.cat  \nUniversitat Autònoma de Barcelona\nAbstract\nThis study evaluates the machine translation (MT) quality of two state-of-the-art \nlarge language models (LLMs) against a traditional neural machine translation (NMT) \nsystem across four language pairs in the legal domain. It combines automatic eval-\nuation metrics (AEMs) and human evaluation (HE) by professional translators to \nassess translation ranking, fluency and adequacy . The results indicate that while \nGoogle Translate generally outperforms LLMs in AEMs, human evaluators rate \nLLMs, especially GPT-4, comparably or slightly better in terms of producing contex-\ntually adequate and fluent translations. This discrepancy suggests LLMs’ potential \nin handling specialized legal terminology and context, highlighting the importance \nRecibido / Received: 09/06/2023\nAceptado / Accepted: 09/06/2023\nPara enlazar con este artículo / To link to this article:\nhttp://dx.doi.org/10.6035/MonTI.2024.16.02\nPara citar este artículo / To cite this article:\nBriVa-iglesias, Vicent; João Lucas  caValheiro camargo & Gokhan  D ogru. (2024) “Large Language \nModels “ad referendum”: how good are they at machine translation in the legal domain?” In: martínez, \nRobert; Anabel BorJa & Łucja Biel (eds.) 2024. Repensar la (des)globalización y su impacto en la traducción: \ndesafíos y oportunidades en la práctica de la traducción jurídica / Rethinking (de)globalisation and its impact on \ntranslation: challenges and opportunities for legal translators. MonTI 16, pp. 75-107.\nEste trabajo se comparte bajo la licencia de Atribución-NoComercial-CompartirIgual 4.0 Internacional  \nde Creative Commons (CC BY-NC-SA 4.0): https://creativecommons.org/licenses/by-nc-sa/4.0/.\n\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n76 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nof human evaluation methods in assessing MT quality . The study underscores the \nevolving capabilities of LLMs in specialized domains and calls for reevaluation of \ntraditional AEMs to better capture the nuances of LLM-generated translations.\nKeywords: Machine translation. Human evaluation of translation quality . Large lan-\nguage models. Translation quality . Legal translation.\nResumen\nEste estudio evalúa la calidad de la traducción automática (TA) de dos grandes mode-\nlos de lengua de última generación frente a un sistema tradicional de traducción \nautomática neural (TAN) en cuatro pares de idiomas en el ámbito jurídico. Combi-\nnamos métricas de evaluación automática con una evaluación humana de traductores \nprofesionales mediante el análisis de la clasificación, la fluidez y la adecuación de \nlas traducciones. Los resultados indican que, mientras que Google Translate suele \nsuperar a los grandes modelos de lengua en las métricas automáticas, los evaluadores \nhumanos valoran a los grandes modelos de lengua, especialmente a GPT-4, de forma \ncomparable o ligeramente mejor en cuanto a fluidez y adecuación. Esta discrepancia \nsugiere el potencial de los grandes modelos de lengua para trabajar terminología \njurídica especializada y contextualizada, lo que pone de relieve la importancia de \nlos métodos de evaluación humana a la hora de evaluar la calidad de la TA. El estu-\ndio subraya la evolución de las capacidades de los grandes modelos de lengua en \ndominios especializados y aboga por una reevaluación de las métricas automáticas \ntradicionales para captar mejor los matices de las traducciones generadas por grandes \nmodelos de lengua.\nPalabras clave: Traducción automática. Evaluación humana de calidad de la traduc-\nción. Calidad de la traducción. Grandes modelos de lengua. Traducción jurídica.\n1. Introduction\nLarge language models (LLMs), an advanced and complex artificial intelli -\ngence (AI) application built upon a vast amount of data for the generation \nof text and images, which can be particularly useful for language-related \ntasks (Naveed et al. 2023), have been the focus of attention in recent AI \nprogress/advancement in both industry and academia. Since the appear -\nance of BERT (Devlin et al. 2019), the application and potential of these \nLLMs, created from billions of data parameters in computationally inten -\nsive training processes, have multiplied and extended to multiple domains, \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 77\nsuch as healthcare (Kung et al. 2023) or the legal field (Trautmann, Petrova \n& Schilder 2022). The capabilities of these models have been increasing \nwith the emergence of language models trained with larger amounts of \ndata, both for written text (Brown et al. 2020) and for spoken text or audio \n(Radford et al. 2022). However, the biggest disruption has been caused by \nthe release of ChatGPT1, which sparked the interest of the general public in \nthese tools, and extended their use beyond research and industry, bringing \nthem closer to everyday use by laypeople, such as suggested and tested by \nYue et al. (2023).\nWith the current hype about the capabilities of these tools and all the \nattention of media, academia and industry focused on AI and LLMs, stud -\nies have recently appeared on their application and disruption in almost \nall areas of our lives, from teaching (Kasneci et al. 2023) to programming \n(White et al. 2023), or many other fields of work. However, there are also \nmany voices that have raised the alarm about the potential dangers of these \nnew technologies in the workplace and the potential loss and/or disrup -\ntion of jobs due to automation (Eloundou et al. 2023), the safety risks of \nfollowing AI-generated advice and recommendations (Oviedo-Trespalacios \net al. 2023), as well as the ethical (Zhuo et al. 2023) and privacy (Sebastian \n2023) risks that may arise in the not too distant future, calling for greater \nregulation and control of these technologies (Hacker Engel & Mauer 2023).\nGiven the current context of AI research focusing on LLMs and their \nuse in different scenarios, this article aims to analyse the capacity of \nthis new technology in the field of multilingual specialised communica -\ntion and, in particular, with respect to the quality of machine translation \n(MT) in the legal domain across four language pairs. Although LLMs have \nbeen receiving a lot of attention, AI has already been in use in translation \nthrough neural machine translation (NMT) for quite some time, and its use \nand assessment can be implemented in the same way (Ragni & Vieira 2022: \n148). Thus, a quality evaluation of the capabilities of LLMs enables us to \nascertain whether they perform better or worse than the current AI tech -\nnologies in use. Will LLMs with their decoder-only structure (Hendy et al. \n1.  Release blog post of ChatGPT by OpenAI. Online: https:/openai.com/blog/chatgpt \n(last accessed: 07/06/2023).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n78 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\n2023) be better machine translators in the legal domain than traditional \nNMT systems with an encoder-decoder architecture?\nThus, we first present a literature review of this new technology, which \nis in its infancy, and its MT capabilities. We then analyse the potential dis -\nruption of AI and LLMs and its potential application in the legal domain. We \nfollow with a comparison of translation quality of several state-of-the-art \nMT systems: a proprietary LLM (GPT-4), an open-source LLM (VICUNA 2) \nand a traditional encoder-decoder NMT system (Google Translate 3). First, \nresults from four automatic evaluation metrics (AEMs) are described to \ngauge the relative strengths and weaknesses of each system. Second, we \nshare the results of a human evaluation (HE) conducted by translators \nwith professional experience in Spanish, Catalan, Turkish and Brazilian \nPortuguese. We finally conclude with a discussion of the results and the \nimplications of using LLMs as the MT system for multilingual communica -\ntion in the legal domain.\n2. Related work: The disruption of AI and MT in the legal domain\nThe development of MT includes quality assessment as a crucial aspect that \nboth academia and industry work on (Way 2020), becoming its own sub -\nfield in MT research (Castilho & Caseli 2023). Evaluation can be performed \nthrough HE and AEMs, with varied practices for different contexts (Castilho \net al. 2018). The improvement in quality of MT systems in the legal field and \ntheir adoption in multiple fields, whether in general industry (ELIS 2022), \npatent institutions like the World International Property Organization 4, or \ninternational institutions such as the European Commission and the crea -\ntion of eTranslation5, a public MT system for the legal field, have also led to \n2.  Product page of VICUNA. Online: https://lmsys.org/blog/2023-03-30-vicuna/ (last \naccessed: 07/06/2023).\n3.  Google Translate. Online: https://translate.google.com/ (last accessed: 07/06/2023).\n4.  WIPO Translate, and MT system used for patent translations. Online: https://www.\nwipo.int/wipo-translate/en/ (last accessed: 07/06/2023).\n5.  The MT system developed by the European Comission, eTranslation. Online: \nhttps://commission.europa.eu/resources-partners/etranslation_en (last accessed: \n07/06/2023).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 79\nthe analysis of the use of these MT systems in legal institutions (Cadwell et \nal. 2016; Lesznyák 2019; Rossi & Chevrot 2019).\nIn terms of literature, MT in the legal world has been observed from dif-\nferent points of view. Firstly, focusing on the quality of automatic systems, \nsuch as Killman (2014) and the use of MT in Spanish Supreme Court judg -\nments. Another example is that of Wiesmann (2019), who analysed how \nNMT worked for translating Italian legal texts into German. In addition, \nMileto (2019) worked with students to explore their opinions on the use \nof MT in legal translation, or Briva-Iglesias (2021), who also compared the \ntranslation quality of MT engines in the legal field with the translation qual-\nity of students specialising in legal translation and refers to the application \nof MT in the legal language and the constant pressures from the language \nservices industry towards a shorter turnaround model. The other point of \nview has been computational, and different attempts have been made to \nimprove MT from the technical side to overcome some of the complexities \ncharacteristic of legal language (Gotti et al. 2008; Koehn & Knowles 2017) \nor the comparison of MT systems with high- or low-resourced language \ncombinations (Bago et al. 2022; Sosoni, O’Shea & Stasimioti 2022).\nAs much as MT has been adopted, the latest paradigm in MT, neural \nsystems, however, produces different types of errors, such as grammati -\ncal errors (Koehn & Knowles 2017) and semantically inadequate words \n(Raunak, Menezes & Junczys-Dowmunt 2021). That potentially leads \nto risks in certain scenarios, such as the legal field (Vieira, O’Hagan & \nO’Sullivan 2021), which complicates the already challenging phenomena of \nanisomorphism, partial or zero equivalence, or differences between differ -\nent legal systems during legal translations (Sarcevic 1997; Engberg 2020). \nFurther, the practice of post-editing (O’Brien 2022) has been occurring in \nthe field as a standard practice or to speed the process (Vardaro et al. 2019; \nKillman & Rodríguez-Castro 2022).\nThis indicates that the legal field has made use of AI for language-re -\nlated tasks focused on translation, but other language tasks have arisen \ninterest as well, such as AI tools providing predictions of judgements by \npresenting them with a specific and detailed legal case (Long et al. 2018), to \nfacilitate the understanding of large amounts of documents in an unknown \nlanguage and screen important information that needs to be translated by a \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n80 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nhuman (this is also called “e-Discovery”; cf. Grossman & Cormack 2010), or \nsmart contracts, which have also gained momentum and relevance recently \n(Clack 2018). However, the most recent and complex application, LLMs, \ncan perform a series of text generation tasks, which include tasks similar to \nthe ones mentioned previously (Naveed et al. 2023), including translation. \nBecause LLMs may also be used for translation (Jiao et al. 2023), studies \nhave begun to compare LLMs with NMT systems, both at sentence and \ndocument-level (Castilho et al. 2023; Wang et al. 2023; Zhang, Haddow & \nBirch 2023).\nLLMs are characterised by the implementation of a machine learning \ntechnique known as few-shot learning (Brown et al. 2020). This approach \nenables AI applications to perform tasks proficiently with a minimal \nnumber of training examples, utilising specific instructions or commands \nreferred to as prompts. Unlike NMT systems, which necessitate extensive \ntraining and fine-tuning on substantial datasets and often show limited \nflexibility beyond their trained purpose, LLMs demonstrate greater adapt -\nability. LLMs are trained on an even greater and diverse linguistic corpora \nin comparison to NMT systems, endowing them with a more comprehen -\nsive understanding of language and context. While NMT systems may be \nhighly specialised and optimised for specific translation tasks, LLMs offer \na broader range of applicability due to their ability to quickly adapt to new \ntasks with minimal additional training while also accounting for context \nin the tasks. Thus, LLMs may carry tasks with one-shot (one example) \nor few-shot (some examples), decreasing the need for task-specific data. \nFor example, Han et al. (2021) demonstrate such a technique to produce \nunsupervised MT output through GPT3 and prompting, which essentially \nis MT without parallel corpora, usually a requirement of NMT systems. \nFurthermore, the literature demonstrates potential in LLMs for transla -\ntion, such as i) stylized MT, where style, genre, register, or dialect may be \ncustomised for the output through prompting (Lyu, Xu & Wang 2023); ii) \ntranslation memory-based MT with LLMs ( Ibid.), which has potential to \nimprove the performance of the translation with fuzzy matches (Moslem et \nal. 2023); or iii) hybrid approaches combining the strengths of LLMs with \nNMT (Hendy et al. 2023).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 81\nAlthough quality evaluation has remained the same with the use of \nNMT (Ragni & Vieira 2022), we have yet to examine if that can be per -\nformed in the same way with LLMs, as there is a potential issue of “existing \nevaluation metrics may not be sufficient to capture the full range of transla-\ntion quality” (Lyu et al. 2023: 4). For this reason, different works approach-\ning translation with LLMs can provide insights in carrying out the eval -\nuation. Hendy et al. (2023) conducted sentence-level and document-level \nevaluation. In the case of the former, different AEMs were used, namely \nCOMET-22 (Rei, C. de Souza et al. 2022), COMETkiwi (Rei, Treviso et al. \n2022), SacreBLEU (Post 2018) and ChrF (Popovi ć 2015), while in that of \nthe latter, an adaptation of COMET was used. Although HE remains the \ngold standard or the norm for obtaining reliable results (Läubli et al. 2020), \nthe evaluations by Hendy et al. (2023) were exclusively automatic. Wang  \net al. (2023) also evaluated LLMs using sentence-level and document-level \nAEMs, specifically, sacreBLEU (Post 2018) both at sentence and docu -\nment-level, TER (Snover et al. 2006), and COMET (Rei, Stewart et al. 2020). \nAdditionally, they employed specific discourse metrics, such as cTT and \naZPT. Focusing on context-related issues arising from LLMs, Castilho et al. \n(2023) employed inter-annotator agreement (IAA) to provide evaluations at \nthe sentence and document levels. The DELA corpus (Castilho et al. 2021) \nalso included context-related issues, such as lexical ambiguity and termi -\nnology. Finally, for domain-specific tests, Karpinska & Iyyer (2023) tested \nBLEURT (Sellam, Das & Parikh 2020) and COMET for literary texts and \nfound that LLMs were able to produce translations that were better than \nthose provided by most NMT systems; at least according to these aforemen-\ntioned AEMs.\nConsidering the potential use for LLMs and how adaptable they can be \nfor different domains and how the few-shot technique impacts its output, \nin this article, the legal domain was chosen to test and check its capability. \nSiu (2023), for instance, focuses on the use of ChatGPT and GPT-4 for pro-\nfessional translators, addressing how LLMs can aid in identifying terms in \ndocuments with domain-specific terminology. However, issues can arise \nif LLMs are used in legal contexts, as evidenced by Noonan (2023), when \nreporting problems of bias, confidentiality, and privacy because we do not \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n82 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nknow how data is handled by the systems, which can even generate inac -\ncurate information. However, Noonan (2023) also suggests that LLMs can \nbe useful for training lawyers and law students. Thus, we believe that eval -\nuating the potential of NMT in comparison to LLMs in legal contexts may \nprovide insights in their potential, issues and evaluation approaches.\n3. Methodology\nInspired by the works mentioned in the previous sections, we aim to com -\npare the quality between two state-of-the-art LLMs and one state-of-the-art \nNMT system in the legal domain for four language pairs conducting both \nautomatic and human evaluations. While there is potential for using LLMs \nin different ways for different tasks (e.g., with written or spoken text), this \narticle focuses on using text-to-text LLMs for MT. Therefore, in this section \nwe present the methodology of the study.\n3.1. MT systems analysed\nFirst, we used GPT-4, the best ranked LLM at the date of writing (May \n2023), according to the Chatbot Arena Leaderboard (Zheng et al. 2023). \nThis leaderboard analyses the performance of LLMs in different lan -\nguage-related tasks, such as question answering, verbosity and reasoning \nability. Being a proprietary model, we do not know the number of param -\neters, but it has been reported to yield the best results in MT tasks (Jiao et \nal. 2023). We obtained GPT-4 output by creating prompts via the paid API.\nSecondly, and to observe if and to what extent there was any differ -\nence between proprietary and open-source LLMs, we analysed VICUNA, \nthe open-source LLM with the best average score in the Chatbot Arena \nLeaderboard on 22 May 20236.\nThirdly, and to compare the translation quality of LLMs with a NMT \nsystem, we used Google Translate (GT) as our state-of-the-art baseline, \nsince it is one of the most widely used systems worldwide and supports the \n6.  LMSYS chatbot leaderboard. Online: https://lmsys.org/blog/2023-05-10-leader -\nboard/ (last accessed: 07/06/2023).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 83\nlanguages we were interested in: Spanish, Catalan, Turkish and Brazilian \nPortuguese.\nIn addition to selecting systems with different characteristics to evalu -\nate the difference in MT quality between a proprietary LLM, an open-source \nLLM and a NMT engine, the selected languages allow us to make an addi -\ntional analysis according to the volume of data available for each language \ncombination. As a reference, in the OPUS corpora (Tiedemann 2012) on 31 \nMay 2023, the English-Spanish combination had 920.7 M of aligned sen -\ntences, and therefore could be considered to be a high-resourced language \ncombination; the English-Brazilian Portuguese combination had 239.9 M \nof aligned sentences, thus being considered a middle-resourced language \ncombination. Finally, the English-Turkish language combination had 82.8 \nM of aligned sentences, and the English-Catalan 33.8 M of aligned sen -\ntences. Hence, these latter language combinations could be considered as \nlow-resourced. Therefore, the intention was to analyse the MT performance \nof these different systems with a selection of languages that are resourced \nto different extents.\n3.2. T ext and translation instructions\nThe text chosen as a test set was a legal contract in English, for which the \nlength (537 words) and difficulty with a type-token ratio (TTR) of 0.305 \nwere controlled. The TTR is a metric used to measure the complexity of a \ntext, and the lower the TTR, the higher the difficulty of the text. Our TTR \nindicates that our source text was a highly specialised, complex text from \nthe legal domain. We controlled text complexity and difficulty to analyse \nMT output under the constraints of legal wording and terminology. The \nchosen text was entered directly into the Google Translate website and the \ntranslation output was generated, while the following prompt was used \nfor both of the LLMs in our study: “Please provide the [Spanish/Catalan/\nTurkish/Brazilian Portuguese] translation of the following text: [TEXT]”, \nas per the recommendations of Jiao et al. (2023). This is recommended to be \nthe best prompt for instructing LLMs to translate. Sharing this information \nwill allow for increasing replicability and reproducibility when comparing \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n84 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\ntexts with similar length and difficulty in the future with newer systems \nand analyse their changes in quality.\n3.3. Automatic evaluation of translation quality\nWe used both human and automatic evaluation methods to compare \nthe performance of each system. Though AEMs are not as reliable as HE \nmethods (Shterionov et al. 2018), they yield replicable, objective and rapid \nresults that may provide preliminary insights about comparative perfor -\nmance of each system. However, they should be complemented with HE to \ndraw more reliable conclusions about each system. In terms of AEMs, we \nused 4 metrics to measure the baseline similarity of each system against \nhuman-translated reference translations: BLEU (Papineni et al. 2002), TER \n(Snover et al. 2006), chrF3 (Popović 2015) and COMET (Rei, Stewart et al. \n2020). We specifically include COMET, a relatively new AEM, because of \nits high correlations with human judgements (Kocmi et al . 2021).\nThirty segments from the legal domain were translated by professional \ntranslators into four target languages. These segments were considered as \nthe gold standard, and were used as reference translations that were then \ncompared to outputs from three different MT systems in Turkish, Brazilian \nPortuguese, Spanish, and Catalan. We used the graphical user interface of \nMATEO (Vanroy et al.  2023) to upload the study files 7 and calculate the \nscores per each metric. Aside from calculating the scores for each metric, \nMATEO allows for taking one particular score of a system as baseline and \ncalculating the significance of its difference against those of other systems. \nThe resulting table illustrates not only the corpus-level results, but also \nthe mean and the 95% confidence intervals that have been calculated with \n(paired) bootstrap resampling.\n3.4. Human evaluation of translation quality\nQuality evaluation is one of the most discussed and analysed issues in \ntranslation and MT research (Castilho & Caseli 2023). HE of quality is \n7.  Study files for each language are provided here: https://drive.google.com/drive/\nfolders/1S-JePjC-VhcX4GMTq0kYwi3fVL899hRh?usp=sharing\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 85\nexpensive, its reproducibility may vary, and is often carried out by non-ex -\npert annotators (Castilho et al. 2018). As a consequence, evaluation by \nexpert annotators is considered a good practice in the field (Läubli et al. \n2020) because they may be able to identify errors that students or non-pro-\nfessional annotators might miss, considering non-experts may not have \nreceived formal or extended training for evaluation (Doherty 2017).\nThus, we carried out HE of translation quality with 3 professional \ntranslators with more than 5 years of professional experience in the lan -\nguage services industry, and higher education degrees in Translation and \nInterpreting. One translator performed the Spanish and Catalan evalua -\ntion, another one the Brazilian Portuguese evaluation, and a third trans -\nlator evaluated the Turkish version. TAUS DQF tools (Görög 2014) were \nused for the HE by following the evaluation methodology of Briva-Iglesias \n(2022). We run two types of HE, which were the most common according \nto both industry and academia procedures:\nFirst, we conducted an MT ranking assessment, where the evaluators \nsaw the MT output of the 3 systems being evaluated and had to assign a \nscore from 1 to 3 according to the quality of the different systems. This \nassessment provided information about which system provided better \ntranslations but did not allow for checking to what degree one system was \nbetter than the other.\nSecond, we conducted a second HE in terms of Adequacy and Fluency \nusing a Likert scale of 1-4, where the evaluators had to assign a score in \nAdequacy and Fluency to each segment. By “Adequacy” we meant the accu-\nracy of a system, i.e., whether the translation respected the message and \ncontent of the original text. Adequacy could be assessed with four differ -\nent scores: “None”, “Little”, “Most”, and “Everything”. By “Fluency”, on \nthe other hand, we meant whether the system wrote coherent sentences in \nthe target language. Fluency could be assessed with four different scores: \n“Incomprehensible”, “Disfluent”, “Good”, and “Flawless”. To homoge -\nnise the annotators’ criteria, the annotation guidelines 8 developed by \n8.  Guidelines sent to evaluators are available on: https://zenodo.org/records/7987955, \n“Translation Quality Evaluation (TQE) guidelines for assessing Adequacy and \nFluency”.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n86 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nBriva-Iglesias, O’Brien & Cowan (2023) were presented to the evaluators \nand used.\n4. Results\nSection 4 explains the results of the different translation quality evalua -\ntions conducted. First, results from AEMs are presented, which allow us to \ngrasp a fast and general idea of the overall quality of a system. Nevertheless, \nas commented above, AEMs present some limitations and, therefore, we \nalso present the results of our HE, so we can then compare both types of \ntranslation quality evaluation and discuss the global results in Section 5.\n4. 1 Automatic Evaluation Results\nTable I below summarises the results of the AEMs for all four target lan -\nguages to view the results clearly. While a higher score means better per -\nformance in COMET, BLEU and chrF2, a lower score indicates better per -\nformance in the case of TER score. An asterisk * indicates that a system dif-\nfers statistically significantly from the baseline (p<0.05). The best system \nis highlighted in bold. When looking at the overall system performance, \nGT consistently performs well across all languages and metrics, often out -\nperforming the other systems; GPT-4 shows competitive performance but \ngenerally falls short of GT’s performance, especially in BLEU, chrF2, and \nTER scores. Vicuna underperforms significantly compared to GT in all \nlanguages and metrics. Its performance is also lower than GPT-4 across \nall comparisons. According to AEMs, fifteen out of sixteen best scores \nare obtained by GT (BLEU in Brazilian Portuguese, chrF3 in Turkish and \nCOMET in Turkish). The remaining best score is achieved by GPT-4 (TER \nin Brazilian Portuguese).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 87\ntarget \nlanguage system comet↑ BLEU↑ chrF2↑ TER↓\nBrazilian \nPortuguese\nBaseline: GT 85.5 34.2 57.7 53.0\nGPT-4 85.7 31.2 56.7 56.9*\nVICUNA 76.4* 22.9* 48.5* 62.6*\nTurkish Baseline: GT 89.8 26.3 59.9 64.1\nGPT-4 87.2* 20.7* 50.9* 73.3*\nVICUNA 57.2* 6.5* 26.8* 94.9*\nSpanish Baseline: GT 84.2 28.6 57.0 53.8\nGPT-4 82.3 25.8* 55.7 57.5*\nVICUNA 73.4* 21.3* 44.4* 69.4*\nCatalan Baseline: GT 84.5 24.7 54.3 59.8\nGPT-4 82.6 23.0 53.8 61.8\nVICUNA 76.3* 17.8* 43.8* 70.1*\nTable 1. Automatic evaluation scores for four language pairs and three MT systems\nIf we only take AEMs into account, an overview of the performance across \nlanguages also highlights the global superiority of GT. In the case of trans -\nlation into Brazilian Portuguese, GPT-4 performs slightly better than GT \nin COMET, but falls behind in BLEU, chrF2, and TER scores, where GT \nis superior. Vicuna statistically significantly underperforms in all metrics \ncompared to GT, which is the baseline. In the case of Turkish, GT outper -\nforms both GPT-4 and Vicuna across all metrics. GPT-4 and Vicuna statis -\ntically significantly underperform compared to GT, with Vicuna showing a \nparticularly large gap. In Spanish translation, GT again leads in all metrics. \nAgain, Vicuna statistically significantly underperforms compared to GT in \nall metrics. Lastly, in Catalan translation, similar to Spanish, GT outper -\nforms GPT-4 and Vicuna in all metrics. Vicuna underperforms statistically \nsignificantly in all metrics compared to GT.\nIn summary, for our legal test set and according to AEMs, GT appears \nto be the most robust system across the languages and metrics tested, with \nGPT-4 being competitive but not consistently surpassing GT. Vicuna shows \nconsiderable underperformance in comparison.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n88 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\n4.2. Human Evaluation Results\nAfter presenting the results of the AEMs, this section presents the HE \nscores, so we can triangulate the data and discern which is the MT system \nthat provides the best MT output in the legal domain for the languages \nanalysed. We conducted different types of HE (ranking, adequacy and flu -\nency) for the different target languages (Brazilian Portuguese, Turkish, \nSpanish, and Catalan). Thus, below you can find a subsection for every \ntype of evaluation. These subsections start with a brief commentary of the \nglobal results, followed by an analysis of the results of every language pair.\n4.2.1. Ranking\nIn terms of overall MT ranking results, in three of the four target languages \nanalysed (Brazilian Portuguese, Spanish and Catalan), we can observe \nthat the evaluators considered GPT-4 to be the system with the highest \nnumber of segments assessed with the ranking 1 score, i.e. GPT-4 offered \nhigher quality segments than GT and Vicuna. Interestingly, these results \ndiffer from those reported by the AEMs. For the remaining target language \n(Turkish), GT was the MT system that received the ranking 1 score more \ntimes. However, if we analyse the data further, we can see that in two of the \nthree languages where GPT-4 was ranked as the best MT system, the dif -\nference with the second MT system (GT) was small. These differences are \nbetter discussed in the analysis of each target language.\nBrazilian Portuguese\nResults for Brazilian Portuguese show that GPT-4 and GT are ranked \nsimilarly, both obtaining the best ranking in 21 segments out of 30. \nNevertheless, GPT-4 has overall a slightly better performance with nine \nsegments obtaining the ranking 2 score, compared to GT, which was only \nranked seven times with this score. Vicuna performed the poorest among \nthe three and obtained the worst ranking in 24 out of 30 segments.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 89\nMonTI 16 (2024: xxx-xxx). | ISSN-e: 1989-9335 | ISSN: 1889-4178 \nIn terms of overall MT ranking results, in three of  the four target \nlanguages analysed (Brazilian Portuguese, Spanish a nd Catalan), \nwe can observe that the evaluators considered GPT-4  t o  b e  t h e  \nsystem with the highest number of segments assessed  with the \nranking 1 score, i.e. GPT-4 offered higher quality segments than \nGT and Vicuna. Interestingly, these results differ from those re-\nported by the AEMs. For the remaining target langua ge (Turkish), \nGT was the MT system that received the ranking 1 sc ore more \ntimes. However, if we analyse the data further, we can see that in \ntwo of the three languages where GPT-4 was ranked a s the best \nMT system, the difference with the second MT system (GT) was \nsmall. These differences are better discussed in the analysis of each \ntarget language. \n \nBrazilian Portuguese \nResults for Brazilian Portuguese show that GPT-4 an d  G T  a r e  \nranked similarly, both obtaining the best ranking i n 21 segments \nout of 30. Nevertheless, GPT-4 has overall a slight ly better perfor-\nmance with nine segments obtaining the ranking 2 score, compared \nto GT, which was only ranked seven times with this score. Vicuna \nperformed the poorest among the three and obtained the worst \nranking in 24 out of 30 segments. \n \n \nFigure 1. MT ranking per MT system in four language pairs \n \nTurkish \nIf we analyse the results for Turkish, GT obtained the best ranking \nin 23 segments out of 30, followed by GPT-4, which obtained the \nFigure 1. MT ranking per MT system in four language pairs\nT urkish\nIf we analyse the results for Turkish, GT obtained the best ranking in 23 \nsegments out of 30, followed by GPT-4, which obtained the best ranking \nscore 13 times. Vicuna ranked last in nearly all the segments, that is, 28 \ntimes out of 30). This finding indicates that, from the perspective of HE, \nGT outperforms LLM-based MT in this specific language pair.\nSpanish\nResults for Spanish show that GPT-4 was the system that obtained the most \nranking 1 scores (24 out of 30 times), being GT the second MT system with \n23 out of 30. This is a close follow-up which indicates that the difference in \nquality between both systems is not very far from the ranking perspective. \nAs in the previous language pairs analysed, VICUNA lagged behind and \nonly obtained the ranking 1 score three times. In addition, VICUNA was \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n90 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nranked as the worst engine in 18 segments, while GPT-4 only obtained this \nscore once, and GT none.\nCatalan\nIn the English to Catalan language pair, we can observe that GPT-4 dis -\ntanced itself from GT a bit more than in other language combinations. \nWhile GPT-4 obtained the best ranking in 24 segments, GT only did so in \n19 segments. It is also worth stressing that GT obtained the worst ranking \non one occasion, while GPT-4 did not obtain any ranking 3 score.\n4.2.2. Fluency\nIn terms of overall fluency results, GT obtained the best results in Brazilian \nPortuguese and Turkish, and tied with GPT-4 in Spanish. In Catalan, GPT-4 \nwas the MT system that was assessed with the best fluency scores. Vicuna \nclearly lagged behind in terms of fluency results in all the target languages \nanalysed. Further discussion of fluency results are included in the analysis \nof each target language.\nBrazilian Portuguese\nFor Brazilian Portuguese, GPT-4 and GT obtained comparable results in \nterms of fluency, although the latter generated one segment that was incom-\nprehensible, and GPT-4 only obtained Flawless and Good scores. VICUNA \nperformed the worst, with the least fluent segments in the majority of the \nsegments.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 91\nMonTI 16 (2024: xxx-xxx). | ISSN-e: 1989-9335 | ISSN: 1889-4178 \n \nFigure 2. Fluency results per MT system in four language pairs \n \nTurkish \nIn terms of Turkish, GT obtained the highest fluency score, obtain-\ning a Flawless score in 27 segments, followed by GP T-4, which \nwas assessed as Flawless in 21 segments. Both MT sy stems had \nonly two Disfluent segments. Vicuna’s Turkish trans lations were \nmostly incomprehensible, receiving this negative score 19 times.  \n \nSpanish \nIn terms of Spanish fluency results, both GPT-4 and  GT obtained \nidentical scores, with 24 segments out of 30 ranked as Flawless and \nsix segments ranked as Good. Once again, VICUNA obt ained the \nworst result, and the Spanish annotator considered that five seg-\nments were Incomprehensible. \n \nCatalan \nIn terms of fluency for Catalan, we can see that GPT-4 was the best \nranked system with 23 segments obtaining a Flawless fluency, \nwhile the second ranked system, GT, obtained this score in 20 \nsegments. This is not a big difference, but GPT-4 o btained no Dis-\nfluent or Incomprehensible scores in any of its segments, while GT \nobtained three Disfluent scores. Again, VICUNA lagg ed behind \nand obtained the worst scores from the different MT  systems ana-\nlysed. \n \n4.2.3. Adequacy \nFigure 2. Fluency results per MT system in four language pairs\nT urkish\nIn terms of Turkish, GT obtained the highest fluency score, obtaining a \nFlawless score in 27 segments, followed by GPT-4, which was assessed as \nFlawless in 21 segments. Both MT systems had only two Disfluent seg -\nments. Vicuna’s Turkish translations were mostly incomprehensible, \nreceiving this negative score 19 times.\nSpanish\nIn terms of Spanish fluency results, both GPT-4 and GT obtained identical \nscores, with 24 segments out of 30 ranked as Flawless and six segments \nranked as Good. Once again, VICUNA obtained the worst result, and the \nSpanish annotator considered that five segments were Incomprehensible.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n92 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nCatalan\nIn terms of fluency for Catalan, we can see that GPT-4 was the best ranked \nsystem with 23 segments obtaining a Flawless fluency, while the second \nranked system, GT, obtained this score in 20 segments. This is not a big \ndifference, but GPT-4 obtained no Disfluent or Incomprehensible scores \nin any of its segments, while GT obtained three Disfluent scores. Again, \nVICUNA lagged behind and obtained the worst scores from the different \nMT systems analysed.\n4.2.3. Adequacy\nIn terms of overall adequacy results, GPT-4 obtained the best results in \nBrazilian Portuguese and Catalan, and tied with GT in Spanish. In Turkish, \nGT was the MT system that was assessed with the best fluency scores. \nVicuna clearly lagged behind in terms of adequacy results in all the target \nlanguages analysed. Further discussion of adequacy results is included \nbelow.\nMonTI 16 (2024: xxx-xxx). | ISSN-e: 1989-9335 | ISSN: 1889-4178 \nIn terms of overall adequacy results, GPT-4 obtaine d the best re-\nsults in Brazilian Portuguese and Catalan, and tied  w i t h  G T  i n  \nSpanish. In Turkish, GT was the MT system that was assessed with \nthe best fluency scores. Vicuna clearly lagged behind in terms of \nadequacy results in all the target languages analys ed. Further dis-\ncussion of adequacy results is included below. \n \nFigure 3. Adequacy results per MT system in four language pairs \n \nBrazilian Portuguese \nAdequacy for GPT-4 and GT was also comparable for t he English \nto Brazilian Portuguese language combination, with GT providing \nonly one segment that contained None of the meaning of the source \ntext. This meant that GPT-4 was more consistent tha n GT for this \nlanguage combination. VICUNA performed the worst, w ith most \nsegments scored with Little adequacy. \n \nTurkish \nIn terms of the adequacy of the Turkish MT output, GT obtained \nthe best scores, with 22 segments assessed as conta ining all the \nmeaning of the source segment, and six segments res pecting most \nof the meaning. GPT-4 followed, with 16 target segm ents respect-\ning all the meaning of the source, and 11 target se gments having \nmost of the source meaning represented. VICUNA’s Turkish trans-\nlations were marked as totally inadequate in 25 out of 30 segments. \n \nSpanish \nFigure 3. Adequacy results per MT system in four language pairs\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 93\nBrazilian Portuguese\nAdequacy for GPT-4 and GT was also comparable for the English to \nBrazilian Portuguese language combination, with GT providing only one \nsegment that contained None of the meaning of the source text. This meant \nthat GPT-4 was more consistent than GT for this language combination. \nVICUNA performed the worst, with most segments scored with Little \nadequacy.\nT urkish\nIn terms of the adequacy of the Turkish MT output, GT obtained the best \nscores, with 22 segments assessed as containing all the meaning of the \nsource segment, and six segments respecting most of the meaning. GPT-4 \nfollowed, with 16 target segments respecting all the meaning of the source, \nand 11 target segments having most of the source meaning represented. \nVICUNA’s Turkish translations were marked as totally inadequate in 25 out \nof 30 segments.\nSpanish\nIn terms of Adequacy results in the English to Spanish combination, GT \nand GPT-4 obtained once again the same results - with 25 target segments \nconserving the whole meaning of the source text, and five segments con -\nserving most of the meaning. These two systems obtained no None or Little \nscores in terms of adequacy, so that meant that the translations produced \nhad high quality. VICUNA, on the other hand, only obtained the best rank-\ning in six of the segments, and the score Most in 12 segments. This meant \nthat 12 of its segments, almost 50% of the total, had little or no adequacy, \ndistancing itself from the other two MT systems analysed.\nCatalan\nFor Catalan, we could observe once again that GPT-4 obtained slightly \nbetter results than GT, as the former system had 22 segments ranked with \nthe best score in terms of adequacy, while GT only obtained this score in \n20 segments. Nevertheless, it is worth stressing that this difference was not \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n94 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nsubstantial. Again, VICUNA obtained the worst results in terms of ade -\nquacy in the English to Catalan language combination.\n5. Discussion of the results\nFor Brazilian Portuguese, HE results showed that GPT-4 and GT were \nconsistently good, with GPT-4 providing more consistency in fluency and \ncontext than GT, as GPT-4 provided no Incomprehensible or Disfluent \nsegments. On the other hand, GT was assigned one segment with the ade -\nquacy score None, and another segment evaluated as Incomprehensible. \nFor the AEMs, GPT-4 scored the best in TER and COMET, while GT scored \nthe best for BLEU and chrF3. This correlates with the HE in the sense that \nGPT-4 captures more of the context, correlating well with a higher score \nin COMET. These results demonstrate that GPT-4 might outperform GT in \nthe respect of context for Brazilian Portuguese, as GPT-4 gave no outputs \nthat were incomprehensible or that were unable to carry the meaning into \nthe target. If we look at specific text samples, GPT-4 maintained the termi -\nnology translated coherently through the whole document. For instance, \n“Trust Loan Sellers” was translated as “Vendedores do Empréstimo \nFiduciário”, while VICUNA could only do so partially as “Vendedores do \nEmpréstimo do Trust”. GT, even though it performed better than VICUNA, \nwas not able to translate “Trust Loan Sellers”, offering an output identical \nto the input, not translating it at all.\nFor Turkish, both HE and AEMs results strongly suggest that GT still \noutperforms both state-of-the-art LLMs systems: the proprietary GPT-4 \nand the open-source LLM system Vicuna. However, the scores achieved by \nGPT-4 were not significantly lower than GT, and when we looked at docu -\nment-level for terminology, we observed that GPT-4 provided better usage \nof terms in its adequate context, though it may not be as accurate as GT \nin sentence level. For example, the term “trust loan” occurred four times \nin the source text, and it was consistently translated as “güven kredisi” by \nGPT-4. On the other hand, GT had three different translations for the same \nsource term in the Turkish translation: “güven kredisi”, “emanet kredi”, \n“güvenlik kredisi”. This capability may be advantageous for LLM systems, \nsince terminology consistency is very important in translation, particularly \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 95\nin legal contexts. Considering that GPT-4 has only been recently launched \nand only a small percentage of its training data is from Turkish, its transla -\ntion quality may improve further with new updates and more training data \nin Turkish and specially in the legal domain.\nFor Spanish, the HE results indicated that both GPT-4 and GT offered \nsimilar quality. Though GPT-4 was in first position in terms of ranking, the \ndifference with GT - the second-best ranked MT system - was only by one \nsegment. Then, if we look at fluency and adequacy results, we can observe \nthat both systems were tied, and obtained the same excellent results. On \nthe other hand, VICUNA obtained poor results, and we can say that it was \nby far the worst performing MT system in the study. Yet, in terms of AEMs, \nwe can see that the results tended to favour GT.\nFor Catalan, we can observe that the human annotator ranked GPT-4 \nto be the best MT system in most of the cases, with a slightly bigger differ -\nence than in other language combinations. If we have a closer look at these \nresults from the perspective of adequacy and fluency scores, GPT-4 also \nmaintained its lead with respect to the other systems, though the differ -\nence regarding GT was not big. VICUNA lagged behind. While the AEMs \ndemonstrated that GT performed better, GPT-4’s scores were not far from \nthose of GT. The performance of the automatic metrics favouring GT mir -\nrored the other language pairs. By examining HE results, the overall better \nquality is not the only element that is worth stressing from GPT-4, but \nalso global terminology coherence. If we compare contextual terminology \nconsistency throughout the translations, like in Turkish, we can observe \nthat GT used both “préstec de fideïcomís” and “Préstec Fiduciari” for the \nsource text “Trust Loan”. Both options are correct in terms of a legal trans -\nlation from English into Catalan, but using two terminological choices is \ninappropriate in some professional translations, as it would be in the case \nof this contract. On the other hand, GPT-4 consistently translated “Trust \nLoan” as “Préstec Fiduciari” in every instance this term appeared.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n96 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\n6. Conclusion\nOur study has been one of the early studies to compare the translation \nquality of two LLMs against a state-of-the-art NMT system in four differ -\nent languages to analyse the relative quality of MT capability offered by \nLLM systems. We chose languages with different characteristics, that is, \na high-resourced language like Spanish, a middle-resourced language like \nBrazilian Portuguese, and two low-resourced languages like Turkish and \nCatalan. All of these languages were analysed in combination with English. \nWe first conducted an automatic quality evaluation using the most used \nAEMs in industry and academia, followed by HE with three professional \ntranslators, who evaluated the machine translations proposed by the differ -\nent systems by following the best practices for assessing translation quality \n(Läubli et al. 2020).\nBy looking at the AEMs alone, we could extract that GT was the best \nperforming system overall in terms of similarity to the reference, gold \nstandard translation. GT obtained the best scores in AEMs in 15 out of 16 \nevaluations, winning by a landslide. However, it is worth stressing that, \nwhile the automatic metrics may provide an initial insight into the perfor -\nmance of the systems, the HE provides a more comprehensive qualitative \nanalysis that investigates further key aspects that a professional translator \nwould check for a legal translation, emphasising terminology or context \ncoherence, for example. As a consequence, HE has been established as the \ngold standard method for translation quality evaluation by academia and \nindustry.\nIf we look at the HE results, we can see that GT no longer obtains the \nbest results in the evaluation. In this evaluation task, GPT-4 and GT obtain \nvery similar results. Human evaluators assessed GPT-4 and GT similarly \nas providing the most accurate and fluent output in most languages com -\nbinations analysed (Brazilian Portuguese, Spanish, and Catalan). If there \nwas a difference in these evaluations, it was by a couple of segments, which \nindicated that the difference was not substantial. Nevertheless, if we looked \nat the MT output more in-depth, we could see that GPT-4 translated key \nconcepts more consistently throughout the whole document and kept using \nthe appropriate legal terminology. GT, on the other hand, tended to use \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 97\ndifferent terms for the same concept, and was changing its choice through -\nout the text. Thus, LLMs, GPT-4 in this case, offered better contextual MT \ncapabilities for specialised legal translation.\nThe only language combination where GPT-4 and GT were not tied was \nfrom English into Turkish. HE results suggested that GT was clearly the \nbest performing MT system both in adequacy and fluency. This result may \nhave happened because GPT-4 may contain less Turkish data within its \ntraining parameters. As both systems are proprietary, we cannot fully find \nthe actual reason for this result. By following this explanation, it is inter -\nesting that Catalan - the other low-resourced language - obtained similar \nresults both for GPT-4 and GT. We think this may be due to the fact that the \nCatalan language has different open-source communities like Softcatalà 9 or \nopen-source initiatives like AINA 10 that have been making efforts to pro -\nduce and share high-quality open data for the revitalization of Catalan on \nthe Internet. The fact that Catalan is a Romance language and is more sim -\nilar to higher resourced languages like Spanish or Portuguese may also be \nan important factor that helps to improve MT results.\nThus, we can conclude that GPT-4 and GT offered similar MT quality \nresults when translating from English into Spanish, Catalan, and Brazilian \nPortuguese. The only exception in the languages analysed is when translat -\ning into Turkish, where GT clearly obtained better results, and terminology \nconsistency was the only advantage for GPT-4 in this language pair.\nTaking these results into account, we suggest using GPT-4 for MT tasks \nin high-resourced languages (specifically, the above-mentioned languages). \nIn this case, GPT-4 provided users with the advantage of maintaining ter -\nminology consistency in specialised domains such as legal texts, possi -\nbly due to the LLMs’ technique of being a few-shot learner, requiring less \ntraining data. This may provide advantages when translating legal texts \nwith new terminology or requirements, which could only be matched by \nNMT systems trained on specific terminology data or translation memories \nwith large volumes of data. Thus, the flexibility of LLMs provides a strong \nadvantage in specialised domains. Considering this finding, LLMs may be \n9.  Online: https://www.softcatala.org/ (last accessed 07/06/2023).\n10.  Online: https://github.com/projecte-aina (last accessed 07/06/2023).\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n98 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nuseful to be introduced to acquaint novice translators with terminology \nconsistency, or to train professional translators who are seeking to special -\nise into legal translation and have little data in this domain either in their \navailable NMT systems or translation memories.\nThe results of this paper also bring some interesting matters into the \nscene. If we were to trust AEMs exclusively and blindly, we would clearly \nsee that GT (a traditional encoder-decoder MT system) obtained better \nresults than LLMs. However, after looking at HE results (which have been \nreported to be the norm and the best practice in translation quality evalua -\ntion), we observed that the previous statement was not true. Is there a pos -\nsibility that AEMs have been developed taking into account how traditional \nNMT worked and the MT output generated by LLMs escapes to the textual \ngeneration form of NMT, therefore being penalised by AEMs? Could this \nmean that we should re-visit whether LLMs should be assessed with tradi -\ntional AEMs, at least in the comparison with encoding-decoding systems \nlike NMT? This aspect requires further analysis and study.\nIn addition, we should comment on the limitations of this piece of \nresearch. LLMs are in a period of constant technological development, \nand a lot has changed from the moment the experiments were first imple -\nmented towards the moment of the writing of the results and the review of \nthe paper. As such, the results may not reflect the most current develop -\nment of the technology. Another limitation is the small test set because we \nonly analysed 30 segments with HE for every MT system per language pair. \nLarger sample text could have potentially provided more reliability. Yet, \nthis paper is presented as a baseline for reviewing the application of LLMs \nfor performing MT tasks on specialised domains and shares a clear meth -\nodology and results that may allow for tracking the development of newer \nMT technologies in the future.\nReferences\nBago, Petra et alii . (2022) “Sharing High-Quality Language Resources in the \nLegal Domain to Develop Neural Machine Translation for under-Resourced \nEuropean Languages.” Revista de Llengua i Dret 78, pp. 9-34.\nBorJa, Anabel & Robert Martínez-Carrasco. (2019) “Future-Proofing Legal \nTranslation: A Paradigm Shift for an Exponential Era.” In: Simonnæs, \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 99\nIngrid & Marita Kristiansen (eds.) 2019. Legal Translation: Current Issues \nand Challenges in Research Methods and Applications . Berlin: Frank & \nTimme, pp. 187-206.\nBriVa-iglesias, Vicent. (2021) “Traducción humana vs. traducción automática: \nanálisis contrastivo e implicaciones para la aplicación de la traducción \nautomática en traducción jurídica.” Mutatis Mutandis  14:2, pp. 571-600. \nhttps://doi.org/10.17533/udea.mut.v14n2a14\nBriVa-iglesias, Vicent. (2022) “English-Catalan Neural Machine Translation: \nState-of-the-Art Technology, Quality, and Productivity.” Tradumàtica  20, \npp. 149-176. https://doi.org/10.5565/rev/tradumatica.303\nBriVa-iglesias , Vicent; Sharon O’Brien & Benjamin R. Cowan. (2023) “The \nImpact of Traditional and Interactive Post-Editing on Machine Translation \nUser Experience, Quality, and Productivity.” Translation, Cognition & \nBehavior 6:1, pp. 60-86. https://doi.org/10.1075/tcb.00077.bri\nBrown, Tom B. et alii . (2020) “Language Models Are Few-Shot Learners.” \narXiv. https://doi.org/10.48550/arXiv.2005.14165\ncaDwell, Patrick et alii. (2016) “Human Factors in Machine Translation and \nPost-Editing among Institutional Translators.” Translation Spaces  5:2, pp. \n222-243. https://doi.org/10.1075/ts.5.2.04cad.\ncao, Deborah. (2007) Translating Law . Bristol: Multilingual Matters. https://\ndoi.org/10.21832/9781853599552\ncastilho, Sheila et al. (2018) “Approaches to Human and Machine Translation \nQuality Assessment.” In: Moorkens, Joss et alii  (eds.) 2018. Translation \nQuality Assessment: From Principles to Practice. Cham: Springer International \nPublishing, pp. 9-38. https://doi.org/10.1007/978-3-319-91241-7_2\ncastilho, Sheila et alii . (2021) “DELA Corpus - A Document-Level Corpus \nAnnotated with Context-Related Issues.” In: Barrault, Loic et alii  (eds.) \n2021. Proceedings of the Sixth Conference on Machine Translation . Punta \nCana: Association for Computational Linguistics, pp. 566-577. Online: \nhttps://aclanthology.org/2021.wmt-1.63\ncastilho, Sheila et alii. (2023) “Do Online Machine Translation Systems Care \nfor Context? What about a GPT Model?” In: Nurminen, Mary et alii  (eds.) \n2023. Proceedings of the 24th Annual Conference of the European Association \nfor Machine Translation.  Tampere: European Association for Machine \nTranslation. Online: https://aclanthology.org/2023.eamt-1.39\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n100 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\ncastilho, Sheila & Helena de Medeiros Caseli. (2023) “Tradução Automática.” \nIn: Marques Seno, Eloize R. et alii (eds.) 2023. Processamento de Linguagem \nNatural: Conceitos, Técnicas e Aplicações em Português . Online: Brasileiras \nem PLN. https://brasileiraspln.com/livro-pln/1a-edicao/\nclack, Christopher. (2018) “Smart Contract Templates: Legal Semantics and \nCode Validation.” Journal of Digital Banking 2:4, pp. 338-352.\nDeVlin, Jacob et alii . (2019) “BERT: Pre-Training of Deep Bidirectional \nTransformers for Language Understanding.” arXiv. https://doi.org/10.48550/\narXiv.1810.04805\nDoherty, Stephen. (2017) “Issues in Human and Automatic Translation Quality \nAssessment.” In: Kenny, Dorothy (ed.) 2017. Human Issues in Translation \nTechnology. London: Routledge, pp. 50-78.\nelis. (2022) “European Language Industry Survey 2022.” Online: ELIS \nResearch. https://fit-europe-rc.org/wp-content/uploads/2022/03/ELIS-\n2022_survey_results_final_report.pdf?x85225\nelounDou, Tyna et alii.  “GPTs Are GPTs: An Early Look at the Labor \nMarket Impact Potential of Large Language Models.” arXiv. https://doi.\norg/10.48550/arXiv.2303.10130\nEMT. (2022) “European Master’s in Translation Competence Framework \n2022.” Online: https://ec.europa.eu/info/sites/default/files/about_the_\neuropean_commission/service_standards_and_principles/documents/\nemt_competence_fwk_2022_en.pdf.\nengBerg, Jan. (2020) “Comparative Law for Legal Translation: Through \nMultiple Perspectives to Multidimensional Knowledge.” International \nJournal for the Semiotics of Law 33:2, pp. 263-282. https://doi.org/10.1007/\ns11196-020-09706-9\ngörög, Attila. (2014) “Quantifying and Benchmarking Quality: The TAUS \nDynamic Quality Framework.” Tradumàtica  12, pp. 443-454. https://doi.\norg/10.5565/rev/tradumatica.66\ngotti, Fabrizio et alii. (2008) “Automatic Translation of Court Judgments.” \nIn: AMTA (ed.) 2008. Proceedings of the 8th Conference of the Association \nfor Machine Translation in the Americas: Government and Commercial Uses \nof MT. Waikiki: Association for Machine Translation in the Americas, pp. \n370-379. Online: https://aclanthology.org/2008.amta-govandcom.11\ngrossman, Maura R. & Gordon V. Cormack. (2010) “Technology-Assisted \nReview in E-Discovery Can Be More Effective and more Efficient than \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 101\nExhaustive Manual Review Annual Survey.” Richmond Journal of Law and \nTechnology 17:3, pp. 1-48.\nhacker, Philipp; Andreas Engel & Marco Mauer. (2023) “Regulating ChatGPT \nand Other Large Generative AI Models.” arXiv. https://doi.org/10.48550/\narXiv.2302.02337\nhan, Jesse Michael et alii . (2021) “Unsupervised Neural Machine Translation \nwith Generative Language Models Only.” arXiv. https://doi.org/10.48550/\narXiv.2110.05448\nhenDy, Amr et alii . (2023) “How Good Are GPT Models at Machine \nTranslation? A Comprehensive Evaluation.” arXiv. https://doi.org/10.48550/\narXiv.2302.09210\nJiao, Wenxiang et alii. (2023) “Is ChatGPT A Good Translator? Yes with GPT-4 \nas the Engine.” arXiv. https://doi.org/10.48550/arXiv.2301.08745\nkarpinska, Marzena & Mohit Iyyer. (2023) “Large Language Models Effectively \nLeverage Document-Level Context for Literary Translation, but Critical \nErrors Persist.” arXiv. https://doi.org/10.48550/arXiv.2304.03245\nkasneci, Enkelejda et alii . (2023). “ChatGPT for Good? On Opportunities \nand Challenges of Large Language Models for Education.” Learning \nand Individual Differences 103, 102274. https://doi.org/10.1016/j.\nlindif.2023.102274\nkenny, Dorothy. (2022) Machine Translation for Everyone: Empowering Users in \nthe Age of Artificial Intelligence. Berlin: Language Science Press.\nkillman , Jeffrey. (2014) “Vocabulary Accuracy of Statistical Machine \nTranslation in the Legal Context.” In: O’Brien, Sharon; Michel Simard & \nLucia Specia (eds.) 2014. Proceedings of the 11th Conference of the Association \nfor Machine Translation in the Americas . Vancouver: Association for \nMachine Translation in the Americas, pp. 85-98. Online: https://aclanthol -\nogy.org/2014.amta-wptp.7\nkillman , Jeffrey & Mónica Rodríguez-Castro. (2022) “Post-Editing vs. \nTranslating in the Legal Context: Quality and Time Effects from English to \nSpanish.” Revista de Llengua i Dret 78, pp. 56-72. http://dx.doi.org/10.2436/\nrld.i78.2022.3831\nkocmi, Tom et alii. (2021) “To Ship or Not to Ship: An Extensive Evaluation of \nAutomatic Metrics for Machine Translation.” In: Barrault, Loic et alii (eds.) \n2021. Proceedings of the Sixth Conference on Machine Translation . Punta \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n102 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nCana: Association for Computational Linguistics, pp. 478-494. Online: \nhttps://aclanthology.org/2021.wmt-1.57\nkoehn, Philipp & Rebecca Knowles. (2017) “Six Challenges for Neural \nMachine Translation.” In: Luong, Thang et alii  (eds.) 2017. Proceedings of \nthe First Workshop on Neural Machine Translation . Vancouver: Association \nfor Computational Linguistics, pp. 28-39. https://doi.org/10.18653/v1/\nW17-3204\nkung, Tiffany H. et alii. (2023) “Performance of ChatGPT on USMLE: Potential \nfor AI-Assisted Medical Education Using Large Language Models.” PLOS \nDigital Health 2:2, e0000198. https://doi.org/10.1371/journal.pdig.0000198\nläuBli, Samuel et alii . (2020) “A Set of Recommendations for Assessing \nHuman–Machine Parity in Language Translation.” Journal of Artificial \nIntelligence Research 67, pp. 653-672. https://doi.org/10.1613/jair.1.11371\nlesznyák , Ágnes. (2019) “Hungarian Translators’ Perceptions of Neural \nMachine Translation in the European Commission.” In: Forcada, Mikel et \nalii (eds.) 2019. Proceedings of Machine Translation Summit XVII: Translator, \nProject and User Tracks . Dublin: European Association for Machine \nTranslation, pp. 16-22. Online: https://aclanthology.org/W19-6703\nlong, Shangbang et alii . (2018) “Automatic Judgment Prediction via Legal \nReading Comprehension.” arXiv. https://doi.org/10.48550/arXiv.1809.06537\nlyu, Chenyang; Jitao Xu & Longyue Wang. (2023) “New Trends in Machine \nTranslation Using Large Language Models: Case Examples with ChatGPT.” \narXiv. https://doi.org/10.48550/arXiv.2305.01181\nmartínez-carrasco, Robert. (2022) “‘Más bellas y más infieles que nunca’. \nUsos y percepciones en materia tecnológica entre el profesorado de traduc-\nción jurídica de España.” Quaderns de Filologia. Estudis Lingüístics 27, pp. \n235-257. https://doi.org/10.7203/qf.0.24618\nmileto, Fiorenza. (2019) “Post-Editing and Legal Translation.” H2D. Revista de \nHumanidades Digitais  1:1. https://doi.org/10.21814/h2d.237\nmoslem, Yasmin et alii . (2023) “Adaptive Machine Translation with Large \nLanguage Models.” arXiv. https://doi.org/10.48550/arXiv.2301.13294\nnaVeeD, Humza et alii. (2023) “A Comprehensive Overview of Large Language \nModels.” arXiv. http://arxiv.org/abs/2307.06435\nnoonan, Nick. (2023) “Creative Mutation: A Prescriptive Approach to the Use \nof ChatGPT and Large Language Models in Lawyering.” SSRN Scholarly \nPaper. Rochester, NY. https://doi.org/10.2139/ssrn.4406907\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 103\no’Brien, Sharon. (2022) “How to Deal with Errors in Machine Translation: \nPost-Editing.” In: Kenny, Dorothy (ed.) 2022. Machine Translation for \nEveryone . Berlin: Language Science Press, pp. 105-120. https://doi.\norg/10.5281/zenodo.6759982\noVieDo-trespalacios, Oscar et alii . (2023) “The Risks of Using ChatGPT to \nObtain Common Safety-Related Information and Advice.” SSRN Scholarly \nPaper. Rochester, NY. https://doi.org/10.2139/ssrn.4346827\npapineni, Kishore et alii . (2002) “Bleu: A Method for Automatic Evaluation of \nMachine Translation.” In: Isabelle, Pierre; Eugene Charniak & Dekang Lin \n(eds.) 2002. Proceedings of the 40th Annual Meeting of the Association for \nComputational Linguistics . Philadelphia: Association for Computational \nLinguistics, pp. 311-318. https://doi.org/10.3115/1073083.1073135\npopoVić, Maja. (2015) “ChrF: Character n-Gram F-Score for Automatic MT \nEvaluation.” In: Bojar, Ond řej et alii (eds.) 2015. Proceedings of the Tenth \nWorkshop on Statistical Machine Translation . Lisbon: Association for \nComputational Linguistics, pp. 392-395. https://doi.org/10.18653/v1/\nW15-3049\npost, Matt. (2018) “A Call for Clarity in Reporting BLEU Scores.” arXiv. https://\ndoi.org/10.48550/arXiv.1804.08771\nraDforD, Alec et alii. (2022) “Robust Speech Recognition via Large-Scale Weak \nSupervision.” arXiv. https://arxiv.org/abs/2212.04356\nragni, Valentina & Lucas Nunes Vieira. (2022) “What has changed with neural \nmachine translation? A critical review of human factors.” Perspectives, 30:1, \npp. 137-158. https://doi.org/10.1080/0907676X.2021.1889005.\nraunak, Vikas et alii.  (2021) “The Curious Case of Hallucinations in Neural \nMachine Translation.” arXiv. https://doi.org/10.48550/arXiv.2104.06683\nrei, Ricardo; José G. C. de Souza et alii . (2022) “2022. ‘COMET-22: Unbabel-\nIST 2022 Submission for the Metrics Shared Task.” In: Koehn, Philipp et alii \n(eds.) 2022. Proceedings of the Seventh Conference on Machine Translation . \nAbu Dhabi: Association for Computational Linguistics, pp. 578-585. \nOnline: https://aclanthology.org/2022.wmt-1.52.\nrei, Ricardo; Craig Stewart et alii . (2020) “COMET: A Neural Framework \nfor MT Evaluation.” ArXiv: 2009.09025 [Cs], October. http://arxiv.org/\nabs/2009.09025\nrei, Ricardo; Marcos Treviso et alii . (2022) “CometKiwi: IST-Unbabel 2022 \nSubmission for the Quality Estimation Shared Task.” In: Koehn, Philipp \nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n104 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\net alii  (eds.) 2022. Proceedings of the Seventh Conference on Machine \nTranslation. Abu Dhabi: Association for Computational Linguistics, pp. \n634-645. Online: https://aclanthology.org/2022.wmt-1.60\nrossi, Caroline & Jean-Pierre Chevrot. (2019) “Uses and Perceptions of Machine \nTranslation at the European Commission.” The Journal of Specialised \nTranslation 31, pp. 177-200. https://shs.hal.science/halshs-01893120\nsarceVic, Susan. (1997) New Approach to Legal Translation. Den Haag: Kluwer \nLaw International.\nseBastian , Glorin. (2023) “Do ChatGPT and Other AI Chatbots Pose a \nCybersecurity Risk? - An Exploratory Study.” SSRN Scholarly Paper.  \nRochester, NY. https://doi.org/10.2139/ssrn.4363843\nsellam, Thibault; Dipanjan Das & Ankur Parikh. (2020) “BLEURT: Learning \nRobust Metrics for Text Generation.” In: Jurafsky, Dan et alii  (eds.) 2020. \nProceedings of the 58th Annual Meeting of the Association for Computational \nLinguistics. Stroudsburg: Association for Computational Linguistics, pp. \n7881-7892. https://doi.org/10.18653/v1/2020.acl-main.704\nshterionoV, Dimitar et alii . (2018) “Human versus automatic quality evalua -\ntion of NMT and PBSMT.” Machine Translation 32, pp. 217-235. https://doi.\norg/10.1007/s10590-018-9220-z\nsiu, Sai Cheong. (2023) “ChatGPT and GPT-4 for Professional Translators: \nExploring the Potential of Large Language Models in Translation.” SSRN \nScholarly Paper. Rochester, NY. https://doi.org/10.2139/ssrn.4448091\nsnoVer, Matthew et alii. (2006) “A Study of Translation Edit Rate with Targeted \nHuman Annotation.” In: AMTA (ed.) 2006. Proceedings of the 7th Conference \nof the Association for Machine Translation in the Americas: Technical Papers . \nCambridge, MA: Association for Machine Translation in the Americas, pp. \n223-231.\nsosoni, Vilelmini; John O’Shea & Maria Stasimioti. (2022) “Translating Law: \nA Comparison of Human and Post-Edited Translations from Greek to \nEnglish.” Revista de Llengua i Dret 78, pp. 92-120. https://doi.org/10.2436/\nrld.i78.2022.3704\ntieDemann, Jörg. (2012) “Parallel Data, Tools and Interfaces in OPUS.” In: \nCalzolari, Nicoletta et alii (eds.) 2012. Proceedings of the Eighth International \nConference on Language Resources and Evaluation . Istanbul: European \nLanguage Resources Association, pp. 2214-2218.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 105\ntrautmann, Dietrich; Alina Petrova & Frank Schilder. (2022) “Legal Prompt \nEngineering for Multilingual Legal Judgement Prediction.” arXiv. https://\ndoi.org/10.48550/arXiv.2212.02199\nVarDaro, Jennifer; Moritz Schaeffer & Silvia Hansen-Schirra. (2019) \n“Translation Quality and Error Recognition in Professional Neural \nMachine Translation Post-Editing.” Informatics  6:3, pp. 41. https://doi.\norg/10.3390/informatics6030041\nVanroy, Bram; Arda Tezcan, & Lieve Macken. (2023). “MATEO: MAchine \nTranslation Evaluation Online.” In: Nurminen, Mary et alii  (eds.) \n2023. Proceedings of the 24th Annual Conference of the European \nAssociation for Machine Translation . Tampere: European Association \nfor Machine Translation, pp. 499-500. http://hdl.handle.net/1854/\nlu-01h2ac8kf9xgq69hzmb2z3jaz9\nVieira , Lucas Nunes; Minako O’Hagan & Carol O’Sullivan. (2021) \n“Understanding the societal impacts of machine translation: a criti -\ncal review of the literature on medical and legal use cases.” Information, \nCommunication & Society 24:11, pp. 1515-1532.\nway, Andy. (2020) “Machine translation: Where are we at today.” In: Angelone, \nErik; Maureen Ehrensberger-Dow & Gary Massey (eds.) 2020. The \nBloomsbury companion to language industry studies . London: Bloomsbury \nAcademic, pp. 311-332.\nwang, Longyue et alii . (2023) “Document-Level Machine Translation with \nLarge Language Models.” arXiv. https://doi.org/10.48550/arXiv.2304.02210\nwhite, Jules et alii. (2023) “ChatGPT Prompt Patterns for Improving Code \nQuality, Refactoring, Requirements Elicitation, and Software Design.” \narXiv. https://doi.org/10.48550/arXiv.2303.07839\nwiesmann, Eva. (2019) “Machine Translation in the Field of Law: A Study \nof the Translation of Italian Legal Texts into German.” Comparative \nLegilinguistics 37:1, pp. 117-153. https://doi.org/10.14746/cl.2019.37.4\nyue, Thomas et alii. (2023) “Democratizing Financial Knowledge with ChatGPT \nby OpenAI: Unleashing the Power of Technology.” SSRN Scholarly Paper . \nRochester, NY. https://doi.org/10.2139/ssrn.4346152.\nzhang, Biao; Barry Haddow & Alexandra Birch. (2023) “Prompting Large \nLanguage Model for Machine Translation: A Case Study.” arXiv. https://doi.\norg/10.48550/arXiv.2301.07069\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\n106 Briva-Iglesias, Vicent; João Lucas Cavalheiro Camargo & Gokhan Dogru\nzheng, Lianmin et alii . (2023) “Judging LLM-as-a-Judge with MT-Bench and \nChatbot Arena.” arXiv. http://arxiv.org/abs/2306.05685\nzhuo, Terry Yue et alii . (2023) “Red Teaming ChatGPT via Jailbreaking: Bias, \nRobustness, Reliability and Toxicity.” arXiv. https://doi.org/10.48550/\narXiv.2301.12867\nBIONOTES\nVicent B riVa-iglesias  is a researcher on machine translation and \nhuman-computer interaction in the Science Foundation Ireland Centre \nfor Research Training in Digitally-Enhanced Reality (d-real), based at the \nDublin City University (DCU). In DCU, he lectures the “Localisation” \nmodule; at Universitat Oberta de Catalunya, he lectures “Introduction \nto Python for Translators/Linguists” and “Professional Translation \nInternships”. His research focuses on human-centered AI and machine \ntranslation, aiming to augment people’s abilities to empower them and \nreduce their cognitive limitations. His academic experience is influenced \nby his professional activity, since he runs AWORDZ Language Engineering, \na small company that provides language engineering, localisation and \ninternationalisation services.\nJoão lucas caValheiro camargo has a B. Ed. in Portuguese and English \nand their respective literatures from Western Paraná State University \n(UNIOESTE) in Brazil. He holds a Specialist degree in English through \ndistance learning and a Master’s in teaching at the same institution. He \nalso holds a Specialist degree in Instructional Design from Instituto de \nDesenho Instrucional. In his Master’s degree research, he designed, imple -\nmented and evaluated two translation courses (in-person and distance \nlearning) on translation hermeneutics. He was a Lecturer at the Western \nParaná State University, teaching English language teachers, Tourism and \nHospitality undergraduates. Currently, he is a PhD student funded by the \nSchool of Applied Languages and Intercultural Studies (SALIS) in Dublin \nCity University. His PhD project aims to design, implement and evalu -\nate training on human evaluation of Machine Translation to Master’s NLP \nstudents.\nMonTI 16 (2024: 75-107) | ISSN-e: 1989-9335 | ISSN: 1889-4178\nLarge Language Models “ad referendum”: how good are they at machine… 107\ngokhan  D ogru is a visiting postdoctoral researcher at ADAPT-DCU \naffiliated with the Faculty of Translation and Interpreting at Universitat \nAutònoma de Barcelona (UAB) in the framework of Margarita Salas Grant. \nHis research interests include terminological quality evaluation in machine \ntranslation, different use cases of MT for professional translators and the \nintersection of translation profession and translation technologies as well \nas localization ."
}