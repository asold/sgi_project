{
  "title": "Large language models and political science",
  "url": "https://openalex.org/W4387663109",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093068520",
      "name": "Mitchell Linegar",
      "affiliations": [
        "California Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2480062988",
      "name": "Rafal Kocielnik",
      "affiliations": [
        "California Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2097604331",
      "name": "R. Michael Alvarez",
      "affiliations": [
        "California Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5093068520",
      "name": "Mitchell Linegar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2480062988",
      "name": "Rafal Kocielnik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097604331",
      "name": "R. Michael Alvarez",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3170344956",
    "https://openalex.org/W6842080761",
    "https://openalex.org/W2582561810",
    "https://openalex.org/W3200788750",
    "https://openalex.org/W4302011534",
    "https://openalex.org/W6748382702",
    "https://openalex.org/W4297580547",
    "https://openalex.org/W6748989285",
    "https://openalex.org/W4229447062",
    "https://openalex.org/W4287887133",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4376653723",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W4316116392",
    "https://openalex.org/W4246557840",
    "https://openalex.org/W2095655043",
    "https://openalex.org/W4249194216",
    "https://openalex.org/W2760062370",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4381586491",
    "https://openalex.org/W4384694753",
    "https://openalex.org/W4241077770",
    "https://openalex.org/W6849457392",
    "https://openalex.org/W2550925836",
    "https://openalex.org/W1997669826",
    "https://openalex.org/W4382202677",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4362706570",
    "https://openalex.org/W1498571457",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W4385570473",
    "https://openalex.org/W2897042519",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4320523283",
    "https://openalex.org/W4312050653",
    "https://openalex.org/W1985949226",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6847363464",
    "https://openalex.org/W2962059918",
    "https://openalex.org/W4366732882",
    "https://openalex.org/W6810595431",
    "https://openalex.org/W6695055480",
    "https://openalex.org/W6790629862",
    "https://openalex.org/W4321596574",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4365601249",
    "https://openalex.org/W4327525621",
    "https://openalex.org/W4375870056",
    "https://openalex.org/W4321161959",
    "https://openalex.org/W3167880579",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W2616399473"
  ],
  "abstract": "Large Language Models (LLMs) are a type of artificial intelligence that uses information from very large datasets to model the use of language and generate content. While LLMs like GPT-3 have been used widely in many applications, the recent public release of OpenAI's ChatGPT has opened more debate about the potential uses and abuses of LLMs. In this paper, we provide a brief introduction to LLMs and discuss their potential application in political science and political methodology. We use two examples of LLMs from our recent research to illustrate how LLMs open new areas of research. We conclude with a discussion of how researchers can use LLMs in their work, and issues that researchers need to be aware of regarding using LLMs in political science and political methodology.",
  "full_text": "TYPE Review\nPUBLISHED /one.tnum/six.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nOPEN ACCESS\nEDITED BY\nKlaus Gerhard Troitzsch,\nUniversity of Koblenz, Germany\nREVIEWED BY\nVladimir Sazonov,\nUniversity of Tartu, Estonia\nSamar Haider,\nUniversity of Pennsylvania, United States\n*CORRESPONDENCE\nMitchell Linegar\nmlinegar@caltech.edu\nRECEIVED /one.tnum/one.tnum July /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/five.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/six.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nLinegar M, Kocielnik R and Alvarez RM (/two.tnum/zero.tnum/two.tnum/three.tnum)\nLarge language models and political science.\nFront. Polit. Sci./five.tnum:/one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Linegar, Kocielnik and Alvarez. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nLarge language models and\npolitical science\nMitchell Linegar /one.tnum*, Rafal Kocielnik /two.tnumand R. Michael Alvarez /one.tnum,/three.tnum\n/one.tnumDivision of Humanities and Social Science, California Institu te of Technology, Pasadena, CA,\nUnited States, /two.tnumComputing and Mathematical Sciences, California Institute o f Technology, Pasadena, CA,\nUnited States, /three.tnumCenter for Science, Society, and Public Policy, California Inst itute of Technology,\nPasadena, CA, United States\nLarge Language Models (LLMs) are a type of artiﬁcial intelligen ce that uses\ninformation from very large datasets to model the use of lang uage and generate\ncontent. While LLMs like GPT-/three.tnum have been used widely in many applications, the\nrecent public release of OpenAI’s ChatGPT has opened more deb ate about the\npotential uses and abuses of LLMs. In this paper, we provide a br ief introduction\nto LLMs and discuss their potential application in political sci ence and political\nmethodology. We use two examples of LLMs from our recent research to illustrate\nhow LLMs open new areas of research. We conclude with a discussion o f how\nresearchers can use LLMs in their work, and issues that researcher s need to be\naware of regarding using LLMs in political science and political me thodology.\nKEYWORDS\nLarge Language Models (LLM), ChatGPT, natural language processing, political science,\npolitical methodology\n/one.tnum. Introduction\nJust 2 or 3 years ago, few political scientists would have heard of Large Language Models.\nBut in just the past year, LLMs have jumped squarely into public consciousness, sparking\nmany discussions and debates about their uses and abuses in many diﬀerent situations. We\nanticipate that many political scientists are now considering how to study the use of LLMs\nin politics and government, or are thinking about using LLMs in their research.\nOur goal in this paper is to introduce political scientists to Large Language Models. We\nwish to inform researchers how they can use LLMs in their work, presenting examples drawn\nfrom new research that use LLMs. We also want to make political scientists better aware of\nthe issues associated with LLMs, and to provide some best practices for how the research\ncommunity should be using these innovative new natural language processing methods.\nThe paper is structured as follows. Next we provide a brief introduction to LLMs,\nfollowed by a section that discusses the current state of the art and available LLM resources.\nWe then present a number of use cases for researchers in Section 4, followed by discussion\nof important current issues regarding using LLMs in research (Sections 5, 6). Importantly,\nwe provide a discussion of best practices for research use of LLMs, before we conclude.\n/two.tnum. What are LLMs?\nLarge language models (LLMs) have recently entered the public conversation about\nartiﬁcial intelligence, as they represent a new and easy-to-use methodology for studying\nlanguage. LLMs are a new approach for natural language processing (NLP), and proponents\nhave argued that LLMs may revolutionalize the analysis of text and language data. Under the\nhood, LLMs take advantage of deep learning techniques, large scale computational resources,\nand huge quantities of training data to generate coherent and contextually relevant text.\nFrontiers in Political Science /zero.tnum/one.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nThis means that LLMs are generally useful in many diﬀerent\napplications, ranging from the analysis of text and language (NLP),\nto the generation of new text content, and thus to the further\ndevelopment of conversational bots. Our paper seeks to outline for\npolitical and social scientists how these new methods can be applied\nin research.\nText and language are information that have long been\nimportant for the study of political science. For example, textual\ndata has been used in political science to study political party\nmanifestos (\nLaver and Garry, 2000 ), political speeches ( Grimmer\nand Stewart, 2013 ), legislator communications with constituents\n(Grimmer, 2013), and social movements ( Kann et al., 2023 ). LLMs\nand other generative AI models can also be used for creating\npolitical content at scale (\nZhang et al., 2023a ) (see Figure 1).\nLLMs have great promise for studying the text and language of\npolitics, producing what will be a better understanding of political\nrhetoric and communications than was possible with previous NLP\nmethods (\nGrimmer and Stewart, 2013 ).\nArchitecturally, LLMs usually are based on neural networks,\nspeciﬁcally transformer models. Transformer models work well\nwith text data as they can detect the complexities of language using\nencoders and decoders. In a transformer model, encoders reduce\nthe dimensionality of the text into embeddings. The decoder then\nproduces some type of output that is based on the embeddings.\nTransformer models then use “self-attention” to better learn the\nlong-term dependencies in sequences of text, which helps them\nstructure the output in more meaningful and realistic ways.\nOf course, like most deep learning methods, LLMs require\ntraining data, lots and lots of high quality and hopefully unbiased\ntext. In order to learn the nuances of language, LLMs need training\ndata from text sources like blogs, social media, books, articles; in\nother words, as much readily-available text data that can be scraped\nfrom public sources. LLMs can be pretrained using these data,\nlearning the ability to predict the next set of words in a sequence\nwith missing words. They can then be ﬁne-tuned for speciﬁc\napplications, using domain-speciﬁc training data; one example\nwould be developing an LLM to summarize research articles from\na discipline like political science.\nAs we will argue in this paper, LLMs have great potential for use\nin political science. On the other hand, they also raise signiﬁcant\nissues for researchers. One important issue is that like many deep\nlearning models, LLMs are “black boxes”—their development and\nestimation is not transparent nor is it easily interpretable by or\nexplainable to humans. LLMs have the potential to be substantially\nbiased, as they rest on the quality and coverage of the data\nthey are trained on. If the training data contains biases, or does\nnot contain text from a wide sampling of sources, the outputs\nproduced by LLMs will be biased. A ﬁnal issue is that LLMs require\nvast computational resources, raising ethical concerns about their\nenvironmental impacts.\n/two.tnum./one.tnum. Understanding language model\narchitectures\nLanguage models (LMs) are computational frameworks\ndesigned to predict the likelihood of a sequence of words. At\ntheir core, they are based on the premise of understanding and\npredicting the probability distribution over sequences of words.\nGiven a sequence of words w1, w2, . . . , wt, the LM aims to predict:\nP(wt+ 1|w1, w2, . . . , wt) (1)\nwhere P(wt+ 1|w1, w2, . . . , wt) denotes the conditional probability\nof the word wt+ 1 occurring next after the words w1, w2, . . . , wt.\nTraditional language models, such as n-gram models, relied on\ncounting the occurrences of word sequences in large text corpora\nto estimate these probabilities. For instance, a bigram model,\nwhich considers only the last word to predict the next one, would\ncompute:\nP(wt+ 1|wt) = count(wt, wt+ 1)\ncount(wt) (2)\nwhere count( wt, wt+ 1) represents the number of times the word\npair ( wt, wt+ 1) appears in the corpus and count( wt) is the number\nof times the word wt appears.\nHowever, with the advent of deep learning, LMs underwent\nsigniﬁcant transformation. Neural network-based models,\nparticularly recurrent neural networks (RNNs), long short-term\nmemory networks (LSTMs), and transformers, began to dominate\nthe scene. These models compute the probability of the next word\nusing a complex function f parameterized by weights θ :\nP(wt+ 1|w1, w2, . . . , wt) = f (w1, w2, . . . , wt; θ ) (3)\nwhere these weights θ are learned by adjusting them to minimize\nthe diﬀerence between the model’s predictions and the actual next\nwords in a large training corpus.\nThe recent emergence of LLMs, like OpenAI’s GPT series,\nleverages the transformer architecture. Beneﬁting from massive\namounts of data and an extensive number of parameters, these\nmodels can memorize rare patterns, generalize across tasks, and\ngenerate coherent texts over long passages.\n/two.tnum./two.tnum. Masking and its role in language model\ntraining\nIn the context of training transformer-based models like\nBERT (Bidirectional Encoder Representations from Transformers),\nmasking plays a pivotal role. Words or tokens in the input sequence\nare randomly “masked” or hidden, and the model is trained to\npredict these masked words based on their context. This is termed\nas the “masked language model” objective. The concept of masking\nallows the model to learn bidirectional representations, as opposed\nto traditional LMs which are unidirectional (either left-to-right or\nright-to-left).\nFor a given sequence w1, w2, . . . , wt where wj is masked, the\nmodel aims to predict:\nP(wj|w1, w2, . . . , wj− 1, wj+ 1, . . . , wt) (4)\nThis process enhances the model’s ability to understand context\nfrom both sides of a word, leading to richer and more robust\nrepresentations.\nFrontiers in Political Science /zero.tnum/two.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nFIGURE /one.tnum\nAn example workﬂow illustrating how multiple Generative AImodels can be combined under a chained workﬂow to produce a political\nadvertisement.\n/two.tnum./three.tnum. Perplexity as a metric for LLMs\nPerplexity is a widely used metric for evaluating the\nperformance of language models. Intuitively, it measures how well\nthe probability distribution predicted by the model aligns with the\ntrue distribution of the words in the text. Mathematically, for a test\nset T consisting of N words, the perplexity P is deﬁned as:\nP(T) = exp\n(\n− 1\nN\nN∑\ni= 1\nlog P(wi|w1, . . . , wi− 1)\n)\n(5)\nA lower perplexity indicates that the model’s predictions are closer\nto the actual distribution of the words in the text. During training,\nminimizing perplexity is equivalent to maximizing the likelihood\nof the data, making it a valuable metric for both training and\nevaluation.\n/three.tnum. Private and open-source models\nResearchers who want to employ LLMs in their work will\ntypically access them in one of two ways: either via an API provided\nby a commercial third-party or by hosting and running open-\nsource versions of the model themselves. Commercial APIs are\nby far the easiest way for researchers to use LLMs for their own\nresearch. Startup costs are low: users are charged for each token\nthey use (“pay-per-use”), which can be done using simple, high-\nlevel APIs. These models, often trained with proprietary data and\ncomputational resources far beyond what is available to the average\nresearcher, also tend to be more powerful, generating more human-\nlike responses more quickly than can be done locally. OpenAI’s\nGPT-4, for example, is consistently found to outperform all other\npublicly available LLMs across a variety of text-based tasks, but\nits training data and architecture are not public knowledge. This\nis typical of private APIs for LLMs, emphasizing their black-box\nstatus. Privacy is another concern with commercial APIs, since\nquerying them typically requires sending data to the service itself.\nAs a result, when using highly sensitive or personally identiﬁable\ndata, using commercial APIs for LLMs (at least in their current\nstate) may not be practical for some research purposes.\nUsing open-source models can address many of the concerns\nraised above: the data, code, and hardware used to train the models\ntend to be publicly available. Since the models are run on private\nmachines and not given to a third party, data privacy concerns\ncan be minimized. A ﬁnal advantage of using open-source models\nis that it is possible to access the raw probabilities output by\nthe model. This is important both for interpretability and for\nquantifying the uncertainty inherent to generation by LLMs, and\ncan mitigate concerns about their black-box nature by making it\neasier to directly examine the probabilities associated with other\nlikely generations.\nRunning open-source models, whether locally or on a remote\nserver, can present several diﬃculties for researchers. LLMs tend\nto be large, and require being loaded onto GPU or TPU memory\nfor inference speeds to be fast enough to be practical for applied\nresearch. Though GPU prices have fallen in recent years, they\ncan still represent a signiﬁcant cost whether researchers purchase\ntheir own GPU and run locally or have access to powerful servers\nFrontiers in Political Science /zero.tnum/three.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\n(from this perspective, open-source models can be described as\n“pay-to-host”). Ongoing research, combined with eﬀorts by the\nopen-source community, have been able to reduce the hardware\nrequirements to run LLMs locally. Reducing the size of model\nweights via quantization and oﬄoading LLM layers to CPUs\nhave both proven successful at reducing barriers to entry. These\nproblems are exacerbated as model size continues to increase.\nThough larger models are generally more powerful, they also\nrequire signiﬁcantly more resources to run than small ones. One\npromising alternative for researchers is to ﬁne-tune small models,\ntraining them on the speciﬁc tasks required for research (\nHu\net al., 2021 ). Refer to Section 6 for more details about how to use\nopen-source models for applied research in practice.\n/four.tnum. Potential applications of LLMs in\npolitical science\nOne of the primary advantages of LLMs over previous models\nis their ﬂexibility. It is often easier to change a prompt than retrain a\nmodel or use a new one. In this section, we highlight ways in which\npolitical science may be aﬀected by this emerging technology.\n/four.tnum./one.tnum. Studying the eﬀects of LLMs\n/four.tnum./one.tnum./one.tnum. Prevalence and impact of AI-generated\ncontent\nPolitical campaigns are already using LLMs to generate\nadvertising content. A recent example is a widely viewed dystopian\nadvertisement on YouTube, titled “Beat Biden”\n/one.tnumand posted by the\noﬃcial RNC YouTube account. This advertisement is described\nas “An AI-generated look into the country’s possible future if\nJoe Biden is re-elected in 2024.” There is no doubt that we will\nsee an explosion of advertisements generated by LLMs and other\ngenerative AI tools in future elections (\nAlvarez et al., 2023 ). In\nFigure 1, we give an example about how such a workﬂow might be\ndeveloped.\nThese technologies are also being used by popular political news\nmedia like The Daily Show /two.tnum(Bloomberg, 2023). Other news media\norganizations will no doubt be using LLMs to generate content for\nthe 2024 U.S. elections and for other elections in other nations.\nWe anticipate that the lines between human-generated news media\ncontent and content generated by LLMs and AI will start to blur in\nthe next few years.\nWe also anticipate that these methodologies will be used to\ngenerate misinformation and fake news content. We will see fake\ncampaign ads, false and misleading fake news content, content\nthat deliberately misinforms voters about the election process to\ndisenfranchise them, and many other uses of these technologies to\nproduce misleading and misinforming materials. LLMs will also be\nused by those producing fake and misleading political content to\naid in avoiding detection and to help develop strategies to facilitate\nthe distribution and targeting of this fake and misleading content.\n/one.tnumhttps://youtu.be/kLMMxgtxQ/one.tnumY\n/two.tnumhttps://www.youtube.com/watch?v=JONzK-AUzro\nLLMs and other generative AI methods have dramatically\nreduced the costs of producing highly-realistic and seemingly real\ncontent. As a proof of concept demonstration, one of the authors\nused various free or inexpensive LLM-based methods to generate\na quite realistic news interview and also a realistic short (but false)\nnews story.\n/three.tnumThe content we produced was made within 30–60 min,\nand demonstrates the ease with which content that could appear\nvery realistic on social media platforms, or on the small screen, can\nbe generated.\nThese many uses of LLMs in politics, media, and the\ngovernment, open the door for important research opportunities\nfor social scientists. On one hand, studying how candidates\nuse LLMs and generative AI in their campaigns will no doubt\nbecome an important research area for those who study political\ncommunications. In particular, it will be important to see how\nthe new technologies change the development and distribution of\ncampaign materials. As LLMs have dramatically reduced the costs\nof producing content, especially audio and video content that in\nthe past would require studios and production teams, we anticipate\nthat many campaigns that in the past would not have used these\ntypes of materials will now do so—for example, candidates running\nfor local and municipal oﬃces, who may now use highly polished\nvideo materials in their campaigns. For the campaigns of national\ninterest that already spend huge amounts in digital advertising,\nLLMs will likely be used to target increasingly speciﬁc groups of\nvoters. For example, presidential campaigns could use LLMs to\ngenerate many diﬀerent versions of campaign ads, explaining the\nramiﬁcations of their policy positions to individual towns, and even\nreferencing speciﬁc local issues and landmarks. The consequences\nof the coming surge of individually-tailored campaign ads are\nnot yet well-understood. Nor are the strategies that campaigns\nwill actually employ. As it will become increasingly diﬃcult to\ntrack political messaging, campaigns may even be able to claim\nopposite policies to diﬀerent low-information groups of voters\nwithout consequence.\n/four.tnum./two.tnum. LLMs and politics: threats and\ncounter-measures\nThe potential for LLMs to aﬀect electoral outcomes has become\nan area of concern for policymakers and academics. In this section,\nwe detail ways in which LLMs are likely to aﬀect future elections\nand highlight potential counter-measures.\n/four.tnum./two.tnum./one.tnum. Threats to electoral integrity and political\ninformation\nIt is well-known that social media is a vector of fake news and\nmisinformation that can threaten elections (\nAllcott and Gentzkow,\n2017). The advent of LLMs will likely increase the amount of\nmisinformation voters are exposed to, as they will dramatically\ndrive down the cost of “micro-targeted” messages.\nOne might imagine a future where misinformation campaigns\nsubtly instruct voters to print their name on ballots in states\n/three.tnum These can be viewed athttps://bit.ly//four.tnum/six.tnumM/six.tnum/seven.tnumyoand https://bit.ly//three.tnumpA/one.tnumas/four.tnum.\nFrontiers in Political Science /zero.tnum/four.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nwhere signature matching is pivotal. Such nuanced misinformation\ncould invalidate thousands of votes. Similarly, the intricacies of\nvoter registration, often a labyrinthine process for many, can be\nmade even more convoluted by LLM-generated misinformation,\npreventing eligible citizens from casting their votes.\nThe capabilities of LLMs extend beyond mere text. Video\ncontent, once the domain of high-cost productions, can now be\nmanipulated or entirely fabricated with the assistance of these\nmodels. This raises the possibility of misleading videos, portraying\npoliticians in fabricated scenarios, or twisting real events to ﬁt a\nparticular narrative (see\nFigure 1).\nThe ability of LLMs to produce vast amounts of genuine\ncontent can also be leveraged to cheaply establish trust with\naudiences. Once this trust is cultivated, it becomes easier to\nintroduce misinformation into the stream of content, making it\nharder for consumers to discern fact from ﬁction.\nBeyond the electoral sphere, we should also consider\nthe implications of LLMs in times of crisis. Misinformation\nduring emergencies can have dire consequences. Picture a\nscenario where false emergency alerts or misleading updates are\ndisseminated during critical events, leading to public panic or\nmisallocated resources.\n/four.tnum./two.tnum./two.tnum. Counter-measures and safeguards\nCountering the threats posed by LLMs requires a multifaceted\napproach. Public awareness campaigns can play a pivotal role.\nBy illuminating the capabilities and potential pitfalls of LLM-\ngenerated content, we can arm the public with the knowledge\nto critically evaluate the information they consume. However,\nas LLMs become more sophisticated, it will become increasingly\ndiﬃcult for consumers to identify whether a given piece of content\nis artiﬁcially generated, or whether the information conveyed is\nboth factual and has the appropriate context.\nOne proposal that has gained traction is “watermarking”\ncontent generated by LLMs. While this proposal will not be\nsuﬃcient to curb misinformation (it is easy to train LLMs not\nto contain a watermark, and very diﬃcult to identify whether\na given piece of text is machine-generated), it may be able to\nprovide a means of veriﬁcation that a given LLM has been trained\nto be more factual, or that the LLM has been endorsed by a\nparticular organization.\nTrained appropriately, LLMs can aid in the rapid fact-checking\nof content, ﬂagging inconsistencies or potential falsehoods. Doing\nso at scale will likely require collaboration with social media\nplatforms. By monitoring and ﬂagging content suspected to\noriginate from LLMs, platforms can provide users with the context\nneeded to evaluate the information.\nThus, if we are correct and there will also be widespread\ndevelopment and distribution of fake and misleading content, this\nwill also open the door for researchers. It will be important to\ncontinue to develop methodologies for the real-time detection of\nfalse and misleading information online (e.g.,\nSrikanth et al., 2021 ).\nResearchers will need to continue to study how misinformation is\nprocessed by individuals and to develop means to counter false and\nmisinformative content (e.g.,\nvan der Linden, 2023 ). These are areas\nof research that will need substantial resources and cooperation\nwith private organizations if academic researchers are to be eﬀective\nin helping to deﬂect the eﬀects of political disinformation.\n/four.tnum./three.tnum. Research uses of LLMs\n/four.tnum./three.tnum./one.tnum. Replacing manual processes\nThe practical use-cases of LLMs extend across the spectrum of\npolitical science and computational social science research (\nZiems\net al., 2023 ). One of the most signiﬁcant beneﬁts of deploying LLMs\nis their ability to replace manual annotation eﬀorts, particularly\nin processing political content. Leveraging the learning capabilities\nof LLMs, researchers can eﬃciently identify elements like toxicity,\npolitical polarity, sentiment, and hate speech within a text. Such\nuse-cases have been addressed by tools like ToxicBERT and\nPerspective API, which harness the power of LLMs to automate\ntasks traditionally performed by humans (\nKocielnik et al., 2023a ;\nLiang et al., 2023 ; Mendelsohn et al., 2023 ).\nWhen it comes to information extraction, LLMs exhibit\nsubstantial advantages over conventional NLP algorithms. While\npre-existing NLP algorithms may perform exceptionally well at\nspeciﬁc tasks, they often fall short in dealing with tasks that require\nan understanding of context. For instance, if a researcher aims to\nidentify the politicians mentioned in a tweet and the sentiment\nexpressed toward them, an LLM using few-shot learning is likely\nto outperform separate Named Entity Recognition and Sentiment\nAnalysis algorithms. The underlying reason lies in the superior\nability of LLMs to interpret context, making them ﬂexible tools for\ncomplex NLP tasks. This understanding of context is part of what\nmakes LLMs useful for document summarization, a potentially\nuseful tool for understanding the large bodies of text often found\nin political science research.\nMoreover, LLMs can play a crucial role in generating new\ncontent for research purposes. Tools like AutoBiasTest (\nKocielnik\net al., 2023b ) and other model written evaluations ( Perez et al.,\n2022) are especially noteworthy for generating politically biased\ncontent, which researchers can further analyze to study the\ndynamics of political bias. Furthermore, LLMs can be prompted\nto generate particular types of content, such as hate speech,\ntoxicity, and stereotypes related to political stances, which can be\nused to study the social perceptions in the underlying data or\nfor experiments related to political communication (\nWest, 2023 ).\nLLMs have also been used in a variety of other text generation\napplications. For instance, LLMs have been used to inject persuasive\ncontent at scale (\nJingnan, 2023 ), and to generate realistic data\nto simulate multiple humans and replicate human subject studies\n(\nAher et al., 2023 ).\n/four.tnum./three.tnum./two.tnum. Understanding political speech\nSlanted news drives polarization ( Martin and Yurukoglu,\n2017), but the aspects of political speech that aﬀect political\npolarization and ideology are not themselves well-understood. In\npart this is because the systematic analysis of speech is diﬃcult.\nLLMs can aide social scientists by simplifying the process of\ninformation extraction, for example by discretizing pieces of text\ninto variables relevant to the research at hand, summarizing\nFrontiers in Political Science /zero.tnum/five.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nlengthy texts, categorizing qualitative data, classifying sentiment,\nor by otherwise reducing the dimensionality of the space of text.\nThis concept is similar to dimension reduction techniques, where\nhigh-dimensional data is transformed into a lower-dimensional\nspace, preserving as much relevant information as possible while\ndiscarding noise or redundancy. Put diﬀerently, we can use the\ncomplexity of LLMs to ask simple and easily interpretable questions\nabout observed political speech. This provides a systematic\nway to understand how speciﬁc variations in speech aﬀect its\npersuasiveness. By focusing on easy-to-interpret answers, this\nprocess is easy to validate, and can replace what would otherwise\nrequire intensive human eﬀort and discretion.\nAnother approach is to build on foundational models for\nunderstanding political speech like NOMINATE scores (\nPoole and\nRosenthal, 1985 ). One example of this is in trying to estimate\nthe ideological position of diﬀerent speakers on cable news. LLMs\ncan simplify the process, for example by providing a new way\nof mapping political speech from a domain that is less-well\nunderstood like slanted cable news to pre-existing ideological\nspaces that researchers may be more comfortable with.\n/four.tnum./four.tnum. Toward multimodal research\nThe application of LLMs in political science research presents\nan opportunity for a uniﬁed approach to analyze multimodal data.\nThis approach consists of interconverting diﬀerent types of data,\nsay a campaign video and its textual summary, facilitating the\nunderstanding of one medium in terms of the other. To illustrate,\na researcher might transform a campaign video into a compact\ntextual format by using an LLM trained for speech recognition to\ntransform spoken language into written text (\nRadford et al., 2023 ),\nand use a multi-modal model to identify and describe key visual\ncomponents of the video, retaining pertinent characteristics. The\nsummary can then be used to regenerate a similar video. This ability\nof LLMs like GPT-4 to process and translate between text and image\ninputs, oﬀers a consistent methodology to map from the video\nand transcript space to a smaller and more informative summary\nspace. We anticipate this technology will be particularly useful in\nnew areas of research in political communication, including for\nstudying the contents of political advertisements, the dynamics of\ntelevised appearances, how news media frames particular political\ncandidates, and for analyzing online political discussions.\n/five.tnum. Issues with using LLMs in research\nThe output of LLMs is inherently a function of the data used\nto train it. This means that biases present in the training data are\nlikely to be perpetuated in the ﬁnal model. Large-scale scrapes of the\ninternet (such as those used in The Pile\nGao et al., 2020 , a common\ndataset used for training LLMs), are likely to contain large amounts\nof such bias. Though eﬀorts can be made to censor the model’s\noutput, this censorship will only prevent the LLM for recreating\nthe internet’s worst tendencies, not prevent it from perpetuating\nsubtle biases.\nHow social bias manifests itself in the output of LLMs is thus\na key concern when deploying LLMs. Biased data can lead models\nto disproportionately represent majority viewpoints, leading to a\nsystematic marginalization of minority data. Such bias can subtly\nyet profoundly skew the outputs of the models, creating outcomes\nthat may reinforce existing societal prejudices. In the context of\npolitical discourse, this could manifest as a bias toward particular\npolitical parties or viewpoints, potentially inﬂuencing research\nconclusions. For example,\nMotoki et al. (2023) ﬁnd that ChatGPT\ndisplays bias toward the Democratic party, and Feng et al. (2023)\nﬁnd that language models have political leanings and can reinforce\npolitical polarization and other social biases. Though uncensored\nversions of models can mitigate this problem to some degree (see\nSection\n7 for more details), these models will likely perpetuate bias\nin other directions.\nSeveral types and sources of social bias in AI systems have been\nidentiﬁed in prior work ( Mehrabi et al., 2021 ). Issues of potential\nsocial and stereotypical bias need to be carefully considered when\napplying AI and especially LLMs to analysis on real-world data\nor in applications impacting society.\nGoogle (2022) presents a\ndetailed discussion about social bias and AI. There are two primary\ncategories of social bias political scientists should be concerned\nwith. The ﬁrst categories concerns the data used to train LLMs.\nThese issues can generally be boiled down to Reporting Bias and\nSelection Bias . The second category concerns the output of LLMs,\nwhether this output constitutes labels or generated text. These\nissues can generally be described as Group Attribution Bias or\nextensions of observed Implicit Bias . Both involve the tendency to\nstereotype minority groups, which may result in diﬀerent degrees\nof accuracy for LLM-based classiﬁers across groups.\nAbid et al.\n(2021) provide a clear example of this, showing that LLMs can\nperpetuate biases against minority groups through stereotypes. In\nparticular, they show that LLMs can convey a strong association\nbetween Muslims and violence.\n/five.tnum./one.tnum. Fairness in AI\nEnsuring fairness in AI is challenging due to the lack of\ninterpretability of the models, and the bias present in the training\ndata, yet crucial, due to the pervasive integration of AI and\nmachine learning systems in diverse applications with direct\nsocietal impact. These applications range from court systems\nassessing reoﬀending probabilities, to medical ﬁelds, childhood\nwelfare systems (\nChouldechova et al., 2018 ), and autonomous\nvehicles. When applying AI to practical scenarios and real-world\ndata it is important to consider aspects of fairness as inherent biases\ncan have detrimental eﬀects on many levels.\nOsoba and Welser\n(2017) list examples of biases in real-world applications of AI,\nincluding bias in AI chatbots, employment matching, ﬂight routing,\nautomated legal aid for immigration algorithms, and search and\nadvertising placement algorithms (\nOsoba and Welser, 2017 ). Bias\ncan also manifest in real-world AI and robotic systems, such as face\nrecognition and voice recognition applications, and search engines\n(\nHoward and Borenstein, 2018 ).\nDiscriminatory behavior in AI systems is a notable problem.\nFor instance, the COMPAS risk assessment software has been\nidentiﬁed as biased and its performance questioned when\ncompared to human judgment (\nLambrecht and Tucker, 2019 ).\nFrontiers in Political Science /zero.tnum/six.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nAnother example is an advertisement algorithm promoting STEM\nﬁeld jobs which, despite intending to be gender-neutral, was found\nto show fewer ads to women due to the gender imbalance in the\ntarget audience. Bias has also been observed in facial recognition\nsystems (\nBuolamwini and Gebru, 2018 ; Raji and Buolamwini, 2019 )\nand recommender systems ( Schnabel et al., 2016 ), often leading to\ndiscrimination against certain populations and subgroups. These\nexamples highlight the critical need for engineers and researchers\nto understand the sources of these biases and take preventive\nmeasures during the design and engineering process of AI systems.\nEnsuring fairness is not just about maintaining balance, but about\nminimizing harm and potential detrimental eﬀects on society.\nFairness in AI, as examined across various research,\nencompasses numerous deﬁnitions rooted in diﬀerent ﬁelds\nincluding political philosophy, education, and machine learning\n(\nMehrabi et al., 2021 ). Various deﬁnitions of fairness in AI exist,\nmaking it challenging to achieve fairness in practices across these\ncompeting deﬁnitions. These deﬁnitions can be grouped into\nthree broad categories: Individual Fairness , which emphasizes\nsimilar predictions for similar individuals; Group Fairness , which\nadvocates for treating diﬀerent groups equally; and Subgroup\nFairness, a hybrid approach aiming to combine the beneﬁts of both\nindividual and group fairness.\n/five.tnum./two.tnum. Other challenges\nAnother issue is that LLMs are sensitive to variations in\nsemantically irrelevant inputs\n/four.tnum, making it diﬃcult to discern the\neﬀects of seemingly minor diﬀerences across prompts. Adding\nto this are the diﬃculties presented by hallucinations, where\nmodels generate false data due to high probability assigned to\nuntrue statements. Detecting and mitigating these hallucinations in\nautomated ways is particularly challenging. Since the hallucinations\nare often based on the model’s training data, they can be highly\ncontext-speciﬁc and diﬃcult to predict. Furthermore, because\nLLMs generate outputs probabilistically, they may not consistently\nproduce the same hallucinations, making these errors even harder\nto catch.\n/five.tnum./three.tnum. Ethical considerations\nDeploying LLMs in social science research has important\nethical considerations. The use of these models often requires\ntrusting in the “black box” nature of the algorithms, particularly\nthose developed by private-sector entities. This means that any\nbiases in the training data can inadvertently be introduced into\nthe research, perpetuating subtle biases that can skew the results.\nIt is crucial for researchers to remember that LLMs are not a\ndeﬁnitive source of truth, but rather a representation of the data\nthey were trained on. Thus, by accepting a model without a\nthorough examination of its biases, researchers implicitly trust\n/four.tnumhttps://huggingface.co/blog/evaluating-mmlu-leaderboard?fbclid=IwA\nR/zero.tnum/four.tnumlwlW/three.tnumeZTXz/seven.tnumYBxpgL/seven.tnumF/four.tnumb/one.tnumpaMwmYpuo/four.tnummdKNgtMkTIRs/seven.tnumJa/five.tnumx/seven.tnumGUAx/four.tnum\nthat the creators of the model have trained it with a bias they\nﬁnd acceptable.\nAnother issue in using LLMs pertains to the lack of explicit\nconsent in using individuals’ data for model training. This\nconcern is particularly pronounced in models designed for chat\nfunctionalities, such as OpenAI and Google’s Bard. In these\ninstances, the models’ terms of use often allow the use of chat data\nfor further model training, potentially infringing upon user privacy.\nMoreover, this issue extends to intellectual property rights, as\ndemonstrated by ongoing lawsuits by artists against organizations\nlike OpenAI.\n/five.tnumThese cases underscore the concern over the use\nof copyrighted material within the training data, again without\nexplicit permission.\n/six.tnum. LLMs and reproducible research\nResearch replication and reproducability have long been\nimportant research best practices in political science ( King, 1995 ;\nAlvarez and Heuberger, 2022 ). LLMs present important challenges\nfor researchers and publishers with respect to replication, in\nparticular regarding transparency in LLM development, clarity\naround the datasets and benchmarks used, standardized model\nevaluation rankings, and journal policies for provision of\nreplication materials prior to publishing.\n/six.tnum./one.tnum. LLM development transparency\nEﬀorts in enhancing the transparency of AI models have been\ngaining momentum over the last few years. Prominent initiatives\nin this regard include the use of “Model Cards” (\nMitchell et al.,\n2019) which provide a detailed snapshot of a model’s purpose,\nperformance, limitations, and biases in a structured manner. They\nact as a kind of report card, providing relevant information about\na model, and making it more interpretable and explainable to\nend-users. Unfortunately, the level of detail and the quality of a\nModel Card relies on the voluntary eﬀort of model developers.\nPlatforms such as HuggingFace provides guides for creating high-\nquality model cards (\nHugging Face, 2023a ), but these are not\nalways followed and in many cases, Model Cards are preﬁlled\nautomatically, often resulting in their poor quality.\nRecent initiatives such as “Interactive Model Cards” (\nCrisan\net al., 2022 ), “AutoBiasTest”(Kocielnik et al., 2023b ), and model\nwritten evaluations focus on interactive tools, that support live\nexploration of model capabilities and limitations using generated\ndatasets or human-in-the-loop evaluation. Recent eﬀorts in AI\ntransparency put emphasis on user-friendly tools that can be used\nby various non-AI experts with relevant social expertise (e.g., social\nscientists, gender studies researchers, ethics experts) as well as\npractitioners in domains where various AI tools can be applied\n(e.g., clinicians, chemists, content writers;\nKocielnik et al., 2019 ;\nRastogi et al., 2023 ). These eﬀorts collectively aim to demystify AI,\nmaking it more accessible, understandable, and ultimately more\naccountable.\n/five.tnumhttps://ymcinema.com//two.tnum/zero.tnum/two.tnum/three.tnum//zero.tnum/two.tnum//one.tnum/five.tnum/midjourney-is-being-class-\naction-sued-for-severe-copyright-infringements\nFrontiers in Political Science /zero.tnum/seven.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\n/six.tnum./two.tnum. Datasets and benchmarks transparency\nIn relation to data used for model training and evaluation,\nthe prominent NeurIPS conference recently introduced a separate\ntrack called the “Datasets and Benchmarks Track” focused\nspeciﬁcally on obtaining high-quality datasets and benchmarks,\nbut also on reﬁnement of existing datasets (\nDenton et al., 2023 ).\nTo try to enforce high quality of submissions, this track requires\nthe use of several transparency tools related to datasets. The\n“Datasheets for Datasets” (\nGebru et al., 2021 ) initiative encourages\ncomprehensive documentation for datasets used in AI model\ntraining, including data collection processes, motivations, biases,\nand ethical considerations. This can be likened to “nutrition labels”\nfor data (\nHolland et al., 2020 ), oﬀering a transparent look at the raw\nmaterials that feed into AI systems. Furthermore, the development\nand implementation of accountability frameworks are essential to\nensure that those who develop and deploy AI systems are held\nresponsible for their actions.\n/six.tnum./three.tnum. Model evaluation rankings\nArguably, most of the well-established evaluation practices for\nLLMs are various benchmarks and rankings (\nRamanathan, 2022 ;\nCeylan, 2023). Open LLM Leaderboard ( Hugging Face, 2023b ) and\nSuper-GLUE ( Wang et al., 2019 ) (a benchmark suite designed to\nevaluate the performance of LLMs on a range of demanding natural\nlanguage understanding tasks) are some of the popular benchmarks\nthat provide frameworks for comparing and evaluating these\nmodels on various aspects, such as accuracy, ﬂuency, coherence,\nand subject relevance.\nBenchmarking LLM performance requires careful selection of\nevaluation tasks, data preparation, and comparative analysis.\nBenchmarking LLMs is crucial not just for performance\nassessment, but also for detecting and mitigating biases, and\nassessing user satisfaction and trust (\nHuang et al., 2023 ). Diﬀerent\nevaluation methods such as Perplexity ( Chiusano, 2022 ), human\nevaluation ( Liang et al., 2022 ), BLEU, ROUGE ( Santhosh, 2023 ),\nsocial bias scores ( Delobelle et al., 2022 ), and diversity measures\nare used for diﬀerent aspects of performance. However, they come\nwith challenges such as subjectivity in human evaluations, lack of\ndiversity in metrics, lack of generalization to real-world scenarios,\nand susceptibility to adversarial attacks.\nTo overcome these challenges, best practices involve using\nmultiple evaluation metrics, enhancing human evaluation, creating\ndiverse reference data, incorporating diverse metrics, augmenting\nevaluation methods with real-world scenarios, and evaluating\nLLMs for robustness against adversarial attacks. Several popular\nAI development frameworks oﬀer standardized evaluation of code-\nbased tools that can be run by developers of LLMs and reproduced\nby other researchers. The most popular one is arguably Eluther AI’s\nLM evaluation harness (\nElutherAI, 2023).\n/six.tnum./four.tnum. LLMs and replication materials\nMost political science research journals either require that\nauthors make their code and data available upon publication in\na public and permanent repository, or they strongly suggest that\nauthors follow this best practice upon publication. Usually this\nmeans that authors will provide some documentation regarding\nhow they collected, preprocessed, analyzed and presented the\ndata—usually in the form of code and the actual data itself.\nResearch journals and professional societies need to provide\nguidance for authors about how to document their use of public\nor private LLMs, and give authors detailed information about what\ninformation meets the standard for good replication materials. If\nan author develops and uses their own LLM for a research project,\nthat might present other challenges for journals, in particular with\nrespect how to archive and curate the training data used for the\nresearcher’s LLM. Training datasets may be large and thus require\nsigniﬁcant storage space, and they main contain information that\nmight be diﬃcult to make public (for example, for copyright\nor privacy reasons). Guidelines for researchers about archiving\nand curating their LLMs to meet professional best practices for\nreplication are needed.\n/seven.tnum. A practical guide to using LLMs in\npolitical science research\nLLMs have wide-ranging applicability, from classiﬁcation to\ndocument summarization to sentiment analysis, as previously\ndiscussed in Section\n4. While these models exhibit good\nperformance on many tasks out of the box, researchers can further\nenhance their results by providing additional training data. This\ncould entail creating longer, more detailed prompts for few-\nshot learning, or ﬁne-tuning the model using techniques such as\nParameter Eﬃcient Fine-tuning (PEFT) or Low-Rank Adaptation\n(LoRA). While powerful, techniques like PEFT and LoRA require\nthe model architecture and weights to be known, which is typically\nonly the case with open-source models.\nPEFT optimizes the training process by ﬁne-tuning only a\nsubset of the model’s parameters, while LoRA trains a rank-\ndecomposition matrix, adding this to pre-trained weights. The\nrank-decomposition matrix is small relative to the pre-trained\nweights, which are kept frozen. Both techniques help reduce the\ncomputational cost and hardware requirements, and make LLMs\na more accessible tool for researchers with limited resources.\nHowever, their ﬂexibility does not make LLMs a text data\npanacea. They can be slow and ineﬃcient compared to other\ntext-processing methods. When complexity arises, LLMs oﬀer\nmore ﬂexible problem-solving approaches, albeit at the risk\nof hallucination – fabricating information with unwarranted\nconﬁdence. They may also struggle with domain-speciﬁc tasks,\nsuch as mathematical problems. Generally, it is advised to avoid\nLLMs where these issues may be problematic.\nHuggingFace is a prominent platform in the ﬁeld of NLP\nand machine learning, and is the recommended way to ﬁnd and\ndownload individual LLM models. It provides a comprehensive,\nopen-source library of pre-trained models for a multitude of\nNLP tasks, making it an invaluable resource for researchers. With\nHuggingFace, researchers can easily download diﬀerent models and\neven upload their custom-trained models, facilitating the sharing\nof research outputs. Moreover, its user-friendly codebase aids in\nstreamlining machine learning tasks. It oﬀers various libraries that\nFrontiers in Political Science /zero.tnum/eight.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nease the implementation of complex tasks like operations across\nmultiple GPUs, model quantization, and the implementation of\ntechniques like PEFT, LoRA, and various methods for quantization.\nGiven these attributes, HuggingFace signiﬁcantly reduces the\nbarriers to entry for researchers venturing into the realm of\nadvanced NLP and LLMs.\nThe choice of model depends largely on the research objectives,\nhardware constraints, and the need for high-speed computations.\nResearchers grappling with hardware limitations may consider\nﬁne-tuning smaller models, which can deliver comparable\nperformance to larger, unspecialized models. Quantization\ntechniques have also proven eﬀective in reducing model size and\nenhancing speed. Notably, combining these techniques can yield\nsubstantial performance improvements (\nDettmers et al., 2023 ).\nA pertinent issue in the use of LLMs, particularly in political\nscience research, is model censorship. For instance, models like\nChatGPT may censor requests related to certain political ﬁgures\nlike Donald Trump but not others like Joe Biden. This black-box\nnature of LLMs introduces biases stemming from the decisions of\nthose who train the models, leading to “censorship” that can limit\nthe scope of research. This issue persists even with open-source\nmodels. However, recent eﬀorts in the open-source community aim\nto release “uncensored” models. Such uncensoring improves the\nmodels’ capability to handle potentially controversial topics, and\nis especially important when deploying these models in politically\ncharged environments.\n/seven.tnum./one.tnum. Model selection\nThere are a dizzying number of LLMs available. How are\nresearchers to know which to use? In addition to using models\nthat perform well on the public benchmarks we have suggested,\nresearchers can choose a particular family of “foundational” models\nto work with, ﬁnetuning for their particular goals. In this section we\nhighlight a few of the key models that have been developed recently.\nWhile most of our discussion has centered around “text-to-text”\nmodels, the ability of LLMs to perform across modalities is one of\nthe abilities we hope to highlight, so we draw a distinction between\nmodels accepting diﬀerent types of inputs and outputs.\nIn\nTable 1, we provide a brief overview of the most commonly\nused LLMs for various tasks. In order to give a sense of the data\nused to train these models, we also detail some of the data used to\ntrain these models.\n/seven.tnum./two.tnum. Text to text models\nThese models focus on generating or transforming textual data.\nThey play a crucial role in Natural Language Processing (NLP) tasks\nsuch as language translation, summarization, question-answering,\ntext generation, and text-focused processing.\nGPT-3.5 and GPT-4: OpenAI’s GPT-3.5 and GPT-4 are\nadvanced language models driving ChatGPT-3.5 and ChatGPT-\n4 chatbot applications. GPT-4 excels over GPT-3.5 in size,\ncomputational power, and memory, handling complex tasks and\nlonger conversations. Although slower and having an hourly\nTABLE /one.tnumBird’s-eye view of the LLM landscape mapping tasks, models,\nand datasets.\nModel\ntype\nExample\ntasks\nExample\nmodels\nExample\ndatasets\nText-based Text\nclassiﬁcation\nGPT-4 Wikipedia\nText labeling T5 Common\nCrawl\nText\ngeneration\nLlaMA BooksCorpus\nFalcon\nImage-based Image\ngeneration\nMidjourney,\nDALL-E\nImageNet\nImage-to-text GPT-4 COCO\nText-to-image Midjourney,\nDALL-E\nMS COCO\ncaptions\nNote that all text-based tasks can be performed using any of the example models.\nprompt limit, GPT-4 can process visual inputs and retain more data\nduring a chat session.\nT5 (Text-To-Text Transfer Transformer) : This model has been\ncreated by Google. T5 frames every NLP task as a text generation\nproblem, making it highly versatile for various tasks. This class\nof models is especially useful for embedding large bodies of text,\nand T5 models have consistently topped HuggingFace’s Massive\nText Embedding Leaderboard (\nMuennighoﬀ et al., 2023 ) since they\nwere released.\nLLaMA (Large Language Model Meta AI): LLaMA, introduced\nby Meta AI, is a state-of-the-art, foundational language model\ndesigned to democratize access to large language models. Available\nin several sizes (7B, 13B, 33B, and 65B parameters), these models\nrequire less computational power and are ideal for ﬁne-tuning\nacross various tasks. Despite presenting some common challenges\nof large language models, like bias and toxicity, LLaMA provides a\nplatform for researchers to test solutions for these problems. The\nmodels are released under a noncommercial license for research\nuse cases.\nOpen-source LLaMA-based models: Building on the foundation\nset by LLaMA, the open-source and research communities\ndeveloped an array of language models that harness the robustness\nand accessibility of the LLaMA framework. This lineage includes\nvarious families of LLMs, such as those inﬂuenced by Alpaca (\nTaori\net al., 2023 ), Vicuna ( Chiang et al., 2023 ), Guanaco ( Dettmers\net al., 2023 ), and WizardLM ( Xu et al., 2023 ). Each of these\nderivatives embodies the democratized vision of LLaMA, optimized\nto run eﬃciently on consumer-grade hardware. The models are\nnot only easy to use but also designed for straightforward ﬁne-\ntuning, making them highly adaptable to speciﬁc research tasks or\napplications. These LLaMA-based models form an important pillar\nin the landscape of open-source AI, embodying the intersection\nof state-of-the-art performance and the ethos of open, accessible\nAI research.\nFalcon: A state-of-the-art language model family created by\nthe Technology Innovation Institute in Abu Dhabi and released\nunder the Apache 2.0 license (\nvon Werra et al., 2023 ). Falcon-\n40B and Falcon-7B are the two base models in this family, with\nthe former topping the charts of the Open LLM Leaderboard\nFrontiers in Political Science /zero.tnum/nine.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nand the latter being best in its weight class. Falcon-40B rivals the\ncapabilities of many current closed-source models, and notably, it\nis an open-sourced model.\n/seven.tnum./three.tnum. Text to image\nAnother category of models takes textual descriptions and\ntransforms them into visual counterparts (\nGozalo-Brizuela and\nGarrido-Merchan, 2023; Zhang et al., 2023b ). These models employ\na two-step process: the language model ﬁrst changes the input text\ninto a latent representation, followed by a generative image model\nthat creates an image conditioned on that representation (\nBorji,\n2022). There are several popular models in this space.\nMidJourney: This AI-driven tool stands out in the arena of\ntext-to-video platforms by eﬃciently transforming textual prompts\ninto corresponding images. It demonstrates a special capability in\nadapting real art styles to create an image of any combination of\nthings the user desires, with an emphasis on creating environments,\nespecially fantasy and sci-ﬁ scenes (\nVartiainen and Tedre, 2023 ). Its\ndramatic lighting eﬀect makes it appear as though the images were\nrendered concept art from a video game. Notably, MidJourney is\nknown for its distinct artistic style, and its Discord bot integration\nadds convenience for the users.\nStable Diﬀusion: Developed by StabilityAI in 2022 (\nStabilityAI,\n2022), Stable Diﬀusion is a text-to-image model that uses a\nunique diﬀusion process ( Rombach et al., 2022 ). Its mechanism\nbegins with just noise and gradually reﬁnes the image until it is\ncompletely noise-free, progressively aligning with the provided text\ndescription. It’s powered by the Latent Diﬀusion Model (LDM), a\nstate-of-the-art text-to-image synthesis method (\nAlammar, 2022 ).\nIt balances between complexity reduction and detail preservation,\nusually resulting in high visual ﬁdelity. Stable Diﬀusion is also\nopen-source and capable of producing highly detailed artwork, but\nit needs an interpretation of complex original prompts.\nDALL-E: Created by OpenAI, the DALL-E model, and its\nsuccessor DALL-E 2 also produce images from text prompts\n(\nDayma et al., 2021 ). They’ve been trained on more than 10\nbillion parameter versions of the GPT-3 transformer model,\nwhich allows them to interpret natural language inputs and\ngenerate corresponding images. The system primarily consists\nof two components: one that changes the user input into an\nimage representation (called Prior), and another that converts\nthis representation into an actual image (called Decoder). The\ntextual and image embeddings used by DALL-E are derived from\nanother network called CLIP (Contrastive Language-Image Pre-\ntraining), also created by OpenAI. This model is known for creating\nsophisticated output images with high level of detail.\n/seven.tnum./four.tnum. Image to text\nSeveral models also support image-to-text generation. Their\ngoal is to convert visual data into textual information. They\nare utilized in a range of applications including generating\ndescriptive captions, object recognition, image-based searches,\nand accessibility features for visually impaired individuals. Some\nprominent models in this space include:\nCLIP (Contrastive Language–Image Pretraining): Developed by\nOpenAI, it is a foundational image-to-text model trained on a large\nvariety of image-text pairs, which enables it to understand and\ngenerate textual descriptions from images (\nRadford et al., 2021 ).\nUnlike models that only understand images or text, CLIP jointly\nlearns to understand both, allowing it to connect the dots between\nvisual and linguistic information, thus leading to more accurate\nand detailed descriptions. It is worth noting that CLIP itself can’t\ngenerate text, but it can evaluate how appropriate a given sentence\nis as a caption for a given image.\nVisionEncoderDecoder: It is a versatile open-source image-to-\ntext model that can integrate any pre-trained Transformer-based\nvision model as the encoder (like ViT, BEiT, DeiT, Swin) and any\npre-trained language model as the decoder (like RoBERTa, GPT2,\nBERT, DistilBERT). This model is adaptable and has multiple\napplications. It can be utilized in image captioning where the\nencoder encodes the image and an autoregressive language model\ngenerates the caption. It is also employed in Optical Character\nRecognition (\nLi et al., 2023 ).\nXception: It is a caption generator model using a pre-trained\ndeep learning network called Xception, which generates descriptive\ntext captions for images (\nChollet, 2017 ). This model has shown\neﬀectiveness due to Xception’s architecture that performs depth-\nwise separable convolutions for increased eﬃciency.\n/eight.tnum. Discussion and conclusion\nLarge Language Models have seen rapid development in recent\nyears and we expect the continued evolution of LLMs to continue\nin the near future. LLMs will be trained on increasingly larger\n(and hopefully more representative) datasets, they will be made\neasier to use, and we anticipate that they will become an important\ncomponent of the tool kit for political and social scientists. Among\nthe developments that we have argued are necessary are increasing\nthe transparency of the models, improving their interpretability and\nexplainability, and reducing their bias.\nAt the same time, we expect to see an explosion in the use\nof LLMs, particularly in electoral politics but perhaps also in\nother areas of governance and policymaking. These applications\nwill spark additional research opportunities for political scientists.\nWe will need to understand better how electoral campaigns use\nLLMs, both for the development of legitimate and informative\ncommunications, but also for the production of misleading and\nmisinforming communications. Governmental agencies will start\nto use LLMs for many purposes, for example, chatbots that can\ninteract with citizens. These uses will need research, and for\nresearchers to scrutinize the LLMs and how the are trained to help\nprevent biases from these models.\nWe also expect that the public will continue to use\nLLMs in many ways. People will uses LLMs to manage their\ncommunications, answering email and creating social media posts.\nLLMs will be used by students, replacing ﬂashcards and tutors,\nand also meaning that their parents will need to become proﬁcient\nwith the tools that their children are using. Many businesses\nwill use LLMs, to build chatbots for communications but to\nalso simply or automate many simple and routine tasks—drafting\nlegal agreements, writing news reports, and developing advertising\nmaterials, for example. These public uses of LLMs will intersect\nFrontiers in Political Science /one.tnum/zero.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nwith government and politics in important ways, again creating\nnew research opportunities for political scientists to study.\nWhile LLMs hold great promise for political scientists, we\nalso are concerned about their ability to quickly and inexpensively\nchurn our false and misleading information. If misinformation\nbecomes rampant, especially in future elections, that could lead to\nsigniﬁcant calls for the regulation of the technology. Some today are\neven calling for halting the development and public dissemination\nof LLMs, which could have a chilling eﬀect on their evolution\nin nations or regions that introduce strong regulatory models\nfor LLMs and more generally, AI. The possibility that regulation\nmight be introduced for LLMs should be a call for researchers to\nunderstand this new technology, to help build them in ways that\nensure their transparent and fair use, and to help policy makers\nnavigate how to support the development and use of LLMs in ways\nthat mitigate social and political harm.\nAuthor contributions\nRA: Conceptualization, Funding acquisition, Investigation,\nMethodology, Project administration, Supervision,\nWriting—original draft, Writing—review and editing. ML:\nConceptualization, Investigation, Methodology, Writing—original\ndraft, Writing—review and editing. RK: Conceptualization,\nInvestigation, Methodology, Writing—original draft,\nWriting—review and editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nAcknowledgments\nWe thank the Caltech Center for Science, Society, and Public\nPolicy for supporting our research on the Ethics of AI.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbid, A., Farooqi, M., and Zou, J. (2021). Large language models associate\nMuslims with violence. Nat. Mach. Intell. 3, 461–463. doi: 10.1038/s42256-021-\n00359-2\nAher, G. V., Arriaga, R. I., and Kalai, A. T. (2023). “Using large language models\nto simulate multiple humans and replicate human subject studies, ” in International\nConference on Machine Learning (PMLR), 337–371.\nAlammar, J. (2022). The Illustrated Stable Diﬀusion: Visualizing Machine Learning\nOne Concept at a Time . Available online at: https://jalammar.github.io/illustrated-\nstable-diﬀusion/ (accessed July 07, 2023).\nAllcott, H., and Gentzkow, M. (2017). Social media and fake news in the 2016\nelection. J. Econ. Perspect. 31, 211–36. doi: 10.1257/jep.31.2.211\nAlvarez, R. M., Eberhardt, F., and Linegar, M. (2023). Generative AI and the Future\nof Elections. Caltech Center for Science, Society, and Public Policy (CSSPP ) Policy Brief.\nAlvarez, R. M., and Heuberger, S. (2022). How (not) to reproduc e: Practical\nconsiderations to improve research transparency in political s cience. Polit. Sci. Polit.\n55, 149–154. doi: 10.1017/S1049096521001062\nBloomberg (2023). Generative AI Takes Stereotypes and Bias from Bad to Worse .\nAvailable online at: https://www.bloomberg.com/graphics/2023-generative-ai- bias/\n(accessed July 07, 2023).\nBorji, A. (2022). Generated faces in the wild: Quantitative co mparison of\nStable Diﬀusion, Midjourney and DALL-E 2. arXiv preprint arXiv:2210.00586 .\ndoi: 10.48550/arXiv.2210.00586\nBuolamwini, J., and Gebru, T. (2018). “Gender shades: intersec tional accuracy\ndisparities in commercial gender classiﬁcation, ” in Proceedings of the 1st Conference\non Fairness, Accountability and Transparency, volume 81 of Proceedin gs of Machine\nLearning Research, eds S. A. Friedler and C. Wilson (PMLR), 77–91.\nCeylan, B. (2023). Large Language Model Evaluation in 2023: 5 Methods . Available\nonline at: https://research.aimultiple.com/large-language-model-evalua tion/ (accessed\nOctober 07, 2023).\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., e t al. (2023). Vicuna: An\nOpen-Source Chatbot Impressing GPT-4 With 90% ChatGPT Quality . Available online\nat: https://lmsys.org/blog/2023-03-30-vicuna/\nChiusano, F. (2022). Two Minutes NLP–Perplexity Explained With Simple\nProbabilities. Available online at: https://bit.ly/3PSLKtr (accessed October 07, 2023).\nChollet, F. (2017). Xception: deep learning with depthwise separa ble convolutions.\narXiv preprint arXiv:1610.02357 . doi: 10.48550/arXiv.1610.02357\nChouldechova, A., Benavides-Prado, D., Fialko, O., and Vaith ianathan, R. (2018).\n“A case study of algorithm-assisted decision making in child m altreatment hotline\nscreening decisions, ” in Conference on Fairness, Accountability and Transparency\n(PMLR), 134–148.\nCrisan, A., Drouhard, M., Vig, J., and Rajani, N. (2022). “In teractive model cards: a\nhuman-centered approach to model documentation, ” in Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Transparency , 427–439.\nDayma, B., Patil, S., Cuenca, P., Saifullah, K., Abraham, T., Le Khac, P., et al. (2021).\nDall-e Mini. Available online at: https://github.com/borisdayma/dalle-mini\nDelobelle, P., Tokpo, E. K., Calders, T., and Berendt, B. (2022). “ Measuring fairness\nwith biased rulers: a comparative study on bias metrics for pre- trained language\nmodels, ” in NAACL 2022: the 2022 Conference of the North American chapter of the\nAssociation for Computational Linguistics: Human Language Technolo gies, 1693–1706.\nDenton, E., Ha, J.-W., and Vanschoren, J. (2023). “Neurips 20 23, ” inThirty-seventh\nConference on Neural Information Processing Systems .\nDettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. ( 2023).\nQLORA: eﬃcient ﬁnetuning of quantized LLMs. arXiv preprint arXiv:2305.14314 .\ndoi: 10.48550/arXiv.2305.14314\nElutherAI (2023). EleutherAI/lm-Evaluation-Harness: A Framework for Few-Shot\nEvaluation of Autoregressive Language Models . Available online at: https://github.com/\nEleutherAI/lm-evaluation-harness (accessed October 07, 2023).\nFeng, S., Park, C. Y., Liu, Y., and Tsvetkov, Y. (2023). From pr etraining data to\nlanguage models to downstream tasks: tracking the trails of polit ical biases leading\nto unfair NLP models. arXiv preprint arXiv:2305.08283 . doi: 10.48550/arXiv.2305.\n08283\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C ., et al. (2020).\nThe pile: an 800GB dataset of diverse text for language modeling. arXiv preprint\narXiv:2101.00027. doi: 10.48550/arXiv.2101.00027\nFrontiers in Political Science /one.tnum/one.tnum frontiersin.org\nLinegar et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fpos./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/seven.tnum/zero.tnum/nine.tnum/two.tnum\nGebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wa llach, H.,\nDaume, H. III, et al. (2021). Datasheets for datasets. Commun. ACM 64, 86–92.\ndoi: 10.1145/3458723\nGoogle (2022). Fairness: Types of Bias. Available online at: https://developers.google.\ncom/machine-learning/crash-course/fairness/types-of-b ias (accessed November 07,\n2023).\nGozalo-Brizuela, R., and Garrido-Merchan, E. C. (2023). ChatGP T is not all\nyou need. A state of the art review of large generative AI models . arXiv preprint\narXiv:2301.04655. doi: 10.48550/arXiv.2301.04655\nGrimmer, J. (2013). Representational Style in Congress: What Legislators Say and\nWhy It Matters . New York, NY: Cambridge University Press.\nGrimmer, J., and Stewart, B. M. (2013). Text as data: the promis e and pitfalls\nof automatic content analysis methods for political texts. Polit. Anal. 21, 267–297.\ndoi: 10.1093/pan/mps028\nHolland, S., Hosny, A., Newman, S., Joseph, J., and Chmielinski, K. (2020). The\ndataset nutrition label. Data Protect. Privacy 12, 1. doi: 10.5040/9781509932771.ch-001\nHoward, A., and Borenstein, J. (2018). The ugly truth about ou rselves and our\nrobot creations: the problem of bias and social inequity. Sci. Eng. Ethics 24, 1521–1536.\ndoi: 10.1007/s11948-017-9975-2\nHu, E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., et al. (2021). LORA:\nlow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 .\ndoi: 10.48550/arXiv.2106.09685\nHuang, Y., Zhang, Q., and Sun, L. (2023). TrustGPT: a benchmar k for\ntrustworthy and responsible large language models. arXiv preprint arXiv:2306.11507 .\ndoi: 10.48550/arXiv.2306.11507\nHugging Face (2023a). Model Cards . Available online at: https://huggingface.co/\ndocs/hub/model-cards (accessed October 07, 2023).\nHugging Face (2023b). Open LLM Leaderboard - a Hugging Face Space by\nHuggingFaceH4. Available online at: https://huggingface.co/spaces/HuggingFaceH4/\nopen_llm_leaderboard (accessed October 07, 2023).\nJingnan, H. (2023). How Generative AI May Empower Political Campaigns and\nPropaganda. Available online at: https://bit.ly/46DNzk5 (accessed November 07, 2023).\nKann, C., Hashash, S., Steinert-Threlkeld, Z., and Alvarez, R. M. (2023). Collective\nidentity in collective action: evidence from the 2020 summer B LM protests. Front. Polit.\nSci. 5, 1185633. doi: 10.3389/fpos.2023.1185633\nKing, G. (1995). Replication, replication. Polit. Sci. Polit. 28, 444–452.\nKocielnik, R., Amershi, S., and Bennett, P. N. (2019). “Will yo u accept an imperfect\nAI? exploring designs for adjusting end-user expectations of A I systems, ” inProceedings\nof the 2019 CHI Conference on Human Factors in Computing Systems , 1–14.\nKocielnik, R., Kangaslahti, S., Prabhumoye, S., Hari, M., Alva rez, R. M., and\nAnandkumar, A. (2023a). “Can you label less by using out-of-do main data? Active &\ntransfer learning with few-shot instructions, ” in Transfer Learning for Natural Language\nProcessing Workshop (PMLR), 22–32.\nKocielnik, R., Prabhumoye, S., Zhang, V., Jiang, R., Alvarez, R. M., and\nAnandkumar, A. (2023b). BiasTestGPT: using ChatGPT for socia l bias testing of\nlanguage models. arXiv preprint arXiv: 2302.07371 .\nLambrecht, A., and Tucker, C. (2019). Algorithmic bias? An em pirical study of\napparent gender-based discrimination in the display of STEM car eer ads. Manage. Sci.\n65, 2966–2981. doi: 10.1287/mnsc.2018.3093\nLaver, M., and Garry, J. (2000). Estimating policy positions fr om political texts. Am.\nJ. Polit. Sci. 44, 619–634. doi: 10.2307/2669268\nLi, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., et al. (2 023).\n“TROCR: transformer-based optical character recognition w ith pre-trained models, ”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 13094–13102.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasu naga, M., et al.\n(2022). Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 .\ndoi: 10.48550/arXiv.2211.09110\nLiang, W., Yuksekgonul, M., Mao, Y., Wu, E., and Zou, J. (2023) . GPT detectors\nare biased against non-native English writers. arXiv preprint arXiv:2304.02819 .\ndoi: 10.48550/arXiv.2304.02819\nMartin, G. J., and Yurukoglu, A. (2017). Bias in cable news: persu asion and\npolarization. Am. Econ. Rev. 107, 2565–2599. doi: 10.1257/aer.20160812\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Gals tyan, A. (2021).\nA survey on bias and fairness in machine learning. ACM Comput. Surv. 54, 1–35.\ndoi: 10.1145/3457607\nMendelsohn, J., Bras, R. L., Choi, Y., and Sap, M. (2023). “From dogwhistles to\nbullhorns: unveiling coded rhetoric with language models, ” in ACL (Toronto, ON).\nMitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., H utchinson, B., et al.\n(2019). “Model cards for model reporting, ” in Proceedings of the Conference on Fairness,\nAccountability, and Transparency , 220–229.\nMotoki, F., Pinho Neto, V., and Rodrigues, V. (2023). More hu man than human:\nmeasuring ChatGPT political bias. Public Choice. doi: 10.1007/s11127-023-01097-2\nMuennighoﬀ, N., Tazi, N., Magne, L., and Reimers, N. (2023). MTEB:\nmassive text embedding benchmark. arXiv preprint arXiv:2210.07316 .\ndoi: 10.48550/arXiv.2210.07316\nOsoba, O. A., and Welser, W. IV (2017). An Intelligence in our Image: The Risks of\nBias and Errors in Artiﬁcial Intelligence . Santa Monica, CA: Rand Corporation.\nPerez, E., Ringer, S., Lukoši ¯ut˙e, K., Nguyen, K., Chen, E., Heiner, S., et al. (2022).\nDiscovering language model behaviors with model-written eva luations. arXiv preprint\narXiv:2212.09251. doi: 10.48550/arXiv.2212.09251\nPoole, K. T., and Rosenthal, H. (1985). A spatial model for legisla tive roll call\nanalysis. Am. J. Polit. Sci. 29, 357–384.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., et al.\n(2021). “Learning transferable visual models from natural lan guage supervision, ” in\nInternational Conference on Machine Learning (PMLR), 8748–8763.\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., an d Sutskever, I.\n(2023). “Robust speech recognition via large-scale weak superv ision, ” inInternational\nConference on Machine Learning (PMLR), 28492–28518.\nRaji, I. D., and Buolamwini, J. (2019). “Actionable auditing: investigating the\nimpact of publicly naming biased performance results of commercia l ai products, ”\nin Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society ,\n429–435.\nRamanathan, B. (2022). Evaluating Large Language Models (LLMs) with Eleuther AI .\nAvailable online at: https://bit.ly/44mnm7R (accessed October 07, 2023).\nRastogi, C., Ribeiro, M. T., King, N., and Amershi, S. (2023) . Supporting human-\nAI collaboration in auditing LLMs with LLMs. arXiv preprint arXiv:2304.09991 .\ndoi: 10.48550/arXiv.2304.09991\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022).\n“High-resolution image synthesis with latent diﬀusion models , ” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 10684–\n10695.\nSanthosh, S. (2023). Understanding BLEU and ROUGE Score for NLP Evaluation .\nAvailable online at: https://medium.com/@sthanikamsanthosh1994/understanding-\nbleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb#:~ :text=While%20BLEU\n%20score%20is%20primarily,the%20reference%20translations%20or%20summaries\n(accessed October 07, 2023).\nSchnabel, T., Swaminathan, A., Singh, A., Chandak, N., and Jo achims, T. (2016).\n“Recommendations as treatments: debiasing learning and eva luation, ” inInternational\nConference on Machine Learning (PMLR), 1670–1679.\nSrikanth, M., Liu, A., Adams-Cohen, N., Cao, J., Alvarez, R. M ., and Anandkumar,\nA. (2021). “Dynamic social media monitoring for fast-evolvi ng online discussions, ”\nin Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, 3576–3584.\nStabilityAI (2022). Stable Diﬀusion v2.1 and DreamStudio Updates . Available\nonline at: https://stability.ai/blog/stablediﬀusion2-1-release7-dec -2022 (accessed July\n07, 2023).\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestri n, C., et al. (2023).\nStanford Alpaca: An Instruction-Following LLaMA Model . Available online at: https://\ngithub.com/tatsu-lab/stanford_alpaca\nvan der Linden, S. (2023). Foolproof: Why Misinformation Infects our Minds and\nHow to Build Immunity . New York, NY: Norton.\nVartiainen, H., and Tedre, M. (2023). Using artiﬁcial intelli gence in craft\neducation: crafting with text-to-image generative models. Digit. Creat. 34, 1–21.\ndoi: 10.1080/14626268.2023.2174557\nvon Werra, L., Belkada, Y., Mangrulkar, S., Tunstall, L., Dehaen e, O., Cuenca, P.,\net al. (2023). The Falcon Has Landed in the Hugging Face Ecosystem . Available online\nat: https://huggingface.co/blog/falcon (accessed July 07, 2023).\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., et al.\n(2019). SuperGLUE: a stickier benchmark for general-purpose lang uage understanding\nsystems. arXiv preprint arXiv:1905.00537 . doi: 10.48550/arXiv.1905.00537\nWest, D. M. (2023). Comparing Google Bard with OpenAI’s ChatGPT on Political\nBias, Facts, and Morality . Available online at: https://bit.ly/44EEbe3 (accessed October\n07, 2023).\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., et al. (2 023). WizardLM:\nempowering large language models to follow complex instructions. arXiv preprint\narXiv:2304.12244. doi: 10.48550/arXiv.2304.12244\nZhang, C., Zhang, C., Li, C., Qiao, Y., Zheng, S., Dam, S. K., e t al. (2023a). One small\nstep for generative AI, one giant leap for AGI: a complete survey o n chatgpt in AIGC\nera. arXiv preprint arXiv:2304.06488 . doi: 10.48550/arXiv.2304.06488\nZhang, C., Zhang, C., Zhang, M., and Kweon, I. S. (2023b). Tex t-to-image\ndiﬀusion model in Generative AI: A survey. arXiv preprint arXiv:2303.07909 .\ndoi: 10.48550/arXiv.2303.07909\nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., and Yang, D. (2023).\nCan large language models transform computational social scien ce? arXiv preprint\narXiv:2305.03514. doi: 10.48550/arXiv.2305.03514\nFrontiers in Political Science /one.tnum/two.tnum frontiersin.org",
  "topic": "Politics",
  "concepts": [
    {
      "name": "Politics",
      "score": 0.6557057499885559
    },
    {
      "name": "Political science",
      "score": 0.44165584444999695
    },
    {
      "name": "Work (physics)",
      "score": 0.42123839259147644
    },
    {
      "name": "Social science",
      "score": 0.3291010856628418
    },
    {
      "name": "Sociology",
      "score": 0.22269156575202942
    },
    {
      "name": "Law",
      "score": 0.12024343013763428
    },
    {
      "name": "Engineering",
      "score": 0.09408628940582275
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122411786",
      "name": "California Institute of Technology",
      "country": "US"
    }
  ]
}