{
    "title": "A Close Look into the Calibration of Pre-trained Language Models",
    "url": "https://openalex.org/W4385571092",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2122846996",
            "name": "Yangyi Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3209524914",
            "name": "Lifan Yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2905176890",
            "name": "Ganqu Cui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2051269448",
            "name": "Zhiyuan Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2127420617",
            "name": "Heng Ji",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3211561698",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W2917792182",
        "https://openalex.org/W3201622928",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2566079294",
        "https://openalex.org/W1599263113",
        "https://openalex.org/W4328028903",
        "https://openalex.org/W2626967530",
        "https://openalex.org/W2136891251",
        "https://openalex.org/W3173591450",
        "https://openalex.org/W4300772090",
        "https://openalex.org/W4285298468",
        "https://openalex.org/W4281662781",
        "https://openalex.org/W1618905105",
        "https://openalex.org/W2956090150",
        "https://openalex.org/W4221145545",
        "https://openalex.org/W2963238274",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4297798436",
        "https://openalex.org/W1985697096",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3212502303",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W3207710400",
        "https://openalex.org/W4286231561",
        "https://openalex.org/W4281748205",
        "https://openalex.org/W3099142828",
        "https://openalex.org/W3113089224",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2952153923",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4285298351",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4385573354",
        "https://openalex.org/W3104939451",
        "https://openalex.org/W4321005039",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W2948194985",
        "https://openalex.org/W2933254221",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2994088087",
        "https://openalex.org/W3159082513",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W4385573323",
        "https://openalex.org/W2912237282",
        "https://openalex.org/W4283211054",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W4287388854",
        "https://openalex.org/W3206843525",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2254249950",
        "https://openalex.org/W582134693",
        "https://openalex.org/W4287752971",
        "https://openalex.org/W2250243742",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3177071108"
    ],
    "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don't learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs' confidence in wrong predictions.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1343–1367\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA Close Look into the Calibration of Pre-trained Language Models\nYangyi Chen∗\nUIUC\nLifan Yuan∗\nHUST\nGanqu Cui, Zhiyuan Liu\nTsinghua University\nHeng Ji\nUIUC\nyangyic3@illinois.edu lievanyuan173@gmail.com\nAbstract\nPre-trained language models (PLMs) may fail\nin giving reliable estimates of their predic-\ntive uncertainty. We take a close look into\nthis problem, aiming to answer two ques-\ntions: (1) Do PLMs learn to become cali-\nbrated in the training process? (2) How effec-\ntive are existing calibration methods? For the\nfirst question, we conduct fine-grained con-\ntrol experiments to study the dynamic change\nin PLMs’ calibration performance in training.\nWe consider six factors as control variables,\nincluding dataset difficulty, available training\nsamples, training steps, the number of tun-\nable parameters, model scale, and pretrain-\ning. We observe a consistent change in cal-\nibration performance across six factors. We\nfind that PLMs don’t learn to become cali-\nbrated in training, evidenced by the contin-\nual increase in confidence, no matter whether\nthe predictions are correct or not. We high-\nlight that our finding somewhat contradicts\ntwo established conclusions: (a) Larger PLMs\nare more calibrated; (b) Pretraining improves\nmodel calibration. Next, we study the ef-\nfectiveness of existing calibration methods in\nmitigating the overconfidence issue. Besides\nunlearnable calibration methods (e.g., label\nsmoothing), we adapt and extend two recently\nproposed learnable methods that directly col-\nlect data to train models to have reasonable\nconfidence estimations. Experimental results\nshow that learnable methods significantly re-\nduce PLMs’ confidence in wrong predictions.\nThe code is available athttps://github.\ncom/lifan-yuan/PLMCalibration.\n1 Introduction\nPre-trained language models (PLMs) are success-\nful in many downstream tasks regarding perfor-\nmance (Wang et al., 2019). In high-stake appli-\ncations, it’s equally essential for PLMs to pos-\nsess a sense of calibration (Vaicenavicius et al.,\n∗Equal contribution\n0.0 0.8 1.6 2.4 3.2 4.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\nAcc\nConf\nECE\nCErrpos\nCErrneg\nUnder-fitted Over-fitted  \nFigure 1: The demonstration of the under-fitted and\nover-fitted states in the training process with RoBERTa\non SST-2.\n2019). However, the confidence scores (a.k.a, pre-\ndictive probability) of existing deep neural net-\nworks cannot serve as reliable estimates of their\nuncertainty (Guo et al., 2017), and a deep under-\nstanding of PLMs calibration is lacking.\nIn this paper, we give a systematical analysis\nof PLMs calibration. We consider two questions\nabout PLMs calibration: (1) Do PLMs learn to\nbecome calibrated in the training process? (2)\nHow effective are existing calibration methods?\nWe first introduce the metrics we adopt for cali-\nbration performance evaluation. The most widely\nused calibration metric ECE (Expected Calibra-\ntion Error (Naeini et al., 2015)) is considered. It\nmeasures the difference between confidence and\naccuracy by portioning samples into various con-\nfidence zones. To give a more comprehensive\nand practical calibration evaluation, we provide an\napplication-driven perspective, describing two un-\ndesirable situations in practice: (1) Correct predic-\ntions (positive) are rejected due to low confidence;\n(2) Wrong predictions (negative) are accepted due\nto high confidence. We propose to measure the\naverage confidence scores on correct and wrong\npredictions respectively to characterize undesir-\nable situations. Two kinds of calibration errors are\nmeasured, denoted as CErrpos and CErrneg.\nFor the first question, we consider the influ-\n1343\nence of six factors on PLMs’ calibration perfor-\nmance, including dataset difficulty, available train-\ning samples, training steps, the number of tunable\nparameters, model scale, and pretraining. Some of\nthem are overlooked in previous empirical stud-\nies (Snoek et al., 2019; Nixon et al., 2019; Min-\nderer et al., 2021). We motivate to conduct fine-\ngrained control experiments to study the dynamic\nchange in PLMs’ calibration performance in train-\ning through manipulating control variables.\nWe empirically observe an overall consistent\nchange in calibration performance across six\nfactors. All six factors influence PLMs’ fitness on\nthe training distribution. This results in two states\nof PLMs considering calibration performance,\nnamely under-fitted and over-fitted states (see\nFig.1). In the under-fitted state, PLMs’ perfor-\nmance and confidence increase at different speeds\nwhen more fitted on the training distribution. In\nthe over-fitted state, PLMs’ confidence continues\nto increase steadily with little change in perfor-\nmance. We find evidence that PLMs don’t learn\nto become calibrated in training: PLMs’ confi-\ndence in their predictions continues to increase\nwhen more fitted on the distribution (e.g.,\nmore tunable parameters, training longer).\nThis results in two miscalibration behaviors: (1)\nIncreasing ECE in the latter over-fitted state, and\n(2) Continually increasing confidence in wrong\npredictions, indicating that PLMs mostly don’t\nknow “what they don’t know”.\nWe highlight our finding presents contradictory\nviews with the two established conclusions: (a)\nLarger PLMs show better calibration (Srivastava\net al., 2022); (b) Pretraining improves model cali-\nbration (Hendrycks et al., 2019b). We identify that\nthe inconsistency lies in: (1) The difficulty of eval-\nuation datasets: the performance doesn’t saturate\nin the considered datasets (e.g., BIG-bench (Sri-\nvastava et al., 2022)). Thus, the evaluation is on\nthe under-fitted state, leaving the miscalibration\nbehavior in the over-fitted state unobserved; (2)\nEvaluation metrics: previous work doesn’t mea-\nsure the confidence in wrong predictions, over-\nlooking the fact that models are becoming more\nconfident in wrong predictions when scaling larger\nand employing pretraining.\nThus, we find that the main issue of PLMs cal-\nibration lies in their overconfidence in wrong pre-\ndictions, which cannot be trivially solved by in-\ncreasing the model scale. So we consider the ef-\nfectiveness of existing calibration methods in mit-\nigating the overconfidence issue. We partition\nexisting calibration methods into unlearnable and\nlearnable groups. Unlearnable methods heuristi-\ncally manipulate the original confidence in pre-\ndictions (e.g., label smoothing). Learnable meth-\nods directly collect data and train models to give\nreasonable confidence scores in their predictions.\nNamely, an extra calibration task is introduced,\nwhich aims to extract features from samples and\nmodels’ preceding performance to predict whether\nmodels’ predictions are correct or not.\nIn our experiments, we identify the superior-\nity of learnable methods compared to unlearn-\nable ones, considering both in-distribution\n(ID) and out-of-distribution (OOD) settings.\nThis is characterized by a sharp decrease in their\nconfidence in wrong predictions when using learn-\nable methods, indicating that they significantly\nmitigate the overconfidence issue. Moreover,\nlearnable methods can maintain a reasonable in-\ncrease in CErr pos, holding consistent correlations\nbetween the drop in confidence and performance\nunder distribution shifts. This shows the differ-\nence from unlearnable methods, which take effect\nby roughly imposing confidence regularization\non models’ predictions (e.g., label smoothing),\nresulting in almost the same amount of increase\nin CErrpos with the decrease in CErrneg.\nTo further understand learnable calibration\nmethods, we consider the influence of more data\nand larger model scales for the calibration task,\nthe adopted model for the calibration task, and\nthe data distribution, on PLMs’ calibration per-\nformance. We highlight three findings: (1) More\ndata and larger model scales for the calibration\ntask both play significant positive roles in PLMs’\ncalibration performance; (2) PLMs can be trained\nto give their uncertainty. This finding is consistent\nwith the concurrent work (Lin et al., 2022). Fur-\nther, we provide an extension to this conclusion.\nWe find that using an extrinsic predictive model\ncan achieve comparable results, given the same\ncalibration training data. Thus, we identify that\nthe success of this paradigm essentially lies in\nthe learnable attribute of the calibration task,\ninstead of the PLMs’ self-checking process; (3)\nPLMs’ calibration performance under distribution\nshifts depends on the evaluation datasets chosen.\nPrevious work shows that PLMs exhibit de-\ngraded calibration performance under distribution\n1344\nshifts (Desai and Durrett, 2020). We find that\nthis conclusion is reversed when the ID datasets\nare harder and PLMs achieve better performance\non OOD datasets. The concrete arguments and\nexplanations are detailed in Appendix E.\n2 Background\nCalibration measure. We can visualize model\ncalibration through reliability diagram (DeGroot\nand Fienberg, 1983). Based on the diagram, we\ncan measure the ECE (Naeini et al., 2015) by\npartitioning samples into different confidence\nzones. The central idea is to measure the ab-\nsolute difference between models’ predictive\nconfidence and accuracy. Although alternative\ntheoretic-motivated metrics have been pro-\nposed (Vaicenavicius et al., 2019; Gupta et al.,\n2021), we still employ ECE in our experiments\ndue to its simplicity and popularity.\nBenchmark & Analysis. Given appropriate\nevaluation metrics, large-scale benchmarks have\nbeen conducted to analyze model calibration un-\nder different settings, spanning model architec-\ntures (Guo et al., 2017; Minderer et al., 2021),\nmodel scales (Dan and Roth, 2021), modali-\nties (Desai and Durrett, 2020; Minderer et al.,\n2021; Kadavath et al., 2022), calibration meth-\nods (Guo et al., 2017; Desai and Durrett, 2020),\nand distribution shifts (Nixon et al., 2019; Kong\net al., 2020). Our work is closely related to Xiao\net al. (2022) that quantifies the uncertainty of\nPLMs. However, previous benchmarks follow\nthe fixed training and evaluation paradigms. In\nthis paper, we instead conduct a fine-grained and\nmore comprehensive empirical evaluation to take\na close look into PLMs calibration from multi-\nple dimensions that have often been overlooked.\nAlso, we consider and conduct a detailed analy-\nsis of the recently proposed learnable calibration\nmethods (Lin et al., 2022; Kadavath et al., 2022).\nMethod. Calibration is essential for out-of-\ndistribution detection (Hendrycks et al., 2019a),\nselective prediction (Varshney et al., 2022), ro-\nbustness (Kumar et al., 2022), and pseudo-\nlabeling (Rizve et al., 2021). Existing calibra-\ntion methods can be partitioned into unlearnable\nand learnable groups. For unlearnable methods,\nthere are mainly four categories. Post-hoc cali-\nbration intends to readjust the output logits refer-\nring to the performance on a held-out validation\nset (Platt et al., 1999; Guo et al., 2017). Regular-\nization methods aim to prevent models from be-\ning over-confident on predictions (Szegedy et al.,\n2016; Pereyra et al., 2017). Data augmenta-\ntion (Hendrycks et al., 2020; Wang et al., 2021)\nand model ensemble (Gal and Ghahramani, 2016;\nLakshminarayanan et al., 2017) have also been\nempirically proven to improve model calibration.\nFor learnable methods, the typical way is to first\ncollect data for the calibration task, and then train\na model to predict whether the given answer is cor-\nrect. The model can be a multi-layer perceptron,\nand the features can be hand-engineered (Ye and\nDurrett, 2022; Zhang et al., 2021b; Si et al., 2022)\nor the last hidden states of PLMs (Kadavath et al.,\n2022). PLMs can also be directly trained to output\ntheir uncertainty by words (Lin et al., 2022).\n3 Evaluation Metrics\nFor basic evaluation, we report accuracy (Acc)\nand average confidence score (Conf) on the test-\ning set. For calibration evaluation, we report ECE\nusing equal-mass binning and 100 bins following\nMinderer et al. (2021). Besides, we provide an\napplication-driven perspective to evaluate model\ncalibration, aiming to quantify two unsatisfied sce-\nnarios due to miscalibration in practice: (1) Cor-\nrect predictions (positive) are rejected due to low\nconfidence; (2) Wrong predictions (negative) are\naccepted due to high confidence. Specifically, we\nconsider the average confidence in correct predic-\ntions Confpos and wrong predictions Confneg re-\nspectively. For unified comparison, we report two\ncalibration error (CErr) cases, CErr pos = 1 −\nConfpos and CErrneg = Confneg. In principle, we\nexpect calibrated models to have both low CErrpos\nand CErr neg, indicating that they reasonably as-\nsign high confidence in correction predictions and\nlow confidence in wrong predictions.\n4 Do PLMs Learn to Become Calibrated?\n4.1 Experimental Setting\nFor model architectures, we choose RoBERTa-\nbase (Liu et al., 2019) and T5-base (Raffel et al.,\n2020), since they represent two classic types of\nPLMs, namely encoder-only and encoder-decoder\nmodels. We experiment with four representative\ntasks in NLP, including sentiment analysis, natural\nlanguage inference, news classification, and topic\nclassification. For datasets, we choose SST-\n2 (Socher et al., 2013a), MNLI (Williams et al.,\n1345\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\nSST-2\n0 1 2 4 8 1632641282565121024204840968192\nShots\n0\n20\n40\n60\n80\n100\n MNLI\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 2: Results of available training samples with T5.\n0.0 0.8 1.6 2.4 3.2 4.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\nSST-2\n0.0 45.0 90.0 135.0 180.0 225.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\n MNLI\n0.0 16.0 32.0 48.0 64.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 3: Results of training steps with T5.\n2018a), AG-News (Zhang et al., 2015), and Ya-\nhoo (Zhang et al., 2015) respectively. We employ\nthe prompt-based learning paradigm (Liu et al.,\n2021) since its superior performance compared to\ntraditional fine-tuning, especially in the few-shot\nsetting. Specifically, we inherit the masked lan-\nguage modeling task in the pre-training stage and\nuse templates to wrap samples into prompts. We\nfine-tune the whole PLMs to fill in the [mask] po-\nsition in the prompt. The manual template and ver-\nbalizer for each dataset are listed in Appendix A.\n4.2 Experimental Results\nWe conduct a fine-grained control study to explore\nthe influence of six factors, including dataset dif-\nficulty, available training samples (Fig.2), training\nsteps (Fig.3), number of tunable parameters\n(Fig.4 and Fig.10), pretraining (Fig.6), and model\nscale (Fig.5). Due to space limits, we show the\ncorresponding results of RoBERTa and results of\nT5 on AG-News in Appendix B. We summarize\nthe overall conclusions and leave the detailed\nexperimental settings and findings in Appendix B.\nWe note that all six factors dynamically influ-\nence PLMs’ fitness on the training distribution,\nwhich we identify as the decisive factor of PLMs’\ncalibration performance. We observe an overall\nconsistent change in calibration performance\nacross six factors, resulting in two PLMs’ states\n(see Fig.1) in training:\nUnder-fitted state. In this state, PLMs’ perfor-\nmance and confidence increase at different speeds\nwhen more fitted on the training distribution.\nThe ECE score fluctuates during this process. In\nprinciple, miscalibration is due to the mismatch\nbetween performance and confidence. However,\nwe look closely into some critical points where\nECE changes sharply (e.g., Fig.2), and empiri-\ncally find that the increase or decrease in ECE\ncan be estimated by comparing the increasing\nrates of PLMs’ performance and confidence.\nWe observe that a larger (smaller) increasing\nrate in performance reduces (increases) ECE.\nThus, high ECE can be partially attributed to\nPLMs’ relatively rapid growth in confidence with\nperformance lagging behind.\nOver-fitted state. In this state, PLMs’ perfor-\nmance doesn’t have a substantial difference due to\ntheir generalization ability (Zhang et al., 2021a).\nHowever, PLMs’ confidence continues to increase\nin this state, resulting in increasing ECE. This is\nespecially obvious when more training steps and\ntunable parameters are introduced (see Fig.3 and\nFig.4). Thus, being more fitted on the training dis-\n1346\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\nSST-2\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\n MNLI\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 4: Results of tunable parameters with T5 (Adapter).\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100\nSST-2\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100\nMNLI\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100 Yahoo\nSmall Base Large3B\nFigure 5: Results of increasing PLMs scales with T5.\ntribution may bring a negative effect on PLMs cal-\nibration. In addition, due to the increase of ECE\nin this state, the evaluation of calibration perfor-\nmance may be sensitive to the training paradigm.\nThis indicates that previous conclusions drawn\nfrom empirical studies should be carefully exam-\nined since the training paradigms may be different\nin model architectures and calibration methods.\nGiven the two states observed, we conclude\nthat PLMs don’t learn to become calibrated in\ntraining, evidenced by the continually increas-\ning confidence in predictions, no matter correct\nor not, in the fitting process. Specifically, this re-\nsults in two miscalibration behaviors: (1) Increas-\ning ECE in the over-fitted state; (2) The consistent\nincrease in CErrneg throughout the whole training\nprocess. This is an undesirable property in prac-\ntice since users may accept wrong predictions due\nto their high confidence, and indicates that PLMs\nmostly don’t know “what they don’t know”.\nWe highlight two of the considered factors,\nnamely pretraining and model scales (Fig.5 and\nFig.6), which are examined in previous work. Our\nfindings present some contradictory views with the\nestablished conclusions: (1) Larger PLMs show\nbetter calibration (Srivastava et al., 2022); (2) Pre-\ntraining improves model calibration (Hendrycks\net al., 2019b). Actually, scaling larger and em-\nploying pretraining are both strategies to increase\nPLMs capacity, making them more fitted on the\ntraining distribution. Our general conclusion can\nalso be applied. We highlight two observations:\n(1) Essentially, the influence of scaling larger and\npretraining on PLMs calibration is dynamically\ndetermined by the relative increase in performance\nand confidence, which is highly relevant to the\nchosen evaluation datasets. For example, the orig-\ninal scaling experiments are conducted on BIG-\nbench (Srivastava et al., 2022), in which the per-\nformance is far from saturation and increasing\nthe model scale brings substantial improvement\nto PLMs performance. This shows consistency\nwith the identified under-fitted state. However,\nwhen the performance score saturates on evalua-\ntion datasets given the certain scale of PLM, scal-\ning larger will only bring up confidence. This re-\nsults in increasing ECE due to the mismatch be-\ntween two trends (e.g., T5 and RoBERTa on Ya-\nhoo); (2) Scaling larger and employing pretraining\nconsistently bring CErr neg higher. This indicates\nthat these two strategies don’t enable PLMs to\nlearn to become calibrated in the training process.\n1347\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100\nSST-2\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100 MNLI\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100 Yahoo\nPretrained Random LSTM TF-IDF BoW\nFigure 6: Results of the pretraining influence with T5.\n5 How Effective are Existing Methods?\n5.1 Calibration Methods\nWe choose representative calibration methods\nfrom each category summarized in Sec. 2. For\nunlearnable methods, we consider vanilla fine-\ntuning (Vanilla), temperature scaling (TS) (Guo\net al., 2017), label smoothing (LS) (Szegedy et al.,\n2016), easy data augmentation (EDA) (Wei and\nZou, 2019), and deep-ensemble (Ensemble) (Lak-\nshminarayanan et al., 2017). For learnable meth-\nods, an extra calibration task is introduced, aim-\ning to train a model to predict whether the original\npredictions are correct or not. Each sample in the\ndataset of the calibration task consists of the orig-\ninal input, the model’s original prediction, and the\nlabel indicating whether the original prediction is\ncorrect or not. We adopt the validation set to gen-\nerate the training set for the calibration task. We\ndescribe the specially designed training paradigms\nof different methods in the following paragraph\nand leave the detailed construction process of the\ncalibration training dataset in Appendix C.\nFor better clarification, we use the main task\nto denote the original task. The predictive model\nfor the calibration task can be a separate extrinsic\nmodel that we use “E-” for denotation. Specifi-\ncally, we adapt the method proposed in Kadavath\net al. (2022) that uses MLP as the extrinsic model\n(E-MLP) and the inputs are the hidden states of\nthe main task model. Based on a similar intuition,\nwe extend this method by using an extra T5 as\nthe extrinsic model (E-T5). An example of the\ntemplate to wrap the sample into an input prompt\nis: “ <original input >, the model’s prediction is\n<prediction>, is the prediction True or False?\nIt’s <mask>.” The probability of the “True”\nclass in the calibration task is deemed as PLMs’\nconfidence in their predictions. The concrete\nmanual template and verbalizer of the calibration\ntask for each dataset are listed in Table 11.\nBesides, the main task model can also be di-\nrectly employed to perform the calibration task.\nWe deem this paradigm as the intrinsic one, de-\nnoted as “I-”. Lin et al. (2022) show that GPT-\n3 (Brown et al., 2020) can be trained to output the\nuncertainty by words. We adapt this method by\nfirst training the model using the main task data,\nand then continuing the training by using the cali-\nbration task data (I-Vanilla). However, this contin-\nual learning paradigm may result in degraded per-\nformance in the main task according to our results.\nTo tackle this, we propose two more practical in-\ntrinsic calibration methods through modifying the\ntraining paradigm. Specifically, we train PLMs\niteratively (I-Iter) or simultaneously (I-Simul) on\nthe original task and the calibration task. The lat-\nter can be achieved due to the unified text-to-text\ntraining paradigm. The input is the same as E-T5.\n5.2 Experimental Setting\nPLMs are expected to tackle out-of-distribution\n(OOD) samples in practice, particularly in the\npresence of adversarial attacks (Chen et al., 2022).\nThus, we experiment with both in-distribution\n(ID) and OOD settings. We consider natu-\nral language inference, sentiment analysis, and\nhate-speech detection tasks due to their well-\nestablished OOD datasets in NLP. Specifically, we\nchoose MNLI (HANS, ANLI), Amazon (SST-5,\nSemEval), and Civil (Hate Speech, Implicit Hate)\nas the ID (OOD) datasets. The references and de-\ntailed descriptions of chosen datasets for ID and\nOOD evaluation are in Appendix A.\n5.3 Experimental Results\nThe results are listed in Table 1 (T5) and Table 4\n(RoBERTa). We summarize the overall conclu-\n1348\nMNLI\nDataset MNLI HANS ANLI\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 86.50 94.85 8.35 3.47 84.1255.06 92.36 37.30 5.96 90.3031.31 85.58 54.27 16.22 86.41TS 86.50 89.222.75 8.44 74.2255.06 83.99 28.93 14.36 81.9731.31 75.48 44.17 26.87 76.56LS 86.19 85.53 3.41 13.06 76.7456.94 83.7426.8016.19 83.6430.50 77.71 47.21 23.77 78.36EDA 86.29 95.44 9.153.06 86.0152.73 92.24 39.504.61 88.7230.34 87.45 57.1113.8688.03Ensemble86.5494.82 8.28 3.53 84.2256.52 91.90 35.38 6.72 90.1531.4185.49 54.09 16.49 86.40\nLearnable\nE-MLP 86.50 89.28 5.52 10.69 89.1055.06 87.38 32.34 12.59 87.3431.31 81.65 50.74 18.39 81.66E-T5 (ours) 86.50 79.43 12.24 15.35 45.8455.06 78.74 35.30 19.11 75.9731.31 41.67 38.68 65.84 45.11I-Vanilla85.58 78.40 12.45 15.69 43.3353.55 68.34 33.38 27.4863.5331.4140.92 38.30 65.43 43.82I-Iter (ours) 86.30 70.86 15.49 24.07 38.9557.12 74.92 28.39 22.16 71.0230.69 37.0228.3768.8439.62I-Simul (ours) 86.53 76.50 17.65 17.1535.6457.1580.26 38.64 15.85 75.0830.66 38.65 46.06 68.40 41.76\nAmazon\nDataset Amazon SST-5 SemEval\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 91.00 95.65 4.86 2.97 82.0569.7382.78 13.52 12.30 71.7255.03 76.83 21.75 17.54 69.94TS 91.00 90.501.39 7.74 73.2069.7371.984.94 23.01 60.6955.03 65.4510.3729.14 58.83LS 91.25 85.75 6.78 13.14 74.0970.67 73.50 5.55 22.53 63.9553.57 69.79 16.23 25.65 64.53EDA 92.00 96.29 4.292.51 82.4667.67 87.58 20.207.97 78.2757.2783.11 25.9611.8776.40Ensemble91.57 95.78 4.21 2.88 81.1469.35 83.00 13.66 12.13 72.0056.34 77.81 21.47 16.52 70.49\nLearnable\nE-MLP 91.00 91.34 5.13 8.66 91.3169.7384.06 14.73 16.04 84.2855.03 75.87 20.83 24.17 75.91E-T5 (ours) 91.00 70.36 20.65 23.02 3.4069.7335.23 38.72 57.70 18.9555.03 27.61 28.30 58.42 10.50I-Vanilla89.14 70.03 19.11 21.792.91 68.23 32.70 38.85 58.3513.4942.52 21.53 21.80 55.844.79I-Iter (ours) 92.2072.66 19.54 21.66 5.5870.67 33.17 38.49 60.59 18.1355.38 26.91 28.86 59.90 10.52I-Simul (ours) 91.87 71.72 20.15 22.38 5.0969.54 31.45 38.26 61.73 15.8855.28 26.35 29.37 60.57 10.17\nCivil\nDataset Civil Hate Speech Implicit Hate\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 86.08 94.23 7.74 3.88 82.1275.52 92.54 17.23 5.88 87.7260.64 89.68 28.83 8.62 87.04TS 86.08 89.653.16 7.79 73.2775.52 86.29 11.13 11.60 79.8460.64 82.24 21.38 15.49 78.71LS 86.30 84.93 5.29 13.62 75.7874.48 83.519.03 14.65 78.1560.64 81.1920.5517.36 78.95EDA 86.87 95.46 8.593.09 85.8373.64 95.20 21.563.57 91.7561.9592.92 30.975.78 90.80Ensemble86.04 94.51 8.46 3.65 83.1075.36 93.57 18.80 5.04 89.3560.83 90.98 30.14 7.50 88.62\nLearnable\nE-MLP 86.08 90.61 4.52 9.40 90.6275.52 88.93 13.41 11.13 89.1060.64 87.41 26.78 12.59 87.42E-T5 (ours) 86.08 66.22 19.87 23.24 0.9975.52 41.80 46.42 55.51 33.5160.64 25.28 40.27 64.82 10.02I-Vanilla75.31 63.39 11.92 15.950.35 75.7339.32 48.19 57.1928.4356.39 22.68 38.30 65.487.38I-Iter (ours) 86.58 69.04 17.53 20.50 1.6174.06 45.69 44.92 52.14 39.5261.29 29.05 38.67 60.89 13.11I-Simul (ours) 87.0670.69 16.55 19.04 1.6273.01 46.63 46.34 50.30 38.3161.14 30.50 40.17 58.65 13.44\nTable 1: Results of T5’s calibration performance under standard distribution shifts. We observe that learnable\nmethods can significantly mitigate the overconfidence issue.\nsions as follows: All calibration methods have\nnegligible influence on PLMs’ performance in the\nID and OOD settings except I-Vanilla. How-\never, PLMs are significantly less calibrated under\nconsidered distribution shifts, especially on chal-\nlenging datasets due to the severe mismatch be-\ntween performance and confidence. For exam-\nple, the vanilla T5 achieves only 30.53% accu-\nracy on ANLI, but its average confidence is up\nto 93.77%. For ID evaluation, we observe lower\nECE, consistent with Desai and Durrett (2020).\nHowever, the conclusion that PLMs are calibrated\non ID data (Desai and Durrett, 2020) is question-\nable given our answer to the first question (see\nSec. 4). The low ECE can be attributed to their\nhigh performance on ID datasets and consistently\nassigning high confidence scores to their predic-\ntions. We further show the conclusion that PLMs\ncalibration degrades under distribution shifts is\none-sided and heavily depends on the evaluation\ndatasets chosen in Appendix E.\nUnlearnable methods. We summarize the\nfindings as follows: (1) Data augmentation and\nmodel ensemble don’t bring substantial benefits\nto PLMs calibration, considering the three cal-\nibration metrics spanning all evaluation datasets\nand two PLMs. The reason lies in their inability\nto relieve the overconfident issue, resulting in the\nsame Cerrneg with the vanilla fine-tuning; (2) TS\nachieves overall better ECE, maintaining a strong\nbaseline method, with LS being the second effec-\ntive method for the unlearnable category. This is\nconsistent with previous empirical studies (Nixon\net al., 2019). However, we can observe almost the\nsame amount of increase in CErr pos with the de-\ncrease in CErr neg. The reason is that these two\nmethods directly impose confidence regularization\non predictions, which don’t actually make PLMs\nhave clear confidence estimations.\nLearnable methods. Compared to unlearnable\nmethods, learnable ones significantly mitigate\nthe overconfidence issue, reflected in the sharp\ndecrease in CErr neg, indicating that learnable\nmethods output very low confidence in wrong\npredictions. But we also observe that learn-\nable methods lower the confidence in correct\npredictions, resulting in increasing CErr pos and\nECE. However, we highlight two observations\nindicating that learnable methods essentially teach\nmodels to have clearer confidence estimations,\ninstead of roughly reducing the confidence like\nLS: (1) Compared to the vanilla version, the\n1349\nDataset SizeDataset Amazon SST-5 SemEval\nSmall\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 91.00 90.411.71 9.59 90.3969.7387.81 18.0812.16 87.7355.0386.86 31.8313.11 86.83E-T5 (ours) 91.00 68.92 22.08 28.16 39.4469.7355.95 15.12 41.71 50.5855.0350.998.54 43.17 43.84I-Vanilla 89.06 68.45 20.61 28.01 39.6263.92 56.4910.6639.82 49.96 51.48 49.47 9.12 44.10 42.64I-Iter (ours) 90.58 68.96 21.62 28.08 40.4769.63 56.69 12.95 41.27 52.0053.72 53.89 10.24 43.31 50.64I-Simul (ours) 91.3780.44 15.44 15.0532.7871.13 66.28 26.97 25.5846.2354.0837.5134.94 53.82 27.30\nMiddle\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 91.00 90.444.35 9.56 90.4169.7385.1815.45 14.69 84.8755.03 78.39 23.3621.63 78.42E-T5 (ours) 91.00 71.03 19.97 22.40 4.6369.7331.73 38.80 61.80 16.8355.03 29.72 26.28 56.23 12.54I-Vanilla 88.25 70.91 17.34 20.163.86 63.07 29.81 34.08 59.4211.4248.08 25.32 23.69 55.537.59I-Iter (ours) 91.6971.76 19.93 22.23 5.4368.23 33.46 36.87 59.79 18.9656.2335.2121.4250.98 17.48I-Simul (ours) 91.38 70.92 20.47 22.80 4.3070.29 32.03 42.12 60.65 14.7254.75 26.18 30.70 59.34 8.67\nLarge\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 91.00 91.345.13 8.66 91.3169.73 84.0614.73 16.04 84.2855.03 75.8720.83 24.17 75.91E-T5 (ours) 91.00 70.36 20.65 23.02 3.4069.73 35.23 38.72 57.70 18.9555.03 27.61 28.30 58.42 10.50I-Vanilla 89.14 70.03 19.11 21.792.91 68.23 32.70 38.85 58.3513.4942.52 21.53 21.80 55.844.79I-Iter (ours) 92.2072.66 19.54 21.66 5.5870.6733.17 38.49 60.59 18.1355.3826.91 28.86 59.90 10.52I-Simul (ours) 91.87 71.72 20.15 22.38 5.0969.54 31.45 38.26 61.73 15.8855.28 26.35 29.37 60.57 10.17\nTable 2: Results of T5’s calibration performance with increasing dataset sizes. We observe a significant improve-\nment in calibration performance when increasing the dataset size from small to middle.\nincrease in CErrpos is significantly lower than the\ndecrease in CErr neg, especially on ID samples;\n(2) Learnable methods give obviously lower\nconfidence in OOD samples, and the average\nconfidence drop is highly relevant to the perfor-\nmance drop under distribution shifts. Thus, the\nlow confidence and relatively higher CErr pos and\nECE on OOD samples may be reasonable.\nFurther, we give a detailed analysis of extrin-\nsic and intrinsic learnable methods and also com-\npare our extended calibration methods with previ-\nous methods: (1) For extrinsic methods, the ex-\ntended E-T5 exhibits significantly better calibra-\ntion performance compared to the adapted E-MLP\nconsidering the mitigation of the overconfidence\nissue. The essential difference mainly lies in the\nextrinsic model for the calibration task. We find\nthat using the larger capacity model as the extrin-\nsic calibrator shows the same trend with shifting\nfrom the vanilla fine-tuning to learnable methods.\nWe further study this scaling effect in Sec. 5.4; (2)\nFor intrinsic methods, the three different training\nparadigms don’t show substantial differences con-\nsidering the calibration performance, and none of\nthem consistently achieves the best performance\non all datasets. As a comparison, our methods\n(I-Iter and I-Simul) address the degraded perfor-\nmance issue of I-Vanilla and make the main task\nperformance match with the vanilla fine-tuning;\n(3) Interestingly, there doesn’t exist a substantial\ndifference between the extrinsic E-T5 method and\nother intrinsic methods, given the same base archi-\ntecture (e.g., T5). This finding leads us to recon-\nsider the conclusion in Lin et al. (2022) that PLMs\ncan be trained to give their uncertainty by words.\nGiven the comparable performance between in-\ntrinsic and extrinsic methods, we provide an exten-\nsion to this conclusion. We identify that the suc-\ncess of this paradigm essentially lies in the learn-\nable attribute of the calibration task, instead of the\nself-checking process of PLMs. Namely, the find-\nings in previous work may not only be attributed\nto the capability of PLMs but also the ”learnable”\nproperty of the calibration task.\n5.4 Emergent Calibration\nIn Sec. 5.3, we identify the potential in learn-\nable methods. However, a detailed exploration\nof learnable calibration methods is lacking. We\nconduct experiments to study the influence of two\nimportant factors, namely the dataset size and the\nmodel scale for the calibration task, on PLMs cal-\nibration. Note that the model scale in this sec-\ntion considers the model adopted for the calibra-\ntion task, instead of the main task.\nDataset size. Table 2 shows the results of dif-\nferent sizes of the calibration dataset. Two basic\nfindings are: (1) The five learnable methods show\na consistent trend when increasing the dataset size,\nindicating that the essence of these methods is the\nsame; (2) The size of datasets for training the cal-\nibration task doesn’t have a substantial influence\non PLMs performance on the main task.\nBeyond these, we observe that there is a sharp\ndifference in calibration performance when in-\ncreasing the dataset size from small to middle. The\ntrend is overall consistent with the one observed\nwhen shifting from vanilla fine-tuning to learnable\n1350\ncalibration methods. The trend can be summarized\nas: (1) For ID samples, we can observe a sharp\ndecrease in CErr neg with relatively less negative\ninfluence on ECE and CErrpos; (2) For OOD sam-\nples, the CErr pos and ECE increase significantly\nalong with increasing the dataset size. However,\ngiven the arguments in Sec. 5.3, we identify that\nPLMs’ calibration performance improves when\ntrained on larger calibration datasets. Besides,\nwe don’t observe further improvement in calibra-\ntion performance when increasing the dataset size\nfrom middle to large. This is consistent with nor-\nmal task training, where increasing the dataset size\ndoesn’t increase performance after a critical point.\nModel scale. Table 5 shows the results of vari-\nous model scales. Two basic findings are: (1) The\nfive learnable methods still show a consistent trend\nwhen scaling larger; (2) We observe a consistent\nconfidence increase when scaling larger, which\nis similar to the trend observed in Sec. 4, where\nincreasing capacity makes PLMs more confident.\nSurprisingly, although the confidence contin-\nues to increase, for ID samples, we observe a\nconsistent decrease in CErr pos with neglectable\ninfluence on ECE and CErr neg when scaling\nlarger. Note that the dataset for the calibration\ntask is collected from ID. Thus, if provided\nenough ID samples for the calibration task train-\ning, scaling larger enables models to better learn\nthe calibration task, ensuring better calibration\nperformance on ID samples. For OOD samples,\nwe don’t observe a consistent trend due to the\ninfluence of various factors. Specifically, when\nusing out-of-the-box to tackle OOD samples,\nthe problem of distribution shifts appears in the\nintroduced calibration task. Whether scaling the\ncalibration-task model larger improves calibration\nperformance under distribution shifts is deter-\nmined by many factors (e.g., the dataset difficulty,\nthe overconfidence issue in the calibration task).\nWe leave it for future exploration.\n6 Conclusion\nWe take a close look into PLMs calibration, mo-\ntivating to answer two central questions: (1) Do\nPLMs learn to become calibrated in the training\nprocess? (2) How effective are existing calibra-\ntion methods? We present a comprehensive em-\npirical study, including the analysis of various de-\ncisive factors and concrete calibration methods.\nBesides the findings that support existing conclu-\nsions, we also provide extensions or contradictory\narguments to some established conclusions.\nLimitations and Future Work\nWe identify two limitations in our work that ne-\ncessitate further investigation and improvement.\nFirst, only empirical results are presented in our\nwork. A theoretical understanding of PLMs cali-\nbration is still lacking. Going forward, we are mo-\ntivated to investigate this problem from the stand-\npoint of feature learning. We see great potential\nin unifying several problems in AI safety (Houben\net al., 2021) from a feature-learning perspective,\nincluding spurious correlations (Gu et al., 2019;\nWang et al., 2022), robustness (Yuan et al., 2021;\nZhang et al., 2022), backdoor learning (Sheng\net al., 2022; Cui et al., 2022), and calibration (Ul-\nmer et al., 2022). Second, we propose three simple\nextended calibration methods based on existing\nones. In our experiments, we evaluate the calibra-\ntion performance of existing and our calibration\nmethods. We make an assumption that we have a\nlarge held-out validation set that can be employed\nas the training dataset for the calibration task. We\ndemonstrate the effectiveness of learnable calibra-\ntion methods in this ideal situation. However, in\npractice, we need to make the decision about how\nto allocate the data for the main task and the cali-\nbration task given limited training samples.\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502) and\nInstitute Guo Qiang at Tsinghua University.\nReferences\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neu-\nral Information Processing Systems 2020, NeurIPS\n2020, December 6-12, 2020, virtual.\nYangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao\nQi, Longtao Huang, Zhiyuan Liu, and Maosong Sun.\n1351\n2022. Why should adversarial perturbations be im-\nperceptible? rethink the research paradigm in ad-\nversarial NLP. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , pages 11222–\n11237. Association for Computational Linguistics.\nGanqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen,\nZhiyuan Liu, and Maosong Sun. 2022. A uni-\nfied evaluation of textual backdoor learning: Frame-\nworks and benchmarks. In NeurIPS.\nSoham Dan and Dan Roth. 2021. On the effects of\ntransformer size on in- and out-of-domain calibra-\ntion. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November,\n2021, pages 2096–2101. Association for Computa-\ntional Linguistics.\nOna de Gibert, Naiara Perez, Aitor Garc´ıa-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11–20, Brussels, Belgium. Association for\nComputational Linguistics.\nMorris H DeGroot and Stephen E Fienberg. 1983. The\ncomparison and evaluation of forecasters. Journal\nof the Royal Statistical Society: Series D (The Statis-\ntician), 32(1-2):12–22.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 295–302, On-\nline. Association for Computational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tun-\ning: A comprehensive study of parameter efficient\nmethods for pre-trained language models. ArXiv\npreprint, abs/2203.06904.\nMai ElSherief, Caleb Ziems, David Muchlinski, Vaish-\nnavi Anupindi, Jordyn Seybolt, Munmun De Choud-\nhury, and Diyi Yang. 2021. Latent hatred: A bench-\nmark for understanding implicit hate speech. InPro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 345–\n363, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of the\n33nd International Conference on Machine Learn-\ning, ICML 2016, New York City, NY, USA, June 19-\n24, 2016, volume 48 of JMLR Workshop and Con-\nference Proceedings, pages 1050–1059. JMLR.org.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor\nO. K. Li. 2019. Improved zero-shot neural ma-\nchine translation via ignoring spurious correlations.\nIn Proceedings of the 57th Conference of the Asso-\nciation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers , pages 1258–1268. Association for\nComputational Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q.\nWeinberger. 2017. On calibration of modern neu-\nral networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70 of Proceedings of Machine Learning Re-\nsearch, pages 1321–1330. PMLR.\nKartik Gupta, Amir Rahimi, Thalaiyasingam Ajan-\nthan, Thomas Mensink, Cristian Sminchisescu, and\nRichard Hartley. 2021. Calibration of neural net-\nworks using splines. In 9th International Con-\nference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nDan Hendrycks, Steven Basart, Mantas Mazeika,\nMohammadreza Mostajabi, Jacob Steinhardt, and\nDawn Song. 2019a. Scaling out-of-distribution de-\ntection for real-world settings. ArXiv preprint ,\nabs/1911.11132.\nDan Hendrycks, Kimin Lee, and Mantas Mazeika.\n2019b. Using pre-training can improve model ro-\nbustness and uncertainty. In Proceedings of the\n36th International Conference on Machine Learn-\ning, ICML 2019, 9-15 June 2019, Long Beach, Cal-\nifornia, USA, volume 97 of Proceedings of Machine\nLearning Research, pages 2712–2721. PMLR.\nDan Hendrycks, Norman Mu, Ekin Dogus Cubuk,\nBarret Zoph, Justin Gilmer, and Balaji Lakshmi-\nnarayanan. 2020. Augmix: A simple data process-\ning method to improve robustness and uncertainty.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nSebastian Houben, Stephanie Abrecht, Maram Akila,\nAndreas B¨ar, Felix Brockherde, Patrick Feifel, Tim\nFingscheidt, Sujan Sai Gannamaneni, Seyed Egh-\nbal Ghobadi, Ahmed Hammam, Anselm Hasel-\nhoff, Felix Hauser, Christian Heinzemann, Marco\nHoffmann, Nikhil Kapoor, Falk Kappel, Marvin\nKlingner, Jan Kronenberger, Fabian K¨uppers, Jonas\nL¨ohdefink, Michael Mlynarski, Michael Mock,\nFiras Mualla, Svetlana Pavlitskaya, Maximilian\nPoretschkin, Alexander Pohl, Varun Ravi Ku-\nmar, Julia Rosenzweig, Matthias Rottmann, Ste-\nfan R ¨uping, Timo S ¨amann, Jan David Schneider,\nElena Schulz, Gesina Schwalbe, Joachim Sicking,\nToshika Srivastava, Serin Varghese, Michael We-\nber, Sebastian Wirkert, Tim Wirtz, and Matthias\n1352\nWoehrle. 2021. Inspect, understand, overcome: A\nsurvey of practical methods for AI safety. CoRR,\nabs/2104.14235.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efficient transfer learning for NLP.\nIn Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA , volume 97 of\nProceedings of Machine Learning Research , pages\n2790–2799. PMLR.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. ArXiv preprint ,\nabs/2207.05221.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in- and out-\nof-distribution data. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1326–1340, On-\nline. Association for Computational Linguistics.\nAnanya Kumar, Tengyu Ma, Percy Liang, and Aditi\nRaghunathan. 2022. Calibrated ensembles can mit-\nigate accuracy tradeoffs under distribution shift. In\nThe 38th Conference on Uncertainty in Artificial In-\ntelligence.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In\nAdvances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 6402–6413.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n3045–3059. Association for Computational Linguis-\ntics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTeaching models to express their uncertainty in\nwords. ArXiv preprint, abs/2205.14334.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nHans Peter Luhn. 1957. A statistical approach to mech-\nanized encoding and searching of literary informa-\ntion. IBM Journal of research and development ,\n1(4):309–317.\nJulian J. McAuley and Jure Leskovec. 2013. From\namateurs to connoisseurs: modeling the evolution\nof user expertise through online reviews. In 22nd\nInternational World Wide Web Conference, WWW\n’13, Rio de Janeiro, Brazil, May 13-17, 2013, pages\n897–908. International World Wide Web Confer-\nences Steering Committee / ACM.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nMatthias Minderer, Josip Djolonga, Rob Romijnders,\nFrances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin\nTran, and Mario Lucic. 2021. Revisiting the cal-\nibration of modern neural networks. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 15682–15694.\nMahdi Pakdaman Naeini, Gregory F. Cooper, and Mi-\nlos Hauskrecht. 2015. Obtaining well calibrated\nprobabilities using bayesian binning. In Proceed-\nings of the Twenty-Ninth AAAI Conference on Ar-\ntificial Intelligence, January 25-30, 2015, Austin,\nTexas, USA, pages 2901–2907. AAAI Press.\nPreslav Nakov, Sara Rosenthal, Zornitsa Kozareva,\nVeselin Stoyanov, Alan Ritter, and Theresa Wilson.\n2013. SemEval-2013 task 2: Sentiment analysis in\nTwitter. In Second Joint Conference on Lexical and\nComputational Semantics (*SEM), Volume 2: Pro-\nceedings of the Seventh International Workshop on\nSemantic Evaluation (SemEval 2013) , pages 312–\n320, Atlanta, Georgia, USA. Association for Com-\nputational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit\nBansal, Jason Weston, and Douwe Kiela. 2020. Ad-\nversarial NLI: A new benchmark for natural lan-\nguage understanding. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4885–4901, Online. Asso-\nciation for Computational Linguistics.\nJeremy Nixon, Michael W. Dusenberry, Linchuan\nZhang, Ghassen Jerfel, and Dustin Tran. 2019. Mea-\nsuring calibration in deep learning. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\nWorkshops, CVPR Workshops 2019, Long Beach,\nCA, USA, June 16-20, 2019, pages 38–41. Computer\nVision Foundation / IEEE.\n1353\nGabriel Pereyra, George Tucker, Jan Chorowski,\nLukasz Kaiser, and Geoffrey E. Hinton. 2017. Reg-\nularizing neural networks by penalizing confident\noutput distributions. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Workshop Track\nProceedings. OpenReview.net.\nJohn Platt et al. 1999. Probabilistic outputs for sup-\nport vector machines and comparisons to regularized\nlikelihood methods. Advances in large margin clas-\nsifiers, 10(3):61–74.\nChristopher Potts, Zhengxuan Wu, Atticus Geiger, and\nDouwe Kiela. 2021. DynaSent: A dynamic bench-\nmark for sentiment analysis. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 2388–2404, Online. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nMamshad Nayeem Rizve, Kevin Duarte, Yogesh S\nRawat, and Mubarak Shah. 2021. In defense\nof pseudo-labeling: An uncertainty-aware pseudo-\nlabel selection framework for semi-supervised learn-\ning. ArXiv preprint, abs/2101.06329.\nXuan Sheng, Zhaoyang Han, Piji Li, and Xiangmao\nChang. 2022. A survey on backdoor attack and de-\nfense in natural language processing. In 22nd IEEE\nInternational Conference on Software Quality, Reli-\nability and Security, QRS 2022, Guangzhou, China,\nDecember 5-9, 2022, pages 809–820. IEEE.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-\nGraber. 2022. Revisiting calibration for question an-\nswering. ArXiv preprint, abs/2205.12507.\nJasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji\nLakshminarayanan, Sebastian Nowozin, D. Sculley,\nJoshua V . Dillon, Jie Ren, and Zachary Nado. 2019.\nCan you trust your model’s uncertainty? evaluat-\ning predictive uncertainty under dataset shift. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada , pages 13969–\n13980.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013a. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1631–1642, Seattle, Washington, USA.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013b. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1631–1642, Seattle, Washington, USA.\nAssociation for Computational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri`a Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. ArXiv preprint ,\nabs/2206.04615.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. 2016. Re-\nthinking the inception architecture for computer vi-\nsion. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016, pages 2818–2826.\nIEEE Computer Society.\nDennis Ulmer, Jes Frellsen, and Christian Hardmeier.\n2022. Exploring predictive uncertainty and calibra-\ntion in NLP: A study on the impact of method & data\nscarcity. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022, pages\n2707–2735. Association for Computational Linguis-\ntics.\nJuozas Vaicenavicius, David Widmann, Carl R. Ander-\nsson, Fredrik Lindsten, Jacob Roll, and Thomas B.\nSch¨on. 2019. Evaluating model calibration in clas-\nsification. In The 22nd International Conference\non Artificial Intelligence and Statistics, AISTATS\n2019, 16-18 April 2019, Naha, Okinawa, Japan, vol-\nume 89 of Proceedings of Machine Learning Re-\nsearch, pages 3459–3467. PMLR.\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022. Investigating selective prediction approaches\nacross several tasks in iid, ood, and adversarial set-\ntings. ArXiv preprint, abs/2203.00211.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nHaotao Wang, Chaowei Xiao, Jean Kossaifi, Zhid-\ning Yu, Anima Anandkumar, and Zhangyang Wang.\n2021. Augmax: Adversarial composition of random\naugmentations for robust training. In Advances in\nNeural Information Processing Systems 34: Annual\nConference on Neural Information Processing Sys-\ntems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 237–250.\n1354\nTianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi\nWang. 2022. Identifying and mitigating spurious\ncorrelations for improving robustness in NLP mod-\nels. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022 , pages 1719–1729.\nAssociation for Computational Linguistics.\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\nmentation techniques for boosting performance on\ntext classification tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018a. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018b. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nYuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie\nNeiswanger, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2022. Uncertainty quantification\nwith pre-trained language models: A large-scale em-\npirical analysis. arXiv preprint arXiv:2210.04714.\nXi Ye and Greg Durrett. 2022. Can explanations be\nuseful for calibrating black box models? In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 6199–6212. Association for Computa-\ntional Linguistics.\nLifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei.\n2021. Bridge the gap between CV and nlp! A\ngradient-based textual adversarial attack framework.\nCoRR, abs/2110.15317.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), ACL 2022, Dublin, Ireland, May 22-\n27, 2022, pages 1–9. Association for Computational\nLinguistics.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2021a. Understand-\ning deep learning (still) requires rethinking general-\nization. Communications of the ACM , 64(3):107–\n115.\nShujian Zhang, Chengyue Gong, and Eunsol Choi.\n2021b. Knowing more about questions can help:\nImproving calibration in question answering. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1958–1970, On-\nline. Association for Computational Linguistics.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun.\n2015. Character-level convolutional networks for\ntext classification. In Advances in Neural Infor-\nmation Processing Systems 28: Annual Conference\non Neural Information Processing Systems 2015,\nDecember 7-12, 2015, Montreal, Quebec, Canada ,\npages 649–657.\nYunxiang Zhang, Liangming Pan, Samson Tan, and\nMin-Yen Kan. 2022. Interpreting the robustness\nof neural NLP models to textual perturbations. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 3993–4007. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015 , pages 19–\n27. IEEE Computer Society.\n1355\nA Datasets\nIn this section, we describe the datasets adopted\nin experiments by tasks. The dataset statistics are\nshown in Table 9. The manual templates and ver-\nbalizers are presented in Table 10.\nSentiment analysis. SST (Socher et al., 2013b)\nis a sentence-level corpus of movie reviews, where\neach sentence is labeled as negative, somewhat\nnegative, neutral, somewhat positive, or positive.\nSST-5 contains the complete corpus with all five\nlabels, while SST-2 discards the label neutral and\npolarizes the remaining 4 classes, i.e., negative\nor somewhat negative vs. somewhat positive or\npositive. Amazon Fine Foods (McAuley and\nLeskovec, 2013), denoted as Amazon for simplic-\nity throughout the paper, is a sentiment analysis\ndataset of reviews on fine foods from Amazon.\nDue to the enormous dataset size in the dataset,\nwe sample 10k samples per class from the dataset.\nSemEval 2016 Task 4 (Nakov et al., 2013) is the\nsentiment analysis in the Twitter task. We con-\nsider Subtask A, where all Twitter texts are la-\nbeled as negative, neutral, or positive. Dynasent\n(Potts et al., 2021) is a challenging and dynami-\ncally evolved dataset, adopting human-in-the-loop\nefforts in dataset construction. We merge the data\nof round 1 and round 2 in our experiments.\nNatural language inference. MNLI (Williams\net al., 2018b) consists of 10 types of written and\nspoken English data and has two versions called\nmatched and mismatched respectively, according\nto whether the domain of the train set and dev/test\nset is matched. We use the matched version in\nour experiment. HANS (McCoy et al., 2019) is\na heuristic analysis dataset for NLI systems, based\non the specific hypotheses about invalid heuristics\nthat may be captured by the NLI model. ANLI\n(Nie et al., 2020) is an adversarial NLI dataset, cre-\nated by an iterative (three rounds in total), human-\nand-model-in-the-loop procedure. We merge the\ndata from all three rounds in our experiments.\nTopic classification. Yahoo Topic Answers\n(Zhang et al., 2015) contains 10 categories of\nquestions and their corresponding answers from\nthe Yahoo! Webscope program. For each sam-\nple, the title and content of the question are con-\ncatenated as one text, and the best answer to the\nquestion is used as a label. Since the original\ntraining dataset is extremely large (1.4 million\nsamples for each category), we randomly sample\n140,000 samples for simplicity. AG News (Zhang\net al., 2015) is a corpus of news articles consist-\ning of 4 classes: World, Sports, Business, and Sci-\nence/Technology. For each article, we construct\nthe text by concatenating the title and description.\nToxic detection. Civil Comments 1 is collected\nfrom the Civil Comments platform. Each com-\nment is annotated with a float toxicity score, scal-\ning from 0 to 1. We follow the official instruc-\ntions to set samples with a toxicity score smaller\nthan 0.5 as label 0 and vice versa. Hate Speech\n(de Gibert et al., 2018), the arguably most popular\ndataset in toxic detection, is collected from Storm-\nfront, a large forum of white nationalists. The test\nset we use is sampled by the author in the offi-\ncial Github repository. Implicit Hate (ElSherief\net al., 2021) consists of hate tweets from extremist\ngroups in the US. Notably, a part of the hate tweets\nis implicit, which contains some subtle tricks to\nconceal the toxicity and evade keyword detection.\nPlain text. BookCorpus (Zhu et al., 2015) col-\nlects a tremendous number of free novel books and\nthus is used in the pre-training stage of pre-trained\nlanguage models. We sample 10k texts for eval-\nuation. Random Words contains 1k meaningless\ntexts, each synthesized by concatenating 20 ran-\ndom words.\nB Additional Results of Control\nExperiments\nFor the empirical control study in the influence of\nsix factors on PLMs calibration, we provide addi-\ntional experimental results. The results of T5-base\non AG News are shown in Fig.7, Fig.8, Fig.9, and\nFig.10. The results of RoBERTa-base are shown in\nFig.11, Fig.12, Fig.13, Fig.14, Fig.15, and Fig.16.\nWe discuss detailed experimental settings and con-\nclusions for each considered factor.\nAvailable training samples. We adopt K-shot\nlearning, where K is the number of samples per\nclass. We experiment with each K five times\non each dataset and report the average perfor-\nmance due to the potential variance in the few-\nshot setting. In this dimension, we additionally\nfind that the trends in average confidence are dif-\nferent in the two model architectures. While\n1https://www.kaggle.com/competitions/\njigsaw-unintended-bias-in-toxicity-\\\nclassification\n1356\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\n AG-News\n0.0 1.2 2.4 3.6 4.8 6.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\n AG-News\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\nAG-News\nAccConfECECErrposCErrneg\nFigure 7: Additional results of T5 on AG-News including the influence of the number of training samples, training\nsteps, and the tunable parameters number.\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100\nAG-News\nFigure 8: Additional results of different PLMs scales\nwith T5 on AG-News.\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100 AG-News\nFigure 9: Additional results of the pretraining in-\nfluence with T5 on AG-News.\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n SST-2\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n MNLI\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n Yahoo\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n AG-News\nAccConfECECErrposCErrneg\nFigure 10: Results of tunable parameters with T5 (Soft-prompt).\nT5 has an obvious confidence drop in the early\nstage, the confidence of RoBERTa seems to con-\ntinually increase along with the number of avail-\nable training samples. This can be partially ex-\n1357\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\nSST-2\n0 1 2 4 8 1632641282565121024204840968192\nShots\n0\n20\n40\n60\n80\n100\n MNLI\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\nAG-News\n0 1 2 4 8 16 32 64 1282565121024\nShots\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 11: Results of available training samples with RoBERTa.\n0.0 0.8 1.6 2.4 3.2 4.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\nSST-2\n0.0 45.0 90.0 135.0 180.0 225.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\nMNLI\n0.0 1.2 2.4 3.6 4.8 6.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\nAG-News\n0.0 16.0 32.0 48.0 64.0\nSteps (×103)\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 12: Results of training steps with RoBERTa.\nplained by the stronger few-shot adaptation of\nRoBERTa since we observe that the performance\nof RoBERTa is significantly higher in extreme\ncases (e.g., K=1,2,4).\nTraining dynamics. We decompose the whole\ntraining process into steps, and measure five met-\nrics during some fixed intervals. In this dimension,\nthe conclusion is consistent with the general one.\nNumber of tunable parameters. To quantita-\ntively explore the influence of the number of tun-\nable parameters on PLMs calibration, we em-\nploy the parameter efficient tuning methods in\nNLP (Houlsby et al., 2019; Zaken et al., 2022;\nDing et al., 2022). We adopt Soft-prompt (Lester\n1358\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100\nSST-2\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100 MNLI\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100\nAG-News\nAcc Conf ECE CErrpos CErrneg\nScale\n0\n20\n40\n60\n80\n100 Yahoo\nTinyMiniSmallLargeBaseMedium\nFigure 13: Results of increasing PLMs scales with RoBERTa.\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100\nSST-2\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100 MNLI\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100\nAG-News\nAcc Conf ECE CErrpos CErrneg\n0\n20\n40\n60\n80\n100 Yahoo\nPretrained Random LSTM TF-IDF BoW\nFigure 14: Results of the pretraining influence with RoBERTa.\net al., 2021) and Adapter (Houlsby et al., 2019)\ntuning due to their simplicity, stability, and prac-\nticality. We experiment with various numbers of\nsoft tokens and bottleneck dimensions of the in-\nserted adapter modules. Only the parameters in\nthe soft tokens and adapter module are tunable.\nWe summarize the extra findings as follows: (1)\nSoft-prompt and Adapter tuning show different\ntrends spanning four datasets; (2) For Soft-prompt\ntuning, the model performance and confidence in-\ncrease continually with more tunable parameters.\nWe can observe that the increasing rates are nearly\nmatched, thus decreasing ECE continually. The\nnegative effect is also the increase in CErrneg due\nto the overconfidence in wrong predictions. This is\nconsistent with the trend we observed in the under-\nfitted state; (3) The world in Adapter tuning is\ndifferent, where increasing capacity cannot bring\n1359\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\nSST-2\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\n MNLI\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\n AG-News\n1 4 16 64 256 1024\nBottleneck Dim\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 15: Results of tunable parameters with RoBERTa (Adapter).\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n SST-2\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n MNLI\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n AG-News\n1 5 10 20 50\nSoft T oken Num\n0\n20\n40\n60\n80\n100\n Yahoo\nAccConfECECErrposCErrneg\nFigure 16: Results of tunable parameters with RoBERTa (Soft-prompt).\nsubstantial performance gains. This is due to the\nstrong capacity of Adapter. However, the overall\nconfidence continues to increase given more ca-\npacity, resulting in increasing ECE and CErr neg,\nwhile the performance stays constant. This is con-\nsistent with the trend we observed in the over-\nfittied state; (4) The implication of experimental\nresults is that blindly increasing model capacity\nmay negatively impact PLMs calibration, espe-\ncially at the critical point when current capacity\nis sufficient to solve the task well.\nModel scale. We consider the scaling law and\nexperiment with various model sizes. For T5,\nwe choose models with small, base, large, and\n3b sizes. For RoBERTa, we choose models with\ntiny, mini, small, medium, base, and large sizes.\n1360\nDynasent\nDataset Dynasent Amazon DSC\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 78.45 86.83 8.38 9.94 75.0786.57 95.28 8.71 3.44 87.0290.0094.40 4.48 4.10 80.85TS 78.45 79.101.02 17.37 66.2786.57 89.92 3.36 8.59 80.3190.0089.260.78 8.90 72.68LS 78.4778.22 3.64 18.89 67.6986.55 85.48 3.42 13.35 77.9189.75 84.61 5.31 13.95 72.02EDA 76.30 89.20 12.917.76 79.4487.1997.07 9.881.75 89.0488.05 95.50 7.452.81 83.03Ensemble78.18 86.76 8.58 9.89 74.7586.37 95.02 8.66 3.71 86.9989.74 94.27 4.56 4.17 80.67\nLearnable\nE-MLP 78.45 78.99 4.45 21.05 79.1186.57 83.152.92 16.85 83.1490.0082.53 7.17 17.48 82.63E-T5 (ours) 78.45 61.63 18.26 33.00 42.0786.57 89.99 6.51 6.94 71.0090.0086.14 6.19 11.03 61.60I-Vanilla78.4761.95 17.91 32.77 42.7284.44 89.89 6.52 6.18 68.5288.84 86.15 5.76 10.77 61.69I-Iter (ours) 77.92 61.45 16.47 33.26 42.7886.03 86.92 2.99 9.9967.9189.45 84.72 4.88 12.54 61.55I-Simul (ours) 78.13 66.36 24.59 25.5137.3485.67 91.26 13.29 5.28 70.5988.61 87.83 12.46 8.4158.61\nTable 3: Results T5’s calibration performance under hard-to-easy distribution shifts.\nMNLI\nDataset MNLI HANS ANLI\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 85.90 96.24 9.50 2.40 87.3654.17 95.09 39.68 2.71 92.3629.78 90.94 61.14 11.28 91.90TS 85.90 86.650.90 11.09 71.8454.17 82.15 26.74 15.43 79.1629.78 75.57 45.57 27.32 76.80LS 86.28 86.88 4.43 11.92 79.3155.59 86.96 31.37 11.47 85.0029.25 81.59 52.37 20.23 82.34EDA 85.99 97.07 11.091.78 90.0558.2496.87 38.631.91 95.1631.3492.00 60.668.81 92.38Ensemble86.60 96.32 9.74 2.37 87.9056.09 96.44 40.35 2.00 94.4530.06 90.47 60.46 11.38 91.26\nLearnable\nE-MLP 85.90 85.82 13.73 14.16 85.6754.17 81.92 29.36 17.87 81.6629.78 81.49 51.71 18.88 81.65E-T5 (ours) 85.90 74.37 18.51 18.93 33.5854.17 74.47 28.79 10.10 56.2329.78 35.21 45.46 74.72 39.43I-Vanilla 85.76 75.23 18.25 18.32 36.4557.28 77.14 32.26 13.23 64.2328.63 37.14 44.78 71.91 40.77I-Iter (ours) 86.6360.04 26.59 33.8520.4153.70 57.7721.7029.34 42.8231.06 21.2931.8883.71 23.55I-Simul (ours) 86.46 74.81 18.91 18.49 32.0156.65 75.84 33.83 13.79 62.2829.16 38.67 45.44 66.86 40.95\nAmazon\nDataset Amazon SST-5 SemEval\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 90.90 98.17 7.28 1.09 90.8470.2994.29 24.05 3.95 90.1456.02 90.45 34.43 7.05 87.26TS 90.90 89.662.02 8.73 73.58 70.2978.157.91 18.42 70.0456.02 70.3414.3225.98 65.65LS 91.89 88.50 6.71 10.64 78.8369.92 84.01 14.20 14.38 80.2855.17 81.64 26.47 15.46 78.08EDA 92.3998.34 5.950.92 89.4666.64 93.98 27.343.82 89.5757.0593.45 36.434.37 90.56Ensemble91.69 98.19 6.50 1.06 89.9369.56 93.67 24.22 4.24 88.9355.94 90.14 34.23 7.19 86.76\nLearnable\nE-MLP 90.90 95.08 9.14 4.94 95.3470.2983.57 22.22 16.18 82.9956.02 77.12 25.42 22.49 76.63E-T5 (ours) 90.90 71.97 19.27 21.20 3.7270.2932.10 45.94 61.74 17.5356.02 23.64 36.13 64.58 8.63I-Vanilla 88.00 71.60 17.13 19.18 3.9764.85 26.74 46.32 65.7512.8644.43 17.51 31.05 66.925.07I-Iter (ours) 90.11 71.34 18.88 21.183.24 66.54 34.13 41.70 58.17 18.8253.28 34.05 27.10 48.25 13.86I-Simul (ours) 90.60 71.07 19.80 21.91 3.4169.35 33.96 44.16 58.75 17.4653.50 24.20 33.35 61.15 7.36\nCivil\nDataset Civil Hate Speech Implicit Hate\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 86.94 98.15 10.091.14 92.9176.99 98.22 21.941.22 96.4162.8896.37 32.02 2.88 95.00TS 86.94 90.942.88 7.29 77.87 76.99 89.70 13.34 8.58 84.1662.8885.5021.1512.72 82.28LS 87.9187.73 9.52 11.79 84.2478.4588.3110.8611.48 87.5462.58 86.79 24.21 12.82 86.13EDA 83.61 97.01 13.40 2.08 92.3577.82 97.28 19.65 2.30 95.8261.53 96.68 35.142.71 95.70Ensemble86.45 97.96 11.52 1.29 93.1676.32 97.58 21.28 1.75 95.4162.77 96.19 33.42 3.08 94.97\nLearnable\nE-MLP 86.94 91.93 12.24 8.09 92.0176.99 88.52 19.66 11.62 88.9862.8883.08 25.45 17.15 83.47E-T5 (ours) 86.94 70.97 15.99 18.62 1.6876.99 46.28 48.83 52.25 41.3762.8830.90 41.57 59.84 15.20I-Vanilla 77.92 69.06 8.92 11.600.83 76.99 45.25 49.59 53.2440.2158.12 29.51 38.32 58.5813.00I-Iter (ours) 85.40 75.36 10.31 12.18 2.4876.15 50.43 49.62 50.02 51.8460.59 34.15 38.04 54.50 16.69I-Simul (ours) 87.25 70.69 16.65 19.22 1.7178.24 45.86 50.64 53.36 43.0362.56 29.60 41.56 60.57 13.17\nTable 4: Results of RoBERTa’s calibration performance under standard distribution shifts.\nOur results support the “scaling improves calibra-\ntion” conclusion in some cases. We observe that\nECE decreases when larger capacity brings sub-\nstantial improvement to PLMs’ performance (e.g.,\nT5 on SST-2 and MNLI). However, when the per-\nformance reaches a plateau value, increasing ca-\npacity only boosts PLMs’ confidence (e.g., T5 and\nRoBERTa on Yahoo). In this case, the ECE in-\ncreases when the PLM’s scale keeps increasing.\nPretraining. We choose the pre-trained\nRoBERTa-base and pre-trained T5-base (Pre-\ntrained), and compare them with several non-\npretrained models, including random initialized\nRoBERTa-base and T5-base (Random), BiL-\nSTM (LSTM) (Hochreiter and Schmidhuber,\n1997), Term Frequency Inverse Document Fre-\nquency (TF-IDF) (Luhn, 1957), and Bag-of-word\n(BoW) (Harris, 1954). We find that pretraining\nonly reduces ECE on relative simpler datasets,\nlike SST-2 and AG-News, but bring negligible\nbenefits on MNLI and Yahoo. This finding shares\nthe same ground with scaling experiments.\nC Construction of the Calibration\nTraining Dataset\nIn this paper, we consider the classification tasks.\nThe construction process can be extended to the\nnatural language generation tasks. We have an an-\nnotated dataset D = {(xi, yi)N\ni=1}for the standard\ntraining on the classification tasks. We typically\n1361\nModel ScaleDataset Amazon SST-5 SemEval\nT5-small\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 87.65 86.414.78 13.59 86.4365.1480.1515.23 19.86 80.1749.23 77.14 27.9122.89 77.17E-T5 (ours) 87.65 67.80 19.85 23.71 7.4965.1428.16 37.29 64.06 13.6349.23 30.45 19.40 50.65 12.12I-Vanilla 81.64 57.35 24.28 30.302.45 55.01 3.95 51.21 93.350.66 44.57 2.17 42.43 95.530.32I-Iter (ours) 87.54 68.20 19.33 22.89 5.6664.10 28.81 36.99 62.99 14.1648.52 32.0517.4947.86 13.13I-Simul (ours) 87.6668.61 19.05 22.63 6.3564.57 29.59 37.57 62.38 14.9550.3835.00 18.89 45.87 15.58\nT5-base\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 91.00 90.444.35 9.56 90.4169.73 85.1815.45 14.69 84.8755.03 78.39 23.3621.63 78.42E-T5 (ours) 91.00 71.03 19.97 22.40 4.6369.73 31.73 38.80 61.80 16.8355.03 29.72 26.28 56.23 12.54I-Vanilla 88.25 70.91 17.34 20.163.86 63.07 29.81 34.08 59.4211.4248.08 25.32 23.69 55.537.59I-Iter (ours) 91.6971.76 19.93 22.23 5.4368.23 33.46 36.87 59.79 18.9656.2335.2121.4250.98 17.48I-Simul (ours) 91.38 70.92 20.47 22.80 4.3070.2932.03 42.12 60.65 14.7254.75 26.18 30.70 59.34 8.67\nT5-large\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nE-MLP 91.58 91.954.70 8.04 91.8973.8583.5210.24 16.52 83.6156.65 78.2621.61 21.74 78.26E-T5 (ours) 91.58 70.10 21.48 23.70 2.6673.8529.96 47.35 64.65 14.7556.65 28.56 29.98 57.52 10.36I-Vanilla 88.88 69.42 19.46 22.121.81 71.79 28.30 46.83 65.1211.5549.00 24.66 25.95 56.306.37I-Iter (ours) 92.96 88.26 10.48 8.74 48.7172.45 70.35 30.29 25.22 58.7158.0884.26 35.21 12.77 80.14I-Simul (ours) 93.3474.45 19.39 20.62 5.4373.66 36.92 45.40 57.27 20.6656.87 40.04 28.43 44.23 19.29\nTable 5: Results of T5’s calibration performance with increasing model scales.\nID Dataset SST-2 Yahoo\nOOD Dataset SST-2 BookcorpusRandom Words Yahoo BookcorpusRandom Words\nMethod Conf EntropyConf EntropyConf EntropyConf EntropyConf EntropyConf Entropy\nUnlearnable\nVanilla 98.04 5.01 93.38 15.97 84.46 34.95 82.76 51.94 47.62 152.43 56.95 126.54\nTS 93.89 18.02 85.07 35.23 72.49 54.69 75.72 76.29 38.43 177.74 47.70 154.00\nLS 88.64 33.90 83.65 40.46 72.31 55.30 74.35 93.81 44.29 168.14 54.08 145.94\nEDA 98.27 4.33 93.73 15.45 83.00 37.15 83.68 46.75 50.59 141.92 69.03 92.58\nEnsemble 97.96 5.20 93.21 16.47 82.75 37.87 82.41 53.01 48.29 150.39 55.87 130.57\nLearnable\nE-MLP 88.62 35.37 86.94 38.69 85.04 42.17 74.93 - 61.80 - 67.57 -\nE-T5 (ours) 55.96 62.11 56.35 64.08 64.02 60.32 60.29 - 13.64 - 22.56 -\nI-Vanilla 56.31 62.13 57.72 63.99 66.47 59.90 60.51 - 13.71 - 22.78 -\nI-Iter (ours) 43.43 57.59 43.24 60.62 56.07 61.10 61.35 - 20.62 - 39.08 -\nI-Simul (ours) 63.24 10.50 65.74 2.25 77.68 0.01 60.52 - 6.44 - 14.67 -\nTable 6: Results on task-irrelevant inputs with T5. We don’t report the entropy results of learnable methods when\nYahoo is adopted as ID dataset since the class numbers are different in unlearnable (10 original classes in Yahoo)\nand learnable methods (2 classes), which will result in unfair comparison.\nfit a model Fon the training dataset by minimiz-\ning the pre-defined loss (e.g., cross-entropy loss).\nWe denote the original task as the main task. Then\nfor the newly introduced calibration task, we need\nto generate a calibration training dataset D∗ for\ntraining. To do so, we first train the model on the\nmain task using the training dataset, and employ\nthe trained model to give predictions on samples\nfrom the validation set. Then the calibration train-\ning dataset D∗ = {(xi, y∗\ni , ci)M\ni=1}can be gener-\nated from the validation set, where xi is the origi-\nnal sample in the validation set,y∗\ni is model’s orig-\ninal prediction, and ci is a binary value that indi-\ncates whether the original prediction is correct or\nnot. Specifically, we perform downsampling to en-\nsure a balanced label distribution.\nIn this paper, we adopt the same process to gen-\nerate the calibration training dataset. But differ-\nent methods may adopt specially designed training\nparadigms to utilize the calibration training data.\nWe described the training details in Sec. 5.1.\nD Additional Results of Calibration\nMethods\nFor exploring the effectiveness of existing calibra-\ntion methods, we provide results with RoBERTa\nin Table 4, Table 7, and Table 8 The results with\nthe model scaling effect are in Table 5.\nE Further Analysis of Distribution Shifts\nIn Sec. 5.3, we show that PLMs are less calibrated\nunder distribution shifts, consistent with previous\nwork (Desai and Durrett, 2020; Minderer et al.,\n2021). However, can we safely conclude that\ndistribution shifts degrade PLMs’ calibration per-\nformance? We study hard-to-easy distribution\nshifts (see Appendix F for the detailed setting) to\nfurther investigate the essence of this problem. In\nthis setting, models are trained on a difficult ID\ndataset and infer on easier OOD datasets. This\n1362\nDynasent\nDataset Dynasent Amazon DSC\nMethod Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg Acc Conf ECE CErrpos CErrneg\nUnlearnable\nVanilla 78.6194.56 17.103.56 88.0685.47 97.84 12.48 1.18 92.0887.93 97.23 9.30 1.74 89.70TS 78.6177.470.95 19.47 66.9685.47 86.612.54 11.24 74.1187.93 85.09 2.99 12.84 70.03LS 76.48 85.95 9.46 12.37 80.4785.8589.34 7.39 9.53 82.5387.1588.19 5.46 10.71 80.75EDA 76.97 95.65 18.74 2.92 90.8584.12 97.92 13.81 1.08 92.6485.53 97.13 11.62 1.64 89.87Ensemble77.67 94.85 17.22 3.44 88.8985.37 97.88 12.521.12 92.1186.69 97.11 10.431.76 89.77\nLearnable\nE-MLP 78.6171.06 19.59 28.81 70.5985.47 85.74 12.10 14.25 85.6987.93 79.3714.4620.61 79.25E-T5 (ours) 78.6164.94 23.76 26.7634.4385.47 85.53 13.23 9.4556.0387.93 81.72 14.91 13.5049.71I-Vanilla77.38 66.71 22.76 24.92 38.0683.85 85.80 12.18 7.99 53.5687.10 82.30 14.25 12.89 49.77I-Iter (ours) 77.89 64.17 21.98 28.43 38.0984.49 87.49 10.00 7.47 60.0687.05 82.83 12.14 12.86 53.81I-Simul (ours) 78.63 65.00 25.56 27.08 35.8483.65 79.79 15.36 13.28 44.3885.79 77.29 17.78 16.91 42.30\nTable 7: Results of RoBERTa’s calibration performance under hard-to-easy distribution shifts.\nID Dataset SST-2 Yahoo\nOOD Dataset SST-2 BookcorpusRandom Words Yahoo BookcorpusRandom Words\nMethod Conf EntropyConf EntropyConf EntropyConf EntropyConf EntropyConf Entropy\nUnlearnable\nVanilla 98.33 4.27 94.85 12.63 96.28 9.97 90.18 26.96 72.17 77.84 78.49 59.14\nTS 93.43 19.62 86.41 32.66 87.50 32.46 71.73 90.13 44.01 163.43 50.51 148.65\nLS 87.88 35.74 83.30 42.64 82.88 44.11 82.08 74.02 67.53 110.10 74.89 93.55\nEDA 98.43 3.67 95.54 10.79 91.55 20.06 94.24 15.08 83.30 44.77 86.10 35.91\nEnsemble 98.24 4.49 94.65 12.87 93.26 15.98 91.22 23.92 75.10 69.13 80.31 54.06\nLearnable\nE-MLP 94.48 15.99 80.75 36.41 63.81 59.36 74.15 - 41.87 - 42.31 -\nE-T5 (ours) 84.79 16.26 63.99 24.34 22.84 27.72 68.71 - 22.70 - 15.20 -\nI-Vanilla 84.83 16.33 65.34 25.09 23.08 28.39 69.55 - 24.84 - 17.78 -\nI-Iter (ours) 56.89 20.06 62.99 21.10 42.25 30.37 76.16 - 54.33 - 48.54 -\nI-Simul (ours) 75.24 9.44 46.51 13.88 8.11 5.44 64.66 - 19.70 - 19.47 -\nTable 8: Results on task-irrelevant inputs with RoBERTa. We don’t report the entropy results of learnable methods\nwhen Yahoo is adopted as ID dataset since the class numbers are different in unlearnable (10 original classes in\nYahoo) and learnable methods (2 classes), which will result in unfair comparison.\ncomes with relatively lower ID and higher OOD\nperformance. Specifically, we consider the senti-\nment analysis task and choose Dynasent (Amazon\nand DSC) as the ID (OOD) datasets. The details\nof the datasets are described in Appendix A.\nThe results of T5 and RoBERTa are shown in\nTable 3 and Table 7 respectively. We observe\ncompletely different results with Sec. 5.3. Across\nall methods, the ECE and CErr pos decrease under\nthe hard-to-easy distribution shifts, contradictory\nto the previous conclusion that PLMs are less cal-\nibrated on OOD samples. In hard-to-easy shifts,\nperformance and confidence both increase due to\nthe relative simpleness of the OOD samples. The\nindication is that PLMs’ relative calibration per-\nformance on ID and OOD samples relies on the\ndataset difficulty, and the conclusion that PLMs\nare less calibrated under distribution shifts is one-\nsided. This is consistent with our empirical study\nin Sec. 4 that emphasizes the influence of dataset\ndifficulty on PLMs calibration.\nTo further investigate the influence of dataset\ndifficulty on PLMs’ calibration performance, we\nevaluate the calibration on task-irrelevant in-\nputs (see Appendix F for the detailed setting) of\nPLMs trained on ID datasets with different diffi-\nculty (e.g., SST-2 and Yahoo). The task-irrelevant\ninputs include plain texts (e.g., bookcorpus) and\nrandom words. Since no golden labels are pro-\nvided, we measure the calibration performance\nthrough maximum confidence scores and predic-\ntive entropy.\nThe results of T5 are shown in Table 6, and\nRoBERTa are shown in Table 8. We show that\nPLMs have unreasonable high confidence in task-\nirrelevant inputs, especially when trained on SST-\n2. Comparing the results when trained on SST-2 or\nYahoo, we find that the ID training dataset has sig-\nnificant influence on PLMs calibration. Still, this\ncan be attributed to the dataset difficulty. We also\nobserve the superior performance of learnable cal-\nibration methods. They produce lower confidence\nscores on plain text and random tokens compared\nto unlearnable ones.\nIn summary, the influence of distribution shifts\non PLMs calibration is dependent on the evalu-\nation datasets chosen. The original conclusion\nthat calibration performance degrades on OOD\nsamples is based on two premises: (1) PLMs are\noverconfident in their wrong predictions, which\nis supported by our experiments; (2) The OOD\ndatasets are harder so PLMs cannot achieve good\n1363\nTask Dataset # Classes Avg.Len Train Dev Test\nSST-2 2 19.23 6920 1821 872\nSentiment\nAnalysis\nAmazon 3 77.86 24000 78741 91606\nSST-5 3 18.75 - - 1067\nSemEval 3 19.61 - - 6000\nNatural\nLanguage\nInference\nMNLI 3 19.36/10.06 373067 19635 9815\nHANS 2 9.15/5.61 - - 30000\nANLI 3 54.40/10.34 - - 3200\nTopic\nClassification\nYahoo 10 96.98 126000 14000 60000\nAG 4 38.5 10000 - 7600\nToxic\nDetection\nCivil 2 52.86 48000 12000 97320\nHate Speech 2 21.55 - - 478\nImplicit Hate 2 17.34 - - 21479\nPlain\nText\nBook Corpus - 13.39 - - 10000\nRandom Words - 20.28 - - 1000\nTable 9: Dataset Statistics.\nTask Dataset Template Verbalizer\nSST-2 It was{”mask”}. {”placeholder”: ”texta”} [bad, good]\nSentiment\nAnalysis\nAmazon It was{”mask”}. {”placeholder”: ”texta”} [bad, good, neutral]\nSST-5 It was{”mask”}. {”placeholder”: ”texta”} [bad, good, neutral]\nSemEval It was{”mask”}. {”placeholder”: ”texta”} [bad, good, neutral]\nMNLI\nGiven the two sentences:\n(1){”placeholder”: ”texta”}.\n(2){”placeholder”: ”textb”}.\nDoes the first sentence entails the second ?{”mask”}.\n[No, Yes, Maybe]\nNatural\nLanguage\nInference\nHANS\nGiven the two sentences:\n(1){”placeholder”: ”texta”}.\n(2){”placeholder”: ”textb”}.\nDoes the first sentence entails the second ?{”mask”}.\n[No, Yes, Maybe]\nANLI\nGiven the two sentences:\n(1){”placeholder”: ”texta”}.\n(2){”placeholder”: ”textb”}.\nDoes the first sentence entails the second ?{”mask”}.\n[No, Yes, Maybe]\nTopic\nClassification\nYahoo A{”mask”}\nquestion :{”placeholder”: ”texta”}{”placeholder”: ”textb”}\n[society, science,\nhealth, education,\ncomputers, sports,\nbusiness, entertainment,\nrelationships, politics]\nAG A{”mask”}news :{”placeholder”: ”texta”}\n{”placeholder”: ”textb”}\n[politics, sports,\nbusiness, technology]\nCivil It was{”mask”}. {”placeholder”: ”texta”} [benign, toxic]\nToxic\nDetection Hate Speech It was{”mask”}. {”placeholder”: ”texta”} [benign, toxic]\nImplicit HateIt was{”mask”}. {”placeholder”: ”texta”} [benign, toxic]\nTable 10: The manual templates and verbalizers adopted for each dataset.\nperformance. The second premise has not always\nbeen satisfied, and we show that the relative\ndataset difficulty significantly influences PLMs’\ncalibration performance on ID and OOD samples.\n1364\nTask Dataset Template Verbalizer\nSST-2 Sentence:{”placeholder”: ”texta”}The predicted sentiment is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nSentiment\nAnalysis\nAmazon Sentence:{”placeholder”: ”texta”}The predicted sentiment is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nSST-5 Sentence:{”placeholder”: ”texta”}The predicted sentiment is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nSemEval Sentence:{”placeholder”: ”texta”}The predicted sentiment is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nMNLI\nGiven the two sentences:{”placeholder”: ”texta”}\nThe predicted relationship between the two sentences is{”placeholder”: ”textb”}\nIs the prediction True or False ? It’s{”mask”}.\nNatural\nLanguage\nInference\nHANS\nGiven the two sentences:{”placeholder”: ”texta”}\nThe predicted relationship between the two sentences is{”placeholder”: ”textb”}\nIs the prediction True or False ? It’s{”mask”}.\n[False, True]\nANLI\nGiven the two sentences:{”placeholder”: ”texta”}\nThe predicted relationship between the two sentences is{”placeholder”: ”textb”}\nIs the prediction True or False ? It’s{”mask”}.\nTopic\nClassificationYahoo Sentence:{”placeholder”: ”texta”}The predicted topic is{”placeholder”: ”textb”}\nIs the prediction True or False ? It’s{”mask”}.\nCivil Sentence:{”placeholder”: ”texta”}The predicted toxicity is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nToxic\nDetection Hate SpeechSentence:{”placeholder”: ”texta”}The predicted toxicity is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nImplicite HateSentence:{”placeholder”: ”texta”}The predicted toxicity is{”placeholder”: ”textb”}.\nIs the prediction True or False ? It’s{”mask”}.\nTable 11: The manual templates and verbalizers of the calibration task for each dataset.\nF Details of Evaluation setting.\nHard-to-easy shift. we choose Dynasent as the\nin-distribution dataset, and choose Amazon and\nDSC as the out-of-distribution datasets. The eval-\nuation metrics are the same as the ones adopted in\nexperiments on standard OOD shifts. This eval-\nuation setting is expected to test the conclusion\nthat PLMs’ calibration performance degrades un-\nder distribution shifts.\nCalibration on task-irrelevant inputs We\nchoose SST-2 and Yahoo as the in-distribution\ndatasets, and choose Bookcorpus and a synthetic\ndataset as out-of-distribution datasets. Each\nsample in the synthetic dataset is constructed\nby composing random words. Well-calibrated\nPLMs should give very low confidence and high\nprobability entropy in the task-irrelevant inputs.\n1365\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe ﬁnal section.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\n4, 5\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1366\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1367"
}