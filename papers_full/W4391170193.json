{
  "title": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine",
  "url": "https://openalex.org/W4391170193",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5103229519",
      "name": "Thomas Savage",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5062747178",
      "name": "Ashwin Nayak",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5092645187",
      "name": "Robert Gallo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5039391391",
      "name": "Ekanath Rangan",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5046725885",
      "name": "Jonathan H. Chen",
      "affiliations": [
        "Stanford Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4384561181",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4385827730",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4384561103",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4367626167",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W6852476831",
    "https://openalex.org/W3088056511",
    "https://openalex.org/W4317214807",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4381480578"
  ],
  "abstract": null,
  "full_text": "BRIEF COMMUNICATION OPEN\nDiagnostic reasoning prompts reveal the potential for large\nlanguage model interpretability in medicine\nThomas Savage 1,2 ✉, Ashwin Nayak 1,2, Robert Gallo3,4, Ekanath Rangan 1 and Jonathan H. Chen 1,2,5,6\nOne of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to\nmake clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop\ndiagnostic reasoning prompts to study whether LLMs can imitate clinical reasoning while accurately forming a diagnosis. Weﬁnd\nthat GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacriﬁcing diagnostic accuracy.\nThis is signiﬁcant because an LLM that can imitate clinical reasoning to provide an interpretable rationale offers physicians a means\nto evaluate whether an LLMs response is likely correct and can be trusted for patient care. Prompting methods that use diagnostic\nreasoning have the potential to mitigate the“black box”limitations of LLMs, bringing them one step closer to safe and effective use\nin medicine.\nnpj Digital Medicine           (2024) 7:20 ; https://doi.org/10.1038/s41746-024-01010-1\nINTRODUCTION\nLarge language models (LLMs) are artiﬁcial intelligence systems\ntrained on large amounts of text data that learn complex language\npatterns and syntactical relationships to both interpret passages\nand generate text output\n1,2 LLMs have received widespread\nattention for their human-like performance on a wide variety of\ntext-generating tasks. Within medicine, initial efforts have\ndemonstrated that LLMs can write clinical notes3, pass standar-\ndized medical exams4, and draft responses to patient questions5,6.\nIn order to integrate LLMs more directly into clinical care, it is\nimperative to better understand their clinical reasoning\ncapabilities.\nClinical reasoning is a set of problem-solving processes\nspeciﬁcally designed for diagnosis and management of a patient’s\nmedical condition. Commonly used diagnostic techniques include\ndifferential diagnosis formation, intuitive reasoning, analytical\nreasoning, and Bayesian inference. Early assessments of the\nclinical reasoning abilities of LLMs have been limited, studying\nmodel responses to multiple-choice questions\n7–11. More recent\nwork has focused on free-response clinical questions and suggests\nthat newer LLMs, such as GPT-4, show promise in diagnosis of\nchallenging clinical cases12,13.\nPrompt engineering is emerging as a discipline in response to\nthe phenomena that LLMs can perform substantially differently\ndepending on how questions and prompts are posed to them14,15.\nAdvanced prompting techniques have demonstrated improved\nperformance on a range of tasks\n16, while also providing insight\ninto how LLMs came to a conclusion (as demonstrated by Wei et\nal. and Lightman et al. in arithmetic reasoning, common sense\nreasoning, and symbolic reasoning) 17,18. A notable example is\nChain-of-thought (CoT) prompting, which involves instructing the\nLLM to divide its task into smaller reasoning steps and then\ncomplete the task step-by-step17. Given that clinical reasoning\ntasks regularly use step-by-step processes, CoT prompts modiﬁed\nto re ﬂect the cognitive processes taught to and utilized by\nclinicians might elicit better understanding of LLM performance\non clinical reasoning tasks.\nIn this paper we evaluate the performance of GPT-3.5 and GPT-\n419 on open-ended clinical questions assessing diagnostic reason-\ning. Speciﬁcally, we evaluate LLM performance on a modi ﬁed\nMedQA USMLE (United States Medical Licensing Exam) dataset20,\nand further evaluate GPT-4 performance on the diagnostically\ndifﬁcult NEJM (New England Journal of Medicine) case series\n21.W e\ncompare traditional CoT prompting with several “diagnostic\nreasoning” prompts that are modeled after the cognitive\nprocesses of differential diagnosis formation, intuitive reasoning,\nanalytical reasoning, and Bayesian inference. This study assesses\nwhether LLMs can imitate clinical reasoning abilities using\nspecialized instructional prompts that combine clinical expertise\nand advanced prompting methods. We hypothesize GPT models\nwill have superior performance with diagnostic reasoning prompts\nin comparison to traditional CoT prompting.\nA modiﬁed version of the MedQA USMLE question dataset was\nused for this study. Questions were converted to free response by\nremoving the multiple-choice options after the question stem.\nOnly Step 2 and Step 3 USMLE questions were included, as Step 1\nquestions focus heavily on memorization of facts rather than\nclinical reasoning skills\n10. Only questions evaluating the task of\ndiagnosing a patient were included to simplify prompt engineer-\ning. A training set of 95 questions was used for iterative prompt\ndevelopment and a test set of 518 questions was reserved for\nevaluation. The full test set can be found in Supplementary Data 1.\nGPT-4 performance was also evaluated on the New England\nJournal of Medicine (NEJM) Case Records series. The NEJM Case\nRecords series is designed as an educational resource for\nphysicians, with each case providing a clinical case description\nfollowed by expert analysis of the case with a clinical diagnosis.\nWe included the 310 most recently published cases in this study.\nTen cases were excluded because they either did not provide a\ndeﬁnitive ﬁnal diagnosis or exceeded the maximum context\nlength of the GPT-4 API. A full list of all cases included (by title and\n1Department of Medicine, Stanford University, Stanford, CA, USA.2Division of Hospital Medicine, Stanford University, Stanford, CA, USA.3Palo Alto Veterans Affairs Medical\nCenter, Palo Alto, CA, USA. 4Department of Health Policy, Stanford University, Stanford, CA, USA. 5Stanford Center for Biomedical Informatics Research, Stanford\nUniversity, Stanford, CA, USA.6Clinical Excellence Research Center, Stanford University, Stanford, CA, USA.✉email: tsavage@stanford.edu\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nDOI number) can be found in Supplementary Data 2. For this\nevaluation, we compared traditional CoT prompting to the highest\nperforming clinical reasoning CoT prompt (differential diagnosis\nreasoning) on the modiﬁed MedQA dataset.\nOne traditional CoT prompt and four clinical reasoning prompts\nwere developed (differential diagnosis, analytical, Bayesian and\nintuitive reasoning). Each prompt included two example questions\n(Table 1) with rationales employing the target reasoning strategy.\nThis is a technique known as few-shot learning\n14. The full prompts\nused for the MedQA dataset are provided in Table 2; the full\nprompts used for the NEJM challenge set are provided in\nSupplementary Note 1.\nExample LLM responses for each prompting strategy can be\nfound in Fig.1 for GPT-3.5 and Fig.2 for GPT-4. Full results can be\nfound in Supplementary Data 1 and 2.\nRESULTS\nGPT-3.5 correctly answered 46% of questions using traditional CoT\nprompting, compared to 31% with zero-shot non-CoT prompting.\nAmong the clinical reasoning prompts, GPT-3.5 achieved the\nhighest performance with intuitive reasoning (48% vs. 46%,\ndifference of +1.7%, CI −2.5% to +5.9%, p = 0.4). Compared to\ntraditional CoT, GPT-3.5 ’s performance was signi ﬁcantly worse\nwith analytic reasoning (40%, difference of −6%, CI −11% to\n−1.5%, p = 0.001) and differential diagnosis formation (38%,\ndifference of −8.9%, CI −14% to −3.4%, p = <0.001), while\nBayesian inference performance nearly missed our threshold for\nstatistical signiﬁcance (42%, difference of −4.4%, CI −9.1% to\n+0.2%, p = 0.02). Results can be referenced in Table3. Inter-rater\nagreement for the MedQA GPT-3.5 evaluation was 97% with a\nCohen’s Kappa of 0.93.\nThe GPT-4 API generated an error for 20 questions of the test\nset, reducing the test set size to 498. Overall, GPT-4 demonstrated\nimproved accuracy over GPT-3.5. GPT-4 achieved an accuracy of\n76% with traditional CoT, 77% with intuitive reasoning (+0.8%, CI\n−3.6% to +5.2%, p = 0.73), 78% with differential diagnosis\n(+2.2%, CI−2.3% to+6.7%, p = 0.24), 78% with analytic reasoning\n(+1.6%, CI −2.4% to +5.6%, p = 0.35), and 72% with Bayesian\nInference (−3.4%, CI −\n9.1% to +1.2%, p = 0.07). Results can be\nfound in Table 4. Inter-rater agreement for the GPT-4 MedQA\nevaluation was 99% with a Cohen’s Kappa of 0.98.\nOn the NEJM challenge case set GPT-4 achieved an accuracy of\n38% with traditional CoT compared to 34% with differential\ndiagnosis CoT (difference of −4.2%, 95% CI −11.4% to +2.1%,\np = 0.09, Table 5). Inter-rater agreement for the GPT-4 NEJM\nevaluation was 97% with a Cohen’s Kappa of 0.93. GPT-4 response\nand rationale results for the entire NEJM test set are included in\nSupplementary Data 2.\nDISCUSSION\nIn this study we found that GPT-3.5 performance was similar with\ntraditional and intuitive reasoning CoT prompts, but signiﬁcantly\nworse with differential diagnosis and analytical CoT prompts.\nBayesian inference CoT also demonstrated worse performance\nthan traditional CoT, but the decrease in performance did not\nmeet our signiﬁcance threshold. Theseﬁndings suggest GPT-3.5 is\nnot able to imitate advanced clinical reasoning processes to arrive\nat an accurate diagnosis. In contrast, GPT-4 demonstrated similar\nperformance between traditional and diagnostic reasoning CoT\nprompts. While these ﬁndings highlight the signiﬁcant advance-\nment in reasoning abilities between GPT-3.5 and GPT-4, diagnostic\nreasoning does not increase GPT-4 accuracy like it would for a\nhuman provider. We propose three possible explanations for this\nﬁnding. First, GPT-4’s reasoning mechanisms could be inherently\ndifferent than human providers and therefore does not derive\nbeneﬁt from diagnostic reasoning strategies. Second, GPT-4 could\nbe explaining its diagnostic evaluation post-hoc in the desired\ndiagnostic reasoning format instead of strictly using the prompted\ndiagnostic reasoning strategy. Third, GPT-4 could have reached a\nmaximal accuracy with the vignette information provided and we\nare thus unable to detect an accuracy difference between\nprompting strategies. Regardless of the underlying reason, we\nobserve GPT-4 has developed the ability to successfully imitate\nclinical reasoning thought processes but cannot apply clinical\nreasoning like a human.\nThe ﬁnding that GPT-4 can successfully imitate the same\ncognitive processes as physicians to arrive accurately at an answer\nis still signiﬁcant because of the potential for interpretability. We\ndeﬁne interpretability as the property that allows a human\noperator to explore qualitative relationships between inputs and\noutputs\n22. A model that generates a clinical reasoning rationale\nwhen suggesting a diagnosis offers the clinician an interpretable\nmeans to assess whether the answer is true or false based on the\nrationale’s factual and logical accuracy. A workﬂow that aligns\nmodel outputs in this way (Fig.3) could mitigate the“black box”\nlimitations of LLMs, as long as physicians recognize that language\nmodels will always be at risk of unpredictable reasoning\nhallucinations, and that rationale logical and factual accuracy still\ndoes not absolutely guarantee answer correctness.\nTable 1. Example MedQA questions.\nExample Question 1\nShortly after undergoing a bipolar prosthesis for a displaced femoral neck fracture of the left hip acquired after a fall the day before, an 80-year-old\nwoman suddenly develops dyspnea. The surgery under general anesthesia with sevoﬂurane was uneventful, lasting 98 min, during which the patient\nmaintained oxygen saturation readings of 100% on 8 l of oxygen. She has a history of hypertension, osteoporosis, and osteoarthritis of her right knee.\nHer medications include ramipril, naproxen, ranitidine, and a multivitamin. She appears cyanotic, drowsy, and is oriented only to person. Her\ntemperature is 38.6 °C (101.5 °F), pulse is 135/min, respirations are 36/min, and blood pressure is 155/95 mm Hg. Pulse oximetry on room air shows an\noxygen saturation of 81%. There are several scattered petechiae on the anterior chest wall. Laboratory studies show a hemoglobin concentration of\n10.5 g/dl, a leukocyte count of 9000/mm\n3, a platelet count of 145,000/mm3, and a creatine kinase of 190 U/l. An ECG shows sinus tachycardia. What is\nthe most likely diagnosis?\nExample Question 2\nA 55-year-old man comes to the emergency department because of a dry cough and severe chest pain beginning that morning. Two months ago, he\nwas diagnosed with inferior wall myocardial infarction and was treated with stent implantation of the right coronary artery. He has a history of\nhypertension and hypercholesterolemia. His medications include aspirin, clopidogrel, atorvastatin, and enalapril. His temperature is 38.5Â°C (101.3 °F),\npulse is 92/min, respirations are 22/min, and blood pressure is 130/80 mm Hg. Cardiac examination shows a high-pitched scratching sound best\nheard while sitting upright and during expiration. The remainder of the examination shows no abnormalities. An ECG shows diffuse ST elevations.\nSerum studies show a troponin I of 0.005 ng/ml (N < 0.01). What is the most likely cause of this patient’s symptoms?\nExample questions used in all MEDQA prompts provided in Table2.\nT. Savage et al.\n2\nnpj Digital Medicine (2024)    20 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nTo demonstrate how clinical reasoning prompts provide\ninterpretability, we include descriptive MedQA examples (Supple-\nmentary Data 4). Incorrect model responses are often accom-\npanied by rationales that provide factual inaccuracy, while logical\nrationales are more often associated with correct responses. We\nfurther quantify this relationship by evaluating 100 GPT-4\ndiagnostic reasoning rationales, where we found incorrect\nanswers were much more likely to have logic errors in their\nTable 2. CoT and diagnostic reasoning prompts.\nTraditional CoT reasoning prompt\nTraditional chain-of-thought (CoT) Prompt:\nProvide a step-by-step deduction that identiﬁes the correct response\n{Example Question 1}\nExample Rationale 1:\nThe patient had a surgical repair of a displaced femoral neck fracture. The patient has petechiae. The patient has a new\noxygen requirement, meaning they are having difﬁculty with their breathing. This patient most likely has a fat embolism.\n{Example Question 2}\nExample Rationale 2:\nThis patient is having chest pain. They recently had a heart attack and has new chest pain, suggesting he may have a\nproblem with his heart. The EKG has diffuse ST elevations and he has a scratching murmur. This patient likely has Dressler\nSyndrome.\nClinical reasoning prompts\nDifferential diagnosis CoT Prompt:\nUse step by step deduction to create a differential diagnosis and then use step by step deduction to determine the correct\nresponse.\n{Example Question 1}\nExample Rationale 1:\nThis patient has shortness of breath after a long bone surgery. The differential for this patient is pulmonary embolism, fat\nembolism, myocardial infarction, blood loss, anaphylaxis, or a drug reaction. The patient has petechiae which makes fat\nembolism more likely. This patient most likely has a fat embolism.\n{Example Question 2}\nExample Rationale 2:\nThis patient has chest pain with diffuse ST elevations after a recent myocardial infarction. The differential for this patient\nincludes: myocardial infarction, pulmonary embolism, pericarditis, Dressler syndrome, aortic dissection, and\ncostochondritis. This patient likely has a high-pitched scratching sound on auscultation associated with pericarditis and\nDressler Syndrome. This patient has diffuse ST elevations associated with Dressler Syndrome. This patient most likely has\nDressler Syndrome.\nIntuitive reasoning CoT Prompt:\nUse symptom, signs, and laboratory disease associations to step by step deduce the correct response.\n{Example Question 1}\nExample Rationale 1:\nThis patient hasﬁndings of petechiae, altered mental status, shortness of breath, and recent surgery suggesting a\ndiagnosis of fat emboli. The patient most likely has a fat embolism.\n{Example Question 2} Example Rationale 2:\nThis patient had a recent myocardial infarction with new development of diffuse ST elevations, chest pain, and a high\npitched scratching murmur which are found in Dressler’s syndrome. This patient likely has Dressler’s Syndrome.\nAnalytic reasoning CoT Prompt:\nUse analytic reasoning to deduce the physiologic or biochemical pathophysiology of the patient and step by step identify the\ncorrect response.\n{Example Question 1}\nExample Rationale 1:The patient recently had large bone surgery making fat emboli a potential cause because the\nbone marrow was manipulated. Petechiae can form in response to capillary inﬂammation caused by fat emboli. Fat micro\nglobules cause CNS microcirculation occlusion causing confusion and altered mental status. Fat obstruction in the\npulmonary arteries can cause tachycardia and shortness of breath as seen in this patient. This patient most likely has a fat\nembolism.\n{Example Question 2}\nExample Rationale 2:\nThis patient had a recent myocardial infarction which can cause myocardial inﬂammation that causes pericarditis and\nDressler Syndrome. The diffuse ST elevations and high pitched scratching murmur can be signs of pericardial\ninﬂammation as the inﬂamed pericardium rubs against the pleura as seen with Dressler Syndrome. This patient likely has\nDressler Syndrome.\nBayesian reasoning CoT Prompt:\nUse step-by-step Bayesian Inference to create a prior probability that is updated with new information in the history to produce a\nposterior probability and determine theﬁnal diagnosis.\n{Example Question 1}\nExample Rationale 1:\nThe prior probability of fat embolism is 0.05% however the patient has petechiae on exam which is seen with fat emboli,\nwhich increases the posterior probability of fat embolism to 5%. Altered mental status increases the probability further\nto 10%. Recent orthopedic surgery increases the probability of fat emboli syndrome to 60%. This patient most likely has a\nfat embolism.\n{Example Question 2}\nExample Rationale 2:\nThe prior probability of Dressler Syndrome is 0.01%. The patient has diffuse ST elevations, increasing the probability of\nDressler Syndrome to 5%. The patient has a scratching murmur which increases the probability to 10%. In the setting of a\nrecent MI the posterior probability of myocardial infarction is 55%. This patient likely has Dressler Syndrome.\nThe traditional and diagnostic CoT prompts evaluated in this study. The provided rationales are in response to the example questions provided in Table 1.\nNote that the full text of the example questions has been omitted for readability.\nT. Savage et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2024)    20 \nrationale compared to correct answers. In total, 65% of incorrect\nanswers had false logic statements in their rationale, with an\naverage of 0.82 inaccuracies per rationale. In contrast, only 18% of\ncorrect answers had false logic statements in their rationale, with\nan average of 0.11 per question (Supplementary Data 5). Our\nresults suggest clinical reasoning rationales provide valuable\ninsight (but not an absolute guarantee) into whether an LLM\nresponse can be trusted and represent a step toward LLM\ninterpretability.\nThe strengths of our investigation are a prompt design that\nleverages chain-of-thought prompting for insight into LLM clinical\nreasoning capabilities as well as the use of free response clinical\ncase questions where previous studies have been limited to\nmultiple-choice or simple open-ended fact retrieval that do not\nchallenge LLM clinical reasoning abilities. We designed our\nevaluation with free response questions both from the USMLE\nas well as NEJM case report series to facilitate rigorous comparison\nbetween prompting strategies.\nA limitation of our study is that while our prompt engineering\nprocess surveyed a wide range of prompt styles we could not test\nall possible diagnostic reasoning CoT prompts. Furthermore our\ninvestigation was limited to only GPT-3.5 and GPT-4, US-centric\nquestion sets, and the English language, therefore we cannot\ngeneralize our ﬁndings to other available models, especially ones\nﬁne-tuned on texts demonstrating clinical reasoning, nor to non-\nEnglish languages and non-US-centric question sets. We hope that\nfuture studies can iterate on our diagnostic reasoning prompts\nand use our open dataset as a benchmark for additional\nevaluation.\nMETHODS\nLLM prompt development\nWe used an iterative process known as prompt engineering to\ndevelop our diagnostic reasoning prompts. During this process,\nwe experimented with several different types of prompts\n(Supplementary Note 2). In each round of prompt engineering,\nwe evaluated GPT-3.5 accuracy on the MEDQA training set\n(Supplementary Data 3). We found prompts that encouraged\nstep-by-step reasoning without specifying what the steps should\nbe, yielded better performance. We also found that prompts that\nfocused on a single diagnostic reasoning strategy provided better\nresults than prompts that combined multiple strategies.\nLLM response evaluation\nLanguage model responses were evaluated by physician authors\nAN, ER, RG and TS, three internal medicine attending physicians\nand one internal medicine resident. Each question was evaluated\nby two blinded physicians. If there was disagreement in the grade\nassigned, a third evaluator determined the ﬁnal grade. Any\nresponse that was felt to be equally correct and speci ﬁc, as\ncompared to the provided answer, was marked as correct.\nPhysicians used UpToDate\n23, MKSAPP24, and StatPearls25 to verify\naccuracy of answers when needed.\nLLM programming and computing resources\nFor this evaluation we used the OpenAI Davinci-003 model via an\nOpenAI API to provide GPT-3.5 responses and GPT-4 model via an\nOpenAI API to provide GPT-4 responses. Prompting of the GPT-3.5\nmodel was performed with the Demonstrate-Search-Predict (DSP)\nQuestion : A 30-year-old African American woman comes to the physician for the evaluation of a dry cough and chest discomfort \nfor the past 3 days. During this period, the patient has had headaches, muscle aches, joint pain, fever, and chills. Ten days ago, \nshe was hiking with her family in Mississippi. The patient has asthma that is treated with an albuterol inhaler. Her mother has a \nlung disease treated with methotrexate. The patient has smoked one pack of cigarettes daily for the past 10 years. Her \ntemperature is 38Â°C (100.4Â°F). Physical examination shows slight wheezes throughout both lung fields. Laboratory studies and \nurinalysis are positive for polysaccharide antigen. Bronchoalveolar lavage using silver/PAS-staining shows macrophages filled \nwith a dimorphic fungus with septate hyphae. What is the most likely cause of this patient’s symptoms?\n \n \nAnswer: Histoplasmosis \n \nTraditional Chain-of-Thought (GPT-3.5) \n‘The patient has a dry cough, chest discomfort, and other symptoms such as fever, chills, headaches, and muscle aches. She has \na positive polysaccharide antigen and bronchoalveolar lavage shows a dimorphic fungus with septate hyphae. This patient likely \nhas Valley Fever.’ \n \nGPT-3.5 Differential Diagnosis CoT (GPT-3.5) \n‘The differential diagnosis for this patient includes: pneumonia, tuberculosis, eosinophilic pneumonia, hypersensitivity pneumonitis, \nand coccidioidomycosis. The patient’s recent history of hiking in Mississippi and the presence of a dimorphic fungus with septate \nhyphae on bronchoalveolar lavage make coccidioidomycosis the most likely cause of her symptoms.’ \n \nGPT-3.5 Intuitive Reasoning CoT (GPT-3.5) \n‘The patient has a history of asthma, which increases the likelihood of asthma exacerbation. The positive result for polysaccharide \nantigen suggests atypical pneumonia. The presence of a dimorphic fungus with septate hyphae suggests the presence of fungal \npneumonia. This patient most likely has fungal pneumonia.’ \n \nAnalytic Reasoning CoT (GPT-3.5) \n‘The patient’s recent history of hiking in Mississippi and the positive polysaccharide antigen and silver/PAS-staining results \nstrongly suggest fungal pneumonia as the cause of her symptoms.’ \n \nBayesian Inference CoT (GPT-3.5) \n‘The prior probability of blastomycosis is 1%, however the patient’s recent travel increases the probability to 5%. The laboratory \nand urinalysis results increase the posterior probability to 30%. The presence of a dimorphic fungus on bronchoalveolar lavage \nincreases the probability to 60%. This patient is likely suffering from blastomycosis.’ \nFig. 1 GPT 3.5 CoT and diagnostic reasoning rationale examples.Example GPT-3.5 rationales responding to a traditional CoT prompt as well\nas diagnostic reasoning prompts. LLM response and rationale results for the entire test set can be found in Supplementary Information 1.\nT. Savage et al.\n4\nnpj Digital Medicine (2024)    20 Published in partnership with Seoul National University Bundang Hospital\nPython module26,27. Self-consistency was applied to all GPT-3.5\nChain-of-Thought prompts28. GPT-4 responses did not use DSP or\nself-consistency because those features were not available for\nGPT-4 at the time of submission. Computing was performed in a\nGoogle CoLab Jupyter Notebook. Full code can be found in\nSupplementary Note 3.\nStatistical evaluation\nStatistical signiﬁcance and conﬁdence intervals were calculated\nagainst traditional CoT using McNemar’s test for paired propor-\ntions, two-tailed. Statistical signiﬁcance was set at an alpha of\n0.0125 to reﬂect multiple hypotheses (four prompts per each\nmodel) by the Bonferroni Correction. Inter-rater agreement was\nassessed using Cohen’s Kappa Statistic. Statistical analysis was\nperformed in R with the epibasix library.\nClinical reasoning rationale logic evaluation\nThe ﬁrst 100 GPT-4 differential diagnosis rationales were evaluated\nfor appropriate logic and medical accuracy. The rationales were\nevaluated by physician authors RG and TS, who are both internal\nmedicine attending physicians.\nFig. 2 GPT 4 CoT and diagnostic reasoning rationale examples.Example GPT-4 rationales responding to the question posed in Fig.1. LLM\nresponse and rationale results for the entire test set can be found in Supplementary Information 1.\nT. Savage et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2024)    20 \nThe reviewers attempted to identify instances of inaccuracy or\nfalse logic in each diagnostic reasoning rationale, blinded to the\nindex question, gold standard answer, or grade of the LLM\nresponse. Reviewers were blinded to the index question to\nsimulate a clinical situation where a physician is evaluating an LLM\ncase interpretation without examining the patient themselves.\nArguments with false logic or inaccuracies were tallied and a\ncomparison was made between rationales supporting correct\nversus incorrect answers. Complete data can be found in\nSupplementary Data 5.\nDATA AVAILABILITY\nAll data used in this manuscript are provided in our Supplementary Information and\nopen accessﬁgshare (10.6084/m9.ﬁgshare.24886593). This includes all prompts, LLM\nresponses and reviewer grades.\nCODE AVAILABILITY\nAll code used in this manuscript is provided in our Supplementary Information and\nopen access ﬁgshare (10.6084/m9.ﬁgshare.24886593).\nReceived: 14 August 2023; Accepted: 10 January 2024;\nREFERENCES\n1. Thirunavukarasu, A. J. et al. Large language models in medicine.Nat. Med. 29,\n1–11 (2023).\n2. Lee, P., Bubeck, S. & Petro, J. Beneﬁts, limits, and risks of GPT-4 as an AI chatbot\nfor medicine. N. Engl. J. Med.388, 2399–2400 (2023).\n3. Nayak, A. et al. Comparison of history of present illness summaries generated by\na chatbot and senior internal medicine residents.JAMA Intern. Med.183, e232561\n(2023).\n4. Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-assisted medical\neducation using large language models.PLoS Digit. Health2, e0000198 (2023).\n5. Ayers, J. W. et al. Comparing physician and arti ﬁcial intelligence chatbot\nresponses to patient questions posted to a public social media forum. JAMA\nIntern. Med. 183, 589–596 (2023).\n6. Sarraju, A. et al. Appropriateness of cardiovascular disease prevention recom-\nmendations obtained from a popular online chat-based arti ﬁcial intelligence\nmodel. JAMA 329, 842–844 (2023).\n7. Singhal, K. et al. Large language models encode clinical knowledge.Nature 620,\n172–180 (2023).\n8. Singhal, K. et al. Towards expert-level medical question answering with large\nlanguage models. Preprint athttps://doi.org/10.48550/arXiv.2305.09617 (2023).\nTable 3. GPT 3.5 MEDQA performance with diagnostic reasoning prompts compared to traditional CoT.\nPrompt Correct responses (%) Difference in percentage (con ﬁdence interval) p valuea\nChain of thought 46% ––\nIntuitive reasoning 48% 1.7% ( −2.5%, 5.9%) 0.4\nAnalytic reasoning 40% −6.0% (−11%, −1.5%) 0.001\nDifferential diagnosis 38% −8.9% (−14%, −3.4%) <0.001\nBayesian inference 42% −4.4% (−9.1%, 0.2%) 0.02\nGPT-3.5 performance on a free-response MEDQA question set with both traditional chain-of-thought model prompting strategies as well as clinical reasoning\nprompts of intuitive reasoning, analytic reasoning, differential diagnosis and Bayesian inference.\naPercentage difference andp value statistics compared to traditional chain-of-thought.\nTable 4. GPT 4 MEDQA performance with diagnostic reasoning prompts compared to traditional CoT.\nPrompt Correct responses (%) Difference in percentage (con ﬁdence interval) p valuea\nChain of thought 76% ––\nIntuitive reasoning 77% 0.8% ( −3.6%, 5.2%) 0.73\nAnalytic reasoning 78% 1.6% ( −2.4%, 5.6%) 0.35\nDifferential diagnosis 78% 2.2% ( −2.3%, 6.7%) 0.24\nBayesian inference 72% −3.4% (−9.1%, 1.2%) 0.07\nGPT-4 performance on a free-response MEDQA question set with both traditional chain-of-thought model prompting strategies as well as clinical reasoning\nprompts of intuitive reasoning, analytic reasoning, differential diagnosis and Bayesian inference.\naPercentage difference andp value statistics compared to traditional chain-of-thought.\nTable 5. GPT 4 challenge set performance with differential diagnosis\nreasoning prompts compared to traditional CoT.\nPrompt Correct\nresponses (%)\nDifference in percentage\n(conﬁdence interval)\np value\nChain of thought 38% ––\nDifferential\ndiagnosis\n34% −4.2% (−11.4%, +2.1%) 0.09\nGPT-4 performance on the NEJM challenge question set with both\ntraditional chain-of-thought and differential diagnosis reasoning\nprompting.\nFig. 3 Proposed LLM workﬂow. aCurrent LLM workﬂow. b Proposed\nLLM workﬂow.\nT. Savage et al.\n6\nnpj Digital Medicine (2024)    20 Published in partnership with Seoul National University Bundang Hospital\n9. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on\nmedical challenge problems. Preprint athttps://doi.org/10.48550/arXiv.2303.13375\n(2023).\n10. Ali, R. et al. Performance of ChatGPT and GPT-4 on neurosurgery written board\nexaminations. Neurosurgery. 93, 1353–1365 (2023).\n11. Ali, R. et al. Performance of ChatGPT, GPT-4, and Google Bard on a neurosurgery\noral boards preparation question bank.Neurosurgery. 93, 1090–1098 (2023).\n12. Strong, E. et al. Chatbot vs medical student performance on free-response clinical\nreasoning examinations. JAMA Intern. Med.183, 1028–1030 (2023).\n13. Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a generative artiﬁcial intelligence\nmodel in a complex diagnostic challenge.JAMA 330,7 8–80 (2023).\n14. Brown, T. B. et al. Language models are few-shot learners. InProc. of the 34th\nInternational Conference on Neural Information Processing Systems (NIPS'20). 159,\n1877–1901 (Curran Associates Inc., Red Hook, NY, USA).\n15. Peng, B., Li, C., He, P., Galley, M. & Gao, J. Instruction tuning with GPT-4. Preprint at\nhttp://arxiv.org/abs/2304.03277 (2023).\n16. Wang, J. et al. Prompt engineering for healthcare: methodologies and applica-\ntions. Preprint athttps://doi.org/10.48550/arXiv.2304.14670 (2023).\n17. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. Preprint athttp://arxiv.org/abs/2201.11903 (2023).\n18. Lightman, H. et al. Let’s verify step by step. Preprint at http://arxiv.org/abs/\n2305.20050 (2023).\n19. OpenAI. 2023. OpenAI GPT-3.5 API [text-davinci-003] and GPT-4 API. Available at:\nhttps://platform.openai.com.\n20. Jin, D. et al. What disease does this patient have? A large-scale open domain\nquestion answering dataset from medical exams. Preprint at https://doi.org/\n10.48550/arXiv.2009.13081 (2020).\n21. Case records of the Massachusetts General Hospital articles. 2020-2023.N. Engl. J.\nMed. Accessed: May 2023. https://www.nejm.org/medical-articles/case-records-\nof-the-massachusetts-general-hospital.\n22. Joyce, D. W., Kormilitzin, A., Smith, K. A. & Cipriani, A. Explainable artiﬁcial intel-\nligence for mental health through transparency and interpretability for under-\nstandability. Npj Digital Med.6,1 –7 (2023).\n23. UpToDate: industry-leading clinical decision support. Wolters Kluwer. Accessed:\nJune 2023. https://www.wolterskluwer.com/en/solutions/uptodate.\n24. MKSAP 19.ACP Online. Accessed: June 2023.https://www.acponline.org/featured-\nproducts/mksap-19 (2023).\n25. StatPearls. NCBI Bookshelf. Accessed: June 2023.https://www.statpearls.com/.\n26. DSP: The Demonstrate\n–Search–Predict Framework. Accessed: March 2023.\nGitHub - stanfordnlp/dspy at v1.https://github.com/stanfordnlp/dspy/tree/v1.\n27. Khattab, O. et al. Demonstrate-Search-Predict: composing retrieval and language\nmodels for knowledge-intensive NLP. Preprint athttp://arxiv.org/abs/2212.14024\n(2023).\n28. Wang, X. et al. Self-consistency improves chain of thought reasoning in language\nmodels. Preprint athttps://doi.org/10.48550/arXiv.2203.11171 (2023).\nACKNOWLEDGEMENTS\nJ.H.C. has received research funding support in part by the NIH/National Institute of\nAllergy and Infectious Diseases (1R01AI17812101), NIH/National Institute on Drug\nAbuse Clinical Trials Network (UG1DA015815— CTN-0136), Gordon and Betty Moore\nFoundation (Grant #12409), Stanford Artiﬁcial Intelligence in Medicine and Imaging—\nHuman-Centered Artiﬁcial Intelligence (AIMI-HAI) Partnership Grant, Google Inc.\nResearch collaboration, American Heart Association— Strategically Focused Research\nNetwork— Diversity in Clinical Trials, and the NIH-NCATS-CTSA grant (UL1TR003142)\nfor common research resources. R.G. is supported by a VA Advanced Fellowship in\nMedical Informatics. The views expressed are those of the authors and not necessarily\nthose of the Department of Veterans Affairs or those of the United States\nGovernment.\nAUTHOR CONTRIBUTIONS\nT.S.: manuscript writing, LLM response review, conceptual design. A.N.: manuscript\nwriting, LLM response review. R.G.: manuscript writing, LLM response review. E.R.:\nmanuscript writing, LLM response review. J.H.C.: manuscript writing, conceptual\ndesign.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-024-01010-1.\nCorrespondence and requests for materials should be addressed to Thomas Savage.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2024\nT. Savage et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2024)    20 ",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.7806304693222046
    },
    {
      "name": "Perception",
      "score": 0.5066138505935669
    },
    {
      "name": "Cognition",
      "score": 0.48047202825546265
    },
    {
      "name": "Personalized medicine",
      "score": 0.42217516899108887
    },
    {
      "name": "Psychology",
      "score": 0.39806294441223145
    },
    {
      "name": "Medicine",
      "score": 0.3568421006202698
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35616931319236755
    },
    {
      "name": "Computer science",
      "score": 0.29729560017585754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.289237380027771
    },
    {
      "name": "Psychiatry",
      "score": 0.18546569347381592
    },
    {
      "name": "Bioinformatics",
      "score": 0.12843209505081177
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210137306",
      "name": "Stanford Medicine",
      "country": "US"
    }
  ]
}