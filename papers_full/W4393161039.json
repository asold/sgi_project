{
  "title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
  "url": "https://openalex.org/W4393161039",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5015801419",
      "name": "Liangtai Sun",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2101669180",
      "name": "Yang Han",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2111045991",
      "name": "Zihan Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1964209566",
      "name": "Da Ma",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5102635259",
      "name": "Zhennan Shen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2485081031",
      "name": "BaoCai Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098210174",
      "name": "Lu Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108696738",
      "name": "Kai Yu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5015801419",
      "name": "Liangtai Sun",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2101669180",
      "name": "Yang Han",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2111045991",
      "name": "Zihan Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1964209566",
      "name": "Da Ma",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5102635259",
      "name": "Zhennan Shen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2485081031",
      "name": "BaoCai Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2098210174",
      "name": "Lu Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108696738",
      "name": "Kai Yu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4311991320",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W6839035525",
    "https://openalex.org/W3088056511",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W6683908837",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6849941170",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4376653782",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4385002382",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W2162821268",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4378942305",
    "https://openalex.org/W4387874094",
    "https://openalex.org/W4381252028"
  ],
  "abstract": "Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a \"dynamic\" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achieves SOTA performance compared to other LLMs, there is still substantial room for improvement, especially for dynamic questions. The codes and data are publicly available on https://github.com/OpenDFM/SciEval.",
  "full_text": "SciEval: A Multi-Level Large Language Model\nEvaluation Benchmark for Scientific Research\nLiangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen*, Kai Yu*\nX-LANCE Lab, Department of Computer Science and Engineering\nMoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\nShanghai Jiao Tong University, Shanghai, China\n{slt19990817, csyanghan, zhao mengxin, mada123}@sjtu.edu.cn\n{ieee-szn, 15368493547, chenlusz, kai.yu}@sjtu.edu.cn\nAbstract\nRecently, there has been growing interest in using Large\nLanguage Models (LLMs) for scientific research. Numerous\nbenchmarks have been proposed to evaluate the ability of\nLLMs for scientific research. However, current benchmarks\nare mostly based on pre-collected objective questions. This\ndesign suffers from data leakage problem and lacks the eval-\nuation of subjective Q/A ability. In this paper, we propose\nSciEval, a comprehensive and multi-disciplinary evaluation\nbenchmark to address these issues. Based on Bloom’s taxon-\nomy, SciEval covers four dimensions to systematically eval-\nuate scientific research ability. In particular, we design a “dy-\nnamic” subset based on scientific principles to prevent eval-\nuation from potential data leakage. Both objective and sub-\njective questions are included in SciEval. These characteris-\ntics make SciEval a more effective benchmark for scientific\nresearch ability evaluation of LLMs. Comprehensive exper-\niments on most advanced LLMs show that, although GPT-4\nachieves SOTA performance compared to other LLMs, there\nis still substantial room for improvement, especially for dy-\nnamic questions. The codes and data are publicly available\non https://github.com/OpenDFM/SciEval.\n1 Introduction\nLarge Language Models (LLMs), such as ChatGPT (Schul-\nman et al. 2022), have attracted widespread attention in\ngeneral scenarios, including information search, code gen-\neration, and more. In the field of science, LLMs have\nalso shown preliminary potential in improving scientific\nresearch efficiency and transforming scientific research\nparadigms (Blanco-Gonzalez et al. 2023; W ANG and MIAO\n2023). In the meanwhile, several scientific LLMs have been\nproposed by researchers (Taylor et al. 2022; Luo et al. 2022;\nFrey et al. 2022). In the general field, there are already\nnumerous evaluation benchmarks to evaluate the language\nunderstanding, language generation and reasoning capabil-\nities of LLMs, such as MMLU (Hendrycks et al. 2020),\nAGIEval (Zhong et al. 2023), and C-EV AL (Huang et al.\n2023), shown in Table 1. Although these benchmarks cover\ndata of science domain, the data sources are usually con-\nfined to educational materials, which can not adequately as-\n*The corresponding authors are Lu Chen and Kai Yu.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nsess the research ability of LLMs and not align with real-life\nscientific research scenarios. In addition, some benchmarks\nhave been proposed to evaluate the scientific capability of\nLLMs, such as MultiMedQA (Singhal et al. 2023), Chem-\nLLMBench (Guo et al. 2023), and MATH (Hendrycks et al.\n2021), while these benchmarks are restricted to a specific\nscientific discipline, leaving a lack of a more general scien-\ntific evaluation benchmark.1 In addition, these benchmarks\n(1) lack evaluation systems for scientific capabilities, (2) are\nall based on objective questions, which are insufficient to as-\nsess scientific abilities, and (3) face the risk of data leakage.\nIn response to this gap, we present SciEval, an English\nbenchmark designed to evaluate advanced abilities of LLMs\nin the scientific domain. SciEval consists of a total of about\n18000 challenging scientific questions, spanning three im-\nportant basic science fields: chemistry, physics and biology,\neach of which is further divided into multiple sub-topics.\nSciEval mainly has the following three characteristics:\n• Multi-level and comprehensive evaluation of the abil-\nity of LLMs in the scientific field. Scientific abil-\nity of LLMs needs to be evaluated from multiple as-\npects. Leveraging cognitive domains of Bloom’s taxon-\nomy (Krathwohl 2002; Forehand 2010), which covers\nsix levels, SciEval evaluates the scientific capabilities\nof large language models across four dimensions: ba-\nsic knowledge, knowledge application, scientific calcu-\nlation, and research ability, where each capability aligns\nwith one or more cognitive levels.\n• Combination of objective and subjective questions.\nSciEval is mainly based on objective questions, which al-\nlow for quick and standard model evaluations, involving\nmultiple-choice, fill-in-the-blank, and judgment ques-\ntions. These questions can help us understand whether\nthe model can correctly understand and memorize sci-\nentific knowledge. However, objective questions are in-\nsufficient to assess scientific capability holistically. To\nbetter assess scientific reasoning and application ability,\nSciEval introduces a small number of subjective ques-\ntions, involving a total of twelve basic science experi-\nments, which is named Experimental Data.\n1Due to the page limitation, we only compare some widely used\nbenchmarks. For more information, we refer to (Chang et al. 2023).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19053\nName Category Ability Source Data Type Dynamic #Data\nMMLU humanities, social\nscience, STEM, other BK, KA, SC exam, book, course objective % 14079\nAGIEval social science, STEM BK, KA, SC exam objective % 8062\nC-EV AL humanities, social\nscience, STEM, other BK, KA, SC exam objective % 12342\nMultiMedQA medical BK, KA, RA exam, research objective % 13115\nChemLLMBench chemistry BK,KA knowledge base objective % 800\nMATH mathematics SC exam objective % 5000\nSciEval science BK, KA,SC, RA community QA,\nknowledge base\nobjective +\nsubjective \" 15901\nTable 1: Dataset comparison of SciEval and some other datasets covering science domain.“BK” stands for Basic Knowledge,\n“KA” stands for Knowledge Application, “SC” stands for Scientific Calculation, and “RA” stands for Research Ability.\n• Dynamic data generation based on basic scientific\nprinciples. The huge amount of training data used for\npre-training LLMs may cause the risk of data leakage for\nevaluation. In order to solve this problem, one of the main\nfeatures of SciEval is the use of Dynamic Data, which\ncan prevent potential data leakage and ensure the fairness\nand credibility of the evaluation results. The Dynamic\nData will be updated regularly, and we will maintain a\nstable version to make a fair comparison of model perfor-\nmance. And the objective questions other than Dynamic\nData are referred to as Static Data.\nWe conduct experiments to evaluate LLMs on SciEval\nin answer-only, chain-of-thought and few-shot settings. Re-\nsults indicate that GPT-4 is the strongest model, with only\nGPT-4, GPT-3.5-turbo and Claude-v1.3 surpassing 60% av-\nerage accuracy on Static Data, signifying considerable op-\nportunities for improvement. With the results of Dynamic\nData, we find that these LLMs have little knowledge about\nmolecules, and most models could only retain near-random\naccuracy in the physics subset. As for Experimental Data,\nsome top-tier models could perform satisfactorily in experi-\nmental principle and design, while almost all models strug-\ngle to analyze the experimental results. With the analysis of\nexperiment results, we claim that training on large-scale sci-\nentific corpus is helpful for the scientific ability of LLMs,\nand most LLMs perform bad on calculation problems, espe-\ncially in physics domain. We hope SciEval can provide an\nexcellent benchmark for the assessment of scientific capa-\nbility of LLMs, and promote wide application in science.\n2 Related Work\n2.1 General Benchmarks for LLMs\nTo evaluate the performance of LLMs across dif-\nferent tasks, several benchmarks have been proposed.\nMMLU (Hendrycks et al. 2020) aims to develop a compre-\nhensive test for evaluating text models in multi-task con-\ntexts. HELM (Liang et al. 2022) offers a comprehensive\nassessment, evaluating LLMs across various aspects, such\nas language understanding and common-sense reasoning.\nBig-Bench (Srivastava et al. 2022) introduces 204 chal-\nlenging tasks covering various domains, aiming to evaluate\ntasks beyond the capabilities of existing language models.\nAGIEval (Zhong et al. 2023) serves as an evaluation frame-\nwork for assessing the performance of foundation models\nin human-centric standardized exams. C-Eval (Huang et al.\n2023) assesses the advanced knowledge and reasoning capa-\nbilities of foundation models in Chinese.\n2.2 Specific Benchmarks for LLMs\nApart from general tasks, specific benchmarks are designed\nfor certain downstream tasks. MultiMedQA (Singhal et al.\n2023) focuses on medical question-answering, evaluating\nLLMs in terms of clinical knowledge and QA abilities.\nMATH (Hendrycks et al. 2021) assesses reasoning and\nproblem-solving proficiencies of LLMs in mathematics. Sci-\nenceQA (Lu et al. 2022) proposes a multi-modal benchmark\nwith a diverse set of science topics and annotations of their\nanswers with corresponding lectures and explanations, col-\nlected from elementary and high school science curricula.\nSCIBENCH (Wang et al. 2023) examines the reasoning ca-\npabilities required for complex scientific problem-solving\nand proposes two datasets of college-level scientific prob-\nlems. Compared to these benchmarks, SciEval (1) evalu-\nates scientific capabilities from multiple aspects, having a\nbroader coverage, (2) uses data of community Q&A, which\nis more flexible and diverse, (3) designs a subset of dynamic\ndata, making an effort to mitigate data leakage.\n3 The SciEval Dataset\n3.1 Scientific Research Evaluation System\nScientific research requires different dimensions of knowl-\nedge, such as understanding and calculation, thence evalu-\nation of scientific ability should be conducted at multiple\nlevels. Bloom’s taxonomy is a set of three hierarchical meth-\nods used for classification of educational learning objectives\ncovering cognitive, affective and psychomotor domains. The\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19054\nSciEval \nFigure 1: The illustration of the evaluation system. SciEval covers three disciplines with amounts of sub-topics, and investigates\nfour abilities, corresponding to six cognitive levels.\ncognitive domain is frequently used to structure curriculum\nlearning objectives, assessments and activities, and is bro-\nken into six levels: Remember, Understand, Apply, Analyze,\nEvaluate and Create, as is shown in Figure 1, which are suit-\nable for the evaluation of scientific capability.\nBased on the cognitive domain of Bloom’s taxonomy, the\nevaluation system of SciEval consists of four knowledge\ndimensions: Basic Knowledge (BK), Knowledge Applica-\ntion (KA), Scientific Calculation (SC), and Research Ability\n(RA). As is shown in Figure 1, BK primarily assesses the\nfundamental scientific knowledge of LLMs. KA focuses on\nhow to apply basic knowledge to solve scientific problems,\nrequiring models to have comprehension, application, and\nanalysis abilities. SC is a specialized application of knowl-\nedge that further examines complex reasoning capabilities\nof LLMs based on their general knowledge application abil-\nities. RA assesses evaluation capabilities at a higher cogni-\ntive level, requiring models to participate in various aspects\nof scientific research, including problem formulation, exper-\nimental design, data analysis, and summarization.\nBased on the evaluation system, we design three different\ntypes of data: Static Data, Dynamic Data, and Experimen-\ntal Data. The Static Data covers all these four knowledge\ndimensions and will remain constant throughout, while the\nDynamic Data examines from the aspects of Knowledge Ap-\nplication and Scientific Calculation and will be regularly up-\ndated to prevent any data leakage. The Experimental Data\ncomprises a set of questions for twelve scientific experi-\nments and can be used to evaluate the Research Ability.\n3.2 Data Collection\nStatic Data The collection steps of Static Data are shown\nin Figure 2. The primary source of Static Data is Socratic\nQ&A2, a community-driven website that covers a wide\nrange of subjects such as science and literature. Specifically,\nwe collect data from the fields of biology, chemistry, and\nphysics. To ensure quality, we employ rule-based methods\n2https://socratic.org\nto preprocess the crawled data. While gathering the ques-\ntions, we found that not all of them are suitable as titles. To\naddress this, we utilize GPT-4 with the “Task 1” prompt, as\ndepicted in Figure 2, to process these questions. Since most\nof the collected questions are open-ended and challenging\nto evaluate, we employ GPT-4 to simplify ground-truth an-\nswers and generate three wrong answers to formulate them\nas multiple-choice questions. Additionally, we classify the\nquestions into their respective knowledge domains. And dur-\ning this process, we manually check the generated content of\nGPT-4 to ensure data quality.\nTo make the dataset more diverse and comprehensive, we\nfurther integrate data from some publicly available datasets:\n• MedQA (Jin et al. 2021) is a free-form multiple-choice\nOpenQA dataset for solving medical problems, collected\nfrom professional medical board exams. We use the test\nset of USMLE, which is the English subset of MedQA.\n• PubMedQA (Jin et al. 2019) is a biomedical question-\nanswering dataset collected from PubMed abstracts. The\ntask of PubMedQA is to answer research questions with\nyes/no/maybe using the corresponding abstracts, which\nis fit for evaluating the literature comprehension ability.\nWe incorporate 1000 expert-annotated data from it and\nframe them as judgment questions.\n• Reagent Selection (Guo et al. 2023) involves the iden-\ntification and proposal of the most fitting reagents for a\nspecific chemical reaction or process, which is a subset\nof ChemLLMBench. We randomly select 40% data and\nformulate them as multiple-choice questions.\nDynamic Data The current training of LLMs often uses\na large amount of data, resulting in a risk of data leakage\nfor evaluation. In order to solve this problem, we design a\n“dynamic” subset, which can generate data dynamically ac-\ncording to scientific principles. The dynamic subset covers\ntwo disciplines, chemistry and physics. For chemistry data,\nwe use the basic information and properties of molecules\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19055\nSocratic\nQ&A\nCrawl & Filter\nRaw Data\nGPT-4\nFiltered Data\nInstruction: Given a question and its ground-truth answer,\njudge whether it is suitable to be used as the title of a\nmultiple-choice question. Your answer should be \"YES\" or\n\"NO\". And please directly give the results without any\nexplanation.\nTask 1\nInstruction: Given a question and a ground-truth answer, please simplify the answer as\nconcise as possible. And I want to generate a 4-choice question using it, please generate\n3 fake answers for me. Note that the length of the simplified answer and these 3 fake\nanswers should be about the same and these 3 fake answers should be as confusing as\npossible. Furthermore, please help me to classify the domain of the question. There are\nthree domains in total: Base Knowledge, Scientific Calculation, Knowledge Application.\nTask 2\nStatic Data\nMedQA PubMedQA Reagent\nSelection\nGPT-4\nFigure 2: Data Collection steps of Static Data\ncrawled from PubChem3 to create data. For physics data, we\nmanually write some Python scripts according to the physics\nformulas. When obtaining the evaluation dataset, we will\nprovide a regenerated version to users and we will update\nit regularly, while at the same time, we will maintain a sta-\nble version of the dynamic data to make a fair comparison.\nExperimental Data To better evaluate the scientific\nthoughts and abilities of LLMs, SciEval introduces a sub-\nset of experimental data, involving 12 different basic scien-\ntific experiments. These experiments are collected from ba-\nsic science experiment courses at university, and each exper-\niment conducts a comprehensive investigation of the ability\nof LLMs in scientific research and experimentation from the\nperspectives of experimental principle, process, and analysis\nand summarization of experimental results.\n3.3 Data Statistics\nSummarized statistics are shown in Table 2, where we only\ncount Static Data. For Dynamic Data, the chemistry part ex-\namines the KA ability and contains 2000 data, while the\nphysics part evaluates the SC ability and involves 890 data.\nAll these questions are in English and we show some data\nexamples in Appendix D.\nAbility Bio Chem Phy\nBasic Knowledge 2147 2914 456\nKnowledge Application 1379 3720 36\nScientific Calculation 301 3401 1165\nResearch Ability 1000 0 0\nTotal 4830 10035 1657\nTable 2: Statistics of Static Data\nFor Static Data, we further split the data into dev, valid,\nand test set. For each data source, each knowledge domain,\nand each discipline, we randomly select 5 data to form the\n3https://pubchem.ncbi.nlm.nih.gov/\ndev set, which can be used for few-shot learning, and we\nsplit the remaining data with a ratio of 1:9 to construct the\nvalid set and test set respectively.\n4 Experiment\nGiven a question and four options, please select the right answer. Your\nanswer should be \"A\", \"B\", \"C\" or \"D\". Please directly give the answer\nwithout any explanation.\nHow many atoms are in 3.5 moles of arsenic atoms?\nA. 1.5 x 10^24 atoms\nB. 3.0 x 10^24 atoms\nC. 2.7 x 10^24 atoms\nD. 2.1 x 10^24 atoms\nAnswer: D\nFigure 3: An example of the prompt we used for AO setting.\nThe red text is the response from the model, while the black\ntext is the inputted prompt.\nGiven a question and four options, please select the right answer. Your\nanswer should be \"A\", \"B\", \"C\" or \"D\". \nHow many atoms are in 3.5 moles of arsenic atoms?\nA. 1.5 x 10^24 atoms\nB. 3.0 x 10^24 atoms\nC. 2.7 x 10^24 atoms\nD. 2.1 x 10^24 atoms\nAnswer: Let's think step by step: To find the number of atoms …… \nTherefore, the answer is D\nFigure 4: An example of the prompt we used for CoT setting.\nThe red text is the response from the model, while the blue\ntext and black text are the inputted prompt.\n4.1 Experiment Setup\nPrompts We evaluate LLMs in both Answer-Only (AO)\nand Chain-Of-Thought (CoT) (Kojima et al. 2022) settings.\nThe prompts we used are shown in Figures 3 and 4 respec-\ntively. Furthermore, we also evaluate using 3-shot setting,\nwhere the three exemplars are selected from the dev set.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19056\nModel Creator #Parameters Access SD DD ED\nGPT-4 OpenAI undisclosed API \" \" \"\nGPT-3.5-turbo OpenAI undisclosed API \" \" \"\nClaude-v1.3 Anthropic undisclosed API \" \" \"\nClaude-instant-v1.1 Anthropic undisclosed API \" \" \"\nERNIE Bot Baidu undisclosed Web \"\nSparkDesk iFLYTEK undisclosed Web \"\nVicuna LMSYS 13B Weights \" \"\nGalactica Meta 30B, 6.7B Weights \" \"\nChatGLM2 Tsinghua 6B Weights \" \"\nChatGLM Tsinghua 6B Weights \" \"\nAlpaca Stanford 7B Weights \" \"\nMOSS Fudan 16B Weights \" \"\nLLaMa Meta 7B, 13B Weights \" \"\nTable 3: Models evaluated in this paper. The “access” columns show whether we have full access to the model weights or we\ncan only access through API or web. SD stands for Static Data, DD stands for Dynamic Data, and ED stands for Experimental\nData. Marking “\"” means we evaluate the corresponding model on this subset.\nModel Static Data Chemistry(DD) Physics(DD) Exp\nBiology Chemistry Physics Avg. Acc. BLEU MSE Acc. Score\nGPT-4 84.49 69.38 65.22 73.93 11.05 23.78 891.09 25.84 93.31\nGPT-3.5-turbo 76.42 64.30 52.30 66.97 7.65 18.86 2008.72 21.80 88.27\nClaude-v1.3 72.58 59.72 54.94 63.45 5.75 21.98 1489.87 26.14 85.73\nClaude-instant-v1.1 70.43 53.36 52.30 58.92 0.45 16.07 8258.46 21.46 87.50\nGalactica-30B 66.48 50.16 44.65 54.96 0.9 4.14 485.99 22.47 -\nVicuna-13B 58.39 53.06 45.13 53.93 0.95 6.50 766.64 21.24 -\nGalactica-6.7B 57.84 50.77 30.99 50.87 1.55 6.47 5519.82 20.79 -\nChatGLM2-6B 58.62 44.00 40.26 48.44 0.2 1.86 3449.44 24.83 -\nChatGLM-6B 52.54 45.36 40.80 47.23 0.75 2.44 10303.90 21.01 -\nAlpaca-7B 56.66 42.43 37.01 46.54 0.2 2.92 428419.27 26.74 -\nMOSS-16B 47.71 33.87 31.73 38.23 0.1 7.37 30505.17 24.27 -\nLLaMa-13B 48.59 33.56 19.48 36.96 0.3 5.21 3707.01 7.08 -\nLLaMa-7B 36.24 26.38 15.02 28.37 0.5 1.26 11305.65 14.38 -\nERNIE Bot - - - - - - - - 61.12\nSparkDesk - - - - - - - - 33.69\nTable 4: Model performances of Answer-Only setting. The leaderboard is sorted by the average accuracy of Static Data.\nModels In order to comprehensively assess the scientific\ncapabilities of Large Language Models (LLMs), we eval-\nuate 15 high-performing LLMs that are widely accessible.\nThese models are selected to represent a diverse range of or-\nganizations and vary in size. The details of these models are\nsummarized in Table 3.\n• GPT-3.5-turbo and GPT-4 (Schulman et al. 2022; Ope-\nnAI 2023) are the strongest GPT model variants from\nOpenAI that have undergone pretraining, instruction tun-\ning, and reinforcement learning from human feedback\n(RLHF, (Ouyang et al. 2022)).\n• Claude 4, developed by Anthropic, is often considered\n4https://www.anthropic.com/index/introducing-claude.\ncomparable to GPT-3.5-turbo. We evaluate both the\nClaude-v1.3 and Claude-instant-v1.1, a lighter version.\n• ERNIE Bot 5 is developed by Baidu, possessing deep se-\nmantic understanding and generation capabilities across\nmodalities and languages. SparkDesk 6 is proposed by\niFLYTEK. It has cross-domain knowledge and language\nunderstanding capabilities and can understand and exe-\ncute tasks based on natural dialogue.\n• LLaMa (Touvron et al. 2023), developed by Meta, is\nprobably the best open-weight foundation model so far.\nVicuna (Zheng et al. 2023) and Alpaca (Taori et al. 2023)\n5https://yiyan.baidu.com/\n6https://xinghuo.xfyun.cn/\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19057\nFigure 5: Accuracy on Answer Only, Chain-of-Thought and 3-Shot settings of each LLMs for Static Data.\nare both fine-turned from LLaMa with supervised in-\nstruction fine-tuning. It is believed that the performance\nof Vicuna is better than that of Alpaca.\n• Galactica (Taylor et al. 2022) is also developed by Meta,\nwhich is trained on a large-scale scientific corpus. It is de-\nveloped to study the use of language models for the auto-\nmatic organization of science and can perform numerous\nscientific tasks, such as citation prediction, scientific QA,\nand molecular property prediction.\n• ChatGLM and ChatGLM2, created by Tsinghua Univer-\nsity, are based on GLM architecture (Du et al. 2022), and\nfurther adapted on conversational data. MOSS (Sun et al.\n2023), developed by Fudan University, is the first pub-\nlicly available Chinese LLM, and it follows a training\nprocedure similar to ChatGPT.\nWe evaluate GPT-3.5-turbo, GPT4 and Claude on all three\nsubsets, including Static Data, Dynamic Data, and Exper-\nimental Data. Since we can only assess ERNIE Bot and\nSparkDesk through web interface, we evaluate these two\nmodels only on the Experimental Data. And for the rest\nLLMs with billions or tens of billions of parameters, since\nthe length of the Experimental Data exceeds the length limit\nof these models7, we evaluate them on Static Data and Dy-\nnamic Data, as is shown in Table 3.\nEvaluation Metrics In the case of Static Data, all ques-\ntions are objective, making accuracy the appropriate evalu-\nation metric. For Dynamic Data, the physics questions are\npresented as multiple-choice questions, which can also be\nevaluated using accuracy. Conversely, the chemistry ques-\ntions involve complex components, such as “What is the\n7The maximum context length of ChatGLM2 is extended to\n32k, while it has limited ability to understand long texts.\nmolecular weight of A?” and “What is the SMILES expres-\nsion of B?”. Hence, for questions with numerical answers,\nwe employ MSE8 as the evaluation metric, while for ques-\ntions with string answers, we utilize the BELU score (Pap-\nineni et al. 2002). Additionally, we also calculate the extract\nmatch scores. As for Experimental Data, each experiment\nconsists of multiple open-ended questions. As a result, we\nassess the model-generated responses manually.\n4.2 Experiment Results\nAnswer-Only Setting Answer-only results of all the mod-\nels on the test set are shown in Table 4 and detailed results of\nStatic Data across different knowledge domains are provided\nin Appendix B. Analyzing the results of Static Data, GPT-\n4 demonstrates significantly superior performance com-\npared to other models. And only GPT-4, GPT-3.5-turbo, and\nClaude-v1.3 achieve an average accuracy exceeding 60%,\nwhich highlights the challenge posed by SciEval.\nFor the results of Dynamic Data, GPT-4 performs the best\nin terms of average accuracy and BLEU score. However, for\ncounting and calculation questions, Galactica-30B yields the\nbest results, indicating its strong aptitude in the field of sci-\nence. Conversely, models with billions or tens of billions of\nparameters perform poorly on the chemistry subset, suggest-\ning their limited knowledge about molecules. Regarding the\nperformance of models on the physics subset, since all ques-\ntions are 4-choices questions, the accuracy should be at least\n25%. However, none of these models achieve satisfactory\nresults in this subset.\nAs for Experimental Data, GPT-series models and\n8If the predictions do not contain any number, we will regard\nthe MSE as 1 × 1010\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19058\nModel Chemistry Physics\nAO CoT 3-Shot AO CoT 3-Shot\nGPT-4 11.05 11.65 ↑ 12.42↑ 25.84 17.98 ↓ 51.01 ↑\nGPT-3.5-turbo 7.65 10.20 ↑ 8.85 ↑ 21.80 47.19 ↑ 25.39 ∼\nGalactica-6.7B 1.55 1.75 ↑ 3.05 ↑ 20.79 23.37 ∼ 21.12 ∼\nVicuna-13B 0.95 1.95 ↑ 1.80 ↑ 21.24 18.65 ∼ 23.37∼\nGalactica-30B 0.90 2.60 ↑ 3.30 ↑ 22.47 14.72 ↓ 22.58 ∼\nChatGLM-6B 0.75 0.80 ↑ 1.15 ↑ 21.01 25.39 ∼ 23.37 ∼\nLLaMa-7B 0.50 0.10 ↓ 1.55 ↑ 18.65 9.66 ↓ 27.53 ↑\nLLaMa-13B 0.30 0.25 ∼ 2.11 ↑ 7.08 5.84 ∼ 22.70 ↑\nChatGLM2-6B 0.20 2.65 ↑ 1.60 ↑ 24.83 25.39 ∼ 26.74 ∼\nAlpaca-7B 0.20 0.65 ↑ 2.10 ↑ 26.71 28.43 ∼ 25.62 ∼\nMOSS-16B 0.10 0.85 ↑ 0.65 ↑ 24.27 25.06 ∼ 26.40 ∼\nTable 5: Results on Answer-Only, Chain-of-Thought and 3-Shot settings of each LLM for Dynamic Data. ↑ means the perfor-\nmance is slightly better than that under Answer-Only setting,↓ means worse, and ∼ means the performance is nearly the same.\nClaude-series models achieve good results, while the other\ntwo models are not. The detailed scores models reached in\neach experiment are shown in Appendix C. However, al-\nthough some models could get a great performance, during\nexperiments, we find that these models are good at exper-\nimental principles and designing, while when it comes to\nanalyzing the experiment results, the performances are not\nsatisfying.\nCoT Setting and 3-Shot setting Comparison of experi-\nment results among Answer-Only, Chain-of-Thought and 3-\nShot settings are shown in Figure 5 and Table 5. 9 And we\nrefer detailed results to Appendix A and B.\nThe experimental results from Static Data reveal that\nsolely the GPT-series LLMs get performance enhancement\nwithin the CoT setting due to the limited CoT capabilities\nof other LLMs. As for the 3-Shot setting, roughly half of\nthe LLMs analyzed demonstrate superior performances rela-\ntive to the Answer-Only setting. The performances of the re-\nmaining LLMs are closely similar to those observed within\nthe Answer-Only setting.\nFrom the experimental results of Dynamic Data, it is ob-\nserved that both CoT and 3-Shot significantly enhance the\nperformance of most Language Learning Models (LLMs) in\nthe chemistry subset. However, the performances achieved\nare still not up to the mark. In the physics subset, the impact\nof CoT and 3-Shot on most LLMs is less pronounced, result-\ning in nearly random performances. Under the CoT setting,\nGPT-3.5-turbo achieves an accuracy of 47.19, suggesting a\nrobust understanding of physical principles. Conversely, the\nperformance of GPT-4 is markedly poor, from which we find\nthat despite its extensive knowledge of physical principles,\nit frequently employs incorrect formulas to solve problems.\nNevertheless, GPT-4 attains an accuracy of 51.01 under 3-\nShot setting, the highest among all models, demonstrating\nits ability to learn from a mere three examples.\n9When evaluating on CoT and 3-Shot settings, Claude-Instant\nand Claude are not available for us, due to the limitation of API.\n4.3 Discussion\nTraining on large-scale scientific corpus is helpful.\nBased on experimental results (Table 4), Galactica (Taylor\net al. 2022), which has been trained on an extensive sci-\nentific corpus, significantly outperforms other LLMs with\na comparable number of parameters, although Galactica is\ntrained with a much smaller amount of data. Remarkably,\nwhen tested on Dynamic Data, Galactica surpasses the GPT-\nseries and Claude-series LLMs in computational problems.\nMost LLMs perform bad on calculation problems, espe-\ncially in physics domain. Detailed results across various\nknowledge domains on Static Data (refer to Appendix B)\nreveal that most LLMs underperform in the Scientific Cal-\nculation domain, while demonstrate relatively superior per-\nformance in other domains, which is particularly acute in\nthe field of physics. Similar issues are also observed in Dy-\nnamic Data and Experimental Data. In the context of Dy-\nnamic Data, the mean square error, employed to evaluate cal-\nculation abilities within the chemistry subset, is exceedingly\nhigh for most LLMs, and almost all LLMs can only achieve\nnearly random performance within the physics subset. Re-\ngarding Experimental Data, our findings indicate that these\nLLMs struggle with the analysis of experimental results.\n5 Conclusion\nIn this paper, we introduce SciEval, a benchmark designed to\nevaluate scientific capabilities of LLMs. SciEval comprises\nabout 18,000 challenging scientific questions, covering three\nfundamental fields of science. SciEval assesses the scientific\nability of LLMs across four dimensions. It incorporates both\nobjective and subjective questions, and employs dynamic\ndata generation to mitigate potential data leakage. We con-\nduct comprehensive experiments on various advanced LLMs\nusing SciEval and perform thorough analyses. Our experi-\nmental results reveal that most LLMs do not perform well\non our benchmark, with the exception of the GPT-series\nand Claude-series LLMs. We hope that SciEval can serve\nas a robust benchmark for assessing scientific capabilities of\nLLMs.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19059\nAcknowledgements\nThis work is funded by the China NSFC Projects (92370206,\nU23B2057, 62106142 and 62120106006) and Shang-\nhai Municipal Science and Technology Major Project\n(2021SHZDZX0102).\nReferences\nBlanco-Gonzalez, A.; Cabezon, A.; Seco-Gonzalez, A.;\nConde-Torres, D.; Antelo-Riveiro, P.; Pineiro, A.; and\nGarcia-Fandino, R. 2023. The role of ai in drug discovery:\nchallenges, opportunities, and strategies. Pharmaceuticals,\n16(6): 891.\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Zhu, K.; Chen, H.;\nYang, L.; Yi, X.; Wang, C.; Wang, Y .; et al. 2023. A sur-\nvey on evaluation of large language models. arXiv preprint\narXiv:2307.03109.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335.\nForehand, M. 2010. Blooms taxonomy. Emerging perspec-\ntives on learning, teaching, and technology, 41(4): 47–56.\nFrey, N.; Soklaski, R.; Axelrod, S.; Samsi, S.; Gomez-\nBombarelli, R.; Coley, C.; and Gadepally, V . 2022. Neural\nscaling of deep chemical models.\nGuo, T.; Guo, K.; Liang, Z.; Guo, Z.; Chawla, N. V .; Wiest,\nO.; Zhang, X.; et al. 2023. What indeed can GPT models do\nin chemistry? A comprehensive benchmark on eight tasks.\narXiv preprint arXiv:2305.18365.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2020. Measuring mas-\nsive multitask language understanding. arXiv preprint\narXiv:2009.03300.\nHendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart,\nS.; Tang, E.; Song, D.; and Steinhardt, J. 2021. Measuring\nmathematical problem solving with the math dataset. arXiv\npreprint arXiv:2103.03874.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; et al. 2023. C-eval: A multi-\nlevel multi-discipline chinese evaluation suite for foundation\nmodels. arXiv preprint arXiv:2305.08322.\nJin, D.; Pan, E.; Oufattole, N.; Weng, W.-H.; Fang, H.; and\nSzolovits, P. 2021. What disease does this patient have? a\nlarge-scale open domain question answering dataset from\nmedical exams. Applied Sciences, 11(14): 6421.\nJin, Q.; Dhingra, B.; Liu, Z.; Cohen, W. W.; and Lu, X. 2019.\nPubmedqa: A dataset for biomedical research question an-\nswering. arXiv preprint arXiv:1909.06146.\nKojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,\nY . 2022. Large language models are zero-shot reason-\ners. Advances in neural information processing systems, 35:\n22199–22213.\nKrathwohl, D. R. 2002. A revision of Bloom’s taxonomy:\nAn overview. Theory into practice, 41(4): 212–218.\nLiang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.;\nYasunaga, M.; Zhang, Y .; Narayanan, D.; Wu, Y .; Kumar,\nA.; et al. 2022. Holistic evaluation of language models.\narXiv preprint arXiv:2211.09110.\nLu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-\nC.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022. Learn to\nexplain: Multimodal reasoning via thought chains for sci-\nence question answering. Advances in Neural Information\nProcessing Systems, 35: 2507–2521.\nLuo, R.; Sun, L.; Xia, Y .; Qin, T.; Zhang, S.; Poon, H.;\nand Liu, T.-Y . 2022. BioGPT: generative pre-trained trans-\nformer for biomedical text generation and mining. Briefings\nin Bioinformatics, 23(6): bbac409.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nSchulman, J.; Zoph, B.; Kim, C.; Hilton, J.; Menick, J.;\nWeng, J.; Uribe, J. F. C.; Fedus, L.; Metz, L.; Pokorny, M.;\net al. 2022. ChatGPT: Optimizing language models for dia-\nlogue. OpenAI blog.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\net al. 2023. Large language models encode clinical knowl-\nedge. Nature, 1–9.\nSrivastava, A.; Rastogi, A.; Rao, A.; Shoeb, A. A. M.; Abid,\nA.; Fisch, A.; Brown, A. R.; Santoro, A.; Gupta, A.; Garriga-\nAlonso, A.; et al. 2022. Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language models.\narXiv preprint arXiv:2206.04615.\nSun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu,\nX.; Shao, Y .; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y .; Zhou,\nZ.; Li, R.; Zhan, J.; Zhou, Y .; Li, L.; Yang, X.; Wu, L.; Yin,\nZ.; Huang, X.; and Qiu, X. 2023. MOSS: Training Conver-\nsational Language Models from Synthetic Data.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-\nford alpaca: An instruction-following llama model.\nTaylor, R.; Kardas, M.; Cucurull, G.; Scialom, T.; Hartshorn,\nA.; Saravia, E.; Poulton, A.; Kerkez, V .; and Stojnic, R.\n2022. GALACTICA: A Large Language Model for Science.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nW ANG, F.; and MIAO, Q. 2023. Novel Paradigm for AI-\ndriven Scientific Research: From AI4S to Intelligent Sci-\nence. Bulletin of Chinese Academy of Sciences (Chinese\nVersion), 38(4): 536–540.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19060\nWang, X.; Hu, Z.; Lu, P.; Zhu, Y .; Zhang, J.; Subramaniam,\nS.; Loomba, A. R.; Zhang, S.; Sun, Y .; and Wang, W. 2023.\nSciBench: Evaluating College-Level Scientific Problem-\nSolving Abilities of Large Language Models.arXiv preprint\narXiv:2307.10635.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv preprint arXiv:2306.05685.\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang, Y .;\nSaied, A.; Chen, W.; and Duan, N. 2023. Agieval: A human-\ncentric benchmark for evaluating foundation models. arXiv\npreprint arXiv:2304.06364.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19061",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7391810417175293
    },
    {
      "name": "Computer science",
      "score": 0.5937813520431519
    },
    {
      "name": "Natural language processing",
      "score": 0.3251391649246216
    },
    {
      "name": "Geography",
      "score": 0.1039426326751709
    },
    {
      "name": "Cartography",
      "score": 0.06015375256538391
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ],
  "cited_by": 20
}