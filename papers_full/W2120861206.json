{
  "title": "A Fast and Simple Algorithm for Training Neural Probabilistic Language Models",
  "url": "https://openalex.org/W2120861206",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287042756",
      "name": "Mnih, Andriy",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A4223290457",
      "name": "Teh, Yee Whye",
      "affiliations": [
        "University College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2124059530",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2080021477",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2963353830",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1423339008",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W2096175520"
  ],
  "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.",
  "full_text": "A fast and simple algorithm for training neural probabilistic\nlanguage models\nAndriy Mnih amnih@gatsby.ucl.ac.uk\nGatsby Computational Neuroscience Unit, University College London\nYee Whye Teh ywteh@gatsby.ucl.ac.uk\nGatsby Computational Neuroscience Unit, University College London\nAbstract\nIn spite of their superior performance, neural\nprobabilistic language models (NPLMs) re-\nmain far less widely used than n-gram mod-\nels due to their notoriously long training\ntimes, which are measured in weeks even for\nmoderately-sized datasets. Training NPLMs\nis computationally expensive because they\nare explicitly normalized, which leads to hav-\ning to consider all words in the vocabulary\nwhen computing the log-likelihood gradients.\nWe propose a fast and simple algorithm for\ntraining NPLMs based on noise-contrastive\nestimation, a newly introduced procedure for\nestimating unnormalized continuous distri-\nbutions. We investigate the behaviour of the\nalgorithm on the Penn Treebank corpus and\nshow that it reduces the training times by\nmore than an order of magnitude without af-\nfecting the quality of the resulting models.\nThe algorithm is also more eﬃcient and much\nmore stable than importance sampling be-\ncause it requires far fewer noise samples to\nperform well.\nWe demonstrate the scalability of the pro-\nposed approach by training several neural\nlanguage models on a 47M-word corpus with\na 80K-word vocabulary, obtaining state-of-\nthe-art results on the Microsoft Research\nSentence Completion Challenge dataset.\nAppearing in Proceedings of the 29th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\n1. Introduction\nBy assigning probabilities to sentences, language mod-\nels allow distinguishing between probable and improb-\nable sentences, which makes such models an impor-\ntant component of speech recognition, machine trans-\nlation, and information retrieval systems. Probabilis-\ntic language models are typically based on the Markov\nassumption, which means that they model the condi-\ntional distribution of the next word in a sentence given\nsome ﬁxed number of words that immediately precede\nit. The group of words conditioned on is called the\ncontext, denoted h, while the word being predicted is\ncalled the target word, denoted w. n-gram models,\nwhich are eﬀectively smoothed tables of normalized\nword/context co-occurrence counts, have dominated\nlanguage modelling for decades due to their simplicity\nand excellent performance.\nIn the last few years neural language models have be-\ncome competitive with n-grams and now routinely out-\nperform them (Mikolov et al., 2011). NPLMs model\nthe distribution for the next word as a smooth function\nof learned multi-dimensional real-valued representa-\ntions of the context words and the target word. Similar\nrepresentations are learned for words that are used in\nsimilar ways, ensuring that the network outputs simi-\nlar probability values for them. Word representations\nlearned by language models are also used for natu-\nral language processing applications such as semantic\nrole labelling (Collobert & Weston, 2008), sentiment\nanalysis (Maas & Ng, 2010), named entity recognition\n(Turian et al., 2010), and parsing (Socher et al., 2011).\nUnfortunately, NPLMs are very slow to train, which\nmakes them unappealing for large-scale applications.\nThis is a consequence of having to consider the entire\nvocabulary when computing the probability of a single\nword or the corresponding gradient. In fact, the time\ncomplexity of this computation scales as the product\nA fast and simple algorithm for training neural probabilistic language models\nof the vocabulary size and the word feature dimen-\nsionality. One way to accelerate this computation is\nto reduce the vocabulary size for the NPLM by using it\nto predict only the most frequent words and handling\nthe rest using an n-gram model (Schwenk & Gauvain,\n2005).\nAlternatively, the vocabulary can be structured into a\ntree with words at the leaves, allowing exponentially\nfaster computation of word probabilities and their gra-\ndients (Morin & Bengio, 2005). Unfortunately, the\npredictive performance of the resulting model is heav-\nily dependent on the tree used and ﬁnding a good tree\nis a diﬃcult problem (Mnih & Hinton, 2009).\nPerhaps a more elegant approach is to keep the model\nthe same and to approximate the expensive gradient\ncomputations using importance sampling (Bengio &\nSen´ ecal, 2003). Unfortunately, the variance in the im-\nportance sampling estimates can make learning unsta-\nble, unless it is carefully controlled (Bengio & Sen´ ecal,\n2008).\nIn this paper we propose an eﬃcient algorithm for\ntraining NPLMs based on noise-contrastive estima-\ntion (Gutmann & Hyv¨ arinen, 2010), which is much\nmore stable than importance sampling. Though it also\nuses sampling to approximate the gradients needed\nfor learning, neither the number of samples nor the\nproposal distribution require dynamic adaptation for\nachieving performance on par with maximum likeli-\nhood learning.\n2. Neural probabilistic language models\nA statistical language model is simply a collection of\nconditional distributions for the next word, indexed\nby its context.1 In a neural language model the condi-\ntional distribution corresponding to context h, Ph(w),\nis deﬁned as\nPh\nθ (w) = exp(sθ(w,h))∑\nw′ exp(sθ(w′,h)), (1)\nwhere sθ(w,h) is the scoring function with parameters\nθ which quantiﬁes the compatibility of word w with\ncontext h. The negated scoring function is sometimes\nreferred to as the energy function (Bengio et al., 2000).\nDepending on the form of sθ(w,h), Eq. 1 can de-\nscribe both neural and maximum entropy language\nmodels (Berger et al., 1996). The main diﬀerence\nbetween these two model classes lies in the features\n1Though almost all statistical language models predict\nthe next word, it is also possible to model the distribution\nof the word preceding the context or surrounded by the\ncontext.\nthey use: neural language models learn their features\njointly with other parameters, while maximum entropy\nmodels use ﬁxed hand-engineered features and only\nlearn the weights for those features. A neural language\nmodel represents each word in the vocabulary using a\nreal-valued feature vector and deﬁnes the scoring func-\ntion in terms of the feature vectors of the context words\nand the next word. In some models, diﬀerent feature\nvector tables are used for the context and the next\nword vocabularies (Bengio et al., 2000), while in oth-\ners the table is shared (Bengio et al., 2003; Mnih &\nHinton, 2007).\nThe feature vectors account for the vast majority of pa-\nrameters in neural language models, which means that\ntheir memory requirements are linear in the vocabu-\nlary size. This compares favourably to the memory re-\nquirements of the n-gram models, which are typically\nlinear in the training set size.\n2.1. Log-bilinear model\nThe training method we propose is directly applicable\nto all neural probabilistic and maximum-entropy lan-\nguage models. For simplicity, we will perform our ex-\nperiments using the log-bilinear language (LBL) model\n(Mnih & Hinton, 2007), which is the simplest neu-\nral language model. The LBL model performs lin-\near prediction in the semantic word feature space and\ndoes not have non-linearities. In spite of its simplic-\nity, the LBL model has been shown to outperform\nn-grams, though the more complex neural language\nmodels (Mikolov et al., 2010; Mnih et al., 2009) can\noutperform it.\nIn this paper we will use a slightly extended version of\nthe LBL model that uses separate feature vector tables\nfor the context words and the target words. Thus a\ncontext word wwill be represented with feature vector\nrw, while a target word wwill be represented with fea-\nture vector qw. Given a context h, the model computes\nthe predicted representation for the target word by\nlinearly combining the feature vectors for the context\nwords using position-dependent context weight matri-\nces Ci:\nˆq=\nn−1∑\ni=1\nCirwi. (2)\nThe score for the match between the context and the\nnext word is computed by taking the dot product be-\ntween the predicted representation and the represen-\ntation of the candidate target word w:\nsθ(w,h) = ˆq⊤qw + bw. (3)\nA fast and simple algorithm for training neural probabilistic language models\nHere bw is the base rate parameter used to model the\npopularity of w. The probability of w in context h\nis then obtained by plugging the above score function\ninto Eq. 1.\n2.2. Maximum likelihood learning\nMaximum likelihood training of neural language mod-\nels is tractable but expensive because computing the\ngradient of log-likelihood takes time linear in the\nvocabulary size. The contribution of a single con-\ntext/word observation to the gradient of the log-\nlikelihood is given by\n∂\n∂θ log Ph\nθ (w) = ∂\n∂θsθ(w,h) −\n∑\nw′\nPh\nθ (w′) ∂\n∂θsθ(w′,h)\n(4)\n= ∂\n∂θsθ(w,h) −EPh\nθ\n[∂\n∂θsθ(w,h)\n]\n.\nThe expectation w.r.t. Ph\nθ (w′) is expensive to evaluate\nbecause it requires computing sθ(w,h) for all words\nin the vocabulary. Since vocabularies typically con-\ntain tens of thousands of words, maximum likelihood\nlearning tends to be very slow.\n2.3. Importance sampling\nBengio and Sen´ ecal (2003) have proposed a method\nfor speeding up training of neural language models\nbased on approximating the expectation in Eq. 4 us-\ning importance sampling. The idea is to generate k\nsamples x1,...,x k from an easy-to-sample-from distri-\nbution Qh(w) and estimate the gradient with\n∂\n∂θ log Ph\nθ (w) ≈ ∂\n∂θsθ(w,h) −1\nV\nk∑\nj=1\nv(xj) ∂\n∂θsθ(xj,h),\n(5)\nwhere v(x) = exp(sθ(x,h))\nQh(w=x) and V = ∑k\nj=1 v(xj). The\nnormalization by V is necessary here because the im-\nportance weights v are computed using the unnormal-\nized model distribution exp( sθ(x,h)). Typically the\nproposal distribution is an n-gram model ﬁt to the\ntraining set, possibly with a context size diﬀerent from\nthe neural model’s.\nThough this approach is conceptually simple, it is non-\ntrivial to use in practice because the high variance of\nthe importance sampling estimates can make learn-\ning unstable. The variance tends to grow as learn-\ning progresses, because the model distribution moves\naway from the proposal distribution. 2 One way to\n2Bengio and Sen´ ecal (2008) argue that this happens be-\ncontrol the variance is to keep increasing the number\nof samples during training so that the eﬀective sam-\nple size stays above some predetermined value (Ben-\ngio & Sen´ ecal, 2003). Alternatively, the n-gram pro-\nposal distribution can be adapted to track the model\ndistribution throughout training (Bengio & Sen´ ecal,\n2008). The ﬁrst approach is simpler but less eﬃ-\ncient because the increasing number of samples makes\nlearning slower. The second approach leads to greater\nspeedups but is considerably more diﬃcult to imple-\nment and requires additional memory for storing the\nadaptive proposal distribution.\n3. Noise-contrastive estimation\nWe propose using noise-contrastive estimation (NCE)\nas a more stable alternative to importance sampling for\neﬃcient training of neural language models and other\nmodels deﬁned by Eq. 1. NCE has recently been in-\ntroduced by Gutmann and Hyv¨ arinen (2010) for train-\ning unnormalized probabilistic models. Though it has\nbeen developed for estimating probability densities, we\nare interested in applying it to discrete distributions\nand so will assume discrete distributions and use prob-\nability mass functions instead of density functions.\nThe basic idea of NCE is to reduce the problem of\ndensity estimation to that of binary classiﬁcation, dis-\ncriminating between samples from the data distribu-\ntion and samples from a known noise distribution. In\nthe language modelling setting, the data distribution\nPh\nd(w) will be the distribution of words that occur af-\nter a particular context h. Though it is possible to use\ncontext-dependent noise distributions, for simplicity\nwe will use a context-independent (unigram) Pn(w).\nWe are interested in ﬁtting the context-dependent\nmodel Ph\nθ (w) to Ph\nd(w).\nFollowing Gutmann and Hyv¨ arinen (2012), we assume\nthat noise samples arektimes more frequent than data\nsamples, so that datapoints come from the mixture\n1\nk+1 Ph\nd(w)+ k\nk+1 Pn(w). Then the posterior probability\nthat sample w came from the data distribution is\nPh(D= 1|w) = Ph\nd(w)\nPh\nd(w) + kPn(w). (6)\nSince we would like to ﬁt Ph\nθ to Ph\nd, we use Ph\nθ in\nplace of Ph\nd in Eq. 6, making the posterior probability\na function of the model parameters θ:\nPh(D= 1|w,θ) = Ph\nθ (w)\nPh\nθ (w) + kPn(w). (7)\ncause neural language models and n-gram models learn\nvery diﬀerent distributions.\nA fast and simple algorithm for training neural probabilistic language models\nThis quantity can be too expensive to compute, how-\never, because of the normalization required for eval-\nuating Ph\nθ (w) (Eq. 1). NCE sidesteps this issue by\navoiding explicit normalization and treating normal-\nization constants as parameters. Thus the model is\nparameterized in terms of an unnormalized distribu-\ntion Ph0\nθ0 and a learned parameter ch corresponding to\nthe logarithm of the normalizing constant:\nPh\nθ (w) = Ph0\nθ0 (w) exp(ch). (8)\nHere θ0 are the parameters of the unnormalized dis-\ntribution and θ= {θ0,ch}.\nTo ﬁt the context-dependent model to the data (for\nthe moment ignoring the fact that it shares parameters\nwith models for other contexts), we simply maximize\nthe expectation of logPh(D|w,θ) under the mixture of\nthe data and noise samples. This leads to the objective\nfunction\nJh(θ) =EPh\nd\n[\nlog Ph\nθ (w)\nPh\nθ (w) + kPn(w)\n]\n+ (9)\nkEPn\n[\nlog kPn(w)\nPh\nθ (w) + kPn(w)\n]\nwith the gradient\n∂\n∂θJh(θ) =EPh\nd\n[ kPn(w)\nPh\nθ (w) + kPn(w)\n∂\n∂θ log Ph\nθ (w)\n]\n−\n(10)\nkEPn\n[ Ph\nθ (w)\nPh\nθ (w) + kPn(w)\n∂\n∂θ log Ph\nθ (w)\n]\n.\nNote that the gradient can also be expressed as\n∂\n∂θJh(θ) =\n∑\nw\nkPn(w)\nPh\nθ (w) + kPn(w)× (11)\n(Ph\nd(w) −Ph\nθ (w)) ∂\n∂θ log Ph\nθ (w),\nand that as k→∞,\n∂\n∂θJh(θ) →\n∑\nw\n(Ph\nd(w) −Ph\nθ (w)) ∂\n∂θ log Ph\nθ (w), (12)\nwhich is the maximum likelihood gradient. Thus as\nthe ratio of noise samples to observations increases,\nthe NCE gradient approaches the maximum likelihood\ngradient.\nIn practice, given a word w observed in context h, we\ncompute its contribution to the gradient by generating\nk noise samples x1,...,x k and using the formula\n∂\n∂θJh,w(θ) = kPn(w)\nPh\nθ (w) + kPn(w)\n∂\n∂θ log Ph\nθ (w)− (13)\nk∑\ni=1\n[ Ph\nθ (xi)\nPh\nθ (xi) + kPn(xi)\n∂\n∂θ log Ph\nθ (xi)\n]\n.\nNote that the weights Ph\nθ (xi)\nPh\nθ (xi)+kPn(xi) are always be-\ntween 0 and 1, which makes NCE-based learning very\nstable (Gutmann & Hyv¨ arinen, 2010). In contrast, the\nweights produced by importance sampling can be ar-\nbitrarily large.\nSince the distributions for diﬀerent contexts share pa-\nrameters, we cannot learn these distributions indepen-\ndently of each other by optimizing oneJh(θ) at a time.\nInstead, we deﬁne a global NCE objective by combin-\ning the per-context NCE objectives using the empirical\ncontext probabilities P(h) as weights:\nJ(θ) =\n∑\nh\nP(h)Jh(θ). (14)\nNote that this is the traditional approach for combin-\ning the per-context ML objectives for training neural\nlanguage models.\n3.1. Dealing with normalizing constants\nOur initial implementation of NCE training learned a\n(log-)normalizing constant ( c in Eq. 8) for each con-\ntext in the training set, storing them in a hash table\nindexed by the context.3 Though this approach works\nwell for small datasets, it requires estimating one pa-\nrameter per context, making it diﬃcult to scale to huge\nnumbers of observed contexts encountered by models\nwith large context sizes. Surprisingly, we discovered\nthat ﬁxing the normalizing constants to 1, 4 instead of\nlearning them, did not aﬀect the performance of the\nresulting models. We believe this is because the model\nhas so many free parameters that meeting the approxi-\nmate per-context normalization constraint encouraged\nby the objective function is easy.\n3.2. Potential speedup\nWe will now compare the gradient computation costs\nfor NCE and ML learning. Suppose c is the context\nsize, d is the word feature vector dimensionality, and\nV is the vocabulary size of the model. Then com-\nputing the predicted representation using Eq. 2 takes\nabout cd2 operations for both NCE and ML. For ML,\ncomputing the distribution of the next word from the\npredicted representation takes about Vd operations.\nFor NCE, evaluating the probability ofknoise samples\nunder the model takes about kd operations. Since the\ngradient computation in each model has the same com-\nplexity as computing the probabilities, the speedup for\n3We did not use the learned normalizing constants when\ncomputing the validation and test set perplexities. Rather\nwe normalized the probabilities explicitly.\n4This amounts to setting the normalizing parameters c\nto 0.\nA fast and simple algorithm for training neural probabilistic language models\nTable 1. Results for the LBL model with 100D feature vec-\ntors and a 2-word context on the Penn Treebank corpus.\nTraining Number of Test Training\nalgorithm samples PPL time (h)\nML 163.5 21\nNCE 1 192.5 1.5\nNCE 5 172.6 1.5\nNCE 25 163.1 1.5\nNCE 100 159.1 1.5\nTable 2. The eﬀect of the noise distribution and the num-\nber of noise samples on the test set perplexity.\nNumber of PPL using PPL using\nsamples unigram noise uniform noise\n1 192.5 291.0\n5 172.6 233.7\n25 163.1 195.1\n100 159.1 173.2\neach parameter update due to using NCE is about\nSpeedup = cd2 + Vd\ncd2 + kd = cd+ V\ncd+ k. (15)\nFor a model with a 2-word context, 100D feature vec-\ntors, and a vocabulary size of 10K, an NCE update\nusing 25 noise samples should be about 45 times faster\nthan an ML update.\nSince the time complexity of computing the predicted\nrepresentation is quadratic in the feature vector di-\nmensionality, it can dominate the cost of the parameter\nupdate, making learning slow even for a small number\nof noise samples. We can avoid this by making context\nweight matrices Ci diagonal, reducing the complexity\nof computing the predicted representation to cd, and\nmaking the speedup factor c+V\nc+k. For the model above\nthis would amount to a factor of 370. The use of di-\nagonal context matrices was introduced by Mnih &\nHinton (2009) to speed up their hierarchical LBL-like\nmodel.\nSince the cost of a parameter update for importance-\nsampling-based learning is the same as for NCE with\nthe same number of noise samples, the algorithm that\nneeds fewer samples to perform well will be faster.\n4. Penn Treebank results\nWe investigated the properties of the proposed algo-\nrithm empirically on the Penn Treebank corpus. As is\ncommon practice, we trained on sections 0-20 (930K\nwords) and used sections 21-22 (74k words) as the val-\nidation set and sections 23-24 (82k words) as the test\nset. The standard vocabulary of 10K most frequent\nwords was used with the remaining words replaced by\na special token. We chose to use this dataset to keep\nthe training time for exact maximum likelihood learn-\ning reasonable.\nThe learning rates were adapted at the end of each\nepoch based on the change in the validation set per-\nplexity since the end of the previous epoch. The rates\nwere halved when the perplexity increased and were\nleft unchanged otherwise. Parameters were updated\nbased on mini-batches of 1000 context/word pairs\neach. Except when stated otherwise, NCE training\ngenerated 25 noise samples from the empirical unigram\ndistribution per context/word observation. Noise sam-\nples were generated anew for each update. We did\nnot use a weight penalty as the validation-score-based\nlearning rate reduction appeared to be suﬃcient to\navoid overﬁtting. All models used a two-word context\nand diﬀerent 100D feature vector tables for context\nand target words.\nOur ﬁrst experiment compared ML learning to NCE\nlearning for various numbers of noise samples. The re-\nsulting test perplexities and training times are shown\nin Table 1. It is clear that increasing the number\nof noise samples produces better-performing models,\nwith 25 samples being suﬃcient to match the ML-\ntrained model. In terms of training time, NCE was\n14 times faster than ML. The number of noise sam-\nples did not have a signiﬁcant eﬀect on the running\ntime because computing the predicted representation\nwas considerably more expensive than computing the\nprobability of (at most) 100 samples. The main rea-\nson the speedup factor was less than 45 (the value pre-\ndicted in Section 3.2) is because NCE took about twice\nas many epochs as ML to converge. Our NCE imple-\nmentation is also less optimized than the ML imple-\nmentation which takes greater advantage of the BLAS\nmatrix routines.\nTo explore the eﬀect of the noise distribution on the\nperformance of the algorithm, we tried generating\nnoise samples from the unigram as well as the uniform\ndistribution. For each noise distribution we trained\nmodels using 1, 5, 25, and 100 noise samples per dat-\napoint. As shown in Table 2, the unigram noise dis-\ntribution leads to much better test set perplexity in\nall cases. However, the perplexity gap shrinks as the\nnumber of noise samples increases, from almost 100\nfor a single noise sample down to under 15 for 100\nnoise samples. In spite of poor test set performance,\na uniform noise distribution did not lead to learning\nA fast and simple algorithm for training neural probabilistic language models\ninstability even when a single noise sample was used.\nIn addition to the ML and NCE algorithms, we also\nimplemented the importance sampling training algo-\nrithm from (Bengio & Sen´ ecal, 2003) to use as a base-\nline, but found it very unstable. It diverged in virtually\nall of our experiments, even with adaptive sample size\nand the target eﬀective sample size set to hundreds.\nThe only run that did not diverge involved learning a\nunigram model using the target unigram as the pro-\nposal distribution, which is the ideal situation for im-\nportance sampling. The cause of failure in all cases was\nthe appearance of extremely large importance weights\nonce the model distribution became suﬃciently diﬀer-\nent from the unigram proposal distribution 5, which is\na known problem with importance sampling. Since\nIS-based methods seem to require well over a hundred\nsamples per gradient computation (Bengio & Sen´ ecal,\n2008), even when an adaptive proposal distribution\nis used, we expect IS-based training to be consider-\nably slower than NCE, which, as we have shown, can\nachieve ML-level performance with only 25 noise sam-\nples.\n5. Sentence Completion Challenge\nTo demonstrate the scalability and eﬀectiveness of\nour approach we used it to train several large neu-\nral language models for the Microsoft Research Sen-\ntence Completion Challenge (Zweig & Burges, 2011).\nThe challenge was designed as a benchmark for seman-\ntic models and consists of SAT-style sentence comple-\ntion problems. Given 1,040 sentences, each of which is\nmissing a word, the task is to select the correct word\nout of the ﬁve candidates provided for each sentence.\nCandidate words have been chosen from relatively in-\nfrequent words using a maximum entropy model to\nensure that the resulting complete sentences were not\ntoo improbable. Human judges then picked the best\nfour candidates for each sentence so that all comple-\ntions were grammatically correct but the correct an-\nswer was unambiguous. Though humans can achieve\nover 90% accuracy on the task, statistical models fare\nmuch worse with the best result of 49% produced\nby a whole-sentence LSA model, and n-gram models\nachieving only about 39% accuracy (Zweig & Burges,\n2011).\nNeural language models are a natural choice for this\ntask because they can take advantage of larger con-\ntexts than traditional n-gram models, which we expect\n5Though using a unigram proposal distribution might\nappear naive, Bengio and Sen´ ecal (2003) reported that\nhigher-order n-gram proposal distributions worked much\nworse than the unigram.\nto be important for sentence completion. We used a\nslightly modiﬁed LBL architecture for our models for\nthis task. In the interests of scalability, we used diag-\nonal context weight matrices which reduced the time\ncomplexity of gradient computations from quadratic\nto linear in the dimensionality of word feature vectors\nand allowed us to use more feature dimensions. Since\nthe task was sentence completion, we made the mod-\nels aware of sentence boundaries by using a special\n“out-of-sentence” token for words in context positions\noutside of the sentence containing the word being pre-\ndicted. For example, this token would be used as the\ncontext word when predicting the ﬁrst word in a sen-\ntence using a model with a single-word context.\nWe score a candidate sentence with a language model\nby using it to compute the probability of each word in\nthe sentence and taking the product of those probabil-\nities as the sentence score. We then pick the candidate\nword that produces the highest-scoring sentence as our\nanswer. Note that this way of using a model with a c-\nword context takes into account cwords on both sides\nof the candidate word because the probabilities of the\nc words following the candidate word depend on it.\nThe models were trained on the standard training set\nfor the challenge containing 522 works from Project\nGutenberg. After removing the Project Gutenberg\nheaders and footers from the ﬁles, we split them into\nsentences and then tokenized the sentences into words.\nWe used the Punkt sentence tokenizer and the Penn\nTreebank word tokenizer from NLTK (Bird et al.,\n2009). We then converted all words to lowercase and\nreplaced the ones that occurred fewer than 5 times\nwith an “unknown word” token, resulting in a vocab-\nulary size of just under 80,000. The sentences to be\ncompleted were preprocessed in the same manner. The\nresulting dataset was then randomly split at the sen-\ntence level into a test and validation sets of 10K words\n(500 sentences) each and a 47M-word training set.\nWe used the training procedure described in Section 4,\nwith the exception of using a small weight penalty to\navoid overﬁtting. Each model took between one and\ntwo days to train on a single core of a modern CPU. As\na baseline for comparison, we also trained several n-\ngram models (with modiﬁed Kneser-Ney smoothing)\nusing the SRILM toolkit (Stolcke, 2002), obtaining\nresults similar to those reported by Zweig & Burges\n(2011).\nSince we selected hyperparameters based on the\n(Gutenberg) validation set perplexity, we report the\nscores on the entire collection of 1,040 sentences, which\nmeans that our results are directly comparable to those\nof Zweig & Burges (2011). As can be seen from Ta-\nA fast and simple algorithm for training neural probabilistic language models\nTable 3. Accuracy on the complete MSR Sentence Com-\npletion Challenge dataset. n × 2 indicates a bidirectional\ncontext. The LSA result is from (Zweig & Burges, 2011).\nModel Context Latent Percent Test\ntype size dim correct PPL\n3-gram 2 36.0 130.8\n4-gram 3 39.1 122.1\n5-gram 4 38.7 121.5\n6-gram 5 38.4 121.7\nLSA sentence 300 49\nLBL 2 100 41.5 145.5\nLBL 3 100 45.1 135.6\nLBL 5 100 49.3 129.8\nLBL 10 100 50.0 124.0\nLBL 5 200 50.8 123.6\nLBL 10 200 52.8 117.7\nLBL 10 300 54.7 116.4\nLBL 10×2 100 44.5 38.6\nLBL 10×2 200 49.8 33.6\nble 3, more word features and larger context leads to\nbetter performance in LBL models in terms of both ac-\ncuracy and perplexity. The LBL models perform con-\nsiderably better on sentence completion than n-gram\nmodels, in spite of having higher test perplexity. Even\nthe LBL model with a two-word context performs bet-\nter than any n-gram model. The LBL model with a\nﬁve-word context, matches the best published result\non the dataset. Note that the LSA model that pro-\nduced that result considered all words in a sentence,\nwhile an LBL model with a c-word contexts considers\nonly the 2 c words that surround the candidate word.\nThe model with a 10-word context and 300D feature\nvectors outperforms the LSA model by a large mar-\ngin and sets a new accuracy record for the dataset at\n54.7%.\nLanguage models typically use the words preceding\nthe word of interest as the context. However, since\nwe are interested in ﬁlling in a word in the middle of\nthe sentence, it makes sense to use both the preceding\nand the following words as the context for the lan-\nguage model, making the context bidirectional. We\ntrained several LBL models with bidirectional con-\ntext to see whether such models are superior to their\nunidirectional-context counterparts for sentence com-\npletion. Scoring a sentence with a bidirectional model\nis both simpler and faster: we simply compute the\nprobability of the candidate word under the model us-\ning the context surrounding the word. Thus a model\nis applied only once per sentence, instead of c+1 times\nrequired by the unidirectional models.\nAs Table 3 shows, the LBL models with bidirectional\ncontexts achieve much lower test perplexity than their\nunidirectional counterparts, which is not surprising be-\ncause they also condition on words that follow the\nword being predicted. What is surprising, however,\nis that bidirectional contexts appear to be consider-\nably less eﬀective for sentence completion than unidi-\nrectional contexts. Though the c-word context model\nand c×2-word context model look at the same words\nwhen using the scoring procedures we described above,\nthe unidirectional model seems to make better use of\nthe available information.\n6. Discussion\nWe have introduced a simple and eﬃcient method for\ntraining statistical language models based on noise-\ncontrastive estimation. Our results show that the\nlearning algorithm is very stable and can produce mod-\nels that perform as well as the ones trained using maxi-\nmum likelihood in less than one-tenth of the time. In a\nlarge-scale test of the approach, we trained several neu-\nral language models on a collection of Project Guten-\nberg texts, achieving state-of-the-art performance on\nthe Microsoft Research Sentence Completion Chal-\nlenge dataset.\nThough we have shown that the unigram noise distri-\nbution is suﬃcient for training neural language models\neﬃciently, context-dependent noise distributions are\nworth investigating because they might lead to even\nfaster training by reducing the number of noise sam-\nples needed.\nRecently, Pihlaja et al. (2010) introduced a family of\nestimation methods for unnormalized models that in-\ncludes NCE and importance sampling as special cases.\nOther members of this family might be of interest for\ntraining language models, though our preliminary re-\nsults suggest that none of them outperform NCE.\nFinally, we believe that NCE can be applied to many\nmodels other than neural or maximum-entropy lan-\nguage models. Probabilistic classiﬁers with many\nclasses are a prime candidate.\nAcknowledgments\nWe thank Vinayak Rao and Lloyd Elliot for their help-\nful comments. We thank the Gatsby Charitable Foun-\ndation for generous funding.\nReferences\nBengio, Yoshua and Sen´ ecal, Jean-S´ ebastien. Quick\ntraining of probabilistic neural nets by importance\nA fast and simple algorithm for training neural probabilistic language models\nsampling. In AISTATS’03, 2003.\nBengio, Yoshua and Sen´ ecal, Jean-S´ ebastien. Adap-\ntive importance sampling to accelerate training of a\nneural probabilistic language model. IEEE Trans-\nactions on Neural Networks , 19(4):713–722, 2008.\nBengio, Yoshua, Ducharme, R´ ejean, and Vincent, Pas-\ncal. A neural probabilistic language model. In NIPS,\npp. 932–938, 2000.\nBengio, Yoshua, Ducharme, Rejean, Vincent, Pascal,\nand Jauvin, Christian. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155, 2003.\nBerger, A.L., Pietra, V.J.D., and Pietra, S.A.D. A\nmaximum entropy approach to natural language\nprocessing. Computational linguistics , 22(1):39–71,\n1996.\nBird, S., Klein, E., and Loper, E. Natural language\nprocessing with Python. O’Reilly Media, 2009.\nCollobert, R. and Weston, J. A uniﬁed architecture\nfor natural language processing: Deep neural net-\nworks with multitask learning. In Proceedings of the\n25th International Conference on Machine Learn-\ning, 2008.\nGutmann, M. and Hyv¨ arinen, A. Noise-contrastive es-\ntimation: A new estimation principle for unnormal-\nized statistical models. In Proc. Int. Conf. on Ar-\ntiﬁcial Intelligence and Statistics (AISTATS2010) ,\n2010.\nGutmann, M.U. and Hyv¨ arinen, A. Noise-contrastive\nestimation of unnormalized statistical models, with\napplications to natural image statistics. Journal of\nMachine Learning Research, 13:307–361, 2012.\nMaas, A.L. and Ng, A.Y. A probabilistic model for\nsemantic word vectors. In NIPS 2010 Workshop on\nDeep Learning and Unsupervised Feature Learning ,\n2010.\nMikolov, T., Karaﬁ´ at, M., Burget, L., ˇCernock` y, J.,\nand Khudanpur, S. Recurrent neural network based\nlanguage model. In Eleventh Annual Conference of\nthe International Speech Communication Associa-\ntion, 2010.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L.,\nand ˇCernock` y, J. Empirical evaluation and combi-\nnation of advanced language modeling techniques.\nIn Proceedings of Interspeech, pp. 605–608, 2011.\nMnih, A. and Hinton, G. Three new graphical models\nfor statistical language modelling. Proceedings of the\n24th International Conference on Machine Learn-\ning, pp. 641–648, 2007.\nMnih, A., Yuecheng, Z., and Hinton, G. Improving a\nstatistical language model through non-linear pre-\ndiction. Neurocomputing, 72(7-9):1414–1418, 2009.\nMnih, Andriy and Hinton, Geoﬀrey. A scalable hier-\narchical distributed language model. In Advances in\nNeural Information Processing Systems, volume 21,\n2009.\nMorin, Frederic and Bengio, Yoshua. Hierarchical\nprobabilistic neural network language model. In\nAISTATS’05, pp. 246–252, 2005.\nPihlaja, M., Gutmann, M., and Hyv¨ arinen, A. A\nfamily of computationally eﬃcient and simple es-\ntimators for unnormalized statistical models. In\nProc. Conf. on Uncertainty in Artiﬁcial Intelligence\n(UAI2010)., 2010.\nSchwenk, Holger and Gauvain, Jean-Luc. Training\nneural network language models on very large cor-\npora. In Proceedings of Human Language Technology\nConference and Conference on Empirical Methods\nin Natural Language Processing, pp. 201–208, 2005.\nSocher, R., Lin, C.C., Ng, A.Y., and Manning, C.D.\nParsing natural scenes and natural language with\nrecursive neural networks. In International Confer-\nence on Machine Learning (ICML) , 2011.\nStolcke, A. SRILM – an extensible language modeling\ntoolkit. In Proceedings of the International Confer-\nence on Spoken Language Processing, volume 2, pp.\n901–904, 2002.\nTurian, J., Ratinov, L., and Bengio, Y. Word repre-\nsentations: A simple and general method for semi-\nsupervised learning. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational\nLinguistics, pp. 384–394, 2010.\nZweig, G. and Burges, C.J.C. The Microsoft Research\nSentence Completion Challenge. Technical Report\nMSR-TR-2011-129, Microsoft Research, 2011.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8501290082931519
    },
    {
      "name": "Treebank",
      "score": 0.7207548022270203
    },
    {
      "name": "Language model",
      "score": 0.6680951118469238
    },
    {
      "name": "Word (group theory)",
      "score": 0.5675445795059204
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5518989562988281
    },
    {
      "name": "Vocabulary",
      "score": 0.5240041017532349
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5190559029579163
    },
    {
      "name": "Noise (video)",
      "score": 0.5181695222854614
    },
    {
      "name": "Training (meteorology)",
      "score": 0.4824264943599701
    },
    {
      "name": "Scalability",
      "score": 0.4518151581287384
    },
    {
      "name": "Speech recognition",
      "score": 0.4363712966442108
    },
    {
      "name": "Sentence",
      "score": 0.4243202209472656
    },
    {
      "name": "Algorithm",
      "score": 0.4024180471897125
    },
    {
      "name": "Machine learning",
      "score": 0.35505348443984985
    },
    {
      "name": "Natural language processing",
      "score": 0.338478684425354
    },
    {
      "name": "Parsing",
      "score": 0.1427694857120514
    },
    {
      "name": "Mathematics",
      "score": 0.08219152688980103
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ]
}