{
  "title": "LaMemo: Language Modeling with Look-Ahead Memory",
  "url": "https://openalex.org/W4224251244",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5020700222",
      "name": "Haozhe Ji",
      "affiliations": [
        null,
        "NetEase (China)",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5018408488",
      "name": "Rongsheng Zhang",
      "affiliations": [
        null,
        "NetEase (China)",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100433327",
      "name": "Zhenyu Yang",
      "affiliations": [
        null,
        "NetEase (China)",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101779299",
      "name": "Zhipeng Hu",
      "affiliations": [
        null,
        "NetEase (China)",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5044042138",
      "name": "Minlie Huang",
      "affiliations": [
        null,
        "NetEase (China)",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3205226109",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2891815651",
    "https://openalex.org/W3093260394",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3204302053",
    "https://openalex.org/W3167303745",
    "https://openalex.org/W4306672396",
    "https://openalex.org/W2963951265"
  ],
  "abstract": "Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5747 - 5762\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLaMemo: Language Modeling with Look-Ahead Memory\nHaozhe Ji\n , Rongsheng Zhang\n , Zhenyu Yang\n , Zhipeng Hu\n , Minlie Huang\n ∗\nThe CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems,\nBeijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China\nFuxi AI Lab, NetEase Inc., China,\n Guangdong OPPO Mobile Telecommunications Corp., Ltd.\njhz20@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn\n{zhangrongsheng, zphu}@corp.netease.com, yangzhenyu@oppo.com\nAbstract\nAlthough Transformers with fully connected\nself-attentions are powerful to model long-term\ndependencies, they are struggling to scale to\nlong texts with thousands of words in language\nmodeling. One of the solutions is to equip the\nmodel with a recurrence memory. However, ex-\nisting approaches directly reuse hidden states\nfrom the previous segment that encodes con-\ntexts in a uni-directional way. As a result, this\nprohibits the memory to dynamically interact\nwith the current context that provides up-to-\ndate information for token prediction. To rem-\nedy this issue, we proposeLook-Ahead Memory\n(LaMemo)1 that enhances the recurrence mem-\nory by incrementally attending to the right-side\ntokens, and interpolating with the old mem-\nory states to maintain long-term information in\nthe history. LaMemo embraces bi-directional\nattention and segment recurrence with an addi-\ntional computation overhead only linearly pro-\nportional to the memory length. Experiments\non widely used language modeling benchmarks\ndemonstrate its superiority over the baselines\nequipped with different types of memory.2\n1 Introduction\nLanguage modeling is an important task that tests\nthe ability of modeling long-term dependencies by\npredicting the current token based on the previous\ncontext (Mikolov and Zweig, 2012; Merity et al.,\n2017). Recently, Transformer-based language mod-\nels achieved remarkable performance by enabling\ndirect interaction between long-distance word pairs.\nHowever, as the computation overhead grows with\nthe length of the input sequence, Transformers can\nonly process a fixed length segment at a time. To\nallow long-term information flow across individual\nsegments, existing approaches augment the model\n∗ Corresponding author\n1We are also inspired by the French word “La Mémoire”,\nmeaning “the memory”.\n2Source code available at https://github.com/\nthu-coai/LaMemo.\n-100 -90 -80 -70 -60 -50 -40 -30 -20 -10\nContext\n10□2\n10□1\nAttention\nLaMemo\nTransformer-XL\nFigure 1: Attention weights on the context (in log-scale)\nin the final layer of Transformer-XL and LaMemo av-\neraged on 15K tokens. Transformer-XL quickly loses\nattention to older contexts, while LaMemo maintains\nawareness to the history with the grow of the context\nlength.\nwith a recurrence memory that stores hidden states\ncomputed in previous time steps (Dai et al., 2019)\nand their compressions (Rae et al., 2020; Martins\net al., 2021) for the target tokens to attend to.\nOne limitation of this approach is that the recur-\nrence memory is only aware of older contexts since\nthey are previously computed to predict the next\nword from left to right. As a result, distant memory\nstates become outdated and less activated by the\ncurrent context, as illustrated in Figure 1. When\nhumans read or write a document, they maintain a\nmemory that records important information from\nthe past and often refresh them under the current\ncontext to keep it up-to-date.\nIn this paper, we propose Look-Ahead Memory\n(LaMemo) where memory states “look ahead” to\nfuture time steps by attending to the token represen-\ntations on their right side to provide up-to-date con-\ntextualization.3 To maintain information from the\nlong-term history, we propose memory interpola-\ntion to take both past and future tokens into consid-\neration, which mimics the bi-directional attention.\nNote that, directly applying bi-directional attention\nto update the memory representations brings an\nadditional complexity of O(M2) (M is the mem-\n3Note that the look-ahead attention does not exceed the\ncurrent step of the autoregressive model to prevent information\nleakage.\n5747\nory length). This is expensive when the memory is\nvery long. LaMemo incrementally attends to the\nright and accumulate the weighted attention sum\nfrom previous segments to simulate the full atten-\ntion in only O(M ×N) complexity (N is the tar-\nget sequence length), which does not increase the\nattention complexity of Transformer-XL, namely\nO(N2 + M ×N). We provide an illustration of\nthis mechanism in Figure 3.\nAnother technique proved to be effective in lan-\nguage modeling is the relative positional encod-\ning (Shaw et al., 2018; Huang et al., 2018; Dai\net al., 2019), which biases the pair-wise attention\nscore purely based on the relative distance of the\ntwo tokens. However its ability to generalize to\nthe attention of the future tokens remains unknown,\nsince both the distance and the direction need to\nbe taken into consideration. In preliminary experi-\nments, we observed the unstability of directly ap-\nplying the relative positional encoding of Dai et al.\n(2019) to this setting. We propose a simple yet ef-\nfective modification based on Dai et al. (2019) that\ndisentangles the bias of the relative distance and the\nattention direction which facilitates the training of\nLaMemo. We give both theoretical and empirical\nanalysis to the unstability issue and demonstrate the\neffectiveness of the proposed disentangled relative\npositional encoding method.\nTo sum up, our contributions are as follows:\n(1) We propose LaMemo, a memory mechanism\nthat incrementally attends to the right-side tokens,\nand interpolates with the old memory, which en-\nables bi-directional interaction with a complexity\nlinear in memory length.\n(2) We propose disentangled relative positional\nencoding, a simple yet effective solution that dis-\nentangles the relative distance and the attention\ndirection that can better generalize to the attention\nof the future tokens.\n(3) We conduct experiments on standard lan-\nguage modeling benchmarks and demonstrate\nLaMemo’s superiority over various baselines equp-\npied with different types of memory mechanisms,\ndespite some having an access to longer contexts.\nComprehensive comparisons show the benefits of\nlearning memory representations contextualized\nwith up-to-date information.\n2 Background\n2.1 Transformer for Language Modeling\nA Transformer (Vaswani et al., 2017) is composed\nof multiple layers of identical blocks, including a\nmulti-head self-attention (Bahdanau et al., 2015)\nthat calculates pair-wise token interaction and a\nfeed-foward layer for position-wise projection with\na non-linear activation. Both two modules are fol-\nlowed by residual connections (He et al., 2016) and\nlayer normalization (Ba et al., 2016) to facilitate\noptimization.\nGiven the input sequence representations of the\ncurrent τ-th segment Xτ = [xτ+1,··· ,xτ+N] ∈\nRN×d where N is the target sequence length and d\nis the hidden state size, they are first mapped into\nqueries Q, keys Kand values V by learned weight\nmatrix to compute self-attention:\nQτ = XτWq,Kτ = XτWk,Vτ = XτWv,\n(1)\nwhere Wq,Wk,Wv ∈Rd×d are learnable projec-\ntion matrices. To perform multi-head self-attention,\nQ,K,V are further split into H heads. For sim-\nplicity, we only consider the case of a single head.\nIn language modeling, the attention map is always\nadded by a causal mask to avoid information leak-\nage from the future when predicting the next token:\nC→\nτ = Causal-Attn(Qτ,Kτ,Vτ)\n= softmax\n(QτK⊤\nτ√\nd\n)\nVτ, (2)\nwhere softmax (·) masks position j > ifor the\ni-th row of the input matrix with−∞before taking\nthe softmax. The resulted context representations\nare concatenated and then projected to the final\noutputs Oτ ∈RN×d with a learnable projection\nmatrix Wo ∈ Rd×d. Finally, the self-attention\noutputs Oτ are added by the input representations\nXτ and fed to the following point-wise non-linear\ntransformation, denoted as f(·):\nf(x) = LN\n(\nFFN\n(\nLN(x)\n)\n+ LN(x)\n)\n, (3)\nwhere LN(·) is the layer normalization and FFN(·)\nis the feed-forward layer, both of which are applied\nto each row vector individually. The final output of\nthis Transformer layer is f(Oτ + Xτ).\nOutputs of the final layer are projected to the\nvocabulary to predict Pr(wt|w1,··· ,wt−1). The\njoint probability of predicting the whole segment\n5748\nPoint-wise Non-linear Transform \nCausal Self Attention\nK, V;\nQ\nFigure 2: The architecture of Transformer-XL augment-\ning with a recurrence memory.\nis the product of these conditional factors. The\nfinal objective is to maximize the following log-\nlikelihood:\nlog Pr(w) =\nN∏\nt=1\nlog Pr(wt|w1,··· ,wt−1). (4)\n2.2 Recurrence Memory Mechanism\nTo enable the Transformer to consider more con-\ntextual information from previous segments, Dai\net al. (2019) proposed to augment the Transformer\nwith a recurrence memory which stores the hidden\nstates of previous time steps as extended keys and\nvalues, as shown in Figure 2. Concretely, let us\nconsider a memory length of M and memory repre-\nsentations Xτ−1 = [xτ−M+1,··· ,xτ] ∈RM×d.\nThe extended key and value matrices are obtained\nby prepend Xτ−1 to Xτ before projection:\n˜X\nsg\nτ = [sg(Xτ−1) ◦Xτ] ∈R(M+N)×d, (5)\nwhere sg(·) stands for stop-gradient which disables\ngradient propagation to previous segments, and\n[·◦·] indicates concatenation of hidden states along\nthe length dimension. Extended by the recurrence\nmemory, each query vector can consider contexts\neven beyond the total context length of the atten-\ntion M + N. As illustrated by Dai et al. (2019),\nthe effective context length grows linearly to the\nnumber of layers and the attention context length\ndue to layer-wise reusing.\nAnother technique necessary to the recurrence\nmemory is the relative positional encodings. By\nconsidering only the relative distance between\ntwo tokens when computing the attention score,\nit avoids temporal confusion caused by indexing\nthe same position across segments and injects use-\nful relative bias. Transformer-XL uses the fixed\nsinusoidal encoding matrix (Vaswani et al., 2017)\nto provide relative distance bias and learns global\nCurrent segment(N = 1)Memory(M = 2)\nFigure 3: Illustration of LaMemo with a memory length\nM = 2 and a target sequence length N = 1 for clarity.\nSolid lines stand for the attention connections computed\nat this iteration while dashed lines represent the previ-\nously computed attention.\nbias terms shared across different layers, which can\nextrapolate to longer contexts with a great reduc-\ntion of parameters compared to Shaw et al. (2018):\nAxl\ni,j = X⊤\ni W⊤\nq WE\nkXj + X⊤\ni W⊤\nq WR\nkRi−j\n+ u⊤WE\nkXj + v⊤WR\nkRi−j, (6)\nwhere Ris the sinusoid encoding matrix, u,vare\nlearnable weight vectors governing the global con-\ntent and position bias, and WE\nk,WR\nk are separate\nkey projection matrices for the content and position\nrespectively.\n3 Method\nIn this section, we describe our method in detail\nwith our motivation to learn better representations\nfor the memory.\n3.1 Look-Ahead Attention\nHuman language is sequential with one word\nfollowing another, but humans process informa-\ntion usually in a non-sequential way and re-\ncontextualize certain contents for several times. For\nexample, when countering complicated contents\nduring reading, humans usually first store them\ntemporarily in the memory and continue to scan for\nrelevant information if any, and revisit those old\ncontents to refresh their meaning quite often. This\ndynamic memory refreshing mechanism enables us\nto thoroughly understand the passage under current\ncontexts.\nExisting recurrence memory however, lacks this\ndynamic contextualization ability. As the represen-\ntations in the recurrence memory are previously\ncomputed conditioned on their past, they are not\naware of the current contexts which provide more\n5749\nrelevant information for the current token predic-\ntion.\nTo address this limitation, we propose a look-\nahead attention that allow the memory to attend to\nthe contexts on their right. Formally, we reuse the\nnotation Xτ = [ xτ+1,··· ,xτ+N] ∈RN×d for\nthe representations of the current target sequence\nand Xτ−1 = [xτ−M+1,··· ,xτ] ∈RM×d for the\nrepresentations of the memory.\nLet us consider the i-th position of the memory\nXτ−1, xi can attend to position xj on its right\n(j > i) without causing information leakage as\nlong as j ≤τ+1. Though appealing, this naïve ap-\nproach requires to calculate an M by M attention\nmap, which would become inefficient and redun-\ndant when M is significantly greater than N. Ac-\ntually, since the target segment moves forwardN\npositions at each iteration, we devise an incremen-\ntal manner of look-ahead attention computation\nthat only requires the newest N positions on the\nright as key-value pairs.\n˜Xτ−1 = [xτ−N+2,··· ,xτ+1] ∈RN×d. (7)\nThen the look-ahead attention results computed\npreviously can be effectively reused and interpo-\nlated with the current ones (§3.2). Concretely, we\nformalize the look-ahead attention as follows:\n˜Kτ−1 = ˜Xτ−1Wk, ˜Vτ−1 = ˜Xτ−1Wv, (8)\nC←\nτ−1 = LookAhead-Attn(Qτ−1, ˜Kτ−1, ˜Vτ−1)\n= softmax\n(Qτ−1 ˜K\n⊤\nτ−1√\nd\n)\n˜Vτ−1, (9)\nwhere softmax (·) masks position j ≤ifor the i-\nth row of the input matrix with−∞before softmax.\nQτ−1 is obtained by Eq. (1), and the projection\nmatrices of query, key and value are all shared with\nthe causal attention. We illustrate this in Figure\n3 where the look-ahead attention (yello paths) in-\ncreases the attention window of each memory state\nto M tokens on its right.\n3.2 Memory Interpolation\nTo save computations for looking-ahead and effec-\ntively reuse the attention results of the past, we\npropose memory interpolation that smoothly inter-\npolates attention results from both the future and\nthe past to provide bi-directional contextualization.\nRecall that in the previous iteration, we have\ncalculated the causal context representations C→\nτ−1\nof Xτ−1 using Eq. 2, where each row is a linear\nCausal Self-Attention\nK, V\n;\nQLook-Ahead AttentionQ\nK, V\nInterpolation\nPoint-wise Non-linear Transform Concatenation\n<latexit sha1_base64=\"vByLmXaQ1xzxbbgYN6bl5Qbtacg=\">AAACAHicbVC7TsMwFHXKq5RXgIGBxaJCYqFKEAjGChbGIlFaqYkix3Faq44d2Q5SFWXhV1gYQIiVz2Djb3DaDNByJMtH59yre+8JU0aVdpxvq7a0vLK6Vl9vbGxube/Yu3sPSmQSky4WTMh+iBRhlJOuppqRfioJSkJGeuH4pvR7j0QqKvi9nqTET9CQ05hipI0U2AdeKFikJon58n4R5J5G2albBHbTaTlTwEXiVqQJKnQC+8uLBM4SwjVmSKmB66Taz5HUFDNSNLxMkRThMRqSgaEcJUT5+fSAAh4bJYKxkOZxDafq744cJarc0VQmSI/UvFeK/3mDTMdXfk55mmnC8WxQnDGoBSzTgBGVBGs2MQRhSc2uEI+QRFibzBomBHf+5EXycNZyL1rO3XmzfV3FUQeH4AicABdcgja4BR3QBRgU4Bm8gjfryXqx3q2PWWnNqnr2wR9Ynz9DYJbT</latexit>\nX ⌧ \u0000 1\n<latexit sha1_base64=\"u0aZsjC242G2XG3atzY8CaEW+NY=\">AAAB/nicbVDNS8MwHE3n15xfVfHkJTgET6MVRY9DLx4nuA9YS0nTdAtLk5KkwigF/xUvHhTx6t/hzf/GdOtBNx+EPN77/cjLC1NGlXacb6u2srq2vlHfbGxt7+zu2fsHPSUyiUkXCybkIESKMMpJV1PNyCCVBCUhI/1wclv6/UciFRX8QU9T4idoxGlMMdJGCuwjLxQsUtPEXPmgCHJPo6wI7KbTcmaAy8StSBNU6AT2lxcJnCWEa8yQUkPXSbWfI6kpZqRoeJkiKcITNCJDQzlKiPLzWfwCnholgrGQ5nANZ+rvjRwlqkxoJhOkx2rRK8X/vGGm42s/pzzNNOF4/lCcMagFLLuAEZUEazY1BGFJTVaIx0girE1jDVOCu/jlZdI7b7mXLef+otm+qeqog2NwAs6AC65AG9yBDugCDHLwDF7Bm/VkvVjv1sd8tGZVO4fgD6zPH1kwlmE=</latexit>\nX ⌧\n<latexit sha1_base64=\"tvIeHX8/88A1xKzpTP8zZx/mlsw=\">AAACDXicbVC7TsMwFHV4lvIKMLJEFCQWqgSBYKzowlgk+pCaUDmu01p14si+AVVRfoCFX2FhACFWdjb+BqfNAC1Hsnx0zr269x4/5kyBbX8bC4tLyyurpbXy+sbm1ra5s9tSIpGENongQnZ8rChnEW0CA047saQ49Dlt+6N67rfvqVRMRLcwjqkX4kHEAkYwaKlnHrq+4H01DvWX1rO71OU0ACyleMh6qQs4OXGynlmxq/YE1jxxClJBBRo988vtC5KENALCsVJdx47BS7EERjjNym6iaIzJCA9oV9MIh1R56eSazDrSSt8KhNQvAmui/u5IcajyhXVliGGoZr1c/M/rJhBceimL4gRoRKaDgoRbIKw8GqvPJCXAx5pgIpne1SJDLDEBHWBZh+DMnjxPWqdV57xq35xValdFHCW0jw7QMXLQBaqha9RATUTQI3pGr+jNeDJejHfjY1q6YBQ9e+gPjM8fBtOcyA==</latexit>\nC  \n⌧ \u0000 1\n<latexit sha1_base64=\"0vqMvbQgXjhN1UGpR51SqHRNkgw=\">AAACDnicbVC7TsMwFHV4lvIqMLJEVJVYqBIEgrGiC2OR6ENqQuS4bmvVsSP7BlRF+QIWfoWFAYRYmdn4G5y2A7QcyfLROffq3nvCmDMNjvNtLS2vrK6tFzaKm1vbO7ulvf2WlokitEkkl6oTYk05E7QJDDjtxIriKOS0HY7qud++p0ozKW5hHFM/wgPB+oxgMFJQqnih5D09jsyX1rO71FNsMASslHzIgtQDnJy4WVAqO1VnAnuRuDNSRjM0gtKX15MkiagAwrHWXdeJwU+xAkY4zYpeommMyQgPaNdQgSOq/XRyTmZXjNKz+1KZJ8CeqL87UhzpfGNTGWEY6nkvF//zugn0L/2UiTgBKsh0UD/hNkg7z8buMUUJ8LEhmChmdrXJECtMwCRYNCG48ycvktZp1T2vOjdn5drVLI4COkRH6Bi56ALV0DVqoCYi6BE9o1f0Zj1ZL9a79TEtXbJmPQfoD6zPH+ntnUU=</latexit>\nC !\n⌧ \u0000 1\n<latexit sha1_base64=\"AMWyfikyhfQLfsNMQnSCHvuMo9A=\">AAACDHicbVC7TsMwFHV4lvIqMLJYVEhMVYJAMFZ0YSwSfUhNqBzHaa06cWTfgKooH8DCr7AwgBArH8DG3+C0HaDlSpaPzjlX997jJ4JrsO1va2l5ZXVtvbRR3tza3tmt7O23tUwVZS0qhVRdn2gmeMxawEGwbqIYiXzBOv6oUeide6Y0l/EtjBPmRWQQ85BTAobqV6quL0Wgx5H5skZ+l7mKD4ZAlJIPeT9zgaS5cdk1e1J4ETgzUEWzavYrX24gaRqxGKggWvccOwEvIwo4FSwvu6lmCaEjMmA9A2MSMe1lk2NyfGyYAIdSmRcDnrC/OzIS6WJf44wIDPW8VpD/ab0Uwksv43GSAovpdFCYCgwSF8nggCtGQYwNIFRxsyumQ6IIBZNf2YTgzJ+8CNqnNee8Zt+cVetXszhK6BAdoRPkoAtUR9eoiVqIokf0jF7Rm/VkvVjv1sfUumTNeg7Qn7I+fwDzuZzT</latexit>\nC !\n⌧\nFigure 4: The architecture of LaMemo with look-ahead\nattention and memory interpolation that refresh the\nmemory dynamically with both the current contexts\nand the long-term history.\ncombination of the weighted token representations\nof the previous tokens. In Sec. 3.1, we describe\nthe look-ahead attention which enables Xτ−1 to\nattend to the contexts on their right and computes\nC←\nτ−1 using Eq. 9. Here, we formulate the memory\ninterpolation as the interpolation between the old\nrepresentations C→\nτ−1 and the new ones C←\nτ−1 with\na coefficient vector ατ−1 ∈RM controlling the\nmemorization of the past activations:\nC↔\nτ−1 = Mem-Interp(C→\nτ−1,C←\nτ−1,ατ−1)\n= ατ−1sg(C→\nτ−1) + (1−ατ−1)C←\nτ−1.\n(10)\nThe resulted C↔\nτ−1 which attend to contexts from\nboth directions, are further fed to the non-linear\ntransformation defined in Eq. 3 to update represen-\ntations in higher layers.\nFor ατ−1, we define it to be the sum of the nor-\nmalized attention weights on the previous tokens\nwhen calculating C→\nτ−1 (Eq. 2):\nατ−1 = sg(s→\nτ−1)\nsg(s→\nτ−1) + s←\nτ−1 + ε, (11)\nwhere s→\nτ−1 is the sum of the unnormalized atten-\ntion score of C→\nτ−1, which is the denominator of\nthe softmax in Eq. 2. Similarly, s←\nτ−1 is the denom-\ninator of the softmax in Eq. 9. εis a small value\nto prevent zero division error in practice. Then Eq.\n10 can be derived into a form that resembles the\nbi-directional attention with the queries attending\nto positions on both sides4 (Appendix A). Figure 4\nshows the architecture of LaMemo.\n4Note that the query vectors for the past and the future\nare under different contextualization in higher layers of the\nmodel.\n5750\nNote that the difference between the hidden state\nreuse in the recurrence memory and our memory\ninterpolation is that they simply reuse the static\nrepresentations to extend the contexts for attention\nwhile we update the memory representations by\naggregating weighted attention sum of the history\nwithout the need to recompute them.\n3.3 Disentangled Relative Positional\nEncodings\nAs the look-ahead attention allows the memory\nto attend to future tokens on its right, we need a\nrelative positional encoding scheme that can gen-\neralize to this setting. We start by considering the\nrelative positional encoding in Transformer-XL,\nas described by Eq. 6. When the i-th query vec-\ntor attending to a position j = i+ ∆ > i, we\nhave Ri−j = R−∆. As defined by Vaswani et al.\n(2017), R∆ ∈RD is composed of sine and cosine\nfunctions with different frequencies. Since the sine\nfunction is odd, sin(−ω∆) = −sin(ω∆), we have\nR−∆ ̸= R∆ so that it can represent attention in\ndifferent directions (±sign of ∆) with the same\nrelative distance (absolute value of ∆).\nHowever, this approach solely relies on the fixed\nsinusoid encodings to represent the relative dis-\ntance and the attention direction. We argue that\ndisentangling them is more effective in capturing\nthese two types of temporal biases and also miti-\ngates the numerical unstability issue. Specifically,\nwe propose to learn two direction-aware global po-\nsition biases to parameterize the sign and query R\nwith the absolute value of the relative distance:\nAdis\ni,j = X⊤\ni W⊤\nq WE\nkXj + X⊤\ni W⊤\nq WR\nkR|i−j|\n+ u⊤WE\nkXj + v⊤\ni−jWR\nkR|i−j|, (12)\nwhere vi−j = v+ if i ≥j else v−. The global\npositional bias now explicitly separates the contri-\nbutions of sgn(i−j) and |i−j|, which can better\ngeneralize to long distance in both forward and\nbackward directions.\nTo illustrate the numerical unstability caused by\nadapting Eq. 6 to j > i, we derive the variance\nof the dot product xTRi−j where xis a random\nvector. We show that the variance undergoes an\noscillation and cannot be properly bounded every-\nwhere when ishifts from i≥jto i<j . Detailed\nanalysis are presented in Appendix B.\n4 Experiments\nWe evaluate LaMemo on both word-level and\ncharacter-level language modeling tasks and com-\npare with existing Transformer baselines aug-\nmented with different types of memory.\n4.1 Datasets and Metrics\nFor word-level language modeling task, we con-\nsider Wikitext-103 (Merity et al., 2017), which is\nthe most widely used word-level language model-\ning benchmark. It contains 103 million tokens for\ntraining from 28 thousand wikipedia articles, with\nan average length of 3.6 thousand tokens per arti-\ncle and a vocabulary size around 260K. We report\nperplexity (ppl) on the dev and test set.\nWe also evaluate on two character-level language\nmodeling benchmarks enwik8 and text8 (Ma-\nhoney, 2011). Both datasets contain 100 million\nWikipedia characters. While enwik8 is unpro-\ncessed, text8 is preprocessed by case lowering and\nfiltering to include only 26 letters from a to z and\nspace. On both datasets, we report bit per character\n(bpc) on the dev and test set.\n4.2 Baselines\nTo directly compare with different types of memory,\nwe consider Transformer-XL and its variations with\nthe same model architecture but different memory\nmechanism.\nTransformer+RPE is the vanilla Trans-\nformer (Vaswani et al., 2017) that uses relative\npositional encodings from Dai et al. (2019) but\ndoes not extend the context with additional\nmemory.\nTransformer-XL (Dai et al., 2019) is a Trans-\nformer model equipped with relative positional en-\ncodings and a recurrence memory comprised of\nhidden states computed in previous time steps to\nextend the context length of the attention.\nCompressive Transformer (Rae et al., 2020)\nextends Transformer-XL with an external compres-\nsive memory that stores compressed hidden states\nat the temporal level using convolutional networks.\n∞-former (Martins et al., 2021) uses continuous\nspace attention to attend over the external memory\nwhich consists of continuous signals. They also\nupdated the external memory with recent hidden\nstates to enable unbounded memory capacity.\n5751\nModel #Params Mem size Ext mem size #FLOPS dev ppl test ppl\nTransformer+RPE 151M 0 0 148M 28.11 29.14\nTransformer-XL (Dai et al., 2019) 151M 150 0 157M 23.42 24.56\nCompressive Transformer (Rae et al., 2020) 161M 150 150 169M - 24.41\n∞-former (Martins et al., 2021) 160M 150 150 235M - 24.22\nLaMemo 151M 150 0 191M 22.98 23.77\nTable 1: Word-level language modeling results on Wikitext-103. We report ppl (perplexity) on dev and test set. We\nalso report the number of parameters, memory size, external memory size, and the number of FLOPS (floating-point\noperations) for computing one step prediction on average.\n4.3 Implementation Details\nWe follow the standard architecture of the\nTransformer-XL (Dai et al., 2019) that has differ-\nent configurations for different tasks. Specifically,\non Wikitext-103, we use a 16-layer Transformer\nwith 10 attention heads and head dimension 41\nequipped with adaptive embeddings (Baevski and\nAuli, 2019). We control the target sequence length\nto be 150 and the memory length 150 for all mod-\nels following the setting of Dai et al. (2019). For\nthe Compressive Transformer and ∞-former, we\nadditionally use an external memory of size 150 fol-\nlowing the setting of Martins et al. (2021).5 On the\ntext8 and enwik8 datasets, we use a 12-layer Trans-\nformer with 8 heads and head dimension 64. The\nlength of the target sequence and the recurrence\nmemory are both set to 512. In the main results\nwe use the identical evaluation setting to the train-\ning phase on all datasets and do not use a longer\nmemory. We use the Pytorch framework (Paszke\net al., 2019) and Apex for mixed-precision training.\nIn practice, we found that calculating the expo-\nnentials (§3.2) may lead to numerical overflow in\nmixed-precision mode, so we compute the loga-\nrithm of the exponential sum using logsumexp\nand logaddexp operator. Further details of the\ndataset and the hyperparameter settings are de-\nscribed in the Appendix C.\n4.4 Main Results\nWe show the results of word-level language mod-\neling benchmark Wikitext-103 in Table 1. We first\nobserve that all the models extended with memo-\nries significantly outperforms Transformer+RPE.\nUnder the same memory length, LaMemo outper-\nforms Transformer-XL with a clear margin, which\ndemonstrates the effectiveness of learning dynamic\nmemory representations over static ones. When\n5The external memory consists of 150 compressed vectors\nfor Compressive Transformer, and 150 radial basis functions\nfor ∞-former respectively.\nModel dev bpc test bpc\nDataset: text8\nTransformer+RPE 1.232 1.303\nTransformer-XL (Dai et al., 2019) 1.172 1.239\nLaMemo 1.128 1.196\nDataset: enwik8\nTransformer+RPE 1.253 1.240\nTransformer-XL (Dai et al., 2019) 1.150 1.128\nLaMemo 1.129 1.107\nTable 2: Character-level language modeling results on\ntext8 and enwik8. We report bpc ( bits-per-character)\non the dev and test set.\ncompared to the compressive memory and the un-\nbounded memory that take longer contexts into\naccount, LaMemo still achieves lower perplexity.\nThis indicates that the look-ahead memory allows\nthe language model to exploit the recent contexts\nto gain performance, while simply increasing the\ncontext length yields marginal improvement. This\nis in accordance with previous findings of how lan-\nguage models utilize contexts (Khandelwal et al.,\n2018; Sun et al., 2021). In terms of the parameters,\nLaMemo has the same number of parameters as\nthe Transformer-XL while other baselines use ad-\nditional parameters in CNN to compress or smooth\nthe hidden states. Lastly, we show the number of\nFLOPS necessary for computing one step predic-\ntion. ∞-former has the highest number of FLOPS\nfor resampling enough points from the continu-\nous signal to update the memory using smoothing\ntechniques. LaMemo also incurs additional com-\nputations to re-contextualize the memory under the\ncurrent context. Note that although the Compres-\nsive Transformer has lower number of FLOPS than\nLaMemo, it has an external memory that consumes\nmore GPU memory.\nWe also present the results of character-level lan-\nguage modeling on text8 and enwik8 datasets in\nTable 2. We observe similar trends as the results on\n5752\nConfiguration Encoding dev ppl test ppl\nFull Ours 22.98 23.77\nw/o mem interp Ours 23.67 24.90\nw/o look-ahead Ours 23.42 24.56\nFull Dai et al. (2019) FAIL FAIL\nTable 3: Ablation study on Wikitext-103. We investigate\nthree model configurations and two encoding schemes.\nthe word-level benchmark, where LaMemo outper-\nforms Transformer-XL by 0.04 on text8 and 0.02\non enwik8 with the same context length. Addition-\nally, we observe that all models exhibit overfitting\non text8, which might be caused by the extremely\nsmall vocabulary size of the dataset.\n4.5 Ablation Study\nWe conduct ablation studies on Wikitext-103 to\nexamine the effects of the proposed techniques, i.e.,\nlook-ahead attention, memory interpolation, and\ndisentangled relative positional encodings.\nWe use the same model achitecture and the same\ntarget and memory length as the main results. We\nfirst study three configurations, including (1) using\nthe Full model setting, (2) ablating the memory\ninterpolation module (w/o mem interp), i.e., set\nthe memorizing coeffecient ατ−1 = 0 , and (3)\nablating the look-ahead attention (w/o look-ahead),\ni.e., only use the causal context representations\nC→\nτ−1 in each layer. As shown in the First three\nrows in Table 3, both the memory interpolation\nand the look-ahead attention are indispensible for\nachieving the best performance. Additionaly, we\nfound that cancelling out memory interpolation\nleads to a worse performance, which indicates that\nthe distant past still provides additional information\nbeyond the current context.\nThe second study targets at studying different en-\ncoding schemes. We substitute our encodings with\nthe RPE of Transformer-XL Dai et al. (2019) and\nrun multiple experiments with 3 different random\nseeds, but all the models fail to converge. We plot\nthe training curves using two encodings in Figure\n8 in Appendix B, where we observe that our dis-\nentangled RPE is more stable during training and\nachieves lower perplexity.\n5 Extrapolating to Longer Contexts\nIn this section, we extrapolate the models to longer\ncontexts during inference to study the effect of\ndynamic contextualization to the distant past.\n2 4 6 8 10\nm\n23\n24\n25\n26Perplexity\nLaMemo\nTransformer-XL\nFigure 5: Test perplexity of LaMemo and Transformer-\nXL when extrapolating to longer contexts during infer-\nence, where mis the ratio of the memory length to the\ntarget length.\n0.5\n1.0\nlayer 15\n0.5\n1.0\nlayer 13\n0.5\n1.0\nlayer 11\n0.5\n1.0\nlayer 9\n0.5\n1.0\nlayer 7\n0.5\n1.0\nlayer 5\n0.5\n1.0\nlayer 3\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150\nMemory index\n0.5\n1.0\nlayer 1\nα\nFigure 6: The memorizing coefficient αof different lay-\ners in a 16-layer model with a same memory and target\nlength of 150. Smaller index means older memory.\nWe fix the length of the target sequence to\n64 and extrapolate the trained models to longer\nmemory length 64 ×mduring inference, where\nm = 1 ,··· ,10. We compare the perplexity of\nLaMemo and Transformer-XL trained on Wikitext-\n103 when augmented by a memory with different\nlength. As shown in Figure 5, LaMemo consis-\ntently achieves lower perplexity than Transformer-\nXL when extraploating to longer contexts, while\nthe performance of both models saturate when m\nis over 7. Additionally, we observe that the gap\nof perplexity between the two models increases\nwhen taking longer contexts into account. This\ndemonstrates the effectiveness of dynamically re-\nfreshing the distant memory representations under\nthe current context.\n5753\n6 Attention Analysis\nIn this section, we analyze the attention distribution\nof LaMemo to validate the effectiveness of utilizing\nbi-directional contexts with look-ahead attention.\nWe first visualize the memorizing coefficientα\nwhich stands for the portion of the past activations\nin the current memory representations. As show\nin Figure 6, we plot αin different layers as a func-\ntion of the memory index averaged on 100 text\nsegments.6 We observe that in lower layers the\nmemory mainly attends to the past (α≈1.0). We\nconjecture that long-term bi-directionality is not\nnecessary for low-level representations such as lex-\nical features. In higher layers, the memory sub-\nstantially utilizes the future contents to refresh the\nhigh-level representations, especially for the old\nmemory state with a small memory index.\nNext, we visualize the attention weight distribu-\ntion on the context tokens when predicting each\ntarget token in Figure 1. For every token, we take\nthe maximal attention weight in each interval of\n5 tokens on its left and scale to a context length\nof 100. The result indicates that LaMemo learns\nbetter memory represetations by attending to the\nright-side tokens, which increases the memory uti-\nlization when predicting the target token.\n7 Case Study\nWe present the generated texts of LaMemo and\nTransformer-XL trained on Wikitext-103 in Ap-\npendix D. Both models maintain a memory size\nof 512, and we seed them with the same context\nrandomly sampled from the test set and generate\n256 tokens using top-p sampling (Holtzman et al.,\n2020) with p= 0.95.\n8 Related Work\nThe Transformer (Vaswani et al., 2017), with its\npair-wise modeling ability of the input, becomes\nprevailing for sequence modeling, especially long\nsequence processing tasks, such as long text gener-\nation (Tan et al., 2021; Ji and Huang, 2021), long\ndocument QA (Beltagy et al., 2020; Ainslie et al.,\n2020), language modeling (Dai et al., 2019; Rae\net al., 2020), video processing (Wu et al., 2019),\nand etc. Specifically, language modeling (Merity\net al., 2017) which requires processing documents\nwith thousands of tokens has become a natural\n6Due to the space limit, we only sample 8 layers from all\nthe 16 layers.\ntestbed for benchmarking this long-term process-\ning ability. However, due to the quadratic time and\nspace complexity of self-attention, scaling to in-\nputs with thousands of tokens is computationally\nprohibitive.\nOne line of work investigated the linear-time\nattention mechanism to mitigate the scability is-\nsue of Transformer. Linformer (Wang et al., 2020)\nprojects the inputs to lower dimension in length\nand approximates the full attention with a low-rank\nfactorization. Linear Transformer (Katharopoulos\net al., 2020) regards the self-attention as a kernel\nfunction and uses a linear dot-product as a substi-\ntute. Choromanski et al. (2021) and Peng et al.\n(2021) proposed to approximate the softmax more\nprecisely with the expectation of the dot-product\nof random features. Although achieving substan-\ntial improvements on benchmarks designated for\nlong inputs (Tay et al., 2021). These methods, how-\never, focus on approximating the full attention with\nlow-rank factorizations or kernel functions, which\ncompromise the expressiveness and robustness of\nthe original softmax attention, are reported to be\ninferior to the simple local attentions on real world\nlanguage processing tasks (Xiong et al., 2021).\nOur work falls in another line, which aug-\nments the Transformer with a parametrized mem-\nory to store critical history information. Memory-\naugmented networks (Graves et al., 2014; Weston\net al., 2015; Sukhbaatar et al., 2015) have been stud-\nied in the context of recurrent neural networks for\na long time, but are mostly restricted to small and\nsynthetic datasets. With the rapid development of\nTransformer, various works start to adapt memories\nto this architecture.\nDai et al. (2019) first extended Transformer with\na recurrence memory that caches hidden states com-\nputed in previous steps for the target tokens to at-\ntend to. Rae et al. (2020) further extended the\ncontext with an external memory that stores com-\npressed hidden states at the temporal level. Martins\net al. (2021) used continuous space attention to\nattend over the old history and updated the mem-\nory with recent hidden states to enable unbounded\nmemory capacity. Wu et al. (2021) proposed to\nuse the encoder-decoder architecture to encode the\nmemory states with previous text segments and\npass this memory to future time steps. Instead of\nusing a fixed-size attention span for different layers,\nSukhbaatar et al. (2019) and Correia et al. (2019)\nproposed to learn dynamic attention spans for dif-\n5754\nferent attention heads, which greatly reduced the\ncomputations. These works focused on enabling\nthe Transformer to access contents in long distance,\nbut did not consider to learn better memory repre-\nsentations by refreshing the old memory under the\ncurrent context. Our work is orthogonal to learning\nadaptive attention spans and can be combined with\nthis technique to reduce the complexity.\n9 Conclusion\nWe present LaMemo, a memory mechanism that\nallows the memory states to incrementally attend\nto the right-side tokens and interpolates with the\nold memory states on the left side, which enables\nthe memory to interact with bi-directional contexts\nwith a complexity linear in memory length. Experi-\nments on three language modeling datasets demon-\nstrate the superiority of LaMemo over baselines\nwith various types of memory mechanisms. We\nalso found that LaMemo increases the utilization of\nolder memory states when predicting the target to-\nkens, and yields a higher performance boost when\nextrapolating to longer memory length, which in-\ndicates the effectiveness of recontextualizing the\nmemory under the current context.\nAcknowledgments\nThis work was supported by the National Science\nFoundation for Distinguished Young Scholars (with\nNo. 62125604) and the NSFC projects (Key project\nwith No. 61936010 and regular project with No.\n61876096). This work was also supported by the\nGuoqiang Institute of Tsinghua University, with\nGrant No. 2019GQG1 and 2020GQG0005. This\nwork was also sponsored by Tsinghua-Toyota Joint\nResearch Fund.\nReferences\nJoshua Ainslie, Santiago Ontañón, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: encoding long and structured inputs in\ntransformers. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 268–284. Association for Computational Lin-\nguistics.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016. Layer normalization. CoRR,\nabs/1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. CoRR,\nabs/2004.05150.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamás\nSarlós, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J. Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nGonçalo M. Correia, Vlad Niculae, and André F. T.\nMartins. 2019. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 2174–2184.\nAssociation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988. Association for Computational Linguis-\ntics.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas,\nNV , USA, June 27-30, 2016, pages 770–778. IEEE\nComputer Society.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\nreit, Noam Shazeer, Curtis Hawthorne, Andrew M.\nDai, Matthew D. Hoffman, and Douglas Eck. 2018.\nAn improved relative self-attention mechanism for\ntransformer with application to music generation.\nCoRR, abs/1809.04281.\n5755\nHaozhe Ji and Minlie Huang. 2021. Discodvt: Gener-\nating long text with discourse-aware discrete varia-\ntional transformer. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 7-11 November, 2021 ,\npages 4208–4224. Association for Computational\nLinguistics.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research , pages 5156–5165.\nPMLR.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky.\n2018. Sharp nearby, fuzzy far away: How neural lan-\nguage models use context. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 284–294.\nAssociation for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMatt Mahoney. 2011. Large text compression bench-\nmark.\nPedro Henrique Martins, Zita Marinho, and André F. T.\nMartins. 2021. ∞-former: Infinite memory trans-\nformer. CoRR, abs/2109.00301.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nTomás Mikolov and Geoffrey Zweig. 2012. Context de-\npendent recurrent neural network language model. In\n2012 IEEE Spoken Language Technology Workshop\n(SLT), Miami, FL, USA, December 2-5, 2012, pages\n234–239. IEEE.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Informa-\ntion Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada, pages\n8024–8035.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah A. Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In 9th International Con-\nference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nChloe Hillier, and Timothy P. Lillicrap. 2020. Com-\npressive transformers for long-range sequence mod-\nelling. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT, New Orleans, Louisiana, USA, June\n1-6, 2018, Volume 2 (Short Papers), pages 464–468.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive atten-\ntion span in transformers. In Proceedings of the 57th\nConference of the Association for Computational Lin-\nguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers , pages 331–335.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 2440–2448.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\nguage models actually use long-range context? In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Repub-\nlic, 7-11 November, 2021, pages 807–822. Associa-\ntion for Computational Linguistics.\nBowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric P.\nXing, and Zhiting Hu. 2021. Progressive generation\nof long text with pretrained language models. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n4313–4324. Association for Computational Linguis-\ntics.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transformers.\nIn 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n5756\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. CoRR, abs/2006.04768.\nJason Weston, Sumit Chopra, and Antoine Bordes. 2015.\nMemory networks. In 3rd International Conference\non Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceed-\nings.\nChao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan,\nKaiming He, Philipp Krähenbühl, and Ross B. Gir-\nshick. 2019. Long-term feature banks for detailed\nvideo understanding. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, pages 284–\n293. Computer Vision Foundation / IEEE.\nQingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu.\n2021. Memformer: The memory-augmented trans-\nformer.\nWenhan Xiong, Barlas Oguz, Anchit Gupta, Xilun\nChen, Diana Liskovich, Omer Levy, Wen-tau Yih,\nand Yashar Mehdad. 2021. Simple local attentions\nremain competitive for long-context tasks. CoRR,\nabs/2112.07210.\n5757\nA Derivation of Memory Interpolation\nWe derive Eq. 10 into the form of standard self-\nattention in the following:\nC↔\nτ−1 = ατ−1sg(C→\nτ−1) + (1−ατ−1)C←\nτ−1.\nWe consider the i-th row of C↔\nτ−1, denoted as c↔\ni .\nWe omit the stop-grad operation sg(·) and substi-\ntute αwith the result from Eq. 11:\nc↔\ni = αic→\ni + (1 −αi)c←\ni\n= s→\ni\ns→\ni + s←\ni\nc→\ni + s←\ni\ns→\ni + s←\ni\nc←\ni ,\nwhere s→\ni , s←\ni is the denominator of the softmax\nwhen computing c→\ni , c←\ni respectively:\ns→\ni =\n∑\nj≤i\nexp\n(q′⊤\ni k′j√\nd\n)\n=\n∑\nj≤i\nsim(q′\ni,k′j),\ns←\ni =\n∑\nj>i\nexp\n(q⊤\ni kj√\nd\n)\n=\n∑\nj>i\nsim(qi,kj),\nwhere (q′\ni,k′\nj) and (qi,kj) are two sets of query-\nkey vectors computed in the previous and this text\nsegment respectively for the same position pair\n(i,j) . Then we have:\nc↔\ni =\n∑\nj≤isim(q′\ni,k′j)∑\nj≤isim(q′\ni,k′j) + ∑\nj>i sim(qi,kj)c→\ni\n+\n∑\nj>i sim(qi,kj)∑\nj≤isim(q′\ni,k′j) + ∑\nj>i sim(qi,kj)c←\ni\n=\n∑\nj≤isim(q′\ni,k′j)v′j + ∑\nj>i sim(qi,kj)vj\n∑\nj≤isim(q′\ni,k′j) + ∑\nj>i sim(qi,kj)\n=\n∑\nj\nβj˜vj,\nwhere ∑\nj βj = 1. Finally, we derive c↔\ni as the\nweighted sum of the value vectors ˜vj from both the\npast (j ≤i) and the future (j >i) of the position i.\nB Unstability Analysis of the RPE in\nTransformer-XL\nWe conjecture that the unstability of Eq. 6 stems\nfrom the terms involving the dot-product of Ri−j\nand another vector. So we start by considering the\nvariance of x⊤Ri−j where x ∈Rd is a random\nvector. Without loss of generality, we assume that\nxhas zero mean and a variance of σ:\nE(xk) = 0, ∀k∈[1,··· ,d]\nVar(xk) = σk,k, ∀k∈[1,··· ,d]\nCov(xk,xl) = σk,l, ∀l̸= k∈[1,··· ,d]\nLet i−j = ∆. According to Vaswani et al. (2017),\nR∆ takes the following form:\nR∆ =[sin(ω1∆),cos(ω1∆),\n··· ,sin(ωd/2∆),cos(ωd/2∆)],\nwhere wk = 10000 −2k/d. Then the dot-product\nx⊤R∆ can be derived into the linear combination\nof sine and cosine functions:\nx⊤R∆ =\nd/2∑\nk=1\nx2k−1 sin(ωk∆) + x2kcos(ωk∆),\nwhere we can easily derive that E(x⊤R∆) = 0 .\nAccording to the variance-expectation formula:\nVar(x) = E[x2] −E[x]2, we can simplify the vari-\nance Var(x⊤R∆) in the following:\nVar(x⊤R∆)\n= E\n[(d/2∑\nk=1\nx2k−1 sin(ωk∆) + x2kcos(ωk∆)\n)2]\n=\nd/2∑\nk=1\nE[x2\n2k−1] sin2(ωk∆) + E[x2\n2k] cos2(ωk∆)\n+ 2\nd/2∑\nk=1\nd/2∑\nl=1,l̸=k\nE[x2k−1x2l] sin(ωk∆) cos(ωl∆).\nWe further simplify the above equation by assum-\ning that all the elements have the same variance\nσs, and all pairs of distinct elements have the same\ncovariance σc:\nVar(x⊤R∆) =\nd/2∑\nk=1\nσs[sin2(ωk∆) + cos2(ωk∆)]\n+ 2\nd/2∑\nk=1\nd/2∑\nl=1,l̸=k\nσcsin(ωk∆) cos(ωl∆)\n= d\n2σs + 2σcg(∆),\nwhere g(x) = ∑d/2\nk=1\n∑d/2\nl=1,l̸=ksin(ωkx) cos(ωlx)\nis an odd function.\nWe consider the value of g(x) when x ≈ 0.\n5758\n−40 −20 0 20 40\nx\n−100\n0\n100\ng(x)\nFigure 7: The plot of g(x) when d = 64. We see that\ng(x) is symmetric with respect to the origin. The value\nof g(x) when xapproaches zero from the left and right\ndiverge greatly.\nSince sin(ωkx) ≈ωkx, cos(ωkx) ≈1, we have:\ng(x) ≈\nd/2∑\nk=1\nd/2∑\nl=1\nωkx\n= d\n2\nd/2∑\nk=1\nwkx\n= xd\n2\nd/2∑\nk=1\n( 1\n100002/d\n)k\n≈ d\n2((108)1/d −1) ·x= γd ·x.\nSince ax ≈1 + xln awhen x≈0, we derive that\nγd ≈ d2\n2 ln 108 with the grow of d. This causes g(x)\nto have a very steep slope near 0. Since g(x) is an\nodd function, the value of g(∆) and g(−∆) will\nhave a huge gap (∆ is a small positive value). To\nvalidate this, we plot the function of g(x) when\nd= 64 in Figure 7.\nOverall, the variance of x⊤R∆ is composed of\ntwo terms, the first being σs multiplied by a con-\nstant factor d/2, and the second beingσcmultiplied\nby g(∆). Note that σs is strictly positive, while σc\ndoes not have this restriction. Due the asymptotic\nbehavior of g(∆) near 0, i.e., O(d2∆), we cannot\nfind a proper σc that makes Var(x⊤R∆) bounded\nby O(dσs) for every ∆ that takes its value from\nboth the positive and negative integers.\nFinally, we plot the training curves of the two\nmodels using the RPE in Transformer-XL (xl-rpe)\nand our disentangled RPE (dis-rpe) in Figure 8\nwhere we observed that the xl-rpe suffers from\nnumerical unstability during training.\n0 10000 20000 30000 40000 50000\nSteps\n3\n4\n5\n6\n7\n8\n9Train Loss\nEncoding\ndis-rpe\nxl-rpe\nFigure 8: Comparison of the training dynamics using\ndifferent encoding schemes: the disentangled RPE (dis-\nrpe) and the RPE of Transformer-XL (xl-rpe).\nDataset train / dev / test\nWikitext-103 103,227,021 / 217,646 / 245,569\nenwik8 88,982,818 / 4,945,742 / 4,943,417\ntext8 89,999,999 / 4,999,999 / 5,000,000\nTable 4: Statistics of the datasets used in the experi-\nments. For Wikitext-103, we use the official split from\nMerity et al. (2017) and present the number of tokens in\neach split. For enwik8 and text8, we use the split from\nDai et al. (2019) and report the number of characters for\neach split.\nC Experimental Details\nC.1 Dataset Details\nWikitext-103 dataset is extracted from the set of\nverified Good and Featured articles on English\nWikipedia. The dataset retains the original case,\npunctuation and numbers, and covers a broad range\nof domains, e.g., science, culture, bibliography,\nand etc. The dataset is available under the Creative\nCommons Attribution-ShareAlike (CC BY-SA) Li-\ncense.\nenwik8 dataset is the test set data of the Large\nText Compression Benchmark which contains the\nfirst 100 million bytes of English Wikipedia dump\non Mar. 3, 2006. All characters are encoded in\nUTF-8. This dataset is licensed under the CC BY-\nSA License.\ntext8 dataset contains the first 100 million bytes\nof the clean text of Wikipedia that retains only\nregular articles and image captions. All the letters\nare converted into lower case, and only letters in\nthe 27 character alphabet, namely letters a-z and\nnonconsecutive spaces, are preserved. This dataset\nis licensed under the CC BY-SA License.\nThe statistics of the three datasets is shown in\nTable 4.\n5759\nC.2 Model Configurations\nWe follow the base model configuration of Dai\net al. (2019). On Wikitext-103, we use the Trans-\nformer model with 16 layers, 10 attention heads\nwith a head dimension of 41. The inner dimension\nsize of the feedforward layer is 2100. We use a\ndropout rate of 0.1 and no attention dropout. To\ncope with the large vocabulary, we use the adap-\ntive embeddings (Baevski and Auli, 2019). We set\nthe memory length to 150 and the target sequence\nlength to 150 as well. On text8 and enwik8 datasets,\nwe use the Transformer model with 12 layers, 8\nattention heads with a head dimension of 64. The\ninner dimension size of the feedforward layer is\n2048. We use a dropout rate of 0.1 and no attention\ndropout. We set the memory length to 512 and\nthe target length to 512. Specifically, our LaMemo\nuses the disentangled relative positional encodings\ndescribed in Sec. 3.3. The look-ahead attention\nshares the query, key and value projection matrices\nwith those in the causal attention.\nC.3 Training Settings\nWe trained the models using Adam (Kingma and\nBa, 2015) optimizer, with no warmup. We used\na learning rate of 2.5 ×10−4 which decayed to\n0 at the end of training with a cosine schedule.\nOn Wikitext-103, we trained the model with 250K\nsteps using a batch size of 64. On enwik8 and\ntext8, we trained the model with 100K7 steps using\na batch size of 40. We conducted our experiments\non 2 Tesla V100.\nC.4 Hyperparameters\nWe present the hyperparameter search space in Ta-\nble 5. The number of hyperparameter search trials\nwas 10. We adopted a manual search to select the\nhyperparameters, and the selection criterion was\nppl/bpc on the dev set. We did not use early stop-\nping during training.\nD Generated Examples\nIn this section, we present the examples gener-\nated by LaMemo and Transformer-XL trained on\nthe Wikitext-103 dataset. Both models maintain\na memory with a length of 512. We randomly se-\nlect a piece of text from the test set as the context\n7We used a smaller number of training steps compared to\nDai et al. (2019), since it would take too long to train one\nmodel.\nHyper-parameter Search Space\nLearning Rate choice[1e-4, 2.5e-4, 5e-4]\nLearning Rate Schedule choise[linear, cosine]\nWarmup Steps choice[0, 1000, 2000]\nMaximum Gradient Norm choice[0.25, 0.5, 1.0]\nEpsilon (Sec. 3.2) choice[1e-6, 1e-5, 1e-4]\nOptimizer Adam\nEpsilon (for Adam) 1e-8\nMomentum (for Adam) β1 = 0.9,β2 = 0.999\nTable 5: Hyperparameter search space. choice indicates\nthat the listed numbers will be chosen with the same\nprobability. Best-found hyperparameters are in bold-\nface.\nand allow both models to generate 256 tokens fol-\nlowing the context. We use top-p sampling with\np= 0.95 and detokenize the context and the gen-\nerated texts to facilitate reading. We present the\nexmples in Table 6 and 7. We present our major\nfindings below:\n• Both models are able to hallucinate imaginary\ncontents fairly relevant to the limited contexts\ngiven as prompts.\n• Transformer-XL sometimes generates topic-\nirrelevant contents without further elaboration\n(marked by underline), while LaMemo stays\non topic more closely during the course of\ngeneration.\n• Transformer-XL suffers more sever repetition\nissues (marked in boldface) than LaMemo\nboth lexically and semantically.\n5760\nContext:\n= Shackleton ( crater ) =\nShackleton is an impact crater that lies at the south pole of the Moon. The peaks along the crater’s rim are exposed to\nalmost continual sunlight, while the interior is perpetually in shadow (a Crater of eternal darkness). The low-temperature\ninterior of this crater functions as a cold trap that may capture and freeze volatiles shed during comet impacts on the\nMoon. Measurements by the Lunar Prospector spacecraft showed higher than normal amounts of hydrogen within the\ncrater, which may indicate the presence of water ice. The crater is named after Antarctic explorer Ernest Shackleton.\n= = Description = =\nThe rotational axis of the Moon lies within Shackleton, only a few kilometers from its center. The crater is 21 km in\ndiameter and 4.2 km deep. From the Earth, it is viewed edge-on in a region of rough, cratered terrain. It is located within\nthe South Pole-Aitken basin on a massif. The rim is slightly raised about the surrounding surface and it has an outer\nrampart that has been only lightly impacted. No significant craters intersect the rim, and it is sloped about 1.5 ° toward\nthe direction 50 – 90 ° from the Earth. The age of the crater is about 3.6 billion years and it has been in the proximity of\nthe south lunar pole for at least the last two billion years.\nBecause the orbit of the Moon is tilted only 5 ° from the ecliptic, the interior of this crater lies in perpetual darkness.\nEstimates of the area in permanent shadow were obtained from Earth-based radar studies. Peaks along the rim of the\ncrater are almost continually illuminated by sunlight , spending about 80 – 90 % of each lunar orbit exposed to the Sun.\nContinuously illuminated mountains have been termed peaks of eternal light and have been predicted to exist since the\n1900s.\nThe shadowed portion of the crater was imaged with the Terrain Camera of the Japanese SELENE spacecraft using the\nillumination of sunlight reflected off the rim. The interior of the crater consists of a symmetrical 30 ° slope that leads\ndown to a 6.6 km diameter floor. The handful of craters along the interior span no more than a few\nLaMemo Generation:\nkilometres of plains.\nA slight rotation is observed in the crater that now bears significant elements like snow,ice, and water ice. In other\nareas of the crater there are indications that the nature of the alpine ice layer provides some protection to the craters by\nincreasing their brightness. The craters seem to be cut by an ice thickness of 10 cm (4 in). Images show their prominence,\nvarying from a visible plateau to a multi-ring topography, indicating that they are present (a major component) in the\ncraters. Their interior has a considerable surface area of dry ice.\nAs seen from Earth, the initial craters are marked by slow-moving metal-bearing mudstone-covered rock that has a\nsurface of substantial amounts of coal, calcium, lead and sodium alloy, carbon monoxide, nitrogen oxides, and silicon\ndioxide. All are developed by processes such as heat transfer by the advancing radiation pressure. The least dense feature\nalong the surface of the Moon is the large resurfacing event. There is no view that these rocks remain melting to about\n90 °C (150 °F), but that their surface shine is dark and their outer layers are light, reflecting vertical oil. This late flank\nand early 1970s discovery was announced in 1982, as a substantial amount of ice.\nA parallel lava\nTransformer-XL Generation:\nkilometres.\nDuring the hundreds of years after its formation, the crater now has significant seismic activity in the vicinity. Most\ncraters are dome-shaped and are usually constructed of sedimentary materials that have been deposited during alpine\nice times. The upper surface of the crater is covered with higher layers of rock that are cut by an extensive thickness\nof debris. The debris reaches the top of a mountain and comes in contact with visible ground planes. People are often\nobserved wearing headgear of degrading materials such as clothing and boots, their shoes or hats, or even working on\nthe surface. Below the crater. As the crater faces the crater it has thick, thin pipes or scarps. A total of more than 200\ncaves have been excavated, down to some 40 m by 20 m. This exceeding the margin of the crater where it actually passes\nthrough is considered to be very high. Other geologic features by the advancing magnetic field have been reported from\nthe crater. However, in 1992, scientists announced they would study this area again.\nThe crater was once a common feature of the Post Lunar System. Its medieval boundaries were not fixed in the orbital\nplane of Mercury. An individual crater had been called “ Discovery crater ” and one referred to as “ Bear crater ”,\nalthough it is likely that an additional crater was called\nTable 6: Example 1 generated by LaMemo and Transformer-XL given a context prompt from the test set of\nWikitext-103. Original Wikipedia page: https://en.wikipedia.org/wiki/Shackleton_(crater).\n5761\nContext:\nNero was not expected to become Emperor because his maternal uncle, Caligula, had begun his reign at the age of\n24 with enough time to produce his own heir. Nero ’s mother, Agrippina, lost favour with Caligula and was exiled in\n39 after her husband ’s death. Caligula seized Nero ’s inheritance and sent him to be brought up by his less wealthy\naunt, Domitia <unk>, who was the mother of Valeria <unk>, Claudius ’s third wife. Caligula, his wife <unk> and their\ninfant daughter Julia Drusilla were murdered on 24 January 41. These events led Claudius, Caligula ’s uncle, to become\nemperor. Claudius allowed Agrippina to return from exile.\nClaudius had married twice before marrying Valeria <unk>. His previous marriages produced three children including\na son, Drusus, who died at a young age. He had two children with <unk> – Claudia Octavia (born 40) and Britannicus\n(born 41). <unk> was executed by Claudius in the year 48.\nIn 49 AD , Claudius married a fourth time, to Nero ’s mother Agrippina, despite her being his niece. To aid Claudius\npolitically, young Nero was adopted in 50 and took the name Nero Claudius Caesar Drusus Germanicus (see adoption\nin Rome). Nero was older than his stepbrother Britannicus, and thus became heir to the throne. Nero was proclaimed\nan adult in 51 at the age of 14. He was appointed proconsul, entered and first addressed the Senate, made joint public\nappearances with Claudius, and was featured in coinage. In 53, he married his stepsister Claudia Octavia.\n= = Emperor (54 – 68 AD) = =\n= = = Early rule = = =\nClaudius died in 54 and Nero, taking the name Nero Claudius Caesar Augustus Germanicus, was established as\nEmperor. Though accounts vary, many ancient historians state Agrippina poisoned Claudius. According to Pliny the\nElder, she used poison mushrooms. It is not known how much Nero knew or if he was even involved in the death of\nClaudius.\nSuetonius wrote “ ... for even if he was not the instigator of the emperor ’s death , he was at least privy to it, as he openly\nadmitted; for he used afterwards to laud mushrooms, the vehicle in which the poison was administered to Claudius, as\n‘ the food of the gods , ’ as the Greek proverb has it. At any rate, after Claudius’ death he vented on him every kind\nof insult, in act and word, charging him now with folly and now with cruelty; for it was a favourite joke of his to say\nthat Claudius had ceased ‘ to play the fool ’ among mortals, lengthening the first syllable of the word <unk>, and he\ndisregarded many of his decrees and acts as the work of a madman and a <unk>. Finally, he neglected to enclose the\nplace where his body was burned except with a low and mean wall. ”\nAccording to Suetonius; Nero became Emperor at the age of 17 when the news of Claudius’ death was made known ,\nmaking him the youngest emperor at that time Although, what Suetonius may have meant is that he was\nLaMemo Generation:\nkilled as a result of the subsequent anti-Caligula propaganda.\nAt some time after 47 AD, Nero retired to his villa at Baiae. The combination of a curse that led to the death of Agrippa,\nand a duel between Claudius and his own son Drusus, may have encouraged the young Emperor to fire his own stela.\n= = = = = Germanicus = = = = =\nAfter becoming emperor, Nero received letters from Suetonius, which finally followed him on 28 May 44 AD with\na letter to Claudius, completing the Emperor ’s plot. The letters were printed in Latin. Claudius’ plot was offered to\nCaligula by Agrippina the Elder, a woman from Kent, but she declined. Further letters from Gaius and Nero are known\nfrom the reigns of Tiberius and Caligul , though none of them mention Caligula.\nGermanicus’ conspiracy had developed into a protracted affair. His wife Antonia, his sister Agrippina ’s sister and\ngrandmother, had refused to accept him because she believed they were connected for communication, but the Via Flavia\nwas left to her husband and Agrippina the Elder, whom she had defeated in her husband ’s palace. Both had been sent\nthrough Gibraltar to compete for power. Gaius and Nero were outraged that Agrippina was supposed to be ready to\nsupport Caligula; Suetonius says that Nero insisted on making\nTransformer-XL Generation:\nkilled as a result of poison and was deemed a good comedian. At now and then he had adopted Nero as his heir.\nAlthough Nero ’s rule was as close as he was to his father to his daughter Agrippina, he was not to physically exhibit\nany ill degree of higher kingship but to deny it. The Augustan History describes him as having been strongly willed,\npossessing an excellent hand and often claiming the title “ Augustus Caesar ”, and often referred to him as Caesar ’s “\npaternal heir ”. The Augustan History, however, asserts that he was no longer in China, and therefore he was raised as a\nRoman Hercules rather than a Roman citizen. Claudius Claudius was added as a junior emperor in 53 AD; he was\ncrowned emperor in 61 AD.\n= = = = As Augustus I = = = =\nClaudius had developed a keen interest in Claudius in divination, a practice which later developed into his incarnation\nas a youth figure in Julius Caesar. Claudius ’ grandfather, Leo I, ascended the throne in 23 AD and spent time in Rome,\nas did Claudius, who defeated Claudius in 42 AD. Claudius departed Rome after the death of Agrippa III in 65 AD.\nDuring the following years, Claudius was temporarily imprisoned in Rome, although possibly simply regulating the use\nof the captive dogs\nTable 7: Example 2 generated by LaMemo and Transformer-XL given a context prompt from the test set of\nWikitext-103. Original Wikipedia page: https://en.wikipedia.org/wiki/Nero.\n5762",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6920216083526611
    },
    {
      "name": "Zhàng",
      "score": 0.6402341723442078
    },
    {
      "name": "Computational linguistics",
      "score": 0.514689028263092
    },
    {
      "name": "Linguistics",
      "score": 0.4983832836151123
    },
    {
      "name": "Natural language processing",
      "score": 0.4428388476371765
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41942524909973145
    },
    {
      "name": "Cognitive science",
      "score": 0.41010504961013794
    },
    {
      "name": "Programming language",
      "score": 0.36010241508483887
    },
    {
      "name": "History",
      "score": 0.1967962086200714
    },
    {
      "name": "Psychology",
      "score": 0.14454177021980286
    },
    {
      "name": "Philosophy",
      "score": 0.13619014620780945
    },
    {
      "name": "China",
      "score": 0.12923851609230042
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210091137",
      "name": "NetEase (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 3
}