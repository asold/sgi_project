{
  "title": "A Simple Long-Tailed Recognition Baseline via Vision-Language Model",
  "url": "https://openalex.org/W3217385406",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5086572854",
      "name": "Teli Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080973846",
      "name": "Shijie Geng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100429685",
      "name": "Mengmeng Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5023198186",
      "name": "Jing Shao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101055775",
      "name": "Jiasen Lu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100732450",
      "name": "Hongsheng Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101986762",
      "name": "Peng Gao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100748135",
      "name": "Yu Qiao",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177200443",
    "https://openalex.org/W3173181308",
    "https://openalex.org/W3140662957",
    "https://openalex.org/W2983156430",
    "https://openalex.org/W2962933664",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W3122855191",
    "https://openalex.org/W2781292787",
    "https://openalex.org/W3210940825",
    "https://openalex.org/W3204449938",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W3203711169",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3034369739",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2963921921",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3146922031",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2963176022",
    "https://openalex.org/W2963691377",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3166596953",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3206189861",
    "https://openalex.org/W3034601242",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W3126272270",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3035730922"
  ],
  "abstract": "The visual world naturally exhibits a long-tailed distribution of open classes, which poses great challenges to modern visual systems. Existing approaches either perform class re-balancing strategies or directly improve network modules to address the problem. However, they still train models with a finite set of predefined labels, limiting their supervision information and restricting their transferability to novel instances. Recent advances in large-scale contrastive visual-language pretraining shed light on a new pathway for visual recognition. With open-vocabulary supervisions, pretrained contrastive vision-language models learn powerful multimodal representations that are promising to handle data deficiency and unseen concepts. By calculating the semantic similarity between visual and text inputs, visual recognition is converted to a vision-language matching problem. Inspired by this, we propose BALLAD to leverage contrastive vision-language models for long-tailed recognition. We first continue pretraining the vision-language backbone through contrastive learning on a specific long-tailed target dataset. Afterward, we freeze the backbone and further employ an additional adapter layer to enhance the representations of tail classes on balanced training samples built with re-sampling strategies. Extensive experiments have been conducted on three popular long-tailed recognition benchmarks. As a result, our simple and effective approach sets the new state-of-the-art performances and outperforms competitive baselines with a large margin. Code is released at https://github.com/gaopengcuhk/BALLAD.",
  "full_text": "A Simple Long-Tailed Recognition Baseline via Vision-Language Model\nTeli Ma1, Shijie Geng1, Mengmeng Wang1, Jing Shao2, Jiasen Lu3\nHongsheng Li4, Peng Gao†1, Yu Qiao1\n1Shanghai Artiﬁcial Intelligence Laboratory 2SenseTime Research\n3PRIOR@Allen Institute for AI 4The Chinese University of Hong Kong\n{mateli, gaopeng ,qiaoyu}@pjlab.org.cn\nAbstract\nThe visual world naturally exhibits a long-tailed distribu-\ntion of open classes, which poses great challenges to modern\nvisual systems. Existing approaches either perform class\nre-balancing strategies or directly improve network modules\nto address the problem. However, they still train models\nwith a ﬁnite set of predeﬁned labels, limiting their super-\nvision information and restricting their transferability to\nnovel instances. Recent advances in large-scale contrastive\nvisual-language pretraining shed light on a new pathway\nfor visual recognition. With open-vocabulary supervisions,\npretrained contrastive vision-language models learn power-\nful multimodal representations that are promising to handle\ndata deﬁciency and unseen concepts. By calculating the\nsemantic similarity between visual and text inputs, visual\nrecognition is converted to a vision-language matching prob-\nlem. Inspired by this, we propose BALLAD to leverage con-\ntrastive vision-language models for long-tailed recognition.\nWe ﬁrst continue pretraining the vision-language backbone\nthrough contrastive learning on a speciﬁc long-tailed tar-\nget dataset. Afterward, we freeze the backbone and further\nemploy an additional adapter layer to enhance the repre-\nsentations of tail classes on balanced training samples built\nwith re-sampling strategies. Extensive experiments have\nbeen conducted on three popular long-tailed recognition\nbenchmarks. As a result, our simple and effective approach\nsets the new state-of-the-art performances and outperforms\ncompetitive baselines with a large margin. Code is released\nat https://github.com/gaopengcuhk/BALLAD .\n1. Introduction\nDuring past years, visual recognition tasks, such as image\nclassiﬁcation [21, 58, 65], object detection [36, 53], semantic\nsegmentation [5, 42, 74], and instance segmentation [20, 24,\n40] have been signiﬁcantly improved. The performance\nmany shot medium shot few shot overall\n40\n50\n60\n70\n80T op-1 Acc(%)\nBALLAD-RN50\nBALLAD-RN50×16\nPaCo-RNX101\nRIDE-RNX50\nBalanced Softmax-RNX101\nResLT-RNX50\nFigure 1. Comparison with the state-of-the-art approaches on\nImageNet-LT.BALLAD with ResNet-50 visual backbone outper-\nforms state-of-the-art models which have more complex backbones\nand longer training epochs by a large margin (up to+7.2% improve-\nment on overall accuracy compared with PaCo [10]), especially\nfor categories without abundant training examples. BALLAD with\nResNet-50×16 achieves a new state-of-the-art performance of\n76.5% top-1 accuracy on ImageNet-LT benchmark dataset. (RN50:\nResNet-50, RN50×16: the EfﬁcientNet method with 16×compute\nof ResNet-50, RNX50: ResNeXt-50, RNX101: ResNeXt-101.)\ngains can be largely attributed to the availability of large-\nscale high-quality datasets [12,33,38]. However, the problem\nof data imbalance has inevitably emerged since real-world\ndata often abide by a long-tailed distribution ( e.g., Pareto\ndistribution [49] or Zipf’s law [78]). In other words, a few\nhead classes dominate the majority of training examples,\nwhereas many rare or ﬁne-grained classes only have limited\nrelevant data points.\nTo alleviate the issue, previous efforts either care-\nfully create more balanced datasets ( e.g., ImageNet [12],\nMSCOCO [38], and Kinetics-400 [31]) with human labors\nor develop more robust algorithms to handle data imbal-\nance. However, since the former is notoriously laborious\nand expensive, many researchers have been devoted to the\nlatter. Formally, long-tailed recognition (LTR) is a research\n1\narXiv:2111.14745v1  [cs.CV]  29 Nov 2021\nﬁeld seeking robust models that 1) are resistant to signiﬁ-\ncant imbalanced class distribution; 2) can deal with few-shot\nlearning of tail classes. Many methods [73] have been pro-\nposed for solving LTR problems. According to the core\ntechnical contributions, they can be divided into two cate-\ngories. Methods in the ﬁrst line focus on class re-balancing\nstrategies [22, 29, 44, 70] such as data re-sampling, loss re-\nweighting, and logit adjustment. The second category fo-\ncuses on improving network modules [9,10,30,54,59,72,75]\nby classiﬁer designing, decoupled training, and representa-\ntion learning. While these methods have achieved signiﬁcant\nprogress, the performance of LTR remains unsatisfactory.\nWhen delving deeper into the utilization of the existing im-\nbalance datasets, we have observed that almost all previous\nefforts are conﬁned to a predetermined manner which de-\nsigns models entirely relying on the visual modality. That\nis to say, they totally ignore the semantic features of the\nraw label text, which may be a promising solution to exert\nadditional supervision on inadequate data sources. There-\nfore, this paper explores whether language modality can be\neffective and complementary information for this task. In\nthe meantime, we could broaden generalization abilities to\nfew-shot categories and zero-shot novel instances.\nRecently, contrastive vision-language models such as\nCLIP [51] and ALIGN [26] brought a breath of fresh air\ninto the vision community. They learn to align vision and\nlanguage representations with a contrastive loss given large-\nscale noisy image-text pairs collected from the web. The\npowerful visual-language representations obtained from pre-\ntraining signiﬁcantly improve the zero-shot classiﬁcation\nperformance in open-vocabulary settings without any addi-\ntional annotations. Motivated by the success of contrastive\nvision-language models and the curiosity of the language ef-\nfect mentioned above, we directly test CLIP on LTR datasets\nunder its zero-shot setting. Surprisingly, the results are bal-\nanced on many-shots (59.4%), medium-shots (57.5%), and\nlow-shots (57.6%) subsets of ImageNet-LT [41] and the\noverall performance (58.2%) is comparable to the state-of-\nthe-art [10]. From which we see the great potential of the\nmultimodality solution for LTR. To further improve the per-\nformance while keep the capability of dealing with data im-\nbalance, an intuitive way is to ﬁnetune the vision-language\nmodels on LTR datasets. However, we ﬁnd it only brings a\nslight gain. Therefore, the core task of our work becomes\nhow to design an effective recipe for training vision-language\nmodels under the circumstances of long-tailed distribution.\nSpeciﬁcally, in this paper, we design a simple frame-\nwork based on contrastive vision-language models for LTR.\nThe training procedure of the framework is broken into two\nphases from the perspective of distribution skewness: A)\nutilizing abundant annotations from LTR datasets; B) tack-\nling few-shot learning of tail classes on balanced data built\nwith re-sampling strategies. In Phase A, we continue pre-\ntraining CLIP backbone on a speciﬁc LTR dataset through\ncontrastive learning. It enables our framework to fully ex-\nploit available training examples and update visual-language\nrepresentations on a new domain. To further facilitate the\nfew-shot learning of tail classes, during Phase B, we freeze\nthe CLIP backbone and employ an auxiliary linear adapter\nfor ﬁnetuning on re-balanced training samples. The adapter\ndynamically combines ﬁxed Phase-A and ﬁnetuned Phase-B\nfeatures via a residual connection to reﬁne the visual rep-\nresentations of tail classes. Compared with ﬁnetuning the\nwhole CLIP backbone directly, the linear adapter reduces the\nnumber of learnable parameters and thus prevents the poten-\ntial overﬁtting of few-shot setups. According to Figure 1, our\nframework clearly achieves better performances than state-\nof-the-art LTR approaches. The improvements are especially\nsigniﬁcant for few-shot and medium-shot classes, demon-\nstrating our approach’s great capability of handling class\nimbalance. Since our framework solves the data imbalance\nvia a linear adapter, we name it as BALLAD (BALanced\nLinear ADapter), which implies the harmony of head and\ntail classes. Our contributions are three folds:\n• We point out the shortcomings of training with ﬁxed class\nlabels and propose to leverage language modality via con-\ntrastive vision-language backbone to facilitate long-tailed\nrecognition.\n• We develop the BALLAD framework consisting of two\nphases to handle head and tail classes successively. Specif-\nically, we keep training the visual and language branches\nof the pretrained vision-language model simultaneously\nat the ﬁrst stage. Then we adopt a linear adapter to tackle\ntail classes with vision-language parameters frozen.\n• We conduct extensive experiments to demonstrate the ef-\nfectiveness of BALLAD . Our simple baseline achieves\nthe new state-of-the-art performances on all benchmarks,\noutperforming the old paradigm by 16.5 points maximally\non ImageNet-LT.\n2. Related Work\nContrastive Vision-Language Model. Contrastive rep-\nresentation learning has been widely adopted to fulﬁll\nself-supervised pretraining in various AI domains [3, 4, 6,\n17, 19, 47]. Recently, the intersection of vision and lan-\nguage [1, 7, 16, 32, 46, 55, 68] also experienced a revolution\nsparked by contrastive representation learning. Contrastive\nvision-language models like CLIP [51] and ALIGN [26]\ndemonstrate promising zero-shot performances on various\nvisual search and recognition tasks. Learning directly from\nnatural language supervisions that contain rich visual con-\ncepts, they are very ﬂexible and robust to distribution vari-\nations across different domains. The success of CLIP and\nALIGN has enlightened many downstream vision-language\ntasks. For instance, DeCLIP [35] proposes to utilize self-,\n2\na photo of a [CLASS]\ndog\ngoldfish\ncat\nhorse\nshots\nText Encoder\na photo of a\ndog\ngoldfish\ncat\nhorse\nshots\nImage Encoder\nPhase A.\nPhase B.\ndog\ngoldfish\ncat\nhorse\nshots\nImage Encoder\nText Encoder\nBalance\ncat\nhorse\ndog\ngoldfish\nLogits\ncat\nhorse\ndog\ngoldfish\nLogits\nLinear\nAdapter\n[CLASS]\nFigure 2. Overview of our BALLAD framework. In Phase A, we keep pretraining the text and image branches of the vision-language\nbackbone on long-tailed data. After Phase A, head classes typically achieve good enough classiﬁcation performance, whereas tail classes are\nstill far from perfect. During Phase B, a linear adapter is adopted to further train the vision-language backbone on balanced training samples.\nAs a result, tail classes enjoy a performance boost while head classes slightly increase or maintain their original classiﬁcation accuracy.\nrepresents training with parameter update while\n represents freezing parameters.\nmulti-view, and nearest-neighbor supervisions among the\nimage-text pairs for data efﬁcient pretraining of CLIP. On\nvisual classiﬁcation tasks, CLIP-Adapter [15] argues that\nﬁne-tuning contrastive vision-language models with linear\nadapters is a better alternative to prompt tuning. For video re-\nlated tasks, VideoCLIP [66] performs contrastive pretraining\nwith video-text pairs for zero-shot video-text understanding.\nActionCLIP [61] presents a new “pretrain, prompt and ﬁne-\ntune” paradigm leveraging pretrained vision-language mod-\nels for zero-shot/few-shot action recognition. CLIP-It [45]\ndesigns a language-guided multimodal transformer based on\nCLIP to address query-focused video summarization. More-\nover, CLIPort [57] combines CLIP with Transporter [69]\nto endow a robot with the ability of semantic understand-\ning and spatial perception. In this paper, we demonstrate\nthat contrastive vision-language models can also facilitate\nvisual recognition under long-tailed class distribution setups\nif properly trained.\nLong-Tailed Recognition. Long-tailed recognition [73] is\na practical and challenging problem in vision domain. Gen-\neral visual models will suffer from severe performance degra-\ndation under such imbalanced class distributions. A great\nnumber of approaches [9,10,13,25,44,48,54,63,67,71] have\nbeen proposed to address LTR from different perspectives.\nAn intuitive solution is to directly re-balance the number of\ntraining samples across all classes [30,75]. However, naively\nadjusting the skewness of training samples may lead to the\noverﬁtting of tail classes. Better alternatives include loss\nre-weighting [22, 29, 37] and logit adjustment [44, 70] based\non label frequencies. Though efﬁcacious for long-tailed dis-\ntribution, above methods all sacriﬁce the performance of\nhead classes at varying levels. To address the limitations,\nresearchers turn to explore new network architectures and\ntraining paradigms. Typically, long-tail recognition models\ncontain two key components – feature extractor and classiﬁer.\nFor each component, there are corresponding approaches by\neither designing better classiﬁer [39, 59, 64] or learning reli-\nable representations [41, 77]. In terms of new training frame-\nworks, existing efforts seek to divide a one-stage training\nparadigm into two stages. For example, decoupled training\napproaches [28,30] conduct representation learning and clas-\nsiﬁer training in a separate manner. Furthermore, ensemble\nlearning schemes [72,75] ﬁrst learn multiple experts with dif-\nferent data sub-groups and then merge their complementary\nknowledge to handle LTR. In contrast, ourBALLAD ﬁrst\nutilizes abundant long-tailed data to reﬁne visual-language\nrepresentations on a new target domain. Then we apply a\nlightweight linear adapter to encourage ﬁne-grained repre-\nsentation learning from balanced samples. The two phases\nsuccessively handle the learning of head and tail classes and\nensure a better balanced performance across all classes.\n3. Our Method\nIn this section, we ﬁrst brieﬂy revisit how contrastive\nvision-language models leverage contrastive objectives to\nachieve efﬁcient and scalable multimodal representation\nlearning. Moreover, we formally present BALLAD frame-\n3\nwork and discuss the advantages of the proposed two-stage\nrepresentation learning for long-tailed class distributions.\n3.1. Contrastive Vision-Language Model\nContrastive vision-language models such as CLIP [51]\nand ALIGN [26] typically follow a dual-encoder architec-\nture with a language encoder Lenc and a visual encoder\nVenc. Given an input image I, Venc is adopted to extract\nthe visual feature for I: fv = Venc(I) ∈Rdv . Likewise,\nLenc is applied to encode an input text sequence T into its\ncorresponding text feature: fl = Lenc(T) ∈Rdl. After\nextracting the feature for each modality, two transformation\nmatrices Wv ∈Rdv×d and Wl ∈Rdl×d are employed to\nproject the original visual and text features into a shared\nembedding space:\nv = W⊤\nv fv\n∥W⊤v fv∥, u = W⊤\nl fl\n∥W⊤\nl fl∥, (1)\nwhere v and u are both d-dimension normalized vectors in\nthe joint multimodal space. During pretraining, contrastive\nvision-language models learn to align image-text pairs inside\na batch. The overall training objective consists of match-\ning losses from two different directions, i.e., Lv→l for text\nretrieval and Ll→v for image retrieval. They both maxi-\nmize the scores of matched pairs while minimize that of\nunmatched ones:\nLv→l = −1\nN\nN∑\ni\nlog exp\n(\nv⊤\ni ui/τ\n)\n∑N\nj=1 exp\n(\nv⊤\ni uj/τ\n), (2)\nLl→v = −1\nN\nN∑\ni\nlog exp\n(\nu⊤\ni vi/τ\n)\n∑N\nj=1 exp\n(\nu⊤\ni vj/τ\n), (3)\nwhere τ denotes the temperature hyperparameter and N\nrepresents the number of image-text pairs in the batch.\nTrained with large-scale image-text pairs under the open-\nvocabulary settings, contrastive vision-language models\nachieve powerful multimodal representations and naturally\npossess the capability of zero-shot visual recognition. A col-\nlection of text descriptions following templates like ”a photo\nof a {CLASS}” is created for candidate classes in target\ndatasets to perform zero-shot prediction. If we represent the\nnormalized test image feature as v and all normalized text\ndescription features as {u1,··· ,uK}, we can thus compute\nthe class probability of the test image as below:\npi = exp\n(\nv⊤ui\n)\n/τ\n∑K\nj=1 exp (v⊤uj) /τ\n, (4)\nwhere pi represents the probability for class i, and Kstands\nfor the total number of candidate classes. Finally, the text la-\nbel with the highest probability is selected as the prediction.\n3.2. Balanced Linear Adapter\nAs stated in Section 1, contrastive vision-language mod-\nels obtain balanced performance for head and tail classes,\nwhereas traditional approaches like PaCo [10] suffer from\nlower performance of tail classes owing to the deﬁciency\nof training samples. Inspired by the zero-shot ability of\ncontrastive vision-language models, we choose CLIP as our\nbackbone for long-tailed recognition. The observation in\nSection 4.3.2 also encourages us to decouple the training\nof long-tailed data into two phases. To be speciﬁc, the ﬁrst\nphase (Phase A) fully utilizes available training data and\nensures the performance for classes with abundant exam-\nples, then the second phase (Phase B) focuses on improving\nthe few-shot learning of tail classes. Note that LWS [30]\nalso adopts a decoupled training framework. However, LWS\ndecouples the training of representation and classiﬁer into\ntwo stages. In contrast, our two phases are for long-tailed\nand balanced training samples respectively and both phases\nconduct representation reﬁnement with contrastive loss.\nPhase A. Recently, Gururangan et al. [18] shows that keep-\ning domain-adaptive and task-adaptive model pretraining can\nlargely improve the performances on target NLP tasks. Simi-\nlarly, for our Phase A, we ﬁnd that continuing the pretraining\nof contrastive vision-language backbone on long-tailed tar-\nget dataset also beneﬁts the learning of classes with abundant\nexamples. In this way, Phase A can make full use of avail-\nable training data regardless of its skewness. Since we focus\non classifying input images into text labels, the pretraining\nof Phase A directly follows the loss deﬁned in Equation (2).\nAs shown in Figure 2, Phase A updates the representations of\nboth text and image encoders on a new domain. After Phase\nA, head classes typically achieve good performance while\ntail classes still require another stage of balanced training.\nPhase B. Tail classes are short of training examples and un-\nder the few-shot settings. Directly training the whole vision-\nlanguage backbone may easily overﬁt to them and lead to\nperformance degradation. Inspired by parameter-efﬁcient\nadapter modules [23], we freeze the vision-language back-\nbone obtained from Phase A and utilize an additional linear\nadapter layer to help our model reﬁne its visual-language\nrepresentation on those infrequent classes. As shown in Fig-\nure 2, the text features would remain the same as Phase A.\nThe only difference lies in the image features. If we assume\nthe original image feature to be f, the weight matrix and\nbias of the linear adapter as W ∈Rd×d and b ∈Rd, then\nwe can represent the reﬁned image feature f⋆ as\nf⋆ = λ·ReLU\n(\nW⊤f + b\n)\n+ (1 −λ) ·f, (5)\nwhere λindicates the residual factor to dynamically combine\nPhase-B ﬁne-tuned image features with the original image\nfeatures of Phase A.\nTo avoid the Phase-B training from biasing towards head\nclasses, we also adopt class-balanced sampling strategy [30]\n4\nto construct a balanced group of training samples. Suppose\nthere are K classes that constitute a total of N training\nsamples in the target dataset. We can represent the number\nof training samples for class j as nj and thus have N =∑K\nj=1 nj. If we assume these classes are already sorted in a\ndecreasing order, then a long-tailed distribution impliesni ≥\nnj if i<j and n1 ≫nK. For class-balanced sampling, we\ndeﬁne the probability of sampling each data point from class\nj to be qj = 1\nK. In other words, to construct a balanced\ngroup of training samples, we will ﬁrst uniformly choose\na class out of the K candidates and then sample one data\npoint from the selected class. Finally, we perform Phase B\nﬁnetuning with Lv→l on the balanced training data.\n4. Experiments\n4.1. Experiment Setup\nDatasets. We conduct our experiments on three long-tailed\nbenchmark datasets, namely ImageNet-LT [41], Places-\nLT [41], and CIFAR100-LT [2]. ImageNet-LT and Places-LT\nwere ﬁrst introduced in [41] for long-tailed recognition re-\nsearch. ImageNet-LT is a long-tailed dataset with 1,000 cat-\negories sampled from the original ImageNet [12] following\nthe Pareto distribution with a power value of α= 6. There\nare 115.8K images in the training split, with maximally\n1,280 images per class and minimally 5 images per class.\nThe testing split maintains the same as the original Ima-\ngeNet [12], where samples per class are balanced. Places-LT\nis a long-tailed version of the original Places2 Database [76].\nThe training split of Places-LT contains with 184.5K images\nfrom 365 categories, with 4,980 images maximally per class\nand minimally 5 images per class. For the testing split, the\nimages of each class is also balanced with 100 images per\nclass. CIFAR100-LT [2] are created by long-tailed imbal-\nance technique [11] which reduces training examples per\nclass based on an exponential decay function. In this paper,\nwe directly use the version from [63] with an imbalance ratio\nρof 100. The training split contains 50K images from 100\ncategories, while the testing split has a uniform 100 images\nfor each class.\nImplementation Details. We use CLIP as the contrastive\nvision-language backbone in all experiments. For the visual\nbranch of CLIP, we vary among ResNet-50, ResNet-100,\nViT-B/16, and ResNet-50×16, which is 16×computation\ncost of ResNet-50 following the style of EfﬁcientNet as in-\ntroduced in [51]. The ResNet-50 is leveraged for all ablation\nstudies by default unless speciﬁed. We use SGD as the op-\ntimizer for all experiments with a momentum of 0.9. The\nbatch size is set to 512. We adopt cosine learning rate sched-\nule to decay learning rates. The initial learning rate of CLIP\nﬁnetuning is set to 1×10−5 for both the visual and language\nencoders, while the learning rate of linear adapter is set to\n0.2 at the start. For data pre-processing, images are resized\nMethod Backbone#EpochsMany Medium Few All\nτ-normalized [30]\nResNet-50 90 56.6 44.2 27.4 46.7ResNet-10190 59.4 47.0 30.6 49.6ResNet-15290 59.6 47.5 32.2 50.1ResNeXt-5090 59.1 46.9 30.7 49.4ResNeXt-10190 59.1 47.0 31.7 49.6ResNeXt-15290 62.2 50.1 35.8 52.8\nLWS [30]\nResNet-50 90 57.1 45.2 29.3 47.7ResNet-10190 60.1 47.6 31.2 50.2ResNet-15290 60.6 47.8 31.4 50.5ResNeXt-5090 60.2 47.2 30.3 49.9ResNeXt-10190 60.5 47.2 31.2 50.1ResNeXt-15290 63.5 50.4 34.2 53.3\nResLT [9] ResNeXt-50180 63.0 50.5 35.5 52.9ResNeXt-101180 63.3 53.3 40.3 55.1\nBalanced Softmax [52]ResNet-50400 66.7 52.9 33.0 55.0ResNeXt-50400 67.7 53.8 34.2 56.2ResNeXt-101400 69.2 55.8 36.3 58.0\nRIDE†[62] ResNet-50100 66.2 52.3 36.5 55.4ResNeXt-50100 68.2 53.8 36.0 56.8\nPaCo‡[10] ResNet-50400 65.0 55.7 38.2 57.0ResNeXt-50400 67.5 56.9 36.7 58.2ResNeXt-101400 68.2 58.7 41.0 60.0\nBALLAD\nResNet-5050+1071.0 66.3 59.5 67.2 (+7.2)ResNet-10150+1074.7 69.1 63.3 70.5 (+10.5)ViT-B/1650+1079.1 74.5 69.875.7 (+15.7)ResNet-50×16 50+1081.1 75.6 67.076.5 (+16.5)\nTable 1. Long-tailed recognition accuracy on ImageNet-LT for dif-\nferent methods and backbones. The red colored numbers represent\nthe improvement of overall accuracy compared with the state-of-\nthe-art performance (PaCo with 60.0% overall accuracy). †: RIDE\nuses 4 experts. ‡: the state-of-the-art model.\nto 224 ×224 unless the ResNet-50 ×16 visual backbone,\nwhich utilizes an image size of 384 ×384. Crop and random\nhorizontal ﬂip are also adopted to augment the original im-\nages for robustness considerations. For BALLAD , we train\nPhase A for 50 epochs and and Phase B for 10 epochs by\ndefault unless speciﬁed. For the hyperparameters, we set the\nresidual factor λto 0.2 and the temperature τ to 1.0. The fea-\nture dimensions of ResNet-50, ResNet-101, ResNet-50×16\nand ViT-B/16 are1024,512,768,512 respectively.\nEvaluation Metrics. We evaluate the models for long-\ntailed recognition on the balanced test splits and report the\ncommonly used top-1 classiﬁcation accuracy of all classes.\nFollowing [30], we divide these classes into three subsets –\nmany-shot, medium-shot, and few-shot categories. Speciﬁ-\ncally, many-shot, medium-shot, and few-shot are decided ac-\ncording to the amount of instances in each category, namely\nmore than 100 images, 20-100 images, and less than 20\nimages, respectively.\n4.2. Performance Comparison\nIn this section, we compare the performance ofBALLAD\nwith long-tailed recognition approaches that report state-of-\nthe-art results on three benchmark datasets, i.e., ImageNet-\nLT, Places-LT, and CIFAR100-LT.\nImageNet-LT. Table 1 shows the long-tailed recognition\nresults on ImageNet-LT. We presentBALLAD variants with\nResNet-50, ResNet-101, ResNet-50×16, and ViT-B/16 as\nthe visual backbone. From the table, we observe that with\nonly 50 + 10epochs (50 epochs for Phase A and 10 epochs\n5\nMethod Backbone#PretrainMany Medium Few All\nOLTR [41] ResNet-152Y 44.7 37.0 25.3 35.9cRT [30] ResNet-152Y 42 37.6 24.9 36.7τ-normalized [30]ResNet-152Y 37.8 40.7 31.8 37.9LWS [30] ResNet-152Y 40.6 39.1 28.6 37.6Balanced Softmax [52]ResNet-152Y 42.0 39.3 30.5 38.6ResLT [9] ResNet-152Y 39.8 43.6 31.4 39.8\nPaCo [10] ResNet-152Y 37.5 47.2 33.9 41.2PaCo†[10] ResNet-152Y 36.1 47.9 35.3 41.2\nBALLAD\nResNet-50 N 46.7 48.0 42.7 46.5 (+5.3)ResNet-101N 48.0 48.6 46.0 47.9 (+6.7)ViT-B/16 N 49.3 50.2 48.4 49.5 (+8.3)ResNet-50×16 N 49.4 50.5 46.6 49.3 (+8.1)\nTable 2. Long-tailed recognition accuracy on Places-LT for differ-\nent methods. The red colored numbers represent improvement of\noverall accuracy compared with the state-of-the-art performance\n(PaCo with 41.2% overall accuracy). #Pretrain: whether pretrain\nvisual backbone on full ImageNet-2012 or not. †: PaCo variant\ntrained with RandAugment [8].\nMethod Backbone Many Medium Few All\nOLTR [41] ResNet-32 61.8 41.4 17.6 41.2\nLDAM+DRW [2]ResNet-32 61.5 41.7 20.2 42.0\nτ-normalized [30]ResNet-32 65.7 43.6 17.3 43.2\ncRT [30] ResNet-32 64.0 44.8 18.1 43.3\nRIDE [62] ResNet-32 69.3 49.3 26.0 49.1\nTADE [72] ResNet-32 65.4 49.3 29.3 49.8\nBALLAD\nResNet-50 62.4 52.3 38.2 51.6 (+1.8)\nResNet-10169.5 59.3 47.1 59.2 (+9.4)\nViT-B/16 84.9 79.7 67.3 77.8 (+28.0)\nResNet-50×16 74.6 62.8 52.0 63.7 (+13.9)\nTable 3. Long-tailed recognition performance comparison on\nCIFAR100-LT with an imbalance ratio of 100. The red numbers\nrepresent the improvement of overall accuracy compared with the\nstate-of-the-art performance (TADE with 49.8% overall accuracy).\nfor Phase B), our smallestBALLAD variant with ResNet-50\nvisual backbone can surpass the largest model of the state-\nof-the-art PaCo [10] using ResNeXt-101 by +7.2%. When\ngradually increasing the size of visual backbone, we ﬁnd the\nperformance of BALLAD also enjoys an improvement. It is\nworth noting that BALLAD with ResNet-50×16 achieves\nan accuracy of 76.5%, which outperforms other state-of-the-\nart models with a large margin.\nPlaces-LT. We further evaluate BALLAD on Places-LT\ndataset and report the results in Table 2. It is a commonly\nused scheme of previous approaches to pretrain their models\non ImageNet-2012 full dataset ﬁrst to enrich the visual rep-\nresentation before ﬁnetuning on Places-LT. However,BAL-\nLAD can directly perform training on Places-LT thanks to\nthe additional language representation of contrastive vision-\nlanguage models. As shown in Table 2, BALLAD with\nResNet-50 visual backbone achieves 46.5% accuracy for all\ncategories, which beats the state-of-the-art model PaCo with\nResNet-152 by +5.3%. This shows BALLAD can not only\nachieve better performance with smaller visual backbone but\nalso save a great amount of training time by skipping the\nImageNet pretraining.\nCIFAR100-LT. We also evaluate the models on CIFAR100-\nLT and show their performances in Table 3. As reported in\nVision Language Many Medium Few All\nrandom random 0.3 0.0 0.0 0.1\nrandom CLIP 0.3 0.0 0.0 0.1\nCLIP random 36.8 2.9 0.0 15.6\nCLIP CLIP 75.5 56.3 41.0 61.6\nTable 4. Ablations of pretrained vision-language weights on\nImageNet-LT dataset. CLIP means using pre-trained weights as\ninitialization and random represents random initialization.\nVision Language Many Medium Few All\n× × 59.4 57.5 57.6 58.2√ × 70.4 65.4 58.0 66.3\n× √ 70.6 65.4 55.9 66.1√ √ 71.3 65.4 54.1 66.1\nTable 5. Different methods of ﬁnetuning CLIP on ImageNet-LT.\n”√” means ﬁnetuning and ”×” means freezing the parameters of\nmodel. Vision and Language denotes the visual and text encoders\nof CLIP respectively. All models are ﬁnetuned for 50 epochs.\nthe table, BALLAD outperforms the state-of-the-art expert-\nbased ensemble methods RIDE [63] and TADE [72] by\n+28.7% and +28.0%, respectively.\n4.3. Ablation Studies\nIn this section, we conduct extensive ablation studies to\nvalidate the design choices of BALLAD from three aspects.\nWe ﬁrst explore how to best utilize vision-language backbone\nfor ﬁnetuning. Moreover, we shows the effectiveness of\nlinear adapter and how to make use of linear adapter for\nbetter performance. Finally, we demonstrate where and how\nto conduct data balancing.\n4.3.1 Vision-Language Models\nWe conduct ablations to demonstrate the effectiveness of\nvision-language backbones as introduced in Section 3.\nThe Effectiveness of Pretrained Weights.In Table 4, we\nvalidate the effectiveness of pretrained CLIP weights com-\npared with random initialized visual and language weights.\nAll the four ablations are conducted on Phase A without data\nbalancing for 50 epochs. The large gaps between random\nand pretrained CLIP initialization demonstrate the advan-\ntage of utilizing pretrained contrastive vision-language mod-\nels. Besides, we ﬁnd that visual encoder has much more\ninﬂuence than text encoder on the performance as random\ninitialized vision encoder drops the accuracy close to zero.\nNote that poor performance of random initialization is pri-\nmarily attributed to short training periods and pretrained\nvision-language weights fastening the convergence largely.\nFinetune the Vision-Language Model. To empirically\ndiscover how to utilize contrastive vision-language models,\nwe probe the ﬁnetuning process by freezing the pre-trained\nimage encoder and text encoder respectively. When both\n6\nRN50 RN101 ViT-B/16 RN50×16\nBackbone\n40\n50\n60\n70\n80T op-1 Acc(%) on ImageNet-LT\nmany shot\nmedium shot\nfew shot\nall\n(a) ImageNet-LT\nRN50 RN101 ViT-B/16 RN50×16\nBackbone\n25\n30\n35\n40\n45\n50\n55T op-1 Acc(%) on Places=LT\nmany shot\nmedium shot\nfew shot\nall (b) Places-LT.\nFigure 3. Comparisons between several visual backbones for\nImageNet-LT (left) and Places-LT (right).\nencoders are frozen, the model directly perform zero-shot\npredictions. From Table 5, we can easily ﬁnd the following\npattern – as more components are ﬁnetuned in CLIP, more\naccuracy improvement is obtained for many-shot categories\nwhereas more accuracy drop happens in few-shot division.\nWe hypothesize it is because themany-shot classes dominate\nthe visual feature space during ﬁnetuning. Therefore, for\nPhase A, it is necessary to adapt CLIP on speciﬁc long-tailed\ndataset as much as possible, and we choose to ﬁnetune both\nthe vision and language branches of CLIP.\nVisual Backbones.We try CLIP with different visual back-\nbones to explore its inﬂuence on ﬁnal performance of BAL-\nLAD. We report the Phase A results of different backbones\nin Figure 3 on both ImageNet-LT and Places-LT benchmarks.\nWhen the visual backbone becomes deeper and larger, the\nﬁnetuned performance is also gradually improved for all,\nmany-shot, and medium-shot categories. Surprisingly, the Vi-\nsion Transformer structure [14] achieves the best accuracy in\nfew-shot subset, probably owing to multi-head self-attention\nmechanism’s ability in capturing minor features.\n4.3.2 Linear Adapter\nWe validate the effectiveness of adopting linear adapter in\nPhase B and explore the key factors that determines the\nperformance of linear adapter.\nThe Effectiveness of Linear Adapter.We design ablations\nto demonstrate the inﬂuence of linear adapter. First, we\nfreeze the parameters of CLIP and ﬁnetune the linear adapter\nfor 10 epochs to mix the original zero-shot visual embedding\nwith the corresponding ﬁnetuned visual features via residual\nconnection. As illustrated in Figure 4, the simple 10-epoch\ntraining improves the performance from 58.2% to 61.8%\neven without data balancing. Moreover, we ﬁnetune both the\nvisual and language encoders of CLIP for50 epochs and then\nﬁnetune the linear adapter for another 10 epochs with CLIP\nparameters ﬁxed. Compared with an equal 60-epoch training\nscheme of ﬁnetuning the visual and language encoder of the\nCLIP, an extra10-epoch ﬁnetuning of linear adapter based\non 50-epoch ﬁnetuning of CLIP backbone can further boost\nthe top-1 accuracy from 66.4% to 67.2%.\nZ-CLIP Z-CLIP+A CLIP Joint-CLIP+A De-CLIP+A\nModels\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67T op-1 Acc on ImageNet-LT\n58.2\n61.8\n66.4 66.0\n67.2\nFigure 4. Ablations of effectiveness of Linear Adapter and decouple\nﬁnetuning. Z-CLIP: zero-shot CLIP model; Z-CLIP+A: ﬁnetune\nadapter based on zero-shot CLIP; CLIP: directly ﬁnetune CLIP;\nJoint-CLIP+A: jointly ﬁnetune CLIP and adapter; De-CLIP+A: the\nBALLAD style which decouples the CLIP and adapter ﬁnetuning\ninto two phases.\nV-Adapter L-Adapter Many Medium Few All\n√ × 71.0 66.3 59.5 67.2\n× √ 71.0 66.2 59.0 67.0√ √ 70.6 66.2 58.4 66.8\nTable 6. Variants of linear adapter. V-Adapter and L-Adapter\nrepresents using linear adapter layer to adapt visual and language\nencoders respectively. All results are trained on ImageNet-LT for\n10 epochs.\nShould the Finetuning of CLIP and Linear Adapter be\nDecoupled? As mentioned in Section 3 and illustrated in\nFigure 2, we decouple the training process into two phases –\nin Phase A, we train both the vision and language encoder of\nCLIP based on pre-trained weights; in Phase B, we freeze the\nparameters of visual and language encoders while only ﬁne-\ntuning the linear adapter. An alternative scheme is to jointly\ntrain the CLIP and linear adapter rather than decoupling the\ntraining processes. According to Figure 4, joint training of\nCLIP and linear adapter (Joint-CLIP+A) leads to a 0.4% ac-\ncuracy drop compared with directly ﬁnetuning CLIP without\nadapter (CLIP). In contrast, the decoupled training of CLIP\nand linear adapter (De-CLIP+A) can largely boost the accu-\nracy from 66.0% to 67.2% and the ascent mainly comes from\ntail classes, which is up to 6.0%. We visualize the joint and\ndecoupling training schemes using t-SNE [60] and present\nthe results in the supplementary. Compared with joint train-\ning, decoupled training better separates the tail-class feature\nembeddings from head-classes. This demonstrates that the\nproposed decoupled training of vision-language model and\nadapter is effective to handle long-tailed distribution.\nVariants of Linear Adapter. Since CLIP has dual en-\ncoders, the auxiliary linear adapter could be added to either\nor both of the two branches. As reported in Table 6, we\ntry linear adapter for adapting visual and language encoders\n7\n0.0 0.2 0.4 0.6 0.8 1.0\nratios\n62\n63\n64\n65\n66\n67T op-1 Acc\nT op-1 Acc of ImageNet-LT with different adapter ratios.\nPhase B\nFigure 5. The top-1 accuracy on ImageNet-LT with different values\nof residual factor λin Phase B of BALLAD.\nrespectively. From the table, we can ﬁnd that applying the\nlinear adapter to the visual branch of CLIP achieves the best\noverall performance and is the optimal choice.\nHyperparameters of the Linear Adapter.Moreover, we\nexplore the inﬂuence of linear adapter’s residual factor λ.\nλ determines the importance of new knowledge obtained\nfrom ﬁnetuning the linear adapter. Note that when λequals\nto 1.0, the classiﬁcation is fully determined by the adapted\nimage features. We explore different values of λfrom 0.0\nto 1.0 and conduct the ablations of Phase B ﬁnetuning on\nImageNet-LT. As shown in Figure 5, the best performance\nof linear adapter can be obtained when λis around 0.2, with\na top-1 accuracy of 67.1% on ImageNet-LT. The empirical\nresults reveal the knowledge of ﬁnetuned CLIP is already\ngood enough to handle most cases, a slight and balanced\nadaptation would further improve the performance.\n4.3.3 Balancing Methods\nBalancing methods can alleviate the severe performance\ndegradation due to class imbalance. In this section, we ex-\nplore different balancing methods for BALLAD to reveal\ntwo signiﬁcant problems: 1) where to utilize balancing meth-\nods, and 2) which balancing methods to apply.\nWhere to balance.Here, we compare balancing the long-\ntailed data distribution on either or both of two phases. The\nexperiments are performed on ImageNet-LT and Places-LT\ndatasets with ResNet-50-backboned CLIP. As mentioned ear-\nlier, many-shot categories dominate the feature space of long-\ntailed distribution. The performance drops of many-shot\ncategories on both datasets, as reported in Table 7, suggest\nthat balancing during Phase A tends to sacriﬁce many-shot\nrepresentations. Since Phase A is mainly designed for updat-\ning representations on a new domain, we thereby abandon\nPhase-A data balancing. When applying balancing strategies\nto Phase B alone, BALLAD can achieve a more balanced\nperformance for different shots and improve the overall top-1\naccuracy thanks to the rich features learned from Phase A.\nHow to balance. Furthermore, we explore different sam-\npling strategies including class-balanced sampling, square-\nDataset Balance Many Medium Few AllPhase A Phase B\nImageNet-LT\n× × 77.3 57.4 39.0 62.6√ × 76.6 58.4 42.7 63.3√ √ 70.7 66.2 58.5 66.9\n× √ 71.0 66.3 59.5 67.2\nPlaces-LT\n× × 52.7 32.9 23.4 38.2√ × 51.3 33.2 25.5 38.2√ √ 44.6 46.7 44.1 45.5\n× √ 46.7 48.0 42.7 46.5\nTable 7. Where to employ balance strategies ablations. On both\nImageNet-LT and Places-LT, balance only in Phase B makeBAL-\nLAD perform the best.\nBalance Methods Many Medium Few All\nClass-balanced 71.0 66.3 59.5 67.2\nSquare-root 75.2 62.8 50.9 66.0\nMix-balanced 72.6 64.9 59.1 67.1\nTable 8. Comparison of different balanced sampling strategies on\nImageNet-LT.\nroot sampling and mix-balanced sampling for Phase B. Class-\nbalanced sampling samples the categories from original\ndataset in equal probability rather than the natural instance-\nbalanced sampling which selects instances regardless of\nclasses. The process can be decoupled into two steps –\nﬁrst selecting classes equally from the list of categories and\nthen randomly sampling a data point from the selected class.\nSquare-root sampling [43] ﬁrst computes the square-root\nof the number of head classes, then re-normalize and con-\nduct sampling according to the resulting distribution. Mix-\nbalanced sampling combines the instance-balanced sampling\nand class-balanced sampling, thus takes advantage of both\nstrategies to avoid overﬁtting at early epochs and under-\nﬁtting at late epochs. Motivated by the [30], we adopt a\nsoft version of mix-balanced sampling to dynamically in-\nterpolates between instance-balanced sampling and class-\nbalanced sampling as learning progresses. As shown in\nTable 8, class-balanced sampling can best beneﬁt medium-\nshot and few-shot categories. Thus we adopt class-balanced\nsampling as the balancing method of BALLAD.\n5. Conclusion\nIn this paper, we proposed BALLAD which tackles long-\ntailed recognition by leveraging contrastive vision-language\nmodels. We decouple BALLAD into two phases for training\nwith long-tailed and balanced samples respectively. We ﬁrst\ncontinue pretraining with contrastive loss to fully utilize\nabundant data to update visual-language representation on\nspeciﬁc domains. After that, we employ an auxiliary linear\nadapter to reﬁne the visual representation of tail classes. We\nhope our simple BALLAD baseline could stimulate more\nfuture researches on exploring vision-language models for\n8\nlong-tailed recognition.\nReferences\n[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision , pages 2425–\n2433, 2015. 2\n[2] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,\nand Tengyu Ma. Learning imbalanced datasets with label-\ndistribution-aware margin loss. In Advances in Neural In-\nformation Processing Systems, pages 1567–1578, 2019. 5,\n6\n[3] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal,\nPiotr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments. In\nAdvances in Neural Information Processing Systems, 2020. 2\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the International Conference on Computer Vision\n(ICCV), 2021. 2\n[5] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834–848, 2017. 1\n[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on\nmachine learning, pages 1597–1607. PMLR, 2020. 2\n[7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\nLearning universal image-text representations. 2019. 2\n[8] Ekin Dogus Cubuk, Barret Zoph, Jon Shlens, and Quoc Le.\nRandaugment: Practical automated data augmentation with\na reduced search space. In Advances in Neural Information\nProcessing Systems, 2020. 6\n[9] Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and\nJiaya Jia. Reslt: Residual learning for long-tailed recognition.\narXiv preprint arXiv:2101.10633, 2021. 2, 3, 5, 6\n[10] Jiequan Cui, Zhisheng Zhong, Shu Liu, Bei Yu, and Jiaya\nJia. Parametric contrastive learning. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 715–724, 2021. 1, 2, 3, 4, 5, 6\n[11] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge\nBelongie. Class-balanced loss based on effective number of\nsamples. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 9268–9277,\n2019. 5\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009. 1, 5\n[13] Qi Dong, Shaogang Gong, and Xiatian Zhu. Class rectiﬁ-\ncation hard mining for imbalanced deep learning. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 1851–1860, 2017. 3\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions, 2021. 7\n[15] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-\nadapter: Better vision-language models with feature adapters.\narXiv preprint arXiv:2110.04544, 2021. 3\n[16] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu, Steven CH\nHoi, Xiaogang Wang, and Hongsheng Li. Dynamic fusion\nwith intra-and inter-modality attention ﬂow for visual question\nanswering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 6639–6648,\n2019. 2\n[17] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Sim-\nple contrastive learning of sentence embeddings. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2021. 2\n[18] Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta,\nKyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. Don’t\nstop pretraining: adapt language models to domains and tasks.\narXiv preprint arXiv:2004.10964, 2020. 4\n[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020. 2\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961–2969, 2017. 1\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1\n[22] Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun\nSeo, Beomsu Kim, and Buru Chang. Disentangling label\ndistribution for long-tailed visual recognition. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6626–6636, 2021. 2, 3\n[23] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-efﬁcient transfer\nlearning for nlp. In International Conference on Machine\nLearning, pages 2790–2799. PMLR, 2019. 4\n[24] Ronghang Hu, Piotr Doll ´ar, Kaiming He, Trevor Darrell,\nand Ross Girshick. Learning to segment every thing. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 4233–4241, 2018. 1\n[25] Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang.\nLearning deep representation for imbalanced classiﬁcation.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 5375–5384, 2016. 3\n9\n[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. In International\nConference on Machine Learning, 2021. 2, 4\n[27] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig.\nHow can we know what language models know?Transactions\nof the Association for Computational Linguistics, 8:423–438,\n2020. 12\n[28] Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng.\nExploring balanced feature spaces for representation learning.\nIn International Conference on Learning Representations ,\n2021. 3\n[29] Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng,\nand Trevor Darrell. Few-shot object detection via feature\nreweighting. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 8420–8429, 2019. 2,\n3\n[30] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan,\nAlbert Gordo, Jiashi Feng, and Yannis Kalantidis. Decoupling\nrepresentation and classiﬁer for long-tailed recognition. In\nInternational Conference on Learning Representations, 2020.\n2, 3, 4, 5, 6, 8\n[31] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 1\n[32] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear\nattention networks. arXiv preprint arXiv:1805.07932, 2018.\n2\n[33] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International journal of computer vision,\n123(1):32–73, 2017. 1\n[34] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimiz-\ning continuous prompts for generation. arXiv preprint\narXiv:2101.00190, 2021. 12\n[35] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli\nOuyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-\npervision exists everywhere: A data efﬁcient contrastive\nlanguage-image pre-training paradigm. arXiv preprint\narXiv:2110.05208, 2021. 2\n[36] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n2117–2125, 2017. 1\n[37] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980–2988, 2017. 3\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014. 1\n[39] Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, and Wen-\nhui Li. Deep representation learning on long-tailed data: A\nlearnable embedding augmentation perspective. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2970–2979, 2020. 3\n[40] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path\naggregation network for instance segmentation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 8759–8768, 2018. 1\n[41] Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang,\nBoqing Gong, and Stella X Yu. Large-scale long-tailed recog-\nnition in an open world. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 2537–2546, 2019. 2, 3, 5, 6, 12\n[42] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 3431–3440, 2015. 1\n[43] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim-\ning He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and\nLaurens Van Der Maaten. Exploring the limits of weakly\nsupervised pretraining. In Proceedings of the European con-\nference on computer vision (ECCV), pages 181–196, 2018.\n8\n[44] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh\nRawat, Himanshu Jain, Andreas Veit, and Sanjiv Kumar.\nLong-tail learning via logit adjustment. In International Con-\nference on Learning Representations, 2021. 2, 3\n[45] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell.\nClip-it! language-guided video summarization. In Advances\nin Neural Information Processing Systems, 2021. 3\n[46] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion\nof visual and language representations by dense symmetric\nco-attention for visual question answering. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6087–6096, 2018. 2\n[47] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 2\n[48] Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang\nYang. Factors in ﬁnetuning deep model for object detection\nwith long-tail distribution. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n864–873, 2016. 3\n[49] Vilfredo Pareto. Cours d’´economie politique. Librairie Droz,\n1964. 1\n[50] Fabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel, Patrick\nLewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\nLanguage models as knowledge bases? In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pages\n2463–2473, 2019. 12\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n10\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\nIn International Conference on Machine Learning, 2021. 2,\n4, 5, 12\n[52] Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao,\nShuai Yi, and Hongsheng Li. Balanced meta-softmax for long-\ntailed visual recognition. arXiv preprint arXiv:2007.10740,\n2020. 5, 6\n[53] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28:91–99, 2015. 1\n[54] Dvir Samuel and Gal Chechik. Distributional robustness\nloss for long-tail learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 9495–\n9504, 2021. 2, 3\n[55] Lei Shi, Kai Shuang, Shijie Geng, Peng Su, Zhengkai\nJiang, Peng Gao, Zuohui Fu, Gerard de Melo, and Sen\nSu. Contrastive visual-linguistic pretraining. arXiv preprint\narXiv:2007.13135, 2020. 2\n[56] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wal-\nlace, and Sameer Singh. Eliciting knowledge from language\nmodels using automatically generated prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235, 2020. 12\n[57] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport:\nWhat and where pathways for robotic manipulation. In Pro-\nceedings of the 5th Conference on Robot Learning (CoRL),\n2021. 3\n[58] Karen Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition. In\nInternational Conference on Learning Representations, 2015.\n1\n[59] Kaihua Tang, Jianqiang Huang, and Hanwang Zhang. Long-\ntailed classiﬁcation by keeping the good and removing the bad\nmomentum causal effect. In Advances in Neural Information\nProcessing Systems, 2020. 2, 3\n[60] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research, 9(11),\n2008. 7, 13\n[61] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip:\nA new paradigm for video action recognition. arXiv preprint\narXiv:2109.08472, 2021. 3\n[62] Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and\nStella Yu. Long-tailed recognition by routing diverse\ndistribution-aware experts. In International Conference on\nLearning Representations, 2020. 5, 6\n[63] Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and\nStella Yu. Long-tailed recognition by routing diverse\ndistribution-aware experts. In International Conference on\nLearning Representations, 2021. 3, 5, 6\n[64] Tz-Ying Wu, Pedro Morgado, Pei Wang, Chih-Hui Ho, and\nNuno Vasconcelos. Solving long-tailed recognition with deep\nrealistic taxonomic classiﬁer. In European Conference on\nComputer Vision, pages 171–189. Springer, 2020. 3\n[65] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017. 1\n[66] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,\nArmen Aghajanyan, Florian Metze, Luke Zettlemoyer, and\nChristoph Feichtenhofer. Videoclip: Contrastive pre-training\nfor zero-shot video-text understanding. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2021. 3\n[67] Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmo-\nhan Chandraker. Feature transfer learning for face recognition\nwith under-represented data. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 5704–5713, 2019. 3\n[68] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep\nmodular co-attention networks for visual question answering.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6281–6290, 2019. 2\n[69] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker,\nJonathan Chien, Maria Attarian, Travis Armstrong, Ivan\nKrasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Trans-\nporter networks: Rearranging the visual world for robotic\nmanipulation. In Proceedings of the 4th Conference on Robot\nLearning (CoRL), 2020. 3\n[70] Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, and\nJian Sun. Distribution alignment: A uniﬁed framework for\nlong-tail visual recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 2361–2370, 2021. 2, 3\n[71] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and\nYu Qiao. Range loss for deep face recognition with long-\ntailed training data. In Proceedings of the IEEE International\nConference on Computer Vision, pages 5409–5418, 2017. 3\n[72] Yifan Zhang, Bryan Hooi, Lanqing Hong, and Jiashi Feng.\nTest-agnostic long-tailed recognition by test-time aggregat-\ning diverse experts with self-supervision. arXiv preprint\narXiv:2107.09249, 2021. 2, 3, 6\n[73] Yifan Zhang, Bingyi Kang, Bryan Hooi, Shuicheng Yan, and\nJiashi Feng. Deep long-tailed learning: A survey. arXiv\npreprint arXiv:2110.04596, 2021. 2, 3\n[74] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2881–2890, 2017. 1\n[75] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen.\nBbn: Bilateral-branch network with cumulative learning\nfor long-tailed visual recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9719–9728, 2020. 2, 3\n[76] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 40(6):1452–1464, 2017. 5\n[77] Linchao Zhu and Yi Yang. Inﬂated episodic memory with\nregion self-attention for long-tailed visual recognition. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4344–4353, 2020. 3\n11\n[78] George Kingsley Zipf. Human behavior and the principle\nof least effort: An introduction to human ecology. Addison-\nWesley Press, Inc., 1949. 1\nAppendices\nThis supplementary ﬁle includes more details and results\nof BALLAD that were not contained in the main manuscript\ndue to the limited paper size. We start with elaborating the\ntwo-phase training of BALLAD in Appendix A. Then we\nprovide the zero-shot long-tailed recognition performance of\nthe vision-language model CLIP in three benchmark datasets\n(Appendix B). Moreover, we perform ablations of different\ntext prompts in Appendix C. Appendix D shows visualiza-\ntion results of the decoupled training of CLIP and linear\nadapter as discussed in Sec. 4.3.2 in main manuscript.\nA. Algorithm\nWe provide the pseudo code of training BALLAD as\nshown in Algorithm 1. In phase A, we keep training the\nvision-language model, updating parameters of both visual\nand language encoders. Afterward, in phase B, we train a\nsingle linear adapter with vision-language model frozen to\nadapt visual features in a balanced way.\nB. Zero-shot Performance\nThe zero-shot long-tailed recognition performances on\nthree benchmark datasets are presented in Table 9. The red\nnumbers show how much BALLAD has improved com-\npared with zero-shot CLIP results. It illustrates the training\nscheme of BALLAD is effective as raising the initial vision-\nlanguage model by a large margin (+12.2% maximally).\nC. Text Prompting\nPrompt engineering is initially proposed for knowledge\nprobing in large pretrained language models [27, 34, 50, 56].\nPrompting is adding extra instructions to task inputs to gen-\nerate speciﬁc outputs from pretrained language model. In\nthis paper, we utilize manually designed prompts following\nCLIP [51]. Speciﬁcally, a prompt template like a photo of\na {CLASS}is adopted to all experiments reported in main\nmanuscript. However, CLIP [51] claims that ensembling\nseveral classiﬁers using different hand-crafted prompts as\nfollows can improve the performance of zero-shot tasks.\n• itap of a {CLASS}.\n• a bad photo of the {CLASS}.\n• a origami {CLASS}.\n• a photo of the large {CLASS}.\n• a {CLASS}in the video game.\nAlgorithm 1Two-phases training of BALLAD\nRequire: Training samples {(I,y)}, visual and language\nencoder Venc, Lenc, linear adapter LA\nInitialize Venc, Lenc with web-data pretrained parameters\nΘv and Θl\nfor epoch = 1,...,N A do ⊿Phase A\nfor minibatch B ∈{(I,y)}do\nfv ←Venc(I) ∈Rdv\nT ←tokenize(y)\nfl ←Lenc(T) ∈Rdl\nProject into embedding space u,v as Eq.(1)\npi ←\nexp(v⊤ui)/τ∑K\nj=1 exp(v⊤uj)/τ\nUpdate Θv and Θl\nend for\nend for\nInitialize ΘLA randomly for LA ⊿Phase B\nFreeze Θv and Θl\nfor epoch = 1,...,N B do\nfor minibatch B ∈{Balanced(I,y)}do\nfv ←λLA(Venc(I)) + (1−λ)Venc(I) ∈Rdv\nT ←tokenize(y)\nfl ←Lenc(T) ∈Rdl\nProject into embedding space u,v as Eq.(1)\npi ←\nexp(v⊤ui)/τ∑K\nj=1 exp(v⊤uj)/τ\nUpdate ΘLA\nend for\nend for\n• art of the {CLASS}.\n• a photo of the small {CLASS}.\nTherefore, we perform ablations on ensembling the multi-\nple prompts and randomly choosing one of them for language\nmodel training. Ablations are conducted on ImageNet-LT\nbenchmark [41] with ResNet-50 visual backbone. Results\nare reported in Table 10. Surprisingly, prompts ensembling\ndecays performance from67.2% to 66.8% rather than raising\naccuracy. Randomly choosing a template from above seven\nprompts also results in performance drop, by 0.3% overall\naccuracy. We hypothesize different prompting templates\nfrom multiple views may confuse the pretrained language\nmodel ﬁnetuning, which is different from zero-shot task.\nThus, we choose a single template a photo of a {CLASS}in\nall our ablations.\nD. Visualization\nAs discussed in Sec. 4.3.2 in main manuscript, decou-\npled training of vision-language model and linear adapter\nlargely boost the performance, especially for few-shot cat-\negories. We visualize the classiﬁcation space of several\n12\nVisual Backbone ImageNet-LT Places-LT CIFAR100-LT\nzero-shot BALLAD zero-shot BALLAD zero-shot BALLAD\nResNet-50 58.2 67.2 (+9) 35.3 46.5 (+11.2) 40.2 51.6 (+11.4)\nResNet-101 61.2 70.5 (+9.3) 36.2 47.9 (+11.7) 47.8 59.2 (+11.4)\nViT-B/16 66.7 75.7 (+9) 37.8 49.5 (+11.7) 66.4 77.8 (+11.4)\nResNet-50×16 69.0 76.5 (+7.5) 37.1 49.3 (+12.2) 52.9 63.7 (+10.8)\nTable 9. Top-1 accuracy of zero-shot CLIP and BALLAD-training.\nText Prompts Many Medium Few All\nSingle prompt 71.0 66.3 59.5 67.2\nRandom single prompt 70.5 66.1 59.8 66.9\nEnsemble prompts 70.5 66.0 59.5 66.8\nTable 10. Comparison of different balanced sampling strategies on\nImageNet-LT.\nfew-shot categories using t-SNE [60] as shown in Fig. 6. It\nis clearly illustrated in Sub-ﬁgure (a) that decoupled training\nachieves much more obvious separation boundary among\ndifferent classes, especially for some easily confusing ones\nsuch as kingsnake (purple), water snake (brown) and sea\nsnake (pink).\n40\n 30\n 20\n 10\n 0 10 20\n20\n10\n0\n10\n20\n(a) Decoupled training.\n20\n 10\n 0 10 20\n30\n20\n10\n0\n10\n20\n30\n40\n50\n(b) Joint training.\n(c) Legends\nFigure 6. Comparisons of training vision-language model and\nlinear adapter decoupled and jointly.\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7911338806152344
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6055156588554382
    },
    {
      "name": "Language model",
      "score": 0.5087612271308899
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.44415199756622314
    },
    {
      "name": "Machine learning",
      "score": 0.41945868730545044
    },
    {
      "name": "Natural language processing",
      "score": 0.39884379506111145
    }
  ]
}