{
  "title": "Transformer-based Automatic Post-Editing Model with Joint Encoder and Multi-source Attention of Decoder",
  "url": "https://openalex.org/W2970326214",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5011535993",
      "name": "WonKee Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074187931",
      "name": "Jaehun Shin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101532821",
      "name": "Jong-Hyeok Lee",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2902757896",
    "https://openalex.org/W2902409908",
    "https://openalex.org/W2963816901",
    "https://openalex.org/W2902385320",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2791510479",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W3006530332",
    "https://openalex.org/W2963344439",
    "https://openalex.org/W2962826395",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2970871182",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962678612",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "This paper describes POSTECHâ€™s submission to the WMT 2019 shared task on Automatic Post-Editing (APE). In this paper, we propose a new multi-source APE model by extending Transformer. The main contributions of our study are that we 1) reconstruct the encoder to generate a joint representation of translation (mt) and its src context, in addition to the conventional src encoding and 2) suggest two types of multi-source attention layers to compute attention between two outputs of the encoder and the decoder state in the decoder. Furthermore, we train our model by applying various teacher-forcing ratios to alleviate exposure bias. Finally, we adopt the ensemble technique across variations of our model. Experiments on the WMT19 English-German APE data set show improvements in terms of both TER and BLEU scores over the baseline. Our primary submission achieves -0.73 in TER and +1.49 in BLEU compare to the baseline.",
  "full_text": "Proceedings of the Fourth Conference on Machine Translation (WMT), V olume 3: Shared Task Papers (Day 2) pages 112â€“117\nFlorence, Italy, August 1-2, 2019.câƒ2019 Association for Computational Linguistics\n112\nAbstract \nThis paper describes POSTECHâ€™s submis-\nsion to the WMT 2019 shared task on Au-\ntomatic Post-Editing (APE). In this paper, \nwe propose a new multi-source APE mod-\nel by extending Transformer. The main \ncontributions of our study are that we 1) \nreconstruct the encoder to generate a joint \nrepresentation of translation (mt) and its \nsrc context, in addition to the conventional \nsrc encoding and 2) suggest two types of \nmulti-source attention layers to compute \nattention between two output s of the en-\ncoder and the decoder state in the decoder. \nFurthermore, we train our model by apply-\ning various teacher -forcing ratios to alle-\nviate exposure bias. F inally, we adopt the \nensemble technique across variations of \nour model . Experiments on the WMT19 \nEnglish-German APE data set show im-\nprovements in terms of both TER and \nBLEU scores over the baseline. Our pri-\nmary submission achieves -0.73 in TER \nand +1.49 in BLEU compared to the base-\nline, and ranks second among all submit-\nted systems. \n1 Introduction  \nAutomatic Post-Editing (APE) is the task of au-\ntomatically correcting errors in a given the ma-\nchine translation (MT) output to generate a better \ntranslation (Chatterjee et al., 2018). Because APE \ncan be regarded as a sequence -to-sequence prob-\nlem, MT techniques have been previously applied \nto t his task. Subsequently, it is only natural that \nneural APE has been proposed following  the ap-\npearance of neural machine translation (NMT). \nAmong the initial approaches to neural APE, a \nlog-linear combination model (Junczys-Dowmunt \nand Grundkiewicz, 2016) that combines bilingual \n                                                           \n* Both authors equally contributed to this work \nand monolingual translations yielded the best re-\nsults. Since then, In order to leverage information \nfrom both MT outputs (mt) and its corresponding \nsource sentences ( src), a multi -encoder model \n(LibovickÃ½ et al., 2016 ) based on multi -source \ntranslation (Zoph and Knight, 2016 ) has become \nthe prevalent approach (Bojar et al., 2017 ). Re-\ncently, with the advent of Transformer (Vaswani \net al., 2017 ), most of the participants in the \nWMT18 APE shared task proposed Transformer-\nbased multi-encoder APE mode ls (Chatterjee et \nal., 2018). \nPrevious multi -encoder APE models employ \nseparate encoders for each  input (src, mt), and \ncombine their outputs in various ways: by 1) se-\nquentially applying attention between the hidden \nstate of the decoder and the two outputs (Junczys-\nDowmunt and Grundkiewicz, 2018; Shin and Lee, \n2018) or 2) simply concatenating them (Pal et al., \n2018; Tebbifakhr et al., 2018 ). However, these \napproaches seem to overlook one of the key dif-\nferences between general multi-source translation \nand APE. Because the errors mt may contain are \ndependent on the MT system, the encoding pro-\ncess for mt should reflect its relationship with the \nsource sentence. Furthermore, we believe that it \nwould be helpful to incorporate information from \nthe source sentence, which should ideally be er-\nror-free, in addition to the jointly encoded mt in \ngenerating post-edited sentence. \nFrom these points of view, we propose a multi-\nsource APE model by extending Transformer to \ncontain a joint multi-source encoder and a decod-\ner that involves a multi-source attention layer to \ncombine the outputs of the en coder. Apart from \nthat, we apply various teacher -forcing ratios at \ntraining time to alleviate exposure bias. Finally, \nwe ensemble model variants for our submission. \nThe remainder of the paper is organized as fol-\nlows: Section 2 describes our model architecture. \nTransformer-based Automatic Post-Editing Model  \nwith Joint Encoder and Multi-source Attention of Decoder \n \n \nWonKee Lee*, Jaehun Shin*, Jong-hyeok Lee \nDepartment of Computer Science and Engineering, \nPohang University of Science and Technology (POSTECH), Republic of Korea \n{wklee, jaehun.shin, jhlee}@postech.ac.kr \n \n \n \n113\nSection 3 summarizes the experimental results, \nand Section 4 gives the conclusion.  \n2 Model Description \nWe adopt Transformer to the APE problem, which \ntakes multiple inputs (src, mt) to generate a post-\nedited sentence (pe). In the following subsections, \nwe describe our modified encoder and decoder.  \n2.1 Encoder \nThe proposed encoder structure for multi -\nsource inputs, as shown in  Figure 1, is an exten-\nsion of what is introduced in Vaswani et al. (2017) \ndeveloped considering single-source input. Simi-\nlar to recent APE studies, our  encoder receives \ntwo sources: src x = (ğ‘¥1,â€¦,ğ‘¥ğ‘‡ğ‘¥)  and mt y =\n(ğ‘¦1,â€¦,ğ‘¦ğ‘‡ğ‘¦), where ğ‘‡ğ‘¥  and ğ‘‡ğ‘¦  denote their se-\nquence lengths respectively, but produce the joint \nrepresentation Eğ‘—ğ‘œğ‘–ğ‘›ğ‘¡ = (ğ‘’1\nğ‘—,â€¦,ğ‘’ğ‘‡ğ‘¦\nğ‘— ), in addition to \nencoded src Eğ‘ ğ‘Ÿğ‘ = (ğ‘’1\nğ‘ ,â€¦,ğ‘’ğ‘‡ğ‘¥\nğ‘  ).  \nJoint representation. Unlike previous studies, \nwhich independently encode two input sources \nusing separate encoding modules, we incorporate \nsrc context information into each hidden state of \nmt through the single encoding module, resulting \nin a joint representation of two sources. As shown \nwith the dashed square in Figure 1, jointly repre-\nsented hidden states are obtained from the residu-\nal connection and multi-head attention that takes \nğ»ğ‘ ğ‘Ÿğ‘ âˆˆ â„ğ‘‡ğ‘¥Ã—ğ‘‘  as keys and values and ğ»ğ‘šğ‘¡ âˆˆ\nâ„ğ‘‡ğ‘¦Ã—ğ‘‘ as queries. Therefore, the joint representa-\ntion of each level of the stack (ğ‘– = 1,â€¦,ğ‘) can be \nexpressed with  MultiHead(Q, K, V) and Lay-\nerNorm described in Vaswani et al. (2017) as fol-\nlows: \n ğ»ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡\nğ‘– = LayerNorm(ğ»ğ‘šğ‘¡\nğ‘– + ğ¶ğ‘ ğ‘Ÿğ‘\nğ‘– )   \nwhere \n ğ¶ğ‘ ğ‘Ÿğ‘\nğ‘– = MultiHead(ğ»ğ‘šğ‘¡\nğ‘– ,ğ»ğ‘ ğ‘Ÿğ‘\nğ‘– ,ğ»ğ‘ ğ‘Ÿğ‘\nğ‘– ) (1)  \nStack-level attention. When applying attention \nacross source and target, the original Transformer \nonly considers source hidden states retrieved from \nthe final stack , whereas our encoder feeds into \neach attention layer the src embeddings from the \nsame level, as can be seen in (1).  \nMasking option. The self-attention layer that is \nthe first attention layer of the mt encoding module \noptionally includes a future mask, which mimics \nthe general decoding process of MT systems that \ndepends only on previously generated words. We \nconduct experiments ( Â§3.2) for two cases: with \nand without this option.  \n2.2 Decoder \nOur decoder is an extension of Transformer de-\ncoder, in which the second multi-head attention \nlayer that originally only refers to single -source \n \nFigure 2: The architecture of the decoder \n \nEncoder\nInput \nEmbedding\nMasked\nMulti-Head \nAttention\nAdd & Norm\nFeed \nForward\nAdd & Norm\nMulti-source \nAttention\nSoftmax\nLinear\nOutput\nProbabilities\nÃ—N\npe\n(Shfited right)\nPositional\nEncoding\n \nFigure 1: The architecture of the proposed encoder  \nâ€“ the dashed square indicates the joint hidden repre-\nsentation of two sources \n \n-  \nInput \nEmbedding\nMulti-Head \nAttention\nAdd & Norm\nFeed \nForward\nInput \nEmbedding\n(Masked)\nMulti-Head \nAttention\nFeed \nForward\nÃ—N\nsrc mt\nPositional\nEncoding\nMulti-Head \nAttention\nAdd & Norm\nAdd & Norm\nAdd & Norm\nEncoder\n          \nAdd & Norm\n114\nencoder states is replaced with a multi-source at-\ntention layer. Figure 2 shows our decoder archi-\ntecture including the multi-source attention layer \nthat attends to both outputs of the encoder. Fur-\nthermore, we construct two types of the multi-\nsource attention layer by utilizing different strate-\ngies in combining attention over two encoder out-\nput states.  \nMulti-source parallel attention. Figure 3a illus-\ntrates the structure of parallel attention.  The de-\ncoder's hidden state simultaneously attends to  \neach output of the multi-source encoder, followed \nby residual connection, and the results are linearly \ncombined by summing them at the end: \n ğ»ğ‘ğ‘ğ‘Ÿğ‘ğ‘™ğ‘™ğ‘’ğ‘™ = ğ»1 + ğ»2   \nwhere \n \nğ»1 = LayerNorm(ğ»ğ‘ğ‘’ + ğ¶ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡) \nğ»2 = LayerNorm(ğ»ğ‘ğ‘’ + ğ¶ğ‘ ğ‘Ÿğ‘) \nğ¶ğ½ğ‘œğ‘–ğ‘›ğ‘¡ = MultiHead(ğ»ğ‘ğ‘’,ğ¸ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡,ğ¸ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡) \nğ¶ğ‘ ğ‘Ÿğ‘ = MultiHead(ğ»ğ‘ğ‘’,ğ¸ğ‘ ğ‘Ÿğ‘,ğ¸ğ‘ ğ‘Ÿğ‘).  \n \nNote that ğ»ğ‘ğ‘’ âˆˆ â„ğ‘‡ğ‘§Ã—ğ‘‘ denotes the hidden states \nfor decoder input pe z = (ğ‘§1,â€¦,ğ‘§ğ‘‡ğ‘§). \nMulti-source sequential attention. As shown in \nFigure 3b, two outputs of the encoder are sequen-\ntially combined with the decoderâ€™s hidden state: \nğ¸ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡ and the decoderâ€™s hidden state are first \nassigned to multi-head attention and residual con-\nnection layers, then the same operation is per-\nformed between the result and ğ¸ğ‘ ğ‘Ÿğ‘.  \n ğ»ğ‘ ğ‘’ğ‘ = LayerNorm(ğ»â€² + ğ¶ğ‘ ğ‘Ÿğ‘)    \nwhere \n \nğ»â€² = LayerNorm(ğ»ğ‘ğ‘’ + ğ¶ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡) \nğ¶ğ‘ ğ‘Ÿğ‘ = MultiHead(ğ»â€²,ğ¸ğ‘ ğ‘Ÿğ‘,ğ¸ğ‘ ğ‘Ÿğ‘) \nğ¶ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡ = MultiHead(ğ»ğ‘ğ‘’,ğ¸ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡,ğ¸ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡).  \n \nThis approach is structurally equivalent to \nJunczys-Dowmunt and Grundkiewicz ( 2018), \nexcept that the encoder states being passed on are \ndifferent. \n3 Experiments \n3.1 Dataset \nWe used the WMT19 official English -German \nAPE dataset (Chatterjee et al., 2018) which con-\nsists of a training and development set. In addition, \nwe adopted the eSCAPE NMT dataset (Negri et \nal., 2018) as additional training data. We extracted \nsentence triplets from the eSCAPE -NMT dataset \naccording to the following criteria, to  which the \nofficial training dataset mostly adheres . Selected \ntriplets have no more than 70 words  in each sen-\ntence, a TER less than or equal to 75, and a recip-\nrocal length ratio within the monolingual pair (mt, \npe) less than 1.4. Table 1 summarizes the statistic \nof the datasets. \n3.2 Training Details \nSettings. We modified the OpenNMT-py (Klein \net al., 2017 ) implementation of Transformer to \nbuild our models. Most hyperparameters such as \nthe dimensionality of hidden states, optimizer set-\ntings, dropout ratio, etc. were copied from the \nâ€œbase modelâ€ described in Vaswani et al. (2017). \nWe adjusted the warm-up learning steps and batch \nsize per triplets to 18k and ~25k, respectively. For \ndata preprocessing, we employed subword encod-\ning (Kudo, 2018) with 32k shared vocabulary. \n \n(a) \n \n(b) \nFigure 3: Illustrations of the multi -source attention \nlayer. (a) and (b) refer to the linear and sequential \ncombinations, respectively. \n   \n    \n      \nMulti-Head \nAttention\nMulti-Head \nAttention\nAdd & Norm Add & Norm\n   \n      \nMulti-Head \nAttention\nMulti-Head \nAttention\nAdd & Norm\nAdd & Norm\n    \nDataset Triplets TER \nofficial training set 13,442 14.89  \nofficial development set 1,000 15.08  \neSCAPE-NMT 7,258,533 60.54  \neSCAPE-NMT-filtered 4,303,876 39.65  \nTable 1: Dataset statistics  â€“ number of sentence tri-\nplets (src, mt, pe) and TER score. \n115\nTwo-step training.  We separated the training \nprocess into two steps: the first phase for training \na generic model, and the second phase to fine -\ntune the model. For the first phase, we trained the \nmodel with a union dataset that is the concatena-\ntion of  eSCAPE-NMT-filtered, and the upsam-\npled official training set by copying 20 times. Af-\nter reaching the convergence point in the first \nphase, we fine-tuned the mod el by running the \nsecond phase using only the official training set.  \nModel variations. In our experiment, we con-\nstructed four types of models in terms of the ex-\nistence of the encoder future mask and the type of \nthe multi-source attention layer in the decoder as \nfollows:  \nï‚· Parallel w/ masking  where the model \ninvolves the multi -source parallel atten-\ntion layer with the encoder mask. \nï‚· Parallel w/o masking  in which the en-\ncoder mask is excluded from Parallel w/ \nmasking.  \nï‚· Sequential w/ masking where the mod-\nel involves the multi -source sequential \nattention layer with the encoder mask. \nï‚· Sequential w/o masking  in which the \nencoder mask is  excluded from Seq. w/ \nmasking. \nTeacher-forcing ratio. During training, because \nthe decoder takes as input the target shifted to the \nright, the ground -truth words are passed to the \ndecoder. However, at inference time, the decoder \nconsumes only previously produced output words, \ncausing exposure bias. To overcome this problem, \nwe have empirically adjusted the teacher -forcing \nratio in  the second phase  of training , so that \nteacher-forcing is applied stochastically in such a \nway that given a ratio ğ›¼, the greedy decoding \noutput of the previous step is fed into the next \ninput with a probability of 1 âˆ’ ğ›¼. \nEnsemble. To leverage all variants in different \narchitectures and teacher-forcing ratios, we com-\nbined them using an ensemble approach  accord-\ning to the following three criteria: \nï‚· Ens_set_1: top-N candidates among  all \nvariants in terms of TER.  \nï‚· Ens_set_2: top-N candidates for variant s \nin each architecture, in terms of TER. \nï‚· Ens_set_3: two candidates for variants in \neach architecture , achieving the best \nTER and BLEU scores, respectively. \n3.3 Results  \nWe trained a generic model for each of the four \nmodel variations mentioned in Â§3.2. Then, we \nfine-tuned those models using various teacher-\nforcing ratio s. For evaluation, we use d TER \n(Snover et al., 2006) and BLEU (Papineni et al., \n2002) scores on the WMT official develop ment \ndataset. Table 2 shows the scores of the generic \nand fine-tuned models according to their architec-\ntures and teacher-forcing ratios. The result shows \nthat adjusting teacher-forcing ratio helps improve \nthe post-editing performance of the models. \nTable 3 gives the results of the ensemble mod-\nels. The ensemble models had slightly worse TER \nscores (+0.02 ~ +0.13) than the best TER score in \nthe fine -tuned variants, but better BLEU  scores \n(+0.09 ~ +0.27) than the best BLEU score. We \nTeacher-\nforcing \nRatios \n Architecture \n \nParallel \nw/ masking  \nParallel \n w/o masking  \nSequential \n w/ masking  \nSequential \nw/o masking \n  TER BLEU   TER BLEU   TER BLEU   TER BLEU \nw/o tuning  15.06  77.18   15.03  77.29   14.89  77.38   15.10  77.19  \n1.00   15.02  77.25   14.95  77.41   14.83  77.54   14.75  77.68  \n0.95   15.07  77.24   14.94  77.24   14.83  77.41   14.53  77.36  \n0.90   14.75  77.54   14.94  77.26   14.79  77.40   14.99  77.26  \n0.85   14.86  77.37   14.95  77.30   14.73  77.50   14.76  77.56  \n0.80    14.98  77.06    14.93  77.15    14.83  77.44    15.34  76.79  \nTable 2: Results of training variants â€“ the columns correspond to their architectures and the rows correspond to \ntheir teacher -forcing ratios. The bold values indicate the best result in the metrics for each architecture.  â€œw/o \ntuning\" refer to generic model. \n116\nselected the three best ensemble models for sub-\nmission, expecting to reap the benefits from lev-\neraging different architectures in the decoding \nprocess. The names and types for submission are \nnoted in Table 3.  \nSubmission results. The results of primary and \ncontrastive submission on the official test set are  \nreported in Table 4. Our primary submission \nachieves improvements of -0.73 in TER and \n+1.49 in BLEU compared to the baseline , and  \nshows better results than the state-of-the-art of the \nlast round with -0.35 in TER and +0.69 in BLEU. \nWhile our primary system ranks second out of 18 \nsystems submitted this year, it shows the highest \nBLEU score. \n4 Conclusion \nIn this paper, we present POSTECH's submissions \nto the WMT19 APE shared task. We propose a \nnew Transformer-based APE model comprising a \njoint multi-source encoder and a decoder with two \ntypes of multi -source attention layers . The pro-\nposed encoder generates a joint representation for \nMT output with optional masking, in addition to \nthe e ncoded source sentence. The proposed de-\ncoder employs two types of multi-source attention \nlayers according to the post-editing strategy. We \nrefine the eSCAPE-NMT dataset and apply two -\nstep training with various teacher -forcing ratios. \nFinally, our ensemble models showed improve-\nments in terms of both TER and BLEU, and out-\nperform not only the baseline but also the best \nmodel from the previous round of the task.  \nReferences \nOndÅ™ej Bojar, Rajen Chatterjee, Christian Federmann, \nYvette Graham, Barry Haddow, Shujian Huang, \nMatthias Huck, Philipp Koehn, Qun Liu, and \nVarvara Logacheva. 2017. Findings of the 2017 \nconference on machine translation (wmt17) . In \nProceedings of the Second Conference on Machine \nTranslation, page 169-214. \nRajen Chatterjee, Matteo Negri, Raphael Rubino, and \nMarco Turchi. 2018. Findings of the WMT 2018 \nShared Task on Automatic Post -Editing. In \nProceedings of the Third Conference on Machine \nTranslation: Shared Task Papers, page 710-725. \nMarcin Junczys-Dowmunt, and Roman Grundkiewicz. \n2016. Log -linear Combinations of Monolingual \nand Bilingual Neural Machine Translation Models \nfor Automatic Post -Editing. In Proceedings of the \nSystems   TER BLEU \nUNBABEL_Primary  16.06  75.96  \nPOSTECH_Primary (top2Ens8)  16.11  76.22  \nPOSTECH_Contrastive (var2Ens8)  16.13  76.21  \nUSSAR-DFKI_Contrastive  16.15  75.75  \nPOSTECH_Contrastive (top1Ens4)   16.17  76.15  \nTebbifakhr et al. (2018)  16.46  75.53  \nJunczys-Dowmunt and Grundkiewicz (2018)  16.50  75.44  \nShin and Lee (2018)  16.70  75.14  \nBaseline   16.84  74.73  \nTable 4: Submission results â€“ the top-5 systems among official results of the WMT19 APE shared task. We \nalso include the previous round results for comparison. The bold values indicate the best result in each metric. \nModels   TER BLEU   Submission Name \nEns_set_1-top4  14.66  77.79   - \nEns_set_1-top6  14.62  77.79   - \nEns_set_1-top8  14.62  77.81   - \nEns_set_2-top1  14.58  77.86   Contrastive (top1Ens4) \nEns_set_2-top2  14.55  77.95   Primary (top2Ens8) \nEns_set_3   14.61  77.86    Contrastive (var2Ens8) \nTable 3: Results of ensemble models  â€“ â€œSubmission Nameâ€ indicates the names (types) for the submission. \nThe bold values indicate the best result in each metric. \n \n \n117\nFirst Conference on Machine Translation: Volume \n2, Shared Task Papers, page 751-758. \nMarcin Junczys-Dowmunt, and Roman Grundkiewicz. \n2018. MS -UEdin Submission to the WMT2018 \nAPE Shared Task: Dual -Source Transformer for \nAutomatic Post -Editing. In Proceedings of the \nThird Conference on Machine Translation: Shared \nTask Papers, page 822-826. \nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean \nSenellart, and Alexander Rush. 2017. OpenNMT: \nOpen-Source Toolkit for Neural Machine \nTranslation. Proceedings of ACL 2017, System \nDemonstrations, 67-72.  \nTaku Kudo. 2018. Subword Regularization: \nImproving Neural Network Translation Models \nwith Multiple Subword Candidates. In Proceedings \nof the 56th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long \nPapers), page 66-75. \nJindÅ™ich LibovickÃ½, JindÅ™ich Helcl, Marek TlustÃ½, \nOndÅ™ej Bojar, and Pavel P ecina. 2016. CUNI \nSystem for WMT16 Automatic Post -Editing and \nMultimodal Translation Tasks . In Proceedings of \nthe First Conference on Machine Translation: \nVolume 2, Shared Task Papers, page 646-654. \nMatteo Negri, Marco Turchi, Rajen Chatterjee, and \nNicola Bertoldi. 2018. ESCAPE: a Large -scale \nSynthetic Corpus for Automatic Post -Editing. In \nProceedings of the Eleventh International \nConference on Language Resources and \nEvaluation (LREC-2018). \nSantanu Pal, Nico Herbig, Antonio KrÃ¼ger, and Josef \nvan Genabith. 2018. A Transformer -Based Multi -\nSource Automatic Post -Editing System . In \nProceedings of the Third Conference on Machine \nTranslation: Shared Task Papers, page 827-835. \nKishore Papineni, Salim Roukos, Todd Ward, and \nWei-Jing Zhu. 2002. BLEU: a method for \nautomatic evaluation of machine translation . In \nProceedings of the 40th annual meeting on \nassociation for computational linguistics , page \n311-318. \nJaehun Shin, and Jong -hyeok Lee. 2018. Multi -\nencoder Transformer Network for Automatic Post -\nEditing. In Proceedings of the Third Conference on \nMachine Translation: Shared Task Papers , page \n840-845. \nMatthew Snover, Bonnie Dorr, Richard Schwartz, \nLinnea Micciulla, and John Makhoul. 2006. A \nstudy of translation edit rate with targeted human \nannotation. In Proceedings o f association for \nmachine translation in the Americas. \nAmirhossein Tebbifakhr, Ruchit Agrawal, Matteo \nNegri, and Marco Turchi. 2018. Multi -source \ntransformer with combined losses for automatic \npost editing . In Proceedings of the Third \nConference on Machine  Translation: Shared Task \nPapers, page 846-852. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz \nKaiser, and Illia Polosukhin. 2017. Attention is all \nyou need . In Advances in Neural Information \nProcessing Systems, page 5998-6008. \nBarret Zoph, and Kevin Knight. 2016. Multi -Source \nNeural Translation. In Proceedings of NAACL-HLT, \npage 30-34. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8292022347450256
    },
    {
      "name": "Transformer",
      "score": 0.7912969589233398
    },
    {
      "name": "Encoder",
      "score": 0.7872351408004761
    },
    {
      "name": "Speech recognition",
      "score": 0.5276250243186951
    },
    {
      "name": "Machine translation",
      "score": 0.5224101543426514
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5202451944351196
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48821118474006653
    },
    {
      "name": "Decoding methods",
      "score": 0.4869118928909302
    },
    {
      "name": "Joint (building)",
      "score": 0.46252378821372986
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4525030851364136
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4247519373893738
    },
    {
      "name": "Natural language processing",
      "score": 0.36649608612060547
    },
    {
      "name": "Algorithm",
      "score": 0.2892270088195801
    },
    {
      "name": "Voltage",
      "score": 0.09103161096572876
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}