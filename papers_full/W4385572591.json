{
  "title": "Bhattacharya_Lab at SemEval-2023 Task 12: A Transformer-based Language Model for Sentiment Classification for Low Resource African Languages: Nigerian Pidgin and Yoruba",
  "url": "https://openalex.org/W4385572591",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5111148056",
      "name": "Nathaniel Hughes",
      "affiliations": [
        "Auburn University at Montgomery"
      ]
    },
    {
      "id": "https://openalex.org/A5015955474",
      "name": "Kevan Baker",
      "affiliations": [
        "Florida Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A2159180551",
      "name": "Aditya Singh",
      "affiliations": [
        "Auburn University at Montgomery"
      ]
    },
    {
      "id": "https://openalex.org/A5100574390",
      "name": "Aryavardhan Singh",
      "affiliations": [
        "Auburn University at Montgomery"
      ]
    },
    {
      "id": "https://openalex.org/A5092596382",
      "name": "Tharalillah Dauda",
      "affiliations": [
        "Auburn University at Montgomery"
      ]
    },
    {
      "id": "https://openalex.org/A2910699993",
      "name": "Sutanu Bhattacharya",
      "affiliations": [
        "Auburn University at Montgomery"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6603175991",
    "https://openalex.org/W4385570519",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W2978357053",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W4287997482",
    "https://openalex.org/W2992924668",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3134606166",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2019759670",
    "https://openalex.org/W4321392937",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3198429080",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4221155692",
    "https://openalex.org/W4210827551",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "Sentiment Analysis is an aspect of natural languageprocessing (NLP) that has been a topicof research. While most studies focus on highresourcelanguages with an extensive amountof available data, the study on low-resource languageswith insufficient data needs attention.To address this issue, we propose a transformerbasedmethod for sentiment analysis for lowresourcesAfrican languages, Nigerian Pidginand Yoruba. To evaluate the effectiveness ofour multilingual language models for monolingualsentiment classification, we participated inthe AfriSenti SemEval shared task 2023 competition.On the official e valuation s et, ourgroup (named as Bhattacharya_Lab) ranked1 out of 33 participating groups in the MonolingualSentiment Classification task (i.e., TaskA) for Nigerian Pidgin (i.e., Track 4), and inthe Top 5 among 33 participating groups inthe Monolingual Sentiment Classification taskfor Yoruba (i.e., Track 2) respectively, demonstratingthe potential for our transformer-basedlanguage models to improve sentiment analysisin low-resource languages. Overall, ourstudy highlights the importance of exploringthe potential of NLP in low-resource languagesand the impact of transformer-based multilinguallanguage models in sentiment analysis forthe low-resource African languages, NigerianPidgin and Yoruba.",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 1502–1507\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nBhattacharya_Lab at SemEval-2023 Task 12: A Transformer-based\nLanguage Model for Sentiment Classification for Low Resource African\nLanguages: Nigerian Pidgin and Yoruba\nNathaniel Hughes1,+, Kevan Baker2,+, Aditya Singh1,\nAryavardhan Singh1, Tharalillah Dauda1, and Sutanu Bhattacharya1,*\n1Department of Computer Science and Computer Information Systems,\nAuburn University at Montgomery, Montgomery, AL, USA\n2Department of Computer Science, Florida Polytechnic University, Lakeland, FL, USA \n1{nhughes4,asingh7,asingh8,tdauda,sbhatta4}@aum.edu\n2{kbaker5651}@floridapoly.edu\nAbstract\nSentiment Analysis is an aspect of natural lan-\nguage processing (NLP) that has been a topic\nof research. While most studies focus on high-\nresource languages with an extensive amount\nof available data, the study on low-resource lan-\nguages with insufficient data needs attention.\nTo address this issue, we propose a transformer-\nbased method for sentiment analysis for low-\nresources African languages, Nigerian Pidgin\nand Yoruba. To evaluate the effectiveness of\nour multilingual language models for monolin-\ngual sentiment classification, we participated in\nthe AfriSenti SemEval shared task 2023 com-\npetition. On the official evaluation set, our\ngroup (named as Bhattacharya_Lab) ranked\n1 out of 33 participating groups in the Mono-\nlingual Sentiment Classification task (i.e., Task\nA) for Nigerian Pidgin (i.e., Track 4), and in\nthe Top 5 among 33 participating groups in\nthe Monolingual Sentiment Classification task\nfor Yoruba (i.e., Track 2) respectively, demon-\nstrating the potential for our transformer-based\nlanguage models to improve sentiment anal-\nysis in low-resource languages. Overall, our\nstudy highlights the importance of exploring\nthe potential of NLP in low-resource languages\nand the impact of transformer-based multilin-\ngual language models in sentiment analysis for\nthe low-resource African languages, Nigerian\nPidgin and Yoruba.\n1 Introduction\nDetecting sentiments or emotions from language is\na prominent area of research in natural language\nprocessing (NLP) (Medhat et al., 2014; Wankhade\net al., 2022). Sentiment analysis (SA) deals with\n+These authors contributed equally to this work \n*corresponding author\nidentifying and classifying emotions from\ntextual data (Pang et al., 2002).\nHowever, most of the studies focus on high\nresource languages like English (Ruder, 2020).\nAs such, low resource languages such as African\nlanguages with a limited amount of data remain\nhighly underrepresented, making the sentiment\nclassification task for African languages is\nchallenging (Mabokela and Schlippe, 2022). In\nrecent years, there has been increasing interest\nin developing sentiment classification models\nfor African languages (Muhammad et al., 2022;\nOgueji et al., 2021).\nHowever, the lack of high-quality labeled datasets,\nas well as the limited research in this area, limit the\nprogress in this field (Muhammad et al.,2023a). To\naddress this challenge, several research initiatives,\nsuch as the SemEval Shared Task (Muhammad\net al., 2023b), have been launched to encourage\nthe development of effective sentiment analysis\nmodels for African languages. We participated in\nthe SemEval Shared Task 2023, specifically the\nMonolingual Sentiment Classification task (i.e.,\nTask A) for two low-resource African languages:\nYoruba and Nigerian Pidgin. The aim of this task\nis to develop NLP models capable of accurately\nclassifying the sentiment of text in these two\nAfrican languages (Track 2 and Track 4).\nIn this paper, we present a transformer-based\nlanguage model trained on two low-resource\nAfrican languages, Yoruba and Nigerian Pidgin,\nto correctly classify sentiments. Our model is\nofficially ranked 1 in the monolingual sentiment\nclassification task (Task A) for Nigerian Pidgin,\nand ranked 5 in the monolingual sentiment\nclassification task (Task A) for Yoruba in the\nAfriSenti SemEval shared task 2023 competition,\ndemonstrating the effectiveness of our approach\n1502\nLanguage Tweet English translation Sentiment\nYoruba E ¸ kú o ¸dún titun oooo!! Happy new year!! Positive\nArugbo koni daa o. Old age is not good. Negative\nNigerian\nPidgin\nYou gat a special place. You have a special place. Positive\nYou still fit open mouth talk na wa oh You’re still talking it’s a wonder Negative\nTable 1: Example of Tweets in Yoruba and Nigerian pidgin languages with English translation and sentiment.\nin correctly classifying the sentiment of the text\nin Yoruba and Nigerian Pidgin by attaining the\nstate-of-the-art performance.\n2 Related Work\nRecent advances in unsupervised learning of text\nrepresentations lead to advancements in natural lan-\nguage processing problems. Pretrained word em-\nbeddings is improved by learning contextualized\nrepresentations (Mikolov et al., 2013; Peters et al.,\n1802), and this is further improved by pretraining\nlanguage models (Radford et al.,2018; Devlin et al.,\n2018) based on transformers (Vaswani et al.,2017).\nMultilingual language models, in which a single\nlanguage model is pretrained on several languages\nwithout any explicit cross-lingual supervision (Con-\nneau et al., 2019).\nHowever, most studies focus on high-resource\nlanguages with relatively large amounts of data. As\nsuch, low resource languages such as African lan-\nguages with a limited amount of data remain highly\nunderrepresented (Muhammad et al., 2022; Ogueji\net al., 2021). (Ezeani et al., 2018; Ogueji and Ahia,\n2019; Alabi et al., 2019; Dossou and Sabry, 2021)\nshow promising results in monolingual cases using\npretrained embeddings for African languages, how-\never, the models are static and trained on a specific\nlanguage. Recent studies on multilingual language\nmodel (Devlin et al., 2018; Liu et al., 2019; Con-\nneau et al., 2019) show superior performance, par-\nticularly for high-resource languages. While such\nmodels use a large training data, there is a need for\nincreased representation of low-resource African\nlanguages in modern multilingual language model\n(Muhammad et al., 2022).\nInspired by a recent study (Ogueji et al., 2021)\nusing a relatively small amount of data, this\nwork focuses on pretraining a multilingual lan-\nguage model solely on low-resource languages\nwithout any transfer from higher-resource lan-\nguages. We also take leverage of using a recently\nreleased dataset, mostly manually annotated, of\nlow resource languages originating from Nigeria\n(Muhammad et al., 2022).\n3 Methodology\n3.1 Languages - Nigerian Pidgin and Yoruba\nWe focus on two African languages, Nigerian Pid-\ngin and Yoruba. Tables 1 and 2 provide the de-\ntails about both languages. In particular, Table 1\npresents sample tweets in both languages along\nwith their English translation. Three classes of\nsentiments are considered: (a) positive sentiment,\ni.e., a tweet characterized by emotions such as hap-\npiness, satisfaction, contentment, joy, excitement,\nand optimism; (b) negative sentiment, i.e., a tweet\ncharacterized by emotions such as anger, frustra-\ntion, disappointment, sadness, and pessimism; and\n(c) neutral, i.e., a tweet characterized by a sense of\nindifference or neutrality. Examples of positive and\nnegative sentiments in both languages are included\nin Table 1. Moreover, Table 2 provides additional\ninformation about both languages. While Nige-\nrian Pidgin belongs to the English Creole family\nof language with around 75M speakers, Yoruba be-\nlongs to the Niger-Congo family with around 42M\nspeakers.\nLanguage Family Speakers\nNigerian Pidgin English Creole 75M\nYoruba Niger-Congo 42M\nTable 2: Information of representative languages\n3.2 Experimental Setup\n3.2.1 Sentiment Classification Model\nWe train a transformer (Vaswani et al.,2017) using\nthe standard masked language modeling objective\n(Devlin et al., 2018) without next-sentence predic-\ntion, which is the same approach used in XLM-\nR (Conneau et al., 2020). We train on text data\nthat includes both languages and sample batches\nfrom both languages, ensuring that the model does\nnot encounter the same language in consecutive\n1503\nbatches. To tokenize the raw text data, we use sub-\nword tokenization with SentencePiece (Kudo and\nRichardson, 2018), which is trained with a unigram\nlanguage model (Kudo, 2018). We follow the sam-\npling method described by (Ogueji et al., 2021)\nwith a parameter alpha of 0.3 to sample training\nsentences. The Huggingface Transformers library\n(Wolf et al., 2020) is used to train the model. We\nuse a maximum sequence length of 256 and train\nthe model for 25 epochs with a batch size of 32,\nwarm-up steps of 100, and a default learning rate\nof 5e-5. Moreover, we optimize the models using\nAdamW (Loshchilov and Hutter, 2017). After fine-\ntuning, our model has 10 layers, 6 attention heads,\n768 hidden units, and 3072 feed-forward size.\n3.2.2 Data sets\nTraining data sets: To train our model, we use the\ntraining set used by (Ogueji et al., 2021) along\nwith AfriSenti training data (Muhammad et al.,\n2023a). Tables 3 and 4 provide details of both\ndata sets. Development data set: As shown in Ta-\nble 3, we use AfriSenti development data. Specif-\nically, it contains 1282 tweets in Nigerian pidgin\nand 2091 tweets in Yoruba, respectively. According\nto AfriSenti, the proportion of tweets in each label\n(positive, negative, and neutral) varies significantly\nfor Nigerian Pidgin than that of Yoruba. Test data\nset: We evaluate the performance of our model on\nthe SemEval official evaluation set. It is worth men-\ntioning that we participated in the SemEval Shared\nTask 2023, specifically the Monolingual Sentiment\nClassification task (i.e., Task A) for Yoruba (i.e.,\nTrack 2) and Nigerian Pidgin (i.e., Track 4).\nDatasets Nigerian Pidgin Yoruba\nTrain 5122 4155\nDev 1282 2091\nTest 4155 4516\nTable 3: AfriSenti data sets split information of rep-\nresentative languages (Muhammad et al., 2023a). The\nnumber of Tweets in each category is reported.\nLanguage # sentences Size (GB)\nNigerian Pidgin 161,842 0.05\nYoruba 149,147 0.03\nTable 4: Training data sets information of representative\nlanguages (Ogueji et al., 2021).\n3.2.3 Performance Evaluation\nWe use precision, recall, and weighted F1 score to\nevaluate the performance of our model. A high pre-\ncision score indicates that the model is making very\nfew false positive predictions, while a high recall\nscore indicates that the model is correctly identify-\ning a high proportion of actual positive instances.\nThe F1 score is a harmonic mean of precision and\nrecall, and the weighted F1 score is a variation\nof F1 score that accounts for class imbalance in\nthe dataset. A high F1 score indicates the better\nperformance.\n4 Results\n4.1 Performance on Development Set\nMethod Nigerian Pidgin Yoruba\nPrecision 0.762 0.798\nRecall 0.760 0.799\nF1 score 0.760 0.799\nTable 5: Performance of our model on the development\nset, containing 1282 Nigerian Pidgin and 2091 Yoruba\ntweets. Here, weighted F1 score is reported.\nTable 5 represents the performance of our model\non the AfriSenti development data set, contain-\ning 1282 and 2091 tweets for Nigerian Pidgin\nand Yoruba, respectively. Our model consis-\ntently achieves high performance, with weighted F1\nscores of 76% and 79.9% for Nigerian Pidgin and\nYoruba, respectively. We observe similar trends\nby using Precision and Recall evaluation metrics.\nIt is worth noting that the proportion of tweets in\neach label (positive, negative, and neutral) varies\nsignificantly for Nigerian Pidgin, whereas this is\nnot as significant for Yoruba according to AfriSenti,\nillustrating that our model works well in both cases.\n4.2 Performance on Nigerian Pidgin Test Set\nFigure 1 shows the head-to-head performance com-\nparison of our method (named Bhattacharya_lab)\nagainst the competitive participating methods\nwhere the ranking was officially released by the\nAfriSenti SemEval organizer. The test set contains\n4155 tweets. We ranked 1 out of 33 participat-\ning groups by attaining a weighted F1 score of\n75.96%, illustrating the superior performance of\nour transformer-based model in sentiment analy-\nsis for the low-resource African language Nigerian\nPidgin. In particular, the performance gap between\n1504\nFigure 2: A head-to-head performance comparison of our method (named Bhattacharya_lab) in Black against the \ncompetitive participating methods in Grey where the ranking was officially released by the AfriSenti SemEval \norganizer (Task A, Track 2) over the Yoruba dataset of 4516 tweets. Our method officially ranked 5  out of 33 \nparticipating groups.\nFigure 1: A head-to-head performance comparison of our method (named Bhattacharya_lab) in Black against the \ncompetitive participating methods in Grey where the ranking was officially released by the AfriSenti SemEval \norganizer (Task A, Track 4) over the Nigerian Pidgin dataset of 4155 tweets. Our method officially ranked 1 out of \n33 participating groups. The inset figure illustrates the performance of top five groups.\n1505\nthe top-ranked method (this work) and the top-5\nranked method (named Pali) is 0.8% F1 score. It is\nworth mentioning the official method description of\nother participating methods are yet not published,\ntherefore, we cannot compare top-ranked methods\nbased on their methodologies.\n4.3 Performance on Yoruba Test Set\nFigure 2 shows the head-to-head performance com-\nparison of our method (named Bhattacharya_lab)\nagainst the competitive participating groups where\nthe ranking was officially released by the AfriSenti\nSemEval organizer. On this dataset of 4516 tweets,\nwe ranked in the Top 5 out of 33 groups with a\nweighted F1 score of 79.86%. In particular, the\nperformance of our work is significantly superior\nto the majority of the participating groups, we per-\nform comparably to the top1-ranked group (79.86%\nvs 80.16%). That is our performance in the Yoruba\nlanguage is consistent with our performance in the\nNigerian Pidgin language, illustrating the impact\nof our transformer-based multilingual language\nmodels in sentiment analysis for the low-resource\nAfrican languages, Nigerian Pidgin and Yoruba.\n5 Conclusion\nThe study of low-resource languages with insuf-\nficient data requires attention since most of the\nresearch concentrates on high-resource languages\nwith a large quantity of available data. We train\na transformer-based model for sentiment analysis\nfor two low-resource African languages namely\nYoruba and Nigerian Pidgin. On the official evalua-\ntion set, our model consistently ranked in the Top 5\nby outperforming most of the participating groups\nin the AfriSenti SemEval shared task 2023 compe-\ntition, illustrating the superior performance of our\ntransformer-based model over existing works.\nOur contribution to this task is significant, as it\ndemonstrates the potential for NLP techniques to\nbe applied to low-resource languages and improve\nsentiment analysis in these languages. In future\nwork, we plan to investigate the effectiveness of\nother pre-trained models, as well as explore the use\nof more advanced techniques such as multi-task\nlearning and transfer learning. Overall, we hope\nthat our work will encourage further research in the\nfield of NLP for low-resource languages and con-\ntribute to the development of language technologies\nthat can benefit underrepresented communities.\nReferences\nJesujoba O Alabi, Kwabena Amponsah-Kaakyire,\nDavid I Adelani, and Cristina Espana-Bonet. 2019.\nMassive vs. curated word embeddings for low-\nresourced languages. the case of yor\\ub\\’a and twi.\narXiv preprint arXiv:1912.02481.\nAlexis Conneau, Alexei Baevski, Ronan Collobert,\nAbdelrahman Mohamed, and Michael Auli. 2020.\nUnsupervised cross-lingual representation learn-\ning for speech recognition. arXiv preprint\narXiv:2006.13979.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBonaventure FP Dossou and Mohammed Sabry. 2021.\nAfrivec: Word embedding models for african lan-\nguages. case study of fon and nobiin. arXiv preprint\narXiv:2103.05132.\nIgnatius Ezeani, Ikechukwu Onyenwe, and Mark Hep-\nple. 2018. Transferred embeddings for igbo similar-\nity, analogy, and diacritic restoration tasks. In Pro-\nceedings of the Third Workshop on Semantic Deep\nLearning, pages 30–38.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. arXiv preprint arXiv:1804.10959.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Fixing weight\ndecay regularization in adam.\nRonny Mabokela and Tim Schlippe. 2022. A sentiment\ncorpus for south african under-resourced languages\nin a multilingual context. In Proceedings of the 1st\nAnnual Meeting of the ELRA/ISCA Special Interest\nGroup on Under-Resourced Languages, pages 70–77.\nWalaa Medhat, Ahmed Hassan, and Hoda Korashy.\n2014. Sentiment analysis algorithms and applica-\ntions: A survey. Ain Shams engineering journal ,\n5(4):1093–1113.\n1506\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nAdvances in neural information processing systems,\n26.\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa’id\nAhmad, Meriem Beloucif, Saif M. Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino Dário Mário António Ali, Davis David,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse Be-\nlay, Wendimu Baye Messelle, Hailu Beshada Balcha,\nSisay Adugna Chala, Hagos Tesfahun Gebremichael,\nBernard Opoku, and Steven Arthur. 2023a. AfriSenti:\nA Twitter Sentiment Analysis Benchmark for African\nLanguages.\nShamsuddeen Hassan Muhammad, Idris Abdulmu-\nmin, Seid Muhie Yimam, David Ifeoluwa Ade-\nlani, Ibrahim Sa’id Ahmad, Nedjma Ousidhoum,\nAbinew Ali Ayele, Saif M. Mohammad, Meriem\nBeloucif, and Sebastian Ruder. 2023b. SemEval-\n2023 Task 12: Sentiment Analysis for African Lan-\nguages (AfriSenti-SemEval). In Proceedings of the\n17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nShamsuddeen Hassan Muhammad, David Ifeoluwa Ade-\nlani, Sebastian Ruder, Ibrahim Sa’id Ahmad, Idris\nAbdulmumin, Bello Shehu Bello, Monojit Choud-\nhury, Chris Chinenye Emezue, Saheed Salahudeen\nAbdullahi, Anuoluwapo Aremu, Alípio Jorge, and\nPavel Brazdil. 2022. NaijaSenti: A nigerian Twitter\nsentiment corpus for multilingual sentiment analy-\nsis. In Proceedings of the Thirteenth Language Re-\nsources and Evaluation Conference, pages 590–602,\nMarseille, France. European Language Resources\nAssociation.\nKelechi Ogueji and Orevaoghene Ahia. 2019. Pidg-\ninunmt: Unsupervised neural machine translation\nfrom west african pidgin to english. arXiv preprint\narXiv:1912.03444.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. In Proceedings of the 1st Work-\nshop on Multilingual Representation Learning, pages\n116–126.\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up? sentiment classification us-\ning machine learning techniques. arXiv preprint\ncs/0205070.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 1802. Deep contextualized word rep-\nresentations. corr abs/1802.05365 (2018). arXiv\npreprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nSebastian Ruder. 2020. Why you should do nlp be-\nyond english. Sebastian Ruder https://ruder. io/nlp-\nbeyond-english.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nMayur Wankhade, Annavarapu Chandra Sekhara Rao,\nand Chaitanya Kulkarni. 2022. A survey on senti-\nment analysis methods, applications, and challenges.\nArtificial Intelligence Review, 55(7):5731–5780.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nA Appendix: Precision, Recall, and\nweighted F1 score\nPrecision measures the proportion of true positive\npredictions out of all predicted positive instances.\nPrecision = True Positives / (True Positives +\nFalse Positives)\nRecall measures the proportion of true positive\npredictions out of all actual positive instances.\nRecall = True Positives / (True Positives + False\nNegatives)\nThe F1 score is a harmonic mean of precision\nand recall, and the weighted F1 score is a variation\nof F1 score that accounts for class imbalance in the\ndataset.\nweighted F1 score = (sum of F1 scores for each\nclass * number of instances in each class) / total\nnumber of instances\n1507",
  "topic": "Yoruba",
  "concepts": [
    {
      "name": "Yoruba",
      "score": 0.8535244464874268
    },
    {
      "name": "Computer science",
      "score": 0.7236232757568359
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5798957943916321
    },
    {
      "name": "Pidgin",
      "score": 0.5639828443527222
    },
    {
      "name": "Natural language processing",
      "score": 0.5579993724822998
    },
    {
      "name": "Dialectology",
      "score": 0.538212239742279
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5192849040031433
    },
    {
      "name": "SemEval",
      "score": 0.501305341720581
    },
    {
      "name": "Transformer",
      "score": 0.4796203076839447
    },
    {
      "name": "Task (project management)",
      "score": 0.4586096405982971
    },
    {
      "name": "Valuation (finance)",
      "score": 0.45429790019989014
    },
    {
      "name": "Linguistics",
      "score": 0.280137836933136
    },
    {
      "name": "Accounting",
      "score": 0.0849771797657013
    },
    {
      "name": "Creole language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I902671590",
      "name": "Auburn University at Montgomery",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32480017",
      "name": "Florida Polytechnic University",
      "country": "US"
    }
  ],
  "cited_by": 3
}