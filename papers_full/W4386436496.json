{
    "title": "Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair",
    "url": "https://openalex.org/W4386436496",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2098166944",
            "name": "Wei, Yuxiang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A4286260787",
            "name": "Xia, Chunqiu Steven",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2367781822",
            "name": "Zhang, Lingming",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3170092793",
        "https://openalex.org/W4362659486",
        "https://openalex.org/W2025791343",
        "https://openalex.org/W4366327299",
        "https://openalex.org/W2767431443",
        "https://openalex.org/W2907705732",
        "https://openalex.org/W2144575244",
        "https://openalex.org/W4378591002",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W6912728327",
        "https://openalex.org/W3208407575",
        "https://openalex.org/W2851896161",
        "https://openalex.org/W2898644864",
        "https://openalex.org/W2867448323",
        "https://openalex.org/W3134686229",
        "https://openalex.org/W3161474773",
        "https://openalex.org/W4382239980",
        "https://openalex.org/W2156723666",
        "https://openalex.org/W2895570420",
        "https://openalex.org/W2740646481",
        "https://openalex.org/W2400994325",
        "https://openalex.org/W2145373440",
        "https://openalex.org/W4311887664",
        "https://openalex.org/W3089621332",
        "https://openalex.org/W2924629359",
        "https://openalex.org/W4317536042",
        "https://openalex.org/W2060333670",
        "https://openalex.org/W3043761819",
        "https://openalex.org/W2537787699",
        "https://openalex.org/W2465133314",
        "https://openalex.org/W2373227884",
        "https://openalex.org/W4312970618",
        "https://openalex.org/W4385302156",
        "https://openalex.org/W2972082064",
        "https://openalex.org/W6931800888",
        "https://openalex.org/W2795030435",
        "https://openalex.org/W4384345708",
        "https://openalex.org/W4308643319",
        "https://openalex.org/W4318902699",
        "https://openalex.org/W4281763794",
        "https://openalex.org/W3160155705",
        "https://openalex.org/W4389519225",
        "https://openalex.org/W3193682477",
        "https://openalex.org/W3100698844",
        "https://openalex.org/W2559655401",
        "https://openalex.org/W3161027892"
    ],
    "abstract": "During Automated Program Repair (APR), it can be challenging to synthesize\\ncorrect patches for real-world systems in general-purpose programming\\nlanguages. Recent Large Language Models (LLMs) have been shown to be helpful\\n\"copilots\" in assisting developers with various coding tasks, and have also\\nbeen directly applied for patch synthesis. However, most LLMs treat programs as\\nsequences of tokens, meaning that they are ignorant of the underlying semantics\\nconstraints of the target programming language. This results in plenty of\\nstatically invalid generated patches, impeding the practicality of the\\ntechnique. Therefore, we propose Repilot, a general code generation framework\\nto further copilot the AI \"copilots\" (i.e., LLMs) by synthesizing more valid\\npatches during the repair process. Our key insight is that many LLMs produce\\noutputs autoregressively (i.e., token by token), resembling human writing\\nprograms, which can be significantly boosted and guided through a Completion\\nEngine. Repilot synergistically synthesizes a candidate patch through the\\ninteraction between an LLM and a Completion Engine, which 1) prunes away\\ninfeasible tokens suggested by the LLM and 2) proactively completes the token\\nbased on the suggestions provided by the Completion Engine. Our evaluation on a\\nsubset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot\\noutperforms state-of-the-art techniques by fixing 27% and 47% more bugs,\\nrespectively. Moreover, Repilot produces more valid and correct patches than\\nthe base LLM with the same budget. While we focus on leveraging Repilot for APR\\nin this work, the overall approach is also generalizable to other code\\ngeneration tasks.\\n",
    "full_text": "Copiloting the Copilots: Fusing Large Language Models with\nCompletion Engines for Automated Program Repair\nYuxiang Wei\nUniversity of Illinois\nUrbana-Champaign, USA\nywei40@illinois.edu\nChunqiu Steven Xia\nUniversity of Illinois\nUrbana-Champaign, USA\nchunqiu2@illinois.edu\nLingming Zhang\nUniversity of Illinois\nUrbana-Champaign, USA\nlingming@illinois.edu\nABSTRACT\nDuring Automated Program Repair (APR), it can be challenging\nto synthesize correct patches for real-world systems in general-\npurpose programming languages. Recent Large Language Models\n(LLMs) have been shown to be helpful ‚Äúcopilots‚Äù in assisting de-\nvelopers with various coding tasks, and have also been directly\napplied for patch synthesis. However, most LLMs treat programs as\nsequences of tokens, meaning that they are ignorant of the underly-\ning semantics constraints of the target programming language. This\nresults in plenty of statically invalid generated patches, impeding\nthe practicality of the technique. Therefore, we propose Repilot, a\ngeneral code generation framework to further copilot the AI ‚Äúcopi-\nlots‚Äù (i.e., LLMs) by synthesizing more valid patches during the\nrepair process. Our key insight is that many LLMs produce outputs\nautoregressively (i.e., token by token), resembling human writing\nprograms, which can be significantly boosted and guided through a\nCompletion Engine. Repilot synergistically synthesizes a candidate\npatch through the interaction between an LLM and a Completion\nEngine, which 1) prunes away infeasible tokens suggested by the\nLLM and 2) proactively completes the token based on the sugges-\ntions provided by the Completion Engine. Our evaluation on a\nsubset of the widely-used Defects4j 1.2 and 2.0 datasets shows that\nRepilot outperforms state-of-the-art techniques by fixing 27% and\n47% more bugs, respectively. Moreover, Repilot produces more valid\nand correct patches than the base LLM with the same budget. While\nwe focus on leveraging Repilot for APR in this work, the overall\napproach is also generalizable to other code generation tasks.\nCCS CONCEPTS\n‚Ä¢ Software and its engineering ‚ÜíSoftware testing and debug-\nging; Automatic programming.\nKEYWORDS\nProgram Repair, Large Language Model, Completion Engine\nACM Reference Format:\nYuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. 2023. Copiloting\nthe Copilots: Fusing Large Language Models with Completion Engines for\nAutomated Program Repair. In Proceedings of the 31st ACM Joint European\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0327-0/23/12. . . $15.00\nhttps://doi.org/10.1145/3611643.3616271\nSoftware Engineering Conference and Symposium on the Foundations of Soft-\nware Engineering (ESEC/FSE ‚Äô23), December 3‚Äì9, 2023, San Francisco, CA, USA.\nACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3611643.3616271\n1 INTRODUCTION\nAutomated Program Repair (APR) seeks to reduce the manual bug-\nfixing effort of developers by automatically synthesizing patches\ngiven the original buggy code [ 20]. State-of-the-art traditional\nAPR tools are mainly based on handcrafted repair templates to\nmatch the buggy code patterns and apply the corresponding code\nchanges [21, 41]. Although outperforming other traditional tech-\nniques [37, 43, 47], such tools can only fix the bug types within the\npreset templates and cannot generalize to new bug types. With the\ndevelopment of Deep Learning (DL) techniques, researchers build\nlearning-based APR [ 29, 72, 74] tools based on Neural Machine\nTranslation (NMT) [57] architecture. They train NMT models to\ntranslate buggy code into correct code by learning from pairs of\nbuggy and fixed code scraped from open-source commits. However,\nas discussed in prior work [67], the training sets of these tools can\nbe limited in size and also contain irrelevant or noisy commits.\nMore recently, researchers have leveraged the growth in the field\nof NLP to directly use Large Language Models (LLMs) [10, 17] for\nAPR [31, 66, 67]. LLMs not only achieve impressive performance on\nmany NLP tasks [7], but are also shown to be reliable ‚Äúcopilots‚Äù1 in\nassisting developers with various coding tasks [4, 40]. The reason\nis that modern LLMs often include large amounts of available open-\nsource code repositories as part of their training dataset. Recogniz-\ning the power of LLMs, researchers have recently applied LLMs for\nAPR: instead of translating buggy code into correct code, LLMs are\ndirectly used to synthesize the correct patch from the surrounding\ncontext. AlphaRepair [67] reformulates the APR problem as a cloze\n(or infilling) task [2, 19]: it first replaces the buggy code snippets\nwith masked tokens and then uses CodeBERT [ 17] to fill correct\ncode in given the surrounding context. Other studies on LLMs for\nAPR have applied even larger LLMs with different repair settings\n(including generating complete patch functions) [33, 53, 66].\nWhile prior LLM for APR techniques achieve state-of-the-art\nbug-fixing performance, they use LLMs in a black-box manner,\nwhere the underlying LLM generate programs according to the to-\nken distribution without any structural or semantic understanding\nof the code. To highlight the limitations with current LLMs for APR\ntools, In Figure 1 we show 3 scenarios where LLM can generate\nincorrect patches. 1 Generating infeasible tokens . In Figure 1.1,\nthe LLM has a high probability (>90%) of generating String to com-\nplete the asString method. However asString is not a valid field\naccess for the objectt and is also not part of the scope of the current\n1One popular AI pair programmer tool (based on Codex [10]) is named Copilot [22].\narXiv:2309.00608v3  [cs.SE]  8 Nov 2023\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\n‚Ä¶ \nabstract class Token { \n   final EndTag asEndTag() { \n   final StartTag asStartTag() {‚Ä¶} \n   final Comment asComment() {‚Ä¶} \n   final Doctype asDoctype() {‚Ä¶} \n   final Character asCharacter() {‚Ä¶} \n‚Ä¶ \n}\nString String name = String name = t.\nasEndTag \nasStartTag \nasComment\nString name = t.asString\n String name = t.asEndTag\nasEndTag\nString name = t.asEndTag\nString name = t.asEndTag().name\nname\nString name = t.asEndTag().name();\nString name String name = t\nLanguage \nModel\nboolean anyOtherEndTag(Token t,  ‚Ä¶ tb) { \n   String name = t.as \n   ‚Ä¶\nCompletion \nEngine\n!\n!\n\"\nString \nName \nEnd \n‚Ä¶\n91%\n0.2%\n3%\n!\n!\n\"\n‚Ä¶\n\"\n!\n !\n91% 0.2%3%\nString\n‚Ä¶\nName End\nString name = t.asEndTag\n‚Ä¶\nString name = t.asEndTag\nName\nTag\n16%\n7%\n‚Ä¶\nasEndTag\n !\n Hard to generate rare tokens2\nfinal EndTag asEndTag() {‚Ä¶}\nString name = t.asEndTag().\nname \nnormalName \ntoString\nname\n !50%\n‚Ä¶toString\n 6%\n No explicit consideration of types3\nabstract class Token { \n   final EndTag asEndTag() {‚Ä¶} \n} \nfinal static class EndTag extends Tag {‚Ä¶} \nstatic abstract class Tag extends Token { \n   final String name() {‚Ä¶} \n}\nString name = t.as\nasEndTag \nasStartTag \nasComment \nasDoctype \nasCharacter\n!String\nName\n !\nEnd\n !\n91%\n0.2%\n3%\n‚Ä¶\nLanguage model \nPredictions Completions\n Generating infeasible tokens1\nFigure 1: Limitations of existing LLM-based APR approaches.\nbuggy method. In this case, the patchs generated using asString\nwill never be correct as it cannot compile. By directly using the\nmodel probabilities, LLMs are likely to generate many patches us-\ning invalid tokens and decrease the likelihood of generating the\ncorrect patch with End (0.2%). 2 Hard to generate rare tokens .\nLLMs usually cannot generate a complete identifier name in one\nstep since it uses subword tokenization [55] to break uncommon\nwords into smaller subwords. These uncommon words manifest\nas rare identifiers in code, where identifier names are CamelCase\nor underscore combinations of multiple words (e.g., asEndTag in\nFigure 1.2). As such, LLMs need to generate these identifiers step\nby step, needing not only multiple iterations but also accurate out-\nput in each step. Since prior approaches [33, 66] sample based on\nprobability, the likelihood of completing a rare token to fix a bug\ncan be extremely low. 3 No explicit consideration of types . In\naddition to potentially generating out-of-scope identifiers, LLMs\ndo not have access to various type information that gives hints to\nthe valid identifiers. In Figure 1.3, the return type of asEndTag() is\nEndTag, whose definition is not explicitly given to the LLM in its\nimmediate context. As such, LLMs do not know the correct mem-\nber fields of EndTag and may generate invalid patches containing\nidentifiers that do not fit the required type. On the contrary, a Com-\npletion Engine has full access to the project and can easily figure\nout the return type of asEndTag() through static analysis on the\nabstract syntax tree of the program. By treating code as a sequence\nof textual tokens, the important type information is not encoded.\nTo address the aforementioned limitations, we propose Repilot,\na framework to further copilot the AI ‚Äúcopilots‚Äù (i.e., LLMs) via\nfusing LLMs with Completion Engines to synthesize more valid\npatches. Completion Engines [48] can parse incomplete programs\nand reason about the semantics in an error-tolerant manner. Our\nkey insight is to liken LLM autoregressive token generation as a hu-\nman developer code writing, where the Completion Engine can provide\nreal-time updates to check if the human/LLMs written partial code is\nvalid. Repilot first uses the LLM to provide the probabilities of gen-\nerating the next token in the patch and then queries the Completion\nEngine to modify the probability list by dynamically zeroing the\nprobabilities of invalid tokens. We can then sample from the new\nprobability list to select the next token. Furthermore, recognizing\nthe ability for Completion Engines to suggest completions, we use\nthis feature whenever there is only one possible identifier suffix\nto complete the context. This not only allows Repilot to generate\npatches with valid rare and long identifiers but also reduces the\nwork of LLMs needed to iteratively generate long identifier names.\nFor example, Repilot directly prunes the String and Name tokens\nin Figure 1.1 as they are infeasible according to the Completion\nEngine, but still accepts the correct End token. In Figure 1.2, the\nCompletion Engine recognizes that asEndTag is the only valid con-\ntinuation to the prefixasEnd, so Repilot directly completes this token\nwithout querying the LLM. To combat the time cost of Completion\nEngine, we implement several optimization techniques to minimize\nthe overhead. Note that the recentSynchromesh work [52] also em-\nploys a Completion Engine for reliable code generation with LLMs.\nHowever, it relies on expert-designed constraints and only targets\ndomain-specific languages (e.g., SQL). Repilot directly works for\ngeneral-purpose programming languages while introducing mini-\nmal overhead and can proactively complete the current generation\nusing the Completion Engine without querying the LLM.\nTo demonstrate the generalizability of Repilot, we instantiate\nRepilot with two LLMs having distinct architectures and sizes:\nCodeT5-large [61], an encoder-decoder LLM with 770 million pa-\nrameters, and InCoder-6.7B [19], a decoder only LLM with 6.7\nbillion parameters, both capable of code infilling from prefix and\nsuffix context. We further implement a Java Completion Engine for\nRepilot based on the Eclipse JDT Language Server [1, 18] since it\nprovides various semantics-based analyses through a consistent\nLanguage Server Protocol [48]. We evaluate Repilot on a subset of\nthe widely studied Defects4J 1.2 and 2.0 datasets [32] and demon-\nstrate state-of-the-art results in both the number of correct fixes\nand compilation rate ‚Äî the percentage of the generated patches\nthat can be successfully compiled. Furthermore, while we evaluated\nRepilot for APR in this work, we believe the overall framework can\nbe easily applied to other code generation tasks, including code\ncompletion [16, 73], program synthesis [40, 52], and test genera-\ntion [14, 65]. In summary, we make the following contributions:\n‚Ä¢Direction. We open a new direction for fusing LLMs with Com-\npletion Engines for more powerful APR and beyond. Compared\nto prior techniques which either perform post-processing to fix\ninvalid generations or use simple static methods to approximate\nthese valid tokens, our approach leverages a powerful Completion\nEngine to directly provide accurate feedback on partial programs\nto avoid invalid token generations.\n‚Ä¢Technique. We implement Repilot, an LLM for APR approach\ninstantiated with the CodeT5 and InCoder models to perform\ncloze-style repair combined with our modified Eclipse JDT Lan-\nguage Server [1, 18] as the Completion Engine. In Repilot, we\nuse the Completion Engine to systematically prune invalid to-\nkens generated by LLMs and to directly complete code given the\ncurrent prefix. Furthermore, we implement optimizations to sig-\nnificantly reduce the overhead of Repilot. We have open-sourced\nour tool at: https://github.com/ise-uiuc/Repilot.\n‚Ä¢Study. We compare Repilot against state-of-the-art APR tools on\nDefects4J 1.2 and 2.0. Repilot is able to achieve new state-of-the-\nart results of 66 Defects4J 1.2 single-hunk bugs and 50 Defects4J\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\n2.0 single-line bugs fixed respectively with 30 more combined\nfixes across both datasets compared to the previous best baseline.\nOur further evaluation shows that Repilot consistently improves\nthe validity and correctness of the generated patches with a\nlimited overhead (7% for CodeT5 and negligible for InCoder).\n2 BACKGROUND AND RELATED WORK\n2.1 Large Language Models for Code\nRecent advances in Natural Language Processing (NLP) have em-\npowered the idea of using Large Language Models (LLMs) that are\npre-trained on enormous corpora of natural language and code for\nvarious code-related tasks [4, 5, 10, 38, 70]. LLMs are based on the\ntransformer architecture [59] that can be categorized intoencoder-\nonly, decoder-only and encoder-decoder. Encoder-only models\nuse only the encoder component by training using Masked Lan-\nguage Modeling (MLM) [15] objective where a small percentage\n(e.g., 15%) of the tokens are masked on. The goal of MLM is to re-\ncover these masked tokens given the surrounding context. Encoder-\nonly models such as CodeBERT [17] and GraphCodeBERT [23] are\ndesigned to provide a representation of the input code to be used for\ndownstream tasks such as code classification [71]. Decoder-only\nmodels, on the other hand, aim to autoregressively generate to-\nkens based on all previously generated tokens. CodeGEN [50, 51],\nCodex [10] and PolyCoder [70] are examples of decoder-only LLMs\nwhere they can be used for code autocompletion tasks. Different\nfrom encoder- and decoder-only LLMs, encoder-decoder models\n(e.g., CodeT5 [60, 61] and PLBART [3]) combine both encoder and\ndecoder together and jointly train both components together. A\ncommonly used pre-training objective for encoder-decoder models\nis Masked Span Prediction (MSP) where random spans (multiple\nconsecutive tokens) are replaced with single masked tokens and the\nmodels learn to fill in the masked span with the correct sequence of\ntokens. Furthermore, decoder-only models like InCoder [19] can\nalso do infilling through the causal language modeling [2] objective.\nInstead of using the decoder to predict the next token in the origi-\nnal training data, similar to MSP, InCoder also replaces random\nspans with masked span tokens. During training, InCoder learns\nto autoregressively recover the original spans. With this training\nstrategy, InCoder can perform infilling with bidirectional context\nsimilar to encoder-decoder models, enabling cloze-style repair.\n2.2 Code Completion\nCode completion is one of the most frequently used features in\nIntegrated Development Environments (IDEs). It substantially al-\nleviates the complexity of software development by interactively\nsuggesting program constructs after the user‚Äôs caret position while\nprogrammers are typing, including identifier names and library\nAPIs. Code completion is now an indispensable infrastructure of\nthe most widely-used programming languages and can be easily\nintegrated into most modern text editors thanks to the presence\nof the Language Server Protocol [48], which standardizes the com-\nmunication between tools and language services. Traditionally, a\nsemantics-based Completion Engine is implemented on top of a se-\nries of complex incremental syntactic and semantic analyses of\nthe target programming language, since it needs to understand\npartially written programs and provide real-time feedback. The\nCompletion Engine has full access to a project repository and its de-\npendencies and can produce suggestions according to its semantic\nunderstanding. Recent advances in LLMs demonstrate the capa-\nbility of generating long and complicated completions. However,\nthey may produce unreasonable programs due to the limitation in\nthe code context size and the loss of program analysis by simply\ntreating programs as token sequences. In this paper, we use the\nterm Completion Engine to refer to the semantics-based one. We\nformally define the expected properties of a Completion Engine in\nour framework in Definition 3.4.\n2.3 Automated Program Repair\nAutomated Program Repair (APR) aims to generate patches given\nthe buggy code location and the bug-exposing tests. Traditionally,\nAPR approach can be categorized as constraint-based [13, 35, 43, 47],\nheuristic-based [36, 37, 63] and template-based [21, 25, 34, 41, 42,\n46]. Among these classic techniques, template-based tools have\nbeen shown to achieve the highest number of bug fixes by using\nhandcrafted repair templates to target specific bug patterns [ 21].\nHowever, these handcrafted patterns cannot cover all types of bugs\nthat exist and as such, template-based tools cannot fix bugs outside\nof their pre-determined templates.\nTo address the issue faced by template-based APR tools, re-\nsearchers resort to Neural Machine Translation (NMT) [57] to de-\nvelop NMT-based APR tools [11, 29, 39, 44, 72, 74]. NMT-based APR\ntools train an NMT model to translate the input buggy code into\nthe correct code through bug-fixing datasets containing pairs of\nbuggy and fixed code. However, these bug-fixing datasets may con-\ntain only a small number/types of bug fixes, especially compared\nto a large amount of available open-source code snippets, due to\nthe difficulty in obtaining bug-fixing commits [67]. Additionally,\nthe datasets can fail to filter out unrelated commits [ 30] such as\nrefactoring, which adds noise to the training datasets. Due to this\nreliance on training using bug-fixing datasets, these NMT-based\ntools also cannot generalize to bug types not seen during training.\nRecently, researchers begin to directly apply LLMs for APR [66].\nAlphaRepair [67] is the first to directly use LLMs for cloze-style\n(or infilling-style) APR: it masks out the buggy code snippet and\nthen uses CodeBERT [17] to directly fill in the correct code given\nthe surrounding context. While AlphaRepair demonstrates the po-\ntential to use encoder-only models for cloze-style APR, other stud-\nies [33, 53, 66] have looked into applying all three types of LLM ar-\nchitecture. FitRepair [64] further improves AlphaRepair via domain-\nspecific fine-tuning and prompting strategies leveraging the plastic\nsurgery hypothesis [6]. Even more recently, researchers have ap-\nplied dialogue-based models for APR [8, 56, 68, 69]. For example,\nChatRepair [69] proposes a fully automated conversational APR\napproach by learning from prior patching attempts, including both\npatch code and test failure information.\nCompared to traditional and NMT-based APR techniques, LLM-\nbased techniques are able to achieve new state-of-the-art bug-fixing\nresults [66, 67]. While the performance is impressive, one particular\nlimitation of these techniques is the lack of guidance in patch gen-\neration. Prior work mainly treats the LLM as a black box and only\nqueries the model via beam search [67] or sampling [33, 53, 66]. This\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\nmeans LLMs, while powerful, may still generate invalid patches\ngiven the current code context.\nIn this work, we address these limitations by using a semantics-\nbased Completion Engine to guide and prune the LLM search space.\nOur approach is orthogonal to recent LLM-based APR techniques\nand can be easily combined with them. In fact, NMT-based APR\ntechniques have also attempted to tackle this problem. CURE [29]\nfirst statically obtains the valid identifiers and forces the NMT model\nto only select from valid identifiers during generation. Recoder [74]\nbuilds an edit-based NMT model to enforce syntax correctness and\nintroduce placeholder tokens and then as a post-processing step,\nRecoder will replace placeholder tokens with statically determined\nvalid identifiers. RewardRepair [72] on the other hand, attempts to\nincrease the number of compilable patches by penalizing uncompi-\nlable patches during training. Compared to these prior techniques,\nRepilot is more general and effective. Repilot does not require any\ndomain-specific training and leverages the incremental analysis of\noff-the-shelf Completion Engines to enforce guaranteed constraints\nto guide LLMs on the fly.\n3 PRELIMINARIES\nIn this section, we first define concepts about programming lan-\nguages used throughout the paper (¬ß3.1). Then we discuss theformal\nabstractions of the two key components used in our Repilot frame-\nwork: Completion Engine (¬ß3.2) and Large Language Model (¬ß3.3).\nThese two abstractions are crucial in that each of them describes a\ncollection of fitting implementations, which forms the reason why\nRepilot is a generalizable framework.\n3.1 Languages with Static Checking\nWe now introduce the concept of programming languages equipped\nwith static checking and define the feasibility of a partial program\nbefore the formulation of the Completion Engine (Definition 3.3).\nDefinition 3.1 (Programming Language with Static Checking). A\nprogramming language with static checking is defined as a pair of\nits character set Œ£pl and its static specification Œ¶ ‚äÜŒ£‚àó\npl as a unary\nrelation on Œ£‚àó\npl.\nPLs = (Œ£pl, Œ¶), (3.1)\nGiven a prog ‚ààŒ£‚àó\npl, the notation Œ¶(prog)(or prog ‚ààŒ¶) states that\nprog is a statically valid program in this language. For statically-\ntyped programming languages like Java, the compilation check is a\nkind of static checking.\nDefinition 3.2 (Static Feasibility of A Partial Program). For a par-\ntially written program prog ‚ààŒ£‚àó\npl, we say it is feasible at the caret\nposition caret with respect to the static specification Œ¶, written as\n(prog, caret)‚ä®Œ¶, if and only if there exists a possible continuation\nafter caret with which completing prog results in a statically valid\nprogram. The definition can be formally written as\n(prog, caret)‚ä®Œ¶ ‚âú‚àÉcont ‚ààŒ£‚àó\npl, Œ¶(prog [caret ‚Üêcont]), (3.2)\nwhere we use the notation prog [caret ‚Üêcont]as the action of\ncompleting prog at caret with cont, i.e.\nprog [caret ‚Üêcont]‚âúprog0..caret ¬∑cont ¬∑progcaret..|prog|. (3.3)\nIn Algorithm 1, we extend this notation to accept arange: N√óN, so\nthat prog [range ‚Üêhunk]specifies the action of replacing prog‚Äôs\ncontents within range with hunk.\n3.2 Abstraction of Completion Engines\nA Completion Engine, showed in Figure 2, provides suggested con-\ntinuations to a partially written program given the caret position.\nDefinition 3.3 (Completion Engine). Formally speaking, a Com-\npletion Engine CE is a pair\nCE = (Œ£pl, complete), (3.4)\nwhere Œ£pl is the character set of the target language, and\ncomplete: (Œ£‚àó\npl, N)‚ÜíP( Œ£‚àó\npl)‚à™{ unknown} (3.5)\nis a function to obtain the completions given a program at some\ncaret position, with unknown indicating the engine cannot deter-\nmine the suggestions from the code context (e.g., when completing\na variable declaration). Note that we make a distinction between\nunknown and empty completions ‚àÖbecause in this paper we are\ninterested in a specific group of strict Completion Engines that\nhelps determine the feasibility of a partial program.\nCompletion \nEngine\n‚Ä¶ \npublic MultiplePiePlot(‚Ä¶) { \n  super(); \n  this.set \n‚Ä¶\nProgram and caret position\nDataset \nDatasetGroup \nBackgroundAlpha \nDataExtractOrder \nDrawingSupplier \nForegroundAlpha \n‚Ä¶\nCompletions\n( /uni03A3‚àó\npl , N ) ‚Üí P( /uni03A3‚àó\npl ) ‚à™ { unknown }\n( /uni03A3‚àó\npl , N )\nP( /uni03A3‚àó\npl ) ‚à™ { unknown }\ncomplete\nFigure 2: Abstraction of a Completion Engine.\nDefinition 3.4 (Strict Completion Engine). Assume that a Comple-\ntion Engine CE can obtain a set of completions given a program\nprog feasible at caret (i.e., (prog, caret)‚ä®Œ¶):\ncompletions = complete(prog, caret)\nwhere completions ‚â† unknown. (3.6)\nThen, CE is said to be strict if and only if, under this condition,\ncontinuing prog with any code that does not match with this set of\ncompletions yields an infeasible program at the new caret position:\n‚àÄc ‚àâ Prefix(completions), (prog‚Ä≤, caret‚Ä≤)Ã∏‚ä®Œ¶,\nwhere prog‚Ä≤= prog [caret ‚Üêc]and caret‚Ä≤= caret +|c|,\nPrefix(¬∑)= {ùëê |ùë† ‚àà¬∑ and ùëê is a prefix of ùë† or vice versa}.\n(3.7)\nThis definition essentially means that a strict Completion Engine\nshould not give incorrect suggestions. It should return unknown\nwhenever unsure. A trivial strict Completion Engine can be the one\nthat always returns unknown.\n3.3 Abstraction of LLMs\nIn this section, we give a formal abstraction of an encoder-decoder\nbased LLM as showed in Figure 3, which in practice is more complex\nbut conforms to the abstraction. The abstraction subsumes decoder-\nonly models and can also describe encoder-only models that use\nthe encoder outputs directly as token probabilities for generation.\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\n/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep ) /uni03A3lm ‚Üí ‚à™ 0 , 1 ]\nDecoder inputs\nEncoder Decoder\nEncoder inputs   \n/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep )\nProbability Map\n! 91%\n3%\n‚Ä¶\n/uni03A3lm ‚Üí ‚à™ 0 , 1 ]\n/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep )/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep )\n/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep )/uni03A3‚àó\nlm ‚Üí EncRep EncRep ‚Üí ( /uni03A3‚àó\nlm ‚Üí DecRep )\n‚Ä¶\nAt\nFigure 3: Abstraction of encoder-decoder based LLM.\nDefinition 3.5 (Large Language Model). Formally, We define an\nencoder-decoder based LLM LM as a 3-tuple\nLM = (Œ£lm, encode, decode), (3.8)\nwhere Œ£lm is a vocabulary consisting of the set of tokens defined\nby the model. The encoder encode is a function that maps from an\ninput sequence to its encoded representation in EncRep:\nencode: Œ£‚àó\nlm ‚ÜíEncRep. (3.9)\nThe decoder decode, defined below, then memorizes the encoded\nrepresentation in EncRep, takes as input a sequence of tokens, and\nproduces as output its decoded representation in DecRep:\ndecode: EncRep ‚Üí\u0000Œ£‚àó\nlm ‚ÜíDecRep\u0001 . (3.10)\nIn this definition, the decoder memorizing the encoded represen-\ntation is modeled as a higher-order function that returns a detailed\ndecoding function given the encoded representation. The decoded\nrepresentation in DecRep essentially assigns a probability to each\ntoken in the vocabulary to state its likelihood of being the next\ntoken in the sequence. Therefore, we can define DecRep as\nDecRep = Œ£lm ‚Üí[0, 1]. (3.11)\n4 APPROACH\nFollowing most recent deep learning based APR tools [67, 72, 74],\nRepilot focuses on fixing single-hunk bugs, where the patch is\nobtained by changing a continuous section of code under perfect\nfault localization. Repilot can be extended for multi-hunk bugs\nby replacing all hunk locations at the same time with separate\ninfilling tokens and using LLM to generate the replacement hunks.\nBenefiting from the era of LLMs, as shown in Figure 4, in this paper,\nwe treat the repair problem as a cloze task [67], where a patch is\nformed by first replacing the buggy hunk with a masked span token\n(<SPAN>) and then using the LLM to direct synthesize the fixed hunk\nfrom the surrounding code context to replace the span token.\nprivate boolean inSpecificScope(...) { \n   int bottom = stack.size() -1; \n   if (bottom > MaxScopeSearchDepth) { \n      bottom = MaxScopeSearchDepth; \n   } \n   final int top = bottom > ‚Ä¶\nBug\nprivate boolean inSpecificScope(...) { \n   final int bottom = stack.size() - 1 \n   final int top = bottom > ‚Ä¶\n¬†\nCloze\nPatch\nprivate boolean inSpecificScope(...) { \n    \n   final int top = bottom > ‚Ä¶\n¬†<SPAN>\nFigure 4: Cloze-style program repair.\n4.1 Overview\nFigure 5 shows an overview of how Repilot synthesizes a program\nthat acts as the repaired hunk of the original buggy program. The\ngeneration loop consists of a loop that keeps updating the gen-\neration with tokens newly generated from the synergy between\nthe language model and Completion Engine. The loop starts by\napplying the current generation as the input to the language model\n( 1 ), which returns a search space of a mapping from a suggested\nnext token to its probability. Repilot then enters a token selection\nphase that repeatedly samples a token from the search space, check-\ning its feasibility, and pruning the search space until a token is\naccepted. Every time a token is sampled, Repilot first checks if it\nhits the memorization ( 2 ), which stores the tokens that are known\nto be feasible or infeasible. The memorization of infeasible tokens\nincludes the use of a prefix tree data structure (Trie) discussed in\n¬ß4.3. When the token hits the memorization and is infeasible, the\nsearch space is pruned by setting this token‚Äôs probability to zero\n( 3 ), and the next sampling will run on the updated search space.\nIn this way, the same token is not sampled again during the token\nselection phase. If the token misses the memorization, the search\nspace is pruned under the guidance of the Completion Engine ( 4 ),\nwhich we elaborate in ¬ß4.2. Provided that the sampled token is re-\njected by the Completion Engine, Repilot zeroes out its probability.\nOtherwise, it is accepted and this token selection process termi-\nnates. The memorization gets updated in both cases ( 5 ). After a\ntoken is accepted ( 6 ), we further leverage the Completion Engine,\ntrying to actively complete the token ( 7 ). The active completion,\ndiscussed in ¬ß4.4, may either produce more tokens or add noth-\ning to the accepted token. Finally, Repilot appends all the newly\ngenerated tokens to the current generation and begins a new loop\nuntil a complete patch is generated. The loop stops when the model\ngenerates the special token end-token.\nAlgorithm 1 details this process and shows how a complete patch\nprogram is generated using what is established in ¬ß3. It additionally\ndescribes how Repilot performs the pre-processing (Lines 3 to 6)\nand formalizes completion-guided pruning procedure illustrated\nin Figure 5 using two functions GuidedPrune and Actively-\nComplete (Lines 7 to 17). In all our algorithms, we use a \"dot-\nnotation\" to specify an entity of a tuple (e.g., LM.encode), but use\nan abbreviation form when the context is clear (e.g.,Œ£lm and Œ£pl for\nLM.Œ£lm and CE.Œ£pl). We also optionally apply type annotations for\nclarification. Note that we simplify the definition of the Completion\nEngine by restricting it to be called with one program. In practice,\na Completion Engine is always initialized with the entire project\nand can provide suggestions based on global information.\n4.2 Completion-Guided Search Space Pruning\nIn this section, we explain the core idea of how Repilot utilizes a\nCompletion Engine to prune the search space of an LLM.\nAlgorithm 2 explains in depth how a Completion Engine helps\nprune the model‚Äôs search space. The function GuidedPrune takes\nas inputs a Completion Engine CE, the current program prog, the\ncurrent caret positioncaret, and the probability maptokens given by\nthe model, and produces a token next-token as the continuation of\nthe program prog at position caret. The function consists of awhile-\nloop (Lines 2 to 11) where Repilot first samples a possible next\ntoken according to the probabilities (Line 3), updates the current\nprogram accordingly (Line 4), and moves the caret after next-token.\nRepilot then invokes the Completion Engine using the function\ncomplete defined in Equation (3.5), given the program prog‚Ä≤and\nthe caret position caret‚Ä≤. If the result is not unknown but there is\nno completion (Line 8), it means that no possible continuation can\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\nToken sampling loop\nName\nEnd\nÓóä\nÓÖå Óóä\nÓ°¨\nÓ°¨\n!\nPruning + Active completion\nÓ°®\nÓ°©Patch\nText\nCompletion \nEngine\nMemorizationPruningÓÖå\nÓÖå\nÓÖå\nÓóä\n‚Ä¶\nLanguage \nModel\nCompletion \nEngine\nActive \nCompletion\nÓ°¨\nCompletion \nEngine\nPruningÓÖå New tokens\nÓóä\n Ó°¨\nEnd TagString name = t.as\nÓóä\n Óóä\n Óóä\n Óóä‚Ä¶\nGenerated tokens\nÓ°®Bug\nÓ°©Patch\nString \nName \nEnd \n‚Ä¶\nÔà∞ÓêÜ\nString*\nÓ†¥\n‚Ä¶\n String* ‚Ä¶\nÔà∞ÓêÜ\nMemorization\n(Trie)\nSearch space\n‚Ä¶\n3%\nName\n91%\nString\nEnd\n0.2%\nPruned search space\n0%\nName\n0%\nString\nEnd\n0.2%\nÓÖå\nÓ°∂\nÔà∞\n‚Ä¶\nPruned search space\nÓóä\nÔà∞\n ÓÖå\n91%\nString\n‚Ä¶\n3%\nName\n0.2%\nEnd\nPruned search space\n0.2%\nEnd\n‚Ä¶\n3%\nName\n91%\nString\nÓóä\nEnd\nÓÖå\nÓ°∂\nPrune with \nMemorization\nObtain probabilities from \nLanguage Model\n1\nCheck memorization2\nUpdate Memorization5\nPrune using Completion Engine4\n3\nToken \naccepted\n6\nActively complete the token7\nUpdate the patch with new tokens8\nFigure 5: Overview of Repilot.\nAlgorithm 1 Main Repair Loop of Repilot\nInputs: Large Language Model LM, Completion Engine CE, Buggy pro-\ngram prog, and Range of buggy hunk range.\nOutput: Patch for the buggy program.\n1: func Repair(LM, CE, prog: Œ£‚àó\npl, range: N √óN) ‚ÜíŒ£‚àó\npl :\n2: ‚ä≤ Initializations based on Definition 3.5 ‚ä≥\n3: encoder-inputs: Œ£‚àó\nlm := BuildInputs(prog, range)\n4: encoded-rep: EncRep := LM.encode(encoder-inputs)\n5: decoder : Œ£‚àó\nlm ‚ÜíDecRep := LM.decode (encoded-rep)\n6: hunk: Œ£‚àó\nlm := ùúÄ\n7: while true do\n8: ‚ä≤ Form patch by replacing buggy hunk with hunk ‚ä≥\n9: patch := prog [range ‚ÜêStr(hunk)]\n10: ‚ä≤ Move caret after the current generation ‚ä≥\n11: caret := range.start +|Str(hunk)|\n12: tokens: Œ£lm ‚Üí[0, 1]:= decoder (hunk)\n13: next-token: Œ£lm := GuidedPrune(CE, patch, caret, tokens)\n14: if next-token = end-token then\n15: return patch\n16: completion-toks: Œ£‚àó\nlm :=ActivelyComplete(CE, patch, caret)\n17: hunk := hunk ¬∑next-token ¬∑completion-toks\nbe formed after next-token, so the token next-token is considered\ninfeasible, thus pruned (Line 9) in this round of search, and the loop\nwill continue (Line 10). Otherwise, we consider the token feasible\nand return next-token (Line 11).\nThe pruning at Line 9 is done by setting the probability of the\nentry next-token of the probability maptokens to zero. The notation\nused at this line is defined subsequently. Assume that\nf : X ‚ÜíY = {x0 ‚Ü¶‚Üíy0, x1 ‚Ü¶‚Üíy1, . . .} (4.1)\nis an arbitrary function, and\na: X ‚áÄ Y = {x‚Ä≤\n0 ‚Ü¶‚Üíy‚Ä≤\n0, x‚Ä≤\n1 ‚Ü¶‚Üíy‚Ä≤\n1, . . .} (4.2)\nis a partial function of the same type, meaning that only a subset of\ninputs in the domain X is associated with an output in the range Y.\nWe define the action of changing the output values of the inputs in\nf using the assignments given by a as\nf [a]= (f ‚àífremoved)‚à™a\nwhere fremoved = {x‚Ä≤‚Ü¶‚Üíf (x‚Ä≤)| x‚Ä≤‚Ü¶‚Üíy‚Ä≤‚ààa}. (4.3)\n4.3 Memorization for Faster Search\nAlgorithm 2 (GuidedPrune) involves a loop of trials and pruning\nactions, which slows down the repair task in some situation. To\nAlgorithm 2 Completion-Guided Search Space Pruning\nInputs: Completion Engine CE, Current Program prog, Caret Position\ncaret, and Token Probability Map tokens.\nOutput: Next token next-token to generate.\n1: func GuidedPrune(CE, prog, caret, tokens: Œ£lm ‚Üí[0, 1]) ‚ÜíŒ£lm :\n2: while true do\n3: next-token: Œ£lm := Sample(tokens)\n4: prog‚Ä≤:= prog [caret ‚ÜêStr(next-token)]\n5: caret‚Ä≤:= caret +|Str(next-token)|\n6: ‚ä≤ completions: P(Œ£‚àó\npl )‚à™{ unknown} ‚ä≥\n7: completions := CE.complete(prog‚Ä≤, caret‚Ä≤)\n8: if completions ‚â† unknown and|completions|= 0 then\n9: tokens := tokens [{next-token ‚Ü¶‚Üí0}]\n10: continue\n11: return next-token\nspeedup its search procedure, we apply several memorization tech-\nniques to reduce the frequency of invoking the Completion Engine\nfor analysis.\nMemorizing rejected tokens. To repair a bug in practice requires\ngenerating plenty of samples, meaning that the same programprog‚Ä≤\nand caret‚Ä≤(Lines 4 to 5) may occur repeatedly in Algorithm 2 (Guid-\nedPrune). Therefore, we can memorize all the tokens pruned at\nLine 9 by storing them in a variable\nrejected : (Œ£‚àó\npl, N)‚ÜíP( Œ£lm), (4.4)\nwhich maps from a program prog and a caret position caret to a set\nof rejected tokens. Then we zero the probabilities of the rejected\ntokens in advance, written as\ntokens := tokens [{tok ‚Ü¶‚Üí0 |tok ‚ààrejected (prog, caret)}], (4.5)\nbefore the while-loop (Line 2) starts.\nMemorizing accepted tokens. Besides rejected tokens, we can also\nmemorize tokens that are accepted before in a variable\naccepted : (Œ£‚àó\npl, N)‚ÜíP( Œ£lm) (4.6)\nto avoid the overhead incurred from querying the Completion\nEngine at Lines 7 to 8.\nBuilding a Prefix Tree of Rejected Tokens. It is common that many\ntokens in the vocabulary of the language model are prefixes of\nanother. And it is obvious that if a token is rejected, meaning that\nno possible continuation can be formed after the token to obtain a\nstatically valid program, then any token sharing such prefix should\nbe rejected. For this reason, we build and keep updating a prefix\ntree, or Trie, of all the rejected tokens given prog and caret, and\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\nchecks if any of the tokens in the Trie is a prefix ofnext-token right\nafter Line 3 in Algorithm 2. If it is the case, Repilot directly skips to\nthe next iteration, avoiding further analysis.\n4.4 Active Completion\nNot only is a Completion Engine able to determine the feasibility of\na possible next token suggested by the model, as shown in ¬ß4.2, but it\ncan also proactively suggest a potential continuation of the current\nprogram without querying the model, just like how developers\nbenefit from autocompletion.\nAlgorithm 3 describes active completion in detail. The function\nActivelyComplete takes three inputs: the Completion EngineCE,\nthe current program prog, and the current caret position caret, and\noutputs a sequence of tokenscompletion-toks as the continuation of\nprog at caret. Initially, Repilot gets the completion result according\nto Equation (3.5), given prog and caret (Line 2), and checks if it is\nunknown (Line 3). If it is the case ( completions = unknown), the\nresult is set to an empty string, meaning no extra completions are\nproduced (Line 4). Otherwise, Repilot calculates the common prefix\nof all the completions (Line 6). Note that the type of the resultant\nvariable completion is a sequence of characters in the Programming\nLanguage alphabet, different from the language model‚Äôs Œ£lm, so\nRepilot further aligns the completion to fit the model‚Äôs vocabulary\n(Line 7). Finally, the result is returned at Line 8.\nAlgorithm 3 Active Completion\nInputs: Completion Engine CE, Program prog, and Caret Position caret.\nOutput: The actively completed tokens completion-toks.\n1: func ActivelyComplete(CE, prog, caret) ‚ÜíŒ£‚àó\nlm :\n2: completions: P(Œ£‚àó\npl )‚à™{ unknown}:= CE.complete(prog, caret)\n3: if completions = unknown then\n4: completion-toks := ùúÄ\n5: else\n6: completion: Œ£‚àó\npl := CommonPrefix(completions)\n7: completion-toks: Œ£‚àó\nlm := AlignTokens(Œ£lm, completion)\n8: return completion-toks\n4.5 Soundness of Repilot\nIn this section, we show the theoretical guarantee of each algorithm\ndiscussed above under the condition that the Completion Engine is\nstrict (Definition 3.4).\nLemma 4.1 (Soundness of Pruning).The tokens pruned away\nin Algorithm 2 ( GuidedPrune) result in infeasbile programs.\nDiscussion. From Equation (3.7) in Definition 3.4, we can de-\nduce that a program is infeasible at some caret position if the Com-\npletion Engine does not returnunknown but the set of completions\nis empty, i.e.,\n|completions|= 0 ‚Üí(prog, caret)Ã∏‚ä®Œ¶\nif completions ‚â† unknown (4.7)\nThe pruning at Algorithm 2 happens at Lines 8 to 9, which is exactly\nwhat is described above. As a result, we can conclude that the\nprogram with next-token appended is infeasible, and hence it is safe\nfor Repilot to abandon the token. ‚ñ°\nLemma 4.2 (Soundness of Memorization).The memorization\ndiscussed in ¬ß4.3 does not affect GuidedPrune‚Äôs behavior.\nDiscussion. The theorem holds because all the memorization\ntechniques mentioned in ¬ß4.3 do not change the semantics ofGuid-\nedPrune but only speed up the process. ‚ñ°\nLemma 4.3 (Soundness of Active Completion).If a program\nis feasible at some caret position, the new program produced by Algo-\nrithm 3 ( ActivelyComplete) is feasible at its new caret position.\nDiscussion. Based on Equation (3.7) from Definition 3.4, any\ncontinuations not matching the set of completions would bring\nabout an infeasible program. In the case where these completions\nhave a shared common prefix, any continuations not starting with\nthis common prefix would be invalid. Therefore, completing the\noriginal program with the common prefix (Line 6 in Algorithm 3)\nis the only way to yield a new feasible program. ‚ñ°\nOn the basis of Lemmas 4.1 to 4.3, we can easily prove that\nRepilot‚Äôs overall algorithm is sound.\nTheorem 4.4 (Overall Soundness).Algorithm 1 ( Repair) does\nnot miss any feasible programs in the language model‚Äôs search space.\nWhen will Repilot fail? Although the theorems are about the\nsoundness of Repilot, i.e., it prunes the search space correctly , it does\nnot provides any guarantee that Repilotproduces a valid patch every\ntime. Therefore, Repilot‚Äôs expected behavior is to be able to obtain\nvalid patches more efficiently, rather than being entirely error-free\nduring the generation.\n5 EXPERIMENTAL SETUP\nIn this paper, we study the following research questions to evaluate\nRepilot.\n‚Ä¢RQ1: How does Repilot‚Äôs bug fixing capability compare with\nstate-of-the-art APR techniques (¬ß6.1)?\n‚Ä¢RQ2: How effective is Repilot in improving the compilation rate\nof patch generation (¬ß6.2)?\n‚Ä¢RQ3: Are all components of Repilot making positive contribu-\ntions to its effectiveness (¬ß6.3)?\n‚Ä¢RQ4: Can Repilot generalize to different subjects of bugs and\nmodels (¬ß6.4)?\nWe first compare the repair performance of Repilot, instantiated\nwith CodeT5, against state-of-the-art APR tools across both tradi-\ntional, NMT-based, and LLM-based tools on the Defects4J datasets\nin RQ1. In RQ2, we then closely evaluate the improvement in compi-\nlation rate ‚Äî percentage of compilable patches generated to demon-\nstrate that Repilot is not only effective in bug repair but can gener-\nate a higher number of compilable patches compared with existing\ntools. Furthermore, we perform a detailed ablation study in RQ3\nto evaluate the contribution of different components of Repilot.\nFinally, in RQ4, we extend our evaluation of Repilot beyond its use\nwith CodeT5 in the previous RQs. We go a step further by imple-\nmenting Repilot with InCoder and assessing the performance of\nRepilot using both CodeT5 and InCoder on single-hunk bugs from\nboth Defects4J 1.2 and 2.0 to demonstrate the generalizability of\nRepilot across different LLMs and bug subjects.\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\n5.1 Implementation\nWe use the Python implementation of the CodeT5-large and the\nInCoder-6.7B models obtained on Hugging Face [ 26]. We build\nour generation pipeline in Python with 5K lines of code and imple-\nment a modified version of the Eclipse JDT Language Server [1, 18]\nin Java with 1.5K additional lines of code, which serves as the\nstrict Completion Engine of our framework. Our default genera-\ntion uses top-p nucleus sampling [24] with ùëù = 1.0, temperature =\n1.0, ùëöùëéùë•-ùë°ùëúùëòùëíùëõùë† = 50 and samples 5000 times per bug for fair\ncomparisons against prior works (¬ß6.1 and ¬ß6.2). Due to the high\ncost of APR, we sample 500 times per bug for the ablation study\n(¬ß6.3) and the generalizability evaluation (¬ß6.4). Following prior\nwork [39, 44, 67, 74], we use a timeout of 5 hours to generate and\nvalidate all patches per bug. We generate and validate patches on a\n32-Core with Ryzen Threadripper PRO 3975WX CPU, 256 GB RAM\nand NVIDIA RTX A6000 GPU, running Ubuntu 20.04.4 LTS with\nJava version OpenJDK 1.8.0_181.\n5.2 Subject Programs\nWe use the popular repair benchmark of Defects4J for our evalu-\nation. Defects4J is a manually curated Java dataset with pairs of\nbuggy and patched versions of the source project along with de-\nveloper test suites for validation. Following prior work and APR\nliterature convention, we separate Defects4J into Defects4J 1.2, con-\ntaining 391 bugs (removing 4 depreciated bugs) from 6 Java source\nprojects, and Defects4J 2.0, containing 438 new bugs from 9 addi-\ntional projects. For Defects4J 1.2, we focus on only the single-hunk\nbugs as Repilot is designed for single-hunk repair. Note this is also\nthe evaluation setting used in the prior baseline [72]. Furthermore,\nwe remove the bugs that are incompatible with our Completion En-\ngine due to engineering issues. In total, we consider 138 single-hunk\nbugs from Defects4J 1.2 and 135 single-hunk bugs from Defects4J\n2.0. For our main evaluation in RQ1, following the same setup as\nprior LLM for APR work [66, 67], we report the results on all 135\nsingle-hunk bugs from Defects4J 1.2 and 76 single-line bugs (a\nsubset of single-hunk bugs) from Defects4J 2.0. Meanwhile, in our\ngeneralizability study (RQ4), we further evaluate Repilot on the full\nset of single-hunk bugs from both Defects4J 1.2 and 2.0 for both\nCodeT5 and InCoder.\n5.3 Compared Techniques\nWe compare Repilot against state-of-the-art baselines from tradi-\ntional, NMT-based, and LLM for APR tools. We evaluate against\nAlphaRepair [ 67] as it is the top performing LLM for APR ap-\nproach. For NMT-based approaches, we choose 6 recent tools:\nRewardRepair [72], Recoder [74], CURE [29], CoCoNuT [44], DL-\nFix [39] and SequenceR [11] based on the NMT architecture. Addi-\ntionally, we compare against 12 traditional APR tools: PraPR [21],\nTBar [41], AVATAR [42], SimFix [28], FixMiner [34], CapGen [63],\nJAID [ 9], SketchFix [ 25], NOPOL [ 13], jGenProg [ 45], jMutRe-\npair [46] and jKali [46]. Altogether, we include 19 APR baselines and\ncompare Repilot against them on Defects4J 1.2 and 2.0. Our evalua-\ntion setting is on perfect fault localization ‚Äì where the ground-truth\nlocation of the bug is given to the APR tool. We note that this is the\npreferred evaluation setting as it eliminates any differences caused\nby different fault localization methods [29, 44, 58, 74]. We follow the\nTable 1: Number of correct fixes on Defects4J 1.2 single-hunk\nand Defects4J 2.0 single-line bugs\nTool Methodology\n#Correct Fixes\nDefects4J 1.2 Defects4J 2.0 Total\nCoCoNuT NMT 30 - -\nDLFix NMT 32 - -\nPraPR Template 35 - -\nTBar Template 41 7 48\nCURE NMT 43 - -\nRewardRepair NMT 45 24 69\nRecoder NMT 51 10 61\nAlphaRepair LLM 52 34 86\nRepilot LLM 66 50 116\nRepilot RewardRepair\nAlphaRepair\nRecoder\nCURE\n8 7Repilot RewardRepair\nAlphaRepair\nRecoder\nOthers\na) with best LLM and NMT-based baselines b) with all APR tools\nFigure 6: Correct fix Venn diagrams on Defects4J 1.2\nconvention used in prior work [21, 29, 41, 74] and directly report\nthe bug fix results obtained from previous studies [21, 67, 72].\n5.4 Evaluation Metrics\n‚Ä¢Plausible patches are patches that pass all test cases but may\nviolate the real user intent.\n‚Ä¢Correct patches are patches that are semantically equivalent\nto the developer patch. Following common APR practice, we\ndetermine semantic equivalency by manually examining each\nplausible patch.\n‚Ä¢Patch compilation rate is also used in many deep learning\nbased APR works [ 29, 72], which indicates the percentage of\ncompilable patches in all generated patches.\n6 RESULT ANALYSIS\n6.1 RQ1: Comparison with Existing Tools\nIn RQ1 and RQ2, we follow the prior approach for cloze-style\nAPR [67] to make use of repair templates for a faithful evaluation.\nInstead of replacing the entire buggy line with model-generated\ncode, these templates systematically keep parts of the buggy line\nto reduce the amount of code the LLM needs to generate. Note\nthat we do not apply any repair templates in RQ3 and RQ4 because\nwe consider a smaller number of samples there (i.e., 500 samples\nas shown in Section 5.1) and also want to focus on the impact of\ndifferent experimental configurations.\nDefects4J 1.2. We first compare Repilot against the state-of-the-art\nAPR tools on single-hunk bugs from Defects4J 1.2. Table 1 shows\nthe number of correct patches produced by Repilot, evaluated in\ncloze-style, along with the baselines. Repilot achieves the new state-\nof-the-art result of 66 correct bug fixes on Defects4J 1.2, outperforming\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\nprivate String getRemainingJSDocLine() {            \nString result = stream.getRemainingJSDocLine();            \nunreadToken = NO_UNREAD_TOKEN;            \nreturn result;            \n}\nunreadToken = NO_UNREAD_TOKEN; \n+\nBug-ID: Closure-133\nPatch Generation Process\nLLM\nCompletion \nEngine\nFigure 7: Unique bug fix by Repilot on Defects4J 1.2\nall previous APR tools . Figure 6a shows the Venn diagram of the\nunique bugs fixed for the top performing LLM- and NMT-based\nAPR tools where Repilot is able to obtain the highest number of\n8 unique bugs Furthermore, Figure 6b compares the unique bugs\nfixed for all top-performing baselines and with all other APR tools\ncombined (Others). We observe that Repilot is able to fix 7 bugs\nthat no other baselines have been able to fix so far.\nTo demonstrate the ability of Repilot to fix difficult bugs, Fig-\nure 7 shows a unique bug ( Closure-133) from Defects4J 1.2 that\nonly Repilot can fix. This bug is fixed by adding the new assign-\nment statement using the global variable NO_UNREAD_TOKEN which is\ndifficult to generate as it does not appear within the surrounding\ncontext of the bug location. Repilot first uses CodeT5 to generate the\ninitial prefix of unread. Then using the Completion Engine, Repilot\nrecognizes that Token is the only semantically correct continuation\nand directly performs active completion to returnunreadToken. Sim-\nilarly for generating NO_UNREAD_TOKEN, Repilot first generates NO_\nand then uses active completion to directly generate this rare iden-\ntifier without having to repeatedly sample the LLM. It is difficult for\nprior LLM- and NMT-based APR tools to generate this fix as LLMs\nor NMT models may not be able to complete this rare identifier\nsince it requires multiple continuous steps to generate. In contrast,\nRepilot, through the use of active completion, can directly generate\nthis rare identifier given only the initial identifier prefix to quickly\narrive at this correct patch.\nDefects4J 2.0. We further evaluate Repilot against baselines eval-\nuated on the single-line bugs in Defects4J 2.0. For these bugs, we\nfollow prior approach for cloze-style APR [67] to make use of repair\ntemplates. Instead of replacing the entire buggy line with model-\ngenerated code, these templates systematically keep parts of the\nbuggy line (e.g., prefix or suffix, method parameters and calls) to re-\nduce the amount of code the LLM needs to generate. We apply these\nrepair templates for Defects4J 2.0 single-line bugs only since they\nare designed for single-line bugs. Table 1 also shows the number\nof correct fixes on Defects4J 2.0 compared with the baselines. We\nobserve that Repilot is able to fix the highest number of bugs 50 (16\nmore than the next best baseline) on Defects4J 2.0. This improvement\nover existing baselines shows that Repilot can generalize to two\nversions of Defects4J datasets and demonstrates the power of repair\ntemplates to boost the performance of LLM-based APR tools.\nFigure 8 shows a unique bug from Defects4J 2.0 that only Repilot\ncan fix. First, Repilot generates the patch up to the caret position.\nThe Completion Engine then captures the exact type of the object\nfrom Token.EndTag to String. Using this information, Repilot cor-\nrectly prunes tokens that are not a part of theString class (e.g., name\nand text). Hence, the generated patch contains a valid String class\nprivate void popStackToClose(Token.EndTag endTag) {\nString elName = endTag.name();\nString elName = endTag.name().toLowerCase();\nElement firstFound = null;\n+\nBug-ID: Jsoup-77\nPatch Generation Process\n-\nString elName = endTag.name().\ntext        \nname\n...        \ntoLower     \nType: Token.EndTag\nType: String\n‚úì\n‚úï\n‚úï\nCompletion Engine\nFigure 8: Unique bug fix by Repilot on Defects4J 2.0\nTable 2: Comparison with existing APR tools on compilation\nrate on Defects4J 1.2. \"-\" denotes data not available.\nTool\n% Compilable Patches\nTop-30 Top-100 Top-1000 Top-5000\nSequenceR 33% - - -\nCoCoNuT 24% 15% 6% 3%\nCURE 39% 28% 14% 9%\nAlphaRepair 25% 22% 16% 13%\nRewardRepair 45% 38% 33% 1 -\nRepilot 66% 62% 58% 59%\n1 This is the top 200 rate for RewardRepair as it does not include top 1000\nmethod of toLowerCase() which correctly fixes this bug. Similar\nto the previous unique bug fix in Defects4J 1.2, prior LLM-based\nAPR tools may waste a lot of time generating semantically incor-\nrect continuations as they do not have access to the type infor-\nmation. Furthermore, NMT-based APR tools such as CURE [ 29],\nover-approximating the list of valid identifiers by statically grabbing\nall the accessible fields, may not generate this fix since a pruned\nidentifier (e.g., name) can also be valid for a different object type.\nRepilot uses the Completion Engine to analyze partial programs\nand realize complex type propagation for effective pruning.\n6.2 RQ2: Compilation Rate Analysis\nWe evaluate the compilation rate of the patches generated by Repi-\nlot compared with prior learning-based APR techniques. Table 2\nshows the percentage of compilable patches on the Defects4J 1.2\ndataset. We observe that across all numbers of patches generated,\nRepilot significantly improves the percentage of compilable patches\ncompared with prior tools. We first notice that LLM-based APR\ntools (Repilot and AlphaRepair), are able to sustain their compila-\ntion rate compared with NMT-based tools (CoCoNuT and CURE)\nwhere the compilation rate drastically decreases as we increase the\nnumber of patches. This shows the ability for LLMs to generate\nlarge amounts of reasonable patches. Repilot is able to sustain a\nnear 60% compilation percentage at 1000 patches generated while\nthe prior approach is barely above 30%.\nCompared with CURE [29], where an overestimation of valid\nidentifiers is obtained via static analysis and used to prune invalid\ntokens generated by NMT model, Repilot leverages the powerful\nCompletion Engine to keep track of the current context to obtain\na more accurate pruning step. Furthermore, compared with Re-\nwardRepair [72], where the compilation rate is boosted through\npenalizing uncompilable patches during training, Repilot directly\nuses a LLM combined with a Completion Engine to avoid this high\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\nTable 3: Component contribution of Repilot\nVariant Generation\nTime\n%Compilable\nPatches\n%Plausible\nPatches\n#Plausible\nFixes\n#Correct\nFixes\nRepilot‚àÖ 0.232s 43.2% 3.95% 56 37\nRepilotp 0.294s 60.7% 5.02% 62 41\nRepilotm\np 0.255s 58.7% 4.82% 60 40\nRepilot 0.248s 63.4% 5.21% 63 42\ncost of training a new model. Additionally, Repilot uses the active\ncompletion ability of Completion Engine to directly generate these\nrare identifiers to further boost the compilation rate. As such, Repi-\nlot is able to achieve the highest percentage of compilable patches\nacross all four different settings.\n6.3 RQ3: Ablation Study\nTo study the contribution of each component of Repilot to its overall\neffectiveness, we conduct an ablation study that aims at justifying\nthe following hypothesis:\n‚Ä¢Algorithm 2 (GuidedPrune) helps LLM to achieve valid (compi-\nlable) patches more efficiently on a pruned search space.\n‚Ä¢Memorization (¬ß4.3) reduces the frequency of querying the Com-\npletion Engine, thus speeding up patch synthesis.\n‚Ä¢Active completion provides further guidance of synthesis that\nand helps Repilot efficiently achieve more valid patches.\n‚Ä¢The plausible rate of patches becomes higher along with the\ncompilation rate.\nTo give grounds for these hypotheses, we set up the following\nfour variants:\n‚Ä¢Repilot‚àÖuses only the base LLM (CodeT5) for patch synthesis.\n‚Ä¢Repilotp applies pruning defined in Algorithm 2.\n‚Ä¢Repilotm\np leverages memorization (¬ß4.3) on top of pruning.\n‚Ä¢Repilot employs active completion for further guidance.\nand evaluate them by comparing them against their efficiency in\ngenerating compilable, plausible patches, and correct patches.\nTable 3 shows the generation time (in seconds per patch), the\ncontribution in terms of the percentage of compilable and plausi-\nble patches among all uniquely generated patches, the number of\nplausible fixes, and the number of correct fixes for each of the four\nvariants on Defects4J 1.2 single-hunk bugs. We first observe that\njust using the base LLM for APR (Repilot‚àÖ), we achieve the lowest\ncompilation rate at 43.2%. By adding the pruning provided by the\nCompletion Engine, we can significantly improve the compilation\nrate to 60.7%, the number of plausible fixes from 56 to 62, and the\nnumber of correct fixes from 37 to 41. Additional improvement is\nmade by adding the active completion technique to achieve the\nfull Repilot with the highest compilation rate at 63.4%, plausible\npercentage 5.21%, the most number of plausible fixes at 63, and the\nmost correct fixes at 42.\nLooking at the patch generation time, starting from Repilot‚àÖ,\nadding pruning via Completion Engine incurs an over 25% overhead.\nHowever, this can be significantly reduced by using memorization\n(Repilotp) to achieve around 10% overhead by avoiding querying\nthe Completion Engine once we know an identifier is invalid. Fur-\nthermore, active completion can further reduce the overhead to\n7% since instead of having to sample the LLM for each step in the\ngeneration, we can actively complete an identifier.\nAs a result, all the components contribute to the overall effective-\nness of Repilot. Repilot can consistently increase the compilation\nand plausible rate, as well as produce more plausible/correct fixes\nwhile incurring minimal overhead compared with directly using\nLLMs for patch synthesis.\n6.4 RQ4: Generalizability\nTo demonstrate the generalizability of Repilot across different sub-\njects of bugs and models, on the one hand, we further evaluate\nRepilot with CodeT5 on all single-hunk bugs of Defects4J 2.0. On\nthe other hand, we additionally instantiate and evaluate Repilot\nwith a largerInCoder-6.7B model. Identical to RQ3, we also conduct\n500 samples in RQ4 due to the high cost of APR.\nTable 4 shows the comparison between the baseline Repilot‚àÖ\nand our full Repilot approach across different subjects of bugs and\nmodels. We consider the same set of Defects4J 1.2 single-hunk bugs\nas in RQ3 and an extra set of Defects4J 2.0 single-hunk bugs.\nUpon investigation, we can see that Repilot with CodeT5 sur-\npasses the baseline on Defects4J 1.2 as illustrated in RQ3. Further-\nmore, on Defects4J 2.0, it can also achieve 18.1 percentage points\n(pp) more compilable and 3.0 pp more plausible patches, as well as 6\nmore plausible fixes and 4 more correct fixes, with a 7.4% overhead.\nMeanwhile, when Repilot is instantiated with InCoder, it still\nproduces more compilable and plausible patches, as well as more\nplausible and correct fixes on both Defects4J 1.2 and Defects4J 2.0\nover the baseline InCoder. It eventually gives 6 more correct fixes\non Defects4J 1.2 and 1 more on Defects4J 2.0.\nOne major difference comparing Repilot with InCoder and\nCodeT5 is that when Repilot is equipped with InCoder, a much\nlarger model than CodeT5, it incurs negligible overhead. This is\nbecause compared to the high cost of autoregressive sampling using\nlarger models, the extra cost from querying the Completion Engine\nis much smaller and thus trivializes the overhead of Repilot when\napplied on larger models. Also, the larger InCoder model, whether\nor not it is applied with Repilot, can consistently fix more bugs\nacross both Defects4J 1.2 and 2.0 than CodeT5, further confirming\nprior finding that larger LLMs often perform better for APR [66].\nOverall, the experimental results indicate that Repilot can gener-\nalize to different sets of bugs (both single-hunk bugs in Defects4J\n1.2 and 2.0) as well as larger LLMs (InCoder)\n7 LIMITATIONS\nFirst, to bring out Repilot‚Äôs full potential, it is important that the\nCompletion Engine can provide useful guidance while remaining\nstrict (Definition 3.4). However, it is generally more difficult to\nbalance the usefulness and strictness of a Completion Engine in\nmany dynamically typed programming languages, such as Python,\ncompared with Java studied in this paper, which is a statically typed\nprogramming language. Meanwhile, there is a growing trend of\ndynamically typed languages adopting support for type hints [12,\n49, 54]. Considering this, we believe that Repilot can still provide\nsignificant advantages in such environments.\nAnother limitation of Repilot lies in the evaluation. On the one\nhand, while it is true that an increase in the compilation rate of\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\nTable 4: Generalizability of Repilot across both subjects of bugs and models\nVariant Model Subject of Bugs Generation Time %Compilable Patches %Plausible Patches #Plausible Fixes #Correct Fixes\nRepilot‚àÖ CodeT5-large Defects4J 1.2 0.232s 43.2% 3.95% 56 37\nRepilot CodeT5-large Defects4J 1.2 0.248s 63.4% 5.21% 63 42\nRepilot‚àÖ CodeT5-large Defects4J 2.0 0.230s 46.7% 9.02% 59 41\nRepilot CodeT5-large Defects4J 2.0 0.247s 64.8% 12.02% 65 45\nRepilot‚àÖ InCoder-6.7B Defects4J 1.2 1.70s 32.4% 3.85% 70 48\nRepilot InCoder-6.7B Defects4J 1.2 1.70s 47.2% 4.96% 78 54\nRepilot‚àÖ InCoder-6.7B Defects4J 2.0 1.67s 34.6% 5.06% 67 45\nRepilot InCoder-6.7B Defects4J 2.0 1.69s 48.0% 6.87% 68 46\nRepilot can lead to the discovery of more plausible and correct fixes,\nit is important to note that a significantly higher compilation rate\ndoes not necessarily translate to a proportionally large increase\nin plausible and correct fixes. On the other hand, Repilot is only\nevaluated with CodeT5 for RQ1 and RQ2 with a 5000 sampling\nbudget. CodeT5 is a rather ‚Äúsmall‚Äù LLM compared to those LLMs\nwith billions of parameters. Although we further include InCoder-\n6.7B as a multi-billion-parameter LLM in RQ4, due to time cost, we\nonly sample 500 times per bug, which may be insufficient to reflect\nthe distribution of the generated patches. Overall, the scope of our\nevaluation considering two LLMs (CodeT5 and InCoder) and one\nprogramming language (Java) is still narrow given that Repilot is\na general framework that can be instantiated with any pair of an\nLLM and a Completion Engine for some programming language.\nFinally, despite the examples we show in the paper, our eval-\nuation lacks strong empirical evidence to support the claim that\nLLMs have difficulty in generating rare tokens and how Repilot\nsolves the problem. Besides, our evaluation limits the application\nof Repilot to patch synthesis, even though we claim that Repilot\ncan be applied to other code generation tasks. In the future, we will\napply and evaluate Repilot on more diverse code generation tasks.\n8 THREATS TO VALIDITY\nInternal. We share the same main internal threat to validity with\nprior APR tools where we have to manually examine each plausible\npatch to determine patch correctness. We address this by carefully\nanalyzing each patch to determine if it is semantically equivalent\nto the reference developer patch. Furthermore, we have released\nour full set of correct patches for public evaluation [62].\nOur use of the CodeT5 model poses another internal threat where\nthe open-source training dataset of GitHub projects [27] may over-\nlap with our evaluation of Defects4J. We follow prior work [66, 67]\nand address this by computing the correct bug fixes of Repilot from\nDefects4J that is part of the CodeT5 training data. In total, 7 out\nof 66 and 6 out of 50 overlap with training data on Defects4J 1.2\nand 2.0 respectively. For comparison fairness, if we were to exclude\nthese 7 and 6 bugs and compare them with the previous baseline\ntools on the remaining bugs, we are still able to achieve the highest\nbug fixes at 59 and 44 (best baseline at 45 and 29). The same threat\napplies to the use of InCoder, but since its detailed training data is\nnot revealed, we are unable to explicitly address this problem. To\nmitigate the problem, we only evaluate InCoder in RQ4, where all\nthe variants face the same potential leakage.\nMoreover, our modified implementation of the completion en-\ngine requires manual inspection to guarantee soundness property.\nIn practice, this is a significant trust base that may introduce false\npositives during pruning. However, our theorem still provides a\npartial guarantee and is able to explain unsoundness. At the same\ntime, our evaluation result justifies our claims and demonstrates\nthe practicality of Repilot.\nFinally, in our evaluation, we follow the convention used in prior\nwork to directly report the bug fix results without reproducing them,\nwhich poses a threat to the reliability of the results. Meanwhile,\nwe only run each of our experiments once, which could introduce\nextra statistical biases.\nExternal. The main external threat to validity comes from our\nevaluation dataset where the performance of Repilot may not gen-\neralize to other datasets. To address this, we compare Repilot against\nstate-of-the-art baselines on both Defects4J 1.2 and 2.0 to show that\nthe performance is sustained across both versions. To address this\nfurther, we plan to evaluate Repilot on additional APR datasets also\nacross different programming languages.\n9 CONCLUSION\nWe propose Repilot ‚Äî the first APR approach to combining the\ndirect usage of LLMs (e.g., CodeT5 and InCoder) with on-the-fly\nguidance provided by Completion Engines. During autoregressive\ntoken generation, Repilot queries the Completion Engine not only\nto prune invalid tokens but also toproactively complete the currently\ngenerated partial program, thereby reducing the search space of the\nLLM. Our evaluation on a subset of the widely-studied Defects4J 1.2\nand 2.0 datasets shows Repilot is able to achieve the state-of-the-art\nresults. Furthermore, Repilot, through the usage of Completion\nEngine, is able to generate more valid and compilable patches than\nprior tools with minimal overhead compared with directly using\nLLMs for APR.\nDATA AVAILABILITY\nWe have open-sourced Repilot, which can be accessed on GitHub\nat https://github.com/ise-uiuc/Repilot. Additionally, an immutable\nartifact for Repilot is publicly available on Zenodo [62].\nACKNOWLEDGMENTS\nWe thank all the reviewers for their insightful comments. We also\nthank Yifeng Ding for his helpful discussion on this work. This\nwork was partially supported by NSF grants CCF-2131943 and\nCCF-2141474, as well as Kwai Inc.\nESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang\nREFERENCES\n[1] 2023. Eclipse JDT LS. https://projects.eclipse.org/projects/eclipse.jdt.ls.\n[2] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu,\nNaman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and\nLuke Zettlemoyer. 2022. CM3: A Causal Masked Multimodal Model of the Internet.\nCoRR abs/2201.07520 (2022). arXiv:2201.07520 https://arxiv.org/abs/2201.07520\n[3] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Unified Pre-training for Program Understanding and Generation.\narXiv:2103.06333 [cs.CL]\n[4] Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le,\nand Charles Sutton. 2021. Program Synthesis with Large Language Models.CoRR\nabs/2108.07732 (2021). arXiv:2108.07732 https://arxiv.org/abs/2108.07732\n[5] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. Proc. ACM\nProgram. Lang. 7, OOPSLA1, Article 78 (apr 2023), 27 pages. https://doi.org/10.\n1145/3586030\n[6] Earl T. Barr, Yuriy Brun, Premkumar Devanbu, Mark Harman, and Federica Sarro.\n2014. The Plastic Surgery Hypothesis. In Proceedings of the 22nd ACM SIGSOFT\nInternational Symposium on Foundations of Software Engineering (Hong Kong,\nChina) (FSE 2014). Association for Computing Machinery, New York, NY, USA,\n306‚Äì317. https://doi.org/10.1145/2635868.2635898\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nCoRR abs/2005.14165 (2020). arXiv:2005.14165 https://arxiv.org/abs/2005.14165\n[8] Jialun Cao, Meiziniu Li, Ming Wen, and Shing-Chi Cheung. 2023. A study on\nPrompt Design, Advantages and Limitations of ChatGPT for Deep Learning\nProgram Repair. CoRR abs/2304.08191 (2023). https://doi.org/10.48550/ARXIV.\n2304.08191 arXiv:2304.08191\n[9] Liushan Chen, Yu Pei, and Carlo A. Furia. 2017. Contract-based program repair\nwithout the contracts. In 2017 32nd IEEE/ACM International Conference on Auto-\nmated Software Engineering (ASE) . 637‚Äì647. https://doi.org/10.1109/ASE.2017.\n8115674\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\nOliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan-\ntanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei,\nSam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large\nLanguage Models Trained on Code. arXiv:2107.03374 [cs.LG]\n[11] Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-No√´l Pouchet, Denys\nPoshyvanyk, and Martin Monperrus. 2021. SequenceR: Sequence-to-Sequence\nLearning for End-to-End Program Repair. IEEE Transactions on Software Engi-\nneering 47, 9 (2021), 1943‚Äì1959. https://doi.org/10.1109/TSE.2019.2940179\n[12] Clojure 2023. Typed Clojure: An Optional Type System for Clojure. https:\n//typedclojure.org.\n[13] Favio DeMarco, Jifeng Xuan, Daniel Le Berre, and Martin Monperrus. 2014. Auto-\nmatic Repair of Buggy If Conditions and Missing Preconditions with SMT. InPro-\nceedings of the 6th International Workshop on Constraints in Software Testing, Verifi-\ncation, and Analysis (Hyderabad, India) (CSTVA 2014). Association for Computing\nMachinery, New York, NY, USA, 30‚Äì39. https://doi.org/10.1145/2593735.2593740\n[14] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming\nZhang. 2023. Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-\nLearning Libraries via Large Language Models. In Proceedings of the 32nd ACM\nSIGSOFT International Symposium on Software Testing and Analysis (Seattle, WA,\nUSA) (ISSTA 2023). Association for Computing Machinery, New York, NY, USA,\n423‚Äì435. https://doi.org/10.1145/3597926.3598067\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Association for Computational Linguistics, Minneapolis, Minnesota,\n4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423\n[16] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan,\nNihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia,\nDan Roth, and Bing Xiang. 2023. CrossCodeEval: A Diverse and Multilingual\nBenchmark for Cross-File Code Completion. arXiv:2310.11248 [cs.LG]\n[17] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A Pre-Trained Model for Programming and Natural Languages. CoRR\nabs/2002.08155. arXiv:2002.08155 https://arxiv.org/abs/2002.08155\n[18] Eclipse Foundation and Yuxiang Wei. 2023. UniverseFly/eclipse.jdt.ls: Modified\nEclipse JDT LS 1.0.3 . https://doi.org/10.5281/zenodo.8278193\n[19] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A\nGenerative Model for Code Infilling and Synthesis. In The Eleventh International\nConference on Learning Representations . https://openreview.net/forum?id=hQwb-\nlbM6EL\n[20] Luca Gazzola, Daniela Micucci, and Leonardo Mariani. 2019. Automatic Software\nRepair: A Survey. IEEE Transactions on Software Engineering 45, 1 (2019), 34‚Äì67.\nhttps://doi.org/10.1109/TSE.2017.2755013\n[21] Ali Ghanbari, Samuel Benton, and Lingming Zhang. 2019. Practical Program\nRepair via Bytecode Mutation. In Proceedings of the 28th ACM SIGSOFT In-\nternational Symposium on Software Testing and Analysis (Beijing, China) (IS-\nSTA 2019). Association for Computing Machinery, New York, NY, USA, 19‚Äì30.\nhttps://doi.org/10.1145/3293882.3330559\n[22] GithubCopilot 2023. GitHub Copilot: Your AI pair programmer. https://github.\ncom/features/copilot.\n[23] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie LIU, Long\nZhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang,\nand Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations\nwith Data Flow. In International Conference on Learning Representations . https:\n//openreview.net/forum?id=jLoC4ez43PZ\n[24] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\nCurious Case of Neural Text Degeneration. InInternational Conference on Learning\nRepresentations. https://openreview.net/forum?id=rygGQyrFvH\n[25] Jinru Hua, Mengshi Zhang, Kaiyuan Wang, and Sarfraz Khurshid. 2018. SketchFix:\nA Tool for Automated Program Repair Approach Using Lazy Candidate Gener-\nation. In Proceedings of the 2018 26th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foundations of Software Engineering\n(Lake Buena Vista, FL, USA) (ESEC/FSE 2018) . Association for Computing Ma-\nchinery, New York, NY, USA, 888‚Äì891. https://doi.org/10.1145/3236024.3264600\n[26] HuggingFace 2023. Hugging Face. https://huggingface.co.\n[27] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic\nCode Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http://arxiv.org/abs/\n1909.09436\n[28] Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. 2018.\nShaping Program Repair Space with Existing Patches and Similar Code. In ISSTA\n2018 (Amsterdam, Netherlands). Association for Computing Machinery, New\nYork, NY, USA, 298‚Äì309. https://doi.org/10.1145/3213846.3213871\n[29] Nan Jiang, Thibaud Lutellier, and Lin Tan. 2021. CURE: Code-Aware Neural\nMachine Translation for Automatic Program Repair. InProceedings of the 43rd\nInternational Conference on Software Engineering (Madrid, Spain) (ICSE ‚Äô21) . IEEE\nPress, 1161‚Äì1173. https://doi.org/10.1109/ICSE43902.2021.00107\n[30] Yanjie Jiang, Hui Liu, Nan Niu, Lu Zhang, and Yamin Hu. 2021. Extracting Concise\nBug-Fixing Patches from Human-Written Patches in Version Control Systems. In\nProceedings of the 43rd International Conference on Software Engineering (Madrid,\nSpain) (ICSE ‚Äô21) . IEEE Press, 686‚Äì698. https://doi.org/10.1109/ICSE43902.2021.\n00069\n[31] Harshit Joshi, Jos√© Cambronero, Sumit Gulwani, Vu Le, Ivan Radicek, and Gust\nVerbruggen. 2023. Repair Is Nearly Generation: Multilingual Program Repair with\nLLMs. AAAI. https://www.microsoft.com/en-us/research/publication/repair-is-\nnearly-generation-multilingual-program-repair-with-llms/\n[32] Ren√© Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Database\nof Existing Faults to Enable Controlled Testing Studies for Java Programs. In\nProceedings of the 2014 International Symposium on Software Testing and Analysis\n(San Jose, CA, USA) (ISSTA 2014). Association for Computing Machinery, New\nYork, NY, USA, 437‚Äì440. https://doi.org/10.1145/2610384.2628055\n[33] Sophia D Kolak, Ruben Martins, Claire Le Goues, and Vincent Josua Hellendoorn.\n2022. Patch Generation with Language Models: Feasibility and Scaling Behavior.\nIn Deep Learning for Code Workshop . https://openreview.net/forum?id=rHlzJh_\nb1-5\n[34] Anil Koyuncu, Kui Liu, Tegawend√© F. Bissyand√©, Dongsun Kim, Jacques Klein,\nMartin Monperrus, and Yves Le Traon. 2020. FixMiner: Mining Relevant Fix\nPatterns for Automated Program Repair. Empirical Softw. Engg. 25, 3 (may 2020),\n1980‚Äì2024. https://doi.org/10.1007/s10664-019-09780-z\n[35] Xuan-Bach D. Le, Duc-Hiep Chu, David Lo, Claire Le Goues, and Willem Visser.\n2017. S3: Syntax- and Semantic-Guided Repair Synthesis via Programming\nby Examples. In Proceedings of the 2017 11th Joint Meeting on Foundations of\nSoftware Engineering (Paderborn, Germany) (ESEC/FSE 2017) . Association for\nComputing Machinery, New York, NY, USA, 593‚Äì604. https://doi.org/10.1145/\n3106237.3106309\nCopiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair ESEC/FSE ‚Äô23, December 3‚Äì9, 2023, San Francisco, CA, USA\n[36] Xuan Bach D. Le, David Lo, and Claire Le Goues. 2016. History Driven Program\nRepair. In SANER (2016) , Vol. 1. 213‚Äì224. https://doi.org/10.1109/SANER.2016.76\n[37] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. 2012.\nGenProg: A Generic Method for Automatic Software Repair. IEEE Transactions\non Software Engineering 38, 1 (2012), 54‚Äì72. https://doi.org/10.1109/TSE.2011.104\n[38] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R√©mi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas\nHubert, Peter Choy, Cyprien de Masson d‚ÄôAutume, Igor Babuschkin, Xinyun Chen,\nPo-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,\nDaniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,\nKoray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation\nwith AlphaCode. Science 378, 6624 (2022), 1092‚Äì1097. https://doi.org/10.1126/\nscience.abq1158 arXiv:https://www.science.org/doi/pdf/10.1126/science.abq1158\n[39] Yi Li, Shaohua Wang, and Tien N. Nguyen. 2020. DLFix: Context-Based Code\nTransformation Learning for Automated Program Repair. InProceedings of the\nACM/IEEE 42nd International Conference on Software Engineering (Seoul, South\nKorea) (ICSE ‚Äô20). Association for Computing Machinery, New York, NY, USA,\n602‚Äì614. https://doi.org/10.1145/3377811.3380345\n[40] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is\nYour Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large\nLanguage Models for Code Generation. arXiv:2305.01210 [cs.SE]\n[41] Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend√© F. Bissyand√©. 2019. TBar:\nRevisiting Template-Based Automated Program Repair. InProceedings of the 28th\nACM SIGSOFT International Symposium on Software Testing and Analysis (Beijing,\nChina) (ISSTA 2019). Association for Computing Machinery, New York, NY, USA,\n31‚Äì42. https://doi.org/10.1145/3293882.3330577\n[42] Kui Liu, Jingtang Zhang, Li Li, Anil Koyuncu, Dongsun Kim, Chunpeng Ge, Zhe\nLiu, Jacques Klein, and Tegawend√© F. Bissyand√©. 2023. Reliable Fix Patterns\nInferred from Static Checkers for Automated Program Repair. ACM Trans. Softw.\nEng. Methodol. 32, 4, Article 96 (may 2023), 38 pages. https://doi.org/10.1145/\n3579637\n[43] Fan Long and Martin Rinard. 2015. Staged Program Repair with Condition Syn-\nthesis. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software\nEngineering (Bergamo, Italy) (ESEC/FSE 2015) . Association for Computing Ma-\nchinery, New York, NY, USA, 166‚Äì178. https://doi.org/10.1145/2786805.2786811\n[44] Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and\nLin Tan. 2020. CoCoNuT: Combining Context-Aware Neural Translation Models\nUsing Ensemble for Program Repair. In Proceedings of the 29th ACM SIGSOFT\nInternational Symposium on Software Testing and Analysis (Virtual Event, USA)\n(ISSTA 2020). Association for Computing Machinery, New York, NY, USA, 101‚Äì114.\nhttps://doi.org/10.1145/3395363.3397369\n[45] Matias Martinez, Thomas Durieux, Romain Sommerard, Jifeng Xuan, and Martin\nMonperrus. 2017. Automatic Repair of Real Bugs in Java: A Large-Scale Experi-\nment on the Defects4j Dataset. Empirical Softw. Engg. 22, 4 (aug 2017), 1936‚Äì1964.\nhttps://doi.org/10.1007/s10664-016-9470-4\n[46] Matias Martinez and Martin Monperrus. 2016. ASTOR: A Program Repair\nLibrary for Java (Demo). In Proceedings of the 25th International Symposium\non Software Testing and Analysis (Saarbr√ºcken, Germany) (ISSTA 2016) . As-\nsociation for Computing Machinery, New York, NY, USA, 441‚Äì444. https:\n//doi.org/10.1145/2931037.2948705\n[47] Sergey Mechtaev, Jooyong Yi, and Abhik Roychoudhury. 2016. Angelix: Scalable\nMultiline Program Patch Synthesis via Symbolic Analysis. In Proceedings of the\n38th International Conference on Software Engineering (Austin, Texas)(ICSE ‚Äô16) .\nAssociation for Computing Machinery, New York, NY, USA, 691‚Äì701. https:\n//doi.org/10.1145/2884781.2884807\n[48] Microsoft 2023. Language Server Protocol. https://microsoft.github.io/language-\nserver-protocol.\n[49] Microsoft 2023. TypeScript. https://www.typescriptlang.org.\n[50] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo\nZhou. 2023. CodeGen2: Lessons for Training LLMs on Programming and Natural\nLanguages. arXiv:2305.02309 [cs.LG]\n[51] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language\nModel for Code with Multi-Turn Program Synthesis. arXiv:2203.13474.\n[52] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher\nMeek, and Sumit Gulwani. 2022. Synchromesh: Reliable Code Generation from\nPre-trained Language Models. In International Conference on Learning Represen-\ntations. https://openreview.net/forum?id=KmtVD97J43e\n[53] Julian Aron Prenner, Hlib Babii, and Romain Robbes. 2022. Can OpenAI‚Äôs Codex\nFix Bugs? An Evaluation on QuixBugs. In APR ‚Äô22 (Pittsburgh, Pennsylvania).\nAssociation for Computing Machinery, New York, NY, USA, 69‚Äì75. https://doi.\norg/10.1145/3524459.3527351\n[54] Python 2023. Type Hints in Python. https://peps.python.org/pep-0484/.\n[55] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. InProceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, Berlin, Germany, 1715‚Äì1725. https:\n//doi.org/10.18653/v1/P16-1162\n[56] Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023.\nAn Analysis of the Automatic Bug Fixing Performance of ChatGPT.\narXiv:2301.08653 [cs.SE]\n[57] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence\nLearning with Neural Networks. In Advances in Neural Information Processing\nSystems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger\n(Eds.), Vol. 27. Curran Associates, Inc. https://proceedings.neurips.cc/paper_\nfiles/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf\n[58] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin\nWhite, and Denys Poshyvanyk. 2019. An Empirical Study on Learning Bug-Fixing\nPatches in the Wild via Neural Machine Translation. ACM Trans. Softw. Eng.\nMethodol. 28, 4, Article 19 (sep 2019), 29 pages. https://doi.org/10.1145/3340544\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You\nNeed. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems (Long Beach, California, USA) (NIPS‚Äô17). Curran Associates\nInc., Red Hook, NY, USA, 6000‚Äì6010.\n[60] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and\nSteven C. H. Hoi. 2023. CodeT5+: Open Code Large Language Models for Code\nUnderstanding and Generation. arXiv:2305.07922 [cs.CL]\n[61] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. 2021. CodeT5:\nIdentifier-aware Unified Pre-trained Encoder-Decoder Models for Code Un-\nderstanding and Generation. In Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing . Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 8696‚Äì8708. https:\n//doi.org/10.18653/v1/2021.emnlp-main.685\n[62] Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. 2023. ESEC/FSE‚Äô23 Arti-\nfact for \"Copiloting the Copilots: Fusing Large Language Models with Completion\nEngines for Automated Program Repair\". https://doi.org/10.5281/zenodo.8281250\n[63] Ming Wen, Junjie Chen, Rongxin Wu, Dan Hao, and Shing-Chi Cheung. 2018.\nContext-Aware Patch Generation for Better Automated Program Repair. InPro-\nceedings of the 40th International Conference on Software Engineering (Gothenburg,\nSweden) (ICSE ‚Äô18) . Association for Computing Machinery, New York, NY, USA,\n1‚Äì11. https://doi.org/10.1145/3180155.3180233\n[64] Chunqiu Steven Xia, Yifeng Ding, and Lingming Zhang. 2023. Revisiting the\nPlastic Surgery Hypothesis via Large Language Models. arXiv:2303.10494 [cs.SE]\n[65] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and\nLingming Zhang. 2023. Universal Fuzzing via Large Language Models.\narXiv:2308.04748 [cs.SE]\n[66] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated\nProgram Repair in the Era of Large Pre-Trained Language Models. InProceedings\nof the 45th International Conference on Software Engineering (Melbourne, Victoria,\nAustralia) (ICSE ‚Äô23) . IEEE Press, 1482‚Äì1494. https://doi.org/10.1109/ICSE48619.\n2023.00129\n[67] Chunqiu Steven Xia and Lingming Zhang. 2022. Less Training, More Repairing\nPlease: Revisiting Automated Program Repair via Zero-Shot Learning. In Pro-\nceedings of the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering (Singapore, Singapore)\n(ESEC/FSE 2022) . Association for Computing Machinery, New York, NY, USA,\n959‚Äì971. https://doi.org/10.1145/3540250.3549101\n[68] Chunqiu Steven Xia and Lingming Zhang. 2023. Conversational Automated\nProgram Repair. CoRR abs/2301.13246 (2023). https://doi.org/10.48550/ARXIV.\n2301.13246 arXiv:2301.13246\n[69] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Going:\nFixing 162 out of 337 bugs for $0.42 each using ChatGPT. arXiv:2304.00385 [cs.SE]\n[70] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022.\nA Systematic Evaluation of Large Language Models of Code. In Proceedings of\nthe 6th ACM SIGPLAN International Symposium on Machine Programming (San\nDiego, CA, USA) (MAPS 2022) . Association for Computing Machinery, New York,\nNY, USA, 1‚Äì10. https://doi.org/10.1145/3520312.3534862\n[71] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. Curran Associates Inc., Red Hook, NY, USA.\n[72] He Ye, Matias Martinez, and Martin Monperrus. 2022. Neural Program Re-\npair with Execution-Based Backpropagation. In Proceedings of the 44th Inter-\nnational Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE\n‚Äô22). Association for Computing Machinery, New York, NY, USA, 1506‚Äì1518.\nhttps://doi.org/10.1145/3510003.3510222\n[73] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao,\nJian-Guang Lou, and Weizhu Chen. 2023. RepoCoder: Repository-Level Code\nCompletion Through Iterative Retrieval and Generation. arXiv:2303.12570 [cs.CL]\n[74] Qihao Zhu, Zeyu Sun, Yuan-an Xiao, Wenjie Zhang, Kang Yuan, Yingfei Xiong,\nand Lu Zhang. 2021. A Syntax-Guided Edit Decoder for Neural Program Repair.\nIn ESEC/FSE 2021 (Athens, Greece). Association for Computing Machinery, New\nYork, NY, USA, 341‚Äì353. https://doi.org/10.1145/3468264.3468544\nReceived 2023-02-02; accepted 2023-07-27"
}