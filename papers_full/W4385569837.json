{
  "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training",
  "url": "https://openalex.org/W4385569837",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101445476",
      "name": "Yan Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952165275",
      "name": "Wangchunshu Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229008646",
      "name": "Ao Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2575499086",
      "name": "Ziming Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141857187",
      "name": "Xinsong Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3200050180",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W4282028325",
    "https://openalex.org/W3152788712",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W2963909453",
    "https://openalex.org/W3168896398",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3202415077",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2963527096",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3214685499",
    "https://openalex.org/W2912371042",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W2963778889",
    "https://openalex.org/W4287245501",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W630532510",
    "https://openalex.org/W4309876087",
    "https://openalex.org/W4221151248",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3176740717",
    "https://openalex.org/W3199725751",
    "https://openalex.org/W4322615157",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2572474373",
    "https://openalex.org/W3033518368"
  ],
  "abstract": "In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval datasets show that while conceptually simpler, CCLM significantly outperforms the prior state-of-the-art with an average absolute improvement of over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained model that surpasses the translate-test performance of representative English vision-language models by zero-shot cross-lingual transfer.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5731–5746\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCross-View Language Modeling: Towards Unified Cross-Lingual\nCross-Modal Pre-training\nYan Zeng∗†\nByteDance\nWangchunshu Zhou∗\nETH Zurich\nAo Luo∗\nWaseda University\nZiming Cheng∗\nShanghai Jiao Tong University\nXinsong Zhang\nByteDance\nAbstract\nIn this paper, we introduce Cross-View Lan-\nguage Modeling, a simple and effective pre-\ntraining framework that unifies cross-lingual\nand cross-modal pre-training with shared archi-\ntectures and objectives. Our approach is mo-\ntivated by a key observation that cross-lingual\nand cross-modal pre-training share the same\ngoal of aligning two different views of the\nsame object into a common semantic space.\nTo this end, the cross-view language model-\ning framework considers both multi-modal data\n(i.e., image-caption pairs) and multi-lingual\ndata (i.e., parallel sentence pairs) as two dif-\nferent views of the same object, and trains the\nmodel to align the two views by maximizing\nthe mutual information between them with con-\nditional masked language modeling and con-\ntrastive learning. We pre-trainCCLM, a Cross-\nlingual Cross-modal Language Model, with\nthe cross-view language modeling framework.\nEmpirical results on IGLUE, a multi-lingual\nmulti-modal benchmark, and two multi-lingual\nimage-text retrieval datasets show that while\nconceptually simpler, CCLM significantly out-\nperforms the prior state-of-the-art with an aver-\nage absolute improvement of over 10%. More-\nover, CCLM is the first multi-lingual multi-\nmodal pre-trained model that surpasses the\ntranslate-test performance of representative\nEnglish vision-language models by zero-shot\ncross-lingual transfer.1\n1 Introduction\nRecently, the tremendous success of self-\nsupervised language model pre-training (Peters\net al., 2018; Radford et al., 2018; Devlin et al.,\n2019; Liu et al., 2019; Radford et al., 2019; Dong\net al., 2019; Raffel et al., 2019; Lewis et al., 2020;\nBrown et al., 2020) has been expanded to the\n∗Equal Contribution. Work done at ByteDance.\n†Correspondence to: zengyan.yanne@bytedance.com\n1The code and pre-trained models are available at https:\n//github.com/zengyan-97/CCLM.\nmulti-lingual (Conneau and Lample, 2019; Con-\nneau et al., 2020; Pfeiffer et al., 2020; Chi et al.,\n2021) and multi-modal (Lu et al., 2019; Tan and\nBansal, 2019; Su et al., 2020; Chen et al., 2020; Li\net al., 2020) domain. Advances on multi-lingual\npre-training enables cutting-edge language tech-\nnology to benefit a much boarder group of users\nincluding non-English speakers. Similarly, multi-\nmodal pre-training makes pre-trained models appli-\ncable to a much larger set of tasks and user groups.\nBoth of these directions make people’s lives in\na multi-lingual multi-modal world easier. There-\nfore, a natural next step is to explore multi-lingual\nmulti-modal pre-training which enables pre-trained\nmodels to solve multi-modal tasks expressed in\nnon-English languages without the need of collect-\ning training data in these languages, which can be\nvery costly for certain low-resource languages.\nWhile appealing, multi-lingual multi-modal pre-\ntraining has its own challenges. Unlike multi-\nlingual pre-training and multi-modal pre-training\nwhere relatively large amount of parallel data is\navailable, there exists only a few multi-lingual\nmulti-modal corpora and their language coverage\nis also limited. Two pioneering works, M 3P (Ni\net al., 2021) and UC2 (Zhou et al., 2021), propose\nto pivot either on English texts or images to align\nmulti-lingual multi-modal representations. Both\nof them introduce a number of new objectives to\nmake use of the anchor for alignment. However,\na recent benchmark on multi-lingual multi-modal\npre-training (Bugliarello et al., 2022) reveals that\nthese multi-lingual multi-modal pre-trained mod-\nels are still falling short: while achieving seem-\ningly promising zero-shot cross-lingual transfer\nperformance on some vision-and-language tasks,\nthey still significantly under-perform “translate-\ntest”, a simple baseline which translates the test\nexamples into English and uses an English-only\nvision-language model for inference. This prevents\nexisting multi-lingual multi-modal models to be\n5731\napplicable in real-world applications. In contrast,\nmulti-lingual pre-trained models such as XLM-\nR (Conneau et al., 2020) significantly outperforms\nthe translate-test baseline in most languages and is\nwidely used in practical applications.\nThis paper aims to fully exploit the potential of\nmulti-lingual multi-modal pre-training. We point\nout two major limitation of current state-of-the-\narts. First, existing methods do not exploit parallel\ntext corpora, which can be easily collected and are\nabundant for many language pairs. Instead, M 3P\nperforms masked language modeling with monolin-\ngual texts in different languages for multi-lingual\nalignment. However, parallel texts are shown to\nbe more helpful according to multi-lingual pre-\ntraining literature (Conneau et al., 2020; Chi et al.,\n2021). Second, a number of new pre-training ob-\njectives involving specific architecture changes and\ndifferent input-output formats are introduced for\nEnglish or image pivoting, making it non-trivial to\ncombine them together for better performance and\nscale to larger data.\nIn this work, we argue that multi-lingual and\nmulti-modal pre-training are essentially achiev-\ning the same goal of aligning two different views\nof a same object into a common semantic space.\nTherefore, we believe these two seemingly dif-\nferent strategies can be combined into a unified\nframework. To this end, we introduce cross-view\nlanguage modeling, a simple and effective frame-\nwork that unifies cross-lingual and cross-modal pre-\ntraining with shared architecture and objectives.\nSpecifically, we consider both multi-modal data\n(i.e., image-caption pairs) and multi-lingual data\n(i.e., parallel sentence pairs) as pairs of two differ-\nent views of the same object. With either multi-\nmodal or multi-lingual data as input, we encode\nthe two views with Transformer models and then\nfuse their representations with a cross-attention\nTransformer model shared for both cross-modal\nand cross-lingual fusion. We train the model to\nalign the two views into a common semantic space\nby maximizing the mutual information between\nthem with a conditional masked language model-\ning objective, a contrastive learning objective, and\na matching objective. In this way, the cross-view\nlanguage modeling framework unifies English piv-\noting and image pivoting schemes seamlessly and\nmakes the best of both worlds.\nTo evaluate the effectiveness of our approach,\nwe pre-train CCLM, a Cross-lingual Cross-modal\nLanguage Model, with the proposed cross-view lan-\nguage modeling framework. Experimental results\nshow that CCLM significantly outperforms prior\nstate-of-the-art with an averaged absolute improve-\nment of over 10% and 30% on multi-lingual vision-\nlanguage understanding and retrieval tasks in terms\nof accuracy and R@1 on IGLUE (Bugliarello\net al., 2022), a recently released multi-lingual multi-\nmodal benchmark. Notably, CCLM is the first\nmulti-lingual vision-language model that surpasses\nthe “translate-test” performance of mono-lingual\nvision-language models via zero-shot cross-lingual\ntransfer, which we believe is a crucial step towards\npractical multi-lingual multi-modal pre-training.\nSince previous work used different pre-training\ndatasets, making direct comparison difficult, we\nalso conduct an in-depth ablation study to investi-\ngate the contribution of different parts in our frame-\nwork. The results show that use of parallel sentence\npairs helps to fully exploit the potential of language\npivoting for multi-lingual multi-modal pre-training\nand also confirm the importance of unified archi-\ntectures and objectives in CCLM.\nContributions. (1) We propose a cross-view\nlanguage modeling framework that unifies multi-\nlingual and multi-modal pre-training with shared\narchitectures and objectives. (2) CCLM advances\nthe state-of-the-art of multi-lingual vision-language\npre-training by a large margin. It also surpasses\nthe translate-test baseline for the first time, demon-\nstrating the potential of multi-lingual multi-modal\npre-training. (3) We further scale up CCLM with\nmassive pre-training data and larger model size.\nWe will release our large-scale pre-trained multi-\nlingual multi-modal models to benefit a larger set\nof tasks and user groups and setup a strong and\neasily reproducible baseline for multi-lingual multi-\nmodal research.\n2 Related Work\nMulti-lingual Pre-training Multilingual\nBERT (Devlin et al., 2019) demonstrates that\ngood cross-lingual transfer results can be achieved\nby performing masked language modeling on\nmulti-lingual corpora with shared vocabulary\nand weight. Later, XLM (Conneau and Lample,\n2019), XLM-R (Conneau et al., 2020), and\nUnicoder (Huang et al., 2019) introduce a number\nof new objectives including translation language\nmodeling (TLM), cross-lingual word recovery, and\ncross-lingual paraphrase classification to improve\n5732\nmulti-lingual pre-training. More recently, MAD-\nX (Pfeiffer et al., 2020) and InfoXLM (Chi et al.,\n2021) further improve multi-lingual pre-training\nvia adapter (Houlsby et al., 2019) and contrastive\nlearning.\nVision-Language Pre-training Inspired by the\nsuccess of language model pre-training, a number\nof work (Lu et al., 2019; Tan and Bansal, 2019; Li\net al., 2020; Chen et al., 2020; Zeng et al., 2021;\nWang et al., 2022; Yu et al., 2022) investigates\nvision-language pre-training on large scale image-\ncaption pairs and proposes a number of objec-\ntives to align vision and language representations,\nincluding masked multi-modal modeling, multi-\nmodal alignment prediction, RoI feature regres-\nsion, image-text matching, to name a few. Vision-\nlanguage pre-training has reshaped the landscape\nof vision-and-language research and pushed the\nstate-of-the-arts on a wide range of vision-language\ntasks (Zhou et al., 2022). However, it is non-trivial\nto collect large scale image-caption pairs in other\nlanguages. As such, most existing vision-language\npre-trained models are limited to English tasks.\nMulti-lingual Multi-modal Pre-training Multi-\nlingual multi-modal pre-training aims to make\nmulti-modal models applicable on non-English\ntexts by cross-lingual transfer. In this paper\nwe mainly consider multi-modal in the vision-\nlanguage context. The key difficulty of multi-\nlingual multi-modal pre-training is the lack of\nnon-English image-text pairs. Two representative\nworks tackle the lack of non-English image-text\npairs by pivoting on either English texts or im-\nages. Specifically, M 3P (Ni et al., 2021) uses\nEnglish as pivot and alternates between English-\nonly vision-language pre-training and multi-lingual\nmasked language modeling. UC 2 (Zhou et al.,\n2021), on the other hand, translates English cap-\ntions into multiple languages and considers images\nas the anchor, achieving state-of-the-art on various\nmulti-lingual vision-language tasks. More recently,\nMURAL (Jain et al., 2021) collects large-scale\nimage-text pairs in 110 languages and pre-trains a\ndual encoder model via contrastive learning. MU-\nRAL achieves new state-of-the-art on multi-lingual\nimage-text retrieval tasks. However, the dual en-\ncoder architecture of MURAL makes it unable to\nperform multi-modal understanding tasks well.\n3 Cross-View Language Modeling\n3.1 Overview\nCross-view language modeling is a simple frame-\nwork that unifies cross-lingual pre-training and\ncross-modal pre-training with shared architecture\nand objectives. CCLM consists of an image en-\ncoder, a cross-lingual text encoder, and a fusion\nmodel. All components are Transformer-based.\nSpecifically, the image encoder (Dosovitskiy et al.,\n2021) first splits an image into non-overlapping\npatches, and then embeds these patches with trans-\nformer layers, yielding {⃗ vcls,⃗ v1,...,⃗ vN1}. For an\nimage of resolution of 224x224 and patch size of\n32x32, we have N1 = 49 . Similarly, the cross-\nlingual text encoder encodes a text input via trans-\nformer layers, yielding {⃗ wcls,⃗ w1,...,⃗ wN2}. N2 is\nthe length of the text input. Then, the fusion model\nfuses text features with the corresponding image\nfeatures or features of the translated text based on\ncross-attention, producing {⃗ xcls,⃗ x1,...,⃗ xN2}.\nAs illustrated in Figure 1, with either (text, im-\nage) pairs or (text, translation) pairs as input, we\nconsider the paired input as two different views\nand train the model to align their representations\nin a common semantic space. This unified cross-\nview perspective allows us to share input-output\nformats, architectures, and training objectives be-\ntween cross-lingual inputs and cross-modal inputs.\nSpecifically, we completely share the fusion model\nfor both cross-lingual fusion and cross-modal fu-\nsion, and optimize the model by contrastive loss,\nmatching loss, and conditional masked language\nmodeling loss for both cross-lingual and cross-\nmodal inputs. We select these objectives because\nthey are universally effective in both cross-lingual\nand cross-modal pre-training literature (Chi et al.,\n2021; Li et al., 2021). We will show that the three\nloss maximize sequence-level and token-level mu-\ntual information between image-caption pairs or\nparallel sentence pairs. On the other hand, we\nempirically find that the three loss are more effec-\ntive for cross-lingual cross-modal pre-training than\ncertain task-specific loss such as masked region-\nto-token language modeling which is specially for\nmulti-modal pre-training or translation language\nmodeling for multilingual pre-training.\n3.2 A Mutual Information Maximization\nPerspective\nIn this section, we explain our approach from an\ninformation-theoretic perspective. Formally, given\n5733\nFigure 1: Illustration of the cross-view language modeling framework. CCLM takes two different views of the same\nobject, i.e., either (a) image-caption pairs or (b) parallel sentence pairs, as input. CCLM first encodes the two views\nseparately. Then the representations of the two views are fused by a Transformer-based model, which is shared for\nboth cross-lingual and cross-modal fusion. CCLM is optimized by maximizing the mutual information between the\ntwo views via conditional masked language modeling loss, contrastive loss, and matching loss.\ntwo random variablesAand B, mutual information\nI(A,B) measures dependencies between the two\nrandom variables. We define A = aand B = b\nas two different views of a data point, which can\nbe either an image-caption pair or a parallel sen-\ntence pair. In this case, we will show that CCLM\nmaximizes a lower bound of I(A,B) for cross-\nlingual cross-modal pre-training by minimizing the\nInfoNCE loss (Oord et al., 2018) defined as:\nLnce = −Ep(A,B)\n[\nlog exp(fθ(a, b))∑\n˜b∈˜B exp(fθ(a,˜b))\n]\n, (1)\nwhere fθ ∈R is a function parameterized by θ\nand ˜Bcontains the positive sample band |˜B|− 1\nnegative samples.\nThe contrastive loss between the image encoder\nand the cross-lingual text encoder is a symmetric\nversion of Lnce:\nLcl = −1\n2Ep(A,B)\n[\nlog exp(fθ(a, b))∑\n˜b∈˜B exp(fθ(a,˜b))\n+ log exp(fθ(a, b))∑\n˜a∈˜A exp(fθ(˜a, b))\n]\n, (2)\nwhere |˜A|= |˜B|= N is the batch size, and\nwe predict (a,b) pairs from in-batch negatives.\nfθ(a,b) = gv(⃗ vcls)⊤gw(⃗ wcls)/τ given an image-\ncaption pair or fθ(a,b) = gw(⃗ wa\ncls)⊤gw(⃗ wb\ncls)/τ\ngiven a translation pair. ⃗ vcls and ⃗ wcls are the output\n[CLS] embedding of the image encoder 2 and the\ncross-lingual text encoder. gv and gw are transfor-\nmations that map the [CLS] embeddings to nor-\nmalized lower-dimensional representations. τ is a\nlearnable temperature parameter.\nSimilarly, the matching loss applied on the out-\nput [CLS] embedding of the fusion model (denoted\nas ⃗ xcls(a,b)) can also be viewed as a symmetric\nversion of Lnce:\nLmatch = −1\n2 Ep(A,B)\n[\nlog exp(fθ(a, b))\nexp(fθ(a, b)) + exp(fθ(a, bneg))\n+ log exp(fθ(a, b))\nexp(fθ(a, b) + expfθ(aneg, b))\n]\n,\n(3)\nwhere we only sample a negative instance for\neach ground-truth (a,b) pair and predict whether\na pair is matched (true or false). In this case,\nfθ(a,b) = ⃗ v⊤\ntrue⃗ xcls(a,b), where ⃗ vtrue is a para-\nmetric vector.\nThe conditional MLM loss can also be inter-\npreted as maximizing mutual information (Kong\net al., 2020) between the context c = (ˆa,b) (ˆa\ndenotes the masked text input, and bis the corre-\nsponding image or translated text) and the masked\ntoken wi in a:\nLmlm = −Ep(C,W)\n[\nlog exp(fθ(c, wi))∑\n˜w∈Vexp(fθ(c, ˜w))\n]\n, (4)\n2Some vision transformers, e.g. Swin-Transformer, use\nthe output of average pooling layer as the [CLS] embedding.\n5734\nwhere fθ(c,wi) = ψ(wi)⊤⃗ xi(ˆa,b). ⃗ xi is the\noutput vector at wi position of the fusion model.\nψ(w) : V→ Rd is a lookup function that maps a\nword token winto a parametric vector. Vis the full\nvocabulary set.\nFinally, the pre-training objective of CCLM is\ndefined as: L = Lcl + Lmatch + Lmlm, where\nthe contrastive loss and matching loss maximize\nsequence-level mutual information while the MLM\nloss maximizes token-level mutual information,\nwhich are complement of each other.\n4 Experiment\n4.1 Experimental Settings\n4.1.1 Pre-training Datasets\nWe pre-train CCLM on the combination of image-\ncaption pairs and parallel multilingual texts. Ap-\npendix A.1 describes compared models in details.\nMulti-modal Data For image-caption pairs, we\nfollow the practice of UC2 to make a fair compar-\nison and use their released translation-augmented\nversion of CC3M dataset. It contains the original\nCC3M image-caption pairs (Sharma et al., 2018)\nand machine-translated captions in five different\nlanguages (German, French, Czech, Japanese, and\nChinese). This multi-modal dataset is widely uti-\nlized by previous work, including UC2, mUNITER\nand xUNITER. We denote this variant as CCLM3M.\nIn additional to this setting, we leverage large-scale\nvision language pre-training by utilizing the pre-\ntrained weights of X 2-VLM (Zeng, 2021; Zeng\net al., 2022) which has been trained on more than\n1B image-text pairs in English. Based on it we\napply the proposed framework for multi-lingual\nmulti-modal pre-training.\nMulti-lingual Data Previous work such as mU-\nNITER, xUNITER, and M3P use large-scale mono-\nlingual texts in different languages, namely multi-\nlingual Wikipedia 101G dataset, for multilingual\nalignment. Differently, we propose to utilize par-\nallel text corpus. We collect a subset of the Wiki-\nMatrix (Schwenk et al., 2021) dataset containing\nparallel texts between English and other languages\nin the IGLUE benchmark. Appendix A.2 shows the\nnumber of pairs per language. In total, the dataset\nconsists of 19M parallel sentence pairs.\n4.1.2 Implementation Details\nCCLMbase consists of 12 Transformer layers for\nthe image encoder and the text encoder respectively.\nCCLMlarge consists of 24 layers for each encoder.\nThe fusion encoder contains 6 Transformer lay-\ners for both CCLMbase (d= 768) and CCLMlarge\n(d = 1024). In total, CCLM base and CCLMlarge\nconsist of ∼420M and ∼970M parameters re-\nspectively. Following existing models such as M3P\nand UC2, we also utilize XLM-R (Conneau et al.,\n2020) as the text encoder. Concretely, CCLM3M is\ninitialized with a pre-trained image encoder (Liu\net al., 2021b) and XLM-R. CCLM is initialized\nwith the pre-trained X2-VLM (Zeng, 2021; Zeng\net al., 2022) and XLM-R.\nIn pre-training, the image encoder takes im-\nages of resolution of 224 ×224 as input for pre-\ntraining. During fine-tuning, we increase the im-\nage resolution to 384 ×384 and interpolate the\npositional embeddings of image patches follow-\ning Dosovitskiy et al. (2021). The maximum se-\nquence length is set to 30 and 64 for image captions\nand parallel multilingual texts respectively. We ap-\nply mixed precision for pre-training. We use the\nAdamW (Loshchilov and Hutter, 2019) optimizer\nwith a weight decay of 0.02. We mix different\ntypes of data in a training batch. Following UC 2,\nto make a fair comparison, we train CCLM3M for\n30 epochs on 8 NVIDIA A100 GPUs and the batch\nsize is set to 1024, which tasks ∼1.5 days. The\nlearning rate is warmed-up to 1e−4 in the first 2500\nsteps and decayed linearly. We train CCLM base\nand CCLMlarge for 40 epochs.\n4.1.3 Downstream Tasks\nWe evaluate CCLM on the IGLUE bench-\nmark (Bugliarello et al., 2022), a recently released\nbenchmark for evaluating multi-lingual multi-\nmodal pre-training, and a multi-lingual image-text\nretrieval benchmark including the multi-lingual ver-\nsion of Flickr30K (Young et al., 2014; Elliott et al.,\n2016) and MSCOCO (Chen et al., 2015). Note that\nCCLM can also be applied on generation tasks such\nas image captioning by following the adaptation\nstrategy of X-VLM (Zeng et al., 2022; Zeng and\nNie, 2021).\nXVNLI: The Cross-lingual Visual NLI dataset\nis collected by combining SNLI (Bowman et al.,\n2015) with its multi-modal (Xie et al., 2019) and\nmulti-lingual (Agi ´c and Schluter, 2018) counter-\nparts. It requires the model to predict if a text-\nhypothesis “entails”, “contradicts”, or is “neutral”\nto an image-premise.\nxGQA: The Cross-lingual Grounded Question An-\nswering task (Pfeiffer et al., 2021) is collected by\nmanually translating the GQA (Hudson and Man-\n5735\nModel NLI QA Reasoning Retrieval\nXVNLI xGQA MaRVL xFlickr&CO WIT\nIR TR IR TR\nTranslate everything to English and use English-only model (Translate-Test)\nUNITER 73.65 50.62 61.92 41.04 37.49 15.43 16.01\nViLBERT 73.45 50.33 62.39 36.97 33.21 15.40 16.93\nVisualBERT 74.12 48.72 62.35 41.64 36.44 15.36 15.75\nVL-BERT 73.86 49.78 64.16 38.18 31.84 15.11 16.09\nFine-tune model on English training set (Zero-Shot)\nmUNITER 53.69 9.97 53.72 8.06 8.86 9.16 10.48\nxUNITER 58.48 21.72 54.59 14.04 13.51 8.72 9.81\nM3P 58.25 28.17 56.00 12.91 11.90 8.12 9.98\nUC2 62.05 29.35 57.28 20.31 17.89 7.83 9.09\nCCLM3M\nbase 74.64 42.36 65.91 67.35 65.37 27.46 28.66\nCCLMbase 74.78 48.12 68.49 76.94 76.22 33.90 35.26\nCCLMlarge 78.95 56.25 74.83 83.78 83.46 43.74 44.88\nTable 1: Results on IGLUE benchmark. R@1 and Accuracy are reported for retrieval tasks (xFlickr&CO and WIT)\nand understanding tasks (XVNLI, xGQA, MaRVL) respectively. In the zero-shot setting, the models are fine-tuned\non English train sets and directly evaluated on target languages. We report few-shot results in Appendix A.4.\nning, 2019) validation set into 7 languages. It re-\nquires a model to answer several types of structured\nquestions about an image. We model GQA as a\ngeneration task following Li et al. (2021).\nMaRVL: The Multicultural Reasoning over Vision\nand Language dataset (Liu et al., 2021a) requires to\ndetermine whether a textual description is true or\nfalse about a pair of images. The MaRVL dataset is\nused for testing and the NLVR2 (Suhr et al., 2019)\ndataset is used for training.\nxFlickr&CO and WIT: The xFlickr&CO dataset\nis collected by combining 1000 images from\nFlickr30K and MSCOCO respectively and crowd-\nsource image descriptions in 6 other lan-\nguages. Similarly, the Wikipedia-based Image Text\ndataset (Srinivasan et al., 2021) is collected from\nWikipedia in 108 languages. We follow the data\npreprocessing and splitting details in IGLUE for\nboth datasets.\nMulti30K: This dataset (Elliott et al., 2016) ex-\ntended Flickr30K (Young et al., 2014) from English\n(en) to German (de), French (fr) and Czech (cs). It\ncontains 31,783 images and provides five captions\nper image in English and German, and one caption\nper image in French and Czech. Dataset splits are\ndefined as the original Flickr30K.\nMSCOCO: This dataset extends the MSCOCO\ncaption dataset (Chen et al., 2015) by translating\nthe captions into Japanese (Yoshikawa et al., 2017)\nand Chinese (Li et al., 2019). The Japanese and\nChinese subsets consist of 820k and 20k captions\nrespectively. Following previous work, we use\nthe same train, dev, and test splits for English and\nJapanese as defined in Karpathy and Li (2015). As\nfor Chinese, we use the COCO-CN split (Li et al.,\n2019).\nFor all retrieval tasks, we follow previous\nwork (Li et al., 2021) and X-VLM (Zeng et al.,\n2021). During fine-tuning, we optimize Lcl and\nLmatch. For inference, we first compute similarity\nfor all images and texts, and then take the top-k\ncandidates and calculate the final ranking scores\nusing the fusion model.\n4.2 Experimental Results\n4.2.1 Results on IGLUE Benchmark\nTable 1 shows CCLM performance on the IGLUE\nbenchmark. First, for zero-shot cross-lingual trans-\nfer, we can see that CCLM 3M\nbase outperforms all\ncompared models by a substantial margin while\npre-trained on the same multi-modal data. Specifi-\ncally, compared to UC2, the prior state-of-the-art,\nCCLM3M\nbase obtains an average accuracy improve-\nment of 11.4% on multi-lingual multi-modal un-\nderstanding tasks including XVNLI, xGQA, and\nMaRVL, and an average R@1 improvement of\n5736\nModel Multi30K MSCOCO\nEN DE FR CS EN ZH JA\nM3P 87.7 82.7 73.9 72.2 88.7 86.2 87.9\nUC2 88.2 84.5 83.9 81.2 88.1 89.8 87.5\nMURALbase 92.2 88.6 87.6 84.2 88.6 - 88.4\nMURALlarge 93.8 90.4 89.9 87.1 92.3 - 91.6\nCCLM3Mbase 95.3 92.4 92.1 91.2 93.1 92.2 93.2\nCCLMbase 97.2 94.6 95.5 94.8 95.4 93.2 95.7\nCCLMlarge 97.8 95.8 96.6 96.2 95.6 94.0 96.1\nTable 2: Results on multi-lingual image-text retrieval\nin all-language fine-tune setting, where a model is fine-\ntuned on the combination of training data in all lan-\nguages. Following previous work, we compute the av-\nerage Recall@K for both image-to-text retrieval and\ntext-to-image retrieval with K = 1, 5, 10, as the eval-\nuation metric. We additionally report results in other\nfine-tune settings in Appendix A.5.\n47.3% and 18.2% on multi-lingual multi-modal\nretrieval datasets including xFlickr&CO and WIT.\nThis confirms that previous multi-lingual multi-\nmodal models fail to fully exploit the potential of\nmulti-lingual multi-modal pre-training and our pro-\nposed cross-view language modeling framework\ncan better align multi-lingual multi-modal repre-\nsentations with unified objectives.\nWe also find that the performance of our frame-\nwork can be significantly improved by leverag-\ning large-scale image-text pre-training in English\n(CCLMbase) and/or scaling up the model size\n(CCLMlarge). Notably, CCLM is the first multi-\nlingual multi-modal pre-trained model that sub-\nstantially outperforms the translate-test results of\nrepresentative English VLMs tested in the IGLUE\nbenchmark. This, for the first time, proves the po-\ntential of multi-lingual multi-modal pre-training on\nbuilding practical real-world applications involving\nvision-language tasks in different languages.\n4.2.2 Results on Multi-lingual Retrieval\nTable 2 gives the results on the multi-lingual image-\ntext retrieval benchmark. When pre-trained on\nthe same multi-modal data, CCLM 3M\nbase substan-\ntially outperforms UC2, the prior state-of-the-art,\nwith an averaged improvement of over 10% (in\nterms of averaged recall) across four languages on\nMulti30K. This confirms that our approach can bet-\nter align multi-lingual multi-modal representations.\nCCLM3M\nbase even outperforms MURAL. This is no-\ntable because MURALlarge is larger than our model\nand is pre-trained on much more data ( ∼450×\nmore image-text pairs and 390×more parallel sen-\ntence pairs). Moreover, we show that CCLM also\noutperforms MURAL without fine-tuning in Ap-\npendix A.5.\nWe also find that the cross-view language model-\ning framework yields better performance if leverag-\ning large-scale pre-training on image-text pairs in\nEnglish (CCLMbase) and/or scaling up the model\nsize (CCLMlarge), which is consistent with the ex-\nperimental results on the IGLUE benchmark. It\nconfirms that the proposed framework is scalable\nto both massive data and larger model size.\n4.2.3 Cross-lingual Transfer Gap\nFigure 2: Visualization of cross-lingual transfer gap.\nIn addition to absolute cross-lingual transfer re-\nsults reported in Table 1 and Table 2, we also\ncompare the cross-lingual transfer gap of differ-\nent models. We visualize the ratio of a model’s\nperformance on non-English languages to its per-\nformance on English test set, in Figure 2. A larger\nradar chat indicates the model has a smaller relative\ntransfer gap and can better transfer its performance\nto non-English test sets. We can see that CCLM’s\nrelative cross-lingual transfer gap is consistently\nsmaller than that of UC 2 across all tasks in the\nIGLUE benchmark (a) and all languages in the\nmulti-lingual retrieval datasets (b). The absolute\ncross-lingual transfer gap is even more significant.\nFor example, in Appendix A.5, we can see that\nfor M3P, the absolute zero-shot cross-lingual trans-\nfer gap between EN-CS and EN-JA in Multi30K\nand MSCOCO are 41.4% and 32.6% respectively.\nThis indicates that masked language modeling on\nunpaired texts in multiple languages are not very ef-\nfective for cross-lingual alignment of multi-modal\nmodels. The gap for UC 2 is reduced to 13.2%\nand 16.4%, demonstrating the effectiveness of us-\ning machine-translated captions for multi-lingual\nmulti-modal pre-training. CCLM 3M\nbase further re-\nduces this gap to 5.4% and 4.4%. This confirms\nthat the proposed cross-view language modeling\n5737\nMethods Multi30K MaRVL xGQA xFlickr&CO\nIR TR\nOurs 92.67 67.05 41.66 63.77 62.13\n-w/o shared cross-attn 92.49 66.67 36.76 63.73 62.01\n-w/o shared FFN 92.24 63.63 35.53 63.15 61.04\n-w/ TLM 91.88 62.65 35.84 58.44 56.73\n-w/ TLM + CL 92.34 65.00 36.13 63.42 61.33\n-w/o parallel sentence pairs 91.90 58.37 28.80 44.11 43.24\nTable 3: Ablation study results. Models w/o shared cross-attention and FFN are ablated variants where these\nmodules are separately parameterized in the cross-lingual fusion model and the cross-modal fusion model. Models\nw/ TLM and TLM + CL are variants where the multi-lingual objectives are that used in XLM-R and InfoXLM,\nwhich are not unified with the multi-modal objectives. All compared models are pre-trained for 15 epochs.\nframework can effectively transfer multi-modal rep-\nresentations from English to other languages with-\nout language-specific fine-tuning. In addition, we\nalso visualize the multi-lingual text representations\nand image representations in CCLM and a baseline\napproach in Appendix A.6, which clearly shows\nour approach can better align multi-lingual image-\ntext representations.\n4.3 Ablation Study\nSince previous work such as M3P, UC2, and MU-\nRAL all use different pre-training datasets, making\ndirect comparison difficult, we conduct an in-depth\nablation study to investigate the contribution of dif-\nferent design choices in the cross-view language\nmodeling framework. We pre-train 5 ablated vari-\nants of CCLM where parallel sentence pairs, uni-\nfied architecture, or unified objectives are ablated.\nAll compared models are pre-trained with the same\nCC3M and WikiMatrix data (except that w/o par-\nallel sentence pairs) for 15 epochs to ensure a fair\ncomparison. The results are shown in Table 3.\nFirst, we find that the use of parallel sentence\npairs plays a very important role. This indicates\nthat previous methods fail to fully exploit the po-\ntential of language pivoting for multi-lingual multi-\nmodal pre-training. On the other hand, CCLM\nvariant trained without parallel sentences in Ta-\nble 3 which uses the same pre-training dataset as\nUC2 still significantly outperforms previous mod-\nels such as M3P and UC2.\nWe then compare other ablated variants which\nall utilized parallel sentence pairs. We find that\nseparate parameterization of cross-attention and\nFFN modules for the cross-lingual and the cross-\nmodal task in the fusion model leads to inferior\nresults, especially for multi-lingual multi-modal\nunderstanding tasks such as xGQA.\nMoreover, we conduct ablation study on loss\nfunctions. We mainly consider multi-lingual ob-\njectives because the multi-modal objective combi-\nnation of itc+mlm+itm is the de-facto choice for\nmulti-modal loss (Li et al., 2021; Zeng et al., 2021).\nWe find that using common objectives in the multi-\nlingual pre-training literature underperforms our\nunified objective. These observations confirm the\nimportance of unifying architectures and objectives\nfor multi-lingual multi-modal pre-training.\n5 Conclusion\nIn this paper, we introduce cross-view language\nmodeling, a simple and effective framework that\nunifies cross-lingual and cross-modal pre-training.\nIt considers cross-lingual and cross-modal pre-\ntraining as the same procedure of aligning the repre-\nsentation of two different views of the same object,\nthus using shared model architectures and train-\ning objectives for multi-lingual multi-modal pre-\ntraining. We train CCLM with the proposed frame-\nwork and show that it advances the state-of-the-art\non all downstream multi-lingual vision-language\ntasks by a large margin. Moreover, it surpasses\nthe translate-test baseline for the first time, demon-\nstrating the potential of multi-lingual multi-modal\npre-training. Furthermore, the experimental results\nalso confirm that the proposed framework is scal-\nable to massive data and larger model sizes. We\nbelieve our model will become a foundation for fu-\nture multi-lingual multi-modal research and serve\nas a strong baseline. Moreover, the cross-view\nlanguage modeling framework also has the poten-\ntial of unifying more modalities such as audio and\nvideo with the same architectures and objectives.\nWe leave this for future work.\n5738\nLimitations\nIn this paper, we pre-train CCLM with moder-\nate multi-modal data, e.g. CC3M, to make a fair\ncomparison with previous work such as M3P and\nUC2. We leverage large-scale vision language pre-\ntraining simply by utilizing the pre-trained weights\nof X2-VLM which has been pre-trained on billion-\nscale image-text pairs in English. Collecting more\nimage-text pairs in different languages will very\nlikely lead to further performance improvements.\nMoreover, there exists larger public available multi-\nlingual datasets, such as MultiUN (Ziemski et al.,\n2016) and OPUS (Tiedemann, 2012). Leverag-\ning more multi-lingual datasets for pre-training\nshould also yield a more powerful multi-lingual\nmulti-modal model.\nAs for social impact, multi-modal pre-trained\nmodels can be used in applications that help people\nwith disability in one modality. Our work makes\nthese applications applicable to minority people\nspeaking non-English, and potentially low-resource\nlanguages. In sum, our work potentially enables\ndeep learning technology to benefit more people,\nand is unlikely to have direct negative social im-\npact.\nAcknowledgements\nWe would like to thank Hang Li, Jiaze Chen, and\nHuiyun Yang at ByteDance for insightful com-\nments in technical discussions. We also thank\nYaoming Zhu at ByteDance for his generous as-\nsistance in data collection and valuable feedback.\nReferences\nŽeljko Agi´c and Natalie Schluter. 2018. Baselines and\ntest data for cross-lingual inference. In Proceed-\nings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018),\nMiyazaki, Japan. European Language Resources As-\nsociation (ELRA).\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nEmanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva\nReddy, Desmond Elliott, Edoardo Maria Ponti, and\nIvan Vuli´c. 2022. Iglue: A benchmark for trans-\nfer learning across modalities, tasks, and languages.\nArXiv preprint, abs/2201.11732.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server. ArXiv preprint,\nabs/1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In European conference on\ncomputer vision, pages 104–120. Springer.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM: An\ninformation-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576–3588, On-\nline. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n5739\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In NeurIPS, pages 13042–13054.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30K: Multilingual English-\nGerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language , pages 70–\n74, Berlin, Germany. Association for Computational\nLinguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 2485–2494,\nHong Kong, China. Association for Computational\nLinguistics.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In IEEE\nConference on Computer Vision and Pattern Recogni-\ntion, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pages 6700–6709. Computer Vision Founda-\ntion / IEEE.\nAashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen,\nSneha Kudugunta, Chao Jia, Yinfei Yang, and Ja-\nson Baldridge. 2021. Mural: multimodal, mul-\ntitask retrieval across languages. ArXiv preprint,\nabs/2109.05125.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128–3137. IEEE Computer\nSociety.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei\nYu, Wang Ling, Zihang Dai, and Dani Yogatama.\n2020. A mutual information maximization perspec-\ntive of language representation learning. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in Neural Information Processing Systems,\n34.\nXirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan,\nZhengxiong Jia, Gang Yang, and Jieping Xu. 2019.\nCoco-cn for cross-lingual image tagging, caption-\ning, and retrieval. IEEE Transactions on Multimedia,\n21(9):2347–2360.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria\nPonti, Siva Reddy, Nigel Collier, and Desmond El-\nliott. 2021a. Visually grounded reasoning across\nlanguages and cultures. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10467–10485, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. 2021b.\nSwin transformer: Hierarchical vision transformer\nusing shifted windows. In 2021 IEEE/CVF Interna-\ntional Conference on Computer Vision, ICCV 2021,\nMontreal, QC, Canada, October 10-17, 2021, pages\n9992–10002. IEEE.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\n5740\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-\n14, 2019, Vancouver, BC, Canada, pages 13–23.\nMinheng Ni, Haoyang Huang, Lin Su, Edward Cui,\nTaroon Bharti, Lijuan Wang, Dongdong Zhang, and\nNan Duan. 2021. M3p: Learning universal repre-\nsentations via multitask multilingual multimodal pre-\ntraining. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 3977–3986.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding. ArXiv preprint, abs/1807.03748.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-\nMartin O Steitz, Stefan Roth, Ivan Vuli´c, and Iryna\nGurevych. 2021. xgqa: Cross-lingual visual question\nanswering. ArXiv preprint, abs/2109.06082.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. ArXiv preprint, abs/1910.10683.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzmán. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620 lan-\nguage pairs from Wikipedia. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 1351–1361, Online. Association for Computa-\ntional Linguistics.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565,\nMelbourne, Australia. Association for Computational\nLinguistics.\nKrishna Srinivasan, Karthik Raman, Jiecao Chen,\nMichael Bendersky, and Marc Najork. 2021. Wit:\nWikipedia-based image text dataset for multimodal\nmultilingual machine learning. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2443–2449.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In Lrec, volume 2012, pages 2214–\n2218. Citeseer.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning , pages\n23318–23340. PMLR.\nNing Xie, Farley Lai, Derek Doran, and Asim Ka-\ndav. 2019. Visual entailment: A novel task for\nfine-grained image understanding. ArXiv preprint,\nabs/1901.06706.\nYuya Yoshikawa, Yutaro Shigeto, and Akikazu\nTakeuchi. 2017. STAIR captions: Constructing a\n5741\nlarge-scale Japanese image caption dataset. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 417–421, Vancouver, Canada. Associ-\nation for Computational Linguistics.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nDanting Zeng. 2021. Multi task learning based frame-\nwork for multimodal classification. In Proceedings\nof the Third Workshop on Multimodal Artificial Intel-\nligence, pages 30–35, Mexico City, Mexico. Associa-\ntion for Computational Linguistics.\nYan Zeng and Jian-Yun Nie. 2021. An investigation\nof suitability of pre-trained language models for dia-\nlogue generation – avoiding discrepancies. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 4481–4494, Online.\nAssociation for Computational Linguistics.\nYan Zeng, Xinsong Zhang, and Hang Li. 2021.\nMulti-grained vision language pre-training: Align-\ning texts with visual concepts. ArXiv preprint ,\nabs/2111.08276.\nYan Zeng, Xinsong Zhang, Hang Li, Jiawei Wang,\nJipeng Zhang, and Wangchunshu Zhou. 2022. X\nΘ 2-vlm: All-in-one pre-trained model for vision-\nlanguage tasks. arXiv preprint arXiv:2211.12402.\nMingyang Zhou, Luowei Zhou, Shuohang Wang,\nYu Cheng, Linjie Li, Zhou Yu, and Jingjing Liu.\n2021. Uc2: Universal cross-lingual cross-modal\nvision-and-language pre-training. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4155–4165.\nWangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xin-\nsong Zhang. 2022. Vlue: A multi-task bench-\nmark for evaluating vision-language models. CoRR,\nabs/2205.15237.\nMichał Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 3530–3534.\n5742\nA Appendix\nA.1 Compared Models\nmUNITER and xUNITER: A multi-lingual vari-\nant of the UNITER (Chen et al., 2020) model pre-\ntrained by Liu et al. (2021a). The model is pre-\ntrained by alternating between a batch of multi-\nmodal English data from CC3M with UNITER\nobjectives and a batch of text-only multilingual\nWikipedia data with the MLM objective. mU-\nNITER and xUNITER differ in their initializa-\ntion: mUNITER and xUNITER are initialized from\nmBERT and XLM-R.\nM3P: A multi-lingual multi-modal model initial-\nized from XLM-R and pre-trained with the combi-\nnation of multilingual masked language modeling,\nmulti-modal code-switched masked language mod-\neling, multi-modal code-switched masked region\nmodeling, and multi-modal code-switched visual-\nlinguistic matching. The code-switched training\nmethod allows the model to explicitly align images\nwith non-English languages. In each multi-modal\nbatch, image-text pairs are fed to the model ei-\nther fully in English or with code-switched words\naccording to a given sampling ratio. Similar to\nmUNITER and xUNITER, the model is trained by\nalternating multi-modal and multi-lingual batches.\nUC2: The state-of-the-art multi-lingual vision-\nlanguage model which relies on (text-only) ma-\nchine translation technologies to obtain CC3M\ndata in five languages (Czech, French, German,\nJapanese, and Mandarin). The model is then\npre-trained on multi-lingual multi-modal batches\nwhere a caption is sampled uniformly from the\navailable languages for each image. As for pre-\ntraining objectives. In addition to conventional\nvision-language pre-training objectives, a visual-\nconditioned translation language modeling objec-\ntive is added to improve multi-lingual multi-modal\nalignment.\nA.2 Details for Multi-lingual Data\nES FR PT RU DE VI ID AR JA ZH\n3,130 2,645 2,322 1,598 1,467 998 974 968 841 783\nEL CS TR DA BG KO BN ET TA SW\n609 509 455 412 353 281 269 241 61 51\nTable 4: The number of parallel sentence pairs per lan-\nguage (K) in the subset of WikiMatrix.\nA.3 Results on English Tasks\nTable 5 reports CCLM performance that is pre-\ntrained on COCO, VG, SBU, and CC3M, on three\ncommon English multi-modal tasks. We can ob-\nserve that CCLM also has very competitive per-\nformance compared to strong English multi-modal\nbaselines.\nMethods VQA2.0 NLVR2 MSCOCO (5K)\ntest-dev dev test-P IR TR\nVinVLbase 75.95 82.05 83.08 58.10 74.60\nALBEF (4M)74.54 80.24 80.50 56.80 73.10\nCCLM4Mbase 77.17 82.66 83.22 60.89 77.72\nTable 5: Results on common English multi-modal\ntasks. R@1 and Accuracy are reported for MSCOCO\n(5K test set) and understanding tasks respectively.\nA.4 Few-Shot Results on IGLUE\nTable 6 gives results on IGLUE benchmark. For\nour models, mean and standard deviation (in brack-\nets) of 3 different runs with different random seeds\nare reported. Results of compared models are di-\nrectly copied from the IGLUE benchmark. In the\nfew-shot setting, the English trained models are\ncontinually fine-tuned with a few labeled exam-\nples in a target language before evaluating on this\nlanguage. We select exactly the same few-shot ex-\namples following IGLUE instructions to ensure our\nresults are compatible with that reported in IGLUE.\nWe omit few-shot evaluation on the WIT dataset\nbecause this setup is also omitted in IGLUE. We\nfind that similar to existing models, CCLM can also\nbenefit from few-shot learning with a few examples\nin the target languages.\nA.5 More Results on Retrieval Tasks\nTable 7 reports results on multi-lingual image-text\nretrieval of CCLM. We follow the practice of prior\nwork and evaluate in three different settings in-\ncluding English-only fine-tuning, single-language\nfine-tuning, and all-language fine-tuning, where\nthe model is fine-tuned on English data, target lan-\nguage data, and the combination of training data in\nall languages, respectively.\nWe also report multi-lingual image-text retrieval\nresults without fine-tuning (zero-shot) in Table 8.\nM3P and UC2 do not report their zero-shot retrieval\nperformances. We can observe that CCLM 3M\nbase\noutperforms MURAL which is pre-trained on much\nlarger data. Besides, the performance gap on non-\n5743\nModel NLI QA Reasoning Retrieval\nXVNLI xGQA MaRVL xFlickr&CO WIT\nIR TR IR TR\nFew-shot train English fine-tuned model on target languages (Few-Shot)\nmUNITER 53.95 37.21 53.41 8.54 9.32 - -\nxUNITER 60.55 40.68 57.46 14.30 13.54 - -\nM3P 59.36 41.04 49.79 13.21 12.26 - -\nUC2 63.68 42.95 58.32 19.79 17.59 - -\nCCLM 3M\nbase 75.15(.03) 50.94(.02) 70.53(.18) 66.04(.05) 68.15(.04) - -\nTable 6: Few-Shot Results on IGLUE benchmark.R@1 and Accuracy are reported for retrieval tasks (xFlickr&CO\nand WIT) and understanding tasks (XVNLI, xGQA, MaRVL) respectively. For our model, mean and standard\ndeviation (in brackets) of 3 different runs with different random seeds are reported.\nModel Multi30K MSCOCO\nEN DE FR CS EN ZH JA\nEnglish-only Fine-tune (Zero-Shot)\nM3P 87.4 58.5 46.0 36.8 88.6 53.8 56.0\nUC2 87.2 74.9 74.0 67.9 88.1 82.0 71.7\nCCLM3Mbase 94.8(.11) 90.3(.08) 90.9(.38) 89.4(.21) 93.2(.05) 91.0(.18) 88.8(.06)\nSingle-Language Fine-tune\nM3P 87.4 82.1 67.3 65.0 88.6 75.8 80.1\nUC2 87.2 83.8 77.6 74.2 88.1 84.9 87.3\nCCLM3Mbase 94.8(.11) 91.9(.16) 90.6(.18) 88.9(.05) 93.2(.05) 90.2(.24) 93.3(.26)\nTable 7: Results on multi-lingual image-text retrieval. We compute the average Recall@K for both image-to-text\nretrieval and text-to-image retrieval with K = 1, 5, 10, as the evaluation metric. For our model, mean and standard\ndeviation (in brackets) of 3 different runs with different random seeds are reported.\nModel Multi30K MSCOCO\nEN DE FR CS EN ZH JA\nMURALbase 82.4 76.2 75.0 64.6 79.2 - 73.4\nCCLM3Mbase 83.7 79.1 76.7 73.9 81.5 79.5 76.8\nTable 8: Zero-shot results on multi-lingual image-text\nretrieval. We compute the average Recall@K for both\nimage-to-text retrieval and text-to-image retrieval with\nK = 1, 5, 10, as the evaluation metric. Results of com-\npared models are directly copied from the corresponding\npapers.\nEnglish test sets of MURAL is larger, which shows\nour model has better cross-lingual transfer ability.\nA.6 Visualization of Representations\nFigure 3 visualizes several examples in\nxFlickr&CO test set in 2D space using t-\nSNE (Van der Maaten and Hinton, 2008). The\nimage representations and text representations are\nthe output [CLS] embeddings of the image encoder\nand the cross-lingual text encoder respectively.\nFigure 3: Visualization of image (denoted by stars)\nand text (denoted by points) representations. For a test\nexample, there are eight texts in different languages.\nPoints and stars in the same color are of the same test\nexample. (a) is the ablated variant of CCLM that does\nnot utilize parallel sentence pairs.\nWe can observe that CCLM’s text representations\nin different languages are more gathered and\nthe distances between text representations and\ncorresponding image representations are relatively\nshorter. This indicates our approach can better\nalign multi-lingual image-text representations.\n5744\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nsection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nsection 6\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nsection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nexperiment section\n□\u0013 B1. Did you cite the creators of artifacts you used?\nexperiment section\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nthey are commonly used datasets\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nthey are commonly used datasets\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nthey are commonly used datasets\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nthey are commonly used datasets\n□\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nthey are commonly used datasets\nC □\u0013 Did you run computational experiments?\nsection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 4.1.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5745\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 4.1.1, 4.1.2, and 4.1.3; we report our best hyperparameter values with code release.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix Table 5 and Table 6. For our model, mean and standard deviation (in brackets) of 3\ndifferent runs with different random seeds are reported.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n5746",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8137158155441284
    },
    {
      "name": "Modal",
      "score": 0.7066682577133179
    },
    {
      "name": "Natural language processing",
      "score": 0.5693134665489197
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5603987574577332
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5152993202209473
    },
    {
      "name": "Language model",
      "score": 0.4822031259536743
    },
    {
      "name": "Sentence",
      "score": 0.46553555130958557
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    }
  ],
  "cited_by": 18
}