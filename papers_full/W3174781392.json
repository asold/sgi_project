{
  "title": "An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models",
  "url": "https://openalex.org/W3174781392",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2107353385",
      "name": "Liu Xueqing",
      "affiliations": [
        "Stevens Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098713757",
      "name": "Chi Wang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3044285325",
    "https://openalex.org/W2963815651",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W2106411961",
    "https://openalex.org/W2913566062",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2556522401",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2131241448",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2097998348",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4211116959",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W4299300709",
    "https://openalex.org/W2996012599",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W2973943791",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2113145584",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3037833767",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963702144",
    "https://openalex.org/W2963069632",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2126559945",
    "https://openalex.org/W3125126313",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2766164908",
    "https://openalex.org/W2250178193",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4394662461",
    "https://openalex.org/W3122890974"
  ],
  "abstract": "Xueqing Liu, Chi Wang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 2286–2300\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2286\nAn Empirical Study on Hyperparameter Optimization for Fine-Tuning\nPre-trained Language Models\nXueqing Liu\nStevens Institute of Technology\nxueqing.liu@stevens.edu\nChi Wang\nMicrosoft Research\nwang.chi@microsoft.com\nAbstract\nThe performance of ﬁne-tuning pre-trained\nlanguage models largely depends on the hyper-\nparameter conﬁguration. In this paper, we in-\nvestigate the performance of modern hyperpa-\nrameter optimization methods (HPO) on ﬁne-\ntuning pre-trained language models. First, we\nstudy and report three HPO algorithms’ per-\nformances on ﬁne-tuning two state-of-the-art\nlanguage models on the GLUE dataset. We\nﬁnd that using the same time budget, HPO of-\nten fails to outperform grid search due to two\nreasons: insufﬁcient time budget and overﬁt-\nting. We propose two general strategies and an\nexperimental procedure to systematically trou-\nbleshoot HPO’s failure cases. By applying the\nprocedure, we observe that HPO can succeed\nwith more appropriate settings in the search\nspace and time budget; however, in certain\ncases overﬁtting remains. Finally, we make\nsuggestions for future work. Our implemen-\ntation can be found in https://github.c\nom/microsoft/FLAML/tree/main/flaml\n/nlp/.\n1 Introduction\nIn the recent years, deep learning and pre-trained\nlanguage models (Devlin et al., 2019; Liu et al.,\n2019; Clark et al., 2020; He et al., 2021) have\nachieved great success in the NLP community. It\nhas now become a common practice for researchers\nand practitioners to ﬁne-tune pre-trained language\nmodels in down-stream NLP tasks. For example,\nthe HuggingFace transformers library (Wolf et al.,\n2020) was ranked No.1 among the most starred\nNLP libraries on GitHub using Python1.\nSame as other deep learning models, the perfor-\nmance of ﬁne-tuning pre-trained language mod-\nels largely depends on the hyperparameter con-\nﬁguration. A different setting in the hyperparam-\n1https://github.com/EvanLi/Github-\nRanking/blob/master/Top100/Python.md\neters may cause a signiﬁcant drop in the perfor-\nmance, turning a state-of-the-art model into a poor\nmodel. Methods for tuning hyperparameters can be\ncategorized as (1) traditional approaches such as\nmanual tuning and grid search, and (2) automated\nHPO methods such as random search and Bayesian\noptimization (BO). Manual tuning often requires\na large amount of manual efforts; whereas grid\nsearch often suffers from lower efﬁciency due to\nthe exponential increase in time cost with the num-\nber of hyperparameters. Automated HPO methods\nwere proposed to overcome these disadvantages.\nRecently, automated HPO methods also become in-\ncreasingly popular in the NLP community (Zhang\nand Duh, 2020; Dodge et al., 2019). For exam-\nple, Bayesian optimization (BO) (Zhang and Duh,\n2020) and Population-based Training (Jaderberg\net al., 2017) both prove to be helpful for improving\nthe performance of the transformer model (Vaswani\net al., 2017) for neural machine translation. The\nHuggingFace library has also added native sup-\nports for HPO in a recent update (version 3.1.0,\nAug 2020).\nWith improved supports, users can now easily\naccess a variety of HPO methods and apply them to\ntheir ﬁne-tuning tasks. However, the effectiveness\nof this step is less understood. To bridge this gap,\nin this paper, we propose an experimental study for\nﬁne-tuning pre-trained language models using the\nHuggingFace library. This study is motivated by\nthe following research questions: First, can auto-\nmated HPO methods outperform traditional tuning\nmethod such as grid search? Second, on which\nNLP tasks do HPO methods work better? Third, if\nHPO does not work well, how to troubleshoot the\nproblem and improve its performance?\nTo answer these questions, we start from a sim-\nple initial study (Section 4) by examining the per-\nformance of three HPO methods on two state-of-\nthe-art language models on the GLUE dataset. The\n2287\ntime budget for HPO in the initial study is set to be\nthe same as grid search. Results of the initial study\nshow that HPO often fails to match grid search’s\nperformance. The reasons for HPO’s failures are\ntwo folds: ﬁrst, the same budget as grid search\nmay be too small for HPO; second, HPO overﬁts\nthe task. With these observations, we propose two\ngeneral strategies for troubleshooting the failure\ncases in HPO as well as an overall experimental\nprocedure (Figure 1). By applying the procedure\n(Section 5), we ﬁnd that by controlling overﬁtting\nwith reduced search space and using a larger time\nbudget, HPO has outperformed grid search in more\ncases. However, the overﬁtting problem still ex-\nists in certain tasks even when we only search for\nthe learning rate and batch size. Finally, we make\nsuggestions for future work (Section 7).\nThe main contributions of this work are:\n• We empirically study the performance of three\nHPO methods on two pre-trained language\nmodels and on the GLUE benchmark;\n• We design an experimental procedure which\nproves useful to systematically troubleshoot\nthe failures in HPO for ﬁne-tuning;\n• We report and analyze the execution results of\nthe experimental procedure, which sheds light\non future work;\n2 Deﬁnition of HPO on Language Model\nFine-Tuning\nGiven a pre-trained language model, a ﬁne-tuning\ntask, and a dataset containing Dtrain, Dval, Dtest,\nthe goal of a hyperparameter optimization algo-\nrithm is to ﬁnd a hyperparameter conﬁguration c,\nso that when being trained under conﬁguration c,\nthe model’s performance on a validation setDval\nis optimized. Formally, the goal of HPO is to ﬁnd\nc∗= arg max\nc∈S\nf(c, Dtrain, Dval)\nwhere Sis called the search spaceof the HPO al-\ngorithm, i.e., the domain where the hyperparameter\nvalues can be chosen from. The function f(·, ·, ·)\nis called the evaluation protocol of HPO, which is\ndeﬁned by the speciﬁc downstream task. For exam-\nple, many tasks in GLUE deﬁne f as the validation\naccuracy. If a task has multiple protocols, we ﬁx f\nas one of them2. After ﬁnding c∗, the performance\nof HPO will be evaluated using the performance of\nthe model trained with c∗on the test set Dtest.\nTo fairly compare the performances of different\nHPO algorithms, the above optimization problem is\ndeﬁned with a constraint in the maximum running\ntime of the HPO algorithm, which we call the time\nbudget for the algorithm, denoted as B. Under\nbudget B, the HPO algorithm can try a number\nof conﬁgurations c1, c2, ··· , cn. The process of\nﬁne-tuning with conﬁguration ci is called a trial.\nFinally, we call the process of running an HPO\nalgorithm A once one HPO run.\n3 Factors of the Study\nIn this paper, we conduct an empirical study to\nanswer the research questions in Section 1. First,\ncan automated HPO methods outperform grid\nsearch? The answer to this question depends on\nmultiple factors, i.e., the NLP task on which HPO\nand grid search are evaluated, the pre-trained lan-\nguage model for ﬁne tuning, the time budget, the\nsearch space for grid search and HPO algorithm,\nand the choice of HPO algorithm. To provide\na comprehensive answer, we need to enumerate\nmultiple settings for these factors. However, it is\ninfeasible to enumerate all possible settings for\neach factor. For instance, there exist unlimited\nchoices for the search space. To accomplish\nour research within reasonable computational\nresources3, for each factor, we only explore the\nmost straight-foward settings. For example, the\nsearch space for grid search is set as the default\ngrid conﬁguration recommended for ﬁne-tuning\n(Table 1), and the search space for HPO is set as a\nstraightforward relaxation of the grid conﬁguration.\nWe explain the settings for each factor in details\nbelow.\n2There are 3 GLUE tasks with multiple validation scores:\nMRPC, STS-B, and QQP (not studied). For MRPC we opti-\nmize the validation accuracy, and for STS-B we optimize the\nPearson score on the validation set.\n3Our experiments were run on two GPU servers, server 1 is\nequipped with 4xV100 GPUs (32GB), and server 2 is a DGX\nserver equipped with 8xV100 GPUs (16GB). To avoid incom-\nparable comparisons, all experiments on QNLI and MNLI\nare run exclusively on server 2, and all other experiments are\nrun exclusively on server 1. To speed up the training, we use\nfp16 in all our experiments. To guarantee the comparability\nbetween different HPO methods, all trials are allocated exactly\n1 GPU and 1 CPU. As a result, all trials are executed in the\nsingle-GPU mode and there never exist two trials sharing the\nsame GPU.\n2288\nHyperparameter Electra-grid Electra-HPO RoBERTa-grid RoBERTa-\nHPO\nlearning rate {3e-5,1e-4,1.5e-4} log((2.99e-5,1.51e-\n4))\n{1e-5,2e-5,3e-\n5}\n(0.99e-\n5,3.01e-5)\nwarmup ratio 0.1 (0, 0.2) 0.06 (0, 0.12)\nattention dropout 0.1 (0, 0.2) 0.1 (0, 0.2)\nhidden dropout 0.1 (0, 0.2) 0.1 (0, 0.2)\nweight decay 0 (0, 0.3) 0.1 (0, 0.3)\nbatch size 32 {16, 32, 64} { 16, 32} { 16, 32, 64}\nepochs 10 for RTE/STS-B,\n3 for other\n10 for RTE/STS-B,\n3 for other\n10 10\nTable 1: The search space for grid search and HPO methods in this paper. For grid search, we adopt the search\nspaces from the Electra (Clark et al., 2020) and RoBERTa (Liu et al., 2019) paper. For each model, we expand the\ngrid search space to a larger, simple search space for HPO.\nNLP Tasks. To study HPO’s performance on\nmultiple NLP tasks, we use the 9 tasks from\nthe GLUE (General Language Understanding\nEvaluation) benchmark (Wang et al., 2018).\nTime Budget . We focus on a low-resource\nscenario in this paper. To compare the performance\nof grid search vs. HPO, we ﬁrst allocate the same\ntime budget to HPO as grid search in our initial\ncomparative study (Section 4). If HPO does not\noutperform grid search, we increase the time\nbudget for HPO. We require that each HPO run\ntakes no more than 8 GPU hours with the NVIDIA\nTesla V100 GPU under our setting. We prune a\ntask if the time for grid search exceeds two hours.\nA complete list of the time used for each remaining\ntask can be found in Table 2.\nNLP task Electra epoch RoBERTa epoch\nWNLI 420 3 660 10\nRTE 1000 10 720 10\nMRPC 420 3 720 10\nCoLA 420 3 1200 10\nSTS-B 1200 10 1000 10\nSST 1200 3 7800 -\nQNLI 1800 3 -\nQQP 7800 3 - -\nMNLI 6600 3 - -\nTable 2: The running time of grid search for each task\n(in seconds) and the corresponding number of epochs.\nPre-trained Language Models. In this paper, we\nfocus on two pre-trained language models: the\nElectra-base model (Clark et al., 2020), and the\nRoBERTa-base model (Liu et al., 2019). Electra\nand RoBERTa are among the best-performing\nmodels on the leaderboard of GLUE as of Jan\n20214. Another reason for choosing the two\nmodels is that they both provide a simple search\nspace for grid search, and we ﬁnd it helpful to\ndesign our HPO search space on top of them.\nWe use both models’ implementations from the\ntransformers library (Wolf et al., 2020) (version =\n3.4.0). Among all the different sizes of RoBERTa\nand Electra (large, base, small), we choose the\nbase size, because large models do not ﬁt into our\n2-hour budget5. With the 2-hour time constraint,\nwe prune tasks where grid search takes longer than\ntwo hours. For Electra, QQP is pruned, whereas\nfor RoBERTa, SST, QNLI, QQP, MNLI are pruned.\nSearch Space for Grid Search and HPO . It is\ngenerally difﬁcult to design an HPO search space\nfrom scratch. In our problem, this difﬁculty is\nfurther ampliﬁed with the limited computational\nresources. Fortunately, most papers on pre-trained\nlanguage models recommend one or a few\nhyperparameter conﬁgurations for ﬁne-tuning. We\nuse them as the conﬁgurations for grid search.\nFor HPO, the performance depends on the search\nspace choice, e.g., it takes more resources to\nexplore a large space than a smaller space close\nto the best conﬁguration. Due to the time budget\nlimits, we focus on a small space surrounding\nthe recommended grid search space, as shown\n4www.gluebenchmark.com\n5Our empirical observation shows that the large models\ntake 1.5 to 2 times the running time of the base models.\n2289\nin Table 1. 6 More speciﬁcally, we convert the\nlearning rate, warmup ratio, attention dropout,\nand hidden dropout to a continuous space by\nexpanding the grid space. For weight decay, since\nthe recommended conﬁguration is 0, we follow\nRay Tune’s search space and set the HPO space to\n(0, 0.3) (Kamsetty, 2020). For epoch number, most\nexisting work uses an integer value between 3 and\n10 (Clark et al., 2020; Liu et al., 2019; Dai et al.,\n2020), resulting in a large range of space we can\npossibly search. To reduce the exploration required\nfor HPO, we skip expanding the search space for\nepoch number and ﬁx it to the grid conﬁguration.\nHPO Algorithms. We compare the performance\nbetween grid search and three HPO algorithms:\nrandom search (Bergstra and Bengio, 2012),\nasynchronous successive halving (ASHA) (Li\net al., 2020), and Bayesian Optimization (Akiba\net al., 2019)+ASHA. We use all HPO methods’\nimplementations from the Ray Tune library (Liaw\net al., 2018) (version 1.2.0). We use BO (with TPE\nsampler) together with the ASHA pruner, because\nwith the small time budget, BO without the pruner\nreduces to random search. As ﬁne-tuning in\nNLP usually outputs the checkpoint with the\nbest validation accuracy, we also let the HPO\nmethods output the best checkpoint of the best\ntrial. This choice is explained in more details in\nAppendix A.1.\n4 Experiment #1: Comparative Study\nusing 1GST\nAs the performance of HPO depends on the time\nbudget, to compare between grid search and HPO,\nwe ﬁrst conduct an initial study by setting the time\nbudget of HPO to the same as grid search. For the\nrest of this paper, we use aGST to denote that the\ntime budget=a×the running time for grid search.\nTable 3 shows the experimental results on Electra\nand RoBERTa using 1GST. For each (HPO method,\nNLP task) pair, we repeat the randomized experi-\nments 3 times and report the average scores. We\nanalyze the results in Section 4.1.\n6The grid search spaces in Table 1 are from Table 7 of\nElectra and Table 10 of RoBERTa. For Electra, we ﬁx the\nhyperparameters for Adam; we skip the layer-wise learning\nrate decay because it is not supported by the HuggingFace\nlibrary. While Electra’s original search space for learning rate\nis [3e-5, 5e-5, 1e-4, 1.5e-4], we have skipped the learning rate\n5e-5 in our experiment.\n4.1 Analysis of the Initial Results\nElectra. By comparing the performance of grid\nsearch and HPO in Table 3 we can make the fol-\nlowing ﬁndings. First, HPO fails to match grid\nsearch’s validation accuracy in the following tasks:\nRTE, STS-B, SST and QNLI. In certain tasks such\nas QNLI and RTE, grid search outperforms HPO\nby a large margin. Considering the fact that grid\nsearch space is a subspace of the HPO space, this\nresult shows that with the same time budget as grid\nsearch (i.e., approximately 3 to 4 trials), it is difﬁ-\ncult to ﬁnd a conﬁguration which works better than\nthe recommended conﬁgurations. Indeed, with 3\nto 4 trials, it is difﬁcult to explore the search space.\nAlthough ASHA and BO+ASHA both search for\nmore trials by leveraging early stopping (Li et al.,\n2020), the trial numbers are still limited (the aver-\nage trial numbers for experiments in Table 3 can be\nfound in Table 6 of the appendix). Second, among\nthe tasks where HPO outperforms grid search’s val-\nidation accuracy, there are 2 tasks (WNLI, MRPC)\nwhere the test accuracy of HPO is lower than grid\nsearch. As a result, the HPO algorithm overﬁts\nthe validation dataset. Overﬁtting in HPO gener-\nally happens when the accuracy is optimized on a\nlimited number of validation data points and can-\nnot generalize to unseen test data (Feurer and Hut-\nter, 2019). (Zhang et al., 2021) also found that\nﬁne-tuning pre-trained language models is prone\nto overﬁtting when the number of trials is large,\nthough they do not compare HPO and grid search.\nFinally, by searching for more trials, ASHA and\nBO+ASHA slightly outperform random search in\nthe validation accuracy, but their test accuracy is\noften outperformed by random search.\nRoBERTa. By observing RoBERTa’s results from\nTable 3, we can see that the average validation ac-\ncuracy of HPO outperforms grid search in all tasks\nexcept for CoLA. It may look like HPO is more\neffective; however, most of the individual runs in\nTable 3 overﬁt. As a result, HPO for ﬁne-tuning\nRoBERTa is also prone to overﬁtting compared\nwith grid search. The complete lists of the overﬁt-\nting cases in Table 3 can be found in Table 8 and\nTable 9 of Appendix A.3.\n4.2 A General Experimental Procedure for\nTroubleshooting HPO Failures\nSince Table 3 shows HPO cannot outperform grid\nsearch using 1GST, and is prone to overﬁtting, we\npropose two general strategies to improve HPO’s\n2290\nWNLI RTE MRPC CoLA STS-B SST QNLI MNLI\nElectra-base, validation\ngrid 56.3 84.1 92.3/89.2 67.2 91.5/91.4 95.1 93.5 88.6\nRS 56.8 82.2 93.0/90.4 68.8 90.1/90.2 94.7 93.0 88.9\nRS+ASHA 57.2 80.3 93.0/90.3 67.9 91.4/91.3 94.9 93.1 88.6\nBO+ASHA 58.2 82.6 93.1/90.4 69.4 91.5/91.3 94.7 93.1 89.2\nElectra-base, test\ngrid 65.1 76.8 91.1/87.9 58.5 89.7/89.2 95.7 93.5 88.3\nRS 64.4 75.6 90.7/87.5 63.0 88.0/87.6 95.1 93.0 88.7\nRS+ASHA 62.6 74.1 90.6/87.3 61.2 89.5/89.1 94.9 92.9 88.5\nBO+ASHA 61.6 75.1 90.7/87.4 64.1 89.7/89.1 94.8 93.0 88.7\nWNLI RTE MRPC CoLA STS-B\nRoBERTa-base, validation\ngrid 56.3 79.8 93.1/90.4 65.1 91.2/90.8\nRS 57.8 80.4 93.3/90.7 64.1 91.2/90.9\nRS+ASHA 57.3 80.8 93.4/90.8 64.5 91.2/90.9\nBO+ASHA 56.3 80.3 93.7/91.4 64.5 91.3/91.0\nRoBERTa-base, test\ngrid 65.1 73.9 90.5/87.1 61.7 89.3/88.4\nRS 64.9 73.5 90.1/86.7 59.1 89.3/88.6\nRS+ASHA 65.1 74.1 90.6/87.3 59.4 89.1/88.3\nBO+ASHA 65.1 73.3 90.4/87.2 60.1 89.1/88.4\nTable 3: Results of the initial comparative study on Electra (top) and RoBERTa (bottom) by varying the GLUE\ntask and HPO method while ﬁxing the search space and time budget. For each (HPO method, task), we rerun the\nexperiment 3 times and report the average.\nperformance. First, we increase the time budget for\nHPO so that HPO can exploit the space with more\ntrials. Second, to control overﬁtting, we propose to\nreduce the search space. More speciﬁcally, we pro-\npose to ﬁx the values of certain hyperparameters\nto the default values in the grid conﬁguration (Ta-\nble 3). The reason is that overﬁtting can be related\nto certain hyperparameter settings of the model.\nFor example, it was shown in ULMFit (Howard\nand Ruder, 2018) that using a non-zero warmup\nstep number can help reduce overﬁtting. Intuitively,\na larger search space is more prone to overﬁtting.\nFor example, by using a warmup search space =\n(0, 0.2), the warmup steps in the best trial found\nby HPO may be much smaller or larger than the\nsteps used by grid search. Other hyperparameters\nwhich are related to overﬁtting of ﬁne-tuning in-\nclude the learning rate (Smith and Le, 2017), batch\nsize (Smith et al., 2017), and the dropout rates (Sri-\nvastava et al., 2014; Loshchilov and Hutter, 2019,\n2018).\nOur proposed procedure for troubleshooting\nHPO failures is depicted in Figure 1. Starting from\nthe full search space and 1GST, we test the HPO\nalgorithm for a few times. If any overﬁtting is ob-\nserved, we reduce the search space and go back\nto testing the HPO algorithm again. On the other\nhand, if no overﬁtting is observed and HPO also\ndoes not outperform grid search, we increase the\ntime budget and also go back to testing the HPO\nalgorithm again. We continue this procedure until\nany of the following conditions is met: ﬁrst, HPO\nsuccessfully outperforms grid search; second, the\nsearch space cannot be further reduced, thus HPO\noverﬁts the task; third, the time budget cannot be\nfurther increased under a user-speciﬁed threshold,\nthus whether HPO can outperform grid search is to\nbe determined for this speciﬁc task.\n5 Experiment #2: Troubleshooting HPO\nIn this section, we evaluate the effectiveness of\nour proposed procedure in Figure 1. To apply the\nprocedure, we need to further consolidate two com-\nponents: ﬁrst, what time budget should we use;\nsecond, which hyperparameterto ﬁx for reducing\nthe search space. For the ﬁrst component, we use a\n2291\nrelatively small list for time budget options {1GST,\n4GST}. For the second component, it is difﬁcult to\nguarantee to reduce overﬁtting by ﬁxing a speciﬁc\nhyperparameter to its grid search values. When\nchoosing the hyperparameter to ﬁx, we refer to the\nconﬁgurations of the best trials which cause the\nHPO results to overﬁt.\nFigure 1: A general experimental procedure for trou-\nbleshooting HPO failure cases.\nstart\nsearch space=           ,  res=1GST\noutperform?\nHPO succeeds\nno\nres > MAX?\nrepeat HPO a few times\nres\nspace\noverfitting?\nno\nTBD\ncan reduce \nHPO overfits task\nyes\nno\nyes\nno\nyes\nyes\n<latexit sha1_base64=\"Na5ydGeHBEyKZihVhvoIrP0tiP4=\">AAAB+3icbVDLSsNAFL3xWesr1qWbwSK4Kokouiy6cVnRPqANYTKdtEMnkzAzEUvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXOfcECWdKO863tbK6tr6xWdmqbu/s7u3bB7WOilNJaJvEPJa9ACvKmaBtzTSnvURSHAWcdoPJTeF3H6lULBYPeppQL8IjwUJGsDaSb9cGEdZjgnl2n/tZmHKe+3bdaTgzoGXilqQOJVq+/TUYxiSNqNCEY6X6rpNoL8NSM8JpXh2kiiaYTPCI9g0VOKLKy2bZc3RilCEKY2me0Gim/t7IcKTUNArMZJFULXqF+J/XT3V45WVMJKmmgsw/MvchHaOiCDRkkhLNp4ZgIpnJisgYS0y0qatqSnAXT14mnbOGe9Fw7s7rzeuyjgocwTGcgguX0IRbaEEbCDzBM7zCm5VbL9a79TEfXbHKnUP4A+vzB8vjlO4=</latexit>\nS full\n5.1 Choosing the Hyperparameter to Fix\nElectra. To decide which hyperparameter to ﬁx,\nwe examine the best trial’s conﬁguration for the\noverﬁtting HPO runs (compared with the grid\nsearch performance). If there is a pattern in a\ncertain hyperparameter of all these conﬁgurations\n(e.g., warmup ratio below 0.1 for Electra), by ﬁxing\nsuch hyperparameters to the values of grid search,\nwe can exclude the other values which may be\nrelated to overﬁtting. We apply this analytical\nstrategy to the initial Electra results in Table 3.\nAmong the 72 runs, 9 runs overﬁt compared with\ngrid search. For each run, we list the hyperpa-\nrameter conﬁgurations of the best trial in Table 8\nof Appendix A.3. For Electra, we have skipped\nshowing weight decay in Table 8, because the HPO\nconﬁguration is never smaller than the grid conﬁgu-\nration, thus does not affect the result of the analysis.\nFor comparative purpose, we also list the hyperpa-\nrameter values of the best trial in grid search. To\nimprove the readability of Table 8, we use 4 dif-\nferent colors (deﬁned in Appendix A.3) to denote\nthe comparison between values of the best trial in\nHPO and values of the best trial in grid search.\nFrom Table 8, we observe that the warmup\nratios are often signiﬁcantly lower than 0.1. We\nskip the analysis on learning rate because its\nsearch space (log((2.99e-5,1.51e-4))) cannot be\nfurther reduced without losing coverage of the grid\nconﬁgurations or continuity; we also skip weight\ndecay because any trial’s value cannot be smaller\nthan 0. Following this empirical observation, we\nhypothesize that ﬁxing the warmup ratio to 0.1 can\nhelp reduce overﬁtting in Electra. We use Sfull\nto denote the original search space and S−wr to\ndenote the search space by ﬁxing the warmup ratio\nto 0.1. If HPO overﬁts in both Sfull and S−wr,\nthe procedure will reduce the search space to the\nminimal continuous space Smin containing the\ngrid search space, which searches for the learning\nrate only.\nRoBERTa. We apply the same analytical strat-\negy to the RoBERTa results in Table 3 and show\nthe hyperparameters of the best trials in Table 9.\nFor RoBERTa, we propose to ﬁx the values of two\nhyperparameters at the same time: the warmup ra-\ntio and the hidden dropout. We denote the search\nspace after ﬁxing them as S−wr−hdo. If HPO over-\nﬁts in both Sfull and S−wr−hdo, the procedure will\nreduce the search space to Smin which contains the\nlearning rate and batch size only.\n5.2 Execution Results of the Procedure\nIn this section, we apply the troubleshooting\nprocedure on the initial HPO results from Table 3\nand observe the execution paths. In Table 10\nand Table 11 of Appendix A.4, we list the full\nexecution results of the procedure for random\nsearch and random search + ASHA. Table 10&11\nhave included only the tasks where the HPO does\nnot succeed in the initial study. In Table 10&11,\nwe show the validation and test accuracy for the\nthree repetitions of HPO runs as well as their\naverage score.\nAn Example of Executing the Procedure. In Fig-\nure 4, we show an example of applying the pro-\ncedure on random search for Electra on RTE. In\nround 0, the validation and test accuracies of all\nthree repetitions are lower than grid search. That\nimplies RS needs more time budget, therefore we\nincrease the budget (marked as ↑res) for RS from\n1GST to 4GST. After the increase, overﬁtting is\ndetected in the 1st repetition of round 1 (valida-\ntion accuracy = 84.5, test accuracy = 74.6). We\nthus reduce the search space (marked as ↓space)\nfrom Sfull to S−wr. In round 2, the 1st repetition\nstill shows (weak) overﬁtting: RS has the same\n2292\nround 0 round 1 round 2 round 3\nval test val test val test val test\ngrid 84.1 76.8 ↑res ↓space ↓space\nrep1 81.9 76.1 84.5 74.6 84.1 76.1 84.8 75.3\nrep2 81.6 75.1 83.8 74.5 83.0 74.0 84.1 75.7\nrep3 83.0 75.7 83.4 74.7 82.3 73.1 83.8 75.2\nAvg 82.2 75.6 83.9 74.6 83.1 74.4 84.2 75.4\nTable 4: An example of executing the exper-\nimental procedure applied to random search for\nElectra on RTE. The grid search accuracy is de-\nnoted using the blue bold font . An HPO\nrun is highlighted in dark grey if it overﬁts and\nmedium grey if it overﬁts weakly .\nvalidation accuracy as grid search (84.1), a smaller\ntest accuracy (76.1), and a smaller validation loss\n(RS’s validation loss = 0.8233, grid search’s valida-\ntion loss = 0.9517). We thus continue reducing the\nsearch space to Smin, and overﬁtting is detected\nagain in the 1st repetition of round 3 (validation\naccuracy = 84.8, test accuracy = 75.3). After round\n3, the search space cannot be further reduced, so\nwe classify this case as ’HPO overﬁts task’.\nWe analyze the execution results in Table 10 and\n11 jointly as follows.\nEffects of Reducing the Search Space. From the\ntwo tables we can observe that reducing the search\nspace can be effective for controlling overﬁtting.\nIn WNLI (Electra), both algorithms outperform\ngrid search after reducing the search space once. In\nWNLI (RoBERTa), ASHA outperforms grid search\nafter reducing the search space twice. We can\nobserve a similar trend in MRPC (Electra), SST\n(Electra), RTE (RoBERTa), and CoLA (RoBERTa).\nHowever, for these cases, overﬁtting still exists\neven after we reduce the search space twice, i.e.,\nusing the minimal search space.\nEffects of Increasing the Time Budget . By\nobserving cases of increased budget in Table 10\nand 11, we can see that this strategy is generally ef-\nfective for improving the validation accuracy. After\nincreasing the time budget, in STS-B (Electra) all\nHPO methods outperform grid search’s validation\nand test accuracy; in SST (Electra-RS) and CoLA\n(RoBERTa) HPO outperforms grid search in only\nthe validation accuracy. In RTE (Electra) and\nQNLI (Electra), however, this increase is not\nenough for bridging the gap with grid search, thus\nHPO remains behind. For RTE (Electra), SST\n(Electra), QNLI (Electra), and CoLA (RoBERTa),\noverﬁtting happens after increasing the time budget\nfrom 1GST to 4GST. After reducing the search\nspace, we still observe overﬁtting in most cases.\nComparisons between RS and ASHA . By com-\nparing the results between random search and\nASHA in Table 10 and 11, we ﬁnd that before in-\ncreasing the budget, RS rarely outperforms ASHA\nin the validation accuracy; however, after the bud-\nget of both RS and ASHA increases to 4GST, the\nbest validation accuracy of RS has consistently\noutperformed ASHA, i.e., in all of RTE (Electra),\nSTS-B (Electra), SST (Electra), and QNLI (Elec-\ntra). That is, the increase in the time budget has\nled to more signiﬁcant (validation) increase in RS\nthan ASHA. This result may be caused by two\nreasons. First, at 1GST, ASHA already samples\na larger number of trials (Appendix A.2), which\nmay be sufﬁcient to cover its search space; on the\nother hand, RS cannot sample enough trials, thus\nincreasing the time budget is more helpful. Second,\nASHA may make mistake by pruning a good trial\nthat shows a bad performance at the beginning.\n5.3 Summary of the Main Findings\nIn Table 5, we list the ﬁnal execution results for\neach task in Electra and RoBERTa. Our main\nﬁndings can be summarized as follows. After\nincreasing the time budget and reducing the\nsearch space, HPO outperforms grid search in the\nfollowing cases: (1) in 3 cases (i.e., CoLA (Elec-\ntra), STS-B (Electra) and MNLI (Electra)), HPO\noutperforms grid search by using the full search\nspace, where STS-B needs more budget; (2) in 4\ncases (i.e., WNLI (Electra), WNLI (RoBERTa),\nMRPC (RoBERTa) and STS-B (RoBERTA)), HPO\nsucceeds after reducing the search space; (3) in\nthe other 7 cases, HPO cannot outperform grid\nsearch even after increasing the time budget and\nreducing the search space. This result shows that\nwhen searching in a continuous space surrounding\nthe recommended grid conﬁgurations, it can be\ndifﬁcult for existing automated HPO methods (e.g.,\nRandom Search, ASHA, Bayesian optimization)\nto outperform grid search (with manually tuned\ngrid conﬁgurations recommended by the language\nmodel) within a short amount of time; even if we\ncan identify a conﬁguration with good validation\nscore, most likely the test score is still worse than\n2293\ntask Execution Results\nWNLI All HPO succeed w/ 1GST, S−wr\nRTE RS overﬁts\nASHA and BO+ASHA TBD\nMRPC All HPO overﬁt\nCoLA All HPO succeed w/ 1GST, Sfull\nSTS-B All HPO succeed w/ 4GST, Sfull\nSST All HPO overﬁt\nQNLI All HPO TBD\nMNLI All HPO succeed w/ 1GST, Sfull\ntask Execution Results\nWNLI ASHA succeeds∗w/ 1GST, S−wr−hdo\nRS and BO+ASHA overﬁt\nRTE All HPO overﬁt\nMRPC ASHA succeeds∗w/ 1GST, S−wr−hdo\nRS and BO+ASHA overﬁt\nCoLA All HPO overﬁt\nSTS-B RS succeeds w/ 1GST, S−wr−hdo\nASHA and BO+ASHA succeed w/\n1GST, Smin\nTable 5: Final results of executing the troubleshoot-\ning procedure on Electra (top) RoBERTa (bottom). ∗\nmeans the risk of overﬁtting still exists based on the\nresult of BO+ASHA.\ngrid search.\nThe Total Running Time for the Procedure .\nThe execution for all experiments in Table 10\nand 11 took 6.8 ×4V100 GPU days. This\nis in contrast to the cost if we enumerate all 5\nfactors in Section 3, which is 16×4V100 GPU days.\nA Caveat on Results in Table 5 . For all study\nresults in this paper (i.e., Table 3, Table 10 and\nTable 11), we have repeated each HPO run three\ntimes. Therefore if a case succeed in Table 5,\nit is because no overﬁtting is detected in the 3\nrepetitions, if we ran more repetitions, the risk of\noverﬁtting can increase. In addition, all results\nare evaluated under transformers version=3.4.0\nand Ray version=1.2.0. If these versions change,\nresults in Table 5 may change.\nAn Analysis on the Relation between Overﬁt-\nting and Train/Validation/Test split. As overﬁt-\nting indicates a negative correlation between the\nvalidation and test accuracy, one hypothesis is that\noverﬁtting is caused by the different distribution of\nthe validation and test set. We thus compare HPO\nruns using the original GLUE spilt and a new split\nwhich uniformly partition the train/validation/test\ndata. The results can be found in Appendix A.5.\n6 Related Work\n6.1 Automated Hyperparameter\nOptimization\nHyperparameter optimization methods for generic\nmachine learning models have been studied for a\ndecade (Feurer and Hutter, 2019; Bergstra et al.,\n2011; Bergstra and Bengio, 2012; Swersky et al.,\n2013). Prior to that, grid search was the most com-\nmon tuning strategy (Pedregosa et al., 2011). It\ndiscretizes the search space of the concerned hy-\nperparameters and tries all the values in the grid. It\ncan naturally take advantage of parallelism. How-\never, The cost of grid search increases exponen-\ntially with hyperparameter dimensions. A simple\nyet surprisingly effective alternative is to use ran-\ndom combinations of hyperparameter values, es-\npecially when the objective function has a low ef-\nfective dimension, as shown in (Bergstra and Ben-\ngio, 2012). Bayesian optimization (BO) (Bergstra\net al., 2011; Snoek et al., 2012) ﬁts a probabilis-\ntic model to approximate the relationship between\nhyperparameter settings and their measured per-\nformance, uses this probabilistic model to make\ndecisions about where next in the space to acquire\nthe function value, while integrating out uncer-\ntainty. Since the training of deep neural networks\nis very expensive, new HPO methods have been\nproposed to reduce the cost required. Early stop-\nping methods (Karnin et al., 2013; Li et al., 2017,\n2020) stop training with unpromising conﬁgura-\ntions at low ﬁdelity (e.g., number of epochs) by\ncomparing with other conﬁgurations trained at the\nsame ﬁdelity. Empirical study of these methods\nis mostly focused on the vision or reinforcement\nlearning tasks, there has been few work focusing on\nNLP models. ASHA was evaluated on an LSTM\nmodel proposed in 2014 (Zaremba et al., 2014). In\n(Wang et al., 2015), the authors empirically studied\nthe impact of a multi-stage algorithm for hyper-\nparameter tuning. In (Zhang and Duh, 2020), a\nlook-up table was created for hyperparameter op-\ntimization of neural machine translation systems.\nIn BlendSearch (Wang et al., 2021), an economical\nblended search strategy was proposed to handle het-\nerogeneous evaluation cost in general and demon-\n2294\nstrates its effectiveness in ﬁne-tuning a transformer\nmodel Turing-NLRv2.7 Some existing work has\naddressed overﬁtting in HPO (L´evesque, 2018) or\nneural architecture search (Zela et al., 2020). For\nHPO, cross validation can help alleviate the overﬁt-\nting when tuning SVM (L´evesque, 2018), which is\nrarely applied in deep learning due to high compu-\ntational cost. For neural architecture search (Zela\net al., 2020), the solution also cannot be applied\nto our case due to the difference between the two\nproblems.\n6.2 Fine-tuning Pre-trained Language\nModels\nAs ﬁne-tuning pre-trained language models has\nbecome a common practice, existing works have\nstudied how to improve the performance of the ﬁne-\ntuning stage. Among them, many has focused on\nimproving the robustness of ﬁne-tuning. For exam-\nple, ULMFit (Howard and Ruder, 2018) shows that\nan effective strategy for reducing the catastrophic\nforgetting in ﬁne-tuning is to use the slanted tri-\nangular learning rate scheduler (i.e., using a non-\nzero number of warmup steps). Other strategies\nfor controlling overﬁtting in ﬁne-tuning include\nfreezing a part of the layers to reduce the number\nof parameters, and gradually unfreezing the lay-\ners (Peters et al., 2019), adding regularization term\nto the objective function of ﬁne-tuning (Jiang et al.,\n2020), multi-task learning (Phang et al., 2018). Ap-\nplying these techniques may reduce overﬁtting in\nour experiments; however, our goal is to compare\ngrid search and HPO, if these techniques are help-\nful, they are helpful to both. To simplify the com-\nparison, we thus focus on ﬁne-tuning the original\nmodel. Meanwhile, the performance of ﬁne-tuning\ncan be signiﬁcantly different with different choices\nof the random seeds (Dodge et al., 2020). To re-\nmove the variance from random seed, we have ﬁxed\nall the random seeds to 42, although HPO can be\nused to search for a better random seed. (Zhang\net al., 2021) identiﬁes the instability of ﬁne-tuning\nBERT model in few-sample cases of GLUE (i.e.,\nRTE, MRPC, STS-B, and CoLA). Similar to our\nwork, they also found that overﬁtting increases\nwhen searching for more trials. However, they\nhave not compared grid search with HPO. There\nare also many discussions on how to control over-\nﬁtting by tuning hyperparameters (in manual tun-\ning), e.g., learning rate (Smith and Le, 2017), batch\n7msturing.org\nsize (Smith et al., 2017), dropout rates (Srivastava\net al., 2014; Loshchilov and Hutter, 2019, 2018),\nwhich may help with designing a search space for\nHPO that overﬁts less.\n7 Conclusions, Discussions and Future\nWork\nOur study suggests that for the problem of ﬁne-\ntuning pre-trained language models, it is difﬁcult\nfor automated HPO methods to outperform manu-\nally tuned grid conﬁgurations with a limited time\nbudget. However, it is possible to design a system-\natic procedure to troubleshoot the performance of\nHPO and improve the performance. We ﬁnd that\nsetting the search space appropriately per model\nand per task is crucial. Having that setting auto-\nmated for different models and tasks is beneﬁcial to\nachieve the goal of automated HPO for ﬁne-tuning.\nFor example, one may consider automatically min-\ning the pattern from Table 8&9 to identify the hy-\nperparameters that likely cause overﬁtting. Further,\nfor the tasks remaining to be unsuitable for HPO,\nother means to reduce overﬁtting is required. One\npossibility is to use a different metric to optimize\nduring HPO as a less overﬁtting proxy of the target\nmetric on test data.\nPrevious work has shown that random seed is\ncrucial in the performance of ﬁne-tuning (Dodge\net al., 2020). Fine-tuning also beneﬁts from en-\nsembling or selecting a few of the best performing\nseeds (Liu et al., 2019). It would be interesting to\nstudy HPO’s performance by adding the random\nseed to the search space for future work.\nIn our study, the simple random search method\nstands strong against more advanced BO and early\nstopping methods. It suggests room for research-\ning new HPO methods specialized for ﬁne-tuning.\nA method that can robustly outperform random\nsearch with a small resource budget will be useful.\nIt is worth mentioning that although we ﬁnd\nHPO sometimes underperforms grid search, the\ngrid search conﬁgurations we study are the default\nones recommended by the pre-trained language\nmodels for ﬁne tuning, therefore they may be al-\nready extensively tuned. We may not conclude that\nHPO is not helpful when manual tuning has not\nbeen done. How to leverage HPO methods in that\nscenario is an open question.\n2295\nReferences\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase,\nTakeru Ohta, and Masanori Koyama. 2019. Op-\ntuna: A next-generation hyperparameter optimiza-\ntion framework. In SIGKDD International Con-\nference on Knowledge Discovery & Data Mining,\npages 2623–2631.\nJames Bergstra and Yoshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. The Jour-\nnal of Machine Learning Research, 13(1):281–305.\nJames S Bergstra, R´emi Bardenet, Yoshua Bengio, and\nBal´azs K´egl. 2011. Algorithms for hyper-parameter\noptimization. In Advances in neural information\nprocessing systems.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 4271–4282. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4171–4186, Minneapo-\nlis, Minnesota. Association for Computational Lin-\nguistics.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn Empirical Methods in Natural Language Process-\ning and the International Joint Conference on Natu-\nral Language Processing, pages 2185–2194, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint arXiv:2002.06305.\nMatthias Feurer and Frank Hutter. 2019. Hyperparam-\neter optimization. In Automated Machine Learning,\npages 3–33. Springer, Cham.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. DEBERTA: Decoding-\nenhanced bert with disentangled attention. In Inter-\nnational Conference on Learning Representations.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁca-\ntion. In Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 328–339, Melbourne, Australia. Association\nfor Computational Linguistics.\nMax Jaderberg, Valentin Dalibard, Simon Osindero,\nWojciech M Czarnecki, Jeff Donahue, Ali Razavi,\nOriol Vinyals, Tim Green, Iain Dunning, Karen Si-\nmonyan, et al. 2017. Population based training of\nneural networks. arXiv preprint arXiv:1711.09846.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efﬁcient ﬁne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2177–2190, Online. Association for Computational\nLinguistics.\nAmog Kamsetty. 2020. Hyperparameter Optimization\nfor HuggingFace Transformers: A guide. https:\n//medium.com/distributed-computing-wit\nh-ray/hyperparameter-optimization-for-\ntransformers-a-guide-c4e32c6c989b .\nZohar Karnin, Tomer Koren, and Oren Somekh. 2013.\nAlmost optimal exploration in multi-armed bandits.\nIn International Conference on Machine Learning,\npages 1238–1246. PMLR.\nJulien-Charles L´evesque. 2018. Bayesian hyperparam-\neter optimization: overﬁtting, ensembles and condi-\ntional spaces.\nLiam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekate-\nrina Gonina, Jonathan Ben-tzur, Moritz Hardt, Ben-\njamin Recht, and Ameet Talwalkar. 2020. A system\nfor massively parallel hyperparameter tuning. In\nMachine Learning and Systems.\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-\ntamizadeh, and Ameet Talwalkar. 2017. Hyperband:\nA novel bandit-based approach to hyperparameter\noptimization. The Journal of Machine Learning Re-\nsearch, 18(1):6765–6816.\nRichard Liaw, Eric Liang, Robert Nishihara, Philipp\nMoritz, Joseph E Gonzalez, and Ion Stoica.\n2018. Tune: A research platform for distributed\nmodel selection and training. arXiv preprint\narXiv:1807.05118.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Fixing weight\ndecay regularization in adam.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\n2296\nFabian Pedregosa, Ga ¨el Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-\ndre Passos, David Cournapeau, Matthieu Brucher,\nMatthieu Perrot, and ´Edouard Duchesnay. 2011.\nScikit-learn: Machine learning in python. JMLR,\n12:2825–2830.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? adapt-\ning pretrained representations to diverse tasks. In\nWorkshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 7–14, Florence, Italy. As-\nsociation for Computational Linguistics.\nJason Phang, Thibault F ´evry, and Samuel R Bow-\nman. 2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv preprint arXiv:1811.01088.\nSamuel L. Smith, Pieter-Jan Kindermans, and Quoc V .\nLe. 2017. Don’t decay the learning rate, increase the\nbatch size. In International Conference on Learning\nRepresentations.\nSamuel L. Smith and Quoc V . Le. 2017. A bayesian\nperspective on generalization and stochastic gradient\ndescent. In International Conference on Learning\nRepresentations.\nJasper Snoek, Hugo Larochelle, and Ryan P Adams.\n2012. Practical bayesian optimization of machine\nlearning algorithms. In Advances in neural informa-\ntion processing systems.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nKevin Swersky, Jasper Snoek, and Ryan P Adams.\n2013. Multi-task bayesian optimization. 26.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 353–355,\nBrussels, Belgium. Association for Computational\nLinguistics.\nChi Wang, Qingyun Wu, Silu Huang, and Amin Saied.\n2021. Economic hyperparameter optimization with\nblended search strategy. In International Confer-\nence on Learning Representations.\nLidan Wang, Minwei Feng, Bowen Zhou, Bing Xi-\nang, and Sridhar Mahadevan. 2015. Efﬁcient hyper-\nparameter optimization for nlp applications. In Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2112–2117.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Empirical Methods in Natural Language Pro-\ncessing: System Demonstrations, pages 38–45, On-\nline. Association for Computational Linguistics.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\nArber Zela, Thomas Elsken, Tonmoy Saikia, Yassine\nMarrakchi, Thomas Brox, and Frank Hutter. 2020.\nUnderstanding and robustifying differentiable archi-\ntecture search. In International Conference on\nLearning Representations.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT ﬁne-tuning. In International Confer-\nence on Learning Representations.\nXuan Zhang and Kevin Duh. 2020. Reproducible and\nefﬁcient benchmarks for hyperparameter optimiza-\ntion of neural machine translation systems. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:393–408.\n2297\nA Appendix\nA.1 HPO Checkpoint Settings\nIn this paper, we report the validation and test accu-\nracy of the best checkpoint (in terms of validation\naccuracy) of the best trial instead of the last check-\npoint of the best trial. While the default setting\nin Ray Tune uses the last checkpoint, when ﬁne-\ntuning pretrained language model without HPO,\nthe best checkpoint is more widely used than the\nlast checkpoint. To further study the difference be-\ntween the two settings, we compare their validation\nand test accuracy of grid search using Electra on\nthree tasks: WNLI, RTE and MRPC. The result\nshows that the validation and test accuracy of the\nbest checkpoint of the best trial are both higher\nthan those of the last checkpoint of the best trial.\nAs a result, we propose and advocate to report the\nbest checkpoint of all the trials for HPO ﬁne-tuning\npretrained language models. The checkpoint fre-\nquencies in our experiment are set to 10 per epoch\nfor larger tasks (SST, QNLI, and MNLI) and 5\nper epoch for smaller tasks (WNLI, RTE, MRPC,\nCoLA and STS-B), with lower frequency in smaller\ntasks to reduce the performance drop caused by fre-\nquent I/Os within a short time.\nA.2 Number of Trials Searched by HPO\nIn Table 6, we show the number of trials searched\nby each HPO algorithms in the initial comparative\nstudy ( Table 3).\nHPO RS ASHA BO+ASHA\nWNLI 4 12 12\nRTE 6 27 38\nMRPC 5 36 36\nCoLA 9 31 30\nSTS-B 4 31 33\nSST 5 33 30\nQNLI 4 26 24\nMNLI 7 31 27\nTable 6: Average numbers of trials searched by each\nHPO algorithm in the initial experiment on Electra.\nA.3 Choosing the Hyperparameter to Fix\nThe hyperparameters of the best trials in overﬁting\nruns are shown in Table 8 and Table 9. We use\ncolors to denote the comparison with the hyper-\nparameter value in grid search: dark grey if the\nvalue is higher than grid search; light grey if the\nvalue is lower than grid search.\nA.4 Execution Results of Procedure\nIn Table 10 and Table 11, we show the execution\nresults of applying the experimental procedure to\nElectra and RoBERTa respectively.\nA.5 An Analysis on Overﬁtting and\nTrain/Validation/Test split\nIn this paper, we have observed that HPO tends\nto overﬁt when the number of trials/time budget\nincreases. In other words, the higher the validation\nscore, the lower the test score. One hypothesis for\nthe reason behind this phenomenon is that the vali-\ndation set has a different distribution than the test\nset. Since GLUE is a collection of NLP datasets\nfrom different sources, it is unclear whether the\nvalidation and test set in all GLUE tasks share the\nsame distribution.\nOrigin Resplit\nvalidation test validation test\n93.3 93.3 91.9 91.8\n93.2 93.2 91.7 91.6\n93.2 93.1 91.6 91.1\n93.1 93.4 91.6 91.5\nTable 7: Comparison of the orders of validation and test\nscores for the original split of GLUE and resplit.\nTo observe whether HPO still overﬁts under a uni-\nformly random split, we have performed the follow-\ning experiment: we merge the training and valida-\ntion folds of QNLI in GLUE, randomly shufﬂe the\nmerged data, and resplit it into train/validation/test\nwith the proportion 8:1:1. We run random search,\nrank all trials based on the validation accuracy,\nand examine the Pearson correlation coefﬁcient\nbetween the top-4 trials’s validation and test ac-\ncuracies (the trials are ranked by the validation\naccuracy), which are listed in Table 7. For the\noriginal GLUE dataset, we also save the best\ncheckpoints of the top 4 trials and submit them\nto the GLUE website to get the test accuracies.\nThe Pearson coefﬁcient of the original dataset is\n(r = −0.1414, p= 0.858) while for resplit it is\n(r = 0.6602, p= 0.339). Thus one potential ex-\nplanation of the observed overﬁtting in this work\nis due to different distribution between validation\nand test data.\n2298\nHPO run val acc test acc lr wr bs hidd. do att. do\nMRPC, grid 92.3/89.2 91.1/87.9 1.0e-4 0.100 32 0.100 0.100\nMRPC, RS, rep 1 92.7/90.0 90.4/87.1 3.9e-5 0.014 16 0.050 0.063\nMRPC, RS, rep 2 93.4/90.9 90.6/87.6 4.3e-5 0.005 16 0.044 0.024\nMRPC, ASHA, rep 1 92.8/90.0 90.8/87.6 6.5e-5 0.075 16 0.038 0.090\nMRPC, ASHA, rep 2 93.4/90.9 90.5/87.4 3.1e-5 0.030 16 0.067 0.097\nMRPC, ASHA, rep 3 92.9/90.0 90.4/86.9 1.3e-4 0.066 32 0.097 0.015\nMRPC, Opt+ASHA, rep 1 93.0/90.4 90.7/87.5 6.4e-5 0.084 16 0.196 0.002\nMRPC, Opt+ASHA, rep 2 93.3/90.7 90.4/86.9 8.0e-5 0.010 32 0.031 0.108\nSST, grid 95.1 95.7 3.0e-5 0.100 32 0.100 0.100\nSST, RS, rep 1 95.4 95.6 3.1e-5 0.011 32 0.006 0.044\nSTS-B, grid 91.5/91.4 89.7/89.2 1.0e-4 0.100 32 0.100 0.100\nSTS-B, Opt+ASHA, rep 1 91.6/91.4 89.6/89.1 4.7e-5 0.015 32 0.028 0.082\nTable 8: Comparison between the hyperparameter values of the best trial of grid search and the best trials (in\nvalidation accuracy) of all the 9 overﬁtting HPO runs (out of 72) in the initial comparative study using Electra\n(Table 3). dark grey indicates the value is higher than grid search; light grey indicates the value is lower than\ngrid search\nHPO run val acc test acc lr wr bs hidd. do att. do wd\nWNLI,grid 56.3 65.1 - 0.060 - 0.100 0.100 0.100\nWNLI,RS,rep 3 60.6 64.4 1.8e-5 0.111 16 0.128 0.122 0.078\nCoLA,grid 65.1 61.7 3.0e-5 0.060 16 0.100 0.100 0.100\nCoLA,ASHA, rep 1 65.5 59.5 2.7e-5 0.020 32 0.090 0.197 0.180\nCoLA,Opt+ASHA,rep 1 65.4 59.4 2.3e-5 0.067 32 0.063 0.117 0.293\nRTE,grid 79.8 73.9 3.0e-5 0.060 16 0.100 0.100 0.100\nRTE,RS,rep 1 80.5 73.6 2.8e-5 0.085 16 0.025 0.173 0.142\nRTE,ASHA,rep 3 80.5 73.2 2.4e-5 0.022 16 0.053 0.137 0.016\nRTE,Opt+ASHA,rep 2 81.9 73.5 2.7e-5 0.024 32 0.083 0.190 0.094\nMRPC,grid 93.1/90.4 90.5/87.1 2.0e-5 0.060 16 0.100 0.100 0.100\nMRPC,RS,rep 2 93.2/90.7 89.6/86.1 2.4e-5 0.094 64 0.019 0.138 0.299\nMRPC,RS,rep 3 93.2/90.4 90.3/86.7 1.4e-5 0.003 16 0.011 0.062 0.176\nMRPC,ASHA,rep 3 93.3/90.7 90.3/86.8 2.7e-5 0.008 16 0.140 0.130 0.255\nMRPC,Opt+ASHA,rep 3 93.5/91.2 89.6/86.2 2.7e-5 0.036 16 0.094 0.153 0.291\nSTS-B,grid 91.2/90.8 89.3/88.4 2.0e-5 0.060 16 0.100 0.100 0.100\nSTS-B,ASHA,rep 1 91.3/91.0 89.0/88.2 2.0e-5 0.042 16 0.004 0.061 0.247\nSTS-B,ASHA,rep 2 91.4/91.1 89.0/88.2 2.1e-4 0.061 16 0.056 0.008 0.226\nSTS-B,Opt+ASHA,rep 1 91.3/90.9 89.1/88.2 2.7e-5 0.052 16 0.096 0.070 0.224\nTable 9: Comparison between the hyperparameter values of the best trial of grid search and the best trials (in\nvalidation accuracy) of all the 11 overﬁtting HPO runs (out of 45) in the initial comparative study using RoBERTa\n(Table 3). dark grey indicates the value is higher than grid search; light grey indicates the value is lower than\ngrid search\n2299\nRandom Search ASHA\nround 0 round 1 round 2 round 3 round 0 round 1 round 2 round 3\nval test val test val test val test val test val test val test val test\nWNLI\n56.3 65.1 ↓space ↓space\n57.7 62.3 57.7 65.8 57.7 63.0 59.2 65.8\n56.3 65.8 57.7 65.1 57.7 59.6 57.7 65.1\n56.3 65.1 57.7 65.1 56.3 65.1 57.7 65.8\n56.8 64.4 57.7 65.3 57.2 62.6 58.2 65.6\nRTE\n84.1 76.8 ↑res ↓space ↓space ↑res\n81.9 76.1 84.5 74.6 84.1 76.1 84.8 75.3 81.9 76.2 83.4 75.3\n81.6 75.1 83.8 74.5 83.0 74.0 84.1 75.7 75.5 72.1 81.9 73.9\n83.0 75.7 83.4 74.7 82.3 73.1 83.8 75.2 83.4 74.1 83.8 74.4\n82.2 75.6 83.9 74.6 83.1 74.4 84.2 75.4 80.3 74.1 83.0 74.5\nMRPC\n89.2 87.9 ↓space ↓space ↓space ↓space\n90.9 87.6 90.7 86.3 90.4 86.5 90.9 87.4 90.0 87.2 90.2 87.6\n90.0 87.1 90.2 87.2 90.7 86.5 90.0 86.9 90.4 87.8 90.9 88.3\n90.2 87.8 90.7 86.9 90.7 87.8 90.0 87.6 89.5 86.0 90.7 87.6\n90.4 87.5 90.5 86.8 90.6 87.4 90.3 87.3 90.4 87.0 90.6 87.8\nSTS-B\n91.4 89.2 ↑res ↑res\n90.8 89.1 91.5 89.4 91.3 89.2 91.5 89.8\n89.6 85.9 91.4 89.6 91.5 89.7 91.4 89.2\n90.1 87.7 91.5 89.9 91.0 88.3 91.4 89.2\n90.2 87.6 91.4 89.6 91.3 89.1 91.4 89.4\nSST\n95.1 95.7 ↓space ↑res ↓space ↑res ↓space ↓space\n95.4 95.6 93.2 93.8 96.0 94.7 95.6 95.2 95.4 95.8 95.5 95.3 95.5 95.2 95.2 94.9\n94.3 95.1 94.7 95.0 95.3 95.7 95.1 95.7 94.4 94.1 95.1 94.7 94.8 94.3 94.2 93.6\n94.5 94.6 95.8 95.7 95.5 95.8 95.0 94.5 95.0 94.9 95.4 95.4 94.5 93.5 94.8 94.5\n94.7 95.1 94.6 94.8 95.6 95.4 95.2 95.1 94.9 94.9 95.3 95.1 94.9 94.3 94.7 94.3\nQNLI\n93.5 93.5 ↑res ↑res\n93.0 92.9 93.2 93.4 92.5 92.4 93.4 93.2\n93.1 93.6 93.3 93.3 93.4 93.0 93.2 93.1\n92.9 92.5 93.3 93.1 93.4 93.4 93.2 93.0\n93.0 93.0 93.3 93.3 93.1 92.9 93.3 93.1\nTable 10: The execution results of applying the procedure on Electra. Each task’s grid search ac-\ncuracy is denoted using the blue bold font . An HPO run is highlighted in dark grey if it overﬁts\nand medium grey if it overﬁts weakly . The average of 3 repetitions is highlighted in\nlight grey if it outperforms grid search’s validation and test accuracy . For STS-B we only report the Spear-\nman correlation, for MRPC we only report the accuracy.\n2300\nRandom Search ASHA\nround 0 round 1 round 2 round 3 round 0 round 1 round 2 round 3\nval test val test val test val test val test val test val test val test\nWNLI\n56.3 65.1 ↓space ↓space ↓space ↓space\n60.6 64.4 62.0 64.4 57.7 62.3 59.2 65.1 59.2 65.1 57.7 65.8\n56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1\n56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1 56.3 65.1\n57.8 64.9 58.2 64.9 56.8 64.2 57.3 65.1 57.3 65.1 56.8 65.3\nRTE\n79.8 73.9 ↓space ↓space ↓space ↓space\n81.2 73.9 80.1 72.8 81.6 72.2 80.5 73.2 80.5 73.3 79.8 72.5\n80.5 73.6 81.2 72.9 75.5 72.1 80.2 74.9 82.0 72.9 79.1 73.4\n79.4 73.1 79.8 73.6 79.8 72.6 80.5 74.1 80.5 73.5 79.8 73.7\n80.4 73.5 80.4 73.1 78.9 72.3 80.8 74.1 80.5 73.3 79.5 73.2\nMRPC\n90.4 87.1 ↓space ↓space ↓space\n90.7 86.1 90.7 86.9 91.2 86.7 90.7 86.8 91.4 87.7\n90.4 86.7 90.4 88.0 90.2 87.6 90.4 87.4 90.4 87.2\n90.9 87.2 91.2 87.2 90.4 87.0 91.4 87.6 90.4 87.6\n90.7 86.7 90.8 87.4 90.6 87.1 90.8 87.3 90.8 87.5\nCoLA\n65.1 61.7 ↑res ↓space ↓space ↓space ↓space\n64.3 60.1 66.0 59.3 65.8 59.2 65.3 60.2 65.5 59.5 65.0 60.9 65.9 58.2\n64.6 60.5 65.0 60.5 65.0 61.7 65.4 62.5 63.6 58.8 62.9 58.4 63.9 58.9\n63.5 56.8 64.4 60.3 65.2 60.7 64.6 58.5 64.6 60.0 64.9 62.0 64.4 59.0\n64.1 59.1 65.1 60.0 65.3 60.5 65.1 60.4 64.5 59.4 64.3 60.4 64.7 58.7\nSTS-B\n90.8 88.4 ↓space ↓space ↓space\n90.8 88.3 91.0 88.9 91.1 88.2 90.9 88.3 90.8 88.6\n90.8 88.9 90.8 88.6 91.0 88.2 90.8 88.5 91.0 88.5\n91.2 88.7 90.9 88.9 90.7 88.5 90.9 88.4 90.9 88.7\n90.9 88.6 90.9 88.8 90.9 88.3 90.8 88.4 90.9 88.6\nTable 11: The execution results of applying the procedure on RoBERTa. Each task’s\ngrid search accuracy is denoted using the blue bold font . An HPO run is highlighted in\ndark grey if it overﬁts and medium grey if it overﬁts weakly . The average of 3 repetitions is highlighted\nin light grey if it outperforms grid search’s validation and test accuracy . For STS-B we only report the Spearman\ncorrelation, for MRPC we only report the accuracy.",
  "topic": "Hyperparameter",
  "concepts": [
    {
      "name": "Hyperparameter",
      "score": 0.855381429195404
    },
    {
      "name": "Computer science",
      "score": 0.7066587805747986
    },
    {
      "name": "Joint (building)",
      "score": 0.5517324805259705
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5234886407852173
    },
    {
      "name": "Natural language processing",
      "score": 0.5082599520683289
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5063101053237915
    },
    {
      "name": "Computational linguistics",
      "score": 0.47169947624206543
    },
    {
      "name": "Machine learning",
      "score": 0.33346083760261536
    },
    {
      "name": "Engineering",
      "score": 0.08921504020690918
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I108468826",
      "name": "Stevens Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}