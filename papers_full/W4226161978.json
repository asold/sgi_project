{
  "title": "Dependency-based Mixture Language Models",
  "url": "https://openalex.org/W4226161978",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2133882261",
      "name": "Zhixian Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101284925",
      "name": "Xiao-jun Wan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2885163090",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2962951611",
    "https://openalex.org/W2963021447",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2785896739",
    "https://openalex.org/W3100163144",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2888799392",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W3176607063",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2970378492",
    "https://openalex.org/W2061504941",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2020382207",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2963248348",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3175530221",
    "https://openalex.org/W2952802110",
    "https://openalex.org/W3105484636",
    "https://openalex.org/W3170110950",
    "https://openalex.org/W2739894144",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W2156024017",
    "https://openalex.org/W2791751435",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W3034552719",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2900260828",
    "https://openalex.org/W2950728969"
  ],
  "abstract": "Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2. In this paper, we introduce the Dependency-based Mixture Language Models. In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context. We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with self-attention. Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7758 - 7773\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nDependency-based Mixture Language Models\nZhixian Yang and Xiaojun Wan\nWangxuan Institute of Computer Technology, Peking University\nCenter for Data Science, Peking University\nThe MOE Key Laboratory of Computational Linguistics, Peking University\nyangzhixian@stu.pku.edu.cn\nwanxiaojun@pku.edu.cn\nAbstract\nVarious models have been proposed to incor-\nporate knowledge of syntactic structures into\nneural language models. However, previous\nworks have relied heavily on elaborate compo-\nnents for a specific language model, usually\nrecurrent neural network (RNN), which makes\nthemselves unwieldy in practice to fit into other\nneural language models, such as Transformer\nand GPT-2. In this paper, we introduce the\nDependency-based Mixture Language Models.\nIn detail, we first train neural language models\nwith a novel dependency modeling objective\nto learn the probability distribution of future\ndependent tokens given context. We then for-\nmulate the next-token probability by mixing\nthe previous dependency modeling probability\ndistributions with self-attention. Extensive ex-\nperiments and human evaluations show that our\nmethod can be easily and effectively applied to\ndifferent neural language models while improv-\ning neural text generation on various tasks.1\n1 Introduction\nSyntactic structures serve as the principle of how\nwords are correctly combined to form sentences.\nIt is widely acknowledged that learning syntactic\nstructures should improve neural text generation\n(Shen et al., 2018; Peng et al., 2019; Du et al.,\n2020). Even though current neural language mod-\nels, such as Transformer (Vaswani et al., 2017) and\nGPT-2 (Radford et al., 2019) have achieved out-\nstanding performance without explicitly modeling\nlatent syntactic structures, these models still fail to\nlearn the long-range syntactic dependencies (Kun-\ncoro et al., 2018; Xu et al., 2021).\nTo leverage explicit syntactic knowledge in nat-\nural language generation (NLG), many methods\nhave been proposed (Wu et al., 2017; Shen et al.,\n2018; Zhang et al., 2019; Kim et al., 2019; Du\n1Our code is available at https:\n//github.com/FadedCosine/\nDependency-Guided-Neural-Text-Generation\net al., 2020). We conclude from previous works\nthat knowledge of syntactic structures can bring\nfour advantages to neural language models:\n(1) Syntactic structures can be modeled to obtain\nbetter representations of natural language sentences\n(Jacob et al., 2018; Williams et al., 2018; Wang\net al., 2019).\n(2) Jointly training syntactic structure parsing\nand language modeling can contribute to each other\n(Shen et al., 2018; Dyer et al., 2016; Kim et al.,\n2019; Du et al., 2020; Shen et al., 2021b).\n(3) Syntactic structures can be used to directly\nmodel the composition of language (Socher et al.,\n2013; Casas et al., 2020) and help with the long-\nrange dependency problem by providing shortcuts\nfor gradient backpropagation (Chung et al., 2017).\n(4) Integrating syntactic structures into a neural\nnetwork can improve generalization via a better in-\nductive bias (Shen et al., 2019; Zhang et al., 2019).\nDespite these advantages, it is not trivial to in-\ncorporate knowledge of syntactic structures into\nneural language models effectively and efficiently.\nSeveral practical problems arise:\n(1) Previous works (Chung et al., 2017; Shen\net al., 2018; Dyer et al., 2016; Kim et al., 2019;\nShen et al., 2019) have relied heavily on elaborate\ncomponents for a specific language model, usually\nrecurrent neural network (RNN) (Sutskever et al.,\n2014). These methods are difficult to be adapted to\nother neural language models, such as Transformer\nand GPT-2.\n(2) If jointly modeling language modeling and\nsyntactic structure parsing, it will require much\nmore time/memory during training or inference.\nTo address these problems while keeping the ad-\nvantages, we explore incorporating knowledge of\nsyntactic structures in a different manner. In this\nwork, we propose a novel dependency modeling ob-\njective to train neural language models to directly\npredict the current token’s future dependent tokens\ngiven the history. We define thefuture dependent to-\n7758\nModels External Parameters? External Networks? Architecture Agnostic?\nRNNG (Dyer et al., 2016) Yes Yes No\nPRPN (Shen et al., 2018) Yes Yes No\nURNNG (Kim et al., 2019) Yes Yes No\nON-LSTM (Shen et al., 2019) Yes No No\nDMLM (Ours) No or Negligible No Yes\nTable 1: The difference between our DMLM and previous neural language models that incorporate knowledge of\nsyntactic structures. Previous models often require external networks and external Parameters. For example, PRPN\nconsists of three neural networks: Parsing Network, Reading Network and Predict Network. ON-LSTM is built\nupon a single LSTM, but it requires two additional gates in the LSTM cells, which leads to external parameters.\nAll these previous models can only be built upon RNN architecture. However, as an architecture-agnostic method,\nDMLM needs no external parameters or networks when built upon Transformer, while it only needs negligible\nexternal parameters when built upon RNN.\nkens of a specific token in a sentence as its children\nand parent in the dependency parse tree that will\nappear in the rest of the sentence. Further, we pro-\npose Dependency-based Mixture Language Models\n(DMLM) that, at each timestep, mixes the previ-\nous dependency modeling probability distributions\nwith self-attention to get the next-token probabil-\nity. As shown in Table 1, the proposed method can\nbe adapted to any neural language model without\nadding external networks or parameters.\nOur core idea can be illustrated in Figure 1 and\nFigure 2: when predicting the next-token \"indi-\ncate\" after reading \"red figures on the screen\", com-\nmon language models are easy to predict an in-\ncorrect word, such as \"indicates\", since the predic-\ntion of these models relies heavily on the recent\nword, \"screen\" in this case. However, our propose\nDMLM will directly look back into the long-range\ncontext, and select the next-token from all the fu-\nture dependent tokens predicted by previous tokens.\nAccording to the underlying dependency structure,\nDMLM pays different weights to different tokens’\nfuture dependent tokens. Thus, the model is more\nlikely to predict \"indicate\" since DMLM tends to\nthink of the next-token as a future dependent token\nof \"figures\" rather than \"screen\".\nWe conduct experiments with different neural\nlanguage models including LSTM (Hochreiter and\nSchmidhuber, 1997), Transformer (Vaswani et al.,\n2017), and GPT-2 (Radford et al., 2019) across dif-\nferent tasks in conditional text generation, uncon-\nditional text generation, and language modeling.\nThrough extensive experiments we demonstrate\nthat DMLM consistently improves the generation\nquality according to both human evaluations and\nautomatic metrics. Compared to other neural lan-\nguage models that incorporate syntactic knowledge,\nindicate\nfigures stocks\nred screen falling\non the\nROOT\nROOT red figures on the screen indicate falling stocks .\n.\nFigure 1: Example of dependency parse tree\nDMLM is architecturally simpler and easier to fit\ninto any neural language model, while possessing\nwide applicability to different text generation tasks.\n2 Methodology\nOur goal is to propose a simple yet effective method\nthat can improve neural text generation by learning\nfrom the underlying syntactic structure, and can fit\ninto any auto-regressive generation model without\nusing additional elaborate components. We first\nintroduce a novel dependency modeling objective\nto force the model to directly predict the future\ndependent tokens of the current token. Based on\nthe dependency modeling, we then present the pro-\nposed DMLM.\n2.1 Dependency Modeling\nIt has been a challenge to equip neural language\nmodels with the capability of modeling long-range\ndependency in text (Dai et al., 2019). In partic-\nular, previous works (Wu et al., 2017) observe\nthat vanilla RNN can hardly capture many sub-\ntle long-range token dependencies effectively. On\n7759\nred\n<BOS> red\nfigures\nfigures\non\non\nthe\nthe\nscreen\nscreen\n+\nAttention\nDistribution\nDependency \nModeling\nDistributions\n× × × × × ×\nindicate\nFinal\nPrediction\nFigure 2: Illustration of DMLM. For each timestep, the language model outputs a dependency modeling distribution,\nwhile the self-attention produces a dependency attention distribution over the context. And then, the next-token\nprobability is the sum of the context’s dependency modeling probability distributions weighed by the dependency\nattention scores. Best viewed in color.\nthe other hand, though self-attention mechanisms\ncan build direct connections between long-distance\ntoken pairs, it is still elusive for Transformer to be\naware of syntactic dependency structures while also\nobtaining strong language modeling performance\n(Shen et al., 2021a).\nThe current neural language models are mostly\ntrained purely using the language modeling objec-\ntive with Maximum Likelihood Estimation (MLE).\nWith the auto-regressive factorization, language\nmodeling can be reduced to modeling the condi-\ntional distribution of the next-token xt given the\ncontext x<t = {x1, . . . , xt−2, xt−1}. However, in\norder to make neural language models aware of\nlong-range dependency and syntactic structures,\nwe propose the dependency modeling objective to\ntrain models to learn the probability distribution\nof the future dependent tokens directly. Following\nAhmed et al. (2019), we define the future depen-\ndent tokens of a specific token in a sentence as its\nchildren and parent in the dependency parse tree\nthat will appear in the rest of the sentence. Taking\nFigure 1 as an example, the future dependent tokens\nof \"figures\" are \"screen\" and \"indicate\", since \"red\"\ndoes not appear after \"figures\" in this sentence.\nSpecifically, given a token sequence x =\n{x1, . . . , xT−1, xT } where T ∈ N denotes the\nsequence length, we first use dependency parser\nto generate a dependency tree. Then, we de-\nrive the future dependent tokens set Zt for\neach token xt−1, where Zt = {xi | i ≥\nt, xi is the child or parent of xt−1}. We train a lan-\nguage model θ to maximize the log-likelihood sum\nof tokens in Zt. This equals to minimize:\nLDM (θ) =−\nTX\nt=1\nX\nzt∈Zt\nlog pdep\nθ (zt | x<t) , (1)\nwhich is the dependency modeling objective.\n2.2 Dependency-based Mixture Language\nModels\nTo give a categorical probability distribution over\nthe next-token, a standard approach for the current\nneural language models is to encode the context\ninto a fixed-size vector followed by an output em-\nbedding layer and a softmax function.\nIn our case, given the context x<t, we\nfirst train the language model to directly learn\nthe probability distribution of xt−1’s future de-\npendent tokens pdep\nθ (w | x<t) by dependency\nmodeling (Section 2.1). We then propose\nDMLM (depicted in Figure 2) that mixes\ndependency modeling probability distributions\nPdep = {pdep\nθ (w | x<1) , . . . , pdep\nθ (w | x<t−1) ,\npdep\nθ (w | x<t)}. All the probability distributions in\nPdep are weighed by self-attention, and summed to\nobtain the final next-token probability distribution.\nWe can easily implement a self-attention in both\nTransformer-based and RNN-based language mod-\nels. For example, in Transformer and GPT-2, the\npenultimate layer seems to naturally learn align-\nments (Garg et al., 2019), so we use its average\nattention weights over all the attentions heads as\nthe dependency attention distribution. In RNN-\nbased models, inspired by Merity et al. (2017) and\n7760\nVaswani et al. (2017), at each timestep, we linearly\nproject the current hidden state ht ∈ RH to a query\nvector qt = WQht and a key vector kt = WKht,\nwhere WQ ∈ RH×H, WK ∈ RH×H, qt ∈ RH,\nand kt ∈ RH. To generate the dependency atten-\ntion, we compute the match between the query qt\nand the context’s keys{k1, . . . , kt−1, kt} by taking\nthe inner product, followed by a softmax to obtain\nthe dependency attention distribution:\ne(t) = {e(t)\n1 , . . . , e(t)\nt−1, e(t)\nt },\ne(t)\ni = qT\nt ki, 1 ≤ i ≤ t,\na(t) = softmax( e(t)\n√\nH\n),\na(t) = {a(t)\n1 , . . . , a(t)\nt−1, a(t)\nt },\n(2)\nwhere e(t) ∈ Rt, and a(t) ∈ Rt. We scale the dot\nproducts by 1√\nH following Vaswani et al. (2017).\nThe dependency attention distribution reveals\nwhich token in the context may have a strong de-\npendency relation with the token to be predicted.\nThus, the neural language model should pay more\nattention to previous tokens with high dependency\nattention scores, i.e., the next-token is more likely\nto be the future dependent token of those tokens\nin the context. Formally, the next-token probabil-\nity is the sum of the context’s dependency model-\ning probability distributions weighed by the depen-\ndency attention scores:\npθ (w | x<t) =\ntX\nτ=1\na(t)\nτ pdep\nθ (w | x<τ ) . (3)\nwhere pdep\nθ (w | x<τ ) is the probability distribution\nof xτ−1’s future dependent tokens, since till now\nthe neural language model is only trained by de-\npendency modeling. Then, we further finetune the\nneural language model using MLE, but with re-\nspect to our modified probability distribution given\nin Equation 3:\nLLM (θ) =−\nTX\nt=1\nlog pθ (xt | x<t) . (4)\nFor each timestep during inference, DMLM out-\nputs a dependency modeling distribution, and we\nstore it in a list. To predict the next-token, DMLM\napplies self-attention in Equation 2 to produce a\ndependency attention distribution over the context,\nand then the next-token probability can be calcu-\nlated by Equation 3, where the list preserves all the\npdep\nθ (w | x<τ ) , 1 ≤ τ ≤ t.\n3 Experiments\nDespite previous works mainly focusing on lan-\nguage modeling, it has always been a thorny is-\nsue whether better language models lead to better\nperformance in downstream tasks. Therefore, we\nshowcase the performance of our proposed DMLM\nin three different tasks: conditional text generation\n(Section 3.1), unconditional text generation (Sec-\ntion 3.2), and language modeling (Section 3.3).\nTo verify the effectiveness and architecturally\ngeneralizability of our method, we conduct the gen-\neration tasks with three dominant neural language\nmodels, including LSTM, Transformer and GPT-\n2. We prefix the base model name with \"DM-\" to\ndenote the corresponding Dependency-based Mix-\nture language model. Specifically, we adopt AWD-\nLSTM (Merity et al., 2018) as our base LSTM, and\nfurther compare our DM-LSTM with PRPN (Shen\net al., 2018) and ON-LSTM (Shen et al., 2019)\nwhich also incorporate knowledge of syntactic\nstructures, and are built on LSTM. In the same\ntask, we use exactly the same hyper-parameters and\nsetups for the pairs of base models and correspond-\ning DM-models. Other details of the experimental\nsetup for each task can be seen in Appendix A.\nFor all the tasks, we use a state-of-the-art parser,\nHPSG Parser2 (Zhou and Zhao, 2019) to get the\ndependency parse tree for each sentence in the\ndatasets. We discuss the impact of the dependency\nparser in Appendix B.\n3.1 Conditional Text Generation\nSetup We take the story ending generation as\nthe conditional text generation task, and eval-\nuate our method on the ROCStories corpus\n(Mostafazadeh et al., 2016), which consists of\n98,161 five-sentences. We follow the prepro-\ncessing3 of Kong et al. (2021) to randomly split\nROCStories by 8:1:1 for training/validation/test,\nrespectively, and delexicalize stories by mask-\ning all the male/female/unknown names with\n\"[MALE]\"/\"[FEMALE]\"/\"[NEUTRAL]\". We fi-\nnally get a word-level vocabulary with 31, 216\nunique tokens. The conditional text generation\ntask is to generate a reasonable ending given a four-\nsentence story context. For all models, we generate\nstories using nucleus sampling (Holtzman et al.,\n2https://github.com/DoodleJZ/\nHPSG-Neural-Parser\n3We use the preprocessed data in https://github.com/thu-\ncoai/Stylized-Story-Generation-with-Style-Guided-Planning\n7761\nModels UNION ↑ BERTScore ↑ B-1 ↑ B-2 ↑ D2 ↑ D3 ↑ SB-2 ↓ SB-3 ↓\nPRPN 83.37 29.11 21.45 6.84 13.22 33.50 95.17 86.76\nON-LSTM 82.18 29.41 22.16 7.33 13.93 35.71 94.98 85.80\nAWD-LSTM 82.98 29.57 22.23 7.31 14.07 35.71 94.92 85.88\nDM-LSTM 83.97⋆ 29.93 22.54 ⋆ 7.63⋆ 14.92 37.44 94.47 ⋆ 84.77⋆\nTransformer 81.39 27.64 21.28 7.01 17.48 42.30 93.18 81.52\nDM-Transformer 84.07⋆ 28.20⋆ 21.49 7.29 ⋆ 17.79 42.08 92.86⋆ 81.36⋆\nGPT-2 84.41 29.02 21.79 7.45 17.09 40.74 93.51 82.55\nDM-GPT-2 85.31⋆ 30.18⋆ 22.81⋆ 8.02⋆ 17.98 43.29 93.18 81.41 ⋆\nTable 2: Automatic evaluation results for the conditional text generation task on Rocstories dataset. ⋆ denotes that\nDM-model significantly outperforms the second best model for t-test (p-value<0.05).\nModels Grammaticality Logicality\nWin(%) Lose(%) Tie(%) κ Win(%) Lose(%) Tie(%) κ\nDM-LSTM vs. PRPN 36.2⋆ 14.5 49.3 0.225 56.5⋆ 17.5 26.0 0.306\nDM-LSTM vs. ON-LSTM 12.8⋆ 6.4 80.8 0.238 48.4⋆ 24.4 27.2 0.409\nDM-LSTM vs. AWD-LSTM 28.0⋆ 14.5 57.5 0.224 43.0⋆ 34.5 22.5 0.214\nDM-Transformer vs. Transformer 18.2⋆ 5.2 76.6 0.358 50.6⋆ 18.6 30.8 0.342\nDM-GPT-2 vs. GPT-2 20.4⋆ 5.0 74.6 0.374 50.6⋆ 18.8 30.6 0.224\nTable 3: Human evaluation results for the conditional text generation task on Rocstories dataset. κ denotes\nthe inter-annotator agreement Krippendorff’s alpha (Hayes and Krippendorff, 2007) score. ⋆ means statistical\nsignificance for Wilcoxon signed-rank test (p-value<0.01). Note that, it is relatively easy for both models to generate\na single sentence that is grammatically correct, so the rate of \"tie\" in Grammaticality is relatively high.\n2020) with p = 0.5.\nWe measure the generated story endings by the\nfollowing automatics metrics: (1) UNION (Guan\nand Huang, 2020): It is a learnable unreferenced\nmetric for evaluating the quality of generated sto-\nries; (2) BERTScore (Zhang et al., 2020): The met-\nric measures the semantic consistency between the\ngenerated and the referenced ones by BERT (De-\nvlin et al., 2019); (3) BLEU (B-n) (Papineni et al.,\n2002): BLEU evaluates n-gram overlap between\nthe generated stories and the references; (4) Dis-\ntinct (D-n) (Li et al., 2016): The proportions of\ndistinct n-grams in the outputs to evaluate the diver-\nsity of generated results. Since Distinct score will\nbecome extremely low for small n, we calculate it\nwith n = 2, 3; (5) Self-BLEU (SB-n) (Zhu et al.,\n2018): The metric is calculated by computing n-\ngrams (n = 2, 3) BLEU score of each generated\ntext with all other generated ones as references.\nSmaller Self-BLEU scores indicate better diversity.\nResults The experimental results of baselines\nand corresponding DM-models are shown in Ta-\nble 2. Note that we do not conduct significant\ntests on Distinct since it is a document-level met-\nric. We can see that, all the DM-models signifi-\ncantly outperform baseline models on almost all\nthe metrics. Furthermore, compared with PRPN\nand ON-LSTM, our DM-LSTM performs signifi-\nModels LM score ↓ RLM score ↓\nPRPN 5.24 5.75\nON-LSTM 5.20 5.59\nAWD-LSTM 5.18 5.64\nDM-LSTM 5.14 5.52\nTransformer 5.00 5.59\nDM-Transformer 4.97 5.49\nGPT-2 4.89 5.55\nDM-GPT-2 4.67 5.47\nTable 4: Results of global metrics for the unconditional\ntext generation task on EMNLP2017 WMT News.\ncantly better in all the metrics. This indicates that\nincorporating knowledge of syntactic structures in\nour proposed way can effectively contribute to both\nthe quality and diversity of the story ending gen-\neration. Moreover, no matter what the base model\nis, our DM-model can substantially improves the\nconditional text generation. This demonstrates that\nour method can be effectively adapted to different\nneural language models, such as the large scale lan-\nguage model, GPT-2, while previous models like\nON-LSTM can only be built on LSTM.\nHuman evaluation To further evaluate the\nfluency and logic of generated stories, follow-\ning (Guan et al., 2020), we conduct pair-wise com-\nparisons between DM-models and corresponding\n7762\nModels Nucleus-p\n0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nPRPN 41.48 45.77 55.32 64.23 83.98 109.3 172.09 302.57\nON-LSTM 37.46 42.98 46.16 56.69 72.36 98.06 152.60 274.43\nAWD-LSTM 37.97 41.80 48.74 57.45 71.77 94.22 146.40 289.13\nDM-LSTM 36.11 39.53 ⋆ 47.67 55.30 69.38 95.95 136.98⋆ 256.51⋆\nTransformer 45.37 46.36 50.90 60.27 70.74 91.65 125.46 222.27\nDM-Transformer 37.74⋆ 40.75⋆ 43.25⋆ 49.92⋆ 60.28⋆ 76.77⋆ 104.03⋆ 182.29⋆\nGPT-2 41.19 44.05 47.86 53.97 63.18 81.45 112.81 192.10\nDM-GPT-2 36.41⋆ 40.99⋆ 41.75⋆ 46.18⋆ 55.36⋆ 67.97⋆ 92.22⋆ 152.98⋆\nTable 5: GPT-2 Perplexity on 1, 000 random samples with various sampling hyper-parameters generated by models\ntrained on EMNLP2017 WMT News dataset. Nucleus sampling is used here with various p. ⋆ denotes that\nDM-model significantly outperforms the second best model for t-test (p-value<0.05).\nModels Human score ↑\nPRPN 0.380\nON-LSTM 0.278\nAWD-LSTM 0.365\nDM-LSTM 0.444\nTransformer 0.400\nDM-Transformer 0.448\nGPT-2 0.468\nDM-GPT-2 0.512\nReal data 0.688\nTable 6: Turing test results of the samples generated\nby models trained on EMNLP2017 WMT News dataset.\nTo reach a good trade-off between quality and diversity,\nwe adopt nucleus sampling with p = 0.7 for all the\nmodels to generate samples.\nbaselines. We randomly sample 100 story endings\nfrom each model. For each pair of stories (one by\nthe DM-model and the other by the baseline, along\nwith the beginning), five annotators are hired to\ngive a preference (win, lose, or tie) from the fol-\nlowing two aspects: (1) Grammaticality: whether\na story ending is natural and fluent; (2) Logicality:\nwhether a story is coherent to the given beginning\nand reasonable in terms of causal and temporal\ndependencies in the context. The detailed question-\nnaire and other details are shown in Appendix D.\nThe average win/lose/tie rates of the human eval-\nuation are shown in Table 3. To measure the inter-\nannotator agreement, we calculate Krippendorff’s\nalpha (Hayes and Krippendorff, 2007) for each\npair-wise comparison, and all the results are fair\nagreement (0.2 ≤ κ ≤ 0.4) or moderate agreement\n(0.4 ≤ κ ≤ 0.6). The results show that our DM-\nmodels significantly outperform baseline models\nin both the grammaticality and logicality.\n3.2 Unconditional Text Generation\nSetup We perform experiments of unconditional\ntext generation on EMNLP2017 WMT News\ndataset4. We use the preprocessed data of a recent\nwork5 (Caccia et al., 2020) that contains 5, 268\ndistinct words with maximum sentence length\n51. The training/validation/test set consists of\n268, 586/10, 000/10, 000 sentences.\nFollowing Caccia et al. (2020), we evaluate the\nmodels with the global metrics (Semeniuta et al.,\n2018): (1) Language Model score (LM score) :\nWe use the oracle Language Model to evaluate\nthe negative log-likelihood of generated text as the\nmetric to reflect quality; (2) Reverse Language\nModel score (RLM score) We train a new Lan-\nguage Model on the generated text, and then eval-\nuate the negative log-likelihood of a held-out set\nof real text. This metric can measure text diversity\nsince the generated text with better diversity would\nhave a broader coverage over the real data space,\nand the new Language Model can be trained better,\nthus leading to lower RLM score. Both the LM\nscore and RLM score are usually evaluated on the\nsentences generated by purely random sampling.\nBesides, to further measure the generation fluency,\nwe directly use the public GPT-2 checkpoint of pre-\ntrained parameters without finetuning to calculate\nGPT-2 Perplexity of generated samples.\nResults Table 4 shows the results of global met-\nrics obtained by various models. All the DM-\nmodels again outperform the baselines. The consis-\ntently lower LM scores indicate that the generated\n4http://statmt.org/wmt17/\ntranslation-task.htm\n5https://github.com/pclucas14/\nGansFallingShort/tree/master/real_data_\nexperiments/data/news\n7763\nModels #Params Dev PPL Test PPL\nPointer Sentinel-LSTM (Merity et al., 2017) 21M 72.4 70.9\nRNNG (Dyer et al., 2016) - - 88.7\nVariational RHN (Zilly et al., 2017) 23M 67.9 65.4\nPRPN (Shen et al., 2018) - - 62.0\nFraternal dropout (Zolna et al., 2018) 24M 58.9 56.8\nURNNG (Kim et al., 2019) - - 85.9\nON-LSTM (Shen et al., 2019) 25M 58.3 56.2\nAWD-LSTM (Merity et al., 2018) 24M 60.0 57.3\nDM-LSTM (Ours) 24M 58.6 56.2\nAWD-LSTM-MoS(Yang et al., 2018) 22M 56.5 54.4\nAWD-LSTM-DOC(Takase et al., 2018) 23M 54.1 52.4\nTable 7: Various language models’ perplexity evaluated on validation and test sets of Penn Treebank dataset. Yang\net al. (2018) and Takase et al. (2018) focus on improving the softmax of LSTM LM, which are orthogonal to ours.\nsentences of DM-models are of better quality, while\nthe consistently lower RLM scores also demon-\nstrate that DM-models can generate more diverse\nsentences meanwhile.\nIn addition, each model is used to generate1, 000\nsentences with various sampling hyper-parameters,\nand GPT-2 Perplexity is further calculated. As\nshown in Table 5, our proposed method can make\nneural language models perform significantly bet-\nter in terms of generation fluency. In particular,\nTransformer-based models can gain more signifi-\ncant improvement from DMLM. We conjecture that\nthis is because, in our implementation, we directly\nuses the penultimate multi-head attention layer of\nTransformer to obtain the dependency attention dis-\ntribution of DMLM. Thus, it can easily inherit all\nthe strengths of Transformer-based models.\nHuman evaluation Following previous\nwork (Yu et al., 2017; Guo et al., 2018), we con-\nduct a Turing test to further evaluate the generated\ntext. In practice, we mix 100 randomly sampled\nsentences from each model, and another 100 sen-\ntences from the real test set. Five annotators are\nhired to judge whether each of the 900 sentences\nis created by human or machines. Each sentence\ngets +1 score when it is regarded as a real one, and\n0 score otherwise. The detailed questionnaire and\nother details are shown in Appendix D.\nThe average score for each model is shown in\nTable 6, from which we can see all the DM-models\nsurpass the baselines. Both automatic evaluations\nand human evaluations indicate that DMLM can\nhelp neural language models generate more read-\nable, fluent, and natural sentences.\n3.3 Language Modeling\nSetup We evaluate the proposed method with\nthe word-level language modeling task by measur-\ning Perplexity (PPL) on the Penn Treebank (PTB)\n(Marcus et al., 1993; Mikolov et al., 2012) corpora.\nThe PTB dataset has a vocabulary size of 10, 000\nunique words, and the training/validation/test set\nconsists of 42, 068/3, 370/3, 761 sentences.\nFor this task, we mainly implement the DMLM\non the RNN-based language model, i.e., AWD-\nLSTM (Merity et al., 2018). For a fair compari-\nson, our DM-LSTM uses exactly the same hyper-\nparameters and setups as AWD-LSTM. Since\nTransformer-based models’ strong performance\nrelies on training with large datasets, it will per-\nform worse than random when trained on a small\ndataset (Shen et al., 2021a). We still report\nTransformer-based models’ language modeling re-\nsults on PTB in Appendix C.\nResults We compare our method with its base\nmodel, AWD-LSTM, and we report the results\nalong with other state-of-the-art models in Table 7.\nCompared with the AWD-LSTM, our DM-LSTM\nreduces the perplexity by 1.4 on the validation\nset and 1.1 on the test set, indicating that incor-\nporating knowledge of syntactic structures in our\nproposed manner can substantially improve lan-\nguage modeling. Compared with other models\nthat also leverage syntactic knowledge, our DM-\nLSTM strongly outperforms RNNG, PRPN, and\nURNNG. Moreover, though DM-LSTM does not\nmake any changes to the architecture of the AWD-\nLSTM language model, it still achieves a compara-\nble perplexity with ON-LSTM. Note that, since our\nmethod is model-agnostic, it can be harmonically\n7764\n/uni00000035/uni00000032/uni00000032/uni00000037\n/uni00000055/uni00000048/uni00000047\n/uni00000049/uni0000004c/uni0000004a/uni00000058/uni00000055/uni00000048/uni00000056\n/uni00000052/uni00000051/uni00000057/uni0000004b/uni00000048\n/uni00000056/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051/uni0000004c/uni00000051/uni00000047/uni0000004c/uni00000046/uni00000044/uni00000057/uni00000048/uni00000049/uni00000044/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000056/uni00000057/uni00000052/uni00000046/uni0000004e/uni00000056\n/uni00000055/uni00000048/uni00000047\n/uni00000049/uni0000004c/uni0000004a/uni00000058/uni00000055/uni00000048/uni00000056\n/uni00000052/uni00000051\n/uni00000057/uni0000004b/uni00000048\n/uni00000056/uni00000046/uni00000055/uni00000048/uni00000048/uni00000051\n/uni0000004c/uni00000051/uni00000047/uni0000004c/uni00000046/uni00000044/uni00000057/uni00000048\n/uni00000049/uni00000044/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni0000004a\n/uni00000056/uni00000057/uni00000052/uni00000046/uni0000004e/uni00000056\n/uni00000011\n10 10\n10 8\n10 6\n10 4\n10 2\n100\nFigure 3: Visualization of dependency attention distri-\nbutions. We left-shift the sentence by one step in the\ny-axis to better display the attention between the pre-\ndicted next-token and the context in each row.\ncombined with other state-of-the-art models, such\nas MoS (Yang et al., 2018) and DOC (Takase et al.,\n2018).\n4 Discussion\n4.1 Visualization\nWe show how our proposed method works by vi-\nsualizing the dependency attention distributions.\nWe use DM-Transformer to generate a sentence:\n\"red figures on the screen indicate falling stocks.\"\nFor each generation step, we record this step’s de-\npendency attention distribution. When we finally\ngenerate the whole sentence, we get 9 distributions\nand plot Figure 3 from them. Each row in Fig-\nure 3 shows the dependency attention distribution\nof the model when generating the corresponding\nY-axis token. When predicting the token \"indicate\",\nDMLM pays great attention to \"figures\". This is\nbecause these two tokens have a direct dependency\nconnection in the dependency parse tree, and our\nmethod successfully captures this relationship. In\naddition, DMLM also helps the model better orga-\nnize dependency information when the next-tokens,\nsuch as \"screen\" and \"stocks\", have dependencies\non more than one token in the context.\n4.2 Case Study\nWe perform case studies for a better understanding\nof the model performance. Table 8 provides ex-\namples of conditional text generation produced by\nour DM-models and other baselines. Obviously, all\nthe DM-models can generate more reasonable and\ncoherent story endings. Additionally, some exam-\nples of unconditional text generation are shown in\nTable 9 and Appendix E. These examples show that\nour DMLM can help base models generate more\nreasonable, readable, fluent, and natural sentences.\n4.3 Computational Complexity\nCompared with vanilla RNN, our DM-RNN indeed\nincreases the computational complexity from O(T)\nto O(T2). In practice, we can follow Merity et al.\n(2017) to set a context window that allows DMLM\nlooks L timesteps into the past at most, where L is\nthe context length. However, our DMLM can effi-\nciently apply to Transformer-based models without\nadditional computational complexity.\n5 Related Works\nMany previous studies have shown that leveraging\nthe knowledge of syntactic structures can improve\nNLG (Chelba, 1997; Roark, 2001; Emami and Je-\nlinek, 2005; Buys and Blunsom, 2015). Mirowski\nand Vlachos (2015) incorporated syntactic depen-\ndencies into the RNN formulation, but they limited\nthe scope to the scoring of complete sentences, not\nto next word prediction. Some other efforts have\nbeen done to integrate dependency structure into\nneural machine translation (NMT) from both the\nsource and target side. Eriguchi et al. (2016) pro-\nposed a tree-to-sequence attentional NMT model\nwhere source-side parse tree was used. Wu et al.\n(2017) involved target syntactic trees into NMT\nmodel to jointly learn target translation and depen-\ndency parsing. Casas et al. (2020) introduced a\nsyntactic inductive bias to NLG in an iterative non-\nautoregressive way.\nFor neural language models, recently, Dyer et al.\n(2016) proposed recurrent neural network gram-\nmar (RNNG) to jointly model syntax and sur-\nface structure by incrementally generating a syn-\ntax tree and sentence. Subsequent work (Kim\net al., 2019) extended the model to an unsuper-\nvised version. Shen et al. (2018) introduced the\nParsing-Reading-Predict Networks (PRPN) to cal-\nculate syntactic distances among words and use\nself-attention to compose previous states. Its sub-\nsequent work (Shen et al., 2019) transferred the\ndistance notion to LSTM cell, and introduced Or-\ndered Neurons LSTM (ON-LSTM).\nHowever, all these methods, mainly based on\nRNN (Sutskever et al., 2014), incorporate knowl-\n7765\nStory context: [FEMALE] bought packets of vegetable seeds from the store . she dug up the dirt in her garden .\n[FEMALE] planted onions , cilantro , and tomatoes . [FEMALE] watered the garden every night .\nGolden Text: by the end of the summer [FEMALE] had enough vegetables to make salsa .\nPRPN: she got to work in the morning and was happy to have a garden .\nON-LSTM: [FEMALE] planted the plants and made it a huge success .\nA WD-LSTM: [FEMALE] was happy to be helping her plants .\nDM-LSTM: soon , [FEMALE] had enough vegetables to grow in her garden !\nTransformer: she went to the store to buy the seeds .\nDM-Transformer: soon , [FEMALE] had her garden full of vegetables !\nGPT-2: [FEMALE] ’s garden grew very quickly and dry .\nDM-GPT-2: [FEMALE] now has fresh fruits and vegetables in her garden .\nTable 8: Examples of conditional text generation on ROCStories dataset.\nGolden Text: what this group does is to take down various different websites it believes to be criminal and\nleading to terrorist acts .\nPRPN: the right point to pay for the purchase of a bike , that ’ s all we want to do to build , build together\nthe support that i need to get here .\nON-LSTM: it ’ s great to know that my experience has changed my mind because i ’ m not going to work\nbecause i ’ ve had to talk about that .\nA WD-LSTM: this is a tragic attack and it is understood that the pair will come up with a package of documents\nwhich may be possible .\nDM-LSTM: the win over bernie sanders was an emotional moment for clinton , who was running in the\ngeneral election , though she lost their state of vermont .\nTransformer: ’ i ’ ve just been in that position so i ’ ve never seen anything like this before , but it ’ s something\ni have to say and i ’ m going to go to and win this series .\nDM-Transformer: in the second quarter of 2015 , the outlook for consumer spending rose 8 . 6 per cent , but for the\nfourth quarter , the company said it expects to expand by 0 . 7 per cent .\nGPT-2: if i had said a bit of pressure , i would probably be in a different position if i was a coach .\nDM-GPT-2: they ’ ve also said that it ’ s difficult to know how many emails clinton actually sent to her in\nrecent weeks or whether she would be the nominee .\nTable 9: Examples of unconditional text generation on EMNLP2017 WMT News dataset.\nedge of syntactic structures by introducing complex\narchitectural changes. Therefore, it can get very\nunwieldy to adapt them to other neural language\nmodels, such as Transformer and GPT-2.\n6 Conclusion\nIn this paper, we introduce Dependency-based\nMixture Language Models, which can incorpo-\nrate knowledge of dependency structures into ar-\nbitrary auto-regressive generation models without\nany changes to the original architectures. Both\nautomatic and human evaluation results in exten-\nsive experiments across different tasks and differ-\nent architectures demonstrate the effectiveness and\ngeneralizability of our method.\nIn the future, we will explore to incorporate the\ndependency labels into our method, and combine\nour DMLM with more neural language models.\nSecond, we would like to integrate other linguistic\nknowledge, such as constituency structures and\nsemantic information, into neural language models\nin our manner.\nAcknowledgements\nThis work was supported by National Key R&D\nProgram of China (No.2018YFB1005100), Bejing\nAcademy of Artificial Intelligence (BAAI) and\nState Key Laboratory of Media Convergence Pro-\nduction Technology and Systems. We appreciate\nthe anonymous reviewers for their helpful com-\nments. Xiaojun Wan is the corresponding author.\nReferences\nMahtab Ahmed, Muhammad Rifayat Samee, and\nRobert E. Mercer. 2019. You only need attention\nto traverse trees. In Proceedings of the 57th Confer-\nence of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2,\n2019, Volume 1: Long Papers, pages 316–322.\n7766\nJan Buys and Phil Blunsom. 2015. Generative incre-\nmental dependency parsing with neural networks. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing of the Asian Federation of Natural Lan-\nguage Processing, ACL 2015, July 26-31, 2015, Bei-\njing, China, Volume 2: Short Papers, pages 863–869.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin. 2020.\nLanguage gans falling short. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020.\nNoe Casas, José A. R. Fonollosa, and Marta R. Costa-\njussà. 2020. Syntax-driven iterative expansion lan-\nguage models for controllable text generation. In\nProceedings of the Fourth Workshop on Structured\nPrediction for NLP@EMNLP 2020, Online, Novem-\nber 20, 2020, pages 1–10.\nCiprian Chelba. 1997. A structured language model. In\n35th Annual Meeting of the Association for Compu-\ntational Linguistics and 8th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, Proceedings of the Conference, 7-12 July\n1997, Universidad Nacional de Educación a Distan-\ncia (UNED), Madrid, Spain, pages 498–500.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186.\nWenyu Du, Zhouhan Lin, Yikang Shen, Timothy J.\nO’Donnell, Yoshua Bengio, and Yue Zhang. 2020.\nExploiting syntactic structure for better language\nmodeling: A syntactic distance approach. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 6611–6628.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural net-\nwork grammars. In NAACL HLT 2016, The 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, San Diego California, USA,\nJune 12-17, 2016, pages 199–209.\nAhmad Emami and Frederick Jelinek. 2005. A neu-\nral syntactic language model. Mach. Learn., 60(1-\n3):195–227.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing, EMNLP-IJCNLP 2019, Hong Kong, China,\nNovember 3-7, 2019, pages 4452–4461.\nJian Guan, Fei Huang, Minlie Huang, Zhihao Zhao,\nand Xiaoyan Zhu. 2020. A knowledge-enhanced\npretraining model for commonsense story generation.\nTrans. Assoc. Comput. Linguistics, 8:93–108.\nJian Guan and Minlie Huang. 2020. UNION: an un-\nreferenced metric for evaluating open-ended story\ngeneration. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020,\npages 9157–9166.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2018. Long text generation via\nadversarial training with leaked information. In Pro-\nceedings of the Thirty-Second AAAI Conference on\nArtificial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artificial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artificial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 5141–\n5148.\nAndrew F Hayes and Klaus Krippendorff. 2007. An-\nswering the call for a standard reliability measure for\ncoding data. Communication methods and measures,\n1(1):77–89.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In In 8th International Conference on\nLearning Representations.\nAthul Paul Jacob, Zhouhan Lin, Alessandro Sordoni,\nand Yoshua Bengio. 2018. Learning hierarchi-\ncal structures on-the-fly with a recurrent-recursive\nmodel for sequences. In Proceedings of The Third\n7767\nWorkshop on Representation Learning for NLP ,\nRep4NLP@ACL 2018, Melbourne, Australia, July\n20, 2018, pages 154–158.\nYoon Kim, Alexander M. Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and Gábor Melis. 2019. Unsuper-\nvised recurrent neural network grammars. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019,\nVolume 1 (Long and Short Papers), pages 1105–1117.\nXiangzhe Kong, Jialiang Huang, Ziquan Tung, Jian\nGuan, and Minlie Huang. 2021. Stylized story\ngeneration with style-guided planning. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6,\n2021, volume ACL/IJCNLP 2021 ofFindings of ACL,\npages 2430–2436.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLstms can learn syntax-sensitive dependencies well,\nbut modeling structure makes them better. In Pro-\nceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2018,\nMelbourne, Australia, July 15-20, 2018, Volume 1:\nLong Papers, pages 1426–1436.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL HLT 2016, The 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSan Diego California, USA, June 12-17, 2016, pages\n110–119.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank. Comput. Lin-\nguistics, 19(2):313–330.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and optimizing LSTM\nlanguage models. In 6th International Conference\non Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings.\nTomáš Mikolov et al. 2012. Statistical language models\nbased on neural networks. Presentation at Google,\nMountain View, 2nd April, 80:26.\nPiotr Mirowski and Andreas Vlachos. 2015. Depen-\ndency recurrent neural language models for sentence\ncompletion. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing of the Asian Federation of\nNatural Language Processing, ACL 2015, July 26-31,\n2015, Beijing, China, Volume 2: Short Papers, pages\n511–517.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James F. Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding\nof commonsense stories. In NAACL HLT 2016, The\n2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, San Diego California,\nUSA, June 12-17, 2016, pages 839–849.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In In Proceedings\nof the 40th Annual Meeting of the Association for\nComputational Linguistics, pages 311–318.\nHao Peng, Roy Schwartz, and Noah A. Smith. 2019.\nPalm: A hybrid parser and language model. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 3642–3649.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nBrian Roark. 2001. Probabilistic top-down parsing and\nlanguage modeling. Comput. Linguistics, 27(2):249–\n276.\nStanislau Semeniuta, Aliaksei Severyn, and Sylvain\nGelly. 2018. On accurate evaluation of gans for lan-\nguage generation. CoRR, abs/1806.04936.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron C. Courville. 2018. Neural language mod-\neling by jointly learning syntax and lexicon. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron C. Courville. 2019. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks. In\n7th International Conference on Learning Represen-\ntations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019.\nYikang Shen, Shawn Tan, Alessandro Sordoni, Siva\nReddy, and Aaron C. Courville. 2021a. Explicitly\nmodeling syntax in language models with incremen-\ntal parsing and a dynamic oracle. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 1660–1672.\n7768\nYikang Shen, Yi Tay, Che Zheng, Dara Bahri, Don-\nald Metzler, and Aaron C. Courville. 2021b. Struct-\nformer: Joint unsupervised induction of dependency\nand constituency structure from masked language\nmodeling. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 7196–7209.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y . Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2013, 18-21 October 2013, Grand Hyatt\nSeattle, Seattle, Washington, USA, A meeting of SIG-\nDAT, a Special Interest Group of the ACL , pages\n1631–1642.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Advances in Neural Information Processing Sys-\ntems 27: Annual Conference on Neural Information\nProcessing Systems 2014, December 8-13 2014, Mon-\ntreal, Quebec, Canada, pages 3104–3112.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 4599–4609.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In In Advances in Neural Information\nProcessing Systems, pages 5998–6008.\nYau-Shian Wang, Hung-yi Lee, and Yun-Nung Chen.\n2019. Tree transformer: Integrating tree structures\ninto self-attention. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7,\n2019, pages 1061–1070.\nAdina Williams, Andrew Drozdov, and Samuel R. Bow-\nman. 2018. Do latent tree learning models identify\nmeaningful structure in sentences? Trans. Assoc.\nComput. Linguistics, 6:253–267.\nShuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li,\nand Ming Zhou. 2017. Sequence-to-dependency neu-\nral machine translation. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 698–707.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun\nShou, Ming Gong, Wanjun Zhong, Xiaojun Quan,\nDaxin Jiang, and Nan Duan. 2021. Syntax-enhanced\npre-trained model. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 5412–5422.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial\nnets with policy gradient. In Proceedings of the\nThirty-First AAAI Conference on Artificial Intelli-\ngence, February 4-9, 2017, San Francisco, Califor-\nnia, USA, pages 2852–2858.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020.\nXinyuan Zhang, Yi Yang, Siyang Yuan, Dinghan Shen,\nand Lawrence Carin. 2019. Syntax-infused vari-\national autoencoder for text generation. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 2069–2078.\nJunru Zhou and Hai Zhao. 2019. Head-driven phrase\nstructure grammar parsing on penn treebank. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 2396–2408.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\npages 1097–1100.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Kout-\nník, and Jürgen Schmidhuber. 2017. Recurrent high-\nway networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning, ICML\n2017, Sydney, NSW, Australia, 6-11 August 2017 ,\nvolume 70, pages 4189–4198.\nKonrad Zolna, Devansh Arpit, Dendi Suhubdy, and\nYoshua Bengio. 2018. Fraternal dropout. In 6th\nInternational Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings.\n7769\nA Experimental Setup\nAll the algorithms are implemented in Pytorch and\ntrained on a machine with 8 NVIDIA GTX 2080Ti\nGPUs.\nA.1 Conditional Text Generation\nThe dataset statistics of ROCStories dataset is re-\nported in Table 10.\nTrain Validation Test\n#Stories 78,529 9,816 9,816\nTable 10: Statistics of ROCStories dataset.\nIn this task, both the DM-LSTM and base LSTM\nare built on a AWD-LSTM language model with an\nembedding size of 400 and hidden layer units 1150.\nThe dropout rates are 0.4, 0.25, 0.4 for the output\nof the last layer, outputs between LSTM layers, and\ninput embedding layers, respectively. The weight\ndropout for the RNN hidden to hidden matrix is\n0.5, and the dropout rate to remove words from\nembedding layer is 0.1. The context length for\nDM-LSTM is set to 56. For PRPN and ON-LSTM,\nwe keep their original settings.\nIn this task, all the models are trained on a singe\nGPU with learning rate 30, weight decay 1.2e − 6.\nLSTM baselines are trained for 500 epochs with\nbatch size 100. DM-LSTM is first trained by de-\npendency modeling objective for 100 epochs with\nbatch size 80, and then by language modeling in\nEquation 4 for 400 epochs with batch size 60 due\nthe computational budgets limit.\nFor both the DM-Transformer and base Trans-\nformer, we use a standard 6-layer Transformer lan-\nguage model with 8 attention heads, embedding\ndimension 512, projection dimension 2048 and\ndropout rate 0.1. During training, we use Adam\noptimizer with β1 = 0.9, β2 = 0.98, weight decay\n0.01 and learning rate 5e − 4, and apply the dy-\nnamic batching provided by fairseq6 to train both\nthe models with 4 GPUs. Transformer is trained for\n60 epochs, while DM-GPT-2 is first trained by de-\npendency modeling for 30 epochs, and then trained\nby language modeling in Equation 4 for 30 epochs.\nWe use the pretrained GPT-2-base model for\nboth the DM-GPT-2 and base GPT-2. In this com-\nparison, we apply the same training settings with\nTransformer-base models except that learning rate\n6https://github.com/pytorch/fairseq\nis set to 5e − 5. GPT-2 is trained for 80 epochs,\nwhile DM-GPT-2 is first trained by dependency\nmodeling for 40 epochs, and then trained by lan-\nguage modeling in Equation 4 for 40 epochs.\nFor all the models, we select the best checkpoint\naccording to the loss of validation set for testing.\nA.2 Unconditional Text Generation\nThe dataset statistics of EMNLP2017 WMT News\ndataset is reported in Table 11.\nTrain Validation Test\n#Stories 268,586 10,000 10,000\nTable 11: Statistics of EMNLP2017 WMT News\ndataset.\nThe context length for DM-LSTM is set to 36.\nLSTM baselines are trained for 500 epochs with\nbatch size 300. DM-LSTM is first trained by de-\npendency modeling objective for 100 epochs with\nbatch size 300, and then by language modeling for\n400 epochs with batch size 200. Besides, all the\nother experimental setups are the same with those\nfor the conditional text generation task.\nA.3 Language Modeling\nThe dataset statistics of Penn Treebank dataset is\nreported in Table 12.\nTrain Validation Test\n#Stories 42,068 3,370 3,761\nTable 12: Statistics of Penn Treebank dataset.\nThe context length for DM-LSTM is set to 16.\nDM-LSTM is trained for 1000 epochs with batch\nsize 20, following (Merity et al., 2018). Besides,\nall the other experimental setups are the same with\nthose for the conditional text generation task.\nB Impact of the Dependency Parser\nIn our work, we use an off-the-shelf dependency\nparser to get the dependency parse trees for de-\npendency modeling. Consequently, the better the\nquality of dependency parsing, the better the per-\nformance of our method. HPSG Parser (Zhou and\nZhao, 2019), the dependency parser we use, is one\nof the state-state-of-the-art parsers. This ensures\nthe high quality of parsing results. Zhou and Zhao\n(2019) trained HPSG Parser with the training set\n7770\nof PTB, and kept the test set held-out. So, when\nwe do language modeling on PTB, the parser will\nnot inject any future predictions that contribute to\ntesting.\nHPSG Parser maintains high-quality on out-\nof-domain text, as shown in its paper (Zhou\nand Zhao, 2019). Most importantly, even on\nthe out-of-domain datasets, i.e., ROCStories and\nEMNLP2017 WMT News, our work can still ob-\ntain a significant improvement, as shown in Sec-\ntion 3.1 and Section 3.2.\nC Language Modeling on\nTransformer-based Models\nThe language modeling results of Transformer-\nbased models evaluated on PTB dataset are shown\nin following Table 13.\nModels #Params Dev PPL Test PPL\nTransformer 24M 100.7 106.7\nDM-Transformer 24M 80.6 84.6\nGPT-2 163M 62.6 55.2\nDM-GPT-2 163M 58.8 51.6\nTable 13: Transformer-based models’ perplexity evalu-\nated on validation and test sets of Penn Treebank dataset.\nThe good performance of Transformer-based\nmodels often rely on training with large datasets,\nbut PTB is a very small dataset. Therefore,\nTransformer-based models perform worse than\nLSTM-based models, as shown in Table 7 and Ta-\nble 13. However, our DM-models still substantially\nreduce the perplexity compared with base models.\nDM-Transformer improves the base Transformer\nby over 20 perplexity points on both the validation\nand test set, and DM-GPT-2 also improves the base\nGPT-2 by almost 4 perplexity points. These results\nfurther confirm the effectiveness our method.\nD Human Evaluation\nWe post the human evaluation questionnaire, as\nshown in Table 14 and Table 15, and then recruit\nfive workers with sufficient high English skills. We\npay each worker 45 US dollars, and let them com-\nplete the evaluation within a week.\nE Generated Examples\nFor a more general comparison, we present more\ngenerated examples of unconditional text genera-\ntion in Table 16.\n7771\nTask Description\nEach story contains about five sentences. For each story, we will put the first four sentences into two\ndifferent systems, and then systems generate the last sentence. The requirement for this manual evaluation\nis to judgewhich story better complies with the English grammar norm, and is more logically related\nto the first four sentences.\nNOTEthat the names in all stories are replaced with \"[MALE]\" or \"[FEMALE]\" or \"[NEUTRAL]\", and\nall the sentences are preprocessed by lowercasing, separating punctuation, and splitting conjunctions.\nThey are not grammar errors. Please ignore these when evaluating and do not allow them to affect your\njudgments.\nEvaluation Criterion\nYou need to compare the stories from two metrics:grammaticalityand logicality. And the two metrics\nare independentof each other. One of the judgments should not have any influence on the other one.\nSpecific criteria for evaluating are as follows:\n1. Grammaticality\nIn the process of evaluating grammaticality, it should be considered whether the statement itself complies\nwith the English standard usage. Then annotate which story is better at grammaticality. You may not care\nabout what the generated sentences are saying butonly if there are any grammatical problems in the\nsentence itself.\n2. Logicality\nIn the process of evaluating logicality, you need to carefully read the whole story including the first four\nsentences and the generated sentence, and compare stories in logicality. Then annotate which story is better\nat logicality in terms of the coherence to the given beginnings and the inter-sentence causal and temporal\ndependencies. In this process, you may encounter sentences that are not completely grammatical.Please\nmake a logical evaluation based on the main part of the sentence (such as some keywords, etc.) and\nwhat you can intuitively feel.Under the circumstances, the story can be judged totally illogical only if\nthe grammar is too poor to understand the meaning or the logic is unreasonable.\nNotes\n· Again, the grammaticality and logicality of the story aretwo independent metrics. Some very logically\ninappropriate generated stories are good in the grammaticality part, and there are some stories with\nobvious grammatical errors but they don’t affect the respective judgment.\n· Sometimes, there may be more than one kind of reasonable story for a beginning. Please do not limit\nyour imagination.As long as the story is logically reasonable, direct, and able to make sense, it can\nbe judged good in logicality.\n· Some stories may not be accurately judged. In the process of determining the comparison of this type\nof two stories, according to your own understanding of the examples and the subjective feelings of the\nstories, choose a better story you think is the most appropriate.Please ensure that your evaluation\ncriterion for different stories is the same.\nTable 14: Human evaluation questionnaire for conditional text generation.\nTask Description\nIn this review, you will read 900 sentences. For each sentence, you should determine whether the\nsentence is written by human. Note: All the sentences are preprocessed by lowercasing, separating\npunctuation, and splitting conjunctions. They are not grammar errors. Some sentences may have a specific\ncontext, or they may be talking about completely fictitious things. Please ignore these when evaluating\nand do not allow them to affect your judgments.\nEvaluation Criterion\nThe judgment can mainly depend on your own understanding and the subjective feelings. But fluency,\nreadability, engagement (whether you felt interested about the sentence), and anything else that you think\nis important can also help you make a decision.\nTable 15: Human evaluation questionnaire for unconditional text generation.\n7772\nGolden Text:\nover 1 , 600 a day have reached greece this month , a higher rate than last july when the crisis was\nalready in full swing .\n\" we ’ re working through a legacy period , with legacy products that are 10 or 20 years old , \" he says .\n’ the first time anyone says you need help , i ’ m on the defensive , but that ’ s all that i know .\nout of those who came last year , 69 per cent were men , 18 per cent were children and just 13 per cent\nwere women .\nPRPN:\nas a mother , i can ’ t work to be working on some kind of stuff , but i ’ m not really sure that the single\nmarket is going to be as bad as i ’ m on .\nin fact , there is a good position to focus on this and that will be a clear opportunity for the us to make\nsure that we do not have any concerns .\nthere ’ s still more opportunities than that , but this is what you ’ re talking about , but it ’ s not right .\nas well as a labour party , the former party member who claimed the vote in the referendum on whether\nto vote to leave the eu should be questioned .\nON-LSTM:\nso they did that because we ’ ve been saying they ’ re going to be fighting for this state , but they ’ re\ngoing to keep going .\nthe official said they were hoping to make a contribution in its strong inflation growth in the future ,\nand that a more conservative leader could look for jobs and be stronger .\nit ’ s something that i think are a good team , the first place to do it and i ’ m really happy .\n’ there ’ s no question that the person we ’ re going to take is probably an important thing to be asked ,\n\" said john .\nA WD-LSTM:\nin this month ’ s election , the u . s . economy has fallen in the past few years , a higher than a decade\nago .\nin the last year i had been an 18 - year - old woman in my two - year - old son .\nit was a great test for me to try to get back on the bench and be there , it ’ s a huge challenge for us .\ni just think it ’ s important for us to do something that would help them in the best way we can to do it .\nDM-LSTM:\n\" the united states has to come to mind that the threat of climate change is less of a serious issue , \" the\npentagon said in a statement .\nin the event of an initial campaign for the democratic nomination , he had released some of the most\ncontroversial ads that they had been speaking about since he was a president .\nthere is an example of a presidential candidate who has been on the debate trail for more than a year .\nthe central bank of japan is set to raise its benchmark interest rate at its first time in nearly a decade .\nTransformer:\nyou can ’ t get away with things that are better than you did at home and hopefully get better than not\nthe first team .\nin the case of the cases , the nsw government said it would accept 10 , 000 additional emergency costs\nif it did not help the industry .\nif there is an oil price that is at stake , it is not as far as the price of oil .\nthe country has promised to build a nationwide population of about 150 , 000 to more than 2 , 000 ,\nwith a budget to help in building more affordable housing .\nDM-Transformer:\nin this particular area , as in the modern world , he is seen as someone who takes the risk of suffering a\nheart attack .\nthat ’ s why we ’ re talking about the second half of the year , and a lot of people have asked us to do\nthe best we can .\nthe vast majority of american voters , particularly those who chose trump , said that he had changed\nthe result .\nso this is a big step , and i ’ m really excited to be part of the new york olympics .\nGPT-2:\nthe reason is that the student community who doesn ’ t know what he ’ s talking about , or who ’ s not\neven a businessman , he ’ s going to take care of itself .\nthe difference is that the reality of \" brexit \" has been the single largest trading partner in the world ,\nand now is it .\nthe game is now used to push for players to learn from them and learn from them and also play in the\nfront of them .\nthe first woman to run for president is to make a case for a woman she wants to make as president of\nthe united states .\nDM-GPT-2:\n\" i just thought that the whole picture was a strange story , \" he said in a telephone interview on\nthursday .\n\" the importance of local authorities is very strong , \" she said in an interview on friday afternoon .\nwe are working closely with the government to resolve this issue and have to work with local authorities\nto resolve the problem .\na final verdict will be held on thursday at the supreme court in washington on march 15 , 2017 .\nTable 16: Examples of unconditional text generation on EMNLP2017 WMT News dataset.\n7773",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8522454500198364
    },
    {
      "name": "Language model",
      "score": 0.7576301693916321
    },
    {
      "name": "Dependency (UML)",
      "score": 0.7522026300430298
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6473219990730286
    },
    {
      "name": "Security token",
      "score": 0.579585611820221
    },
    {
      "name": "Artificial neural network",
      "score": 0.5507614612579346
    },
    {
      "name": "Transformer",
      "score": 0.49808192253112793
    },
    {
      "name": "Probability distribution",
      "score": 0.47104644775390625
    },
    {
      "name": "Context model",
      "score": 0.42961281538009644
    },
    {
      "name": "Natural language processing",
      "score": 0.4239237606525421
    },
    {
      "name": "Recurrent neural network",
      "score": 0.423576295375824
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4125812351703644
    },
    {
      "name": "Machine learning",
      "score": 0.3342239260673523
    },
    {
      "name": "Mathematics",
      "score": 0.06639960408210754
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 3
}