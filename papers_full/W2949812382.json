{
    "title": "Combining semantic and syntactic structure for language modeling",
    "url": "https://openalex.org/W2949812382",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4301004716",
            "name": "Bod, Rens",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1535015163",
        "https://openalex.org/W2113641487",
        "https://openalex.org/W1992254746",
        "https://openalex.org/W2092654472",
        "https://openalex.org/W2950439283",
        "https://openalex.org/W3021452258",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2049633694",
        "https://openalex.org/W2343954916",
        "https://openalex.org/W1649222155",
        "https://openalex.org/W2067694419",
        "https://openalex.org/W191085059"
    ],
    "abstract": "Structured language models for speech recognition have been shown to remedy the weaknesses of n-gram models. All current structured language models are, however, limited in that they do not take into account dependencies between non-headwords. We show that non-headword dependencies contribute to significantly improved word error rate, and that a data-oriented parsing model trained on semantically and syntactically annotated data can exploit these dependencies. This paper also contains the first DOP model trained by means of a maximum likelihood reestimation procedure, which solves some of the theoretical shortcomings of previous DOP models.",
    "full_text": "arXiv:cs.CL/0110051   24 Oct 2001\nCOMBINING SEMANTIC AND SYNTACTIC STRUCTURE FOR\nLANGUAGE MODELING\nRens Bod\nInformatics Research Institute, University of Leeds, Leeds LS2 9JT &\nInstitute for Logic, Language and Computation, University of Amsterdam\nrens@scs.leeds.ac.uk\nABSTRACT\nStructured language models for speech recognition have been\nshown to remedy the weaknesses of n -gram models. All\ncurrent structured language models, however, are limited in\nthat they do not take into account dependencies between\nnon-headwords. We show that non-headword dependencies\ncontribute significantly to improved word error rate, and that\na data-oriented parsing model trained on semantically and\nsyntactically annotated data can exploit these dependencies.\nThis paper contains the first published experiments with a\ndata-oriented parsing model trained by means of a maximum\nlikelihood reestimation procedure.\n1. INTRODUCTION\nStructured language models for speech recognition have\nrecently gained a considerable interest. They have been\nshown to outperform the 3-gram language model on various\ndomains and they can be efficiently parsed in a left-to-right\nmanner (Chelba & Jelinek 1998; Chelba 2000). Although it\nhas been reported that higher order n-gram models perform as\nwell as structured language models (Goodman 2000), it is\nwidely recognized that n-gram models have intrinsic\nlimitations in that they cannot capture lexical dependencies\nthat are structurally rather than sequentially related.\nChelba & Jelinek (1998) have proposed a head-\nlexicalized stochastic grammar as the basis for a structured\nlanguage model. Their grammar associates each nonterminal\nwith its lexical head as in Collins (1999). While Chelba &\nJelinek's language model outperforms a deleted interpolation\n3-gram model, their head-lexicalized grammar is also limited\nin that it cannot capture dependencies between non-\nheadwords, such as more and than in Wall Street Journal\nphrases like more people than cargo, or more couples\nexchanging rings in 1988 than in the previous year, where\nneither more nor than are headwords of these phrases (see\nChelba 2000: 12-16).\nA model which does not apply any a priori\nrestrictions on the lexical dependencies is the Data-Oriented\nParsing or DOP model (e.g. Bod 1993, 1998a/b; Sima'an\n1999). The DOP model learns a stochastic tree-substitution\ngrammar (STSG) from a treebank by extracting all subtrees\n(up to a certain depth) from the treebank and assigning\nprobabilities to the subtrees that are proportional to their\nempirical treebank frequencies. Since subtrees can be\nlexicalized at their frontiers with one or more words, DOP\ntakes into account both headword and non-headword\ndependencies. For example, the dependency between more\nand than is captured by a subtree where more and than are\nthe only frontier words. In Bod (2000a) we have shown that\nnon-headword dependencies contribute to higher parse\naccuracy on the Wall Street Journal corpus (Marcus et al.\n1993), resulting in improved performance over previous\nsystems such as Collins (1999) or Charniak (2000).\nIn the current paper, we propose the DOP model as\na language model for speech recognition and test it on the\nOVIS spoken language corpus (10,000 syntactically and\nsemantically annotated sentences with corresponding word-\ngraphs -- see Bod 1998a/b). While DOP performs well as a\nparsing model (Bod 2000a), it achieves rather disappointing\nresults as a language model: it performs worse than the 3-\ngram model on OVIS word-graphs. We therefore developed a\nnew DOP model that reestimates the subtree probabilities by\nmeans of maximum likelihood training. This maximum\nlikelihood DOP model does outperform the 3-gram model.\nWe show that the elimination of subtrees with two or more\nnon-headwords leads to a significant deterioration of the word\nerror rate (WER). We also investigate the contribution of\nsemantic information on the WER.\nThe rest of this paper is organized as follows. We\nfirst shortly describe the problem of language modeling in\nspeech recognition. We then explain the DOP approach and\nshow how it can be trained by means of a maximum\nlikelihood reestimation procedure belonging to the class of\nexpectation-maximization algorithms. Finally, we give an\nexperimental evaluation of the various language models on\nthe OVIS corpus.\n2. LANGUAGE MODELS\nIn the statistical formulation of the speech recognition\nproblem, the recognizer aims to find the word string\nW  = arg max P (W ) P (A | W )W\n^\nwhere A  is the acoustic signal, P(A | W ) is the acoustic\nprobability that when W  is spoken the signal A  results, and\nP(W ) is the a priori probability that the speaker will utter W .\nIt is the task of a language model to estimate P(W ), while the\nacoustic model estimates P(A | W ). Most language models\nestimate the probability of W = w 1, w2, ..., wn by the well-\nknown n-gram model\nP(W ) = Π i P(w i | w i−n+ 1, w i−n+ 2, ... , w i−1 )\nwhere in most cases n = 3. The shortcomings of the 3-gram\nmodel are well-known: it cannot capture lexical\ndependencies that are structurally rather than linearly related,\nsuch as between dog and barked in the sentence The dog on\nthe hill barked. Morever, dog  and barked can be arbitrarily\nwidely separated, such as in The dog near the house on the hill\nbarked, etc. These dependencies can only be captured by a\nstructured language model, such as a head-lexicalized\nstochastic grammar which associates each nonterminal of a\ncontext-free rule with a lexical head (Chelba & Jelinek\n1998). Since dog  and barked are both phrasal headwords, a\nhead-lexicalized grammar can capture the dependency\nbetween these two words. However, as said in the\nintroduction, head-lexicalized grammars run into problems if\nthere are dependencies between words that are not headwords\nof phrases, such as more and than in the Wall Street Journal\nconstruction more people than cargo, where the relevant\ndependency is between more and than rather than between\npeople and cargo, but neither more nor than are taken as\nheadwords by a head-lexicalized grammar (see Chelba 2000:\n12-16). Moreover, non-headwords can be arbitrarily widely\nseparated as in more couples exchanging rings in 1988 than in\nthe previous year. Non-headword dependencies can only be\ncaptured by a model which does not apply a priori restrictions\non the lexical dependencies, such as the Data-Oriented\nParsing (DOP) model.\n3. A DOP-BASED LANGUAGE MODEL\nThe DOP model presented in Bod (1993, 1998a) learns a\nstochastic tree-substitution grammar (STSG) from a treebank\nby taking all subtrees seen in that treebank to form a\ngrammar. A substitution operation is used to combine\nsubtrees into parse trees for new sentences (see Bod 2000b\nfor some alternative DOP models). As an example, consider\na very simple treebank consisting of only two trees:\n(1)\nNP VP\nS\nNP\nMary\nV\nlikes\nJohn\nNP VP\nS\nNPVPeter\nhates Susan\nNew sentences may be derived by combining subtrees from\nthis treebank by means of a node-substitution operation\nindicated as °. Node-substitution identifies the leftmost\nnonterminal frontier node of one tree with the root node of a\nsecond tree. Under the convention that node-substitution is\nleft-associative, a new sentence such as Mary likes Susan\ncan be derived by combining subtrees from this treebank, as\nin (2):\n(2)\nNP VP\nS\nNPV\nlikes\nNP\nMary\nNP\nSusan NP VP\nS\nNPMary V\nlikes Susan\n=° °\nOther derivations may yield the same parse tree; for instance:\n(3)\nNP VP\nS\nNPV\nNP\nMary NP VP\nS\nNPMary V\nlikes Susan\n=\nSusan\nV\nlikes\n° °\nThe DOP model in Bod (1993) computes the probability of a\nsubtree t as the probability of selecting t among all corpus-\nsubtrees that can be substituted on the same node as t. This\nprobability is equal to the number of occurrences of t, | t |,\ndivided by the total number of occurrences of all subtrees t'\nwith the same root label as t. Let r(t) return the root label of\nt. Then we may write:\nP(t)  =   \n| t |\nΣ t': r(t')=r(t)  | t' |\nThe probability of a derivation D = t1 ° ... ° tn is computed by\nthe product of the probabilities of its subtrees ti:\nP(t1 ° ... ° tn)  =  Π i P(ti)\nAs we have seen, there may be several distinct derivations\nthat generate the same parse tree. The probability of a parse\ntree T is thus the sum of the probabilities of all derivations D\nthat generate T :\nP(T)  =  ΣD   P(T, D )\nTo use DOP as a language model for speech, we need to\ncompute the probability of a word string W . Since a word\nstring may have several different parse trees T , the\nprobability of W  equals the sum of the probabilities of all\nparse trees T  that yield W (which is also equal to the sum of\nthe probabilities of all derivations D  that generate W ):\nP(W ) = ΣT P (W , T)\nNote that the DOP model considers counts of subtrees of a\nwide range of sizes and lexicalizations in computing the\nprobability of a tree: everything from counts of single-level\nrules to counts of entire trees This means that the model is\nsensitive to the frequency of large subtrees while taking into\naccount the smoothing effects of counts of small subtrees.\nAlthough in figures (2) and (3) the subtrees are\nlexicalized with only one word, there is no upper bound in\nDOP on the number of words in a subtree frontier (though for\ncomputational reasons the size of the subtrees is sometimes\nrestricted). This means that DOP can capture dependencies\nbetween headwords as well as between non-headwords. For\nexample, the previously mentioned dependency between the\nnon-headwords more  and than is captured by a subtree in\nwhich more  and than are the only frontier words.\nJohnson (1998) has pointed out that the way DOP\nassigns probabilities to subtrees (by the relative frequency\nestimator) does not maximize the likelihood of the corpus.\nAlthough this may not seem very important from a\nperformance point of view, as DOP obtains state-of-the-art\nparsing accuracy on the Wall Street Journal corpus (Bod\n2000a), we will test in this paper also a DOP model which\nreestimates the subtree probabilities by a maximum\nlikelihood reestimation procedure belonging to the class of\nexpectation-maximization (EM) algorithms (Dempster et al.\n1977).1 We will refer to this new, maximum likelihood DOP\nmodel as \"ML-DOP\". Unfortunately, reestimation with ML-\nDOP is computationally very expensive because of the large\nnumber of parameters. We have therefore only very recently\nbeen able to train an ML-DOP model on a non-trivial corpus.\nThe exposition of the following reestimation technique for\nDOP is heavily based on a technical report by Magerman\n(1993).\n1 Bonnema et al. (1999) present an alternative probability\nmodel which estimates the probability of a subtree as the\nprobability that it has been involved in the derivation of a\ncorpus tree. It is not yet known how this estimator compares\nto the EM algorithm.\nIt is important to realize that there is an implicit\nassumption in DOP that all derivations of a parse tree\ncontribute equally to the total probability of the parse tree.\nThis is equivalent to saying that there is a hidden component\nto the model, and that DOP can be trained using an EM\nalgorithm to determine the maximum likelihood estimate for\nthe training data. The EM algorithm for this new ML-DOP\nmodel is related to the Inside-Outside algorithm for context-\nfree grammars, but the reestimation formula is complicated\nby the presence of subtrees of depth greater than 1. To derive\nthe correct reestimation formula, it is useful to consider the\nstate space of all possible derivations of a tree.\nThe derivations of a parse tree T can be viewed as a\nstate trellis, where each state contains a partially constructed\ntree in the course of a leftmost derivation of T. st denotes a\nstate containing the tree t which is a subtree of T. The state\ntrellis is defined as such:\nThe initial state, s0, is a tree with depth zero, consisting\nof simply a root node labeled with S .\nThe final state, sT, is the given parse tree T.\nA state st is connected forward to all states stf such that tf\n= t ° t', for some t'. Here the appropriate t' is defined to be\ntf − t.\nA state st is connected backward to all states stb such that\nt = tb ° t', for some t'. Again, t' is defined to be t −  tb.\nThe construction of the state lattice and assignment of\ntransition probabilities according to the ML-DOP model is\ncalled the forward pass. The probability of a given state,\nP(s), is referred to as \nα (s). The forward probability of a state\nst is computed recursively\nα (st) = Σ α (st  ) P (t −  tb).b\nstb\nThe backward probability of a state, referred to as β(s), is\ncalculated according to the following recursive formula:\nβ(st) = Σ β(st  ) P(tf − t)f\nfst\nwhere the backward probability of the goal state is set equal\nto the forward probability of the goal state, \nβ (sT) = α (sT).\nThe update formula for the count of a subtree t is\n(where r(t) is the root label of t):\nct(t) =      Σ\n      β(st  )α (st  )P (t | r(t))f b\nα (sgoal)st  :∃st ,tb°t=tfb f\nThe updated probability distribution, P'(t | r(t)), is defined to\nbe\nP'(t | r(t)) =  \nct(t)\nct(r(t))\nwhere ct(r(t)) is defined as\nct(r(t)) = Σ ct(t')\nt': r(t')=r(t)\n4. COMPARING ML-DOP TO THE 3-\nGRAM MODEL ON THE OVIS CORPUS\nThe OVIS corpus consists of 10,000 user utterances (and\ncorresponding word-graphs) about Dutch public transport\ninformation that are syntactically and semantically\nannotated. Each syntactic label is compositionally enriched\nwith a semantic formula as described in Bonnema et al.\n(1997) and Bod (1998b, 1999). The following figure gives an\nexample annotation for the OVIS sentence Ik wil niet vandaag\nmaar morgen naar Almere (literally: \"I want not today but\ntomorrow to Almere\"). As can be seen, the pre-lexical nodes\ncontain the meanings of the underlying lexical items, while\nthe higher nodes contain a formula scheme indicating how\nthe meaning of the constituent is built up out of the meanings\nof its subconstituents. These schemes use the variable d1 to\nindicate the meaning of the leftmost daughter constituent, d2\nto indicate the meaning of the second daughter node\nconstituent, etc. Moreover, the \"#\" in the example refers to\nthe denied information while the \"!\" refers to the corrected\ninformation. For more details on the OVIS corpus and its\nannotation convention, see Bonnema et al. (1997) or Bod\n(1998a, 1999).\n(4)\n  MP\n[d1d2]\nADV\n   #\n MP\ntoday\nniet vandaag\n    MP\ntomorrow\nCON\n   !\nmaar morgen\n           P\ndestination.place\nnaar\n       NP\ntown.almere\nalmere\n MP\nd1.d2\n      MP\n(d1;[d2d3])\n   MP\n(d1;d2)\n  VP\nd1.d2\n   V\nwants\nwil\nik\n   S\nd1.d2\nPER\nuser\nThus the meaning of Ik wil niet vandaag maar morgen naar\nAlmere is compositionally built up out of the meanings of its\nsubconstituents. Substituting the meaning representations into\nthe corresponding variables yields the semantics of the top-\nnode S: user.wants.(([# today];[! tomorrow]);\ndestination.place.town.almere). An advantage of using\ncompositional semantic annotations is that the treebank\nsubtrees can directly be extracted and employed by the DOP\nmodel for computing syntactic/semantic structures for new\nsentences (Bod 1998a/b).\nSo far, the OVIS corpus has mainly been used by\nDOP to compute the most probable semantic interpretation\nfrom a word-graph by means of a Viterbi-style bottom-up\nparsing algorithm (see e.g. Bod 1998b). However, the DOP\nmodel may just as well be used to compute the most\nprobable word string from each word-graph, by summing up\nthe probabilities of all derivations that yield the same string.\nSince the computation of the most probable string by DOP is\nNP-hard (Sima'an 1996), we employ a Viterbi n best search\nand estimate the most probable string by the 1,000 most\nprobable derivations. If DOP does not find a derivation for a\ncomplete word-graph path, we compute the 1,000 most\nprobable combinations of subderivations of partial paths. For\nfurther details on word-graph parsing with DOP we refer to\nBod (1998b) or Sima'an (1999).\nWe split the OVIS corpus into 10 random divisions\nof 90%/10% training/test data, and used each training set to\nderive resp. the 3-gram model (using Katz smoothing -- see\nKatz 1987), the original DOP model, and the ML-DOP\nmodel. The original DOP model was obtained by extracting\nall subtrees up to depth 4 from the training set and assigning\nthem probabilities proportional to their empirical relative\nfrequencies. For the ML-DOP model, we also extracted all\nsubtrees up to depth 4 from the training set which were then\ntrained on the training set trees by maximum likelihood\nreestimation. The updated probabilities were iteratively\nreestimated until the decrease in cross-entropy became\nnegligible. We never needed more than 30 iterations to\nobtain good convergence. The 1000 word-graphs for each test\nset were used as input, while the user utterances were kept\napart. The word error rate (WER) for each model was\naveraged over all 10 test sets. The following table shows the\nresults (where SimpleDOP refers to the original DOP model\nbased on simple relative frequencies).\nModel WER\n3-gram 18.7%\nSimpleDOP 19.8%\nML-DOP 17.0%\nWERs of the language models for OVIS word-graphs\nThe table shows that the 3-gram model outperforms the\nsimple DOP model with 1.1% WER. This difference is\nstatistically significant according to paired t-testing.\nHowever, ML-DOP outperforms the 3-gram model with 1.7%\nWER, which was also statistically significant. We next\ntested a version of ML-DOP where we eliminated all subtrees\nwith two or more non-headwords. This led to a statistically\nsignificant deterioration of 0.9% WER. This result indicates\nthat non-headword dependencies are important for predicting\nthe uttered string, and should not be discarded as in head-\nlexicalized grammars (Chelba 2000; Collins 1999, 2000;\nCharniak 2000). Finally, we tested a version of ML-DOP\nwhere we eliminated the semantic annotations; this also led\nto a deterioration in WER of 1.3% which was statistically\nsignificant. Thus, compositional semantic annotations\ncontribute to significantly better word error rate.\n5. CONCLUSIONS\nPrevious structured language models did not take into\naccount non-headword dependencies. We have shown that a\nmaximum likelihood DOP model can effectively exploit non-\nheadword dependencies, which significantly contribute to\nimproved word error rate on the OVIS spoken language\ncorpus. We have also seen that the maximum likelihood DOP\nmodel outperforms the 3-gram model, while the original DOP\nmodel performs worse than the 3-gram model. Finally, we\nfound that semantic annotations contribute to significantly\nbetter word error rate. As future research, we want to test ML-\nDOP on Wall Street Journal utterances. This may be\nespecially interesting as the DOP model has been shown to\nobtain state-of-the-art parsing performance on the WSJ (see\nBod 2000a).\nREFERENCES\nR. Bod, 1993. Using an Annotated Language Corpus as a\nVirtual Stochastic Grammar, Proceedings AAAI'93 ,\nWashington D.C.\nR. Bod, 1998a. Beyond Grammar: An Experience-Based\nTheory of Language, CSLI Publications, distributed by\nCambridge University Press.\nR. Bod, 1998b. Spoken Dialogue Interpretation with the DOP\nModel. Proceedings COLING-ACL'98 , Montreal,\nCanada.\nR. Bod, 1999. Context-Sensitive Dialogue Processing with\nthe DOP model, Natural Language Engineering 5(4),\n309-323.\nR. Bod, 2000a. Parsing with the Shortest Derivation,\nProceedings COLING'2000, Saarbrücken, Germany.\nR. Bod, 2000b. An Improved Parser for Data-Oriented\nLexical-Functional Analysis. Proceedings ACL'2000 ,\nHong Kong, China.\nR. Bonnema, R. Bod and R. Scha, 1997. A DOP Model for\nSemantic Interpretation, Proceedings ACL/EACL-97 ,\nMadrid, Spain.\nR. Bonnema, P. Buying and R. Scha, 1999. A New\nProbability Model for Data-Oriented Parsing.\nProceedings of the Amsterdam Colloquium 1999.\nAmsterdam.\nE. Charniak, 2000. A Maximum-Entropy-Inspired Parser.\nProceedings ANLP-NAACL'2000 , Seattle, Washington.\nC. Chelba, 2000. Exploiting Syntactic Structure for Natural\nLanguage Modeling , PhD-thesis, Johns Hopkins\nUniversity, Maryland.\nC. Chelba and F. Jelinek, 1998. Exploiting Syntactic\nStructure for Natural Language Modeling, Proceedings\nCOLING-ACL'98 , Montreal, Canada.\nM. Collins, 1999. Head-Driven Statistical Models for Natural\nLanguage Parsing , PhD-thesis, University of\nPennsylvania, PA.\nM. Collins, 2000. Discriminative Reranking for Natural\nLanguage Parsing, Proceedings ICML'2000, Stanford,\nCa.\nA. Dempster, N. Laird and D. Rubin, 1977. Maximum\nLikelihood from Incomplete Data via the EM Algorithm,\nJournal of the Royal Statistical Society, 39:1-38.\nJ. Goodman, 2000. The State-of-the-Art in Language Modeling,\ntutorial notes, ANLP-NAACL'2000 , Seattle, Washington.\nM. Johnson, 1998. The DOP Estimation Method is Biased and\nInconsistent, squib.\nS. Katz, 1987. Estimation of Probabilities from Sparse Data\nfor the Language Model Component of a Speech\nRecognizer. IEEE Transactions on Acoustics, Speech and\nSignal Processing, 35:400-01.\nD. Magerman, 1993. Expectation-Maximization for Data-\nOriented Parsing, IBM Technical Report, Yorktown\nHeights, NY.\nM. Marcus, B. Santorini and M. Marcinkiewicz, 1993.\nBuilding a Large Annotated Corpus of English: the Penn\nTreebank, Computational Linguistics 19(2), 313-330.\nK. Sima'an, 1996. Computational Complexity of Probabilistic\nDisambiguation by means of Tree Grammars,\nProceedings COLING-96, Copenhagen, Denmark.\nK. Sima'an, 1999. Learning Efficient Disambiguation. PhD\nthesis, ILLC dissertation series number 1999-02.\nUtrecht/Amsterdam, The Netherlands."
}