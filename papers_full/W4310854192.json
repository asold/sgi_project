{
  "title": "A transformer-based generative adversarial network for brain tumor segmentation",
  "url": "https://openalex.org/W4310854192",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099410890",
      "name": "Liqun Huang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2114385372",
      "name": "En-Jun Zhu",
      "affiliations": [
        "Capital Medical University",
        "Beijing Anzhen Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2098309033",
      "name": "Long Chen",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098406446",
      "name": "Zhaoyang Wang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2137826344",
      "name": "Senchun Chai",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2134284540",
      "name": "Baihai Zhang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099410890",
      "name": "Liqun Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114385372",
      "name": "En-Jun Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098309033",
      "name": "Long Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098406446",
      "name": "Zhaoyang Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137826344",
      "name": "Senchun Chai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134284540",
      "name": "Baihai Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6787935328",
    "https://openalex.org/W4281621674",
    "https://openalex.org/W2751069891",
    "https://openalex.org/W3018757597",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6718140377",
    "https://openalex.org/W6754808139",
    "https://openalex.org/W6767529820",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3192911000",
    "https://openalex.org/W2916412824",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W6631782140",
    "https://openalex.org/W6675026286",
    "https://openalex.org/W2888443510",
    "https://openalex.org/W6791637211",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W4206768567",
    "https://openalex.org/W2962291057",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W6729966448",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3166117684",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2000269560",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6730845260",
    "https://openalex.org/W6785652829",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6640054144",
    "https://openalex.org/W1641498739",
    "https://openalex.org/W2125389028",
    "https://openalex.org/W6755875945",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3186852274",
    "https://openalex.org/W6729482032",
    "https://openalex.org/W3006456511",
    "https://openalex.org/W3003624604",
    "https://openalex.org/W6628973269",
    "https://openalex.org/W6731892127",
    "https://openalex.org/W4293584584",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2002479918",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W6792591559",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4205273962",
    "https://openalex.org/W6791764970",
    "https://openalex.org/W2622064152",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W4200220454",
    "https://openalex.org/W3135185854",
    "https://openalex.org/W2587828787",
    "https://openalex.org/W4281790811",
    "https://openalex.org/W6736210646",
    "https://openalex.org/W2604785265",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W3101639073",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W2395611524"
  ],
  "abstract": "Brain tumor segmentation remains a challenge in medical image segmentation tasks. With the application of transformer in various computer vision tasks, transformer blocks show the capability of learning long-distance dependency in global space, which is complementary to CNNs. In this paper, we proposed a novel transformer-based generative adversarial network to automatically segment brain tumors with multi-modalities MRI. Our architecture consists of a generator and a discriminator, which is trained in min–max game progress. The generator is based on a typical “U-shaped” encoder–decoder architecture, whose bottom layer is composed of transformer blocks with Resnet. Besides, the generator is trained with deep supervision technology. The discriminator we designed is a CNN-based network with multi-scale L 1 loss, which is proved to be effective for medical semantic image segmentation. To validate the effectiveness of our method, we conducted exclusive experiments on BRATS2015 dataset, achieving comparable or better performance than previous state-of-the-art methods. On additional datasets, including BRATS2018 and BRATS2020, experimental results prove that our technique is capable of generalizing successfully.",
  "full_text": "TYPE Original Research\nPUBLISHED /three.tnum/zero.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nOPEN ACCESS\nEDITED BY\nShu Zhang,\nNorthwestern Polytechnical\nUniversity, China\nREVIEWED BY\nYuan Xue,\nJohns Hopkins University,\nUnited States\nShiqiang Ma,\nTianjin University, China\nLu Zhang,\nUniversity of Texas at Arlington,\nUnited States\n*CORRESPONDENCE\nSenchun Chai\nchaisc/nine.tnum/seven.tnum@/one.tnum/six.tnum/three.tnum.com\n†These authors have contributed\nequally to this work and share ﬁrst\nauthorship\nSPECIALTY SECTION\nThis article was submitted to\nBrain Imaging Methods,\na section of the journal\nFrontiers in Neuroscience\nRECEIVED /two.tnum/seven.tnum September /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /zero.tnum/seven.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /three.tnum/zero.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nHuang L, Zhu E, Chen L, Wang Z,\nChai S and Zhang B (/two.tnum/zero.tnum/two.tnum/two.tnum) A\ntransformer-based generative\nadversarial network for brain tumor\nsegmentation.\nFront. Neurosci./one.tnum/six.tnum:/one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Huang, Zhu, Chen, Wang, Chai\nand Zhang. This is an open-access\narticle distributed under the terms of\nthe\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution\nor reproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s)\nare credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nA transformer-based generative\nadversarial network for brain\ntumor segmentation\nLiqun Huang /one.tnum†, Enjun Zhu /two.tnum†, Long Chen /one.tnum, Zhaoyang Wang /one.tnum,\nSenchun Chai /one.tnum* and Baihai Zhang /one.tnum\n/one.tnumThe School of Automation, Beijing Institute of Technology, Beijing, China, /two.tnumDepartment of Cardiac\nSurgery, Beijing Anzhen Hospital, Capital Medical University , Beijing, China\nBrain tumor segmentation remains a challenge in medical image se gmentation\ntasks. With the application of transformer in various compute r vision tasks,\ntransformer blocks show the capability of learning long-dist ance dependency\nin global space, which is complementary to CNNs. In this paper, we pro posed\na novel transformer-based generative adversarial network to au tomatically\nsegment brain tumors with multi-modalities MRI. Our architectu re consists\nof a generator and a discriminator, which is trained in min–max gam e\nprogress. The generator is based on a typical “U-shaped” encode r–decoder\narchitecture, whose bottom layer is composed of transformer block s with\nResnet. Besides, the generator is trained with deep supervis ion technology.\nThe discriminator we designed is a CNN-based network with multi- scale\nL/one.tnumloss, which is proved to be eﬀective for medical semantic image\nsegmentation. To validate the eﬀectiveness of our method, we co nducted\nexclusive experiments on BRATS/two.tnum/zero.tnum/one.tnum/five.tnum dataset, achieving comparableor better\nperformance than previous state-of-the-art methods. On addi tional datasets,\nincluding BRATS/two.tnum/zero.tnum/one.tnum/eight.tnum and BRATS/two.tnum/zero.tnum/two.tnum/zero.tnum, experimental results provethat our\ntechnique is capable of generalizing successfully.\nKEYWORDS\ngenerative adversarial network, transformer, deep learning, autom atic segmentation,\nbrain tumor\n/one.tnum. Introduction\nSemantic medical image segmentation is an indispensable step in computer-aided\ndiagnosis ( Stoitsis et al., 2006 ; Le, 2017 ; Razmjooy et al., 2020 ; Khan et al., 2021 ). In\nclinical practice, tumor delineation is usually performed manually or semi-manually,\nwhich is time-consuming and labor-intensive. As a result, it is of vital importance\nto explore automatic volumetric segmentation methods with the help of medica l\nimages to accelerate the computer-aided diagnosis. In this paper, we focus on t he\nsegmentation of brain tumors with the help of magnetic resonance imaging (MRI )\nconsisting of multi-modality scans. The automatic segmentation of gliomas rem ains one\nof the most challenging medical segmentation problems stemming from some as pects,\nsuch as arbitrary shape and location, poorly contrasted, and blurred boundar y with\nsurrounding issues.\nFrontiers in Neuroscience /zero.tnum/one.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nSince the advent of deep learning, Convolutional Neural\nNetworks (CNN) have achieved great success in various\ncomputer vision tasks, ranging from classiﬁcation (\nLeCun et al.,\n1998; Krizhevsky et al., 2012 ; Simonyan and Zisserman, 2014 ;\nSzegedy et al., 2015 ; Huang et al., 2017 ), object detection\n(Girshick et al., 2014 ; Girshick, 2015 ; Ren et al., 2015 ; Liu\net al., 2016 ; Redmon et al., 2016 ; He et al., 2017 ; Redmon and\nFarhadi, 2017 , 2018; Bochkovskiy et al., 2020 ) to segmentation\n(Chen et al., 2014 , 2017; Long et al., 2015 ; Ronneberger et al.,\n2015; Lin et al., 2017 ). Fully Convolution Networks (FCN Long\net al., 2015 ) and in particular “U-shaped” encoder–decoder\narchitectures have realized state-of-the-art results in medical\nsemantic segmentation tasks. U-Net (\nRonneberger et al., 2015 ),\nwhich consists of symmetric encoder and decoder, uses the\nskip connections to merge the extracted features from encoder\nwith those from decoder at diﬀerent resolutions, aiming at\nrecovering the lost details during downsampling. Owing to\nthe impressive results in plenty of medical applications, U-Net\nand its variants have become the mainstream architectures in\nmedical semantic segmentation.\nIn spite of their prevalence, FCN-based approaches are\nincapable of modeling long-range dependency because of its\nintrinsic limited receptive ﬁeld and the locality of convolution\noperations. Inspired by the great success of transformer-based\nmodels in Natural Language Processing (NLP) (\nDevlin et al.,\n2018; Radford et al., 2018 ; Liu et al., 2019 ; Yang et al.,\n2019; Clark et al., 2020 ), a growing number of researchers\npropose to apply the self-attention mechanism to medical image\nsegmentation, attempting to overcome the limitations brought\nby the inductive bias of convolution, so as to extract the long-\nrange dependency and context–dependent features. Especially,\nunlike prior convolution operations, transformers encode a\nsequence of patches and leverage the power of self-attention\nmodules to pre-train on a large-scale dataset for downstream\ntasks, like Vision Transformer (ViT) (\nDosovitskiy et al., 2020 )\nand its variants.\nSimultaneously, for the Transformers applied in medical\nimage segmentation, Generative Adversarial Networks (GAN)\nhas revealed great performance in semantic segmentation.\nIn a typical GAN architecture used for segmentation, GAN\nconsists of two competing networks, a discriminator and a\ngenerator. The generator learns the capability of contexture\nrepresentations, minimizing the distance between prediction\nand masks, while the discriminator on the contrary maximizes\nthe distance to distinguish the diﬀerence between them. The\ntwo networks are trained in an alternating fashion to improve\nthe performance of the other. Furthermore, some GAN-based\nmethods like SegAN (\nXue et al., 2018 ) achieve more eﬀective\nsegmentation performance than FCN-based approaches.\nIn this paper, we explore the integrated performance of\ntransformer and generative adversarial network in segmentation\ntasks and propose a novel transformer-based generative\nadversarial network for brain tumor segmentation. Owing to\nthe attention mechanism, transformer has a global receptive\nﬁeld from the very ﬁrst layer to the last layer, instead of\nfocusing solely on the local information from convolution\nkernel in each layer, thus contributing to the pixel-level\nclassiﬁcation and being more suitable for medical segmentation\ntasks. Besides, CNN learns representative features at diﬀerent\nresolutions through cascading relationships, while the attention\nmechanism pays more attention to the relationship between\nfeatures, thus transformer-based methods are easily-generalized\nand not completely dependent on the data itself, such as\nexperiments with incomplete images input in\nNaseer et al.\n(2021). Inspired by some attempts ( Wang W. et al., 2021 ;\nHatamizadeh et al., 2022 ) of fusing transformer with 3D CNNs,\nwe design an encoder–decoder generator with deep supervision,\nwhere both encoder and decoder are 3D CNNs but the bridge\nof them is composed of transformer blocks with Resnet. In\nthe contrast of typical “U-shaped” decoder–encoder network,\nour transformer block is designed to replace the traditional\nconvolution-based bottleneck, for the reason that the self-\nattention mechanism inside transformer can learn long-range\ncontextual representations while the ﬁnite kernel size limits\nthe CNN’s capability of learning global information. For pixel-\nwise brain tumor segmentation task, replacing CNN with\ntransformer blocks on the bottleneck contributes to capturing\nmore features from encoder. Inspired by SegAN (\nXue et al.,\n2018), we adopt the multi-scale L1 loss to our method with only\none generator and one discriminator, measuring the distance of\nthe hierarchical features between generated segmentation and\nground truth. Experimental results on BRATS2015 dataset show\nthat our method achieves comparable or better performance\nthan some previous state-of-the-art methods. Compared to\nexisting methods, the main contributions of our approach are\nlisted as follows:\n• A novel transformer-based generative adversarial network\nis proposed to address the brain tumor segmentation task\nwith multi-modalities MRI. To enhance the eﬃciency of\nbrain tumor segmentation, our method incorporates the\nconcepts of “Transformer” and “Generative adversarial”.\nThe generator makes use of the transformer blocks\nto facilitate the process of learning global contextual\nrepresentations. As far as we are aware, our work is\namong the very ﬁrst ones to explore the combination\nof transformer and generative adversarial networks\nand achieve excellent performance in the brain tumor\nsegmentation task.\n• Our generator exploits transformer with Resnet module\nin 3D CNN for segmenting multi-modalities MRI brain\ntumors. Building upon the encoder–decoder structure,\nboth encoder and decoder in our proposed generator are\nmainly composed of traditional 3D convolution layers,\nwhile the bottom layer of the “U-shaped” structure is\na transformer with Resnet module. With Resnet, the\nFrontiers in Neuroscience /zero.tnum/two.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\ntransformer block captures both global and local spatial\ndependencies eﬀectively, thus preparing embedded features\nfor progressive upsampling to full resolution predicted\nmaps.\n• Our loss functions are suitable and eﬀectively applied in\ngenerator and discriminator. Adopting the idea of deep\nsupervision (\nZhu Q. et al., 2017 ), we take the output of the\nlast three decoder layers of generator to calculate weighted\nloss for better gradient propagation. Besides, we leverage a\nCNN-based discriminator to compute multi-scale L1 norm\ndistance of hierarchical features extracted from ground\ntruth and segmentation maps, respectively.\n• The exclusive experimental results evaluated on\nBRATS2015 dataset show the eﬀectiveness of each\npart of our proposed methods, including transformer with\nResnet module and loss functions. Comparing to existing\nmethods, the proposed method can obtain signiﬁcant\nimprovements in brain tumor segmentation. Moreover,\nour method successfully generalizes in other brain tumor\nsegmentation datasets: BRATS2018 and BRATS2020.\nThe following outlines the structure of this paper: Section\n2 reviews the related work. Section 3 presents the detail of\nour proposed architecture. Section 4 describes the experimental\nsetup and evaluates the performance of our method. Section 5\nsummarizes this work.\n/two.tnum. Related works\n/two.tnum./one.tnum. Vision transformer\nThe Transformers were ﬁrst proposed by Vaswani et al.\n(2017) on machine translation tasks and achieved a quantity\nof state-of-the-art results in NLP tasks ( Devlin et al., 2018 ;\nRadford et al., 2018 ). Dosovitskiy et al. (2020) then applied\nTransformers to image classiﬁcation tasks by directly training\na pure Transformer on sequences of image patches as words\nin NLP , and achieved state-of-the-art benchmarks on the\nImageNet dataset. In object detection,\nCarion et al. (2020)\nproposed transformer-based DETR, a transformer encoder–\ndecoder architecture, which demonstrated accuracy and run-\ntime performance on par with the highly optimized Faster\nR-CNN (\nRen et al., 2015 ) on COCO dataset.\nRecently, various approaches were proposed to explore\nthe applications of the transformer-based model for semantic\nsegmentation tasks.\nChen et al. (2021) proposed TransUNet,\nwhich added transformer layers to the encoder to achieve\ncompetitive performance for 2D multi-organ medical image\nsegmentation. As for 3D medical image segmentation,\nWang W.\net al. (2021) exploited Transformer in 3D CNN for segmenting\nMRI brain tumors and proposed to use a transformer in\nthe bottleneck of “U-shaped” network on BRATS2019 and\nBRATS2020 datasets. Similarly,\nHatamizadeh et al. (2022)\nproposed an encoder–decoder network named UNETR, which\nemployed transformer modules as the encoder and CNN\nmodules as the decoder, for the brain tumor and spleen\nvolumetric medical image segmentation.\nCompared to these approaches above, our method is\ntailored for 3D segmentation and is based on generative\nadversarial network. Our generator produces sequences fed into\ntransformer by utilizing a backbone encoder–decoder CNN,\nwhere the transformer with Resnet module is placed in the\nbottleneck. With Resnet, the encoder captures features not only\nfrom CNN-based encoder but also from transformer blocks.\nMoreover, the last three output layers of the encoder are\nconsidered to calculate the loss function for better performance.\nNetworks like UNETR employ transformer layers as encoder\nin low-dimension semantic level, and taking this network as\nbackbone in our method without pre-training easily leads to\nmodel collapse during the adversarial training phase. Therefore,\nwe do not choose these networks as our backbone. We ﬁnd that\ntaking transformer as encoder in low-dimension semantic level\nneeds quantities of pre-training tasks on other datasets to get\ngood results, like TransUNet and UNETR above. As shown in\nour experiments Section 4.6, transformer-based encoder in low-\ndimension semantic level performances inferior to CNN-based\none when training from scratch. Therefore, we choose to apply\ntransformer only in bottleneck, and remain the low-dimension\nencode layers as convolutional layers. In this way, we can train\nfrom scratch, meanwhile achieving good performance.\n/two.tnum./two.tnum. Generative adversarial networks\nThe GAN (\nGoodfellow et al., 2014 ) is originally introduced\nfor image generation ( Mirza and Osindero, 2014 ; Chen et al.,\n2016; Odena et al., 2017 ; Zhu J.-Y. et al., 2017 ), making the core\nidea of competing training with a generator and a discriminator,\nrespectively, known outside of ﬁxed circle. However, there exists\na problem that it is troublesome for the original GAN to remain\nin a stable state, hence making us cautious to balance the training\nlevel of the generator and the discriminator in practice. Arjovsky\net al. proposed Wasserstein GAN (WGAN) as a thorough\nsolution to the instability by replacing the Kullback-Leibler (KL)\ndivergence with the Earth Mover (EM) distance.\nVarious methods (\nIsola et al., 2017 ; Han et al., 2018 ; Xue\net al., 2018 ; Choi et al., 2019 ; Dong et al., 2019 ; Oh et al., 2020 ;\nDing et al., 2021 ; He et al., 2021 ; Nishio et al., 2021 ; Wang\nT. et al., 2021 ; Zhan et al., 2021 ; Asis-Cruz et al., 2022 ) were\nproposed to explore the possibility of GAN in medical image\nsegmentation.\nXue et al. (2018) used U-Net as the generator\nand proposed a multi-scale L1 loss to minimize the distance\nof the feature maps of predictions and masks for the medical\nimage segmentation of brain tumors.\nOh et al. (2020) took\nresidual blocks into account under the framework of pix2pix\nFrontiers in Neuroscience /zero.tnum/three.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\n(Isola et al., 2017 ) and segmented the white matter in FDG-PET\nimages. Ding et al. (2021) took an encoder–decoder network as\nthe generator and designed a discriminator based on Condition\nGAN (CGAN) on BRATS2015 dataset, adopting the image labels\nas the extra input.\nUnlike these approaches, our method incorporates the\nconcepts of “Transformer” and “GAN.” Our discriminator is\nbased on CNN instead of transformer. In our opinion, owing to\nthe attention mechanism inside transformer, transformer has a\nmore global receptive ﬁeld than CNN with limited kernel size,\nthus contributing to pixel-level classiﬁcation and being more\nsuitable for medical segmentation tasks. However, for image-\nlevel medical classiﬁcation, transformer-based discriminator\nseems to be less appropriate for its weakness of requiring\nhuge datasets to support pre-training, while CNN is strong\nenough for classiﬁcation tasks without pre-training. Motivated\nby viewpoints above, in our method, the transformer-based\ngenerator and CNN-based discriminator are combined to\nfacilitate the progress of segmentation under the supervision of\na multi-scale L1 loss.\n/three.tnum. Materials and methods\n/three.tnum./one.tnum. Overall architecture\nThe overview of our proposed model is presented\nin Figure 1. Our framework consists of a generator and\ndiscriminator for competing training. The generator G is\na transformer-based encoder–decoder architecture. Given\na multi modalities (T1, T1c, T2, and FLAIR) MRI scan\nX ∈ RC×H×W×D with 3D resolution (H, W, D) and C\nchannels, we utilize 3D CNN-based down-sampling encoder\nto produce high dimension semantic feature maps, and then\nthese semantic information ﬂow to 3D CNN-based up-sampling\ndecoder through the intermediate Transformer block with\nResnet (\nHe et al., 2016 ). With skip connection, the long-range\nand short-range spatial relations extracted by encoder from\neach stage ﬂow to the decoder. For deep supervision (\nZhu Q.\net al., 2017 ), the output of decoder consists of three parts: the\noutput of last three convolution layers after sigmoid. Inspired\nby\nXue et al. (2018), the discriminator D we used has the similar\nstructure as encoder in G, extracting hierarchical feature maps\nfrom ground truth (GT) and prediction separately to compute\nmulti-scale L1 loss.\n/three.tnum./two.tnum. Generator\nEncoder is the contracting path which has seven spatial\nlevels. Patches of size 160 × 192 × 160 with four channels are\nrandomly cropped from brain tumor images as input, followed\nby six down-sampling layers with 3D 3 ×3×3 convolution (stride\n= 2). Each convolution operation is followed by an Instance\nNormalization (IN) layer and a LeakyReLU activation layer.\nAt the bottom of the encoder, we leverage the transformer\nwith Resnet module to model the long-distance dependency\nin a global space. The feature maps produced by the encoder\nis sequenced ﬁrst and then create the feature embeddings by\nsimply fusing the learnable position embeddings with sequenced\nfeature map by element-wise addition. After the position\nembeddings, we introduce L transformer layers to extract the\nlong-range dependency and context dependent features. Each\ntransformer layer consists of a Multi-Head Attention (MHA)\nblock after layer normalization (LN) and a feed forward network\n(FFN) after layer normalization. In attention block, the input\nsequence is fed into three convolution layers to produce three\nmetrics: queries Q, keys K and values V. To combine the\nadvantages of both CNN and Transformer, we simply short cut\nthe input and output of Transformer block. Thus, as in\nVaswani\net al. (2017) and Wang W. et al. (2021), given the input X,\nthe output of the transformer with Resnet module Y can be\ncalculated by:\nY = x + yL (1)\nyi = FFN\n(\nLN\n(\ny\n′\ni\n))\n+ y\n′\ni (2)\ny\n′\ni = MHA\n(\nLN\n(\nyi−1\n))\n+ yi−1 (3)\nMHA (Q, K, V) = Concat\n(\nhead1, ..., headh\n)\nWO (4)\nheadi = Attention(Q, K, V) = softmax\n(\nQKT/\n√\ndk\n)\nV (5)\nwhere yi denotes the output of ith ( i ∈ [1, 2, ..., L]) Transformer\nlayer, y0 denotes X, WO are projection metrics, dk denotes the\ndimension of K.\nUnlike the encoder, the decoder uses 3D 2 × 2 × 2 transpose\nconvolution for up-sampling, followed by skip connection and\ntwo 3D 3 × 3 × 3 convolution layers. For a better gradient ﬂow\nand a better supervision performance, a technology called deep\nsupervision is introduced to utilize the last three decoder levels\nto calculate loss function. Concretely, we downsampled the GT\nto the same resolution with these outputs, thus making weighted\nsum of loss functions in diﬀerent levels.\nThe detailed structure of our transformer-based generator\nis presented in\nTable 1. In the encoder part, patches of size\n160 × 192 × 160 voxels with four channels are randomly\ncropped from the original brain tumor images as input. At each\nlevel, there are two successive 3 × 3 × 3 unbiased convolution\nlayers followed by normalization, activation layers and dropout\nlayers. Beginning from the second level, the resolution of the\nfeature maps is reduced by a factor of 2. These features, e.g.,\nareas of white matter, edges of brain, dots and lines, etc., are\nextracted by suﬃcient convolution kernels for next blocks. The\nFrontiers in Neuroscience /zero.tnum/four.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nFIGURE /one.tnum\nOverall architecture of our proposed method. In this ﬁgure, “Conv/one.tnum” represents convolutional layer with kernel /one.tnum× /one.tnum× /one.tnum, “Conv/three.tnum” with kernel\n/three.tnum× /three.tnum× /three.tnum, “IN” represents InstanceNorm layer, “LeReLU” means LeakyReLU activation layer.\ntransformer block enriches the global contextual representation\nbased on the attention mechanism, forcing features located in\nthe desired regions unchanged while suppressing those in other\nregions. The shortcut branch crossing the transformer block\nfusing the features from both encoder part and transformer\nblock by element-wise addition, indicating that our generator is\ncapable of learning short-range and long-range spatial relations\nwith neither extra parameter nor computation complexity.\nAccording to the attributes of Resnet (\nHe et al., 2016 ), y =\nf (x) + x, where f (x) in our method represents transformer\nblocks, x is the output of CNN-based encoder, whose contexture\nrepresentations in feature maps are relatively short-range\nthan transformer’s. With Resnet, the element-wise addition of\nf (x) and x can directly fuse the short-range spatial relations\nfrom CNN-based encoder and long-range spatial relations\nfrom transformer-based bottleneck. Additionally, unlike neural\nnetwork layers, element-wise addition is a math operation with\nno more memory cost and negligible computation time cost.\nThe decoder part contains amounts of upsampling layers and\nskip connection to progressively recover semantic information\nas well as resolution. The ﬁrst upsampling layer is implemented\nby interpolation while the other upsampling layers adapt the\nform of deconvolution with stride set to 2. At level i ∈ [1, 5],\nthe encoder block Di doubles the spatial resolution, followed by\nskip connection to fuse high-level (from Di) and low-level (from\nencoder block Ei) contextual representation so as to segment the\ndesired tumor regions. For a better supervision performance,\nthe outputs of Di where i ∈ [1, 3] are fed into 1 × 1 × 1\nconvolution layer and sigmoid layer to predict segmentation\nmaps with diﬀerent resolution. Accordingly, the ground truth\nis downsampled to diﬀerent shapes such that they match the\nshapes of those segmentation maps.\nOur generator’s vital part is the transformer with Resnet\nmodule. As shown in\nTable 1, our transformer with Resnet\nmodule consists of transformer block and Resnet, while\ntransformer block is composed of position encodings module,\nseveral transformer layers depicted in\nFigure 2 and features\nprojection module. To make use of the order of the input\nsequence reshaped from bottom layer feature maps, we\nintroduce a learnable positional encoding vector to represent\nsome information about position of tokens in the sequence,\ninstead of sine and cosine functions. After position encoding\nand normalization, the input sequence is fed into three diﬀerent\nlinear layers to create queries, keys, and values. Then, we\ncompute the dot products of keys with queries. To avoid\nextremely small gradients after softmax function, we scale the\ndot-products by a factor related to dimensions of queries,\nas shown in Equation 5. Multiplying scaled weights with\nFrontiers in Neuroscience /zero.tnum/five.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nTABLE /one.tnum The detailed structure of proposed generator.\nStage Name Details Output size\nEncoder E1 [Conv3, IN, LeReLU, Dropout] 64*160*192*160\n[Conv3, IN, LeReLU, Dropout]\nE2 [Conv3(stride2), IN, LeReLU, Dropout] 96*80*96*80\n[Conv3, IN, LeReLU, Dropout]\nE3 [Conv3(stride2), IN, LeReLU, Dropout] 128*40*48*40\n[Conv3, IN, LeReLU, Dropout]\nE4 [Conv3(stride2), IN, LeReLU, Dropout] 192*20*24*20\n[Conv3, IN, LeReLU, Dropout]\nE5 [Conv3(stride2), IN, LeReLU, Dropout] 256*10*12*10\n[Conv3, IN, LeReLU, Dropout]\nE6 [Conv3(stride2), IN, LeReLU, Dropout] 384*5*6*5\n[Conv3, IN, LeReLU, Dropout]\nE7 [Conv3(stride2), IN, LeReLU, Dropout] 512*3*3*3\n[Conv3, IN, LeReLU, Dropout]\nTransformer ResTransBlock Reshape 512*3*3*3\nPE\nTransformer Layer*4\nReshape\nResnet\nDecoder D6 Upsample 384*5*6*5\n[Conv3, IN, LeReLU, Dropout] x 2\nD5 Deconv 256*10*12*10\nConcat\n[Conv3, IN, LeReLU, Dropout]\n[Conv3, IN, LeReLU, Dropout]\nD4 Deconv 192*20*24*20\nConcat\n[Conv3, IN, LeReLU, Dropout]\n[Conv3, IN, LeReLU, Dropout]\nD3\nDeconv 128*40*48*40\nConcat\n[Conv3, IN, LeReLU, Dropout]\n[Conv3, IN, LeReLU, Dropout]\nOutput3 Conv1 + Sigmoid 4*40*48*40\nD2\nDeconv 96*80*96*80\nConcat\n[Conv3, IN, LeReLU, Dropout]\n[Conv3, IN, LeReLU, Dropout]\nOutput2 Conv1 + Sigmoid 4*80*96*80\nD1\nDeconv 64*160*192*160\nConcat\n[Conv3, IN, LeReLU, Dropout]\n[Conv3, IN, LeReLU, Dropout]\nOutput1 Conv1 + Sigmoid 3*160*192*160\nvalues, we obtain a single attention output, which is then\nconcatenated with other heads’ outputs to produce the multi-\nhead attention outputs. Subsequently, normalization, dropout,\nand multi-layer perception (MLP) layers are utilized to produce\nthe transformer layer’s ultimate output. While convolution\nlayers have local connections, shared weights, and translation\nequivariance, attention layers are global. We take advantage of\nboth by residual connection to learn both short-range and long-\nrange spatial relations with no more memory cost and negligible\ncomputational time cost.\nFrontiers in Neuroscience /zero.tnum/six.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nFIGURE /two.tnum\nThe structure of transformer layer.\n/three.tnum./three.tnum. Discriminator and loss function\nTo distinguish the diﬀerence between the prediction and GT,\nthe discriminator D extracts features of GT and prediction to\ncalculate L1 norm distance between them. The discriminator is\ncomposed of six similar blocks. Each of these blocks consists\nof a 3 × 3 × 3 convolution layer with a stride of 2, a batch\nnormalization layer and a LeakyReLU activation layer. Instead\nof only using the ﬁnal output of D, we leverage the jth output\nfeature f i\nj (x) extracted by ith (i ∈ [1, 2, . . . , L]) layers from image\nx to calculate multi-scale L1 loss ℓD as follows:\nℓD\n(\nx, x\n′ )\n= 1\nL ∗ M\nL∑\ni=1\nM∑\nj=1\n\n\n f i\nj (x) − f i\nj\n(\nx\n′ ) \n\n 1\n(6)\nwhere M denotes the number of extracted features of a layer in\nD.\nReferring to the loss function of GAN (\nGoodfellow et al.,\n2014), our loss function of the whole adversarial process is\ndescribed as follows:\nmin\nθG\nmax\nθD\nL (θG, θD) = Ex∼Pdata\n(\nℓD\n(\nG(x), y\n))\n+Ex∼Pdata\n(\nℓdeep_bce_dice\n(\nG(x), y\n) ) (7)\nwhere x, y denote the input image and ground truth,\nrespectively, ℓdeep_bce_dice denotes that the segmentation maps\nof generator are used to calculate the BCE loss together with the\nDice loss under deep supervision. Concretely, ℓdeep_bce_dice is a\nweighted sum of ℓdeep_bce_dice (pi, yi),i ∈ [1, 2, 3] for prediction\npi and mask yi where i denotes the ith level of decoder ( Di).\nThe detailed training process is presented in\nAlgorithm 1,\nwhich interprets the procedure of sampling data and following\nupdating discriminator and generator with corresponding loss\nfunction respectively.\n1:fornumber of training epochesdo\n2: forsteps of training discriminatordo\n3: Get n input images frompdata\n{\nx1, ..., xn}\nand\ncorresponding labels\n{\ny1, ..., yn}\n.\n4: Update discriminator by maximizing the loss\nbelow:\n1\nn\nn∑\ni=1\n[\nℓD\n(\nG\n(\nxi)\n, yi)]\n5: Clip the weights of discriminator.\n6: end for\n7: Get n input images frompdata\n{\nx1, ..., xn}\nand\ncorresponding labels\n{\ny1, ..., yn}\n.\n8: Update generator by minimizing the loss\nbelow:\n1\nn\nn∑\ni=1\n[\nℓdeep_bce_dice\n(\nG\n(\nxi)\n, yi)\n+ ℓD\n(\nG\n(\nxi)\n, yi)]\n9:end for\nAlgorithm /one.tnum. The detailed training process.ℓdeep_bce_dice represents\nBCE Dice loss with deep supervision, ℓD represents multi-scale L/one.tnumloss.\n/four.tnum. Experimental results\n/four.tnum./one.tnum. Dataset\nIn the experiments, we evaluated our method using the\nBrain Tumor Image Segmentation Challenge 2015 (BRATS2015)\ndataset. In BRATS2015, the training dataset contains manual\nannotation by clinical experts for 220 patient cases with high-\ngrade glioma (HGG) and 55 patient cases with low-grade glioma\n(LGG), whereas 110 patient cases are supplied in the online\ntesting dataset without annotation. Four 3D MRI modalities—\nT1, T1c, T2, and FLAIR—are used for all patient cases, as\ndepicted in\nFigure 3. Each modality has the origin size 240 ×\n240×155 with the same voxel spacing. The ground truth has ﬁve\nclasses: background (label 0), necrosis (label 1), edema (label 2),\nnon-enhancing tumor (label 3), and enhancing tumor (label 4).\nFrontiers in Neuroscience /zero.tnum/seven.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nFIGURE /three.tnum\nHGG and LGG cases with four modalities: T/one.tnum, T/one.tnumc, T/two.tnum, FLAIR on BRATS/two.tnum/zero.tnum/one.tnum/five.tnum dataset.\nWe divided the 275 training cases into a training set and a\nvalidation set with the ratio 9:1 both in HGG and LGG. During\ntraining and validation, we padded the origin size 240 ×240×155\nto size 240 × 240 × 160 with zeros and then randomly cropped\ninto size 160 × 192 × 160, which makes sure that the most image\ncontent is included.\n/four.tnum./two.tnum. Evaluation metric\nTo evaluate the eﬀectiveness of a segmentation method, the\nmost basic thing is to compare it with the ground truth. In\nthe task of brain tumor segmentation, there are three main\nevaluation metrics compared with the ground truth: Dice,\nPositive predictive Value (PPV), and Sensitivity, deﬁned as\nfollows:\nDice (P, T) = 1\n2 ×\n⏐\n⏐P1\n⋂ T1\n⏐\n⏐\n(|P1| + |T1|) (8)\nPPV (P, T) =\n⏐\n⏐P1\n⋂ T1\n⏐\n⏐\n|P1| (9)\nSensitivity (P, T) =\n⏐\n⏐P0\n⋂ T0\n⏐\n⏐\n|T0| (10)\nwhere P represents the prediction segmented by our proposed\nmethods, T represents the corresponding ground truth. P1\nand T1 denote the brain tumor region in P and T, P0 and\nT0 denote the other region except brain tumor in P and T,\nrespectively, |·| calculates the number of voxels inside region, ∩\ncalculates the intersection of two regions. When Dice is larger,\nPPV and Sensitivity are larger at the same time, the predicted\nsegmentation is considered to be more similar to ground truth,\nproving that the segmentation method is more eﬀective.\n/four.tnum./three.tnum. Implementation details\nExperiments were run on NVIDIA A100-PCIE (4 × 40GB)\nsystem for 1,000 epochs (about 3 days) using the Adam\noptimizer (\nKingma and Ba, 2014 ). The target segmentation\nmaps are reorganized into three tumor subregions: whole tumor\n(WT), tumor core (TC), and enhancing tumor (ET). The initial\nlearning rate is 0.0001 and batch size is 4. The data augmentation\nconsists of three parts: (1) padding the data from 240 ×240×155\nto 240×240×160 with zeros; (2) randomizing the data’s cropping\nfrom 240 ×240×160 to 160 ×192×160; (3) random ﬂipping the\ndata across three axes by a probability with 0.5. Impacted by the\nvolumetric input size, the number of parameters of our network\nis larger than common 2D networks, generator: 58.0127M,\ntransformer blocks inside generator: 11.3977M, discriminator:\n75.4524M. Both the Dice loss in deep supervision and multi-\nscale L1 loss are employed to train the network in competing\nprogress. In inference, we converted the transformed three\nsubregions (WT, TC, ET) back to the original labels. Specially, we\nreplace the enhancing tumor with necrosis when the possibility\nFrontiers in Neuroscience /zero.tnum/eight.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nof enhancing tumor in segmentation map is less than the\nthreshold, which is chosen according to the online testing scores.\n/four.tnum./four.tnum. Impact of the number of generators\nand discriminators\nAs the BRATS2015 is a multi-label segmentation task,\nour architecture can be implemented with schemes where the\nnumber of generators and discriminators are diﬀerent. Each\nimplementation scheme in\nTable 2 is speciﬁcally described as\nfollows:\n• 1G-1D. The network is composed of one generator and\none discriminator. The generator outputs three-channel\nsegmentation maps corresponding to three brain tumor\nsubregions, while the discriminator is fed with three-class\nmasked images concatenated in channel dimension.\n• 1G-3D. The network is composed of one generator and\nthree discriminators. The generator outputs three-channel\nsegmentation maps while the discriminators output three\none-channel maps, each for one class.\n• 3G-3D. The network is composed of three generators and\nthree discriminators. Each generator or discriminator is\nbuilt for one class. There are three pairs of generators\nand discriminators, indicating that each pair is trained\nindependently for one class.\n/four.tnum./five.tnum. Evaluating the transformer with\nResnet module\nTo evaluate the eﬀectiveness of the transformer with Resnet\nmodule, we conduct some ablation experiments. We design the\nbottom layer of our proposed generator with diﬀerent schemes\nas follows:\n• Transformer with Resnet. The bottom layer is composed of\nTransformer with Resnet we proposed.\n• Transformer w/o Resnet. The bottom layer is composed\nof Transformer block, ranging from projection, position\nembedding to transformer layers, without shortcut crossing\nthem.\n• CNN with Resnet. The bottom layer is composed of\nconvolutional layers together with a shortcut crossing\nthem.\n• Shortcut. The bottom layer is simply a shortcut connection\nfrom the encoder part to the decoder part.\nTABLE /two.tnum Results of diﬀerent number of generators and discriminators.\nMethod Dice Positive predictive value Sensitivity\nWhole Core Enha. Whole Core Enha. Whole Core Enha.\n1G-3D 0.85 0.73 0.63 0.83 0.79 0.59 0.90 0.73 0.73\n1G-1D 0.84 0.72 0.62 0.82 0.78 0.58 0.89 0.72 0.71\n3G-3D 0.81 0.68 0.60 0.83 0.74 0.62 0.84 0.70 0.63\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nTABLE /three.tnum Results of diﬀerent bottom layer in generator.\nMethod Dice Positive predictive value Sensitivity\nWhole Core Enha. Whole Core Enha. Whole Core Enha.\nTransformer with Resnet 0.85 0.73 0.63 0.83 0.79 0.59 0.90 0.73 0.73\nTransformer w/o Resnet 0.85 0.71 0.61 0.83 0.79 0.60 0.90 0.6 9 0.68\nCNN with Resnet 0.83 0.68 0.58 0.80 0.78 0.58 0.91 0.66 0.62\nShortcut 0.82 0.67 0.60 0.82 0.77 0.63 0.87 0.67 0.63\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nTABLE /four.tnum Results of diﬀerent discriminators training from scratch.\nMethod Dice Positive predictive value Sensitivity\nWhole Core Enha. Whole Core Enha. Whole Core Enha.\nCNN-based 0.85 0.73 0.63 0.83 0.79 0.59 0.90 0.73 0.73\nTransformer-based 0.79 0.66 0.58 0.79 0.77 0.55 0.86 0.64 0. 66\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nFrontiers in Neuroscience /zero.tnum/nine.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nTABLE /five.tnum Results of diﬀerent loss function.\nMethod Dice Positive predictive value Sensitivity\nWhole Core Enha. Whole Core Enha. Whole Core Enha.\nOur method 0.85 0.73 0.63 0.83 0.79 0.59 0.90 0.73 0.73\nw/o deep supervision 0.85 0.72 0.61 0.83 0.78 0.57 0.90 0.73 0. 71\nSingle-scale L1 loss 0.84 0.72 0.61 0.82 0.78 0.58 0.89 0.72 0.71\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nFIGURE /four.tnum\nDetailed evaluation curves of diﬀerent loss function.\nTABLE /six.tnum Performance of some methods on BRATS/two.tnum/zero.tnum/one.tnum/five.tnum testing dataset.\nMethod Dice Positive predictive value Sensitivity\nWhole Core Enha. Whole Core Enha. Whole Core Enha.\nUNET (Ronneberger et al., 2015 ) 0.80 0.63 0.64 0.83 0.81 0.78 0.80 0.58 0.60\nToStaGAN (Ding et al., 2021 ) 0.85 0.71 0.62 0.87 0.86 0.63 0.87 0.68 0.69\n3D Fusing ( Zhao et al., 2018 ) 0.84 0.73 0.62 0.89 0.76 0.63 0.82 0.76 0.67\nFSENet (Chen et al., 2018 ) 0.85 0.72 0.61 0.86 0.83 0.66 0.85 0.68 0.63\nSegAN (Xue et al., 2018 ) 0.85 0.70 0.66 0.92 0.80 0.69 0.80 0.65 0.62\nOur method 0.85 0.73 0.63 0.83 0.79 0.59 0.90 0.73 0.73\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nThe comparation results are shown in Table 3. From\nthe results, we demonstrate the transformer’s superiority and\nirreplaceability, and we can conclude that transformer with\nResnet module make the best of features from transformer\nblock and convolutional encoder to improve the segmentation\nperformance.\n/four.tnum./six.tnum. Evaluating the CNN-based\ndiscriminator\nWe select the CNN-based discriminator instead of\nthe transformer-based one as our ﬁnal discriminator in\nour architecture, due to our opinion that transformer-\nbased multi-layers discriminator requires huge datasets to\nsupport pre-training. To prove that, we conduct ablation\nexperiments to compare their performance by training from\nscratch. The transformer-based discriminator is implemented\nusing the inspiration of\nJiang et al. (2021). Table 4 shows\nthe results on BRATS2015 testing dataset using diﬀerent\ndiscriminators, from which our CNN-based discriminator\nshows its superior capability of classifying the ground truth\nand segmentation outputs from scratch. Without pre-training,\nthe CNN-based discriminator appears to be better than the\ntransformer-based one.\n/four.tnum./seven.tnum. Evaluating the loss function\nIn this section, we evaluate the eﬀectiveness of the loss\nfunction in our proposed methods. As shown in Equation 7,\nour loss function is divided into two parts: the deep supervision\nFrontiers in Neuroscience /one.tnum/zero.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nFIGURE /five.tnum\nExperimental results with corresponding slices on BRATS/two.tnum/zero.tnum/one.tnum/five.tnum validation set. The red arrows locate the mainly diﬀerent regions b etween GT and\nsegmentation results.\nloss and multi-scale L1 loss. We conduct two ablation\nexperiments: one model with single-scale L1 loss, the other\nmodel without deep supervision loss. It is worth noting that\nthe implementation of these models is the same as 1G-\n3D where the network consists of one generator and three\ndiscriminators and employs the transformer with Resnet module\nin the bottom layer. From\nTable 5, we ﬁnd that our loss\nfunction achieves better performance under the same other\nexperimental environment.\nThe detailed segmentation evaluation scores curves with\ndiﬀerent loss function are depicted in\nFigure 4. It is clear that the\nsegmentation performance of all approaches steadily increases\nFrontiers in Neuroscience /one.tnum/one.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nFIGURE /six.tnum\nFrom left to right is the visualization results of an original image i n FLAIR modality, ground truth, model in our method, model in the form o f\nUNET + GAN, model UNET. From up to down in each column is three segmenta tion maps predicted with the same method. The blue boxes\noutline the diﬀerence between results from diﬀerent methods.\nas the number of epochs increases until it reaches a steady state.\nRanging from WT, TC to ET, our method shows an increasing\nperformance boost over other methods. As a consequence, our\nmethod yields the best results in all evaluation metrics listed in\nTable 5.\n/four.tnum./eight.tnum. Comparison with other methods\nTo obtain a more robust prediction, we ensemble 10\nmodels trained with the whole training dataset to average the\nsegmentation probability maps. We upload the results of our\nmethods on the BRATS2015 dataset and get the testing scores\ncomputed via the online evaluation platform, as listed in\nTable 6.\nFigure 5 shows our qualitative segmentation output\non BRATS2015 validation set. This ﬁgure illustrates\ndiﬀerent slices of diﬀerent patient cases in ground truth\nand predictions separately.\n/four.tnum./nine.tnum. Qualitative analysis\nTo demonstrate the performance of our proposed method,\nwe randomly choose a slice of one patient on BRATS2015\nvalidation set to visualize and compare the result in\nFigure 6.\nIn Figure 6, images in the same column are produced from the\nsame method, and images in the same row are belonging to\nthe same segmentation label. Concretely, the column FLAIR\nrepresents the original image with modality of FLAIR, while\nother columns are segmentation maps with corresponding\ncategories and colors: WT is yellow, TC is green, and ET\nis red. The column UNET represents that the corresponding\nthree segmentation maps are inferenced with model UNET.\nThe model of the column UNET plus GAN is built based\non UNET, with an addition of GAN, where the generator is\nUNET with deep supervision and discriminator is a CNN-based\nnetwork with multi-scale L1 loss. A deep insight of\nFigure 6\nreveals that with the help of deep supervision and multi-scale\nL1 loss, the UNET+GAN method segments fuller edges and\nricher details than UNET method. When the transformer block\nis applied, our method produces more smooth borders on the\ntumor core regions, and more complete contours on enhancing\ntumor regions. The reason for this improvement seems to\nbe that the transformer with Resnet module can eﬀectively\nmodel the short-range and long-range dependency, and collect\nboth local and global contexture representation information.\nOwing to more complete features, our method achieves the\nbetter performance.\nFrontiers in Neuroscience /one.tnum/two.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nTABLE /seven.tnum Comparison to other methods on BRATS/two.tnum/zero.tnum/one.tnum/eight.tnum validation\ndataset.\nMethod Dice(mean) Hausdorﬀ(mm)\nEnha. Whole Core Enha. Whole Core\nMyronenko (2018) 0.7664 0.8836 0.8154 3.7731 5.9044 4.8091\nHu et al. (2019) 0.7178 0.8824 0.7481 2.8000 4.4800 7.0700\nChandra et al. (2018) 0.7406 0.8719 0.7990 5.5757 5.0379 9.5884\nLiu (2018) 0.7639 0.8958 0.7905 4.0714 4.4924 8.1971\nOur method 0.7686 0.9021 0.8089 5.7116 5.4183 9.4049\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\nTABLE /eight.tnum Comparison to other methods on BRATS/two.tnum/zero.tnum/two.tnum/zero.tnum validation\ndataset.\nMethod Dice(mean) Hausdorﬀ(mm)\nEnha. Whole Core Enha. Whole Core\nTang et al. (2020) 0.703 0.893 0.790 34.306 4.629 10.071\nZhou et al. (2022) 0.647 0.818 0.759 44.400 10.000 14.600\nAnand et al. (2020) 0.710 0.880 0.740 38.310 6.880 32.000\nZhang et al. (2021) 0.700 0.880 0.740 38.600 7.000 30.200\nOur method 0.708 0.903 0.815 37.579 4.909 7.494\nWhole, whole tumor; Core, tumor core; Enha., enhancing tumor.\n/four.tnum./one.tnum/zero.tnum. Generalization on other datasets\nTo evaluate generalization of our proposed method, we\nconduct additional experiments on other datasets relative to\nbrain tumor segmentation, BRATS2018 and BRATS2020, which\nare composed of more practical patient cases. These datasets\ndiﬀer from BRATS2015 dataset in labels, number of cases\nand diﬃculty. The detailed inference performance are listed in\nTables 7, 8. On BRATS2018 validation dataset, our proposed\nmethod achieves Dice score of 0.7686, 0.9021, and 0.8089,\nand Hausdorﬀ (HD) of 5.7116, 5.4183, and 9.4049 mm on\nET, WT, and TC, respectively. On BRATS2020 validation\ndataset, our method also realizes Dice score of 0.708, 0.903,\nand 0.815 and HD of 37.579, 4.909, and 7.494 mm on\nET, WT, and TC, respectively. These excellent scores reveal\nthe great generalization of our transformer-based generative\nadversarial network.\n/five.tnum. Discussion and conclusion\nIn this paper, we explored the application of a transformer-\nbased generative adversarial network for segmenting 3D\nMRI brain tumors. Unlike many other encoder–decoder\narchitectures, our generator employs a transformer with Resnet\nmodule to eﬀectively model the long-distance dependency in a\nglobal space, simultaneously inheriting the advantage of CNNs\nfor learning the capability of local contexture representations.\nMoreover, the application of deep supervision improves the\nﬂowability of gradient to some extent. Our discriminator is\napplied to measure the norm distance of hierarchical features\nfrom predictions and masks. Particularly, we calculate multi-\nscale L1 loss between the generator segmentation maps and\nground truth. Experimental results on BRATS2015, BRATS2018,\nand BRATS2020 datasets show a better performance of our\nproposed method in comparison of other state-of-the-art\nmethods, which proves the superior generalization of our\nmethod in brain tumor segmentation.\nData availability statement\nThe dataset BRATS2015 ( Menze et al., 2014 ; Kistler et al.,\n2013) for this study can be found in the https://www.smir.ch/\nBRATS/Start2015. The dataset BRATS2018, BRATS2020 ( Menze\net al., 2014 ; Bakas et al., 2017 , 2018) and online evaluation\nplatform can be found in this https://ipp.cbica.upenn.edu.\nAuthor contributions\nLH: conceptualization, methodology, software, project\nadministration, writing—original draft, writing—review, and\nediting. EZ: medical expert. LC: validation and project\nadministration. ZW and BZ: supervision. SC: supervision,\nresources, formal analysis, and funding acquisition. All authors\ncontributed to the article and approved the submitted version.\nFunding\nThis work was funded in part by the National Natural\nScience Foundation of China (Grant Nos. 82170374 and\n82202139), and also supported in part by the Capital\nMedical Funds for Health Improvement and Research\n(CHF2020-1-1053).\nConﬂict of interest\nThe authors declare that the research was conducted in\nthe absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nFrontiers in Neuroscience /one.tnum/three.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nReferences\nAnand, V. K., Grampurohit, S., Aurangabadkar, P., Kori, A., Kh ened, M.,\nBhat, R. S., et al. (2020). “Brain tumor segmentation and surv ival prediction\nusing automatic hard mining in 3d CNN architecture, ” in International MICCAI\nBrainlesion Workshop(Virtual: Springer), 310–319.\nAsis-Cruz, J. D., Krishnamurthy, D., Jose, C., Cook, K. M., a nd\nLimperopoulos, C. (2022). Fetalgan: automated segmentation of fetal\nfunctional brain mri using deep generative adversarial lear ning and multi-\nscale 3D u-Net. Front. Neurosci . 16, 887634. doi: 10.3389/fnins.2022.8\n87634\nBakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Ki rby, J. S.,\net al. (2017). Advancing the cancer genome atlas glioma MRI collec tions\nwith expert segmentation labels and radiomic features. Sci. Data. 4, 1–13.\ndoi: 10.1038/sdata.2017.117\nBakas, S., Reyes, M., Jakab, A., Bauer, S., Rempﬂer, M., Crimi, A., et al. (2018).\nIdentifying the best machine learning algorithms for brain tu mor segmentation,\nprogression assessment, and overall survival prediction in th e BRATS challenge.\narXiv [Preprint]. arXiv: 1811.02629. Available online at: https://arxiv.org/pdf/1811.\n02629.pdf\nBochkovskiy, A., Wang, C.-Y., and Liao, H.-Y. M. (2020). Yolo v4: optimal\nspeed and accuracy of object detection. arXiv preprint arXiv:2004.10934 .\ndoi: 10.48550/arXiv.2004.10934\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). “End-to-end object detection with transformers, ” in European Conference\non Computer Vision(Glasgow: Springer), 213–229.\nChandra, S., Vakalopoulou, M., Fidon, L., Battistella, E., Estie nne, T., Sun, R., et\nal. (2018). “Context aware 3-D residual networks for brain tu mor segmentation, ”\nin International MICCAI Brainlesion Workshop(Granada), 74–82.\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., et al. (202 1). Transunet:\ntransformers make strong encoders for medical image segmen tation. arXiv preprint\narXiv:2102.04306. doi: 10.48550/arXiv.2102.04306\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille , A. L. (2014).\nSemantic image segmentation with deep convolutional nets an d fully connected\ncrfs. arXiv preprint arXiv:1412.7062. doi: 10.48550/arXiv.1412.7062\nChen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., and Yuille , A. L.\n(2017). Deeplab: Semantic image segmentation with deep convo lutional nets,\natrous convolution, and fully connected crfs. IEEE Trans. Pattern. Anal. Mach.\nIntell. 40, 834–848. doi: 10.1109/TPAMI.2017.2699184\nChen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I ., and Abbeel, P.\n(2016). “Infogan: interpretable representation learning by in formation maximizing\ngenerative adversarial nets, ” in Advances in Neural Information Processing Systems,\nVol. 29(Barcelona).\nChen, X., Liew, J. H., Xiong, W., Chui, C.-K., and Ong, S.-H. ( 2018). “Focus,\nsegment and erase: an eﬃcient network for multi-label brain tu mor segmentation, ”\nin Proceedings of the European Conference on Computer Vision (ECCV)(Munich),\n654–669.\nChoi, J., Kim, T., and Kim, C. (2019). “Self-ensembling with ga n-based data\naugmentation for domain adaptation in semantic segmentatio n, ” inProceedings of\nthe IEEE/CVF International Conference on Computer Vision(Seoul), 6830–6840.\nClark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. (2020). E lectra: pre-\ntraining text encoders as discriminators rather than gener ators. arXiv preprint\narXiv:2003.10555. doi: 10.48550/arXiv.2003.10555\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ert: pre-training\nof deep bidirectional transformers for language understand ing. arXiv preprint\narXiv:1810.04805. doi: 10.48550/arXiv.1810.04805\nDing, Y., Zhang, C., Cao, M., Wang, Y., Chen, D., Zhang, N., et al. (2021).\nTostagan: an end-to-end two-stage generative adversarial network for brain tumor\nsegmentation. Neurocomputing 462, 141–153. doi: 10.1016/j.neucom.2021.07.066\nDong, X., Lei, Y., Wang, T., Thomas, M., Tang, L., Curran, W. J ., et al. (2019).\nAutomatic multiorgan segmentation in thorax ct images using u-net-gan. Med.\nPhys. 46, 2157–2168. doi: 10.1002/mp.13458\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai,\nX., Unterthiner, T., et al. (2020). An image is worth 16x16 wor ds:\ntransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\ndoi: 10.48550/arXiv.2010.11929\nGirshick, R. (2015). “Fast R-CNN, ” in Proceedings of the IEEE International\nConference on Computer Vision(Santiago: IEEE), 1440–1448.\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). “Ri ch feature\nhierarchies for accurate object detection and semantic seg mentation, ” in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(Columbus, OH: IEEE), 580–587.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Fa rley, D., Ozair,\nS., et al. (2014). “Generative adversarial nets, ” in Advances in Neural Information\nProcessing Systems, Vol. 27(Montreal, QC).\nHan, Z., Wei, B., Mercado, A., Leung, S., and Li, S. (2018). Spi ne-gan:\nSemantic segmentation of multiple spinal structures. Med. Image Anal. 50, 23–35.\ndoi: 10.1016/j.media.2018.08.005\nHatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A. , Landman,\nB., et al. (2022). “UNETR: transformers for 3D medical image s egmentation, ” in\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision\n(Waikoloa, HI: IEEE), 574–584.\nHe, K., Gkioxari, G., Dollár, P., and Girshick, R. (2017). “Mask R- CNN, ” in\nProceedings of the IEEE International Conference on Computer Vision(Venice:\nIEEE), 2961–2969.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” inProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (Las Vegas, NV: IEEE), 770–778.\nHe, R., Xu, S., Liu, Y., Li, Q., Liu, Y., Zhao, N., et al. (2021). Three-\ndimensional liver image segmentation using generative adve rsarial networks\nbased on feature restoration. Front. Med. 8, 794969. doi: 10.3389/fmed.2021.\n794969\nHu, K., Gan, Q., Zhang, Y., Deng, S., Xiao, F., Huang, W., et al. (2019).\nBrain tumor segmentation using multi-cascaded convolutiona l neural\nnetworks and conditional random ﬁeld. IEEE Access 7, 92615–92629.\ndoi: 10.1109/ACCESS.2019.2927433\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2 017). “Densely\nconnected convolutional networks, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(Honolulu, HI: IEEE), 4700–4708.\nIsola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017). “Imag e-to-image\ntranslation with conditional adversarial networks, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Honolulu, HI: IEEE),\n1125–1134.\nJiang, Y., Chang, S., and Wang, Z. (2021). Transgan: two pure t ransformers\ncan make one strong gan, and that can scale up. Adv. Neural Inf. Process. Syst. 34,\n14745–14758. doi: 10.48550/arXiv.2102.07074\nKhan, M. Z., Gajendran, M. K., Lee, Y., and Khan, M. A. (2021). Deep\nneural architectures for medical image semantic segmentat ion. IEEE Access 9,\n83002–83024. doi: 10.1109/ACCESS.2021.3086530\nKingma, D. P., and Ba, J. (2014). Adam: a method for stochasti c optimization.\narXiv preprint arXiv:1412.6980. doi: 10.48550/arXiv.1412.6980\nKistler, M., Bonaretti S., Pfahrer, M., Niklaus, R., and Buchle r, P. (2013). The\nvirtual skeleton database: An open access repository for biome dical research and\ncollaboration. J. Med. Internet Res. 15, e245. doi: 10.2196/jmir.2930\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). “Ima genet classiﬁcation\nwith deep convolutional neural networks, ” in Advances in Neural Information\nProcessing Systems, Vol. 25 (Lake Tahoe).\nLe L, Yefeng Z, Gustavo C, Lin Y. (2017). “Deep learning and conv olutional\nneural networks for medical image computing - precision medic ine, high\nperformance and large-scale datasets, ” in Advances in Computer Vision and Pattern\nRecognition (Springer).\nLeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Grad ient-\nbased learning applied to document recognition. Proc. IEEE 86, 2278–2324.\ndoi: 10.1109/5.726791\nLin, G., Milan, A., Shen, C., and Reid, I. (2017). “Reﬁnenet: mu lti-path\nreﬁnement networks for high-resolution semantic segmentat ion, ” inProceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition(Honolulu, HI:\nIEEE), 1925–1934.\nLiu, M. (2018). “Coarse-to-ﬁne deep convolutional neural ne tworks for multi-\nmodality brain tumor semantic segmentation, ” in MICCAI BraTs Conference\n(Granada).\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. -Y., et al. (2016).\n“Ssd: single shot multibox detector, ” in European Conference on Computer Vision\n(Amsterdam: Springer), 21–37.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (20 19). Roberta:\na robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\ndoi: 10.48550/arXiv.1907.11692\nFrontiers in Neuroscience /one.tnum/four.tnum frontiersin.org\nHuang et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/five.tnum/four.tnum/nine.tnum/four.tnum/eight.tnum\nLong, J., Shelhamer, E., and Darrell, T. (2015). “Fully convolutio nal networks for\nsemantic segmentation, ” inProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition(Boston), 3431–3440.\nMenze, B., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahan i, K., Kirby, J., et\nal. (2014). The multimodal brain tumor image segmentation ben chmark (BRATS).\nIEEE Trans. Med. Imag.34, 1993–2024. doi: 10.1109/TMI.2014.2377694\nMirza, M., and Osindero, S. (2014). Conditional generative adversarial nets.\narXiv preprint arXiv:1411.1784. doi: 10.48550/arXiv.1411.1784\nMyronenko, A. (2018). “3D MRI brain tumor segmentation usin g autoencoder\nregularization, ” in International MICCAI Brainlesion Workshop (Granada:\nSpringer), 311–320.\nNaseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahba z Khan, F., and\nYang, M.-H. (2021). Intriguing properties of vision transfor mers. Adv. Neural Inf.\nProcess. Syst. 34, 23296–23308. doi: 10.48550/arXiv.2105.10497\nNishio, M., Fujimoto, K., Matsuo, H., Muramatsu, C., Sakamo to, R.,\nand Fujita, H. (2021). Lung cancer segmentation with transf er learning:\nusefulness of a pretrained model constructed from an artiﬁcia l dataset\ngenerated using a generative adversarial network. Front. Artif. Intell. 4, 694815.\ndoi: 10.3389/frai.2021.694815\nOdena, A., Olah, C., and Shlens, J. (2017). “Conditional image synthesis\nwith auxiliary classiﬁer gans, ” in International Conference on Machine Learning\n(Sydney), 2642–2651.\nOh, K. T., Lee, S., Lee, H., Yun, M., and Yoo, S. K. (2020). Sema ntic segmentation\nof white matter in fdg-pet using generative adversarial netw ork. J. Digit. Imaging\n33, 816–825. doi: 10.1007/s10278-020-00321-5\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018).\nImproving Language Understanding by Generative Pre-Training. (2018). Available\nonline at: https://s3-us-west-2.amazonaws.com/openai-assets/rese arch-covers/\nlanguage-unsupervised/language_understanding_paper.pdf\nRazmjooy, N., Ashourian, M., Karimifard, M., Estrela, V. V., Loschi, H. J.,\nDo Nascimento, D., et al. (2020). Computer-aided diagnosis of skin cancer: a\nreview. Curr. Med. Imaging 16, 781–793. doi: 10.2174/15734056166662001290\n95242\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). “You only\nlook once: uniﬁed, real-time object detection, ” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition(Las Vegas, NV: IEEE),\n779–788.\nRedmon, J., and Farhadi, A. (2017). “Yolo9000: better, faste r, stronger, ” in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(Honolulu, HI: IEEE), 7263–7271.\nRedmon, J., and Farhadi, A. (2018). Yolov3: an incremental im provement. arXiv\npreprint arXiv:1804.02767. doi: 10.48550/arXiv.1804.02767\nRen, S., He, K., Girshick, R., and Sun, J. (2015). “Faster R-CN N: towards\nreal-time object detection with region proposal networks, ” in Advances in Neural\nInformation Processing Systems, Vol. 28 (Montreal, QC: Sydney).\nRonneberger, O., Fischer, P., and Brox, T. (2015). “U-net: c onvolutional\nnetworks for biomedical image segmentation, ” in International Conference on\nMedical Image Computing and Computer-Assisted Intervention(Munich: Springer),\n234–241.\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutio nal\nnetworks for large-scale image recognition. arXiv preprint arXiv:1409.1556 .\ndoi: 10.48550/arXiv.1409.1556\nStoitsis, J., Valavanis, I., Mougiakakou, S. G., Golemati, S., N ikita, A., and\nNikita, K. S. (2006). Computer aided diagnosis based on medic al image processing\nand artiﬁcial intelligence methods. Nucl. Instrum. Methods Phys. Res. A: Accel.\nSpectrom. Detect. Assoc. Equip.569, 591–595. doi: 10.1016/j.nima.2006.08.134\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelo v, D., et al. (2015).\n“Going deeper with convolutions, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(Boston, MA: IEEE), 1–9.\nTang, J., Li, T., Shu, H., and Zhu, H. (2020). “Variational-au toencoder\nregularized 3D multiresunet for the brats 2020 brain tumor seg mentation, ” in\nInternational MICCAI Brainlesion Workshop(Virtual: Springer), 431–440.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems. p. 5998–6008.\nWang, T., Wang, M., Zhu, W., Wang, L., Chen, Z., Peng, Y., et al. (2021). Semi-\nmsst-gan: a semi-supervised segmentation method for cornea l ulcer segmentation\nin slit-lamp images. Front. Neurosci. 15, 793377. doi: 10.3389/fnins.2021.793377\nWang, W., Chen, C., Ding, M., Yu, H., Zha, S., and Li, J. (2021) . “Transbts:\nmultimodal brain tumor segmentation using transformer, ” in International\nConference on Medical Image Computing and Computer-Assisted Intervention\n(Virtual: Springer), 109–119.\nXue, Y., Xu, T., Zhang, H., Long, L. R., and Huang, X. (2018). S egan: adversarial\nnetwork with multi-scale l1 loss for medical image segmentation . Neuroinformatics\n16, 383–392. doi: 10.1007/s12021-018-9377-x\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R. , and Le, Q. V.\n(2019). “Xlnet: generalized autoregressive pretraining for la nguage understanding, ”\nin Advances in Neural Information Processing Systems, Vol. 32(Vancouver, BC).\nZhan, Q., Liu, Y., Liu, Y., and Hu, W. (2021). Frontal cortex s egmentation\nof brain pet imaging using deep neural networks. Front. Neurosci. 15, 796172.\ndoi: 10.3389/fnins.2021.796172\nZhang, W., Yang, G., Huang, H., Yang, W., Xu, X., Liu, Y., et al. ( 2021). Me-net:\nmulti-encoder net framework for brain tumor segmentation. Int. J. Imaging Syst.\nTechnol. 31, 1834–1848. doi: 10.1002/ima.22571\nZhao, X., Wu, Y., Song, G., Li, Z., Zhang, Y., and Fan, Y. (2018) . A deep learning\nmodel integrating fcnns and crfs for brain tumor segmentati on. Med. Image Anal.\n43, 98–111. doi: 10.1016/j.media.2017.10.002\nZhou, J., Ye, J., Liang, Y., Zhao, J., Wu, Y., Luo, S., et al. (20 22). scse-\nnl v-net: A brain tumor automatic segmentation method based on spatial and\nchannel “squeeze-and-excitation” network with non-local blo ck. Front. Neurosci.\n16, 916818. doi: 10.3389/fnins.2022.916818\nZhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017). “Unpai red image-to-image\ntranslation using cycle-consistent adversarial networks, ” in Proceedings of the IEEE\nInternational Conference on Computer Vision(Venice: IEEE), 2223–2232.\nZhu, Q., Du, B., Turkbey, B., Choyke, P. L., and Yan, P. (2017) . “Deeply-\nsupervised cnn for prostate segmentation, ” in 2017 International Joint Conference\non Neural Networks (IJCNN)(Anchorage, AK: IEEE), 178–184.\nFrontiers in Neuroscience /one.tnum/five.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.780056357383728
    },
    {
      "name": "Discriminator",
      "score": 0.7631417512893677
    },
    {
      "name": "Transformer",
      "score": 0.7283099889755249
    },
    {
      "name": "Segmentation",
      "score": 0.6528749465942383
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6140080094337463
    },
    {
      "name": "Encoder",
      "score": 0.5541601777076721
    },
    {
      "name": "Deep learning",
      "score": 0.46424400806427
    },
    {
      "name": "Architecture",
      "score": 0.43598127365112305
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4129161834716797
    },
    {
      "name": "Computer vision",
      "score": 0.35097557306289673
    },
    {
      "name": "Engineering",
      "score": 0.09490296244621277
    },
    {
      "name": "Telecommunications",
      "score": 0.07112812995910645
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}