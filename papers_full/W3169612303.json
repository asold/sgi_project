{
  "title": "Video Super-Resolution Transformer",
  "url": "https://openalex.org/W3169612303",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225581206",
      "name": "Cao, Jiezhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2062210512",
      "name": "Li Yawei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1913798565",
      "name": "Zhang Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742174613",
      "name": "Van Gool Luc",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3101803587",
    "https://openalex.org/W3176148916",
    "https://openalex.org/W2965669158",
    "https://openalex.org/W3119964427"
  ],
  "abstract": "Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer.",
  "full_text": "Video Super-Resolution Transformer\nJiezhang Cao1 Yawei Li1,âˆ— Kai Zhang1 Luc Van Gool1,2\n1Computer Vision Lab, ETH ZÃ¼rich, Switzerland 2KU Leuven, Belgium\n{jiezhang.cao, yawei.li, kai.zhang, vangool}@vision.ee.ethz.ch\nAbstract\nVideo super-resolution (VSR), with the aim to restore a high-resolution video\nfrom its corresponding low-resolution version, is a spatial-temporal sequence\nprediction problem. Recently, Transformer has been gaining popularity due to its\nparallel computing ability for sequence-to-sequence modeling. Thus, it seems to be\nstraightforward to apply the vision Transformer to solve VSR. However, the typical\nblock design of Transformer with a fully connected self-attention layer and a token-\nwise feed-forward layer does not fit well for VSR due to the following two reasons.\nFirst, the fully connected self-attention layer neglects to exploit the data locality\nbecause this layer relies on linear layers to compute attention maps. Second, the\ntoken-wise feed-forward layer lacks the feature alignment which is important for\nVSR since this layer independently processes each of the input token embeddings\nwithout any interaction among them. In this paper, we make the first attempt to\nadapt Transformer for VSR. Specifically, to tackle the first issue, we present a\nspatial-temporal convolutional self-attention layer with a theoretical understanding\nto exploit the locality information. For the second issue, we design a bidirectional\noptical flow-based feed-forward layer to discover the correlations across different\nvideo frames and also align features. Extensive experiments on several benchmark\ndatasets demonstrate the effectiveness of our proposed method. The code will be\navailable at https://github.com/caojiezhang/VSR-Transformer.\n1 Introduction\nVideo super-resolution (VSR) refers to the task of enhancing low-resolution (LR) video to high-\nresolution (HR) one and has been successfully applied in some computer vision applications, such as\nvideo surveillance [13] and high-definition television [26]. Generally, VSR can be formulated as a\nsequence modeling problem that can be solved by some sequence-to-sequence models, such as RNN\n[6], LSTM [11] and Transformer [27]. Compared with RNN and LSTM, Transformer gains particular\ninterest largely due to its recursion-free nature for parallel computing and modeling capacity for\nlong-term dependencies of the input sequence. Specifically, a Transformer block consists of two\nkinds of layers: a fully connected self-attention layer and a token-wise feed-forward layer, with skip\nconnections in both layers. Although Transformer has shown to work well for various computer\nvision tasks, directly applying it for VSR may suffer from two main limitations.\nFirst, while the locality is well-known to be crucial for VSR, the fully connected self-attention\n(FCSA) layer neglects to leverage such information in a video sequence. Typically, most existing\nvision Transformer methods (e.g., ViT [8] and IPT [5]) split an image into several patches or tokens,\nwhich may damage the local spatial information [17] to some extent since the contents (e.g., lines,\nedges, shapes, and even objects) are divided into different tokens. In addition, this layer focuses on\nglobal interaction between the token embeddings by using several fully connected layers to compute\nattention maps which are irrelevant to local information. As a result, the FCSA layer neglects to\nâˆ—Co-first author.\nPreprint. Under review.\narXiv:2106.06847v3  [cs.CV]  4 Jul 2023\nexploit the local information. For VSR, it has been shown that temporal information is of vital\nimportance. When a region in some frames is occluded, the missing information can be recovered by\nother neighboring frames. Yet, it is still unclear for vision Transformer how to exploit the correlations\namong neighboring frames to improve the performance of VSR.\nSecond, the token-wise feed-forward layer cannot align features between video frames since this layer\nindependently processes each of the input token embeddings without any interaction across them.\nAlthough such interaction is contained in the FCSA layer, it ignores the feature propagation in video\nframes which contains rich bidirectional information. The feature propagation and feature alignment\nare crucial components in VSR since they help to exploit and align such information in a video\nsequence. However, most existing vision Transformer methods [33] lack both feature propagation\nand alignment. Without feature propagation, these methods generally fail to jointly capture past and\nfuture information. As a result, the features may be unaligned to each frame, which leads to inferior\nperformance. In short, explicit feature alignment and propagation mechanism are worth studying\nto improve the VSR performance. In addition, it is impractical to directly use the fully connected\nlinear layer to VSR since it has an expensive computational cost for many high-dimensional frames.\nTherefore, it is very necessary and important to explore a new feed-forward layer to perform the\nfeature propagation and alignment for VSR.\nTo address these two limitations, we propose a new Transformer for video super-resolution, called\nVSR-Transformer, which consists of a spatial-temporal convolutional self-attention (STCSA) layer\nand a bidirectional optical flow-based feed-forward (BOFF) layer. First, the STCSA layer exploits\nthe locality from all token embeddings by introducing convolutional layers. Then the BOFF layer\nlearns the spatial-temporal information with the feature propagation and alignment.\nThe main contributions of this paper are summarized as follows:\nâ€¢ We propose a spatial-temporal convolutional attention layer to exploit the locality and spatial-\ntemporal data information through different layers. We provide a theoretical analysis to support\nthat our layer has an advantage over the fully connected self-attention layer.\nâ€¢ We design a new bidirectional optical flow-based feed-forward layer to use the interaction across\nall frame embeddings. This layer is able to improve the performance of VSR by performing feature\npropagation and alignment. Moreover, it alleviates the limitations of the traditional Transformer.\nâ€¢ We provide extensive experiments on several benchmark datasets to demonstrate the effectiveness\nof VSR-Transformer against state-of-the-art methods, especially for the limited number of frames.\n2 Related Work\nWith the help of deep neural networks [2, 3, 9], super-resolution (SR) which aims to reconstruct HR\nimages/videos from LR images/videos has drawn significant attention. Recently, several attempts use\nTransformer to solve SR. For example, TTSR [35] proposes a texture Transformer by transferring HR\ntextures from the reference image to the LR image. IPT [5] develops a new pre-trained model to study\nthe low-level computer vision task, including SR. However, it is non-trivial and difficult to directly\nextend these Transformer-based image SR methods to VSR. Generally, existing VSR approaches\n[4, 12, 14, 15, 26, 29] can be mainly divided into two frameworks: sliding-window and recurrent.\nVideo super-resolution. Earlier sliding window methods [ 1, 25, 34] predict the optical flow\nbetween LR frames and perform the alignment by spatial warping. To improve the performance of\nVSR, TDAN [26] uses deformable convolutions (DCNs) [7, 28] to adaptively align the reference\nframe and each supporting frame at the feature level. Motivated by TDAN, EDVR [ 29] propose\na video restoration framework by further enhancing DCNs to improve the feature alignment in a\nmulti-scale fashion. This method first devises a pyramid cascading and deformable (PCD) alignment\nmodule to handle large motions and then uses a temporal and spatial attention (TSA) module to fuse\nimportant features. To implicitly handle motions, DUF [15] leverages dynamic upsampling filters.\nIn addition, some approaches take a recurrent framework. For example, RSDN [ 12] proposes a\nrecurrent detail-structural block to exploit previous frames to super-resolved the current frame. RRN\n[14] adopts a residual mapping between layers with identity skip connections to stabilize the training\nof RNN and meanwhile to boost the super-resolution performance. BasicVSR [4] adopts a typical\nbidirectional recurrent network coupled with a simple optical flow-based feature alignment for VSR.\n2\n3 Preliminary and Problem Definition\nNotation. We use a calligraphic letter X or D to denote a sequence data or distribution, a bold\nupper case letter X to denote a matrix, a bold lower case letter x to denote a vector, a lower case\nletter x to denote an element of a matrix. Let Ïƒ1(Â·) be the softmax operator applied to each column of\nthe matrix, i.e., the matrix has non-negative elements with each column summing to 1. Let Ïƒ2(Â·) be a\nReLU activation function, and let Ï•(Â·) be a layer normalization function. Let [T] be a set {1, . . . , T}.\nTo develop our method, we first give some definitions of the function distance and k-pattern function.\nDefinition 1 (Function distance) Given a functions f : RdÃ—n â†’ RdÃ—n and a target function\nfâˆ— : RdÃ—n â†’ RdÃ—n, we define a distance between these two function as:\nLfâˆ—,D(f) := EXâˆ¼D [â„“(f(X), fâˆ—(X))] . (1)\nFor a ground-truth Y = fâˆ—(X), we denote the loss by LD(f). To capture the locality of data, we\ndefine the k-pattern function as follows.\nDefinition 2 (k-pattern [20]) A function f:Xâ†’Y is a k-pattern if for some g:{Â±1}kâ†’Y and index\njâˆ—: f(x) = g(xjâˆ—...jâˆ—+k). We call a function hu,W (x) = P\njâŸ¨u(j), v(j)\nW âŸ© can learn a k-pattern\nfunction from a feature v(j)\nW of data x with a layer u(j) âˆˆ Rq if for Ïµ >0, we have Lf,D(hu,W ) â‰¤ Ïµ.\nNote that the feature v(j)\nW can be learned by a convolutional attention network or a fully connected\nattention network parameterized by W. This definition takes a vector as an example, and it can\nbe extended to a matrix or a tensor. In Definition 2, a k-pattern depends only on a small pattern of\nconsecutive bits of the input. Any function can learn the locality of data means that we should learn a\nhypothesis h such that Lf,D(hu,W ) â‰¤ Ïµ.\nVideo super-resolution. Let D be a distribution of videos, and let {V1, . . . ,VT } âˆ¼ Dbe a low-\nresolution (LR) video sequence, where Vt âˆˆ R3Ã—WÃ—H is the t-th LR frame. We use a feature\nextractor to learn features X = {X1, . . . ,XT } from LR video frames, where Xt âˆˆ RCÃ—WÃ—H is\nthe t-th feature. The goal of VSR is to learn a non-linear mapping F to reconstruct high-resolution\n(HR) frames Ë†Y by fully utilizing the spatial-temporal information across the sequence, i.e.,\nbY â‰œ\n\u0010\nbY1, . . . ,bYT\n\u0011\n= F(V1, . . . ,VT ), (2)\nGiven the ground-truth HR frames Y={Y1, . . . ,YT }, where Yt is the t-th HR frame. Then we\nminimize a loss function between the generated HR frame bYt and the ground-truth HR frame Yt, i.e.,\nbF = arg min\nF\nLD (F) â‰œ bED,tâˆˆ[T]\nh\nd\n\u0010\nbYt, Yt\n\u0011i\n, (3)\nwhere bE[Â·] is an empirical expectation, d(Â·, Â·) is a distance metric, e.g., â„“1-loss, â„“2-loss and Charbon-\nnier loss [29]. For the VSR problem, one can use a sequence modeling method, such as RNN [ 6],\nLSTM [11] and Transformer [27]. In practice, Transformer gains particular interest since it avoids\nrecursion and thus allows parallel computing in practice.\nTransformer block. A Transformer block is a sequence-to-sequence function, which consists\nof a self-attention layer and a token-wise feed-forward layer with both having a skip connection.\nSpecifically, given an input X âˆˆ RdÃ—n consisting of d-dimensional embeddings of n tokens, the\nTransformer block map a sequence RdÃ—n to another sequence RdÃ—n, respectively, i.e.,\nf1(X) = Ï•\n\u0012\nX +\nXh\ni=1\nWi\no(Wi\nvX)Ïƒ1((Wi\nkX)âŠ¤(Wi\nq X))\n\u0013\n, (4)\nf2(X) = Ï•\n\u0000\nf1(X) + W2Ïƒ2(W1 Â· f1(X) + b11âŠ¤\nn ) + b21âŠ¤\nn\n\u0001\n, (5)\nwhere Wi\nv, Wi\nk, Wi\nq âˆˆ RmÃ—d are linear layers mapping an input to value, key and query, respectively.\nAlso, Wi\noâˆˆRdÃ—m, W1âˆˆRrÃ—d, W2âˆˆRdÃ—r are linear layers, and b1 âˆˆ Rr, b2âˆˆRd are bias. Here, h\nis the number of heads, m is the head size, and r is the hidden layer size of the feed-forward layer.\nHowever, it is non-trivial to apply Transformer to VSR due to some intrinsic limitations. i) The\nfully connected self-attention layer neglects to leverage locality information in a video. ii) The\ntoken-wise feed-forward layer independently processes each of the input token embeddings, leading\nto misaligned features. To address these, we propose a new Transformer for VSR.\n3\nFeature\nExtractor\nSpatial-temporal\nConvolutional\nSelf-attention\nReconstruction\nModule\nVideo Super-resolution Transformer\nNorm\n&\nAdd\nNorm\n&\nAdd\nð‘Ã—\n+\n+\nposition upsampling\nBidirectional\nOptical flow-based\nFeed-forward\nFigure 1: The framework of video super-resolution Transformer. Given a low-resolution (LR) video,\nwe first use an extractor to capture features of the LR videos. Then, a spatial-temporal convolutional\nself-attention and an optical flow-based feed-forward network model a sequence of continuous\nrepresentations. Note that these two layers both have skip connections. Last, the reconstruction\nnetwork restores a high-resolution video from the representations and the upsampling frames.\n4 Video Super-Resolution Transformer\nIn this paper, we aim to propose a new Transformer for the video super-resolution problem, called\nVSR-Transformer. As illustrated in Figure 1, our proposed method consists of a feature extractor,\na Transformer encoder, and a reconstruction network. Specifically, given a sequence of videos, we\nfirst use a stack of residual blocks to extract features of the videos. Then, the VSR-Transformer\nencoder maps the features to a sequence of continuous representations. Last, the reconstruction\nmodule restores a high-resolution video from the representations.\n4.1 Spatial-Temporal Convolutional Self-attention\nTo verify the drawback of the fully connected self-attention (FCSA) layer, we first provide a theoretical\nanalysis for whether it learns k-patterns with gradient descent. Let D be the uniform distribution,\nand let f(X) = Î  i,jâˆˆIxi,j, where I is some set of k consecutive bits. When a fully connected\nattention layer is initialized as a permutation invariant distribution, the initial gradient is very small.\nSpecifically, we have the following theorem,\nTheorem 1 Let n be the size of image and q be the size of u. We assume m = 1 and |ui| â‰¤1. and\nthe weights are initialized as some permutation invariant distribution over Rn, and for all x we have\nhFCSA\nu,W (x) âˆˆ [âˆ’1, 1] which satisfies Definition 2. Then, the following holds:\nEWâˆ¼W\n\r\r\r\r\nâˆ‚\nâˆ‚W Lf,D\n\u0000\nhFCSA\nu,W\n\u0001\r\r\r\r\n2\n2\nâ‰¤ qn min\n(\u0012\nnâˆ’1\nk\n\u0013âˆ’1\n,\n\u0012\nnâˆ’1\nkâˆ’1\n\u0013âˆ’1)\n. (6)\nProof Please see the proofs in the supplementary materials. â–¡\nFrom this theorem, the initial gradient is small if k = â„¦(log n). When q is not sufficiently large, the\nfully connected attention layer may result in the gradient vanishing issue. It implies that the gradient\ndescent will be â€œstuckâ€ upon the initialization, and thus will fail to learn the k-pattern function.\nTherefore, the fully connected self-attention layer cannot use the spatial information of each frame\nsince the local information is not encoded in the embeddings of all tokens. Moreover, this issue may\nbecome more serious when directly using such layers in video super-resolution.\nTo address this, we propose a new spatial-temporal convolutional self-attention (STCSA) layer. As\nillustrated in Figure 2, given the feature maps of input video frames X âˆˆRTÃ—CÃ—WÃ—H, we use\nthree independent convolutional neural networks Wq, Wk and Wv to capture the spatial information\nof each frame. Here, the kernel size is 3 Ã— 3, the stride is 1 and padding is 1. Different from\nVision Transformer (ViT) [8], it uses a linear projection to extract several patches when taking\nan image as an input. In contrast, motivated by COLA-Net [ 21], we use the unfold operation to\nextract sliding local patches with stride s and patch size of WpÃ—Hp after inputting each frame to our\nTransformer. Then, we obtain three groups of 3D patches, and each group has N=T W H/(WpHp)\npatches with the dimension of each patch being d=CÃ—WpÃ—Hp. Then, we generate query, key and\nvalue Q, K, V âˆˆRTN Ã—CÃ—WpÃ—Hp, i.e.,\nQ = Îº1(Wq âˆ— X), K = Îº1(Wk âˆ— X), V = Îº1(Wv âˆ— X), (7)\nwhere Îº1(Â·) is a unfold operation. Next, we reshape each patch into a new query matrix and key\nmatrix Q = Ï„(Q) and K = Ï„(K) with the size of dÃ—N, where Ï„(Â·) is a composition of the unfold\n4\nunfold\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nunfold\nunfold\nfold\nâ€¦ â€¦\nâ€¦ â€¦ â€¦â€¦ â€¦\nunfold\nt\nð‘‡\nð‘Š\nð»\nð¶\nfold\nCNN\nCNN\nCNN\nCNN\nð‘¸!\nð‘²\nð’¬\nð’¦\nð’± multiplication\naddition\nð‘Š!\nð»!\nð¶\nð‘‡ð‘Šð»/(ð‘Š!ð»!)\nð‘‡\nð¶\nð‘Š\nð»\nFigure 2: Illustration of the spatial-temporal convolutional self-attention. The unfold operation is to\nextract sliding local patches from a batched input feature map, while the fold operation is to combine\nan array of sliding local patches into a large feature map.\nand reshape operation. Then, we calculate the similarity matrix Ïƒ1(QâŠ¤K) and aggregate with the\nvalue V to obtain a feature map. Note that the similarity matrix is related to all embedding tokens of\nthe whole video frames. Therefore, it implies that the spatial-temporal information is captured in our\nproposed layer. Last, we use the fold operation Îº2(Â·) to combine these tensors of updating sliding\nlocal patches into a feature map with the size of CÃ—TÃ—WÃ—H and obtain the final feature map by\nusing an output layer Wo. This process can be viewed as the inverse process of the unfold operation.\nSummarizing the above, we define the spatial-temporal convolutional self-attention layer as:\nf1(X) = Ï•\nï£«\nï£¬ï£­X +\nXh\ni=1\nWi\no âˆ— Îº2\nï£«\nï£¬ï£­Îº1(Wi\nv âˆ— X)| {z }\nV\nÏƒ1(ÎºÏ„\n1(Wi\nk âˆ— X)| {z }\nK\nâŠ¤ ÎºÏ„\n1(Wi\nq âˆ— X)\n| {z }\nQ\n)\nï£¶\nï£·ï£¸\nï£¶\nï£·ï£¸, (8)\nwhere ÎºÏ„\n1(Â·)=Ï„â—¦Îº1(Â·) is a composition of the reshape operation Ï„ and the fold operation Îº1. In the\nexperiment, we use a single head ( i.e., h=1) to achieve good performance. By using our spatial-\ntemporal convolutional attention layer, we next provide the following theorem for the STCSA layer\nabout how to learn k-patterns with gradient descent.\nTheorem 2 Assume we initialize each element of weights uniformly drawn from {Â±1/k}. Fix some\nÎ´ >0, some k-pattern f and some distribution D. Then is q >2k+3 log(2k/Î´), and let hSTCSA\nu(s),W(s) be\na function satisfying Definition 2, with probability at least 1 âˆ’ Î´ over the initialization, when training\na spatial-temporal convolutional self-attention (STCSA) layer using gradient descent with Î·, we have\n1\nS\nSX\ns=1\nLf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\nâ‰¤ Î·2S2nk5/22k+1 + k222k+1\nqÎ·S + Î·nqk. (9)\nProof Please see the proofs in the supplementary materials. â–¡\nFrom the theorem, the loss Lf,D(hSTCSA\nu(s),W(s) ) can be small with finite S steps in the optimization, and\nthus the spatial-temporal convolutional self-attention layer using gradient descent is able to learn\nthe k-pattern function. It implies that our proposed layer captures the locality of each frame. These\nresults verify that our spatial-temporal convolutional self-attention layer achieves an advantage over\nthe fully connected self-attention layer.\nSpatial-temporal positional encoding. The architecture of the proposed VSR-Transformer is\npermutation-invariant, while the VSR task requires precise spatial-temporal position information.\nTo address this, we propose to use 3D fixed positional encodings [31] and add them to the input of\nthe attention layer. Specifically, the positional encodings contain two spatial positional information\n(i.e., horizontal and vertical) and one temporal positional information. Then, we formulate the\nspatial-temporal positional encoding (PE) as follows:\nPE(pos, i) =\n\u001a\nsin(pos Â· Î±k), for i = 2k,\ncos(pos Â· Î±k), for i = 2k + 1, (10)\nwhere Î±k=1/100002k/ d\n3 , k is an integer in [0, d/6), â€˜ posâ€™ is the position in the corresponding\ndimension, and d is the size of the channel dimension. Note that the dimension d should be divisible\nby 3 since the positional encodings of the three dimensions should be concatenated to form the final\nd channel positional encodings.\n5\nForward propagation\nforward\noptical flow\nbackward\noptical flow Backward propagation\nwarp\nwarp\nfusion\nð‘…!\nð‘…\"\nForward\noptical flows\nBackward\noptical flows\nFigure 3: Illustration of the bidirectional optical flow-based feed-forward layer. Given a video\nsequence, we first bidirectionally estimate the forward and backward optical flows and wrap the\nfeature maps with the responding optical flows. Then we learn a forward and backward propagation\nnetwork to produce two sequences of features from concatenated wrapped features and LR frames.\nLast, we fusion these two feature sequences into one feature sequence.\n4.2 Bidirectional Optical Flow-based Feed-Forward\nThe fully connected feed-forward layer in the traditional Transformer consists of two linear layers\nwith a ReLU activation in between, which is applied to each token separately and identically. In\nthis way, this layer neglects to exploit the correlations among tokens of different frames, which may\nlead to poor performance. To address this, we propose to model the correlations among all frames.\nMotivated by flow-based methods [4], we propose a bidirectional optical flow-based feed-forward\nlayer by using optical flow for spatial alignment, as shown in Figure 3 (left). Specifically, given\nfeature maps X outputted by the spatial-temporal convolutional self-attention layer, we first learn\nbidirectional optical flow âƒ—O and âƒ—O between neighboring frames. Then, we obtain backward features\nâƒ—X and forward features âƒ—X by using a warping function Ï‰(Â·, Â·) along with the backward and forward\npropagation, i.e.,\nâƒ—X = Ï‰\n\u0010\nX, âƒ—O\n\u0011\n, âƒ—X = Ï‰\n\u0010\nX, âƒ—O\n\u0011\n, (11)\nwhere âƒ—O, âƒ—O âˆˆRTÃ—2Ã—WÃ—H are backward and forward optical flows, respectively. In practice, we\nuse SPyNet [23] as a function s(Â·, Â·) to bidirectionally estimate the optical flows, i.e.,\nâƒ—Ot =\n\u001a\ns(V1, V1), if t = 1,\ns(Vtâˆ’1, Vt), if t âˆˆ (1, T], âƒ—Ot =\n\u001a\ns(Vt+ 1, Vt), if t âˆˆ [1, T),\ns(VT , VT ), if t = T, (12)\nwhere âƒ—Ot and âƒ—Ot are the t-th element of âƒ—O and âƒ—O, respectively. Note that the function s(Â·, Â·) is\npre-trained and updated in the training. Here, we estimate the identical optical flow at the start and\nend of a video for the backward and forward propagation, respectively, as shown in Figure 3 (right).\nThen, we aggregate the video frames and warped feature maps to maintain the video information.\nTo learn the correlation among neighboring frames, we propose to use convolutional backward and\nforward propagation networks. We modify the fully connected feed-forward layer (i.e., Eqn. (5)) as:\nf2(X) = Ï•\nï£«\nï£¬ï£¬ï£­f1(X) + Ï\nï£«\nï£¬ï£¬ï£­ âƒ—W1 âˆ— Ïƒ2\n\u0010\nâƒ—W2 âˆ—\nh\nV, âƒ—X\ni\u0011\n| {z }\nbackward propagation\n+ âƒ—W1 âˆ— Ïƒ2\n\u0010\nâƒ—W2 âˆ—\nh\nV, âƒ—X\ni\u0011\n| {z }\nforward propagation\nï£¶\nï£·ï£·ï£¸\nï£¶\nï£·ï£·ï£¸, (13)\nwhere Ï(Â·) is a fusion module. Note that we take a two-layered network as an example, âƒ—W1 and âƒ—W2\nare the weights of the backward propagation network, and âƒ—W1 and âƒ—W2 are the weights of the forward\npropagation network. In practice, we extend the case of two-layered networks to multi-layered neural\nnetworks R1 and R2, then we rewrite Eqn. (13) as follows:\nf2(X) = Ï•\n\u0010\nf1(X) + Ï\n\u0010\nR1\n\u0010\nV, âƒ—X\n\u0011\n+ R2\n\u0010\nV, âƒ—X\n\u0011\u0011\u0011\n, (14)\nwhere R1 and R2 are flexible networks, and we set them to be a stack of Residual ReLU networks in\nthe experiment. Compared with ViT [8], our model is able to capture the correlation among different\nframes. Different from BasicVSR [ 4], it recurrently estimates the optical flows and features. In\ncontrast, our VSR-Transformer avoids recursion and thus allows parallel computing.\n6\nTable 1: Quantitative comparison (PSNR/SSIM) on REDS4 for 4Ã— VSR. The results are tested on\nRGB channels. Red and blue indicate the best and the second best performance, respectively. â€˜ â€ â€™\nmeans a method trained on 5 frames for a fair comparison.\nMethod Params (M) Clip_000 Clip_011 Clip_015 Clip_020 Average (RGB)\nBicubic - 24.55/0.648926.06/0.726128.52/0.803425.41/0.7386 26.14/0.7292\nRCAN [36] - 26.17/0.737129.34/0.825531.85/0.888127.74/0.8293 28.78/0.8200\nTOFlow [34] - 26.52/0.754027.80/0.785830.67/0.860926.92/0.7953 27.98/0.7990\nDUF [15] 5.8 27.30/0.793728.38/0.805631.55/0.884627.30/0.8164 28.63/0.8251\nEDVR-M [29] 3.3 27.75/0.815331.29/0.873233.48/0.913329.59/0.8776 30.53/0.8699\nEDVR-L [29] 20.6 28.01/0.8250 32.17/0.8864 34.06/0.9206 30.09/0.8881 31.09/0.8800\nBasicVSRâ€  [4] 6.3 27.67/0.811431.27/0.874033.58/0.913529.71/0.8803 30.56/0.8698\nIconVSRâ€  [4] 8.7 27.83/0.818231.69/0.879833.81/0.916429.90/0.8841 30.81/0.8746\nVSR-Transformer 32.6 28.06/0.8267 32.28/0.8883 34.15/0.9199 30.26/0.8912 31.19/0.8815\nOurs\n GT\nBicubic\n BasicVSR\n IconVSR\nEDVR-L\nBicubic\n BasicVSR\n IconVSR\nEDVR-L\n Ours\n GT\nFigure 4: Qualitative comparison on the REDS4 dataset for 4Ã— VSR. Zoom in for the best view.\n5 Experiments\nTraining datasets. (i) REDS [22] contains 240 training clips, 30 validation clips and 30 testing clips,\nwhere each with 100 consecutive frames. According to EDVR [29], we use REDS4 as the test set\nwhich contains the 000, 011, 015, and 020 clips. The remaining training and validation clips are\nregrouped as our training dataset which has 266 clips. For fair comparisons, all VSR models are\ntrained on 5 frames. (ii) Vimeo-90K [34] consists of 4,278 videos with 89,800 high-quality video\nclips (i.e., 720p or higher) collected from Vimeo.com, which covers a large variety of scenes and\nactions. We use Vid4 [18] and Vimeo-90K-T [34] as test sets.\nEvaluation metrics. We use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index\n(SSIM) [32] to evaluate the quality of images generated by the VSR methods. To measure the\nefficiency of different networks, we also compare the model sizes.\nExperiment details. We compare our VSR-Transformer with the following state-of-the-art VSR\nmethods: RCAN [36], VESPCN [1], SPMC [25], TOFlow [34], FRVSR [24], DUF [15], RBPN [10],\nEDVR [29], BasicVSR [4] and IconVSR [4]. Our experiments are implemented based on BasicSR\n[30], with 8 NVIDIA TITAN RTX GPUs. We use Bicubic down-sampling to get LR images from HR\nimages. The channel size in each residual block is set to 64. We set the number of Transformer blocks\nto be the number of frames. We use Adam optimizer [ 16] with Î²1=0.9, Î²2=0.99, and use Cosine\nAnnealing [19] to decay the learning rate from 2Ã—10âˆ’4 to 10âˆ’7. More details of data augmentation,\nexperiment settings, and the network architectures can be referred to in the supplementary materials.\n7\nTable 2: Quantitative comparison (PSNR/SSIM) onVimeo-90K-T for 4Ã— VSR. Red and blue indicate\nthe best and the second best performance, respectively.\nAverage Bicubic RCAN [36]TOFlow [34]DUF [15]RBPN [10]EDVR-L [29]BasicVSR[4]IconVSR [4]Ours(Channel)(1 Frame)(1 Frame)(7 Frames)(7 Frames)(7 Frames)(7 Frames)(7 Frames)(7 Frames)(7 Frames)RGB 29.79/0.848333.61/0.910133.08/0.905434.33/0.9227 -/- 35.79/0.937435.31/0.932235.54/0.934735.88/0.9380Y 31.32/0.868435.35/0.925134.83/0.922036.37/0.938737.07/0.943537.61/0.948937.18/0.945037.47/0.947637.71/0.9494\nTable 3: Quantitative comparison (PSNR/SSIM) on Vid4 for 4Ã— VSR. Red and blue indicate the best\nand the second best performance, respectively. Y denotes the evaluation on Y channels.\nMethods Params (M)Calendar (Y)City (Y) Foliage (Y) Walk (Y) Average (Y)\nBicubic - 20.39/0.572025.16/0.602823.47/0.566626.10/0.797423.78/0.6347\nRCAN [36] - 22.33/0.725426.10/0.696024.74/0.664728.65/0.871925.46/0.7395\nVESPCN [1] - -/- -/- -/- -/- 25.35/0.7557\nSPMC [25] - 22.16/0.746527.00/0.757325.43/0.720828.91/0.876125.88/0.7752\nTOFlow [34] - 22.47/0.731826.78/0.740325.27/0.709229.05/0.879025.89/0.7651\nFRVSR [24] 5.1 -/- -/- -/- -/- 26.69/0.822\nRBPN [10] 12.2 23.99/0.80727.73/0.80326.22/0.75730.70/0.90927.12/0.818\nEDVR-L [29] 20.6 24.05/0.814728.00/0.812226.34/0.763531.02/0.915227.35/0.8264\nBasicVSR [4] 6.3 -/- -/- -/- -/- 27.24/0.8251\nIconVSR [4] 8.7 -/- -/- -/- -/- 27.39/0.8279\nVSR-Transformer (Ours)43.8 24.08/0.812527.94/0.810726.33/0.763531.10/0.916327.36/0.8258\n5.1 Resuts on REDS\nWe compare our proposed method with the state-of-the-art VSR methods on REDS. For fair compar-\nisons, we train BasicVSR and IconVSR [4] with 5 frames to produce high resolution videos. With\nthe same amount of information, it is helpful to compare the performance of all VSR methods.\nQuantitative results. From Table 1, our method has the highest PSNR and comparable SSIM values,\nwhich verifies the superiority of our method. When training with 5 frames, BasicVSR and IconVSR\ndegrade severely and they are worse than EDVR. It implies that their success largely derived from\nthe aggregation of long-term sequence information. More importantly, our model with 64 channels\nachieves better performance than EDVR-L with 128 channels. On the other hand, although our model\nsize is larger than other methods, it gains large improvement for VSR, especially for a small number\nof frames. In practice, our model size can be smaller than directly using most existing Transformers\nin VSR due to many linear layers. We leave this limitation on the model size in future work.\nQualitative results. From Figure 4, our VSR-Transformer is able to recover finer details and sharper\nedges, including square patterns, the horizontal and vertical strip patterns. In contrast, when training\non 5 frames of REDS, BasicVSR and IconVSR are worse than EDVR and fail to generate sharp\nimages. Therefore, these results also verify the superiority of our method on the VSR task. More\nqualitative results on REDS are shown in the supplementary materials.\n5.2 Resuts on Vimeo-90K\nIn this experiment, we evaluate the performance of the VSR-Transformer on Vimeo-90K-T. We train\nour all models on Vimeo-90K and then evaluate them on Vimeo-90K-T and Vid4.\nQuantitative results. From Table 2, the VSR-Transformer achieves the highest PSNR and SSIM,\nand thus outperforms other VSR methods although the model size is larger than other methods. Here,\nthe model size in Table 3 is different from Table 1 because the model size equals to the number of\nframes. In contrast, BasicVSR is much worse than EDVR and our method since the number of frames\nis small. When testing on Vid4, the generalization ability of our model is better than EDVR and is\nworse than BasicVSR and IconVSR. The possible reason is that BasicVSR and IconVSR are tested\non all frames of the Vid4, while the VSR-Transformer and EDVR are tested on 7 frames. Moreover,\nthere may exist a distribution bias between Vimeo-90K-T and Vid4.\nQualitative results. As shown in Figure 5, the VSR-Transformer is able to generate sharp and\nrealistic HR frames. In contrast, BasicVSR and IconVSR often produce blurry HR images because of\na small number of frames. In addition, the texture generated by EDVR is blurry and messy. More\nqualitative results on the Vimeo-90K-t and Vid4 datasets are shown in the supplementary materials.\n8\nBicubic\nBicubic\nBasicVSR\nBasicVSR\nIconVSR\nIconVSR\nEDVR-L\nEDVR-L\nOurs\nOurs\n GT\nGT\nFigure 5: Qualitative comparison on Vimeo-90K-T for 4Ã— VSR. Zoom in for the best view.\nw/o optical flow\nw/ optical flow\nw/o optical flow\nw/ optical flow\nFigure 6: Ablation study on REDS for 4Ã— VSR. Here, w/o and w/ optical flow mean the VSR-\nTransformer without and with the optical flow, respectively. Zoom in for the best view.\n5.3 Ablation Study\nWe investigate the effectiveness of optical flows in our VSR-Transformer on REDS. By removing\nSPyNet in the bidirectional optical flow-based feed-forward layer, we directly use a stack of Residual\nReLU networks in the experiment. For the quantitative comparison, the VSR-Transformer without\noptical flows has the PSNR of 30.37, which has worse performance than that with optical flows. From\nFigure 6, the VSR-Transformer with optical flow is able to generate HR frames with finer details\nand sharper edges. It means that the optical flow is important in the bidirectional optical flow-based\nfeed-forward layer and it helps to perform feature propagation and alignments. More ablation studies\ncan be found in the supplementary materials.\n6 Conclusion\nIn this paper, we have proposed a novel Transformer framework for video super-resolution, namely\nVSR-Transformer. Instead of directly applying existing vision Transformer for VSR, we present a\nspatial-temporal convolutional self-attention layer to leverage locality information. Moreover, we\nprovide a theoretical analysis to verify that the spatial-temporal convolutional self-attention layer has\nan advantage over the fully connected self-attention layer. Then, we develop a bidirectional optical\nflow-based feed-forward layer to exploit the correlations among different frames. With the help of\nthis layer, we are able to perform both feature propagation and alignment. Extensive experiments on\nseveral benchmark datasets demonstrate the effectiveness of our proposed method.\n9\nReferences\n[1] J. Caballero, C. Ledig, A. Aitken, A. Acosta, J. Totz, Z. Wang, and W. Shi. Real-time video\nsuper-resolution with spatio-temporal networks and motion compensation. In IEEE Conference\non Computer Vision and Pattern Recognition, 2017.\n[2] J. Cao, Y . Guo, Q. Wu, C. Shen, J. Huang, and M. Tan. Adversarial learning with local\ncoordinate coding. In International Conference on Machine Learning, 2018.\n[3] J. Cao, L. Mo, Y . Zhang, K. Jia, C. Shen, and M. Tan. Multi-marginal wasserstein gan. In\nAdvances in Neural Information Processing Systems, 2019.\n[4] K. C. Chan, X. Wang, K. Yu, C. Dong, and C. C. Loy. Basicvsr: The search for essential\ncomponents in video super-resolution and beyond. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2020.\n[5] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained\nimage processing transformer. In Advances in Neural Information Processing Systems, 2021.\n[6] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio. Empirical evaluation of gated recurrent neural\nnetworks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n[7] J. Dai, H. Qi, Y . Xiong, Y . Li, G. Zhang, H. Hu, and Y . Wei. Deformable convolutional networks.\nIn IEEE international conference on computer vision, 2017.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[9] Y . Guo, J. Chen, J. Wang, Q. Chen, J. Cao, Z. Deng, Y . Xu, and M. Tan. Closed-loop matters:\nDual regression networks for single image super-resolution. In IEEE Conference on Computer\nVision and Pattern Recognition, 2020.\n[10] M. Haris, G. Shakhnarovich, and N. Ukita. Recurrent back-projection network for video\nsuper-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n[11] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735â€“\n1780, 1997.\n[12] T. Isobe, X. Jia, S. Gu, S. Li, S. Wang, and Q. Tian. Video super-resolution with recurrent\nstructure-detail network. In European Conference on Computer Vision, 2020.\n[13] T. Isobe, S. Li, X. Jia, S. Yuan, G. Slabaugh, C. Xu, Y .-L. Li, S. Wang, and Q. Tian. Video\nsuper-resolution with temporal group attention. In IEEE Conference on Computer Vision and\nPattern Recognition, 2020.\n[14] T. Isobe, F. Zhu, X. Jia, and S. Wang. Revisiting temporal modeling for video super-resolution.\nIn The British Machine Vision Conference, 2020.\n[15] Y . Jo, S. W. Oh, J. Kang, and S. J. Kim. Deep video super-resolution network using dynamic\nupsampling filters without explicit motion compensation. In IEEE conference on computer\nvision and pattern recognition, 2018.\n[16] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\n[17] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool. Localvit: Bringing locality to vision\ntransformers. arXiv preprint arXiv:2104.05707, 2021.\n[18] C. Liu and D. Sun. On bayesian adaptive video super resolution. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 36(2), 2013.\n[19] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts.arXiv preprint\narXiv:1608.03983, 2016.\n10\n[20] E. Malach and S. Shalev-Shwartz. Computational separation between convolutional and fully-\nconnected networks. In International Conference on Learning Representations, 2021.\n[21] C. Mou, J. Zhang, X. Fan, H. Liu, and R. Wang. Cola-net: Collaborative attention network for\nimage restoration. IEEE Transactions on Multimedia, 2021.\n[22] S. Nah, S. Baik, S. Hong, G. Moon, S. Son, R. Timofte, and K. Mu Lee. Ntire 2019 challenge\non video deblurring and super-resolution: Dataset and study. In IEEE Conference on Computer\nVision and Pattern Recognition Workshops, 2019.\n[23] A. Ranjan and M. J. Black. Optical flow estimation using a spatial pyramid network. In IEEE\nconference on computer vision and pattern recognition, 2017.\n[24] M. S. Sajjadi, R. Vemulapalli, and M. Brown. Frame-recurrent video super-resolution. In IEEE\nConference on Computer Vision and Pattern Recognition, 2018.\n[25] X. Tao, H. Gao, R. Liao, J. Wang, and J. Jia. Detail-revealing deep video super-resolution. In\nIEEE International Conference on Computer Vision, 2017.\n[26] Y . Tian, Y . Zhang, Y . Fu, and C. Xu. Tdan: Temporally-deformable alignment network for\nvideo super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, 2020.\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,\n2017.\n[28] H. Wang, D. Su, C. Liu, L. Jin, X. Sun, and X. Peng. Deformable non-local network for video\nsuper-resolution. IEEE Access, 2019.\n[29] X. Wang, K. C. Chan, K. Yu, C. Dong, and C. Change Loy. Edvr: Video restoration with\nenhanced deformable convolutional networks. In IEEE Conference on Computer Vision and\nPattern Recognition Workshops, 2019.\n[30] X. Wang, K. Yu, K. C. Chan, C. Dong, and C. C. Loy. Basicsr. https://github.com/\nxinntao/BasicSR, 2020.\n[31] Y . Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, and H. Xia. End-to-end video instance\nsegmentation with transformers. arXiv preprint arXiv:2011.14503, 2020.\n[32] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from\nerror visibility to structural similarity. IEEE Transactions on Image Processing, 13(4), 2004.\n[33] S. Wu, X. Xiao, Q. Ding, P. Zhao, Y . Wei, and J. Huang. Adversarial sparse transformer for\ntime series forecasting. In Advances in Neural Information Processing Systems, 2020.\n[34] T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman. Video enhancement with task-oriented\nflow. International Journal of Computer Vision, 127(8), 2019.\n[35] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture transformer network for image\nsuper-resolution. In CVPR, 2020.\n[36] Y . Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y . Fu. Image super-resolution using very deep\nresidual channel attention networks. In European Conference on Computer Vision, 2018.\n11\nSupplementary Materials: Video Super-Resolution Transformer\nOrganization. In the supplementary materials, we provide detailed proofs for all theorems and\nlemmas of our paper, and more experiment settings and results. We organize our supplementary\nmaterials as follows. For the theory part, we provide preliminaries of the proposed method in Section\nA. In Sections B and C, we provide detailed proofs of Theorem 1 and Theorem 2, respectively. For\nExperiment part, we provide more experiment details and network architectures in Section D. In\nSection E, we provide more ablation studies for our proposed method. In Section F, we provide more\nqualitative results on several benchmark datasets. In Section G, we provide more quantitative results\non the Vid4 dataset.\nA Preliminaries\nNotation. Throughout the paper, we use the following notations. We use a calligraphic letter X to\ndenote a sequence data, a bold upper case letters X to denote a matrix, a bold lower case letter x\nto denote a vector, a lower case letter x to denote a element of a matrix. Let Ïƒ1(Â·) be the softmax\noperator applied to each column of the matrix, i.e., the matrix has non-negative elements with each\ncolumn summing to 1. Let Ïƒ2(Â·) be a ReLU activation function, and let Ï•(Â·) be a layer normalization\nfunction. Let [T] be a set {1, . . . , T}. Let 1{Â·} be an indicator function, where 1{A} = 1 if A is true\nand 1{A} = 0 if A is false. Let ED[Â·] be an expectation with respect to the distribution D.\nTo develop our method, we first give some definitions of the function distance and k-pattern function.\nDefinition 3 (Function distance) Given a functions f : RdÃ—n â†’ RdÃ—n and a target function\nfâˆ— : RdÃ—n â†’ RdÃ—n, we define a distance between these two function as:\nLfâˆ—,D(f) := EXâˆ¼D [â„“(f(X), fâˆ—(X))] . (15)\nFor a ground-truth Y = fâˆ—(X), we denote the loss as LD(f). In the proofs of the theorem, we use a\nhing-loss â„“(Ë†y, y) = max{1 âˆ’ Ë†yy, 0} as an example. To capture the locality of data, we define the\nk-pattern function as follows.\nDefinition 4 (k-pattern function [1]) A function f:Xâ†’Y is a k-pattern if for some g:{Â±1}kâ†’Y\nand index jâˆ—: f(x) = g(xjâˆ—...jâˆ—+k). We call a function hu,W (x) = P\njâŸ¨u(j), v(j)\nW âŸ© can learn a\nk-pattern function from a feature v(j)\nW of data x with a layer u(j) âˆˆ Rq if for Ïµ >0, we have\nLf,D(hu,W ) â‰¤ Ïµ. (16)\nNote that the feature v(j)\nW can be learned by a convolutional network or a fully-connected network\nfollowed by a ReLU activation function Ïƒ2(Â·). With the same linear layer u, if the convolutional\nnetwork or the fully-connected network can learn the local pattern of data if Lf,D(hu,W ) â‰¤ Ïµ.\nWe use the following theorem about the convergence of online gradient-descent. This theorem verifies\nthat the gradient-descent converges to a good solution.\nTheorem 3 (Online Gradient Descent [ 2]) Fix some Î·, and let f1, . . . , fS be some sequence of\nconvex functions. Fix some Î¸1, and update Î¸s+1=Î¸sâˆ’Î·âˆ‡fs(Î¸s). For every Î¸âˆ— the following holds:\n1\nS\nSX\ns=1\nfs(Î¸s)â‰¤ 1\nS\nSX\ns=1\nfs(Î¸âˆ—)+ 1\n2Î·S âˆ¥Î¸âˆ—âˆ¥2+âˆ¥Î¸1âˆ¥ 1\nS\nSX\ns=1\nâˆ¥âˆ‡fs(Î¸s)âˆ¥+Î· 1\nS\nSX\ns=1\nâˆ¥âˆ‡fs(Î¸s)âˆ¥2. (17)\n12\nB Proofs of Theorem 1\nTheorem 1 Let n be the size of image and q be the size of u. We assume m = 1 and |ui| â‰¤1. and\nthe weights are initialized as some permutation invariant distribution over Rn, and for all x we have\nhFCSA\nu,W (x) âˆˆ [âˆ’1, 1] which satisfies Definition 2. Then, the following holds:\nEWâˆ¼W\n\r\r\r\r\nâˆ‚\nâˆ‚W Lf,D\n\u0000\nhFCSA\nu,W\n\u0001\r\r\r\r\n2\n2\nâ‰¤ qn min\n(\u0012\nnâˆ’1\nk\n\u0013âˆ’1\n,\n\u0012\nnâˆ’1\nkâˆ’1\n\u0013âˆ’1)\n. (18)\nProof Follows by the proofs of [1], we complete the following proofs. Denote Ï‡Iâ€² = Î iâˆˆIâ€²xi, so\nf(x) = Ï‡I with I = [k]. By calculating the gradient to w(i)\nj :\nâˆ‚\nâˆ‚w(i)\nj\nLf,D\n\u0000\nhFCSA\nu,W\n\u0001\n=Exâˆ¼D\n\"\nâˆ‚\nâˆ‚w(i)\nj\nâ„“\n\u0000\nhFCSA\nu,W , f(x)\n\u0001\n#\n= âˆ’ Exâˆ¼D\n\u0002\nxjuiÏƒâ€²\n2\n\u0000\u0002\nWo(Wvx)Ïƒ1\n\u0000\n(Wkx)âŠ¤(Wqx)\n\u0003\ni\n\u0001\u0001\nÏ‡I(x)\n\u0003\n= âˆ’ Exâˆ¼D\nh\nxjuiÏƒâ€²\n2\n\u0010D\nw(i), x\nE\u0011\nÏ‡I(x)\ni\n,\nwhere the last equation follows by the assumption m=1 and the loss function â„“(Â·, Â·), and there\nexists w(i) such that the second line is satisfied. Fix some permutation Ï€ : [ n]â†’[n]. Let\nÏ€(x)=(xÏ€(1), . . . , xÏ€(n)), for IâŠ†[n], we let Ï€(I) = âˆªjâˆˆI{Ï€(j)}. Notice that for all x, z :\nÏ‡I(Ï€(x))=Ï‡Ï€(I) and âŸ¨Ï€(x), zâŸ©=âŸ¨x, Ï€âˆ’1(z)âŸ©. Denote Ï€(hFCSA\nu,W )(x) = Pk\ni=1 uiÏƒ2(âŸ¨Ï€(w(i)), xâŸ©),\nand denote Ï€(D) the distribution of Ï€(x) where xâˆ¼D. Notice that since D is the uniform distribution,\nwe have Ï€(D) = D. Therefore, for every permutation Ï€ with Ï€(j) = j we have:\nâˆ’ âˆ‚\nâˆ‚w(i)\nj\nLÏ‡Ï€(I),D\n\u0000\nhFCSA\nu,W\n\u0001\n=Exâˆ¼D\nh\nxjuiÏƒâ€²\n2\n\u0010D\nw(i), x\nE\u0011\nÏ‡Ï€(I)(x)\ni\n=Exâˆ¼Ï€(D)\nh\nxjuiÏƒâ€²\n2\n\u0010D\nw(i), Ï€âˆ’1(x)\nE\u0011\nÏ‡I(x)\ni\n=Exâˆ¼D\nh\nxjuiÏƒâ€²\n2\n\u0010D\nÏ€\n\u0010\nw(i)\n\u0011\n, x\nE\u0011\nÏ‡I(x)\ni\n= âˆ’ âˆ‚\nâˆ‚w(i)\nj\nLÏ‡I,D\n\u0000\nÏ€(hFCSA\nu,W )\n\u0001\n.\nFix some I âŠ†[n] with |I| = k and j âˆˆ [n]. Now, let Pj be a set of permutations satisfying: (i) For\nall Ï€1, Ï€2 âˆˆ Pj with Ï€1 Ì¸= Ï€2 we have Ï€1(I) Ì¸= Ï€2(I); (ii) For all Ï€ âˆˆ Pj we have Ï€(j) = j. Note\nthat if j /âˆˆ Ithen the maximal size of such Pj is\n\u0000nâˆ’1\nk\n\u0001\n, and if j âˆˆ Ithen the maximal size is\n\u0000nâˆ’1\nkâˆ’1\n\u0001\n.\nDenote gj(x) = xjuiÏƒâ€²\n2(âŸ¨w(i), xâŸ©). We denote the inner-product âŸ¨Ïˆ, Ï•âŸ©D = Exâˆ¼D[Ïˆ(x)Ï•(x)] and\nthe induced norm âˆ¥Ïˆâˆ¥D =\np\nâŸ¨Ïˆ, ÏˆâŸ©D. Since {Ï‡Iâ€²}Iâ€²âŠ†[n] is an orthonormal basis with respect to\nâŸ¨Â·, Â·âŸ©D from Parsevalâ€™s equality, we have\nX\nÏ€âˆˆPj\n \nâˆ‚\nâˆ‚w(i)\nj\nLÏ‡I,D\n\u0000\nÏ€\n\u0000\nhFCSA\nu,W\n\u0001\u0001\n!2\n=\nX\nÏ€âˆˆP\n \nâˆ‚\nâˆ‚w(i)\nj\nLÏ‡Ï€(I),D\n\u0000\nhFCSA\nu,W\n\u0001\n!2\n=\nX\nÏ€âˆˆP\nâŸ¨gj, Ï‡Ï€(I)âŸ©2\nD â‰¤\nX\nIâ€²âŠ†[n]\nâŸ¨gj, Ï‡Iâ€²âŸ©2\nD = âˆ¥gjâˆ¥2\nD â‰¤ 1.\nSo, from the above we get that, taking Pj of maximal size:\nEÏ€âˆ¼Pj\n \nâˆ‚\nâˆ‚w(i)\nj\nLÏ‡I,D\n\u0000\nÏ€\n\u0000\nhFCSA\nu,W\n\u0001\u0001\n!2\nâ‰¤ |Pj|âˆ’1 â‰¤ min\n(\u0012\nnâˆ’1\nk\n\u0013âˆ’1\n,\n\u0012\nnâˆ’1\nkâˆ’1\n\u0013âˆ’1)\n.\nNow, for some permutation invariant distribution of weights we have:\nEW\n \nâˆ‚\nâˆ‚w(i)\nj\nLÏ‡I,D\n\u0000\nhFCSA\nu,W\n\u0001\n!2\n= EW EÏ€âˆ¼Pj\n \nâˆ‚\nâˆ‚w(i)\nj\nLÏ‡I,D\n\u0000\nÏ€\n\u0000\nhFCSA\nu,W\n\u0001\u0001\n!2\nâ‰¤ |Pj|âˆ’1.\nSumming over all neurons we get:\nEW\n\r\r\r\r\nâˆ‚\nâˆ‚W LÏ‡I,D\n\u0000\nhFCSA\nu,W\n\u0001\r\r\r\r\n2\n2\nâ‰¤ qn min\n(\u0012\nnâˆ’1\nk\n\u0013âˆ’1\n,\n\u0012\nnâˆ’1\nkâˆ’1\n\u0013âˆ’1)\n.\nâ–¡\n13\nC Proofs of Theorem 2\nTheorem 2 Assume we initialize each element of weights uniformly drawn from {Â±1/k}. Fix some\nÎ´ >0, some k-pattern f and some distribution D. Then is q >2k+3 log(2k/Î´), and let hSTCSA\nu(s),W(s) be\na function satisfying Definition 2, with probability at least 1 âˆ’ Î´ over the initialization, when training\na spatial-temporal convolutional self-attention (STCSA) layer using gradient descent with Î·, we have\n1\nS\nSX\ns=1\nLf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\nâ‰¤ Î·2S2nk5/22k+1 + k222k+1\nqÎ·S + Î·nqk. (19)\nProof From Lemma 2, with probability at least 1 âˆ’ Î´ over the initialization, there exist\nuâˆ—(1), . . . ,uâˆ—(nâˆ’k) with âˆ¥uâˆ—(1)âˆ¥ â‰¤2k+1k/âˆšq and âˆ¥uâˆ—(j)âˆ¥ = 0 for j >1 such that hSTCSA\nuâˆ—,W(0) (x) =\nf(x), and so Lf,D(hSTCSA\nu(s),W(s) ) = 0. Based on Theorem 3, since Lf,D(hSTCSA\nu,W ) is convex with respect\nto u, we have:\n1\nS\nSX\ns=1\nLf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\nâ‰¤ 1\nS\nSX\ns=1\nLf,D\n\u0010\nhSTCSA\nuâˆ—,W(s)\n\u0011\n+ 1\n2Î·S\nnâˆ’kX\nj=1\n\r\r\ruâˆ—(j)\n\r\r\r\n2\n+Î· 1\nS\nSX\ns=1\n\r\r\r\r\nâˆ‚\nâˆ‚uLf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r\n2\nâ‰¤ 1\nS\nSX\ns=1\nLf,D\n\u0010\nhSTCSA\nuâˆ—,W(s)\n\u0011\n+ 2(2kk)2\nqÎ·S + Î·nqk\nâ‰¤ 1\nS\nSX\ns=1\nLf,D(hSTCSA\nuâˆ—,W(0) ) + Î·2S2nk3/2âˆšq\nnâˆ’kX\nj=1\n\r\r\ruâˆ—(j)\n\r\r\r + 2(2kk)2\nqÎ·S + Î·nqk\nâ‰¤Î·2S2nk5/22k+1 + 2(2kk)2\nqÎ·S + Î·nqk,\nwhere the first line follows by Theorem 3, and second line holds by the property of uâˆ—(j), the third\nline hold by Lemma 1, and the fourth line follows by the inequality âˆ¥uâˆ—(1)âˆ¥ â‰¤2k+1k/âˆšq. â–¡\nLemma 1 Given the learning rate Î· and steps S, for every uâˆ—, we have:\n\f\f\fLf,D\n\u0010\nhSTCSA\nuâˆ—,W(S)\n\u0011\nâˆ’ Lf,D\n\u0010\nhSTCSA\nuâˆ—,W(0)\n\u0011\f\f\f â‰¤ Î·2S2nk3/2âˆšq\nnâˆ’kX\nj=1\n\r\r\ruâˆ—(j)\n\r\r\r.\nProof Based on the result of âˆ¥W(S)âˆ’W(0)âˆ¥ in Lemma 3 and the assumption of Ïƒ2, we have\n\f\f\fLf,D\n\u0010\nhSTCSA\nuâˆ—,W(S)\n\u0011\nâˆ’ Lf,D\n\u0010\nhSTCSA\nuâˆ—,W(0)\n\u0011\f\f\f\n=\n\f\f\fExâˆ¼D\nh\nâ„“(hSTCSA\nuâˆ—,W(S) (x), f(x))\ni\nâˆ’ Exâˆ¼D\nh\nâ„“(hSTCSA\nuâˆ—,W(0) (x), f(x))\ni\f\f\f\nâ‰¤Exâˆ¼D\nh\f\f\fhSTCSA\nuâˆ—,W(S) (x) âˆ’ hSTCSA\nuâˆ—,W(0) (x)\n\f\f\f\ni\n=Exâˆ¼D\nï£®\nï£°\n\f\f\f\f\f\f\nnâˆ’kX\nj=1\nD\nuâˆ—(j), Ïƒ2\n\u0010\nW(S)xj...j+k\n\u0011\nâˆ’ Ïƒ2\n\u0010\nW(0)xj...j+k\n\u0011E\n\f\f\f\f\f\f\nï£¹\nï£»\nâ‰¤Exâˆ¼D\nï£®\nï£°\nnâˆ’kX\nj=1\n\r\r\ruâˆ—(j)\n\r\r\r\n\r\r\rW(S) âˆ’ W(0)\n\r\r\râˆ¥xj...j+kâˆ¥\nï£¹\nï£»\nâ‰¤Î·2S2nk3/2âˆšq\nnâˆ’kX\nj=1\n\r\r\ruâˆ—(j)\n\r\r\r,\nwhere the second and third lines follow by the definition of the loss function, the fourth line follows\nby the assumption of the activation, and the last line holds by Lemma 3. â–¡\n14\nLemma 2 Assume we initialize each element of weights uniformly drawn from{Â±1/k}, and fix some\nÎ´ >0. Then if q >2k+3 log(2k/Î´) with probability at least 1 âˆ’ Î´ over the choice of the weights, for\nevery k-pattern f there exist uâˆ—(1), . . . ,uâˆ—(nâˆ’k) âˆˆ Rq with âˆ¥uâˆ—(jâˆ—)âˆ¥ â‰¤2k+1k/âˆšq and âˆ¥uâˆ—(j)âˆ¥ = 0\nfor j Ì¸= jâˆ— such that hSTCSA\nuâˆ—,W (x) = f(x).\nProof For some z âˆˆ {Â±1}k, then for every w(i) âˆ¼ {Â±1/k}k, we have P[sign(w(i)) = z] = 2âˆ’k.\nDenote by â„¦z âŠ† [q] the subset of indexes satisfying sign(w(i)) = z, for every i âˆˆ â„¦z, and note that\nEw|â„¦z| â‰¥q2âˆ’k. From Chernoff bound:\nP[|â„¦z| â‰¤q2âˆ’kâˆ’1] â‰¤ eâˆ’q2âˆ’k/8 â‰¤ Î´2âˆ’k (20)\nby choosing q >2k+3 log(2k/Î´). Thus, using the union bound with probability at least 1 âˆ’ Î´, for\nevery z âˆˆ {Â±1}k we have |â„¦z| â‰¥q2âˆ’kâˆ’1. Then, we have\nÏƒ2\n\u0010D\nw(i), z\nE\u0011\n= 1\nk 1\nn\nsign\n\u0010\nw(i)\n\u0011\n= z\no\n.\nFix some k-pattern f, where f(x) = g(xjâˆ—...jâˆ—+k). For every i âˆˆ â„¦z we choose uâˆ—(jâˆ—)\ni = k\n|â„¦z|g(z)\nand uâˆ—(j) = 0 for every j Ì¸= jâˆ—. Therefore, we have\nhSTCSA\nuâˆ—,W (x) =\nnâˆ’kX\nj=1\nD\nuâˆ—(j), Ïƒ2\n\u0010\u0002\nWo âˆ— ((Wv âˆ— X)Ïƒ1\n\u0000\n(Wk âˆ— X)âŠ¤(Wq âˆ— X)\n\u0001\u0003\nj\n\u0011E\n=\nnâˆ’kX\nj=1\nD\nuâˆ—(j), Ïƒ2\n\u0010\n[(Wo âˆ— Wv) âˆ— X]j\n\u0011E\n=\nnâˆ’kX\nj=1\nD\nuâˆ—(j), Ïƒ2 (Wxj...j+k)\nE\n=\nX\nzâˆˆ{Â±1}k\nX\niâˆˆâ„¦z\nuâˆ—\ni\n(jâˆ—)Ïƒ2\n\u0010D\nw(i), xjâˆ—...jâˆ—+k\nE\u0011\n=\nX\nzâˆˆ{Â±1}k\n1{z = xjâˆ—...jâˆ—+k}g(z)\n= g(xjâˆ—...jâˆ—+k)\n= f(x),\nwhere the first lines follows the definition ofhSTCSA\nuâˆ—,W (x) in Definition 2. The second line is based on the\nassumption m = 1 such that Ïƒ1((Wk âˆ—X )âŠ¤(Wq âˆ—X )) = 1 and the property of convolution. The third\nline follows by the fact that there exists a weight W such that Wxjâˆ—...jâˆ—+k = [(Wo âˆ— Wv) âˆ— X]j.\nThe fourth line Note that by definition of uâˆ—(jâˆ—), we have .\n\r\r\ruâˆ—(jâˆ—)\n\r\r\r\n2\n=\nX\nzâˆˆ{Â±1}k\nX\niâˆˆâ„¦z\nk2\n|â„¦z|2 â‰¤ 4(2kk)2\nq .\nâ–¡\n15\nLemma 3 Given the learning rate Î· and steps S, the norm difference satisfies\n\r\r\rW(S) âˆ’ W(0)\n\r\r\r â‰¤ Î·2S2nkâˆšq.\nProof Based on the definition of Lf,D and hSTCSA\nu(s),W(s) , we have\n\r\r\r\r\nâˆ‚\nâˆ‚u(j) Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r =\n\r\r\r\rExâˆ¼D\n\u0014 âˆ‚\nâˆ‚u(j) â„“\n\u0010\nhSTCSA\nu(s),W(s) , f(x)\n\u0011\u0015\r\r\r\r\n=\n\r\r\rExâˆ¼D\nh\nÏƒ2(W(s)xj...j+k)â„“â€²\n\u0010\nhSTCSA\nu(s),W(s) , f(x)\n\u0011i\r\r\r\nâ‰¤Exâˆ¼D\nh\r\r\rW(s)xj...j+k\n\r\r\r\ni\nâ‰¤\np\nqk\nFrom the updates of gradient-descent we have:\n\r\r\r\r\nâˆ‚\nâˆ‚W Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r =\n\r\r\r\r\r\r\nExâˆ¼D\nï£®\nï£°\nnâˆ’kX\nj=1\nu(j,s)xâŠ¤\nj...j+kÏƒâ€²\n2(W(s)xj...j+k)â„“â€²\n\u0010\nhSTCSA\nu(s),W(s) , f(x)\n\u0011\nï£¹\nï£»\n\r\r\r\r\r\r\nâ‰¤\n\r\r\r\r\r\r\nExâˆ¼D\nï£®\nï£°\nnâˆ’kX\nj=1\n\r\r\ru(j,s)\n\r\r\râˆ¥xj...j+kâˆ¥\nï£¹\nï£»\n\r\r\r\r\r\r\nâ‰¤\n\r\r\r\r\r\r\nExâˆ¼D\nï£®\nï£°\nnâˆ’kX\nj=1\nâˆš\nk\n\r\r\r\r\rÎ·\nSX\ns=1\nâˆ‚\nâˆ‚u(j) Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r\r\nï£¹\nï£»\n\r\r\r\r\r\r\nâ‰¤\n\r\r\r\r\r\r\nExâˆ¼D\nï£®\nï£°\nnâˆ’kX\nj=1\nâˆš\nkÎ·\nSX\ns=1\n\r\r\r\r\nâˆ‚\nâˆ‚u(j) Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r\nï£¹\nï£»\n\r\r\r\r\r\r\nâ‰¤(n âˆ’ k)Î·Skâˆšq\nBy the updates of gradient-descent:\n\r\r\rW(S)âˆ’W(0)\n\r\r\r=\n\r\r\r\r\rÎ·\nSX\ns=1\nâˆ‚\nâˆ‚W Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r\r\nâ‰¤Î·\nSX\ns=1\n\r\r\r\r\nâˆ‚\nâˆ‚W Lf,D\n\u0010\nhSTCSA\nu(s),W(s)\n\u0011\r\r\r\r\nâ‰¤Î·2S2nkâˆšq\nâ–¡\n16\nD More Experiment Details and Network Architecture\nMore experiment details. The batch size is set to be 2 per GPU. We use Bicubic down-sampling\nto get LR images from HR images. The channel size in each residual block is set to 64. We set the\nnumber of Transformer blocks to be the number of frames. We randomly crop a sequence of LR\nimage patches with the size of 64Ã—64. We augment the training data with random horizontal flips\nand 90â—¦ rotations. All frames are normalized to the fixed resolution 448Ã—256. We use the pre-trained\nSPyNet as our flow estimation module. Note that the SPyNet in our model is updated in the training.\nWe use Adam optimizer with Î²1=0.9, Î²2=0.99, and train our model with 60w iterations. Then, we\nuse Cosine Annealing to decay the learning rate from 2Ã—10âˆ’4 to 10âˆ’7. On the REDS dataset, we\nset the periods as [300000, 300000, 300000, 300000], the restart weights as [1, 0.5, 0.5, 0.5]. On the\nVimeo-90K dataset, we set the periods as [200000, 200000, 200000, 200000, 200000, 200000], the\nrestart weights as [1, 0.5, 0.5, 0.5, 0.5, 0.5]. We use the Charbonnier loss in our method.\nNetwork architecture. We show the network architecture of the VSR-Transformer in Table 4. In the\nspatial-temporal convolutional self-attention block, we use three independent convolutional neural\nnetworks to capture the spatial information of each frame. Last, we use an output convolutional layer\nto obtain the final feature map. In each bidirectional optical flow-based feed-forward block, we use\n30 residual blocks for the backward and forward propagation networks, where N is the number of\nframes. Last, we use a fusion layer to fuse the feature maps generated by the backward and forward\npropagation networks. Here, the kernel size is 3 Ã— 3, the stride is 1 and padding is 1. The feature\nextractor has 5 residual blocks. The resconstruction module has 30 residual blocks. We use the\nfollowing abbreviations: T: the number of frames, C: the number of channels, H: the height size of\ninput image, W: the width size of input image, I: the the number of input channels, O: the number\nof output channels, K: kernel size, S: stride size, P: padding size, G: groups, PixelShuffle: the pixel\nshuffle with the upscale factor of 2, LeakyReLU: the Leaky ReLU activation function with a negative\nslope of 0.01.\nTable 4: Network architecture of the VSR-Transformer.\nSpatial-temporal convolutional self-attention block\nPart Inputâ†’Output shape Layer information\nQuery layer (T, C, H, W)â†’(T, C, H, W) CONV-(I64, O64, K3x3, S1, P1, G64)\nKey layer (T, C, H, W)â†’(T, C, H, W) CONV-(I64, O64, K3x3, S1, P1, G64)\nValue layer (T, C, H, W)â†’(T, C, H, W) CONV-(I64, O64, K3x3, S1, P1, G64)\nCNN layer (T, C, H, W)â†’(T, C, H, W) CONV-(I64, O64, K3x3, S1, P1)\nBidirectional optical flow-based feed-forward block\nBackward (T, C+3, H, W)â†’(T, C, H, W) Residual Block: CONV-(I67, O64, K3x3, S1, P1), LeakyReLU\nForward (T, C+3, H, W)â†’(T, C, H, W) Residual Block: CONV-(I67, O64, K3x3, S1, P1), LeakyReLU\nFusion (T,2C, H, W)â†’(T, C, H, W) CONV-(I128, O64, K1x1, S1, P1), LeakyReLU\nTable 5: Network architecture of the feature extractor and reconstruction network.\nFeature extractor\nPart Inputâ†’Output shape Layer information\nExtractor (T, C, H, W)â†’(T, C, H, W) Residual Block: CONV-(I64, O64, K3x3, S1, P1), LeakyReLU\nReconstruction network\nReconstruction (T, C, H, W)â†’(T, C, H, W) Residual Block: CONV-(I64, O64, K3x3, S1, P1), LeakyReLU\nUpsampling (T, C, H, W)â†’(T, C,2H,2W) CONV-(I64, O256, K3x3, S1, P1), PixelShuffle, LeakyReLU\n(T, C,2H,2W)â†’(T, C,4H,4W) CONV-(I64, O256, K3x3, S1, P1), PixelShuffle, LeakyReLU\nCNN layer (T, C,4H,4W)â†’(T,3,4H,4W) CONV-(I64, O3, K3x3, S1, P1), LeakyReLU\n17\nTable 6: Ablation study (PSNR/SSIM) on REDS4 for 4Ã— VSR.\nMethod Clip_000 Clip_011 Clip_015 Clip_020 Average (RGB)\nw/o STCSA 28.00/0.8247 32.00/0.8847 34.04/0.9189 30.14/0.8889 31.05/0.8793\nw/o BOFF 27.67/0.8129 31.06/0.8683 33.39/0.9123 29.36/0.8729 30.37/0.8666\ntrain w/ 3 frames 27.59/0.8152 31.44/0.8747 33.64/0.9140 29.66/0.8809 30.58/0.8712\nVSR-Transformer 28.06/0.8267 32.28/0.8883 34.15/0.9199 30.26/0.8912 31.19/0.8815\nw/o STCSA\n Ours\nw/ 3 frames Ours\nw/o BOFF w/ 3 frames\nw/o STCSA Oursw/o BOFF w/ 3 frames\nw/o STCSA\n w/o BOFF\nw/ 3 frames Oursw/o STCSA w/o BOFF\nFigure 7: Ablation study on REDS for 4Ã— VSR. Here, w/o STCSA and w/o BOFF mean the VSR-\nTransformer without the spatial-temporal convolutional self-attention (STCSA) layer and bidirectional\noptical flow-based feed-forward (BOFF) layer, respectively.\nE More Ablation Studies\nE.1 Effectiveness of Spatial-Temporal Convolutional Self-Attention\nWe investigate the effectiveness of spatial-temporal convolutional self-attention (STCSA) layer in our\nmodel on REDS. Specifically, we remove this layer in our model, and evaluate the performance in\nTable 6. For the quantitative comparison, the model without the STCSA layer has worse performance\nthan the VSR-Transformer. From Figure 7, our model is able to generate HR frames with finer details\nand sharper edges. It means that the STCSA layer is important in the VSR-Transformer and it helps\nto exploit the locality of data and fuse information among different frames.\nE.2 Effectiveness of Bidirectional Optical Flow-based Feed-Forward\nWe investigate the effectiveness of bidirectional optical flow-based feed-forward (BOFF) and optical\nflows in our VSR-Transformer on REDS. By removing this layer, we directly use a stack of Residual\nReLU networks in the experiment. In Table 6, the model without the BOFF layer has worse\nperformance than the VSR-Transformer. From Figure 7, the VSR-Transformer with optical flow is\nable to generate HR frames with finer details and sharper edges. It means that the optical flow is\nimportant in the BOFF layer and it helps to perform feature propagation and alignments.\nE.3 Impact on Number of Frames\nWe investigate the impact on the number of frames when training our VSR-Transformer on REDS.\nSpecifically, we train our model with 3 frames. In Table 6. Training with small number of frames has\ndegraded performance. In contrast, our model is able to generate high-resolution frames, as shown\nin Figure 7. Therefore, training with more frames helps to restore missing information from other\nneighboring frames. In the future, we will train the VSR-Transformer with more frames.\n18\nF More Qualitative Results\nF.1 Results on REDS4\nOurs GT\nBicubic BasicVSR IconVSR\nEDVR-L\nBicubic BasicVSR IconVSR\nEDVR-L Ours GT\nFigure 8: Qualitative comparison on the REDS4 dataset for 4Ã— VSR. Zoom in for the best view.\nF.2 Results on Vimeo-90K\nBicubic\nBicubic\nBasicVSR\nBasicVSR\nIconVSR\nIconVSR\nEDVR-L\nEDVR-L\nOurs\nOurs GT\nGT\nFigure 9: Qualitative comparison on Vimeo-90K-T for 4Ã— VSR. Zoom in for the best view.\n19\nF.3 Results on Vid4\nBicubic\nBicubic\nBasicVSR\nBasicVSR\nIconVSR\nIconVSR\nEDVR-L\nEDVR-L\nOurs\nOurs GT\nGT\nFigure 10: Qualitative comparison on Vid4 for 4Ã— VSR. Zoom in for the best view.\nG More Results on Vid4\nFrom Table 7, the VSR-Transformer achieves comparable PSNR and SSIM compared with EDVR-L.\nIn contrast, BasicVSR and IconVSR are much worse than EDVR and our method since the number\nof frames is small. Note that Table 7 shows that BasicVSR and IconVSR are trained and tested\non 7 frames, which is different from Table 3 in the paper. It implies that BasicVSR and IconVSR\nlargely relies on the aggregation of long-term sequence information, and thus have poor performance\nespecially when both training and testing on small number of frames. These results verify the\neffectiveness and the generalization ability of our model.\nTable 7: Quantitative comparison (PSNR/SSIM) on Vid4 for 4Ã— VSR. Red and blue indicate the best\nand the second best performance, respectively. Y denotes the evaluation on Y channels. â€˜â€ â€™ means a\nmethod trained and tested on 7 frames for a fair comparison.\nMethods Calendar (Y) City (Y) Foliage (Y) Walk (Y) Average (Y)\nEDVR-L [3] 24.05/0.814728.00/0.812226.34/0.763531.02/0.9152 27.35/0.8264\nBasicVSRâ€  [4] 23.57/0.790527.60/0.793126.02/0.748530.42/0.904926.91/0.8093\nIconVSRâ€  [4] 23.76/0.798427.70/0.799726.13/0.750830.54/0.907227.04/0.8140\nVSR-Transformer (Ours)24.08/0.812527.94/0.810726.33/0.763531.10/0.9163 27.36/0.8258\n20\nReferences\n[1] eran malach and Shai Shalev-Shwartz Computational Separation Between Convolutional and\nFully-Connected Networks. In International Conference on Learning Representations, 2021.\n[2] A. Daniely and E. Malach. Learning Parities with Neural Networks. In Advances in Neural\nInformation Processing Systems, 2020.\n[3] Wang, Xintao and Chan, Kelvin CK and Yu, Ke and Dong, Chao and Change Loy, Chen Edvr:\nVideo Restoration with Enhanced Deformable Convolutional Networks. In IEEE Conference\non Computer Vision and Pattern Recognition Workshops, 2019.\n[4] Chan, Kelvin CK and Wang, Xintao and Yu, Ke and Dong, Chao and Loy, Chen Change\nBasicVSR: The Search for Essential Components in Video Super-Resolution and Beyond. In\nIEEE Conference on Computer Vision and Pattern Recognition, 2020.\n21",
  "topic": "Exploit",
  "concepts": [
    {
      "name": "Exploit",
      "score": 0.7559095025062561
    },
    {
      "name": "Computer science",
      "score": 0.7144311666488647
    },
    {
      "name": "Transformer",
      "score": 0.704201877117157
    },
    {
      "name": "Security token",
      "score": 0.6121118068695068
    },
    {
      "name": "Locality",
      "score": 0.5532115697860718
    },
    {
      "name": "Locality of reference",
      "score": 0.48824265599250793
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35448455810546875
    },
    {
      "name": "Engineering",
      "score": 0.11929970979690552
    },
    {
      "name": "Parallel computing",
      "score": 0.11311686038970947
    },
    {
      "name": "Computer network",
      "score": 0.11291974782943726
    },
    {
      "name": "Electrical engineering",
      "score": 0.11019441485404968
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Cache",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 132
}