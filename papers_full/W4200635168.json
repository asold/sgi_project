{
  "title": "Pose-Guided Feature Disentangling for Occluded Person Re-identification Based on Transformer",
  "url": "https://openalex.org/W4200635168",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2102445713",
      "name": "Hong Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A3017295035",
      "name": "Pinhao Song",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2342495564",
      "name": "Tianyu Guo",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1981201842",
      "name": "Wei Shi",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1977132994",
      "name": "Tao Wang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2102445713",
      "name": "Hong Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A3017295035",
      "name": "Pinhao Song",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2342495564",
      "name": "Tianyu Guo",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1981201842",
      "name": "Wei Shi",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2559085405",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6730734845",
    "https://openalex.org/W3014167589",
    "https://openalex.org/W2982281627",
    "https://openalex.org/W2782217382",
    "https://openalex.org/W3097870364",
    "https://openalex.org/W2938145879",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2963709182",
    "https://openalex.org/W2798590501",
    "https://openalex.org/W3174656328",
    "https://openalex.org/W2947101691",
    "https://openalex.org/W2796364723",
    "https://openalex.org/W3166362606",
    "https://openalex.org/W2903493803",
    "https://openalex.org/W2922510913",
    "https://openalex.org/W2981393440",
    "https://openalex.org/W3044395687",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W2796664602",
    "https://openalex.org/W2992949251",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3008625353",
    "https://openalex.org/W2931208564",
    "https://openalex.org/W2783855081",
    "https://openalex.org/W2979938149",
    "https://openalex.org/W3011820993",
    "https://openalex.org/W2769994766",
    "https://openalex.org/W2954765307",
    "https://openalex.org/W3010766832",
    "https://openalex.org/W3181317745",
    "https://openalex.org/W2739031953",
    "https://openalex.org/W6687888618",
    "https://openalex.org/W2937349443",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W6743440100",
    "https://openalex.org/W2943407549",
    "https://openalex.org/W3045817950",
    "https://openalex.org/W3014752368",
    "https://openalex.org/W6749556591",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2963049565",
    "https://openalex.org/W3175823695",
    "https://openalex.org/W2962691289",
    "https://openalex.org/W3034611771",
    "https://openalex.org/W2997987796",
    "https://openalex.org/W2963781481",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2890159224",
    "https://openalex.org/W2963330186",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4293177262",
    "https://openalex.org/W2963047834",
    "https://openalex.org/W2980073905",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W3034303554",
    "https://openalex.org/W2964201641",
    "https://openalex.org/W3035539956",
    "https://openalex.org/W2988964414",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2531440880",
    "https://openalex.org/W4287330514",
    "https://openalex.org/W2964304299",
    "https://openalex.org/W3034580371",
    "https://openalex.org/W3115484111",
    "https://openalex.org/W2990317318",
    "https://openalex.org/W2992183111",
    "https://openalex.org/W3096285474",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2984145721"
  ],
  "abstract": "Occluded person re-identification is a challenging task as human body parts could be occluded by some obstacles (e.g. trees, cars, and pedestrians) in certain scenes. Some existing pose-guided methods solve this problem by aligning body parts according to graph matching, but these graph-based methods are not intuitive and complicated. Therefore, we propose a transformer-based Pose-guided Feature Disentangling (PFD) method by utilizing pose information to clearly disentangle semantic components (e.g. human body or joint parts) and selectively match non-occluded parts correspondingly. First, Vision Transformer (ViT) is used to extract the patch features with its strong capability. Second, to preliminarily disentangle the pose information from patch information, the matching and distributing mechanism is leveraged in Pose-guided Feature Aggregation (PFA) module. Third, a set of learnable semantic views are introduced in transformer decoder to implicitly enhance the disentangled body part features. However, those semantic views are not guaranteed to be related to the body without additional supervision. Therefore, Pose-View Matching (PVM) module is proposed to explicitly match visible body parts and automatically separate occlusion features. Fourth, to better prevent the interference of occlusions, we design a Pose-guided Push Loss to emphasize the features of visible body parts. Extensive experiments over five challenging datasets for two tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is superior promising, which performs favorably against state-of-the-art methods. Code is available at https://github.com/WangTaoAs/PFD_Net",
  "full_text": "Pose-Guided Feature Disentangling for Occluded Person\nRe-identiﬁcation Based on Transformer\nTao Wang, Hong Liu\u0003, Pinhao Song, Tianyu Guo, Wei Shi\nKey Laboratory of Machine Perception\nPeking University, Shenzhen Graduate School\n{taowang, levigty}@stu.pku.edu.cn,{hongliu, pinhaosong, pkusw}@pku.edu.cn\nAbstract\nOccluded person re-identiﬁcation is a challenging task as hu-\nman body parts could be occluded by some obstacles (e.g.\ntrees, cars, and pedestrians) in certain scenes. Some existing\npose-guided methods solve this problem by aligning body\nparts according to graph matching, but these graph-based\nmethods are not intuitive and complicated. Therefore, we\npropose a transformer-based Pose-guided Feature Disentan-\ngling (PFD) method by utilizing pose information to clearly\ndisentangle semantic components (e.g. human body or joint\nparts) and selectively match non-occluded parts correspond-\ningly. First, Vision Transformer (ViT) is used to extract the\npatch features with its strong capability. Second, to prelimi-\nnarily disentangle the pose information from patch informa-\ntion, the matching and distributing mechanism is leveraged\nin Pose-guided Feature Aggregation (PFA) module. Third,\na set of learnable semantic views are introduced in trans-\nformer decoder to implicitly enhance the disentangled body\npart features. However, those semantic views are not guar-\nanteed to be related to the body without additional supervi-\nsion. Therefore, Pose-View Matching (PVM) module is pro-\nposed to explicitly match visible body parts and automati-\ncally separate occlusion features. Fourth, to better preven-\nt the interference of occlusions, we design a Pose-guided\nPush Loss to emphasize the features of visible body parts.\nExtensive experiments over ﬁve challenging datasets for t-\nwo tasks (occluded and holistic Re-ID) demonstrate that our\nproposed PFD is superior promising, which performs favor-\nably against state-of-the-art methods. Code is available at\nhttps://github.com/WangTaoAs/PFD\nNet\n1 Introduction\nPerson Re-Identiﬁcation (Re-ID) aims to identify a specif-\nic person across multiple non-overlapping cameras (Zheng,\nYang, and Hauptmann 2016). It is an important subject in\nthe ﬁeld of computer vision and has a wide range of applica-\ntion backgrounds, such as video surveillance, activity anal-\nysis, security, and smart city. In recent years, holistic person\nRe-ID achieves great progress, and various of methods (Sun\net al. 2018; Shi, Liu, and Liu 2020; Zhang, Zhang, and Liu\n2021) have been proposed. However, in real scenes, such as\n\u0003Corresponding author: hongliu@pku.edu.cn\nCopyright c\r2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nPose Estimation\nEncoders\nEncoders\nDecoders\nDecoders\nTransformer\nTr\nansformer\nMatching strategy\nPo\nse Estimation Learnable semantic views \nOcclusions or noise\nSimilarity matching\nsplit\nsplit\nFigure 1: Illustration of Pose-guided Feature Disentangling\n(PFD) method in occluded person Re-ID. PFD represents an\noccluded person image by using the transformer to implic-\nitly disentangle disciminative features and explicitly using\npose information to guide the separation of the non-occluded\nfeatures and occluded features.\nstations, airports, shopping malls, person can be easily oc-\ncluded by some obstacles (e.g., trees, cars, pedestrians), it is\ndifﬁcult to match people with incomplete and invisible body\nparts. Therefore, the occluded person re-identiﬁcation task\n(Zhuo et al. 2018; Miao et al. 2019; Jia et al. 2021a) is of\nimportant practical signiﬁcance.\nCompared with holistic person Re-ID, there are two ma-\njor challenges for occluded person Re-ID task: (1) Due to the\nexistence of occlusion, various noises have been introduced\nthat result in mismatching. (2) The occlusion may have fea-\ntures similar to human body parts, leading to the failure of\nfeature learning. Some early methods (Miao et al. 2019) u-\ntilize pose information to indicate non-occluded body parts\non the spatial feature map and directly divide global features\ninto partial features. These methods are intuitive but requires\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2540\nstrict spatial feature alignment. Some recent pose-guided\nmethods (Gao et al. 2020; Wang et al. 2020) use graph-based\napproaches to model topology information by learning node-\nto-node or edge-to-edge correspondence to further mine the\nvisible parts. However, these methods still suffer the prob-\nlem mentioned in the challenge (2). Therefore, in this pa-\nper, to solve the above problems we explore the possibility\nof combining additional pose clues with transformers with-\nout spatial alignment. As the Figure 1 shows, we propose\nPFD, a Pose-guided Feature Disentangling transformer Re-\nID network that utilizes pose information to clearly disen-\ntangle semantic components (e.g. human body or joint part-\ns), and force the similarities between the occluded features\nand the non-occluded features to be as inconsistent as pos-\nsible, which could strengthen the learning of discriminative\nfeatures while reducing background noise to solve the prob-\nlem of challenge (1), and effectively alleviates the failure of\nfeature learning mentioned in challenge (2).\nSpeciﬁcally, the proposed PFD includes a visual context\ntransformer encoder, a pose estimator, a Pose-guided Fea-\nture Aggregation (PFA) module, a part view based trans-\nformer decoder, and a Pose-View Matching (PVM) module.\nIn the visual context transformer encoder, we adopt a trans-\nformer based image classiﬁcation model (i.e., ViT (Dosovit-\nskiy et al. 2020)) and the camera perspective information to\ncapture the robust global context information. PFA is devel-\noped to embed the pose information into the global context\nfeatures and part features. The features obtained from PFA\ncould preliminarily indicate visible body parts. In part view\nbased transformer decoder, a set of learnable semantic views\nare introduced to implicitly enhance the disentangled body\npart features. Each part view feature corresponds to the dis-\ncriminative part of the occlusion image. However, without\nadditional supervision, we can only learn features implicitly\nand cannot constrain the learnable semantic views to cap-\nture accurate parts of the human body. Therefore, we pro-\npose a Pose-View Matching (PVM) module, which implicit-\nly learns discriminative features and explicitly matches vis-\nible body parts, thereby separating the human body features\nfrom the occluded features and reducing the interference of\nnoise mentioned in challenge (1). In addition, to avoid the\nfailure of feature learning mentioned in challenge (2), we\ndesign a Pose-guided Push Loss to reduce the similarity be-\ntween human body features and occlusion features.\nThe main contributions of this paper can be summarized\nas the following:\n(1) We propose a novel pose-guided feature disentangling\ntransformer for occluded person Re-ID by using pose\ninformation to clearly disentangle semantic components\n(e.g. human body or joint parts) and selectively match\nnon-occluded parts correspondingly.\n(2) We design a Pose-guided Push Loss to help focus on hu-\nman body parts and alleviate the interference of occlu-\nsion and noise, which avoids the failure of feature learn-\ning.\n(3) To prove the effectiveness of our method, we perform\nexperiments on occluded, holistic Re-ID datasets. Ex-\ntensive experimental results demonstrate the proposed\nmethod performs favorably against state-of-the-art meth-\nods.\n2 Related Work\n2.1 Occluded Person Re-Identiﬁcation\nOccluded person Re-ID is more challenging compared with\nholistic Re-ID due to body information incompleteness. Ex-\nisting methods can be basically divided into three categories,\nhand-craft splitting based methods, methods using addition-\nal clues, and methods based on the transformer.\nMethods based on hand-craft splitting handle the occlu-\nsion problem by measuring the similarity relationship of the\naligned patches. Sun et al. (Sun et al. 2018) propose a net-\nwork named Part-based Convolution Baseline (PCB) which\nuniformly partition the feature map and learn local features\ndirectly. Sun et al. (Sun et al. 2019c) propose a region based\nmethod VPM which perceives the visible region through\nself-supervision. Jia et al. (Jia et al. 2021b) propose MoS\nwhich formulates the occluded person Re-ID as a set match-\ning problem by using Jaccard similarity coefﬁcient between\nthe corresponding partten set.\nSome methods leverage external cues to locate the human\nbody part such as segmentation, pose estimation or body\nparsing. Song et al.(Song et al. 2018) propose a mask-guided\ncontrastive attention model to learn features separately from\nthe body. Miao et al. (Miao et al. 2019) introduce Pose-\nGuided Feature Alignment (PGFA) that utilizes pose infor-\nmation to mine discriminative parts. Gao et al. (Gao et al.\n2020) propose a Pose-guided Visible Part Matching (PVPM)\nmodel to learn discriminative part features with pose-guided\nattentions. Wang et al. (Wang et al. 2020) propose HOReID\nthat introduces the high-order relation and human-topology\ninformation to learn robust features.\nRecently, methods based on transformer are emerging,\nand the transformer has two major capalities. First, trans-\nformer has been proven to have powerful feature extraction\ncapabilities. He et al. (He et al. 2021) investigate a pure\ntransformer framework named TransReID that combines the\ncamera perspective information and achieves good perfor-\nmance on both person Re-ID and Vehicle Re-ID. Second,\ntransformer has the ability to learn the disentangled features.\nLi et al. (Li et al. 2021) is the ﬁrst one to propose Part Aware\nTransformer (PAT) for occluded person Re-ID, which could\ndisentangle robust human part discovery.\nDifferent from above methods, our method combines the\npose information and transformer architecture to clearly dis-\nentangle more discriminative features and effectively allevi-\nate the failure of feature learning caused by the occlusion.\n2.2 Visual Transformer\nTransformer (Vaswani et al. 2017) has made great achieve-\nments in the ﬁeld of natural language processing. Inspired\nby the self-attention mechanism, many researchers apply\ntransformers in computer vision. For example, ViT (Doso-\nvitskiy et al. 2020) processes images directly as sequences\nand achieves state-of-the-art performance in image recog-\nnition. DETR (Carion et al. 2020) performs cross-attention\nbetween object query and feature map to transform detection\n2541\nPositional \nEn\ncoding\nLinear \npr\nojection\nTransformer \nEn\ncoder\nPose Estimation\nHeatmaps\nCamera \nIn\nformation\nEmbedding\nTransformer \nDe\ncoder\nLearnable \nse\nmantic views\nFC\n…\nࢌ૚\nࢌ࢜\nࡺ࢜\nࢌ૛\nPose Guided \nPu\nsh Loss\nPose guided \nFe\nature \nAggregation\nSet\nK-part \nViews Set\nPose\nse-\ne-View Matching Module\nCamera\nVisual Context Encoder\nPart View Based Decoder\nPosese-\ne-guided Feature Aggregation\nView feature with high \nke\ny-point confidence  \nView feature with low \nke\ny-point confidence  \nSplit image patches  Ex tracted semantic representationLearnable semantic views \nAvgPooling\nMatched line Un matched line\nAvgPooling\nDot productAdd\nࢌ࢔ࢋ\nࢌࢍ\n࢈\nࢌ࢔ࢋ\nࢌࢊ࢖\nIdentity Loss\nTr\niplet Loss\nࢌ࢖ࢍ\n澽\nTransformer Layer澽\n澽\n澽\n…\nconcat\nࢌ૚\nࢌࢍ\nࡷ࢖\nࢌ૛\n澽\n澽\n澽\nܜܑܔܘܛ\nPart featuresࢌ࢚࢘ࢇ࢖ \n澽 Encoder global \nfe\natures ࢌ࢈ࢍ\n…\nFigure 2: The proposed PFD consists of four parts. The ﬁrst part is Visual Context Encoder, which encodes the camera in-\nformation into embeddings to capture global context information. The second part is Pose-guided Feature Aggregation (PFA)\nthat leverages the matching and distributing mechanism to preliminarily indicate visible body parts. The third part is Part View\nbased Decoder that disentangles the pose-guided feature into discriminative view set under the guidance of the Nv learnable\nsemantic views. The fourth part is Pose-View Matching module (PVM), which regards the obtained view set and pose-guided\nfeature set as a set matching problem. In addition, Pose-guided Push Loss is proposed to emphasize the features of visible body\nparts. For more details, please refer to the proposed method.\nproblem into a one-to-one matching problem, which elimi-\nnates the need for hand-crafted components in object detec-\ntion.\n3 Proposed Method\nIn this section, we introduce the proposed Pose-Guided Fea-\nture Disentangling (PFD) transformer in detail. An overview\nof our method is shown in Figure 2.\n3.1 Visual Context Transformer Encoder\nWe build our encoder based on transformer-based image\nclassiﬁcation model (i:e:ViT(Dosovitskiy et al. 2020)). Giv-\nen a person imagex∈RH\u0002W\u0002C, where H,W,Cdenote the\nheight, width, and channel dimension respectively. We ﬁrst\nsplit the xinto N ﬁxed-sized patches {xi\np|i = 1;2;:::;N }\nby using a sliding window. The step size can be denoted as\nS, the size of each image patch as P, and the number of\npatches N can be described as:\nN = ⌊H+ S−P\nS ⌋×⌊W + S−P\nS ⌋; (1)\nwhere ⌊·⌋is the ﬂoor function. When S is equal to patch\nsize P, The divided patches are non-overlapping. When S\nis smaller than P, the generated patches are overlapping,\nwhich can alleviate the loss of the spatial neighborhood in-\nformation of the image. The transformer encoder only re-\nquires sequence as input, so a trainable linear projection\nfunction f(·) is performed on ﬂatten patches to map the\npatches to D dimensions, and ﬁnally obtain the patch em-\nbeddings E ∈RN\u0002D, (i:e:;Ei = f(xi);i = 1;2;:::;N ).\nA learnable [class] token xclass is prepended to the patch\nembeddings, and the output [class] token serves as encoder\nglobal feature representation fgb. In order to retain the po-\nsition information, we apply learnable positional encodings.\nHowever, the features are very susceptible to the variation of\nthe camera, so we follow the method in (He et al. 2021) to\nlearn camera perspective information. Then the ﬁnal input\nsequence can be described as:\nEinput = {xclass; Ei}+ PE + \u0015cmCid; (2)\nwhere PE is positional embeddings, Cid ∈R(N+1)\u0002D is\ncamera embeddings, and the Cid is same for the same im-\nages. \u0015cm is a hyper-parameter to balance the weight of\n2542\ncamera embeddings. Next, the input embeddings Einput\nwill be processed by mtransformer layers. The ﬁnal output\nfen ∈R(N+1)\u0002D of encoder can be divided into two parts\n(encoder global features and part features):fgb ∈R1\u0002D and\nfpart ∈RN\u0002D. In order to learn more discriminative fea-\ntures on human parts, the part features fpart are split into K\ngroups in order, and the size of each group is (N==K) ×D.\nThen, each group that concatenates the encoder global fea-\nture fgb ∈R1\u0002D will be fed into a shared transformer layer\nto learn Kgroup part local feature fgp = [f1\ngp;f2\ngp;:::;f K\ngp].\nEncoder Supervision Loss. We adopt cross-entropy loss as\nidentity loss and triplet loss for encoder global features and\ngroup features. The encoder loss function can be formulated\nas:\nLen = Lid(P(fgb)) + 1\nK\nKX\ni=1\nLid(P(fi\ngp))\n+ Ltri(fgb) + 1\nK\nKX\ni=1\nLtri(fi\ngp);\n(3)\nwhere P(·) denotes the probability prediction function.\n3.2 Pose-guided Feature Aggregation\nOccluded person images suffer from less body information,\nand non-body part information can be ambiguous, which\ncauses performance degradation. Thus, we employ a human\npose estimator to extract keypoint information from images.\nPose Estimation. Given a person image x, the estimator\nextract M landmarks from input image. Then the landmarks\nare utilized to generate heatmapsH = [h1;h2;:::;h M ]. Each\nheatmap is downsampled to the size of(H=4) ×(W=4). The\nmaximum response point of each heatmap corresponds to a\njoint point. We set a threshold\rto ﬁlter out high conﬁdence\nlandmarks and low conﬁdence landmarks. But unlike (Miao\net al. 2019), we do not set the heatmap whose landmark is\nsmaller than \rto 0. Instead, a label li ∈{0;1};i= 1;:::;M\nwill be assigned for each heatmap. Formally, heatmap label\ncan be illustrated as:\nli =\n\u001a0 ci <\r\n1 ci ≥\r (i= 1;:::;M); (4)\nwhere ci denotes the conﬁdence score of i-th landmark.\nPose-guided Feature Aggregation. In order to integrate\nthe the pose information, we set K = M, which is exact-\nly equal to the number of keypoints. Then, a fully connect-\ned layer is applied to heatmaps H to obtain the heatmaps\nH\n0\n, whose dimension is same as the group part local fea-\nture fgp. Next, the heatmaps H\n0\nmutiply fgp element-wisely\nand obtain the pose-guided feature P = [P1;P2;:::;P M ].\nAlthough P has explicitly encoded the information of dif-\nferent parts of the human body, we hope to ﬁnd the part of\nthe information from fgp that contributes the most to a cer-\ntain body part. Thus, we develop a matching and distributing\nmechanism, which regards the part local feature and pose-\nguided feature as a set similarity measurement problem. Fi-\nnally, we can obtain the pose-guided feature aggregation set\nS = {Si|i= 1;2;:::;M }. For eachPi, we can ﬁnd the most\nsimilar feature in fgp, and then two features are added to\nform the Si. Formally,\nk= arg max\nj\n(\n\nPi;fj\ngp\n\u000b\n||Pi||||fj\ngp||\n); (5)\nSi = Pi + fk\ngp; (6)\nwhere i = 1;2;:::;K , ⟨·;·⟩denotes the inner product, fk\ngp\ndenotes the most similar one to Pi in fgp.\n3.3 Part View Based Transformer Decoder\nIn this section, we deﬁne a set of learnable semantic part\nviews to learn the discriminative body parts. The learn-\nable semantic part views can be denoted as Z = {Zi|i =\n1;2;:::;N v}, andZ ∈RNv\u0002D will be added to each cross-\nattention layer as queries. As shown in Figure 2, the keys and\nvalues are from the combination of pose heatmap H and the\noutput of encoder fen. An average pooling layer is applied\nto heatmap H and then multiply the fen, ﬁnally output the\nfde ∈R(N+1)\u0002D. Formally, queries, keys and values can be\nformulated as:\nQi = ZiWq;Kj = fj\ndeWk;Vj = fj\ndeWv; (7)\nwhere i = 1;2;:::;N v;j = 1;2;:::;D , linear projection-\ns Wq ∈ RD\u0002dk , Wk ∈ RD\u0002dk , and Wv ∈ RD\u0002dv\nare applied to semantic part views and feature fde, respec-\ntively. Next, we can obtain the Nv part views set v =\n{vi|i= 1;2;:::;N v}by implementing the multi-head atten-\ntion mechanism and two fully-connected layer, which is the\nsame as (Vaswani et al. 2017).\nPose-View Matching Module. In the cross-attention\nmechanism, the Nv part semantic views can learn some dis-\ncriminative features. However, it is unknown which part or\nwhat kind of the information has been learned. Therefore,\nin order to obtain features related to the human skeleton, we\npropose a pose-view matching module. Since each feature\nof the pose-guided feature aggregation set S is related to a\ncertain keypoint information of the human body, we can ﬁnd\nthe part view vi related to the certain keypoint of the human\nbody by calculating the similarity between the part view vi\nand Si. The matched semantic part view vi and the pose-\nguided feature aggregation feature Si are added to produce\nthe ﬁnal view feature set Fv = {fi\nv|i = 1;2;:::;N v}. For-\nmally,\nk= arg max\nj\n( ⟨vi;Sj⟩\n||vi||||Sj||); (8)\nfi\nv = vi + Sk; (9)\nsince the conﬁdence score of the landmarks can indicate\nwhich part of feature contains human body information, the\nheatmap label li can guide us to split the view feature set\nFv into two parts. Features with heatmap label li = 1 in\nview set feature form a high-conﬁdence keypoint view fea-\nture set Fh = {fi\nh|i = 1;2;:::;L }, and the rest form the\nlow-conﬁdence keypoint view feature set Fl = {fi\nl |i =\n1;2;:::;N v −L}, whereLdenotes the number of features in\nFv whose heatmap label is equal to 1.\n2543\nDecoder Supervision Loss. In order to focus on more\nnon-occluded body features, we propose a Pose-guided Push\nLoss:\nfph = AvgPooling(Fh); (10)\nfpl = AvgPooling(Fl); (11)\nLp = 1\nB\nBX\ni\nD\nfi\nph;fi\npl\nE\n||fi\nph||||fi\npl||; (12)\nwhere B denotes the training batch size. The motivation\nof this loss is obvious. Human body parts and non-human\nbody parts should not have strong similarities. If Fh and\nFl are similar, then Lp will be large and the learnable se-\nmatic part views will adjust themselves adaptively. In order\nto guide the decoder view feature representation learning,\nan average-pooling layer is applied to high-conﬁdence key-\npoints view feature setFh to obtain the pose-guided decoder\nglobal feature fph, then identity loss and triplet loss are em-\nployed to guide pose-guided decoder global feature fph and\nhigh-conﬁdence feature fh learning as in Eq.13.\nLde = Lid(P(fph)) +1\nL\nLX\ni=1\nLid(P(fi\nh))\n+ Ltri(fph) +1\nL\nLX\ni=1\nLtri(fi\nh):\n(13)\n3.4 Training and Inference\nIn the training stage, the pose estimation uses a pre-trained\nmodel, and the rest of components (such as encoder, decoder\nand so on) are trained together with the overall objective\nloss, which is formulated as Eq.14.\nL= \u0015enLen + \u0015deLde + Lp; (14)\nwhere \u0015en and \u0015de are the scale factor of encoder loss and\ndecoder loss respectively, and both are set to 0.5.\nIn the test stage, we concatenate the encoder global fea-\nture fgb, pose-guided decoder global featurefph, group local\npart feature fgp and high-conﬁdence keypoint view feature\nFh as representation F, ingnoring the low-conﬁdence key-\npoint view feature Fl. However, high-conﬁdence keypoint\nview feature Fh has variable length, and the network is dif-\nﬁcult to implement. Thus, we ﬁx the length of it to Nv by\npadding zeros.\nF=\n\u0002\nfgb;fph;f1\ngp;:::;f K\ngp;f1\nh;:::;f L\nh\n\u0003\n: (15)\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nTo illustrate the effectiveness of our method, We evaluate\nour method on ﬁve Re-ID datasets for two tasks including\noccluded person Re-ID and holistic person Re-ID.\nOccluded-Duke (Miao et al. 2019) consists of 15,618\ntraining images, 2,210 occluded query images and 17,661\ngallery images. It is a subdataset of DukeMTMC-reID\n(Zheng, Zheng, and Yang 2017), whichtians occluded im-\nages and remove some overlapping images.\nOccluded-REID (Zhuo et al. 2018) is captured by the\nmobile phone, which consist of 2,000 images of 200 occlud-\ned persons.Each identity has ﬁve full-body person images\nand ﬁve occluded person images with different types of se-\nvere occlusions.\nMarket-1501 (Zheng et al. 2015) contains 1,501 iden-\ntities observed from 6 camera viewpoints, 12,936 training\nimages of 751 identities, 19,732 gallery images, and 2,228\nqueries.\nDukeMTMC-reID (Zheng, Zheng, and Yang 2017) con-\ntains 36,411 images of 1,404 identities captured from 8 cam-\nera viewpoints. It contains 16,522 training images, 17,661\ngallery images and 2,228 queries.\nMSMT17 (Wei et al. 2018) contains 125,441 images of\n4101 identities captured from 15 camera viewpoints. It con-\ntains 32,621 training images. During inference, 82,161 im-\nages are randomly selected as gallery and other 11,659 im-\nages are considered as query.\nEvaluation Metrics. We adopt Cumulative Matching\nCharacteristic (CMC) curves and Mean average precision\n(mAP) to evaluate the quality of different Re-ID models.\n4.2 Implementation Details\nBoth training and testing images are resized to 256 ×128.\nThe training images are augmented with random horizon-\ntal ﬂipping, padding, random cropping and random erasing\n(Zhong et al. 2020). The initial weights of encoder are pre-\ntrained on ImageNet-21K and then ﬁnetuned on ImageNet-\n1K. In this paper, the number of the split group K and the\nnumber of the estimated human landmarks are both set to\n17. The number of decoder layer is set to 2 on Occluded-\nDuke and 6 on the other datasets. The hidden dimension D\nis set to 768. The transformer decoder is same with (Vaswani\net al. 2017). The batch size is set to 64 with 4 images per ID.\nThe learing rate is initialized at 0.008 with cosine learning\nrate decay. To detect landmarks from images, we adopt HR-\nNet (Sun et al. 2019b) pre-trained on the COCO dataset. The\nthreshold \ris set to 0.2.\n4.3 Comparison with the State-of-the-Art\nWe compare our method with the state-of-the-art method-\ns on ﬁve benchmarks including occluded person ReID and\nholistic person ReID.\nResults on Occluded-Duke and Occluded-REID. Ta-\nble 1 shows the results on two occluded datasets. As\ntable shows, three kinds of methods are compared: (1)\nhand-crafted splitting based methods including Part-Aligned\n(Zhao et al. 2017) and PCB (Sun et al. 2018). (2) occluded\nReID methods including Part Bilinear (Suh et al. 2018), PD-\nGAN (Ge et al. 2018), Ad-Occluded (Huang et al. 2018),\nFPR (He et al. 2019), PGFA (Miao et al. 2019), PVPM\n(Gao et al. 2020), GASM (He and Liu 2020), HOReID\n(Wang et al. 2020), ISP (Zhu et al. 2020) and MoS (Jia\net al. 2021b). (3) Transformer based occluded ReID meth-\nods including PAT (Li et al. 2021) and TransReID (He et al.\n2021). From the table, we can observe that our proposed\nmethod PFD achieves 67.7%/79.8% Rank-1 accuracy and\n60.1%/81.3% mAP on Occluded-Duke and Occluded-REID\n2544\nMethods Occluded-Duke Occluded-REID\nRank-1 mAP Rank-1 mAP\nPart-Aligned (ICCV 17) 28.8 44.6 - -\nPCB (ECCV 18) 42.6 33.7 41.3 38.9\nPart Bilinear (ECCV 18) 36.9 - - -\nFD-GAN (NIPS 18) 40.8 - - -\nAd-Occluded (CVPR 18) 44.5 32.2 - -\nFPR (ICCV 19) - - 78.3 68.0\nPGFA (ICCV 19) 51.4 37.3 - -\nPVPM (CVPR 20) 47.0 37.7 66.8 59.5\nGASM (ECCV 20) - - 74.5 65.6\nISP (ECCV 20) 62.8 52.3 - -\nHOReID (CVPR 20) 55.1 43.8 80.3 70.2\nMoS (AAAI 21) 61.0 49.2 - -\nTransReID (ICCV 21) 64.2 55.7 - -\nPAT (CVPR21) 64.5 53.6 81.6 72.1\nPFD (Ours) 67.7 60.1 79.8 81.3\nTransReID\u0003 (ICCV 21) 66.4 59.2 - -\nPFD\u0003 (Ours) 69.5 61.8 81.5 83.0\nTable 1: Performance comparison with state-of-the-art\nmethods on Occluded-Duke, Occluded-REID. ”*” means\nthe encoder is with a small step sliding-window setting.\ndatasets, respectively, and outperforms all kinds of method-\ns in Occluded-Duke. Futher PFD \u0003 achieves higher Rank-1\nand mAP with a small step sliding-window setting. Com-\npared with PGFA, PVPM and HOReID, which are SOTA\nmethods with keypoints information, our method surpass-\nes them by at least +12.6% Rank-1 accuracy and +16.3%\nmAP on Occluded-Duke dataset. Compared to the compet-\ning transformer based methods PAT, our method surpasses\nit by at least +3.2% Rank-1 accuracy and +6.5% mAP on\nOccluded-Duke and +9.2% mAP on Occluded-REID.\nThe reasons for the superior performance of PFD can be\nattributed to the following points. First, compared with C-\nNN, the transformer has better feature representation abil-\nity and can pay better attention to discriminative features.\nSecond, the disentangled features obtained from our method\ncan indicate the body part information in cluttered scenes,\nleading to clear semantic guidance when matching, which is\nmore effective than spatial alignment. Third, the proposed\npose-guided push loss efﬁciently weakens the interference\nof occlusions and background clutters.\nResults on Holistic ReID datasets. To verify the effec-\ntiveness of our model on the holistic ReID task, we con-\nduct experiments on three holistic ReID datasets includ-\ning Market-1501, DukeMTMC-reID and MSMT17. Table\n2 shows the results on Market-1501 and DukeMTMC-reID\ndatasets. There are four types of methods in the comparison:\n(1) part feature based methods including PCB (Sun et al.\n2018), DSR (He et al. 2018), BOT (Luo et al. 2019b) and\nVPM (Sun et al. 2019c). (2) global feature based methods\nincluding MVPM (Sun et al. 2019a), SFT (Luo et al. 2019a),\nCAMA (Yang et al. 2019), IANet (Hou et al. 2019) and Cir-\ncle (Sun et al. 2020). (3) extra cue based methods including\nSPReID (Kalayeh et al. 2018), P2Net (Guo et al. 2019), PG-\nFA (Miao et al. 2019), AANet (Tay, Roy, and Yap 2019) and\nHOReID (Wang et al. 2020). (4) transformer based method-\nMethods Market-1501 DukeMTMC\nRank-1 mAP Rank-1 mAP\nPCB (ECCV 18) 92.3 77.4 81.8 66.1\nDSR (CVPR 18) 83.6 64.3 - -\nBOT (CVPRW 19) 94.1 85.7 86.4 76.4\nVPM (CVPR 19) 93.0 80.8 83.6 72.6\nMVPM (ICCV 19) 91.4 80.5 83.4 70.0\nSFT (ICCV 19) 93.4 82.7 86.9 73.2\nCAMA (CVPR 19) 94.7 84.5 85.8 72.9\nIANet (CVPR 19) 94.4 83.1 87.1 73.4\nCircle (CVPR 20) 94.2 84.9 - -\nSPReID (CVPR 18) 92.5 81.3 84.4 70.1\nP2Net (ICCV 19) 95.2 85.6 86.5 73.1\nPGFA (CVPR 19) 91.2 76.8 82.6 65.5\nAANet (CVPR 19) 93.9 82.5 86.4 72.6\nHOReID (CVPR 20) 94.2 84.9 86.9 75.6\nTransReID (ICCV 21) 95.0 88.2 89.6 80.6\nPAT (CVPR21) 95.4 88.0 88.8 78.2\nPFD (Ours) 95.5 89.6 90.6 82.2\nTransReID\u0003 (ICCV 21) 95.2 88.9 90.7 82.0\nPFD\u0003 (Ours) 95.5 89.7 91.2 83.2\nTable 2: Performance comparison with state-of-the-art mod-\nels on Market-1501 and DukeMTMC-reID datasets.\ns including TransReID (He et al. 2021) and PAT (Li et al.\n2021). From the table, we can observe that our proposed\nmethod achieve competitive results. Speciﬁcally, our method\nachieves SOTA performance (95.5%/90.6% Rank-1 accu-\nracy and 89.5%/82.2% mAP, respectively) on Market-1501\nand DukeMTMC-reID datasets. Compared with transformer\nbased method PAT, our method surpasses it by +1.6% mAP\non Market-1501 and +1.8%/+4% Rank-1 accruacy/mAP on\nDukeMTMC. We also conduct experiments on the proposed\nmethod on the MSMT17 dataset. Several methods are com-\npared, including MVPM (Sun et al. 2019a), SFT (Luo et al.\n2019a), OSNet (Zhou et al. 2019), IANet (Hou et al. 2019),\nDG-Net (Zheng et al. 2019), CBN (Zhuang et al. 2020),\nCirecle (Sun et al. 2020), RGA-SC (Zhang et al. 2020), and\nSAN (Jin et al. 2020). From the table 3 we can see that pro-\nposed PFD achieves competitive performance. Speciﬁcal-\nly, our method achieves 82.7% Rank-1 accuracy and 65.1%\nmAP on MSMT17. It can be seen that although our method\nis not designed for holistic reid tasks, it can still achieve\ncompetitive results, which reﬂects the robustness of our pro-\nposed method.\n4.4 Ablation Study\nIn this part, we conduct ablation studies on Occluded-Duke\ndataset to analyze the effectiveness of each component.\nEffectiveness of proposed Modules. Table 4 shows the\nexperimental results. Index-1 denotes that the pure trans-\nformer encoder-decoder architecture. We can see that the\nperformance can reach 58.2% rank-1 accuracy and 48.3%\nmAP, which even shows better performance than pose-\nguided SOTA method HOReID. This is because the self-\nattention mechanism can focus on more discriminative fea-\ntures than CNN. From index-2, when pose-guided feature\naggregation is added, the performance is greatly improved\n2545\nMethods Rank-1 mAP\nMVPM (ICCV 19) 71.3 46.3\nSFT (ICCV 19) 73.6 47.6\nOSNet (ICCV 19) 78.7 52.9\nIANet (CVPR 19) 75.5 46.8\nDG-Net (CVPR 19) 77.2 52.3\nCBN (ECCV 20) 72.8 42.9\nCircle (CVPR 20) 76.3 -\nRGA-SC (CVPR 20) 80.3 57.5\nSAN (AAAI 20) 79.2 55.7\nPFD (Ours) 82.7 65.1\nPFD\u0003 (Ours) 83.8 64.4\nTable 3: Performance comparison with state-of-the-art mod-\nels on MSMT17.\nIndex PFA PVM Lp R-1 R-5 R-10 mAP\n1 58.2 74.5 80.1 48.3\n2 X 63.7 77.8 82.3 56.2\n3 X 62.4 76.7 81.0 54.6\n4 X X 64.3 77.6 82.1 56.7\n5 X X 67.0 80.0 84.4 59.5\n6 X X X 67.7 80.1 85.0 60.1\nTable 4: Ablation study over Occluded-Duke.\nby +5.5% rank-1 accuracy and +7.9% mAP. This shows that\nthe introduction of pose information and correct aggregation\ncan bring good performance improvements. From index-3,\nwe can see that our proposed PVM is also effective. And by\ncomparing index-3 and index-5, we discover that combina-\ntion of PFA and PVM can increase performance by +8.8%\nrank-1 accuracy and +11.2% mAP, which indicates that pose\ninformation and correct matching is very important. From\nindex-5 and index-6, we can see that our overall model can\nachieve optimal performance, which shows the effectiveness\nof the Pose-guided Push Loss.\nAnalysis of the number of Semantic views. The num-\nber of semantic viewsNv determines the granularity of view\nfeatures. As shown in Table 5, the performance of our pro-\nposed PFD is robust to Nv. With Nv increases, the perfor-\nmance keeps improving before Nv arrives 17, which is ex-\nactly equal to the number of keypoints. So, we conclude that\n17 semantic views may be able to capture the corresponding\n17 key point features.\nAnalysis of the number of Transformer Layers. We\nperform quantitative experiments to ﬁnd the most suitable\nnumber of decoder layer. As shown in Figure 3(a), when the\ndecoder is removed, the performance of the model is great-\nly reduced. It can be seen that only the features obtained by\nthe encoder are not robust enough, and the learnable seman-\ntic view in the decoder can implicitly learn more important\nfeatures, which enhances the features from the encoder. we\nobserve that when the number of decoder layer is set to 2, the\nbest performance can be achieved. And with the increase of\nthe number of layers, there is almost no improvement in per-\nformance. This is because the resolution of the images in the\ndata set is small, and the content is relatively simple.\nThe Impact of the Threshold \r. The threshold \r is de-\nNv R-1 R-5 R-10 mAP\n1 65.5 79.1 84.0 57.1\n5 66.7 79.9 83.7 58.4\n10 66.9 79.5 83.9 58.9\n15 67.4 80.0 84.0 59.1\n17 67.7 80.1 85.0 60.1\n20 66.9 79.4 84.3 59.0\nTable 5: Parameter analysis for the number of semantic\nviews Nv.\n澳\n57.3\n67\n.5 67.7 66.8 67.3 66.9 67.5\n51.2\n59.9 60.1\n59.5 60 59.6\n58.9\n51\n54\n5\n7\n60\n63\n55\n57\n59\n61\n63\n65\n67\n69\n0 1 2 3 4 5 6\nmAP (%)\nRank-1 (%)\nNumber of decoder layer\nRank-1\nmAP\n(a)\n澳\n65.5\n67\n.3 67.7 67.4 66.9 67.2\n66.3\n64.9\n56.8\n58.8\n60.1 59.9\n59.3 59.5\n58.6\n57.4\n54\n56\n5\n8\n60\n62\n64\n66\n55\n57\n59\n61\n63\n65\n67\n69\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7\nmAP (%)\nRank-1 (%) \nThreshold Ȗ\nRank-1\nmAP\n(b)\nFigure 3: Parameter analysis for the number of decoder (a)\nand the threshold \r(b).\nﬁned in Eq 4 to indicate the high conﬁdence landmarks,\nwhich could help PVM explicitly match visible body part-\ns. We conduct ablation study on threshold \r by changing it\nfrom 0 to 0.7. From the Figure 3(b), when \r is set to 0.2,\nwe can get the best performance. When the value of \r is\ntoo small, PVM may consider all landmarks as human body\nareas, thereby introducing noise. Conversely, when the \r is\ntoo large, a certain body area information may be lost. It is\nworth noting that when gamma is set to 0, a lot of noise is\nintroduced, but our method can still achieve 65.5% Rank-\n1 accuracy, which is still SOTA performance on Occluded-\nDuke. This shows that our method is robust to pose noise,\nand further indicates why it could achieve good results on\nthe holistic datasets.\nMethods R-1 R-5 R-10 mAP\nHR-Net(CVPR 19) 67.7 80.1 85.0 60.1\nAlphaPose(ICCV 17) 65.9 78.9 82.6 57.8\nOpenPose(CVPR 17) 64.1 77.8 81.2 55.6\nTable 6: Performance of PFD with different pose estimator.\n2546\n澳\n澪澩澢澭 澪澪 澪 澪澢澥 澪澩澢澬\n澪澧澢澬\n澩澭澢澪\n澩澪澢澦\n澩澬澢澬 澩澬澢澫 澩澬澢澫 澩澬澢澨\n澩澪\n澩澦澢澧\n澨澫澢澫\n澨澩\n澩澤\n澩\n澩\n澪澤\n澪澩\n澫澤\n澩澤\n澩澦\n澩澨\n澩澪\n澩澬\n澪澤\n澪澦\n澪澨\n澪澪\n澪澬\n澫澤\n澤澢澥 澤澢澩 澥 澩 澥澤 澥澩 澦澤\n濡澵濄 澜澙澝\n濆濕濢濟澡澥澔澜澙澝\n濈濜濙澔濧濨濕濢濘濕濦濘澔濘濙濪濝濕濨濝濣濢澔煙 濣濚澔澻濕濩濧濧濝濕濢澔濢濣濝濧濙澔\n濆澥\n濡澵濄\nFigure 4: The impact of adding Gaussian noise to the esti-\nmated heatmap.\nThe Impact of Pose Estimation.We adopt three different\npose estimation algorithms, HRNet (Sun et al. 2019b), Al-\nphaPose (Fang et al. 2017), and OpenPose (Cao et al. 2017)\nin PFD. From the Table 6, the results shows that the PFD still\ncould achieve state-of-the-art performance by using less re-\nliable landmark estimators. Besides, we add Gaussian noise\nN(\u0016;\u001b) to the estimated heatmap by changing \u001b from 0.1\nto 20. From Fig 4, we ﬁnd that the model is robust to pose\nnoise when \u001bis less than 10.\n4.5 Visualization\nWe visualize decoder cross-attention for the different learn-\nable semantic views and fuse them together to form attention\nheatmap. As Figure 5 shows, the fused learnable semantic\nviews can almost accurately localize the unobstructed part\nof the human body, which proves the effectiveness of our\nproposed method.\nFigure 5: Visualization of decoder attention heatmaps of\nlearned semantic views.\n5 Conclusion\nIn this paper, we propose a transformer based Pose-guided\nFeature Disentangling (PFD) method for the occluded Re-\nID task that utilizes pose information to clearly disentan-\ngle semantic components. PFD contains a transformer based\nencoder-decoder architecture, two matching modules (P-\nFA and PVM), and a Pose-guided Push Loss. The ViT\nbased encoder extracts the patch features with its strong\ncapability. Then the PFA module preliminarily indicates\nvisible body parts by matching estimated pose heatmap-\ns and patch features. In decoder, we deﬁne a set of learn-\nable semantic views to learn the discriminative body part-\ns, and then the PVM module is proposed to enhance the\nencoder features by matching the most similar features be-\ntween view set and pose guided feature aggregation set. Be-\nsides, PVM could automatically separate the occlusion fea-\ntures with the guidance of pose estimation. At last, a Pose-\nguided Push Loss is proposed to better eliminate the inter-\nference of occlusion noises by pushing the distance between\nvisible parts and occluded parts in the embedding space.\nFinally, we conduct experiments on ﬁve popular dataset-\ns including Occluded-Duke, Occluded-REID, Market-1501,\nDukeMTMC-reID and MSMT17, and the competitive re-\nsults demonstrate the effectiveness of the proposed method.\nAcknowledgments\nThis research was supported by National Key R&D Program\nof China (No. 2020AAA0108904), Science and Technology\nPlan of Shenzhen (No. JCYJ20200109140410340).\nReferences\nCao, Z.; Simon, T.; Wei, S.-E.; and Sheikh, Y . 2017. Re-\naltime Multi-Person 2D Pose Estimation Using Part Afﬁnity\nFields. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision (ECCV), 213–229.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFang, H.-S.; Xie, S.; Tai, Y .-W.; and Lu, C. 2017. RMPE:\nRegional Multi-person Pose Estimation. In ICCV.\nGao, S.; Wang, J.; Lu, H.; and Liu, Z. 2020. Pose-guided\nvisible part matching for occluded person ReID. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 11744–11752.\nGe, Y .; Li, Z.; Zhao, H.; Yin, G.; Yi, S.; Wang, X.; and Li, H.\n2018. Fd-gan: Pose-guided feature distilling gan for robust\nperson re-identiﬁcation. arXiv preprint arXiv:1810.02936.\nGuo, J.; Yuan, Y .; Huang, L.; Zhang, C.; Yao, J.-G.; and\nHan, K. 2019. Beyond human parts: Dual part-aligned rep-\nresentations for person re-identiﬁcation. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 3642–3651.\nHe, L.; Liang, J.; Li, H.; and Sun, Z. 2018. Deep spatial\nfeature reconstruction for partial person re-identiﬁcation:\nAlignment-free approach. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 7073–7082.\nHe, L.; and Liu, W. 2020. Guided saliency feature learning\nfor person re-identiﬁcation in crowded scenes. In European\nConference on Computer Vision (ECCV), 357–373.\n2547\nHe, L.; Wang, Y .; Liu, W.; Zhao, H.; Sun, Z.; and Feng,\nJ. 2019. Foreground-aware pyramid reconstruction for\nalignment-free occluded person re-identiﬁcation. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 8450–8459.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang, W.\n2021. Transreid: Transformer-based object re-identiﬁcation.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV).\nHou, R.; Ma, B.; Chang, H.; Gu, X.; Shan, S.; and Chen,\nX. 2019. Interaction-and-aggregation network for person\nre-identiﬁcation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n9317–9326.\nHuang, H.; Li, D.; Zhang, Z.; Chen, X.; and Huang, K. 2018.\nAdversarially occluded samples for person re-identiﬁcation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 5098–5107.\nJia, M.; Cheng, X.; Lu, S.; and Zhang, J. 2021a. Learn-\ning Disentangled Representation Implicitly via Transformer\nfor Occluded Person Re-Identiﬁcation. arXiv preprint arX-\niv:2107.02380.\nJia, M.; Cheng, X.; Zhai, Y .; Lu, S.; Ma, S.; Tian, Y .; and\nZhang, J. 2021b. Matching on sets: Conquer occluded per-\nson re-identiﬁcation without alignment. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence (AAAI), vol-\nume 35, 1673–1681.\nJin, X.; Lan, C.; Zeng, W.; Wei, G.; and Chen, Z. 2020.\nSemantics-aligned representation learning for person re-\nidentiﬁcation. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence (AAAI), volume 34, 11173–11180.\nKalayeh, M. M.; Basaran, E.; G¨okmen, M.; Kamasak, M. E.;\nand Shah, M. 2018. Human semantic parsing for person re-\nidentiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 1062–\n1071.\nLi, Y .; He, J.; Zhang, T.; Liu, X.; Zhang, Y .; and Wu,\nF. 2021. Diverse Part Discovery: Occluded Person Re-\nIdentiﬁcation With Part-Aware Transformer. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2898–2907.\nLuo, C.; Chen, Y .; Wang, N.; and Zhang, Z. 2019a. Spec-\ntral feature transformation for person re-identiﬁcation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 4976–4985.\nLuo, H.; Gu, Y .; Liao, X.; Lai, S.; and Jiang, W. 2019b.\nBag of tricks and a strong baseline for deep person re-\nidentiﬁcation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition Work-\nshops (CVPRW).\nMiao, J.; Wu, Y .; Liu, P.; Ding, Y .; and Yang, Y . 2019.\nPose-guided feature alignment for occluded person re-\nidentiﬁcation. In Proceedings of the IEEE/CVF Internation-\nal Conference on Computer Vision (ICCV), 542–551.\nShi, W.; Liu, H.; and Liu, M. 2020. Identity-sensitive loss\nguided and instance feature boosted deep embedding for per-\nson search. Neurocomputing, 415: 1–14.\nSong, C.; Huang, Y .; Ouyang, W.; and Wang, L. 2018.\nMask-guided contrastive attention model for person re-\nidentiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 1179–\n1188.\nSuh, Y .; Wang, J.; Tang, S.; Mei, T.; and Lee, K. M.\n2018. Part-aligned bilinear representations for person re-\nidentiﬁcation. In Proceedings of the European Conference\non Computer Vision (ECCV), 402–419.\nSun, H.; Chen, Z.; Yan, S.; and Xu, L. 2019a. Mvp match-\ning: A maximum-value perfect matching for mining hard\nsamples, with application to person re-identiﬁcation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 6737–6747.\nSun, K.; Xiao, B.; Liu, D.; and Wang, J. 2019b. Deep high-\nresolution representation learning for human pose estima-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 5693–5703.\nSun, Y .; Cheng, C.; Zhang, Y .; Zhang, C.; Zheng, L.; Wang,\nZ.; and Wei, Y . 2020. Circle loss: A uniﬁed perspec-\ntive of pair similarity optimization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 6398–6407.\nSun, Y .; Xu, Q.; Li, Y .; Zhang, C.; Li, Y .; Wang, S.; and\nSun, J. 2019c. Perceive where to focus: Learning visibility-\naware part-level features for partial person re-identiﬁcation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 393–402.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018.\nBeyond part models: Person retrieval with reﬁned part pool-\ning (and a strong convolutional baseline). In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\n480–496.\nTay, C.-P.; Roy, S.; and Yap, K.-H. 2019. Aanet: Attribute\nattention network for person re-identiﬁcations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 7134–7143.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS), 5998–6008.\nWang, G.; Yang, S.; Liu, H.; Wang, Z.; Yang, Y .; Wang, S.;\nYu, G.; Zhou, E.; and Sun, J. 2020. High-order information\nmatters: Learning relation and topology for occluded person\nre-identiﬁcation. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n6449–6458.\nWei, L.; Zhang, S.; Gao, W.; and Tian, Q. 2018. Per-\nson transfer gan to bridge domain gap for person re-\nidentiﬁcation. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 79–88.\nYang, W.; Huang, H.; Zhang, Z.; Chen, X.; Huang, K.; and\nZhang, S. 2019. Towards rich feature discovery with class\nactivation maps augmentation for person re-identiﬁcation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 1389–1398.\n2548\nZhang, Z.; Lan, C.; Zeng, W.; Jin, X.; and Chen, Z. 2020.\nRelation-aware global attention for person re-identiﬁcation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 3186–3195.\nZhang, Z.; Zhang, H.; and Liu, S. 2021. Person Re-\nIdentiﬁcation Using Heterogeneous Local Graph Attention\nNetworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 12136–\n12145.\nZhao, L.; Li, X.; Zhuang, Y .; and Wang, J. 2017.\nDeeply-learned part-aligned representations for person re-\nidentiﬁcation. In Proceedings of the IEEE/CVF Internation-\nal Conference on Computer Vision (ICCV), 3219–3228.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian,\nQ. 2015. Scalable person re-identiﬁcation: A benchmark. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 1116–1124.\nZheng, L.; Yang, Y .; and Hauptmann, A. G. 2016. Person re-\nidentiﬁcation: Past, present and future. arXiv preprint arX-\niv:1610.02984.\nZheng, Z.; Yang, X.; Yu, Z.; Zheng, L.; Yang, Y .; and Kautz,\nJ. 2019. Joint discriminative and generative learning for\nperson re-identiﬁcation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2138–2147.\nZheng, Z.; Zheng, L.; and Yang, Y . 2017. Unlabeled samples\ngenerated by gan improve the person re-identiﬁcation base-\nline in vitro. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 3754–3762.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence (AAAI), vol-\nume 34, 13001–13008.\nZhou, K.; Yang, Y .; Cavallaro, A.; and Xiang, T. 2019.\nOmni-scale feature learning for person re-identiﬁcation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 3702–3712.\nZhu, K.; Guo, H.; Liu, Z.; Tang, M.; and Wang, J. 2020.\nIdentity-guided human semantic parsing for person re-\nidentiﬁcation. In European Conference on Computer Vision\n(ECCV), 346–363.\nZhuang, Z.; Wei, L.; Xie, L.; Zhang, T.; Zhang, H.; Wu, H.;\nAi, H.; and Tian, Q. 2020. Rethinking the distribution gap of\nperson re-identiﬁcation with camera-based batch normaliza-\ntion. In European Conference on Computer Vision (ECCV),\n140–157.\nZhuo, J.; Chen, Z.; Lai, J.; and Wang, G. 2018. Occluded\nperson re-identiﬁcation. In 2018 IEEE International Con-\nference on Multimedia and Expo (ICME), 1–6.\n2549",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7833304405212402
    },
    {
      "name": "Transformer",
      "score": 0.727493405342102
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6199376583099365
    },
    {
      "name": "Computer vision",
      "score": 0.5200405120849609
    },
    {
      "name": "Graph",
      "score": 0.5151094794273376
    },
    {
      "name": "Matching (statistics)",
      "score": 0.43242335319519043
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42755916714668274
    },
    {
      "name": "Pose",
      "score": 0.42475005984306335
    },
    {
      "name": "Feature matching",
      "score": 0.4163574278354645
    },
    {
      "name": "Feature extraction",
      "score": 0.35238176584243774
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1288515031337738
    },
    {
      "name": "Engineering",
      "score": 0.09825417399406433
    },
    {
      "name": "Voltage",
      "score": 0.09019666910171509
    },
    {
      "name": "Mathematics",
      "score": 0.07229804992675781
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ]
}