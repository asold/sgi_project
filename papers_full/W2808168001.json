{
  "title": "Navigating with Graph Representations for Fast and Scalable Decoding of Neural Language Models",
  "url": "https://openalex.org/W2808168001",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2348023029",
      "name": "Zhang, Minjia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1971393576",
      "name": "Liu Xiaodong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2423932408",
      "name": "Wang, Wenhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao, Jianfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747894005",
      "name": "He Yuxiong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2086179657",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2410983263",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2019789576",
    "https://openalex.org/W1496383623",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2128678576"
  ],
  "abstract": "Neural language models (NLMs) have recently gained a renewed interest by achieving state-of-the-art performance across many natural language processing (NLP) tasks. However, NLMs are very computationally demanding largely due to the computational cost of the softmax layer over a large vocabulary. We observe that, in decoding of many NLP tasks, only the probabilities of the top-K hypotheses need to be calculated preciously and K is often much smaller than the vocabulary size. This paper proposes a novel softmax layer approximation algorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a given context, a set of K words that are most likely to occur according to a NLM. We demonstrate that FGD reduces the decoding time by an order of magnitude while attaining close to the full softmax baseline accuracy on neural machine translation and language modeling tasks. We also prove the theoretical guarantee on the softmax approximation quality.",
  "full_text": "Navigating with Graph Representations for Fast and\nScalable Decoding of Neural Language Models\nMinjia Zhang Xiaodong Liu Wenhan Wang Jianfeng Gao Yuxiong He\nMicrosoft\n{minjiaz,xiaodl,wenhanw,jfgao,yuxhe}@microsoft.com\nAbstract\nNeural language models (NLMs) have recently gained a renewed interest by achiev-\ning state-of-the-art performance across many natural language processing (NLP)\ntasks. However, NLMs are very computationally demanding largely due to the\ncomputational cost of the softmax layer over a large vocabulary. We observe that,\nin decoding of many NLP tasks, only the probabilities of the top- K hypotheses\nneed to be calculated preciously and Kis often much smaller than the vocabulary\nsize. This paper proposes a novel softmax layer approximation algorithm, called\nFast Graph Decoder (FGD), which quickly identiﬁes, for a given context, a set\nof K words that are most likely to occur according to a NLM. We demonstrate\nthat FGD reduces the decoding time by an order of magnitude while attaining\nclose to the full softmax baseline accuracy on neural machine translation and\nlanguage modeling tasks. We also prove the theoretical guarantee on the softmax\napproximation quality.\n1 Introduction\nDrawing inspiration from biology and neurophysiology, recent progress for many natural language\nprocessing (NLP) tasks has been remarkable with deep neural network based approaches, including\nmachine translation [1–3], sentence summarization [4], dialogue agents [5], speech recognition [6–8],\nand conversational bots [9, 10]. Such approaches often employ a neural language model (NLM) as a\ndecoder at inference time to generate a sequence of tokens (e.g., words) given an input, typically via\nbeam search [1–13].\nOne long-recognized issue of decoding using NLMs is the computational complexity, which easily\nbecomes a bottleneck when the vocabulary size is large. Consider a beam search decoder using a\nNLM. At each decoding step, a recurrent neural network [ 14, 15] ﬁrst generates a context vector\nbased on each partial hypothesis in the beam. It then uses a softmax layer to compute a normalized\nword probability distribution over the vocabulary [16–18]. The softmax layer consists of an inner\nproduct operator that projects the context vector into a vocabulary-sized vector of scores, followed by\na softmax function that transforms a vocabulary-sized logits into a vector of probabilities. Finally, the\ndecoder selects top-Kwords with the highest probabilities given the context (i.e., top-Kmaximum\nsubset of inner product), and stores the expended hypotheses and their probabilities in the beam. The\nmost computationally expensive part in this process is the softmax layer, where the complexity is linear\nwith respect to the vocabulary size. In this paper we strive to develop new softmax approximation\nmethods for fast decoding.\nMany techniques have been proposed to speed up the softmax layer in training, such as hierarchical\nsoftmax [19] and sampling-based approaches [20–23]. However, most of them cannot be directly\napplied to decoding because they rely on knowing the words to be predicted and need to calculate the\nprobability of all words to ﬁnd the most likely prediction during decoding. Other works speed up\nsoftmax inference in training and decoding by reducing the cost of computing each word’s probability\nusing some approximation [22, 24–26]. However, the complexity of softmax as a whole is still linear\narXiv:1806.04189v1  [cs.CL]  11 Jun 2018\nwith respect to the size of the vocabulary. We observe that in beam-search-based decoding of many\nNLP tasks we only need to identify the top-K words that are most likely to occur given a context.\nCan we ﬁgure out a search algorithm that can identify these Kwords without going over the entire\nlarge vocabulary? Our answer is yes. Before we present our approach, we review brieﬂy the ﬁnding\nin biological science that motivates our research.\nIn spite of the large number of words in a vocabulary, human brain is capable of managing them\neffectively and navigating the massive mental lexicon very efﬁciently. How is it possible? How is\nthe vocabulary stored and represented in human brain? One of the theories from biological science\nindicates that human language has a character of complex network [ 27–29], where the intrinsic\nrelational structure, which refers to the fact that words are related to each other and thus form a small\nworld graph, provides some hints on how the lexicon is mentally organized. To predict the next\nword given a context, humans never need to examine every word in the vocabulary stored in their\nbrains. Instead, a person can immediately identify a small set of K candidate words that are most\nsemantically related to the context, and then pick the most proper word among the candidates. We\nbelieve that if we can represent the vocabulary of a NLM using a similar data structure ofsmall world\ngraph, we can signiﬁcantly improve the decoding efﬁciency of NLMs because at each decoding step\nsoftmax only needs to explicitly compute the probabilities of K words, where K is much smaller\nthan the vocabulary size.\nWe propose a Fast Graph Decoder (FGD) to approximate the softmax layer of a NLM in the beam\nsearch decoding process. First, we construct a small world graph representation [30, 31] of a NLM\nvocabulary. The nodes in the graph are words, each being represented using a continuous vector\nwhich is transformed from its word embedding vector in the NLM. The edges in the graph encode\nthe word-word distances in a well-deﬁned metric space. Then, at each decoding step, we identify\nfor a given context (i.e., a partial hypothesis in the beam) the top-Khypotheses and compute their\nprobabilities in the softmax layer of the NLM. We prove that ﬁnding the top-K hypotheses in the\nsoftmax layer is equivalent to ﬁnding the Knearest neighbors using FGD in the small world graph,\nand the latter can be performed approximately using an efﬁcient graph navigating method [32, 33].\nWe also prove that the decoding error due to use of the approximated Knearest neighbor search with\ngraph navigation is theoretically bounded.\nWe validate the effectiveness of our approach on two NLP tasks, neural machine translation and\nlanguage modeling. Empirical results show that FGD achieves an order of magnitude speedup while\nattaining the accuracy, in comparison with existing state-of-the-art approaches.\nIn the rest of the paper, Section 2 details the softmax implementation and the challenge. Section 3 de-\nscribes FGD and gives theoretical justiﬁcations. Section 4 presents experimental results. Conclusions\nare drawn in Section 5.\n2 Motivation and Challenge\nThe softmax layer of a NLM is the computational bottleneck at the decoding time in many NLP tasks.\nConsider a NLM that uses a two–layer LSTM and a vocabulary size of|V|[16–18]. The total number\nof ﬂoating point operations (FLOPS) per LSTM step is 2(layer) ×(I+ D) ×D×4 ×2 1, where I\nand Drepresent the input and hidden dimension, respectively. The number of FLOPS of the softmax\nlayer is D×|V|×2, which is proportional to |V|. Assuming that the dimension of the input/hidden\nlayers of the LSTM is 500 and the vocabulary size is 50K, the LSTM part has 8M FLOPS whereas\nthe softmax layer has 50M FLOPS. The softmax layer dominates the computational cost of the NLM,\nand even more so with a larger vocabulary.\nThis decoding bottleneck limits NLMs’ application in many interactive services such as Web search,\nonline recommendation systems and conversational bots, where low latency, often at the scale of\nmilliseconds, is demanded. In addition, unlike model training where we can leverage massive\nparallelism power of GPUs, decoding needs to run in various clients ranging from PC, mobile, to IoT\n(Internet of Things), most of which have limited hardware resources and where GPUs are not always\navailable [34]. Therefore, fast decoding is crucial to broaden the applicability of NLMs.\n1It times 4 because an LSTM has 3 gates and 1 memory cell, and it times 2 because each weight value causes\na multiply–and–add operation.\n2\n3 Approach\nWe propose Fast Graph Decoder (FGD) to approximate the softmax layer in NLMs. FGD works in\ntwo steps, as illustrated in Figure 1. First, for any given NLM, we construct a small world graph\nto represent its vocabulary. Second, at each step in beam search decoding, we ﬁnd for each partial\nhypothesis in the beam (i.e., context) the top-K words that are most likely to occur, and store the\nexpended hypotheses and their probabilities in the beam.\nFGD is expected to be far more efﬁcient than the full softmax in that\n• when expanding a partial hypothesis we only need to explicitly compute the probabilities of\ntop-Kwords instead of every word in a vocabulary, and\n• there is an efﬁcient search method of identifying the top-Kwords on the small world graph\nwithout going over the entire vocabulary.\nIn what follows, we will present in turn\n• Why do we choose the small world graph representation in FGD? (Section 3.1)\n• How to construct a small world graph to represent the vocabulary of a NLM? (Section 3.2)\n• How to identify top-Khypotheses for a given context on small world graph? (Section 3.3)\nh\nx1 x2 xV…x3\nsoftmax\np1 p2 pV…p3\nTop K hypotheses\n \nIPPT\nFGD search path\nLong-range edge\nShort-range edge\nClosest neighbor\nOffline Inference\nO(D x log(|V|))\n(a) (b) (c)\nTop K hypotheses\nO(D x |V|)\n  \n  \n  \n  \n  \n  \n  \n  \nFigure 1: Overview of FGD: (a) illustrates one decoding step using a NLM with a vocabulary V,\nwhere given a context vector h∈RD top-Khypotheses are selected among all possible hypotheses\ngenerated by V using softmax. The complexity of the softmax layer is O(D×|V|). (b) shows the\ntransformation from the word embedding vectors of the NLM vocabulary, x1,x2,..,x |V |, to a small\nworld graph representation which encodes word-word distances in a well-deﬁned metric space. This\ntransformation, incurring once ofﬂine, is essential for FGD to perform fast decoding. (c) shows one\nstep decoding using FGD. For a given context vectorh, FGD identiﬁes top-Khypotheses by traversing\nthe small world graph and produces their probabilities. The complexity of the FGD-approximated\nsoftmax layer is O(D×log |V|).\n3.1 Why Small World Graphs?\nIn FGD, ﬁnding top-K words for a given context is implemented by ﬁnding K nearest neighbors\nin a vector space. The small world graph has been recently introduced to address the problem of\nnearest neighbor search [ 32, 33]. Research shows that navigation in small world graph exhibits\nO(log N) search complexity (N represents the number of nodes in the graph), and performs well in\nhigh dimensionality [32, 33, 35].\nTo get logarithmic nearest neighbor search complexity, the small world graph needs to hold the small\nworld properties which we will detail in the following sections, such as great local connectivity (as in\na lattice graph) combined with a small graph diameter (as in a random graph) [36], and a well-deﬁned\npair-wise metric distance among words.\n3\n3.2 Small World Graph Construction\nDenote G= (X,E) as a small world graph with the set X as graph nodes and Eas the graph edges.\nGiven a NLM vocabulary, which is a set of word embedding vectorsX = [x1,x2,...,x |V |],xi ∈RD,\nthe construction of its small world graph Gtakes two steps.\n1. Constructing the node set X: as illustrated in Figure 1(b), for each word embedding vector\nxi in X, we apply a transformation called Inner Product Preserving Transformation (IPPT)\nto obtain X = [x1,x2,..., x|V |],xi ∈RD+2 that establishes the equivalence of ﬁnding\ntop-Kmaximum subset of inner product in X and searching for top-Knearest neighbors in\nX with a given distance metric ρ. (Section 3.2.1)\n2. Constructing the edge set E: Given X, we impose the set Ebased on the distance metric ρ\nto form a small world graph. (Section 3.2.2)\n3.2.1 Constructing X via Inner Product Preserving Transformation\nUsing inner product over word embedding vectors to measure word-word distance is deﬁcient because\nit lacks very basic properties that need to hold for distance (i.e., the inverse of similarity) functions in\nmetric spaces (e.g., Euclidean spaces) – identity of indiscernibles and triangle inequality [37]. For\nexample, under the Euclidean space, two points are the same iff their distance is 0. The inner product\nof a point xto itself is ∥x∥2, but there can be other points whose inner product to xis smaller than\n∥x∥2. The search process on small world graphs relies on these properties to converge and achieve\ntheir efﬁciency [32].\nTo create a small world graph with a well-deﬁned metric distance between words, we present a new\nmethod called Inner Product Preserving Transformation (IPPT) to convert word embedding vectors to\nhigher dimensional vectors. We establish the equivalence of ﬁnding top-Kmaximum subset of inner\nproduct and searching for top-K nearest neighbor with a distance metric in the higher dimension\nspace. We use the notation ⟨·,·⟩for the inner product and ρ(·,·) for the distance in Euclidean space.\nThus ρ(ξ,η) = ∥ξ−η∥2 =\n√\n⟨ξ−η,ξ −η⟩.\nIn lemma 3.1, we show that it is possible to deﬁne particular transformation functions for word\nembedding vectors and the given context vector, respectively, so that the transformation is inner\nproduct preserving. We present the proof of lemma 3.1 in Appendix A.\nLemma 3.1. Let xi ∈ RD and bi ∈ R be the word embedding vector and bias at posi-\ntion i in the softmax layer, respectively, for 1 ≤ i ≤ |V|. Choose a constant U such that\nU ≥maxi∈V\n√\n∥xi∥2\n2 + b2\ni . Let [; ] represents vector concatenation. Deﬁne the transformation\nfunction for word embedding vectors φ : {(ξ,η) ∈RD ×R : ∥ξ∥2\n2 + η2 ≤U2}−→ RD+2 as\nφ(x,b) =\n[\nx; b;\n√\nU2 −∥x∥2\n2 −b2\n]\n. Deﬁne the transformation of the given context vector h∈RD\nas h= [h; 1; 0]∈RD+2. These transformations are inner product preserving in that for any i∈V,\nwe have ⟨h,xi⟩+ bi = ⟨h,φ(xi,bi)⟩= 1\n2\n(\nU2 + 1 +∥h∥2\n2 −ρ(h,φ(xi,bi))2)\n.\nThe above lemma indicates that the transformation is both \"order preserved\" and \"value preserved\".\nThe former means that given a context vector, its top-Kclosest words identiﬁed using inner product\nin the word embedding space are the same as identiﬁed according to the Euclidean distance in the\ntransformed space. The latter means that the ratio of the two distance scores between the context\nand a word computed in the two spaces respectively is a constant. Lemma 3.1 implies that given a\ncontext, we can ﬁnd the same top-Khypotheses in either space. In the context of decoding using a\nNLM, the lemma implies that given a context, ﬁnding its top-Kwords using the softmax layer in the\nNLM is equivalent to ﬁnding its Knearest neighbors in the small world graph constructed from the\nNLM vocabulary. This is formally stated in Theorem 3.2.\nDeﬁnition 1 (Top-Kmaximum (minimum) subset). Let V be the set of vocabulary, and1 ≤K ≤\n|V|. We call Ka top-K maximum (minimum) subset for a function f : V →R, if |K|= K and\nf(vi) ≥f(vj) (f(vi) ≤f(vj)) for all vi ∈K and vj ̸∈K.\nTheorem 3.2. Suppose 1 ≤K ≤|V|and consider a ﬁxed context vector h. Let K⊆ V be a top-K\nmaximum subset for vi ↦→⟨h,xi⟩+ bi. Then Kis also a top-Kminimum subset for the Euclidean\ndistance vi ↦→ρ(h,φ(xi,bi)).\n4\n3.2.2 Small World Graph Construction\nNow, we are ready to present the algorithm for small world graph construction. The algorithm, termed\nas FGD–P (P for Preprocessing), is presented in Algorithm 1.\nGiven a trained NLM with its vocabulary represented as word embedding vectorsX, FGD–P ﬁrst\nconstructs the node set X using IPPT as described above (in line 4-9 in Algorithm 1).\nAlgorithm 1 Ofﬂine preprocessing algorithm FGD–P\n1: Input: Trained weights of the softmax layer X, and bias vector b.\n2: Output: Small world graph G, and Umax.\n3: Hyperparameter: Small world graph neighbor degree M.\n4: for all iin (0..|X|−1) do\n5: ˜X:i ←[X:i; bi] ⊿Word embedding and bias fusion\n6: Umax ←maxi ∥˜X:i∥2\n7: for all iin 0..(|˜W|−1) do\n8: ∆i ←\n√\nUmax\n2 −∥ ˜X:i∥2\n2 ⊿Calculate the normalizer\n9: X:i ←[ ˜X:i; ∆i]\n10: G←CreateSwg(X,M ) ⊿Build small world graph\nThen, FGD–P forms the ﬁnal graph using G= CreateSwg(X,M ) by inserting edges among nodes\n(line 10). We need to ensure that Gis constructed in the way that all the small world properties\nare hold. We thus explore existing algorithms that have devoted to constructing a small world\ngraph. Among the most accomplished algorithms, HNSW (Hierarchical Navigable Small Worlds) has\nrecently attained outstanding speed-accuracy trade-offs [33], and thus employed by us in this study.\nWe brieﬂy describe the main ideas and refer readers to Malkov and Yashunin [33] for more details.\nThe small world graph is built incrementally by iteratively inserting each word vectorxi in X as a\nnode in G. Each node will generate M (i.e., the neighbor degree) out-going edges. Among those,\nM −1 are short–range edges, which connect xi to M −1 nearest neighbors according to their\npair-wise Euclidean distance to xi (e.g., the edge between x1 and x2 in Figure 1 (b)). The rest is a\nlong–range edge that connects xi to a randomly picked node, which does not necessarily connect two\nclosest nodes but may connect other isolated clusters (e.g., the edge between x3 and x|V | in Figure 1\n(b)). It is theoretically justiﬁed that constructing Gby inserting these two types of edges guarantees\nthe graph small world properties [32, 33, 36] of the resulting G.\nThe constructed small world graph using all xi becomes the ground layer L0 of G. CreateSwg then\ncreates a hierarchical small world graph by selecting a chain of subsets V = L0 ⊇L1 ⊇... ⊇Ll\nof nodes as \"layers\". This is similar to the HNSW [ 33]. Each node in Lk is randomly selected to\nLk+1 with a ﬁxed probability 1/M. On each layer, the edges are deﬁned so that the overall structure\nbecomes a small world graph, and the number of layers is bounded by O(log |V|/log M) [32].\n3.3 Decoding as Searching Small World Graphs\nFGD–I (I for Inference) (Algorithm 2) shows how FGD is used for fast decoding (as in Figure 1 (c)). It\nﬁrst transforms the context vector hto [h; 1; 0]∈Rd+2(line 4). Then, it calls SearchSwg(G,h,K )\nto search in the small world graph to identify the top-Khypotheses using the search method from\nHNSW [33], which will be brieﬂy described below.\nThe search of the graph starts from its top layer and uses a greedy search to ﬁnd the node with the\nclosest distance to has an entry point to descend to the lower layers. The upper layers route hto\nan entry point in the ground layer that is already close to the nearest neighbors to h. Once reaching\nthe ground layer, SearchSwvg employs prioritized breath-ﬁrst search: It exams its neighbors and\nstores all the visited nodes in a priority queue based on their distances to the context vector. The\nlength of the queue is bounded by efSearch, a hyperparameter that controls the trade-off between\nsearch time and accuracy. When the search reaches a termination condition (e.g., the number of\ndistance calculation), SearchSwg returns the results of top-Khypotheses and their distances to h.\nWe transform the distance value back to the original inner product space (line 5– 7), as described\n5\nin Section 3.2.1. FGD–I generates the output by computing a softmax distribution over the inner\nproduct of the top-Kreturned results (line 8).\nAlgorithm 2 Online inference algorithm FGD–I\n1: Input: Context vector h, small world graph G, and Umax.\n2: Output: Probability distribution P over top-Kword hypotheses.\n3: Hyperparameter: Candidate queue length efSearch.\n4: h←[h; 1; 0] ⊿Map context vector from RD to RD+2\n5: IK,DK ←SearchSwg(G,h,K) ⊿ Return top-Khypotheses with minimal distance to h\n6: for all iin 0..(K−1) do\n7: S[IK\n:i ] ←1\n2\n(\n∥h∥2\n2 + Umax\n2 −DK\n:i\n2)\n⊿ Map Euclidean distance back to inner product\n8: P ←exp(S)/∑exp(S) ⊿ Compute top-Ksoftmax probability distribution\nIn practice, setting K = |V|is both slow and unnecessary. An approximated approach with\nK <<|V|is often much more efﬁcient without sacriﬁcing much accuracy. We provide a theoretically\nderived error bound of approximating softmax via computing a probability distribution using only\ntop-Kdistance scores in Appendix B. In Section 4, we will demonstrate empirically the effectiveness\nof our approach.\n4 Evaluation\nSummary of main results. In this section, we present the results of FGD on two different tasks:\nneural machine translation (NMT) and language modeling (LM).\n1. On NMT, FGD obtains more than 14X speedup on softmax execution time than full-softmax\nwith close to baseline BLUE score, and obtains 30X speedup at the cost of losing 0.67\nBLEU score.\n2. On LM, FGD scales with a logarithmic increase of execution time and outperforms full-\nsoftmax by an order of magnitude with large vocabulary.\nSetup. We implement FGD in Python using numpy 2. To construct the small world graph, we\nemploy a state-of-the-art framework NMSLIB [33, 38]. The execution time is given as the averaged\nper-step decoding time in milliseconds, measured on a 64-bit Linux Ubuntu 16.04 server with two\nIntel Xeon CPU E5-2650 v4 @ 2.20GHz processor with single thread regime so that all algorithms\nare compared under the same amount of hardware resource.\n4.1 Neural Machine Translation\nNMT is a sequence–to–sequence model which contains an RNN encoder and an RNN decoder.\nThe decoder contains an output projection at every step to predict the next word. Decoding time\nand BLEU score [39] are the two major metrics for this evaluation. The lower the decoding time\nwithout sacriﬁcing much BLEU score, the better the result. We train a global attention-based [40]\nencoder–decoder model with a two-unidirectional-stacked LSTM [ 1, 2] using the OpenNMT-py\ntoolkit [41] on the IWSLT’14 German-English corpus [42]. We set the LSTM hidden dimension size\nto 200. The model is optimized with SGD using an initial learning rate of 1.0 and a dropout [ 43]\nratio of 0.3. The dataset is tokenized and preprocessed using the OpenNMT data preprocessor with\n|V|= 50,000 frequent words [23, 40]. BLEU score is computed with the Moses toolkit [44].\nOnce the model is trained, we process the trained weights in the softmax layer using FGD–P ofﬂine.\nIt takes three minutes on our server to construct the small world graph. During online processing,\nthe hyperparameter, efSearch, decides the length of the candidate queue to track nearest neighbors,\nwhich offers the trade-off between the online decoding speed and the BLEU score quality. We test\ndifferent efSearch values and identify [20, 200] as a good range.\nDecoding time and BLEU score comparison with existing methods. Two approaches are used\nfor comparison: 1) a full-softmax approach; 2) a state-of-the-art approach, called SVD-softmax [24].\n2http://www.numpy.org/\n6\nSVD-softmax improves the inference speed by approximating softmax layer using singular vector\ndecomposition (SVD). It includes two steps: it ﬁrst estimates the probability of each word using\na small part of the softmax weight matrix, and then performs a reﬁnement on top- V most likely\nwords based on the previous estimated results. It reduces the complexity from O(|V|× D) to\nO(|V|×D+ |V|×D), where 1 ≤D < D. As suggested by [ 24], we use two conﬁgurations of\nSVD-softmax: SVD-a3 and SVD-b4.\nFigure 2 shows the main results — FGD achieves signiﬁcantly lower execution time than the existing\nmethods with comparable BLEU scores.\nComparing with full softmax, when efSearch is 20, FGD reduces the execution time from 6.3ms\nto 0.21ms, achieving 30X speedup at the cost of losing 0.67 BLEU score. By increasing efSearch\nto 50, FGD obtains nearly the same BLEU score as the full-softmax baseline, while reducing the\nexecution time from 6.3ms to 0.43ms and achieving more than 14X speedup.\nFor SVD-softmax, We also observe that SVD-b approaches a BLEU score close to the full-softmax\nbaseline, but it is much slower than FGD in terms of the execution time (5.53ms vs 0.43ms). SVD-a\nshows slightly better performance than SVD-b but with a lower BLEU score. Although the theoretical\nspeedup of SVD-a is 5.5X, it gets only 1.3X speedup in practice because top-V most likely words\nselected in the ﬁrst step appear at discontinuous location on memory, which causes non-negligible\nmemory copy cost to bring them to a continuous space for the second step calculation.\nFigure 2: Execution time of the softmax layer and BLEU score of NMT model with FGD, SVD-\nsoftmax (SVD), and Full-softmax (Full). [20, 50, 100, 200] are the hyperparameter of efSearch\nin FGD. Execution time is displayed as the height of the bar chart, in millisecond (lower is better).\nBLEU scores are labeled with colored numbers on the top (higher is better).\nSensitivity of sequence lengths. Figure 3 reports the results with efSearch = 100. FGD is on\na par with the full softmax baseline uniformly on different lengths (without statistically signiﬁcant\ndifference). It demonstrates the robustness of the proposed approach.\nSensitivity of beam sizes. We vary the beam size among 1, 2, 5, 10, which are typical settings\nused by prior work [1–3, 45]. Table 1 shows that, when efSearch is equal or larger than 50, FGD\nobtains the BLEU scores close to the full softmax baseline under all beam sizes without statistical\nsigniﬁcance.\nFigure 3: BLEU score breakdown by sen-\ntence length (setting efSearch=100).\nefSearch Beam = 1 Beam = 2 Beam = 5 Beam = 10\n20 26.69 27.65 27.81 27.62\n50 27.55 28.76 29.06 28.9\n100 27.63 28.94 29.28 29.1\n200 27.53 28.99 29.28 29.22\nFull 27.36 28.91 29.45 29.34\nTable 1: BLEU score on NMT task, with various beam\nsizes.\nInternals of FGD. To reveal the internals of FGD, we analyze two metrics, precision@K (or\nequivalently P@K) and dist_cnt. Precision@K measures the proportion of overlap between retrieved\n3The preview window width D is set to 16, and the reﬁnement window width V is set to 2500.\n4The preview window width D is set to 16, and the reﬁnement window width V is set to 5000.\n7\ntop-Khypotheses and expected top-Khypotheses, based on what top-Kon a full-softmax would\nreturn. dist_cntmeasures the number of distance computation in FGD under a given efSearch.\nTable 2 reports precision@K when Kis 1, 2, 5, and 10, which correspond to beam size 1, 2, 5, and\n10 respectively, and dist_cntwith vs. without FGD. Overall, FGD achieves fairly high precision.\nIn particular, gradually increasing efSearch leads to higher precision at the expense of increased\nnumber of distance computation. This matches the observation that higher efSearch leads to higher\nBLEU score (Figure 2) and also longer execution time (Table 1). Further increasing efSearch\nleads to little extra precision improvement but signiﬁcantly more distance computation because\nthe precision is getting close to 1, which explains why FGD can get close to baseline BLEU score\n(Table 1). We also observe that under the same efSearch, further increasing Ksometimes leads to\nslightly worse precision if efSearch is not big enough (e.g., efSearch is 20), as the highest ranked\nwords not visited during the graph search are deﬁnitely lost. On the other hand, the computation\nof distance grows proportional to the increase of efSearch. Comparing with the full-softmax, the\namount of distance computation is reduced by 10–50 times, which explains the speedup of decoding\ntime (Figure 2).\nefSearch P@1 P@2 P@5 P@10 dist_cnt (FGD/ Full)\n20 0.939 0.934 0.929 0.918 981 / 50K\n50 0.974 0.974 0.973 0.971 1922 / 50K\n100 0.986 0.986 0.987 0.987 3310 / 50K\n200 0.992 0.993 0.994 0.994 5785 / 50K\nTable 2: Precision and distance computation results on the NMT model.\n4.2 Language Modeling\nThis section evaluates the impact of vocabulary size and word embedding dimension on FGD using\nlanguage modeling 5 on WikiText-2 [46]. The model uses a two–layer LSTM6.\nImpact of vocabulary size. We explore multiple models with different vocabulary size of 10,000\n(10K), 20,000 (20K), 40,000 (40K), and 80,000 (80K). The vocabulary is created by tokenizing raw\ntexts via Moses toolkit [44] and choosing the correspondingly topmost frequent words on the raw\nWikiText-2 dataset [46]. Both input and hidden dimension are set to 256.\nTable 3 shows the impact of search quality by varying vocabulary size from 10K to 80K. With the\nsame efSearch, FGD generally obtains better precision results for smaller vocabulary; With the\nsame vocabulary size, bigger efSearch is better for high precision. With efSearch being 200, FGD\nis getting very close to 1.\n|V| P@K FGD (efSearch)\n20 50 100 200\n10K P@1 0.870 0.938 0.989 1.000\nP@10 0.909 0.972 0.992 0.998\n20K P@1 0.845 0.932 0.975 0.995\nP@10 0.871 0.955 0.987 0.997\n40K P@1 0.808 0.912 0.936 0.980\nP@10 0.845 0.931 0.961 0.991\n80K P@1 0.832 0.933 0.966 0.982\nP@10 0.858 0.945 0.978 0.994\nTable 3: Precision of FGD on WikiText-\n2 dataset varying vocabulary size.\nFigure 4: Scalability of WikiText-2 language model\nvarying vocabulary size.\nFigure 4 shows the decoding time of varying vocabulary sizes on the full softmax baseline and\nFGD (settings efSearch={50, 200} for the sake of readability). As expected, the execution time all\n5https://github.com/pytorch/examples/tree/master/word_language_model\n6 The models are trained with stochastic gradient descent (SGD) with an initial learning rate of 20 [47]. The\nbatch size is set to 20, and the network is unrolled for 35 timesteps. Dropout is applied to LSTM layers with a\ndropout ratio of 0.3 [43]. Gradient clipping is set to 0.25 [48].\n8\nincreases with the increase in vocabulary size. However, compared to the baseline, FGD provides\na shorter execution time consistently. As the vocabulary size increases, the execution time of the\nbaseline increases almost linearly, whereas FGD’s execution time increases much more slowly. This\nis because the complexity of softmax is O(D×|V|), which is linear to the size of the vocabulary,\nwhereas the complexity of FGD is O(D×log |V|), which is logarithmic to |V|. Therefore, FGD\nscales much better and the improvement becomes more signiﬁcant with larger vocabulary sizes. In\nparticular, FGD is more than an order of magnitude faster than the baseline when the vocabulary\nsize is medium or large. For example, FGD achieves more than 30X speedup with |V|= 80Kwhen\nefSearch = 50 (Appendix 5 includes a speedup graph).\nSensitivity of word embedding dimension. We also test various word embedding dimensions.\nFGD gets high precision consistently with an order of magnitude execution time reduction (see\nAppendix C).\n5 Conclusion\nWe propose a novel softmax layer approximation algorithm, calledFast Graph Decoder (FGD), which\nquickly navigates, for a given context, on a small world graph representation of word embeddings to\nsearch for a set of Kwords that are most likely to be the next words according to NLMs. On neural\nmachine translation and neural language modeling, we demonstrate that FGD reduces the decoding\ntime by an order of magnitude (e.g., 14X speedup comparing with the full softmax baseline) while\nattaining similar accuracy on neural machine translation and language modeling tasks. As the further\nwork, we also like to explore how to speed up NLMs training with large vocabularies.\n9\nReferences\n[1] Kyunghyun Cho, Bart van Merriënboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder for\nStatistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP’ 14, pages 1724–1734, 2014.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning\nto Align and Translate. arXiv preprint arXiv:1409.0473, 2014.\n[3] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to Sequence Learning with Neural Networks.\nIn Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information\nProcessing Systems, NIPS ’14, pages 3104–3112, 2014.\n[4] Alexander M. Rush, Sumit Chopra, and Jason Weston. A Neural Attention Model for Abstractive Sentence\nSummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP ’15, pages 379–389, 2015.\n[5] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep Reinforcement\nLearning for Dialogue Generation. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP ’16, pages 1192–1202, 2016.\n[6] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech Recognition with Deep Recurrent\nNeural Networks. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\n’13, pages 6645–6649, 2013.\n[7] Awni Y . Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,\nSanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y . Ng. Deep Speech: Scaling up\nend-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.\n[8] Geoffrey Zweig, Chengzhu Yu, Jasha Droppo, and Andreas Stolcke. Advances in all-neural speech\nrecognition. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\n’17, pages 4805–4809, 2017.\n[9] Oriol Vinyals and Quoc V . Le. A Neural Conversational Model. arXiv preprint arXiv:1506.05869, 2015.\n[10] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-\nYun Nie, Jianfeng Gao, and Bill Dolan. A neural network approach to context-sensitive generation of\nconversational responses. In NAACL-HLT, May 2015.\n[11] Alexander M. Rush, Sumit Chopra, and Jason Weston. A Neural Attention Model for Abstractive Sentence\nSummarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP ’15, pages 379–389, 2015.\n[12] Katja Filippova, Enrique Alfonseca, Carlos A. Colmenares, Lukasz Kaiser, and Oriol Vinyals. Sentence\nCompression by Deletion with LSTMs. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, EMNLP ’15, pages 360–368, 2015.\n[13] Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau. Building\nEnd-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models. volume 16 of\nAAAI ’16, pages 3776–3784, 2016.\n[14] Sepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–\n1780, 1997.\n[15] Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. Empirical Evaluation of Gated\nRecurrent Neural Networks on Sequence Modeling. arXiv preprint arXiv:1412.3555, 2014.\n[16] Tomas Mikolov, Martin Karaﬁát, Lukás Burget, Jan Cernocký, and Sanjeev Khudanpur. Recurrent Neural\nNetwork Based Language Model. In 11th Annual Conference of the International Speech Communication\nAssociation, INTERSPEECH ’10, pages 1045–1048, 2010.\n[17] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A Neural Probabilistic Language\nModel. The Journal of Machine Learning Research, 3:1137–1155, 2003.\n[18] Rafal Józefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the Limits of\nLanguage Modeling. CoRR, arXiv preprint abs/1602.02410, 2016.\n[19] Frederic Morin and Yoshua Bengio. Hierarchical Probabilistic Neural Network Language Model. In\nProceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics, AISTATS ’05.\n[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed Representa-\ntions of Words and Phrases and their Compositionality. In Advances in Neural Information Processing\nSystems 26: 27th Annual Conference on Neural Information Processing Systems, NIPS ’13.\n[21] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. In Proceedings of the 29th International Conference on Machine Learning, ICML ’12.\n1\n[22] Wenlin Chen, David Grangier, and Michael Auli. Strategies for Training Large V ocabulary Neural\nLanguage Models. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics, ACL ’16, 2016.\n[23] Sébastien Jean, KyungHyun Cho, Roland Memisevic, and Yoshua Bengio. On Using Very Large Target\nV ocabulary for Neural Machine Translation. InProceedings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th International Joint Conference on Natural Language Processing\nof the Asian Federation of Natural Language Processing, ACL, pages 1–10, 2015.\n[24] Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, and Wonyong Sung. SVD-Softmax: Fast Softmax\nApproximation on Large V ocabulary Neural Networks. InAdvances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems 2017 , NIPS ’17, pages\n5469–5479, 2017.\n[25] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. Breaking the Softmax Bottleneck:\nA High-Rank RNN Language Model. CoRR, arXiv preprint abs/1711.03953, 2017.\n[26] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard M. Schwartz, and John Makhoul.\nFast and Robust Neural Network Joint Models for Statistical Machine Translation. In Proceedings of the\n52nd Annual Meeting of the Association for Computational Linguistics, ACL ’14.\n[27] Ramon Ferrer i Cancho and Richard V Solé. The small world of human language. Proceedings of the\nRoyal Society of London B: Biological Sciences, 268(1482):2261–2265, 2001.\n[28] Adilson E Motter, Alessandro PS De Moura, Ying-Cheng Lai, and Partha Dasgupta. Topology of the\nConceptual Network of Language. Physical Review E, 65(6):065102, 2002.\n[29] Sergey N Dorogovtsev and José Fernando F Mendes. Language as an Evolving Word Web. Proceedings of\nthe Royal Society of London B: Biological Sciences, 268(1485):2603–2606, 2001.\n[30] Jeffrey Travers and Stanley Milgram. The Small World Problem. Phychology Today, 1(1):61–67, 1967.\n[31] Mark EJ Newman. Models of the Small World. Journal of Statistical Physics, 101(3-4):819–841, 2000.\n[32] Yury Malkov, Alexander Ponomarenko, Andrey Logvinov, and Vladimir Krylov. Approximate nearest\nneighbor algorithm based on navigable small world graphs. Information Systems, pages 61–68, 2014.\n[33] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search using\nHierarchical Navigable Small World graphs. CoRR, arXiv preprint abs/1603.09320, 2016.\n[34] Mung Chiang and Tao Zhang. Fog and IoT: An Overview of Research Opportunities. IEEE Internet of\nThings Journal, 3(6):854–864, 2016.\n[35] Jon Kleinberg. The Small-world Phenomenon: An Algorithmic Perspective. In Proceedings of the 32\nAnnual ACM Symposium on Theory of Computing, STOC ’00, pages 163–170, 2000.\n[36] Duncan J. Watts. Small Worlds: The Dynamics of Networks Between Order and Randomness. 1999.\n[37] James R Munkres. Elements of Algebraic Topology. CRC Press, 2018.\n[38] Leonid Boytsov and Bilegsaikhan Naidan. Engineering Efﬁcient and Effective Non-metric Space Library.\nIn Similarity Search and Applications - 6th International Conference, SISAP ’13, pages 280–293, 2013.\n[39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A Method for Automatic\nEvaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for\nComputational Linguistics, ACL ’02, pages 311–318, 2002.\n[40] Thang Luong, Hieu Pham, and Christopher D. Manning. Effective Approaches to Attention-based Neural\nMachine Translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, EMNLP, pages 1412–1421, 2015.\n[41] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. OpenNMT: Open-\nSource Toolkit for Neural Machine Translation. arXiv preprint arXiv:1701.02810, 2017.\n[42] Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. Report on the\n11th IWSLT evaluation campaign, IWSLT 2014. In Proceedings of the International Workshop on Spoken\nLanguage Translation, 2014.\n[43] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:\nA Simple Way to Prevent Neural Networks from Overﬁtting. Journal of Machine Learning Research,\n15(1):1929–1958, 2014.\n[44] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-\nstantin, and Evan Herbst. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings\nof the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages\n177–180, 2007.\n2\n[45] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\n[46] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture Models.\nCoRR, arXiv preprint abs/1609.07843, 2016.\n[47] Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-\nSTAT’2010, pages 177–186. Springer, 2010.\n[48] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. In Proceedings of the 30th International Conference on Machine Learning , ICML, pages\n1310–1318, 2013.\n3\nA Proof of Lemma and Theorem\nLemma 3.1 proof. For the ﬁrst equation can be proved by expanding the inner prod-\nuct according to the deﬁnition of the transformation. Note that ⟨˜h,φ(xi,bi)⟩ =⣨\n[h; 1; 0],\n[\nxi; bi;\n√\nU2 −∥xi∥2 −b2\ni\n]⟩\n= ⟨h,xi⟩+ 1 ·bi + 0 ·\n√\nU2 −∥xi∥2 −b2\ni = ⟨h,xi⟩+ bi.\nThe second equation follows from the relation between the inner product and the distance in Eu-\nclidean spaces: ρ(˜h,φ(xi,bi)2 = ∥˜h−φ(xi,bi)∥2 = ∥˜h∥2 + ∥φ(xi,bi)∥2 −2⟨˜h,φ(xi,bi)⟩=\n∥h∥2 + 1 +U2 −2⟨˜h,φ(xi,bi)⟩.\nTheorem 3.2 proof. By Lemma 3.1, the distance ρ(˜h,φ(xi,bi)) depends strictly monotonically de-\ncreasingly on ⟨h,xi⟩+ bi. The claim of the theorem then follows straightforwardly upon this\nfact.\nB A Bound on Top- K Softmax Approximation\nIn this section we derive an error bound of top-Khighest ranked words approximation on softmax\ntoward a probability distribution. Let V be the set of vocabulary. Let si (1 ≤i ≤|V|) be the\nscores produced using exhaustive search, sorted in decreasing order. Suppose Lis a lower bound\nfor {si}, i.e., L ≤min si. Typically one chooses L to be a negative number with sufﬁciently\nlarge absolution value. If there is no known lower bound, one may also set L = −∞and agrees\nthat exp(−∞) = 0. The probability distribution generated by applying softmax on si is given by\npi = exp(si)/∑\ni exp(si). Using exhaustive search we are able to compute pi as exact. However\nwith approximated techniques, we are only able to obtain an approximation ˆpi of the distribution. In\nreal applications we typically only care about how ˆpi differs from pi for the top-Kwords. The error\nin such approximations comes from two sources: (1) the accuracy of the approximation to obtain the\napproximated top-K; and (2) the approximation of ∑\ni exp(si). In the following theorem we give a\nquantitative analysis of how large the relative error could be.\nTheorem B.1. Let V, si, L, pi be as above. Let K⊆{ 1,..., |V|}be the ground truth top- K\nindices. Suppose an approximation top- K softmax algorithm gives K′ ⊆{1,..., |V|}as the ap-\nproximated top-K indices. Let ˆsi be the approximated score the algorithm assigns to the i-th\nword in V, and let s′ = mini∈K′ ˆsi. Assume that (i) the algorithm assigns exact scores ˆsi = si\nto those i ∈K′; and (ii) it assigns a score ˆsi for i ̸∈K′ such that L ≤ˆsi ≤s′. The approxi-\nmated probability distribution is given by ˆpi := exp(ˆsi)/∑\ni exp(ˆsi). Let K′′ = {i : si ≥s′}.\nThen the relative error of probability distribution approximation is bounded by |ˆpi −pi|/pi ≤∑\ni∈K′′\\K′ (exp(si) −exp(L)) + (|V|−|K′′|)(exp(s′) −exp(L))\n∑\ni∈K′ exp(ˆsi) + (|V|−K) exp(L) for any i∈K∩K ′.\nProof. First note that pi = exp(si)∑\ni exp(si) and that ˆpi = exp(ˆsi)∑\ni exp(ˆsi). Since i ∈K∩K ′, we have\nsi = ˆsi. We then deduce that\n|ˆpi −pi|\npi\n= |∑\ni exp(si) −∑\ni exp(ˆsi)|∑\ni exp(ˆsi) . (1)\nWe then proceed to bound both the numerator and the denominator.\nTo ﬁnd an upper bound for the numerator, ﬁrst note that\n⏐⏐⏐⏐⏐\n∑\ni\nexp(si) −\n∑\ni\nexp(ˆsi)\n⏐⏐⏐⏐⏐≤\n⏐⏐⏐⏐⏐\n∑\ni∈K′′\nexp(si) −\n∑\ni∈K′′\nexp(ˆsi)\n⏐⏐⏐⏐⏐+\n⏐⏐⏐⏐⏐⏐\n∑\ni̸∈K′′\nexp(si) −\n∑\ni̸∈K′′\nexp(ˆsi)\n⏐⏐⏐⏐⏐⏐\n.\n(2)\nFor the ﬁrst summand, ﬁrst observe that K⊆K ′′ and K′ ⊆K′′. Therefore\n⏐⏐⏐⏐⏐\n∑\ni∈K′′\nexp(si) −\n∑\ni∈K′′\nexp(ˆsi)\n⏐⏐⏐⏐⏐=\n⏐⏐⏐⏐⏐⏐\n∑\ni∈K′\n(exp(si) −exp(ˆsi)) +\n∑\ni∈K′′\\K′\n(exp(si) −exp(ˆsi))\n⏐⏐⏐⏐⏐⏐\n.\n4\nThe condition (i) implies that the ﬁrst sum is zero. The second sum is always non-negative\nsince exp(si) ≥exp(s′) ≥exp(ˆsi) for i ∈K′′\\K′. Thus\n⏐⏐⏐∑\ni∈K′′\\K′ (exp(si) −exp(ˆsi))\n⏐⏐⏐ =∑\ni∈K′′\\K′ (exp(si) −exp(ˆsi)) ≤∑\ni∈K′′\\K′ (exp(si) −exp(L)). For the second summand in the\nright hand side of Equation (2), note that\n⏐⏐⏐∑\ni̸∈K′′ exp(si) −∑\ni̸∈K′′ exp(ˆsi)\n⏐⏐⏐≤∑\ni̸∈K′′ |exp(si) −\nexp(ˆsi)|. Since both si,ˆsi ∈ [L,s′] for all i ̸∈ K′′, we have ∑\ni̸∈K′′ |exp(si) −exp(ˆsi)| ≤∑\ni̸∈K′′ (exp(s′) −exp(L)) = ( |V|−|K ′′|) (exp(s′) −exp(L)). This shows the upper bound of\nthe numerator.\nFor the denominator in the right hand side of Equation (1), simply note that ∑\ni exp(ˆsi) =∑\ni∈K′ exp(ˆsi) + ∑\ni̸∈K′ exp(ˆsi) ≥∑\ni∈K′ exp(ˆsi) + (|V|− K) exp(L). This then concludes\nthe proof of the theorem.\nIt is worthy to point out that the numerator of the above error bound can be rewritten as∑\ni∈K′′\\K′ (exp(si) −exp(s′)) + (|V|− K)(exp(s′) −exp(L)). Intuitively, the theorem states\nthat the accuracy of the softmax probability approximation for the top-K words depends on three\nquantities: (i) ∑\ni∈K′′\\K′ (exp(si) −exp(s′)), which measures how many words are “missed” by the\napproximation of top-Kwords. (ii) exp(s′) −exp(L), which measures the distribution of the scores\nfound by the approximation. The smaller (i) and (ii) are (relative to ∑\ni∈K′ exp(ˆsi), the better the\napproximation is.\nWe also observe that when the precision at Kis 1 for the approximation algorithm, then the bound\ndepends only on the sum of exponential scores and the smallest top- K score retrieved by the\nalgorithm.\nCorollary B.1.1. Let the notations be the same as in the above theorem. Assume that the precision\nat Kof the approximation is 1. Further assume that all the scores si are distinct. Then the relative\nerror to the approximated softmax probability distribution is bounded above by |ˆpi −pi|/pi ≤\n(|V|−K)(exp(s′) −exp(L))∑\ni∈K′ exp(ˆsi) + (|V|−K) exp(L) for any i∈K∩K ′.\nProof. It sufﬁces to show thatK′′ = K′. Since the precision at Kis 1, we haveK= K′, which means\ns′ = mini∈K′ ˆsi = mini∈K si. Now assume i ∈K′′, then by deﬁnition of K′′, si ≥minj∈K sj.\nSince all scores are distinct, this shows that si is amongst the top- K, i.e., si ∈K = K′. Thus\nK′′ ⊆K′. The other direction of inclusion is trivial.\nC Additional Results\nSpeedup with different vocabulary size. Figure 5 shows the speedup for FGD (with different\nefSearch) over the execution time of full-softmax for vocabulary size 10K, 20K, 40K, and 80K.\nWhen efSearch = 20, FGD achieves more than 65X speedup over the baseline with a vocabulary\nsize 80K. Even with smaller vocabulary size, FGD still achieves roughly an order of magnitude\nspeedup. Overall, FGD achieves speedup over the baseline consistently and scales well with different\nefSearch values.\nSensitivity of word embedding dimension. Table 4 reports the precision with varying word vector\nembedding dimension 128, 256, and 512 on the WikiText-2 language modeling. The vocabulary size\nis set to the default 33,728. In most cases, efSearch being 50 or 100 is sufﬁcient to provide high\nprecision (e.g., > 0.95). Over 0.99 precision can be reached when efSearch is 200. This indicates\nthat FGD offers high precision with different word embedding dimensions.\nFigure 6 compares with the execution time of FGD and full-softmax, FGD achieves an order of\nmagnitude reduction of execution time.\n5\nFigure 5: Performance of FGD, normalized to full-softmax execution time. Higher is better.\nD P@K FGD (efSearch)\n20 50 100 200\n128 P@1 0.913 0.993 0.998 0.999\nP@10 0.819 0.934 0.976 0.992\n256 P@1 0.832 0.917 0.958 0.992\nP@10 0.866 0.944 0.976 0.995\n512 P@1 0.854 0.921 0.968 0.988\nP@10 0.884 0.950 0.979 0.995\nTable 4: Precision of FGD on WikiText-2 dataset varying word vector embedding dimension.\nFigure 6: Scalability of WikiText-2 language model varying word vector embedding dimension.\n6",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.931186854839325
    },
    {
      "name": "Computer science",
      "score": 0.751873254776001
    },
    {
      "name": "Decoding methods",
      "score": 0.6866169571876526
    },
    {
      "name": "Vocabulary",
      "score": 0.6802110075950623
    },
    {
      "name": "Language model",
      "score": 0.57705157995224
    },
    {
      "name": "Machine translation",
      "score": 0.5613723993301392
    },
    {
      "name": "Scalability",
      "score": 0.5566146969795227
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5246967077255249
    },
    {
      "name": "Natural language processing",
      "score": 0.4959302842617035
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47419673204421997
    },
    {
      "name": "Artificial neural network",
      "score": 0.4554426968097687
    },
    {
      "name": "Graph",
      "score": 0.4478875696659088
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.42435258626937866
    },
    {
      "name": "Speech recognition",
      "score": 0.34202128648757935
    },
    {
      "name": "Theoretical computer science",
      "score": 0.316204309463501
    },
    {
      "name": "Algorithm",
      "score": 0.2810412645339966
    },
    {
      "name": "Linguistics",
      "score": 0.09245431423187256
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": []
}