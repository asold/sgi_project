{
  "title": "Diagnostic performance of multimodal large language models in radiological quiz cases: the effects of prompt engineering and input conditions",
  "url": "https://openalex.org/W4409517691",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Han, Taewon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746830184",
      "name": "Jeong， Woo Kyoung",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Shin, Jaeseung",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W4378783233",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W4367595583",
    "https://openalex.org/W4382182493",
    "https://openalex.org/W4388093101",
    "https://openalex.org/W4403322813",
    "https://openalex.org/W4387296778",
    "https://openalex.org/W4386865118",
    "https://openalex.org/W4362522726",
    "https://openalex.org/W4394838401",
    "https://openalex.org/W4403681525",
    "https://openalex.org/W4405239765",
    "https://openalex.org/W4404732108",
    "https://openalex.org/W4402891150",
    "https://openalex.org/W4400456285",
    "https://openalex.org/W4402891242",
    "https://openalex.org/W4403032308",
    "https://openalex.org/W4396808600",
    "https://openalex.org/W4392502104",
    "https://openalex.org/W4402190386",
    "https://openalex.org/W4404330874",
    "https://openalex.org/W4402193259",
    "https://openalex.org/W4402166980",
    "https://openalex.org/W4393027756",
    "https://openalex.org/W4406667919"
  ],
  "abstract": "Purpose: This study aimed to evaluate the diagnostic accuracy of three multimodal large language models (LLMs) in radiological image interpretation and to assess the impact of prompt engineering strategies and input conditions.Methods: This study analyzed 67 radiological quiz cases from the Korean Society of Ultrasound in Medicine. Three multimodal LLMs (Claude 3.5 Sonnet, GPT-4o, and Gemini-1.5-Pro-002) were evaluated using six types of prompts (basic [without system prompt], original [specific instructions], chain-of-thought, reflection, multiagent, and artificial intelligence [AI]–generated). Performance was assessed across various factors, including tumor versus non-tumor status, case rarity, difficulty, and knowledge cutoff dates. A subgroup analysis compared diagnostic accuracy between imaging-only inputs and combined imaging-descriptive text inputs.Results: With imaging-only inputs, Claude 3.5 Sonnet achieved the highest overall accuracy (46.3%, 186/402), followed by GPT-4o (43.5%, 175/402) and Gemini-1.5-Pro-002 (39.8%, 160/402). AI-generated prompts yielded superior combined accuracy across all three models, with improvements over the basic (5.5%, P=0.035), chain-of-thought (4.0%, P=0.169), and multiagent prompts (3.5%, P=0.248). The integration of descriptive text significantly enhanced diagnostic accuracy for Claude 3.5 Sonnet (46.3% to 66.2%, P&lt;0.001), GPT-4o (43.5% to 57.5%, P&lt;0.001), and Gemini-1.5-Pro-002 (39.8% to 60.4%, P&lt;0.001). Model performance was significantly influenced by case rarity for GPT-4o (rare: 6.7% vs. non-rare: 53.9%, P=0.001) and by knowledge cutoff dates for Claude 3.5 Sonnet (post-cutoff: 23.5% vs. pre-cutoff: 64.0%, P=0.005).Conclusion: Claude 3.5 Sonnet achieved the highest diagnostic accuracy in radiological quiz cases, followed by GPT-4o and Gemini-1.5-Pro-002. The use of AI-generated prompts and the integration of descriptive text inputs enhanced model performance.",
  "full_text": "220  Ultrasonography 44(3), May 2025 e-ultrasonography.org\nDiagnostic performance of multimodal \nlarge language models in radiological quiz \ncases: the effects of prompt engineering \nand input conditions\nTaewon Han, Woo Kyoung Jeong, Jaeseung Shin\nDepartment of Radiology and Center for Imaging Sciences, Samsung Medical Center, \nSungkyunkwan University School of Medicine, Seoul, Korea\nhttps://doi.org/10.14366/usg.25012\neISSN: 2288-5943\nUltrasonography 2025;44:220-231\nPurpose:  This study aimed to evaluate the diagnostic accuracy of three multimodal large \nlanguage models (LLMs) in radiological image interpretation and to assess the impact of prompt \nengineering strategies and input conditions.\nMethods: This study analyzed 67 radiological quiz cases from the Korean Society of Ultrasound \nin Medicine. Three multimodal LLMs (Claude 3.5 Sonnet, GPT-4o, and Gemini-1.5-Pro-002) \nwere evaluated using six types of prompts (basic [without system prompt], original [specific \ninstructions], chain-of-thought, reflection, multiagent, and artificial intelligence [AI]–generated). \nPerformance was assessed across various factors, including tumor versus non-tumor status, case \nrarity, difficulty, and knowledge cutoff dates. A subgroup analysis compared diagnostic accuracy \nbetween imaging-only inputs and combined imaging-descriptive text inputs.\nResults: With imaging-only inputs, Claude 3.5 Sonnet achieved the highest overall accuracy \n(46.3%, 186/402), followed by GPT-4o (43.5%, 175/402) and Gemini-1.5-Pro-002 (39.8%, \n160/402). AI-generated prompts yielded superior combined accuracy across all three models, \nwith improvements over the basic (5.5%, P=0.035), chain-of-thought (4.0%, P=0.169), and \nmultiagent prompts (3.5%, P=0.248). The integration of descriptive text significantly enhanced \ndiagnostic accuracy for Claude 3.5 Sonnet (46.3% to 66.2%, P<0.001), GPT-4o (43.5% to \n57.5%, P<0.001), and Gemini-1.5-Pro-002 (39.8% to 60.4%, P<0.001). Model performance \nwas significantly influenced by case rarity for GPT-4o (rare: 6.7% vs. non-rare: 53.9%, P=0.001) \nand by knowledge cutoff dates for Claude 3.5 Sonnet (post-cutoff: 23.5% vs. pre-cutoff: 64.0%, \nP=0.005).\nConclusion: Claude 3.5 Sonnet achieved the highest diagnostic accuracy in radiological quiz \ncases, followed by GPT-4o and Gemini-1.5-Pro-002. The use of AI-generated prompts and the \nintegration of descriptive text inputs enhanced model performance.\nKeywords: Artificial intelligence; Natural language processing; Diagnostic imaging; \nMedical informatics\nKey points: Artificial intelligence–generated prompts and the inclusion of descriptive text inputs \nsignificantly improved the diagnostic accuracy of three multimodal large language models in \nradiologic interpretation. Model performance was also influenced by prompt strategies, case \nrarity and difficulty, as well as the models’ knowledge cutoff dates.\nReceived: January 20, 2025\nRevised: February 17, 2025\nAccepted: March 11, 2025\nCorrespondence to:\nWoo Kyoung Jeong, MD, Department \nof Radiology and Center for Imaging \nScience, Samsung Medical Center, \nSungkyunkwan University School of \nMedicine, 81 Irwon-ro, Gangnam-gu, \nSeoul 06351, Korea\nTel. +82-2-3410-0518 \nFax. +82-2-3410-0049\nE-mail: jeongwk@gmail.com, \nwookyoung.jeong@samsung.com\nORIGINAL ARTICLE\nThis is an Open Access article distributed under the \nterms of the Creative Commons Attribution Non-\nCommercial License (http://creativecommons.org/\nlicenses/by-nc/4.0/) which permits unrestricted non-\ncommercial use, distribution, and reproduction in \nany medium, provided the original work is properly \ncited.\nCopyright © 2025 Korean Society of \nUltrasound in Medicine (KSUM)\nHow to cite this article: \nHan H, Jeong WK, Shin J. Diagnostic perfor-\nmance of multimodal large language models \nin radiological quiz cases: the effects of \nprompt engineering and input conditions. \nUltrasonography. 2025 May;44(3):220-231.\n\nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 221\nIntroduction\nThe emergence of large language models (LLMs) has marked a \nsignificant advancement in artificial intelligence and generated \nconsiderable interest due to their potential to transform medical \npractice [1,2]. LLMs, such as generative pretrained transformers (GPT) \nlike ChatGPT (OpenAI), exhibit exceptional capabilities in natural \nlanguage processing tasks, including clinical question answering, \ntext summarization, and contextual analysis [3 -9]. These models \nare trained on comprehensive datasets that incorporate scientific \nliterature, medical publications, and diverse digital resources across \nmultiple disciplines and languages [2,10,11].\nRecent developments have introduced multimodal LLMs that \nextend beyond text analysis to include the interpretation of audio, \nvisual, and video data [12,13]. These advanced systems have \nshown promising results in various medical applications, including \ndiagnostic assessment, clinical documentation, and disease \nidentification, achieving these outcomes without additional medical-\nspecific training [12-16].\nIn radiology, LLMs have been successfully applied to data mining \nand structured reporting [17 -19]. Recent technological progress \nhas led to enhanced multimodal LLMs, including OpenAI's GPT, \nAnthropic's Claude, and Google's Gemini [20]. These updated \nversions demonstrate improved diagnostic accuracy compared to \ntheir previous iterations, underscoring the importance of continuous \nmodel development [21,22]. However, as new systems continue to \nemerge, a systematic evaluation of their performance and clinical \nutility remains essential to ensure proper implementation and to \nminimize potential risks of misuse. Notably, a previous study using \nNew England Journal of Medicine Image Challenge cases suggested \nthat LLMs could provide correct answers even without image inputs, \nand that their performance was influenced more by text input length \nthan by image interpretation [21].\nTherefore, a study was designed to examine the radiological \nimaging interpretation capabilities of three widely used multimodal \nLLMs using cases with multiple imaging inputs while minimizing \nclinical text information. The study utilized image-based diagnostic \nchallenges with multiple-choice questions from the publicly available \neducational repository of the Korean Society of Ultrasound in \nMedicine (KSUM), which provides bi-monthly content to subscribers. \nAdditionally, this study examined various factors affecting model \nperformance, including the effects of prompt engineering, question \ntypes, case rarity, difficulty levels, and the knowledge cutoff dates of \nthe LLMs. \nMaterials and Methods\nCompliance with Ethical Standards \nThis study utilized publicly accessible educational datasets, for which \ninstitutional review board approval and informed consent were \nnot required. The study was conducted in accordance with the MI-\nCLEAR-LLM guidelines for reporting research involving LLMs in \nmedical imaging [23]. \nData Collection\nA total of 303 case discussion quizzes from the KSUM digital \nplatform (https://www.ultrasound.or.kr/) between July 28, 2000 \nand November 25, 2024 were initially considered. To ensure a \nstandardized assessment of diagnostic accuracy, 236 cases that \ndid not utilize a multiple-choice format were excluded, leaving 67 \nquiz cases for final inclusion in this study (Fig. 1). A radiologist (T.H. \nwith 3 years of experience in radiology) systematically extracted \ndata, including imaging data, question content with multiple-choice \noptions, imaging information, and reference answer descriptions. \nTo focus on image analysis, clinical information for all cases was \nstandardized to include only patient demographics (age and sex) \nand the chief complaint (e.g., \"52-year-old female patient with \nleft breast swelling\"). In 42 cases that lacked image descriptions \nin the reference data, a radiologist (W.K.J. with 26 years of \nexperience in radiology) composed case image descriptions while \nremaining blinded to the reference answers. Cases were classified \nby subspecialty, diagnostic category, and rarity based on the KSUM \ndigital platform case discussion interface. Human performance \nmetrics were established using KSUM subscriber response statistics, \nwith difficulty levels stratified into quartiles based on correct \nresponse rates.\nMultimodal LLM Analysis\nThree multimodal LLMs were evaluated: (1) GPT-4o (Alias: 2024-11-\n20) (knowledge cutoff: October 2023; OpenAI, San Francisco, CA, \nUSA), (2) Claude 3.5 Sonnet (Alias: 2024-06-20) (knowledge cutoff: \nApril 2024; Anthropic, San Francisco, CA, USA), and (3) Gemini-\n1.5-Pro-002 (Alias: 2024-09-24) (knowledge cutoff: September \n2024; Google, Mountain View, CA, USA). Application programming \ninterfaces were used to access each model between December \n1 and 24, 2024. Generation parameters were standardized with \na temperature setting of 1.0, which previously demonstrated the \nhighest accuracy [24]. Independent sessions were conducted for \neach case to avoid sequential bias. Performance evaluation included \ncomparisons based on pre- and post-knowledge cutoff dates and \nassessments of accuracy across various factors (tumor versus non-\ntumor, rare versus non-rare cases, and difficulty levels). Accuracy \nTaewon Han, et al.\n222  Ultrasonography 44(3), May 2025 e-ultrasonography.org\nwas measured using responses from the first attempt, with JSON-\nformatted textual outputs obtained for analysis. To evaluate \nrepeatability, the answering process was repeated across five \ndistinct sessions.\nPrompt Engineering Protocol\nThe experimental protocol incorporated user prompts that consisted \nof structured question text (including the primary question and \nfive multiple-choice options), imaging details (modality, plane, and \nacquisition parameters), and radiological images extracted from \nthe KSUM case discussion database, without any supplementary \ninstructions. To assess the influence of prompt engineering on \ndiagnostic performance across the three multimodal LLMs, six \ndistinct zero-shot system prompts were implemented based on \nprevious studies [25,26]: (1) Basic prompt: The control condition \nwithout a system prompt. (2) Original prompt: Contained \nspecific instructions for radiological interpretation and diagnostic \nassessment. (3) Chain-of-thought prompt: Included the instruction, \n\"…Must use a chain-of-thought approach: clearly outline your \nreasoning step by step…\". (4) Reflection prompt: Contained \nthe directive, \"…Self-Reflection Process: To ensure accuracy and \ncomprehensiveness, engage in a self-reflection process after \ngenerating the initial answer…\". (5) Multiagent prompt: Employed \na multiagent workflow with instructions such as, \"…MULTIPLE \nAGENT WORKFLOW ROLE: …Role 1: Clinical Context Analysis…\nRole 2: Radiologic Image Analysis…Role 3: Reflection and Chain-of-\nThought Final Answer…\". (6) Artificial intelligence (AI)–generated \nprompt: Utilized Claude’s prompt generation tool to create \noptimized prompt templates for specialized diagnostic tasks (https://\nconsole.anthropic.com/dashboard). Comprehensive details for all six \nsystem prompts are provided in Supplementary Table 1.\nSubgroup Analysis of Image-Only vs. Combined Imaging–\nDescriptive Text Input\nTo evaluate the impact of supplementary descriptive text input, \ncases were analyzed under two distinct conditions: (1) Imaging-\nOnly protocol, which included radiological images with text input \ncontaining the question elements and imaging information, and (2) \nCombined protocol, which incorporated radiological images with \ntext input containing imaging information, question elements, and \ncomprehensive radiologic image descriptions drawn from both the \nKSUM case discussion quiz reference answer section (25 cases) and \nradiologist-written descriptions (42 cases) (Fig. 2). The assessment \nemployed all six prompt engineering strategies (basic, original, \nchain-of-thought, reflection, multiagent, and AI-generated prompts). \nModel responses were obtained in standardized JSON format for \neach case. \nStatistical Analysis\nStatistical comparisons of diagnostic accuracy among the three LLMs \nacross six system prompts were performed using the Cochran('s) \nQ test. For significant findings (P<0.1) of the Cochran('s) Q test \nFig. 1.  Workflow of the selection and \nanalysis of study cases. Flowchart shows \nthe selection process of quiz cases from the \nKorean Society of Ultrasound in Medicine \ndatabase and subsequent analysis \nmethodology. AI, artificial intelligence.\nCase discussion quiz from the Korean Society of Ultrasound in \nMedicine between July 28, 2000 and November 25, 2024 (n=303)\nA total of 67 quiz were included\nDiagnostic Accuracy\nA total of 3 multimodal large language models\n█ GPT-4o (2024-11-20)\n█ Gemini-1.5-Pro-002 (2024-09-24)\n█ Claude 3.5 Sonnet (2024-06-20)\nInput (user prompt)\n█ Question text (question and multiple choices)\n█ Image text (information)\n█ Image text (description)\n█ Extracted image from quiz\n6 System prompt\n█ Basic\n█ Original\n█ Chain of thought\n█ Reflection\n█ Multiagent\n█ Al generate\nExclusion\nQuiz without multiple choices (n=236)\nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 223\nwas conducted using the original and AI-generated prompts that \ndemonstrated the highest performance. Results were expressed as \nodds ratios and 95% confidence intervals. Statistical significance \nwas established at P<0.05, except for the Cochran('s) Q test. \nRepeatability was evaluated with the Fleiss κ statistic. All analyses \nwere performed using SPSS statistical software (version 27.0 for \nWindows, IBM Corp., Armonk, NY , USA) and MedCalc version 22.02 \n(MedCalc Software, Ostend, Belgium).\n[27], subsequent post hoc analyses were performed using the \nMcNemar test. For multiple comparisons, P-values were adjusted \nusing the Bonferroni correction. The association between LLM \ndiagnostic performance and categorical variables (tumor vs. non-\ntumor, rare vs. non-rare cases, difficulty levels, and knowledge cutoff \ndate) was evaluated using the chi-square test or the Fisher exact \ntest. To identify determinants of diagnostic accuracy across the \nthree multimodal LLMs, multivariable logistic regression analysis \nFig. 2. Representative example of multimodal input format. The standardized input format combining radiological images with structured \ntext components (question, imaging information, and descriptive informations) used for model evaluation is demonstrated.\nText input (Image information) Text input (Image description)\nUser prompt\nRadiological images have been uploaded along with this message.\nImage: Image 1.jpg\nInformation: Pelvic Radiography\nImage: Image 2.jpg\nInformation: Longitudinal view of spinal US\nRadiological images have been uploaded along with this message.\nImage: Image 1.jpg\nInformation: Pelvic Radiography\nDescription: Pelvic Radiography show hypoplastic L5 and absent sacrum.\nImage: Image 2.jpg\nInformation: Longitudinal view of spinal US\nDescription: Spinal US show high termination of conus medullaris at T12 \nlevel with blunted shape.\n<image>\n[Radiological images have been attached]\n</image>\n<clinical_context>\n3-days-old male patient with multiple anomalies.\n</clinical_context>\n<multiple choice options>\nA. Filar cyst\nB. Fibrolipomas of the filum terminale\nC. Ventriculus terminalis\nD. Caudal regression syndrome\nE. Dermal sinus tract\n</multiple choice options>\nSelect the diagnosis with the highest probability based on the radiological findings and clinical context.\nSHOULD Provide your final answer in the following JSON format:\n{\n\"answer\": \"A\" or \"B\" or \"C\" or \"D\" or \"E\"\n}\nText input (Question)\nImage 1.jpg Image 2.jpg\nImaging input\nTaewon Han, et al.\n224  Ultrasonography 44(3), May 2025 e-ultrasonography.org\nResults\nCharacteristics of KSUM Case Discussion Dataset\nAfter applying exclusion criteria, 67 quiz cases with radiological \nimages were selected from an initial pool of 303 cases, with 236 \ncases excluded due to the absence of a multiple-choice format (Fig. \n1). Each case included multiple imaging inputs (mean±standard \ndeviation, 3.6±1.1; median [range], 4 [1 -5] images per case) \nacross various modalities. The cases spanned diverse radiological \nsubspecialties: breast (n=9), cardiovascular (n=6), gastrointestinal \n(n=9), genitourinary (n=9), hepatobiliary (n=8), head and neck (n=6), \nmusculoskeletal (n=9), pediatric (n=9), and thyroid (n=2). The mean \naccuracy rate from KSUM subscriber responses was 55.2%, with a \nmedian of 57.0% (range, 6.0% to 88.0%). The distribution of correct \nanswers showed a predominance of option D (n=22), followed by \noption B (n=13), option C (n=11), option D (n=11), and option E \n(n=10). Among the study cohort, 37 cases (55.2%) were classified \nas tumor-related, and 15 cases (22.4%) were categorized as rare \nconditions. Detailed case characteristics are provided in Table 1.\nDiagnostic Performance of Multimodal LLMs Using Various \nSystem Prompts\nFig. 2 illustrates a representative user prompt that includes both \ntextual and imaging inputs for the three multimodal LLMs. The \ntext component comprised imaging information, clinical queries, \nmultiple-choice options, and radiologic image descriptions used for \nthe subgroup analysis.\nDiagnostic accuracy varied among the three multimodal LLMs \nunder different prompt conditions. GPT-4o achieved 43.3% accuracy \nwith both the original and chain-of-thought prompts, while the AI-\ngenerated prompt yielded the highest accuracy at 46.3%, although \nthe differences were not statistically significant (P=0.765). Gemini-\n1.5-Pro-002 demonstrated optimal performance with the original \nand reflection prompts, but this did not reach statistical significance \n(P=0.635). Claude 3.5 Sonnet showed the lowest accuracy (41.8%) \nwith the reflection prompt and the highest accuracy (53.7%) with \nthe AI-generated prompt, representing a significant difference \nof 11.9% (P=0.039). The AI-generated prompt achieved the \nhighest overall accuracy (46.3%) in the combined accuracy of all \nthree models, showing improvements compared to basic 40.8% \n(difference: 5.5%, P=0.035), chain-of-though 42.3% (difference: \n4.0%, P=0.169), and multi-agent prompts 42.8% (difference: \n3.5%, P=0.248). None of the three LLMs exceeded the human \naccuracy benchmark of 55.2% (Fig. 3). Repeatability across all LLMs \nwas confirmed by Fleiss’ kappa values (0.79 for GPT-4o, 0.82 for \nGemini-1.5-Pro-002, and 0.82 for Claude 3.5 Sonnet), as shown in \nSupplementary Table 2.\nComparative analysis across system prompts revealed significant \ninter-model differences in diagnostic accuracy when using AI-\ngenerated prompts (P=0.096). Post hoc analysis demonstrated that \nTable 1. Characteristics of the case discussion quiz from the \nKorean Society of Ultrasound in Medicine \nCharacteristic Value\nCorrection rate (%) 57.0 (6.0-88.0)\n<25 5 (7.5)\n25-49 23 (34.3)\n50-75 24 (35.8)\n>75 15 (22.4)\nImage number per case 3.6±1.1\n1-3 29 (43.3)\n>3 38 (56.7)\nModality \nUS 67\nRadiography 6\nCT 11\nMRI 20\nNuclear imaging 5\nOthers\na)\n4\nAnswer distribution\nA 11 (16.4)\nB 13 (19.4)\nC 11 (16.4)\nD 22 (32.8)\nE 10 (14.9)\nSubspecialty\nBreast 9 (13.4)\nCardiovascular 6 (9.0)\nGastrointestinal 9 (13.4)\nGenitourinary 9 (13.4)\nHepatobiliary 8 (11.9)\nHead and neck 6 (9.0)\nMusculoskeletal 9 (13.4)\nPediatrics 9 (13.4)\nThyroid 2 (3.0)\nClassification\nTumor 37 (55.2)\nNon-tumor 30 (44.8)\nRarity\nRare case 15 (22.4)\nNon-rare case 52 (77.6)\nValues are presented as median (range), number (%), or mean±standard deviation. \na)\nOthers include imaging for aspiration fluid (n=2) and endoscopy (n=2). \nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 225\nClaude 3.5 Sonnet outperformed Gemini-1.5-Pro (53.7%, 36/67 vs. \n38.8%, 26/67; P=0.041), although no significant differences were \nobserved in other multiple comparisons. In the combined analysis \nof all six prompts, Claude 3.5 Sonnet achieved significantly higher \naccuracy (46.3%, 186/402) compared to Gemini-1.5-Pro-002 \n(39.8%, 160/402; P=0.014) (Table 2).\nPerformance Analysis under Different Input Factors\nTable 3 presents the diagnostic accuracy of all three LLMs using the \noriginal and AI-generated prompts, stratified by tumor versus non-\ntumor status, human accuracy rate, case rarity, and knowledge cutoff \ndates.\nIn the classification analysis (tumor vs. non-tumor), no significant \nperformance differences were observed across the LLMs with either \nprompt type. All models demonstrated enhanced accuracy in cases \nwith higher human accuracy rates. Regarding case rarity, GPT-4o \nwith the original prompt showed significantly higher accuracy in \nnon-rare cases (53.9%, 28/52) compared to rare cases (6.7%, 1/15; \nFig. 3. Diagnostic accuracy across different prompt engineering strategies. These graphs show diagnostic accuracy of three multimodal \nlarge language models (A, GPT-4o; B, Gemini-1.5-Pro; C, Claude 3.5 Sonnet; D, total) using six distinct prompt engineering approaches. The \nhorizontal line indicates the human performance benchmark (55.2%). AI, artificial intelligence. *P<0.05.\nBasic Original Chain of thought\nReflection Multiagent AI generate\nImaging-Only input\nGPT-4o (2024-11-20)\n40.3% 43.3% 43.3% 44.8% 43.3% 46.3%\nHuman\n=55.2%\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nA\nGemini-1.5-Pro\n40.3% 41.8%\n37.3%\n41.8% 38.8% 38.8%\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nB\nClaude 3.5 Sonnet (2024-06-20)\n41.8%\n47.8% 46.3%\n41.8%\n46.3%\n53.7%\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nC\nTotal multimodal LLM\n40.8% 44.3% 42.3% 42.8% 42.8%\n46.3%\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nD\n\nTaewon Han, et al.\n226  Ultrasonography 44(3), May 2025 e-ultrasonography.org\nP=0.001). This disparity was mitigated using AI-generated prompts \n(non-rare: 40.0%, 6/15; P=0.796). Claude 3.5 Sonnet maintained \nsignificantly higher accuracy in non-rare cases (61.5%, 32/52) \nversus rare cases (26.7%, 4/15) when using AI-generated prompts \n(P=0.021). \nRegarding knowledge cutoff dates, Claude 3.5 Sonnet \ndemonstrated significantly higher accuracy for pre-cutoff cases \n(original prompt: 56.0%, 28/50; AI-generated prompt: 64.0%, \n32/50) compared to post-cutoff cases (23.5%, 4/17 for both \nprompts; P=0.026 and P=0.005, respectively). GPT-4o and \nGemini-1.5-Pro-002 exhibited no significant temporal variations in \nperformance. \nMultivariable logistic regression analysis (Supplementary Table \n3) identified the human accuracy rate as a consistent predictor of \ndiagnostic performance across all LLMs. With original prompts, the \nodds ratios were significantly associated with performance: GPT-4o \n(odds ratio [OR], 5.8 [95% confidence interval (CI), 1.5 to 22.4]), \nGemini-1.5-Pro-002 (OR, 6.4 [95% CI, 1.8 to 22.6]), and Claude \nTable 3. Diagnostic accuracy of multimodal LLMs under different input factors \nModel Classification P-value Correction rate P-value Rarity P-value Knowledge cutoff date P-valueTumor Non-tumor ≤50% >50% Rare Non-rare Before After\nOriginal prompt\nGPT-4o 40.5 \n(15/37)\n46.7\n(14/30)\n0.798 21.4 \n(6/28)\n59.0 \n(23/39)\n0.005* 6.7\n(1/15)\n53.9 \n(28/52)\n0.001* 48.7 \n(19/39)\n35.7\n(10/28)\n0.418\nGemini-1.5-Pro 43.2 \n(16/37)\n40.0\n(12/30)\n0.985 21.4 \n(6/28)\n56.4 \n(22/39)\n0.009* 33.3 \n(5/15)\n44.2 \n(23/52)\n0.558 43.3 \n(26/60)\n28.6\n(2/7)\n0.690\nClaude 3.5 Sonnet 51.4 \n(19/37)\n43.3\n(13/30)\n0.684 28.6 \n(8/28)\n61.5 \n(24/39)\n0.016* 26.7 \n(4/15)\n53.9 \n(28/52)\n0.082 56.0 \n(28/50)\n23.5\n(4/17)\n0.026*\nCochran('s) Q test\na)\n0.465 0.794 0.670 0.861 0.039 0.382 - -\nAI-generated prompt\nGPT-4o 37.8 \n(14/37)\n56.7\n(17/30)\n0.197 25.0 \n(7/28)\n61.5 \n(24/39)\n0.007* 40.0 \n(6/15)\n48.1 \n(25/52)\n0.796 48.7 \n(19/39)\n42.9\n(12/28)\n0.821\nGemini-1.5-Pro 35.1 \n(13/37)\n43.3\n(13/30)\n0.665 21.4 \n(6/28)\n51.3 \n(20/39)\n0.027* 26.7 \n(4/15)\n42.3 \n(22/52)\n0.372 40.0 \n(24/60)\n28.6\n(2/7)\n0.697\nClaude 3.5 Sonnet 59.5 \n(22/37)\n46.7\n(14/30)\n0.425 35.7 \n(10/28)\n66.7 \n(26/39)\n0.024* 26.7 \n(4/15)\n61.5 \n(32/52)\n0.021* 64.0 \n(32/50)\n23.5\n(4/17)\n0.005*\nCochran('s) Q test\na)\n0.014 0.420 0.368 0.229 0.368 0.060 - -\nValues are presented as percentage (number/total). \nLLM, large language model; AI, artificial intelligence.\na)\nCochran('s) Q test significance level set at P<0.1, *P<0.05.\nTable 2. Comparison of diagnostic accuracy among multimodal LLMs across different system prompt types\nSystem prompt type GPT-4o Gemini-1.5-Pro Claude 3.5 Sonnet Cochrane's Q test\na)\nPost-hoc analysis\nb)\nBasic prompt 40.3 (27/67) 40.3 (27/67) 41.8 (28/67) 0.961 -\nOriginal prompt 43.3 (29/67) 41.8 (28/67) 47.8 (32/67) 0.648 -\nChain-of-thought prompt 43.3 (29/67) 37.3 (25/67) 46.3 (31/67) 0.311 -\nReflection prompt\nc)\n44.8 (30/67) 41.8 (28/67) 41.8 (28/67) 0.867 -\nInitial answer 44.8 (30/67) 41.8 (28/67) 49.3 (33/67) - -\nReflection rate 0.0 (0/67) 1.5 (1/67) 17.9 (12/67) - -\nMultiagent prompt 43.3 (29/67) 38.8 (26/67) 46.3 (31/67) 0.495 -\nAI generate prompt 46.3 (31/67) 38.8 (26/67) 53.7 (36/67) 0.096 Ge vs. Cl (P=0.041), Ge vs. Gp (P=0.359), \nGp vs. Cl (P=0.424)\nTotal prompt 43.5 (175/402) 39.8 (160/402) 46.3 (186/402) 0.046 Ge vs. Cl (P=0.014), Ge vs. Gp (P=0.195), \nGp vs. Cl (P=0.343)\nValues are presented as percentage (number/total). \nLLM, large language model; AI, artificial intelligence.\na)\nCochran('s) Q test significance level set at P<0.1. \nb)\nP<0.017 was considered statistically significant following Bonferroni correction for multiple comparisons, Ge vs. \nCl=Gemini-1.5-Pro vs. Claude 3.5 Sonnet, Ge vs. GP=Gemini-1.5-Pro vs. GPT-4o, Gp vs. Cl=GPT-4o vs. Claude 3.5 Sonnet. \nc)\nResults shown are for revised answers.\nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 227\n3.5 Sonnet (OR, 5.9 [95% CI, 1.6 to 22.2]). Similar patterns were \nobserved with AI-generated prompts: GPT-4o (OR, 4.3 [95% CI, 1.4 \nto 13.5]), Gemini-1.5-Pro-002 (OR, 3.7 [95% CI, 1.1 to 12.2]), and \nClaude 3.5 Sonnet (OR, 6.4 [95% CI, 1.6 to 26.6]). \nAdditional significant determinants included case rarity, which \nnotably reduced diagnostic accuracy for GPT-4o with the original \nprompt (OR, 0.1 [95% CI, 0.0 to 0.6]). The knowledge cutoff date \nemerged as a significant factor specifically for Claude 3.5 Sonnet \nwhen using the AI-generated prompt (OR, 0.2 [95% CI, 0.1 to 0.9]).\nSubgroup Analysis of Image-Only vs. Combined Imaging–\nDescriptive Text Input\nFig. 4 displays the performance variations between imaging-only \nand combined imaging-descriptive text inputs. The human accuracy \nbenchmark was 55.2%. GPT-4o showed significant improvements \nwith combined inputs using the basic prompt (58.2% vs. 40.3%, \nFig. 4. Impact of descriptive text integration on model performance. These graphs show side-by-side comparison of diagnostic accuracy \nbetween imaging only input versus combined imaging and descriptive text inputs across different prompt engineering strategies for each \nmultimodal large language model (A,  GPT-4o; B, Gemini-1.5-Pro; C, Claude 3.5 Sonnet; D, total). The horizontal line indicates the human \nperformance benchmark (55.2%). AI, artificial intelligence. *P<0.05, **P<0.01.\nBasic Original Chain of thought\nReflection Multiagent AI generate\nImaging-only input\nBasic Original Chain of thought\nReflection Multiagent AI generate\nCombined imaging-descriptive input\nGPT-4o (2024-11-20)\nHuman\n=55.2%\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nA\nGemini-1.5-Pro\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nB\nClaude 3.5 Sonnet (2024-06-20)\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nC\nTotal multimodal LLM\nHuman\nAccuracy (%)\n100\n80\n60\n40\n20\n0\nD\nTaewon Han, et al.\n228  Ultrasonography 44(3), May 2025 e-ultrasonography.org\nP=0.002), chain-of-thought prompt (61.2% vs. 43.3%, P=0.004), \nmultiagent prompt (56.7% vs. 43.3%, P=0.031), and AI-generated \nprompt (58.2% vs. 46.3%, P=0.049). Gemini-1.5-Pro-002 \ndemonstrated significant improvements across all prompt types, \nwith an overall accuracy of 60.4% compared to 39.8% (P<0.001), \nand the most substantial improvement was observed with the AI-\ngenerated prompt (62.7% vs. 38.8%, P<0.001). Claude 3.5 Sonnet \nachieved the highest overall performance with combined inputs \n(66.2% vs. 46.3%, P<0.001), particularly with the AI-generated \nprompt (71.6% vs. 53.7%, P=0.004), and showed significant \nimprovements across all prompt types. Overall, the addition of \ndescriptive text significantly improved diagnostic accuracy across all \nmodels and prompt types (61.4% vs. 43.2%, P<0.001).\nFurthermore, the improvement in diagnostic accuracy with \ndescriptive text was significant across all prompt strategies: basic \n(20.9% improvement, P<0.001), original (16.9%, P<0.001), \nchain-of-thought (18.9%, P<0.001), reflection (16.4%, P<0.001), \nmultiagent (17.9%, P<0.001), and AI-generated prompts (17.9%, \nP<0.001) (Table 4). Detailed performance metrics for descriptive text \ninputs are provided in Supplementary Table 4.\nDiscussion\nThis study evaluated the diagnostic performance of three multimodal \nLLMs—Claude 3.5 Sonnet, GPT-4o, and Gemini-1.5-Pro-002—\nin interpreting radiological cases from the KSUM case discussion \nrepository. Among all tested models and prompt engineering \nstrategies, Claude 3.5 Sonnet with AI-generated prompts achieved \nthe highest numerical accuracy. Nevertheless, the performance of all \nmodels remained below human diagnostic benchmarks, highlighting \nthe current limitations of autonomous radiological image \ninterpretation by LLMs. The integration of descriptive text inputs \nled to significant improvements in diagnostic accuracy across most \nscenarios. These findings underscore the critical role of contextual \ninformation in enhancing interpretative accuracy [28,29].\nMultiple factors influenced diagnostic accuracy, including the \nprompt engineering methodology, various input factors, and model \npretraining characteristics [29 -31]. Original prompts yielded \nsuperior performance compared to basic prompts, aligning with \nprevious research [30,32]. However, chain-of-thought prompts \nproduced similar accuracy to original prompts. GPT-4o, known for its \ncapabilities in specialized scientific reasoning [33], along with the \nother evaluated LLMs, naturally employed a step-by-step approach \nin their diagnostic processes during the conversational testing. \nThis suggests that explicit chain-of-thought instructions in system \nprompts may be redundant [26], as these advanced models have \nalready integrated analytical reasoning into their base responses \n[34].\nAnalysis of reflection prompts revealed distinct behavioral patterns \namong models. While GPT-4o and Gemini-1.5-Pro-002 exhibited \nminimal answer revision, Claude 3.5 Sonnet demonstrated a 17.9% \nrevision rate. However, its revised responses showed decreased \naccuracy (41.8%, 28/67) compared to the initial answers (49.3%, \n33/67). Only two cases improved from incorrect to correct answers, \nwhereas seven cases changed from correct to incorrect following \nrevisions, typically due to recalibration between general and atypical \nTable 4. Comparison of diagnostic accuracy between image-only and combined imaging-descriptive text input across different \nsystem prompt types\nPrompt type\nGPT-4o Gemini-1.5-Pro Claude 3.5 Sonnet Total multimodal LLMs\nImage \nonly\nCombined \ndescription P-value Image \nonly\nCombined \ndescription P-value Image \nonly\nCombined \ndescription P-value Image \nonly\nCombined \ndescription P-value\nBasic prompt 40.3\n(27/67)\n58.2\n(39/67)\n0.002 40.3\n(27/67)\n59.7\n(40/67)\n0.003 41.8\n(28/67)\n67.2\n(45/67)\n<0.001 40.8\n(82/201)\n61.7\n(124/201)\n<0.001\nOriginal prompt 43.3\n(29/67)\n55.2\n(37/67)\n0.063 41.8\n(28/67)\n59.7\n(40/67)\n0.007 47.8\n(32/67)\n68.7\n(46/67)\n0.003 44.3\n(89/201)\n61.2\n(123/201)\n<0.001\nChain-of-thought \nprompt\n43.3\n(29/67)\n61.2\n(41/67)\n0.004 37.3\n(25/67)\n61.2\n(41/67)\n0.001 46.3\n(31/67)\n61.2\n(41/67)\n0.013 42.3\n(85/201)\n61.2\n(123/201)\n<0.001\nReflection \nprompt\na)\n44.8\n(30/67)\n55.2\n(37/67)\n0.115 41.8\n(28/67)\n58.2\n(39/67)\n0.008 41.8\n(28/67)\n64.2\n(43/67)\n<0.001 42.8\n(86/201)\n59.2\n(119/201)\n<0.001\nMultiagent \nprompt\n43.3\n(29/67)\n56.7\n(38/67)\n0.031 38.8\n(26/67)\n61.2\n(41/67)\n0.001 46.3\n(31/67)\n64.2\n(43/67)\n0.004 42.8\n(86/201)\n60.7\n(122/201)\n<0.001\nAI generate \nprompt\n46.3\n(31/67)\n58.2\n(39/67)\n0.049 38.8\n(26/67)\n62.7\n(42/67)\n<0.001 53.7\n(36/67)\n71.6\n(48/67)\n0.004 46.3\n(93/201)\n64.2\n(129/201)\n<0.001\nTotal prompt 43.5\n(175/402)\n57.5\n(231/402)\n<0.001 39.8\n(160/402)\n60.4\n(243/402)\n<0.001 46.3\n(186/402)\n66.2\n(266/402)\n<0.001 43.2\n(521/1206)\n61.4\n(741/1206)\n<0.001\nValues are presented as percentage (number/total). \na)\nReflection prompt shows revised answer.\nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 229\nscenario interpretations. Notably, with the integration of descriptive \ntext, the revised accuracy remained essentially unchanged (64.2%, \n43/67), with two cases improving and two cases worsening. This \npattern suggests that diagnostic uncertainty in imaging analysis may \nstem from insufficient pretraining on radiological images relative \nto textual data [35]. Moreover, the implementation of multiagent \nprompts did not replicate previously reported performance \nenhancements, possibly due to lengthy prompt structures (293 \nwords) and limitations imposed by single-session chat and single-\nsystem prompts, which may have constrained effective agent \ncollaboration [25,26,32]. This finding indicates opportunities \nfor future research and development in optimizing multiagent \narchitectures.\nThe AI-generated prompt, developed as an enhancement of \nthe original prompt structure, consistently demonstrated superior \nperformance despite its substantial length (295 words), indicating \na reduced dependence on human-engineered prompts. This effect \nwas particularly pronounced with Claude 3.5 Sonnet, which \nexhibited enhanced performance with Anthropic-developed \ntechnology. This suggests that prompts generated using the same \nunderlying architecture may be more effective for that specific \nmodel. Furthermore, as shown in Supplementary Table 5, in the \nanalysis excluding AI-generated prompts, performance differences \nbetween models were not statistically significant for imaging-only \ninput, although Claude maintained numerically higher accuracy \n(44.8%). This finding implies that the superior performance initially \nobserved may be partially attributed to the alignment between \nClaude’s architecture and its AI-generated prompts, highlighting the \nimportance of platform-specific prompt optimization strategies. \nRegarding case rarity, GPT-4o initially performed poorly on rare \ncases with the original prompt (6.7%, 1/15) compared to non-rare \ncases (53.9%, 28/52; P=0.001). However, the implementation of AI-\ngenerated prompts substantially improved rare case performance \n(40.0%, 6/15), making it comparable to non-rare cases (48.1%, \n25/52; P=0.796). Additionally, the multivariable analysis revealed \nthat while the correction rate was consistently positively associated \nwith diagnostic accuracy (OR, 4.3 to 6.4; P<0.05), the negative \nimpact of case rarity (OR, 0.1; P=0.014) was effectively mitigated \nthrough AI-generated prompts (OR, 1.0; P=0.984). This suggests \nthat although the model’s pretraining and supervised fine-tuning \ndata likely emphasized typical cases, optimized prompt engineering \ncan help overcome this limitation. For future development, post-\ntraining strategies should deliberately incorporate more atypical and \nrare cases to improve model performance across a broader spectrum \nof radiologic presentations.\nTemporal analysis revealed varying patterns in model performance \nrelative to knowledge cutoff dates. GPT-4o and Gemini-1.5-Pro-002 \nshowed no significant differences between pre- and post-cutoff \ncases. In contrast, Claude 3.5 Sonnet demonstrated significantly \nhigher accuracy for pre-cutoff cases (64.0%, 32/50) compared to \npost-cutoff cases (23.5%, 4/17; P=0.005). However, this finding \nshould be interpreted cautiously due to the inherent complexity \nin determining effective knowledge cutoffs. Recent research has \nsuggested that reported cutoff dates may not accurately reflect \nthe temporal alignment of various resources within LLM training \ndata, implying that the actual temporal boundaries affecting \nmodel performance may differ from the reported dates [36]. \nThese limitations, combined with proprietary restrictions on \naccessing detailed pretraining data, underscore the need for more \ncomprehensive investigations into the temporal effects on LLM \nperformance in medical imaging applications. \nThe present study demonstrated that adding descriptive text inputs \nimproved model performance, which aligns with previous studies \nin radiological image analysis [37]. They confirm that radiologic \nimage descriptions and medical history are strong contributors to \nLLM performance in imaging analysis. It is acknowledged that the \nquality and specificity of these text descriptions may influence model \nperformance. Standardized methods for investigating how different \nlevels of text input quality—regarding imaging descriptions, medical \nhistory, and structure—affect model accuracy are needed in future \nstudies to provide valuable insights for optimizing multimodal LLM \napplications in radiology.\nThis study had several limitations. First, the relatively small \nsample size may limit the generalizability of the findings and \nmay have affected the ability to detect statistically significant \ndifferences between individual models after multiple comparison \ncorrections. Future studies with larger datasets across various \nradiological conditions would be valuable to validate the findings \nand potentially identify additional patterns in model performance. \nSecond, the analysis did not include a detailed evaluation of the \nLLMs’ reasoning processes behind their answer selections, making \nit difficult to determine whether correct responses resulted from \ngenuine understanding or mere pattern recognition. Third, the \nstudy’s reliance on multiple-choice questions for performance \nevaluation may not fully represent real-world clinical scenarios \nthat typically require free-text responses. Future studies should \nincorporate free-text radiological interpretations and reporting \nthat more closely reflect clinical practice. Fourth, methodological \nconstraints prevented the evaluation of text-only inputs (description-\nonly) due to variations in user and system prompts, which would \nhave compromised direct comparisons. Fifth, human performance \nmetrics from the KSUM website may be subject to reporting bias, \nas they only reflect responses from users who voluntarily submitted \nanswers online, potentially affecting the representativeness of the \nTaewon Han, et al.\n230  Ultrasonography 44(3), May 2025 e-ultrasonography.org\ncomparative analysis. Lastly, due to the lack of open technologies \nfor prompt generation across different platforms, it was not possible \nto test how various model architectures might respond to platform-\nspecific prompt optimization techniques. Future studies should \nexplore this aspect to better understand the relationship between \nmodel architecture and prompt engineering effectiveness.\nClaude 3.5 Sonnet, when utilizing AI-generated prompts, \ndemonstrated the highest diagnostic accuracy among the \nevaluated multimodal LLMs, although it did not reach human \nperformance benchmarks. Consequently, autonomous radiological \nimage interpretation by LLMs remains limited for direct clinical \nimplementation. Given the significant enhancement in performance \nachieved through the integration of descriptive text inputs, \ncombining radiologist-generated descriptive content with LLM \nanalysis holds potential as a supportive diagnostic tool. \nORCID: Taewon Han: https://orcid.org/0000-0003-0243-1608; Woo Kyoung Jeong: \nhttps://orcid.org/0000-0002-0676-2116; Jaeseung Shin: https://orcid.org/0000-\n0002-6755-4732\nAuthor Contributions\nConceptualization: Han T, Jeong WK, Shin J. Data acquisition: Han T, \nJeong WK. Data analysis or interpretation: Han T, Jeong WK. Drafting \nof the manuscript: Han T. Critical revision of the manuscript: Han T, \nJeong WK, Shin J. Approval of the final version of the manuscript: all \nauthors.\nConflict of Interest\nWoo Kyoung Jeong serves as Editor for the Ultrasonography, but has \nno role in the decision to publish this article. All remaining authors \nhave declared no conflicts of interest.\nSupplementary Material\nSupplementary Table 1. System prompts used in the study (https://\ndoi.org/10.14366/usg.25012).\nSupplementary Table 2. Analysis of model response repeatability \nusing Fleiss’ kappa statistics across three large language models \n(https://doi.org/10.14366/usg.25012).\nSupplementary Table 3. Multivariable logistic regression analysis of \nfactors affecting diagnostic accuracy across different models using \noriginal and AI-generated system prompts (https://doi.org/10.14366/\nusg.25012).\nSupplementary Table 4. Comparison of diagnostic accuracy among \nmultimodal LLMs using different prompt types with combined \nimaging-descriptive text input (https://doi.org/10.14366/\nusg.25012).\nSupplementary Table 5. Comparison of diagnostic accuracy among \nmultimodal LLMs using total prompts excluding AI generate prompt \nwith imaging only and combined imaging-descriptive text input \n(https://doi.org/10.14366/usg.25012).\nReferences\n 1. Tian S, Jin Q, Yeganova L, Lai PT, Zhu Q, Chen X, et al. Opportunities \nand challenges for ChatGPT and large language models in \nbiomedicine and health. Brief Bioinform 2023;25:bbad493.\n 2. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language \nmodels are unsupervised multitask learners [Internet]. OpenAI Blog, \n2019 [cited 2024 Dec 10]. Available from: https://cdn.openai.com/\nbetter-language-models/language_models_are_unsupervised_\nmultitask_learners.pdf.\n 3. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. \nGpt-4 technical report. Preprint arXiv at: https://doi.org/10.48550/\narXiv.2303.08774 (2023).\n 4. Tang L, Sun Z, Idnay B, Nestor JG, Soroush A, Elias PA, et \nal. Evaluating large language models on medical evidence \nsummarization. NPJ Digit Med 2023;6:158.\n 5. Jin Q, Leaman R, Lu Z. Retrieve, summarize, and verify: how will \nChatGPT affect information seeking from the medical literature? J \nAm Soc Nephrol 2023;34:1302-1304.\n 6. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. \nLarge language models encode clinical knowledge. Nature \n2023;620:172-180.\n 7. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of \nGPT-4 on medical challenge problems. Preprint arXiv at: https://doi.\norg/10.48550/arXiv.2303.13375 (2023).\n 8. Lievin V , Hother CE, Motzfeldt AG, Winther O. Can large language \nmodels reason about medical questions? Patterns (N Y) \n2024;5:100943.\n 9. Nori H, Lee YT, Zhang S, Carignan D, Edgar R, Fusi N, et al. Can \ngeneralist foundation models outcompete special-purpose tuning? \ncase study in medicine. Preprint arXiv at: https://doi.org/10.48550/\narXiv.2311.16452 (2023).\n10. Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ. Building \nmachines that learn and think like people. Behav Brain Sci \n2017;40:e253.\n11. Wu T, He S, Liu J, Sun S, Liu K, Han QL, et al. A brief overview of \nChatGPT: the history, status quo and potential future development. \nIEEE/CAA J Automat Sin 2023;10:1122-1136.\n12. Zhou Y , Ong H, Kennedy P , Wu CC, Kazam J, Hentel K, et al. \nEvaluating GPT-V4 (GPT-4 with vision) on detection of radiologic \nfindings on chest radiographs. Radiology 2024;311:e233270.\nMultimodal LLMs in radiology with minimal text\ne-ultrasonography.org Ultrasonography 44(3), May 2025 231\n13. Sun Z, Ong H, Kennedy P , Tang L, Chen S, Elias J, et al. Evaluating \nGPT4 on impressions generation in radiology reports. Radiology \n2023;307:e231259.\n14. Yan Z, Zhang K, Zhou R, He L, Li X, Sun L. Multimodal ChatGPT for \nmedical applications: an experimental study of GPT-4V . Preprint \narXiv at: https://doi.org/10.48550/arXiv.2310.19061 (2023).\n15. Su X, Wang Y , Gao S, Liu X, Giunchiglia V , Clevert DA, et al. \nKGARevion: an AI agent for knowledge-intensive biomedical QA. \nPreprint arXiv at: https://doi.org/10.48550/arXiv.2410.04660 \n(2024).\n16. Kitamura FC, Topol EJ. The initial steps of multimodal AI in \nradiology. Radiology 2023;309:e232372.\n17. Fink MA, Bischoff A, Fink CA, Moll M, Kroschke J, Dulz L, et al. \nPotential of ChatGPT and GPT-4 for data mining of free-text CT \nreports on lung cancer. Radiology 2023;308:e231362.\n18. Adams LC, Truhn D, Busch F , Kader A, Niehues SM, Makowski MR, \net al. Leveraging GPT-4 for post hoc transformation of free-text \nradiology reports into structured reporting: a multilingual feasibility \nstudy. Radiology 2023;307:e230725.\n19. Lehnen NC, Dorn F , Wiest IC, Zimmermann H, Radbruch A, Kather \nJN, et al. Data extraction from free-text reports on mechanical \nthrombectomy in acute ischemic stroke using ChatGPT: a \nretrospective analysis. Radiology 2024;311:e232741.\n20. Chen D, Huang RS, Jomy J, Wong P , Yan M, Croke J, et al. \nPerformance of multimodal artificial intelligence chatbots evaluated \non clinical oncology cases. JAMA Netw Open 2024;7:e2437711.\n21. Suh PS, Shim WH, Suh CH, Heo H, Park KJ, Kim PH, et al. Comparing \nlarge language model and human reader accuracy with New \nEngland Journal of Medicine image challenge case image inputs. \nRadiology 2024;313:e241668.\n22. Morishita M, Fukuda H, Yamaguchi S, Muraoka K, Nakamura T, \nHayashi M, et al. An exploratory assessment of GPT-4o and GPT-4 \nperformance on the Japanese National Dental Examination. Saudi \nDent J 2024;36:1577-1581.\n23. Park SH, Suh CH, Lee JH, Kahn CE, Moy L. Minimum reporting items \nfor clear evaluation of accuracy reports of large language models in \nhealthcare (MI-CLEAR-LLM). Korean J Radiol 2024;25:865-868.\n24. Suh PS, Shim WH, Suh CH, Heo H, Park CR, Eom HJ, et al. \nComparing diagnostic accuracy of radiologists versus GPT-4V and \nGemini Pro vision using image inputs from diagnosis please cases. \nRadiology 2024;312:e240273.\n25. Li Y , Zhang S, Wu R, Huang X, Chen Y , Xu W, et al. MATEval: a \nmulti-agent discussion framework for advancing open-ended \ntext evaluation. In: Database systems for advanced applications. \nDASFAA 2024. Lecture notes in computer science, Vol. 14856. \nSingapore: Springer, 2024;415-426.\n26. Lee JH, Shin J. How to optimize prompting for large language \nmodels in clinical research. Korean J Radiol 2024;25:869-873.\n27. Higgins JP , Green S. Cochrane handbook for systematic reviews of \ninterventions. Chichester: Cochrane Collaboration and John Wiley & \nSons Ltd., 2008.\n28. Gunes YC, Cesur T, Camur E, Gunbey Karabekmez L. Evaluating \ntext and visual diagnostic capabilities of large language models on \nquestions related to the Breast Imaging Reporting and Data System \nAtlas 5(th) edition. Diagn Interv Radiol 2025;31:111-129.\n29. Mukherjee P , Hou B, Suri A, Zhuang Y , Parnell C, Lee N, et al. \nEvaluation of GPT large language model performance on RSNA \n2023 case of the day questions. Radiology 2024;313:e240609.\n30. Cesur T, Gunes YC. Optimizing diagnostic performance of ChatGPT: \nthe impact of prompt engineering on thoracic radiology cases. \nCureus 2024;16:e60009.\n31. Schramm S, Preis S, Metz MC, Jung K, Schmitz-Koep B, Zimmer \nC, et al. Impact of multimodal prompt elements on diagnostic \nperformance of GPT-4(V) in challenging brain MRI cases. Preprint \nmedRxiv at: https://doi.org/10.1101/2024.03.05.24303767 (2024).\n32. Hayden N, Gilbert S, Poisson LM, Griffith B, Klochko C. Performance \nof GPT-4 with vision on text- and image-based ACR diagnostic \nradiology in-training examination questions. Radiology \n2024;312:e240153.\n33. Hurst A, Lerer A, Goucher AP , Perelman A, Ramesh A, Clark A, et al. \nGpt-4o system card. Preprint arXiv at: https://doi.org/10.48550/\narXiv.2410.21276 (2024).\n34. Shahriar S, Lund BD, Mannuru NR, Arshad MA, Hayawi K, Bevara \nRV , et al. Putting GPT-4o to the sword: a comprehensive evaluation \nof language, vision, speech, and multimodal proficiency. Appl Sci \n2024;14:7782.\n35. Agbareia R, Omar M, Soffer S, Glicksberg BS, Nadkarni GN, \nKlang E. Visual-textual integration in LLMs for medical diagnosis: \na quantitative analysis. Preprint medRxiv at: https://doi.\norg/10.1101/2024.08.31.24312878 (2024).\n36. Cheng J, Marone M, Weller O, Lawrie D, Khashabi D, Van Durme B. \nDated data: tracing knowledge cutoffs in large language models. \nPreprint arXiv at: https://doi.org/10.48550/arXiv.2403.12958 \n(2024).\n37. Schramm S, Preis S, Metz MC, Jung K, Schmitz-Koep B, Zimmer \nC, et al. Impact of multimodal prompt elements on diagnostic \nperformance of GPT-4V in challenging brain MRI cases. Radiology \n2025;314:e240689.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.9390679597854614
    },
    {
      "name": "Radiological weapon",
      "score": 0.8821439743041992
    },
    {
      "name": "Radiology",
      "score": 0.42481717467308044
    },
    {
      "name": "Radiological imaging",
      "score": 0.4244266748428345
    },
    {
      "name": "Medical physics",
      "score": 0.395435094833374
    }
  ]
}