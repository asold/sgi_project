{
  "title": "Aggregating Nested Transformers.",
  "url": "https://openalex.org/W3164540605",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2231835763",
      "name": "Zizhao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003434428",
      "name": "Han Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098624950",
      "name": "L Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099725907",
      "name": "Ting Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2798413642",
      "name": "Tomas Pfister",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W2950557962",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2964137095",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3099570996",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2964274719",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3159732141",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W2765793020",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3174525637",
    "https://openalex.org/W2962971773",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3135404760",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3159344844",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W3174480456",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3022265721",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W2953030256",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2994759459",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3034512672",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2996690341",
    "https://openalex.org/W3179869055"
  ],
  "abstract": "Although hierarchical structures are popular in recent vision transformers, they require sophisticated designs and massive datasets to work well. In this work, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical manner. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture with minor code changes upon the original vision transformer and obtains improved performance compared to existing methods. Our empirical results show that the proposed method NesT converges faster and requires much less training data to achieve good generalization. For example, a NesT with 68M parameters trained on ImageNet for 100/300 epochs achieves $82.3\\%/83.8\\%$ accuracy evaluated on $224\\times 224$ image size, outperforming previous methods with up to $57\\%$ parameter reduction. Training a NesT with 6M parameters from scratch on CIFAR10 achieves $96\\%$ accuracy using a single GPU, setting a new state of the art for vision transformers. Beyond image classification, we extend the key idea to image generation and show NesT leads to a strong decoder that is 8$\\times$ faster than previous transformer based generators. Furthermore, we also propose a novel method for visually interpreting the learned model. Source code is available this https URL.",
  "full_text": "Nested Hierarchical Transformer: Towards Accurate, Data-Efﬁcient and\nInterpretable Visual Understanding\nZizhao Zhang1 Han Zhang2 Long Zhao2 Ting Chen2 Sercan Ö. Arık1 Tomas Pﬁster1\n1Google Cloud AI 2Google Research\nAbstract\nHierarchical structures are popular in recent vision transform-\ners, however, they require sophisticated designs and massive\ndatasets to work well. In this paper, we explore the idea of\nnesting basic local transformers on non-overlapping image\nblocks and aggregating them in a hierarchical way. We ﬁnd\nthat the block aggregation function plays a critical role in\nenabling cross-block non-local information communication.\nThis observation leads us to design a simpliﬁed architecture\nthat requires minor code changes upon the original vision\ntransformer. The beneﬁts of the proposed judiciously-selected\ndesign are threefold: (1) NesT converges faster and requires\nmuch less training data to achieve good generalization on both\nImageNet and small datasets like CIFAR; (2) when extending\nour key ideas to image generation, NesT leads to a strong de-\ncoder that is 8×faster than previous transformer-based genera-\ntors; and (3) we show that decoupling the feature learning and\nabstraction processes via this nested hierarchy in our design\nenables constructing a novel method (named GradCAT) for vi-\nsually interpreting the learned model. Source code is available\nhttps://github.com/google-research/nested-transformer.\nIntroduction\nVision Transformer (ViT) (Dosovitskiy et al. 2021) model\nand its variants have received signiﬁcant interests recently\ndue to their superior performance on many core visual appli-\ncations (Cordonnier, Loukas, and Jaggi 2020; Liu et al. 2021).\nViT ﬁrst splits an input image into patches, and then patches\nare treated in the same way as tokens in NLP applications.\nFollowing, several self-attention layers are used to conduct\nglobal information communication to extract features for clas-\nsiﬁcation. Recent work (Dosovitskiy et al. 2021; Cordonnier,\nLoukas, and Jaggi 2020) shows that ViT models can achieve\nbetter accuracy than state-of-the-art convnets (Tan and Le\n2019; He et al. 2016) when trained on datasets with tens\nor hundreds of millions of labeled samples. However, when\ntrained on smaller datasets, ViT usually underperforms its\ncounterparts based on convolutional layers. Addressing this\ndata inefﬁciency is important to make ViT applicable to other\napplication scenarios, e.g. semi-supervised learning (Sohn\net al. 2020) and generative modeling (Goodfellow et al. 2014;\nZhang et al. 2019).\nLack of inductive bias such as locality and translation\nequivariance, is one explanation for the data inefﬁciency of\nViT models. Cordonnier, Loukas, and Jaggi (2020) discov-\nered that transformer models learn locality behaviors in a\ndeformable convolution manner (Dai et al. 2017): bottom\nlayers attend locally to the surrounding pixels and top layers\nfavor long-range dependency. On the other hand, global self-\nattention between pixel pairs in high-resolution images is\ncomputationally expensive. Reducing the self-attention range\nis one way to make the model training more computationally\nefﬁcient (Beltagy, Peters, and Cohan 2020). These type of in-\nsights align with the recent structures with local self-attention\nand hierarchical transformer (Han et al. 2021; Vaswani et al.\n2021; Liu et al. 2021). Instead of holistic global self-attention,\nthese perform attention on local image patches. To promote\ninformation communication across patches, they propose spe-\ncialized designs such as the “haloing operation” (Vaswani\net al. 2021) and “shifted window” (Liu et al. 2021). These are\nbased on modifying the self-attention mechanism and often\nyields in complex architectures. Our design goal on the other\nhand keeping the attention as is, and introducing the design\nof the aggregation function, to improve the accuracy and data\nefﬁciency, while bringing interpretability beneﬁts.\nThe proposed NesT model stacks canonical transformer\nblocks to process non-overlapping image blocks individually.\nCross-block self-attention is achieved by nesting these trans-\nformers hierarchically and connecting them with a proposed\naggregation function. Fig. 1 illustrates the overall architecture\nand the simple pseudo code to generate it. Our contributions\ncan be summarized as:\n1. We demonstrate integrating hierarchically nested trans-\nformers with the proposed block aggregation func-\ntion can outperform previous sophisticated (local) self-\nattention variants, leading to a substantially-simpliﬁed\narchitecture and improved data efﬁciency. This provides\na novel perspective for achieving effective cross-block\ncommunication.\n2. NesT achieves impressive ImageNet classiﬁcation accu-\nracy with a signiﬁcantly simpliﬁed architectural design.\nE.g., training a NesT with 38M/68M parameters obtains\n83.3%/83.8% ImageNet accuracy.The favorable data\nefﬁciency of NesT is embodied by its fast convergence,\nsuch as achieving 75.9%/82.3% training with 30/100\nepochs. Moreover, NesT achieves matched accuracy\non small datasets compared with popular convolutional\narXiv:2105.12723v4  [cs.CV]  30 Dec 2021\n6 7 8 9\n32 4 5\n1\nGolden retriever\nLinear projection\nBlock aggregation\n6 7\n3\n8\n4\n9\n5\nBlock aggregation\nBlocking image to \npatches [6, 7, 8, 9, …]\n(#block, seqlen, d)\n(#block/4, seqlen, d)\n… …\n(#block/16, seqlen, d)\nPseudo code: NesT\n# embed and block image to (#block,seqlen,d)\nx = Block(PatchEmbed(input_image))\nfor i in range(num_hierarchy):\n# apply transformer layers T_i within each block\n# with positional encodings (PE)\ny = Stack([T_i(x[0] + PE_i[0]), ...])\nif i < num_hierarchy - 1:\n# aggregate blocks and reduce #block by 4\nx = Aggregate(y, i)\nh = GlobalAvgPool(x) # (1,seqlen,d) to (1,1,d)\nlogits = Linear(h[0,0]) # (num_classes,)\ndef Aggregate(x, i):\nz = UnBlock(x) # unblock seqs to (h,w,d)\nz = ConvNormMaxPool_i(x) # (h/2,w/2,d)\nreturn Block(z) # block to seqs\nFigure 1: (Left) Illustration of NesT with nested transformer hierarchy; (right) the simple pseudo code to generate the architecture.\nEach node T_i processes an image block. The block aggregation is performed between hierarchies (num_hierarchy= 3here) to\nachieve cross-block communication on the image (feature map) plane.\narchitectures. E.g., training a NesT with 6M parame-\nters using a single GPU results in 96% accuracy on\nCIFAR10 .\n3. We show that when extending this idea beyond classiﬁ-\ncation to image generation, NesT can be repurposed into\na strong decoder that achieves better performance than\nconvolutional architectures meanwhile has comparable\nspeed, demonstrated by 64 ×64 ImageNet generation,\nwhich is an important to be able to adopt transformers\nfor efﬁcient generative modeling.\n4. Our proposed architectural design leads to decoupled\nfeature learning and abstraction, which has signiﬁcant\ninterpretability beneﬁts. To this end, we propose a novel\nmethod called GradCAT to interpret NesT reasoning\nprocess by traversing its tree-like structure. This provid-\ning a new type of visual interpretability that explains\nhow aggregated local transformers selectively process\nlocal visual cues from semantic image patches.\nRelated Work\nVision transformer-based models (Cordonnier, Loukas, and\nJaggi 2020; Dosovitskiy et al. 2021) and self-attention mech-\nanisms (Vaswani et al. 2021; Ramachandran et al. 2019) have\nrecently attracted signiﬁcant interest in the research commu-\nnity, with explorations of more suitable architectural designs\nthat can learn visual representation effectively, such as inject-\ning convolutional layers (Li et al. 2021; Srinivas et al. 2021;\nYuan et al. 2021) and building local or hierarchical structures\n(Zhang et al. 2021b; Wang et al. 2021b). Existing methods\nfocus on designing a variety of self-attention modiﬁcations.\nHierarchical ViT structures becomes popular both in vision\n(Liu et al. 2021; Vaswani et al. 2021) and NLP (Zhang, Wei,\nand Zhou 2019; Santra, Anusha, and Goyal 2020; Liu and\nLapata 2019; Pappagari et al. 2019). However, many meth-\nods often add signiﬁcant architectural complexity in order to\noptimize accuracy.\nOne challenge for vision transformer-based models is data\nefﬁciency. Although the original ViT (Dosovitskiy et al. 2021)\ncan perform better than convolutional networks with hun-\ndreds of millions images for pre-training, such a data require-\nment is not always practical. Data-efﬁcient ViT (DeiT) (Tou-\nvron et al. 2020, 2021) attempts to address this problem by\nintroducing teacher distillation from a convolutional network.\nAlthough promising, this increases the supervised training\ncomplexity, and existing reported performance on data efﬁ-\ncient benchmarks (Hassani et al. 2021; Chen et al. 2021) still\nsigniﬁcantly underperforms convolutional networks. Since\nViT has shown to improve vision tasks beyond image classiﬁ-\ncation, with prior work studying its applicability to generative\nmodeling (Parmar et al. 2018; Child et al. 2019; Jiang, Chang,\nand Wang 2021; Hudson and Zitnick 2021), video understand-\ning (Neimark et al. 2021; Akbari et al. 2021), segmentation\nand detection (Wang et al. 2021a; Liang et al. 2020; Kim\net al. 2021), interpretability (Chefer, Gur, and Wolf 2021;\nAbnar and Zuidema 2020), a deeper understanding of the\ndata efﬁciency and training difﬁculties from the architectural\nperspective is of signiﬁcant impact.\nProposed Method\nMain Architecture\nAccording to Fig. 1, our overall design stacks canonical trans-\nformer layers to conduct local self-attention on every image\nblock independently, and then nests them hierarchically. Cou-\npling of processed information between spatially adjacent\nblocks is achieved through a proposed block aggregation\nbetween every two hierarchies. The overall hierarchical struc-\nture can be determined by two key hyper-parameters: patch\nsize S×Sand number of block hierarchies Td. All blocks\ninside each hierarchy share one set of parameters.\nGiven an input of image with shapeH×W×3, each image\npatch with size S×Sis linearly projected to an embedding\nin Rd. Then, all embeddings are partitioned to blocks and\nﬂattened to generate input X ∈Rb×Tn×n×d, where bis the\nbatch size, Tn is the total number of blocks at bottom of the\nTop1: Blenheim spaniel Top2: Tabby cat\nTop3: Tigger cat Top4: Sussex spaniel\nFigure 2: Example results of the proposed GradCAT. Given the left input image (containing four objects), the ﬁgure visualizes\nthe top-4 class traversal results (4 colors) using an ImageNet-trained NesT (with three tree hierarchies). Each tree node denotes\nthe averaged activation value (ˆhl deﬁned in Algorithm 1). The traversals can correctly ﬁnd the model decision path along the tree\nto locate an image patch belonging to the objects of given target classes.\nNesT hierarchy, and nis the sequence length (the number of\nembeddings) at each block. Note that Tn ×n= H×W/S2.\nInside each block, we stack a number of canonical trans-\nformer layers, where each is composed of a multi-head self-\nattention (MSA) layer followed by a feed-forward fully-\nconnected network (FFN) with skip-connection (He et al.\n2016) and Layer normalization (LN) (Ba, Kiros, and Hin-\nton 2016). Trainable positional embedding vectors (Touvron\net al. 2020) are added to all sequence vectors in Rd to encode\nspatial information before feeding into the block function T:\nmultiple ×\n{y= x+ MSANesT(x′,x′,x′), x′= LN(x)\nx= y+ FFN(LN(y))\n(1)\nThe FFN is composed of two layers:max(0,xW1 +b)W2 +b.\nGiven input X ∈Rb×Tn×n×d, since all blocks at one NesT\nhierarchy share the same parameters, MSANesT basically\nMSA is applied (Vaswani et al. 2017) to all blocks in parallel:\nMSANesT(Q,K,V ) =Stack(block1,..., blockTn ),\nwhere blocki = MSA(Q,K,V )WO. (2)\nblocki has shape b×n×d. Lastly, we build a nested hierar-\nchy with block aggregation – every four spatially connected\nblocks are merged into one. The overall design makes NesT\neasy to implement, requiring minor code changes to the orig-\ninal ViT.\nBlock Aggregation\nFrom a high-level view, NesT leads to hierarchical represen-\ntations, which share similarity with several pyramid designs\n(Zhang et al. 2021b; Wang et al. 2021b). However, most of\nthese works use global self-attention throughout the layers,\ninterleaved with (spatial) down-sampling. In contrast, we\nshow that NesT, which leverages local attention, can lead to\nsigniﬁcantly improved data efﬁciency. In local self-attention,\nnon-local communication is important to maintain transla-\ntional equivariance (Vaswani et al. 2021). To this end, Halonet\n(Vaswani et al. 2021) allows the query to attend to slightly\nlarger regions than the assigned block. Swin Transformer\n(Liu et al. 2021) achieves this by shifting the block partition\nwindows between consecutive self-attention layers to connect\nadjacent blocks; applying special masked self-attention to\nguarantee spatial continuity. However, both add complexity to\nthe self-attention layers and such sophisticated architectures\nare not desired from implementation perspective.\nOn the other hand, every block in NesT processes informa-\ntion independently via standard transformer layers, and only\ncommunicate and mix global information during the block\naggregation step via simple spatial operations (e.g. convolu-\ntion and pooling). One key ingredient of block aggregation is\nto perform it in the image plane so that information can be\nexchanged between nearby blocks. This procedure is summa-\nrized in Fig. 1. The output Xl ∈Rb×#block×n×d at hierarchy\nlis unblocked to the full image plane Al ∈Rb×H′×W′×d′\n. A\nnumber of spatial operations are applied to down-sample fea-\nture maps A′\nl ∈Rb×H′/2×W′/2×d. Finally, the feature maps\nare blocked back to Xl+1 ∈Rb×#block/4×n×d′\nfor hierarchy\nl+ 1. The sequence length nalways remains the same and\nthe total number of blocks is reduced by a factor of 4, until\nreduced to 1 at the top (i.e. #block/4(Td−1) = 1). Therefore,\nthis process naturally creates hierarchically nested structure\nwhere the “receptive ﬁeld” expands gradually. d′ ≥d de-\npends on the speciﬁc model conﬁguration.\nOur block aggregation is specially instantiated as a 3 ×3\nconvolution followed by LN and a3 ×3 max pooling. Figure\nA2 in Appendix explains the core design and the importance\nof applying it on the image plane (i.e. full image feature maps)\nversus the block plane (i.e. partial feature maps corresponding\nto 2 ×2 blocks that will be merged). The small information\nexchange through the small convolution and max. pooling\nkernels across block boundaries are particularly important.\nWe conduct comprehensive ablation studies to demonstrate\nthe importance of each of the design components.\nNote that the resulting design shares some similarities with\nrecent works that combine transformer and convolutional\nnetworks (Wu et al. 2021; Yuan et al. 2021; Bello 2021)\nas specialized hybrid structures. However, unlike these, our\nproposed method aims to solve cross-block communications\nin local self-attention, and the resulting architecture is simple\nas a stacking of basic transformer layers.\nTransposed NesT for Image Generation\nThe data efﬁciency and straightforward implementation of\nNesT makes it desirable for more complex learning tasks.\nWith transpose the key ideas from NesT to propose a de-\ncoder for generative modeling, and show that it has better\nperformance than convolutional decoders with comparable\nspeed. Remarkably, it is nearly a magnitude faster than the\ntransformer-based decoder TransGAN (Jiang, Chang, and\nWang 2021).\nCreating such a generator is straightforward by transpos-\ning NesT (see Table A6 of Appendix for architecture de-\ntails). The input of the model becomes a noise vector and\nthe output is a full-sized image. To support the gradually\nincreased number of blocks, the only modiﬁcation to NesT\nis replacing the block aggregation with appropriate block\nde-aggregation, i.e. up-sampling feature maps (we use pixel\nshufﬂe (Shi et al. 2016)). The feature dimensions in all hi-\nerarchies are (b,nd) → (b,1,n,d ) → (b,4,n,d ′),..., →\n(b,#blocks,n, 3). The number of blocks increases by a fac-\ntor of 4. Lastly, we can unblock the output sequence tensor to\nan image with shape H×W ×3. The remaining adversarial\ntraining techniques are based on (Goodfellow et al. 2014;\nZhang et al. 2019) as explained in experiments. Analogous to\nour results for image classiﬁcation, we show the importance\nof careful block de-aggregation design, in making the model\nsigniﬁcantly faster while achieving better generation quality.\nGradCAT: Interpretability via Tree Traversal\nDifferent from previous work, the nested hierarchy with the\nindependent block process in NesT resembles a decision tree\nin which each block is encouraged to learn non-overlapping\nfeatures and be selected by the block aggregation. This unique\nbehavior motivates us to explore a new method to explain\nthe model reasoning, which is an important topic with sig-\nniﬁcant real world impact in convnets (Selvaraju et al. 2017;\nSundararajan, Taly, and Yan 2017).\nAlgorithm 1: GradGAT\nDeﬁne: Al denotes the feature maps at hierarchy l. Yc is the logit\nof predicted class c. [·]2×2 indexes one of 2×2 partitions of input\nmaps.\nInput: {Al|l= 2,...,T d},αTd = ATd , P = []\nOutput: The traversal path P from top to bottom\nfor l= [Td,..., 2] do\nhl = αl ·(−∂Yc\nαl\n) # obtain target activation maps\nˆhl = AvgPool2×2(hl) ∈R2×2\nn∗\nl = arg maxˆhl, P = P + [n∗\nl] # pick the maximum index\nαl = Al[n∗\nl]2×2 # obtain the partition for the index\nend for\nWe present a gradient-based class-aware tree-traversal\n(GradCAT) method (Algorithm 1). The main idea is to ﬁnd\nthe most valuable traversal from a child node to the root\nnode that contributes to the classiﬁcation logits the most.\nIntuitively, at the top hierarchy, each of four child nodes pro-\ncesses one of2×2 non-overlapping partitions of feature maps\nATd . We can use corresponding activation and class-speciﬁc\ngradient features to trace the high-value information ﬂow\nTable 1: Test accuracy on CIFAR with input size32×32. The\ncompared convolutional architectures are optimized models\nfor CIFAR. All transformer-based architectures are trained\nfrom random initialization with the same data augmentation.\nDeiT uses S = 2. Swin and our NesT uses S = 1. ⋆ means\nmodel tends to diverge.\nArch. base Method C10 (%) C100 (%)\nConvolutional Pyramid-164-48 95.97 80.70\nWRN28-10 95.83 80.75\nTransformer\nfull-attention\nDeiT-T 88.39 67.52\nDeiT-S 92.44 69.78\nDeiT-B 92.41 70.49\nPVT-T 90.51 69.62\nPVT-S 92.34 69.79\nPVT-B 85.05⋆ 43.78⋆\nCCT-7/3×1 94.72 76.67\nTransformer\nlocal-attention\nSwin-T 94.46 78.07\nSwin-S 94.17 77.01\nSwin-B 94.55 78.45\nNesT-T 96.04 78.69\nNesT-S 96.97 81.70\nNesT-B 97.20 82.56\nrecursively from the root to a leaf node. The negative gradi-\nent −∂Yc\nAl\nprovides the gradient ascent direction to maximize\nthe class clogit, i.e., a higher positive value means higher\nimportance. Fig. 2 illustrates a sample result.\nExperiments\nWe ﬁrst show the beneﬁt of NesT for data efﬁcient learning\nand then demonstrate beneﬁts for interpretability and genera-\ntive modeling. Finally, we present ablation studies to analyze\nthe major constituents of the methods.\nExperimental setup. We follow previous work (Dosovitskiy\net al. 2021) to generate three architectures that have compa-\nrable capacity (in number of parameters and FLOPS), noted\nas tiny (NesT-T), small (NesT-S), and base (NesT-B). Most\nrecent ViT-based methods follow the training techniques of\nDeiT (Touvron et al. 2020). We follow the settings with minor\nmodiﬁcations that we ﬁnd useful for local self-attention (see\nAppendix for all architecture and training details). We do not\nexplore the speciﬁc per-block conﬁgurations (e.g. number\nof heads and hidden dimensions), which we believe can be\noptimized through architecture search (Tan and Le 2019).\nComparisons to Previous Work\nCIFAR. We compare NesT to recent methods on CIFAR\ndatasets (Krizhevsky, Hinton et al. 2009) in Table 1, to inves-\ntigate the data efﬁciency. It is known that transformer-based\nmethods usually perform poorly on such tasks as they typ-\nically require large datasets to be trained on. The models\nthat perform well on large-scale ImageNet do not necessary\nwork perform on small-scale CIFAR, as the full self-attention\nbased models require larger training datasets. DeiT (Touvron\net al. 2020) performs poorly and does not improve given\nTable 2: Comparison on the ImageNet dataset. All models are\ntrained from random initialization. ViT-B/16 uses an image\nsize 384 and others use 224.\nArch. base Method #Params Top-1 acc. (%)\nConvolutional\nResNet-50 25M 76.2\nRegNetY-4G 21M 80.0\nRegNetY-16G 84M 82.9\nTransformer\nfull-attention\nViT-B/16 86M 77.9\nDeiT-S 22M 79.8\nDeiT-B 86M 81.8\nTransformer\nlocal-attention\nSwin-T 29M 81.3\nSwin-S 50M 83.0\nSwin-B 88M 83.3\nNesT-T 17M 81.5\nNesT-S 38M 83.3\nNesT-B 68M 83.8\nTable 3: Comparison on ImageNet benchmark with\nImageNet-22K pre-training.\nViT-B/16 Swin-B Nest-B\nImageNet Acc. (%) 84.0 86.0 86.2\nbigger model size. PVT (Wang et al. 2021b) has also a full\nself-attention based design, though with a pyramid structure.\nPVT-T seems to perform better than DeiT-T when model\nsize is small, however, the performance largely drops and\nbecomes unstable when scaling up, further suggesting that\nfull self-attention at bottom layers is not desirable for data\nefﬁciency. Other transformer-based methods improve slowly\nwith increasing model size, suggesting that bigger models\nare more challenging to train with less data. We attribute this\nto to their complex design (i.e. shifted windows with masked\nMSA) requiring larger training datasets, while NesT ben-\neﬁting from a judiciously-designed block aggregation. We\nalso include comparisons with convolutional architectures\nthat are speciﬁcally optimized for small CIFAR images and\nshow that NesT can give better accuracy without any small\ndataset speciﬁc architecture optimizations (while still being\nlarger and slower, as they do not incorporate convolutional\ninductive biases). The learning capacity and performance of\nNesT get better with increased model size. Most variants of\nNesT in Fig. A1 of Appendix outperform compared methods\nwith far better throughput. E.g., NesT3-T (S = 2) leads to\n94.5% CIFAR10 accuracy with 5384 images/s throughout,\n10×faster than the best compared result 94.6% accuracy.\nMore details can be found in Appendix.\nImageNet. We test NesT on standard ImageNet 2012 bench-\nmarks (Deng et al. 2009) with commonly used 300 epoch\ntraining on TPUs in Table 2. The input size is 224 ×224\nand no extra pre-training data is used. DeiT does not use\nteacher distillation, so it can be viewed as ViT (Dosovitskiy\net al. 2021) with better data augmentation and regularization.\nNesT matches the performance of prior work with a signif-\nicantly more straightforward design (e.g. NesT-S matches\nthe accuracy of Swin-B, 83.3%). The results of NesT suggest\nthat correctly aggregating the local transformer can improve\nthe performance of local self-attention.\nImageNet-22K. We scale up NesT to ImageNet-22K fol-\nlowing the exact training schedules in (Liu et al. 2021;\nDosovitskiy et al. 2021). The pre-training is 90 epoch on\n224×224 ImageNet21K images and ﬁnetuning is 30 epoch\non 384×384 ImageNet images. Table 3 compares the results.\nNesT again achieves competitive results, with a signiﬁcantly\nmore straightforward design.\nVisual Interpretability\nGradGAT results. Fig. 3 (left) shows the explanations ob-\ntained with the proposed GradGAT. For GradGAT, each tree\nnode corresponds to a value that reﬂects the mean activation\nstrength. Visualizing the tree traversal through image blocks,\nwe can get insights about the decision making process of\nNesT. The traversal passes through the path with the highest\nvalues. As can be seen, the decision path can correctly locate\nthe object corresponding to the model prediction. The Lighter\nexample is particularly interesting because the ground truth\nclass – lighter/matchstick – actually deﬁnes the bottom-right\nmatchstick object, while the most salient visual features (with\nthe highest node values) are actually from the upper-left red\nlight, which conceptually shares visual cues with a lighter.\nThus, although the visual cue is a mistake, the output predic-\ntion is correct. This example reveals the potential of using\nGradGAT to conduct model diagnosis at different tree hierar-\nchies. Fig. A5 of Appendix shows more examples.\nClass attention map (CAM) results. In contrast to ViT\n(Dosovitskiy et al. 2021) which uses class tokens, NesT uses\nglobal average pooling before softmax. This enables con-\nveniently applying CAM-like (Zhou et al. 2016) methods\nto interpret how well learned representations measure ob-\nject features, as the activation coefﬁcients can be directly\nwithout approximate algorithms. Fig. 3(right) shows quan-\ntitative evaluation of weakly-supervised object localization,\nwhich is a common evaluation metric for CAM-based meth-\nods (Zhou et al. 2016), including GradCAM++ (Chattopad-\nhay et al. 2018) with ResNet50 (He et al. 2016), DeiT with\nRollout attention (Abnar and Zuidema 2020), and our NesT\nCAM (Zhou et al. 2016). We follow (Gildenblat 2021) in\nusing an improved version of Rollout. NesT with standard\nCAM, outperforms others that are speciﬁcally designed for\nthis task. Fig. 4 shows a qualitative comparison (details in\nthe Appendix), exemplifying that NesT can generate clearer\nattention maps which converge better on objects.\nOverall, decoupling local self-attention (transformer\nblocks) and global information selection (block aggregation),\nwhich is unique to our work, shows signiﬁcant potential for\nmaking models easier to interpret.\nGenerative Modeling with Transposed NesT\nWe evaluate the generative ability of Transposed NesT on\nImageNet (Russakovsky et al. 2015) where all images are\nresized to 64 ×64 resolution. We focus on the unconditional\nimage generation setting to test the effectiveness of different\ndecoders. We compare Transposed NesT to TransGAN (Jiang,\nChang, and Wang 2021), that uses a full Transformer as the\ngenerator, as well as a convolutional baseline following the\nLighter\nMouse\nradio telescope\nRadio telescope\nWooden spoon\nMethod\nTop-1 loc.\nerr (%)\nDeiT RollOut§ 67.6\nADL (Choe and Shim 2019) 55.1\nACoL (Zhang et al. 2018b) 54.2\nR50 GradCAM++§ 53.5\nNesT CAM 51.8\nFigure 3: Left: Output visualization of the proposed GradGAT. Tree nodes annotate the averaged responses to the predicted class.\nWe use a NesT-S with three tree hierarchies. Right: CAM-based weakly supervised localization comparison on the ImageNet\nvalidation set. §indicates results obtained by us.\nResNet50 GradCAM++ DeiT Rollout NesT CAMInput Image\nKing \npenguin\nHouse \nﬁnch\nBittern\nGround truth\nFigure 4: Visualization of CAM-based attention results. All models are trained on ImageNet. CAM (vanilla) with NesT achieves\naccurate attention patterns on object regions, yielding ﬁner attention to objects than DeiT Rollout (Abnar and Zuidema 2020)\nand less noise than ResNet50 GradCAM++ (Chattopadhay et al. 2018).\nwidely-used architecture from (Zhang et al. 2019) (its com-\nputationally expensive self-attention module is removed).\nFig. 5 shows the results. Transposed NesT obtains signiﬁ-\ncantly faster convergence and achieves the best FID and In-\nception score (see Fig. A6 of Appendix for results). Most im-\nportantly, it achieves 8×throughput over TransGAN, show-\ning its potential for signiﬁcantly improving the efﬁciency\nof transformer-based generative modeling. More details are\nexplained in the Appendix.\nIt is noticeable from Fig. 5 (middle) that appropriate un-\nsampling (or block de-aggregation) impacts the generation\nquality. Pixel shufﬂe (Shi et al. 2016) works the best and the\nmargin is considered surprisingly large compared to other al-\nternatives widely-used in convnets. This aligns with our main\nﬁndings in classiﬁcation, suggesting that judiciously injecting\nspatial operations is important for nested local transformers\nto perform well.\nAblation Studies\nWe summarize key ablations below (more in Appendix).\nFast convergence. NesT achieves fast convergence, as\nshown in Fig. 6 (top) for Imagenet training with\n{30,60,100,300}epochs. NesT-B merely loses 1.5% when\nreducing the training epoch from 300 to 100. The results\nsuggest that NesT can learn effective visual features faster\nand more efﬁciently.\nLess sensitivity to data augmentation. NesT uses several\nkinds of data augmentation following (Touvron et al. 2020).\nAs shown in Fig. 6 (right) and Fig. A4, our method shows\nhigher stability in data augmentation ablation studies com-\npared to DeiT. Data augmentation is critical for global self-\nattention to generalize well, but reduced dependence on do-\nmain or task dependent data augmentation helps with gener-\nalization to other tasks.\nImpact of block aggregation. Here we show that the de-\nsign of block aggregation is critical for performance and\ndata efﬁciency. We study this from four perspectives: (1)\nwhether unblocking to the full image plane is necessary;\n(2) how to use convolution; (3) what kinds of pooling to\nuse; and (4) whether to perform query down-sampling inside\n200 400 600 800 1000\nIterations (1000x)\n20\n30\n40\n50FID\nComparison of architectures\nConvNet\nTransGAN\nTransposed NesT\n200 400 600 800 1000\nIterations (1000x)\n20\n40\n60\n80\n100FID\nComparison of block de-aggregration\nPS-C3\nC3-PS\nNN-C3\nC3-NN (failed)\nPS (Ours)\nMethod\n#Params\n(millions)\nThroughput\n(images/s)\nConvnet\n(Zhang et al. 2019) 77.8M 709.1\nTransGAN\n(Jiang, Chang, and Wang 2021) 82.6M 67.7\nTransposed NesT 74.4M 523.7\nFigure 5: Left: FID comparison for 64 ×64 ImageNet generation at different training iterations. Middle: FID comparison of\ndifferent popular un-sampling methods for block de-aggregation, including combinations of pixel shufﬂing (PS), Conv3x3 (C3),\nand nearest neighbor (NN). Right: The number of parameters and throughput of compared generators.\n50 100 150 200 250 300\nTotal training epochs\n55\n60\n65\n70\n75\n80\n85ImageNet accuracy\nDeiT-S\nSwin-S\nNesT-S\nNesT-B\nRemoved\nAugmentation Accuracy (%)\nImageNet\nDeiT-B NesT-T\nNone 81.8 81.5\nRandomErasing 4.3 81.4\nRandAugment 79.6 81.2\nCutMix&MixUp 75.8 79.8\nFigure 6: Top: Training convergence. NesT achieves better\nperformance than DeiT with the same total epoch of training\n(each point is a single run). Bottom: Data augmentation abla-\ntions. Results of DeiT-B (Touvron et al. 2020) are reported\nby its paper. NesT shows less reliance on data augmentation.\nself-attention (Vaswani et al. 2021). Fig. 7 and Fig. A3 of\nAppendix compare the results of different plausible designs.\nThe results show that: (1) when performing these spatial\noperations, it is important to apply it on the holistic image\nplane versus the block plane although both can reasonably\nintroduce spatial priors; (2) small kernel convolution is sufﬁ-\ncient and has to be applied ahead of pooling; (3) max. pooling\nis far better than other options, such as stride-2 sub-sampling\nand average pooling; (4) sub-sampling the query sequence\nlength (similar to performing sub-sampling on the block\nplane as illustrated in Fig. A2), as used by Halonet (Vaswani\net al. 2021), performs poorly on data efﬁcient benchmarks.\nWe also experiment PatchMerge from Swin Transformer (Liu\net al. 2021) on both CIFAR and ImageNet. Our block aggre-\ngation closes the accuracy gap on ImageNet, suggesting that\na conceptually negligible difference in aggregating nested\ntransformers can lead to signiﬁcant differences in model per-\nformance.\nOurs Conv3x3 PatchMergeAvgPool3x3\n82\n84\n86\n88\n90\n92\n94\n96Accuracy\nCIFAR10\nImage plane\nBlock plane\nOurs Conv3x3 PatchMergeAvgPool3x3\n55\n60\n65\n70\n75\n80\n CIFAR100\nImage plane\nBlock plane\nOurs Conv3x3 PatchMergeAvgPool3x3\n79.8\n80.0\n80.2\n80.5\n80.8\n81.0\n81.2\n81.5\nImageNet\nImage plane\nBlock plane\nFigure 7: Demonstration of the impact of block aggrega-\ntion on CIFAR and ImageNet. NesT-T is used. Conv3x3 has\nstride 2. AvgPool3x3 on ImageNet is followed by Conv1x1\nto change hidden dimensions of self-attention. Four plausible\nblock aggregation designs are shown in x-axis, and applied\non the image plane and block plane both for comparison.\nNote that Ours in x-axis is Conv3x3 followed by LN and\nMaxPool3x3 (stride 2). More alternatives are validated in\nFig. A3 of Appendix.\nConclusion\nWe have shown that aggregating nested transformers can\nmatch the accuracy of previous more complex methods with\nsigniﬁcantly improved data efﬁciency and convergence speed.\nIn addition, we have shown that this idea can be extended to\nimage generation, where it provides signiﬁcant speed gains.\nFinally, we have shown that the decoupled feature learning\nand feature information extraction in this nested hierarchy\ndesign allows for better feature interpretability through a new\ngradient-based class-aware tree traversal method. In future\nwork we plan to generalize this idea to non-image domains.\nReferences\nAbnar, S.; and Zuidema, W. 2020. Quantifying attention ﬂow\nin transformers. arXiv preprint arXiv:2005.00928. 2, 5, 6, 12\nAkbari, H.; Yuan, L.; Qian, R.; Chuang, W.-H.; Chang, S.-F.;\nCui, Y .; and Gong, B. 2021. V ATT: Transformers for Mul-\ntimodal Self-Supervised Learning from Raw Video, Audio\nand Text. arXiv preprint arXiv:2104.11178. 2\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer normal-\nization. arXiv preprint arXiv:1607.06450. 3\nBello, I. 2021. Lambdanetworks: Modeling long-range inter-\nactions without attention. arXiv preprint arXiv:2102.08602.\n3\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150. 1\nChattopadhay, A.; Sarkar, A.; Howlader, P.; and Balasub-\nramanian, V . N. 2018. Grad-cam++: Generalized gradient-\nbased visual explanations for deep convolutional networks.\nIn WACV. 5, 6, 12\nChefer, H.; Gur, S.; and Wolf, L. 2021. Generic Attention-\nmodel Explainability for Interpreting Bi-Modal and Encoder-\nDecoder Transformers. arXiv preprint arXiv:2103.15679.\n2\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.\nA simple framework for contrastive learning of visual repre-\nsentations. In ICML. 10\nChen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-\nton, G. 2020b. Big self-supervised models are strong semi-\nsupervised learners. NeurIPS. 10\nChen, Z.; Xie, L.; Niu, J.; Liu, X.; Wei, L.; and Tian, Q. 2021.\nVisformer: The Vision-friendly Transformer. arXiv preprint\narXiv:2104.12533. 2\nChild, R.; Gray, S.; Radford, A.; and Sutskever, I. 2019.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509. 2\nChoe, J.; and Shim, H. 2019. Attention-based dropout layer\nfor weakly supervised object localization. In CVPR. 6\nCordonnier, J.-B.; Loukas, A.; and Jaggi, M. 2020. On the\nrelationship between self-attention and convolutional layers.\nICLR. 1, 2\nCubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-\ndaugment: Practical automated data augmentation with a\nreduced search space. In CVPR Workshops. 10\nDai, J.; Qi, H.; Xiong, Y .; Li, Y .; Zhang, G.; Hu, H.; and Wei,\nY . 2017. Deformable convolutional networks. InICCV. 1\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei,\nL. 2009. Imagenet: A large-scale hierarchical image database.\nIn CVPR. 5\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2021. An image is worth 16x16\nwords: Transformers for image recognition at scale. ICLR. 1,\n2, 4, 5, 10, 12\nGildenblat, J. 2021. Exploring Explainability for Vision\nTransformers. 5, 12\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial networks. NeurIPS. 1, 4\nHan, D.; Kim, J.; and Kim, J. 2017. Deep pyramidal residual\nnetworks. In CVPR. 12\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y . 2021.\nTransformer in transformer.arXiv preprint arXiv:2103.00112.\n1\nHassani, A.; Walton, S.; Shah, N.; Abuduweili, A.; Li, J.; and\nShi, H. 2021. Escaping the Big Data Paradigm with Compact\nTransformers. arXiv preprint arXiv:2104.05704. 2, 10, 12\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR. 1, 3, 5, 10, 12\nHeusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and\nHochreiter, S. 2017. GANs trained by a two time-scale update\nrule converge to a local nash equilibrium. In NeurIPS. 11\nHoffer, E.; Ben-Nun, T.; Hubara, I.; Giladi, N.; Hoeﬂer, T.;\nand Soudry, D. 2020. Augment Your Batch: Improving Gen-\neralization Through Instance Repetition. In CVPR. 10\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger, K. Q.\n2016. Deep networks with stochastic depth. In ECCV. 10\nHudson, D. A.; and Zitnick, C. L. 2021. Generative Ad-\nversarial Transformers. arXiv preprint arXiv:2103.01209.\n2\nJiang, Y .; Chang, S.; and Wang, Z. 2021. Transgan: Two\ntransformers can make one strong gan. arXiv preprint\narXiv:2102.07074. 2, 4, 5, 7\nKarras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.;\nand Aila, T. 2020. Analyzing and improving the image quality\nof stylegan. In CVPR. 11\nKim, B.; Lee, J.; Kang, J.; Kim, E.-S.; and Kim, H. J. 2021.\nHOTR: End-to-End Human-Object Interaction Detection\nwith Transformers. CVPR. 2\nKingma, D. P.; and Ba, J. 2014. Adam: A method for stochas-\ntic optimization. In ICLR. 11\nKrizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple\nlayers of features from tiny images. 4\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. LocalViT: Bringing Locality to Vision Transformers.\narXiv preprint arXiv:2104.05707. 2\nLiang, J.; Homayounfar, N.; Ma, W.-C.; Xiong, Y .; Hu, R.;\nand Urtasun, R. 2020. Polytransform: Deep polygon trans-\nformer for instance segmentation. In CVPR. 2\nLiu, Y .; and Lapata, M. 2019. Hierarchical transform-\ners for multi-document summarization. arXiv preprint\narXiv:1905.13164. 2\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030. 1, 2, 3, 5, 7, 10, 12, 13\nLoshchilov, I.; and Hutter, F. 2018. Fixing weight decay\nregularization in adam. 10\nMescheder, L.; Geiger, A.; and Nowozin, S. 2018. Which\ntraining methods for GANs do actually converge? In ICML.\n11\nNeimark, D.; Bar, O.; Zohar, M.; and Asselmann, D. 2021.\nVideo transformer network.arXiv preprint arXiv:2102.00719.\n2\nPappagari, R.; Zelasko, P.; Villalba, J.; Carmiel, Y .; and De-\nhak, N. 2019. Hierarchical transformers for long document\nclassiﬁcation. In Automatic Speech Recognition and Under-\nstanding Workshop (ASRU), 838–844. 2\nParmar, N.; Vaswani, A.; Uszkoreit, J.; Kaiser, L.; Shazeer,\nN.; Ku, A.; and Tran, D. 2018. Image transformer. In ICML.\n2\nRadosavovic, I.; Kosaraju, R. P.; Girshick, R.; He, K.; and\nDollár, P. 2020. Designing network design spaces. In CVPR.\n12\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Lev-\nskaya, A.; and Shlens, J. 2019. Stand-alone self-attention in\nvision models. NeurIPS. 2\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. ImageNet large scale visual recognition challenge.\nIJCV. 5\nSantra, B.; Anusha, P.; and Goyal, P. 2020. Hierarchical\ntransformer for task oriented dialog systems. arXiv preprint\narXiv:2011.08067. 2\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual expla-\nnations from deep networks via gradient-based localization.\nIn ICCV. 4, 12\nShi, W.; Caballero, J.; Huszár, F.; Totz, J.; Aitken, A. P.;\nBishop, R.; Rueckert, D.; and Wang, Z. 2016. Real-time\nsingle image and video super-resolution using an efﬁcient\nsub-pixel convolutional neural network. In CVPR. 4, 6, 11\nSohn, K.; Berthelot, D.; Li, C.-L.; Zhang, Z.; Carlini, N.;\nCubuk, E. D.; Kurakin, A.; Zhang, H.; and Raffel, C. 2020.\nFixmatch: Simplifying semi-supervised learning with consis-\ntency and conﬁdence. NeurIPS. 1\nSrinivas, A.; Lin, T.-Y .; Parmar, N.; Shlens, J.; Abbeel, P.;\nand Vaswani, A. 2021. Bottleneck transformers for visual\nrecognition. arXiv preprint arXiv:2101.11605. 2\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nattribution for deep networks. In ICML. 4\nTan, M.; and Le, Q. 2019. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. In ICML. 1, 4\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and Jégou, H. 2020. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877. 2, 3, 4, 6, 7, 10, 12\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJégou, H. 2021. Going deeper with image transformers.arXiv\npreprint arXiv:2103.17239. 2\nVaswani, A.; Ramachandran, P.; Srinivas, A.; Parmar, N.;\nHechtman, B.; and Shlens, J. 2021. Scaling Local Self-\nAttention For Parameter Efﬁcient Visual Backbones. CVPR.\n1, 2, 3, 7, 13\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nis all you need. NeurIPS. 3\nWang, H.; Zhu, Y .; Adam, H.; Yuille, A.; and Chen, L.-C.\n2021a. MaX-DeepLab: End-to-End Panoptic Segmentation\nwith Mask Transformers. CVPR. 2\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021b. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122. 2, 3, 5, 12\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. arXiv preprint arXiv:2103.15808. 3\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.;\nTay, F. E.; Feng, J.; and Yan, S. 2021. Tokens-to-token vit:\nTraining vision transformers from scratch on imagenet.arXiv\npreprint arXiv:2101.11986. 2, 3\nYun, S.; Han, D.; Oh, S. J.; Chun, S.; Choe, J.; and Yoo,\nY . 2019. Cutmix: Regularization strategy to train strong\nclassiﬁers with localizable features. In ICCV. 10\nZagoruyko, S.; and Komodakis, N. 2016. Wide residual\nnetworks. BMVC. 12\nZhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.\n2018a. mixup: Beyond empirical risk minimization. ICLR.\n10\nZhang, H.; Goodfellow, I.; Metaxas, D.; and Odena, A. 2019.\nSelf-attention generative adversarial networks. In ICML. 1,\n4, 6, 7\nZhang, H.; Koh, J. Y .; Baldridge, J.; Lee, H.; and Yang, Y .\n2021a. Cross-Modal Contrastive Learning for Text-to-Image\nGeneration. In CVPR. 11\nZhang, H.; Zhang, Z.; Odena, A.; and Lee, H. 2020. Con-\nsistency regularization for generative adversarial networks.\nICLR. 11\nZhang, P.; Dai, X.; Yang, J.; Xiao, B.; Yuan, L.; Zhang, L.;\nand Gao, J. 2021b. Multi-Scale Vision Longformer: A New\nVision Transformer for High-Resolution Image Encoding.\narXiv preprint arXiv:2103.15358. 2, 3\nZhang, X.; Wei, F.; and Zhou, M. 2019. HIBERT: Docu-\nment level pre-training of hierarchical bidirectional trans-\nformers for document summarization. arXiv preprint\narXiv:1905.06566. 2\nZhang, X.; Wei, Y .; Feng, J.; Yang, Y .; and Huang, T. S.\n2018b. Adversarial complementary learning for weakly su-\npervised object localization. In CVPR. 6\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. In AAAI. 10\nZhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba,\nA. 2016. Learning deep features for discriminative localiza-\ntion. In CVPR. 5, 12\nAppendix\nWe provide more experimental results below to complete the\nexperimental sections of the main paper.\nNesT Architecture and Training Details\nArchitecture conﬁguration. The focus on this paper is how\nto aggregating nested transformers and its extended usage.\nWe do not focus on the speciﬁc per-block hyper-parameters\n(e.g. number of heads and number of MSA layers). We mainly\nfollow previous work to obtain right architectures that has\nsimilar capacity (e.g. number of parameters and throughput).\nRecall that the overall hierarchy can be determined by two\nkey hyper-parameters: patch size S×Sand hierarchy depth\nTd. Just like how ResNet (He et al. 2016) adapts to small and\nlarge input sizes, NesT also has different conﬁguration for\nsmall input size and large input size. We follow (Touvron\net al. 2020; Liu et al. 2021) to conﬁgure the number of head,\nhidden dimensions for the tiny, small and base versions. For\n32 ×32 image size, we follow (Touvron et al. 2020). Specif-\nically, we setup the same number of repeated MSANesT per\nblock hierarchy. In each hierarchy, the number of hidden di-\nmensions and the number of heads are the same as well. For\n224 ×224 image size, we follow (Liu et al. 2021). Therefore,\ndifferent hierarchy has a gradually increased number of head,\nhidden dimensions, and number of repeated MSANesT layers.\nTable A1 speciﬁes details.\nNesT Hierarchy Variants\nWe study ﬂexible variants to understand how the hierarchical\nstructure of NesT impacts accuracy. When increasing the\nhierarchy depth by one, every four blocks are splitted to\nprocess four image partitions (see Figure 1 of the main paper).\nA deeper hierarchy structure makes each block focus on a\nnarrower range of pixel information (i.e., shorter sequence\nlength).\nWe test combinations with S = {1,2} and\ndepth={2,3,4,5} on NesT-{T, S, B}. Figure A1 com-\npares different variants on two CIFAR datasets. Shallower\nNesT has clear accuracy drop (although the total number\nof self-attention layers are the same). It is because, when\ndepth= 1, the model degenerates to a global self-attention\nmethod, such as ViT (Dosovitskiy et al. 2021). Depth = 5\nhas marginal decrease, because the sequence length of all\nblocks is only n = 2×2. Note that we denote NesT4-B,\nS = 1as the base NesT with hierarchy depth 4 and word\npatch size 1 ×1. We use the conﬁguration NesT4,S = 1as\nthe default for the most CIFAR experiments (subscription is\nomitted sometimes).\nData Augmentation. We apply the commonly used data aug-\nmentation and regularization techniques as (Touvron et al.\n2020), which include a mixture of data augmentation (MixUp\n(Zhang et al. 2018a), CutMix (Yun et al. 2019), RandAug-\nment (Cubuk et al. 2020), RandomErasing (Zhong et al.\n2020)) and regularization like Stochastic Depth (Huang et al.\n2016). Repeated augmentation (Hoffer et al. 2020) in DeiT is\nnot used. In addition, for ImageNet models, we also add color\njittering similar to (Chen et al. 2020a,b) which seems to re-\nduce dependency on local texture cues and slightly improves\ngeneralization (∼0.3% on ImageNet).\nTraining details. We use a base learning rate 2.5e-6 per de-\nvice. We use the AdamW optimizer (Loshchilov and Hutter\n2018) and set the weight decay 0.05. The warm-up epoch\nis 20. The initial learning rate is linearly scaled by a factor\nof total_batch_size/256. For ImageNet training, the total\nbatch size can be 1024 or 2048 when using distributed train-\ning on the TPU hardware. We use [0.2,0.3,0.5] stochastic\ndeath rates for NesT-T, NesT-S, and NesT-B models, respec-\ntively. All transformer layer weights and block aggregation\nweights are initialized using truncated normal distribution.\nWe use a 0.1 stochastic depth drop rate for all CIFAR\nmodels and the warmup is 5 epoch. The CIFAR results of\ncompared transformer based methods, besides CCT-7/3×1\n(Hassani et al. 2021), in Table 1 of the main paper are trained\nby us. We train these models using their suggested hyper-\nparameters and we ﬁnd it works nearly optimal on CIFAR\nby searching from a set of learning rate and weight decay\ncombinations.\nImpact of Block Aggregation\nFigure A3 shows detailed studies of different block aggre-\ngation functions to complete results in Figure 7 of the main\npaper. Although many of them has tiny difference, it is in-\nteresting that the impact to performance is non-trivial. In\naddition, we ﬁnd perform query down-sampling inside self\nattention makes transformers more difﬁcult to train because\nthe skip connection also needs proper down-sampling.\nAblation Studies\nData augmentation sensitivity. NesT uses several kinds\nof data augmentation types following DeiT (Touvron et al.\n2020). As shown in Figure A4, our method shows the high sta-\nbility in augmentation ablation studies compared with DeiT.\nWe speculate the underline reason is that learning effective\nvision cues is much easier in local attention than in global\nattention, so Swin Transformer also shows comparable sta-\nbility. More comparison on ImageNet will be left as future\nwork.\nWeak teacher distillation.We also explore the teacher distil-\nlation proposed by (Touvron et al. 2020), which suggests the\ninductive bias introduced by convnet teachers are helpful for\nViT data efﬁciency. Table A4 provides detailed distillation\nstudy on CIFAR datasets. With such a weak teacher distilla-\ntion, NesT-B is able to achieve 84.9% CIFAR100 accuracy\nwith 300 epochs and even 86.1% with 1000 epoch training\nusing 2 GPUs.\nConvnet teacher distillation (Touvron et al. 2020) is ef-\nfective to further improve our method as well as DeiT. As\nexplained in (Touvron et al. 2020), the inductive biases of\nconvnets have positive implicit effects to the transformer\ntraining. Because data augmentation can improve teacher per-\nformance, we question the inductive biases brought by data\naugmentation is useful or not. Based on our experiments, it\nseems data augmentation negatively affect the effectiveness\nof teacher distillation. If the teacher and target model are both\ntrained with strong augmentation, the performance decreases\neither for a small teacher or a big teacher. In other words,\nour study suggests that training a high accuracy teacher us-\ning strong augmentation negatively impact the distillation\nTable A1: Architecture details of NesT. In each block, the structure is deﬁned using the protocol [d,h] ×a,b, where [d,h] refers\nto [hidden dimensions, number of heads]; arefers to the number of repeated transformer layers V in Equation 1 of the main\npaper; brefers to the total number of blocks in that hierarchy. Tiny, Small, and Base models have different setup and they are\nspeciﬁed below. Note that once the hierarchy is ﬁxed. The sequence length of all blocks are consistent. Conﬁgurations of each\nblock for CIFAR and ImageNet are different.\nInput size Seq.\nlength\nNesT Hierarchy (Froward direction is 5 to 1)\n1 2 3 4 5\n32 ×32\nS = 1\nd= [192,384,768] and h= [3,6,12] for model T, S, and B\n8 ×8 [ d,h] ×4,1 [ d,h] ×4,4 [ d,h] ×4,16 - -\n4 ×4 [ d,h] ×3,1 [ d,h] ×3,4 [ d,h] ×3,16 [ d,h] ×3,64 -\n2 ×2 [ d,h] ×2,1 [ d,h] ×2,4 [ d,h] ×2,16 [ d,h] ×2,64 [d,h] ×2,256\n224 ×224\nS = 4\nd= [96,96,128], h= [3,6,12], and k= [8,20,20] for model T, S, and B\n14 ×14 [d,h] ×2,1 [2d,2h] ×2,4 [4d,4h] ×k,16 - -\n200 500 1000 3000 6000\n76\n78\n80\n82CIFAR100 Accuracy\nDepth=2\nDepth=3\nDepth=4\nDepth=5\nNesT-T, S=1\nNesT-S, S=1\nNesT-B, S=1\nNesT-T, S=2\nNesT-S, S=2\nNesT-B, S=2\n200 500 1000 3000 6000\nThroughput (images / s)\n94\n95\n96\n97CIFAR10 Accuracy\nDepth\nNode seq.\nlength n\nS = 1 S = 2\n2 256 64\n3 64 16\n4 16 4\n5 4 -\nFigure A1: Comparison of NesT hierarchy variants with different depth, word size S×S, and model size. The right table\nspeciﬁes the resulting sequence length given hierarchy depth and Scombinations.\neffectiveness. Future veriﬁcation on ImageNet will be left for\nfuture work.\nNumber of heads. We realize different architecture design\nuses different number of heads for MSA. We attempt to\nunderstand the effectiveness of the different conﬁgurations.\nWe experiment number of head from 1 to 96 given a ﬁxed\nd= 768hidden dimension using NesT4-B. Table A5 shows\nCIFAR10 results on NesT. It is interesting ﬁnd the number\nof heads affects less to the ﬁnal performance.\nGenerative Modeling\nNesT can become a decoder with minor changes. For fair\ncomparison in terms of model capacity, we conﬁgure NesT\nfollowing the architecture design of TransGAN for image\ngeneration. Table A6 speciﬁes the architectural details. The\nblock aggregation layer is swapped to a block de-aggregation\nlayer to achieve the gradually increased image size. Pixel\nshufﬂe (PS) (Shi et al. 2016) is leveraged to increase the\nimage size at block de-aggregation by a factor of two while\nthe hidden dimension is reduced to a quarter of the input.\nWe adopt the same discriminator architecture as (Kar-\nras et al. 2020) where R1 gradient penalty (Mescheder,\nGeiger, and Nowozin 2018) is applied during training.\nAdam (Kingma and Ba 2014) is utilized for optimization with\nβ1 = 0and β2 = 0.99. The learning rate is 0.0001 for both\nthe generator and discriminator with mini-batches of size\n256. We use Fréchet Inception Distance (FID) (Heusel et al.\n2017) for assessing image quality, which has been shown to\nbe consistent with human judgments of realism (Heusel et al.\n2017; Zhang et al. 2020, 2021a). Lower FID values indicate\ncloser distances between synthetic and real data distributions.\nFigure A6 shows the inception score of different compared\nmethods on 64 ×64 image generation.\nInterpretabilty\nGradGAT results. GradGAT always traverses from the root\nnode to one of the leaf node. Since each node of the bottom\nlayer only corresponds to a small non-overlapping patch of\nTable A2: Test accuracy on CIFAR with input size32×32. Compared convnets are optimized models for CIFAR. All transformer\nbased models are trained from random initialization with the same data augmentation. The number of parameters (millions),\nGPU memory (MB), and inference throughput (images/s) on single GPU are compared. We minimize the word size S×Sfor\neach transformer based method. DeiT uses S = 2. Swin and our NesT uses S = 1. ⋆ means models tends to diverge.\nArch. base Method #Params GPU Throughput C10 (%) C100 (%)\nConvnet Pyramid-164-48 (Han, Kim, and Kim 2017) 1.7M 126M 3715.9 95.97 80.70\nWRN28-10 (Zagoruyko and Komodakis 2016) 36.5M 202M 1510.8 95.83 80.75\nTransformer\nfull-attention\nDeiT-T (Touvron et al. 2020) 5.3M 158M 1905.3 88.39 67.52\nDeiT-S (Touvron et al. 2020) 21.3M 356M 734.7 92.44 69.78\nDeiT-B (Touvron et al. 2020) 85.1M 873M 233.7 92.41 70.49\nPVT-T (Wang et al. 2021b) 12.8M 266M 1478.1 90.51 69.62\nPVT-S (Wang et al. 2021b) 24.1M 477M 707.2 92.34 69.79\nPVT-B (Wang et al. 2021b) 60.9M 990M 315.1 85.05⋆ 43.78⋆\nCCT-7/3×1 (Hassani et al. 2021) 3.7M 94M 3040.2 94.72 76.67\nTransformer\nlocal-attention\nSwin-T (Liu et al. 2021) 27.5M 183M 2399.2 94.46 78.07\nSwin-S (Liu et al. 2021) 48.8M 311M 1372.5 94.17 77.01\nSwin-B (Liu et al. 2021) 86.7M 497M 868.3 94.55 78.45\nNesT-T 6.2M 187M 1616.9 96.04 78.69\nNesT-S 23.4M 411M 627.9 96.97 81.70\nNesT-B 90.1M 984M 189.8 97.20 82.56\nTable A3: Comparison on the ImageNet benchmark. The number of parameters (millions), GFLOPS, and inference throughput\n(images/s) evaluated on a single GPU are also compared. All models are trained from random initialization without extra\npre-training.\nArch. base Method Size #Params GFLOPS Throughput Top-1 acc. (%)\nConvnet\nResNet-50 (He et al. 2016) 224 25M 3.9G 1226.1 76.2\nRegNetY-4G (Radosavovic et al. 2020) 224 21M 4.0G 1156.7 80.0\nRegNetY-16G (Radosavovic et al. 2020) 224 84M 16.0G 334.7 82.9\nTransformer\nfull-attention\nViT-B/16 (Dosovitskiy et al. 2021) 384 86M 55.4G 85.9 77.9\nDeiT-S (Touvron et al. 2020) 224 22M 4.6G 940.4 79.8\nDeiT-B (Touvron et al. 2020) 224 86M 17.5G 292.3 81.8\nTransformer\nlocal-attention\nSwin-T (Liu et al. 2021) 224 29M 4.5G 755.2 81.3\nSwin-S (Liu et al. 2021) 224 50M 8.7G 436.9 83.0\nSwin-B (Liu et al. 2021) 224 88M 15.4G 278.1 83.3\nNesT-T 224 17M 5.8G 633.9 81.5\nNesT-S 224 38M 10.4G 374.5 83.3\nNesT-B 224 68M 17.9G 235.8 83.8\nthe whole image, visualizing GradCAT is less meaningful\nwhen the targeting object is large and centered. We ﬁnd it is\ntrue for the majority of ImageNet images although we ﬁnd\nour results fairly promising for most ImageNet images that\nhave small objects. Exploring more comprehensive studies\non image datasets with non-centered objects is be left for\nfuture work.\nThe proposed GradCAT is partially inspired by how Grad-\nCAM (Selvaraju et al. 2017) in convnets uses gradient infor-\nmation to improve visual attention. Nevertheless, the actual\ndetailed design and serving purposes are distinct.\nClass attention map results. Figure 4 of the main paper\ncompares the qualitative results of CAM, including Grad-\nCAM++ (Chattopadhay et al. 2018) with ResNet50 (He et al.\n2016), DeiT with Rollout attention (Abnar and Zuidema\n2020), and our NesT CAM (Zhou et al. 2016). We follow\n(Gildenblat 2021) to use an improved version of Rollout,\nwhich is better than the original version. When converting\nCAM generated by different methods to bounding boxes, the\nbest threshold of each method varies. We search the best\nthreshold [0, 1] using 0.05 as the interval to ﬁnd the best\nnumber for each method on the ImageNet 50k validation set.\nIt is promising to ﬁnd that NesT CAM can outperform meth-\nods for this task and our baselines. We only use the single\nforward to obtain bounding boxes.\n3x3 Conv + Norm\nOption 1: Block plane Option 2: Image plane\n: 1x1 Padding\n4 2\n10 7\n9\n20\n12 3 8\n        20\nZoomed view \n3x3 \nstride-2 \nMaxPool\n16 blocks from 16 local \ntransformers\n4 blocks to 4 local \ntransformers\nBlock aggregation : 3x3 sliding window kernel\nOutput sequences from local \ntransformers at hierarchy L\nInput sequences to local \ntransformers at hierarchy L+1\nUnblock to images Block to \nsequences\nFigure A2: Illustration of block aggregation and a comparison when applying to the block plane versus on the image plane.\nAlthough both perform convolution and pooling spatially, performing block aggregation on the image plane allows information\ncommunication among blocks (different color palettes) that belong to different merged blocks at the upper hierarchy.\nTable A4: Teacher distillation studies on CIFAR datasets. Left: The top two rows of the table are teacher supervised accuracy\non CIFAR100. The bottom two rows show accuracy using these trained teachers with standard or strong augmentation. Right:\nTeacher (PN-164-48 with standard augmentation) distillation effects on DeiT and the proposed NesT. DeiT and NesT are always\ntrained with strong augmentation.\nTeacher Target model Standard Strong\n- PN-164-48 80.7 81.5\n- PN-164-270 83.4 84.9\nPN-164-48 NesT-B 84.5 83.7\nPN-164-270 NesT-B 84.9 83.8\nDistillation \u0017 \u0013\nDataset C10 C100 C10 C100\nDeiT-B 92.4 70.5 95.5 81.5\nNesT-B 97.2 82.6 97.1 84.5\nNotation of\nspatial operations\nC3: 3 ×3 Conv\nLN: LayerNorm\nMX3: 3 ×3 MaxPool\nS2: 2 ×2 sub-sampling\nA VG3:3 ×3 AvgPool\nD4: 4 ×1 1D Conv\nPM: Patch merge\nC3-LN-MX3PC3-LN-MX3 C3-MX3 C5-LN-MX5 MX3-C3-LN C1-LN-MX3\n60\n70\n80\n90\n100CIFAR accuracy\nLearnable kernels (outside MSA)\nC10 (image-plane)\nC10 (node-plane)\nC100 (image-plane)\nC100 (node-plane)\nC3-LN-MX3\nC3-S2\nC3-LN-AVG3\nC3 PM MX3 AVG3 S2\n60\n70\n80\n90\n100CIFAR accuracy\nPooling operations (outside MSA)\nC10 (image-plane)\nC10 (block-plane)\nC100 (image-plane)\nC100 (block-plane)\nC3-LN-MX3 C3-MX3 D4-MX4\nS2\n60\n65\n70\n75\n80\n85\n90\n95CIFAR accuracy\nQuery sub-sampling (inside MSA)\nC10 (query)\nC100 (query)\nC3-LN-MX3\nC3 AVG3 PM\n79.8\n80.0\n80.2\n80.5\n80.8\n81.0\n81.2\n81.5ImageNet accuracy\nSelected combinations (outside MSA)\nImageNet (image-plane)\nImageNet (block-plane)\nFigure A3: Study the impact of block aggregation on CIFAR and ImageNet. NesT-T is used. We study from different perspectives\nas explained in the text of the main paper. We verify ImageNet with NesT-T in the bottom-right ﬁgure using a subset of\nrepresentative block aggregation options found on CIFAR datasets. Patch merge (Liu et al. 2021) and 2x2 sub-sampling (Vaswani\net al. 2021) are used by previous methods. Since NesT for ImageNet has different hidden dimensions at different hierarchies,\nA VG3 on ImageNet is followed by a 1x1 convolution to map hidden dimensions. The chosen combinations are speciﬁed in\nx-axis. The leftmost x-axis point (C3-LN-MX3) of each ﬁgure is ours.\nNone Erasing RA Mixup CutMix\nRemove (individually)\n91\n92\n93\n94\n95\n96\n97CIFAR10 accuracy\nDeiT-S\nSwin-T\nNesT-S\nNone RA CutMix&MixUp Erasing\nRemove (consecutively)\n84\n86\n88\n90\n92\n94\n96CIFAR10 accuracy\nDeiT-S\nSwin-T\nNesT-S\nFigure A4: Data augmentation ablation studies on CIFAR10, either removing augmentation individually (middle) or removing\n(from left to right of x-axis) consecutively (right). None means all are used.\nTable A5: Study the impact of number of heads in MSA on the CIFAR10 dataset withNesT4-B. When #head=96, the hidden\ndimension used for computing Attention is only 8. However, it can still lead to similar accuracy.\n#head in MSA 1 2 3 6 12 24 48 96\nHidden dimension d 768 384 256 128 64 32 16 8\nAccuracy 97.1 96.85 96.92 97.07 97.21 97.01 97.03 97.08\nTricycle\nSafety pin\nLighter\nDough\nFigure A5: More output visualization of the proposed GradGAT.\nTable A6: Architecture details of NesT as image generator. d= 1024and h= 4. The input is a reshaped noise vector. At the last\nhierarchy, there are 64 image blocks. Since the sequence length is 8 ×8, it is easy to see that the output image size is 64 ×64. At\nhierarchy 1, the hidden dimension is 1024/64 = 16. Then a LayerNorm followed by Conv1x1 maps the hidden dimension to the\noutput with shape 64 ×64 ×3.\nInput size Seq.\nlength\nNesT Hierarchy (Froward direction is 4 to 1)\n1 2 3 4\n1 ×n×d 8 ×8 [ d/64,h] ×2,64 [ d/16,h] ×3,16 [ d/4,h] ×3,4 [ d,h] ×5,1\n200 400 600 800 1000\nIterations (1000x)\n10\n11\n12\n13\n14\n15Inception score\nComparison of architectures\nConvNet\nTransGAN\nTransposed NesT\n200 400 600 800 1000\nIterations (1000x)\n5.0\n7.5\n10.0\n12.5\n15.0Inception score\nComparison of block de-aggregration\nPS-C3\nC3-PS\nNN-C3\nC3-NN (failed)\nPS (Ours)\nFigure A6: Left: Inception score of 64 ×64 ImageNet generation of different architectures. Right: Inception score with different\nun-pooling options. The models used to report the results are the same models in Figure 5 of the main paper.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7922064065933228
    },
    {
      "name": "Transformer",
      "score": 0.7116196155548096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47984644770622253
    },
    {
      "name": "Architecture",
      "score": 0.464758038520813
    },
    {
      "name": "Code (set theory)",
      "score": 0.4137512445449829
    },
    {
      "name": "Computer engineering",
      "score": 0.3863498568534851
    },
    {
      "name": "Machine learning",
      "score": 0.3281252384185791
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32241931557655334
    },
    {
      "name": "Engineering",
      "score": 0.08606955409049988
    },
    {
      "name": "Electrical engineering",
      "score": 0.07540881633758545
    },
    {
      "name": "Programming language",
      "score": 0.07277610898017883
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": []
}