{
  "title": "Autonomous chemical research with large language models",
  "url": "https://openalex.org/W4389991792",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5065327102",
      "name": "Daniil A. Boiko",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5060793099",
      "name": "Robert MacKnight",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5004653421",
      "name": "Ben Kline",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5048633127",
      "name": "Gabriel dos Passos Gomes",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6811129797",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W3214740101",
    "https://openalex.org/W4387694367",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W6767858076",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2830440988",
    "https://openalex.org/W3211377821",
    "https://openalex.org/W4307410169",
    "https://openalex.org/W2334592091",
    "https://openalex.org/W2968071222",
    "https://openalex.org/W3042021489",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W6847753483",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3083669401",
    "https://openalex.org/W3180230246",
    "https://openalex.org/W3023843058",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W1991678929",
    "https://openalex.org/W4311172319",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2784918212",
    "https://openalex.org/W2785942661",
    "https://openalex.org/W4386465483",
    "https://openalex.org/W4396723768"
  ],
  "abstract": "Abstract Transformer-based large language models are making significant strides in various fields, such as natural language processing 1–5 , biology 6,7 , chemistry 8–10 and computer programming 11,12 . Here, we show the development and capabilities of Coscientist, an artificial intelligence system driven by GPT-4 that autonomously designs, plans and performs complex experiments by incorporating large language models empowered by tools such as internet and documentation search, code execution and experimental automation. Coscientist showcases its potential for accelerating research across six diverse tasks, including the successful reaction optimization of palladium-catalysed cross-couplings, while exhibiting advanced capabilities for (semi-)autonomous experimental design and execution. Our findings demonstrate the versatility, efficacy and explainability of artificial intelligence systems like Coscientist in advancing research.",
  "full_text": "570 | Nature | Vol 624 | 21/28 December 2023\nArticle\nAutonomous chemical research with large \nlanguage models\nDaniil A. Boiko1, Robert MacKnight1, Ben Kline2 & Gabe Gomes1,3,4 ✉\nTransformer-based large language models are making significant strides in various \nfields, such as natural language processing1–5, biology6,7, chemistry8–10 and computer \nprogramming11,12. Here, we show the development and capabilities of Coscientist, an \nartificial intelligence system driven by GPT-4 that autonomously designs, plans and \nperforms complex experiments by incorporating large language models empowered \nby tools such as internet and documentation search, code execution and experimental \nautomation. Coscientist showcases its potential for accelerating research across six \ndiverse tasks, including the successful reaction optimization of palladium-catalysed \ncross-couplings, while exhibiting advanced capabilities for (semi-)autonomous \nexperimental design and execution. Our findings demonstrate the versatility, efficacy \nand explainability of artificial intelligence systems like Coscientist in advancing \nresearch.\nLarge language models (LLMs), particularly transformer-based models, \nare experiencing rapid advancements in recent years. These models \nhave been successfully applied to various domains, including natural \nlanguage1–5, biological6,7 and chemical research8–10 as well as code gen-\neration11,12. Extreme scaling of models13, as demonstrated by OpenAI, \nhas led to significant breakthroughs in the field 1,14. Moreover, tech-\nniques such as reinforcement learning from human feedback15 can \nconsiderably enhance the quality of generated text and the models’  \ncapability to perform diverse tasks while reasoning about their  \ndecisions16.\nOn 14 March 2023, OpenAI released their most capable LLM to date, \nGPT-414. Although specific details about the model training, sizes and \ndata used are limited in GPT-4’s technical report, OpenAI research-\ners have provided substantial evidence of the model’s exceptional \nproblem-solving abilities. Those include—but are not limited to—high \npercentiles on the SAT and BAR examinations, LeetCode challenges \nand contextual explanations from images, including niche jokes 14. \nMoreover, the technical report provides an example of how the model \ncan be used to address chemistry-related problems.\nSimultaneously, substantial progress has been made toward the auto-\nmation of chemical research. Examples range from the autonomous \ndiscovery17,18 and optimization of organic reactions19 to the develop-\nment of automated flow systems20,21 and mobile platforms22.\nThe combination of laboratory automation technologies with power-\nful LLMs opens the door to the development of a sought-after system \nthat autonomously designs and executes scientific experiments. T o \naccomplish this, we intended to address the following questions. What \nare the capabilities of LLMs in the scientific process? What degree of \nautonomy can we achieve? How can we understand the decisions made \nby autonomous agents?\nIn this work, we present a multi-LLMs-based intelligent agent (here-\nafter simply called Coscientist) capable of autonomous design, plan-\nning and performance of complex scientific experiments. Coscientist \ncan use tools to browse the internet and relevant documentation, \nuse robotic experimentation application programming interfaces \n(APIs) and leverage other LLMs for various tasks. This work has \nbeen done independently and in parallel to other works on autono -\nmous agents23–25, with ChemCrow 26 serving as another example in \nthe chemistry domain. In this paper, we demonstrate the versatil -\nity and performance of Coscientist in six tasks: (1) planning chemi -\ncal syntheses of known compounds using publicly available data; \n(2) efficiently searching and navigating through extensive hardware \ndocumentation; (3) using documentation to execute high-level com-\nmands in a cloud laboratory; (4) precisely controlling liquid han -\ndling instruments with low-level instructions; (5) tackling complex \nscientific tasks that demand simultaneous use of multiple hardware \nmodules and integration of diverse data sources; and (6) solving \noptimization problems requiring analyses of previously collected  \nexperimental data.\nCoscientist system architecture\nCoscientist acquires the necessary knowledge to solve a complex \nproblem by interacting with multiple modules (web and documen -\ntation search, code execution) and by performing experiments. \nThe main module (‘Planner’) has the goal of planning, based on the \nuser input by invoking the commands defined below. The Planner \nis a GPT-4 chat completion instance serving the role of an assistant. \nThe initial user input along with command outputs are treated as \nuser messages to the Planner. System prompts (static inputs defin -\ning the LLMs’ goals) for the Planner are engineered 1,27 in a modular \nfashion, described as four commands that define the action space: \n‘GOOGLE’ , ‘PYTHON’ , ‘DOCUMENTATION’ and ‘EXPERIMENT’ . The \nPlanner calls on each of these commands as needed to collect knowl-\nedge. The GOOGLE command is responsible for searching the inter -\nnet with the ‘Web searcher’ module, which is another LLM itself.  \nhttps://doi.org/10.1038/s41586-023-06792-0\nReceived: 20 April 2023\nAccepted: 27 October 2023\nPublished online: 20 December 2023\nOpen access\n Check for updates\n1Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA. 2Emerald Cloud Lab, South San Francisco, CA, USA. 3Department of Chemistry, Carnegie Mellon \nUniversity, Pittsburgh, PA, USA. 4Wilton E. Scott Institute for Energy Innovation, Carnegie Mellon University, Pittsburgh, PA, USA. ✉e-mail: gabegomes@cmu.edu\nNature | Vol 624 | 21/28 December 2023 | 571\nThe PYTHON command allows the Planner to perform calculations to \nprepare the experiment using a ‘Code execution’ module. The EXPERI-\nMENT command actualizes ‘ Automation’ through APIs described by \nthe DOCUMENTATION module. Like GOOGLE, the DOCUMENTA -\nTION command provides information to the main module from a \nsource, in this case documentation concerning the desired API. In \nthis study, we have demonstrated the compatibility with the Open -\ntrons Python API and the Emerald Cloud Lab (ECL) Symbolic Lab  \nLanguage (SLL). T ogether, these modules make up Coscientist, which \nreceives a simple plain text input prompt from the user (for example, \n“perform multiple Suzuki reactions”). This architecture is depicted  \nin Fig. 1.\nFurthermore, some of the commands can use subactions. The \nGOOGLE command is capable of transforming prompts into appro-\npriate web search queries, running them against the Google Search \nAPI, browsing web pages and funneling answers back to the Planner. \nSimilarly, the DOCUMENTATION command performs retrieval and sum-\nmarization of necessary documentation (for example, robotic liquid \nhandler or a cloud laboratory) for Planner to invoke the EXPERIMENT  \ncommand.\nThe PYTHON command performs code execution (not reliant upon \nany language model) using an isolated Docker container to protect the \nusers’ machine from any unexpected actions requested by the Planner. \nImportantly, the language model behind the Planner enables code to be \nfixed in case of software errors. The same applies to the EXPERIMENT \ncommand of the Automation module, which executes generated code \non corresponding hardware or provides the synthetic procedure for \nmanual experimentation.\nWeb search module\nT o demonstrate one of the functionalities of the Web Searcher \nmodule, we designed a test set composed of seven compounds to \nsynthesize, as presented in Fig.  2a. The Web Searcher module ver\n-\nsions are represented as ‘search-gpt-4’ and ‘search-gpt-3.5-turbo’ . \nOur baselines include OpenAI’s GPT-3.5 and GPT-4, Anthropic’s \nClaude 1.328 and Falcon-40B-Instruct29—considered one of the best \nopen-source models at the time of this experiment as per the OpenLLM  \nleaderboard30.\nWe prompted every model to provide a detailed compound synthesis, \nranking the outputs on the following scale (Fig. 2):\n•\t5 for a very detailed and chemically accurate procedure description\n•\t4 for a detailed and chemically accurate description but without \nreagent quantities\n•\t3 for a correct chemistry description that does not include step-  \nby-step procedure\n•\t2 for extremely vague or unfeasible descriptions\n•\t1 for incorrect responses or failure to follow instructions\n•\tAll scores below 3 indicate task failure. It is important to note that \nall answers between 3 and 5 are chemically correct but offer varying \nlevels of detail. Despite our attempts to better formalize the scale, \nlabelling is inherently subjective and so, may be different between \nthe labelers.\nAcross non-browsing models, the two versions of the GPT-4 model \nperformed best, with Claude v.1.3 demonstrating similar performance. \nGPT-3 performed significantly worse, and Falcon 40B failed in most \ncases. All non-browsing models incorrectly synthesized ibuprofen \na\nb\nc\nCoscientist\nPlannerWeb searcher Automation\nGoogle\nSearch API\nInternet\nDocker\ncontainer\nPerformed experiments\nto validate the agent\nHardware API\ndocumentation\nInput prompt from scientist\nPhysical world\nhardware\n• Cloud laboratory\n• Liquid handler\n• Manual\n   experimentation\nThe module uses LLMs\nThe module does not use LLMs\nGOOGLE EXPERIMENT\nPYTHON\nCommand used by LLM\nDOCUMENTATION\nDocs searcherCode \nsubmission\nBROWSE\nGOOGLE\nGenerating\nSLL code for\na cloud\nlaboratory\nSearching for\norganic syntheses\nonline\nLiquid handler’s\npipettes\nLaptop, accessing\na web server with\ndeployed Coscientist\nHeater–shaker\nmodule\n– Controlling a liquid handler\n– Using a liquid handler and\n   UV-Vis together\n– Performing\n   cross-coupling reactions\n– Optimizing reaction\n   conditions\nDocs index\nRetrieval and\nsummarizationCode execution\nFig. 1 | The system’s architecture.  a, Coscientist is composed of multiple \nmodules that exchange messages. Boxes with blue background represent LLM \nmodules, the Planner module is shown in green, and the input prompt is in red. \nWhite boxes represent modules that do not use LLMs. b , Types of experiments \nperformed to demonstrate the capabilities when using individual modules or \ntheir combinations. c , Image of the experimental setup with a liquid handler. \nUV-Vis, ultraviolet visible.\n572 | Nature | Vol 624 | 21/28 December 2023\nArticle\n(Fig. 2c). Nitroaniline is another example; although some generaliza-\ntion of chemical knowledge might inspire the model to propose direct \nnitration, this approach is not experimentally applicable as it would \nproduce a mixture of compounds with a very minor amount of the \nproduct (Fig. 2b). Only the GPT-4 models occasionally provided the \ncorrect answer.\nThe GPT-4-powered Web Searcher significantly improves on synthe-\nsis planning. It reached maximum scores across all trials for acetami-\nnophen, aspirin, nitroaniline and phenolphthalein (Fig. 2b). Although \nit was the only one to achieve the minimum acceptable score of three \nfor ibuprofen, it performed lower than some of the other models for \nethylacetate and benzoic acid, possibly because of the widespread \nnature of these compounds. These results show the importance of \ngrounding LLMs to avoid ‘hallucinations’31. Overall, the performance \nof GPT-3.5-enabled Web Searcher trailed its GPT-4 competition, mainly \nbecause of its failure to follow specific instructions regarding output \nformat.\nExtending the Planner’s action space to leverage reaction data -\nbases, such as Reaxys32 or SciFinder33, should significantly enhance \nthe system’s performance (especially for multistep syntheses). \nAlternatively, analysing the system’s previous statements is another \napproach to improving its accuracy. This can be done through advanced \nprompting strategies, such as ReAct34, Chain of Thought35 and Tree of  \nThoughts36.\nDocumentation search module\nAddressing the complexities of software components and their inter-\nactions is crucial for integrating LLMs with laboratory automation. A \nkey challenge lies in enabling Coscientist to effectively utilize technical \ndocumentation. LLMs can refine their understanding of common APIs, \nsuch as the Opentrons Python API37, by interpreting and learning from \nrelevant technical documentation. Furthermore, we show how GPT-4 \ncan learn how to programme in the ECL SLL.\nOur approach involved equipping Coscientist with essential docu-\nmentation tailored to specific tasks (as illustrated in Fig. 3a), allowing \nit to refine its accuracy in using the API and improve its performance \nin automating experiments.\nInformation retrieval systems are usually based on two candidate \nselection approaches: inverted search index and vector database38–41. \nFor the first one, each unique word in the search index is mapped to the \ndocuments containing it. At inference time, all documents containing \nwords from a query are selected and ranked based on various manually \ndefined formulas42. The second approach starts by embedding the \ndocuments with neural networks or as term frequency–inverse docu-\nment frequency embedding vectors43, followed by the construction \nof a vector database. Retrieval of similar vectors from this database \noccurs at inference time, usually using one of the approximate nearest \nneighbour search algorithms44. When strategies such as Transformer \nAcetaminophen Aspirin Benzoic acid Ethylacetate Ibuprofen Nitroaniline Phenolphthalein\n0\n1\n2\n3\n4\n5\nAcceptable performance\nsearch-gpt-4\nsearch-gpt-3.5-turbo\ngpt-4\ngpt-4-0314\ngpt-3.5-turbo\nclaude-1.3\nfalcon-40b-instruct\n2\n1\n3\n5\nb\na\nc\nIncorrect synthesis steps but makes chemical sense\n(GPT-3.5, no search)\nCorrect synthesis, including detailed experimental procedure\n(GPT-4 with search)\nIncorrect synthesis steps, does not make chemical sense (GPT-4, no search)\nCorrect synthesis logic but no reagents and experimental procedure\nCorrectness\nLevel of detail\nTask\ncomplexity\nAverage label\nNH2\nN+\nO\nO–\nNH2\nHNO3\nH2SO4\nNH2\nN+\nO\nO–\nNH2\nHCl/H2OAc2O/AcOH\nNH\nHNO3\nH2SO4 NH\nN+\nO\nO–\nO\nAc2O\nAlCl3, HCl\nO\nOH\nCl2\nUV light\nCl KMnO4\nNaOH, H2O\nO\nOH\n(1) C2H5COCl, py\n(2) NaOH, H2O\nO\nOH\nO OH Cl MgCl\nO\nOH\nre/f_lux\nO\nFig. 2 | Coscientist’s capabilities in chemical synthesis planning tasks.  a, Comparison of various LLMs on compound synthesis benchmarks. Error bars \nrepresent s.d. values. b , Two examples of generated syntheses of nitroaniline. c, Two example of generated syntheses of ibuprofen. UV, ultraviolet.\nNature | Vol 624 | 21/28 December 2023 | 573\nmodels are used, there are more chances to account for synonyms \nnatively without doing synonym-based query expansion, as would be \ndone in the first approach45.\nFollowing the second approach, all sections of the OT-2 API documen-\ntation were embedded using OpenAI’s ada model. T o ensure proper use \nof the API, an ada embedding for the Planner’s query was generated, \nand documentation sections are selected through a distance-based \nvector search. This approach proved critical for providing Coscientist \nwith information about the heater–shaker hardware module necessary \nfor performing chemical reactions (Fig. 3b).\nA greater challenge emerges when applying this approach to a \nmore diverse robotic ecosystem, such as the ECL. Nonetheless, we can \nexplore the effectiveness of providing information about the ECL SLL, \nwhich is currently unknown to the GPT-4 model. We conducted three \nseparate investigations concerning the SLL: (1) prompt-to-function;  \n(2) prompt-to-SLL; and (3) prompt-to-samples. Those investigations \nare detailed in Supplementary Information section ‘ECL experiments’ .\nFor investigation 1, we provide the Docs searcher with a documenta-\ntion guide from ECL pertaining to all available functions for running \nexperiments46. Figure 3c summarizes an example of the user provid-\ning a simple prompt to the system, with the Planner receiving rele -\nvant ECL functions. In all cases, functions are correctly identified for  \nthe task.\nFigure 3c,d continues to describe investigation 2, the prompt-to-SLL \ninvestigation. A single appropriate function is selected for the task, \nand the documentation is passed through a separate GPT-4 model to \nperform code retention and summarization. After the complete docu-\nmentation has been processed, the Planner receives usage information \nto provide EXPERIMENT code in the SLL. For instance, we provide a \nsimple example that requires the ‘ExperimentHPLC’ function. Proper \nuse of this function requires familiarity with specific ‘Models’ and \n‘Objects’ as they are defined in the SLL. Generated code was success-\nfully executed at ECL; this is available in Supplementary Information. \nThe sample was a caffeine standard sample. Other parameters (column, \nmobile phases, gradients) were determined by ECL’s internal software \n(a high-level description is in Supplementary Information section \n‘HPLC experiment parameter estimation’). Results of the experiment \nare provided in Supplementary Information section ‘Results of the \nHPLC experiment in the cloud lab’ . One can see that the air bubble \nwas injected along with the analyte’s solution. This demonstrates \nthe importance of development of automated techniques for qual -\nity control in cloud laboratories. Follow-up experiments leveraging \nweb search to specify and/or refine additional experimental param -\neters (column chemistry, buffer system, gradient and so on) would be \nrequired to optimize the experimental results. Further details on this \ninvestigation are in Supplementary Information section ‘ Analysis of \nECL documentation search results’ .\nA separate prompt-to-samples investigation, investigation 3, was \nconducted by providing a catalogue of available samples, enabling the \nidentification of relevant stock solutions that are on ECL’s shelves. T o \nshowcase this feature, we provide the Docs searcher module with all \n1,110 Model samples from the catalogue. By simply providing a search \nterm (for example, ‘ Acetonitrile’), all relevant samples are returned. \nThis is also available in Supplementary Information.\nControlling laboratory hardware\nAccess to documentation enables us to provide sufficient information \nfor Coscientist to conduct experiments in the physical world. T o initiate \nthe investigation, we chose the Opentrons OT-2, an open-source liquid \nhandler with a well-documented Python API. The ‘Getting Started’ \npage from its documentation was supplied to the Planner in the system \nprompt. Other pages were vectorized using the approach described \nabove. For this investigation, we did not grant access to the internet \n(Fig. 4a).\nWe started with simple plate layout-specific experiments. Straight-\nforward prompts in natural language, such as “colour every other line \nwith one colour of your choice” , resulted in accurate protocols. When \nexecuted by the robot, these protocols closely resembled the requested \nprompt (Fig. 4b–e).\nUltimately, we aimed to assess the system’s ability to integrate multi-\nple modules simultaneously. Specifically, we provided the ‘UVVIS’ com-\nmand, which can be used to pass a microplate to plate reader working \nin the ultraviolet–visible wavelength range. T o evaluate Coscientist’s \ncapabilities to use multiple hardware tools, we designed a toy task; in \n3 wells of a 96-well plate, three different colours are present—red, yellow \nand blue. The system must determine the colours and their positions \non the plate without any prior information.\nc\na OT-2 implementation\nECL implementation\nPlanner\nInitial OT-2 API documentation\nrequest from Planner\nDOCUMENTATION\nheat and shake mixtures \nusing the OT-2 robot\nQuery\nembedding\n[          ]...\n[        ]...\n[        ]...\n[        ]...\n...\nPrecompiled text\nembeddings for sections \nof API documentation ‘Hardware modules’\nPlanner\nInitial cloud laboratory API\ndocumentation request from Planner\nDOCUMENTATION\nanalyse a mixture to\nsee what is in it\n[          ]...\n[        ]...\n[        ]...\n[        ]...\n...\nText embeddings \nfor 114 ECL \nexperiment functions\nb Valid OT-2 API code\nProper usage of heater–shaker module\nVector\nsearch\nVector\nsearch\nTargeted experiment options are\nset by the Planner\nd Valid ECL SLL code\nAPI usage\ninformation\nprompt-to-OT-2\nPrompt-to-SLL\nExperimentHPLC[Samples] => Protocol\n Experimental Principles...\n Instrumentation...\n Experiment Options...\n Sample Parameters...\n ...\n# Generated HPLC Experiment SLL Function Call\nExperimentHPLC[\n   Object[Sample, ...],\n   Instrument -> Model[Instrument, ...]\n]\n# Heat and shake the reaction\nhs_mod.set_target_temperature(75)\nhs_mod.wait_for_temperature()\nhs_mod.set_and_wait_for_shake_speed(500)\n# Deactivate heater and shaker\nhs_mod.deactivate_heater()\nhs_mod.deactivate_shaker()\nhs_mod.open_labware_latch()\nQuery\nembedding\nFig. 3 | Overview of documentation search. a, Prompt-to-code through ada \nembedding and distance-based vector search. b , Example of code for using  \nOT-2’s heater–shaker module. c, Prompt-to-function/prompt-to-SLL (to symbolic \nlaboratory language) through supplementation of documentation. d, Example \nof valid ECL SLL code for performing high-performance liquid chromatography \n(HPLC) experiments.\n574 | Nature | Vol 624 | 21/28 December 2023\nArticle\nThe Coscientist’s first action was to prepare small samples of the \noriginal solutions (Extended Data Fig. 1). Ultraviolet-visible meas -\nurements were then requested to be performed by the Coscientist \n(Supplementary Information section ‘Solving the colours problem’ \nand Supplementary Fig. 1). Once completed, Coscientist was pro -\nvided with a file name containing a NumPy array with spectra for each \nwell of the microplate. Coscientist subsequently generated Python \ncode to identify the wavelengths with maximum absorbance and \nused these data to correctly solve the problem, although it required \na guiding prompt asking it to think through how different colours  \nabsorb light.\nIntegrated chemical experiment design\nWe evaluated Coscientist’s ability to plan catalytic cross-coupling \nexperiments by using data from the internet, performing the neces-\nsary calculations and ultimately, writing code for the liquid handler. T o \nincrease complexity, we asked Coscientist to use the OT-2 heater–shaker \nmodule released after the GPT-4 training data collection cutoff. The \navailable commands and actions supplied to the Coscientist are shown \nin Fig. 5a. Although our setup is not yet fully automated (plates were \nmoved manually), no human decision-making was involved.\nThe test challenge for Coscientist’s complex chemical experimen-\ntation capabilities was designed as follows. (1) Coscientist is pro -\nvided with a liquid handler equipped with two microplates (source \nand target plates). (2) The source plate contains stock solutions of \nmultiple reagents, including phenyl acetylene and phenylboronic \nacid, multiple aryl halide coupling partners, two catalysts, two bases \nand the solvent to dissolve the sample (Fig. 5b). (3) The target plate \nis installed on the OT-2 heater–shaker module (Fig. 5c). (4) Coscien-\ntist’s goal is to successfully design and perform a protocol for Suzuki–\nMiyaura and Sonogashira coupling reactions given the available  \nresources.\nT o start, Coscientist searches the internet for information on the \nrequested reactions, their stoichiometries and conditions (Fig. 5d). \nThe correct coupling partners are selected for the corresponding \nreactions. Designing and performing the requested experiments, the \nstrategy of Coscientist changes among runs (Fig. 5f). Importantly, the \nsystem does not make chemistry mistakes (for instance, it never selects \nphenylboronic acid for the Sonogashira reaction). Interestingly, the \nbase DBU (1,8-diazabicyclo[5.4.0]undec-7-ene) is selected more often \nwith the PEPPSI–IPr (PEPPSI, pyridine-enhanced precatalyst prepara-\ntion stabilization and initiation; IPr, 1,3-bis(2,6-diisopropylphenyl)\nimidazol-2-ylidene) complex, with that preference switching in Sonoga-\nshira reaction experiments; likewise, bromobenzene is chosen more \noften for Suzuki than for Sonogashira couplings. Additionally, the \nmodel can provide justifications on specific choices (Fig. 5g), dem -\nonstrating the ability to operate with concepts such as reactivity and \nselectivity (more details are in Supplementary Information section \n‘ Analysis of behaviour across multiple runs’). This capability highlights \na potential future use case to analyse the reasoning of the LLMs used by \nperforming experiments multiple times. Although the Web Searcher \nvisited various websites (Fig. 5h), overall Coscientist retrieves Wikipe-\ndia pages in approximately half of cases; notably, American Chemical \nSociety and Royal Society of Chemistry journals are amongst the top \nfive sources.\nCoscientist then calculates the required volumes of all reactants \nand writes a Python protocol for running the experiment on the \nOT-2 robot. However, an incorrect heater–shaker module method \nname was used. Upon making this mistake, Coscientist uses the Docs \nsearcher module to consult the OT-2 documentation. Next, Coscientist \nmodifies the protocol to a corrected version, which ran successfully \n(Extended Data Fig. 2). Subsequent gas chromatography–mass spec-\ntrometry analysis of the reaction mixtures revealed the formation of \nthe target products for both reactions. For the Suzuki reaction, there \nis a signal in the chromatogram at 9.53 min where the mass spectra \nmatch the mass spectra for biphenyl (corresponding molecular ion \nmass-to-charge ratio and fragment at 76 Da) (Fig. 5i). For the Sonoga-\nshira reaction, we see a signal at 12.92 min with a matching molecular \nion mass-to-charge ratio; the fragmentation pattern also looks very \nclose to the one from the spectra of the reference compound (Fig. 5j). \nDetails are in Supplementary Information section ‘Results of the  \nexperimental study’ .\nAlthough this example requires Coscientist to reason on which rea-\ngents are most suitable, our experimental capabilities at that point \nlimited the possible compound space to be explored. T o address this, \nwe performed several computational experiments to evaluate how a \nsimilar approach can be used to retrieve compounds from large com-\npound libraries47. Figure 5e shows Coscientist’s performance across five \ncommon organic transformations, with outcomes depending on the \nPlanner\nDocs searcher\nCode execution\nEXPERIMENT\nUVVIS\nOpen source\nliquid handling\nsystem\nUV-Vis plate reader\na\nbc\nColour every other \nrow of a 96-well \nplate with one \ncolour of your \nchoice. Remember \nthat for me to\nsee it, you should \nput at least \n10 /uni03BCl.\n<setup description>\nDraw a red cross \nusing food \ncolouring in the \ncenter of\n96-well plate.\n<setup description>\n“Getting started”\nin system prompt\nVectorized tutorial\nand API reference\nDraw a blue \ndiagonal starting \nfrom lower left\n(H1) in the \n96-well plate. \nRemember that for \nme to see it, you \nshould put at\nleast 10 /uni03BCl.\n<setup description>\nDraw a 3 × 3 \nrectangle using \nyellow colour at \nupper left part of \nthe 96-well plate.\nRemember that for \nme to see it, you \nshould put at least \n10 /uni03BCl.\n<setup description>\nDOCUMENTATION\nPYTHON\nde\nFig. 4 | Robotic liquid handler control capabilities and integration with analytical tools.  a, Overview of Coscientist’s configuration. b, Drawing a red cross.  \nc, Colouring every other row. d, Drawing a yellow rectangle. e , Drawing a blue diagonal.\nNature | Vol 624 | 21/28 December 2023 | 575\nqueried reaction and its specific run (the GitHub repository has more \ndetails). For each reaction, Coscientist was tasked with generating \nreactions for compounds from a simplified molecular-input line-entry \nsystem (SMILES) database. T o achieve the task, Coscientist uses web \nsearch and code execution with the RDKit chemoinformatics package.\nChemical reasoning capabilities\nThe system demonstrates appreciable reasoning capabilities, enabling \nthe request of necessary information, solving of multistep problems \nand generation of code for experimental design. Some researchers \nbelieve that the community is only starting to understand all the capa-\nbilities of GPT-4 (ref. 48). OpenAI has shown that GPT-4 could rely on \nsome of those capabilities to take actions in the physical world during \ntheir initial red team testing performed by the Alignment Research \nCenter14.\nOne of the possible strategies to evaluate an intelligent agent’s rea-\nsoning capabilities is to test if it can use previously collected data to \nguide future actions. Here, we focused on the multi-variable design \nand optimization of Pd-catalysed transformations, showcasing \nCoscientist’s abilities to tackle real-world experimental campaigns \ninvolving thousands of examples. Instead of connecting LLMs to an  \nB1 B2 B3 B4\nReactivity/rates\nRequired for \nthe reaction\nAll options \nare suitable\nCommonly used\nAvailability\nLeaving groups\nSide reactions\nHigher selectivity\nSuzuki\nB1 B2 B3 B4\nSonogashira\n1234\nA\nB\nC\nD\nE\n0% 100%\n66% 30% 1% 0%\n91% 8%\n89% 6%\n8%\nSuzuki\n1234\nA\nB\nC\nD\nE\n100% 0%\n84% 12% 0% 1%\n84% 15%\n75% 19%\n10%\nSonogashira\nC1 C2\nD1\nD2\n92% 75%\n8% 25%\nC1 C2\nD1\nD2\n93% 45%\n7% 55%\n0 0.5\nFraction of URLs\narkat-usa.org\nsemanticscholar.org\nresearchgate.net\nreagents.acsgcipr.org\nncbi.nlm.nih.gov\nhepatochem.com\nencyclopedia.pub\nsigmaaldrich.com\nonlinelibrary.wiley.com\nsciencedirect.com\npubs.rsc.org\npubs.acs.org\norganic-chemistry.org\nchem.libretexts.org\nen.wikipedia.org\n10 20\nTime (min)\n0\n0.5\n1.0Rel. intensity\nTIC (standard)\nTIC\n0 100 200\nm/z\n0\n0.5\n1.0Rel. intensity\nSpectrum at\n9.53 min\n0 100 200\nm/z\n0\n0.5\n1.0Rel. intensity\nSpectrum of\nbiphenyl standard\n10 20\nTime (min)\n0\n0.5\n1.0Rel. intensity\n0 100 200\nm/z\n0\n0.5\n1.0Rel. intensity\nSpectrum at\n12.92 min\n0 100 200\nm/z\n0\n0.5\n1.0Rel. intensity\nSpectrum of\ntolane standard\nA1 A2\nC1 C2\nD1 D2\nE1\nB1 B2 B3 B4\nBB4444\nUser prompt\n...\n...\nWeb searcher\nGOOGLE\nGoogle Seach API\nSource plate\nThe liquid handler \nsetup scheme\nInternet\na\nd\nb\nc e\n1 20 /uni03BCl tips\n2 300 /uni03BCl tips\n5 Source plate\n        (deep 96-well)\n10 Heater–shaker\n        module with\n        target plate\n12 3\n4 5 6\n78 9\n10 11\n<RX\u0003QHHG\u0003WR\u0003SHUIRUP\u00036X]XNL\u0003\nDQG\u00036RQRJDVKLUD\u0003UHDFWLRQV\u0003\nXVLQJ\u0003DYDLODEOH\u0003\nUHDJHQWV\u0011\u0011\u0011\n$Q\u0003RSWLPDO\u0003VHW\u0003RI\u0003FRQGLWLRQV\u0003IRU\u0003\n6X]XNL±0L\\DXUD\u0003FURVV\u0010FRXSOLQJ\u0003UHDFWLRQV\u0003\nFDQ\u0003EH\u0003REWDLQHG\u0011\u0011\u0011\n7KH\u0003RSWLPDO\u0003FRQGLWLRQV\u0003IRU\u0003WKH\u0003\n6RQRJDVKLUD\u0003UHDFWLRQ\u000f\u0003D\u0003FURVV\u0010FRXSOLQJ\u0003\nUHDFWLRQ\u0003RI\u0003D\u0003YLQ\\O\u0003RU\u0003DU\\O\u0003KDOLGH\u0003\nZLWK\u0003D\u0003WHUPLQDO\u0011\u0011\u0011\nPlanner’s output\n)LUVW\u000f\u0003OHW\nV\u0003ILQG\u0003WKH\u0003\nDSSURSULDWH\u0003FRQGLWLRQV\u0003IRU\u0003ERWK\u0003\n6X]XNL\u0003DQG\u00036RQRJDVKLUD\u0003\nUHDFWLRQV\u0011\u0011\u0011\n\u0011\u0011\u0011\n*22*/(\u00036X]XNL\u0003UHDFWLRQ\u0003FRQGLWLRQV\u0003\nRSWLPDO\n\u0011\u0011\u0011\n*22*/(\u00036RQRJDVKLUD\u0003UHDFWLRQ\u0003FRQGLWLRQV\u0003\nRSWLPDO\n\u0011\u0011\u0011\u0003)RU\u0003WKH\u00036X]XNL\u0003UHDFWLRQ\u000f\u0003ZH\u0003\nQHHG\u0003DQ\u0003DU\\OERURQLF\u0003DFLG\u0003\u000b$\u0015\f\u0003DQG\u0003\nDQ\u0003DU\\O\u0003KDOLGH\u0011\u0003%URPREHQ]HQH\u0003\u000b%\u0015\f\u0003\nZRXOG\u0003EH\u0003D\u0003VXLWDEOH\u0003FKRLFH\u0003IRU\u0003WKH\u0003\nDU\\O\u0003KDOLGH\u000f\u0003EHFDXVH\u0003LW\nV\u0003PRUH\u0003\nUHDFWLYH\u0003WKDQ\u0003FKORUREHQ]HQH\u0011\u0011\u0011\n...\n...\n...\n\u0013\u0011\u0014\u001c\u0015\u0016\u0013\u001a\u0019\u001c\u0015\u0016\u0013\u001a\u0019\u001c\u0015\u0016\u0015\u0003\u0013\u0011\u0013\u0014\u001b\u0018\u0014\u001b\u0018\u0014\u001b\u0018\u0014\u001b\u0018\u0014\u001b\u0018\u0014\u001a\n\u0011\u0011\u0011\n'2&80(17$7,21\u0003KRZ\u0003WR\u0003XVH\u0003\nKHDWHU±VKDNHU\u0003PRGXOH\n8VLQJ\u0003WKH\u0003LQIRUPDWLRQ\u0003IURP\u0003WKH\u0003\nGRFXPHQWDWLRQ\u000f\u0003,\u0003ZLOO\u0003QRZ\u0003FRUUHFW\u0003\nWKH\u0003SURWRFRO\u0003IRU\u0003WKH\u00036X]XNL\u0003DQG\u0003\n6RQRJDVKLUD\u0003UHDFWLRQV\u0003XVLQJ\u0003WKH\u0003\nKHDWHU±VKDNHU\u0003PRGXOH\u0011\n(;3(5,0(17\nIURP\u0003RSHQWURQV\u0003LPSRUW\u0003SURWRFROBDSL\u0003\n\u0011\u0011\u0011\n\u0011\u0011\u0011\u0003)RU\u0003WKH\u00036X]XNL\u0003UHDFWLRQ\u000f\u0003WKH\u0003\nOLPLWLQJ\u0003UHDJHQW\u0003LV\u0003SKHQ\\OERURQLF\u0003DFLG\u0003\n\u000b$\u0015\f\u0003ZLWK\u0003D\u0003FRQFHQWUDWLRQ\u0003RI\u0003\u0013\u0011\u0014\u0016\u0003\nPPRO\u0003PO\n±\u0014\u0011\u0003)RU\u0003WKH\u00036RQRJDVKLUD\u0003UHDFWLRQ\u000f\u0003\nWKH\u0003OLPLWLQJ\u0003UHDJHQW\u0003LV\u0003SKHQ\\ODFHW\\OHQH\u0003\n\u000b$\u0014\f\u0003ZLWK\u0003D\u0003FRQFHQWUDWLRQ\u0003RI\u0003\u0014\u0011\u0016\u0018\u00030\u0003RU\u0003\n\u0014\u0011\u0016\u0018\u0003PPRO\u0003PO\n±\u0014\u0011\n3<7+21\nGHI\u0003FDOFXODWHBYROXPHV\u000bFRQFHQWUDWLRQ\u000f\n\u0003\u0003\u0003\u0003GHVLUHGBPPRO \u0013\u0011\u0013\u0015\u0018\f\u001d\n\u0003\u0003\u0003\u0003YROXPH\u0003 \u0003GHVLUHGBPPRO\u0003\u0012\u0003\nFRQFHQWUDWLRQ\n\u0003\u0003\u0003\u0003UHWXUQ\u0003YROXPH\n\u0006\u00036X]XNL\u0003UHDFWLRQ\nFRQFB$\u0015\u0003 \u0003\u0013\u0011\u0014\u0016\u0003\u0006\u0003PPRO\u0012P/\n\u0011\u0011\u0011\nPlanner\nDocs\nsearcher\nCode\nexecution\nEXPERIMENT\nUVVIS\nOpen source\nliquid handling system\nUV-Vis\nplate reader\n“Getting started”\nin system prompt\nVectorized tutorial\nand API reference\nDOCUMENTATION\nPYTHON\nValid reaction number\nstandard deviation\nAverage\nTotal reaction number\nstandard deviation\nhg ji\nB\nOHHO\nPd PPh3Ph3P\nCl\nCl N\nN\nDiPP\nDiPP\nPd\nCl\nCl\nN\nN N\nN\nCl\nX\nR\nleft pipette,\n20 /uni03BCl single channel\nright pipette,\n300 /uni03BCl single channel\nf\n0 0.5 1.0\nValid reactions\n0\n1\n2\n3\n4\n5Total reactions proposed\nDiels–Alder reaction\nMichael addition\nEsteri/f_ication\nBuchwald–Hartwig amination\nMizoroki–Heck reaction\nTotal number of reactions\n0 0.05 0.10\nValid reactions\n0\n0.02\n0.04\n0.06\n0.08\n0.10\nTotal reactions proposed\nA1\nC1 C2\nD1 D2E1 —/uni00A0DMF\nB1 — X = I, R = H\nB2 — X = Br, R = H\nB3 — X = Cl, R = H\nB4 — X = I, R = NO\n2\nA2\nTIC (standard)\nTIC\nFig. 5 | Cross-coupling Suzuki and Sonogashira reaction experiments \ndesigned and performed by Coscientist.  a, Overview of Coscientist’s \nconfiguration. b, Available compounds (DMF , dimethylformamide; DiPP , \n2,6-diisopropylphenyl). c, Liquid handler setup. d , Solving the synthesis \nproblem. e, Comparison of reagent selection performance with a large  \ndataset. f, Comparison of reagent choices across multiple runs. g, Overview  \nof justifications made when selecting various aryl halides. h , Frequency of \nvisited URLs. i , Total ion current (TIC) chromatogram of the Suzuki reaction \nmixture (top panel) and the pure standard, mass spectra at 9.53 min (middle \npanel) representing the expected reaction product and mass spectra of the \npure standard (bottom panel). j, TIC chromatogram of the Sonogashira reaction \nmixture (top panel) and the pure standard, mass spectra at 12.92 min (middle \npanel) representing the expected reaction product and mass spectra of the \npure standard (bottom panel). Rel., relative.\n576 | Nature | Vol 624 | 21/28 December 2023\nArticle\noptimization algorithm as previously done by Ramos et al.49, we aimed \nto use Coscientist directly.\nWe selected two datasets containing fully mapped reaction condi-\ntion spaces where yield was available for all combinations of variables. \nOne is a Suzuki reaction dataset collected by Perera et al.50, where these \nreactions were performed in flow with varying ligands, reagents/bases \nand solvents (Fig. 6a). Another is Doyle’s Buchwald–Hartwig reaction \ndataset51 (Fig. 6e), where variations in ligands, additives and bases were \nrecorded. At this point, any reaction proposed by Coscientist would be \nwithin these datasets and accessible as a lookup table.\nWe designed the Coscientist’s chemical reasoning capabilities test \nas a game with the goal of maximizing the reaction yield. The game’s \nactions consisted of selecting specific reaction conditions with a \nsensible chemical explanation while listing the player’s observations \n−1.0\n−0.5\n0\n0.5\n1.0\n12345678 91 01 11 21 31 41 51 61 71 81 92 0\nNumber of iterations\n−1.0\n−0.5\n0\n0.5\n1.0\nAverage Random Maximum\n−1.0\n−0.5\n0\n0.5\n1.0\nNormalized maximum advantage\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nNumber of iterations\n−1.0\n−0.5\n0\n0.5\n1.0\nNormalized advantage\n5 10 15\nNumber of iterations\n0\n0.1\nNormalized maximum\nadvantage derivative\nGPT-4 with prior information GPT-4 without prior information\n5 10 15\nNumber of iterations\n0\n0.1\nNormalized\nadvantage\nderivative\n−1.0\n−0.5\n0\n0.5\n1.0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nNumber of iterations\n−1.0\n−0.5\n0\n0.5\n1.0\n−1.0\n−0.5\n0\n0.5\n1.0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nNumber of iterations\n−1.0\n−0.5\n0\n0.5\n1.0\n−1.0\n−0.5\n0\n0.5\n1.0\nNormalized maximum advantage\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nNumber of iterations\n−1.0\n−0.5\n0\n0.5\n1.0\nNormalized advantage\na\nd\nf\ng\ne\nc\nb\nGPT -4 with prior information (10 data points) GPT -4 without prior information\nGPT -4 without prior information, compound names GPT -4 without prior information, SMILES strings\nGPT -3.5 without prior information\nThe model continuously impr oves its strategy\nbased on newly collected data.\nThe model continuously impr oves its strategy\nbased on newly collected data.\nNA for Bayesian optimization\ndoes not incr ease over time\nNA for Bayesian optimization\ndoes not incr ease over time\nPrior information impr oves\ninitial conditions...\nPrior information impr oves\ninitial conditions...\n...yet, at the limit, the models\nconver ge to the same NMA.\n...yet, at the limit, the models\nconver ge to the same NMA.\nFor some compounds,\nthe model starts with\na very bad guess.\nFor some compounds,\nthe model starts with\na very bad guess.\nThe small number of examples for GPT -3.5\nunder the /f_ixed budget is due to its failur e\nto follow the pr ovided schema.\nThe small number of examples for GPT -3.5\nunder the /f_ixed budget is due to its failur e\nto follow the pr ovided schema.\n7KH\u0003\\LHOG\u0003RI\u0003WKLV\u0003UHDFWLRQ\u0003LV\u0003\u0017\u0014\b\n^\n\u0003\u0003\u0003\u0003\u0005\\RXU\u0003REVHUYDWLRQ\u0005\u001d\u0003\u00057KH\u0003\\LHOG\u0003\nKDV\u0003LPSURYHG\u0003EXW\u0003QRW\u0003VLJQLILFDQWO\\\u0011\u0003\n/HW\nV\u0003WU\\\u0003WKH\u0003ILUVW\u0003OLJDQG\u0003DJDLQ\u0003DQG\u0003\nFKDQJH\u0003WKH\u0003DGGLWLYH\u0011\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005OLJDQG\u0005\u001d\u0003\n\u0005&&\u000b&\f&\u000b& &\u000b&\u000b&\f&\f& &\u0014&\u000b&\f&\f &\u0014&\u0015 &\u0010\n& && &\u00153\u000b&\u000b&\f\u000b&\f&\f&\u000b&\f\u000b&\f&\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005DGGLWLYH\u0005\u001d\u0003\u0005&F\u0014FF\u000bRQ\u0014\fF\u0015FFFFF\u0015\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005EDVH\u0005\u001d\u0003\u0005&1\u0014&&&1\u0015&&&1 &\u0014\u0015\u0005\n`\n7KH\u0003\\LHOG\u0003RI\u0003WKLV\u0003UHDFWLRQ\u0003LV\u0003\u0018\u0013\b\n^\n\u0003\u0003\u0003\u0003\u0005\\RXU\u0003REVHUYDWLRQ\u0005\u001d\u0003\u00057KH\u0003\\LHOG\u0003\nKDV\u0003LPSURYHG\u0003WR\u0003\u0017\u0014\b\u0011\u0003/HW\nV\u0003WU\\\u0003\nDQRWKHU\u0003DGGLWLYH\u0003WKDW\u0003KDV\u0003D\u0003VWURQJHU\u0003\nHOHFWURQ\u0010ZLWKGUDZLQJ\u0003JURXS\u0003WR\u0003IXUWKHU\u0003\nLQFUHDVH\u0003WKH\u0003\\LHOG\u0011\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005OLJDQG\u0005\u001d\u0003\n\u0005&&\u000b&\f&\u000b& &\u000b&\u000b&\f&\f& &\u0014&\u000b&\f&\f &\u0014&\u0015 &\u0010\n& && &\u00153\u000b&\u000b&\f\u000b&\f&\f&\u000b&\f\u000b&\f&\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005DGGLWLYH\u0005\u001d\u0003\u0005&&2&\u000b 2\fF\u0014FF\u000b&\fRQ\u0014\u0005\u000f\n\u0003\u0003\u0003\u0003\u0005EDVH\u0005\u001d\u0003\u0005&1\u0014&&&1\u0015&&&1 &\u0014\u0015\u0005\n`\nN\nR 1\n+\nN\nN\nMe\nR 2\nTHP N\nN\nN\nMe\nTHP\nPd(OAc)2,\nLi, Bj, Sk\n1 min, 100 ºC\nNH 2\nMe\n+\nX PdL(OTf)L i\nadditive A j\nB k, DMSO\nH\nN\nMe\nR\nR\nO N\nO\nO\nON\nNo rm al ized adva nt ag e =\nyi – — /uni03A3j yj\n1\nn\nmax j yj – — /uni03A3j yj\n1\nn\nAverage Bayesian optimization Random Maximum\nFig. 6 | Results of the optimization experiments. a, A general reaction \nscheme from the flow synthesis dataset analysed in c and d. b, The mathematical \nexpression used to calculate normalized advantage values. c , Comparison of \nthe three approaches (GPT-4 with prior information, GPT-4 without prior \ninformation and GPT-3.5 without prior information) used to perform the \noptimization process. d , Derivatives of the NMA and normalized advantage \nvalues evaluated in c , left and centre panels. e , Reaction from the C–N cross- \ncoupling dataset analysed in f  and g. f, Comparison of two approaches  \nusing compound names and SMILES string as compound representations.  \ng, Coscientist can reason about electronic properties of the compounds, even \nwhen those are represented as SMILES strings. DMSO, dimethyl sulfoxide.\nNature | Vol 624 | 21/28 December 2023 | 577\nabout the outcome of the previous iteration. The only hard rule was \nfor the player to provide its actions written in JavaScript Object Nota-\ntion ( JSON) format. If the JSON file could not be parsed, the player is \nalerted of its failure to follow the specified data format. The player had \na maximum of 20 iterations (accounting for 5.2% and 6.9% of the total \nspace for the first and second datasets, respectively) to finish the game.\nWe evaluate Coscientist’s performance using the normalized advan-\ntage metric (Fig. 6b). Advantage is defined as the difference between a \ngiven iteration yield and the average yield (advantage over a random \nstrategy). Normalized advantage measures the ratio between advantage \nand maximum advantage (that is, the difference between the maximum \nand average yield). The normalized advantage metric has a value of \none if the maximum yield is reached, zero if the system exhibits com-\npletely random behaviour and less than zero if the performance at \nthis step is worse than random. An increase in normalized advantage \nover each iteration demonstrates Coscientist’s chemical reasoning \ncapabilities. The best result for a given iteration can be evaluated using \nthe normalized maximum advantage (NMA), which is the normalized \nvalue of the maximum advantage achieved until the current step. As \nNMA cannot decrease, the valuable observations come in the form \nof the rate of its increase and its final point. Finally, during the first \nstep, the values for NMA and normalized advantage equal each other, \nportraying the model’s prior knowledge (or lack thereof) without any \ndata being collected.\nFor the Suzuki dataset, we compared three separate approaches: (1) \nGPT-4 with prior information included in the prompt (which consisted \nof 10 yields from random combinations of reagents); (2) GPT-4; or (3) \nGPT-3.5 without any prior information (Fig. 6c). When comparing GPT-4 \nwith the inclusion and exclusion of prior information, it is clear that \nthe initial guess for the former scenario is better, which aligns with \nour expectations considering the provided information about the \nsystem’s reactivity. Notably, when excluding prior information, there \nare some poor initial guesses, whereas there are none when the model \nhas prior information. However, at the limit, the models converge to \nthe same NMA. The GPT-3.5 model plots have a very limited number \nof data points, primarily because of its inability to output messages \nin the correct JSON schema as requested in the prompt. It is unclear if \nthe GPT-4 training data contain any information from these datasets. \nIf so, one would expect that the initial model guess would be better \nthan what we observed.\nThe normalized advantage values increase over time, suggesting that \nthe model can effectively reuse the information obtained to provide \nmore specific guidance on reactivity. Evaluating the derivative plots \n(Fig. 6d) does not show any significant difference between instances \nwith and without the input of prior information.\nThere are many established optimization algorithms for chemical \nreactions. In comparison with standard Bayesian optimization52, both \nGPT-4-based approaches show higher NMA and normalized advantage \nvalues (Fig. 6c). A detailed overview of the exact Bayesian optimization \nstrategy used is provided in Supplementary Information section ‘Bayes-\nian optimization procedure’ . It is observed that Bayesian optimization’s \nnormalized advantage line stays around zero and does not increase \nover time. This may be caused by different exploration/exploitation \nbalance for these two approaches and may not be indicative of their \nperformance. For this purpose, the NMA plot should be used. Changing \nthe number of initial samples does not improve the Bayesian optimiza-\ntion trajectory (Extended Data Fig. 3a). Finally, this performance trend \nis observed for each unique substrate pairings (Extended Data Fig. 3b).\nFor the Buchwald–Hartwig dataset (Fig. 6e), we compared a version \nof GPT-4 without prior information operating over compound names \nor over compound SMILES strings. It is evident that both instances \nhave very similar performance levels (Fig.  6f). However, in certain \nscenarios, the model demonstrates the ability to reason about the \nreactivity of these compounds simply by being provided their SMILES \nstrings (Fig. 6g).\nDiscussion\nIn this paper, we presented a proof of concept for an artificial intelligent \nagent system capable of (semi-)autonomously designing, planning and \nmultistep executing scientific experiments. Our system demonstrates \nadvanced reasoning and experimental design capabilities, addressing \ncomplex scientific problems and generating high-quality code. These \ncapabilities emerge when LLMs gain access to relevant research tools, \nsuch as internet and documentation search, coding environments \nand robotic experimentation platforms. The development of more \nintegrated scientific tools for LLMs has potential to greatly accelerate \nnew discoveries.\nThe development of new intelligent agent systems and automated \nmethods for conducting scientific experiments raises potential con-\ncerns about the safety and potential dual-use consequences, particu-\nlarly in relation to the proliferation of illicit activities and security \nthreats. By ensuring the ethical and responsible use of these pow\n-\nerful tools, we are continuing to explore the vast potential of LLMs \nin advancing scientific research while mitigating the risks associ -\nated with their misuse. A brief dual-use study of Coscientist is pro -\nvided in Supplementary Information section ‘Safety implications:  \nDual-use study’ .\nTechnology use disclosure\nThe writing of the preprint version of this manuscript was assisted by \nChatGPT (specifically, GPT-4 being used for grammar and typos). All \nauthors have read, corrected and verified all information presented in \nthis manuscript and Supplementary Information.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-023-06792-0.\n1. Brown, T. et al. in Advances in Neural Information Processing Systems Vol. 33  \n(eds Larochelle, H. et al.) 1877–1901 (Curran Associates, 2020).\n2. Thoppilan, R. et al. LaMDA: language models for dialog applications. Preprint at  \nhttps://arxiv.org/abs/2201.08239 (2022).\n3. Touvron, H. et al. LLaMA: open and efficient foundation language models. Preprint at \nhttps://arxiv.org/abs/2302.13971 (2023).\n4. Hoffmann, J. et al. Training compute-optimal large language models. In Advances in \nNeural Information Processing Systems 30016–30030 (NeurIPS, 2022).\n5. Chowdhery, A. et al. PaLM: scaling language modeling with pathways. J. Mach. Learn. \nRes. 24, 1–113 (2022).\n6. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a \nlanguage model. Science 379, 1123–1130 (2023).\n7. Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation \nand mining. Brief Bioinform. 23, bbac409 (2022).\n8. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: a pre-trained transformer for \ncomputational chemistry. Mach. Learn. Sci. Technol. 3, 015022 (2022).\n9. Kim, H., Na, J. & Lee, W. B. Generative chemical transformer: neural machine learning  \nof molecular geometric structures from chemical language via attention. J. Chem. Inf. \nModel. 61, 5804–5814 (2021).\n10. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B. Leveraging large language \nmodels for predictive chemistry. Preprint at https://chemrxiv.org/engage/chemrxiv/\narticle-details/652e50b98bab5d2055852dde (2023).\n11. Xu, F. F., Alon, U., Neubig, G. & Hellendoorn, V. J. A systematic evaluation of large \nlanguage models of code. In Proc. 6th ACM SIGPLAN International Symposium on \nMachine Programming 1–10 (ACM, 2022).\n12. Nijkamp, E. et al. CodeGen: an open large language model for code with multi-turn \nprogram synthesis. In Proc. 11th International Conference on Learning Representations \n(ICLR, 2022).\n13. Kaplan, J. et al. Scaling laws for neural language models. Preprint at https://arxiv.org/\nabs/2001.08361 (2020).\n14. OpenAI. GPT-4 Technical Report (OpenAI, 2023).\n15. Ziegler, D. M. et al. Fine-tuning language models from human preferences. Preprint at \nhttps://arxiv.org/abs/1909.08593 (2019).\n16. Ouyang, L. et al. Training language models to follow instructions with human \nfeedback. In Advances in Neural Information Processing Systems 27730–27744 \n(NeurIPS, 2022).\n578 | Nature | Vol 624 | 21/28 December 2023\nArticle\n17. Granda, J. M., Donina, L., Dragone, V., Long, D.-L. & Cronin, L. Controlling an organic \nsynthesis robot with machine learning to search for new reactivity. Nature 559, 377–381 \n(2018).\n18. Caramelli, D. et al. Discovering new chemistry with an autonomous robotic platform \ndriven by a reactivity-seeking neural network. ACS Cent. Sci. 7, 1821–1830 (2021).\n19. Angello, N. H. et al. Closed-loop optimization of general reaction conditions for heteroaryl \nSuzuki–Miyaura coupling. Science 378, 399–405 (2022).\n20. Adamo, A. et al. On-demand continuous-flow production of pharmaceuticals in a compact, \nreconfigurable system. Science 352, 61–67 (2016).\n21. Coley, C. W. et al. A robotic platform for flow synthesis of organic compounds informed \nby AI planning. Science 365, eaax1566 (2019).\n22. Burger, B. et al. A mobile robotic chemist. Nature 583, 237–241 (2020).\n23. Auto-GPT: the heart of the open-source agent ecosystem. GitHub https://github.com/\nSignificant-Gravitas/AutoGPT (2023).\n24. BabyAGI. GitHub https://github.com/yoheinakajima/babyagi (2023).\n25. Chase, H. LangChain. GitHub https://github.com/langchain-ai/langchain (2023).\n26. Bran, A. M., Cox, S., White, A. D. & Schwaller, P. ChemCrow: augmenting large-language \nmodels with chemistry tools. Preprint at https://arxiv.org/abs/2304.05376 (2023).\n27. Liu, P. et al. Pre-train, prompt, and predict: a systematic survey of prompting methods in \nnatural language processing. ACM Comput. Surv. 55, 195 (2021).\n28. Bai, Y. et al. Constitutional AI: harmlessness from AI feedback. Preprint at https://arxiv.org/\nabs/2212.08073 (2022).\n29. Falcon LLM. TII https://falconllm.tii.ae (2023).\n30. Open LLM Leaderboard. Hugging Face https://huggingface.co/spaces/HuggingFaceH4/\nopen_llm_leaderboard (2023).\n31. Ji, Z. et al. Survey of hallucination in natural language generation. ACM Comput. Surv. 55, \n248 (2023).\n32. Reaxys https://www.reaxys.com (2023).\n33. SciFinder https://scifinder.cas.org (2023).\n34. Yao, S. et al. ReAct: synergizing reasoning and acting in language models. In Proc.11th \nInternational Conference on Learning Representations (ICLR, 2022).\n35. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language models.  \nIn Advances in Neural Information Processing Systems 24824–24837 (NeurIPS, 2022).\n36. Long, J. Large language model guided tree-of-thought. Preprint at https://arxiv.org/\nabs/2305.08291 (2023).\n37. Opentrons Python Protocol API. Opentrons https://docs.opentrons.com/v2/ (2023).\n38. Tu, Z. et al. Approximate nearest neighbor search and lightweight dense vector reranking \nin multi-stage retrieval architectures. In Proc. 2020 ACM SIGIR on International \nConference on Theory of Information Retrieval 97–100 (ACM, 2020).\n39. Lin, J. et al. Pyserini: a python toolkit for reproducible information retrieval research with \nsparse and dense representations. In Proc. 44th International ACM SIGIR Conference on \nResearch and Development in Information Retrieval 2356–2362 (ACM, 2021).\n40. Qadrud-Din, J. et al. Transformer based language models for similar text retrieval and \nranking. Preprint at https://arxiv.org/abs/2005.04588 (2020).\n41. Paper QA. GitHub https://github.com/whitead/paper-qa (2023).\n42. Robertson, S. & Zaragoza, H. The probabilistic relevance framework: BM25 and beyond. \nFound. Trends Inf. Retrieval 3, 333–389 (2009).\n43. Data Mining. Mining of Massive Datasets (Cambridge Univ., 2011).\n44. Johnson, J., Douze, M. & Jegou, H. Billion-scale similarity search with GPUs. IEEE Trans. \nBig Data 7, 535–547 (2021).\n45. Vechtomova, O. & Wang, Y. A study of the effect of term proximity on query expansion.  \nJ. Inf. Sci. 32, 324–333 (2006).\n46. Running experiments. Emerald Cloud Lab https://www.emeraldcloudlab.com/guides/\nrunningexperiments (2023).\n47. Sanchez-Garcia, R. et al. CoPriNet: graph neural networks provide accurate and  \nrapid compound price prediction for molecule prioritisation. Digital Discov. 2, 103–111 \n(2023).\n48. Bubeck, S. et al. Sparks of artificial general intelligence: early experiments with GPT-4. \nPreprint at https://arxiv.org/abs/2303.12712 (2023).\n49. Ramos, M. C., Michtavy, S. S., Porosoff, M. D. & White, A. D. Bayesian optimization of \ncatalysts with in-context learning. Preprint at https://arxiv.org/abs/2304.05341 (2023).\n50. Perera, D. et al. A platform for automated nanomole-scale reaction screening and \nmicromole-scale synthesis in flow. Science 359, 429–434 (2018).\n51. Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G. Predicting reaction \nperformance in C–N cross-coupling using machine learning. Science 360, 186–190 \n(2018).\n52. Hickman, R. et al. Atlas: a brain for self-driving laboratories. Preprint at https://chemrxiv.\norg/engage/chemrxiv/article-details/64f6560579853bbd781bcef6 (2023).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nData availability\nExamples of the experiments discussed in the text are provided in the \nSupplementary Information. Because of safety concerns, data, code \nand prompts will be only fully released after the development of US \nregulations in the field of artificial intelligence and its scientific appli-\ncations. Nevertheless, the outcomes of this work can be reproduced \nusing actively developed frameworks for autonomous agent develop-\nment. The reviewers had access to the web application and were able \nto verify any statements related to this work. Moreover, we provide a \nsimpler implementation of the described approach, which, although \nit may not produce the same results, allows for deeper understanding \nof the strategies used in this work.\nCode availability\nSimpler implementation as well as generated outputs used for quan-\ntitative analysis are provided at https://github.com/gomesgroup/\ncoscientist.\nAcknowledgements We thank the following Carnegie Mellon University Chemistry groups  \nfor their assistance with providing the chemicals needed for the Coscientist’s experiments: \nSydlik, Garcia Borsch, Matyjaszewski and Ly. We give special thanks to the Noonan group  \n(K. Noonan and D. Sharma) for providing access to chemicals and gas chromatography–mass \nspectrometry analysis. We also thank the team at Emerald Cloud Lab (with special attention  \nto Y. Benslimane, H. Gronlund, B. Smith and B. Frezza) for assisting us with parsing their \ndocumentation and executing experiments. G.G. is grateful to the Carnegie Mellon University \nCloud Lab Initiative led by the Mellon College of Science for its vision of the future of physical \nsciences. G.G. thanks Carnegie Mellon University; the Mellon College of Sciences and its \nDepartment of Chemistry; and the College of Engineering and its Department of Chemical \nEngineering for the start-up support. D.A.B. was partially funded by the National Science \nFoundation Center for Chemoenzymatic Synthesis (Grant no. 2221346). R.M. was funded by \nthe National Science Foundation Center for Computer-Assisted Synthesis (Grant no. 2202693).\nAuthor contributions D.A.B. designed the computational pipeline and developed the ‘Planner’, \n‘Web searcher’ and ‘Code execution’ modules. R.M. assisted in designing the computational \npipeline and developed the ‘Docs searcher’ module. B.K. analysed the behaviours of the Docs \nsearcher module to enable Coscientist to produce experiment code in Emerald Cloud Lab’s \nSymbolic Lab Language. D.A.B. assisted and oversaw Coscientist’s chemistry experiments. \nD.A.B., R.M. and G.G. designed and performed initial computational safety studies. D.A.B. \ndesigned and graded Coscientist’s synthesis capabilities study. D.A.B. co-designed with G.G. \nand performed the optimization experiments. R.M. performed the large compound library \nexperiment and Bayesian optimization baseline runs. G.G. designed the concepts, performed \npreliminary studies and supervised the project. D.A.B., R.M. and G.G. wrote this manuscript.\nCompeting interests G.G. is part of the AI Scientific Advisory Board of Emerald Cloud Lab. \nExperiments and conclusions in this manuscript were made before G.G.’s appointment to this \nrole. B.K. is an employee of Emerald Cloud Lab. D.A.B. and G.G. are co-founders of aithera.ai,  \na company focusing on responsible use of artificial intelligence for research.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-023-06792-0.\nCorrespondence and requests for materials should be addressed to Gabe Gomes.\nPeer review information Nature thanks Sebastian Farquhar, Tiago Rodrigues and the other, \nanonymous, reviewer(s) for their contribution to the peer review of this work.\nReprints and permissions information is available at http://www.nature.com/reprints.\nArticle\nExtended Data Fig. 1 | Using UV-Vis and liquid handler to solve food colouring \nidentification problem.  Guiding prompt in the third message is shown in \nbold. In the first message the user prompt is provided, then code for sample \npreparation is generated, resulting data is provided as NumPy array, which is \nthen analysed to give the final answer.\nExtended Data Fig. 2 | Code, generated by Coscientist. The generated code \ncan be split into the following steps: defining metadata for the method, loading \nlabware modules, setting up the liquid handler, performing required reagent \ntransfers, setting up the heater-shaker module, running the reaction, and \nturning the module off.\nArticle\nExtended Data Fig. 3 | Additional results on comparison with Bayesian optimization. a, GPT-4 models compared with Bayesian optimization performed \nstarting with different number of initial samples. b , Compound-by-compound comparison of differences between advantages.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7491552829742432
    },
    {
      "name": "Documentation",
      "score": 0.6299862861633301
    },
    {
      "name": "Automation",
      "score": 0.5485549569129944
    },
    {
      "name": "Transformer",
      "score": 0.5314183235168457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4668391942977905
    },
    {
      "name": "The Internet",
      "score": 0.450673371553421
    },
    {
      "name": "Natural language",
      "score": 0.44993144273757935
    },
    {
      "name": "Software engineering",
      "score": 0.44717642664909363
    },
    {
      "name": "Natural language understanding",
      "score": 0.4117688238620758
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3484974503517151
    },
    {
      "name": "Systems engineering",
      "score": 0.3265610337257385
    },
    {
      "name": "Programming language",
      "score": 0.2841845154762268
    },
    {
      "name": "World Wide Web",
      "score": 0.17139098048210144
    },
    {
      "name": "Engineering",
      "score": 0.11686700582504272
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 538
}