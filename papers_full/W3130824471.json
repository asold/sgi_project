{
  "title": "Using Transformer Based Ensemble Learning to Classify Scientific Articles",
  "url": "https://openalex.org/W3130824471",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3214557468",
      "name": "Ghosh, Sohom",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3213764685",
      "name": "Chopra Ankush",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4237670065",
    "https://openalex.org/W1557759681",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1988001261",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3127943372",
    "https://openalex.org/W3159904855",
    "https://openalex.org/W6637031373"
  ],
  "abstract": null,
  "full_text": "Using Transformer based Ensemble Learning to\nclassify Scientiﬁc Articles\nSohom Ghosh⋆[0000−0002−4113−0958] and Ankush Chopra⋆⋆[0000−0002−9970−8038]\nArtiﬁcial Intelligence, CoE, Fidelity Investments, Bengaluru, Karnataka, India\n{sohom1ghosh,ankush01729}@gmail.com\nAbstract. Many time reviewers fail to appreciate novel ideas of a re-\nsearcher and provide generic feedback. Thus, proper assignment of re-\nviewers based on their area of expertise is necessary. Moreover, read-\ning each and every paper from end-to-end for assigning it to a reviewer\nis a tedious task. In this paper, we describe a system which our team\nFideLIPI submitted in the shared task of SDPRA-2021 1 [14]. It com-\nprises four independent sub-systems capable of classifying abstracts of\nscientiﬁc literature to one of the given seven classes. The ﬁrst one is\na RoBERTa [10] based model built over these abstracts. Adding topic\nmodels / Latent dirichlet allocation (LDA) [2] based features to the ﬁrst\nmodel results in the second sub-system. The third one is a sentence level\nRoBERTa [10] model. The fourth one is a Logistic Regression model\nbuilt using Term Frequency Inverse Document Frequency (TF-IDF) fea-\ntures. We ensemble predictions of these four sub-systems using majority\nvoting to develop the ﬁnal system which gives a F1 score of 0.93 on\nthe test and validation set. This outperforms the existing State Of The\nArt (SOTA) model SciBERT’s [1] in terms of F1 score on the valida-\ntion set. Our codebase is available athttps://github.com/SDPRA-2021/\nshared-task/tree/main/FideLIPI\nKeywords: Scientiﬁc Text Classiﬁcation · Ensemble learning · Trans-\nformers\n1 Introduction\nDue to the ever-increasing number of research paper submissions per conference,\nit has become extremely diﬃcult to manually assign appropriate reviewers to a\npaper based on their expertise. Improper assignment of reviewer leads to poor\nquality of reviews. Thus, an automated system capable of determining which\n⋆ Equal Contribution\n⋆⋆ Equal Contribution\n1 https://sdpra-2021.github.io/website/ (accessed January 25, 2021)\nIn: Gupta M., Ramakrishnan G. (eds) Trends and Applications in Knowledge Dis-\ncovery and Data Mining. PAKDD 2021. Lecture Notes in Computer Science, vol\n12705. Springer, Cham. https://doi.org/10.1007/978-3-030-75015-2 11\nThis is the author’s version.\narXiv:2102.09991v2  [cs.CL]  4 May 2021\n2 Sohom Ghosh and Ankush Chopra\ncategory a research paper belongs to is necessary to develop. A similar shared\ntask has been hosted at The First Workshop & Shared Task on Scope Detection\nof the Peer Review Articles (SDPRA-2021) (Collocated with the 25 th Paciﬁc-\nAsia Conference on Knowledge Discovery and Data Mining 2 (PAKDD-2021))\n[14]. This paper narrates the approach our team FideLIPI followed while par-\nticipating in this challenge. It is structured into ﬁve main sections. This section\nintroduces readers to the problem that we have tried to solve, related works that\nhave been done previously and our contributions. The next section familiarizes\nreaders with the dataset, pre-processing and feature engineering steps. The sec-\ntion following it describes the system we developed and how we generated the\nsubmissions we have made. The subsequent section discusses the experiments\nwe performed and their results. After that, we conclude the paper by revealing\nour plans to improve the system further.\n1.1 Problem Description\nGiven an abstract A of a scientiﬁc article, we need to assign one of the seven\nclasses (CL, CR, DC, DS, LO, NI, SE) 3 to it.\n1.2 Related Works\nBeltagy et al. in their paper [1] propose SciBERT which is a language model\ntrained speciﬁcally for scientiﬁc articles. They outperformed the BERT [7] base\nmodel in domains like Bio-medicine and Computer Science. In the paper [4]\nCao et al. describe how contents as well as citations can be used to classify\nscientiﬁc documents. They try out diﬀerent Machine Learning models like K\nNearest Neighbours, Nearest Centroid and Naive Bayes. One of the pioneering\nwork has been done by Ghanem et al. [8]. While participating in KDD CUP\n2002 (Task 1) they proposed a novel method for extracting frequently appearing\nkeywords. They use these patterns as input to a Support Vector Machine (SVM)\nclassiﬁer [6]. Borrajo et al. [3] studied how over-sampling, under-sampling, use of\ndictionaries help in classifying scientiﬁc articles relating to bio-medicine. They\nmainly dealt with three kinds of classiﬁers: K Nearest Neighbour, SVM and\nNaive-Bayes. For evaluation, they used Precision, Recall, F-measure and Utility.\nThey achieved best results by using sub-sampling along with NLPBA, Protein\ndictionaries and a SVM classiﬁer [6].\n1.3 Our Contributions\nOur contributions are as follows:\n– We have developed a system capable of assigning a class to the abstract of a\ngiven scientiﬁc article. Our model (F1: 0.928) surpassed the existing SOTA\nmodel SciBERT’s [1] performance (F1: 0.926) in the given validation set.\n2 https://www.pakdd2021.org/ (accessed January 25, 2021)\n3 These abbreviations have been expanded in Table 1\nUsing Transformer based Ensemble Learning to classify Scientiﬁc Articles 3\n– For enhancing reproducibility and transparency, we have open-sourced the\nsystem. It is available here4.\n2 Dataset\nThis section narrates in details the data we are dealing with and the pre-\nprocessing steps we followed.\n2.1 Data Description\nThe dataset [13] for the shared task of The First Workshop and Shared Task\non Scope Detection of the Peer Review Articles (SPDRA 2021) [14] consists of\n16,800 training instances, 11,200 validation instances and 7,000 test instances.\nThey belong to seven classes as described in Table 1.\nTable 1.Data Description\nCategory Train Validation Test\nComputation and Language (CL) 2,740 1,866 1,194\nCryptography and Security (CR) 2,660 1,835 1,105\nDistributed and Cluster Computing (DC) 2,042 1,355 803\nData Structures and Algorithms (DS) 2,737 1,774 1,089\nLogic in Computer Science (LO) 1,811 1,217 772\nNetworking and Internet Architecture (NI) 2,764 1,826 1,210\nSoftware Engineering (SE) 2,046 1,327 827\nTOTAL 16,800 11,200 7,000\n2.2 Pre-processing & Feature Engineering\nFor Model-1 narrated in section 3.1, we keep the raw abstracts as it is and we do\nnot pre-process them. For Model-2, after converting the abstracts to lowercase,\nremoving stop words and lemmatizing them, we empirically decide to extract 50\ntopics using Topic Modelling (LDA) [2]. This is narrated in section 3.2. We used\nNLTK5 and Gensim6 libraries to achieve this. As pre-processing steps of Model-\n3 described in section 3.3, we remove newline characters and extract sentences\nfrom abstracts of scientiﬁc articles using Spacy 7 library. For Model-4 mentioned\nin section 3.4, we remove stop words and create TF-IDF based features with\nn-grams ranging from 1 to 4. We further ignore the terms with document fre-\nquency strictly lesser than 0.0005. We selected these hyper-parameters through\nexperimentation. This resulted in 22,151 features.\n4 https://github.com/SDPRA-2021/shared-task/tree/main/FideLIPI\n5 https://www.nltk.org/ (accessed January 25, 2021)\n6 https://radimrehurek.com/gensim/ (accessed January 25, 2021)\n7 https://spacy.io/ (accessed January 25, 2021)\n4 Sohom Ghosh and Ankush Chopra\n3 Methodology\nIn this section, we describe each of the sub-systems/models which are ensem-\nbled to create the ﬁnal system. Moreover, we elucidate the process we followed\nto generate each of the three submissions we have made.\nNOTE: All of the hyper-parameters mentioned in this section have been ob-\ntained through rigorous experimentation described in section 4.1 . The hyper-\nparameters of the RoBERTa [10] models used here are mentioned in Table 3.\n3.1 System Description of Model-1 (RoBERTa)\nIt is a RoBERTa [10] based model built on the raw text corpus of the abstract.\nIts task is to predict probability of each of the given classes. We ﬁnally select\nthe class having maximum probability.\n3.2 System Description of Model-2 (RoBERTa+LDA)\nThis model is similar to the one described in section 3.1. It additionally takes\n50 Topic Modelling (LDA) [2] based features. Additional dropout of 0.3 is im-\nplemented in the classiﬁcation layer of this RoBERTa [10] model.\n3.3 System Description of Model-3 (RoBERTa on sentences)\nIn this model, instead of considering the whole abstract as input, we split it into\nindividual sentences. Sentences having a number of tokens greater or equal to\nten are considered while training the RoBERTa [10] based model. While scoring\nthe validation set we considered only those sentences which have a number of\ntokens greater or equal to six. These numbers have been obtained empirically.\nTo decide the ﬁnal label of a given abstract, we add the logarithmic probabilities\nof predictions of the individual sentences for each of the seven classes. We then,\nselect the class for which this value is maximum.\n3.4 System Description of Model-4 (TF-IDF + Logistic Regression)\nThis is a simple logistic regression model built using scikit-learn 8 library over\n22,151 TF-IDF features. Its hyper-parameters are as follows: maximum number\nof iterations = 100, penalty = l2, tolerance = 0.0001.\n3.5 Submissions\nAs per the rules of this shared task, each team could submit at-most three sets\nof predictions for the test set. Our ﬁrst submission is an ensemble of all the four\nmodels described above using majority voting technique. It has been depicted in\nFigure 1. In this technique, the class which gets the maximum number of vote\nis selected as the ﬁnal class. Whenever there is a tie between two classes, we\nrandomly choose one of them. The second and third submissions are results of\nModel-2 (refer to section 3.2) and Model-4 (refer to section 3.4) respectively.\n8 https://scikit-learn.org/ (accessed January 25, 2021)\nUsing Transformer based Ensemble Learning to classify Scientiﬁc Articles 5\nFig. 1.Ensemble Model Architecture. E m = Embedding, T m = Topic Models, FC =\nFully Connected Layer, C1 to C2 are the classes corresponding to CL, CR and so on\n4 Experiments, Results and Discussion\n4.1 Experimental Setup\nWe started with zero-shot learning [5] and pre-trained models like BERT [7],\nRoBERTa [10] and T5 [12], since the transformer-based pre-trained models are\nproducing state-of-the-art results on most of the Natural Language Processing\n(NLP) tasks. We used text classiﬁcation module of the Simple Transformer li-\nbrary9 to run the multi-class classiﬁcation using the BERT [[7] model. The Sim-\nple Transformer library provides an easy interface to run diﬀerent NLP tasks\nwhile using the HuggingFace [15] Transformers at the back-end. We ﬁne-tune\nthe underlying pre-trained model while training it for the task. We ran classi-\nﬁcation using BERT-base [7] for 10 epochs and saved model after each epoch.\nBased on the saturation of validation set performance improvement, we chose\nthe right epoch for both the models.\nT5 [12] uses both encoder and decoder parts of the transformer. Although\nboth input and output of the model need to be text sequence, it can still be\nused eﬀectively for the text classiﬁcation task. We utilized the T5 Model [12]\nclass of Simple Transformer to train a model in multi-class classiﬁcation setting\nusing T5-base [12] pre-trained model. We used the HuggingFace [15] library\nfor training a classiﬁer using the pre-trained RoBERTa [10] model. We passed\nthe abstracts through the RoBERTa [10] model and took the embedding of the\n[CLS] token which was passed through a classiﬁcation head set up to train for\nmulti-class classiﬁcation.\nWe decided to perform further experimentation with the RoBERTa [10]\nmodel since it had the best performance among the three vanilla model that\nwe built. We created a representation of the abstracts using TF-IDF, and LDA\n[2] based topic modelling techniques. We combined the LDA [2] and TF-IDF\nfeatures with the RoBERTa [10] individually and trained two models. We could\nsee the slight improvement in the performance when we combined LDA [2] with\nthe RoBERTa [10] compared to vanilla RoBERTa [10] model. RoBERTa [10] has\na limitation of 512 tokens as input. Many of the abstracts have more than 512\n9 https://simpletransformers.ai/ (accessed January 25, 2021)\n6 Sohom Ghosh and Ankush Chopra\ntokens. Thus, the part towards the tail of the abstract was not getting utilized for\nprediction in these cases. So, we decided to train a sentence level model, where\nwe ﬁrst tokenized the abstract into sentences using Spacy. These tokenized sen-\ntences were assigned the same token as the abstract they were part of. We then\ntrained a classiﬁcation model using this data using Simple Transformer library\nwith RoBERTa [10] base model. We observed that very short sentences seldom\ncarry enough information to be able to predict the right class just using their\nconstituent words. Hence, we built four models ﬁrst three by not considering\na sentence that had less than 1, 6 and 10 words and the fourth one by taking\nall the sentences. We found that the model where we had taken sentences with\nmore than 10 words did better than all other models. While scoring these mod-\nels on the validation data, we scored the individual sentence and averaged the\noutput probability for all the sentences from an abstract. Then, the class with\nthe highest average probability was assigned as the prediction for the abstract.\nWe experimented with the length during the predictions as well and found that\ntaking sentences with more than 5 words tend to perform the best. Finally, we\nbench-marked our model with one an existing SOTA model which is SciBERT\n[1].\n4.2 Results and Discussion\nThe results10 we obtained are presented in Table 2. Here F1 refers to the weighted\nF1 score. Details relating to models 1,2,3 and 4 have been mentioned in the\nprevious section of this paper.\nTable 2.Performance of various models. Bold highlights the best performing models.\nUnderline denotes existing State Of The Art (SOTA) model.\nModel Train (F1)Validation (F1)Test (F1)\nZero shot learning 0.114 0.113\nBERT 0.999 0.898\nT5 0.977 0.882\nSciBERT (SOTA) 0.991 0.926\nModel-1 (RoBERTa) 0.999 0.916\nModel-2 (RoBERTa+LDA) 0.968 0.919 0.912\nModel-3 (RoBERTa on sentences) 0.999 0.913\nModel-4 (TF-IDF+Logistic Regression) 0.958 0.915 0.916\nEnsemble (Model 1-2-3-4) 0.999 0.928 0.929\n10 NOTE: The F1 scores corresponding to the test set have been provided by the SD-\nPRA team after evaluating three of our submissions. Since the number of submissions\nwas restricted to three, we do not have these numbers for the other models which\nwe have developed.\nUsing Transformer based Ensemble Learning to classify Scientiﬁc Articles 7\nTable 3.Hyper-parameters of various models\nModel Max. TokenBS (Train)BS (Valid)# Epochsη Optimizer\nRoBERTa-1512 32 256 13 0.00002 ADAM[9]\nRoBERTa-2512 32 8 3 0.00002 ADAM[9]\nRoBERTa-3512 8 8 10 0.00004 AdamW[11]\nBERT 512 8 8 10 0.00004 AdamW[11]\nT5 512 8 8 7 0.00100 ADAM[9]\nSciBERT 512 32 256 5 0.00002 ADAM[9]\nThe hyper-parameters corresponding to the best performing versions of RoBERTa\n[10] models described in sections 3.1 (RoBERTa-1), 3.2 (RoBERTa-2), 3.3 (RoBERTa-\n3), BERT [7] model, T5 [12] model and SciBERT [1] model are mentioned in\nTable 3. Max. Token is the maximum number of tokens, BS means the batch\nsize and η represents the learning rate.\nWhile performing the experiments, we observed that the Logistic Regression\nmodel takes the least amount of training time as compared to other transformer-\nbased models. In each of these models, we saw that the performance was worst\nfor the class Distributed and Cluster Computing (DC) and it was best for the\nclass Computation and Language (CL). On analysing Table 2 we see that the\nF1 scores are 0.999 on the training set for BERT [7], Model-1, Model-3 and\nthe ensemble model. However, the ensemble model performs best for Validation\nset (F1 = 0.928). We further observe that zero-shot learning (F1=0.114 on the\ntraining set and F1=0.113 on the validation set) performs worse than all other\nmodels. This is because it has not seen the training data. This also conﬁrms that\ngeneric models do not perform well on close-ended domain-speciﬁc tasks.\n5 Conclusion and Future Works\nAnalysing the results mentioned in the previous section, we conclude that the\nindividual models’ performances are comparable. We further observe that the\nensembling technique outperforms existing SOTA model SciBERT [1] on the\nvalidation set in terms of F1.\nIn future, we would like to experiment by replacing the RoBERTa [10] based\nembeddings with SciBERT [1] in the sub-systems mentioned in sections 3.1,\n3.2 and 3.3. Furthermore, we want to study how replacing the majority voting\nmethod of section 3.5 with a meta classiﬁer or a fully connected dense layer\naﬀects the overall performance. Finally, we shall be working on reducing the\nmodel size.\nReferences\n1. Beltagy, I., Lo, K., Cohan, A.: SciBERT: A pretrained language model for scientiﬁc\ntext. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Conference on Natural Language\n8 Sohom Ghosh and Ankush Chopra\nProcessing (EMNLP-IJCNLP). pp. 3615–3620. Association for Computational Lin-\nguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/v1/D19-1371,\nhttps://www.aclweb.org/anthology/D19-1371\n2. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn.\nRes. 3(null), 993–1022 (Mar 2003)\n3. Borrajo, M., Romero, R., Iglesias, E., Marey, C.: Improving imbalanced scientiﬁc\ntext classiﬁcation using sampling strategies and dictionaries. Journal of Integrative\nBioinformatics 8 (12 2011). https://doi.org/10.1515/jib-2011-176\n4. Cao, M.D., Gao, X.: Combining contents and citations for scientiﬁc document\nclassiﬁcation. In: Zhang, S., Jarvis, R. (eds.) AI 2005: Advances in Artiﬁcial Intel-\nligence. pp. 143–152. Springer Berlin Heidelberg, Berlin, Heidelberg (2005)\n5. Chang, M.W., Ratinov, L.A., Roth, D., Srikumar, V.: Importance of semantic\nrepresentation: Dataless classiﬁcation. In: AAAI (2008)\n6. Cortes, C., Vapnik, V.: Support-vector networks. Machine learning 20(3), 273–297\n(1995)\n7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers). pp. 4171–4186. Association for Computational Linguistics, Minneapo-\nlis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://www.\naclweb.org/anthology/N19-1423\n8. Ghanem, M.M., Guo, Y., Lodhi, H., Zhang, Y.: Automatic scientiﬁc text classiﬁ-\ncation using local patterns: Kdd cup 2002 (task 1). SIGKDD Explor. Newsl. 4(2),\n95–96 (Dec 2002). https://doi.org/10.1145/772862.772876, https://doi.org/10.\n1145/772862.772876\n9. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2017)\n10. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach (2019), http://arxiv.org/abs/1907.11692, cite arxiv:1907.11692\n11. Loshchilov, I., Hutter, F.: Fixing weight decay regularization in adam. CoRR\nabs/1711.05101 (2017), http://arxiv.org/abs/1711.05101\n12. Raﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,\nLi, W., Liu, P.J.: Exploring the limits of transfer learning with a uniﬁed text-\nto-text transformer. Journal of Machine Learning Research 21(140), 1–67 (2020),\nhttp://jmlr.org/papers/v21/20-074.html\n13. Reddy, Saichethan; Saini, N.: “sdpra 2021 shared task data”, mendeley data,\nv1, (2021). https://doi.org/10.17632/njb74czv49.1, https://data.mendeley.com/\ndatasets/njb74czv49/1\n14. Reddy, S., Saini., N.: Overview and insights from scope detection of the peer review\narticles shared tasks 2021 (forthcoming). In: Proceedings of The First Workshop\n& Shared Task on Scope Detection of the Peer Review Articles (SDPRA 2021)\n(2021)\n15. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\nRault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C.,\nJernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame, M., Lhoest, Q., Rush,\nA.M.: Transformers: State-of-the-art natural language processing. In: Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing:\nSystem Demonstrations. pp. 38–45. Association for Computational Linguistics,\nOnline (Oct 2020), https://www.aclweb.org/anthology/2020.emnlp-demos.6",
  "topic": null,
  "concepts": []
}