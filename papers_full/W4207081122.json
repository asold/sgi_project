{
  "title": "PCAT-UNet: UNet-like network fused convolution and transformer for retinal vessel segmentation",
  "url": "https://openalex.org/W4207081122",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2124640931",
      "name": "Danny Chen",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2107443204",
      "name": "Wenzhong Yang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2096375633",
      "name": "Liejun Wang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3127608164",
      "name": "Sixiang Tan",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A4209844321",
      "name": "Jiangzhaung Lin",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2479186328",
      "name": "Wenxiu Bu",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2124640931",
      "name": "Danny Chen",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2107443204",
      "name": "Wenzhong Yang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2096375633",
      "name": "Liejun Wang",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A3127608164",
      "name": "Sixiang Tan",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A4209844321",
      "name": "Jiangzhaung Lin",
      "affiliations": [
        "Xinjiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2479186328",
      "name": "Wenxiu Bu",
      "affiliations": [
        "Xinjiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2898910301",
    "https://openalex.org/W3010373058",
    "https://openalex.org/W2166524747",
    "https://openalex.org/W3169847394",
    "https://openalex.org/W2163344010",
    "https://openalex.org/W2119399382",
    "https://openalex.org/W2153351686",
    "https://openalex.org/W2945729873",
    "https://openalex.org/W2149640982",
    "https://openalex.org/W2488605601",
    "https://openalex.org/W2049822456",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2979865275",
    "https://openalex.org/W3096125778",
    "https://openalex.org/W2809144587",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3168825659",
    "https://openalex.org/W3144660641",
    "https://openalex.org/W3204614423",
    "https://openalex.org/W4293680532",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2150769593",
    "https://openalex.org/W2145305441",
    "https://openalex.org/W2072130234",
    "https://openalex.org/W3013766724",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W2979605896",
    "https://openalex.org/W3036597192",
    "https://openalex.org/W3160609330",
    "https://openalex.org/W2923997689",
    "https://openalex.org/W3122151326",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "The accurate segmentation of retinal vessels images can not only be used to evaluate and monitor various ophthalmic diseases, but also timely reflect systemic diseases such as diabetes and blood diseases. Therefore, the study on segmentation of retinal vessels images is of great significance for the diagnosis of visually threatening diseases. In recent years, especially the convolutional neural networks (CNN) based on UNet and its variant have been widely used in various medical image tasks. However, although CNN has achieved excellent performance, it cannot learn global and long-distance semantic information interaction well due to the local computing characteristics of convolution operation, which limits the development of medical image segmentation tasks. Transformer, currently popular in computer vision, has global computing features, but due to the lack of low-level details, local feature information extraction is insufficient. In this paper, we propose Patches Convolution Attention based Transformer UNet (PCAT-UNet), which is a U-shaped network based on Transformer with a Convolution branch. We use skip connection to fuse the deep and shallow features of both sides. By taking advantage of the complementary advantages of both sides, we can effectively capture the global dependence relationship and the details of the underlying feature space, thus improving the current problems of insufficient extraction of retinal micro vessels feature information and low sensitivity caused by easily predicting of pixels as background. In addition, our method enables end-to-end training and rapid inference. Finally, three publicly available retinal vessels datasets (DRIVE, STARE and CHASE_DB1) were used to evaluate PCAT-UNet. The experimental results show that the proposed PCAT-UNET method achieves good retinal vessel segmentation performance on these three datasets, and is superior to other architectures in terms of AUC, Accuracy and Sensitivity performance indicators. AUC reached 0.9872, 0.9953 and 0.9925, Accuracy reached 0.9622, 0.9796 and 0.9812, Sensitivity reached 0.8576, 0.8703 and 0.8493, respectively. In addition, PCAT-UNET also achieved good results in two other F1-Score and Specificity indicators.",
  "full_text": "RESEA RCH ARTICL E\nPCAT-UNet: UNet-like network fused\nconvolution and transformer for retinal vessel\nsegmentation\nDanny Chen\nID\n1,2\n, Wenzhong Yang\n1,2\n*, Liejun Wang\n1\n, Sixiang Tan\n1,2\n, Jiangzhaung Lin\n2\n,\nWenxiu Bu\n1,2\n1 College of Informatio n Science and Engineerin g, Xinjiang University, Urumqi, Xinjiang, China, 2 Key\nLaboratory of Multilingual Informatio n Technolo gy in Xinjiang Uygur Autonom ous Region, Xinjiang University,\nUrumqi, Xinjiang, China\n* ywz_xy @163.com\nAbstract\nThe accurate segmentation of retinal vessels images can not only be used to evaluate and\nmonitor various ophthalmic diseases, but also timely reflect systemic diseases such as dia-\nbetes and blood diseases. Therefore, the study on segmentation of retinal vessels images is\nof great significance for the diagnosis of visually threatening diseases. In recent years,\nespecially the convolutional neural networks (CNN) based on UNet and its variant have\nbeen widely used in various medical image tasks. However, although CNN has achieved\nexcellent performance, it cannot learn global and long-distance semantic information inter-\naction well due to the local computing characteristics of convolution operation, which limits\nthe development of medical image segmentation tasks. Transformer, currently popular in\ncomputer vision, has global computing features, but due to the lack of low-level details, local\nfeature information extraction is insufficient. In this paper, we propose Patches Convolution\nAttention based Transformer UNet (PCAT-UNet), which is a U-shaped network based on\nTransformer with a Convolution branch. We use skip connection to fuse the deep and shal-\nlow features of both sides. By taking advantage of the complemen tary advantages of both\nsides, we can effectively capture the global dependence relationship and the details of the\nunderlying feature space, thus improving the current problems of insufficient extraction of\nretinal micro vessels feature information and low sensitivity caused by easily predicting of\npixels as background. In addition, our method enables end-to-end training and rapid infer-\nence. Finally, three publicly available retinal vessels datasets (DRIVE, STARE and CHAS-\nE_DB1) were used to evaluate PCAT-UNet. The experimental results show that the\nproposed PCAT-UNET method achieves good retinal vessel segmentation performance on\nthese three datasets, and is superior to other architectures in terms of AUC, Accuracy and\nSensitivity performance indicators. AUC reached 0.9872, 0.9953 and 0.9925, Accuracy\nreached 0.9622, 0.9796 and 0.9812, Sensitivity reached 0.8576, 0.8703 and 0.8493,\nrespectively. In addition, PCAT-UNET also achieved good results in two other F1-Score and\nSpecificity indicators.\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 1 / 22\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Chen D, Yang W, Wang L, Tan S, Lin J,\nBu W (2022) PCAT-UN et: UNet-like network fused\nconvolutio n and transformer for retinal vessel\nsegmentati on. PLoS ONE 17(1): e0262689. https://\ndoi.org/10.13 71/journal.pone .0262689\nEditor: Gulistan Raja, Univers ity of Engineering &\nTechnology , Taxila, PAKISTA N\nReceived: October 1, 2021\nAccepted: January 2, 2022\nPublished: January 24, 2022\nCopyright: © 2022 Chen et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: Data are available at:\nhttps://blog s.kingston.ac. uk/retinal/cha sedb1/\nhttps://drive .grand-chal lenge.org/ http://ceca s.\nclemson.edu /*ahoover/s tare/.\nFunding: This research was funded by [the\nNational Natural Science Foundation of China]\ngrant number [No. U1603115 ], [Tiansh an\nInnovatio n Team of Xinjiang Uyqur Autonom ous\nRegion] grant number [No. 2020D14044] and\n[Science and Technology Project of Autonom ous\nRegion] grant number [No. 2020A02001- 1]. The\nfunders had no role in study design, data collection\n1 Introduction\nRetinal examination can provide important clinical information for the diagnosis of various\nretinal diseases such as diabetic retinopathy. However, artificial retinal examination requires\nsome clinicians or experts with strong expertise to screen a large number of retinal vessels\nimages. This process is time-consuming, tedious and difficult to implement batch processing,\nand may lead to diagnostic errors due to subjective factors [1]. In order to alleviate the shortage\nof medical resources and reduce the workload of doctors and specialists, retinal diseases\nrequire a computer-assisted automatic, high-performance retinal vessels segmentation system\nfor pre-screening and other examinations. The automatic retinal vessels segmentation system\ncan quickly and accurately obtain the structural characteristics of retinal vessels, in which\nbranch points and curves can be used to aid the diagnosis and analysis of cardiovessels disease\nand diabetic retinopathy. The change characteristics of retinal vessels width obtained after seg-\nmentation can be used to detect and analyze hypertension [2]. Therefore, the current research\non automatic segmentation of retinal blood vessels is an important development direction in\nthis field, and is also of great significance to the study of related retinal diseases [3, 4].\nIn recent decades, automatic segmentation of retinal vessels has been developing rapidly,\nand researchers have proposed a large number of retinal vessel segmentation methods. Some\ntraditional methods [5–10] have successfully performed automatic segmentation of retinal ves-\nsels images and obtained good segmentation, but they cannot fully characterize the image fea-\ntures, resulting in inadequate detection of retinal structural features and unsatisfactory\nsegmentation accuracy, which still cannot meet the needs of clinical diagnosis for auxiliary\nophthalmologists.\nCompared with traditional methods, CNN method combines the advantages of medical\nimage segmentation method and semantic segmentation method, making them achieve\nremarkable performance. Many previous excellent works [4, 11–15] have shown excellent seg-\nmentation performance in retinal vessels segmentation, which proves that CNN has strong fea-\nture representation learning and recognition ability. But due to the local inherent convolution\noperation, with the increase of the expansion of training data and the network layer, these\nmethods are hard to learn an explicit global and long-term semantic information interaction\n[16], the algorithm segmentation results in small blood vessels loss bifurcation and complex\ndiscrete curvature form, retinal vessels characteristics to distinguish between edge and back-\nground region is not obvious.\nRecently, Transformer [17], as an efficient network structure that relies on self-attention to\ncapture global information over long distances, has made remarkable achievements in the field\nof natural language processing. Many people consider that global information is also needed\nin visual tasks, and proper Transformer applications can help overcome the limitations of\nCNN. Therefore, researchers have devoted a great deal of effort to explore suitable Trans-\nformer for visual tasks. Early on, literature [18] tried to use CNN [19] to extract deep features,\nwhich were then fed into Transformer for processing and regression. Dosovitskiy [20] and\nCao [21] both proposed a pure Transformer network to classify and segment images respec-\ntively, and achieved great success. They split the image into multiple patches and use each\nvectorized patch as a word/tag in the NLP so that Transformer can be adopted directly. Subse-\nquently, building on the success of VIT, there has been a large literature of better Transformer\nbased architectures [22–27] that have achieved better performance than CNN. However, the\nvision transformer still has the problem of a large amount of calculation and insufficient local\ninformation extraction.\nAiming at the above problems, we proposed Patches Convolution Attention based Trans-\nformer UNet(PCAT-UNet) architecture, especially for 2D retinal vessels image segmentation.\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 2 / 22\nand analysis, decision to publish, or preparation of\nthe manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nIts goal is to combine Transformer and CNN more effectively and perfectly, complementing\neach other’s shortcomings. As shown in Fig 1, inspired by the success of [21, 28–30], we pro-\nposed Patches Convolution Attention Transformer (PCAT) block. It can better combine the\nadvantages of local feature extraction in CNN with the advantages of global information\nextraction in Transformer. In addition, Feature Grouping Attention Module(FGAM) is pro-\nposed to obtain more detailed feature maps of multi-scale characteristic information to supple-\nment the detailed information of retinal vessels. The features obtained are input to encoders\nand decoders built on both sides based on PCAT for feature fusion to further learn deep\nfeature representation. On this basis, we use PCAT block and FGAM to construct a new\nU-shaped network architecture with skip connections named PCAT-UNet. A large number of\nexperiments on DRIVE, STARE and CHASE_DB1 datasets show that the proposed method\nachieves good results, improves the classification sensitivity, and has good segmentation per-\nformance. In conclusion, our contribution can be summarized as follows:\nFig 1. The proposed archite cture of PCAT-UNet. Our PCAT-UNe t is composed of encoder and decoder constructed by PCAT blocks, convolut ional\nbranch constructe d FGAM, skip connectio n and right output layer. In our network, the PCAT block is used as a structur ally sensitive skip connection\nto achieve better informat ion fusion. Finally, the side output layer uses the fused enhanced feature map to predict the vessels segmentati on map of each\nlayer along the decoder path.\nhttps://doi.o rg/10.1371/j ournal.pone .0262689.g001\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 3 / 22\n• We designed two basic modules, PCAT and FGAM, which are both used to extract refined\nfeature maps with rich multi-scale feature information and to integrate feature information\nextracted from them to achieve complementary functions.\n• On the basis of [29], we improved the self-attention of the original Transformer and pro-\nposed the attention between different patches in the feature map and the attention between\npixels in a patch. They are Cross Patches Convolution Self-Attention (CPCA) and Inner\nPatches Convolution Self-Attention (IPCA). This not only reduces the amount of calculation\nas the input resolution increases, but also effectively maintains the interaction of global\ninformation.\n• Based on the PCAT block, we constructed a U-shaped architecture composed of encoder\nand decoder with Skip Connection. The encoder extracts spatial and semantic information\nof feature images by down-sampling. The decoder up-samples the deep features after fusion\nto the input resolution and predicts the corresponding pixel-level segmentation. Further-\nmore, we integrate the convolutional branch based on FGAM module into the middle of the\nU-shaped structure, and input the feature information extracted from it into the encoder\nand decoder on both sides to fuse depth and shallow features to improve the learning ability\nof the network.\n• In addition, we added the DropBlock layer after the convolutional layer in the convolutional\nbranch and side output, which can effectively suppress over-fitting phenomenon in training.\nExperiments show that our model has some performance advantages over the CNN-based\nretinal vessel segmentation model, which improves its classification sensitivity and has good\nsegmentation comprehensive performance.\n2 Relate work\n2.1 Early traditional methods\nAmong many effective traditional methods proposed, Chaudhuri et al. [3] first proposed a\nfeature extraction operator based on the optical and spatial characteristics of the object to be\nrecognized. Gaussian function was used to encode the vessels image, achieving good segmen-\ntation effect, but the structural characteristics of retinal vessels were ignored, and the calcula-\ntion time was long. Subsequently, Soares et al. [5] used two-dimensional Gabor filter to extract\nretinal vessels image features, and then used Bayesian classifier to divide pixels into two catego-\nries, namely retinal vessels and background. Yan et al. [9] used mathematical morphology to\nsmooth vessels edge information, enhance retinal vessels images and suppress the influence of\nbackground information, then applied fuzzy clustering algorithm to segment the enhanced ret-\ninal vessels images, and finally purified the fuzzy segmentation results and extracted vessels\nstructures.\n2.2 Network based on CNN\nFor the past ten years, convolutional neural network based retinal vessel segmentation\nmethods have been able to predict the pixel points of each vessel and non-vessel category in\nretinal vessels images, and obtain structural information including the bifurcation pattern,\nscale size and complex curvature of retinal vessels. Wang et al. [11] used CNN to extract the\nfeatures of retinal vessels, and then combined with random forest (RF) method to segment\nthe vessels, which improved the segmentation accuracy but required too long training time.\nIn addition, Ronneberger et al. [12] proposed u-NET based on FCN. It uses skip-connection\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 4 / 22\nto fuse shallow features extracted from encoder and deep features extracted from decoder,\nso as to obtain more detailed features and improve the precision of edge segmentation of\nmicro vessels. Di Li et al. [31] adopted a new residual structure and applied attention\nmechanism to improve segmentation performance in the process of jumping connection.\nOliveira et al. [15] proposed a patch-based full-convolutional network segmentation method\nfor retinal vessels. Compared with traditional methods, CNN method combines the advan-\ntages of medical image segmentation method and semantic segmentation method, and\nshows good segmentation performance in retinal vessels datasets, which proves that CNN\nhas strong feature learning and recognition ability. Based on the merits of CNN, we propose\na Feature Grouping Attention Module(FGAM), which can extract rich multi-scale feature\nmaps.\n2.3 Network based on visual transformer\nIn reference [20, 23, 26, 28, 29, 32, 33], transformer is introduced into visual tasks as an image\nextraction device. It is helpful to overcome the problem that CNN needs to stack more layers\nto expand the receptive field by using the multi head self attention mechanism and properly\nadjusting the transformer. In [20], ViT (Vision Transformer) takes the two-dimensional image\nblock embedded with location information as the input of image recognition task, and has\nachieved the same performance as the network based on CNN on the large dataset. Literature\n[23] extends the applicability of transformer and proposes a hierarchical network. Swin Trans-\nformer is taken as the backbone of the network structure, and each patch is used as a window\nto extract the internal correlation of patches, and offset windows are used to capture more fea-\ntures. However, the interaction between local information interaction and adjacent patches in\n[23] lacks global information interaction. Literature [29] proposed a new attention mechanism\nin transformer, which captures local information by alternating attention within image blocks\ninstead of the whole image, and applies attention to capture global information between image\nblocks divided from single-channel feature maps. Performance achieved is comparable to cur-\nrent CNN-based and Transformer-based networks. Therefore, inspired by this, we proposed a\nPCAT block based on the improvement of CAB (Cross Attention Block) in [29]. It can effec-\ntively maintain local and global information interaction while avoiding the huge increase of\ncomputation with the increase of input resolution.\n2.4 Self-attention/tra nsformer combined with CNN\nIn recent years, [32–36] has been committed to combining CNN and transformer more effec-\ntively, trying to introduce self-attention mechanism into CNN, and even using self-attention\nlayer to replace part or all of the popular spatial convolutional layer, so as to break the domi-\nnant position of CNN in medical image segmentation. In [37], skip connection with additional\nattention gate is integrated into U-shaped structure for medical image segmentation, but it is\nstill based on CNN method. In [33, 35, 36], the author combines transformer with CNN to\ndesign a powerful encoder for medical image segmentation. At present, most combinations of\nTransformer and CNN are applied to medical image segmentation such as heart segmentation\n[32] and multi-modal brain tumor segmentation [38]. Different from the above methods, we\npropose to integrate the convolutional branch based on CNN into the U-shaped structure\nbased on Transformer to explore the application potential of our model in retinal vessels seg-\nmentation to improve the segmentation ability of the model by utilizing the complementarity\nof Transformer and CNN, and try to explore the application potential of our model in retinal\nvessels segmentation.\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 5 / 22\n3 Method\nIn this section, we will detail general framework which we proposed for a fusion convolution\nand transformer like UNet network for retinal vessels segmentation. Specifically, section 3.1\nmainly describes the overall network framework proposed by us, as shown in Fig 1. Then, in\nSections 3.2 and 3.3, we detail the two main base modules: Patches Convolution Attention\nBased Transformer(PCAT) block and Feature Grouping Attention Module (FGAM). Finally,\nin Section 3.4, we describe other important modules of the network model.\n3.1 Architecture overview\nThe overall architecture we propose of PCAT-UNet as shown in Fig 1. PCAT -UNet consists\nof encoder, decoder, convolution branch, skip connection and side output layer. Its basic units\nare PCAT block and FGAM. Patch embedding module is adopted according to to [23, 29], and\n4 × 4 convolution with stride 4 is used to divide it into\nH\n4\n�\nW\n4\npatch tokens without overlap to\ntransform 2D input image into one-dimensional sequence embedding. Each patch was flat-\ntened to 48 (4 × 4 × 3) elements, and the feature channel of each token was extended from 48\nto C with a linear embedding layer. Then we superimposed several PCAT blocks and patch\nmerging layers for feature extraction at different scales to obtain layered element representa-\ntions. The patch merging layer implements down-sampling and adding channel dimensions,\nand the PCAT block completes the learning process of feature representation.\nDue to the redundancy of image information in middle convolutional branch, we con-\nvolved the input image first and preliminarily extracted the shallow features, which can reduce\nthe memory usage. Then, the convoluted feature map is input into the convolutional branch\nstacked by five FGAM to extracte local feature information. Because the local feature of convo-\nlution can better extract the detailed features of the retinal microvessels, we input them to the\nencoder and decoder, which can integrate the local feature information of the image with the\nglobal feature information learned by transformer module to supplement the detailed features\nwell.\nInspired by U-Net [12], which has a good symmetric structure, we designed a symmetric\ndecoder based on PCAT. The decoder is composed of PCAT blocks and patch restoring layer\nstacked. Unlike the patch merging layer, the patch restoring layer is specifically designed for\nup-sampling. In order to reduce the loss of spatial information caused by down-sampling, we\npropose a skip connection, which fuses the output of the previous module of each patch restor-\ning layer with the output of the middle convolutional branch, and the fusion result is the input\nof each patch restoring layer. Each patch restoring layer reshapes the input feature maps into\nlarge feature maps with a resolution of 2 times. Then, after deep features produced by each lay-\ner’s PCAT block are fused with spatial local features from the convolutional branch, up-sam-\npling is performed at the corresponding multiples to restore the resolution of the feature map\nto the input resolution. Finally, DropBlock, Batch Normalization, and ReLu are performed on\nthe features sampled at each layer. And then through 1 × 1 convolution to output the pixel\nlevel segmentation prediction graph. We describe each module in detail below.\n3.2 Patch convolution attention based transformer block\nIn Fig 2(a), There are two continuous PCAT(Patch Convolution Attention-based Trans-\nformer) blocks, in which feature channels and resolution remain unchanged. The traditional\ntransformer is built on the basis of multi-head self-attention (MHSA) [20], which concatenates\nthe results of multiple heads and transforms them using a FeedForward network. Similarly,\neach PCAT block consists of a LayerNorm (LN) layer, multi-head attention modules, residual\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 6 / 22\nlinks, and MLPs. However, the multi-head Attention modules we have continuously used are\nCross Patches Convolution Self-Attention (CPCA) Module and Inner Patches Convolution\nSelf-Attention (IPCA) module. The CPCA module is used to extract the attention between\npatches in a feature map, while the IPCA module is used to extract and integrate the global fea-\nture information between pixels in one patch. Based on such patches partitioning mechanism,\ncontinuous PCAT blocks can be expressed as:\nY\nn\n¼ CPC AðLN ðY\nn\u0000 1\nÞÞ þ Y\nn\u0000 1\nð1Þ\nY\nn\n¼ MLPðLN ð Y\nn\nÞÞ þ Y\nn\nð2Þ\nY\nnþ1\n¼ IPCAðL N ðY\nn\nÞÞ þ Y\nn\nð3Þ\nY\nnþ1\n¼ MLPðLN ð Y\nnþ1\nÞÞ þ Y\nnþ1\nð4Þ\nwhere Y\nn−1\nand Y\nn+1\nare the input and output of the PCAT block respectively. Y\nn\nand Y\nn+1\nare\nthe output of the CPCA module and IPCA module respectively. The size of image patches on\nthe CPCA module and IPCA module is 8×8, and the number of heads is 1 and 8 respectively.\n3.2.1 Cross Patches Convolution Self-Attention. The size of the receptive field has a\ngreat influence on the segmentation effect for retinal vessel segmentation. Convolutional ker-\nnels are often stacked to enlarge the receptive field in models based on convolutional neural\nnetworks. In practice, we expect the final receptive field to extend over the entire frame. Trans-\nformer [20, 26] is naturally able to capture global information, but not enough local details. As\neach single-channel feature map naturally has global spatial information, we proposed the\nCross Patches Convolution Self-attention (CPCA) module inspired by this. The single-channel\nfeature map is divided into patches, which are stacked and reshaped into original shapes by\nFig 2. Structure diagram of PCAT block and PCA. (a) two consecutive PCAT blocks.C PCA and IPCA are multi-\nhead self-attent ion modules with cross and inner patching configurations , respectively. (b)The detailed structure of the\nPCA, a MHSA based on convolu tion.\nhttps://d oi.org/10.1371/j ournal.pon e.0262689.g0 02\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 7 / 22\nCPSA module. Patch Convolution self-attention (PCA) in the CPCA module can extract\nsemantic information between patches in each single-channel feature map, and the informa-\ntion exchange between these patches is very important for obtaining global information in the\nwhole feature map. This operation is similar to the depth-separable convolution used in [39,\n40], which can reduce the number of parameters and operation cost.\nWe improved and proposed the CPCA Module based on [29], and expected that the final\nreceptive field in the experiment could include the whole feature map. Specifically, the CPCA\nModule divides each single-channel feature map into patches of\nH\nN\n�\nW\nN\nin size, and then uses\nPCA in the CPCA module to obtain different features among patches in each single-channel\nfeature map. Since the number of heads is set to be the same as the patches size, which is not\nuseful for performance, and each head self-attention could have noticed different semantic\ninformation among image patches, the number of heads is set to 1 in our experiment.\nThe multi-head self-attention adopted by CPCA Module is PCA. A little different from the\nprevious work [23, 29, 41], PCA is the MHSA based on convolution, and its detailed structure\nis shown in Fig 2(b). Specifically, the divided patches X 2 R\nM\n2\n�d\nare projected into query\nmatrix Q 2 R\nM\n2\n�d\nkey matrix K 2 R\nM\n2\n�d\nand value matrix V 2 R\nM\n2\n�d\nthrough three 1 × 1 linear\nconvolutions, where M\n2\nand d represent the number of patches and dimension of query or key\nrespectively. In the next step, we reshape K and V into 2D space, conduct a 3 × 3 convolution\noperation and Sigmoid operation successively for the reshaped K, and then expand the result\ninto a one-dimensional sequence to obtain K. Similarly, after 3 × 3 convolution operation, the\nreshaped V is also expanded into a 1-dimensional sequence to obtain V. Transformer has a\nglobal receptive field and can better obtain long-distance dependence, but it lacks in obtaining\nlocal detail information. However, we apply a two-dimensional convolution operation on K\nand V to better supplement local detail feature information. So, the proposed formula for self-\nattention is now:\nAtten tio nðQ; K ; V Þ ¼ Sof tma x\nQ � ðsðdðK ÞÞÞ\nT\nffiffi ffi\nd\np\n !\ndðV Þ þ V ð5Þ\nAtt enti onðQ; K ; V Þ ¼ Soft max\nQ � ðsðdðK ÞÞÞ\nT\nffiffi ffi\nd\np þ B\n !\ndðV Þ þ V ð6Þ\nwhere Q; K ; V 2 R\nM\n2\n�d\nare query, key and value matrices, M\n2\nand d are patch number and\nquery or key dimension respectively. B 2 R\nM\n2\n�M\n2\nwhose value is taken from the deviation\nmatrix 2 R\n(2M−1)×(2M+1)\n, δ represents a 3 × 3 convolution operation on the input, σ represents\nthe sigmoid function, and then expands the result into a one-dimensional sequence, so\nK = σ(δ(K)), V = (δ(V). The acquired attention is added with a residual connection V to sup-\nplement the information lost by convolution, and the final self-attention is obtained.\n3.2.2 Inner Patches Convolution Self-Attention. Our PCAT block is designed to com-\nbine attention between patches with attention within patches. Different semantic information\nbetween patches can be obtained through CPCA to realize the information exchange of the\nwhole feature map. But the interrelationship between pixels within patches is also crucial. The\nproposed Inner Patches Convolution self-attention (IPCA) module considers the relationship\nbetween pixels within Patches and can obtain the Attention between pixels within each patch.\nInspired by the local feature extraction characteristics of CNN, we introduce the locality of\nconvolution method in CNN into Transformer. IPCA Module divides multiple-channel fea-\nture images into patches with a size of\nH\nN\n�\nW\nN\n, regards each patch divided as an independent\nattention range, and uses multi-head self-attention for self-attention of all pixels in each patch\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 8 / 22\ninstead of the whole feature map. Multi-head self-attention used in IPCA module is also PCA.\nHowever, CPCA module extracts self-attention from a complete single-channel feature map,\nwhile IPCA Module is self-attentional from pixels in each patch of multi-channel feature\nmaps. In addition, PCA of the two are slightly different. The self-attention in IPCA Module\nadopts relative position encoding, so its self-attention calculation is shown in Eq (6).\n3.3 Feature Grouping Attention Module\nIn [30], a new pyramid segmentation attention (EPSA) module is used to build an efficient\nbackbone network. The structure of EPSA module is shown in Fig 3(a), which has strong\nmulti-scale representation ability and can adaptively re-calibrate the weight of cross-dimen-\nsional channels. However, for the images of retinal blood vessels processed by us, the large-\nscale convolution adopted by PSA in EPSA module is easy to cause the loss of features of tiny\nblood vessels, thus failing to obtain good segmentation results of marginal blood vessels. On\nthis basis, we propose an effective Feature Grouping Attention Module(FGAM), as shown in\nFig 3(b). Different from EPSA module, FGAM only adopts a 1×1 convolution, replacing the\noriginal final convolution layer with a 2×2 pooling layer to realize the function of sampling\nunder the convolution branch, and adding a DropBlock [42] after convolution and FGA.\nDropBlock combined with Batch Normalization layer (BN) and ReLu activation unit, can\neffectively prevent network overfitting and speed up network convergence.\nIn addition, in order to reduce the loss of vessel edge features, FGA(feature grouping atten-\ntion) in FGAM does not employ the original pyramid structure (e.g. group convolution kernels\nare 3, 5, 7, 9), but small scale grouping convolution is used (group convolution kernels are 1, 1,\n3, 3), and different multi-group convolution kernels are used for parallel processing to obtain\nchannel feature maps with different spatial resolutions. Specifically, our FGA divides the chan-\nnels of input feature map X into part S(default is 4) on average, and applies multiple groups of\nconvolution (default group size of each part is 2,4,8,8 respectively) to the corresponding part\nto extract small-scale features, so as to obtain different feature maps on the channel. Then,\nSEWeight module [43] was used to extract the attention of feature images in different\nFig 3. Structure comparison of EPSA module and FGAM. (a) EPSA module; (b) FGAM.\nhttps://d oi.org/10.1371/j ournal.pon e.0262689.g0 03\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 9 / 22\nproportions, and Softmax was used to calibrate the channel attention vector. Finally, the re-cal-\nibrated weight and corresponding feature maps were used for element-level product operation.\nTherefore, using FGA, we can integrate multi-scale spatial information and cross-channel\nattention into each feature grouping block, obtain better information fusion between local and\nglobal channel attention, and obtain feature maps with richer details. This process follows the\nmethod of [30] and will not be described in this paper.\nIn order to verify the segmentation performance of FGAM in the proposed model on reti-\nnal vessels, we replaced FGAM in the proposed method PCAT-UNet with EPSA module in\nSections 4.2.2 ablation experiment to obtain the PCAT-UNet (EPSA) model. Then, its segmen-\ntation performance on DRIVE, STARE and CHASE_DB datasets was compared with that of\nthe proposed method in this paper, as shown in Table 9. Finally, we will analyze the compari-\nson results in Sections 4.4.2 below.\n3.4 Other important modules\n3.4.1 Patch embedding layer. Our patch embedding layer consists of an up-sampling and\nlinear projection layer (which is a 4 × 4 convolution with stride 4.). For retinal vessels images,\nthe larger the pixel, the better the segmentation effect. Therefore, the 512 × 512 input image\nwas up-sampling twice, and the image was enlarged to 1024 × 1024 pixels. Then it is divided\ninto non-overlapping\nH\nN\n�\nW\nN\n(H = 1024, W = 1024) patches of tokens, and each fragment is flat-\ntened into 48 elements. Finally, the feature dimension of each token is extended from 48 to C\nwith a linear embedding layer.\n3.4.2 Patch merging layer. In the encoder based on PCAT module, the patch merging\nlayer is used to reduce the number of tokens and increase the feature dimension. Specifically,\npatches are divided into four parts, which can reduce the resolution of features by a factor of 2\nand increase the dimension of features by a factor of 4 by splicing. Feature information is easily\nlost during down-sampling, so we consider using FGAM to obtain better information fusion\nbetween local and global channel attention to supplement detail features. FGAM here main-\ntains the feature dimension and resolution unchanged. Finally, a linear layer is applied to the\nfusion feature, turning the feature dimension into twice the input dimension. This process is\nrepeated three times in the encoder.\n3.4.3 Patch restoring layer. Corresponding to the encoder is a symmetric decoder based\non the PCAT module. Patch restoring layer is used in the decoder to up-sample deep features\nand reduce their feature dimensions. Like the patch merging layer, patch restoring layer also\nuses FGAM to obtain better information fusion between local and global channel attention,\nand then adopts deconvolution operation to reshape the input into a feature map with higher\nresolution (2× up-sampling), and then reduces its feature dimension to\n1\n2\nof the input dimen-\nsion. This process is repeated four times in the decoder.\n3.4.4 DropBlock. DropBlock [42] is a structured form of dropout that can effectively pre-\nvent network overfitting problems. In the feature map, it discards contiguous regions rather\nthan individual random units. In this way, part of redundant semantic information can be\neffectively discarded and more effective information can be encouraged to be learned on the\nnetwork. We use a DropBlock, a Batch Normalization (BN) layer, and a ReLu activation unit\nto construct a structured module. This module is used in both the side output layer in our net-\nwork and in FGAM. See Figs 1 and 3(b).\n4 Experiment\nIn this chapter, we describe model experiments to evaluate our architecture for retinal vessel\nsegmentation tasks in detail. We describe the datasets and implementation details for the\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 10 / 22\nexperiment in Sections 4.1 and Sections 4.2, respectively. Then, the evaluation criteria are\nintroduced in Sections 4.3. Finally, Sections 4.4 introduces experimental results and analysis,\nincluding comparison with existing advanced methods and ablation experiments to prove the\neffectiveness of our network.\n4.1 Datasets\nWe used three publicly available datasets: DRIVE [44], STARE [45] and CHASE_DB1 [46] to\nconduct retinal vessels segmentation experiments to evaluate the performance of our\napproach. Detailed information about these three datasets is shown in Table 1.\nAll three datasets have different image formats, sizes and numbers. The DRIVE dataset con-\ntains 40 color retinal vessels images in tif format (565 × 584). The dataset consists of 20 train-\ning images and 20 test images. The STARE dataset consisted of 10 with and 10 without lesions\ncolor retinal vessels images in a PPM (605 × 700) format. The CHASE_DB1 dataset contains\n28 images in JPG (999 × 960) format. Unlike the DRIVE dataset, the STARE and CHASE_DB1\ndatasets do not have a formal classification of training and test datasets. To better compare\nwith other methods, we follow the same protocol for data segmentation as [47]. For the\nSTARE dataset, 16 images were selected as the training set and the remaining 4 images as the\ntest set. In the CHASE_DB1 dataset, 20 images were selected as the training set and the\nremaining 8 images as the test set. In addition, the DRIVE dataset comes with a FoV mask,\nwhile the other two datasets do not provide FoV masks. For fair evaluation, we use FoV masks\nfor the two datasets provided by [47]. Finally, each dataset test set contains two sets of com-\nments, and in keeping with the other methods, we used the comments from the first group as\nground truth to evaluate our model.\nIn Table 1, we noticed that the original size of the three datasets was not suitable for the\ninput image size of our network, so we adjusted the image size of the datasets and unified it\ninto 512×512 image as the input. In addition, to increase the number of training samples, we\nadopted the histogram stretching data enhancement method for all three datasets, doubling\nthe training sets of the three original datasets to 40, 32 and 40 images respectively. In Fig 4, ret-\ninal vessels images of these three datasets, corresponding retinal vessels images generated by\nhistogram stretching enhancement method and corresponding FOV mask can be seen.\n4.2 Implementation details\n4.2.1 Settings. In the experiment of retinal vessel segmentation, we train the network for\n250 epochs. The initial learning rate is 0.001 and decays 10 times every 50 epochs. And we set\nthe batch size to 4. Adam optimizer and binary cross entropy loss function were used to train\nthe PCAT-UNet network. The entire network trains from scratch, with no additional training\ndata. We trained the network on DRIVE, STARE and CHASE_DB1 training sets and evaluated\nit on the respective validation sets for each dataset. Our experiment was conducted on an NVI-\nDIA Tesla V100 32GB GPU.\n4.2.2 Implementation. The DropBlock module is used in both FGAM and the side output\nlayer in our network. DropBlock takes two main parameters: block_size, which specifies the\nTable 1. The specific information of DRIVE, STARE and CHASE_DB 1 datasets.\nDatasets Quantity Resolut ion Train-Test split\nDRIVE 40 565 × 584 Official train-test split\nSTARE 20 605 × 700 First 16 for train, last 4 for test\nCHASE_D B1 28 999 × 960 First 20 for train, last 8 for test\nhttps://do i.org/10.1371/j ournal.pone .0262689.t001\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 11 / 22\nsize of the block to be discarded, and γ, which specifies the number of features to be discarded.\nThe DropBlock parameters in FGAM are set to block_size = 7, γ = 0.9, and the DropBlock\nparameters in the side output layer are set to block_size = 5, γ = 0.75. The input and output\nsizes of the network are 512 x 512.\n4.2.3 Data augmentation. The input of our network is the image of the entire retinal\nblood vessels, and the output is consistent with the input size. And we observe that data aug-\nmentation can overcome the problem of network overfitting, so we use some data augmenta-\ntion methods to improve the network performance. The augmentation methods used in the\nFig 4. Retinal vessels images of the dataset (line 1), correspond ing retinal vessels images generated by histogram stretchi ng enhancemen t\n(line 2) and correspond ing FOV mask (line 3); (a) DRIVE (b) STARE (c) CHASE _DB1.\nhttps://do i.org/10.1371/j ournal.pone .0262689.g00 4\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 12 / 22\nnetwork include gaussian transform with probability 0.5, where Sigma = (0, 0.5), random rota-\ntion in the range [0,20˚], random horizontal flip with probability 0.5, and gamma contrast\nenhancement in the range [0.5, 2]. These methods can enhance the training image and avoid\nnetwork overfitting, which can greatly improve the performance of the model.\n4.3 Evaluation criteria\nThe process of retinal blood vessel segmentation is the classification of pixels. All pixels are\nclassified as vessel pixels or background pixels. To evaluate the performance of our model, we\ncompared the segmentation results with the corresponding ground truth and divided the com-\nparison results of each pixel into true positive (TP), false positive (FP), false negative (FN) and\ntrue negative (TN). Then, we adopted specificity (Sp), sensitivity (Se), accuracy (ACC),\nF1-Score (F1) and AUC: area under the receiver operating characteristic (ROC) as the mea-\nsurement indexes:\nSp ¼\nTN\nTN þ FP\nð7Þ\nSe ¼ Rec all ¼\nTP\nTP þ FN\nð8Þ\nAcc ¼\nTP þ TN\nTP þ FN þ TN þ FP\nð9Þ\nPrec isio n ¼\nTP\nTP þ FP\nð10Þ\nF 1 ¼ 2 �\nPrec isi on � Rec all\nPrec isi on þ Rec all\n¼\n2TP\n2TP þ FP þ FN\nð11Þ\nwhere, TP represents positive truth value when a vessel pixel in ground truth is correctly classi-\nfied in the predicted image, TN represents negative truth value when a non-vessel pixel in\nground truth is correctly classified in the predicted image, FN misclassifies the vessel pixels in\nground truth as non-vessel pixels in the predicted image, and FP means that the non-vessel\npixels in the ground truth are wrongly marked as the vessel pixels in the predicted image. Pre-\ncision and Recall mean Precision Ratio and Recall Ratio respectively, while the ROC curve rep-\nresents the proportion of vessels correctly classified as vessel pixels versus misclassified as non-\nvessel pixels. AUC refers to the area under the ROC curve, which can be used to measure the\nsegmentation performance. The closer the AUC value is to 1, the closer the system perfor-\nmance is to perfection.\n4.4 Experimental results and analysis\n4.4.1 Compare with existing methods. We compared PCAT-UNet architecture with the\nbest existing retinal vessel segmentation methods on DRIVE, STARE and CHASE_DB1 data-\nsets, including UNet [12], DFUNet [1], LadderNet [48], Denseblock-UNet [49], DEUNet [50],\nIterNet [24], DGUNet [51] and Pyramid U-Net [52], etc. We summarize the release years of\nthese methods and the performance comparisons across the three datasets in Tables 2–4. All\nthree tables report traditional measures such as F1-score, Sensitivity, Specificity, Accuracy and\nAUC-ROC. For these three datasets, we only count pixels in the FOV.\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 13 / 22\nAs shown in Tables 2–4, our model achieves the best performance on DRIVE, STARE, and\nCHASE_DB1 datasets, significantly outperforming UNet-derived network architectures.\nAmong them, the sensitivity, accuracy and AUC-ROC values (three main indicators of this\ntask) obtained by method we proposed were the highest in the three datasets, which were\n0.8576/0.8703/0.8483, 0.9622/0.9796/0.98 12, 0.9872/0.9953/0.9925 respectively. In addition,\nour model achieved the highest F1-score on STARE and CHASE_DB1 datasets (0.8836 and\n0.8273, respectively), and the highest specificity on DRIVE and CHASE_DB1 datasets (0.9932\nTable 2. Performance comparis on of different segmentat ion metho ds on the DRIVE dataset.\nMethods Year F1-score SE SP ACC AUC\nHuman Observer - N.A 0.7760 0.9724 0.9472 0.8742\nU-Net [12] 2015 0.8142 0.7537 0.9820 0.9531 0.9755\nResidu al UNet [53] 2018 0.8149 0.7726 0.9820 0.9553 0.9779\nR2UNet [53] 2018 0.8171 0.7792 0.9813 0.9556 0.9784\nDFUNet [1] 2019 0.8190 0.7863 0.9805 0.9558 0.9778\nLadderNet [48] 2019 0.8202 0.7856 0.9810 0.9561 0.9793\nDEUNet [50] 2019 0.8270 0.7940 0.9816 0.9567 0.9772\nIterNet [47] 2020 0.8205 0.7735 0.9838 0.9573 0.9816\nDGUNet [51] 2020 N.A 0.7614 0.9837 0.9604 0.9846\nNest U-Net [54] 2021 0.7863 0.8060 0.9869 0.9512 0.9748\nPyramid U-Net [52] 2021 N.A 0.8213 0.9807 0.9615 0.9815\nPCAT-U Net(Ours) 2021 0.8160 0.8576 0.9932 0.9622 0.9872\nhttps://do i.org/10.1371/j ournal.pone .0262689.t002\nTable 3. Performance comparis on of different segmentat ion metho ds on the STARE dataset.\nMethods Year F1-score SE SP ACC AUC\nHuman Observer - N.A 0.8952 0.9384 0.9349 0.9898\nU-Net [12] 2015 0.8373 0.8270 0.9842 0.9690 0.9830\nDenseBloc k-UNet [49] 2018 0.7691 0.6807 0.9916 0.9651 0.9755\nDFUNet [1] 2019 0.7629 0.6810 0.9903 0.9639 0.9758\nIterNet [47] 2020 0.8146 0.7715 0.9886 0.9701 0.9881\nNest U-Net [54] 2021 0.8230 0.8230 0.9945 0.9641 0.9620\nPCAT-UNet(O urs) 2021 0.8836 0.8703 0.9937 0.9796 0.9953\nhttps://do i.org/10.1371/j ournal.pone .0262689.t003\nTable 4. Performance comparis on of different segmentat ion metho ds on the CHASE _DB1 dataset.\nMethods Year F1-score SE SP ACC AUC\nHuman Observer - N.A 0.7686 0.9779 0.9560 0.8733\nU-Net [12] 2015 0.7783 0.8288 0.9701 0.9578 0.9772\nDenseBlock-U Net [49] 2018 0.8006 0.8178 0.9775 0.9631 0.9826\nDFUNet [1] 2019 0.8001 0.7859 0.9822 0.9644 0.9834\nLadderNet [48] 2019 0.8031 0.7978 0.9818 0.9656 0.9839\nDEUNet [50] 2019 0.8037 0.8074 0.9821 0.9661 0.9812\nIterNet [47] 2020 0.8073 0.7970 0.9823 0.9655 0.9851\nDGUNet [51] 2020 N.A 0.7993 0.9868 0.9783 0.9869\nPyramid U-Net [52] 2021 N.A 0.8035 0.9787 0.9639 0.9832\nPCAT-UNet(O urs) 2021 0.8273 0.8493 0.9966 0.9812 0.9925\nhttps://do i.org/10.1371/j ournal.pone .0262689.t004\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 14 / 22\nand 0.9966, respectively). Excellent sensitivity indicated a higher True Positive Rate of our\nmethod compared to other methods including second-person observers, and excellent speci-\nficity indicated a lower False Positive Rate of our method than other methods. This further\nindicates that, compared with other methods, our proposed method can identify more True\nPositive and True Negative retinal vessel pixels. In Fig 5, we can see ROC Curves for the three\ndatasets and the high and low True Positive Rates (TPR) and False Positive Rates shown in the\nfigures. Therefore, according to the above analysis results, PCAT-UNet architecture we pro-\nposed achieves the most advanced performance in the retinal vessel segmentation challenge.\nTable 2 shows that compared with the maximum value of each indicator in the existing\nmethod on the DRIVE dataset, the PCAT-UNet proposed by us achieved higher performance,\nin which SE increased by 3.63%, SP by 0.63%, ACC by 0.07% and AUC by 0.26%. F1-score\nalso scored well, with a difference of just 1.1%. We compared the prediction results of the\nmethod we proposed with UNet method on DRIVE dataset, as shown in Fig 6. The enlarged\ndetails of some specific blood vessels are given in the figure. Some unsegmented microvessels\nand important cross blood vessels in the segmentation map of UNet can be seen in the seg-\nmentation map of the method in this paper. Therefore, the segmentation figure of the method\nin this paper is more accurate and closer to ground truths than that of the method in UNet.\nAs can be seen from Table 3, compared with the maximum values of each indicator of exist-\ning methods, the method in this paper achieved higher performance in STARE dataset, in\nwhich F1-Score, SE, ACC and AUC increased by 4.63%, 4.33%, 0.95% and 0.72% respectively.\nMoreover, the results of SP are very competitive, with a difference of only 0.08%. In addition,\nFig 7 also demonstrates that our approach is more efficient. We can see Fig 7 showing the seg-\nmentation results of original retinal images, ground truths, UNet method, and our method on\nthe STARE dataset. It can be observed from the enlarged image that the proposed method has\na stronger ability to detect the pixels of cross vessels and can effectively identify the pixels of\nretinal vessels. The segmentation result is more accurate than that of UNet method.\nAs can be seen from Table 4, our proposed PCAT-UNet method is superior to the most\nadvanced method in all indicators of the CHASE_DB1 dataset. Among them, compared with\nthe maximum value of each indicator in the existing methods in the table, our experimental\nresults increased F1-Score by 2%, SE by 2.05%, SP by 0.98%, ACC by 0.29% and AUC by\n0.56%. Fig 8 also provides a comparison of the segmentation results of original Retinal images,\nground truths, UNet method and this method on the CHASE_DB1 dataset. From the enlarged\nFig 5. ROC Curves on (a) DRIVE (b) STARE (c) CHASE _DB1.\nhttps://doi.o rg/10.1371/j ournal.pone .0262689.g005\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 15 / 22\npicture of details, we can see that the segmentation effect of the proposed method is better\nthan that of UNet method in the segmentation of micro-vessels, blood vessel intersection and\nblood vessel edge, which further proves the superiority of our method.\n4.4.2 Ablation study. To investigate the effects of different factors on model performance,\nwe conducted extensive ablation studies on DRIVE, STARE and CHASE_DB1 datasets. Specif-\nically, we explore the influence of the backbone, FGAM, convolutional branch and Dropblock\non our model. The research results are shown in Tables 5–7. Backbone. In the ablation experi-\nment of this paper, we will build a backbone with the pure Transformer module. Specifically,\nbackbone is a U-shaped network composed of encoder constructed by PCAT Block and Patch\nEmbedding Layer and decoder constructed by PCAT Block and Patch Restoring Layer.\nIn these three tables, the first line is the experimental result of backbone built with pure\ntransformer, and the second line is the backbone with FGAM (in Patch Embedding Layer and\nPatch Restoring Layer). The third line is the backbone with FGAM (in Patch Embedding\nLayer and Patch Restoring Layer) and convolution branch. The last line results in backbone\nwith FGAM (in Patch Embedding Layer and Patch Restoring Layer), convolution branch and\nDropBlock, that is, PCAT-UNet proposed by us. As can be seen from the three tables, the net-\nwork constructed by pure Transformer segmented retinal vessels and achieved good results,\nFig 6. Compar ison of example vessel segmentation results on DRIVE dataset: (a) original retinal images; (b) ground truths; (c) segmentat ion\nresults for UNet; (d) segmenta tion results for PCAT-UNet. The first row of the image is the whole image, and the second row is the zoomed in\narea of the marked red border and blue border in the image.\nhttps://doi.o rg/10.1371/j ournal.pone .0262689.g006\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 16 / 22\nbut the network combined with Transformer and convolution achieved better segmentation\nperformance. In addition, the introduction of DropBlock also plays an important role in the\nsegmentation results. This proves the effectiveness of these modules for our network.\nIn terms of time consumption, we compare PCAT-UNET with the Backbone, Backbone\n+FGAM and Backbone+FGAM +Conv-Branch methods of the proposed model. In the experi-\nment of this paper, all the above algorithms are implemented with Pytorch, and 250 iterations\nof DRIVE dataset (including 20 original training graphs and 20 data-enhanced graphs) are\ntested on NVIDIA Tesla V100 32GB GPU. The number of parameters and running time are\nshown in Table 8.\nAs can be seen from Table 9, PCAT-UNet (EPSA) method (using EPSA module to replace\nFGAM in PCAT-UNet method proposed in this paper) also achieved good results in retinal\nvessel segmentation. Its accuracy, specificity and AUC value were close to PCAT-UNET\n(FGAM) method, but its sensitivity was lower. It indicates that this method has information\nloss in vessel feature extraction and needs further improvement. However, the PCAT-UNet\n(FGAM) method proposed in this paper significantly improved the sensitivity index, indicat-\ning that the small-scale grouping convolution in FGAM can fully extract and completely\nrecover the vessel edge information. FGA also focuses on the blood vessels, reducing the\nFig 7. Compar ison of example vessel segmentation results on STARE dataset: (a) original retinal images; (b) ground truths; (c) segmentatio n\nresults for UNet; (d) segmen tation results for PCAT-UNet. The first row of the image is the whole image, and the second row is the zoomed in area of\nthe marke d red border and blue border in the image.\nhttps://doi.o rg/10.1371/j ournal.pone .0262689.g007\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 17 / 22\nFig 8. Compar ison of example vessel segmentation results on CHASE _DB1 dataset: (a) original retinal images; (b) ground truths; (c)\nsegmentatio n results for UNet; (d) segmentation results for PCAT-U Net. The first row of the image is the whole image, and the second row is the\nzoomed in area of the marked red border and blue border in the image.\nhttps://doi.o rg/10.1371/j ournal.pone .0262689.g008\nTable 5. Ablation study results on DRIVE dataset.\nNetwork F1-score SE SP ACC AUC\nBackbone 0.8043 0.7719 0.9885 0.9606 0.9854\nBackbone +FGAM 0.8092 0.7791 0.9909 0.9612 0.9862\nBackbone +FGAM+C onv-Branch 0.8118 0.7763 0.9899 0.9615 0.9865\nPCAT-UNet 0.8160 0.8576 0.9932 0.9622 0.9872\nhttps://do i.org/10.1371/j ournal.pone .0262689.t005\nTable 6. Ablation study results on STARE dataset.\nNetwork F1-score SE SP ACC AUC\nBackbone 0.8497 0.8518 0.9878 0.9750 0.9936\nBackbone +FGAM 0.8578 0.8750 0.9911 0.9766 0.9944\nBackbone +FGAM+C onv-Branch 0.8597 0.8650 0.9901 0.9770 0.9948\nPCAT-UNet 0.8836 0.8703 0.9937 0.9796 0.9953\nhttps://do i.org/10.1371/j ournal.pone .0262689.t006\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 18 / 22\ninfluence of background noise region on the segmentation results. It has certain rationality\nand validity on algorithm level.\n5 Conclusion\nIn this paper, we propose a U-shaped network with convolution branching based on Trans-\nformer. In this network, the multi-scale feature information obtained in the convolutional\nbranch is transmitted to the encoder and decoder on both sides, which can supplement the\nspatial information loss caused by the down-sampling operation. In this way, local features\nobtained in CNN can be better integrated with global information obtained in Transformer,\nso as to obtain more detailed feature information of vessels. Our method achieves the most\nadvanced segmentation performance on DRIVE, STARE and CHASE_DB1 datasets, and\ngreatly improves the extraction of small blood vessels, providing a powerful help for clinical\ndiagnosis. Our long-term goal is to take the best of both Transformer and CNN and combine\nthem more effectively and perfectly, and our proposed PCAT-UNET approach is a meaningful\nstep toward achieving this goal.\nAuthor Contributions\nConceptualization: Danny Chen.\nData curation: Danny Chen.\nFormal analysis: Danny Chen.\nTable 7. Ablation study results on CHASE_DB1 dataset.\nNetwork F1-score SE SP ACC AUC\nBackbone 0.8104 0.8149 0.9932 0.9797 0.9906\nBackbone +FGAM 0.8188 0.8269 0.9966 0.9806 0.9919\nBackbone +FGAM+C onv-Branch 0.8193 0.8174 0.9979 0.9809 0.9922\nPCAT-UNet 0.8273 0.8493 0.9967 0.9812 0.9925\nhttps://do i.org/10.1371/j ournal.pone .0262689.t007\nTable 8. Quantitat ive comparis on of parameter and time consumpt ion.\nNetwork Params (G) Train time(s) Test time(s/image )\nBackbone 40.1 4875 0.1103\nBackbone +FGAM 55.9 9700 0.1302\nBackbone +FGAM+C onv-Branch 57.4 12800 0.1584\nPCAT-UNe t 57.4 12550 0.1547\nhttps://d oi.org/10.1371/j ournal.pon e.0262689.t00 8\nTable 9. Performance comparis on of EPSA module and FGAM on three datasets .\nDatasets Method F1-score SE SP ACC AUC\nDRIVE PCAT-UNe t(EPSA) 0.8146 0.7990 0.9872 0.9617 0.9867\nPCAT-UNe t(FGAM) 0.8160 0.8576 0.9932 0.9622 0.9872\nSTARE PCAT-UNe t(EPSA) 0.8578 0.8610 0.9924 0.9770 0.9945\nPCAT-UNe t(FGAM) 0.8836 0.8703 0.9937 0.9796 0.9953\nCHASE_DB1 PCAT-UNe t(EPSA) 0.8270 0.8375 0.9966 0.9809 0.9927\nPCAT-UNe t(FGAM) 0.8273 0.8493 0.9967 0.9812 0.9925\nhttps://do i.org/10.1371/j ournal.pone .0262689.t009\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 19 / 22\nFunding acquisition: Wenzhong Yang, Liejun Wang.\nInvestigation: Danny Chen.\nMethodology: Danny Chen.\nProject administration: Wenzhong Yang, Liejun Wang.\nResources: Danny Chen.\nSoftware: Wenzhong Yang.\nSupervision: Wenzhong Yang.\nValidation: Danny Chen.\nVisualization: Danny Chen.\nWriting – original draft: Danny Chen.\nWriting – review & editing: Danny Chen, Wenzhong Yang, Sixiang Tan, Jiangzhaung Lin,\nWenxiu Bu.\nReferences\n1. Jin Q, Meng Z, Pham TD, Chen Q, Wei L, Su R. DUNet: A deformab le network for retinal vessel seg-\nmentation . Knowled ge-Based Systems. 2019; 178:149– 162. https://doi.or g/10.101 6/j.knosys. 2019.04.\n025\n2. Wu Y, Xia Y, Song Y, Zhang Y, Cai W. NFN+: A novel network followed network for retinal vessel seg-\nmentation . Neural Networ ks. 2020; 126:153–162. https://doi.or g/10.1016/ j.neunet.2020.0 2.018 PMID:\n32222424\n3. Chaudhur i S, Chatterjee S, Katz N, Nelson M, Goldbaum M. Detecti on of blood vessels in retinal images\nusing two-diimen sional matched filters. IEEE Transaction s on medical imaging. 1989; 8(3):263–2 69.\nhttps://doi.or g/10.110 9/42.34715 PMID: 18230524\n4. Wu H, Wang W, Zhong J, Lei B, Wen Z, Qin J. SCS-Net: A Scale and Contex t Sensitive Networ k for\nRetinal Vessel Segmen tation. Medical Image Analysis. 2021; 70:1020 25. https://doi.or g/10.101 6/j.\nmedia.20 21.102025 PMID: 33721692\n5. Soares JV, Leandro JJ, Cesar RM, Jelinek HF, Cree MJ. Retinal vessel segment ation using the 2-D\nGabor wavelet and supervised classifica tion. IEEE Transaction s on medical Imaging. 2006; 25\n(9):1214–1 222. https:// doi.org/10.11 09/TMI.2006 .879967 PMID: 16967806\n6. Martinez- Perez ME, Hughes AD, Thom SA, Bharath AA, Parker KH. Segmen tation of blood vessels\nfrom red-free and fluorescei n retinal images. Medical Image Analysis. 2007; 11(1):47–6 1. https://doi.\norg/10.1016/ j.media.200 6.11.004 PMID: 172044 45\n7. Salazar-G onzalez AG, Li Y, Liu X. Retinal blood vessel segment ation via graph cut. In: 2010 11th Inter-\nnational Conferen ce on Control Automatio n Robotics & Vision. IEEE; 2010. p. 225–230.\n8. Ghoshal R, Saha A, Das S. An improved vessel extraction scheme from retinal fundus images. Multime-\ndia Tools and Applications . 2019; 78(18):252 21–25239. https:/ /doi.org/10.10 07/s1104 2-019-771 9-9\n9. Yang Y, Huang S, Rao N. An automatic hybrid method for retinal blood vessel extraction. Interna tional\nJournal of Applied Mathem atics & Compu ter Science. 2008; 18(3). PMID: 28955157\n10. Zhang J, Dashtboz org B, Bekkers E, Pluim JP, Duits R, ter Haar Romeny BM. Robust retinal vessel\nsegmentatio n via locally adaptive derivative frames in orientat ion scores. IEEE transaction s on medical\nimaging . 2016; 35(12):263 1–2644. https:// doi.org/10.11 09/TMI.201 6.2587062 PMID: 275140 39\n11. Wang S, Yin Y, Cao G, Wei B, Zheng Y, Yang G. Hierarchic al retinal blood vessel segmentatio n based\non feature and ensemble learning. Neurocom puting. 2015; 149:708 –717. https://doi. org/10.1016/j .\nneucom.201 4.07.059\n12. Ronneber ger O, Fischer P, Brox T. U-net: Convolution al networks for biomedi cal image segment ation.\nIn: Interna tional Conference on Medical image computing and comput er-assisted intervention.\nSpringer; 2015. p. 234–241 .\n13. Zhang S, Fu H, Yan Y, Zhang Y, Wu Q, Yang M, et al. Attention guided network for retinal image seg-\nmentation . In: Interna tional Conferen ce on Medical Image Comp uting and Computer- Assisted Interven-\ntion. Springer; 2019. p. 797–80 5.\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 20 / 22\n14. Lan Y, Xiang Y, Zhang L. An Elastic Interactio n-Based Loss Function for Medical Image Segmen tation.\nIn: Interna tional Conference on Medical Image Comp uting and Computer- Assisted Interventi on.\nSpringer; 2020. p. 755–764 .\n15. Oliveira A, Pereira S, Silva CA. Retinal vessel segmentatio n based on fully convolutio nal neural net-\nworks. Expert Systems with Applications . 2018; 112:229 –242. https://doi.or g/10.101 6/j.eswa.2018 .06.\n034\n16. Chen J, Lu Y, Yu Q, Luo X, Adeli E, Wang Y, et al. Transune t: Transforme rs make strong encoder s for\nmedical image segment ation. arXiv preprint arXiv:210204 306. 2021;.\n17. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attentio n is all you need. In:\nAdvances in neural informati on processing systems; 2017. p. 5998–6 008.\n18. Hu R, Singh A. Transfor mer is all you need: Multimodal multitask learning with a unified transfor mer.\narXiv e-prints. 2021; p. arXiv–2102.\n19. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recogni tion. In: Procee dings of the IEEE\nconferen ce on computer vision and pattern recognition; 2016. p. 770–778.\n20. Dosovitskiy A, Beyer L, Kolesniko v A, Weissenbor n D, Zhai X, Unterthine r T, et al. An image is worth\n16x16 words: Transfor mers for image recognit ion at scale. arXiv preprint arXiv:201011 929. 2020;.\n21. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, et al. Swin-Unet: Unet-like Pure Transforme r for\nMedical Image Segmentation. arXiv preprint arXiv:2 10505537. 2021;.\n22. Heo B, Yun S, Han D, Chun S, Choe J, Oh SJ. Rethinking spatial dimension s of vision transforme rs.\narXiv preprint arXiv:210316 302. 2021;.\n23. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transfor mer: Hierarchi cal vision transforme r using\nshifted windows. arXiv preprint arXiv:2 10314030. 2021;.\n24. Wang W, Xie E, Li X, Fan DP, Song K, Liang D, et al. Pyramid vision transforme r: A versatile backbone\nfor dense predicti on without convolutio ns. arXiv preprint arXiv:21 0212122. 2021;.\n25. Fan H, Xiong B, Mangala m K, Li Y, Yan Z, Malik J, et al. Multiscale vision transforme rs. arXiv preprint\narXiv:210411 227. 2021;.\n26. Touvron H, Cord M, Douze M, Massa F, Sablayrolle s A, Je ´ gou H. Training data-e fficient image trans-\nformers & distillation through attention . In: Internat ional Conference on Machine Learning. PMLR; 2021.\np. 10347–103 57.\n27. Strudel R, Garcia R, Laptev I, Schmid C. Segmenter: Transfor mer for Semantic Segmen tation. arXiv\npreprint arXiv:210505 633. 2021;.\n28. Wu H, Xiao B, Codella N, Liu M, Dai X, Yuan L, et al. Cvt: Introduci ng convolutio ns to vision transform-\ners. arXiv preprint arXiv:2 10315808. 2021;.\n29. Lin H, Cheng X, Wu X, Yang F, Shen D, Wang Z, et al. CAT: Cross Attentio n in Vision Transforme r.\narXiv preprint arXiv:210605 786. 2021;.\n30. Zhang H, Zu K, Lu J, Zou Y, Meng D. Epsanet: An efficient pyramid split attention block on convolutio nal\nneural network. arXiv preprint arXiv:210514 447. 2021;.\n31. Li D, Rahardja S. BSERe sU-Net: An attention -based before-ac tivation residual U-Net for retina l vessel\nsegmentatio n. Comp uter Methods and Programs in Biomed icine. 2021; 205:106070. https://doi. org/10.\n1016/j.cmpb .2021.10 6070 PMID: 338577 03\n32. Gao Y, Zhou M, Metaxas D. UTNet: A Hybrid Transfor mer Archite cture for Medical Image Segmen ta-\ntion. arXiv preprint arXiv:210 700781. 2021;.\n33. Wu YH, Liu Y, Zhan X, Cheng MM. P2T: Pyramid Pooling Transfor mer for Scene Understand ing. arXiv\npreprint arXiv:210612 011. 2021;.\n34. Valanarasu JMJ, Oza P, Hacihalilo glu I, Patel VM. Medical transfor mer: Gated axial-at tention for medi-\ncal image segmentatio n. arXiv preprint arXiv:2 10210662. 2021;.\n35. Hatamiza deh A, Yang D, Roth H, Xu D. Unetr: Transfor mers for 3d medical image segmentatio n. arXiv\npreprint arXiv:210310 504. 2021;.\n36. Zhang Y, Liu H, Hu Q. Transfuse: Fusing transformers and cnns for medical image segmentatio n. arXiv\npreprint arXiv:210208 005. 2021;.\n37. Schlempe r J, Oktay O, Schaap M, Heinrich M, Kainz B, Glocker B, et al. Attention gated networks:\nLearning to leverage salient regions in medical images. Medical image analysis. 2019; 53:197–207.\nhttps://doi.or g/10.101 6/j.media.2 019.01.012 PMID: 30802813\n38. Wang W, Chen C, Ding M, Li J, Yu H, Zha S. TransBTS : Multimodal Brain Tumor Segmen tation Using\nTransfor mer. arXiv preprint arXiv:210304 430. 2021;.\n39. Chollet F. Xception : Deep learning with depthwise separab le convolutio ns. In: Procee dings of the IEEE\nconferen ce on computer vision and pattern recognition; 2017. p. 1251–1258.\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 21 / 22\n40. Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, et al. Mobilen ets: Efficient convolu-\ntional neural networks for mobile vision applicati ons. arXiv preprint arXiv:1 70404861. 2017;.\n41. Hu H, Zhang Z, Xie Z, Lin S. Local relation networks for image recognition. In: Procee dings of the IEEE/\nCVF International Conferen ce on Comp uter Vision; 2019. p. 3464–3 473.\n42. Ghiasi G, Lin TY, Le QV. Dropblo ck: A regulariza tion method for convolutio nal networks . arXiv preprint\narXiv:181012 890. 2018;.\n43. Hu J, Shen L, Sun G. Squeez e-and-excitatio n network s. In: Procee dings of the IEEE conferen ce on\ncomputer vision and pattern recognition; 2018. p. 7132–7141.\n44. Staal J, Abràmoff MD, Niemeijer M, Viergever MA, Van Ginnek en B. Ridge-bas ed vessel segmentatio n\nin color images of the retina. IEEE transaction s on medical imaging. 2004; 23(4):501 –509. https://doi.\norg/10.1109/ TMI.2004.82 5627 PMID: 150840 75\n45. Hoover A, Kouznetso va V, Goldbaum M. Locating blood vessels in retina l images by piecewise thresh-\nold probing of a matched filter response . IEEE Transaction s on Medical imaging . 2000; 19(3):203– 210.\nhttps://doi.or g/10.110 9/42.845178 PMID: 10875704\n46. Owen CG, Rudnicka AR, Mullen R, Barman SA, Monekos so D, Whincup PH, et al. Measuring retinal\nvessel tortuosity in 10-year-ol d children: validatio n of the computer- assisted image analysis of the retina\n(CAIAR) program. Investigativ e ophthalm ology & visual science. 2009; 50(5):2004 –2010. https://doi.\norg/10.1167/ iovs.08-301 8 PMID: 19324866\n47. Li L, Verma M, Nakashim a Y, Nagahar a H, Kawasak i R. Iternet: Retinal image segmentatio n utilizing\nstructural redundancy in vessel networks. In: Procee dings of the IEEE/CV F Winter Conference on\nApplications of Computer Vision; 2020. p. 3656–3665.\n48. Zhuang J. LadderN et: Multi-path network s based on U-Net for medical image segmentatio n. arXiv pre-\nprint arXiv:181007 810. 2018;.\n49. Li X, Chen H, Qi X, Dou Q, Fu CW, Heng PA. H-DenseUNet : hybrid densely connected UNet for liver\nand tumor segmentatio n from CT volumes. IEEE transaction s on medical imaging. 2018; 37(12):266 3–\n2674. https://d oi.org/10.110 9/TMI.2018. 2845918 PMID: 29994201\n50. Wang B, Qiu S, He H. Dual encodin g u-net for retinal vessel segmentatio n. In: International Conferen ce\non Medical Image Computing and Computer- Assisted Interventi on. Springer; 2019. p. 84–92.\n51. Yin P, Yuan R, Cheng Y, Wu Q. Deep guidan ce network for biomedica l image segmentat ion. IEEE\nAccess. 2020; 8:116106–116 116. https://doi.or g/10.1109 /ACCESS.2 020.3002835\n52. Zhang J, Zhang Y, Xu X. Pyramid U-Net for Retinal Vessel Segmen tation. In: ICASSP 2021-2021 IEEE\nInternational Conferen ce on Acoustics, Speech and Signal Processin g (ICASSP). IEEE; 2021.\np. 1125–1129 .\n53. Alom MZ, Yakopcic C, Hasan M, Taha TM, Asari VK. Recurren t residual U-Net for medical image seg-\nmentation . Journal of Medical Imaging. 2019; 6(1):01400 6. https://doi.or g/10.1117/ 1.JMI.6.1.0140 06\nPMID: 309448 43\n54. Wang C, Zhao Z, Yu Y. Fine retinal vessel segmentatio n by combining Nest U-net and patch-learni ng.\nSoft Comput ing. 2021; 25(7):5519 –5532. https:// doi.org/10.10 07/s0050 0-020-055 52-w\nPLOS ONE\nPCAT-UN et\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02626 89 January 24, 2022 22 / 22",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7669090032577515
    },
    {
      "name": "Segmentation",
      "score": 0.7007356286048889
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6964039206504822
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6543371081352234
    },
    {
      "name": "Pixel",
      "score": 0.4989590644836426
    },
    {
      "name": "Computer vision",
      "score": 0.48344549536705017
    },
    {
      "name": "Feature extraction",
      "score": 0.48070675134658813
    },
    {
      "name": "Inference",
      "score": 0.479610800743103
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4779655635356903
    },
    {
      "name": "Image segmentation",
      "score": 0.4284197688102722
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I96908189",
      "name": "Xinjiang University",
      "country": "CN"
    }
  ],
  "cited_by": 63
}