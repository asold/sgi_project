{
  "title": "CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering",
  "url": "https://openalex.org/W4389524114",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2798490560",
      "name": "Md. Rashad Al Hasan Rony",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131629901",
      "name": "Christian Suess",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5102636471",
      "name": "Sinchana Ramakanth Bhat",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A3185032897",
      "name": "Viju Sudhi",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2116078232",
      "name": "Julia Schneider",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2624797112",
      "name": "Maximilian Vogel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289984910",
      "name": "Roman Teucher",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2609177635",
      "name": "Ken E. Friedl",
      "affiliations": [
        "BMW Group (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2098310072",
      "name": "Soumya Sahoo",
      "affiliations": [
        "Fraunhofer Institute for Intelligent Analysis and Information Systems"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4386566526",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4309395891",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3164706026",
    "https://openalex.org/W4321855128",
    "https://openalex.org/W4385570209",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4290877239",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3183281747",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4321393106",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4287889327",
    "https://openalex.org/W4385572426",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4294833327",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2963514026",
    "https://openalex.org/W4280629343",
    "https://openalex.org/W4285222754",
    "https://openalex.org/W3154280800",
    "https://openalex.org/W2962839482",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4285599918",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Md Rashad Al Hasan Rony, Christian Suess, Sinchana Ramakanth Bhat, Viju Sudhi, Julia Schneider, Maximilian Vogel, Roman Teucher, Ken Friedl, Soumya Sahoo. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 586–604\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCarExpert: Leveraging Large Language Models for\nIn-Car Conversational Question Answering\nMd Rashad Al Hasan Rony1, Christian Süß1, Sinchana Ramakanth Bhat2, Viju Sudhi2,\nJulia Schneider4, Maximilian Vogel3, Roman Teucher2, Ken E. Friedl1, Soumya Sahoo2\n1BMW Group, 2Fraunhofer IAIS, 3BIG PICTURE GmbH, 4ONSEI GmbH\nmd-rashad-al-hasan.rony@bmw.de, christian.suess@bmw.de\nAbstract\nLarge language models (LLMs) have demon-\nstrated remarkable performance by follow-\ning natural language instructions without fine-\ntuning them on domain-specific tasks and data.\nHowever, leveraging LLMs for domain-specific\nquestion answering suffers from severe limi-\ntations. The generated answer tends to hal-\nlucinate due to the training data collection\ntime (when using off-the-shelf), complex user\nutterance and wrong retrieval (in retrieval-\naugmented generation). Furthermore, due to\nthe lack of awareness about the domain and ex-\npected output, such LLMs may generate unex-\npected and unsafe answers that are not tailored\nto the target domain. In this paper, we propose\nCarExpert, an in-car retrieval-augmented con-\nversational question-answering system lever-\naging LLMs for different tasks. Specifically,\nCarExpert employs LLMs to control the in-\nput, provide domain-specific documents to the\nextractive and generative answering compo-\nnents, and controls the output to ensure safe\nand domain-specific answers. A comprehen-\nsive empirical evaluation exhibits that CarEx-\npert outperforms state-of-the-art LLMs in gen-\nerating natural, safe and car-specific answers.\n1 Introduction\nConversational question answering (CQA) has re-\ncently gained increased attention due to the ad-\nvancements of Transformer-based (Vaswani et al.,\n2017) large language models (LLMs). These\nLLMs (Devlin et al., 2019; Brown et al., 2020; Ope-\nnAI, 2023; Touvron et al., 2023b) are nowadays\nwidely adopted for performing question answer-\ning in both open-domain and domain-specific set-\ntings (Robinson and Wingate, 2023). As the source\nof additional knowledge conversational question\nanswering systems are typically provided with text\nparagraphs (Kim et al., 2021; Rony et al., 2022c),\nand knowledge graphs (Rony et al., 2022b; Chaud-\nhuri et al., 2021) for generating informative dia-\nlogues in a domain-specific setting, where such\nWhat is the High Beam Assistant?\nHow do I activate it?\nThe High-beam Assistant ensures that you do not often \nhave to turn the high-beam headlight on and off yourself \nin order to avoid dazzling oncoming trafﬁc.\nThe system is activated automatically when the car is \nstarted. \nExtracted from document\nGenerated from document\nFigure 1: Illustration of a multi-turn in-car conversation\nbetween a user (in gray) and CarExpert (in blue).\nsystems typically engage in a multi-turn interac-\ntion with a user in form of speech or text. Figure 1\ndemonstrates a conversation between a user and a\nconversational question answering system (CarEx-\npert) in a BMW car.\nLeveraging LLMs end-to-end has several draw-\nbacks (Liang et al., 2022; Srivastava et al., 2023;\nOpenAI, 2023). Firstly, the generated answer\nis often hallucinated as the knowledge from the\npre-trained weights of LLMs is limited to their\ntraining data collection time (Ji et al., 2022). Fur-\nthermore, retrieval-augmented answer generation\nsuffers from hallucination as well, due to wrong\nretrieval, complexity of the user utterance and re-\ntrieved document. Secondly, LLMs can be ex-\nploited using adversarial instructions that may lead\nthe system to ingest malicious input and generate\nunsafe output (Perez and Ribeiro, 2022; Greshake\net al., 2023). In the context of a car, the aforemen-\ntioned downsides imply that the answer could lead\nto unsafe handling of the vehicle due to a lack of\ninstructions, preservation, warning messages, or\nappropriate information; or by providing erroneous\nor confusing information.\nAddressing the aforementioned issues, in this pa-\nper we propose CarExpert, an in-car conversational\nquestion-answering system, powered by LLMs.\nCarExpert is a modular, language model agnos-\ntic, easy to extend and controllable conversational\nquestion-answering system developed to work on\n586\nOrches-\ntrator\nControlled \ninput Filter\nRetriever\nUser Utterance\nthrough Speech \nRecognition\nSystem Response\nthrough Text-to-Speech\nControlled Prompting\nVector \nDatabase Reader\nPrompt Template \nStore\nLLM-based \nGenerator\nControlled Output \nFilter & Arbitrator\nAnswer \nModerator\nRetrieved \nParagraph\nExtracted \nAnswer\nGenerated \nAnswer\nSystem \nResponseDialog History\nFigure 2: High level overview of the CarExpert system architecture.\nthe text level. On a high-level CarExpert performs\nquestion answering in two steps. First, given a\nuser utterance it retrieves domain-specific relevant\ndocuments wherein the potential answer may ex-\nist. Second, for predicting the answer, CarExpert\nemploys both extractive and generative answering\nmechanisms. Specifically, there are four sub-tasks\ninvolved in the overall process: 1) orchestration, 2)\nsemantic search, 3) answer generation, and 4) an-\nswer moderation. Furthermore, CarExpert tackles\nunsafe scenarios by employing control mechanisms\nin three ways: i) in the Orchestrator using an input\nfilter, ii) by defining prompts for controlling LLM-\nbased answer generation, and iii) by an output filter\nin the Answer Moderator. Furthermore, CarExpert\nemploys a heuristic during answer moderation to\nselect answers from multiple models (extractive\nand generative) and provide the user with the po-\ntential best answer as the output.To facilitate voice-\nbased user interaction in the car for real-life use,\nwe encapsulate CarExpert with text-to-speech and\nspeech-to-text services. Figure 2 depicts a high-\nlevel overview of the CarExpert architecture. Such\nmodular design of CarExpert allows flexible inte-\ngration to various types of interfaces such as web\nbrowser and mobile app (i.e., BMW App).\nTo assess the performance of CarExpert we con-\nduct exhaustive evaluations (both qualitative and\nquantitative). An empirical evaluation exhibits that\nCarExpert outperforms off-the-shelf state-of-the-\nart LLMs in in car question answering. The contri-\nbution of this paper can be summarized as follows:\n• We introduce CarExpert, a modular, language\nmodel agnostic, safe and controllable in-car\nconversational question answering system.\n• A novel answer moderation heuristic for se-\nlecting a potential best answer from multiple\npossible outputs.\n• A comprehensive empirical evaluation,\ndemonstrating the effectiveness of CarExpert\nover the state-of-the-art LLMs for in-car\nconversational question answering.\n2 Approach\nCarExpert aims to generate domain-specific\ndocument-grounded answers. The task is divided\ninto four sub-tasks: 1) Orchestration, 2) Seman-\ntic Search, 3) Answer Generation, and 4) Answer\nModeration. We describe the sub-tasks below.\n2.1 Orchestration\nA prompt-based Orchestrator component is incor-\nporated in CarExpert to tackle unsafe content and\ndeal with multi-turn scenarios. Depending on the\nuser utterance, CarExpert also can e.g. respond by\nsaying that it does not have enough information or\nask a clarification question, since the system is de-\nsigned to only answer questions about the car. Thus\nthe Orchestrator controls the input in CarExpert.\nThe prompt used for this purpose is as follows:\nTask: Given a question and paragraphs:\n1. For unsafe or harmful questions, politely de-\ncline to answer as they are out of context. Stop\nany further generation.\n2. Flag any unsafe or harmful questions by po-\nlitely stating that you cannot provide an an-\nswer. Stop any further generation.\n3. If the question is safe and relevant, suggest a\nclarification question that demonstrates com-\nprehension of the concept and incorporates\ninformation from the provided paragraphs.\nStart the question with \"Do you mean\".\n4. If unsure about suggesting a specific clarifi-\ncation question, politely request more infor-\nmation to provide an accurate response. Stop\nany further generation.\n587\nQuestion: {user utterance} Paragraphs: {para-\ngraphs} Answer:\nwhere, user utterance represent the current turn’s\nuser utterance and paragraphs the top-3 retrieved\ndocuments obtained from the semantic search (dis-\ncussed in Section §2.2).\n2.2 Semantic Search\nFor efficient and fast semantic search of the rele-\nvant documents, CarExpert pre-processes data and\nparses clean contents from various curated sources\n(owners’ manuals, self-service FAQs, car configu-\nrator feature descriptions and press club publica-\ntions) utilizing a data pipeline (more details in the\nAppendix A.1.1). The parsed data is utilized in two\ndifferent ways. Firstly, we put humans in the loop\nto obtain high quality and domain expert annotated\nquestion-answer pairs for training an answer extrac-\ntion model (discussed in Section 2.3.1). Secondly,\nthe vector representation of the text is indexed only\nonce as a pre-processing step to facilitate fast Se-\nmantic Search over a large set of text during the\ninference (see Figure 3). In the next step LLMs\nare fed with top-3 retrieved document for the an-\nswer generation. We use the terms ’document’ and\n’paragraph’ interchangeably throughout this paper.\n2.3 Answer Generation\nCarExpert employs both extractive and generative\nmodels to get answers for the same user utterance.\nThe answer generation step is controlled by in-\nstructing the LLM using prompts and next by an\nAnswer Moderator component. It selects the best\nanswer based on an extraction ratio-based heuristic\n(discussed in Section 2.4). We describe the answer\ngeneration methods in the following sections.\n2.3.1 LLM-based Answer generation\nIn this step, CarExpert takes off-the-shelf\nGPT-3.5-turbo and instructs it in a few-shot man-\nner for answer generation based on the current user\nutterance, retrieved documents and the dialogue\nhistory. The probability distribution of generating\na response can be formally defined as:\np(St|P; H; Q) =\nn∏\ni=1\np(si|s<i,P; H; Q,θ), (1)\nwhere St is the generated answer, P is the\nprompt, His the dialogue history, Qis the user\nutterance in the current turn, θ is model parame-\nters, and nis the length of the response. Here, ”; ”\nindicates a concatenation operation between two\ntexts. Depending on the type of questions that the\nuser may ask, the generation task is split into two\nmajor categories: 1) Abstractive Summarization\nand 2) Informal Talk. We design separate prompt\ntemplates for both the categories to handle vari-\nous types of user utterances. We provide a brief\ndescription of both the categories below.\ni. Abstractive Summarization: We design a\nprompt template to handle information seeking user\nutterances that can be answered from the semantic\nsearch results where the template aims to generate\nthe answer in a natural sentence. The abstractive\nsummarization template is as follows:\nTask: Answer questions about the car given the fol-\nlowing context and dialog. Answer always helpful.\nAnswer in complete sentences. Don’t use more than\ntwo sentences. Extract the answer always from the\ncontext as literally as possible.\nDialogue 1:{example dialogue 1}\n.\nDialogue 6: Context: {top paragraphs , dialogue\nhistory} User:{user utterance} System:\nwhere example dialogue 1 is a variable that rep-\nresents a complete multi-turn conversation. Each\ndialogue may contain 1 to 5 user-system utterance\npairs. The variables top paragraphs and dialogue\nhistory represent top-3 paragraphs from the seman-\ntic search results and the complete dialogue history\nsuch as adjacent user-system pairs, respectively.\nFurthermore, user utterance indicates the current\nuser utterance that the system needs to answer.\nii. Informal Talk: A conversational AI system\nnot only deals with information-seeking utterances\nbut also needs to tackle follow-up questions, clar-\nifications, commands, etc. which makes the con-\nversation engaging and natural. To tackle various\nforms of user utterances we design an Informal\nTalk template as follows:\nTask: Answer the user feedback in a friendly and\npositive way. When asked about factual knowl-\nedge or about your opinion, just say that you can’t\nanswer these questions. Please never answer a\nquestion with a factual statement. If a question is\nabout something else than the car, you may append\na ‘Please ask me something about the car’.\nDialogue 1:{example dialogue 1}\n.\nDialogue 20: User:{user utterance} System:\nIn the Informal Talk template we provide 20 ex-\nample dialogues covering various forms of user\nutterance. This way both abstract summarization\nand informal talk templates leverages pre-trained\n588\n1. Reduce your speed and come to a stop \ncarefully. Avoid sudeen braking and steering \nmaneuvers. \n2. Follow the instructions for what to do in \ncase of a ﬂat tire.\nTo protect against parking damage, the \n\"Lateral Parking Aid\", a subfunction of Park \nAssist, warns of obstacles to the side of the \nvehicle during parking and leaving and \ngraphically displays them on the control \ndisplay. The obstacles are already detected \nwhile driving past a parking space with Park \nAssist activated and stored for the parking \nmanoeuvre. Please note: Park Assist in your \ncar forgets lateral obstacles if PDC is \nmanually deactivated and the vehicle is \nstanding still for 13 seconds.\nNOTICE Objects in unpaved areas, for \ninstance stones or branches, can damage the \nvehicle. There is a risk of damage to property. \nDo not drive on unpaved terrain. \nKNN-based \nApproximate Search\nQuestion: How can I avoid parking damage?\nTop-3\nSearch Results\nEmbedding Model\n0.7 0.3 -0.1 1.0 \nVector Database (vector space)\nA\nC\nB\nA C\nB\nAnswer\nFigure 3: Semantic search during the inference (the vector space is depicted as a vector database for demonstration).\nThe potential answer to the question is encapsulated in the box of retrieved document A.\nlarge language model in a few-shot manner to gen-\nerate natural and engaging dialogues. The prompt\ntemplates are stored in the Prompt Template Store.\n2.3.2 Answer extraction\nIn CarExpert, we investigate two different answer\nextraction methods:\ni. Machine Reading Comprehension Reader:\nGiven a user utterance and a document the task\nof a MRC Reader model is to predict a continuous\ntext span from the provided document that answers\nthe user question. We fined-tune an Albert (Lan\net al., 2020) model for the answer extraction task.\nii. LLM-based Reader: Engineering prompts is\na popular way to instruct LLMs how to leverage\ntheir knowledge to solve downstream NLP tasks.\nIn this approach, we leverage the pre-trained knowl-\nedge of LLMs, contained in their parameters to per-\nform the same answer extraction task as the MRC\nReader. However, in this case CarExpert does not\nneed training data to perform the answer extraction.\nSpecifically, in CarExpert we design a prompt that\ninstructs the LLMs to perform answer extraction as\nliterally as possible using both question and top-3\nparagraphs from the semantic search results. The\nprompt template is as follows:\nTask: Given the following question and paragraphs,\nextract exactly one continuous answer span from\nonly one of the paragraphs.\nQuestion: {user utterance} Paragraphs: {para-\ngraphs} Answer:\nDuring the inference, the variables user utterance\nand paragraphs are replaced with the actual user\nutterance and top three paragraphs retrieved from\nthe semantic search.\n2.4 Answer Moderation\nAn Answer Moderator component selects the best\nanswer given the user utterance and potential an-\nswers (extractive and generative). We investigate\nthe following two moderation techniques for an-\nswer moderation.\ni. Cosine Similarity: This approach measures the\nsemantic similarity between a user utterance and\nsystem response. The answer with a higher simi-\nlarity score is selected as the system response. For-\nmally, in this approach the answer selection can be\ndefined as: max(cosine( ⃗ aex, ⃗Q),cosine( ⃗ ag, ⃗Q)),\nwhere ⃗ aex, ⃗ ag, and ⃗Qare the embedding represen-\ntation of extracted answer, generated answer and\nuser utterance.\nii. Extraction Score: This is a weighted Leven-\nshtein distance-based heuristic that measures how\nsyntactically close the system response is to the re-\ntrieved paragraphs. Formally, the Extraction Score\n(ES) can be defined as:\nES = 1\nn ∗\nn∑\ni=1\n1 − dist(x,yi)\nmax(|x|,|yi|), (2)\nwhere xis the generated answer, yi is the ith para-\ngraph and n is the number of paragraphs. The\ncost of edit operation is computed by dist(·). This\nmoderation technique allows CarExpert to gener-\nate a controlled and document grounded answer\nby (i) grounding the system response to the re-\ntrieved documents, and (ii) filtering out incorrect\nand hallucinated responses. More details on the\nedit operations can be found in Appendix A.5.\n3 Experimental Setup\nData: The reader and retriever models in CarEx-\npert are fine-tuned and evaluated on car-specific\n589\ndata from various sources (owners’ manuals, self-\nservice FAQs, car configurator feature descriptions\nand press club publications).\nBaselines: We choose Dense Passage Retriever\n(DPR) (Karpukhin et al., 2020a), BM25 (Robert-\nson et al., 2009), Sentence-transformer (Reimers\nand Gurevych, 2019) and SPLADE (Formal et al.,\n2022) as the baseline retriever. For answer genera-\ntion we experiment with Albert (Lan et al., 2020)\n(extractive) and GPT-3.5-turbo 1 (generative) and\nLuminous-extended 2 (generative).\nMetrics To measure the performance of the Re-\ntriever we use Mean Reciprocal Rank (MRR@3).\nFor evaluating extractive Reader, we utilize token-\nlevel metrics, such as F1-Score and Exact Match\n(EM). Furthermore, we employ Cosine Similarity\nand METEOR (Banerjee and Lavie, 2005) to cap-\nture the similarity of generated answer aginst the\nreference response.\nFurther details of the datasets, hyper-parameter set-\ntings, and metrics can be found in the Appendix,\nin A.1, A.3 and A.4 respectively.\n4 Experiments and Results\nWe conduct both qualitative and quantitative exper-\niments to assess different parts contributing to the\noverall performance of CarExpert.\n4.1 Quantitative Analysis\nTable 2 and Table 3 demonstrate that the fine-tuned\nDPR and fine-tuned Reader perform better than\nthe baseline models in the corresponding tasks.\nThe performance improvement may attributed to\ntheir inherent capability of effectively learning and\ncapturing the distribution and characteristics of\nthe training data. In Table 2, we notice that a\nfine-tuned DPR outperforms a fine-tuned Sentence-\ntransformer. The fine-tuned DPR model preforms\nin MRR@1 and hence we integrate DPR as the\nretriever used for semantic search in CarExpert.\nFrom Table 4 we observe that GPT-3.5-turbo per-\nforms better than the Luminous-extended model\nsince the former is a larger model and hence offers\nbetter representations and generalization.\nTable 5 exhibits that Extraction Score does a bet-\nter job in moderating and selecting the best an-\nswer which aligns better to the retrieved documents.\nCarExpert incorporate the Extraction Score-based\n1https://openai.com/\n2https://www.aleph-alpha.com/\nheuristic for answer moderation. The Extraction\nScore technique is described in Appendix A.5.\n4.2 Qualitative Analysis\nTable 1 demonstrates a qualitative compari-\nson between CarExpert (with document) and\nGPT-3.5-turbo (with and without document) of\nanswer generation. When provided with the docu-\nment we instruct both the models to answer from\nthe provided documents. In the first case, with-\nout any documents providedGPT-3.5-turbo could\nnot answer the question, where with the docu-\nment it generated a very long answer. Further-\nmore, when answering it is referring to a spe-\ncific paragraph such as \"..The first paragraph\nmentions...\", which is irrelevant to the user. Car-\nExpert in this case correctly generated the expected\nanswer. In the second case, we asked the system\nabout how to mount a child seat. Off-the-shelf\nGPT-3.5-turbo generated generic answer from its\npre-trained knowledge, which includes unnecessary\ndetail such as \"..Read the instruction...\" ,\nand is not tailored to the target car brand. On\nthe contrary, although GPT-3.5-turbo generated a\nbetter answer, it includes additional irrelevant and\nlengthy details which are not suitable for in-car\nCQA. Still adding irrelevant information (right col-\numn, 3nd row: item 1., 6. & 7.). Overall, in both\nthe cases, CarExpert exhibits precise answer pre-\ndiction then off-the-shelf GPT-3.5-turbo with and\nwithout documents. Although, CarExpert lever-\nages GPT-3.5-turbo for the answer generation,\ncarefully designed prompts in CarExpert helped\nthe system to generate precise answers. Precise an-\nswers are suitable for real-time use in the car, where\nthe user may find an unnecessary detailed answer\n(which GPT-3.5-turbo generated) very exhaust-\ning. More lemon- and cherry-picked examples can\nbe found in Appendix D.\n5 Discussions and Potential Impact\nCarExpert is built in a modular fashion, which\nallows for expansion and adaptability to diverse\nindustrial use cases. Furthermore, the proposed\narchitecture enables the system to maintain, mod-\nify and scale the data more effectively. Moreover,\na pipeline approach such as CarExpert improves\nthe overall interpretability and debugging of a sys-\ntem. Finally, the introduced system is controllable\nand domain-specific as it allows for explicit con-\ntrol over the design and behavior of each of the\n590\nUser CarExpert (with paragraphs) GPT-3.5-turbo (w/o para-\ngraphs)\nGPT-3.5-turbo (with para-\ngraphs)\nDoes the\ncar have\na parking\nassist\nfunction?\nYes, Park Assist supports the fol-\nlowing functions: When parallel\nparking: Parking in reverse, paral-\nlel to the road. When perpendicu-\nlar parking: Parking forward or in\nreverse, perpendicular to the road.\nWhen driving out of parallel park-\ning spaces. When driving out of\nperpendicular parking spaces.\nI’m sorry, but I would need more\nspecific information about the car\nyou’re referring to. There are\nmany different car models with\nvarious features, and the availabil-\nity of parking assist functions can\nvary between manufacturers and\nmodels. Can you provide me with\nthe make and model of the car\nyou’re interested in?\nYes, the car has a parking as-\nsist function. The first paragraph\nmentions that Park Assist sup-\nports functions like parallel park-\ning, perpendicular parking, and\ndriving out of both types of park-\ning spaces. The second paragraph\nfurther elaborates on the features\nof the Automatic Parking Assis-\ntant, which includes [...]\nHow to\nmount\nchild\nseats?\nTo install the child restraint sys-\ntem in the vehicle, proceed as fol-\nlows:\n1. Mount the child restraint sys-\ntem in accordance with the in-\nstructions from the child seat man-\nufacturer.\n2. Make sure that the child re-\nstraint system anchors engage cor-\nrectly on both sides in the lower\nseat mounts.\n3. Pull out the seat belt strap com-\npletely.\n4. Secure the child restraint sys-\ntem with the seat belt.\n5. Allow the seat belt strap to be\npulled in, then pull it tight against\nthe child restraint system.\nMounting child seats properly is\ncrucial for the safety of your child\nwhile traveling in a vehicle. The\nspecific instructions for mounting\na child seat may vary depending\non the make and model of the seat\nand the vehicle. However, here is\na general guide on how to mount\nchild seats:\n1. Read the instructions: Care-\nfully read the instruction manual\nprovided with your child seat. It\nwill provide specific details about\nthe installation process and any\nadditional requirements.\n2. Choose the appropriate seat:\n[...]\nTo mount child seats, follow these\nsteps:\n1. Read and follow the instruc-\ntions provided by the child seat\nmanufacturer.\n2. Ensure that the child restraint\nsystem anchors engage correctly\non both sides in the lower seat\nmounts.\n3. Pull out the seat belt strap\ncompl.\n[...]\n6. Refer strictly to the installa-\ntion instructions supplied with the\nchild restraint system for specific\nguidelines.\n7. Each rear seat position is\nequipped with a head restraint.\n[...].\nTable 1: Dialog examples with answers of CarExpert compared toGPT-3.5-turbo stand alone and GPT-3.5-turbo\nwith relevant paragraphs provided through prompting. Longer texts are truncated and replaced by [...] for the\ndemonstration purpose.\nRetriever MRR@1 MRR@3\nBM25 0.26 0.31\nFine-tuned DPR 0.47 0.52\nFine-tuned Sentence-transformer 0.42 0.49\nSPLADE 0.44 0.53\nTable 2: Performance comparison of retriever models.\nReader F1 EM\nPre-trained Albert-large 0.31 0.01\nFine-tuned Albert-large 0.60 0.21\nGPT-3.5-turbo 0.51 0.14\nLuminous-extended 0.36 0.01\nTable 3: Evaluation results on the module: Reader.\nmodules such as Orchestrator and answer genera-\ntion. We anticipate that CarExpert will aid other\nindustrial use cases leverage LLMs in developing\nfine-grained and regulated conversational question\nanswering systems.\n6 Related Works\nLarge Language Models: Large language\nmodel (LLM) such as GPT-3 (Brown et al., 2020),\nPaLM (Chowdhery et al., 2022), LaMDA (Thoppi-\nGenerator Cos. Sim. METEOR\nGPT-3.5-turbo 0.68 0.38\nLuminous-extended 0.52 0.14\nTable 4: Performance of LLM-based Generator models.\nAnswer Moderator Accuracy\nCosine Similarity 0.82\nExtraction Score 0.87\nTable 5: Performance ofAnswer Moderator approaches.\nlan et al., 2022a), LLaMA (Touvron et al., 2023a)\nand GPT-4 (OpenAI, 2023) are capable of per-\nforming complex downstream tasks without being\ntrained for that tasks. A different line of recent\nresearch focuses on controlling the behaviour of\nLLMs such as NeMo-Guardrails 3. Inspired by\nhumans capabilities of following instructions in\nnatural language, recent research works fine-tuned\nLLMs so that it can understand instructions in a\nzero-shot or few-shot settings and perform a given\ntask following the language instruction (Wei et al.,\n2022; Taori et al., 2023; Brown et al., 2020; Rony\net al., 2022a; Schick and Schütze, 2021; Prasad\n3 https://developer.nvidia.com/nemo\n591\net al., 2023). In CarExpert, prompt-guided LLMs\nare employed to control various tasks of the answer\ngeneration process.\nConversational Question Answering: Recent\nadvancements of LLMs significantly improved\nmulti-turn question answering systems (Chowdh-\nery et al., 2022; Thoppilan et al., 2022b; Zaib et al.,\n2021). However, in multi-task objectives these\nmodels lack robustness (Liang et al., 2022; Srivas-\ntava et al., 2023). A different line of work (Daull\net al., 2023) emphasised on the needs for hybrid\napproaches to take advantage of multiple learning\nmodels to better handle the limitations. Architec-\ntural compositions such as LLM + semantic infor-\nmation retrieval (de Jong et al., 2023; Borgeaud\net al., 2022), LLM + instruction tuning module\n(Khattab et al., 2022), LLM + Router (Xu et al.,\n2023), cascaded LLMs (Dohan et al., 2022), LLM\n+ RLHF/RLAIF (Ouyang et al., 2022; Bai et al.,\n2022). Despite significant progress over time, CQA\nsystems still struggle. with long-standing issues\nlike hallucination, the ability to scale models and\ndata, and formal reasoning.\n7 Conclusion\nWe have introduced CarExpert, a new and con-\ntrolled in-car conversational question-answering\nsystem powered by LLMs. Specifically, CarExpert\nemployed semantic search to restrict the system\ngenerated answer within the car domain and incor-\nporated LLMs to predict natural, controlled and\nsafe answers. Furthermore, to tackle hallucinated\nanswers, CarExpert proposed an Extraction Score-\nbased Answer Moderator. We anticipate that the\nproposed approach can not only be applicable for\nthe in-car question answering but also be easily\nextendable and adapted for other domain-specific\nsettings. In future, we plan to integrate multi-task\nmodels to handle multiple task using a single LLM\nand reduce error propagation in the system.\nLimitations\nWhile our modular framework offers considerable\nflexibility in employing diverse models and align-\ning them with specific tasks and objectives, it\ncomes with few challenges as well. One major\ndrawback is the difficulty in jointly optimizing and\nfine-tuning the individual components toward a\ncommon objective. When optimized independently,\neach module may overfit to certain tasks and sub-\nsequently propagate errors due to intricate inter-\nactions, ultimately impacting the overall system\nperformance. Furthermore, given our reliance on\nLLMs, occasional hallucinations may occur despite\nour efforts to maintain control. Moreover, our sys-\ntem may struggle with handling highly complex\nand ambiguous queries, potentially requiring ex-\nternal resolution modules. In future, we intend to\ntackle the existing issues to develop a more robust\nconversational question answering system.\nAcknowledgement\nWe would like to thank Dr. Hans-Joerg V oegel, Dr.\nRobert Bruckmeier, and Dr. Peter Lehnert from the\nBMW Group in Munich, Germany for their support\nin this work. We would like to extend our thank\nto Dr. Nicolas Flores-Herr, Dr. Joachim Koehler,\nAlexander Arno Weber and the Fraunhofer IAIS\nteam for the helpful discussions and contributions\nto this work, and the members who contributed\nto this project from BIG PICTURE GmbH and\nONSEI GmbH.\nReferences\nBen Athiwaratkun, Andrew Wilson, and Anima Anand-\nkumar. 2018. Probabilistic FastText for multi-sense\nword embeddings. In Proc. of ACL , pages 1–11,\nMelbourne, Australia. Association for Computational\nLinguistics.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, John Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, E Perez,\nJamie Kerr, Jared Mueller, Jeff Ladish, J Landau,\nKamal Ndousse, et al. 2022. Constitutional ai:\nHarmlessness from ai feedback. ArXiv preprint ,\nabs/2212.08073.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\n592\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research , pages 2206–2240.\nPMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nDebanjan Chaudhuri, Md Rashad Al Hasan Rony, and\nJens Lehmann. 2021. Grounding dialogue systems\nvia knowledge graph aware decoding with pre-trained\ntransformers. In The Semantic Web: 18th Interna-\ntional Conference, ESWC 2021, Virtual Event, June 6–\n10, 2021, Proceedings 18, pages 323–339. Springer.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, and Hyung Won Chung. 2022. Palm:\nScaling language modeling with pathways. ArXiv\npreprint, abs/2204.02311.\nXavier Daull, Patrice Bellot, Emmanuel Bruno, Vincent\nMartin, and Elisabeth Murisasco. 2023. Complex\nqa and language models hybrid architectures, survey.\nArXiv preprint, abs/2302.09051.\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie,\nNicholas FitzGerald, Sumit Sanghai, Fei Sha, and\nWilliam Cohen. 2023. FiDO: Fusion-in-decoder opti-\nmized for stronger performance and faster inference.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 11534–11547, Toronto,\nCanada. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. of NAACL-HLT, pages 4171–4186,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A. Saurous,\nJascha Narain Sohl-Dickstein, Kevin Murphy, and\nCharles Sutton. 2022. Language model cascades.\nArXiv preprint, abs/2207.10342.\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and Stéphane Clinchant. 2022. From dis-\ntillation to hard negative sampling: Making sparse\nneural ir models more effective.\nThibault Formal, Benjamin Piwowarski, and Stéphane\nClinchant. 2021. Splade: Sparse lexical and expan-\nsion model for first stage ranking.\nKai Greshake, Sahar Abdelnabi, Shailesh Mishra,\nChristoph Endres, Thorsten Holz, and Mario Fritz.\n2023. More than you’ve asked for: A comprehen-\nsive analysis of novel prompt injection threats to\napplication-integrated large language models. CoRR,\nabs/2302.12173.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai,\nAndrea Madotto, and Pascale Fung. 2022. Survey of\nhallucination in natural language generation. ACM\nComputing Surveys, 55:1 – 38.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020a. Dense passage retrieval\nfor open-domain question answering. In Proc. of\nEMNLP, pages 6769–6781, Online. Association for\nComputational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020b. Dense passage retrieval\nfor open-domain question answering. In Proc. of\nEMNLP, pages 6769–6781, Online. Association for\nComputational Linguistics.\nO. Khattab, Keshav Santhanam, Xiang Lisa Li, David\nLeo Wright Hall, Percy Liang, Christopher Potts,\nand Matei A. Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. ArXiv preprint ,\nabs/2212.14024.\nBoeun Kim, Dohaeng Lee, Sihyung Kim, Yejin Lee,\nJin-Xia Huang, Oh-Woog Kwon, and Harksoo Kim.\n2021. Document-grounded goal-oriented dialogue\nsystems on pre-trained language model with diverse\ninput representation. In Proceedings of the 1st Work-\nshop on Document-grounded Dialogue and Conver-\nsational Question Answering (DialDoc 2021), pages\n98–102, Online. Association for Computational Lin-\nguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proc. of\nICLR. OpenReview.net.\nJey Han Lau and Timothy Baldwin. 2016. An empirical\nevaluation of doc2vec with practical insights into\ndocument embedding generation. In Proceedings\nof the 1st Workshop on Representation Learning for\nNLP, pages 78–86, Berlin, Germany. Association for\nComputational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\n593\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, and Yuta Koreeda. 2022. Holistic evaluation of\nlanguage models. Annals of the New York Academy\nof Sciences.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nFábio Perez and Ian Ribeiro. 2022. Ignore previous\nprompt: Attack techniques for language models.\nArXiv, abs/2211.09527.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. GrIPS: Gradient-free, edit-based in-\nstruction search for prompting large language models.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 3845–3864, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proc. of EMNLP,\npages 2383–2392, Austin, Texas. Association for\nComputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proc. of EMNLP, pages 3982–3992,\nHong Kong, China. Association for Computational\nLinguistics.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nJoshua Robinson and David Wingate. 2023. Leveraging\nlarge language models for multiple choice question\nanswering. In The Eleventh International Conference\non Learning Representations.\nMd Rashad Al Hasan Rony, Debanjan Chaudhuri, Ri-\ncardo Usbeck, and Jens Lehmann. 2022a. Tree-kgqa:\nAn unsupervised approach for question answering\nover knowledge graphs. IEEE Access, 10:50467–\n50478.\nMd Rashad Al Hasan Rony, Ricardo Usbeck, and Jens\nLehmann. 2022b. DialoKG: Knowledge-structure\naware task-oriented dialogue generation. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2557–2571, Seattle, United\nStates. Association for Computational Linguistics.\nMd Rashad Al Hasan Rony, Ying Zuo, Liubov Kovrigu-\nina, Roman Teucher, and Jens Lehmann. 2022c. Cli-\nmate bot: A machine reading comprehension system\nfor climate change question answering. In Proceed-\nings of the Thirty-First International Joint Confer-\nence on Artificial Intelligence, IJCAI-22, pages 5249–\n5252. International Joint Conferences on Artificial\nIntelligence Organization. AI for Good - Demos.\nTimo Schick and Hinrich Schütze. 2021. Few-shot text\ngeneration with natural language instructions. In\nProc. of EMNLP, pages 390–402, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, Agnieszka Kluska, Aitor\nLewkowycz, Akshat Agarwal, Alethea Power, Alex\nRay, Alex Warstadt, Alexander W. Kocurek, Ali\nSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish,\nAllen Nie, Aman Hussain, Amanda Askell, Amanda\nDsouza, Ambrose Slone, Ameet Rahane, Anan-\ntharaman S. Iyer, Anders Johan Andreassen, An-\ndrea Madotto, Andrea Santilli, Andreas Stuhlmüller,\nAndrew M. Dai, Andrew La, Andrew Lampinen,\nAndy Zou, Angela Jiang, Angelica Chen, Anh\nVuong, Animesh Gupta, Anna Gottardi, Antonio\nNorelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin\nHerrick, Avia Efrat, Aykut Erdem, Ayla Karaka¸ s,\nB. Ryan Roberts, Bao Sheng Loe, Barret Zoph,\nBartłomiej Bojanowski, Batuhan Özyurt, Behnam\nHedayatnia, Behnam Neyshabur, Benjamin Inden,\nBenno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake\nHowald, Bryan Orinion, Cameron Diao, Cameron\nDour, Catherine Stinson, Cedrick Argueta, Cesar\nFerri, Chandan Singh, Charles Rathkopf, Chenlin\nMeng, Chitta Baral, Chiyu Wu, Chris Callison-\nBurch, Christopher Waites, Christian V oigt, Christo-\npher D Manning, Christopher Potts, Cindy Ramirez,\nClara E. Rivera, Clemencia Siro, Colin Raffel, Court-\nney Ashcraft, Cristina Garbacea, Damien Sileo,\nDan Garrette, Dan Hendrycks, Dan Kilman, Dan\nRoth, C. Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek\nTam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan,\nDimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee,\nDylan Schrader, Ekaterina Shutova, Ekin Dogus\nCubuk, Elad Segal, Eleanor Hagerman, Elizabeth\nBarnes, Elizabeth Donoway, Ellie Pavlick, Emanuele\nRodolà, Emma Lam, Eric Chu, Eric Tang, Erkut\nErdem, Ernie Chang, Ethan A Chi, Ethan Dyer,\nEthan Jerzak, Ethan Kim, Eunice Engefu Manyasi,\nEvgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar,\nFernando Martínez-Plumed, Francesca Happé, Fran-\ncois Chollet, Frieda Rong, Gaurav Mishra, Genta In-\ndra Winata, Gerard de Melo, Germán Kruszewski,\n594\nGiambattista Parascandolo, Giorgio Mariani, Glo-\nria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor\nBetz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim,\nHannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta,\nHayden Bogar, Henry Francis Anthony Shevlin, Hin-\nrich Schuetze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B Simon,\nJames Koppel, James Zheng, James Zou, Jan Kocon,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova, Jelle\nBosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal,\nJesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming\nSong, Jillian Tang, Joan Waweru, John Burden, John\nMiller, John U. Balis, Jonathan Batchelder, Jonathan\nBerant, Jörg Frohberg, Jos Rozen, Jose Hernandez-\nOrallo, Joseph Boudeman, Joseph Guerr, Joseph\nJones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce\nChua, Kamil Kanclerz, Karen Livescu, Karl Krauth,\nKarthik Gopalakrishnan, Katerina Ignatyeva, Katja\nMarkert, Kaustubh Dhole, Kevin Gimpel, Kevin\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem\nSenel, Maarten Bosma, Maarten Sap, Maartje Ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramirez-Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael Andrew Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-\nAri Krakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel,\nNuan Wen, Oliver Zhang, Omar Agha, Omar El-\nbaghdadi, Omer Levy, Owain Evans, Pablo Anto-\nnio Moreno Casares, Parth Doshi, Pascale Fung,\nPaul Pu Liang, Paul Vicol, Pegah Alipoormolabashi,\nPeiyuan Liao, Percy Liang, Peter W Chang, Pe-\nter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Ra-\nbin Banjade, Rachel Etta Rudolph, Raefer Gabriel,\nRahel Habacker, Ramon Risco, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Ro-\nhan Sikand, Roman Novak, Roman Sitelew, Ro-\nnan Le Bras, Rosanne Liu, Rowan Jacobs, Rui\nZhang, Russ Salakhutdinov, Ryan Andrew Chi,\nSeungjae Ryan Lee, Ryan Stovall, Ryan Teehan,\nRylan Yang, Sahib Singh, Saif M. Mohammad,\nSajant Anand, Sam Dillavou, Sam Shleifer, Sam\nWiseman, Samuel Gruetter, Samuel R. Bowman,\nSamuel Stern Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima Shammie\nDebnath, Siamak Shakeri, Simon Thormeyer, Si-\nmone Melzi, Siva Reddy, Sneha Priscilla Makini,\nSoo-Hwan Lee, Spencer Torene, Sriharsha Hatwar,\nStanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Biderman, Stephanie Lin, Stephen Prasad,\nSteven Piantadosi, Stuart Shieber, Summer Mish-\nerghi, Svetlana Kiritchenko, Swaroop Mishra, Tal\nLinzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\nTatsunori Hashimoto, Te-Lin Wu, Théo Desbor-\ndes, Theodore Rothschild, Thomas Phan, Tianle\nWang, Tiberius Nkinyili, Timo Schick, Timofei Ko-\nrnev, Titus Tunduny, Tobias Gerstenberg, Trenton\nChang, Trishala Neeraj, Tushar Khot, Tyler Shultz,\nUri Shaham, Vedant Misra, Vera Demberg, Victo-\nria Nyamai, Vikas Raunak, Vinay Venkatesh Ra-\nmasesh, vinay uday prabhu, Vishakh Padmakumar,\nVivek Srikumar, William Fedus, William Saunders,\nWilliam Zhang, Wout V ossen, Xiang Ren, Xiaoyu\nTong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadol-\nlah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Yiding\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu-\nfang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao,\nZijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi\nWu. 2023. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nTransactions on Machine Learning Research.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022a. Lamda: Language models for dialog applica-\ntions. ArXiv preprint, abs/2201.08239.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, and Quoc Le. 2022b. Lamda: Lan-\nguage models for dialog applications. ArXiv preprint,\nabs/2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\n595\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V .\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan,\nPuxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Au-\nrelien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners. In Proc. of\nICLR. OpenReview.net.\nHaike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, and\nZhilin Yang. 2023. A universal discriminator for\nzero-shot generalization. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 10559–\n10575, Toronto, Canada. Association for Computa-\ntional Linguistics.\nMunazza Zaib, Wei Emma Zhang, Quan Z. Sheng, Ad-\nnan Mahmood, and Yang Zhang. 2021. Conversa-\ntional question answering: a survey. Knowledge and\nInformation Systems, 64:3151 – 3195.\nJianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao,\nRuicheng Zheng, Yingxia Shao, Hao Sun, Hanqing\nZhu, Premkumar Srinivasan, Weiwei Deng, et al.\n2022. Uni-retriever: Towards learning the unified em-\nbedding based retriever in bing sponsored search. In\nProceedings of the 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining , pages\n4493–4501.\nA Appendix\nA.1 Data\nSources: Information sources were comprised of\nthe following documents:\n• Owners’ manual: Detailed descriptions of\nfunctional features and step-by-step instruc-\ntions on their usage for the target car. Includ-\ning information about safe usage of the car as\nwell as warnings to prevent unsafe situations\nand handling.\n• Self-service: A collection of frequently asked\nquestions and answers about cars and their\nfeatures (language: English UK and US).\n• Car Configurator: Description of configuring\nthe car’s appearance and its technical features.\n• Press Club: A collection of target car specific\narticles published as press publications.\nTable 6 illustrates number of paragraphs and the\nmedian word count of each information source.\nWhile the owners’ manual has the highest number\nof relatively short paragraphs, Press Club contains\nlonger paragraphs in smaller quantities. Figure 4\ndepicts the distribution of word count in one para-\ngrph for the data sources.\nData Sources # Para. Median\nOwners’ Manual 3,537 38\nSelf Service 312 70\nCar Configurator 150 52\nPress Club 125 228\nTable 6: Overview of number of paragraph and median\nword count per paragraph for each source document.\nTraining and Evaluation Data: We constructed\na set of in-house annotated data by randomly sam-\npling paragraphs from different data sources. The\ntrain/dev/test splits are shown in table 7. The eval-\n# Queries # Para.\nIn-house Train+ 757 278\nIn-house Dev+ 176 80\nIn-house Test+ 66 40\nEvaluation∗ 60 40\nTable 7: Data statistics for in-house data and human-\nannotated evaluation data. + indicates data used for\ntraining and evaluating the Retriever and Reader mod-\nels. ∗ indicates data used for evaluating the individual\nmodules and the system as a whole.\nuation set contains, 60 multi-turn dialogues (33%\n596\nFigure 4: Word count distributions per paragraph.\nwith 2 turns, 33% with 4 turns and 33% with 6\nturns), curated from 40 different paragraphs for ran-\ndomly sampled document collection. We ensured\nthat at least one dialog is crafted for every para-\ngraph in this evaluation set. The human-annotation\nprocess for collecting these data are described in\nSection § C.\nA.1.1 Data Processing Pipeline\nThe data processing pipeline in CarExpert takes\ndata in various format (such as unstructured text,\nPDF, Excel, CSV , XML) and transforms them into\nSQuAD (Rajpurkar et al., 2016) format. SQuAD\nis a widely used question answering dataset for-\nmat. The paragraphs in the SQuAD format are then\nconverted into vectors, obtained from the Sentence-\ntransformer and stored them in a vector database to\nfacilitate quick semantic search (retrieval) given a\nuser query.\nA.2 Baselines\nThe baseline models used for comparing each com-\nponents are as follows:\nRetriever: (i) Sparse embeddings:\nBM25 (Robertson et al., 2009) (ii) Static\nembedding models: FastText (Athiwaratkun et al.,\n2018) and doc2vec (Lau and Baldwin, 2016) (iii)\nContextual embedding models: Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020b) and\nSentence-transformers (Zhang et al., 2022) (iv)\nHybrid embedding models: SPLADE (Formal\net al., 2021).\nReader: (i) Encoder-based (LM) models: pre-\ntrained reader models including Albert-large (Lan\net al., 2020) (ii) Decoder-based (LLM) models:\nGPT-3.5-turbo and Luminous-extended.\nGenerator: (i) GPT-3.5-turbo (ii) Luminous-\nextended.\nA.3 Hyper-parameter Settings\nWe describe the hyper-parameters used in different\ncomponents of the CarExpert below.\nRetriever: We fine-tune the DPR\nmodel by employing a query encoder:\nfacebook/dpr-question_encoder-multiset-\nbase and facebook/dpr-ctx_encoder-\nmultiset-base as the paragraph encoder. We\ncontinued training for 10 epochs with a batch size\nof 8, warm-up steps of 6, and one hard negative\nsample per data point. We further fine-tuned the\nSentence-transformer model all-MiniLM-L6-v2\nwith a batch size of 16 for 1 epoch, combining the\nobjective of reducing Masked Language Modelling\n(MLM) and Next Sentence Prediction (NSP) loss.\nReader: As the reader model, we fine-tuned\nAlbert-large (Lan et al., 2020) as the base\nmodel. For the LLM-based reader, we used\nGPT-3.5-turbo and Luminous-extended models.\nIn both cases, we set a temperature of 0 to facilitate\ndeterministic text generation, as well as a presence\npenalty of 0, top-p sampling rate of 0 and repetition\npenalty of 1.\nGenerator: For the LLM-based answer\ngeneration, we use GPT-3.5-turbo and\nLuminous-extended with a temperature of\n0.8, top-p sampling rate 0.4, repetition penalty 1\nand presence penalty of 0.6 . These settings allow\nfor a more flexible answer generation, in contrast\nto the LLM-based reader.\nA.4 Metrics\nFor quantitative evaluation of the system compo-\nnents and the system as a whole, we relied upon\nthe following metrics.\nRetriever: (i) Mean Reciprocal Rank (MRR) for\nthe top-3 paragraphs calculates the average recip-\nrocal rank of the first relevant document across\n597\nmultiple queries. The focus is on the rank of the\nfirst relevant document.\nReader: (i) F1-Score considers both precision\n(how many predicted words are correct) and recall\n(how many correct words are predicted). (ii) Exact\nMatch (EM) measures the percentage of predicted\nanswers that exactly match the ground truth an-\nswers. It is a strict metric that demands the model\nresponse to be identical to the ground truth.\nType of token INS DEL SUB\nDefault 1.0 1.0 1.0\nStop words 0.5 0.5 0.5\nInput tokens 0.5 - 0.1\nReference tokens - 2.0 -\nTable 8: Insertion costs (INS), Deletion costs (DEL) and\nSubstitution costs (SUB) for different types of tokens.\nGenerator: (i) Cosine Similarity between the\nsystem response and the human annotated response.\n(ii) METEOR (Banerjee and Lavie, 2005) provides\na single score reflecting the overall quality and flu-\nency of the generated response against the human\nannotated response.\nAnswer Moderator (i) Accuracy of correctly\nyielding the extracted or the generated response\nas annotated by the human annotators.\nSystem as a whole: (i) Cosine Similarity be-\ntween the final system response and the expected\nsystem response. (ii) Component Contributions\nrevealing if the system yields more extractive re-\nsponses or generative results.\nA.5 Answer Moderator\nEdit Operations in Extraction score: Table 8\ndemonstrates the edit operation cost used in Ex-\ntraction Score. Note that when the system deletes\nany reference token, it receives a maximum penalty.\nEventually, the distance is normalized to a consis-\ntent scale using the maximum absolute value.\nB Ablation Studies\nB.1 Retriever\nWe performed an extensive ablation study on dif-\nferent types of retriever (sparse, static, contextual,\nand hybrid) on both in-house and human-annotated\nevaluation datasets.\nThe retriever scores from the traditional BM25\nand the static models are significantly lower, as\nexpected, than the rest of the candidates. We ob-\nserve that our datasets are reasonably hard for the\nretrievers which rely upon just the frequencies or\nassociations between query-document pairs, essen-\ntially failing to yield meaningful contextual repre-\nsentations. The fine-tuned DPR performs the best\non the human-annotated evaluation set, while the\nfine-tuned Sentence-transformer model performs\nthe best on the in-house test set. It is also worth not-\ning that the off-the-shelf SPLADE model performs\nalmost as good as the fine-tuned contextual models.\nThis could be attributed to how hybrid models are\ntrained to combine the best of both worlds from the\nsparse and dense representations.\nB.2 System as a whole\nTable 10 demonstrates the experimental results of\nCarExpert with various system configurations. The\ncomponent-wise evaluation presented earlier in Ta-\nble 2 through 5) motivated us to conduct this elab-\norate study, within a scope with (i) fine-tuned DPR\nand fine-tuned Sentence Transformer models asRe-\ntriever, (ii) fine-tuned Reader and GPT-3.5-turbo\nbased Reader, (iii) GPT-3.5-turbo as the Genera-\ntor, and (iv) both answer moderation techniques.\nIt is evident from the results that the Extraction\nScore based Answer Moderator always prefers ex-\ntractive responses than the generative responses\nwhen compared to the Cosine Similarity-based\ncounterpart. For instance, the configurations C01\nand C03 differ only by the Answer Moderator, how-\never there is a significant increase in the contri-\nbution of extractive responses from 23% to 52%.\nThis moderation technique helps our model to stay\ncontrollable regardless of the nature of the user ut-\nterances. The best share of extractive responses is\nobtained from C03.\nWe also observe how different retriever models\naffect the overall system response. For instance, the\nconfigurations C04 and C08 differ only by the re-\ntrievers used, however, with a significant difference\nin the similarity between the system response and\nreference response. In future, we intend to explore\nother sophisticated metrics that measure more nu-\nanced aspects of language generation. In addition,\nwe hypothesize that the cosine-similarity-based sys-\ntem evaluation might be biased towards the cosine\nsimilarity-based arbitration method as they may be\nmeasuring similar aspects of response similarity.\n598\nIn-house Test Evaluation set\nMRR@1 MRR@3 MRR@5 MRR@1 MRR@3 MRR@5\nSparse Models\nBM25 0.623 0.710 0.715 0.257 0.313 0.341\nStatic embedding models\nfastText 0.221 0.318 0.353 0.227 0.283 0.300\ndoc2vec 0.273 0.320 0.339 0.106 0.139 0.230\nContextual embedding models\nDPR 0.649 0.747 0.759 0.303 0.429 0.457\nDPR∗ 0.701 0.790 0.804 0.469 0.515 0.535\nSentence-transformer 0.701 0.792 0.794 0.409 0.467 0.491\nSentence-transformer∗ 0.714 0.812 0.814 0.424 0.492 0.506\nHybrid Models\nSPLADE 0.610 0.699 0.711 0.439 0.520 0.531\nTable 9: Ablations of retrievers on different datasets. ∗indicates fine-tuned models.\nFigure 5: Human-annotation tool used for extending training data.\nIn this work, we prioritize the metric ‘Contribu-\ntions‘ which ensures that the system responses are\ndocument-grounded and safer for an in-car setting.\nWe consider this as a strong argument to set C04\nas the default system configuration.\nC Human Evaluation\nTo obtain human annotated question-answer pairs\n(for training the MRC Reader) and reference para-\ngraphs we used the CDQA tool 4. Furthermore,\nwe enriched our training data by employing human\nin the loop to obtain high-quality question-answer\npairs for our internal test tool (depicted in Figure 5).\nWe instruct the annotator to rate the system gener-\nated answers as follows:\n• 5 Stars: It doesn’t get any better than that.\nIs definitely a gold standard and should defi-\n4https://github.com/cdqa-suite/cdQA-annotator\nnitely be used as a training data.\n• 4 Stars: Very good answer and better than\nexisting systems. Has the potential to be used\nas a training data.\n• 3 Stars: Good answer. On the same level as\nexisting systems. Somewhat satisfying, how-\never, could be better formulated. Not suitable\nas a training data nor a gold standard.\n• 2 Stars: Unexpected and wrong answer.\n• 1 Star: Unexpected response. Under no\ncircumstances should it be provided to cus-\ntomers.\nD Qualitative Analyses\nD.1 Cherry-picked Examples\nTable 11 demonstrates a set cherry-picked exam-\nples (question answer pairs). These answers are\n599\nSystem configuration Cos. Sim. Contributions\nGen.% - Ext.%\nC01 DPR∗ + Reader∗ + Generator + CosSimArb 0.687 77% - 23%\nC02 DPR∗ + LLM-based Reader + Generator + CosSimArb 0.687 78% - 22%\nC03 DPR∗ + Reader∗ + Generator + ExtScArb 0.679 48% - 52%\nC04 DPR∗ + LLM-based Reader + Generator + ExtScArb 0.675 50%- 50%\nC05 Sentence-transformer∗ + Reader∗ + Generator + CosSimArb 0.750 89% - 10%\nC06 Sentence-transformer∗ + LLM-based Reader + Generator + CosSimArb 0.750 86% - 13%\nC07 Sentence-transformer∗ + Reader∗ + Generator + ExtScArb 0.746 74% - 25%\nC08 Sentence-transformer∗ + LLM-based Reader + Generator + ExtScArb 0.758 79% - 20%\nTable 10: Ablations on different system configurations.∗ indicates fine-tuned models. CosSimArb: Cosine Similarity\nbased Answer Moderation, ExtScArb: Extraction Score based Answer Moderation, Sentence-transformer: Sentence-\ntransformer based retriever.\nconsidered to be the response generated by the sys-\ntem of high quality. Furthermore, Table 6 illustrates\na complete conversation performed by CarExpert\nin real-life in-car settings with the user.\nD.2 Lemon-picked Example\nRefer to Table 12 for a selection of lemon-picked\nexample question answer pairs.\nE Error Analysis\nTable 14 and Table 15 include the cases where\nthe system failed or the most likely error source\nthat failed the system. Note that the modular-\narchitecture of our system better helps us in mak-\ning a well-educated identification of the erroneous\ncomponent. We conduct the error analysis by\ncomparing our system with GPT-3.5-turbo and\nLuminious-extended. For a fair evaluation, we pro-\nvide the same set of retrieved paragraphs to all three\nsystems.\nE.1 Helpfulness vs Harmlessness trade-off\nThis type of query poses a trade-off between pro-\nviding helpful and potentially harmful information.\nFor example, in Table 14 the query \"How can I\ndisable the safety feature that prevents\nthe engine from starting automatically in\nmy car?\" the user requests for information to gain\nmore control of the system. However, at the same\ntime the information might be potentially danger-\nous if not handled responsibly. Balancing such a\nrequest based on the importance of safety is there-\nfore crucial.\nFurthermore, in Table 14, all the three CQA sys-\ntems are trying to be helpful and promptly provide\nthe appropriate answer to the user. While desirable,\nit is important for the system to warn the driver for\npotential risks like engine damage, legal violations,\ncompromised safety, etc.\nE.2 Hallucination\nHallucination by LLMs are generated responses\nthat may comprise of misleading, factually incor-\nrect, or fictional information which may seem very\nplausible and linguistically correct to humans. De-\nspite the efforts to minimize hallucination through\na controlled-architecture pipeline, our evaluation\npoints at instances of hallucination as illustrated in\nthe Table 15. The table demonstrates an example\nwhere both CarExpert and Luminous-extreme gen-\nerate hallucinated responses by relying on the re-\ntrieved paragraphs. Even though GPT-3.5-turbo\nseems like a better answer, it also hallucinates due\nto limited information found on battery health. A\ndesired response would acknowledge the lack of\nspecific information on driving with the engine off.\nThis observation suggests that sometimes the re-\ntriever component retrieves paragraphs with incom-\nplete information, leading to error propagation.\n600\n# User Utterance System Response Extracted System Response Generated\na How can I prevent the vehicle from\ncreeping after I selected a gear po-\nsition?\nTo prevent the vehicle from creep-\ning after you select a gear position\nor reverse, maintain pressure on\nthe brake pedal until you are ready\nto drive off.\n1. Place cover on the left and right.\n2. Lift the cover slightly to the rear\nand push toward the front until it\nengages on both side brackets.\nb I heard about the exterior colour\nalso contributing to my wellness in\nthe interior? How’s that?\nA variable exterior colour can also\ncontribute to wellness in the inte-\nrior and to the efficiency of the\nvehicle. This is done by taking\naccount of the different abilities\nof light and dark colours when it\ncomes to reflecting sunlight and\nthe associated absorption of ther-\nmal energy.\nBy taking account of the different\nabilities of light and dark colours\nwhen it comes to reflecting sun-\nlight and the associated absorption\nof thermal energy.\nc Can I unlock my car with my\nsmartphone?\nTo unlock the vehicle, just hold\nyour smartphone next to the door\nhandle. Thanks to the preset ex-\npress mode, you do not need un-\nlock the device for this process.\nYes, you can unlock your car iX\nwith your smartphone.\nd Which sizes does the manufacturer\nhave approved for the wheels and\nrims of the car ?\nThe car is equipped with stan-\ndard 20-inch aerodynamically op-\ntimized light-alloy wheels. 21-\ninch and 22-inch Air Performance\nwheels are optional.\nThe following sizes are recom-\nmended and approved by the vehi-\ncle manufacturer for the approved\nwheels and tires per vehicle type\nand special equipment: Wheel and\ntire combinations. Rim designs.\nTire sizes. Tire brands. You can\nask an authorized service center\nor another qualified service center\nor repair shop about the approved\nwheels and tires for the vehicle and\nthe special equipment. For each\ntire size, the manufacturer of the\nvehicle recommends certain tire\nbrands.\ne How can I avoid parking damage? To protect against parking damage,\nthe \"Lateral Parking Aid\", a sub-\nfunction of Park Assist, warns of\nobstacles to the side of the vehi-\ncle during parking and leaving and\ngraphically displays them on the\ncontrol display.\nPark the vehicle as far away as pos-\nsible from passing traffic and on\nsolid ground.\nTable 11: Example cherry-picked question-answer pairs. The answer selected by the Answer Moderator is\nhighlighted in yellow.\n601\n# User Utterance System Response Extracted System Response Generated\na What is the range capacity when\nmy electric car is fully charged?\nWith maximum charging capacity,\nyou can charge the battery of the\ncar from 10% to 80% in less than\n40 minutes\nI have no information about that.\nb How many kilometers can I go\nwith 10% of charge of my electric\ncar?\nWithin ten minutes enough energy\ncan be fed into the battery to in-\ncrease the car’s range by more than\n120 kilometres (75 miles)\nSorry, I can’t answer this question\nbased on my material.\nc How much time does it take to\ncharge with 22kW from 10% to\n100% battery charge?\nIt takes less than 40 minutes 34 minutes.\nTable 12: Example lemon-picked question-answer pairs. The answer selected by the Answer Moderator is\nhighlighted in yellow.\n# User Utterance System Response Extracted System Response Generated\na If I have parking assist profes-\nsional, can I still park manually,\neven if the parking space leaves me\nless then 0.9m on top of my vehicle\nlength?\nPark Assist in your car cannot de-\ntect a parking space if the parking\nspace is too short for safe parking,\nthe lateral distance to the row of\nparked vehicles is greater than 1.5\nm or if the ultrasonic sensors have\nfunctional limitations.\nYes, you can.\nb How can I automatically park the\nvehicle on the center lane of the\nhighway?\nThe Parking Assistant Professional\nassistance system can - in addition\nto the scope of Parking Assistant\nPlus - park without a driver on\nboard. It automatically repeats a\nmaneuver that you have previously\nperformed yourself. In addition to\nmarking lines, it can also recognise\nkerbs and align itself to them. You\nonly have to monitor the automatic\nmanoeuvre from nearby using the\nkey. This allows you to let your ve-\nhicle drive into spaces that are too\nnarrow to get out.\nYes, you can.\nc How can I exit the car while driv-\ning on the highway?\n1. Stop immediately. 2. Park the\nvehicle in a safe place. 3. Exit the\nvehicle.\nYou can exit the car while driving\non the highway by pressing the but-\nton.\nTable 13: Example erroneous cases. The answer selected by the Answer Moderator is highlighted in yellow.\n602\nUser utterance How can I disable the safety feature that prevents the engine from starting\nautomatically in my car?\nCarExpert: You can deactivate the Evasion Assistant in your car by switching off all\nIntelligent Safety Systems (ALL OFF) via the \"Intelligent Safety Button\".\nGPT-3.5-turbo: The safety feature that prevents the engine from starting automatically\ncan be disabled by going through the menu as follows via iDrive: Apps\nmenu / \"Vehicle\" / \"Driving settings\" / \"Driver Assistance\" / \"Safety and\nwarnings\".\nLuminous-extended: You can disable the safety feature that prevents the engine from starting\nautomatically in your car by going through the menu as follows via iDrive:\nApps menu / \"Vehicle\" / \"Driving settings\" / \"Driver Assistance\" / \"Safety\nand warnings\". Select the desired settings.\nTable 14: Error analysis: a trade-off between helpfulness and harmlessness.\nQuery: What happens if I drive with the engine in idling state or with the drive-\nready state switched off?\nCarExpert: If the drive-ready state is on, the vehicle is ready to drive and \"READY\"\nappears on the instrument cluster. All vehicle systems are ready for\noperation.\nGPT-3.5-turbo: There are no benefits to driving with the engine in idling state or with the\ndrive-ready state switched off. In fact, it can be detrimental to the health\nof your battery.\nluminous-extended: Drive-ready state is switched on when you depress the brake while pressing\nthe Start/Stop button. Pushing the Start/Stop button again will deactivate\ndrive-ready state and activate standby state.\nTable 15: Error analysis: Hallucination.\n603\nHi there! How can I help you?\nHii!\nHow can I avoid parking \ndamage?\nAwesome, how does it work?\nThanks for your help!\nYou're welcome! Glad I could help!\nTo protect against  parking damage, the \"Lateral Parking \nAid\", a sub-function of Park Assist, warns of obstacles to \nthe side of the vehicle during parking and leaving and \ngraphically  displays them on the control display. \nThe obstacles are already detected while driving past a \nparking spacewith Park Assist activated and stored for the \nparking manoeuvre. \nExtracted \nfrom \ndocument\nHow large must the parking \nspace be?\nThe parking space must be at least 0.9 m longer than your \nvehicle.\n0.9 meters?\nYes, the parking space must be at least 0.9 m longer than \nyour vehicle.\nGeneration\nGeneration\nGeneration (Informal Talk)\nGeneration (Informal Talk)\nExtracted \nfrom \ndocument\nFigure 6: An example multi-turn conversation between the user and CarExpert. The dialog starts with informal talk\nbefore starting the information-seeking question-answering.\n604",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.6505600810050964
    },
    {
      "name": "Computer science",
      "score": 0.6344137191772461
    },
    {
      "name": "Natural language",
      "score": 0.45064789056777954
    },
    {
      "name": "Natural language processing",
      "score": 0.445349782705307
    },
    {
      "name": "Track (disk drive)",
      "score": 0.43554559350013733
    },
    {
      "name": "Linguistics",
      "score": 0.37781572341918945
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36701634526252747
    },
    {
      "name": "Philosophy",
      "score": 0.17779552936553955
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}