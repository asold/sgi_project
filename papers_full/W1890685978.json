{
    "title": "Contextual Information and Specific Language Models for Spoken Language Understanding",
    "url": "https://openalex.org/W1890685978",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5060649349",
            "name": "Paolo Baggia",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5082314268",
            "name": "Morena Danieli",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5028690295",
            "name": "Elisabetta Gerbino",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5050466312",
            "name": "L. Moisa",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5053619454",
            "name": "C. Popovici",
            "affiliations": [
                "Telecom Italia Lab"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1622397659",
        "https://openalex.org/W3720092",
        "https://openalex.org/W2158310889",
        "https://openalex.org/W1875550513",
        "https://openalex.org/W2963286281",
        "https://openalex.org/W1562973825",
        "https://openalex.org/W1487752133",
        "https://openalex.org/W2104906421",
        "https://openalex.org/W1989937443",
        "https://openalex.org/W2099161509",
        "https://openalex.org/W2535075100"
    ],
    "abstract": "In this paper we explain how contextual expectations are generated and used in the task-oriented spoken language understanding system Dialogos. The hard task of recognizing spontaneous speech on the telephone may greatly benefit from the use of specific language models during the recognition of callers' utterances. By 'specific language models' we mean a set of language models that are trained on contextually appropriated data, and that are used during different states of the dialogue on the basis of the information sent to the acoustic level by the dialogue management module. In this paper we describe how the specific language models are obtained on the basis of contextual information. The experimental result we report show that recognition and understanding performance are improved thanks to the use of specific language models.",
    "full_text": "arXiv:cmp-lg/9711006v1  19 Nov 1997\nContextual Information and Speciﬁc Language Models for Spo ken\nLanguage Understanding\nP. Baggia, M. Danieli, E. Gerbino, L. M. Moisa and C. Popovici\nCSELT – Centro Studi e Laboratori Telecomunicazioni\nVia G. Reiss-Romoli 274 , I-10148, Torino, Italia\n{baggia,danieli,gerbino}@cselt.it\n{loreta,cosmin}@obelix.cselt.it\nAbstract\nIn this paper we explain how contextual\nexpectations are generated and used in the\ntask-oriented spoken language understand-\ning system Dialogos. The hard task of rec-\nognizing spontaneous speech on the tele-\nphone may greatly beneﬁt from the use of\nspeciﬁc language models during the recog-\nnition of callers’ utterances. By ’speciﬁc\nlanguage models’ we mean a set of language\nmodels that are trained on contextually ap-\npropriated data, and that are used during\ndiﬀerent states of the dialogue on the ba-\nsis of the information sent to the acoustic\nlevel by the dialogue management module.\nIn this paper we describe how the speciﬁc\nlanguage models are obtained on the ba-\nsis of contextual information. The exper-\nimental result we report show that recog-\nnition and understanding performance are\nimproved thanks to the use of speciﬁc lan-\nguage models.\n1 Introduction\nUnderstanding natural dialogue over the telephone is\na complex task. Usually, the performance of speech\nrecognizers on public telephone networks are lower\nthan the ones obtained with microphonic input in\nlaboratory trials. The characteristics of natural di-\nalogue are intrinsically challenging for speech recog-\nnition: spoken language is often featured by frag-\nmentary input, extra-linguistic phenomena (such as\nblows and hesitations), repetitions, and miscommu-\nnications.\nThese features make a great impact in the per-\nformance of speech recognizers: the consequences\nare often an increased speech recognition error rate\nand a decreased usability of such systems, due to\nthe necessity of very long and tedious repair subdi-\nalogues. This situation may lead to the choice of\ncontrolling the complexity of the dialogue by con-\nstraining the form of the interaction between hu-\nmans and systems. Although this choice allows to\navoid some recognition errors (Danieli and Gerbino,\n1995; Potjer et al., 1996), it is very far from auto-\nmatically increasing the users’ satisfaction in using\na very system-driven dialogue system (Walker et al.,\n1997; Billi, Castagneri, and Danieli, 1997).\nIn order to have today usable spoken dialogue\nsystems in a telephone environment (according to\nthe state-of-the-art speech recognition technology),\na possible solution is to limit the complexity of the\ntask the systems have to perform, still allowing a\nnatural style of interaction. Under this respect we\nmust take into account the fact that most of the\ncurrent domains of application of telephone speech\nrecognition (such as the ﬂight or railway domains, or\nsome email agent applications) do not require a very\ncomplex task ﬂow structure. On the other hand,\nwe believe that if we exploit early in the recognition\nprocess of an utterance some contextual information\nabout the dialogue focus on hand, we can get bet-\nter recognition performance. In this paper we will\nshow that this solution is viable by describing how it\nhas been implemented in the spoken dialogue system\nDialogos.\nIn the literature on automatic spoken dialogue,\nthere is an increasing awareness that the problems of\nspontaneous speech have to be approached in terms\nof combining diﬀerent knowledge sources: acoustic,\nlinguistic and contextual information. In particu-\nlar, the use of contextual information and mixed-\ninitiative dialogue strategies have proved useful in\nincreasing the naturalness of human-machine inter-\nactions and the overall performance of spoken dia-\nlogue systems (Smith and Hipp, 1994). The con-\ntextual information can be expressed in terms of\npragmatic− based expectations about what the user\ncould probably say in her next utterance. As it was\nmentioned above, in this paper we claim that this\nkind of information may be used not only at the di-\nalogue level, but also for selecting speciﬁc language\nmodels at the acoustic level. Speciﬁc language mod-\nels can be deﬁned as a set of language models which\nare trained on contextually appropriated data, i.e.\nusers’ sentences uttered in the same dialogue con-\ntext. Speciﬁc language models may be used during\nthe recognition on the basis of the information sent\nto the acoustic level by the dialogue manager.\nThis paper explains how speciﬁc language models\nare obtained on the basis of the contextual infor-\nmation, and how they are used in Dialogos, a spo-\nken dialogue system able to understand spontaneous\nspeech on the telephone, in the domain of railway\ntime-table information. We will report experimental\nresults that show that the switching between speciﬁc\nlanguage models improves signiﬁcantly recognition\nand understanding performance of telephonic spon-\ntaneous speech. In section 2, we will give a brief\ndescription of the system architecture and function-\nalities, then we will introduce the knowledge that\ncontributes to design the contextual information and\nhow such information can be used to avoid recogni-\ntion errors. Section 4 presents how the speciﬁc lan-\nguage models are obtained based on the contextual\ninformation and gives an experimental evaluation.\n2 Dialogos Architecture and\nFunctionalities\nDialogos is a real time spoken dialogue system for\nthe Italian language. The system has been devel-\noped during the past few years by CSELT’s speech\nrecognition and understanding group. It works on\nthe public telephone network and it does not require\nany training session to be used by inexperienced sub-\njects. The application domain consists of Italian rail-\nway timetable; the dictionary contains 3,471 words,\nincluding 2,983 proper names of the Italian railway\nstations.\nDialogos is composed of a set of modules: the\nacoustical front-end, the acoustic processor, the lin-\nguistic processor, the dialogue manager and the text-\nto-speech synthesizer (which is ELOQUENS, a com-\nmercial TTS system designed at CSELT). A tele-\nphone interface connects the the acoustical front-end\nand the synthesizer to the public telephone network,\nwhile the dialogue manager is connected to the rail-\nway timetable database. All the system is software\nonly and completely integrated. It can run on a DEC\nalpha or on a PC Pentium equipped with a Dialogic\nD41E board. The railway timetable database runs\non a PC Pentium; a detailed description of the dif-\nferent modules is given in (Albesano et al., 1997).\nThe acoustical front-end performs feature extrac-\ntion and acoustic-phonetic decoding. The acous-\ntic modeling is based on a hybrid HMM-NN (Hid-\nden Markov Model - Neural Network) model. The\ntraining of the acoustic model simultaneously ﬁnds\nthe best segmentation of words into phonemes and\nof phonemes into states and trains the neural net-\nwork to discriminate between these states. The\nrecognition algorithm is based on frame synchronous\nViterbi decoding. During the recognition phase, a\nstatistic class-based bigram language model is used,\nwhile for re-scoring the n-best hypotheses a statis-\ntic trigram model is used. The linguistic processor\nstarts from the best-decoded sequence and performs\na multi-step robust partial parsing; at the end of\nthe analysis it constructs the deep semantic repre-\nsentation of the user utterance in the form of a case\nframe and send it to the dialogue module. The di-\nalogue manager interprets the semantic structure of\nthe user’s utterances on the basis of the dialogue his-\ntory and of the contextual knowledge. The explana-\ntion of the communication problems dealt with by\nthe dialogue system is given in (Danieli, 1996).\n3 Contextual Information\nIn order to get a natural interaction with the user,\na dialogue system has to take advantage from many\ntypes of contextual information: in the area of spo-\nken human-machine dialogue the emphasis is on the\nsystem reasoning in terms of communicative acts, or\ndialogue acts. That is done at very diﬀerent degrees\nof complexity: for example, the ARTIMIS system\n(Bretier and Sadek, 1997) explicitly uses a model\nof interaction where the communication between ac-\ntive agents is modeled in a theory of action, while\nseveral spoken dialogue systems allow a constrained\nand system-driven form of interaction. The dialogue\nmanager of Dialogos uses a task-based focus struc-\nture, and it provides the speaker with a ﬁxed-mixed\ninitiative capability. By “ﬁxed–mixed initiative ′′ we\nrefer to an interaction style where the user is driven\nto supply the system with the task parameters it\nneeds to access the database, but the user may still\nhave the control of the interaction if she decides to\nsupply more information than the one requested in\na single turn, or to correct some piece of information\nshe previously oﬀered. The dialogue manager is able\nto initiate clariﬁcation and correction subdialogues,\nand to detect speaker’s initiated repairs, both when\nthey are explicit and when they are performed by\nindirect speech acts.\nAt each turn of the dialogue the contextual infor-\nmation results from the current discourse focus and\nfrom the history of the user-system interaction from\nthe beginning up to the current turn. In Dialogos, at\neach dialogue turn the contextual information is rep-\nresented by the focused task parameter (for exam-\nple, the departure time), and by the dialogue move\ntriggered in next system turn (for example, the gen-\neration of a request dialogue act concerning the de-\nparture time).\n3.1 An example of interaction\nAs it was mentioned above, automatic speech recog-\nnition over the telephone may be error prone: the\ndialogue system has to be able both to guide the\ncaller in using the system appropriately, and to de-\ntect recognition or interpretation errors that might\nhave occurred in previous turns. For accomplishing\nthis task, the dialogue system takes advantage from\nthe global history of the interaction and it only ac-\ncepts interpretations of user’s input that are coher-\nent with that history. For example, let us consider\nthe following dialogue excerpt:\nT0-S: Hello, This is Train Enquiry Service.\nPlease speak after the tone.\nPlease state your departure and your destination.\nDA-REQUEST=dep-city,arr-city\nT1-U: Mila(no)- Milano di sera.\nPartenze BLOW da Milano a Roma.\nNOISE NO MILANO SERA.\nDEPARTURE BLOW FROM MILANO ROMA\n<conﬁrm=NO, dep-city=MILANO,\narr-city=ROMA, part-day=EVENING >\nT2-S: Do you want to go from Milano to Roma\nleaving in the evening?\nDA-VERIFY=dep-city,arr-city,part-day\nT3-U: Si ... da Milano a Roma di sera.\nYES BLOW FROM MILANO ROMA EVENING\n<conﬁrm=YES, dep-city=MILANO ,\narr-city=ROMA, part-day=EVENING >\nT4-S: There are many trains in the evening.\nWhich hour do you want to leave?\nDA-REQUEST=dep-time\nT5-U: Alle otto.\nAT EIGHT\n< hour=EIGHT >\nT6-S: Train 243 leaves from Milano Centrale at 8:20 p.m.;\nit arrives at Roma Termini at 6 a.m.\nDo you need additional information about this train?\n. . .\nFigure 1: Excerpt from the Dialogos corpus\nIn the example, on the left, the letter ′′T′′ stands\nfor ′′Turn′′, the letters ′′U′′ and ′′S′′ stand for ′′User′′\nand ′′System′′, respectively. Each user’s turn reports\nin Italian the original user’s utterance and the best\ndecoded sequence (i.e. the recognizer output); we\ntranslated into English and capitalized the best de-\ncoded sequence. The task-oriented semantic frame\n(produced by the parser) has been put between an-\ngles. The system turns have been only reported in\ntheir English translation. They are followed by the\nindication of the dialogue act they implement.\nIn T0 the system prompts the user for obtaining\nthe points of departure and destination, by trigger-\ning a DA-REQUEST concerning the task parame-\nters dep-city and arr-city. In T1 the user hesitates,\nthen she utters the name of the departure city, ”Mi-\nlano”. The ﬁrst part of the word, ”Mila-” was mis-\nrecognized as a noise, and the last syllable was rec-\nognized as ”no”: the parser interpreted it as the\nnegation ”no”. In this initial dialogue context there\nwas nothing to be denied, and the dialogue module\nis able to discard this negation and to address the\nuser with the verify dialogue act (DA-VERIFY) of\nT2-S. T3-U is the user’s acknowledge. After having\nconsulted the data in the railway database, the sys-\ntem realizes that the number of railway connections\nbetween Milano and Roma in the evening is high,\nand it suggests the user to choose a precise depar-\nture time (T4-S) (DA-REQUEST). That is done in\nuser’s turn T5-U.\nAll the dialogue acts triggered by the system turns\nT0-S, T2-S, and T4-S were sent to the language\nmodeling: on the basis of that information this mod-\nule was able to predict the speciﬁc language models\nto be activated during the recognition of T1-U, T3-\nU, and T5-U.\n3.2 An example of how predictions work\nIn this section we will compare the diﬀerent behavior\nof the speech recognizer when it uses a single lan-\nguage model and when it is supplied with speciﬁc\nlanguage models. Figure 2 reports an excerpt from\na telephone dialogue where the system was asking\nfor departure time (T8-S) and the user chose seven\no’clock as departure hour (T9-U).\nT8-S: Which hour do you want to leave?\nDA-REQUEST=dep-time\nT9-U: Alle sette.\nAT SEVEN\n. . .\nFigure 2: Excerpt from a dialogue\nIn recognizing the utterance in T9-U, the recog-\nnizer had to assign probabilities to three diﬀerent\nword sequences, the ones we report in the ﬁrst col-\numn of Table 1. The ﬁrst one is single word denoting\na town in Northern Italy, the second one is the re-\nally uttered phrase, and the third is a phrase which\nincludes another town name ( to Lecce). As we can\nobserve in the second column the use of a context-\nindependent language model in the recognizer would\nhave led the system to choose the third sequence,\nsince it got the best phonetic score. On the contrary,\nthe contextually specialized language model had the\nopportunity of assigning higher probabilities to the\nword sequences containing words denoting time ex-\npressions; in this particular case, the second word\nsequence (the really uttered one) got a better result,\nas we can see by considering the scores reported in\nthe third column.\nSequences Single LM Contextual LM\nAlessandria 0.25 0.05\nAlle sette 0.30 0.60\nA Lecce 0.35 0.20\nTable 1: Diﬀerent probabilities assigned by single\nLM and speciﬁc LMs\nThe system was able to activate the language\nmodel specialized for time expressions because it had\nconsidered the particular dialogue act triggered by\nthe dialogue manager, that is a DA-REQUEST, and\nthe semantic class of the parameter that was been\nrequested, that is a time expression. The activated\nlanguage model was a model trained on a class of\nsentence that occurred in human-machine dialogues\nin dialogue context related or similar to the current\none.\n4 Language Modeling Adaptation\nAlthough statistical language modeling for speech\nrecognition has been a wide studied research ﬁeld,\nonly recently the research community has focused\nspeciﬁcally on language modeling for spoken dia-\nlogue systems (SDS).\nIn a SDS there are novel problems, such as the\ndiﬃculty to gather a large enough sentence database\nfor the training of reliable language models (Popovici\nand Baggia, 1997): for example the language model\nin the Air Travel Information System (ATIS) is\ntrained on only 250,000 words (Ward and Issar,\n1994). Another problem, which is the topic of this\nSection, is how to take advantage of the expectations\ngenerated by the dialogue module in the language\nmodeling.\nUsually a recognizer uses a unique language model\n(LM) during all the dialogue interaction, neglecting\nthe opportunity to make use of dialogue expecta-\ntions. The adaptation of the LM to a dialogue con-\ntext consists in a better modeling of the linguistic\nconstraints at that particular point in the dialogue.\nThis can be done by training a speciﬁc LM for each\ndialogue context, which only uses the user utterances\nacquired in that speciﬁc context. The main problem\nis that the amount of data acquired in a dialogue\ncontext can greatly vary, so that it can be very small\nand consequently insuﬃcient to train a reliable LM.\nA preliminary work (Gerbino et al., 1995) showed\nthat in a task oriented dialogue the use of diﬀer-\nent language models applied in focused dialogue con-\ntexts (such as requests of city, data and time) im-\nproved the recognition performance. These ﬁndings\nwere also conﬁrmed by (Eckert et al., 1996) which\ndescribes the combination of statistic language mod-\nels and linguistic language models. The idea was\nfurtherly expanded by (Popovici and Baggia, 1997)\nwith the generation of models for each point in a\ndialogue. In the following this method, which is in-\ntegrated into the Dialogos system, is described and\nexperimental results are given.\n4.1 Language modeling adaptation in\nDialogos\nFor the adaptation of the language modeling in the\nDialogos system, the material acquired in a large\nﬁeld trial was used. The corpus was composed of\nnear 2,000 dialogues (19,697 utterances) collected\nfrom 493 naive users calling from all over Italy.\nAlthough the whole training-set is quite large, for\nmany dialogue contexts the training data were in-\nsuﬃcient. Therefore many of them were clustered\ntogether, on the basis of the following criteria.\nThe contexts were classiﬁed according to the ty-\npology of the dialogue acts (DA-REQUEST, DA-\nVERIFY). Then, the parameters associated to the\ndialogue act were taken into account, the ones which\nexpress the same semantic concept were clustered\ntogether (i.e. week-day and relative-day into dep-\ndate). Finally, in the case of the conﬁrmation of too\nmany parameters, only the ﬁrst two were considered.\nFollowing these criteria the original 70 dialogue\ncontexts were grouped into 10 classes. For each class\na speciﬁc LM was created, for a detailed description\nsee (Popovici and Baggia, 1997). The obtained LMs\nare listed below:\n• four classes for the veriﬁcation of each one of\nthe four parameters;\n• one class for the conjoint veriﬁcation of the de-\nparture and the arrival city;\n• four classes for the requests of each single pa-\nrameter;\n• one class for the conjoint request of departure\nand arrival;\nTable 2 shows the distribution of the training ma-\nterial for above mentioned classes.\nClass of question No. of No. of\nUtt. Words\nDA-REQUEST dep-city 375 873\nDA-REQUEST dep-city, arr-city 1,808 6,954\nDA-REQUEST arr-city 374 846\nDA-REQUEST time 1,291 3,945\nDA-REQUEST date 1,797 4,943\nDA-VERIFY dep-city 506 914\nDA-VERIFY dep-city, arr-city 1,804 3,508\nDA-VERIFY arr-city 398 655\nDA-VERIFY time 1,386 2,056\nDA-VERIFY date 1,565 2,317\nTable 2: Distribution of the training material for the\nspeciﬁc LM\nIt can be remarked that the amount of training\ndata for the single parameters dep-city and arr-city\nis rather small. This is because the dialogue strategy\nﬁrst asks dep-city and arr-city together, therefore\nthe request or conﬁrmation of a single city occurs\nonly in the case of a recovery subdialogue.\nAn other point is that, for DA-VERIFY, more\nthen 65% of the training material contains single-\nwords utterances (simple Yes/No), so that the eﬀec-\ntive training data for the more complex conﬁrma-\ntions is very limited.\n4.2 Context-independent vs.\ncontext-dependent LMs\nIn this section two experimental settings are com-\npared:\ncontext-independent: only a single LM trained\non the whole training-set and used in each point\nin the dialogue;\ncontext-dependent: the set of ten LMs described\nabove which are selected according to the con-\ntextual information of the point in which the\nuser utterance was produced.\nThe comparison is done at the perplexity values\n(PP), at recognition level (WA - Word Accuracy),\nand at the understanding level (SU - Sentence Un-\nderstanding rate) 1.\nThe results presented below were obtained using a\ntest-set of 1,540 spontaneous speech utterances from\nthe Dialogos corpus. For a clearer analysis the test-\nset was split up into two groups: the answers to sys-\ntem requests (748 utterances), “Requests” column\n1The evaluation at the understanding level is done\non the task-oriented semantic case-frame which is ﬁlled\nwith relevant words in the utterance. The SU accounts\nfor the exact match between the case-frame generated on\nthe recognized utterance and a manually corrected one,\nsee also (Albesano et al., 1997).\nin the following Tables; the answers to the conﬁr-\nmations (792 utterances), “Conﬁrms” column. Also\nthe global results are given, “Global” column.\nKind of LM Requests Conﬁrms Global\ncontext-indep. 60.0 9.9 28.9\ncontext-dep. 38.5 8.5 20.8\nTable 3: Comparison between Language Models at\nPerplexity Level\nTable 3 shows a considerable PP reduction, 36%\nfor the requests, that is 28% on the global results.\nThis suggests a probable improvement of recognition\nperformance on answers to the system requests. It is\nwell known that low perplexity value decrease does\nnot sensibly improve recognition results. For conﬁr-\nmations the PP values are very low because, as pre-\nviously mentioned, the training database contains\na majority of single-word answers, simply “Yes” or\n“No”, but even in this case the PP is reduced of the\n14%.\nKind of LM Requests Conﬁrms Global\ncontext-indep. 74.6 71.9 73.2\ncontext-dep. 78.9 72.0 75.1\nTable 4: Comparison between LMs at recognition\nlevel using the WA metrics\nTable 4 shows the improvements focalised on the\nrequests, with an error rate reduction of 17%. In\ncase of conﬁrmations, due to the scarcity of more\ncomplex sentence patterns, some speciﬁc LMs were\nnot so robust, especially for the two classes of conﬁr-\nmation of a single city parameters (see DA\nVERIFY\ndep-city and DA VERIFY arr-city in Table 2).\nIn this case the speciﬁc LMs were substituted in\nthe context-dependent experiment with the context-\nindependent. It is worth noticing that the opportu-\nnity to use a more robust model in a speciﬁc context\nis always possible in the case of multiple LMs, such\nas the context-independent case.\nKind of LM Requests Conﬁrms Global\ncontext-indep. 67.4 84.6 76.2\ncontext-dep. 71.3 85.1 78.4\nTable 5: Comparison between LMs at understanding\nlevel using the SU metrics\nThe analysis of the results reported in Table 5\nshows that the improvements obtained at the recog-\nnition level are maintained even at the understand-\ning level, with a global error rate reduction of 12%.\nAlthough the improvements for the conﬁrmations\nobtained at the recognition level is limited, at the\nunderstanding level it is quite relevant, 3% of error\nreduction. This fact shows that the use of the con-\ntextual information increases overall the recognition\nand understanding of the words which convey the\nsemantic content of the utterance.\n4.3 Implementational Issues\nThe speciﬁc LMs were integrated, and they are cur-\nrently in use, in the Dialogos system 2, but the use\nof a set of speciﬁc LMs, instead of a single one, re-\nquired to take into account of size and time issues\nto meet the constraint of a real-time system running\neither on a workstation or a PC platform.\nThe idea of dynamically re-loading a new model\nin each dialogue state was discarded because it was\na too time consuming activity, so that we chose to\nload all the set of LMs at the start-up time and then\nat each point in the dialogue just to switch from a\nmodel to another in a very fast way.\nIn order to reduce the size of the LMs a number\nof techniques have been studied, such a the word\nclustering or the use of a criteria that allows the\ndiscard of some probabilities in a LM. In our system\na word clustering algorithm was used on each model\nto reduce the number of word classes and therefore\nthe size of the model itself. The clustering algorithm\nused was a Maximum Likelihood method described\nin (Moisa and Giachin, 1995).\nIn the speciﬁc LMs of the Dialogos system, the\nword classes were reduced from 358 to 120 classes\nwith a reduction of the size of the whole set of LMs\nby 6 times. The adoption of the word clustered LMs\neven increases the robustness of the models to new\nevents.\n5 Conclusions\nIn this paper we have shown that the usability of\ntelephone applications of spoken dialogue systems\nmay be enhanced by the use of speciﬁc (dialogue\nstate dependent) language models during the recog-\nnition of users’ turns. We have illustrated the kind\nof contextual knowledge that allows the triggering\nof speciﬁc language models.\nThe performance of speciﬁc language models show\na general improvement both at the recognition and\nat the understanding level. The improvement is\n2For instance the Dialogos system has been recently\ntested during the ELSNET Olimpics“Testing Spoken Di-\nalogue Information Systems over the Telephone” at the\nEurospeech-97 Conference in Rhodes.\nhigher in the case of answers to system requests,\nand this suggests a further improvement, because it\nimplies a higher number of positive replies to the\nfollowing conﬁrmations and a reduction of some re-\ncovery subdialogues.\nThis kind of speciﬁc language models have been\nalready integrated into the real-time spoken dialogue\nsystem Dialogos.\nAcknowledgements\nThe authors Loreta Moisa and Cosmin Popovici\nwere researchers of ICI (Istitutul de Cercetari in In-\nformatica, Bucarest, Romania): the work reported\nin this paper was implemented while they were vis-\niting CSELT.\nReferences\n[Albesano et al.1997] Albesano, Dario, Paolo Bag-\ngia, Morena Danieli, Roberto Gemello, Elisa-\nbetta Gerbino, and Claudio Rullent. 1997. A\nRobust System for Human-Machine Dialogue in\nTelephony-Based Applications. To appear in In-\nternational Journal of Speech Technology, Kluwer\nAcademic Publishers. Vol.2, Nr. 2, December\n1997.\n[Billi, Castagneri, and Danieli1997] Billi, Roberto,\nGiuseppe Castagneri, and Morena Danieli. 1997.\nField trial evaluations of two diﬀerent information\ninquir systems. In Speech Communications. To\nappear.\n[Bretier and Sadek1997] Bretier,\nPhilippe, and David Sadek. 1997. A Rational\nAgent as the Kernel of a Cooperative Spoken Di-\nalogue System: Implementing a Logical Theory of\nInteraction. In J. P. Mueller, M. J. Wooldridge,\nand N. R. Jennings, editors, Intelligent Agents III\n- Proceedings of the Third International Workshop\non Agent Theories, Architectures, and Languages\n(ATAL-96). Lecture Notes in Artiﬁcial Intelli-\ngence, Springer-Verlag, Heidelberg, Germany.\n[Danieli1996] Danieli, Morena. 1996. On the Use of\nExpectations for Detecting and Repairing Human-\nMachine Miscommunications. In Proceedings of\nAAAI-96 Workshop on Detecting, Preventing and\nRepairing Human-Machine Miscommunications.\nPortland, Oregon, pages 87–93.\n[Danieli and Gerbino1995] Danieli, Morena and Elis-\nabetta Gerbino. 1995. Metrics for evaluating di-\nalogue strategies in a spoken language system. In\nProceedings of the 1995 AAAI Spring Symposium\non Empirical Methods in Discourse Interpretation\nand Generation, pages 34–39.\n[Eckert et al.1996] Eckert, Wieland, Florian Gall-\nwitz, and Heinrich Niemann. 1996. Combining\nStochastic and Linguistic Language Models for\nRecognition od Spontaneous Speech. In Proceed-\nings of ICASSP-96, Atlanta, vol. 1, pp. 423–427.\n[Gerbino et al.1995] Gerbino, Elisabetta, Paolo Bag-\ngia, Egidio Giachin, and Claudio Rullent. 1995.\nAnalysis and Evaluation of Spontaneous Speech\nUtterances in Focused Dialogue Contexts. In Pro-\nceedings of ESCA Workshop on Spoken Dialogue\nSystems, Vigso, Denmark, pp. 185–188.\n[Moisa and Giachin1995] Moisa, Loreta M., and\nEgidio Giachin. 1995. Automatic Clustering of\nWords for Probabilistic Language Models. In Pro-\nceedings of EUROSPEECH-95, Madrid, Spain,\nVol. 2, pp. 1249–1253.\n[Popovici and Baggia1997] Popovici, Cosmin, and\nPaolo Baggia. 1997. Specialized Language Mod-\nels Using Dialogue Predictions. In Proceedings of\nICASSP-97, Munich, Germany, vol. 2, pp. 815–\n818.\n[Popovici and Baggia1997] Popovici, Cosmin, and\nPaolo Baggia. 1997. Language Modelling for\nTask-Oriented Domains. To appear in Proceed-\nings of EUROSPEECH-97, Rhodos, Greece.\n[Potjer et al.1996] Potjer, J., A. Russel, L. Boves,\nand E. den Os. 1996. Subjective and Objective\nEvaluation of Two Types of Dialogues in a Call\nAssistance Service. In 1996 IEEE Third Work-\nshop: Interactive Voice Technologies for Telecom-\nmunications Applications, IVTTA, pages 89–92.\nIEEE.\n[Smith and Hipp1994] Smith, Ronnie W. and\nD. Richard Hipp. 1994. Spoken Natural Language\nDialogue Systems: A Practical Approach, Oxford\nUniversity Press, New York - Oxford.\n[Walker et al.1997] Walker, Marilyn A., Donald Hin-\ndle, Jeanne Fromer, Giuseppe Di Fabbrizio, and\nCraig Mestel. 1997. Evaluating Competing Agent\nStrategies For A Voice Email Agent. To appear\nin Proceedings of Eurospeech-97, Rhodes, Greece.\n[Ward and Issar1994] Ward, Wayne and Sunil Issar.\n1994. Recent Improvement in the CMU Spoken\nLanguage Understanding System In Proceedings\nof ARPA HLT Workshop, March, pp. 213–214."
}