{
  "title": "The Impact of Activation Patterns in the Explainability of Large Language Models – A Survey of recent advances",
  "url": "https://openalex.org/W4399038319",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4377437549",
      "name": "Mateus R. Figênio",
      "affiliations": [
        "Universidade Tecnológica Federal do Paraná"
      ]
    },
    {
      "id": "https://openalex.org/A69137607",
      "name": "André Santanchè",
      "affiliations": [
        "Universidade Estadual de Campinas (UNICAMP)",
        "Hospital de Clínicas da Unicamp"
      ]
    },
    {
      "id": "https://openalex.org/A4200781293",
      "name": "Luiz Gomes-Jr",
      "affiliations": [
        "Universidade Tecnológica Federal do Paraná"
      ]
    },
    {
      "id": "https://openalex.org/A4377437549",
      "name": "Mateus R. Figênio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A69137607",
      "name": "André Santanchè",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200781293",
      "name": "Luiz Gomes-Jr",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2899032424",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W3093452197",
    "https://openalex.org/W4385572928",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3038035611",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2753415590",
    "https://openalex.org/W2963123635",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4285798540"
  ],
  "abstract": "The performance benchmarks of Natural Language Processing (NLP) tasks have been overwhelmed by Large Language Models (LLMs), with their capabilities outshining many previous approaches to language modeling. But, despite the success in these tasks and the more ample and pervasive use of these models in many daily and specialized fields of application, little is known of how or why they reach the outputs they do. This study reviews the development of Language Models (LMs), the advances in their explainability approaches, and focuses on assessing methods to interpret and explain the neural network portion of LMs (specially of Transformer models) as means of better understanding them.",
  "full_text": "The Impact of Activation Patterns in the Explainability of\nLarge Language Models – A Survey of recent advances\nMateus R. Figˆenio1, Andr´e Santanch´e2, Luiz Gomes-Jr1\n1Departamento Acadˆemico de Inform´atica (DAINF)\nUniversidade Tecnol´ogica Federal do Paran´a (UTFPR)\n80.230-901 – Curitiba – PR – Brazil\n2Departamento de Sistemas de Informac ¸˜ao - DSI\nUniversidade Estadual de Campinas (UNICAMP)\n13083-970 – Campinas – SP – Brasil\nmateusfigenio@alunos.utfpr.edu.br, santanch@unicamp.br, lcjunior@utfpr.edu.br\nAbstract. The performance benchmarks of Natural Language Processing (NLP)\ntasks have been overwhelmed by Large Language Models (LLMs), with their\ncapabilities outshining many previous approaches to language modeling. But,\ndespite the success in these tasks and the more ample and pervasive use of these\nmodels in many daily and specialized fields of application, little is known of how\nor why they reach the outputs they do. This study reviews the development of\nLanguage Models (LMs), the advances in their explainability approaches, and\nfocuses on assessing methods to interpret and explain the neural network por-\ntion of LMs (specially of Transformer models) as means of better understanding\nthem.\n1. Introduction\nThe field of Natural Language Processing (NLP), an area of research intended of enabling\ncomputers to process large natural language datasets, has been revolutionized since the\nintroduction of deep learning (DL) models. Now, Large Language Models (LLMs) have\ndemonstrated stellar performance on hard NLP tasks, such as text summarization, ma-\nchine translation, question answering and dialog. Companies have launched many prod-\nucts and integrations based on new text generation models, such as OpenAI’s ChatGPT\nor Googles’s Gemini, from note-taking apps with broader self-completing capabilities, to\nmore interactive customer service applications and integration with search engines that\nsummarize the results of a search for the user.\nHowever, our understanding of the inner workings of Language Models (LMs)\nbased on neural network (NN) approaches lags behind the advancements in their size,\ncomplexity, architecture and broader use by society. To know how such models\nare designed and how they operate is different from understanding how its resulting\nproperties, such as connections and weights, lead the system to a given prediction\n[Lillicrap and Kording 2019]. This lack of interpretability and explainability erodes trust\nand disavows the application of these models in contexts where the reasoning behind a\ndecision is critical to its implementation, like those of the medical field. Meaningful ex-\nplanations can also benefit the development of deep learning systems by aiding to verify\nif a system works as intended, can help improve the system by better understanding its\nflaws, may lead to insights to specialists in its field of application and guarantee that the\nsystem complies to legislation, as argued by [Samek et al. 2017].\nAs such, the main goal of this study is to assess the state of explainability tools\nfor Large Language Models (LLMs), with a specific focus on tools targeted to neuron\nactivation explainability. In Section 2 we contextualize what language models are and\nthe history of their development, as well as existing explainability tools and frameworks.\nThen in Section 3 we take a deeper look at recent papers of neuron activation explainabil-\nity, which are further discussed in Section 4 where we summarize our findings, and in 5\nwe conclude our study.\n2. Foundations\nTo contextualize this study and to set common ground on therms and meanings, in this\nsection we define what are Language Models, briefly recall the development of Neural\nNetwork approaches in the field, from simple networks to Transformers, and present the\ndevelopment of explainability tools in the broader context of Natural Language Process-\ning (NLP).\n2.1. Language Models\nThe term Language Model (LM) refers to any system trained for the single task of pre-\ndicting a series of tokens, whether letters, words or sentences, sequentially or not, given\na previous or adjacent context [Bender and Koller 2020].\nThe first approaches for LMs were based on statistical patterns extracted\nfrom large corpora. These approaches used n-grams as the unit for probability\nestimation, with large n (i.e. large sequence of words) yielding better models\n[Manning and Schutze 1999]. However, these models could only capture dependencies\nin the n-word window, with poor coherence for larger or followings sequences.\n2.1.1. Neural Networks in NLP\nWith the introduction of NN models in the computing field, there was a surge of ini-\ntial NN approaches for language modeling, but, due to its similar implementation to\nthose of statistical approaches, it were equally limited in the scope of sequence length.\nThis changed with the introduction of the Long Short-term Memory (LSTM) Model\n[Hochreiter and Schmidhuber 1997], which was a version of the Recurrent Neural Net-\nworks (RNN) that addressed its issues and made viable its use in real world applications.\nIt allowed for the NN to have greater persistence of information over sequence data such\nas time series or, in the case of NLP, text. That is accomplished through the implementa-\ntion of a feedback loop, in which the output of the activation function is refeed to the cell\ntrough summation, allowing both the current and previous values to influence its com-\nputation. This, along with LSTM new features, allowed longer sequences to be fed into\nthe network and for the model to take a broader context into consideration, favoring its\napplication in the context of NLP.\nDespite its advances, neural networks were still hindered in the context of NLP\nthanks to the serial nature of its training, where one input had to be processed before the\nnext one could be computed, that made for its training to be costly and time-consuming.\n2.1.2. Transformers\nTransformers discard recurrence in favor of relaying solely on the attention mechanism,\nwhich functions as a mean of assigning how an input token relates to the tokens in its\nsurrounding context. This allows the model to take the entire context of a token in account\nwhen computing it independently, and, so, has no need for the recurrence mechanism to\nremember previous context and is no longer limited to a sequential training.\nThe model was present by [Vaswani et al. 2023] for sequence to sequence ma-\nchine translation. It’s architected in an encoder and decoder structure, where the encoder\ntakes the input sequence in the original language and processes it, then the decoder takes\nany previous translations to the target language that occurred before, or just a begging of\nsentence token, computes it, joins both of these processed inputs (the current sequence\nto translate with the previous translated portion) and outputs the predicted final sequence\nto the target language. The encoder and the decoder are composed of: input word em-\nbeddings, that convert the tokens to word vectors; positional encodings, that encode the\nrelative position of the word in the phrase; multi-head self-attention layers, that relate the\ntoken to its context; and feed-foward neural network (FFN) layers, that compute over all\nthe previous information together. The difference between the encoder and the decoder,\nis that the decoder is composed by an initial encoder portion and additional encoder-\ndecoder attention layers that combine the processed input text by the separate encoder\nwith the processed output translated text by its own initial encoder portion. The result of\nthis computation is then run through a final feed-foward, linear and softmax layers.\nIn this process, every input token is computed separately, while still considering\nits whole context, which enables the parallelization and distributed training of the model.\nWith this, the Transformer model circumvents the limitations of sequential training of\nprevious implementations, making possible greater models trained with greater datasets.\nAnother aspect of the Transformer, that goes hand in hand with its larger training capa-\nbilities, is its capability of being able to be fine-tuned for a specific context of application\nafter its main batch of training. This allows a base model to be extensively trained on a\nlarge corpus and then easily fine-tuned to specific applications. Finally, its structure can\nbe modified to achieve more specific goals, either by changing the number of attention\nand FFN layers or by ditching the encoder or the decoder entirely. For example, BERT is\ncomposed of stacked encoders, while GPT is composed of stacked decoders.\n2.1.3. Large Language Models\nThe innovations brought by the Transformer model, alongside its performance on various\nNLP tasks, led to the trend of larger and larger models such as OpenAI’s GPT-3, that\nreaches 175 billion parameters and 570GB of training data, which is orders of magnitude\nabove previous models developed for NLP. This earned the coinage of the term Large\nLanguage Models (LLMs) and the emergence of an area of research dedicated to them.\nOne of the latest developments in the LLMs field is the InstructGPT model\n[Ouyang et al. 2022], pioneered by OpenAI, that serves as the base for the new assistant\nmodels. Its Reinforcement Learning from Human Feedback (RLHF) utilizes a surrogate\nmodel, trained on preferred sentences ranked by human subjects, to generate sentences\nthat are used to refine a conventional base model to fallow instructions in more human\naligned ways and to generate a more human aligned text.\nWith this, research of LLMs has been categorized into two training paradigms:\ntraditional fine-tuning and prompting [Zhao et al. 2024]. Explainability works focused on\nthe first paradigm mainly deal with question such as how the model acquires foundational\nunderstating of language from its base training and how the fine-tuning process influences\nits ability to solve domain specific tasks. Whereas, explainability works dealing with the\nsecond paradigm aim to understand how base models (not further trained to align with\nhuman preferences) leverage its pre-trained knowledge to respond to prompts and how\nassistant models (that were trained to better align with human preferences) come to be\nable to interact with users in open-ended conversations. The differences between the two\nparadigms makes so that the methods through which they can be understood are different.\nBeyond their size and dimension, there is the added layer of difficulty of new types of\ntraining that brings back questions about what patterns and information do exactly these\nmodels capture from a general training of token prediction and what is learned from an\napproach targeted to a specific task and to generate human oriented text.\n2.2. Explainability and LLMs\nOne of the first works that specifically surveyed for tools of explainability of deep learning\nmodels in NLP and that categorized them according to a framework of thought was that of\n[Zini and Awad 2022], who proposed its framework based on three fundamental questions\nof model explanation: how they are explained, what is explained and which models are\nexplained.\nThe authors justify the necessity of explainability tools specific to models that op-\nerate on language processing tasks by contrasting the particular set of challenges posed\nby digitally processing human language to those posed by other applications, like signal,\nimage or data processing. Many tools have been developed for neural models of image\nprocessing, for example, but these fail to provide meaningful explanations for the predic-\ntions of language models, mostly due to the inherent differences of processing images to\nprocessing language. A letter or word has a different relationship to a sentence than a\npixel has to an image, while both are the object in training and operation of models.\n2.2.1. Explainability in NLP\nTackling the framework proposed by the authors, the question of “how” dictates if the\nexplanation is post-hoc, after the processing of a prediction, or if the model itself is in-\nterpretable by design. This is well exemplified by decision tress, where each branching\npath clearly defines a rule for decision-making so that its resulting outputs are inherently\nexplainable. A neural model, however, is much less interpretable (black-box) due to its\ncomplexity and non-linear relation of the inputs to its outputs, although even neural mod-\nels can be reconfigured and retrained to be more interpretable.\nThe question of “what” covers what element of the model is being explained\nbranched in three categories: input level, like the analysis of neural model embeddings\nand how they represent words; processing level, that focuses on the inner representations\nof neural models be it attention, specific weights and connections and how information is\nstored and used inside the NN; and output level, which aims to explain individual model\noutputs in respect to input features or the models inner workings.\nLastly, “which” model is explained refers to the explanation tool being model ag-\nnostic or model specific, since some tools are tailored to a model’s specific architecture\nand where others are independent of any models’ configuration. This is better represented\nby output explainability, where performance tests on established datasets enable compar-\nison between different model architectures and training forms.\n2.2.2. Explainability of LLMs Inputs\nThe first step of a LLMs processing is constituted by word embedding, a process by which\na token of a word is represented as dense vector so that it can be computed numerically.\nAlthough they can effectively and efficiently encode semantic and syntactic information,\nthese high dimension embeddings are hard to interpret, which is not only essential for\nthe making the whole of the model interpretable, but it is also desirable to ensure that the\nmodel is fair and efficient in its vocabulary representation.\nThe approaches developed for explainability of embedding revolve around: spar-\nsification of embedding spaces that aim to remove redundancies or unnecessary dimen-\nsions, rotation of embedding spaces to understand concept dimensions, integrating exter-\nnal lexicon and ontological knowledge to align the embeddings to human representations\nand, finally, evaluating embedding interpretability in standardized tests. As a example,\n[Raganato and Tiedemann 2018] present a general evaluation of BERT, mainly focusing\non the role of its attention mechanism in word representation, loosely fitting this category\nfor Transformer input explainability.\n2.2.3. Explainability of LLMs Inner Representations\nThe main approaches to explain and interpret the inner representations of LMs are divided\nin visualization and analysis approaches, the first pertaining to any method trough which\none can visualize how the model has computed any given input or how its insides are\nconfigured, and the second refers to mathematical or statistical ways to understand the\ninner workings of the models.\nTransformer inner representations explainability has, since its beginning, been\nextensively based on visualizing its attention mechanisms [Zini and Awad 2022], due\nto this mechanism more interpretable nature than that of the neurons of the FFN lay-\ners. The approaches vary in scale, from visualizing how individual attention heads\nevaluate tokens [Vaswani et al. 2023], to visualize how attention interacts across differ-\nent attentions heads [Strobelt et al. 2019], how attention values flow across the model\n[DeRose et al. 2020] and how individual attention heads relate to concepts given by the\nuser [Hoover et al. 2019].\nNow, turning to the network portion of Transformer models, there are probing and\nneuron activation explainability approaches. Probing is a technique based in the idea of\ntraining a shallow classifier over a model’s parameters to understand what they captured,\nin a sense an indirect approach to understand the model’s NN. Whereas, neuron activation\nis a direct approach that intends on understanding and explaining the neurons themselves,\nindividually and collectively, and how they can be modified and deconstructed to alter\ntheir behavior. This last approach is the focus of this study.\n2.2.4. Explainability of LLMs Outputs\nThe works in this category of explainability aim on providing evidence to support a\nmodel’s decision. They are divided into post-hoc interpretation, where a models inputs\nand features are perturbed to observe changes into its output, and inherently interpretable\nmodels, which provide confidence intervals to its predication or a reasoning for its con-\nclusion, be it by the model on prediction or a reference to an external ground truth system\nlike Knowledge Graphs (KG). Works of inherently interpretable models are scarce, with\n[Zini and Awad 2022] reasoning that this is due to the large computational costs involved\nin retraining LLMs to make them interpretable.\n3. Understanding and Manipulating Activation Patterns in LLMs\nThe Transformer model, currently used in the most successful LLMs, use several mech-\nanisms to capture linguistic and task-specific patterns. The focus of this survey is the\nactivation of the hidden layers of the deep neural network inside any transformer model.\nThe activation patterns can be used both to understand and to manipulate the models.\n3.1. Neuron Relevance Ranking\nA straight forward line of work aims to identify important neurons and relate individual\nneurons to linguistic properties. Such is the work of [Bau et al. 2018], that, following the\nintuition that different Neural Machine Translation (NMT) models developed to act on\nthe same languages will share similar properties, developed an unsupervised method to\ndiscover the neurons that relate to these shared properties. With this, the authors were\nable to modify the activations of individual neurons to control the model resulting outputs\nin predicable ways.\nAnother approach in this line of work is to use supervised methods to find impor-\ntant neurons in relation to specific language properties. Proposing a supervised method of\nneuron ranking, [Dalvi et al. 2019] aims to evaluate what is the impact of specific neurons\nin various language tests and how distributed or focused information is in NMT models.\nIts supervised approach of Linguistic Correlation Analysis classifies neurons in regard to\ntheir relevance to an expected linguistic property in the model input. To evaluate if their\nranking was meaningful, the authors used ablation to compare how a model configured\nonly with top neurons compared in performance to a model configure only with the bot-\ntom neurons. Although the architectures of the aforementioned methods were not that of\nthe Transformer, the authors claim that their findings can be extended to it and to different\ncomponents of NMT, such as the encoder and decoder.\nThe encoding of linguistic information often goes beyond a single neuron, encom-\npassing a subset of the available dimensions. [Hennigen et al. 2020] propose a method\nbased on Gaussian probes that identifies the subset of neurons associated with several\nlinguistic properties. The authors focus on encodings generated by BERT and fastText,\ntherefore limiting the analysis to the last hidden states of the networks.\n3.2. Neuron Information Retrieval\nAlternatively, another line of work is based on a mechanistic interpretability of neural\nlanguage models, investigating neurons and their connections in similar terms as those of\ncircuits. This interpretation was initially proposed to explain vision models, which can\nbe intuitively understood as being a system composed of simpler building blocks. This\napproach was extended to explain neural networks hidden representations, such as it can\nbe fruitfully applied to the LM context.\nIn [Geva et al. 2021], it is demonstrated that the FFN layers of a Transformer\nmodel acts like key value pairs that store memories related to patterns in its training data\nthat amass the probability in favor of a specific output vocabulary. These patterns have\nbeen found not only to be human interpretable, but shown that shallow layers capture shal-\nlow patters of text, while the upper layers capture more syntactic patterns. The authors\nanalyzed how cells are related to specific memories, and how the aggregated memories of\nmultiple cells combine to produce a distribution different from what each cell individu-\nally could provide. Following this work, [Geva et al. 2022] explored how the FFN layers\nupdate the representation of the output vocabulary space in distilled human interpreted\nconcepts. By decomposing the updates of the FFN layers to the vocabulary space in value\nvectors and analyzing how these vectors relate to concept annotated vectors, they discov-\nered that each update can be decomposed in human interpretable concepts and that they\ncan be altered and interfered to achieve more desirable outcomes, like less toxic models.\nUnderstanding the patterns of activation was a foundation step for\n[Meng et al. 2022], which were able to identify the main layers associated with\nfactual recalling in the LLMs tested. The authors identified the importance of the\nactivation in middle layers during the processing of subject tokens (in a sort of priming\nfor subsequent fact retrieval). After identifying the key activation regions, the authors\nintroduce changes in the model to make it generate different facts. This type of\npost-training intervention was shown to be effective and can represent a complementary\napproach for LLM tuning.\n4. Discussion\nIn revising previous surveys, the focus of the development of explainability methods ap-\npears to have shifted. In their assessment, previous to the prompting boom of late 2022,\n[Zini and Awad 2022] noted that its referenced works pointed to explainability develop-\nment being more dedicated towards understanding the inner workings of LMs than to\nexplaining particular outputs. Subsequently, however, the survey of [Zhao et al. 2024]\nshows that explainability research following the surge of new prompting models has\nmoved to more heavily develop works concerned with output analysis, including ap-\nproaches that utilized LMs to help evaluate and explain other LMs properties and outputs.\nThe attention mechanism is the focus of most of previous and current research\ntowards better understanding Transformer’s inner representations, either due to its more\ninherently more interpretable and visualizable behavior, or to its spotlight in the Trans-\nformer model development. In the latter years, new work has been developed to under-\nstand and explain the role of neural processing in the Transformer, from approaches that\naim to find neurons responsible for the models’ language capabilities, to works searching\nfor where specific information and factual knowledge is encoded.\nThis last line of work seems to be the most promising in making the models more\ninterpretable, their decisions more explicable, and their use more safe. The direct anal-\nysis of a model’s inner representations allows the study of what information influenced\nits output, how it did, and enables for the alteration of this information towards more de-\nsirable outcomes, like more accurate and less toxic models. In contrast, output analysis\napproaches seem limited with its incapacity to remedy the model’s failings, only being\nable to identify it.\n5. Conclusion\nThis study presents a comprehensive background of LM explainability, firstly contextual-\nizing the development of LMs, then presenting an explainability framework of thought for\ntools of explainability geared towards these models, and concluding with a focused review\nof works concerned with LMs hidden representation explainability and interpretability.\nThe methods reviewed in this study present a promising line of work for making black-\nbox LM more transparent, interpretable, explainable and safe.\nReferences\nBau, A., Belinkov, Y ., Sajjad, H., Durrani, N., Dalvi, F., and Glass, J. (2018). Identifying\nand controlling important neurons in neural machine translation.\nBender, E. M. and Koller, A. (2020). Climbing towards NLU: On meaning, form, and\nunderstanding in the age of data. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nDalvi, F., Durrani, N., Sajjad, H., Belinkov, Y ., Bau, A., and Glass, J. (2019). What\nis one grain of sand in the desert? analyzing individual neurons in deep nlp models.\nProceedings of the AAAI Conference on Artificial Intelligence, 33(01):6309–6317.\nDeRose, J. F., Wang, J., and Berger, M. (2020). Attention flows: Analyzing and compar-\ning attention mechanisms in language models.\nGeva, M., Caciularu, A., Wang, K. R., and Goldberg, Y . (2022). Transformer feed-forward\nlayers build predictions by promoting concepts in the vocabulary space.\nGeva, M., Schuster, R., Berant, J., and Levy, O. (2021). Transformer feed-forward layers\nare key-value memories.\nHennigen, L. T., Williams, A., and Cotterell, R. (2020). Intrinsic probing through dimen-\nsion selection. arXiv preprint arXiv:2010.02812.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computa-\ntion, 9:1735–80.\nHoover, B., Strobelt, H., and Gehrmann, S. (2019). exbert: A visual analysis tool to\nexplore learned representations in transformers models.\nLillicrap, T. P. and Kording, K. P. (2019). What does it mean to understand a neural\nnetwork?\nManning, C. and Schutze, H. (1999).Foundations of statistical natural language process-\ning. MIT press.\nMeng, K., Bau, D., Andonian, A., and Belinkov, Y . (2022). Locating and editing factual\nassociations in gpt. Advances in Neural Information Processing Systems , 35:17359–\n17372.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C.,\nAgarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. (2022). Training\nlanguage models to follow instructions with human feedback. Technical report, Ope-\nnAI. Dispon´ıvel em: https://arxiv.org/abs/2203.02155.\nRaganato, A. and Tiedemann, J. (2018). An analysis of encoder representations in\ntransformer-based machine translation. In Linzen, T., Chrupała, G., and Alishahi, A.,\neditors, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 287–297, Brussels, Belgium. Association\nfor Computational Linguistics.\nSamek, W., Wiegand, T., and M ¨uller, K.-R. (2017). Explainable artificial intelligence:\nUnderstanding, visualizing and interpreting deep learning models.\nStrobelt, H., Gehrmann, S., Behrisch, M., Perer, A., Pfister, H., and Rush, A. M. (2019).\nSeq2seq-vis: A visual debugging tool for sequence-to-sequence models. IEEE Trans-\nactions on Visualization and Computer Graphics, 25(1):353–363.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L.,\nand Polosukhin, I. (2023). Attention is all you need.\nZhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., Wang, S., Yin, D., and Du, M.\n(2024). Explainability for large language models: A survey. ACM Trans. Intell. Syst.\nTechnol. Just Accepted.\nZini, J. E. and Awad, M. (2022). On the explainability of natural language processing\ndeep models. ACM Computing Surveys, 55(5):1–31.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7697886228561401
    },
    {
      "name": "Transformer",
      "score": 0.6395617723464966
    },
    {
      "name": "Language model",
      "score": 0.6122003197669983
    },
    {
      "name": "Artificial neural network",
      "score": 0.5165354609489441
    },
    {
      "name": "Natural language",
      "score": 0.45629218220710754
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4350959360599518
    },
    {
      "name": "Natural language processing",
      "score": 0.39800262451171875
    },
    {
      "name": "Machine learning",
      "score": 0.3271917998790741
    },
    {
      "name": "Engineering",
      "score": 0.11289381980895996
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1283613182",
      "name": "Universidade Tecnológica Federal do Paraná",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I4210148978",
      "name": "Hospital de Clínicas da Unicamp",
      "country": "BR"
    },
    {
      "id": "https://openalex.org/I181391015",
      "name": "Universidade Estadual de Campinas (UNICAMP)",
      "country": "BR"
    }
  ]
}