{
  "title": "RealFormer: Transformer Likes Residual Attention",
  "url": "https://openalex.org/W3176948526",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2265624539",
      "name": "Ruining He",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3016985753",
      "name": "Anirudh Ravula",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A311418139",
      "name": "Bhargav Kanagal",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3127151792",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962761235",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W3035618017",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970467549",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W4288621368",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Transformer is the backbone of modern NLP models.In this paper, we propose Real-Former, a simple and generic technique to create Residual Attention Layer Transformer networks that significantly outperform the canonical Transformer and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA, Natural Questions, and OpenKP.We also observe empirically that RealFormer stabilizes training and leads to models with sparser attention.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 929–943\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n929\nRealFormer: Transformer Likes Residual Attention\nRuining He, Anirudh Ravula, Bhargav Kanagal, Joshua Ainslie\nGoogle Research\n{ruininghe,braineater,bhargav,jainslie}@google.com\nAbstract\nTransformer is the backbone of modern NLP\nmodels. In this paper, we propose Real-\nFormer, a simple and generic technique to\ncreate Residual Attention Layer Transformer\nnetworks that signiﬁcantly outperform the\ncanonical Transformer and its variants (BERT,\nETC, etc.) on a wide spectrum of tasks\nincluding Masked Language Modeling,\nGLUE, SQuAD, Neural Machine Translation,\nWikiHop, HotpotQA, Natural Questions, and\nOpenKP. We also observe empirically that\nRealFormer stabilizes training and leads to\nmodels with sparser attention. Source code\nand pre-trained checkpoints for RealFormer\ncan be found at https://github.com/\ngoogle-research/google-research/\ntree/master/realformer.\n1 Introduction\nTransformer (Vaswani et al., 2017) architectures\nare the backbone of numerous state-of-the-art NLP\nmodels such as BERT (Devlin et al., 2019),\nGPT (Radford et al., 2019), and Meena (Adiwar-\ndana et al., 2020), and have seen wide success\nacross both academia and industry. Typically, a\nTransformer network consists of a stack of residual\nlayers. The original design follows a “Post-LN”\nstructure which adds Layer Norm (LN) as a “post-\nprocessing” step for each sub-layer, as shown in\nFigure 1 (a). It has been adopted by various state-of-\nthe-art models including BERT, XLNet (Yang et al.,\n2019), RoBERTa (Liu et al., 2019), ALBERT (Lan\net al., 2019), Transformer-XL (Dai et al., 2019),\nand ETC (Ainslie et al., 2020). Another notable\ndesign is to reorganize the order of modules to\ncreate a “direct”/clean path to propagate embed-\ndings of tokens in the input sequence through the\nwhole network, as shown in Figure 1 (b). 1 This\n1Note that a ﬁnal LN module is usually added at the very\ntop of the whole network.\ndesign adds LN as a “pre-processing” step for\neach sub-layer, and is often referred to as “Pre-LN”\nand used by some well-known extra large models\nsuch as GPT-2 (Radford et al., 2019) and Mega-\ntron (Shoeybi et al., 2019). In some respect, Post-\nLN and Pre-LN are analogous to ResNet v1 (He\net al., 2016a) and ResNet v2 (He et al., 2016b)\nrespectively in the Computer Vision literature. Al-\nthough ResNet v2 is usually preferable to v1 for\nComputer Vision, it does not appear to be the case\nfor Pre-LN Transformer in the NLP literature. It\nis likely that the particularities of self-attention\nmodules and Transformer architectures potentially\nfavor (at least slightly) different designs compared\nto traditional convolutional neural networks.\nIn this paper, we propose a simple and generic\ntechnique to show that it is beneﬁcial to create a “di-\nrect” path to propagate raw attention scores through\nTransformer-based networks. Our technique is\ncalled Residual Attention Layer Transformer, or\nRealFormer in short. We also use RealFormer to\ndenote the resulting Transformer networks when-\never no confusion may arise. Without losing gener-\nality, taking the standard Transformer encoder as\nan example, each RealFormer layer takes the raw\nattention scores of all attention heads from the pre-\nvious layer and adds “residual scores” (computed\nthe same way as attention scores in regular Trans-\nformers) on top, as shown in Figure 1 (c). The sum\nof the two scores is then used to compute attention\nprobabilities via softmax.\nIn other words, RealFormer can be seen as\nadding simple skip connections to a backbone\nTransformer. Since it does not add expensive multi-\nplication ops, performance is expected to be compa-\nrable.2 Note that our technique can also be applied\nstraightforwardly for different Transformer varia-\n2In certain settings, we ﬁnd it helpful for RealFormer to\nuse running mean of attention scores instead of running sum,\nthough it adds some negligible amount of multiplications.\n930\nFigure 1: Comparison of different styles of Transformer layers: (a) The prevalent Post-LN layer used by ( e.g.)\nBERT; (b) Pre-LN layer used by (e.g.) GPT-2 that creates a “direct” path to propagate token embeddings; (c) Our\nRealFormer layer that creates a “direct” path to propagate attention scores (by adding a simple skip edge on top\nof (a)). Note that here we are showing Transformer encoder for demonstration purposes only; RealFormer can be\napplied straightforwardly for different Transformer variations (e.g., when decoders are involved).\ntions and even when decoders are involved.\nSpeciﬁcally, our main contributions include:\n•We present RealFormer, a simple, generic,\nand cheap technique to improve Transformer-\nbased networks. It adds no parameters or\nhyper-parameters, and usually takes no more\nthan a few lines of code changes to implement.\n•We show that RealFormer can be used as\na drop-in replacement of Transformer in\nBERT, outperforming both Post-LN and Pre-\nLN Transformers across a wide spectrum of\nmodel sizes for pre-training. In terms of ﬁne-\ntuning, it even achieves competitive down-\nstream results when pre-trained with only half\nthe number of epochs of the baselines.\n•We further demonstrate the genericity of Real-\nFormer by using it as a drop-in replacement of\ntwo recent state-of-the-art Transformer varia-\ntion models: ADMIN (Liu et al., 2020) from\nthe Neural Machine Translation (NMT) do-\nmain, and ETC (Ainslie et al., 2020) that ex-\ntends Transformer to handle long and struc-\ntured inputs. We show that RealFormer can\nimprove these models signiﬁcantly on various\ntasks and lead to new state-of-the-art results.\n•Qualitatively, we observe that attention in Re-\nalFormer tends to be sparser and more cor-\nrelated across layers compared to baselines,\nwhich we believe may have some regulariza-\ntion effects that could stabilize training and\nbeneﬁt ﬁne-tuning.\n2 Related Work\nVaswani et al. (2017) proposed Transformer ini-\ntially for NMT and it has profoundly changed the\nNLP ﬁeld ever since.\nRadford et al. (2018) demonstrated that genera-\ntive pre-training of a Transformer-based language\nmodel (GPT) on a diverse corpus of unlabeled text\ncan give large gains to downstream NLP tasks\nthat suffer from scarce labeled data. Following\nthis thread, Devlin et al. (2019) proposed to pre-\ntrain a bidirectional Transformer encoder (BERT)\nwith a novel Masked Language Modeling as the\nmain optimization objective. Since then, advances\non many NLP tasks have been dominated by the\nself-supervised general-purpose pre-training, task-\nspeciﬁc ﬁne-tuning paradigm. Following BERT,\nthere has been a large stream of work that explores\nbetter self-supervision objectives (e.g., Yang et al.\n(2019); Clark et al. (2020)), larger pre-training data\nand better hyper-parameters (e.g., Liu et al. (2019)),\nmodel parameter sharing (e.g., Lan et al. (2019)),\nmulti-task pre-training (e.g., Sun et al. (2020); Raf-\nfel et al. (2020)). These efforts typically employ a\nPost-LN Transformer at their core. In this paper we\nadopt BERT to test different Transformer architec-\ntures because it is widely used and representative\n931\nof this body of work.\nAnother notable thread of work focuses on im-\nproving the efﬁciency/scalability of Transformer.\nTypically, they try to reduce the quadratic com-\nplexity of the self-attention mechanism with re-\nspect to sequence length via low-rank methods\n(e.g., Wang et al. (2020)), ﬁxed strided attention\npatterns (e.g., Child et al. (2019)), learnable atten-\ntion patterns (e.g., Kitaev et al. (2020); Roy et al.\n(2020)), memory-based global & local attention\n(e.g., Ainslie et al. (2020); Beltagy et al. (2020);\nZaheer et al. (2020)), and so on. These methods\nare particularly useful when dealing with long doc-\numents that go beyond the capacity of standard\nTransformer models. We would refer the reader\nto Tay et al. (2020) for a detailed survey. Real-\nFormer is orthogonal to these methods as it fo-\ncuses on improving various Transformer networks\nwith an universal technique which can apply to\nthese models as well. In this paper, we will use\nRealFormer to improve a state-of-the-art model,\nETC (Ainslie et al., 2020), from this line of work\nto demonstrate the universality of RealFormer.\nSome recent work ( e.g., Wang et al. (2019b);\nXiong et al. (2020); Zhang et al. (2018); Huang\net al. (2020); Zhang et al. (2019)) has studied nor-\nmalization and parameter initialization schemes\nfor Transformers, though most evaluations focus\nonly on NMT to the best of our knowledge. In this\nstrand, Liu et al. (2020) recently proposed ADMIN,\nwhich achieved state-of-the-art results on multiple\npopular NMT benchmarks. In this paper, we will\ntake ADMIN as an example to (1) evaluate Re-\nalFormer in settings involving decoders, and (2)\nshow that it is possible to apply RealFormer on top\nof this line of work.\n3 RealFormer\n3.1 Standard Transformer\nThere is an encoder and a decoder in Trans-\nformer (Vaswani et al., 2017). Since they work\nin a similar way, here we only introduce the en-\ncoder and refer the reader to the original paper for\ncomplete details.\nThere are two sub-layers inside each layer of a\nTransformer encoder. The ﬁrst sub-layer contains a\nMulti-Head Attention module that computes output\nembeddings of a set of queries (Q) by aggregating\nthe embeddings (V) of a set of keys (K):\nMultiHead (Q,K,V ) =\nConcat (head1,...,head h) WO,\nwhere headi = Attention (QWQ\ni ,KW K\ni ,VW V\ni ).\nQand K are matrices with dimension dk and V\nis a matrix with dimension dv. WQ\ni , WK\ni , and\nWV\ni are matrices that linearly project queries, keys,\nand values into the “attention space” of the i-th\nhead. WO is a matrix that linearly transforms the\nconcatenation of the outputs of all heads.\nThe attention function is typically imple-\nmented with a Scaled Dot-Product Attention mod-\nule (Vaswani et al., 2017) which computes a\nweighted sum of the values:\nAttention (Q′,K′,V ′) =Softmax (Q′K′T\n√dk\n) V′,\nwhere matrix Q′K′T\n√dk\ncontains the raw attention\nscores for each (query, key) pair. These scores\nare normalized via the Softmax function for each\nquery and then act as weights for the corresponding\nvectors in V′.\nThe second sub-layer contains a fully-connected\nFeed-Forward Network (FFN) module with one\nhidden layer:\nFFN (x) =σ(xW1 + b1) W2 + b2,\nwhere σ is an activation function usually imple-\nmented with ReLU or GELU ( e.g., Devlin et al.\n(2019)). FFN is applied to each position in the se-\nquence separately and identically. Finally, there are\nLayer Norm (LN) modules inserted into the above\ntwo sub-layers to stabilize training.\nAs shown in Figure 1, there are two canonical de-\nsigns of the Transformer network which only differ\nin the ways they organize the modules. Post-LN is\nthe original architecture proposed by Vaswani et al.\n(2017) which normalizes the outputs at the end\nof each sub-layer. In contrast, Pre-LN normalizes\nsub-layer inputs instead and creates a direct path\n(without LN in the way) to propagate embeddings\nof the tokens in the sequence.\n3.2 Residual Attention Layer Transformer\nRealFormer uses a Post-LN style Transformer 3\nas backbone and adds skip edges to connect\n3Potentially we could also use Pre-LN, but Post-LN tends\nto outperform Pre-LN, as we will show in Section 4.\n932\nMulti-Head Attention modules in adjacent lay-\ners, as shown in Figure 1 (c). More for-\nmally, it adds Prev, the pre-softmax atten-\ntion scores from the previous layer with shape\n(#heads,from seq len,to seq len),4 as one ad-\nditional input to the Multi-Head Attention module\nin the current layer:\nResidualMultiHead (Q,K,V,Prev ) =\nConcat (head1,...,head h) WO,\nwhere headi = ResidualAttention (QWQ\ni ,\nKWK\ni ,VW V\ni ,Prev i) and Previ is the slice of\nPrev with shape (from seq len,to seq len) cor-\nresponding to headi. ResidualAttention adds\n“residual scores” on top of Previ and then com-\nputes the weighted sum as usual:\nResidualAttention (Q′,K′,V ′,Prev ′) =\nSoftmax (Q′K′T\n√dk\n+ Prev′) V′.\n(1)\nFinally, new attention scores Q′K′T\n√dk\n+ Prev′are\npassed over to the next layer.\nImplementing RealFormer takes no more than\nadding a few lines of code to the backbone Trans-\nformer. Note that the RealFormer technique can\nbe straightforwardly applied for Transformer varia-\ntions and even when there are more than one type\nof attention modules in the network. For example,\nthere are encoder self-attention, encoder-decoder\nattention, and decoder self-attention modules for\nmachine translation. In such cases, RealFormer\nsimply adds skip edges to create multiple direct\npaths, one for each type of attention module.\nDiscussion. Adding skip edges is equivalent to\nusing a softmax over the running sum of the at-\ntention scores (to get attention probabilities). This\nmight be sub-optimal for very deep networks due\nto the linear scaling nature of sum. Empirically, we\nﬁnd it helpful to use running mean instead in such\ncases, which can be viewed as adding a tempera-\nture (i.e., #traversed layers) to the softmax function\nin Eq. 1 of each RealFormer layer.\n4 Experiments\nTo demonstrate that RealFormer is general-purpose,\nwe conduct comprehensive empirical studies on\na variety of tasks including (masked) language\n4Batch dimension is omitted for ease of discussion.\nmodeling, machine translation, and long document\nmodeling, based on corresponding state-of-the-art\nmodels: BERT, ADMIN, and ETC. To evaluate its\nrobustness, we only do minimal (if at all) hyper-\nparameter tuning for RealFormer and initialize all\nparameters the same way as the backbone Trans-\nformers. More aggressive hyper-parameter tuning\nor better initialization might further improve Real-\nFormer, though we leave them for future work. De-\ntails of our experiments are included in Appendix.\n4.1 BERT\nBERT (Devlin et al., 2019) has been the standard\nway of transferring knowledge from large unla-\nbeled text corpora by pre-training a bidirectional\nTransformer encoder. Numerous downstream NLP\ntasks suffering from scarcity of supervised data\nhave beneﬁted considerably by ﬁne-tuning a pre-\ntrained BERT model. This drives us to adopt BERT\nas the main evaluation setup for RealFormer.\nExperiment setup. Our experiments are based\non the ofﬁcial BERT repository 5. We follow the\nstandard pre-training setup (dataset: Wikipedia +\nBookCorpus, vocab: uncased 30K, max sequence\nlength: 512 6, dropout: 10%, learning rate: 1e-4,\nlearning rate schedule: warm up and then linearly\ndecay to 0, weight decay: 0.01, optimizer: AdamW,\nobjective: Masked Language Modeling + Next\nSentence Prediction, etc.) to compare three Trans-\nformer models: Post-LN, Pre-LN, and RealFormer.\nWe experiment with Transformer architectures with\na wide spectrum of sizes as detailed in Table 1. For\nsimplicity, all models are pre-trained 1M steps with\na mini-batch size of 512 (except that xLarge uses\n256 to avoid TPU OOM). Note that we use a larger\nmini-batch size than Devlin et al. (2019), i.e., dou-\nbling the amount of pre-training epochs, to show\nmore complete behavior of different models.\nWe use exactly the same setup for all three\nTransformer architectures except that for the Pre-\nLN Transformer we follow the initialization strat-\negy suggested by Radford et al. (2019) and Child\net al. (2019).7 Note that for simplicity RealFormer\nreuses all hyper-parameter setups from Post-LN\nTransformer unless otherwise speciﬁed. We use\nrunning sum of attention scores for all RealFormer\n5https://github.com/google-research/\nbert\n6Unlike BERT which uses a reduced sequence length for\nthe ﬁrst 90% of steps, we always use 512 for simplicity.\n7We also tried BERT-style initialization in our pilot experi-\nments but without success.\n933\n(a) BERT-Small\n (b) BERT-Base\n (c) BERT-Large\n (d) BERT-xLarge\nFigure 2: Development set MLM accuracy (best viewed in color). Improvement gap of RealFormer over the best\nbaseline tends to increase with model size. Note that these are without hyper-parameter tuning for RealFormer. (As\nwe will show later, RealFormer can beneﬁt from larger learning rates and even double the gap size over Post-LN.)\nModel L H A I P\nBERT-Small 4 512 8 2,048 30M\nBERT-Base 12 768 12 3,072 110M\nBERT-Large 24 1,024 16 4,096 340M\nBERT-xLarge 36 1,536 24 6,144 1B\nTable 1: Model architectures for BERT evaluation. L:\n#layers, H: hidden size, A: #heads, I: intermediate size,\nP: approximate #parameters.\nModel Post-LN Pre-LN RealFormer\nBERT-Small 61.57% 61.67% 61.70%\nBERT-Base 70.20% 69.74% 70.42%\nBERT-Large 73.64% 73.21% 73.94%\nBERT-xLarge 73.72% 73.53% 74.76%\nTable 2: Development set MLM accuracy after pre-\ntraining 1M steps. RealFormer outperforms baselines\nmore as model size increases.\nmodels except xLarge (for which we use running\nmean for reasons discussed in Section 3.2).\nAll experiments are performed on 128 or 256\nTPU v3 cores depending on model sizes (see Ap-\npendix A.1 for details).\n4.1.1 Pre-training Results\nTo evaluate pre-trained models, we report Masked\nLanguage Modeling (MLM) accuracy 8 on a ran-\ndomly held-out development set. As shown in Ta-\nble 2, RealFormer outperforms the two baseline\nTransformers considerably with the gap increasing\nwith model size. Our hypothesis is that larger mod-\nels are inherently harder to train (e.g., we observe\nthat BERT with Post-LN is unstable and sometimes\neven diverges for xLarge) and RealFormer can help\nregularize the model and stabilize training.\n8All methods achieved similar (and great) results on Next\nSentence Prediction presumably because it is much easier.\nWe also report the pre-training curves in Fig-\nure 2. One interesting ﬁnding is that the Pre-LN\nTransformer seems to favor the combination of\nextra large models and a small number of steps,\nthough it is consistently outperformed by the other\ntwo in “regular-sized” settings or given enough\npre-training budget.\n4.1.2 Downstream Results\nTo evaluate downstream performance, we ﬁne-tune\nthe above pre-trained BERT-Large models on both\nsentence-level (i.e., GLUE) and token-level ( i.e.,\nSQuAD) NLP tasks.\nGLUE. General Language Understanding Evalu-\nation (GLUE) is a canonical benchmark proposed\nby Wang et al. (2019a) for evaluating models across\na diverse set of NLU tasks. Following the ﬁne-\ntuning recipe in Devlin et al. (2019), we use a mini-\nbatch size of 32 for all models on all tasks. For each\n(task, model) pair, we select number of ﬁne-tuning\nepochs in {2, 3, 4}and learning rate in {6e-6, 8e-\n6, 1e-5, 2e-5, 3e-5, 4e-5, 5e-5 }.9 For each setup,\nwe run the experiment ﬁve times and report the\nbest median performance and the corresponding\nstandard deviation on the development set.\nResults are tabulated in Table 3. We exclude\nthe problematic WNLI task following Devlin et al.\n(2019). For each task, we report metric(s) sug-\ngested by Wang et al. (2019a). RealFormer\nachieves the best overall performance and outper-\nforms both baselines on most tasks, testifying its\nstrength at tackling sentence-level tasks.\nSQuAD. The Stanford Question Answering\nDataset (SQuAD v1.1) is a reading comprehension\ndataset consisting of 100K crowd-sourced question-\nanswer pairs, where the answer to each question is\n9We use a slightly wider range than Devlin et al. (2019) to\nbetter accommodate all three models.\n934\nTask Post-LN Pre-LN RealFormer\nMNLI-m 85.96±0.11 85.03±0.12 86.28±0.14\nMNLI-nm 85.98±0.14 85.05±0.19 86.34±0.30\nQQP 91.29±0.10 91.29±0.16 91.34±0.03\nQQP (F1) 88.34±0.15 88.33±0.26 88.28±0.08\nQNLI 92.26±0.15 92.35±0.26 91.89±0.17\nSST-2 92.89±0.17 93.81±0.13 94.04±0.24\nCoLA (MC) 58.85±1.31 58.04±1.50 59.83±1.06\nSTS-B (PC) 90.08±0.27 90.06±0.33 90.11±0.56\nSTS-B (SC) 89.77±0.26 89.62±0.28 89.88±0.54\nMRPC 87.50±0.67 86.76±5.64 87.01±0.91\nMRPC (F1) 91.16±0.45 90.69±3.16 90.91±0.65\nRTE 71.12±2.52 68.59±1.52 73.65±0.90\nOverall 84.01 83.47 84.53\nTable 3: GLUE development set results of ﬁne-tuning\nBERT-Large models in Table 2. Default metric: ac-\ncuracy, MC: Matthews correlation, PC: Pearson corre-\nlation, SC: Spearman correlation. Overall: ﬁrst aver-\nage metrics within each task (if there are 1+) and then\nacross tasks. Numbers in smaller font are standard de-\nviations. All numbers are scaled by 100.\nSQuAD Public Post-LN Pre-LN RealFormer\nv1.1 (F1) 90.9 91.68 ±0.12 91.06±0.09 91.93±0.12\nv1.1 (EM) 84.1 85.15 ±0.13 83.98±0.24 85.58±0.15\nv2.0 (F1) 81.9 82.51 ±0.12 80.30±0.12 82.93±0.05\nv2.0 (EM) 78.7 79.57 ±0.12 77.35±0.16 79.95±0.08\nTable 4: SQuAD development set results of ﬁne-tuning\nBERT-Large models in Table 2. EM: exact match. Pub-\nlic: Post-LN results from Devlin et al. (2019). Numbers\nin smaller font are standard deviations. All numbers are\nscaled by 100.\na segment of text from the corresponding reading\npassage (Rajpurkar et al., 2016). SQuAD v2.0, a\nlater version, further extends with over 50K unan-\nswerable questions written adversarially by crowd-\nworkers to look similar to answerable ones.\nWe follow the ﬁne-tuning recipe in Devlin et al.\n(2019) for all three Transformer models on these\ntwo datasets without using any additional data such\nas TriviaQA (Joshi et al., 2017). For both v1.1 and\nv2.0, we select mini-batch size in{32, 48}, number\nof ﬁne-tuning epochs in {2, 3, 4 }, and learning\nrate in {2e-5, 3e-5, 4e-5, 5e-5 }. For each setup,\nwe run the experiment ﬁve times and report the\nbest median performance and the corresponding\nstandard deviation on the development set. As we\ncan see from Table 4, RealFormer outperforms the\ntwo baselines considerably, attesting its strength at\ntackling token-level tasks.\nTask Post-LN\n(500K)\nPost-LN\n(1M)\nRealFormer\n(500K)\nGLUE 83.84 84.01 84.34\nv1.1 (F1) 91.46±0.18 91.68±0.12 91.56±0.09\nv1.1 (EM) 84.87±0.24 85.15±0.13 85.06±0.12\nv2.0 (F1) 81.44±0.50 82.51±0.12 82.52±0.55\nv2.0 (EM) 78.64±0.48 79.57±0.12 79.54±0.54\nOverall 83.97 84.37 84.51\nTable 5: Downstream development set results of ﬁne-\ntuning BERT-Large with Post-LN and RealFormer pre-\ntrained with different number of steps. v*: SQuAD ver-\nsion, EM: exact match. Overall: First average across\nSQuAD and then GLUE. Numbers in smaller font are\nstandard deviations. All numbers are scaled by 100.\n4.1.3 Research Questions\nHow well does RealFormer perform with half\nthe pre-training budget? Although RealFormer\nhas outperformed both Post-LN and Pre-LN con-\nsiderably when pre-training 1M steps, we are also\ninterested in investigating its potential when the\npre-training budget is more limited. For this pur-\npose, we experiment with BERT-Large models. In\nparticular, we take the 500K step checkpoint of the\npre-trained RealFormer in Table 2 and ﬁne-tune\nit on GLUE and SQuAD datasets using exactly\nthe same procedure as described above. Compari-\nson results against the strongest baseline, Post-LN\nTransformer pre-trained 500K (checkpoint) and 1M\nsteps respectively, are collected in Table 5. We can\nsee that RealFormer with merely half the amount\nof pre-training epochs can beat Post-LN (1M) on\nGLUE with a signiﬁcant margin, and almost match\nits performance on SQuAD.\nDoes a larger learning rate help?As suggested\nby some recent work ( e.g., Xiong et al. (2020)),\nPre-LN Transformer may beneﬁt from using larger\nlearning rates. To this end, we follow the pre-\ntraining procedure detailed earlier and switch to a\nlarger learning rate, 2e-4, to pre-train BERT-Large\nwith the three Transformer models. Development\nset MLM accuracy with training steps can be found\nin Figure 3. We ﬁnd that both Pre-LN and Re-\nalFormer can reap some beneﬁts of using larger\nlearning rates with RealFormer seeming to bene-\nﬁt slightly more in this case (73.94% →74.31%)\ncompared to Pre-LN (73.21% →73.46%). Post-\nLN diverges with the learning rate of 2e-4. Note\nthat it also means that RealFormer can outperform\nPost-LN, the strongest baseline, actually with a\n935\nFigure 3: Development set MLM accuracy of BERT-\nLarge with different learning rates (best viewed in\ncolor). RealFormer seems to beneﬁt slightly more from\nusing a larger, non-default learning rate compared to\nPre-LN, while Post-LN diverges with 2e-4.\nprominent gap, 0.67% (i.e., 74.31% - 73.64%), for\npre-training, though with only minimal learning\nrate tuning.\nIs attention sparser in RealFormer? We con-\nduct one empirical study to observe the qualitative\ndifferences between RealFormer and Post-/Pre-LN\nTransformers. We randomly sample 8,192 exam-\nples from the held-out development set and visual-\nize the distribution of attention probabilities of each\ntoken in these examples across heads in all layers.\nIn particular, for each (token, layer, head) triplet,\nwe compute the entropy of the attention probabil-\nities as the “sparsity measure” of attention. Intu-\nitively, as entropy gets lower, the attention weight\ndistribution becomes more skewed and therefore\nattention is sparser.\nIn a similar fashion to Ramsauer et al. (2020), we\nuse violin plots to show the entropy distributions of\nthe pre-trained BERT-Base model with RealFormer\nfrom Table 2 (see Figure 4). Plots for the two\nbaseline Transformers in Table 2 are included in\nAppendix A.4. Each row is a layer in BERT-Base\nand each column is an attention head.\nWe ﬁnd that attention tends to get sparser for\nlater (upper) layers for all three Transformers.\nHowever, RealFormer differs from the two base-\nlines in the following ways:\n•RealFormer has signiﬁcantly sparser attention\nfor top layers (layer 9-11);\n•RealFormer tends to have lower variance\nacross all layers, which means that attention\ndensity is less input-dependent.\nWe hypothesize that the above two properties might\nbe a sign of stableness and beneﬁt ﬁne-tuning.\nDropout Post-LN Pre-LN RealFormer\n0%10 71.16% 69.80% 71.30%\n10% 73.64% 73.21% 73.94%\n20% 73.21% 72.97% 73.66%\nTable 6: Development set MLM accuracy of BERT-\nLarge with different dropout rates.\nDo attention heads in layerLresemble those in\nlayer L−1? Since RealFormer uses a residual\nattention scheme, it is interesting to show to what\nextent an attention head is “relying on” the cor-\nresponding head in the previous layer. To this\nend, we take each of the three pre-trained BERT-\nBase models in Table 2 and compute the Jensen-\nShannon Divergence (JSD) between attention prob-\nabilities in each pair of vertically adjacent heads,\ni.e., JSD (headL\ni ,headL−1\ni ), for 1 ≤L <12 and\n0 ≤i< 12.\nAppendix A.5 demonstrates detailed JSD distri-\nbutions of Post-LN and RealFormer respectively\nbased on 8,192 held-out examples. We observe\nthat RealFormer tends to have signiﬁcantly lower\nJSD values (i.e., indicating more “similar” attention\nacross layers), especially for heads in middle layers.\nThis might mean that RealFormer has some regular-\nization advantages and provides one hypothesis for\nwhy it tends to outperform Post-LN more for larger\nmodels. Note that headL\ni can still be useful even if\nit has exactly the same attention probabilities with\nheadL−1\ni because of the existence of the FFN sub-\nlayer and the potential differences in value matrices\n(i.e., V′in Eq. 1).\nIs residual attention really necessary? One\nmay wonder whether increasing dropout rate can al-\nready regularize large models well so that residual\nattention is redundant. To this end, we experiment\nwith different dropout rates for pre-training BERT-\nLarge with different Transformers (following the\nprocedures in Section 4.1.1). Results are collected\nin Table 6, from which we can see that (1) Real-\nFormer outperforms the two baselines across all\ndropout settings, and (2) simply increasing dropout\nrate can not regularize Transformer models as well\nas what residual attention appears to be doing.\n4.2 ADMIN\nTo evaluate the genericity of RealFormer, here we\ntry it on top of ADMIN (Liu et al., 2020), a state-of-\n10When dropout rate is 0%, we use early stopping for all\nmodels due to overﬁtting.\n936\nFigure 4: Distribution of entropies of the attention probabilities of the tokens of 8,192 held-out examples using the\npre-trained BERT-Base withRealFormer (see Section 4.1.1). For better legibility, (1) attention heads in each layer\nare ordered by their medians of entropies, and (2) distributions are color-coded based on the median of entropies:\nRED (median >4.5), YELLOW (1.5 ≤median ≤4.5), BLUE (median <1.5), i.e., colder colors mean sparser\nattention. There is a clear trend that higher layers tend to have sparser attention.\nthe-art NMT model without using either additional\ndata or data augmentation. ADMIN adopts Post-\nLN as the backbone, which we simply replace with\nRealFormer. In particular, we add three types of\nskip edges for encoder-encoder, encoder-decoder,\nand decoder-decoder attention respectively to the\nPost-LN Transformer. Empirically, RealFormer\nwith running mean of attention scores tends to out-\nperform running sum for our experiments, therefore\nhere we use the former exclusively for brevity.\nWe use two popular NMT benchmarks,\nWMT’14 En-De and WMT’14 En-Fr, and follow\nLiu et al. (2020) for all training setups on both\nbenchmarks except that in all cases (1) we select\nthe peak learning rate from {5e-4, 1e-3, 1.2e-3 }\nand use a linear learning rate decay schedule (in-\nstead of inverse sqrt);11 (2) we train RealFormer\nonly 50 epochs (in contrast, ADMIN trains 100\nepochs on En-De and 50 epochs on En-Fr); and (3)\nwe average across the last 25 checkpoints (while\nADMIN uses the last 10). More checkpoints are\nhelpful for us (especially for large models) presum-\n11With inverse sqrt decay, we ﬁnd that RealFormer tends\nto favor larger peak learning rates than what Liu et al. (2020)\nuses, and we have also seen improvements in most cases.\nably because the last few are not “diverse” enough\nas learning rate decays to 0.\nOur experiments are performed on NVIDIA\nA100 GPUs, based on the ofﬁcial ADMIN reposi-\ntory12. We follow Liu et al. (2020) to conﬁgure the\namount of GPUs to use for different setups.\nBLEU scores on test sets are collected in Table 7.\nFor fair comparisons, we also run ADMIN using\nour above setups and report results in the same\ntable. Following Liu et al. (2020), all networks\n(including both encoders and decoders) share the\nsame width setup (hidden size 512, intermediate\nsize 2048, 8 heads) and only vary in depth. Real-\nFormer outperforms all baselines across all depths\nconsiderably with a new state-of-the-art BLEU\nscore (43.97) on En-Fr for models not using addi-\ntional data or data augmentation to the best of our\nknowledge. One interesting observation here is that\nRealFormer does not always lead to larger improve-\nment gaps for larger models, which might be due\nto the checkpoint averaging mechanism (which po-\ntentially regularizes large models reasonably well).\n12https://github.com/LiyuanLucasLiu/\nTransformer-Clinic\n937\nModel En-De En-Fr\n6L-6L 12L-12L 18L-18L 6L-6L 60L-12L\nPost-LN 27.80 failed failed 41.29 failed\nPre-LN 27.27 28.26 28.38 40.74 43.10\nADMIN 27.90 28.58 29.03 41.47 43.80\nADMIN† 28.06 28.85 29.11 41.65 43.72\nOurs 28.17 29.06 29.35 41.92 43.97\nTable 7: Test set BLEU scores on two WMT’14 bench-\nmarks using different sizes of models. xL-yL: #En-\ncoder layers-#Decoder layers. First three rows are\nfrom Liu et al. (2020). Ours is switching the backbone\nof ADMIN from Post-LN to RealFormer. †Our run of\nADMIN using the same setups as RealFormer.\n4.3 ETC\nExtended Transformer Construction (ETC) is\na recent sparse attention mechanism proposed\nby Ainslie et al. (2020) and Zaheer et al. (2020)\nto handle long context. It has achieved state-of-\nthe-art results on four natural language bench-\nmarks requiring long and/or structured inputs. Here\nwe evaluate RealFormer on top of ETC models\non these benchmarks including WikiHop (Welbl\net al., 2018), HotpotQA (Yang et al., 2018), Nat-\nural Questions (Kwiatkowski et al., 2019), and\nOpenKP (Xiong et al., 2019). They vary signif-\nicantly in terms of dataset size, context length, and\nstructure in text inputs. Please refer to Ainslie et al.\n(2020) for more details.\nOur experiments are based on the ofﬁcial ETC\nrepository13. We take the ETC-Large model (24\nlayers, 1024 hidden size, 16 heads), add residual at-\ntention edges (i.e., using running sum), and follow\nall the pre-training and ﬁne-tuning recipes as well\nas hardware setups detailed in Ainslie et al. (2020).\nFor each ﬁne-tuning setup, we run the experiment\nﬁve times and report the best median performance\nand the corresponding standard deviation on the de-\nvelopment set in Table 8. RealFormer can improve\nETC consistently across all four benchmarks.\nAs of June 2021, we are ranked the ﬁrst on\nthe WikiHop leaderboard 14 with a test accuracy\nof 84.4% (2.1% absolute improvement over the\nprevious best result).\n13https://github.com/google-research/\ngoogle-research/tree/master/etcmodel\n14http://qangaroo.cs.ucl.ac.uk/\nleaderboard.html\nTask Metric ETC Ours\nWikiHop Accuracy 78.92±0.14 79.21±0.38\nHotpotQA\nAns. F1 80.38±0.13 80.86±0.16\nSup. F1 89.07±0.06 89.21±0.12\nJoint F1 73.12±0.19 73.57±0.19\nNatural\nQuestions\nLong Ans. F1 77.70±0.15 77.93±0.31\nShort Ans. F1 58.54±0.41 59.10±0.81\nAverage F1 68.07±0.17 68.51±0.56\nOpenKP F1@3 44.06±0.08 44.27±0.08\nTable 8: Development set results of ETC-Large. Ours\nis adding residual attention edges to ETC. Numbers in\nsmaller font are standard deviations. All numbers are\nscaled by 100.\n5 Conclusions\nWe propose RealFormer, a simple, generic, and\ncheap technique based on the novel idea of residual\nattention to improve Transformer-based networks.\nQuantitatively, we show that RealFormer can im-\nprove a diverse set of state-of-the-art Transformer-\nbased models considerably for tasks like Masked\nLanguage Modeling, Neural Machine Translation,\nand long document modeling. Qualitatively, we\nshow that RealFormer tends to have comparatively\nsparser attention, both within heads and across\nheads in adjacent layers.\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In 12th USENIX Symposium\non Operating Systems Design and Implementation ,\npages 265–283.\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 268–284.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\n938\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\nComputer Vision and Pattern Recognition , pages\n770–778.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016b. Identity mappings in deep residual net-\nworks. In European Cconference on Computer Vi-\nsion, pages 630–645.\nXiao Shi Huang, Felipe Perez, Jimmy Ba, and Mak-\nsims V olkovs. 2020. Improving transformer op-\ntimization through better initialization. In Inter-\nnational Conference on Machine Learning , pages\n4475–4483.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1601–1611.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Interna-\ntional Conference on Learning Representations.\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu\nChen, and Jiawei Han. 2020. Understanding the dif-\nﬁculty of training transformers. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5747–5763.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI Blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nHubert Ramsauer, Bernhard Sch ¨aﬂ, Johannes Lehner,\nPhilipp Seidl, Michael Widrich, Lukas Gruber,\nMarkus Holzleitner, Milena Pavlovi ´c, Geir Kjetil\nSandve, Victor Greiff, et al. 2020. Hop-\nﬁeld networks is all you need. arXiv preprint\narXiv:2008.02217.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. arXiv\npreprint arXiv:2003.05997.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-LM: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\nERNIE 2.0: A continual pre-training framework for\n939\nlanguage understanding. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence , pages 8968–\n8975.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019a.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F Wong, and Lidia S Chao.\n2019b. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:287–302.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Cam-\npos, and Arnold Overwijk. 2019. Open domain web\nkeyphrase extraction beyond language modeling. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, pages 5178–5187.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. 2020. On layer\nnormalization in the transformer architecture. In In-\nternational Conference on Machine Learning, pages\n10524–10533.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, pages 5753–5763.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big Bird: Transformers for longer se-\nquences. In Advances in Neural Information Pro-\ncessing Systems.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019. Im-\nproving deep transformer with depth-scaled initial-\nization and merged attention. In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint\nConference on Natural Language Processing, pages\n897–908.\nHongyi Zhang, Yann N Dauphin, and Tengyu Ma.\n2018. Fixup initialization: Residual learning with-\nout normalization. In International Conference on\nLearning Representations.\n940\nModel Post-LN Pre-LN RealFormer\nBERT-Small 5.4 hrs 5.3 hrs 5.9 hrs\nBERT-Base 20 hrs 20 hrs 23 hrs\nBERT-Large 58 hrs 58 hrs 66 hrs\nBERT-xLarge 136 hrs 137 hrs 137 hrs\nTable 9: Pre-training time of different BERT models\nin Table 2. We use 128 TPU v3 cores and mini-batch\nsize 512 for BERT-Small/Base/Large, and 256 TPU v3\ncores and mini-batch size 256 for BERT-xLarge.\nA Appendices\nA.1 Training Details: BERT\nAll our experiments are conducted on TPUs based\non https://github.com/google-research/\nbert, the ofﬁcial BERT repository in Tensor-\nFlow (Abadi et al., 2016).\nPre-training. We use 128 TPU v3 cores (i.e., 64\nchips) for BERT-Small/Base/Large and 256 TPU\nv3 cores ( i.e., 128 chips) for BERT-xLarge. Ta-\nble 9 demonstrates the time used to pre-train each\nmodel 1M steps. We can see that there is 10%-15%\nperformance drop when adding residual attention\nedges for all sizes except xLarge. Our suspicion\nis that additions are not as optimized as other ops\nlike matrix multiplications on TPU v3 cores. There\nis a much smaller performance drop for xLarge\nthough, which might indicate that addition scales\nnicely compared to other ops on TPU v3 cores. As\nwe will show later in Appendix A.2, performance\ndrop on GPUs is almost negligible across different\nTransformer sizes, suggesting that it is hardware-\ndependent.\nFine-tuning. We use 8 TPU v2 cores ( i.e., 4\nchips) to ﬁne-tune each model. Best hyper-\nparameter conﬁgurations for BERT-Large with Re-\nalFormer on GLUE and SQuAD are collected in\nTable 10. We include RealFormer pre-trained both\n1M and 500K steps, corresponding to the results in\nTable 3, 4, and 5.\nA.2 Training Details: ADMIN\nAll our NMT experiments are conducted on\nNVIDIA A100 GPUs based on https://github.\ncom/LiyuanLucasLiu/Transformer-Clinic, the\nofﬁcial ADMIN repository implemented via\nfairseq (Ott et al., 2019). We use the same scripts\nto collect and process data and evaluate all models.\nFollowing Liu et al. (2020), we use different\nnumber of GPUs for different setups, as detailed in\nTask 500K-step 1M-step\nBS LR EP BS LR EP\nMNLI 32 2e-5 2 32 1e-5 4\nQQP 32 3e-5 4 32 2e-5 4\nQNLI 32 3e-5 4 32 2e-5 2\nSST-2 32 3e-5 2 32 1e-5 4\nCoLA 32 2e-5 4 32 1e-5 3\nSTS-B 32 2e-5 3 32 2e-5 4\nMRPC 32 2e-5 4 32 1e-5 4\nRTE 32 1e-5 4 32 1e-5 4\nSQuAD v1.1 48 3e-5 2 48 3e-5 2\nSQuAD v2.0 32 5e-5 2 48 5e-5 2\nTable 10: Hyper-parameter conﬁgurations on GLUE\nand SQuAD for best-performing BERT-Large with Re-\nalFormer (pre-trained 500K steps and 1M steps respec-\ntively). BS: mini-batch size, LR: learning rate, EP:\n#ﬁne-tuning epochs.\nModel En-De En-Fr\n6L-6L 12L-12L 18L-18L 6L-6L 60L-12L\nADMIN 9.3 hrs 16 hrs 23 hrs 28 hrs 70 hrs\nOurs 9.4 hrs 16 hrs 23 hrs 28 hrs 72 hrs\n#GPUs 4 4 4 8 16\nTable 11: Number of GPUs and training time used\nfor each model. Ours is switching the backbone of\nADMIN from Post-LN to RealFormer (using running\nmean of attention scores).\nTable 11. For our runs of ADMIN and RealFormer\n(i.e., the last two rows in Table 7), learning rate\nis set to 5e-4 on WMT’14 En-De and 1.2e-3 on\nWMT’14 En-Fr across different model sizes. We\ntrain all models 50 epochs across the two bench-\nmarks and average across the last 25 checkpoints\n(corresponding to the last 25 epochs).\nTraining time comparison of ADMIN and our\nmodel using the same setups is shown in Table 11.\nAdding residual attention edges and using running\nmean of attention scores do not incur signiﬁcant\nperformance drop on GPUs across different model\nsizes.\nA.3 Training Details: ETC\nAll our experiments are conducted on TPU v3 cores\nbased on the ofﬁcial ETC repository in Tensor-\nFlow: https://github.com/google-research/\ngoogle-research/tree/master/etcmodel.\nPre-training. As is the case with ETC-Large\n(Ainslie et al., 2020), we ﬁnd that pre-training\nETC-Large with RealFormer can also beneﬁt sig-\n941\nTask Instance Statistics Hyper-parameter\n#Training Median Length 95%tile Length Max Length BS LR EP\nWikiHop 43,738 1,541 3,994 20,337 32 4e-5 15\nHotpotQA 90,447 1,227 1,810 3,560 32 4e-5 5\nNatural Questions 307,373 4,004 17,137 156,551 64 3e-5 2\nOpenKP 133,724 761 4,546 89,183 64 3e-5 2\nTable 12: Statistics of benchmarks and the hyper-parameter conﬁgurations for best-performing ETC-Large with\nRealFormer. BS: mini-batch size, LR: learning rate, EP: #ﬁne-tuning epochs.\nniﬁcantly from lifting weights from RoBERTa (Liu\net al., 2019). Note however that we lift from the\nsame RoBERTa checkpoint as our ETC-Large base-\nline, which could be disadvantageous to our model\nsince RoBERTa is pre-trainedwithout residual at-\ntention.\nFine-tuning. Statistics of the four benchmarks\nand the corresponding best hyper-parameter con-\nﬁgurations for ETC-Large with RealFormer are\ncollected in Table 12. On Natural Questions and\nOpenKP, we simply reuse the best conﬁgurations\nfor our ETC-Large baselines as reported in Ainslie\net al. (2020). On WikiHop and HotpotQA, we\nfollow the hyper-parameters search space speci-\nﬁed in Ainslie et al. (2020) for ETC-Large. 15 In\naddition, on WikiHop we found it to be slightly bet-\nter (development set accuracy 79.21 vs 78.96) to\nturn off RealFormer during ﬁne-tuning (i.e., adding\nno residual attention but still loading from our\npre-trained RealFormer checkpoint); therefore we\nadopted this setup for WikiHop in Table 8 and our\nleaderboard submission.\nA.4 Entropy Distribution of Pre-trained\nBaseline Transformer Models\nViolin plots demonstrating the entropy distributions\nof the pre-trained BERT-Base models with Post-\nLN and Pre-LN Transformers from Table 2 are\nincluded in Figure 5.\nA.5 Jensen-Shannon Divergence of Different\nPre-trained Transformers\nWe use violin plots to show the Jensen-Shannon\nDivergence distributions of the pre-trained BERT-\nBase models with Post-LN and RealFormer from\nTable 2 respectively (see Figure 6). Each row is a\npair of adjacent layers in BERT-Base and each col-\numn is an attention head. Instead of computing one\n15On WikiHop, number of ﬁne-tuning epochs is selected\nfrom {5, 10, 15} instead of {5, 10} for both ETC-Large and\nour model. We added 15 here following the ofﬁcial ETC\nrepository.\nscalar value for each head pair, we show the full\ndistribution based on the tokens in 8,192 held-out\nexamples, i.e., each data point is the JSD between\nthe attention probabilities of a token at these two\nheads. For better legibility, we color code these\nplots to help distinguish head pairs with relatively\n“similar” attention (BLUE: median<0.25) and rel-\natively “distinct” attention (RED: median>0.75)\nfrom the rest (YELLOW: 0.25 ≤median ≤0.75).\nNote that JSD results from Post-LN are used\nonly as a reference; we expect them to be “ran-\ndom” because there is no correspondence between\nheads in adjacent layers for Post-/Pre-LN. Proof:\nAn equivalent Post-/Pre-LN can be constructed by\npermuting the order of attention heads in a layer\n(and the corresponding variables).\n942\n(a) Post-LN\n(b) Pre-LN\nFigure 5: Distribution of entropies of the attention probabilities of the tokens of 8,192 held-out examples using\nthe pre-trained BERT-Base with Post-LN and Pre-LN Transformer respectively (see Section 4.1.1). For better\nlegibility, (1) attention heads in each layer are ordered by their medians of entropies, and (2) distributions are\ncolor-coded based on the median of entropies: RED (median > 4.5), YELLOW (1.5 ≤median ≤4.5), BLUE\n(median <1.5), i.e., colder colors mean sparser attention. Note that here top layers (layer 9-11) tend to have larger\nentropies compared to RealFormer, which means that attention is relatively denser.\n943\n(a) RealFormer\n(b) Post-LN\nFigure 6: Distribution of Jensen-Shannon Divergence (JSD) of attention probabilities in (vertically) adjacent atten-\ntion heads, i.e., JSD(headL\ni ,headL−1\ni ). Based on 8,192 held-out examples using the pre-trained BERT-Base with\nRealFormer and Post-LN Transformer respectively (see Section 4.1.1). Distributions are color-coded based on\nthe median of JSDs: RED (median > 0.75), YELLOW (0.25 ≤median ≤0.75), BLUE (median < 0.25). I.e.,\ncolder color means more “similar” attention heads across adjacent layers.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8264023661613464
    },
    {
      "name": "Residual",
      "score": 0.7544400691986084
    },
    {
      "name": "Computer science",
      "score": 0.6939977407455444
    },
    {
      "name": "Language model",
      "score": 0.5505212545394897
    },
    {
      "name": "Architecture",
      "score": 0.5463462471961975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4875435531139374
    },
    {
      "name": "Machine learning",
      "score": 0.33624207973480225
    },
    {
      "name": "Algorithm",
      "score": 0.27970725297927856
    },
    {
      "name": "Engineering",
      "score": 0.18714109063148499
    },
    {
      "name": "Electrical engineering",
      "score": 0.15383148193359375
    },
    {
      "name": "Voltage",
      "score": 0.09241461753845215
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}