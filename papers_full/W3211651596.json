{
  "title": "Considering Nested Tree Structure in Sentence Extractive Summarization with Pre-trained Transformer",
  "url": "https://openalex.org/W3211651596",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5028543952",
      "name": "Jingun Kwon",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100772473",
      "name": "Naoki Kobayashi",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016936747",
      "name": "Hidetaka Kamigaito",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5035876897",
      "name": "Manabu Okumura",
      "affiliations": [
        "Tokyo Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251803607",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2307381258",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2998696494",
    "https://openalex.org/W2774983917",
    "https://openalex.org/W2600702321",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2138909885",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035050380",
    "https://openalex.org/W4288419263",
    "https://openalex.org/W2768957049",
    "https://openalex.org/W2804268364",
    "https://openalex.org/W2962783338",
    "https://openalex.org/W2986758709",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2963676814",
    "https://openalex.org/W2896807716"
  ],
  "abstract": "Sentence extractive summarization shortens a document by selecting sentences for a summary while preserving its important contents. However, constructing a coherent and informative summary is difficult using a pre-trained BERT-based encoder since it is not explicitly trained for representing the information of sentences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4039‚Äì4044\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n4039\nConsidering Nested Tree Structure in Sentence Extractive Summarization\nwith Pre-trained Transformer\nJingun Kwon, Naoki Kobayashi, Hidetaka Kamigaito and Manabu Okumura\nTokyo Institute of Technology\n{kwonjingun,kobayasi,kamigaito}@lr.pi.titech.ac.jp\noku@pi.titech.ac.jp\nAbstract\nSentence extractive summarization shortens a\ndocument by selecting sentences for a sum-\nmary while preserving its important contents.\nHowever, constructing a coherent and informa-\ntive summary is difÔ¨Åcult using a pre-trained\nBERT-based encoder since it is not explicitly\ntrained for representing the information of sen-\ntences in a document. We propose a nested\ntree-based extractive summarization model on\nRoBERTa (NeRoBERTa), where nested tree\nstructures consist of syntactic and discourse\ntrees in a given document. Experimental re-\nsults on the CNN/DailyMail dataset showed\nthat NeRoBERTa outperforms baseline mod-\nels in ROUGE. Human evaluation results\nalso showed that NeRoBERTa achieves signiÔ¨Å-\ncantly better scores than the baselines in terms\nof coherence and yields comparable scores to\nthe state-of-the-art models.\n1 Introduction\nDocument summarization is a task of creating a\nconcise summary from a given document while\nkeeping the original content. In general, sentence\nextraction methods, which select sentences in a doc-\nument to create its summary, have the advantages\nof truthfulness compared with abstractive methods\n(Cao et al., 2018) and of Ô¨Çuency compared with\nword extraction methods (Xu et al., 2020).\nNeural networks have achieved great success in\nsentence extraction-based document summariza-\ntion (Cheng and Lapata, 2016; Zhou et al., 2018).\nRecently, Liu and Lapata (2019) proposed BERT-\nSUM, which utilizes BERT (Devlin et al., 2019)\nfor sentence representations to create a summary.\nAlthough the use of BERT resulted in signiÔ¨Åcant\nperformance improvement, this method decides\nthe selection for each sentence independently. Xu\net al. (2020) proposed DISCOBERT by consider-\ning inter-sentence information through discourse\ngraphs to construct a coherent summary. Although\nthey achieved remarkable scores in ROUGE, it was\n‚ãØ\nùë§!\"‚ãØùë§!!ùë§!#\n‚ãØ\nùë§#\"‚ãØùë§#!ùë§##\nùëüùëúùëúùë°#\n‚ãØ\nùë§$\"‚ãØùë§$!ùë§$#\nùëÜ#\nùëüùëúùëúùë°! ùëüùëúùëúùë°$‚ãØ\n‚ãØ\n0 1 0\n‚ãØ\nIntra \nùëÜ! ùëÜ$\n(DiscoBERT)\nInter\n(NeRoBERTa)\nNested\n(1: select, 0: delete)\nFigure 1: Different from the previous work, DIS-\nCOBERT (Xu et al., 2020), NeRoBERTa selects sen-\ntences by considering both intra- and inter-sentence re-\nlationships as a nested tree structure.\nstill difÔ¨Åcult to construct a coherent summary com-\npared to BERTSUM in human evaluation. Zhong\net al. (2020) attempted to change the paradigm\nby formulating summary-level extraction with a\nRoBERTa encoder and achieved the state-of-the-\nart results on the CNN/DailyMail dataset.\nIn spite of the successful results of the above\nBERT-related methods, their sentence representa-\ntions have room for improvement. As Liu et al.\n(2019) reported, ‚Äú[CLS]‚Äù, a pre-deÔ¨Åned token for\nindicating sentence representations on BERT, is\ninsufÔ¨Åcient to express sentence information. Even\nin RoBERTa, it is also a problem due to the lack\nof next sentence prediction in its pretraining step.\nTherefore, for further improving summarization\nperformance, we need to consider how to repre-\nsent sentences in a BERT-related model and how to\ncapture relationships between such sentence repre-\nsentations. It is a key to create a coherent and infor-\nmative summary with sentence extraction methods.\nTo tackle this problem, we propose a nested\ntree-based extractive summarization model on\nRoBERTa (NeRoBERTa). NeRoBERTa can extract\ncoherent sentences for a summary of a given doc-\nument by utilizing nested tree structures 1 of two\n1Kikuchi et al. (2014) considered the nested tree struc-\nture in the traditional non-neural tree-trimming method. Their\nmethod extracted words by tracking their parent words and\n4040\ndifferent trees, syntactic and discourse dependency\ntrees (Zhao and Huang, 2017). Figure 1 shows the\nproposed NeRoBERTa to select sentences from\na given document. Different from the previous\nworks that focused on inter-sentence information\nusing discourse graphs (Ishigaki et al., 2019; Xu\net al., 2020), NeRoBERTa considers both intra- and\ninter-sentence information (syntactic and discourse\ngraphs) together as a nested tree. The nested tree is\nencoded as a vector space representation through a\ngraph attention network (VeliÀáckovi¬¥c et al., 2018) on\na BERT-based encoder. In this tree, we can explic-\nitly represent sentence information at ‚Äúroot‚Äù words\nfor each syntactic dependency tree without relying\nonly on ‚Äú[CLS]‚Äù tokens.\nThis representation is useful to extract informa-\ntive and coherent sentences in that it can capture\nkeywords in a sentence for considering textual co-\nherence to other sentences. Furthermore, based\non the representation, we can also capture inter-\nactions between sentences through discourse de-\npendency trees, succeeding in extracting coherent\nsentences. It is also possible to consider even long-\ndistance relationships as higher-order dependency\nrelationships in this structure, such as relation-\nships between children and their ancestors. Thus,\nNeRoBERTa considers textual coherence through\nboth syntactic and discourse trees to capture long-\ndistance interactions between sentences.\nExperimental results on the CNN/DailyMail\ndataset showed that our NeRoBERTa outperforms\nRoBERTa-based strong baselines in ROUGE. Un-\nlike the previous work (Xu et al., 2020), NeR-\noBERTa successfully constructs a coherent sum-\nmary and is comparable to the state-of-the-art meth-\nods in human evaluation.\n2 Nested Tree Structure\nIn this section, we describe how we construct two\ndifferent types of graphs for a nested tree structure:\na discourse graph and a syntactic graph.\nWe obtain discourse dependency relationships\nbetween sentences in a document through an RST\nparser. A given document can be parsed into a tree\nformat with the RST parser, where each leaf node\nis an EDU, a text span in the document. Each text\nspan has two types, nucleus and satellite. While\nthe nucleus spans contain semantically salient in-\nformation, the satellite spans support and modify\nthe nucleus ones.\nsentences to construct a summary for a given document.\nWe use the recent state-of-the-art RST\nparser2 (Kobayashi et al., 2020) to build an\nRST discourse tree (RST-DT) for all documents\nand convert it to an Inter-Sentential RST-DT\n(ISRST-DT). The ISRST-DT is Ô¨Årst converted\ninto a dependency-based discourse tree (ISDEP-\nDT) using the method described in (Hirao\net al., 2013). Then, parent-child dependency\nrelationships for each sentence can be formed.\nWe construct a directed graph for the discourse\ndependencies (Ishigaki et al., 2019).\nA dependency parser is used to build up\nthe syntactic dependency relationships between\nwords (Manning et al., 2014). We construct an\nundirected graph for the syntactic dependencies\nby following the previous settings (Marcheggiani\nand Titov, 2017).\n3 Our Model\nIshigaki et al. (2019) consider dependency informa-\ntion through hierarchical attention modules (Kami-\ngaito et al., 2018) trained in supervised attention\nfor dependency heads (Kamigaito et al., 2017). Un-\nlike the previous work, our model uses constructed\ngraph information through graph encoder layers\nthat directly focus on the relationships between\nnodes deÔ¨Åned by edges in the graph. We explain\nthe details of our model in this section.\nLet wi be the i-th token in a document D =\n{w1,w2,...,w n}. Our model predicts p(1|D,k),\nthe probability of the k-th sentence in D being\nkept in a summary through the following modules.\n3.1 Pre-trained Document Encoder\nWe append ‚Äú[CLS]‚Äù and ‚Äú[SEP]‚Äù tokens between\nsentences to encode a whole document (Liu and\nLapata, 2019). Then, BERT is used to build up a\nrepresentation hi for each token wi as follows:\n{h1,h2,...,h n}= BERT({w1,w2,...,w n}).\nInstead of BERT, we consider RoBERTa as well.\nHowever, RoBERTa cannot be directly used in\nplace of BERT for sentence-level extraction be-\ncause RoBERTa does not consider the two types\nof tokens for the segment boundaries. To address\nthis issue, we use randomly initialized segment\nembeddings, Wtype ‚ààR2,768, instead of the orig-\ninal embeddings for keeping the same condition\nas BERT. The number comes from the pre-trained\n2We used the RST-parser using the RoBERTa embeddings\n4041\nsegment embedding weights of the original BERT,\nwhich indicate the next sentence prediction step.\nThen, the encoded hidden states, {h1,h2,...,h n},\nare fed into our graph encoders.\n3.2 Graph Encoders\nGraph Notation:Let Vd and Vs be nodes for sen-\ntences and words, and Es and Ed be edges be-\ntween the nodes in Vs and Vd, respectively. We\ndenote constructed discourse and syntactic graphs\nas Gd = (Vd,Ed) and Gs = (Vs,Es), respectively.\nWe append undirected edges between ‚Äú[CLS]‚Äù and\n‚Äúroot‚Äù tokens in each sentence to Es because the\nparent of a ‚Äúroot‚Äù token would be a sentence repre-\nsentation.\nGAT Networks: We use Graph Attention Net-\nworks (GAT) (VeliÀáckovi¬¥c et al., 2018) to encode\neach graph Gon hidden states of BERT as follows:\nfi = F2(hi),hi ‚ààRd,n, (1)\nni = N(drop(fi) +hi), (2)\nŒ±i,j = Softmaxj(L(F1[Wnnnl ‚à•Wnnnl])), (3)\nh‚Ä≤\ni = ‚à•K\nk=1T(\n‚àë\nj‚ààNi\nŒ±k\ni,jWk\na hj),h‚Ä≤\ni ‚ààRK√ód,n, (4)\nh‚Ä≤‚Ä≤\ni = ReLU(M(h‚Ä≤\ni)),h‚Ä≤‚Ä≤\ni ‚ààRd,n, (5)\nhG\ni = N(drop(h‚Ä≤‚Ä≤\ni ) +ni), (6)\nwhere Fi indicates i-th times stacked feed-forward\nnetworks. N is layer normalization.Wn and Wa are\nlearnable weights. L and T denote a non-linearity\nactivation function, LeakyReLU, and a hyperbolic\ntangent, respectively. Œ±i,j indicates normalized at-\ntention coefÔ¨Åcients through a softmax function.\n‚à•indicates concatenation, and ni represents con-\nnected nodes to node i in graph G. ReLU is an\nactivation function. M is a learnable weight. Af-\nter hi is fed into the graph encoder, we obtain hG\ni ,\nwhich contains either syntactic or discourse graph\ninformation based on all tokens.\nThe syntactic and discourse graphs are indepen-\ndently encoded. Then, they are concatenated as\nhroot\nk =ReLU(W(hGs\nr(k) ‚à•hGd\nr(k))), where r(k) indi-\ncates the position of a root in the k-th sentence. For\nthe Ô¨Ånal representations to predict labels, we use\nhroot\nk to represent the k-th sentence.\n3.3 Objective Function & Inference\nWe deÔ¨Åne p(1|D,k) =œÉ(WM(hroot\nk ) +b), where\nM is a two-stacked multi-head attention, œÉ is a\nsigmoid function, and W and bare weight parame-\nters (Liu and Lapata, 2019). Let yi ‚àà{1,0}be an\noracle label and Y = {y1,y2,...,y n}be its set for\na document. We use ‚àí‚àë\nyk‚ààY log(yk|x,k) as our\nobjective function. In the inference step, we score\nthe k-th sentence with p(1|D,k) and sort the sen-\ntences in descending order. Then, we keep the top\nmsentences as a summary, where mis the number\nof sentences to be extracted.\n4 Experiments\n4.1 Experimental Settings\nDataset: We used the non-anonymized\nCNN/DailyMail dataset (Hermann et al., 2015).\nBased on the standard split, we divided the dataset\ninto 287,226, 13,368 and 11,490 articles for\ntraining, validation, and test datasets, respectively.\nParameter Settings: We used PyTorch with the\nTorch Geometric (Fey and Lenssen, 2019) to build\nup entire architectures with graph encoders. The\n‚Äúbert-based-uncased‚Äù and ‚Äúroberta-based‚Äù models\nin transformers3 were used to encode maximum\n768 tokens of each tokenized document. The best\nmodel was selected based on the lowest ‚Äúloss‚Äù\nscore on the validation dataset. A greedy search\nwas used to construct the oracle summary by maxi-\nmizing the sum of ROUGE-1-F and ROUGE-2-F\nagainst the gold summary.\nFor the syntactic graph encoder, we stacked GAT\nNetworks. To track n-order dependency informa-\ntion, we simply added n-order nodes and edges to\nGd and Gs. The number of attention heads was\nset to 6 in each graph encoder. To represent each\nword vector, we used a Ô¨Årst sub-word vector. We\nemployed a traditional method of selecting top 3\nsentences to construct a summary (Liu and Lap-\nata, 2019). Trigram blocking was used to reduce\nredundancy and to improve informativeness for all\nmodels (Paulus et al., 2018).\nCompared Methods: We compared our proposed\nmethods with some baselines. The proposed meth-\nods are as follows:\nNeRoBERTa considers our nested tree structure\nfor both syntactic and discourse information.\nSynRoBERTa and DiRoBERTa independently\nconsider only either syntactic or discourse tree\nstructure, respectively.\nThe baselines, which include state-of-the-art mod-\nels, are as follows:\nBERTSUM introduces a method for learning a sen-\ntence boundary in a BERT-based model for the doc-\nument summarization task (Liu and Lapata, 2019).\n3https://github.com/huggingface/transformers\n4042\nDISCOBERT constructs a summary based on\nEDU-level extraction, incorporating discourse and\ncoreference information (Xu et al., 2020).\nMatchSum attempts to shift the paradigm\nfrom sentence-level to summary-level extraction\nduring the extractive document summarization\ntask (Zhong et al., 2020).\nRoBERTa encodes input documents using a\n‚Äúroberta-based‚Äù model.\n4.2 Automatic Evaluation\nWe utilized ROUGE-metrics for the evaluation. The\nexperimental results on the CNN/DailyMail dataset\nare shown in Table 1. The Ô¨Årst block contains Lead-\n3 and Oracle scores. The second block includes\nBERT-based previous studies including state-of-\nthe-art models. The last block includes scores for\nour models and for re-implemented BERTSUM.\nOur strong baseline RoBERTa outperformed\nBERTSUM. The gain might be from using a bigger\ndataset with the dynamic masking pattern applied\nin the pre-trained RoBERTa. SynRoBERTa and\nDiRoBERTa show that considering syntactic or dis-\ncourse information was beneÔ¨Åcial. NeRoBERTa\n(ns = {1,2},nd = {1}) (in bold), that considers\nsyntactic and discourse information simultaneously,\nfurther improved the performance. It outperformed\nRoBERTa with a clear margin, speciÔ¨Åcally, 0.31\npoints in the R-1-F score.\nAs can be seen in Figure 2, RoBERTa can im-\nprove the prediction loss compared with BERT-\nSUM. SynRoBERTa(ns = {1,2}), which explic-\nitly incorporates keywords information through\nsyntactic information, can further improve the per-\nformance of RoBERTa. This shows that consider-\ning keywords information through syntactic struc-\ntures is beneÔ¨Åcial to construct the sentence rep-\nresentations for considering textual coherence to\nother sentences.\n4.3 Human Evaluation and Analysis\nHuman evaluation was conducted for randomly\nsampled 100 documents from the test dataset.\n‚ÄúAmazon Mturk‚Äù was used for the experiments,\nand human evaluators graded scores from 1 to 5\n(5 is the best) in terms of four evaluation crite-\nria.5 Because summaries from DISCOBERT were\nworse than ones from BERTSUM in their human\n4The paired-bootstrap-resampling (Koehn, 2004) was used\n(p < 0.05).\n540 human evaluators who obtained both US high school\nand US bachelor degrees participated in the experiments.\nModel R-1-F R-2-F R-L-F\nLead3 40.12 17.52 36.44\nOracle 55.05 32.72 51.38\nBERTSUM 43.25 20.24 39.63\nDISCOBERT 43.77 20.85 40.67\nMatchSum 44.41 20.86 40.55\nBERTSUM 43.28 20.11 39.68\nRoBERTa 43.55 20.40 39.94\nSynRoBERTa(ns={1}) 43.73 20.58 40.10\nSynRoBERTa(ns={1,2}) 43.63 20.51 40.02\nDiRoBERTa(nd={1}) 43.64 20.45 40.02\nNeRoBERTa(ns={1},nd={1}) 43.74 20.53 40.13\nNeRoBERTa(ns={1,2},nd={1}) 43.86‚Ä† 20.64‚Ä† 40.20‚Ä†\nTable 1: Experimental results on the CNN/DailyMail\ndataset. ns and nd indicate the order of dependency\nrelationships considered for syntactic and discourse\ngraphs, respectively.‚Ä† indicates the improvement is sig-\nniÔ¨Åcant with a 0.95 conÔ¨Ådence interval estimated with\nthe ROUGE script compared to RoBERTa.\nFigure 2: Validation losses for BERTSUM, RoBERTa,\nand SynRoBERTa (ns = {1,2}) . ‚Äú[CLS]‚Äù and\n‚Äú[ROOT]‚Äù indicate the tokens of sentence representa-\ntions for predicting labels.\nevaluation (Xu et al., 2020), we evaluated only\nsummaries from RoBERTa, NeRoBERTa (ns =\n{1,2},nd = {1}), and MatchSum. Table 2 shows\nthe results. Coh, Infor, Read, and Redun indicate\ncoherence, informativeness, readability, and redun-\ndancy, respectively. As we expected, the proposed\nNeRoBERTa, which considers a nested tree struc-\nture, could capture coherence better than our strong\nbaseline, RoBERTa. In addition, NeRoBERTa was\ncomparable to the current state-of-the-art model,\nMatchSum. The informativeness score for Match-\nSum was lower than RoBERTa and NeRoBERTa.\nTable 3 shows example extracted sentences from\na document and their discourse graph. In this ex-\nample, the discourse information alone was not\nenough in that S3 and S10 have the same dis-\ncourse information, while S3 is more similar to\nthe third sentence in the gold summary. RoBERTa\nand DiRoBERTa constructed the same summary in-\n4043\nModel Coh Infor Read Red\nMatchSum 4.06 4.11 4.09 4.17\nRoBERTa 4.02 4.14 4.09 4.12\nNeRoBERTa 4.08‚Ä† 4.14 4.10 4.16\nTable 2: Human evaluation results. ‚Ä†indicates that the\nimprovement with NeRoBERTa from RoBERTa was\nstatistically signiÔ¨Åcant.4\nS1Barcelona club president josep maria bartomeu has insisted that the la ligaleaders have no plans to replace luis enrique and they‚Äôre ‚Äôvery happy‚Äô with him.S3Despite speculation this season that enrique will be replaced in the summer,bartomeu refuted these claims and says he‚Äôs impressed with how the managerhas performed.S4Luis enrique only took charge at the club last summer and has impressedduring his tenure.S5Barcelona president josep maria bartemou says the club are ‚Äôvery happy‚Äôwith enrique‚Äôs performance.S10Enrique‚Äôs side comfortably dispatched of champions league chasingvalencia on saturday, with goals from luis suarez and lionel messi.S11luis suarez opened the scoring for barcelona [...] Ô¨Çying ValenciaGoldBarcelona president josep bartomeu says the club are happy with enrique.barca are currently top of la liga and closing in on the league title.enrique‚Äôs future at the club has been speculated over the season.click here for all the latest barcelona news.\nTable 3: Example extracted sentences from RoBERTa,\nDiRoBERTa (nd = {1}), NeRoBERTa (ns =\n{1,2},nd = {1}), and MatchSum models. Arrows in-\ndicate the discourse graphs. The sentences in red are\nselected by all models. The sentence in blue is selected\nby NeRoBERTa and the sentence in purple is selected\nby RoBERTa and DiRoBERTa. S1 is the Ô¨Årst sentence\nof the document. Gold denotes the gold summary.\ncluding S10. On the other hand, NeRoBERTa could\nextract S3, which is coherent to S4 and S5, shar-\ning important keywords ‚Äúenrique‚Äù and ‚Äúbartomeu‚Äù.\nThis is because our GAT network for syntactic in-\nformation can capture keywords in the sentence to\nconsider textual coherence to other sentences. Al-\nthough NeRoBERTa constructed a summary with\nthree sentences, MatchSum extracted only two sen-\ntences of S4 and S5. In this case, MatchSum might\nbe less informative than NeRoBERTa.\n5 Conclusion\nIn this paper, we proposed NeRoBERTa, which in-\ncorporates syntactic and discourse information as a\nnested tree structure to create an informative and\ncoherent summary. The experimental results on the\nCNN/DailyMail dataset showed that our method\nimproves the performance over the baseline meth-\nods both in the automatic and human evaluations.\nAcknowledgements\nWe would like to gratefully acknowledge the anony-\nmous reviewers for their helpful comments and\nfeedbacks. This work was supported by Google AI\nFocused Research Award.\nReferences\nZiqiang Cao, Furu Wei, W. Li, and Sujian Li. 2018.\nFaithful to the original: Fact aware neural abstractive\nsummarization. ArXiv, abs/1711.04434.\nJianpeng Cheng and Mirella Lapata. 2016. Neural sum-\nmarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 484‚Äì494. Association for Com-\nputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186. Association for Computational Linguis-\ntics.\nMatthias Fey and Jan E. Lenssen. 2019. Fast graph\nrepresentation learning with PyTorch Geometric. In\nICLR Workshop on Representation Learning on\nGraphs and Manifolds.\nKarl Moritz Hermann, Tom√°≈° KoÀácisk√Ω, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Proceedings of the 28th Inter-\nnational Conference on Neural Information Process-\ning Systems - Volume 1, NIPS‚Äô15, page 1693‚Äì1701.\nMIT Press.\nTsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,\nNorihito Yasuda, and Masaaki Nagata. 2013. Single-\ndocument summarization as a tree knapsack prob-\nlem. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1515‚Äì1520. Association for Computational\nLinguistics.\nTatsuya Ishigaki, Hidetaka Kamigaito, Hiroya Taka-\nmura, and Manabu Okumura. 2019. Discourse-\naware hierarchical attention network for extractive\nsingle-document summarization. In Proceedings of\nthe International Conference on Recent Advances in\nNatural Language Processing (RANLP 2019), pages\n497‚Äì506. INCOMA Ltd.\nHidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu Hi-\nrao, and Masaaki Nagata. 2018. Higher-order syn-\ntactic attention network for longer sentence compres-\nsion. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers). Association for Com-\nputational Linguistics.\n4044\nHidetaka Kamigaito, Katsuhiko Hayashi, Tsutomu\nHirao, Hiroya Takamura, Manabu Okumura, and\nMasaaki Nagata. 2017. Supervised attention for\nsequence-to-sequence constituency parsing. In Pro-\nceedings of the Eighth International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 7‚Äì12, Taipei, Taiwan. Asian\nFederation of Natural Language Processing.\nYuta Kikuchi, Tsutomu Hirao, Hiroya Takamura, Man-\nabu Okumura, and Masaaki Nagata. 2014. Single\ndocument summarization based on nested tree struc-\nture. In Proceedings of the 52nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 315‚Äì320. Association\nfor Computational Linguistics.\nNaoki Kobayashi, Tsutomu Hirao, Hidetaka Kami-\ngaito, Manabu Okumura, and Masaaki Nagata. 2020.\nTop-down rst parsing utilizing granularity levels in\ndocuments. Proceedings of the AAAI Conference on\nArtiÔ¨Åcial Intelligence, 34(05):8099‚Äì8106.\nPhilipp Koehn. 2004. Statistical signiÔ¨Åcance tests for\nmachine translation evaluation. In Proceedings of\nthe 2004 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730‚Äì3740. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nChristopher Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven Bethard, and David McClosky.\n2014. The Stanford CoreNLP natural language pro-\ncessing toolkit. In Proceedings of 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations , pages 55‚Äì60. As-\nsociation for Computational Linguistics.\nDiego Marcheggiani and Ivan Titov. 2017. Encoding\nsentences with graph convolutional networks for se-\nmantic role labeling. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 1506‚Äì1515. Association\nfor Computational Linguistics.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In International Conference on Learn-\ning Representations.\nPetar VeliÀáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li√≤, and Yoshua Bengio.\n2018. Graph Attention Networks. International\nConference on Learning Representations.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.\n2020. Discourse-aware neural extractive text sum-\nmarization. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5021‚Äì5031. Association for Computational\nLinguistics.\nKai Zhao and Liang Huang. 2017. Joint syntacto-\ndiscourse parsing and the syntacto-discourse tree-\nbank. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2117‚Äì2123. Association for Computational\nLinguistics.\nMing Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,\nXipeng Qiu, and Xuanjing Huang. 2020. Extractive\nsummarization as text matching. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics , pages 6197‚Äì6208. Associa-\ntion for Computational Linguistics.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,\nMing Zhou, and Tiejun Zhao. 2018. Neural docu-\nment summarization by jointly learning to score and\nselect sentences. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 654‚Äì663.\nAssociation for Computational Linguistics.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9094696044921875
    },
    {
      "name": "Computer science",
      "score": 0.7808359861373901
    },
    {
      "name": "Natural language processing",
      "score": 0.676017165184021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6676620841026306
    },
    {
      "name": "Sentence",
      "score": 0.6610308885574341
    },
    {
      "name": "Transformer",
      "score": 0.6029301881790161
    },
    {
      "name": "Tree (set theory)",
      "score": 0.5372815728187561
    },
    {
      "name": "Tree structure",
      "score": 0.5286785960197449
    },
    {
      "name": "Encoder",
      "score": 0.5064717531204224
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.45000317692756653
    },
    {
      "name": "Algorithm",
      "score": 0.12240934371948242
    },
    {
      "name": "Mathematics",
      "score": 0.11839085817337036
    },
    {
      "name": "Statistics",
      "score": 0.1051827073097229
    },
    {
      "name": "Binary tree",
      "score": 0.07967564463615417
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}