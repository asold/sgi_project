{
  "title": "Co-Scale Conv-Attentional Image Transformers",
  "url": "https://openalex.org/W3152694111",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2116275890",
      "name": "Weijian Xu",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2096640705",
      "name": "Yi-Fan Xu",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2320927890",
      "name": "Tyler Chang",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2134014566",
      "name": "Zhuowen Tu",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2997154779",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6767071366",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2133155955",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W6764322716",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6792695861",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763676353",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6788467338",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W6763367864",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W6771571646",
    "https://openalex.org/W3104953317",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W2944223741",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2969825080",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2949530802",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3202406646"
  ],
  "abstract": "In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",
  "full_text": "Co-Scale Conv-Attentional Image Transformers\nWeijian Xu* Yifan Xu* Tyler Chang Zhuowen Tu\nUniversity of California San Diego\n{wex041, yix081, tachang, ztu}@ucsd.edu\nAbstract\nIn this paper, we present Co-scale conv-attentional image\nTransformers (CoaT), a Transformer-based image classifier\nequipped with co-scale and conv-attentional mechanisms.\nFirst, the co-scale mechanism maintains the integrity of\nTransformers’ encoder branches at individual scales, while\nallowing representations learned at different scales to ef-\nfectively communicate with each other; we design a series\nof serial and parallel blocks to realize the co-scale mecha-\nnism. Second, we devise a conv-attentional mechanism by\nrealizing a relative position embedding formulation in the\nfactorized attention module with an efficient convolution-like\nimplementation. CoaT empowers image Transformers with\nenriched multi-scale and contextual modeling capabilities.\nOn ImageNet, relatively small CoaT models attain superior\nclassification results compared with similar-sized convolu-\ntional neural networks and image/vision Transformers. The\neffectiveness of CoaT’s backbone is also illustrated on ob-\nject detection and instance segmentation, demonstrating its\napplicability to downstream computer vision tasks.\n1. Introduction\nA notable recent development in artificial intelligence is\nthe creation of attention mechanisms [ 38] and Transform-\ners [31], which have made a profound impact in a range of\nfields including natural language processing [7, 20], docu-\nment analysis [ 39], speech recognition [ 8], and computer\nvision [9, 3]. In the past, state-of-the-art image classifiers\nhave been built primarily on convolutional neural networks\n(CNNs) [15, 14, 27, 26, 11, 36] that operate on layers of\nfiltering processes. Recent developments [ 30, 9] however\nbegin to show encouraging results for Transformer-based\nimage classifiers.\nIn essence, both the convolution [15] and attention [38]\noperations address the fundamental representation problem\nfor structured data (e.g. images and text) by modeling the\nlocal contents, as well as the contexts. The receptive fields\n* indicates equal contribution.\nCode at https://github.com/mlpc-ucsd/CoaT.\n0 10 20 30 40 50 60 70 80 90\nNumber of Parameters (Millions)\n70\n72\n74\n76\n78\n80\n82\n84ImageNet Top-1 Accuracy (%)\nCoaT-Li(T)\nCoaT-Li(Mi)\nCoaT-Li(S)\nCoaT-Li(M)\nCoaT(T)\nCoaT(Mi)\nCoaT(S)\nDeiT-T\nDeiT-S\nDeiT-B\nT2T-ViT-7\nT2T-ViT-12\nT2T-ViT-14\nT2T-ViT-19\nPVT-T\nPVT-S\nPVT-M\nPVT-L\nR18\nR50\nR101\nViT-B\nCoaT-Lite (ours)\nCoaT (ours)\nDeiT (baseline)\nT2T\nPVT\nResNet\nViT\nFigure 1. Model Size vs. ImageNet Accuracy. Our CoaT model\nsignificantly outperforms other image Transformers. Details are in\nTable 2.\nin CNNs are gradually expanded through a series of con-\nvolution operations. The attention mechanism [ 38, 31] is,\nhowever, different from the convolution operations: (1) the\nreceptive field at each location or token in self-attention\n[31] readily covers the entire input space since each token\nis “matched” with all tokens including itself; (2) the self-\nattention operation for each pair of tokens computes a dot\nproduct between the “query” (the token in consideration)\nand the “key” (the token being matched with) to weight the\n“value” (of the token being matched with).\nMoreover, although the convolution and the self-attention\noperations both perform a weighted sum, their weights are\ncomputed differently: in CNNs, the weights are learned dur-\ning training but fixed during testing; in the self-attention\nmechanism, the weights are dynamically computed based\non the similarity or affinity between every pair of tokens.\nAs a consequence, the self-similarity operation in the self-\nattention mechanism provides modeling means that are po-\ntentially more adaptive and general than convolution oper-\nations. In addition, the introduction of position encodings\nand embeddings [31] provides Transformers with additional\nflexibility to model spatial configurations beyond fixed input\nstructures.\nOf course, the advantages of the attention mechanism are\nnot given for free, since the self-attention operation com-\nputes an affinity/similarity that is more computationally de-\nmanding than linear filtering in convolution. The early de-\nvelopment of Transformers has mainly focused on natural\nlanguage processing tasks [31, 7, 20] since text is “shorter”\nthan an image, and text is easier to tokenize. In computer\nvision, self-attention has been adopted to provide added mod-\neling capability for various applications [34, 37, 44]. With\nthe underlying framework increasingly developed [ 9, 30],\nTransformers start to bear fruit in computer vision [3, 9] by\ndemonstrating their enriched modeling capabilities.\nIn the seminal DEtection TRansformer (DETR) [3] algo-\nrithm, Transformers are adopted to perform object detection\nand panoptic segmentation, but DETR still uses CNN back-\nbones to extract the basic image features. Efforts have re-\ncently been made to build image classifiers from scratch, all\nbased on Transformers [9, 30, 33]. While Transformer-based\nimage classifiers have reported encouraging results, perfor-\nmance and design gaps to the well-developed CNN models\nstill exist. For example, in [9, 30], an input image is divided\ninto a single grid of fixed patch size. In this paper, we de-\nvelop Co-scale conv-attentional image Transformers (CoaT)\nby introducing two mechanisms of practical significance to\nTransformer-based image classifiers. The contributions of\nour work are summarized as follows:\n• We introduce a co-scale mechanism to image Trans-\nformers by maintaining encoder branches at separate\nscales while engaging attention across scales. Two\ntypes of building blocks are developed, namely a serial\nand a parallel block, realizing fine-to-coarse, coarse-\nto-fine, and cross-scale image modeling.\n• We design a conv-attention module to realizerelative\nposition embeddings with convolutions in the factor-\nized attention module that achieves significantly en-\nhanced computation efficiency when compared with\nvanilla self-attention layers in Transformers.\nOur resulting Co-scale conv-attentional image Transformers\n(CoaT) learn effective representations under a modularized\narchitecture. On the ImageNet benchmark, CoaT achieves\nstate-of-the-art classification results when compared with the\ncompetitive convolutional neural networks (e.g. Efficient-\nNet [29]), while outperforming the competing Transformer-\nbased image classifiers [9, 30, 33], as shown in Figure 1.\n2. Related Works\nOur work is inspired by the recent efforts [9, 30] to realize\nTransformer-based image classifiers. ViT [9] demonstrates\nthe feasibility of building Transformer-based image classi-\nfiers from scratch, but its performance on ImageNet [ 23]\nis not achieved without including additional training data;\nDeiT [30] attains results comparable to convolution-based\nclassifiers by using an effective training strategy together\nwith model distillation, removing the data requirement in [9].\nBoth ViT [9] and DeiT [30] are however based on a single\nimage grid of fixed patch size.\nThe development of our co-scale conv-attentional trans-\nformers (CoaT) is motivated by two observations: (1) multi-\nscale modeling typically brings enhanced capability to rep-\nresentation learning [11, 22, 32]; (2) the intrinsic connection\nbetween relative position encoding and convolution makes it\npossible to carry out efficient self-attention using conv-like\noperations. As a consequence, the superior performance of\nthe CoaT models shown in the experiments comes from two\nof our new designs in Transformers: (1) a co-scale mecha-\nnism that allows cross-scale interaction; (2) a conv-attention\nmodule to realize an efficient self-attention operation. Next,\nwe highlight the differences of the two proposed modules\nwith the standard operations and concepts.\n• Co-Scale vs. Multi-Scale . Multi-scale approaches\nhave a long history in computer vision [35, 19]. Con-\nvolutional neural networks [ 15, 14, 11] naturally im-\nplement a fine-to-coarse strategy. U-Net [22] enforces\nan extra coarse-to-fine route in addition to the standard\nfine-to-coarse path; HRNet [32] provides a further en-\nhanced modeling capability by keeping simultaneous\nfine and coarse scales throughout the convolution lay-\ners. In a parallel development [ 33] to ours, layers of\ndifferent scales are in tandem for the image Transform-\ners but [33] merely carries out a fine-to-coarse strategy.\nThe co-scale mechanism proposed here differs from the\nexisting methods in how the responses are computed\nand interact with each other: CoaT consists of a series\nof highly modularized serial and parallel blocks to en-\nable attention with fine-to-coarse, coarse-to-fine, and\ncross-scale information on tokenized representations.\nThe joint attention mechanism across different scales\nin our co-scale module provides enhanced modeling\npower beyond existing vision Transformers [9, 30, 33].\n• Conv-Attention vs. Attention. Pure attention-based\nmodels [21, 13, 44, 9, 30] have been introduced to the\nvision domain. [ 21, 13, 44] replace convolutions in\nResNet-like architectures with self-attention modules\nfor better local and non-local relation modeling. In\ncontrast, [9, 30] directly adapt the Transformer [31] for\nimage recognition. Recently, there have been works\n[1, 6] enhancing the attention mechanism by introduc-\ning convolution. LambdaNets [1] introduce an efficient\nself-attention alternative for global context modeling\nand employ convolutions to realize the relative posi-\ntion embeddings in local context modeling. CPVT [6]\ndesigns 2-D depthwise convolutions as the conditional\npositional encoding after self-attention. In our conv-\nattention, we: (1) adopt an efficient factorized atten-\ntion following [1]; (2) extend it to be a combination of\ndepthwise convolutional relative position encoding and\nconvolutional position encoding, related to CPVT [6].\nDetailed discussion of our network design and its rela-\ntion with LambdaNets [1] and CPVT [6] can be found\nin Section 4.1 and 4.2.\n3. Revisit Scaled Dot-Product Attention\nTransformers take as input a sequence of vector represen-\ntations (i.e. tokens) x1, ...,xN , or equivalently X ∈ RN×C.\nThe self-attention mechanism in Transformers projects each\nxi into corresponding query, key, and value vectors, using\nlearned linear transformationsWQ, WK, and WV ∈ RC×C.\nThus, the projection of the whole sequence generates rep-\nresentations Q, K, V∈ RN×C. The scaled dot-product\nattention from original Transformers [31] is formulated as :\nAtt(X) =softmax\n\u0010QK⊤\n√\nC\n\u0011\nV (1)\nIn vision Transformers [ 9, 30], the input sequence of\nvectors is formulated by the concatenation of a class token\nCLS and the flattened feature vectors x1, ...,xHW as image\ntokens from the feature maps F ∈ RH×W×C, for a total\nlength of N = HW + 1. The softmax logits in Equation 1\nbecome not affordable for high-resolution images (i.e. N ≫\nC) due to its O(N2) space complexity and O(N2C) time\ncomplexity. To reduce the length of the sequence, ViT [9, 30]\ntokenizes the image by patches instead of pixels. However,\nthe coarse splitting (e.g. 16×16 patches) limits the ability to\nmodel details within each patch. To address this dilemma, we\npropose a co-scale mechanism that provides enhanced multi-\nscale image representation with the help of an efficient conv-\nattentional module that lowers the computation complexity\nfor high-resolution images.\n4. Conv-Attention Module\n4.1. Factorized Attention Mechanism\nIn Equation 1, the materialization of the softmax logits\nand attention maps leads to the O(N2) space complexity\nand O(N2C) time complexity. Inspired by recent works [5,\n25, 1] on linearization of self-attention, we approximate the\nsoftmax attention map by factorizing it using two functions\nϕ(·), ψ(·) : RN×C → RN×C′\nand compute the second\nmatrix multiplication (keys and values) together:\nFactorAtt(X) =ϕ(Q)\n\u0010\nψ(K)⊤V\n\u0011\n(2)\nThe factorization leads to a O(NC ′ + NC + CC′) space\ncomplexity (including output of ϕ(·), ψ(·) and intermediate\nsteps in the matrix product) and O(NCC ′) time complex-\nity, where both are linear functions of the sequence length\nN. Performer [5] uses random projections in ϕ and ψ for a\nDepthwiseConv*\nDepthwiseConv*\nQK VSoftmax\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"6jD5Hw2TX91vjBX4cGtTP5U08xg=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPdZJD0XEDbnrlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHi9aQIQ==</latexit>\nC ⇥ N\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"ptFsCPtc2X6bc7IQTNlxKtjTw1c=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqsdCLx4r2A9sQ9lsN+3SzSbsToQS+i+8eFDEq//Gm//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMGnO/88S1EbF6wGnC/YiOlAgFo2ilxwbpo4i4IY1BueJW3QXIOvFyUoEczUH5qz+MWRpxhUxSY3qem6CfUY2CST4r9VPDE8omdMR7lipq1/jZ4uIZubDKkISxtqWQLNTfExmNjJlGge2MKI7NqjcX//N6KYa3fiZUkiJXbLkoTCXBmMzfJ0OhOUM5tYQyLeythI2ppgxtSCUbgrf68jppX1W9WtW7v67U3TyOIpzBOVyCBzdQhztoQgsYKHiGV3hzjPPivDsfy9aCk8+cwh84nz97KpAW</latexit>\nC ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\nDepthwiseConvreshape\n<latexit sha1_base64=\"gCWcveOpRcv7c4laoKcpyWoAeYU=\">AAAB+nicbVC7TsMwFHV4tZRXCiOLRYXEVCUMwEalLhVTkehDaqPKcZ3WquNE9g1VFfonsDCAECtfwsY3sLOC+xig5UiWj865V/fe48eCa3CcD2tldW19I5PdzG1t7+zu2fn9uo4SRVmNRiJSTZ9oJrhkNeAgWDNWjIS+YA1/UJ74jVumNI/kDYxi5oWkJ3nAKQEjdex8pQ08ZBo35n+5YxecojMFXibunBQuv+6/PzNXw2rHfm93I5qETAIVROuW68TgpUQBp4KNc+1Es5jQAemxlqGSmDFeOl19jI+N0sVBpMyTgKfq746UhFqPQt9UhgT6etGbiP95rQSCCy/lMk6ASTobFCQCQ4QnOeAuV4yCGBlCqOJmV0z7RBEKJq2cCcFdPHmZ1E+L7lnRvXYKJQfNkEWH6AidIBedoxKqoCqqIYqG6AE9oWfrznq0XqzXWemKNe85QH9gvf0Af/mX5g==</latexit>\nH ⇥ W ⇥ C\nreshapeconcat\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\n<latexit sha1_base64=\"5ewkXw24Xj/axQ4aqODOZAF40uU=\">AAAB8XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOhF09SwX5gG8pmu2mXbjZhdyKU0H/hxYMiXv033vw3btsctPXBwOO9GWbmBYkUBl332ymsrW9sbhW3Szu7e/sH5cOjlolTzXiTxTLWnYAaLoXiTRQoeSfRnEaB5O1gXJ/57SeujYjVA04S7kd0qEQoGEUrPd6RHoqIG1Lvlytu1Z2DrBIvJxXI0eiXv3qDmKURV8gkNabruQn6GdUomOTTUi81PKFsTIe8a6mido2fzS+ekjOrDEgYa1sKyVz9PZHRyJhJFNjOiOLILHsz8T+vm2J442dCJSlyxRaLwlQSjMnsfTIQmjOUE0so08LeStiIasrQhlSyIXjLL6+S1kXVu6p695eVmpvHUYQTOIVz8OAaanALDWgCAwXP8ApvjnFenHfnY9FacPKZY/gD5/MHjDmQIQ==</latexit>\nN ⇥ C\nFactorizedAttention Convolutional Relative PositionEncodingConvolutional Relative Position Encoding\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP…\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"Kk581vKii6sNAjwOjTO0Dq5wlPA=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GNBBI8V7Ae0oWy2m3bpZpPuToQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbmd+64lrI2L1iJOE+xEdKBEKRtFK7e6QYnbXnPbKFbfqzkFWiZeTCuSo98pf3X7M0ogrZJIa0/HcBP2MahRM8mmpmxqeUDaiA96xVNGIGz+b3zslZ1bpkzDWthSSufp7IqORMZMosJ0RxaFZ9mbif14nxfDGz4RKUuSKLRaFqSQYk9nzpC80ZygnllCmhb2VsCHVlKGNqGRD8JZfXiXNi6p3VfUeLis1N4+jCCdwCufgwTXU4B7q0AAGEp7hFd6csfPivDsfi9aCk88cwx84nz8JUY/q</latexit>\nˆEV\nConvolutional Relative Position Encoding\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP…\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\n<latexit sha1_base64=\"Kk581vKii6sNAjwOjTO0Dq5wlPA=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GNBBI8V7Ae0oWy2m3bpZpPuToQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbmd+64lrI2L1iJOE+xEdKBEKRtFK7e6QYnbXnPbKFbfqzkFWiZeTCuSo98pf3X7M0ogrZJIa0/HcBP2MahRM8mmpmxqeUDaiA96xVNGIGz+b3zslZ1bpkzDWthSSufp7IqORMZMosJ0RxaFZ9mbif14nxfDGz4RKUuSKLRaFqSQYk9nzpC80ZygnllCmhb2VsCHVlKGNqGRD8JZfXiXNi6p3VfUeLis1N4+jCCdwCufgwTXU4B7q0AAGEp7hFd6csfPivDsfi9aCk88cwx84nz8JUY/q</latexit>\nˆEV\n<latexit sha1_base64=\"boTMl6NO5OSwzJIJRP1dMHzPyuI=\">AAAB9XicbVC7SgNBFJ31mcRX1NJmMAgWEnYt1DJgY5mAeUCyCbOzs8mQ2dll5q4alm1s/AUbC0XsxH+x82t08ig08cCFwzn3cu89Xiy4Btv+spaWV1bX1nP5wsbm1vZOcXevoaNEUVankYhUyyOaCS5ZHTgI1ooVI6EnWNMbXo795g1TmkfyGkYxc0PSlzzglICRurVu2gF2BykP+1nWK5bssj0BXiTOjJQq+Qf//fv+pNorfnb8iCYhk0AF0brt2DG4KVHAqWBZoZNoFhM6JH3WNlSSkGk3nVyd4SOj+DiIlCkJeKL+nkhJqPUo9ExnSGCg572x+J/XTiC4cFMu4wSYpNNFQSIwRHgcAfa5YhTEyBBCFTe3YjogilAwQRVMCM78y4ukcVp2zspOzaRhoyly6AAdomPkoHNUQVeoiuqIIoUe0TN6sW6tJ+vVepu2LlmzmX30B9bHDwxQloE=</latexit>\nQ img\n<latexit sha1_base64=\"q8NZ5bgOf7WwavqP5IGTEcO0Ll8=\">AAAB9XicbVC5TgMxEPVyhnCFo6OxiJCool0KoCMSBZRBIoeUbCKv4yRWbO/KngXCav+DhgKEKKHhS+go+ROco4CEJ4309N6MZuYFkeAGXPfLmZtfWFxazqxkV9fWNzZzW9sVE8aasjINRahrATFMcMXKwEGwWqQZkYFg1aB/PvSrN0wbHqprGETMl6SreIdTAlZqVppJA9gdJFx207SVy7sFdwQ8S7wJyZ993H9fvO0mpVbus9EOaSyZAiqIMXXPjcBPiAZOBUuzjdiwiNA+6bK6pYpIZvxkdHWKD6zSxp1Q21KAR+rviYRIYwYysJ2SQM9Me0PxP68eQ+fUT7iKYmCKjhd1YoEhxMMIcJtrRkEMLCFUc3srpj2iCQUbVNaG4E2/PEsqRwXvuOBdufmii8bIoD20jw6Rh05QEV2iEiojijR6QE/o2bl1Hp0X53XcOudMZnbQHzjvP3Illso=</latexit>\nV img\n<latexit sha1_base64=\"12kkXleLSgTMdECDKCmjbYibHmU=\">AAAB6HicbVC7SgNBFL0bXzG+4qOzGQyCVdi1UDsDFlomYB6QBJmd3E3GzM4uM7NCXPIFNhaK2PoBVn6JnaV/4uRRaOKBC4dzzuU+/FhwbVz3y8ksLC4tr2RXc2vrG5tb+e2dmo4SxbDKIhGphk81Ci6xargR2IgV0tAXWPf7FyO/fodK80hem0GM7ZB2JQ84o8ZKlfJNvuAW3THIPPGmpHD+cf99+b6X2vxnqxOxJERpmKBaNz03Nu2UKsOZwGGulWiMKevTLjYtlTRE3U7Hiw7JoVU6JIiULWnIWP3dkdJQ60Ho22RITU/PeiPxP6+ZmOCsnXIZJwYlmwwKEkFMREZXkw5XyIwYWEKZ4nZXwnpUUWbsb3L2Cd7syfOkdlz0TopexS2UXJggC/twAEfgwSmU4ArKUAUGCA/wBM/OrfPovDivk2jGmfbswh84bz/KCJCW</latexit>\nP\n*\n<latexit sha1_base64=\"Kk581vKii6sNAjwOjTO0Dq5wlPA=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GNBBI8V7Ae0oWy2m3bpZpPuToQS+ie8eFDEq3/Hm//GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ2aWd3b/+gfHjUNHGqGW+wWMa6HVDDpVC8gQIlbyea0yiQvBWMbmd+64lrI2L1iJOE+xEdKBEKRtFK7e6QYnbXnPbKFbfqzkFWiZeTCuSo98pf3X7M0ogrZJIa0/HcBP2MahRM8mmpmxqeUDaiA96xVNGIGz+b3zslZ1bpkzDWthSSufp7IqORMZMosJ0RxaFZ9mbif14nxfDGz4RKUuSKLRaFqSQYk9nzpC80ZygnllCmhb2VsCHVlKGNqGRD8JZfXiXNi6p3VfUeLis1N4+jCCdwCufgwTXU4B7q0AAGEp7hFd6csfPivDsfi9aCk88cwx84nz8JUY/q</latexit>\nˆEV\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥\nSum\n<latexit sha1_base64=\"MKq8PXcJJiL/m/XT3LLhDVvOKjw=\">AAAB7XicbVDLSgNBEOz1GeMr6tHLYBA8hV0R9Rj04jGCeUCyhNnJbDJmdmaZ6RVCyD948aCIV//Hm3/jJNmDJhY0FFXddHdFqRQWff/bW1ldW9/YLGwVt3d29/ZLB4cNqzPDeJ1pqU0ropZLoXgdBUreSg2nSSR5MxreTv3mEzdWaPWAo5SHCe0rEQtG0UmNDoqE226p7Ff8GcgyCXJShhy1bumr09MsS7hCJqm17cBPMRxTg4JJPil2MstTyoa0z9uOKuqWhOPZtRNy6pQeibVxpZDM1N8TY5pYO0oi15lQHNhFbyr+57UzjK/DsVBphlyx+aI4kwQ1mb5OesJwhnLkCGVGuFsJG1BDGbqAii6EYPHlZdI4rwSXleD+oly9yeMowDGcwBkEcAVVuIMa1IHBIzzDK7x52nvx3r2PeeuKl88cwR94nz+4I486</latexit>\n⇥ Multiplication*Convolution\nConvolutional Position Encoding\nInput Feature Map\nOutput Feature Map\nHadamardProductMatrix ProductElementwise Sum\nFigure 2. Illustration of the conv-attentional module. We apply\na convolutional position encoding to the image tokens from the\ninput. The resulting features are fed into a factorized attention with\na convolutional relative position encoding.\nprovable approximation, but with the cost of relatively large\nC′. Efficient-Attention [25] applies the softmax function for\nboth ϕ and ψ, which is efficient but causes a significant per-\nformance drop on the vision tasks in our experiments. Here,\nwe develop our factorized attention mechanism following\nLambdaNets [1] with ϕ as the identity function and ψ as the\nsoftmax:\nFactorAtt(X) = Q√\nC\n\u0010\nsoftmax(K)⊤V\n\u0011\n(3)\nwhere softmax(·) is applied across the tokens in the se-\nquence in an element-wise manner and the projected chan-\nnels C′ = C. In LambdaNets [1], the scaling factor 1/\n√\nC\nis implicitly included in the weight initialization, while our\nfactorized attention applies the scaling factor explicitly. This\nfactorized attention takes O(NC + C2) space complexity\nand O(NC 2) time complexity. It is noteworthy that the\nproposed factorized attention following [ 1] is not a direct\napproximation of the scaled dot-product attention, but it\ncan still be regarded as a generalized attention mechanism\nmodeling the feature interactions using query, key and value\nvectors.\n4.2. Convolution as Position Encoding\nOur factorized attention module mitigates the compu-\ntational burden from the original scaled dot-product atten-\ntion. However, because we compute L = softmax(K)⊤V ∈\nRC×C first, L can be seen as a global data-dependent linear\ntransformation for every feature vector in the query map Q.\nSerial Block\nSerial Block\nSerial Block\nInput Image\nParallel Block\nParallel Block\nSerial BlockParallel Block\nParallel Block\nParallel Block\nParallel Block\nParallel Group\n………Parallel GroupImage and Class Tokens\nConcat\nLinear1000-class logits\nConv-Att\nFFN\nConv-Att\nFFN\nConv-Att\nFFN\nParallel Block\nParallel Block\nParallel Block\nParallel Group\nSerial Block\nSerial Block\nSerial Block\nInput Image\nSerial Block\n1000-class logitsLinear\n(a) CoaT-Lite (b) CoaT\nImage and Class Tokens\n<latexit sha1_base64=\"0pjc2PBKzPC4TJvj2h7g5hSLQ7Q=\">AAACFHicbZDLSsNAFIZP6q3WW9Slm8EiCEJJalGXhW66rGAv0IQymU7aoZMLMxOhhDyEG1/FjQtF3Lpw59s4bbOwrT8M/HznHM6c34s5k8qyfozCxubW9k5xt7S3f3B4ZB6fdGSUCELbJOKR6HlYUs5C2lZMcdqLBcWBx2nXmzRm9e4jFZJF4YOaxtQN8ChkPiNYaTQwrxxfYJI2s/S6miFHsYBKtGDdJdYY1AZm2apYc6F1Y+emDLlaA/PbGUYkCWioCMdS9m0rVm6KhWKE06zkJJLGmEzwiPa1DbFe5KbzozJ0ockQ+ZHQL1RoTv9OpDiQchp4ujPAaixXazP4X62fKP/OTVkYJ4qGZLHITzhSEZolhIZMUKL4VBtMBNN/RWSMdSJK51jSIdirJ6+bTrVi31Ts+1q5buVxFOEMzuESbLiFOjShBW0g8AQv8AbvxrPxanwYn4vWgpHPnMKSjK9fnQad2A==</latexit>\nH\n32\n⇥\nW\n32\n⇥ C 4\n<latexit sha1_base64=\"fgcMoFPb51ekf9JsummRAdICe20=\">AAACG3icbZDLSsNAFIYn9VbrLerSzWApuCpJleqy0E2XFewF2hAm00k7dHJh5kQoIe/hxldx40IRV4IL38Zpm4Vt/WHg5zvncOb8Xiy4Asv6MQpb2zu7e8X90sHh0fGJeXrWVVEiKevQSESy7xHFBA9ZBzgI1o8lI4EnWM+bNuf13iOTikfhA8xi5gRkHHKfUwIauWZt6EtC01aW2vUMD4EHTOEl662wpnuNK9iux+CaZatqLYQ3jZ2bMsrVds2v4SiiScBCoIIoNbCtGJyUSOBUsKw0TBSLCZ2SMRtoGxK9z0kXt2W4oskI+5HULwS8oH8nUhIoNQs83RkQmKj12hz+Vxsk4N85KQ/jBFhIl4v8RGCI8DwoPOKSURAzbQiVXP8V0wnRwYCOs6RDsNdP3jTdWtWuV+37m3LDyuMoogt0ia6QjW5RA7VQG3UQRU/oBb2hd+PZeDU+jM9la8HIZ87RiozvX6a3n9E=</latexit>\nH\n16\n⇥\nW\n16\n⇥ C 3\n<latexit sha1_base64=\"jgOVPjMcP+lPLkx1/nbPKAXmJP4=\">AAACGXicbZDLSsNAFIYn9VbrLerSzWApuCpJkdploZsuK9gLtCFMppN26OTCzIlQQl7Dja/ixoUiLnXl2zhts6itPwz8fOcczpzfiwVXYFk/RmFnd2//oHhYOjo+OT0zzy96KkokZV0aiUgOPKKY4CHrAgfBBrFkJPAE63uz1qLef2RS8Sh8gHnMnIBMQu5zSkAj17RGviQ0bWdpI8Mj4AFTeIX666jl1nAF2/UYXLNsVa2l8Laxc1NGuTqu+TUaRzQJWAhUEKWGthWDkxIJnAqWlUaJYjGhMzJhQ21Dovc56fKyDFc0GWM/kvqFgJd0fSIlgVLzwNOdAYGp2qwt4H+1YQJ+w0l5GCfAQrpa5CcCQ4QXMeExl4yCmGtDqOT6r5hOic4FdJglHYK9efK26dWqdr1q39+Wm1YeRxFdoWt0g2x0h5qojTqoiyh6Qi/oDb0bz8ar8WF8rloLRj5zif7I+P4Fsj6fXg==</latexit>\nH\n8\n⇥\nW\n8\n⇥ C 2\n<latexit sha1_base64=\"9w4/MZFPtW0OPOnUYpZsZXC0PFo=\">AAACGXicbZDLSsNAFIYn9VbrLerSzWApuCqJSHVZ6KbLCvYCTQiT6aQdOrkwcyKUkNdw46u4caGIS135Nk7bLGrrDwM/3zmHM+f3E8EVWNaPUdra3tndK+9XDg6Pjk/M07OeilNJWZfGIpYDnygmeMS6wEGwQSIZCX3B+v60Na/3H5lUPI4eYJYwNyTjiAecEtDIMy0nkIRm7Ty7ybEDPGQKL1F/FbU8G9ew3UjAM6tW3VoIbxq7MFVUqOOZX84opmnIIqCCKDW0rQTcjEjgVLC84qSKJYROyZgNtY2I3udmi8tyXNNkhINY6hcBXtDViYyESs1CX3eGBCZqvTaH/9WGKQR3bsajJAUW0eWiIBUYYjyPCY+4ZBTETBtCJdd/xXRCdC6gw6zoEOz1kzdN77puN+r2/U21aRVxlNEFukRXyEa3qInaqIO6iKIn9ILe0LvxbLwaH8bnsrVkFDPn6I+M71+jr59V</latexit>\nH\n4\n⇥\nW\n4\n⇥ C 1\nConv1D\nFigure 3. CoaT model architecture. (Left) The overall network architecture of CoaT-Lite. CoaT-Lite consists of serial blocks only, where\nimage features are down-sampled and processed in a sequential order. (Right) The overall network architecture of CoaT. CoaT consists of\nserial blocks and parallel blocks. Both blocks enable the co-scale mechanism.\nThis indicates that if we have two query vectorsq1, q2 ∈ RC\nfrom Q and q1 = q2, then their corresponding self-attention\noutputs will be the same:\nFactorAtt(X)1 = q1√\nC\nL = q2√\nC\nL = FactorAtt(X)2 (4)\nWithout the position encoding, the Transformer is only com-\nposed of linear layers and self-attention modules. Thus, the\noutput of a token is dependent on the corresponding input\nwithout awareness of any difference in its locally nearby\nfeatures. This property is unfavorable for vision tasks such\nas semantic segmentation (e.g. the same blue patches in the\nsky and the sea are segmented as the same category).\nConvolutional Relative Position Encoding. To enable vi-\nsion tasks, ViT and DeiT [ 9, 30] insert absolute position\nembeddings into the input, which may have limitations\nin modeling relations between local tokens. Instead, fol-\nlowing [24], we can integrate a relative position encoding\nP = {pi ∈ RC, i= −M−1\n2 , ...,M−1\n2 } with window size\nM to obtain the relative attention map EV ∈ RN×C; in at-\ntention formulation, if tokens are regarded as a 1-D sequence:\nRelFactorAtt(X) = Q√\nC\n\u0010\nsoftmax(K)⊤V\n\u0011\n+ EV (5)\nwhere the encoding matrix E ∈ RN×N has elements:\nEij = 1(i, j)qi · pj−i, 1 ≤ i, j≤ N (6)\nin which 1(i, j) = 1{|j−i|≤(M−1)/2}(i, j) is an indicator\nfunction. Each element Eij represents the relation from\nquery qi to the value vj within window M, and (EV )i\naggregates all related value vectors with respect to query\nqi. Unfortunately, the EV term still requires O(N2) space\ncomplexity and O(N2C) time complexity. In CoaT, we\npropose to simplify the EV term to ˆEV by considering each\nchannel in the query, position encoding and value vectors as\ninternal heads. Thus, for each internal head l, we have:\nE(l)\nij = 1(i, j)q(l)\ni p(l)\nj−i, ˆEV\n(l)\ni = P\nj E(l)\nij v(l)\nj (7)\nIn practice, we can use a 1-D depthwise convolution to\ncompute ˆEV :\nˆEV\n(l)\n= Q(l) ◦ Conv1D(P(l), V(l)), (8)\nˆEV = Q ◦ DepthwiseConv1D(P, V) (9)\nwhere ◦ is the Hadamard product. It is noteworthy that\nin vision Transformers, we have two types of tokens, the\nclass (CLS) token and image tokens. Thus, we use a 2-D\ndepthwise convolution (with window sizeM ×M and kernel\nweights P) and apply it only to the reshaped image tokens\n(i.e. Qimg, Vimg ∈ RH×W×C from Q, Vrespectively):\nˆEV\nimg\n= Qimg ◦ DepthwiseConv2D(P, Vimg) (10)\nˆEV = concat( ˆEV\nimg\n, 0) (11)\nConvAtt(X) = Q√\nC\n\u0010\nsoftmax(K)⊤V\n\u0011\n+ ˆEV (12)\nBased on our derivation, the depthwise convolution can be\nseen as a special case of relative position encoding.\nConvolutional Relative Position Encoding vs Other Relative\nPosition Encodings. The commonly referenced relative po-\nsition encoding [24] works in standard scaled dot-product\nattention settings since the encoding matrix E is combined\nwith the softmax logits in the attention maps, which are\nnot materialized in our factorized attention. Related to our\nConv-AttentionFeed-Forward\nClass TokenImage Tokens\nConv-AttentionFeed-Forward\nFlatten\nReshape\nPatch Embed\nInput Feature Maps\nOutput Feature Maps\nTo Linear LayerOr Parallel Block\nSerial Block\nSerial Block\nSerial Block\nInput Image\nParallel Block\nParallel Block\nSerial BlockParallel Block\nParallel Block\nParallel Block\nParallel Block\nParallel Group\n………Parallel GroupImage and Class Tokens\nConcatLinear1000-class logits\nConv-Att\nFFN\nConv-Att\nFFN\nConv-Att\nFFN\nParallel Block\nParallel Block\nParallel Block\nParallel Group\nSerial Block\nSerial Block\nSerial Block\nSerial Block\nInput Image\nSerial Block\n1000-class logitsLinear\n(a) CoaT-Lite (b) CoaT\nImage and Class Tokens\n<latexit sha1_base64=\"0pjc2PBKzPC4TJvj2h7g5hSLQ7Q=\">AAACFHicbZDLSsNAFIZP6q3WW9Slm8EiCEJJalGXhW66rGAv0IQymU7aoZMLMxOhhDyEG1/FjQtF3Lpw59s4bbOwrT8M/HznHM6c34s5k8qyfozCxubW9k5xt7S3f3B4ZB6fdGSUCELbJOKR6HlYUs5C2lZMcdqLBcWBx2nXmzRm9e4jFZJF4YOaxtQN8ChkPiNYaTQwrxxfYJI2s/S6miFHsYBKtGDdJdYY1AZm2apYc6F1Y+emDLlaA/PbGUYkCWioCMdS9m0rVm6KhWKE06zkJJLGmEzwiPa1DbFe5KbzozJ0ockQ+ZHQL1RoTv9OpDiQchp4ujPAaixXazP4X62fKP/OTVkYJ4qGZLHITzhSEZolhIZMUKL4VBtMBNN/RWSMdSJK51jSIdirJ6+bTrVi31Ts+1q5buVxFOEMzuESbLiFOjShBW0g8AQv8AbvxrPxanwYn4vWgpHPnMKSjK9fnQad2A==</latexit>\nH\n32\n⇥\nW\n32\n⇥ C 4\n<latexit sha1_base64=\"fgcMoFPb51ekf9JsummRAdICe20=\">AAACG3icbZDLSsNAFIYn9VbrLerSzWApuCpJleqy0E2XFewF2hAm00k7dHJh5kQoIe/hxldx40IRV4IL38Zpm4Vt/WHg5zvncOb8Xiy4Asv6MQpb2zu7e8X90sHh0fGJeXrWVVEiKevQSESy7xHFBA9ZBzgI1o8lI4EnWM+bNuf13iOTikfhA8xi5gRkHHKfUwIauWZt6EtC01aW2vUMD4EHTOEl662wpnuNK9iux+CaZatqLYQ3jZ2bMsrVds2v4SiiScBCoIIoNbCtGJyUSOBUsKw0TBSLCZ2SMRtoGxK9z0kXt2W4oskI+5HULwS8oH8nUhIoNQs83RkQmKj12hz+Vxsk4N85KQ/jBFhIl4v8RGCI8DwoPOKSURAzbQiVXP8V0wnRwYCOs6RDsNdP3jTdWtWuV+37m3LDyuMoogt0ia6QjW5RA7VQG3UQRU/oBb2hd+PZeDU+jM9la8HIZ87RiozvX6a3n9E=</latexit>\nH\n16\n⇥\nW\n16\n⇥ C 3\n<latexit sha1_base64=\"jgOVPjMcP+lPLkx1/nbPKAXmJP4=\">AAACGXicbZDLSsNAFIYn9VbrLerSzWApuCpJkdploZsuK9gLtCFMppN26OTCzIlQQl7Dja/ixoUiLnXl2zhts6itPwz8fOcczpzfiwVXYFk/RmFnd2//oHhYOjo+OT0zzy96KkokZV0aiUgOPKKY4CHrAgfBBrFkJPAE63uz1qLef2RS8Sh8gHnMnIBMQu5zSkAj17RGviQ0bWdpI8Mj4AFTeIX666jl1nAF2/UYXLNsVa2l8Laxc1NGuTqu+TUaRzQJWAhUEKWGthWDkxIJnAqWlUaJYjGhMzJhQ21Dovc56fKyDFc0GWM/kvqFgJd0fSIlgVLzwNOdAYGp2qwt4H+1YQJ+w0l5GCfAQrpa5CcCQ4QXMeExl4yCmGtDqOT6r5hOic4FdJglHYK9efK26dWqdr1q39+Wm1YeRxFdoWt0g2x0h5qojTqoiyh6Qi/oDb0bz8ar8WF8rloLRj5zif7I+P4Fsj6fXg==</latexit>\nH\n8\n⇥\nW\n8\n⇥ C 2\n<latexit sha1_base64=\"9w4/MZFPtW0OPOnUYpZsZXC0PFo=\">AAACGXicbZDLSsNAFIYn9VbrLerSzWApuCqJSHVZ6KbLCvYCTQiT6aQdOrkwcyKUkNdw46u4caGIS135Nk7bLGrrDwM/3zmHM+f3E8EVWNaPUdra3tndK+9XDg6Pjk/M07OeilNJWZfGIpYDnygmeMS6wEGwQSIZCX3B+v60Na/3H5lUPI4eYJYwNyTjiAecEtDIMy0nkIRm7Ty7ybEDPGQKL1F/FbU8G9ew3UjAM6tW3VoIbxq7MFVUqOOZX84opmnIIqCCKDW0rQTcjEjgVLC84qSKJYROyZgNtY2I3udmi8tyXNNkhINY6hcBXtDViYyESs1CX3eGBCZqvTaH/9WGKQR3bsajJAUW0eWiIBUYYjyPCY+4ZBTETBtCJdd/xXRCdC6gw6zoEOz1kzdN77puN+r2/U21aRVxlNEFukRXyEa3qInaqIO6iKIn9ILe0LvxbLwaH8bnsrVkFDPn6I+M71+jr59V</latexit>\nH\n4\n⇥\nW\n4\n⇥ C 1\n…\nFigure 4. Schematic illustration of the serial block in CoaT. In-\nput feature maps are first down-sampled by a patch embedding\nlayer, and then tokenized features (along with a class token) are\nprocessed by multiple conv-attention and feed-forward layers.\nwork, the main results of the original LambdaNets [1] use\na 3-D convolution to compute EV directly and reduce the\nchannels of queries and keys to CK where CK < C, but\nit costs O(NCC K) space complexity and O(NCC KM2)\ntime complexity, which leads to relatively heavy compu-\ntation when channel sizes CK, Care large. A recent up-\ndate in LambdaNets [ 1] provides an efficient variant with\ndepth-wise convolution under resource constrained scenarios.\nOur factorized attention computes ˆEV with only O(NC )\nspace complexity and O(NCM 2) time complexity, aiming\nto achieve better efficiency.\nConvolutional Position Encoding. We then extend the\nidea of convolutional relative position encoding to a general\nconvolutional position encoding case. Convolutional relative\nposition encoding models local position-based relationships\nbetween queries and values. Similar to the absolute posi-\ntion encoding used in most image Transformers [9, 30], we\nwould like to insert the position relationship into the input\nimage features directly to enrich the effects of relative posi-\ntion encoding. In each conv-attentional module, we insert\na depthwise convolution into the input features X and con-\ncatenate the resulting position-aware features back to the\ninput features following the standard absolute position en-\ncoding scheme (see Figure 2 lower part), which resembles\nthe realization of conditional position encoding in CPVT [6].\nCoaT and CoaT-Lite share the convolutional position en-\ncoding weights and convolutional relative position encoding\nweights for the serial and parallel modules within the same\nscale. We set convolution kernel size to 3 for the convolu-\ntional position encoding. We set convolution kernel size to\n3, 5 and 7 for image features from different attention heads\nfor convolutional relative position encoding.\nThe work of CPVT [6] explores the use of convolution\nas conditional position encodings by inserting it after the\nfeed-forward network under a single scale ( H\n16 × W\n16 ). Our\nwork focuses on applying convolution as relative position\nencoding and a general position encoding with the factorized\nattention.\nConv-Attentional Mechanism The final conv-attentional\nmodule is shown in Figure 2: We apply the first convo-\nlutional position encoding on the image tokens from the\ninput. Then, we feed it into ConvAtt(·) including factorized\nattention and the convolutional relative position encoding.\nThe resulting map is used for the subsequent feed-forward\nnetworks.\n5. Co-Scale Conv-Attentional Transformers\n5.1. Co-Scale Mechanism\nThe proposed co-scale mechanism is designed to intro-\nduce fine-to-coarse, coarse-to-fine and cross-scale informa-\ntion into image transformers. Here, we describe two types of\nbuilding blocks in the CoaT architecture, namely serial and\nparallel blocks, in order to model multiple scales and enable\nthe co-scale mechanism.\nCoaT Serial Block. A serial block (shown in Figure 4)\nmodels image representations in a reduced resolution. In a\ntypical serial block, we first down-sample input feature maps\nby a certain ratio using a patch embedding layer, and flatten\nthe reduced feature maps into a sequence of image tokens.\nWe then concatenate image tokens with an additional CLS\ntoken, a specialized vector to perform image classification,\nand apply multiple conv-attentional modules as described in\nSection 4 to learn internal relationships among image tokens\nand the CLS token. Finally, we separate the CLS token from\nthe image tokens and reshape the image tokens to 2-D feature\nmaps for the next serial block.\nCoaT Parallel Block. We realize a co-scale mechanism\nbetween parallel blocks in each parallel group (shown in\nFigure 5). In a typical parallel group, we have sequences\nof input features (image tokens and CLS token) from serial\nblocks with different scales. To enable fine-to-coarse, coarse-\nto-fine, and cross-scale interaction in the parallel group, we\ndevelop two strategies: (1) direct cross-layer attention; (2)\nattention with feature interpolation. In this paper, we adopt\nattention with feature interpolation for better empirical per-\nformance. The effectiveness of both strategies is shown in\nSection 6.4.\nDirect cross-layer attention. In direct cross-layer attention,\nwe form query, key, and value vectors from input features\nfor each scale. For attention within the same layer, we use\nthe conv-attention (Figure 2) with the query, key and value\nvectors from current scale. For attention across different\nlayers, we down-sample or up-sample the key and value\nConv-Att\nFFN\nConv-Att\nFFN\nConv-Att\nFFN\nParallel Block\nParallel Block\nParallel Block\nParallel Group\nConv-Att\nFFN\nConv-Att\nFFN\nConv-Att\nFFN\nCross-Att\nFFNFFNFFN\nConv-Att\nCross-Att\nConv-AttCross-Att\nCross-Att\nCross-AttCross-Att\nConv-Att\nw/o Co-Scale Co-Scale w/ Direct Cross-Layer AttentionCo-Scale w/ Feature Interpolation\nFigure 5. Schematic illustration of the parallel group in CoaT. For “w/o Co-Scale”, tokens learned at the individual scales are combined\nto perform the classification but absent intermediate co-scale interaction for the individual paths of the parallel blocks. We propose two\nco-scale variants, namely direct cross-layer attention and attention with feature interpolation. Co-scale with feature interpolation is adopted\nin the final CoaT-Lite and CoaT models reported on the ImageNet benchmark.\nTable 1. Architecture details of CoaT-Lite and CoaT models. Ci represents the hidden dimension of the attention layers in block i; Hi\nrepresents the number of attention heads in the attention layers in block i; Ri represents the expansion ratio for the feed-forward hidden\nlayer dimension between attention layers in block i. Multipliers indicate the number of conv-attentional modules in block i.\nBlocks Output CoaT-Lite CoaTTiny Mini Small Medium Tiny Mini Small\nSerial Block(S1) 56×56\n\nC1= 64H1= 8R1= 8\n\n×2\n\nC1= 64H1= 8R1= 8\n\n×2\n\nC1= 64H1= 8R1= 8\n\n×3\n\nC1= 128H1= 8R1= 4\n\n×3\n\nC1= 152H1= 8R1= 4\n\n×2\n\nC1= 152H1= 8R1= 4\n\n×2\n\nC1= 152H1= 8R1= 4\n\n×2\nSerial Block(S2) 28×28\n\nC2= 128H2= 8R2= 8\n\n×2\n\nC2= 128H2= 8R2= 8\n\n×2\n\nC2= 128H2= 8R2= 8\n\n×4\n\nC1= 256H1= 8R1= 4\n\n×6\n\nC2= 152H2= 8R2= 4\n\n×2\n\nC2= 216H2= 8R2= 4\n\n×2\n\nC1= 320H1= 8R1= 4\n\n×2\nSerial Block(S3) 14×14\n\nC3= 256H3= 8R3= 4\n\n×2\n\nC3= 320H3= 8R3= 4\n\n×2\n\nC3= 320H3= 8R3= 4\n\n×6\n\nC1= 320H1= 8R1= 4\n\n×10\n\nC3= 152H3= 8R3= 4\n\n×2\n\nC3= 216H3= 8R3= 4\n\n×2\n\nC1= 320H1= 8R1= 4\n\n×2\nSerial Block(S4) 7×7\n\nC4= 320H4= 8R4= 4\n\n×2\n\nC4= 512H4= 8R4= 4\n\n×2\n\nC4= 512H4= 8R4= 4\n\n×3\n\nC1= 512H1= 8R1= 4\n\n×8\n\nC4= 152H4= 8R4= 4\n\n×2\n\nC4= 216H4= 8R4= 4\n\n×2\n\nC1= 320H1= 8R1= 4\n\n×2\nParallel Group\n\n28×2814×147×7\n\n\n\nC4= 152H4= 8R4= 4\n\n×6\n\nC4= 216H4= 8R4= 4\n\n×6\n\nC1= 320H1= 8R1= 4\n\n×6\n#Params 5.7M 11M 20M 45M 5.5M 10M 22M\nvectors to match the resolution of other scales, which en-\nables fine-to-coarse and coarse-to-fine interaction. We then\nperform cross-attention, which extends the conv-attention\nwith queries from the current scale with keys and values\nfrom another scale. Finally, we sum the outputs of conv-\nattention and cross-attention together and apply a shared\nfeed-forward layer. With direct cross-layer attention, the\ncross-scale information is fused in a cross-attention fashion.\nAttention with feature interpolation. Instead of performing\ncross-layer attention directly, we also present attention with\nfeature interpolation. First, the input image features from\ndifferent scales are processed by independent conv-attention\nmodules. Then, we down-sample or up-sample image fea-\ntures from each scale to match the dimensions of other scales\nusing bilinear interpolation, or keep the same for its own\nscale. The features belonging to the same scale are summed\nin the parallel group, and they are further passed into a shared\nfeed-forward layer. In this way, the conv-attentional module\nin the next step can learn cross-scale information based on\nthe feature interpolation in the current step.\n5.2. Model Architecture\nCoaT-Lite. CoaT-Lite, Figure 3 (Left), processes input im-\nages with a series of serial blocks following a fine-to-coarse\npyramid structure. Given an input image I ∈ RH×W×C,\neach serial block down-samples the image features into lower\nresolution, resulting in a sequence of four resolutions:F1 ∈\nR\nH\n4 ×W\n4 ×C1 , F2 ∈ R\nH\n8 ×W\n8 ×C2 , F3 ∈ R\nH\n16 ×W\n16 ×C3 , F4 ∈\nR\nH\n32 ×W\n32 ×C4 . In CoaT-Lite, we obtain the CLS token in the\nlast serial block, and perform image classification via a linear\nprojection layer based on the CLS token.\nCoaT. Our CoaT model, shown in Figure 3 (Right), con-\nsists of both serial and parallel blocks. Once we obtain\nmulti-scale feature maps {F1, F2, F3, F4} from the serial\nblocks, we pass F2, F3, F4 and corresponding CLS tokens\ninto the parallel group with three separate parallel blocks.\nTo perform classification with CoaT, we aggregate theCLS\nTable 2. CoaT performance on ImageNet-1K validation set.\nOur CoaT models consistently outperform other methods while\nbeing parameter efficient. ConvNets and ViTNets with similar\nmodel size are grouped together for comparison. “#GFLOPs” and\nTop-1 Acc are measured at input image size. “*” results are adopted\nfrom [33].\nArch. Model #ParamsInput #GFLOPsTop-1 Acc.\nConvNetsEfficientNet-B0 [29]5.3M 2242 0.4 77.1%ShuffleNet [43] 5.4M 2242 0.5 73.7%\nViTNetsDeiT-Tiny [30] 5.7M 2242 1.3 72.2%CPVT-Tiny [6] 5.7M 2242 - 73.4%CoaT-LiteTiny(Ours) 5.7M 2242 1.6 77.5%CoaTTiny(Ours) 5.5M 2242 4.4 78.3%\nConvNetsEfficientNet-B2[29] 9M 2602 1.0 80.1%ResNet-18∗[11] 12M 2242 1.8 69.8%\nViTNetsPVT-Tiny [33] 13M 2242 1.9 75.1%CoaT-LiteMini(Ours) 11M 2242 2.0 79.1%CoaTMini(Ours) 10M 2242 6.8 81.0%\nConvNetsEfficientNet-B4 [29]19M 3802 4.2 82.9%ResNet-50∗[11] 25M 2242 4.1 78.5%ResNeXt50-32x4d* [36]25M 2242 4.3 79.5%\nViTNetsDeiT-Small [30] 22M 2242 4.6 79.8%PVT-Small [33] 24M 2242 3.8 79.8%CPVT-Small [6] 22M 2242 - 80.5%T2T-ViTt-14 [40] 22M 2242 6.1 81.7%Swin-T [17] 29M 2242 4.5 81.3%CoaT-LiteSmall(Ours) 20M 2242 4.0 81.9%CoaTSmall(Ours) 22M 2242 12.6 82.1%\nConvNetsEfficientNet-B6 [29]43M 5282 19 84.0%ResNet-101∗[11] 45M 2242 7.9 79.8%ResNeXt101-64x4d∗[36] 84M 2242 15.6 81.5%\nViTNetsPVT-Large [33] 61M 2242 9.8 81.7%T2T-ViTt-24 [40] 64M 2242 15 82.6%DeiT-Base [30] 86M 2242 17.6 81.8%CPVT-Base [6] 86M 2242 - 82.3%Swin-B [17] 88M 2242 15.4 83.5%Swin-B [17] 88M 3842 47 84.5%CoaT-LiteMedium(Ours) 45M 2242 9.8 83.6%CoaT-LiteMedium(Ours) 45M 3842 28.7 84.5%\ntokens from all three scales.\nModel Variants. In this paper, we explore CoaT and CoaT-\nLite with several different model sizes, namely Tiny, Mini,\nSmall and Medium. Architecture details are shown in Ta-\nble 1. For example, tiny models represent those with a 5M\nparameter budget constraint. Specifically, these tiny mod-\nels have four serial blocks, each with two conv-attentional\nmodules. In CoaT-Lite Tiny architectures, the hidden dimen-\nsions of the attention layers increase in later blocks. CoaT\nTiny sets the hidden dimensions of the attention layers in\nthe parallel group to be equal, and performs the co-scale\nmechanism within the six parallel groups. Mini, small and\nmedium models follow the same architecture design but with\nincreased embedding dimensions and increased numbers of\nconv-attentional modules within blocks.\n6. Experiments\n6.1. Experiment Details\nImage Classification. We perform image classification on\nthe standard ILSVRC-2012 ImageNet dataset [23]. The stan-\ndard ImageNet benchmark contains 1.3 million images in\nthe training set and 50K images in the validation set, cov-\nTable 3. Object detection and instance segmentation results\nbased on Mask R-CNN on COCO val2017 . Experiments are\nperformed under the MMDetection framework [4]. “*” results are\nadopted from Detectron2.\nBackbone #Params\n(M)\nw/ FPN 1× w/ FPN 3×\nAPb APm APb APm\nResNet-18* 31.3 34.2 31.3 36.3 33.2\nPVT-Tiny [33] 32.9 36.7 35.1 39.8 37.4\nCoaT-LiteMini(Ours) 30.7 41.4 38.0 42.9 38.9\nCoaTMini(Ours) 30.2 45.1 40.6 46.5 41.8\nResNet-50* 44.3 38.6 35.2 41.0 37.2\nPVT-Small [33] 44.1 40.4 37.8 43.0 39.9\nSwin-T [17] 47.8 43.7 39.8 46.0 41.6\nCoaT-LiteSmall(Ours) 39.5 45.2 40.7 45.7 41.1\nCoaTSmall(Ours) 41.6 46.5 41.8 49.0 43.7\nTable 4. Object detection and instance segmentation results\nbased on Cascade Mask R-CNN on COCO val2017 . Experi-\nments are performed using the MMDetection framework [4].\nBackbone #Params\n(M)\nw/ FPN 1× w/ FPN 3×\nAPb APm APb APm\nSwin-T [17] 85.6 48.1 41.7 50.4 43.7\nCoaT-LiteSmall(Ours) 77.3 49.1 42.5 48.9 42.6\nCoaTSmall(Ours) 79.4 50.4 43.5 52.2 45.1\nTable 5. Object detection results based on Deformable DETR\non COCO val2017. DD ResNet-50 represents the baseline result\nusing the official checkpoint. ResNet-50 and our CoaT-Lite as DD\nbackbones are directly comparable due to similar model size.\nBackbone Deformable DETR (Multi-Scale)\nAP AP 50 AP75 APS APM APL\nDD ResNet-50 [46] 44.5 63.7 48.7 26.8 47.6 59.6\nDDCoaT-LiteSmall(Ours) 47.0 66.5 51.2 28.8 50.3 63.3\nDDCoaTSmall(Ours) 48.4 68.5 52.4 30.2 51.8 63.8\nering 1000 object classes. Image cropping sizes are set to\n224×224. For fair comparison, we perform data augmen-\ntation such as MixUp [ 42], CutMix [ 41], random erasing\n[45], repeated augmentation [12], and label smoothing [28],\nfollowing identical procedures in DeiT [30].\nAll experimental results for our models in Table 2 are re-\nported at 300 epochs, consistent with previous methods [30].\nAll models are trained with the AdamW [18] optimizer under\nthe NVIDIA Automatic Mixed Precision (AMP) framework.\nThe learning rate is scaled as 5 × 10−4 × global batch size\n512 .\nObject Detection and Instance Segmentation. We con-\nduct object detection and instance segmentation experiments\non the Common Objects in Context (COCO2017) dataset\n[16]. The COCO2017 benchmark contains 118K training\nimages and 5K validation images. We evaluate the gener-\nalization ability of CoaT in object detection and instance\nsegmentation with the Mask R-CNN [10] and Cascade Mask\nR-CNN [2]. We use the MMDetection [4] framework and\nfollow the settings from Swin Transformers [17]. In addition,\nwe perform object detection based on Deformable DETR\n[46] following its data processing settings.\nFor Mask R-CNN optimization, we train the model with\nthe ImageNet-pretrained backbone on 8 GPUs via AdamW\noptimizer with learning rate 0.0001. The training period\ncontains 12 epochs in 1× setting and 36 epochs in 3× setting.\nFor Cascade R-CNN experiments, we use three detection\nheads, with the same optimization and training period as\nMask R-CNN. For Deformable DETR optimization, we train\nthe model with the pretrained backbone for 50 epochs, using\nan AdamW optimizer with initial learning rate 2 × 10−4,\nβ1 = 0.9, and β2 = 0.999. We reduce the learning rate by a\nfactor of 10 at epoch 40.\n6.2. CoaT for ImageNet Classification\nTable 2 shows top-1 accuracy results for our models on\nthe ImageNet validation set comparing with state-of-the-art\nmethods. We separate model architectures into two cate-\ngories: convolutional networks (ConvNets), and Transform-\ners (ViTNets). Under different parameter budget constraints,\nCoaT and CoaT-Lite show strong results compared to other\nConvNet and ViTNet methods.\n6.3. Object Detection and Instance Segmentation\nTables 3 and 4 demonstrate CoaT object detection and\ninstance segmentation results under the Mask R-CNN and\nCascade Mask R-CNN frameworks on the COCO val2017\ndataset. Our CoaT and CoaT-Lite models show clear perfor-\nmance advantages over the ResNet, PVT [33] and Swin [17]\nbackbones under both the 1× setting and the 3× setting. In\nparticular, our CoaT models bring a large performance gain,\ndemonstrating that our co-scale mechanism is essential to\nimprove the performance of Transformer-based architectures\nfor downstream tasks.\nWe additionally perform object detection with the De-\nformable DETR (DD) framework in Table 5. We compare\nour models with the standard ResNet-50 backbone on the\nCOCO dataset [ 16]. Our CoaT backbone achieves 3.9%\nimprovement on average precision (AP) over the results of\nDeformable DETR with ResNet-50 [46].\n6.4. Ablation Study\nEffectiveness of Position Encodings. We study the effec-\ntiveness of the combination of the convolutional relative\nposition encoding (CRPE) and convolutional position en-\ncoding (CPE) in our conv-attention module in Table 6. Our\nCoaT-Lite without any position encoding results in poor\nperformance, indicating that position encoding is essential\nfor vision Transformers. We observe great improvement\nfor CoaT-Lite variants with either CRPE or CPE, and the\ncombination of CRPE and CPE leads to the best perfor-\nmance (77.5% top-1 accuracy), making both position encod-\ning schemes complementary rather than conflicting.\nEffectiveness of Co-Scale. In Table 7, we present perfor-\nmance results for two co-scale variants in CoaT, direct cross-\nlayer attention and attention with feature interpolation. We\nalso report CoaT without co-scale as a baseline. Comparing\nto CoaT without a co-scale mechanism, CoaT with feature\nTable 6. Effectiveness of position encodings. All experiments\nare performed with the CoaT-Lite Tiny architecture. Performance\nis evaluated on the ImageNet-1K validation set.\nModel CPE CRPE Top-1 Acc.\nCoaT-Lite Tiny✗ ✗ 68.8%\n✗ ✓ 75.0%\n✓ ✗ 75.9%\n✓ ✓ 77.5%\ninterpolation shows performance improvements on both im-\nage classification and object detection (Mask R-CNN w/\nFPN 1×). Attention with feature interpolation offers a clear\nadvantage over direct cross-layer attention due to less com-\nputational complexity and higher accuracy.\nTable 7. Effectiveness of co-scale. All experiments are performed\nwith the CoaT Tiny architecture. Performance is evaluated on the\nImageNet-1K validation set and the COCO val2017 dataset.\nModel #Params Input #GFLOPsTop-1 Acc. @input APb APm\nCoaT w/o Co-Scale 5.5M2242 4.4 77.8% 41.6 37.9CoaT w/ Co-Scale- Direct Cross-Layer Attention5.5M2242 4.8 77.0% 42.1 38.3- Attention w/ Feature Interp.5.5M2242 4.4 78.3% 42.5 38.6\nComputational Cost. We report FLOPs, FPS, latency,\nand GPU memory usage in Table 8. In summary, CoaT\nmodels attain higher accuracy than similar-sized Swin Trans-\nformers, but CoaT models in general do have larger la-\ntency/FLOPs. The current parallel groups in CoaT are more\ncomputationally demanding, which can be mitigated by re-\nducing high-resolution parallel blocks and re-using their\nfeature maps in the co-scale mechanism in future work. The\nlatency overhead in CoaT is possibly because operations (e.g.\nlayers, position encodings, upsamples/downsamples) are not\nrunning in parallel.\nTable 8. ImageNet-1K validation set results compared with the concurrent work\nSwin Transformer[17]. Computational metrics are measured on a single V100 GPU.\nModel #ParamsInput GFLOPs FPS Latency MemTop-1 Acc. Top-5 Acc.\nSwin-T [17] 28M 2242 4.5 755 16ms 222M81.2% 95.5%CoaT-LiteSmall(Ours) 20M 2242 4.0 634 32ms224M81.9% 95.6%CoaTSmall(Ours) 22M 2242 12.6 111 60ms371M82.1% 96.1%\nSwin-S [17] 50M 2242 8.7 437 29ms 372M83.2% 96.2%Swin-B [17] 88M 2242 15.4 278 30ms 579M83.5% 96.5%CoaT-LiteMedium(Ours)45M 2242 9.8 319 52ms429M83.6% 96.7%\nSwin-B [17] 88M 3842 47.1 85 33ms 1250M84.5% 97.0%CoaT-LiteMedium(Ours)45M 3842 28.7 97 56ms937M84.5% 97.1%\n7. Conclusion\nIn this paper, we present a Transformer based image clas-\nsifier, Co-scale conv-attentional image Transformer (CoaT),\nin which cross-scale attention and efficient conv-attention\noperations have been developed. CoaT models attain strong\nclassification results on ImageNet, and their applicability to\ndownstream computer vision tasks has been demonstrated\nfor object detection and instance segmentation.\nAcknowledgments. This work is supported by NSF\nAward IIS-1717431. Tyler Chang is partially supported by\nthe UCSD HDSI graduate fellowship.\nReferences\n[1] Irwan Bello. Lambdanetworks: Modeling long-range interac-\ntions without attention. In ICLR, 2021.\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High\nquality object detection and instance segmentation. TPAMI,\n2019.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue\nWu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan,\nXingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins,\nJared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethink-\ning attention with performers. In ICLR, 2021.\n[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882, 2021.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. In NAACL-HLT, 2019.\n[8] Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: a\nno-recurrence sequence-to-sequence model for speech recog-\nnition. In Proc. IEEE Int. Conf. Acoust. Sph. Sig Process. ,\n2018.\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n[12] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoefler, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In CVPR, 2020.\n[13] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019.\n[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classification with deep convolutional neural networks.\nIn NIPS, 2012.\n[15] Yann LeCun, L ´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014.\n[17] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021.\n[18] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019.\n[19] David G Lowe. Distinctive image features from scale-\ninvariant keypoints. IJCV, 60(2):91–110, 2004.\n[20] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018.\n[21] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NIPS, 2019.\n[22] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015.\n[23] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nIJCV, 115(3):211–252, 2015.\n[24] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. In NAACL-\nHLT, 2018.\n[25] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and\nHongsheng Li. Efficient attention: Attention with linear\ncomplexities. In WACV, pages 3531–3539, 2021.\n[26] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015.\n[27] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR, 2015.\n[28] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In CVPR, 2016.\n[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019.\n[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv´e J´egou. Training\ndata-efficient image transformers & distillation through atten-\ntion. arXiv preprint arXiv:2012.12877, 2020.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017.\n[32] J Wang, K Sun, T Cheng, B Jiang, C Deng, Y Zhao, D\nLiu, Y Mu, M Tan, X Wang, et al. Deep high-resolution\nrepresentation learning for visual recognition. TPAMI, 2020.\n[33] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In ICCV, 2021.\n[34] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In CVPR, 2018.\n[35] Andrew Witkin. Scale-space filtering: A new approach to\nmulti-scale description. In Proc. IEEE Int. Conf. Acoust. Sph.\nSig Process., volume 9, pages 150–153, 1984.\n[36] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017.\n[37] Saining Xie, Sainan Liu, Zeyu Chen, and Zhuowen Tu. Atten-\ntional shapecontextnet for point cloud recognition. In CVPR,\n2018.\n[38] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption genera-\ntion with visual attention. In ICML, 2015.\n[39] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei,\nand Ming Zhou. Layoutlm: Pre-training of text and layout\nfor document image understanding. In SIGKDD, 2020.\n[40] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. In ICCV, 2021.\n[41] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In ICCV, 2019.\n[42] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. mixup: Beyond empirical risk minimization. In\nICLR, 2018.\n[43] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufflenet: An extremely efficient convolutional neural net-\nwork for mobile devices. In CVPR, 2018.\n[44] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020.\n[45] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proc. AAAI\nConf. Artificial Intell., pages 13001–13008, 2020.\n[46] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6789219975471497
    },
    {
      "name": "Computer science",
      "score": 0.6691668033599854
    },
    {
      "name": "Encoder",
      "score": 0.6245779991149902
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5932118892669678
    },
    {
      "name": "Embedding",
      "score": 0.4873434007167816
    },
    {
      "name": "Computer vision",
      "score": 0.46976611018180847
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4493482708930969
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43067386746406555
    },
    {
      "name": "Engineering",
      "score": 0.17073607444763184
    },
    {
      "name": "Electrical engineering",
      "score": 0.08390164375305176
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 6
}