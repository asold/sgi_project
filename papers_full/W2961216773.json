{
    "title": "Locality-constrained Spatial Transformer Network for Video Crowd Counting",
    "url": "https://openalex.org/W2961216773",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5002275591",
            "name": "Yanyan Fang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5059405859",
            "name": "Biyun Zhan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5077359939",
            "name": "Wandi Cai",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5034339267",
            "name": "Shenghua Gao",
            "affiliations": [
                "ShanghaiTech University"
            ]
        },
        {
            "id": "https://openalex.org/A5083412798",
            "name": "Bo Hu",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W603908379",
        "https://openalex.org/W1910776219",
        "https://openalex.org/W2519281173",
        "https://openalex.org/W2741077351",
        "https://openalex.org/W2123175289",
        "https://openalex.org/W2463631526",
        "https://openalex.org/W2161969291",
        "https://openalex.org/W2495387757",
        "https://openalex.org/W2116022929",
        "https://openalex.org/W2781155895",
        "https://openalex.org/W2975535607",
        "https://openalex.org/W2964209782",
        "https://openalex.org/W2964203052",
        "https://openalex.org/W2207893099",
        "https://openalex.org/W2109064712",
        "https://openalex.org/W2886443245",
        "https://openalex.org/W2964018834",
        "https://openalex.org/W2058907003",
        "https://openalex.org/W2962854645",
        "https://openalex.org/W2952145882",
        "https://openalex.org/W2963816519",
        "https://openalex.org/W2075875861",
        "https://openalex.org/W1976959044"
    ],
    "abstract": "Compared with single image based crowd counting, video provides the spatial-temporal information of the crowd that would help improve the robustness of crowd counting. But translation, rotation and scaling of people lead to the change of density map of heads between neighbouring frames. Meanwhile, people walking in/out or being occluded in dynamic scenes leads to the change of head counts. To alleviate these issues in video crowd counting, a Locality-constrained Spatial Transformer Network (LSTN) is proposed. Specifically, we first leverage a Convolutional Neural Networks to estimate the density map for each frame. Then to relate the density maps between neighbouring frames, a Locality-constrained Spatial Transformer (LST) module is introduced to estimate the density map of next frame with that of current frame. To facilitate the performance evaluation, a large-scale video crowd counting dataset is collected, which contains 15K frames with about 394K annotated heads captured from 13 different scenes. As far as we know, it is the largest video crowd counting dataset. Extensive experiments on our dataset and other crowd counting datasets validate the effectiveness of our LSTN for crowd counting.",
    "full_text": "LOCALITY-CONSTRAINED SPATIAL TRANSFORMER NETWORK\nFOR VIDEO CROWD COUNTING\nYanyan Fanga ,Biyun Zhan a, Wandi Cai a, Shenghua Gao b, Bo Hu a,∗\naSchool of information and technology, Fudan University,bShanghaitech University\n{yyfang, byzhan15, wdcai15, bohu}@fudan.edu.cn, gaoshh@shanghaitech.edu.cn\nABSTRACT\nCompared with single image based crowd counting, video\nprovides the spatial-temporal information of the crowd that\nwould help improve the robustness of crowd counting. But\ntranslation, rotation and scaling of people lead to the change\nof density map of heads between neighbouring frames. Mean-\nwhile, people walking in/out or being occluded in dynamic\nscenes leads to the change of head counts. To alleviate these\nissues in video crowd counting, a Locality-constrained Spatial\nTransformer Network (LSTN) is proposed. Speciﬁcally, we\nﬁrst leverage a Convolutional Neural Networks to estimate the\ndensity map for each frame. Then to relate the density maps\nbetween neighbouring frames, a Locality-constrained Spatial\nTransformer (LST) module is introduced to estimate the den-\nsity map of next frame with that of current frame. To facil-\nitate the performance evaluation, a large-scale video crowd\ncounting dataset is collected, which contains 15K frames\nwith about 394K annotated heads captured from 13 differ-\nent scenes. As far as we know, it is the largest video crowd\ncounting dataset. Extensive experiments on our dataset and\nother crowd counting datasets validate the effectiveness of\nour LSTN for crowd counting. All our dataset are released\nin https://github.com/sweetyy83/Lstn_fdst_\ndataset.\nIndex Terms— Convolutional Neural Network; Locality-\nconstrained Spatial Transformer Network; Video Crowd\nCounting\n1. INTRODUCTION\nCrowd counting has been widely used in computer vision be-\ncause of its potential applications in video surveillance, trafﬁc\ncontrol, and emergency management. However, most previ-\nous works [1][2][3] focus on single image based crowd count-\ning. In real applications, we have videos at hand, and usually\nthe movement of crowd is predictable and consistent [4]. In\nthis paper, we target at exploiting the spatial-temporal con-\nsistency among neighbouring frames for more robust video\ncrowd counting.\nPrevious crowd counting methods can be roughly catego-\nrized into detection-based approaches and regression-based\n∗Corresponding author.\napproaches. Detection based approaches count crowd by de-\ntecting heads or pedestrians, but these approaches usually fail\nto detect tiny [5] or occluded [6] heads/bodies which are very\ncommon in real scenarios. Thus regression-based approaches\nare more commonly used. Recently, in light of the success\nof Convolutional Neural Networks (CNN) for image classiﬁ-\ncation, it also has been introduced to crowd counting, where\nCNN is used to learn a mapping from an input image to its\ncorresponding density map. To leverage the spatial-temporal\nconsistency among neighbouring frames for more accurate\ndensity maps in videos, LSTM [7] or ConvLSTM [8] based\napproaches have been proposed which accumulate features of\nall history frames with LSTM or ConvLSTM for density map\nestimation. These approaches have demonstrated their effec-\ntiveness for video crowd counting, but they leverage history\ninformation in an implicit way, and as people walk in/out or\nare occluded, the identities of the crowd in the history frame\nmay be totally different from the ones in current frame. Con-\nsequently, the features from these history may even hurt the\ndensity map estimation of current frame.\nRather than using LSTM or ConvLSTM to implicitly\nmodel the spatial-temporal dependencies in videos, in this\npaper, we propose to leverage a Locality-constrained Spatial\nTransformer (LST) module to explicitly model the spatial-\ntemporal correlation between neighbouring frames. Specif-\nically, on one hand, given the same population of the crowd,\nprevious work [4] has shown that the trajectories of crowd\ncan be well predicted. But because of the change of perspec-\ntive, distance, rotation, and lighting, the appearance of the\nsame person may visually change a lot, and thus it sometimes\nmay be not easy to directly re-identify the people in two ad-\njacent frames. But density map ignores the appearances of\nthe people and is only related to the location of heads. Since\npeople’s trajectories are predictable, the density map of one\nframe probably can be warped from that of its previous frame\nwith some transformations, including scaling and translation\ncaused by people walking away from or towards camera, rota-\ntion caused by the motion of camera, etc.. On the other hand,\nfor videos, some people walk in/out of the imaging range of\ncamera or are occluded. In these cases, it is infeasible to esti-\nmate the density maps for those people from previous frames.\nBy taking all these factors together, in our LST, rather than\narXiv:1907.07911v1  [cs.CV]  18 Jul 2019\nwarping the density map for the whole frame, we propose\nto divide each frame into blocks. Given two blocks with the\nsame location but from two neighbouring frames, we use their\nsimilarity to weight the difference between the ground-truth\ndensity map of the block and the one warped from the density\nmap of the other block. If these two blocks are similar, they\nprobably correspond to the same population, then the differ-\nence between ground-truth density map and warped density\nmap should be smaller. If someone walks in/out or is oc-\ncluded, then we allow the warped density map from previous\nframe to be slightly different from the ground-truth. Further,\nsince only the spatial-temporal dependencies between neigh-\nbouring frames are used, our model can get rid of the effect of\nirrelevant history frames in density map estimation. Experi-\nments validate the effectiveness of our model for video crowd\ncounting.\nA large-scale dataset with multiple scenes is desirable\nfor video crowd counting. But most existing datasets are\ntoo small and with only a few scenes. For example, the\nWorldExpo’10 dataset, which is the largest one in previ-\nous works, only contains 5 scenes. Thus we propose to\nbuild a new large-scale video crowd counting dataset named\nFudan-ShanghaiTech (FDST) with more scenes. Speciﬁcally,\nFDST dataset contains 15,000 frames with 394,081 annotated\nheads captured from 13 difference scenes, including shopping\nmalls, squares, hospitals, etc.. The dataset is much larger than\nthe WorldExpo’10 dataset, which only contains 3980 frames\nwith 199,923 annotated heads. Further, we provide the frame-\nwise annotation while WordExPo’10 only provides the anno-\ntation for every 30 seconds. Therefore FDST dataset is more\nsuitable for video crowd counting evaluation.\nThe main contributions of our work can be summarized as\nfollows: i) we propose a Locality-constrained Spatial Trans-\nformer Network (LSTN), which explicitly models the spatial-\ntemporal dependencies between neighbouring frames to fa-\ncilitate the video crowd counting; ii) we collect a large-scale\nvideo crowd counting dataset with frame-wise ground-truth\nannotation, which would facilitate the performance evaluation\nin video crowd counting; iii) extensive experiments validate\nthe effectiveness of our model for video crowd counting.\n2. RELATED WORK\nSince our work is related to deep learning based crowd count-\ning, here we only brieﬂy discuss recent works on deep learn-\ning based crowd counting.\nCrowd counting for single image. Recent works\n[3][9][10] have shown the effectiveness of CNN for density\nmap estimation in single image crowd counting. To improve\nthe robustness of crowd counting for areas with different head\nsizes and densities, different network architectures have been\nproposed, including MCNN [3], Hydra CNN [11], Switch-\nCNN [9], CSRNet [10], which basically leverages networks\nwith different local receptive ﬁelds for density maps estima-\ntion. Further, recently, people also propose to leverage detec-\ntion [12] or localization [13] tasks to assist the crowd counting\ntask. But these single image crowd counting methods may\nlead to inconsistent head counts for neighbouring frames in\nvideo crowd counting.\nVideo crowd counting. Most previous works focus on\nsingle image crowd counting and there are only a few works\non video crowd counting. Recently, Xiong et al. [8] pro-\npose to leverage ConvLSTM to integrate history features and\nfeatures of current frame for video crowd counting, which\nhas shown its effectiveness for video crowd counting. Fur-\nther, Zhang et al. [7] also propose to use LSTM for vehicle\ncounting in videos. However, all these LSTM based methods\nmay be affected by those irrelevant history, and do not ex-\nplicitly consider the spatial-temporal dependencies in videos,\nwhereas our solution models such dependencies in neighbour-\ning frames with LST explicitly. Thus our solution is more\nstraightforward.\nSpatial transformer network (STN).Recently, Jader-\nberg et al. [14]introduce a differentiable Spatial Transformer\n(ST) module which is capable to model the spatial transfor-\nmation between input and output. Such ST module can be\neasily plugged into many existing networks and trained in\nan end-to-end manner, and has shown its effectiveness for\nface alignment [15][16] and face recognition [17]. Further, it\nalso has been applied for density map estimation in a coarse-\nto-ﬁne based single image crowd counting framework [18].\nBut different from [18], we propose to leverage ST to relate\ndensity maps between neighbouring frames for video crowd\ncounting.\nDensity map regression module\nVGG-16 VGG-16\nLSTLST\nLocality-constrained spatial transformer module\nreg\ntM\nShared\nGT\ntM\nregl\nregl\n1\nreg\ntM \n1\nGT\ntM \nShared\n1\nLST\ntM \n1\nGT\ntM \nLSTl\n2\nLST\ntM \n2\nGT\ntM \nLSTl\nShared\nShared\ntX\n1tX \nFig. 1: The structure of the LSTN module for video crowd\ncounting.\n3. OUR APPROACH\nOur network architecture is shown in Fig. 1. It consists of\ntwo modules: density map regression module and Locality-\nconstrained Spatial Transformer (LST) module. The density\nmap regression module takes each frame as input and esti-\nmates its corresponding density map, and then the LST mod-\nule takes the estimated density map as input to predict the\ndensity map of next frame.\n3.1. Density map regression module\nDensity map generation is very important for the performance\nof density map based crowd counting. Given one frame with\nN heads, if the ith head is centered at pi, we represent it as a\ndelta function δ(p−pi). Hence the ground-truth density map\nof this frame can be calculated as follows:\nM=\nN∑\ni=1\nδ(p−pi) ∗Gσ(p). (1)\nHere Gσ(p) is a 2D Gaussian kernel with variance σ:\nGσ(p) = 1\n2πσ2 e−(x2+y2)\n2σ2 (2)\nIn other words, if a pixel is near the annotated point, it has\nhigher probability belonging to a head. Once the density\nmap is deﬁned, the density map regression module maps\neach frame to its corresponding density map. We denote\nthe ground-truth density map of tth (t = 1,...,T ) frame as\nMGT\nt , and denote the density map estimated by density map\nregression module as Mreg\nt . Then the objective of density\nmap regression module can be written as follows:\nℓreg = 1\n2T\nT∑\nt=1\n∥Mreg\nt −MGT\nt ∥2 (3)\nIn our implementation, we use VGG-16 network in our\ndensity map regression module.\n3.2. LST module\nFor the same population of crowd in videos, many previous\nworks have shown that the trajectories of these people can be\nwell predicted. Thus the density map of previous frame would\nhelp the density map prediction of current frame. However,\nin all video crowd counting datasets, the correspondence of\npeople in neighbouring frames are not provided, which pre-\nvents directly learning a mapping from head coordinates in\nprevious frame to those in current frame. Further, because\nof the change in perspective, distance, rotation, and lighting\ncondition in neighbouring frames as well as occlusion, the ap-\npearance of the same person may visually change a lot, which\nmakes directly re-identifying the person in two frames difﬁ-\ncult. But density map ignores the appearances of the people\nand is only related to the location of heads. Now that people’s\ntrajectories are predictable, we can leverage the density map\nof previous frame to estimate the density map of current frame\nfor the same group of people. Speciﬁcally, the deformation of\nthe density map for the same group people in neighbouring\nframes includes scaling and translation if people walk away\nfrom or towards camera, or rotation if there exists some mo-\ntion for camera, for example, caused by wind or vibration of\nground.\nRecent work [18] has shown the effectiveness of spatial\ntransformer (ST) module for learning the transform between\ninput and output. Thus ST can be used to learn the map-\nping for the same group of people between the two neigh-\nbouring frames. However, in practice, people walk in/out the\nrange of camera, and some people may be occluded, which\nrestricts the application of ST. Thus, in this paper, we propose\nan LST, which is essentially a weighted ST for each image\nblock. Speciﬁcally, we divide each frame into many blocks.\nGiven two blocks with the same spatial coordinates but from\ntwo neighbouring frames, we use their similarity to weight the\ndifference between the ground-truth density map of one block\nand the density map transformed from the other block. If\nthese two blocks are similar, they probably correspond to the\nsame population, then the difference between ground-truth\ndensity map and transformed density map should be smaller.\nIf someone walks in/out or is occluded, then we allow the es-\ntimated density map to be slightly different from the ground-\ntruth. By minimizing such difference over all blocks and all\nframes, the dependencies between neighbouring frames can\nbe exploited for video crowd counting.\nWe denote the mapping function of LST module as fLST\nwhich takes the estimated density map of the tth frame as\ninput to estimate the density map of the (t+ 1)th frame. We\nuse MLST\nt+1 to denote the density map of the (t+ 1)th frame\nestimated by LST. Then\nMLST\nt+1 = fLST(Mreg\nt ; Aθ) (4)\n[ xs\ni\nys\ni\n]\n= Γθ(Gi) = Aθ\n\n\nxt\ni\nyt\ni\n1\n\n\n=\n[ θ11 θ12 θ13\nθ21 θ22 θ23\n]\n\nxt\ni\nyt\ni\n1\n\n\n(5)\nwhere (xt\ni ,yt\ni) are the target coordinates of the sampling grid\nΓθ in the output density maps, (xs\ni ,ys\ni) are the source coordi-\nnates in the input density maps that deﬁne the sample points,\nand Aθ denotes the transformation matrix [14].\nWe evenly divide each frame It, MGT\nt+1 and MLST\nt+1 into\nH×W blocks, and use It(i,j), MGT\nt+1(i,j) and MLST\nt+1 (i,j)\nto denote the block in the jth column and the ith row for\nthe tth frame, its ground-truth density map and density map\nestimated by LST. Then the objective of LST can be written\nTable 1: Details of some datasets: Num is the total number of frames; FPS is the number of frames per second; Max is the\nmaximal crowd count in one frame; Min is the minimal crowd count in one frame; Ave is the average crowd count in one frame;\nTotal is total number of labeled people.\nDataset Resolution Num FPS Max Min Ave Total\nUCSD 238 ×158 2000 10 46 11 24.9 49,885\nMall 640 ×480 2000 <2 53 13 31.2 62,316\nWorldExpo 576 ×720 3980 50 253 1 50.2 199,923\nOurs 1920 ×1080\n1280 ×720 15000 30 57 9 26.7 394,081\nas follows.\nℓLST = 1\n2T\nT−1∑\nt=1\n∑\n1≤i≤H\n1≤j≤W\nS(It(i,j),It+1(i,j))\n×∥MLST\nt+1 (i,j) −MGT\nt+1(i,j)∥2\n2\n(6)\nwhere S(It(i,j),It+1(i,j)) denotes the similarity between\nthe corresponding temporal neighbouring blocks, which can\nbe measured as follows\nS(It(i,j),It+1(i,j)) = exp(−∥It(i,j) −It+1(i,j)∥2\n2\n2β2 ).\n(7)\n3.3. Loss function\nWe combine the losses of the density map regression mod-\nule and that of the LST module, and arrive at the following\nobjective function\nℓ= ℓreg + λℓLST, (8)\nwhere λis a weight used to balance ℓreg and ℓLST.\nIn the training process, an Adam optimizer is used with a\nlearning rate at 1e-8 on our dataset. To reduce over-ﬁtting, we\nadopt the batch-normalization, and the batch-size is 5.\nOnce our network is trained, in the testing phase, we can\ndirectly estimate the density map of each frame and integrate\nthe density map to get the estimated head counts.\n3.4. Implementation details\nThe variance in gaussian based density map generation γ =\n3, and the β used in similarity measurement is 30 on FDST\ndataset. We resize all frames to 640 ×360 pixels. We ﬁrst\npretrain density map regression module, then we ﬁne-tune the\nwhole network by ﬁx the ﬁrst 10 layers in VGG-16. For the\nnumber of blocks, we ﬁx W = 2 on all datasets. On the Mall\ndataset and our dataset, we ﬁx H = 1 , and H = 2 on the\nUCSD dataset. We set λ= 0.001 on FDST dataset†.\n†Because the ground-truth are annotated 2fps on Expo’10 and ROI’s are\nalso marked, therefore the population of two neighbouring frames change\na lot. Thus this dataset is not suitable for performance evaluation of our\nmethod.\n4. EXPERIMENTS\n4.1. Evaluation metric\nFollowing work [19], we adopt both the mean absolute error\n(MAE) and the mean squared error (MSE) as metrics to eval-\nuate the performance of different methods, which are deﬁned\nas follows:\nMAE = 1\nT\nT∑\ni=1\n|zi −ˆzi|,MSE =\n√1\nT\nT∑\ni=1\n(zi −ˆzi)2 (9)\nwhere T is the total number of frames of all testing video\nsequences, zi and ˆzi are the actual number of people and es-\ntimated number of people in the ith frame respectively.\n4.2. Fudan-ShanghaiTech Video Crowd counting dataset\nExisting video crowd counting datasets are too small in terms\nof number of both frames as well as scenes. Hence, we intro-\nduce a new large-scale video crowd counting dataset. Specif-\nically, we collected 100 videos captured from 13 different\nscenes, and FDST dataset contains 150,000 frames, with a to-\ntal of 394,081 annotated heads. It takes more than 400 hours\nto annotate FDST dataset. As far as we know, this dataset is\nthe largest video crowd counting dataset. Table.1 shows the\nstatistics of our dataset and other relevant datasets.\nThe training set of FDST dataset consists of 60 videos,\n9000 frames and the testing set contains the remaining 40\nvideos, 6000 frames. We compare our method with MCNN\n[3] which achieves state-of-the-art performance for single im-\nage crowd counting, ConvLSTM [8] which is state-of-the-art\nvideo crowd counting method. We also report the perfor-\nmance of our method without LST. All results are shown in\nTable. 2. We can see that our method achieves the best per-\nformance. Further the improvement of our method compared\nwith the one without LST shows the effectiveness of LST.\nIt is worth noting that because there are many scenes in our\ndataset, and it is not easy to train the ConvLSTM, therefore\nthe performance of ConvLSTM is even worse than single im-\nage based method. We also show the density map estimated\nby our LSTN in Fig. 2.\nTable 2: Results of different methods on our dataset.\nMethod MAE MSE\nMCNN [3] 3.77 4.88\nConvLSTM [8] 4.48 5.82\nOurs without LST 3.87 5.16\nOur Method 3.35 4.45\nInput \nGround truth\nEstimation\ntX\n2tX \n3tX \n1tX \nFig. 2: The density maps estimated by our method on our\ndataset.\n4.3. The UCSD dataset\nWe also evaluate our method with the UCSD dataset [20],\nwhich contains 2000 frames captured by surveillance cameras\nin the UCSD campus. The resolution of frames is 238 ×158\npixels and the rate of frame is 10 fps. The number of person\nin each frame varies from 11 to 46. By following the same\nsetting with [20], we use frames from 601 to 1400 as training\ndata, and the remaining 1200 frames as testing data.\nFollowing [10], we use bilinear interpolation to resize\neach frame into 952 ×632. Table. 3 shows the accuracy of\ndifferent methods on this dataset. We can see that our method\nalso outperforms ConvLSTM based method on this dataset.\nTable 3: Results of different methods on the UCSD dataset.\nMethod MAE MSE\nKernel Ridge Regression [21] 2.16 7.45\nRidge Regression [22] 2.25 7.82\nGaussian Process Regression [20] 2.24 7.97\nCumulative Attribute Regression [23] 2.07 6.86\nZhang et al [2] 1.60 3.31\nMCNN [3] 1.07 1.35\nSwitch-CNN [9] 1.62 2.10\nCSRNet [10] 1.16 1.47\nFCN-rLSTM [7] 1.54 3.02\nConvLSTM [8] 1.30 1.79\nBidirectional ConvLSTM [8] 1.13 1.43\nOur Method 1.07 1.39\n4.4. The Mall dataset\nThe Mall dataset is captured in a shopping mall with a surveil-\nlance camera [22]. This video-based dataset consists of 2000\nframes in the dimension of640×480 pixels, with over 60,000\nlabeled pedestrians. Region of Interest (ROI) and perspective\nmap are also provided. According to the train-test setting in\n[22], we use the ﬁrst 800 frames for training and the remain-\ning 1200 frames for testing. The performance of different\nmethods are shown in Table. 4, our model also achieves state-\nof-the-art performance in terms of both MAE and MSE.\nTable 4: Results of different methods on the Mall dataset.\nMethod MAE MSE\nKernel Ridge Regression [21] 3.51 18.10\nRidge Regression [22] 3.59 19.00\nGaussian Process Regression [20] 3.72 20.10\nCumulative Attribute Regression [23] 3.43 17.70\nCOUNT Forest [24] 2.50 10.00\nConvLSTM [8] 2.24 8.50\nBidirectional ConvLSTM [8] 2.10 7.60\nOur Method 2.00 2.50\n4.5. The importance of similarity term in LST\nIn our LSTN, we use the similarity between temporal neigh-\nbouring blocks to weight the difference between the warped\ndensity map and its ground-truth. The underlying assumption\nis that if two blocks are similar, then the population within\nthese two blocks probably correspond to the same group of\npeople and then spatial transformer works well. But if the\nsimilarity is lower, which means people walk in/out or are\noccluded, then it is less possible to infer the density map of\nblock in the temporal neighbouring frame.We compare the\nresults with/without similarity term on UCSD, Mall, FDST\ndataset, and the results are shown in Table.5. We can see that\nsimilarity term always boots the performance of video crowd\ncounting, which validates our assumption.\nTable 5: Comparing the performance of LSTN with/without\nsimilarity term on UCSD, Mall and our dataset.\nWith Without\nMethod MAE MSE MAE MSE\nUCSD 1.07 1.39 1.11 1.41\nMall 2.00 2.50 2.18 2.70\nOurs 3.35 4.45 3.81 5.10\n5. CONCLUSION\nIn this paper, a Locality-constrained Spatial Transformer Net-\nwork (LSTN) is proposed to explicitly relate the density maps\nof neighbouring frames for video crowd counting. Speciﬁ-\ncally, we ﬁrst leverage a density map regression module to es-\ntimate the density map of each frame. Considering that people\nmay walk in/out or are occluded, we divide each frame into\nblocks, and use the similarity between two temporal neigh-\nbouring blocks to weight the difference between the ground-\ntruth density map and the estimated one from the other block.\nWe further build a large-scale video crowd counting dataset\nfor performance evaluation, and as far as we know, FDST\ndataset is the larget video crowd counting dataset in terms\nof the number of both scenes and frames. Extensive experi-\nments validate the effectiveness of our LSTN for video crowd\ncounting.\n6. REFERENCES\n[1] M. Fu, P. Xu, X. Li, Q.Liu, M.Ye, and C.Zhu, “Fast\ncrowd density estimation with convolutional neural net-\nworks,” Engineering Applications of Artiﬁcial Intelli-\ngence, pp. 81 – 88, 2015.\n[2] Cong Zhang, Hongsheng Li, Xiaogang Wang, and Xi-\naokang Yang, “Cross-scene crowd counting via deep\nconvolutional neural networks,” in CVPR, June 2015.\n[3] Y . Zhang, D. Zhou, S. Chen, S. Gao, and Y . Ma, “Single-\nimage crowd counting via multi-column convolutional\nneural network,” in CVPR, June 2016, pp. 589–597.\n[4] B. Federico, L. Giuseppe, Ballan L, and A. Bimbo,\n“Context-aware trajectory prediction,” international\nconference on pattern recognition, 2017.\n[5] N. Dalal and B. Triggs, “Histograms of oriented gradi-\nents for human detection,” pp. 886–893, 2005.\n[6] Oncel Tuzel, Fatih Porikli, and Peter Meer, “Pedestrian\ndetection via classiﬁcation on riemannian manifolds,”\nTPAMI, vol. 30, no. 10, pp. 1713–1727, 2008.\n[7] S. Zhang, G. Wu, J. P. Costeira, and J. M. F. Moura,\n“Fcn-rlstm: Deep spatio-temporal neural networks for\nvehicle counting in city cameras,” in ICCV, Oct 2017,\npp. 3687–3696.\n[8] X. Feng, X. Shi, and D. Yeung, “Spatiotemporal model-\ning for crowd counting in videos,” inICCV. IEEE, 2017,\npp. 5161–5169.\n[9] Deepak Babu Sam, Shiv Surya, and R. Venkatesh Babu,\n“Switching convolutional neural network for crowd\ncounting,” in CVPR, July 2017.\n[10] Y . Li, X. Zhang, and D. Chen, “Csrnet: Dilated con-\nvolutional neural networks for understanding the highly\ncongested scenes,” in CVPR, 2018, pp. 1091–1100.\n[11] Daniel D. Onoro-Rubio and R. L ´opez-Sastre, “Towards\nperspective-free object counting with deep learning,” in\nECCV. Springer, 2016, pp. 615–629.\n[12] J. Liu, C. Gao, D. Meng, and A. Hauptmann, “Deci-\ndenet: counting varying density crowds through atten-\ntion guided detection and density estimation,” inCVPR,\n2018, pp. 5197–5206.\n[13] M. Tayyab H. Idrees, K. Athrey, D. Zhang, S. Al-\nmaadeed, N. Rajpoot, and M. Shah, “Composition loss\nfor counting, density map estimation and localization in\ndense crowds.,” arXiv: Computer Vision and Pattern\nRecognition, 2018.\n[14] Max Jaderberg, Karen Simonyan, Andrew Zisserman,\net al., “Spatial transformer networks,” in Advances in\nneural information processing systems, 2015, pp. 2017–\n2025.\n[15] Dong Chen, Gang Hua, Fang Wen, and Jian Sun, “Su-\npervised transformer network for efﬁcient face detec-\ntion,” in ECCV. Springer, 2016, pp. 122–138.\n[16] Yuanyi Zhong, Jiansheng Chen, and Bo Huang, “To-\nward end-to-end face recognition through alignment\nlearning,” IEEE signal processing letters , vol. 24, no.\n8, pp. 1213–1217, 2017.\n[17] Wanglong Wu, Meina Kan, Xin Liu, Yi Yang, Shiguang\nShan, and Xilin Chen, “Recursive spatial transformer\n(rest) for alignment-free face recognition,” in CVPR,\n2017, pp. 3772–3780.\n[18] Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli\nOuyang, and Liang Lin, “Crowd counting using\ndeep recurrent spatial-aware network,” arXiv preprint\narXiv:1807.00601, 2018.\n[19] Karunya Tota and Haroon Idrees, “Counting in dense\ncrowds using deep features,” 2015.\n[20] A. B. Chan, Zhang-Sheng John Liang, and N. Vascon-\ncelos, “Privacy preserving crowd monitoring: Counting\npeople without people models or tracking,” in CVPR,\nJune 2008, pp. 1–7.\n[21] S. An, W. Liu, and S. Venkatesh, “Face recognition us-\ning kernel ridge regression,” in CVPR, June 2007, pp.\n1–7.\n[22] Ke Chen, Chen Change Loy, Shaogang Gong, and Tao\nXiang, “Feature mining for localised crowd counting,”\nin In BMVC.\n[23] K. Chen, S. Gong, T. Xiang, and C. C. Loy, “Cumulative\nattribute space for age and crowd density estimation,” in\nCVPR, June 2013, pp. 2467–2474.\n[24] V . Pham, T. Kozakaya, O. Yamaguchi, and R. Okada,\n“Count forest: Co-voting uncertain number of targets\nusing random forest for crowd density estimation,” in\nICCV, Dec 2015, pp. 3253–3261."
}