{
  "title": "Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale",
  "url": "https://openalex.org/W4310854123",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2513088188",
      "name": "Laurent Sartran",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2164707803",
      "name": "Samuel Barrett",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2509308542",
      "name": "Adhiguna Kuncoro",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A4226351232",
      "name": "Miloš Stanojević",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A297118547",
      "name": "Phil Blunsom",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2107310219",
      "name": "Chris Dyer",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6784217290",
    "https://openalex.org/W6791396022",
    "https://openalex.org/W2308720496",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W4229781645",
    "https://openalex.org/W6725939724",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2190736972",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W6811063797",
    "https://openalex.org/W2098050104",
    "https://openalex.org/W6810220367",
    "https://openalex.org/W3022853932",
    "https://openalex.org/W1818785862",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2554915555",
    "https://openalex.org/W2949629417",
    "https://openalex.org/W3117738520",
    "https://openalex.org/W6760434389",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W6771313149",
    "https://openalex.org/W3176899693",
    "https://openalex.org/W2970378492",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6604455236",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W3174281149",
    "https://openalex.org/W2123893795",
    "https://openalex.org/W3153543512",
    "https://openalex.org/W6754880608",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W6769722308",
    "https://openalex.org/W6639082767",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W6766668698",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W3129394092",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W6747692789",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W4237040408",
    "https://openalex.org/W3014096773",
    "https://openalex.org/W2885212328",
    "https://openalex.org/W2559655401",
    "https://openalex.org/W4233907442",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4385573607",
    "https://openalex.org/W2987266335"
  ],
  "abstract": "Abstract We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.",
  "full_text": null,
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9068423509597778
    },
    {
      "name": "Computer science",
      "score": 0.8671180009841919
    },
    {
      "name": "Language model",
      "score": 0.6682465076446533
    },
    {
      "name": "Transformer",
      "score": 0.6584039926528931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5788378715515137
    },
    {
      "name": "Natural language processing",
      "score": 0.5766025185585022
    },
    {
      "name": "Sentence",
      "score": 0.5439003705978394
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.5135262608528137
    },
    {
      "name": "Syntax",
      "score": 0.4482637643814087
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ]
}