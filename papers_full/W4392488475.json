{
  "title": "COLA: Cross-city Mobility Transformer for Human Trajectory Simulation",
  "url": "https://openalex.org/W4392488475",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2047520290",
      "name": "Wang Yu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A4226842711",
      "name": "Zheng, Tongya",
      "affiliations": [
        "Hangzhou City University",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2370663916",
      "name": "Liang, Yuxuan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2351326629",
      "name": "Liu, Shunyu",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1683047014",
      "name": "Song, Mingli",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2884738862",
    "https://openalex.org/W4287179788",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W3092103025",
    "https://openalex.org/W3157716083",
    "https://openalex.org/W2788114581",
    "https://openalex.org/W3080501557",
    "https://openalex.org/W2105767494",
    "https://openalex.org/W1964461063",
    "https://openalex.org/W4280606008",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W3012808657",
    "https://openalex.org/W3177200443",
    "https://openalex.org/W4382449675",
    "https://openalex.org/W4290944372",
    "https://openalex.org/W2104167780",
    "https://openalex.org/W2808478781",
    "https://openalex.org/W3175110359",
    "https://openalex.org/W4312703862",
    "https://openalex.org/W3127001596",
    "https://openalex.org/W3213170289",
    "https://openalex.org/W2966783050",
    "https://openalex.org/W2463983882",
    "https://openalex.org/W3171998492",
    "https://openalex.org/W2329660289",
    "https://openalex.org/W2911752602",
    "https://openalex.org/W2617955281",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W4290943391",
    "https://openalex.org/W4321648808",
    "https://openalex.org/W4312794196",
    "https://openalex.org/W4327662934",
    "https://openalex.org/W2991288769",
    "https://openalex.org/W4214576162",
    "https://openalex.org/W2796157539",
    "https://openalex.org/W1626398438",
    "https://openalex.org/W3103904664",
    "https://openalex.org/W4256361765"
  ],
  "abstract": "Human trajectory data produced by daily mobile devices has proven its\\nusefulness in various substantial fields such as urban planning and epidemic\\nprevention. In terms of the individual privacy concern, human trajectory\\nsimulation has attracted increasing attention from researchers, targeting at\\noffering numerous realistic mobility data for downstream tasks. Nevertheless,\\nthe prevalent issue of data scarcity undoubtedly degrades the reliability of\\nexisting deep learning models. In this paper, we are motivated to explore the\\nintriguing problem of mobility transfer across cities, grasping the universal\\npatterns of human trajectories to augment the powerful Transformer with\\nexternal mobility data. There are two crucial challenges arising in the\\nknowledge transfer across cities: 1) how to transfer the Transformer to adapt\\nfor domain heterogeneity; 2) how to calibrate the Transformer to adapt for\\nsubtly different long-tail frequency distributions of locations. To address\\nthese challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA)\\nwith a dedicated model-agnostic transfer framework by effectively transferring\\ncross-city knowledge for human trajectory simulation. Firstly, COLA divides the\\nTransformer into the private modules for city-specific characteristics and the\\nshared modules for city-universal mobility patterns. Secondly, COLA leverages a\\nlightweight yet effective post-hoc adjustment strategy for trajectory\\nsimulation, without disturbing the complex bi-level optimization of\\nmodel-agnostic knowledge transfer. Extensive experiments of COLA compared to\\nstate-of-the-art single-city baselines and our implemented cross-city baselines\\nhave demonstrated its superiority and effectiveness. The code is available at\\nhttps://github.com/Star607/Cross-city-Mobility-Transformer.\\n",
  "full_text": "COLA: Cross-city Mobility Transformer for Human Trajectory\nSimulation\nYu Wang\nyu.wang@zju.edu.cn\nZhejiang University\nHangzhou, China\nTongya Zhengâˆ—\ndoujiang_zheng@163.com\nZhejiang University,\nHangzhou City University\nHangzhou, China\nYuxuan Liang\nyuxliang@outlook.com\nHong Kong University of Science and\nTechnology (Guangzhou)\nGuangzhou, China\nShunyu Liu\nliushunyu@zju.edu.cn\nZhejiang University\nHangzhou, China\nMingli Song\nbrooksong@zju.edu.cn\nZhejiang University, Shanghai\nInstitute for Advanced Study of\nZhejiang University\nHangzhou, China\nABSTRACT\nHuman trajectory data produced by daily mobile devices has proven\nits usefulness in various substantial fields such as urban planning\nand epidemic prevention. In terms of the individual privacy concern,\nhuman trajectory simulation has attracted increasing attention from\nresearchers, targeting at offering numerous realistic mobility data\nfor downstream tasks. Nevertheless, the prevalent issue of data\nscarcity undoubtedly degrades the reliability of existing deep learn-\ning models. In this paper, we are motivated to explore the intriguing\nproblem of mobility transfer across cities, grasping the universal pat-\nterns of human trajectories to augment the powerful Transformer\nwith external mobility data. There are two crucial challenges aris-\ning in the knowledge transfer across cities: 1) how to transfer the\nTransformer to adapt for domain heterogeneity ; 2) how to calibrate\nthe Transformer to adapt for subtly different long-tail frequency distri-\nbutions of locations . To address these challenges, we have tailored a\nCross-city mObiLity trAnsformer (COLA) with a dedicated model-\nagnostic transfer framework by effectively transferring cross-city\nknowledge for human trajectory simulation. Firstly, COLA divides\nthe Transformer into the private modules for city-specific character-\nistics and the shared modules for city-universal mobility patterns.\nSecondly, COLA leverages a lightweight yet effective post-hoc ad-\njustment strategy for trajectory simulation, without disturbing the\ncomplex bi-level optimization of model-agnostic knowledge trans-\nfer. Extensive experiments of COLA compared to state-of-the-art\nsingle-city baselines and our implemented cross-city baselines have\ndemonstrated its superiority and effectiveness. The code is available\nat https://github.com/Star607/Cross-city-Mobility-Transformer.\nâˆ—Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0171-9/24/05. . . $15.00\nhttps://doi.org/10.1145/3589334.3645469\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Modeling methodologies ;\nModeling and simulation ; Model development and analysis ;\nKEYWORDS\nHuman mobility; Transfer learning; Simulation; Transformer\nACM Reference Format:\nYu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, and Mingli Song. 2024.\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation.\nIn Proceedings of the ACM Web Conference 2024 (WWW â€™24), May 13â€“17,\n2024, Singapore, Singapore. ACM, New York, NY, USA, 12 pages. https://doi.\norg/10.1145/3589334.3645469\n1 INTRODUCTION\nThe mobile internet has offered significant convenience for mo-\nbile devices to record individual mobility trajectories, which has\nshown its usage in various fields such as urban planning [33, 46],\ntraffic control [30, 34] and epidemic prevention [32]. For example,\nleveraging the epidemiological model with mobility networks can\nconduct detailed analysis and counterfactual experiments to inform\neffective and equitable policy response for COVID-19 [3]. The con-\nvenience and usefulness of mobility data have also aroused the com-\nprehensive concern of the individual privacy, which facilitates the\nwidespread demands of human trajectory simulation [19]. There-\nfore, simulating human mobility behaviors necessitates modeling\nuser intentions from a collective perspective rather than an indi-\nvidual one, striking a balance between generating realistic human\ntrajectories and preserving individual privacy in the meanwhile.\nRecent deep learning models [9, 10, 14, 20, 43â€“45] have largely\npromoted the synthetic quality of human trajectories based on\nthe advanced sequence generation techniques. On the one hand,\nrecurrent models [9, 14, 20] involve the inductive bias of human\ntrajectory sequence. DeepMove [9] devises an attention mechanism\nfor the long history of individual trajectories to retrieve related\ninformation; CGE [14] exploits the spatio-temporal contextual in-\nformation of individual trajectories with a unified location graph.\nNonetheless, recurrent models are difficult to generate the high-\nfidelity trajectories from scratch because they rely on historical\narXiv:2403.01801v1  [cs.LG]  4 Mar 2024\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\n(a) Universal patterns (b) Top50 location distribution\nNew York\nYahoo (Japan)\nHome\nFirm\nCompany\nStore\nHome Shop\nFigure 1: Motivation and challenges of human trajectory\nsimulation across cities.\ntrajectory. On the other hand, adversarial-based methods [10, 43â€“\n45] incorporate high-order semantics of human mobility such as\ngeographical relations [10], activity dynamics [44] and Maslowâ€™s\nHierarchy of Needs [45], and meanwhile maximize the long-term\ngeneration reward based on a two-player min-max game. Despite\ntheir efforts, the severe scarcity of human trajectory data will lead\nthese dedicated models to a sub-optimal solution.\nThe prevalent issue of data scarcity motivates us to transfer\nthe universal patterns of human mobility from abundant external\ncities to help improve the synthetic quality on our target city. As\nillustrated in Fig. 1(a), daily activities of urban citizens are usu-\nally driven by similar intentions including working, entertainment,\ncommuting, shopping, rest, etc. These common intentions exhibit\nuniversal patterns of human trajectories across different cities and\nresult in the similar long-tail frequency distributions of locations,\nas shown in Fig. 1(b). The data scarcity of human trajectory can\nbe largely alleviated if the mobility knowledge across cities can be\nappropriately transferred.\nHowever, cross-city mobility transfer poses rather particular\nchallenges compared to spatio-temporal transfer across cities [22,\n28, 36], which work on air quality metrics [36], pandemic cases [28]\nor traffic speed [22]. Firstly, locations of the external city hardly\ninteract with locations of the target city, causing the location em-\nbeddings non-transferrable across cities, which is called domain\nheterogeneity in knowledge transfer. In contrast, spatio-temporal\ntransfer usually deals with metrics of the same feature space like air\nquality metrics, alleviating the transfer difficulty. Secondly, differ-\nent cities present subtly different long-tail frequency distributions\nof locations due to the urban culture or geographical effects. The\nsubtle differences necessitate carefully calibrating existing overcon-\nfident deep neural networks [18] during the knowledge transfer\nprocess. The aforementioned challenges require us to rethink the\nprinciples of mobility transfer across cities.\nTo address these challenges, we introduce the powerful Trans-\nformer [29, 31] block in a transfer learning framework to learn\nthe universal patterns of human mobility based on the attention\nsimilarities between tokens (locations), which has demonstrated\nits generalization ability in many NLP tasks. Concretely, we have\ntailored a Cross-city mObiLity trAnsformer with a model-agnostic\ntransfer framework [11, 28], dubbed COLA, to deal with the domain\nheterogeneity and different long-tail frequency distributions of lo-\ncations across cities. Firstly, COLA divides the Transformer into\nprivate modules accounting for city-specific characteristics and\nshared modules accounting for city-universal knowledge, named\nHalf-open Transformer. It places the attention computation mecha-\nnism into shared modules to better facilitate the pattern transfer\namong urban human trajectories. Once transferred, the target city\ncan exhibit its specific mobility behaviors with the private modules\nincluding non-transferrable location embeddings and their latent\nrepresentations. Secondly, COLA aligns its prediction probabili-\nties of locations with the real long-tail frequency distribution in a\npost-hoc manner [26] to remedy the overconfident problems [18].\nCompared to the iterative optimization of re-weighted loss func-\ntions [48, 50], the post-hoc adjustment of the prediction probabil-\nities works only for the target city after the mobility transfer is\nfinished, leaving the minimum changes to the complex optimization\nof the transfer framework. COLA can effectively adapt the powerful\nTransformer for cross-city mobility transfer with these dedicated\ndesigns.\nIn conclusion, our main contributions are summarized as follows:\nâ€¢We investigate the intriguing problem of cross-city human\ntrajectory simulation, with identification of particular chal-\nlenges compared to spatio-temporal transfer across cities.\nâ€¢We have designed the dedicated method COLA with a model-\nagnostic transfer framework, which leverages our proposed\nHalf-open Transformer to split private and shared modules\nand calibrates the prediction probabilities for city-specific\ncharacteristics.\nâ€¢We conduct extensive experiments on human trajectory\ndatasets of four cities and demonstrate the superiority of\nCOLA compared to state-of-the-art single-city baselines and\nour implemented cross-city baselines.\n2 RELATED WORK\nHuman Trajectory Simulation. Researches on human trajectory\nsimulation can be categorized as Markov-based methods, RNN-\nbased methods and Adversarial-based methods. (1) Markov-based\nmethods [13, 42] characterizing human trajectory with finite pa-\nrameters of clear physical meaning are satisfactory in some cases\nbut unable to capture complex patterns. (2) RNN-based methods [8,\n9, 14, 20] can be directly employed to generate trajectories, while\nthey are trained for short-term goals (location prediction, which\nemphasizes recovering user-specific real data) and fail to generate\nhigh-quality long-term trajectories (trajectory simulation, which\nhighlights replicating the characteristics of user-anonymous real\ndata). (3) Adversarial-based methods [6, 10, 43â€“45] can efficiently\ncapture complex mobility patterns leveraging prior knowledge of\nhuman trajectories, while they struggle to achieve satisfactory per-\nformance on data-scarce cities.\nCross-city Transfer Learning. Knowledge transfer [15, 23] aims\nto tackle machine learning problems in data-scarce scenarios. In\nthe field of urban computing [ 1, 2, 16], itâ€™s an ongoing research\nchallenge to achieve cross-city knowledge transfer, reduction of\ndata collection costs and higher learning efficiency. Spatio-temporal\nmethods [24, 28, 35, 36, 41] focus on city transfer within homoge-\nneous domains, where source and target domains share the same\nfeature space. However, mobility transfer is more challenging due\nto the respective geographic semantics of different cities in the\nlocations of the source city and the target city, i.e., heterogeneous\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nSource City 1\nSource City 2\nTarget City\nSource City 2 Embedding\nTarget City Embedding\nLayer Norm\nMLP\nLayer Norm\n+ +\n2 Ã—\nLayerNorm\nMLP\nLayerNorm+ +\n2 Ã—\nLayer Norm\nMLP\nLayer Norm\n+ +\n2 Ã—\nLayer Norm\nMLP\nLayer Norm\n+ +\n2 Ã—\nSource City 1 Loss\nInternal Loss\nMeta Gradient\nSource City 2 Loss\nInternal Loss\nMeta Gradient\nInternal Update\nMeta Update\nMeta Clone\nTarget City Loss\nInternal Loss\nHuman Trajectory \nSimulation\nPrivate Module\n(â… ) (â…¢)\n(â…¥) Post-hoc Adjustment\nAttention\nQKV\nAttention\nQKV\nAttention\nQKV\nAttention\nQKV\nSource City 1 Embedding\nMeta Model Embedding\n(â…¡)\n(â…¡)\n(â…£)\n(â…¤)\nFigure 2: The overall framework of COLA. (I) Initialize the shared parameters of the source model with the meta model. (II)\nOptimize the source model with its internal loss. (III) Update the meta model based on the gradient evaluated on the source\ncity. (IV) Initialize the shared parameters of the target model with the meta model updated by all source cities. (V) Optimize the\ntarget model with its internal loss. (VI) Simulate human trajectories with Post-hoc Adjustment technique.\ndomains. Particularly, He et al. [19] plan the mobility of a new city\nin a top-down paradigm with abundant annotated data, while we\nfocus on learning the human mobility patterns from the external\ntrajectory data in data scarce scenario.\nLong-tail Learning. The data bias [5, 37] caused by long-tail dis-\ntribution of labels can be mitigated by re-sampling methods [4, 25,\n49], class-sensitive learning [7, 48, 50] and logit adjustment meth-\nods [21, 26, 38]. However, in cross-city human trajectory simulation\ntask, itâ€™s crucial not only to avoid undesirable bias towards dom-\ninant labels but also accurately capture the city-specific long-tail\ncharacteristics across multiple cities.\n3 METHODOLOGY\nIn this section, we propose a Cross-city mObiLity trAnsformer\n(COLA) for human trajectory simulation, whose framework is pre-\nsented in Fig. 2. In this framework, we design Half-open Trans-\nformer dividing private and shared parameters to adapt for domain\nheterogeneity. During simulation, we leverage Post-hoc Adjustment\nto calibrate the overconfident probabilities of the model, where the\ndynamic effects for different locations are further analyzed.\n3.1 Overall Framework of COLA\nCOLA achieves the knowledge transfer across cities based on a dedi-\ncated model-agnostic transfer framework. The dataset for cross-city\nhuman trajectory simulation includes meta training set Mtr and\nmeta test set Mğ‘¡ğ‘’ . Specifically, Mtr = {Dğ‘˜ |ğ‘˜ âˆˆ [ğ¾]}is from\nğ¾ source cities, where Dğ‘˜ = (Dğ‘˜\ntrain,Dğ‘˜\ntest), and Mğ‘¡ğ‘’ = {ËœD}=\n{( ËœDtrain, ËœDtest)}. D= {xğ‘š |ğ‘š âˆˆ[ğ‘€]}represents real-world mo-\nbility trajectories of each city, whereğ‘€is the number of trajectories.\nEach trajectory x = (x1,x2,Â·Â·Â· ,xğ‘‡ )is a spatiotemporal sequence\nwith length ğ‘‡. Our goal is leveraging common patterns learnt from\ntrajectories of other cities to improve the quality of simulated tra-\njectories on our target city.\nDifferent from the classical meta learning [ 11] sharing all pa-\nrameters of models, the parameters in our Half-open Transformer\nTare divided into private parameters from location embeddings\nand private attention module for city-specific characteristics, and\nshared parameters from shared attention module for city-universal\npatterns to handle domain heterogeneity. As illustrated in Fig. 2, the\nshared parameters of the meta model are updated by source models,\nwhich then initialize the shared parameters of the target model to\neffectively transfer the universal patterns from source cities to the\ntarget city. Meanwhile, the private parameters of source models and\nthe target model are consistently updated through Internal Update .\nLet Î˜meta, Î˜src, and Î˜tgt be the parameters of Tfor the meta,\nsource and target models respectively. In order to capture the uni-\nversal patterns coexisting across cities, the shared parameters of\neach source model are initialized from the current meta model. The\nprocess of cloning the shared parameters from the meta model is\nnamed Meta Clone . In the framework of COLA, a source model\nfirstly conducts Meta Clone as follows:\nÎ˜src = {ğœƒ|âˆ€ğœƒ âˆˆÎ˜shared\nmeta }âˆª{ğœƒ|âˆ€ğœƒ âˆˆÎ˜private\nsrc }, (1)\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\nâ€¦\nClassifierAttention Module\nğªğ‘¡\nğ¤ğ‘¡\nğ¯ğ‘¡\nâ„“1\nâ„“2\nâ„“n\nProj\nğ–ğ‘\nğ¡ğ‘¡\nprivate\nğ¡ğ‘¡\nshared ğœ¶ğ‘¡\nà·\nğ’•â€²â‰¤ğ’•\nğœ¶ğ’•ğ’•â€²ğ¯ğ’•â€²\nProj\nğ¡ğ‘¡\nğ¿\nğ–ğ‘˜\nğ–ğ‘£\nCausal\nAttention\nğ¡ğ‘¡\nğ¿-1\nFigure 3: Half-open attention in the Transformer.\nwhere Î˜src refers to Î˜ğ‘˜src for simplicity. The shared parameters\ncloned from the meta model provide the source model with a good\nstarting point for training. Subsequently, the source model performs\nInternal Update through the trajectory pretraining task:\nÎ˜src = Î˜src âˆ’ğ›¼srcâˆ‡Î˜src L(TÎ˜src (Dğ‘˜\ntrain)), (2)\nwhere ğ›¼src is the learning rate for the source model and Lis the\nloss function defined as Eq. (12). Once the optimization is complete,\nthe source model acquires the prior knowledge of human mobility\nin this city. Then the meta model executes Meta Update , guided by\nthe meta gradient evaluated on the source city, to enable the rapid\nadaptation of the target city:\nÎ˜meta = Î˜meta âˆ’ğ›¼metaâˆ‡Î˜src L(TÎ˜src (Dğ‘˜\ntest)), (3)\nwhere ğ›¼meta is the learning rate for the meta model. TheMeta Clone,\nInternal Update , and Meta Update are successively performed by\nall source cities, resulting in the meta model that incorporates the\nuniversal patterns observed in all the source cities. Afterwards, the\ntarget model employs Meta Clone to efficiently learn the common\npatterns from source cities and perform Internal Update based on\nthe data of the target city:\nÎ˜tgt = {ğœƒ|âˆ€ğœƒ âˆˆÎ˜shared\nmeta }âˆª{ğœƒ|âˆ€ğœƒ âˆˆÎ˜private\ntgt }, (4)\nÎ˜tgt = Î˜tgt âˆ’ğ›¼tgtâˆ‡Î˜tgt L(TÎ˜tgt (ËœDtrain)), (5)\nwhere ğ›¼tgt is the learning rate for the target model. There are three\ndifferent loops ğ¸meta, ğ¸src and ğ¸tgt for optimization. Unlike the\nclassical meta learning where target domain updates ğ¸tgt epochs,\nÎ˜private\ntgt updates ğ¸meta Ã—ğ¸tgt epochs to synchronize the update with\nmeta model. After the optimization of Î˜tgt, COLA samples Ë†xğ‘¡+1\nwith the following Post-hoc Adjustment:\nË†xğ‘¡+1 âˆ¼T( x1:ğ‘¡ )/ğœ‹ğœ, ğ‘¡+1 â‰¤ğ‘‡, (6)\nwhere ğœ‹ is the empirical location frequencies on the training sam-\nples, ğœ is a hyperparameter and ğœ > 0. The details of Post-hoc\nAdjustment are illustrated in Section 3.3 and the overall algorithm\nof COLA is presented in Alg. 1.\n3.2 Half-open Transformer\nIn the Half-open Transformer, in addition to location embeddings,\nValue of the attention module is set as private for domain het-\nerogeneity adaptation, because they encapsulates the city-specific\ncharacteristics. Query and Key of the attention module compute\nthe attention weights of a location to all locations appeared in the\nhistorical trajectory to capture city-universal patterns, suggesting\ntheir capability for sharing across cities.\nLet P= {â„“ğ‘– |ğ‘– âˆˆ[ğ‘]}denote the set of city locations â„“, where\nğ‘ is the number of locations. For a trajectory x = (x1,x2,Â·Â·Â· ,xğ‘‡ ),\nwe discrete each location xğ‘¡ in the trajectory and encode it with\none-hot vectors: hğ‘¡ = Embprivate (xğ‘¡ ),ğ‘¡ âˆˆ[ğ‘‡]. Then, we map it into\nprivate and shared representations by Proj for further extracting\nprivate characteristics and shared patterns in the trajectories:\nhprivate\nğ‘¡ = Projprivate (hğ‘¡ ),\nhshared\nğ‘¡ = Projshared (hğ‘¡ ),\n(7)\nwhere Proj is a projection function comprising of an identity layer\nor a position-wise MLP to employ the non-linear capability. Based\non the distinct representation of private characteristics and shared\npatterns, we employ a causal self-attention mechanism to generate\nthe trajectory sequentially. The causal attention allows locations to\nonly attend to preceding locations observed in trajectories during\ntraining, which ensures that the model does not have access to\nthe future trajectory when predicting, making the training process\nmore efficient [29]. Therefore, we project hshared\nğ‘¡ and hprivate\nğ‘¡ re-\nspectively with independent operations of two category into Query\nvectors qğ‘¡ , Key vectors kğ‘¡ and Value vectors vğ‘¡ :\nvğ‘¡ = ğ‘“(hprivate\nğ‘¡ ; Wğ‘£),\n(qğ‘¡,kğ‘¡ )= ğ‘“(hshared\nğ‘¡ ; Wğ‘,Wğ‘˜ ).\n(8)\nwhere Wğ‘ and Wğ‘˜ are shared parameters like hshared\nğ‘¡ , while Wğ‘£\nare private parameters similar to hprivate\nğ‘¡ , all of them are weighted\nmatrices. After that, we utilize causal scaled dot-product attention\non them to derive the attention coefficients for all appeared loca-\ntions and compute the weighted sum of Value vectors to serve as\nthe feature representation of the past trajectory:\nğ›¼ğ‘¡ğ‘¡ â€² = exp(qğ‘¡ Â·kğ‘¡â€²/\nâˆš\nğ‘‘)\nÃ\nğ‘¡â€²â‰¤ğ‘¡ exp(qğ‘¡ Â·kğ‘¡â€²/\nâˆš\nğ‘‘)\n,\nzğ‘¡ = MultiHead(\nâˆ‘ï¸\nğ‘¡â€²â‰¤ğ‘¡\nğ›¼ğ‘¡ğ‘¡ â€²vğ‘¡â€²)Wğ‘œ,\n(9)\nwhere ğ‘‘is the dimension ofKey vectors and Wğ‘œ is the weighted ma-\ntrix for output. Furthermore, we use stacking operations to model\nthe relationships from different subspaces and generate a com-\nprehensive representation of the past trajectory. Let hğ¿âˆ’1\nğ‘¡ be the\nrepresentation at ğ¿âˆ’1-th layer. The weighed representation at this\nlayer zğ¿âˆ’1\nğ‘¡ is added to hğ¿âˆ’1\nğ‘¡ and then subjected to layer normal-\nization to obtain the intermediate representation Â¯hğ‘¡ . Subsequently,\nthe representation at ğ¿-th layer hğ¿\nğ‘¡ can be obtained with a non-\nlinear operation followed by another layer normalization. It can be\nexpressed as follows:\nÂ¯hğ‘¡ = LayerNorm(hğ¿âˆ’1\nğ‘¡ +zğ¿âˆ’1\nğ‘¡ ),\nhğ¿\nğ‘¡ = LayerNorm(MLP(Â¯hğ‘¡ )).\n(10)\nThen we use a linear layer to process the feature to obtain the\nlogits of all locations. The logit at location â„“ğ‘– can be expressed\npğ‘š,ğ‘–\nğ‘¡+1 = ğœ”private\nğ‘– hğ¿\nğ‘¡ , whereğœ”private\nğ‘– is private and same as the weights\nof â„“ğ‘– embedding. Furthermore, the predicted probability of location\nâ„“ğ‘– can be obtained after normalizing:\nË†yğ‘š,ğ‘–\nğ‘¡+1 =\nexp(pğ‘š,ğ‘–\nğ‘¡+1)\nÃ\nğ‘— âˆˆ[ğ‘ ]exp(pğ‘š,ğ‘—\nğ‘¡+1 )\n, (11)\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nwhich is used for the post-hoc sampling in the simulation phase.\nDuring the training phase, the Transformer Tis optimized with\nthe following internal loss function:\nL= âˆ’ 1\nğ‘€ğ‘‡\nğ‘€âˆ‘ï¸\nğ‘š=1\nğ‘‡ âˆ’1âˆ‘ï¸\nğ‘¡=1\nyğ‘š\nğ‘¡+1 Â·log(Ë†yğ‘š\nğ‘¡+1). (12)\n3.3 Post-hoc Adjustment\nOn the one hand, as illustrated in Fig. 1 (b), the overall visited\nfrequency distributions of locations appeared in trajectories from\nfour cities all exhibit long-tailed characteristics. Under the circum-\nstances, some locations are visited occasionally but cannot be disre-\ngarded. Nevertheless, the paucity of occasionally visited locations\nposes a significant challenge in terms of generalisation. Further-\nmore, naive learning on such data is susceptible to an undesirable\nbias towards dominant locations [26]. Due to the prevalent overcon-\nfident problem of deep learning models [18], our proposed Trans-\nformer even exaggerates the real long-tailed frequency distribution\nof locations that severely overlooks the low-frequency locations.\nOn the other hand, the long-tailed visited frequency distribu-\ntions of locations from multiple cities display subtly differences,\nespecially in the enlarged part of Fig. 1 (b), which is crucial in de-\ntermining the appropriate long-tail learning method for addressing\nthe class imbalance problem. Traditional long-tail learning methods\nencompass two strands of work: post-hoc normalisation of class\nweights and loss modification to account for varying class penalties.\nBecause loss modification requires loss functions with different\nweights for models of various cities, which hinders the transfer\nof universal mobility patterns across cities and also increase the\ntraining cost, it is unsuitable for calibrating the imbalance of loca-\ntions in trajectories during training. Therefore, COLA leverages the\npost-hoc adjustment method to calibrate our overconfident proba-\nbilities of locations instead of pursuing the optimal Bayesian error\nof long-tailed learning [26], written as follows:\nËœyğ‘š,ğ‘–\nğ‘¡+1 =\nexp(pğ‘š,ğ‘–\nğ‘¡+1)/ğœ‹ğœ\nğ‘–\nÃ\nğ‘— âˆˆ[ğ‘ ]exp(pğ‘š,ğ‘—\nğ‘¡+1 )/ğœ‹ğœ\nğ‘—\n,ğœ > 0. (13)\nThe penalization term ğœ‹ğœ\nğ‘– towards high-frequency locations offers\na lightweight yet effective way to simulate the real long-tailed\nfrequency distribution of locations. During the simulation phase,\nwe sample Ë†xğ‘š\nğ‘¡+1 âˆ¼Ëœyğ‘š\nğ‘¡+1 from T(xğ‘š\n1:ğ‘¡ )following the above calibrated\nprobabilities of locations.\nProposition 1. Suppose that the probability density function of\nlocations follows Zipfâ€™s law ğœ‹(ğ‘¥) âˆ¼ğ‘ğ‘¥âˆ’ğ›¾,ğ›¾ > 0, ğ‘¥ âˆˆ N+is the\nindex of a location, the post-hoc adjustment dynamically scales the\npair-wise probabilities of two locations as: Ëœyğ‘š,ğ‘–\nğ‘¡+1\nËœyğ‘š,ğ‘—\nğ‘¡+1\n= Ë†yğ‘š,ğ‘–\nğ‘¡+1\nË†yğ‘š,ğ‘—\nğ‘¡+1\nÂ·(ğ‘–/ğ‘—)ğœ Â·ğ›¾ .\nLet ğ‘– < ğ‘— for simplicity, where ğ‘–,ğ‘— are indices of locations sorted\nby their frequencies. It indicates that high-frequency locations are\npenalized in proportion to their frequency, while low-frequency\nlocations are rewarded in contrast. In situations where both loca-\ntions exhibit similar frequencies, the unadjusted probabilities of the\nmodels play a crucial role.\nAlgorithm 1 COLA\nRequire: source cities and the target city dataset: Mtr and Mte;\nlearning rates: ğ›¼src, ğ›¼meta, ğ›¼tgt; Training epochs: ğ¸meta, ğ¸src, ğ¸tgt.\n1: Initialize Î˜meta randomly\n2: for epoch ğ‘’ğ‘š âˆˆğ¸meta do\n3: for each city Dğ‘˜ âˆˆMtr do\n4: Meta Clone for Î˜ğ‘˜src with Eq. (1)\n5: # Iterate ğ¸src epochs to update Î˜ğ‘˜src\n6: Internal Update for Î˜ğ‘˜src with Eq. (2)\n7: Meta Update for Î˜meta with Eq. (3)\n8: end for\n9: Meta Clone for Î˜tgt with Eq. (4)\n10: # Iterate ğ¸tgt epochs to update Î˜tgt\n11: Internal Update for Î˜tgt with Eq. (5)\n12: end for\n13: Perform simulation with Eq. (6)\nTable 1: The statistics of four datasets.\nDataset # Users # Locations # Visits # AvgStep/Day\nGeoLife 153 32,675 34,834 8.9\nYahoo 10,000 16,241 188,061 18.8\nNew York 1,189 9,387 19,040 6.9\nSingapore 1,461 11,509 38,522 7.0\n4 EXPERIMENT\nTo demonstrate the effectiveness of the proposed COLA method,\nwe conduct extensive experiments to answer the following research\nquestions:\nâ€¢RQ1: How does COLA perform compared to state-of-the-art\nsingle-city baselines and our implemented cross-city baselines\nin human trajectory simulation task?\nâ€¢RQ2: How do different components of COLA contribute to the\nfinal performance?\nâ€¢RQ3: Can COLA generate synthetic data of high quality for\npractical applications?\nâ€¢RQ4: How do hyperparameter settings influence the performance\nof COLA?\n4.1 Experimental Setup\n4.1.1 Datasets. We evaluate the performance of COLA and base-\nlines on four public datasets: GeoLife [ 51], Yahoo (Japan) [ 39],\nNew York and Singapore [40]. The four datasets are preprocessed\nfollowing the protocol [ 10] of human trajectory simulation. For\nconvenience and universality of modeling, an hourly time slot is\nadopted as the basic simulation unit. Moreover, only the trajecto-\nries with at least six visit records per day are considered in our\nstudy. For Yahoo, we select 10,000 high-quality trajectories from its\npreprocessd data. The data statistics of preprocessed datasets are\npresented in Table 1. The daily average length of trajectories in New\nYork and Singapore is generally smaller than that in GeoLife and\nYahoo, which aggravates the irregularity and thereby increases the\ndifficulty of modeling. For each dataset, the training, valid and test\nsets follow a ratio of 7:1:2. Details are provided in Appendix A.1.\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\nTable 2: The performance comparison between COLA and baselines for human trajectory simulation. All experimental results\nare conducted over five trials for a fair comparison. Note that a lower JSD value indicates a better performance. AVG is the\naverage rank across six metrics. Bold and underline mean the best and the second-best results. â€œ*\" implies statistical significance\nfor ğ‘ < 0.05 under paired t-test.\nDataset GeoLife Yahoo\nMetrics (JSD)â†“ Distance Radius Duration DailyLoc G-rank I-rank AVG â†“ Distance Radius Duration DailyLoc G-rank I-rank AVG â†“\nMarkov 0.0316 0.1341 0.0135 0.1424 0.0830 0.0931 9.2 0.2008 0.5037 0.1682 0.5031 0.0406 0.0574 7.8\nIO-HMM 0.2384 0.4138 0.0100 0.1124 0.1110 0.0799 9.7 0.5690 0.6772 0.0847 0.6716 0.0388 0.1031 11.3\nDeepMove 0.2064 0.3743 0.0099 0.1269 0.1805 0.0584 8.0 0.5400 0.6775 0.0622 0.6061 0.2590 0.1079 11.3\nGAN 0.2224 0.3887 0.0108 0.1114 0.0812 0.0590 7.5 0.5705 0.6786 0.0852 0.6748 0.0400 0.0989 12.0\nMoveSim 0.2203 0.3851 0.1292 0.1424 0.0860 0.0736 10.7 0.4579 0.5065 0.0767 0.5843 0.0331 0.0838 8.0\nTrajGAIL 0.0527 0.1466 0.0136 0.1130 0.0806 0.0903 8.7 0.2098 0.5545 0.3306 0.5496 0.0358 0.0628 8.3\nCGE 0.3636 0.6637 0.0107 0.1125 0.0783 0.0768 8.8 0.3060 0.5610 0.0853 0.5339 0.0474 0.0740 9.7\nACT-STD 0.2007 0.3613 0.6931 0.6931 0.1752 0.0629 10.8 0.1438 0.3965 0.0839 0.1280 0.6931 0.0740 7.5\nLSTM 0.0305 0.1224 0.0133 0.1496 0.0799 0.0595 7.0 0.0352 0.1702 0.0011 0.0231 0.0326 0.0633 3.5\nSeqGAN 0.0383 0.1294 0.0100 0.1140 0.0793 0.0562 5.2 0.2906 0.2614 0.0255 0.1576 0.0335 0.0608 5.0\nMobFormer 0.0300 0.1220 0.0278 0.2481 0.0776 0.0911 7.3 0.2323 0.3646 0.1423 0.4497 0.0348 0.0552 6.3\nCrossLSTM 0.0300 0.1219 0.0097 0.1118 0.0788 0.0566 2.8 0.0223 0.1695 0.0004 0.0033 0.0358 0.0458 2.8\nCrossSeqGAN 0.2315 0.4083 0.0110 0.1118 0.0794 0.0597 8.2 0.5166 0.6656 0.0718 0.6383 0.0400 0.1031 10.3\nCOLA 0.0294* 0.1205* 0.0096 0.1103 * 0.0780 0.0559* 1.2 0.0161* 0.1462* 0.0002* 0.0029* 0.0294* 0.0447* 1.0\nDataset New York Singapore\nMetrics (JSD)â†“ Distance Radius Duration DailyLoc G-rank I-rank AVG â†“ Distance Radius Duration DailyLoc G-rank I-rank AVG â†“\nMarkov 0.0354 0.0917 0.0060 0.0595 0.1470 0.0693 8.3 0.0245 0.0462 0.0097 0.1506 0.1793 0.0631 9.3\nIO-HMM 0.1630 0.3283 0.0055 0.1166 0.1615 0.0624 11.2 0.1839 0.2524 0.0066 0.1597 0.2000 0.0612 11.3\nDeepMove 0.1632 0.3216 0.0006 0.0266 0.1501 0.0554 8.8 0.1850 0.2569 0.0011 0.0507 0.1031 0.0541 7.7\nGAN 0.1601 0.3010 0.0046 0.0963 0.0914 0.0584 9.3 0.1929 0.2543 0.0059 0.1316 0.1688 0.0556 10.7\nMoveSim 0.1320 0.2365 0.0067 0.3241 0.0813 0.0461 8.2 0.0950 0.1411 0.0043 0.3731 0.1220 0.0539 7.5\nTrajGAIL 0.3588 0.4714 0.0160 0.6899 0.0899 0.0490 11.5 0.2019 0.2707 0.2205 0.6924 0.1627 0.0430 10.7\nCGE 0.1486 0.4002 0.0243 0.4170 0.0995 0.0438 10.2 0.1341 0.4535 0.1279 0.5706 0.1636 0.0524 10.0\nACT-STD 0.1428 0.3341 0.4554 0.6346 0.1420 0.0479 10.8 0.0895 0.1758 0.3271 0.5236 0.1820 0.0542 10.2\nLSTM 0.0265 0.0922 0.0015 0.0322 0.0084 0.0423 3.7 0.0072 0.0271 0.0008 0.0181 0.0146 0.0555 4.3\nSeqGAN 0.0338 0.1088 0.0068 0.1396 0.0154 0.0527 7.8 0.0670 0.1046 0.0020 0.0384 0.0146 0.0541 5.0\nMobFormer 0.0264 0.0917 0.0005 0.0112 0.0080 0.0508 3.3 0.0069 0.0269 0.0053 0.0593 0.0243 0.0555 5.7\nCrossLSTM 0.0266 0.0916 0.0016 0.0271 0.0091 0.0457 4.0 0.0070 0.0269 0.0009 0.0095 0.0181 0.0551 3.8\nCrossSeqGAN 0.1257 0.2332 0.0034 0.0776 0.0089 0.0530 6.8 0.1701 0.2262 0.0040 0.1040 0.0069 0.0555 7.3\nCOLA 0.0263 0.0911* 0.0004 0.0111 0.0078 * 0.0368* 1.0 0.0067* 0.0267* 0.0007 0.0086 * 0.0141 0.0534 1.5\n4.1.2 Baselines. We compare the proposed COLA with the follow-\ning two categories ofstate-of-the-art baselines. Single-City Baselines:\nMarkov [13], IO-HMM [42], LSTM [20], DeepMove [9], GAN [17],\nSeqGAN [43], MoveSim [10], TrajGAIL [6], CGE [14], ACT-STD [44],\nand MobFormer [31]. Cross-City Baselines: CrossLSTM and CrossSe-\nqGAN are two most effective single-city baselines equipped with\nthe classical MAML framework for cross-city transfer learning.\nDetails are provided in Appendix A.2.\n4.1.3 Evaluation Metrics. We adopt six standard metrics as em-\nployed in previous works [10, 27] to assess the quality of simulated\noutcomes. These metrics calculate the mobility trajectory distribu-\ntions from various perspectives: (1) Distance: moving distance of\neach adjacent locations in individual trajectories (spatial perspec-\ntive); (2) Radius: root mean square distance from a location to the\ncenter of its trajectory (spatial perspective); (3) Duration: dwell du-\nration among locations (temporal perspective); (4)DailyLoc: propor-\ntion of unique daily visited locations to the length of the trajectory\nfor everyone (preference perspective); (5) G-rank: visited frequency\nof the top-100 locations (preference perspective); (6) I-rank: the in-\ndividual version of G-rank (preference perspective). We further use\nJensen-Shannon divergence (JSD) [12] to measure the discrepancy\nof the distributions between simulated and real-world trajectories,\nwhich is defined as: JSD(ğ‘âˆ¥ğ‘)= ğ»((ğ‘+ğ‘)/2)âˆ’ 1\n2 (ğ»(ğ‘)+ğ»(ğ‘)),\nwhere ğ‘and ğ‘are two compared distributions, andğ» is the entropy.\nA lower JSD indicates a closer match to the statistical character-\nistics, suggesting a superior simulated outcome. Specifically, we\ncalculate the average rank, denoted asAVG, across these six metrics\nfor a clear comparison.\n4.1.4 Experiment Settings. (1) The single-city baselines are trained\nwith 250 epochs. (2) The cross-city baselines are trained with 5,\n1 and 50 epochs for meta, source city and target city updating,\nwhere the training epochs of the target city is in line with single-\ncity baselines. When a city serves as the target city, the remaining\nthree cities are considered as the source cities. The learning rate\nfor the city models (both source and target cities) and the meta\nmodel is set to 1e-3 and 5e-4, respectively. (3) All baselines use a\nhidden dimension of 96 and a batch size of 32. The other specific\nhyperparameters of the baselines follow the settings reported in\ntheir respective papers. For COLA, the number of linear layers in\nMLP for extracting private and shared patterns is searched over {1,\n2, 3}, and the coefficient ğœ for post-hoc adjustment is searched over\n{0.001, 0.01, 0.1, 0.25, 0.5, 1}.\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nY N S Y+N Y+S N+S ALL\n(a) Distance\n0.00\n0.02\n0.04\n0.06JSD\nY N S Y+N Y+S N+S ALL\n(b) Radius\n0.00\n0.05\n0.10\n0.15\n0.20\nY N S Y+N Y+S N+S ALL\n(c) Duration\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nY N S Y+N Y+S N+S ALL\n(d) DailyLoc\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125JSD\nY N S Y+N Y+S N+S ALL\n(e) G-rank\n0.00\n0.05\n0.10\n0.15\nY N S Y+N Y+S N+S ALL\n(f) I-rank\n0.00\n0.02\n0.04\n0.06\nFigure 4: The performance comparison of COLA on GeoLife\nwith different combinations of source cities. All experimental\nresults are conducted over five trials for a fair comparison.\n4.2 RQ1: Overall Performance\nIn this section, we compare our model with the sing-city baselines\nand our implemented cross-city baselines over the four real-world\ncity datasets in the human trajectory simulation task. We present\nthe average performance of each method under five trials in Table 2.\nThe results yield the following observations:\nâ€¢COLA steadily outperforms baselines. COLA achieves the best\nAVG on all cities with ranking 1st on 21 metrics and ranking 2nd\non 3 metrics over twenty-four metrics of the four datasets. Itâ€™s\nworth noting that COLA consistently ranks first on New York\nwith the lowest number of trajectories and Yahoo with the most,\nwhich validates the simulated data of COLA exhibits exceptional\nfidelity from spatial, temporal and preference perspective.\nâ€¢Cross-city baselines exhibit distinct results. CrossLSTM signifi-\ncantly improves the performance of LSTM on almost all datasets,\nespecially on GeoLife where AVG increases from 7.0 to 2.8. How-\never, CrossSeqGAN deteriorates the performance of SeqGAN\nespecially on Yahoo where AVG drops from 5.0 to 10.3, which is\ndue to its difficulty of convergence with meta-learning, unlike\nLSTM without generative adversarial framework.\nâ€¢Transformer is a highly promising model for human trajectory sim-\nulation. MobFormer, a causal-attention Transformer, averagely\nsurpasses 85% single-city baselines on four cities. On New York,\nit even surpasses two cross-city baselines with a reduction of\nJSD up to 69%. It indicates that the attention mechanism which\ncaptures the global attention to the history sequences are more\nsuitable for the human trajectory simulation task.\nâ€¢The limitation of single-city baselines. Markov performs well in\nspatial metrics but poorly in temporal and preference metrics.\nMost baselines underperform due to their excessive annotation\nrequirements which is unsuitable for data-scarce scenario, e.g.\nIO-HMM and ACT-STD, or due to the modelâ€™s overcomplexity\nwhich hinders its convergence, e.g. GAN, MoveSim, TrajGAIL and\nCGE. Nevertheless, LSTM capturing long-term dependencies and\nSeqGAN based on policy gradient achieve superior performance.\n4.3 RQ2: Ablation Studies\n4.3.1 Performance across different cities. We conduct experiments\non various source cities to explore the performance of transfer\nTable 3: Ablation study on half-open attention (HA) and post-\nhoc adjustment (PO), where NONE means without HA and\nPO. Bold means the best result.\nDataset Method Distance Radius Duration DailyLoc G-rank I-rank\nGeoLife\nNONE 0.0356 0.1447 0.0143 0.1521 0.0920 0.0663\nw/o HA 0.0347 0.1443 0.0126 0.1498 0.0914 0.0648\nw/o PO 0.0344 0.1427 0.0138 0.1329 0.0872 0.0634\nCOLA 0.0296 0.1213 0.0096 0.1110 0.0772 0.0559\nYahoo\nNONE 0.0299 0.1587 0.0006 0.0059 0.0358 0.0511\nw/o HA 0.0260 0.1553 0.0004 0.0043 0.0330 0.0504\nw/o PO 0.0258 0.1526 0.0003 0.0039 0.0360 0.0502\nCOLA 0.0161 0.1462 0.0002 0.0029 0.0294 0.0447\nNew York\nNONE 0.0279 0.0959 0.0004 0.0139 0.0116 0.0457\nw/o HA 0.0275 0.0957 0.0004 0.0137 0.0112 0.0416\nw/o PO 0.0270 0.0949 0.0005 0.0124 0.0108 0.0451\nCOLA 0.0263 0.0911 0.0004 0.0111 0.0078 0.0368\nSingapore\nNONE 0.0133 0.0397 0.0031 0.0356 0.0240 0.0555\nw/o HA 0.0130 0.0394 0.0006 0.0103 0.0235 0.0555\nw/o PO 0.0101 0.0357 0.0027 0.0353 0.0196 0.0551\nCOLA 0.0067 0.0267 0.0007 0.0086 0.0141 0.0534\nlearning across cities. Fig. 4 shows the results of selecting GeoLife\nas the target city (see more details of the other cities in B). For\nsimplicity, GeoLife, Yahoo, New York and Singapore are denoted\nas â€™Gâ€™, â€™Yâ€™, â€™Nâ€™ and â€™Sâ€™, respectively. Obviously, leveraging all cities\nleads to a steady and better performance across almost all met-\nrics, indicating that multi-city knowledge transfer can enhance the\nmodel generalizability. Specifically, the combination of Yahoo and\nNew York achieves the best performance on Duration and Dailyloc\nmetrics, likely due to their similarity of work habits influenced by\neconomic development degree. different cultural traditions within\ncities. However, it is worth noting that the combination of New\nYork and Singapore exhibits the worst performance on Distance\nand Radius metrics. This discrepancy may be attributed to their\nvarying geographical layouts and commuting distances.\n4.3.2 Half-open Attention and Post-hoc Adjustment. In this part,\nwe investigate the effectiveness of the half-open attention and the\npost-hoc adjustment in COLA. As shown in Table 3, our model with-\nout the two modules still outperforms the best-performed baselines\npresented in Table 2, which demonstrates the power of our basic\nframework (i.e., Transformer with transfer learning). Specifically,\nremoving the half-open attention module leads to a noteworthy\ndeterioration on almost all metrics, especially on the metrics from\nspatial and preference perspective, which indicates that itâ€™s neces-\nsary to distinguish city-specific characteristics and city-universal\nmobility patterns when transferring. In addition, when the post-\nhoc adjustment module is removed, the performance significantly\ndeclines on almost all metrics, especially onDuration metric, which\nsuggests that post-hoc adjustment effectively corrects the devia-\ntion caused by the overconfident deep models thus improving the\nsimulation credibility for highly visited locations. Furthermore, the\noverall performance of the model degrades further when removing\nthe half-open attention module.\n4.4 RQ3: Practical Applications\nIn applications that depend on individual trajectories, directly shar-\ning real location records is often infeasible due to privacy concerns.\nIn such cases, COLA can be used to generate simulated data that\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\n500 1000 1500\nNumber of Data\n0.000\n0.012\n0.024F1 Score\nGeolife\nreal\nbaseline\nours\n500 1000 1500\nNumber of Data\n0.000\n0.035\n0.070F1 Score\nYahoo\nreal\nbaseline\nours\nFigure 5: Location prediction in the fully simulated scenario.\n500 1000 1500\nNumber of Real Data\n0.000\n0.012\n0.024F1 Score\nGeolife\nreal\nreal+baseline\nreal+ours\n500 1000 1500\nNumber of Real Data\n0.000\n0.035\n0.070F1 Score\nYahoo\nreal\nreal+baseline\nreal+ours\nFigure 6: Location prediction in the hybrid scenario. For sce-\nnarios with different numbers of real-world trajectories (i.e.\n500, 1000, 1500), additional 1000 simulated trajectories are\nincluded for data augmentation.\ncan mask sensitive information while preserving the availability\nof real data. To evaluate the effectiveness of simulated individ-\nual trajectories, we conduct experiments with two categories of\nsimulated data: (1) fully simulated scenarios for enhanced privacy\nprotection, and (2) hybrid scenarios (combines real and simulated\ndata) for data augmentation. We choose location prediction as a\nrepresentative application [27, 47] which serves as the foundation\nfor various trajectory-related problems, including location recom-\nmendation and planning. In addition, we employ a widely used\nLSTM model with attention mechanism to predict future locations\nbased on historical trajectories.\nAs depicted in Fig. 5, for GeoLife which is relatively sparser than\nYahoo, simulated high-quality trajectories with less noise based\non the real data enables better identification of mobility patterns,\nyielding superior outcomes compared to the real data. Moreover,\nour framework outperforms the best baseline, indicating the superi-\nority of our method. For Yahoo, the performance of our framework\naligns more closely with the real data compared to the best baseline,\nshowcasing the utility of our simulated data. Fig. 6 illustrates that\nthe model using combined data achieves better performance than\nonly using real data. In the hybrid scenario, COLA also surpasses\nthe best baseline. Additionally, the combination of real data and\nsimulated data generated by COLA improves performance as the\nsize of real data increases. The above experiments demonstrate the\npractical benefits of simulated human trajectories.\n4.5 RQ4: Parameter Sensitivity\nWe investigate two crucial hyperparameters in COLA: the number\nof linear layers in the projection function of the attention mod-\nule and the coefficient ğœ for post-hoc adjustment during trajectory\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.030\n0.035\n0.040\n0.045\n0.050JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.120\n0.125\n0.130\n0.135\n0.140\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.004\n0.006\n0.008\n0.010\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.06\n0.08\n0.10\n0.12JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.070\n0.075\n0.080\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.056\n0.058\n0.060\n0.062\n1 layer 2 layers 3 layers\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.030\n0.035\n0.040\n0.045\n0.050JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.120\n0.125\n0.130\n0.135\n0.140\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.004\n0.006\n0.008\n0.010\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.06\n0.08\n0.10\n0.12JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.070\n0.075\n0.080\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.056\n0.058\n0.060\n0.062\nFigure 7: The performance comparison of COLA on GeoLife\nusing different layers and post-hoc coefficients ğœ. All experi-\nmental results are conducted over five trials.\nsimulation. Grid searches are performed over {1, 2, 3} and {0.001,\n0.01, 0.1, 0.25, 0.5, 1.0} for these two hyperparameters, considering\nall metrics across the four datasets. Fig. 7 demonstrates the robust\nperformance of COLA across various hyperparameter settings, in-\ndicating that different values of hyperparameters do not diminish\nits superiority over the baselines. Specifically, using two layers can\nyield satisfactory results, possibly because increasing the depth\noften results in overfitting, reducing the model generalizability.\nConversely, decreasing the depth is prone to underfitting, which\nfails to fully capture the mobility patterns in trajectories. Further-\nmore, for GeoLife, using a smaller coefficient (i.e. 0.25) for post-hoc\nadjustment can lead to improved performance across most metrics.\n5 CONCLUSION\nIn this paper, we have tailored a Cross-city mObiLity trAnsformer\nwith a model-agnostic transfer framework called COLA to simu-\nlate human trajectories, which tackles domain heterogeneity and\novercomes the overconfident problem of deep models. Extensive\nexperimental results demonstrate the superiority of COLA over\nstate-of-the-art single-city and our implemented cross-city base-\nlines. Nonetheless, there lacks a unified large mobility model upon\nmillions of human trajectory data from global cities due to domain\nheterogeneity. Inspired by the remarkable progress of GPT, we\nwill explore the pre-training potential of Transformer for human\ntrajectory simulation task in the future.\nACKNOWLEDGMENTS\nThis work is supported by the Starry Night Science Fund of Zhe-\njiang University Shanghai Institute for Advanced Study (No. SN-\nZJU-SIAS-001), National Natural Science Foundation of China (No.\nU20B2066), Zhejiang Province High-Level Talents Special Sup-\nport Program \"Leading Talent of Technological Innovation of Ten-\nThousands Talents Program\" (No. 2022R52046), Zhejiang Province\n\"Pioneering Soldier\" and \"Leading Goose\" R&D Project (No. 2023C01027),\nNingbo Natural Science Foundation (No. 2023J281), Guangzhou-\nHKUST (GZ) Joint Funding Program (No. 2024A03J0620) and the\nadvanced computing resources provided by the Supercomputing\nCenter of Hangzhou City University.\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\nREFERENCES\n[1] Di Chai, Leye Wang, and Qiang Yang. 2018. Bike flow prediction with multi-\ngraph convolutional networks. In ACM SIGSPATIAL International Conference on\nAdvances in Geographic Information Systems .\n[2] Di Chai, Leye Wang, Junxue Zhang, Liu Yang, Shuowei Cai, Kai Chen, and Qiang\nYang. 2022. Practical Lossless Federated Singular Vector Decomposition over\nBillion-Scale Data. In ACM Knowledge Discovery and Data Mining .\n[3] Serina Chang, Emma Pierson, Pang Wei Koh, Jaline Gerardin, Beth Redbird, David\nGrusky, and Jure Leskovec. 2021. Mobility network models of COVID-19 explain\ninequities and inform reopening. Nature 589, 7840 (2021), 82â€“87.\n[4] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer.\n2002. SMOTE: synthetic minority over-sampling technique. Journal of Artificial\nIntelligence Research 16 (2002), 321â€“357.\n[5] Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan\nHe. 2023. Bias and debias in recommender system: A survey and future directions.\nACM Transactions on Information Systems 41, 3 (2023), 1â€“39.\n[6] Seongjin Choi, Jiwon Kim, and Hwasoo Yeo. 2021. TrajGAIL: Generating urban\nvehicle trajectories using generative adversarial imitation learning. Transporta-\ntion Research Part C: Emerging Technologies 128 (2021), 103091.\n[7] Charles Elkan. 2001. The foundations of cost-sensitive learning. In International\nJoint Conference on Artificial Intelligence .\n[8] Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan\nLiu. 2022. DEPTS: deep expansion learning for periodic time series forecasting.\nInternational Conference on Learning Representations (2022).\n[9] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng\nJin. 2018. Deepmove: Predicting human mobility with attentional recurrent\nnetworks. In International World Wide Web Conferences .\n[10] Jie Feng, Zeyu Yang, Fengli Xu, Haisu Yu, Mudan Wang, and Yong Li. 2020.\nLearning to simulate human mobility. In ACM Knowledge Discovery and Data\nMining.\n[11] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-\nlearning for fast adaptation of deep networks. In International Conference on\nMachine Learning .\n[12] Bent Fuglede and Flemming Topsoe. 2004. Jensen-Shannon divergence and\nHilbert space embedding. InIEEE International Symposium on Information Theory .\n[13] Sebastien Gambs, Marc-Olivier Killijian, and Miguel Nunez del Prado Cortez.\n2012. Next place prediction using mobility markov chains. In Proceedings of the\nFirst Workshop on Measurement, Privacy, and Mobility .\n[14] Qiang Gao, Fan Zhou, Ting Zhong, Goce Trajcevski, Xin Yang, and Tianrui Li.\n2022. Contextual Spatio-Temporal Graph Representation Learning for Reinforced\nHuman Mobility Mining. Information Sciences 606 (2022), 230â€“249.\n[15] Xu Geng, Yilun Jin, Zhengfei Zheng, Yu Yang, Yexin Li, Han Tian, Peibo Duan,\nLeye Wang, Jiannong Cao, Hai Yang, et al. 2021. CityNet: A Multi-city Multi-\nmodal Dataset for Smart City Applications. arXiv preprint arXiv:2106.15802\n(2021).\n[16] Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping Ye, and Yan\nLiu. 2019. Spatiotemporal Multi-Graph Convolution Network for Ride-Hailing\nDemand Forecasting. In AAAI Conference on Artificial Intelligence .\n[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\nnetworks. Commun. ACM 63, 11 (2020), 139â€“144.\n[18] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration\nof modern neural networks. In International Conference on Machine Learning .\n[19] Tianfu He, Jie Bao, Ruiyuan Li, Sijie Ruan, Yanhua Li, Li Song, Hui He, and Yu\nZheng. 2020. What is the Human Mobility in a New City: Transfer Mobility\nKnowledge Across Cities. In International World Wide Web Conferences .\n[20] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory.Neural\ncomputation 9, 8 (1997), 1735â€“1780.\n[21] Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim,\nand Buru Chang. 2021. Disentangling label distribution for long-tailed visual\nrecognition. In IEEE Conference on Computer Vision and Pattern Recognition .\n[22] Renhe Jiang, Zhaonan Wang, Jiawei Yong, Puneet Jeph, Quanjun Chen, Yasumasa\nKobayashi, Xuan Song, Shintaro Fukushima, and Toyotaro Suzumura. 2023.\nSpatio-temporal meta-graph learning for traffic forecasting. In AAAI Confer-\nence on Artificial Intelligence .\n[23] Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang.\n2022. Domain adaptation for time series forecasting via attention sharing. In\nInternational Conference on Machine Learning .\n[24] Yilun Jin, Kai Chen, and Qiang Yang. 2022. Selective Cross-City Transfer Learning\nfor Traffic Prediction via Source City Region Re-Weighting. InACM Knowledge\nDiscovery and Data Mining .\n[25] Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2008. Exploratory undersampling\nfor class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics:\nSystems 39, 2 (2008), 539â€“550.\n[26] Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain,\nAndreas Veit, and Sanjiv Kumar. 2021. Long-tail learning via logit adjustment. In\nInternational Conference on Learning Representations .\n[27] Kun Ouyang, Reza Shokri, David S Rosenblum, and Wenzhuo Yang. 2018. A\nnon-parametric generative model for human trajectories.. In International Joint\nConference on Artificial Intelligence .\n[28] George Panagopoulos, Giannis Nikolentzos, and Michalis Vazirgiannis. 2021.\nTransfer graph neural networks for pandemic forecasting. In AAAI Conference\non Artificial Intelligence .\n[29] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.\nImproving language understanding by generative pre-training. (2018).\n[30] Zezhi Shao, Zhao Zhang, Wei Wei, Fei Wang, Yongjun Xu, Xin Cao, and Chris-\ntian S Jensen. 2022. Decoupled Dynamic Spatial-Temporal Graph Neural Network\nfor Traffic Forecasting. The VLDB Journal 15, 11 (2022), 2733â€“2746.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Annual Conference on Neural Information Processing Systems .\n[32] Srinivasan Venkatramanan, Adam Sadilek, Arindam Fadikar, Christopher L Bar-\nrett, Matthew Biggerstaff, Jiangzhuo Chen, Xerxes Dotiwalla, Paul Eastham,\nBryant Gipson, Dave Higdon, et al. 2021. Forecasting influenza activity using\nmachine-learned mobility map. Nature communications 12, 1 (2021), 1â€“12.\n[33] Dongjie Wang, Lingfei Wu, Denghui Zhang, Jingbo Zhou, Leilei Sun, and Yanjie\nFu. 2023. Human-instructed Deep Hierarchical Generative Learning for Auto-\nmated Urban Planning. In AAAI Conference on Artificial Intelligence .\n[34] Jingyuan Wang, Jiawei Jiang, Wenjun Jiang, Chao Li, and Wayne Xin Zhao. 2021.\nLibCity: An Open Library for Traffic Prediction. InACM SIGSPATIAL International\nConference on Advances in Geographic Information Systems .\n[35] Leye Wang, Xu Geng, Xiaojuan Ma, Feng Liu, and Qiang Yang. 2019. Cross-\ncity transfer learning for deep spatio-temporal prediction. In International Joint\nConference on Artificial Intelligence .\n[36] Ying Wei, Yu Zheng, and Qiang Yang. 2016. Transfer knowledge between cities.\nIn ACM Knowledge Discovery and Data Mining .\n[37] Junkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, and Xiang-\nnan He. 2023. Understanding Contrastive Learning via Distributionally Robust\nOptimization. In Annual Conference on Neural Information Processing Systems .\n[38] Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang, and Dahua Lin. 2021. Adversarial\nrobustness under long-tailed distribution. In IEEE Conference on Computer Vision\nand Pattern Recognition .\n[39] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki,\nEsteban Moro, and Alex Pentland. 2023. Metropolitan Scale and Longitu-\ndinal Dataset of Anonymized Human Mobility Trajectories. arXiv preprint\narXiv:2307.03401 (2023).\n[40] Dingqi Yang, Daqing Zhang, and Bingqing Qu. 2016. Participatory cultural\nmapping based on collective behavior data in location-based social networks.\nACM Transactions on Intelligent Systems and Technology 7, 3 (2016), 1â€“23.\n[41] Huaxiu Yao, Yiding Liu, Ying Wei, Xianfeng Tang, and Zhenhui Li. 2019. Learning\nfrom multiple cities: A meta-learning approach for spatial-temporal prediction.\nIn International World Wide Web Conferences .\n[42] Mogeng Yin, Madeleine Sheehan, Sidney Feygin, Jean-FranÃ§ois Paiement, and\nAlexei Pozdnoukhov. 2017. A generative model of urban activities from cellular\ndata. IEEE Transactions on Intelligent Transportation Systems 19, 6 (2017), 1682â€“\n1696.\n[43] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. Seqgan: Sequence\ngenerative adversarial nets with policy gradient. In AAAI Conference on Artificial\nIntelligence.\n[44] Yuan Yuan, Jingtao Ding, Huandong Wang, Depeng Jin, and Yong Li. 2022. Ac-\ntivity trajectory generation via modeling spatiotemporal dynamics. In ACM\nKnowledge Discovery and Data Mining .\n[45] Yuan Yuan, Huandong Wang, Jingtao Ding, Depeng Jin, and Yong Li. 2023. Learn-\ning to Simulate Daily Activities via Modeling Dynamic Human Needs. In Inter-\nnational World Wide Web Conferences .\n[46] Liang Zhang, Cheng Long, and Gao Cong. 2023. Region Embedding With Intra\nand Inter-View Contrastive Learning. IEEE Transactions on Knowledge and Data\nEngineering 35, 9 (2023), 9031â€“9036.\n[47] Pengfei Zhang, Haris N Koutsopoulos, and Zhenliang Ma. 2023. DeepTrip: A\nDeep Learning Model for the Individual Next Trip Prediction With Arbitrary\nPrediction Times. IEEE Transactions on Intelligent Transportation Systems 24, 6\n(2023), 5842â€“5855.\n[48] Yifan Zhang, Peilin Zhao, Shuaicheng Niu, Qingyao Wu, Jiezhang Cao, Junzhou\nHuang, and Mingkui Tan. 2019. Online adaptive asymmetric active learning\nwith limited budgets. IEEE Transactions on Knowledge and Data Engineering 33, 6\n(2019), 2680â€“2692.\n[49] Zizhao Zhang and Tomas Pfister. 2021. Learning fast sample re-weighting without\nreward data. In IEEE Conference on Computer Vision and Pattern Recognition .\n[50] Peilin Zhao, Yifan Zhang, Min Wu, Steven CH Hoi, Mingkui Tan, and Junzhou\nHuang. 2018. Adaptive cost-sensitive online classification. IEEE Transactions on\nKnowledge and Data Engineering 31, 2 (2018), 214â€“228.\n[51] Yu Zheng, Xing Xie, Wei-Ying Ma, et al. 2010. GeoLife: A collaborative social\nnetworking service among user, location and trajectory. IEEE Data Eng. Bull. 33,\n2 (2010), 32â€“39.\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\nA EXPERIMENT SETUP\nA.1 Datasets\nWe evaluate the performance of COLA and baselines on four pub-\nlicly available datasets:\nâ€¢GeoLife [51]: GeoLife consists of 17,621 trajectories collected by\n182 users over a peroid of five years (from April 2007 to August\n2012), which are primarily located in Beijing, China.\nâ€¢Yahoo (Japan) [39]1: The dataset contains 100K individualsâ€™\ntrajectories across 90 days in a metropolitan area provided by\nYahoo Japan Corporation, which is called Yahoo for simplicity.\nâ€¢New York and Singapore [40]: The two datasets are derived\nfrom Foursquare2, which captures user behavior worldwide. We\nselect the records from New York and Singapore within it.\nA.2 Baselines\nWe compare the proposed COLA with the following two categories\nof state-of-the-art baselines:\nâ€¢Single-City Baselines : Markov [13] is a well-known probabil-\nity method, which treats locations of trajectories as states and\ncalculates transition probabilities of locations. IO-HMM [42] is\nan extension of the traditional Hidden Markov Model (HMM).\nLSTM [20] is a basic recurrent model for sequence prediction.\nDeepMove [9] is a recurrent network with history attention.\nGAN [17] is a prominent generative framework, where both\ngenerator and discriminator are performed by two LSTMs in our\nsetting. SeqGAN [43] extends GAN by using a stochastic policy\nin reinforcement learning (RL) to address the challenge of se-\nquence generation. MoveSim [10] is an extension to SeqGAN by\nintroducing the prior knowledge of urban structure and design-\ning two loss functions. TrajGAIL [6] is a generative adversarial\nimitation learning framework based on a partially observable\nMarkov decision process. CGE [14] is a static graph-based repre-\nsentation for human motion built by check-ins reflecting usersâ€™\ngeographical preferences and visiting intentions. ACT-STD [44]\ncaptures the spatiotemporal dynamics underlying trajectories\nwith neural differential equations for human activity modelling.\nMobFormer is directly implemented by a Transformer [31] to\nhighlight its capability in contrast to existing recurrent models.\nâ€¢Cross-City Baselines : To conduct a comprehensive comparison\nusing cross-city dataset, we further incorporate the two most\neffective baselines, LSTM and SeqGAN, into the proposed transfer\nlearning framework, called CrossLSTM and CrossSeqGAN.\nB PERFORMANCE OF CITY NUMBERS\nwe present the results of transferring with various source cities\nfor Yahoo, New York and Singapore in Fig. A1, Fig. A2 and Fig. A3\nrespectively. Generally, transfer with three source cities exhibits\nmost stable and best performance on six metrics. Specifically, for\nYahoo, utilizing the remaining three cites as source cities yields\nsuperior results onDistance and Duration metrics due to the comple-\nmentarity of spatial and temporal patterns in their trajectories; for\nNew York, leveraging three cities for transfer surpasses onDuration\nand I-rank metrics because of the effective capturing of temporal\n1https://connection.mit.edu/humob-challenge-2023\n2https://foursquare.com/\nG N S G+N G+S N+S ALL\n(a) Distance\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030JSD\nG N S G+N G+S N+S ALL\n(b) Radius\n0.00\n0.05\n0.10\n0.15\n0.20\nG N S G+N G+S N+S ALL\n(c) Duration\n0.00000\n0.00025\n0.00050\n0.00075\n0.00100\n0.00125\nG N S G+N G+S N+S ALL\n(d) DailyLoc\n0.000\n0.005\n0.010\n0.015JSD\nG N S G+N G+S N+S ALL\n(e) G-rank\n0.00\n0.02\n0.04\n0.06\n0.08\nG N S G+N G+S N+S ALL\n(f) I-rank\n0.00\n0.02\n0.04\n0.06\nFigure A1: The performance comparison of COLA on Yahoo\nwith different combinations of source cities. All experimental\nresults are conducted over five trials for a fair comparison.\nG Y S G+Y G+S Y+S ALL\n(a) Distance\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030JSD\nG Y S G+Y G+S Y+S ALL\n(b) Radius\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nG Y S G+Y G+S Y+S ALL\n(c) Duration\n0.0000\n0.0001\n0.0002\n0.0003\n0.0004\n0.0005\nG Y S G+Y G+S Y+S ALL\n(d) DailyLoc\n0.000\n0.005\n0.010\n0.015\n0.020JSD\nG Y S G+Y G+S Y+S ALL\n(e) G-rank\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\nG Y S G+Y G+S Y+S ALL\n(f) I-rank\n0.00\n0.01\n0.02\n0.03\n0.04\nFigure A2: The performance comparison of COLA on New\nYork with different combinations of source cities. All ex-\nperimental results are conducted over five trials for a fair\ncomparison.\nG Y N G+Y G+N Y+N ALL\n(a) Distance\n0.000\n0.002\n0.004\n0.006\n0.008JSD\nG Y N G+Y G+N Y+N ALL\n(b) Radius\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\nG Y N G+Y G+N Y+N ALL\n(c) Duration\n0.000\n0.002\n0.004\n0.006\nG Y N G+Y G+N Y+N ALL\n(d) DailyLoc\n0.00\n0.02\n0.04\n0.06JSD\nG Y N G+Y G+N Y+N ALL\n(e) G-rank\n0.000\n0.005\n0.010\n0.015\n0.020\nG Y N G+Y G+N Y+N ALL\n(f) I-rank\n0.00\n0.02\n0.04\n0.06\nFigure A3: The performance comparison of COLA on Singa-\npore with different source cities. All experimental results are\nconducted over five trials for a fair comparison.\npatterns and personal preferences in trajectories; for Singapore,\nthe average performance using three source cities on six metrics\nis also better than other combinations of source cities, especially\non Duration and DailyLoc, suggesting the effective capturing of\ntemporal patterns in trajectories.\nItâ€™s noteworthy that while some combinations of source cities\nachieve the optimal results on one or several metrics, their perfor-\nmance is less consistent and worse on more metrics. For example,\nCOLA: Cross-city Mobility Transformer for Human Trajectory Simulation WWW â€™24, May 13â€“17, 2024, Singapore, Singapore\n500 1000 1500\nNumber of Data\n0.00\n0.02\n0.04F1 Score\nNew York\nreal\nbaseline\nours\n500 1000 1500\nNumber of Data\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05F1 Score\nSingapore\nreal\nbaseline\nours\nFigure A4: Location prediction in fully simulated scenarios.\n500 1000 1500\nNumber of Real Data\n0.00\n0.02\n0.04F1 Score\nNew York\nreal\nreal+baseline\nreal+ours\n500 1000 1500\nNumber of Real Data\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05F1 Score\nSingapore\nreal\nreal+baseline\nreal+ours\nFigure A5: Location prediction in hybrid scenarios. For sce-\nnarios with different numbers of real-world trajectories (i.e.\n500, 1000, 1500), additional 1000 simulated trajectories are\nincluded for data augmentation.\nTable A1: MAPE of simulation results for COVID-19 spread-\ning with different synthetic data.\nMAPE Exposed Infectious Removed\nLSTM 1.0323 Â±0.1502 0.8768 Â±0.1313 0.6206 Â±0.1262\nSeqGAN 1.3645 Â±0.1689 1.2761 Â±0.1528 1.0117 Â±0.1524\nMoveSim 0.7518 Â±0.1686 0.6314 Â±0.1515 0.4540 Â±0.1661\nCOLA 0.4322Â±0.0157 0.2332 Â±0.0125 0.1434 Â±0.0544\ntransferring from Singapore to New York outperforms on Distance\nmetric but demonstrates the poorest average performance on the\nother five metrics compared to other combinations of source cities.\nNevertheless, all results using cross-city knowledge significantly\noutperform the baselines of this paper, validating the effectiveness\nof cross-city transfer learning for human mobility simulation.\nC PRACTICAL APPLICATIONS\nC.1 Location Prediction\nThe practical application results in fully simulated and hybrid sce-\nnarios for New York and Singapore are illustrated in Fig. A4 and\nFig. A5. As depicted in Fig. A4, both our framework and the best\nbaseline surpass the real data by capturing more significant mobil-\nity patterns with less noise, emphasizing the utility of the simulated\ndata. Fig. A5 illustrates that the model using combined data obtains\nbetter performance than only using real data. Furthermore, COLA\nalso outperforms the best baseline in hybrid scenarios. In addition,\nthe combination of real data and the simulated data generated by\nCOLA improves the performance as the size of real data increases.\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.030\n0.035\n0.040\n0.045\n0.050JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.120\n0.125\n0.130\n0.135\n0.140\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.004\n0.006\n0.008\n0.010\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.06\n0.08\n0.10\n0.12JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.070\n0.075\n0.080\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.056\n0.058\n0.060\n0.062\n1 layer 2 layers 3 layers\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.0160\n0.0165\n0.0170\n0.0175\n0.0180JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.1425\n0.1450\n0.1475\n0.1500\n0.1525\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.00025\n0.00030\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.0025\n0.0030JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.025\n0.030\n0.035\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.04\n0.05\n0.06\nFigure A6: The performance comparison of COLA on Yahoo\nusing different layers and post-hoc coefficient ğœ. All experi-\nmental results are conducted over five trials.\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.030\n0.035\n0.040\n0.045\n0.050JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.120\n0.125\n0.130\n0.135\n0.140\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.004\n0.006\n0.008\n0.010\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.06\n0.08\n0.10\n0.12JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.070\n0.075\n0.080\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.056\n0.058\n0.060\n0.062\n1 layer 2 layers 3 layers\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.026\n0.027\n0.028JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.092\n0.094\n0.096\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.00025\n0.00050\n0.00075\n0.00100\n0.00125\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.010\n0.012\n0.014\n0.016JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.005\n0.010\n0.015\n0.020\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.040\n0.045\n0.050\nFigure A7: The performance comparison of COLA on New\nYork using different layers and post-hoc coefficient ğœ. All\nexperimental results are conducted over five trials.\nThe above experiments showcase the practical benefits of simulated\nhuman trajectories.\nC.2 Epidemic Simulation\nTo further access the effectiveness of COLA, we perform SEIR Mod-\neling of COVID-19 by utilizing the parameters from the simulation\nexperiments of these two papers [10, 44]. As demonstrated in Ta-\nble A1, COLA obtains superior performance while maintaining\nminimal standard errors.\nD PARAMETER SENSITIVITY\nThe grid search results for two crucial hyperparameters (layers\nand post-hoc coefficients ğœ) in COLA on Yahoo, New York and\nSingapore are presented in Fig. A6, Fig. A7 and Fig. A8, respectively.\nFor Yahoo, the model with two projection layers and a coefficient of\n0.5 yields superior performance. For New York, leveraging a small\ncoefficient (ğœ = 0.25) can exhibit good performance. Meanwhile,\nusing one layer enables the extraction of both private and shared\nrepresentation without overfitting. For Singapore, a deeper layer\nproduces improved results for better non-linear representation.\nWWW â€™24, May 13â€“17, 2024, Singapore, Singapore Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, & Mingli Song\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.030\n0.035\n0.040\n0.045\n0.050JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.120\n0.125\n0.130\n0.135\n0.140\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.004\n0.006\n0.008\n0.010\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.06\n0.08\n0.10\n0.12JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.070\n0.075\n0.080\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.056\n0.058\n0.060\n0.062\n1 layer 2 layers 3 layers\n0.0010.01 0.1 0.25 0.5 1.0\n(a) Distance\n0.007\n0.008\n0.009\n0.010JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(b) Radius\n0.025\n0.030\n0.0010.01 0.1 0.25 0.5 1.0\n(c) Duration\n0.0005\n0.0010\n0.0015\n0.0020\n0.0025\n0.0010.01 0.1 0.25 0.5 1.0\n(d) DailyLoc\n0.01\n0.02\n0.03JSD\n0.0010.01 0.1 0.25 0.5 1.0\n(e) G-rank\n0.02\n0.03\n0.0010.01 0.1 0.25 0.5 1.0\n(f) I-rank\n0.052\n0.054\n0.056\n0.058\n0.060\nFigure A8: The performance comparison of COLA on Sin-\ngapore using different layers and post-hoc coefficient ğœ. All\nexperimental results are conducted over five trials.\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\nDistance\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18Attention Score\n0 5 10 15 20\nTime\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Attention Score\nFigure A9: The distribution of spatial and temporal attention\nscores for GeoLife dataset.\n0 5 10 15 20 25 30\nDistance\n0.085\n0.090\n0.095\n0.100\n0.105Attention Score\n0 5 10 15 20\nTime\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n0.225Attention Score\nFigure A10: The distribution of spatial and temporal atten-\ntion scores for Yahoo dataset.\n0 5 10 15 20 25 30\nDistance\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Attention Score\n0 5 10 15 20\nTime\n0.15\n0.20\n0.25\n0.30\n0.35Attention Score\nFigure A11: The distribution of spatial and temporal atten-\ntion scores for New York dataset.\nAdditionally, setting ğœ = 0.25 can better adjust the exaggerated\nprobability caused by long-tail frequency distribution of locations.\n0 5 10 15 20 25 30\nDistance\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\n0.28Attention Score\n0 5 10 15 20\nTime\n0.150\n0.175\n0.200\n0.225\n0.250\n0.275\n0.300\n0.325Attention Score\nFigure A12: The distribution of spatial and temporal atten-\ntion scores for Singapore dataset.\nE INTERPRETABILITY\nWe find out the city-universal human mobility patterns through\nthe visualization of attention scores among locations based on the\nwell trained model. Specifically, we calculate the average attention\nscores between location pairs within a trajectory, considering both\nthe spatial dimension (ranging from 0km to 100km) and the tem-\nporal dimension (ranging from 0h to 23h), which are presented\nin Fig. A9, Fig. A10, Fig. A11 and Fig. A12. In terms of the spatial\ndimension, the attention scores for GeoLife, Yahoo and Singapore\ninitially range from 0.10 to 0.27 at 0km, gradually decreasing to 0\nas the distance increases. As for New York which has the sparsest\ntrajectory data, its distribution of spatial attention scores is oscil-\nlating as the distance increases. Regarding the temporal dimension,\nthe attention scores for GeoLife and New York rapidly increase to\na peak of 0.36 to 0.46 at 1h, and then sharply decline till 23h. As for\nYahoo and Singapore, their distributions of spatial attention scores\ndirectly decrease over time. These common attention patterns pro-\nvide valuable explanations for the city-universal mobility patterns\nof COLA, which are crucial for sharing Q and K across cities.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7385354042053223
    },
    {
      "name": "Transformer",
      "score": 0.62889564037323
    },
    {
      "name": "Urban computing",
      "score": 0.6165059208869934
    },
    {
      "name": "Mobility model",
      "score": 0.5128514766693115
    },
    {
      "name": "Cola (plant)",
      "score": 0.4664100408554077
    },
    {
      "name": "Distributed computing",
      "score": 0.25474119186401367
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.2522750496864319
    },
    {
      "name": "Engineering",
      "score": 0.10985627770423889
    },
    {
      "name": "Voltage",
      "score": 0.10575604438781738
    },
    {
      "name": "Electrical engineering",
      "score": 0.10040536522865295
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4400573310",
      "name": "Hangzhou City University",
      "country": null
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 10
}