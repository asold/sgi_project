{
  "title": "HIT-SCIR at SemEval-2020 Task 5: Training Pre-trained Language Model with Pseudo-labeling Data for Counterfactuals Detection",
  "url": "https://openalex.org/W3115632781",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2122094799",
      "name": "Xiao Ding",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3116048998",
      "name": "Dingkui Hao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2133326917",
      "name": "Yuewei Zhang",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2138975985",
      "name": "Kuo Liao",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106246561",
      "name": "Zhongyang Li",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098930276",
      "name": "Bing Qin",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098738246",
      "name": "Ting Liu",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2265846598",
    "https://openalex.org/W2964266863",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2119821739",
    "https://openalex.org/W2903571680",
    "https://openalex.org/W2962681511",
    "https://openalex.org/W1511173698",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2140785063",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2035228640",
    "https://openalex.org/W2155806188",
    "https://openalex.org/W2100805904",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2251559320",
    "https://openalex.org/W4287694168",
    "https://openalex.org/W2006458146",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2917286209",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2004763266"
  ],
  "abstract": "We describe our system for Task 5 of SemEval 2020: Modelling Causal Reasoning in Language: Detecting Counterfactuals. Despite deep learning has achieved significant success in many fields, it still hardly drives today’s AI to strong AI, as it lacks of causation, which is a fundamental concept in human thinking and reasoning. In this task, we dedicate to detecting causation, especially counterfactuals from texts. We explore multiple pre-trained models to learn basic features and then fine-tune models with counterfactual data and pseudo-labeling data. Our team HIT-SCIR wins the first place (1st) in Sub-task 1 — Detecting Counterfactual Statements and is ranked 4th in Sub-task 2 — Detecting Antecedent and Consequence. In this paper we provide a detailed description of the approach, as well as the results obtained in this task.",
  "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 354–360\nBarcelona, Spain (Online), December 12, 2020.\n354\nHIT-SCIR at SemEval-2020 Task 5: Training Pre-trained Language\nModel with Pseudo-labeling Data for Counterfactuals Detection\nXiao Ding, Dingkui Hao, Yuewei Zhang, Kuo Liao, Zhongyang Li, Bing Qin, Ting Liu∗\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology, China\n{xding, dkhao, ywzhang, kliao, zyli, qinb, tliu}@ir.hit.edu.cn\nAbstract\nWe describe our system for Task 5 of SemEval 2020: Modelling Causal Reasoning in Language:\nDetecting Counterfactuals. Despite deep learning has achieved signiﬁcant success in many ﬁelds, it\nstill hardly drives today’s AI to strong AI, as it lacks of causation, which is a fundamental concept\nin human thinking and reasoning. In this task, we dedicate to detecting causation, especially\ncounterfactuals from texts. We explore multiple pre-trained models to learn basic features and\nthen ﬁne-tune models with counterfactual data and pseudo-labeling data. Our team HIT-SCIR\nwins the ﬁrst place (1st) in Sub-task 1 — Detecting Counterfactual Statements and is ranked 4th\nin Sub-task 2 — Detecting Antecedent and Consequence. In this paper we provide a detailed\ndescription of the approach, as well as the results obtained in this task 1.\n1 Introduction\nDeep learning technologies have been truly remarkable and have caught many attentions of researchers.\nHowever, deep learning systems’ lack of understanding of causal relations is perhaps the biggest roadblock\nto giving them human-level intelligence (Pearl, 2019). Causation is a fundamental concept in human\nthinking and reasoning, which indicates a special semantic relation between the cause with the effect\n(Stukker et al., 2008). Pearl and Mackenzie (2018) propose that there are three levels of causation, and the\ntop level is the counterfactual analysis. Counterfactual statements describe events that did not actually\nhappen or cannot happen, as well as the possible consequence if the events have had happened (Yang et\nal., 2020). SemEval 2020 Task 5 consists of two subtasks which aim to detect counterfactual descriptions\nin sentences, this paper presents solutions to both of two subtasks.\nSubtask1 — Detecting counterfactual statements refers to determining whether a given statement is\ncounterfactual or not in several domains. For example, given the sentence: “Her post-traumatic stress\ncould have been avoided if a combination of paroxetine and exposure therapy had been prescribed two\nmonths earlier.”, the system is required to determine whether it is a counterfactual statement or not. This\ncategorization task is evaluated by three evaluation indexes: Precision, Recall and F1. The challenges of\nthis subtask include: a) positive and negative samples are unevenly distributed in the training data and test\ndata. b) the patterns of counterfactual statements can be hardly deﬁned by rules or learned by traditional\nword embedding based models.\nSubtask2 — Detecting antecedent and consequence aims to locate antecedent and consequent in\ncounterfactuals. A counterfactual statement can be converted to a contrapositive with a true antecedent and\nconsequent (Goodman and Nelson, 1947). Consider the “post-traumatic stress” example discussed above;\nit can be transposed into “because her post-traumatic stress was not avoided, (we know) a combination of\nparoxetine and exposure therapy was not prescribed”. Such knowledge can be not only used for analyzing\nthe speciﬁc statement but also be accumulated across corpora to develop domain causal knowledge (e.g.,\na combination of paroxetine and exposure may help cure post-traumatic stress). The results are evaluated\nby four indexes: Precision, Recall, F1 and Exact Match. Unlike normal sequence labeling tasks, in some\n∗Corresponding author\n1Our codes are available on https://github.com/haodingkui/semeval2020-task5-subtask1 (subtask 1) and\nhttps://github.com/AutismNLP/Semeval2020-Task5-Subtask2 (subtask 2).\n355\ncases there is only an antecedent part while without a consequent part in a counterfactual statement. For\ninstance, “Thanks for the article on this new term that ﬁts me so well, wish all your articles were worthy of\npraise.”, there is only the antecedent part “wish all your articles were worthy of praise” in this sentence.\nRecently, pre-trained language models have achieved great success in various NLP tasks (Devlin et al.,\n2018; Yang et al., 2019; Liu et al., 2019). Hence, we ﬁrst explore multiple pre-trained language models\n(i.e., BERT, XLNet and RoBERTa) stacking methods with ﬁne-tuned in counterfactual data. These models\nare basically trained in a supervised fashion with labeled data. However, There is not very sufﬁcient\nlabeled data in Task 5. Hence, we use a simple way of training neural networks in a semi-supervised\nfashion. Basically, each pre-trained language model is trained in a supervised fashion with labeled data.\nFor unlabeled data, Pseudo-Labels, created by just picking up the class which has more votes from\npre-trained language models, are used as if they were true labels.\n2 Related Work\nSubtask 1 is a text classiﬁcation task. Traditional machine learning systems, such as Naive Bayes\n(Domingos and Pazzani, 1997) and Support Vector Machines (Cortes and Vapnik, 1995) perform well in\nthis task. In recent years, text classiﬁcation has made breakthroughs in deep learning. A few of studies\n(Kim, 2014; Liu et al., 2016; Lai et al., 2015) made a series of improvements to the structure of deep\nneural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has\nachieved very competitive performance in multiple domains of text classiﬁcation. In addition, pre-trained\nlanguage models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al.,\n2019) have also achieved high performances in various NLP tasks.\nSubtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden\nMarkov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et\nal., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long Short-\nTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture\ndependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016;\nPeters et al., 2017). In similar manner, ﬁne-tuning the pre-trained model to complete the task of sequence\nlabeling such as BERT has achieved excellent performances.\n3 Methodology\nFor two subtasks (i.e., text classiﬁcation task and sequence labeling task), we start by ﬁne-tuning pre-\ntrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set\nwith pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single\npre-trained models. Finally, we use the ensemble model to classify counterfactual statements for subtask\n1 and utilize a CRF model to perform sequence labeling for extracting antecedent and consequence in\nsubtask 2. We will introduce each module in details in the following sections.\n3.1 Fine-tuning Pre-Trained Models\nBERT (Devlin et al., 2018) is a revolutionary self-supervised pre-training technique that learns to predict\nintentionally hidden (masked) sections of text. Crucially, the representations learned by BERT have been\nshown to generalize well to downstream tasks, and when BERT was ﬁrst released in 2018 it achieved\nstate-of-the-art results on many NLP benchmark datasets.\nXLNet (Yang et al., 2019) is a new unsupervised language representation learning method based on a\nnovel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-\nXL as the backbone model, exhibiting excellent performance for language tasks involving long context.\nOverall, XLNet also achieves state-of-the-art results on various downstream language tasks.\nRoBERTa (Liu et al., 2019) builds on BERT’s language masking strategy and modiﬁes key hyper-\nparameters in BERT, including removing BERT’s next-sentence pre-training objective, and training with\nmuch larger mini-batches and learning rates. RoBERTa is trained on an order of magnitude more data\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n356\nthan BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better\nto downstream tasks compared to BERT.\nFor subtask 1, we use the default models for sentence classiﬁcation implemented in the huggingface\nlibrary. For subtask 2, which can be formulated as a sequence labeling task, sentences are fed into\nRoBERTa and then into a classiﬁcation layer, producing a sequence of tag scores for each sentence. The\nsub-token entries are removed from the sentences and the remaining tokens are passed to the CRF layer.\nThe maximum context tokens are selected and concatenated to form the ﬁnal predicted tags. Generally,\nwe use Viterbi algorithm to simplify the calculation of CRF and adopt the standard CRF loss function\n3.2 Pseudo-Labeling\nPseudo-labeling is a simple but effective semi-supervised learning method that can improve the per-\nformance of machine learning models by utilizing unlabeled data (Lee, 2013). In subtask 1, ﬁrstly we\nﬁne-tune different pre-trained language models on the training set as pseudo-label predictors, then we use\nthese classiﬁers to provide pseudo-labels on sentences in the test set. For each pseudo-labeled sentence in\nthe test set, we add it into the training set if and only if all classiﬁers agree on the labeling of this sentence.\nIn subtask 2, we use k roberta-large models trained by k-fold cross-validation as pseudo label predictors\nto make predictions on the test set, and we add pseudo labels into the training set when at least n (n ≤k)\nmodels agree with them.\n3.3 Ensemble\nEnsemble has shown its power on effectively improving the robustness and accuracy of each individual\nprediction models (Opitz and Maclin, 1999; Rokach, 2010). By ensembling predictions from models\nwith different hyper-parameters or architectures, we can get better results than each individual model. In\nour system, XLNet and RoBERTa have different training objectives, BERT and RoBERTa have different\nhyper-parameters. Thus, we propose to combine their probability predictions via weighted average\nensemble, and then optimize the classiﬁcation threshold for optimal F1 score on the combined prediction.\n3.4 Sequence Labeling with CRF\nIn subtask 2, we use CRF model to perform sequence labeling task and extract antecedents and conse-\nquences in the counterfactual statements. With an additional CRF output layer, our model can calculate\nnot only the emission score but also the transition score. The transition score from CRF is considered to\nguarantee the integrity and correctness of the output label. We label antecedents and consequences in the\ncounterfactual statements with BIO tags.\n4 Experimental Settings\n4.1 Data\nSemEval 2020 Task 5 released the training and test dataset, and the detailed statistics are shown in Table 1.\nTraining Test\n#Positive instances 1,454 -\n#Negative instances 11,546 -\n#Total 13,000 7,000\n(a) Data distribution of subtask 1.\nTraining Test\n#Positive instances 3,031 -\n#Negative instances 520 -\n#Total 3,551 1,950\n(b) Data distribution of subtask 2.\nTable 1: Data distribution of SemEval2020-Task5\n4.2 Evaluation Metrics\n•Subtask1: Precision, Recall, and F1. The evaluation will verify whether the predicted binary “label”\nis the same as the desired “label” which is annotated by human workers, and then calculate its\nprecision, recall, and F1 scores.\n357\n•Subtask2: Exact Match, Precision, Recall, and F1. Exact Match will represent what percentage of\nboth predicted antecedents and consequences are exactly matched with the desired outcome that is\nannotated by human workers. F1 score is a token level metric and will be calculated according to the\nsubmitted antecedent startid, antecedent endid, consequent startid, consequent endid.\n4.3 Experimental Details\nExperimental Environment.Our experimental codes relied heavily on BERT, RoBERTa and XLNet\nmodels, which are implemented in the Huggingface Transformers package(2.5.1) with Pytorch(1.2.0).\nComputations were performed on Tesla P100-PCIE-16GB GPUs.\nHyper-Parameters of Pseudo-Labeling.For pseudo-labeling in subtask 1, we train a bert-large-cased\nmodel, an xlnet-large-cased model and a roberta-large model on the whole training set as pseudo label\npredictors, respectively. We use an AdamW optimizer to tune the parameters with learning rate = 2e-5,\nepsilon = 1e-8, max seq length = 128, batch size = 16, seed = 42 and we train each model for 4 epochs.\nThen we use these pseudo-label predictors to predict on the test set and get 6,867 pseudo-labels which all\npredictors agree with them. For subtask 2, we use ﬁve different roberta-large models trained in 5-fold\ncross-validation as pseudo-label predictors to make predictions on the test set, and we add the pseudo-label\nto the training set if and only if at least N models agree with it. We also use an AdamW optimizer to tune\nthe parameters with learning rate = 1e-5, epsilon = 1e-8, max seq length = 250, batch size = 8, seed =\n19,970,514 and we train each model for 5 epochs. Then we can obtain 822 pseudo-labels when N = 4\nand 524 pseudo-labels when N = 5.\nHyper-Parameters of Our System.We ﬁrstly add all the pseudo-labeled sentences to the training\nset, then we train a bert-large-cased model, an xlnet-large-cased model and a roberta-large model on\nthe training set, respectively. After that, we use weighted average ensemble to combine the predicted\nprobabilities of BERT (weight=1), XLNet (weight=1) and RoBERTa (weight=2). Due to the imbalanced\ndistribution of the training set for subtask 1, models trained on it tend to predict more negative labels with\na threshold of 0.5. Hence, we used a lower threshold (0.3437), which is the best threshold for the average\nF1 of 10-fold cross-validation on the training set, for the ﬁnal submission. The hyper-parameters of our\nsystem used in both subtasks are shown in Table 2.\nBERT(st1) XLNet(st1) RoBERTa(st1) RoBERTa(st2)\nbatch size 12 12 8 8\nlearning rate 2e-5 2e-5 2e-5 5e-6\nmax epochs 3 3 3 3\nmax sequence length 128 128 128 250\nrandom seed 42 42 42 19970514\nTable 2: Hyper-Parameters of our system for subtask 1 and subtask 2\n5 Results\nAs there is no validation set in Task 5, we use cross-validation of the training set as the validation set. We\ntune our system on the validation set and report the experimental results in Table 3 and Table 4. Then we\nchoose the system that achieved the best performance on the validation set to be evaluated on the test set,\nand report the experimental results in Table 5 and Table 6.\nTable 3 shows results of 10-fold cross-validation on the training set of subtask 1. We can ﬁnd that\nmodels trained with extra pseudo-labels get higher F1 score than models trained only with the original\ntraining data. The weighted average ensemble model of single models trained with extra pseudo labels\noutperforms all single models and achieves the highest F1 score.\nTable 4 shows results of 5-fold cross-validation on the training set of subtask 2. We can ﬁnd that the\nmodel trained with extra pseudo-labels from single model gets higher Recall, Precision and Exact Match\nthan the model trained with the original training data.\nTable 5 shows results on the test set of subtask 1. We can ﬁnd that the results on the test data are similar\nto the results on the validation set. We can also see that models trained with pseudo-labels gain better\n358\n# Model Comments F1 (%) Recall (%) Precision (%)\n1 BERT bert-large-cased 87.50 85.10 90.05\n2 XLNet xlnet-large-cased 86.53 84.55 88.61\n3 RoBERTa roberta-large 89.49 88.27 90.76\n5 BERT+PL bert-large-cased 88.39 86.09 90.81\n6 XLNet+PL xlnet-large-cased 88.29 86.04 90.65\n7 RoBERTa+PL roberta-large 89.91 88.50 91.37\n8 RoBERTa+BERT+XLNet ensemble+PL 5+6+7 90.72 91.13 90.32\nTable 3: Results of 10-fold cross-validation on subtask 1. PL is the abbreviation for Pseudo-Labeling\n# Model Comments F1 (%) Recall (%) Precision (%) Exact Match(%)\n1 RoBERTa+CRF roberta-large 86.30 87.50 85.00 46.60\n2 RoBERTa+CRF+1commonPL roberta-large 85.20 88.30 86.20 49.10\n3 RoBERTa+CRF+4commonPL roberta-large 82.70 86.70 83.40 45.30\n4 RoBERTa+CRF+5commonPL roberta-large 84.00 87.60 84.80 47.10\nTable 4: Results of 5-fold cross-validation on subtask 2. 4commonPL means that we add a pseudo-labeled\nsample of the test set into the training set when 4 models agree with it.\nF1 score than models trained with only the original training data. The ensemble model (#8) trained with\npseudo-labels achieves the best result.\nTable 6 shows that the models trained with only the original training data obtained better performance\nthan models trained with pseudo-labels. Pseudo-labels on the test set do not provide obvious help. A\npossible explanation is that the pseudo-labels cannot contain enough high-conﬁdence labeling data to\nprovide any beneﬁt. In addition, in order to ensure the correctness of the label format, we used some rules\non the optimal model. For example, if the start of the antecedent is labeled as -1 by the best model, then\nwe ﬁnd reasonable result from output of other models.\n6 Conclusion\nCounterfactuals inference is a challenging problem in causation, which is crucial for today’s AI system.\nIn this paper, we designed and implemented an ensemble model for detecting counterfactuals. It achieved\nthe top F1 score of 90.90% on the Evaluation-Subtask 1 leaderboard and 4th F1 score of 84.10% on the\nEvaluation-Subtask 2 leaderboard. Task 5 mainly explores how to detect counterfactuals, which is the\nbasis of counterfactuals inference. We believe that causation, especially counterfactuals will attract more\nattention from researchers driven by this task.\nReferences\nCorinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine Learning, 20(3):273–297.\n# Model Comments F1 Recall Precision\n1 BERT bert-large-cased 88.10 86.60 89.60\n2 XLNet xlnet-large-cased 88.90 87.70 90.10\n3 RoBERTa roberta-large 89.20 88.60 89.70\n4 RoBERTa+BERT+XLNet ensemble 1+2+3 89.70 90.80 88.60\n5 BERT+PL bert-large-cased 89.00 88.50 89.60\n6 XLNet+PL xlnet-large-cased 89.80 89.80 89.70\n7 RoBERTa+PL roberta-large 89.50 88.60 90.50\n8 RoBERTa+BERT+XLNet ensemble+PL 5+6+7 90.90 91.90 90.00\nTable 5: Results on the test set of subtask 1.\n359\n# Model Comments F1 Recall Precision Exact Match\n1 RoBERTa+CRF roberta-large 0.818 0.826 0.836 0.470\n2 RoBERTa+CRF+PL roberta-large 0.808 0.812 0.828 0.461\n3 RoBERTa+CRF+4commonPL roberta-large 0.791 0.791 0.816 0.457\n4 RoBERTa+CRF+5commonPL roberta-large 0.785 0.798 0.796 0.430\n5 RoBERTa+CRF+Rule roberta-large 0.841 0.846 0.868 0.471\nTable 6: Results on the test set of subtask 2.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nPedro Domingos and Michael J Pazzani. 1997. On the optimality of the simple bayesian classiﬁer under zero-one\nloss. Machine Learning, 29(2):103–130.\nGoodman and Nelson. 1947. The problem of counterfactual conditionals. Journal of Philosophy, 44(5):113.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991.\nYoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. arXiv preprint arXiv:1408.5882.\nSiwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Recurrent convolutional neural networks for text classiﬁca-\ntion. In Twenty-ninth AAAI conference on artiﬁcial intelligence.\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural\narchitectures for named entity recognition. arXiv preprint arXiv:1603.01360.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. arXiv: Computation and Language.\nDong-hyun Lee. 2013. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural\nnetworks.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Recurrent neural network for text classiﬁcation with multi-\ntask learning. arXiv preprint arXiv:1605.05101.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nGang Luo, Xiaojiang Huang, Chin-Yew Lin, and Zaiqing Nie. 2015. Joint entity recognition and disambiguation.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 879–888.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf.arXiv preprint\narXiv:1603.01354.\nDavid W Opitz and Richard Maclin. 1999. Popular ensemble methods: an empirical study. Journal of Artiﬁcial\nIntelligence Research, 11(1):169–198.\nAlexandre Passos, Vineet Kumar, and Andrew McCallum. 2014. Lexicon infused phrase embeddings for named\nentity resolution. arXiv preprint arXiv:1404.5367.\nJudea Pearl and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. Basic Books.\nJudea Pearl. 2019. The seven tools of causal inference, with reﬂections on machine learning. Communications of\nThe ACM, 62(3):54–60.\nMatthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence\ntagging with bidirectional language models. arXiv preprint arXiv:1705.00108.\nLev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Pro-\nceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009) , pages\n147–155.\nLior Rokach. 2010. Ensemble-based classiﬁers. Artiﬁcial Intelligence Review, 33(1):1–39.\n360\nNinke Stukker, Ted Sanders, and Arie Verhagen. 2008. Causality in verbs and in discourse connectives: Converg-\ning evidence of cross-level parallels in dutch linguistic categorization.Journal of Pragmatics, 40(7):1296–1322.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information process-\ning systems, pages 5754–5764.\nXiaoyu Yang, Stephen Obadinma, Huasha Zhao, Qiong Zhang, Stan Matwin, and Xiaodan Zhu. 2020. SemEval-\n2020 task 5: Counterfactual recognition. In Proceedings of the 14th International Workshop on Semantic\nEvaluation (SemEval-2020), Barcelona, Spain.",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.9457696676254272
    },
    {
      "name": "Counterfactual conditional",
      "score": 0.945107102394104
    },
    {
      "name": "Computer science",
      "score": 0.8157470226287842
    },
    {
      "name": "SemEval",
      "score": 0.7280866503715515
    },
    {
      "name": "Task (project management)",
      "score": 0.7278558611869812
    },
    {
      "name": "Causation",
      "score": 0.7109783887863159
    },
    {
      "name": "Antecedent (behavioral psychology)",
      "score": 0.7057109475135803
    },
    {
      "name": "Artificial intelligence",
      "score": 0.691830039024353
    },
    {
      "name": "Natural language processing",
      "score": 0.6034960150718689
    },
    {
      "name": "Causal reasoning",
      "score": 0.5494161248207092
    },
    {
      "name": "Training set",
      "score": 0.4230201244354248
    },
    {
      "name": "Machine learning",
      "score": 0.38682612776756287
    },
    {
      "name": "Psychology",
      "score": 0.15632516145706177
    },
    {
      "name": "Cognition",
      "score": 0.09903842210769653
    },
    {
      "name": "Epistemology",
      "score": 0.09145477414131165
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}