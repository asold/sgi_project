{
  "title": "Complaint Identification in Social Media with Transformer Networks",
  "url": "https://openalex.org/W3116085416",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3093785641",
      "name": "Mali Jin",
      "affiliations": [
        "University of Sheffield"
      ]
    },
    {
      "id": "https://openalex.org/A93365683",
      "name": "Nikolaos Aletras",
      "affiliations": [
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2037452669",
    "https://openalex.org/W2794120785",
    "https://openalex.org/W2727664736",
    "https://openalex.org/W2601057296",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2205003908",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970252517",
    "https://openalex.org/W118913126",
    "https://openalex.org/W2230941446",
    "https://openalex.org/W2166434810",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2971226772",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2944958965",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W624669820",
    "https://openalex.org/W2969035964",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2767673215",
    "https://openalex.org/W2028856827",
    "https://openalex.org/W1966797434",
    "https://openalex.org/W1948823840",
    "https://openalex.org/W2774683704",
    "https://openalex.org/W2946268098",
    "https://openalex.org/W4251968720",
    "https://openalex.org/W2963355830",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2549993617",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2491595835",
    "https://openalex.org/W2207187831",
    "https://openalex.org/W2953005365",
    "https://openalex.org/W3034761782",
    "https://openalex.org/W2466778245",
    "https://openalex.org/W2066116976",
    "https://openalex.org/W2157827826",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2592433683"
  ],
  "abstract": "Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1765–1771\nBarcelona, Spain (Online), December 8-13, 2020\n1765\nComplaint Identiﬁcation in Social Media with Transformer Networks\nMali Jin Nikolaos Aletras\nDepartment of Computer Science, University of Shefﬁeld\n{mjin6, n.aletras}@sheffield.ac.uk\nAbstract\nComplaining is a speech act extensively used by humans to communicate a negative inconsis-\ntency between reality and expectations. Previous work on automatically identifying complaints\nin social media has focused on using feature-based and task-speciﬁc neural network models.\nAdapting state-of-the-art pre-trained neural language models and their combinations with other\nlinguistic information from topics or sentiment for complaint prediction has yet to be explored.\nIn this paper, we evaluate a battery of neural models underpinned by transformer networks which\nwe subsequently combine with linguistic information. Experiments on a publicly available data\nset of complaints demonstrate that our models outperform previous state-of-the-art methods by a\nlarge margin achieving a macro F1 up to 87.\n1 Introduction\nComplaining is a basic speech act, usually triggered by a discrepancy between reality and expectations\ntowards an entity or event (Olshtain and Weinbach, 1985; Cohen and Olshtain, 1993; Kowalski, 1996).\nSocial media has become a popular platform for expressing complaints online (Preotiuc-Pietro et al.,\n2019) where customers can directly address companies regarding issues with services and products.\nComplaint detection aims to identify a breach of expectations in a given text snippet. However, the use\nof implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism,\nwarnings and threats (Pawar et al., 2015) make it a challenging task. Identifying and classifying com-\nplaints automatically is important for: (a) improving customer service chatbots (Coussement and Van den\nPoel, 2008; Lailiyah et al., 2017; Yang et al., 2019a); (b) linguists to analyze complaint characteristics\non large scale (V´asquez, 2011; Kakolaki and Shahrokhi, 2016); and (c) psychologists to understand the\nbehavior of humans that express complaints (Sparks and Browning, 2010).\nPrevious work has focused on binary classiﬁcation between complaints and non-complaints in various\ndomains (Preotiuc-Pietro et al., 2019; Jin et al., 2013; Coussement and Van den Poel, 2008). Further-\nmore, some studies have performed more ﬁne-grained complaint classiﬁcation. For instance, complaints\ndirected to public authorities have been categorized based on their topics (Forster and Entrup, 2017; Mer-\nson and Mary, 2017) or the responsible departments (Laksana and Purwarianti, 2014; Gunawan et al.,\n2018; Tjandra et al., 2015). Other categorizations are based on possible hazards and risks (Bhat and Cu-\nlotta, 2017) as well as escalation likelihood (Yang et al., 2019a). Most of these previous studies have used\nsupervised machine learning models with features extracted from text (e.g. bag-of-words, topics, fea-\ntures extracted from psycho-linguistic dictionaries) or task-speciﬁc neural models trained from scratch.\nAdapting state-of-the-art pre-trained neural language models based on transformer networks (Vaswani et\nal., 2017) such as BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019b) has yet to be explored.\nIn this paper, we focus on the binary classiﬁcation of Twitter posts into complaints or not (2019). We\nadapt and evaluate a battery of pre-trained transformers which we subsequently combine with external\nlinguistic information from topics and emotions.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n1766\nDomain Complaints Non-complaints\nFood 95 35\nApparel 141 117\nRetail 124 75\nCars 67 25\nServices 207 130\nSoftware 189 103\nTransport 139 109\nElectronics 174 112\nOther 96 33\nTotal 1232 739\nTable 1: Data set statistics across 9 domains.\nContributions (1) New state-of-the-art results on complaint identiﬁcation in Twitter, improving macro\nFI by 8.0% over previous work by Preotiuc-Pietro et al. (2019); (2) A qualitative analysis of the limita-\ntions of transformers in predicting accurately whether a given text is a complaint or not.\n2 Complaint Prediction Task and Data\nGiven a text snippet (i.e. tweet), we aim to classify it as a complaint or not. For that purpose, we use the\ndata set1 by Preotiuc-Pietro et al. (2019) which contains tweets written in English that were manually\nannotated as complaints or not. It includes 1,232 complaints (62.4%) and 739 non-complaints (37.6%)\nover 9 domains (e.g. food, technology, etc.). Data statistics are shown in Table 1. We opted using this\ndata set because (1) it is publicly available; and (2) it allows a direct comparison with existing methods.\nWe also use the data for distant supervision 2 collected by Preotiuc-Pietro et al. (2019). This extra\n‘noisy’ data source contains 18,218 complaint tweets collected by querying Twitter API with certain\ncomplaint related hashtags (e.g. #badbusiness, #badcustomerservice, etc.) and the same amount of non-\ncomplaint tweets that were sampled randomly.\n3 Transformer-based Models\nTransformer architectures trained on language modeling have been recently adapted to downstream tasks\ndemonstrating state-of-the-art performance (Weller and Seppi, 2019; Gupta and Durrett, 2019; Ma-\nronikolakis et al., 2020). In this paper, we adapt and subsequently combine transformers with external\nlinguistic information for complaint prediction.\nBERT, ALBERT and RoBERTa Bidirectional Encoder Representations from Transformers\n(BERT) (Devlin et al., 2018) learns language representations by jointly conditioning on both left and\nright contexts using transformers. It is trained on masked language modeling where some of the tokens\nare randomly masked with the aim to predict them using only the context.\nWe further experiment with ALBERT (Lan et al., 2019) and RoBERTa (Liu et al., 2019). ALBERT\nuses two parameter-reduction methods to address memory limitations and long training time of BERT:\n(a) factorized embedding parameterization; (b) cross-layer parameter sharing. RoBERTa is an extension\nof BERT trained on more data with larger batch size using dynamic masking (i.e. changeable masked\ntokens of each sequence during training epochs). We adapt BERT, ALBERT and RoBERTa by adding a\nlinear layer with a sigmoid activation and then ﬁne-tune it on the complaint classiﬁcation data.\nXLNet XLNet (Yang et al., 2019b) uses a similar architecture to BERT to learn bidirectional contextual\ninformation. Instead of masked tokens used in BERT, XLNet maximizes the expected log-likelihood of\nall possible factorization orders. We adapt and ﬁne-tune the XLNet model for complaint prediction\nsimilar to BERT.\n1https://github.com/danielpreotiuc/complaints-social-media\n2We use the noisy but larger distantly supervised data to ﬁrst adapt all models on the complaint classiﬁcation task. Then we\nﬁne-tune them using the smaller original complaint data set.\n1767\nM-BERT To combine our model with external linguistic information, we adapt the Multimodal BERT\n(M-BERT) (Rahman et al., 2019) model structure that has been introduced for multimodal modeling\n(text, image, speech). Instead of cross-modal interactions, we inject extra linguistic information as al-\nternative views of the data into the pre-trained BERT model. We use (a) Emotion, a 9 dimensional\nvector obtained by quantifying six basic emotions of Ekman (1992) for each tweet using a predictive\nmodel by V olkova and Bachrach (2016); (b) Topics, a 200 dimensional vector representing word fre-\nquencies in word clusters designed to identify semantic themes in tweets by Preotiuc-Pietro et al. (2015;\n2015). To inject external linguistic information to M-BERT, 3 we ﬁrst project the linguistic information\ninto vectors with similar size to the BERT CLS embeddings. Then we concatenate word representations\nobtained from BERT and the linguistic information (Emotion, Topics or Emotion+Topics) to generate\ncombined embeddings. During concatenation, an Attention Gating Mechanism called Multimodal Shift-\ning Gate (Wang et al., 2019) is applied to control the importance of each representation. Finally, the\ncombined embeddings are fed to BERT for ﬁne-tuning. The rest of the architecture is the same as BERT.\n4 Experiments\nBaselines We compare the transformer-based models with two previous approaches for complaint iden-\ntiﬁcation by Preotiuc-Pietro et al. (2019) and a transfer learning method: (1) Logistic Regression with\nbag-of-words trained using the original and distantly supervised complaint data ( LR-BOW + Dist. Su-\npervision); (2) A Long-Short Term Memory ( LSTM) network (Hochreiter and Schmidhuber, 1997)\nthat takes as input a tweet, maps its words to embeddings and subsequently passes them through the\nLSTM to obtain a contextualized representation which is ﬁnally fed to the output layer; (3) Adapting\nthe pre-trained Universal Language Model Fine-tuning (ULMFiT) model (Howard and Ruder, 2018) for\ncomplaint prediction. ULMFiT uses a AWD-LSTM (Merity et al., 2017) encoder for language modeling.\nHyper-parameters We use BERT, ALBERT and RoBERTa Base uncased models; ﬁne-tuning them\nwith learning rate l = 1e-5, l ∈{1e-4, 1e-5, 2e-5, 1e-6 }. We use the Base cased pre-trained XLNet\ntuning the learning rate over the same range as for BERT models. For ULMFiT, we use AWD-LSTM\ntrained on Wikitext-103. We simplify the default ﬁne-tuning by only unfreezing the last 1 layer, the last\n2 layers and all layers with learning rates l1 = 1e−4\n2.64 , l2 = 1e−4\n2.63 and l3 = 1e-3 respectively. For M-BERT,\nthe size of feature embeddings (Emotion, Topics and Emotion+Topics) is h = 200, h ∈{200, 400, 768}\nwith dropout d = 0.1, d ∈{.1, .5}using the same parameters as BERT. The maximum sequence length\nis set to 49 covering 95% of tweets in the training set.\nEvaluation Following Preotiuc-Pietro et al. (2019), we use a nested 10-fold cross-validation approach\nto conduct our experiments for complaint prediction. In the outer 10 loops, 9 folds are used for training\nand one for testing; while in the inner loops, a 3-fold cross-validation method is applied where 2 folds\nare used for training and one for validation. During training, an early stopping method is applied based\non the validation loss. We measure predictive performance using the mean Accuracy, Precision, Recall\nand macro F1 over 10 folds (we also report the standard deviations).\nPredictive Results Table 2 shows results of the Transformer-based models as well as the baselines on\nthe complaint prediction task. All transformer-based models (BERT, ALBERT, RoBERTa and XLNet)\nperform better than the previous feature-based (LR-BOW + Dist. Supervision) and the non-transformer\ntransfer learning baseline (ULMFiT), indicating a better capability on capturing idiosyncrasies of com-\nplaints syntax and semanatics. BERT outperforms other models overall across all metrics reaching a\nmacro F1 up to 87, which is 8% higher than the previous state-of-the-art (Preotiuc-Pietro et al., 2019).\nThe results of RoBERTa are close to BERT with 86.6 macro F1 while ALBERT and XLNet achieve\nlower performance.\nDistant supervision is beneﬁcial only to ULMFiT and M-BERT while BERT and other transformer\nmodels perform worse, which are consistent with results of Bataa and Wu (2019) for sentiment analysis.\n3We experiment by injecting emotion (M-BERT - Emotion) or topical information (M-BERT - Topics) and their combi-\nnation (M-BERT - Emotion+Topics)\n1768\nModel Acc P R F1\nPreotiuc-Pietro et al. (2019)\nLR-BOW + Dist. Supervision 81.2 - - 79.0\nLSTM 80.2 - - 77.0\nTransfer Learning Baseline\nULMFiT 82.4 ± .04 81.1 ± .04 81.8 ± .04 81.2 ± .05\nULMFiT + Dist. Supervision 83.3 ± .05 82.5 ± .05 81.8 ± .04 81.9 ± .05\nTransformers\nBERT 88.0 ± .03 87.1 ± .03 87.3 ± .03 87.0 ± .03\nALBERT 85.9 ± .03 84.8 ± .03 84.6 ± .03 84.6 ± .03\nRoBERTa 87.6 ± .03 86.6 ± .03 86.9 ± .03 86.6 ± .03\nXLNet 83.9 ± .04 83.2 ± .04 82.3 ± .03 82.4 ± .05\nM-BERT - Emotion 87.3 ± .03 86.5 ± .04 86.0 ± .03 86.1 ± .04\nM-BERT - Topics 87.5 ± .03 86.7 ± .04 86.5 ± .03 86.4 ± .03\nM-BERT - Emotion+Topics 87.1 ± .03 86.4 ± .03 85.6 ± .03 85.9 ± .03\nBERT + Dist. Supervision 87.8 ± .03 87.0 ± .04 86.7 ± .03 86.7 ± .04\nALBERT + Dist. Supervision 83.9 ± .04 82.6 ± .04 82.7 ± .04 82.6 ± .04\nRoBERTa + Dist. Supervision 85.2 ± .04 84.4 ± .05 84.0 ± .04 84.0 ± .04\nXLNet + Dist. Supervision 82.1 ± .05 81.7 ± .05 79.9 ± .05 80.1 ± .05\nM-BERT - Emotion + Dist. Supervision 87.7 ± .04 86.9 ± .04 87.2 ± .03 86.8 ± .04\nM-BERT - Topics + Dist. Supervision 87.6 ± .05 87.0 ± .05 86.9 ± .04 86.7 ± .05\nM-BERT - Emotion+Topics + Dist. Supervision 87.8 ± .04 87.1 ± .05 87.0 ± .04 86.9 ± .04\nTable 2: Accuracy (Acc), Precision (P), Recall (R) and F1-Score (F1) for complaint prediction ( ±std.\ndev.) Best results are in bold.\nAlso, results of M-BERT models are comparable to BERT, among which M-BERT - Topics is slightly\nbetter with 86.4 macro F1. We notice that injecting external linguistic information in BERT’s structure\nfor ﬁne-tuning does not help in our case without substantially hurting performance. We speculate that\nmodifying BERT embeddings by injecting extra linguistic information is not complementary to BERT’s\ntext representations.\nError Analysis We also investigate the limitations in predicting capacity of our best performing model\n(BERT). We randomly analyze 100 cases in predictive results, where 50 cases were misclassiﬁed as non-\ncomplaints and another 50 cases were misclassiﬁed as complaints. In cases where complaints were\nmisclassiﬁed as non-complaints, 26% errors are due to implicit expressions while 14% errors are be-\ncause complaints contain irony. In the former situation, complaints express weak emotional intensity\nwithout explicit reproach, where complainers imply their dissatisfaction instead of directly complaining\nor mentioning the cause (Trosborg, 2011). The following tweet is a typical example:\nIt started yesterday , but I try again it could work normal. But since last night its just like this <url>\nSuch expressions rarely include words related to complaints (e.g. ‘disappointed’, ‘bad service’) and are\ntherefore difﬁcult to be correctly classiﬁed. In the latter situation, complaints are expressed in an ironic\nway using terms such as ‘congratulations’, ‘thank you’ and ‘brilliant’. For instance, the following text\nwas wrongly classiﬁed as a non-complaint:\nThank you so much for making a box that shreds apart even when carried by both handles.\nIn cases where non-complaints were misclassiﬁed as complaints, errors can be roughly divided into four\ncategories: (1) 26% errors are because certain terms appear frequently in complaints during training such\nas ‘thank you’, ‘dm’, ‘lost’, ‘work’. The following non-complaint was wrongly classiﬁed as a complaint:\nBTW <user> – <user> did me right, and replaced my two failed batteries under warranty. I’m happy :) thanks\n<user>!\nIt contains similar words with the following complaint in the same fold (similarities highlighted in bold):\nWas happy to ﬁnd out <user> had an app to watch all their shows, until 6 episodes in it stops working. Thanks!\n<user>\n1769\nTest\nTrain\nF A R C Se So T E O\nFood - 69.7 49.8 76.5 53.2 85.7 61.8 73.3 56.8 76.2 59.2 71.2 52.7 74.4 61.1 83.0 48.7\nApparel 75.0 69.7 - 74.2 81.5 78.8 74.3 72.7 76.3 73.2 81.8 66.7 74.1 75.7 75.9 81.6 84.2\nRetail 74.9 75.9 72.8 80.0 - 80.0 73.0 75.4 75.9 75.9 80.7 70.0 74.9 72.7 75.3 86.6 79.5\nCars 76.1 57.1 70.2 62.1 75.5 65.1 - 70.2 51.6 75.1 70.6 71.3 62.3 74.4 61.4 82.1 71.8\nServices 79.8 64.7 71.1 82.4 77.2 76.5 77.5 75.4 - 73.8 78.6 73.4 76.3 75.5 79.4 78.9 83.0\nSoftware 74.6 69.3 70.4 80.0 73.5 77.5 79.1 78.0 77.9 76.4 - 73.4 75.2 76.7 76.2 81.7 82.0\nTransport 72.5 62.2 70.5 73.5 77.1 80.0 80.0 79.2 74.4 76.3 75.8 75.3 - 72.4 70.4 82.0 82.6\nElectronics 69.9 72.9 72.2 78.5 69.1 78.9 73.0 75.4 77.0 78.0 71.0 72.4 69.8 69.7 - 82.1 80.8\nOther 65.9 64.8 75.2 74.8 79.2 72.2 81.7 69.2 76.3 69.9 76.5 77.9 70.6 70.6 72.8 70.8 -\nAll 48.9 77.5 67.5 87.7 72.6 85.8 73.9 80.9 72.0 81.1 65.8 85.1 64.9 81.4 67.6 82.0 81.9 88.2\nTable 3: F1-score of models in Preotiuc-Pietro et al. (2019) (left) and BERT (right) trained from one\ndomain and tested on other domains. Domains include Food (F), Apparel (A), Retail (R), Cars (C),\nServices (Se), Software (So), Transport (T), Electronics (E) and Other (O). The All line shows results on\ntraining on all categories except the category in testing. Best results are in bold.\n(2) 22% errors due to interrogative tone, which is common in complaints. An example is “Folks , what\nis cost of text message to a us number?” (3) 22% errors are from negation words such as “No luck with\npc or phone. ” (4) 12% errors are because texts contain negative sentiment such as “This would be a\nterrible idea <url>” are likely to be classiﬁed as complaints incorrectly since words such as ‘terrible’\nare widely used to express dissatisfaction. However, there are not enough cues to indicate violation of\nexpectations. According to the statistics, the proportion of complaints misclassiﬁed as non-complaints\n(15.22%) is higher than that of non-complaints misclassiﬁed as complaints (10.25%) indicating implicit\nand ﬁgurative expressions as well as unknown factors in complaints are more challenging to identify.\nCross Domain Experiments Finally, we use BERT to train models on one domain and test on an-\nother as well as training on all domains except the one that the model is tested on. Table 3 shows the\nperformance of models in Preotiuc-Pietro et al. (2019) (left) and BERT (right) across 9 domains. We\nﬁrst observe that BERT results in nearly half of the cases when training on a single domain are lower\nthan LR-BOW (especially ‘Food’, ‘Car’ and ‘Other’) while BERT trained on all domains performs bet-\nter across all testing domains, achieving a macro F1 up to 88.2 when tested on ‘Other’. This indicates\nthat, ﬁne-tuning BERT on a small training data set (‘Food’, ‘Car’ and ‘Other’ are three of the domains\nwith the smaller amount of data) is not enough to make it perform well. In contrast, it achieves better\nperformance consistently on larger data sets (All). We also notice that BERT performs robustly for do-\nmain pairs where the domains are either used for training or testing. For example, training on ‘Apparel’\nachieves high performance when testing on ‘Software’ (81.8 F1) and vice versa (80.0 F1). Furthermore,\ndomain relevance affect predictive performance. For example, BERT trained on ‘Transport’ achieves\n79.2 F1 when tested on ‘Car’, which is the highest performance compared to other training domains\nsince these two domain share common vocabulary (see ‘Car’ column for BERT).\n5 Conclusion\nWe evaluated a battery of transformer networks on the Twitter complaint identiﬁcation task and obtained\n87 macro F1, which outperforms the previous state-of-the-art results of Preotiuc-Pietro et al. (2019). We\nfurther presented a thorough analysis of the limitations of our models in predicting complaints. In future\nwork, we intend to explore more in how we can combine other sources of linguistic information with\ntransformers as well as information from other modalities such as images.\nAcknowledgements\nNikolaos Aletras is supported by ESRC grant ES/T012714/1.\n1770\nReferences\nEnkhbold Bataa and Joshua Wu. 2019. An Investigation of Transfer Learning-Based Sentiment Analysis in\nJapanese. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\n4652–4657. Association for Computational Linguistics.\nShreesh Kumara Bhat and Aron Culotta. 2017. Identifying leading indicators of product recalls from online\nreviews using positive unlabeled learning and domain adaptation. In Eleventh International AAAI Conference\non Web and Social Media.\nAndrew D Cohen and Elite Olshtain. 1993. The Production of Speech Acts by EFL Learners. Tesol Quarterly,\n27(1):33–56.\nKristof Coussement and Dirk Van den Poel. 2008. Improving customer complaint management by automatic\nemail classiﬁcation using linguistic style features as predictors. Decision Support Systems, 44(4):870–882.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of Deep Bidirec-\ntional Transformers for Language Understanding. InProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), page 4171–4186.\nPaul Ekman. 1992. An Argument for Basic Emotions. Cognition & Emotion, 6(3-4):169–200.\nJ Forster and B Entrup. 2017. A Cognitive Computing Approach for Classiﬁcation of Complaints in the Insur-\nance Industry. In IOP Conference Series: Materials Science and Engineering , volume 261, page 012016. IOP\nPublishing.\nD Gunawan, RP Siregar, RF Rahmat, and A Amalia. 2018. Building automatic customer complaints ﬁltering\napplication based on Twitter in Bahasa Indonesia. In Journal of Physics: Conference Series, volume 978, page\n012119. IOP Publishing.\nAditya Gupta and Greg Durrett. 2019. Effective Use of Transformer Networks for Entity Tracking. arXiv preprint\narXiv:1909.02635.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735–\n1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 328–339. Association for Computational Linguistics.\nJiahua Jin, Xiangbin Yan, You Yu, and Yijun Li. 2013. Service Failure Complaints Identiﬁcation in Social Media:\nA Text Classiﬁcation Approach.\nLeila Nasiri Kakolaki and Mohsen Shahrokhi. 2016. Gender Differences in Complaint Strategies among Iranian\nUpper Intermediate EFL Students. Studies in English Language Teaching4 (1), pages 1–15.\nRobin M Kowalski. 1996. Complaints and complaining: Functions, antecedents, and consequences. Psychologi-\ncal Bulletin, 119(2):179.\nM Lailiyah, S Sumpeno, and IK E Purnama. 2017. Sentiment Analysis of Public Complaints Using Lexical Re-\nsources Between Indonesian Sentiment Lexicon and Sentiwordnet. In2017 International Seminar on Intelligent\nTechnology and Its Applications (ISITIA), pages 307–312. IEEE.\nJanice Laksana and Ayu Purwarianti. 2014. Indonesian Twitter Text Authority Classiﬁcation For Government\nin Bandung. In 2014 International Conference of Advanced Informatics: Concept, Theory and Application\n(ICAICTA), pages 129–134. IEEE.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. arXiv preprint\narXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach.\narXiv preprint arXiv:1907.11692.\n1771\nAntonios Maronikolakis, Danae S´anchez Villegas, Daniel Preotiuc-Pietro, and Nikolaos Aletras. 2020. Analyzing\npolitical parody in social media. In Proceedings of the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 4373–4384.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and Optimizing LSTM Language\nModels. arXiv preprint arXiv:1708.02182.\nFebina Merson and Roseline Mary. 2017. A Text Mining Approach to Identify and Analyse Prominent Issues from\nPublic Complaints. International Journal of Advanced Research in Computer and Communication Engineering,\n6(3).\nE Olshtain and L Weinbach. 1985. Complaints: A study of speech act behavior among native and non-native\nspeakers of Hebrew.\nSachin Pawar, Nitin Ramrakhiyani, Girish K Palshikar, and Swapnil Hingmire. 2015. Deciphering Review Com-\nments: Identifying Suggestions, Appreciations and Complaints. In International Conference on Applications of\nNatural Language to Information Systems, pages 204–211. Springer.\nDaniel Preot ¸iuc-Pietro, Vasileios Lampos, and Nikolaos Aletras. 2015. An analysis of the user occupational class\nthrough Twitter content. In Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npages 1754–1764.\nDaniel Preotiuc-Pietro, Mihaela Gaman, and Nikolaos Aletras. 2019. Automatically Identifying Complaints in\nSocial Media. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics ,\npages 5008–5019.\nDaniel Preot ¸iuc-Pietro, Svitlana V olkova, Vasileios Lampos, Yoram Bachrach, and Nikolaos Aletras. 2015. Study-\ning user income through language, behaviour and affect in social media. PLOS ONE, 10(9):1–17, 09.\nWasifur Rahman, Md Kamrul Hasan, Amir Zadeh, Louis-Philippe Morency, and Mohammed Ehsan Hoque. 2019.\nM-BERT: Injecting Multimodal Information in the BERT Structure. arXiv preprint arXiv:1908.05787.\nBeverley A Sparks and Victoria Browning. 2010. Complaining in Cyberspace: The Motives and Forms of Hotel\nGuests’ Complaints Online. Journal of Hospitality Marketing & Management, 19(7):797–818.\nSuhatati Tjandra, Amelia Alexandra Putri Warsito, and Judi Prajetno Sugiono. 2015. Determining Citizen Com-\nplaints to The Appropriate Government Departments using KNN Algorithm. In 2015 13th International Con-\nference on ICT and Knowledge Engineering (ICT & Knowledge Engineering 2015), pages 1–4. IEEE.\nAnna Trosborg. 2011. Interlanguage Pragmatics: Requests, Complaints, and Apologies , volume 7. Walter de\nGruyter.\nCamilla V´asquez. 2011. Complaints online: The case of TripAdvisor. Journal of Pragmatics, 43(6):1707–1717.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention Is All You Need. In Advances in Neural Information Processing Systems ,\npages 5998–6008.\nSvitlana V olkova and Yoram Bachrach. 2016. Inferring Perceived Demographics from User Emotional Tone\nand User-Environment Emotional Contrast. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1567–1578.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2019. Words Can\nShift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 33, pages 7216–7223.\nOrion Weller and Kevin Seppi. 2019. Humor Detection: A Transformer Gets the Last Laugh. arXiv preprint\narXiv:1909.00252.\nWei Yang, Luchen Tan, Chunwei Lu, Anqi Cui, Han Li, Xi Chen, Kun Xiong, Muzi Wang, Ming Li, Jian Pei, et al.\n2019a. Detecting Customer Complaint Escalation with Recurrent Neural Networks and Manually-Engineered\nFeatures. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 2 (Industry Papers), pages 56–63.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019b. XL-\nNet: Generalized Autoregressive Pretraining for Language Understanding. In Advances in Neural Information\nProcessing Systems, pages 5754–5764.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7732586860656738
    },
    {
      "name": "Complaint",
      "score": 0.7536444664001465
    },
    {
      "name": "Transformer",
      "score": 0.7101014852523804
    },
    {
      "name": "Social media",
      "score": 0.6336724162101746
    },
    {
      "name": "Artificial neural network",
      "score": 0.6023704409599304
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5217269659042358
    },
    {
      "name": "Natural language processing",
      "score": 0.5215504169464111
    },
    {
      "name": "Language model",
      "score": 0.5168564915657043
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5072771906852722
    },
    {
      "name": "Sentiment analysis",
      "score": 0.49783945083618164
    },
    {
      "name": "Machine learning",
      "score": 0.43889135122299194
    },
    {
      "name": "Macro",
      "score": 0.42382222414016724
    },
    {
      "name": "Feature engineering",
      "score": 0.4135449230670929
    },
    {
      "name": "Deep learning",
      "score": 0.15724551677703857
    },
    {
      "name": "World Wide Web",
      "score": 0.13095346093177795
    },
    {
      "name": "Engineering",
      "score": 0.1254546344280243
    },
    {
      "name": "Voltage",
      "score": 0.08901813626289368
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ],
  "cited_by": 23
}