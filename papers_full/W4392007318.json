{
  "title": "Analyzing Public Sentiment on the Amazon Website: A GSK-Based Double Path Transformer Network Approach for Sentiment Analysis",
  "url": "https://openalex.org/W4392007318",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4209822824",
      "name": "Lella Kranthi Kumar",
      "affiliations": [
        "VIT-AP University"
      ]
    },
    {
      "id": "https://openalex.org/A2977259269",
      "name": "Venkata Nagaraju Thatha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4309064213",
      "name": "Pamula Udayaraju",
      "affiliations": [
        "SRKR Engineering College"
      ]
    },
    {
      "id": "https://openalex.org/A2663024160",
      "name": "D. Siri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186189170",
      "name": "G. Uday Kiran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2624166656",
      "name": "B.N Jagadesh",
      "affiliations": [
        "VIT-AP University"
      ]
    },
    {
      "id": "https://openalex.org/A2609137028",
      "name": "Ramesh Vatambeti",
      "affiliations": [
        "VIT-AP University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3186126799",
    "https://openalex.org/W3208530246",
    "https://openalex.org/W6801468585",
    "https://openalex.org/W3167586230",
    "https://openalex.org/W3154201795",
    "https://openalex.org/W3162226099",
    "https://openalex.org/W4214485311",
    "https://openalex.org/W4318815986",
    "https://openalex.org/W6796961802",
    "https://openalex.org/W3213996819",
    "https://openalex.org/W3193358494",
    "https://openalex.org/W4200550752",
    "https://openalex.org/W3157187508",
    "https://openalex.org/W4290610305",
    "https://openalex.org/W3194063622",
    "https://openalex.org/W3163181833",
    "https://openalex.org/W3127732444",
    "https://openalex.org/W4289865915",
    "https://openalex.org/W3119075751",
    "https://openalex.org/W4321351836",
    "https://openalex.org/W3175698312",
    "https://openalex.org/W4318499279",
    "https://openalex.org/W4321021942",
    "https://openalex.org/W6854381869",
    "https://openalex.org/W3133063166",
    "https://openalex.org/W4378087456",
    "https://openalex.org/W4367047069",
    "https://openalex.org/W4206021551",
    "https://openalex.org/W4317608819",
    "https://openalex.org/W4311057500",
    "https://openalex.org/W6855945335",
    "https://openalex.org/W4319990409",
    "https://openalex.org/W4210545034",
    "https://openalex.org/W4225939240",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4387406163",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386327568",
    "https://openalex.org/W2998621280",
    "https://openalex.org/W2963647456",
    "https://openalex.org/W2995428710",
    "https://openalex.org/W4386491607",
    "https://openalex.org/W4220961503",
    "https://openalex.org/W4386081833",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3171876138",
    "https://openalex.org/W3201223563",
    "https://openalex.org/W4384918874"
  ],
  "abstract": "Sentiment Analysis (SA) holds considerable significance in comprehending public perspectives and conducting precise opinion-based evaluations, making it a prominent theme in natural language processing research. With the increasing trend of online shopping and social media usage, there is a constant influx of diverse data types such as images, videos, audio, and text. Notably, text stands out as the most crucial form of unstructured data, demanding heightened attention from researchers. Given the voluminous nature of data, various methodologies have been proposed to effectively mine big datasets for valuable insights. The challenge of accurately identifying polarity in extensive customer evaluations persists due to the intricacies associated with handling large textual datasets derived from reviews, comments, tweets, and posts. This study addresses this challenge by presenting a straightforward architecture, the Double Path Transformer Network (DPTN), designed to model both global and local information for comprehensive review categorization. To enhance the synergy between the attention path and the convolutional path, the study advocates a parallel design that combines a robust self-attention mechanism with a convolutional network. The research employs the gaining-sharing knowledge optimization (GSK) approach to fine-tune hyperparameters, thereby improving the model&#x2019;s classification accuracy. Additionally, the investigation demonstrates that optimization algorithms and deep learning collaboratively manage class imbalances with finesse, even in the absence of explicit measures for such concerns. In the experiment analysis of the proposed model ultimately achieved an accuracy of 95.",
  "full_text": " \nVOLUME XX, 2017   1 \n \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nAnalyzing Public Sentiment on the Amazon \nWebsite: A GSK-based Double Path \nTransformer Network Approach for Sentiment \nAnalysis \nLella Kranthi Kumar1, Venkata Nagaraju Thatha2, Pamula Udayaraju3, D.Siri4, G Uday \nKiran5, B.N. Jagadesh6 , Ramesh Vatambeti7  \n1,6,7 School of Computer Science and Engineering, VIT-AP University, Vijayawada 522237, India. \n2Department Department of Information Technology, MLR Institute of Technology, Hyderabad 500043, India. \n3Department of Computer Science and Engineering, SRKR Engineering College, Bhimavaram ‚Äì 534204, India. \n4Department of Computer Science and Engineering, Gokaraju Rangaraju Institute of Engineering and Technology, Hyderabad 500090, India. \n5Department of CSE (AI & ML), B V Raju Institute of Technology, Narsapur-502313, India. \nCorresponding Author: Lella Kranthi Kumar (e-mail: kranthi1231@gmail.com) \nABSTRACT Sentiment Analysis (SA) holds considerable significance in comprehending public \nperspectives and conducting precise opinion -based evaluations, making it a prominent theme in natural \nlanguage processing research. With the increasing trend of online shopping and social media usage, there is \na constant influx of diverse data types such as images, videos, audio, and text. Notably, text stands out as the \nmost crucial form of unstructured data, demanding heightened attention from researchers. Given the \nvoluminous nature of data, various methodologies have been proposed to effectively mine big datasets for \nvaluable insights. The challenge of accurately identifying polarity in extensive customer evaluations persists \ndue to the intricacies associated with handling large textual datasets derived from reviews, comments, tweets, \nand posts. This study addresses this  challenge by presenting a straightforward architecture, the Double Path \nTransformer Network (DPTN), designed to model both global and local information for comprehensive \nreview categorization. To enhance the synergy between the attention path and the conv olutional path, the \nstudy advocates a parallel design that combines a robust self -attention mechanism with a convolutional \nnetwork. The research employs the gaining -sharing knowledge optimization (GSK) approach to fine -tune \nhyperparameters, thereby improvi ng the model's classification accuracy. Additionally, the investigation \ndemonstrates that optimization algorithms and deep learning collaboratively manage class imbalances with \nfinesse, even in the absence of explicit measures for such concerns. In the experiment analysis of the proposed \nmodel ultimately achieved an accuracy of 95. \nINDEX TERMS  Amazon Review, Double Path Transformer Network, Gaining‚Äìsharing knowledge optimization, \nSentiment analysis, Unstructured data  \nI. INTRODUCTION \nA. BACKGROUND OF SENTIMENT ANALYSIS \nSince the dawn of civilization, the art of communication has \nbeen fundamental to strengthening interpersonal bonds. \nSocial media has developed into an effective instrument for \nnetworking, and as a result, almost every part of society uses \nit nowadays [1]. Online marketplaces make up the bulk of \nsocial media. The majority of consumers now choose to shop \nonline due to the fast expansion of e -commerce knowledge. \nDepending on the customer's experience, people can use \nsocial media to offer positive or negative f eedback on a \nvariety of circumstances, items, and resources. Because they \nhelp enhance the services, negative remarks are crucial to the \ncompany's success. Sentiment analysis is useful in this \ncontext [2]. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n2                                                            VOLUME XX, 2017 \nBy analysing the tone of reviews written about various \nproducts, sentiment analysis helps to reveal how customers \nfeel about these items. Sentiment analysis is typically \nperformed at three levels, according to various research \nstudies: sentence -level, docu ment-level, and phrase -level \n[3]. The process of sentiment analysis is illustrated in Figure \n1, with its sub-stages. \n \nFIGURE 1. Sentiment Analysis‚Äôs Stages \nB. IMPORTANCE OF REVIEWS IN SENTIMENT \nANALYSIS \nResearch into people's thoughts, feelings, attitudes, and \nemotions is known as sentiment analysis. According to [4], \nit's one of the hotspots for NLP research. The field of data \nmining has been extensively researched. Hence, this study \nhas expanded beyond the realm of science to encompass the \nfields of management and social science. As platforms like \nTwitter, microblogs, and chat rooms continue to grow in \npopularity, sentiment analysis is becoming an increasingly \nimportant tool [5]. Now, more than ever befo re, a vast \nquantity of opinions can be collected and analyzed digitally. \nOpinions are fundamental to all human endeavors, which is \nwhy sentiment analysis technologies find widespread usage \nin both commercial and non -profit sectors. As a result, they \nhave a  major impact on how we act [6]. Everyone else's \nviews and values shape our worldview, which in turn \ninfluences our beliefs, perceptions, and choices. It is for this \nreason that people frequently ask for other people's opinions \nwhen a decision needs to be made [7]. Both people and \nbusinesses can benefit from this. Customers rate and review \nitems on Amazon. The purpose of these reviews varies from \nproduct to product, but generally speaking, they help \nbusinesses improve their offerings and draw attention to \nthose that have received unfavorable feedback [8]. \nThe expansion of the internet is influencing the business \nsector since it speeds up the flow of information. Some \ncustomers will share their thoughts about a product they've \npurchased on review sites or social media [9]. Among the \nmany internet marketplaces, Amazon.com stands out. To \nhandle massive amounts of text, an opinion engine is \nrequired. The process of sentiment analysis makes use of the \ntext mining technique. Through the processing and analysis \nof massive amounts of data, text mining enables the \nuncovering of previously unseen information or trends [10]. \nThey state that text mining is able to help with problems like \nprocessing, organizing, grouping, and analyzing massive \nvolumes of unstructured material. Classification is one \nmining technique.  \nC. ADVANCED ARTIFICIAL INTELLIGENCE \nTECHNIQUES FOR SENTIMENT ANALYSIS \nAn ever -growing role for machine learning in sentiment \nanalysis has emerged within the last decade [11]. There has \nbeen a meteoric rise in the application of sentiment analysis \nwith the introduction and development of deep learning \nprocedures. Lack of clea rly defined confidence metrics for \npredictions made by such methods is a mutual matter in \nmachine learning [12], making it impossible to determine \nwhere in the review (feature) space trustworthy predictions \ncan be anticipated. \nIn the group of confidence predictors, conformal \nprediction offers a structure that lets data be shared, which \nlets very specific confidence measures be used at the instance \nlevel [13]. It is important to note that conformal prediction \ndoes not add any add itional requirements when creating \nconfidence predictors because all machine learning methods \ntypically assume exchangeability or, more frequently, that \nthe data is IID (Independent and Identically Distributed) \n[14]. \nUsing the word vector network, nearly 5.2 million reviews \nfrom the following categories were analyzed for sentiment \non Amazon: beauty, books, electronics, and home [15]. This \nwas done in conjunction with density -based conformal \nprediction, and one of the three datasets studied was the 50k \nIMDb review dataset, which is well-balanced (1:1). \nD. MOTIVATION OF THE RESEARCH WORK \nIn response to the increasing demand for SA, businesses and \norganizations are focusing on improving their public \nrelations, launching campaigns, strengthening their weak \nspots, and expanding their clientele. Feedback from \nconsumers regarding a company's go ods and services is \nhighly valued [16]. In addition, political groups like to know \nhow the general public perceives them and how the media \ncovers them. These days, SA is more concerned with \ndeciphering the feelings conveyed in reviews posted on \nsocial media. Many fields have begun to adopt SA, including \nthose dealing with harassment, politics, entertainment, \nsports, and even medicine [17]. \nCurrent research topics in SA include better natural \nlanguage processing (NLP) methods, data mining for \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   3 \n \npredictive investigation, and text contextual comprehension \n[18]. It has been common practice to employ support vector \nmachines (SVM) to address various NLP issues for quite \nsome time. Neural network (NN) techniques built on dense \nvector illustrations have  lately attained state -of-the-art \npresentations in various NLP -related applications [19]. In \ntheir early days, deep learning NNs showed great promise in \ntasks involving pattern recognition and computer vision. \nThis development has algorithms for handling complex NLP \ntasks like sentiment analysis. \nE. CONTRIBUTION OF THE STUDY  \nWhen the system's developers realized how crucial \nsentiment analysis was for online life, they set out to improve \nthe process. In terms of computer complexity, the proposed \nstrategy yields the same or better outcomes with the highest \nlevel of certainty and the lowest amount of effort expended \n[20]. The study has looked into how dif ferent preprocessing \nsteps, like cleaning and normalizing the data, removing \nhashtags and punctuation, changing the text to lowercase, \nand tokenizing, affected customer reviews. The research in \nthis paper employs optimized deep learning methods to \nexamine customer reviews on Amazon. \n‚Ä¢ At first, data is retrieved from Amazon.com and \nsubjected to a series of pre-processing procedures in order to \nclean it up. \n‚Ä¢ The characteristics are formed from the pre -\nprocessed texts using various word embedding approaches. \n‚Ä¢ The research suggests a hierarchical dual -path \nbackbone to enhance review detection performance by \ngiving CNN global attention. \n‚Ä¢ We suggest a straightforward yet efficient solution \ncalled a bidirectional connection module that allows two \nparallel ways to communicate context information. \n‚Ä¢ Without utilizing complicated and computationally \nintensive modules, the entire model produces features that \nare robust to scale through the use of a multi -head attention \nblock during the concentrating phase. \n‚Ä¢ At last, the study was assessed using various \nmetrics, including precision, recall, accuracy, and F1 -score, \nin conjunction with pre-existing methods. \nF. PAPER ORGANIZATION \nSection 1 begins with a brief overview of sentiment analysis \nand its significance before moving on to review \nclassification. Section 2 provides a comprehensive analysis \nof current models along with their limitations. Section 3 \nprovides the study materials, while Section 4 provides a brief \nclarification of the suggested model. As promised in Section \n5, here is the experimental data to back up the effectiveness \nof the suggested model. Section 6 concludes with a \ndemonstration of future work. \n \n \nII. RELATED WORK \n     A convolutional neural network (CNN) model for  \nnegative/positive sentiment classification in text reviews has \nbeen provided by Qorich & El Ouazzani [21]. To find the \nbest model, we also compared our suggested CNN model \nwith other models' word embedding representations. Using \nthe Amazon reviews dataset, the tests show that several \nmodel designs can obtain acceptabl e performances. Results \nshow that stop -words should be included in sentiment \nanalysis tasks; removing them can lead to a wrong prediction \nof sentiment. In practice, compared to the CNN model that \ndidn't use stop words, th e one that did saw an improvement \nof 2% in accuracy. Additionally, we proved that on large -\nscale datasets, using a random initialization method \noutperforms supervised and embedded model vectors. The \nmodel can learn more accurate features with less \ncomputational effort after training the word embedding \nrepresentation. In addition, our CNN model outperformed \nthe baseline ML and DL approaches, and we increased CNN \naccuracy to 90% on the Amazon reviews dataset. \n \nResearchers V ollero et al. [22] set out to use social media  \ncomments made by Amazon customers to determine what \nfactors most influence the company's stock price. Using \nNLP-based methods, they analyzed the content and \nsentiment of user comments extracted from the Facebook \npages of three major Italian retailers over the course of two \nyears (2016 -2018) to determine the extent to which these \nretailers' service attributes were associated with Amazon -\nrelated customer dissatisfaction. Consumers have a lot to say \nabout the impact of Amazon on retailers of consumer \nelectronics, particularly in the areas of pricing, service, in -\nstore personnel, and after -sale assistance. Consumers' \nunfavorable views in Facebook comments, when compared \nwith similar assessments on the I talian Amazon website, \nindicate that Amazon's service standards have increased \nconsumer expectations and decreased consumer satisfaction \nwhen interacting with other businesses. Beyond the \ncommonly recognized determinants of price and logistics, \nthey propose additional study to elucidate Amazonification \nin terms of customer impatience and discontent more \nbroadly. \n \nThe goal of the research by Venkataraman and Jadhav [23] \nwas to analyze customer reviews of mobile phones, \ncategorize them into different star ratings, and then find out \nhow accurate the ratings were in expressing the mood. This \ncould only be accomplished once the data had been cleansed \nand pre -processed. The TF -IDF approach and word \nembedding were then used to convert the text into numerical \nnumbers. Ultimately, various methods such as Support \nVector Machines, Logistic Regression, and Ensemble \nmodels were employed, and the accuracy of their \nperformance was studied. A number of criteria are utilized \nfor evaluation, including recall, accuracy, precision, and the \nF1-Score. On a balanced classifier with Unigram performs \nbetter. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n4                                                            VOLUME XX, 2017 \nA unique hybrid recommender system has been proposed by \nElahi et al. [24] that can analyze reviews and extract feelings \nto use in making recommendations. To provide suggestions \nfor those who can add extra data, like the review sentiment, \nthey deployed sophisticated algorithms. In several cases, \nsuch as the music industry, they found no strong c orrelation \nbetween the ratings and the sentiments expressed in user \nevaluations. As a result, sentiment may serve as an additional \nindicator of user input, revealing a new facet of consumer \npreferences. To evaluate the effectiveness of their suggested \nhybrid recommender system, they have taken into account \nboth star ratings and review sentiment. For this study, they \nused two popular datasets ‚ÄîAmazon Digital Music and \nGames‚Äîand demonstrated that the suggested hybrid \nrecommender system outperformed several bas elines. Two \nevaluation scenarios were used to make the comparisons: \none where the ratings were taken as user feedback and the \nother where the review sentiments were used as user input. \n \nTo recover the rating matrix and aid in recommendation, Liu \nand Zhao [25] provide a sentiment analysis and matrix \nfactorization (SAMF) based recommendation system. This \nsystem employs deep learning technologies and topic models \nto fully mine the implicit information in reviews. First, LDA \n(Latent Dirichlet Allocation) is used to construct user topic \ndistribution from reviews, which comprise both user reviews \nand item reviews. On the basis of topic likelihood, the user \nfeature matrix and object feature matri x are generated. The \nsecond step is to combine the user feature matrix with the \nitem feature matrix to form a user-item preference matrix. In \nthe third step, the user -item rating matrix is created by \nintegrating the original rating matrix with matrix. Lastly, the \nuser-item rating matrix is updated and modified using BERT \n(Bidirectional Encoder which quantifies the sentiment info \nin the reviews and integrates it with the rating matrix. Lastly, \nrating prediction and Top -N suggestion are accomplished \nusing the  updated user -item rating matrix. Results from \nexperiments conducted using Amazon datasets show that the \nsuggested SAMF outperforms other traditional algorithms \nwhen it comes to suggestion performance. \n \nIn their publication, Jin et al. [26] introduced the Amazon -\nM2 Multilingual Multi -Locale Shopping Session Dataset. \nWith millions of user sessions across six countries and \nlanguages, this dataset is the first of its kind. English, \nGerman, Italian, and Spani sh are the most common product \nlanguages. Surprisingly, the dataset can aid in improving \npersonalization and understanding of user preferences, \nwhich in turn can enable new activities and improve existing \nones. This work introduces three tasks: next -product title \ngeneration‚Äîto assess the possibilities of the dataset. By \ncompleting the aforementioned tasks, we are able to compare \nvarious algorithms on our suggested dataset, which allows us \nto gain valuable insights for future studies and applications. \nThe KDD CUP 2023 competition we ran, which was based \non the suggested dataset and challenges, also had thousands \nof entries. \n \nAs recommendations to users, Lakshmi & Bhavani [27] have \nprovided the top k links predicted by link prediction \nmeasures. At first, they modify various preexisting link \nprediction methods and apply them to the recommendation \nissue. A recommendation framewor k based on the problem \nin social networks is proposed as the main contribution of \nthis work. This framework makes use of incorporates \ntemporal data that is available on existing links. We test the \nsuggested method on three different datasets: one from \nMovieLens, one from Epinions and Amazon, and one from \nTripAdvisor, which contains ratings for hotels. When it \ncomes to enhancing recommendation quality, the results \nreveal that link prediction techniques based on temporal \nprobabilistic info work better. Regard ing Movie Lens, \nEpinions, TripAdvisor, and Amazon specifically, the area \nunder the ROC curve (AUROC) is improved by 10%, 23%, \n17%, and 9% respectively, as compared to the usual item -\nbased collaborative filtering method using the temporal \ncooccurrence proba bility measure. Area under Normalized \nRank-Score both show comparable enhanced performance. \n \nUsing the Transformer Perfect‚Äîa deep learning perfect that \neffectively integrates textual and utility matrix info ‚ÄîHo et \nal., [28] have projected a new approach. In order to get the \nmost out of the Transformer model, the research \nrecommends feature extraction strategies that are well-suited \nto each data source. With a reduction ranging from 10.79% \nto datasets, the experimental results show that the suggested \nmodel considerably enhances recommendation accuracy \nwhen associated with the baseline for the metric.  The \nsuggested model also reduces the MAE measure by 34.82% \nto 56.17% for the Amazon sub -datasets in comparison to \nSVD, widely recognized as one of the best representations \nfor recommender systems. With improvements in Precision \nof up to 108%, MAE of up to  65.37%, and RMSE of up to \n59.24%, our suggested model surpasses the graph -based \nmodel in terms of performance. Results from experiments \nconducted on Amazon datasets further show that our \nsuggested strategy outperforms a model that just relies on \nutility matrix data by incorporating information from textual \nsources. \n \nConcerning user behavior data in FedRecs, Yuan et al. [29] \nhave investigated the privacy concern. To be more precise, \nwe conduct the initial comprehensive investigation into \nassaults on FedRecs that aim to infer membership at the \ninteraction level. After d esigning an attacker that targets \ninteractions through membership inference, the protection \nmechanism known as Local Differential Privacy (LDP) is \nused to counter this attack. Unless recommendation \nperformance is severely impaired, actual research reveals \nthat LDP is ineffective against such novel threats. We \ndevelop a straightforward defense mechanism to lessen the \nimpact of interaction-level membership attacks by lowering \nthe attacker's inference accuracy while maintaining \nrecommendation presentation. The efficacy of our solutions \nis demonstrated by extensive trials carried out with two \npopular datasets. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   5 \n \n \nFor improved recommendation quality, Hiriyannaiah et al., \n[30] have taken data sparsity into account using a new neural \nCF-based DeepLSGR model. It is a bidirectional model that \nuses user-submitted text reviews to forecast ratings and then \nmakes recommenda tions based on those predictions. The \nmodel's hidden layers include (LSTM) and Gated Recurrent \nUnits (GRU). Its performance in studies using the OpinRank \nand Amazon Fine Food Reviews datasets was 97% accurate, \n61% recall, and 0.87 RMSE. By comparing it to previous \nworks, it is clear that DeepLSGR offers better \nrecommendations. \n \nTo overcome the issue of data distortion caused by sparsity, \nKuo & Li [31] used the particle swarm optimization method \n(PSO) to find the optimal similarity of customer ratings. In \naddition, the features of customer feedback were extracted \nusing representations from transformers (BERT). Lastly, the \nPSO was used to mix the features of various data kinds and \nfind the right weight matrix. If rating and review data were \ncombined, recommendation performance may be \nsignificantly enhanced. Furthermore, the suggested approach \nsurpassed multiple prior approaches in terms of mean \nsquared error and mean complete error when tested on six \nAmazon datasets. \n \nThe collaboration Variational Graph Auto-Encoder (CVGA) \nis a new end-to-end graph recommendation model projected \nby Zhang et al., [32]. It encodes user -item collaboration \ninteraction bipartite graph using the info propagation and \naggregation principles. In stead of learning user or item \nembeddings, these associations are used to infer the \nprobability distribution of user behavior for parameter \nestimation. In doing so, we make a plausible and elegant \nreconstruction of the full user -item interaction graph \nrendering to the known probability distribution. Looking at \nit through the lens, we can solve the graph recommendation \ntask with almost linear time complexity by transforming it \ninto a graph creation problem. We show that CVGA can be \ntrained quicker than state-of-the-art baselines for tasks while \nretaining comparable performance across four datasets. \nAdditional research confirms that CVGA is capable of \nsolving the data sparsity issue and works just as well with \nmassive datasets. \n \nA new method that uses LLMs to build personalized \nreasoning graphs has been proposed by Wang et al., [33]. To \nreflect the user's interests in a way that can be understood, \nthese graphs connect their profile with their behavioral \nsequences using causal and logical deductions. Chained \ngraph reasoning, knowledge base self -improvement makes \nup our technique, LLM reasoning graphs (LLMRG). In order \nto enhance traditional recommender systems, the resulting \nreasoning is encoded networks; this process does not \nnecessitate any new information about users or items. Our \nmethod shows how LLMs can make personalized reasoning \ngraphs that enable recommender systems to be more rational \nand interpretable. Recommendations can take advantage of \nLLM-derived reasoning graphs and constructed \nrecommendation systems with LLMRG. On both \nbenchmarks and real-world scenarios, we show that LLMRG \nimproves base recommendation models. \n \nA new method for recommendation based on the semantic \nawareness of HIN embeddings, SemHE4Rec, was proposed \nby Pham et al., [34]. To learn user and item representations \nin HIN efficiently, we offer the SemHE4Rec model, which \nincludes two embedding approache s. Then, the matrix \nfactorization (MF) procedure is made easier with these user \nrepresentations that are rich in structural details. To begin, \nthere is the time -honored method of co -occurrence \nrepresentation learning (CoRL), which seeks to understand \nthe f requency with which users' and things' structural \ncharacteristics occur together. By means of meta -paths, the \nlinkages between these structural elements are depicted. \nSpecifically, we use the heterogeneous Skip -gram \narchitecture and the famous walk approac h to achieve this. \nThe second way for embedding is an SRL approach, which \nstands for semantic -aware representation learning. The \nrecommendation task's focus is the primary focus of the SRL \nembedding technique. Lastly, in order to complete the \nrecommendation task, all of the learned user and item \nillustrations are integrated with the extended MF and \noptimized together. In contrast to the state -of-the-art \nrecommendation techniques that use HIN embedding, the \nproposed SemHE4Rec performs well on real-world datasets. \nThe experiments also show that combining representation \nlearning improves recommendation performance. \n \nLastly, the literature research reveals that there are numerous \nobstacles impacting the effectiveness and accuracy of \nsentiment analysis and social media content evaluation. \nIII. MATERIALS AND METHODOLOGY \nA total of twelve groups consisting of five -core product \nappraisals retrieved from Amazon.com (Table 1) [35 -36] \ncomprise the data utilised in this study. The old grading \nsystem that used numbers from 1 to 5 was changed to a \nbinary system that used the numbers 1-2 for negative and the \nnumbers 3-5 for neutral and positive. The imbalance ratios \nof all datasets range from around 6.7:1 to 17.4:1, indicating \nthat they are significantly lopsided (Table 1). \n \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \nVOLUME XX, 2017   6 \n \nTABLE I \n DATASET CHARACTERISTICS \nTraining seta Test set \nCategory Successfu\nlb \nClass unbiassed \nand positive \nnumber of \nreviews \nClass neg \nnumber of \nreviews \nPercentage \nof class \nnegative \nreviews \nClass \nnegative \nmedian \nwords/ \nreview \nClass \nneutral \nand \npositive \nMedian \nwords/ \nreview \nClass \nneutral \nand \noptimistic \nnumber of \nreviews \nClass \nnegative \nof number \nreviews \nPercent\nage of \nclass \nnegative \nreviews \nClass \nnegative \nmedian \nwords/ \nreview \nClass neutral \nand positive \nmiddle words/ \nreview \ncds_and_vinyl yes 954,874 \n(375,5601) \n76,341 \n(19,648) \n7.4 \n(5.0) 105 (76) 99 (44) 39\\3,054 13,2\\94 3.3 36 13 \nbooks yes 11,656,232 \n(9,191,149) \n1,001,745 \n(692,288) \n7.9 \n(7.0) 88\\ (69) 62 (49) 13,60\\1,15\n9 828,64\\\\3 5.7 46 36 \nelectronics yes 2,268,955 \n(1,906,881) \n311,990 \n(248,8661) \n12.1 \n(11.5) 75 (67) 52 (46) 3,6299964 455,27\\8 11.1 42 22 \noffice_products yes 202,853 \n(185,812) \n18,947 \n(16,190) \n8.5 \n(8.0) 70 (63) 42 (39) 507,459 34,864 6.4 39 17 \ncell_phones_and_acc\nessories yes 276,950 \n(269,914) \n41,077 \n(39,370) \n12.9 \n(12.7) 45 (44) 37 (36) 709,108 97,290 12.1 32 21 \nsports_and_ outdoors No 694,7924 \n(667,185) \n57,191 \n(54,142) \n7.6 \n(7.5) 57 (55) 43 (42) 1,846,242\n5 149,287 7.5 36 22 \ngrocery_and_ \ngourmet_food No 31277,867 \n(285,490) \n29,125 \n(26,034) \n8.5 \n(8.4) 55 (53) 40 (38) 695,311 59,874 7.9 31 17 \narts_crafts_and_ \nsewing No 109,150 \n(107,346) \n6,265 \n(6,118) \n5.4 \n(5.4) 50 (49) 32 (31) 323,548 18,922 5.5 33 15 \nclothing_shoes_ and_ \njewelry No 2,119,165 \n(2,069,843) \n225,723 \n(220,110) \n9.6 \n(9.6) 41 (41) 34 (3\\4) 7,810,225 873,00\\5 10.1 27 19 \naValues in parenthesis are for dated 2011‚Äì 14 only. \nbDeep learning/Conformal Forecast perfect positively built (yes) or not (no). \nA. DATA PRE-PROCESSING \nText data analysis relies heavily on data pre-processing [37]. \nTweets, blogs, reviews, and other forms of textual content \nmight have repeats and redundancies, which can make text \ndata more complex. The study has looked into how different \npreprocessing steps, like cleaning and normalizing the data, \nremoving hashtags and punctuation, changing the text to \nlowercase, and tokenizing, affected customer reviews. Data \npre-processing is a filtering procedure that is used in data \nnormalization. For instance, data pre -processing may \ninvolve normalizing the data, tokenizing words, eliminating \nstop words, padding, and excess spaces, transforming text \ndata to lowercase, and removing hash tagging. The data in \nthe necessary format was achieved through the \nimplementation of numerous tasks in this job. \n1) ERASE PUNCTUATION \nBetween forty and fifty percent of every given piece of \nwritten text is punctuation. The results of any sentiment \nanalysis model are unaffected by punctuation. These \npunctuations are completely irrelevant to the sentiment \nanalysis, so it is crucial that yo u eliminate them. Here, we \ndisplayed the statistics in their normalized form after \nremoving any punctuation from the text. The resulting text is \ncondensed and made easier to understand. The input data \nwas stripped of any punctuation. \n2) CONVERT THE TEXT DATA TO LOWERCASE \nCustomers post content in reviews that doesn't adhere to \nstandard grammar rules; for example, the text has both \nlowercase and uppercase letters. The study makes extensive \nuse of approaches that are responsive to specific cases. \nConsequently, the classifier  struggles to identify the text's \npolarity. If the entire text is formatted according to industry \nstandards, this problem should go away. In contrast, the \nlower (txt) statement is utilized when the same procedure is \nto be executed manually. It changes all capital letters to \nlowercase while keeping all other characters in their original \nforms. Changing all capital letters to lowercase is \ndemonstrated in the following example: One possible way to \nlowercase \"I Am A Senior Big Data Analyst in Islamabad\" \nis to say \"I am a senior big data analyst in Islamabad.\" \n3) TOKENIZATION OF THE TEXT \nTokenization is a method for segmenting text streams into \nsmaller pieces of text, such as phrases. Tokens consist of \nbroken bits of text. The goal of this method is to simplify \ndifficult textual topics. When tokens are used, data mining \ngets easier. Semant ics and sentiment analysis both benefit \nfrom tokenization, and lexical evaluation relies heavily on it. \nAn integral part of the natural language processing pipeline \nis tokenization. The study will not be able to begin model \ncreation unless the text is clea ned properly. Word tokenize \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   7 \n \nand phrase tokenize are the two subsets of tokenization. This \nformatted data can be used for: \n‚Ä¢ Count the number of words in the text. \n‚Ä¢ Determine the word frequency. At this point, the \ntext data is broken down into words. Miniature word or \nsymbol packets are created from a huge and complicated \nrecord. For instance, \"I am a data analyst in Islamabad\" is \none possible tokenization of the aforementioned text. \n4) REMOVAL OF STOP WORDS \nText files often contain repeated words. Consequently, \nremoving the stop words is of the utmost importance. The \nuse of stop words never adds value to anything written. \nWords of this type tend to appear frequently in written \nworks. This selected data has th e stop words removed from \nit. This method improves system efficiency while reducing \ntextual content. \n5) REMOVAL OF THE HYPERLINK \nLinks have lost all significance in any database. The \nconnections are only useful in a functional sense. To refine \nthe text's polarity, the study exclusively uses tweets, \ncomments, and reviews as representations of ideas and \nfeelings based on the gathered data. Therefore, removing the \nlinks from the databases is of the utmost importance. \n6) REMOVAL OF HASH TAG \nAdditionally, hash tagging is quite trendy right now. \nHashtags are frequently used in customer feedback. \nHashtags take up a lot of space. When it comes to sentiment \nanalysis, hashtags don't work. For the classifiers, these \nsimply increase the level of unce rtainty. Therefore, it is \nabsolutely necessary to eliminate hashtags. The training data \nis made more concise and obvious by removing the hashtags \nfrom the datasets. \n7) REMOVAL OF UNNECESSARY SPACES \nAdditionally, hash tagging is quite trendy right now. \nHashtags are frequently utilized in customer feedback. A lot \nof space is consumed by hashtags. When it comes to \nsentiment analysis, hashtags don't work. For the classifiers, \nthese simply increase the level of uncertainty. Therefore, it \nis absolutely necessary to eliminate hashtags. The training \ndata is made more concise and obvious by removing the \nhashtags from the datasets. \n8) PADDING \nThe classifier has a hard time with sentiment analysis \nbecause the consumer review databases have both very brief \nand very lengthy reviews. The sum of pixels that are added \nto the review when the network evaluates it is known as \nCNN-related padding. Padding is just a few zeros added to \nthe end of our input review to make sure every customer \nreview is the same length. \n9) POS TAGGING \nClassifying training data words according to their \npredetermined grammatical form is the goal of POS. This \ngroup takes word meaning into account. Putting labels on \npoint-of-sale systems is no picnic. While point -of-sale \nlabeling can't fix the severe discovery problem in opinion \nanalysis, it helps a lot with many other issues. This procedure \ngathered several viewpoints and aspects for a product \nreview. To define a specific feature, one uses the altered POS \ntagger. The review gives users access to the grammar  \nrelations-provided POS tagging mechanism. To establish the \nspeech part of the examination, utilize In addition, POS tags \nidentified appropriate substantive and substantial potential \nproblems. The observable output is generated in this POS \nprocess by means  of a concealed Markov model (HMM), \nwhereby tags are concealed. Finding a mathematically \noptimal tag sequence (C) is always the goal when POS is \ntagging. \nùëÉ(ùê∂|ùëä)    (1) \nwhere C denotes ùê∂1, ùê∂2, ùê∂3, . . . , ùê∂ùëá, and W denotes \nùëä1, ùëä2, ùëä3, and ùëäùëá. \nB. Feature Extraction using Word Embedding \nA neural word-to-vector model, Word2Vec [38], predicts the \nvector of a needed word by using its surrounding words. The \nWord2Vec embedding model typically makes use of two \nlearning strategies: skip -gram and CBOW. The CBOW \nmethod uses the current word to bridge the gap between \ncontext words; therefore, it doesn't change the sequence o f \nnearby words, in contrast to the skip -gram strategy, which \nprioritizes nearby words over faraway ones. Using solely \nlocal context, CBOW and skip -gram both learn mutual \nvector representations for every word. \nIn contrast to Word2Vec, GloVe [39] neural word \nembedding takes the full context of words into account. In \nthe GloVe word embedding, a neural network is employed \nto decompose a matrix into a word vector. Because the \nGloVe embedding model takes into account  the association \namong word pairs and provides additional meanings to the \nneural network, it outperforms Word2Vec [39] in word \nresemblance and analogy tests. The weights of common \nword pairings, including \"the\" and \"a,\" are decreased via \nGloVe embedding as  well. On the other hand, a co -\noccurrence matrix ‚Äîthe foundation of the GloVe model ‚Äî\nrequires a substantial amount of memory for storage \npurposes. \nSince learning is a shared difficulty, FastText [40] follows \nWord2Vec's approach of learning the vector of each word \ntogether with the n -grams inserted within each word. The \nnext stage in training is to create a single vector by averaging \nthe representatio n values. Neural word embedding can \nencode substantial sub -word info using these models, \nalthough GloVe. When it comes to neural word embedding \nmodels, FastText is light years ahead of Word2Vec. \nTransformer bidirectional encoder representations; the \nacronym is BERT [41]. By conditioning the right and left \ncontext at all levels, BERT aims to bidirectional \nrepresentations from unlabeled data. For this reason, it is \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n8                                                            VOLUME XX, 2017 \npossible to train a pre -trained BERT model for tasks, \nincluding sentiment investigation, question answering, and \nlanguage translations. No major changes to the design are \nrequired with this method. When put into practice, BERT is \nboth simple and effective. \nIV. PROPOSED METHODOLOGY \nA. CONSTRUCTION OF THE PROPOSED NETWORK  \nFigure 2 depicts the structure of the model that we have \nsuggested. A backbone with Transformer enhancement, an \nattention decoder head with several heads, and a post -\nprocessing algorithm make up the pipeline. \nThe research uses a dual -path approach to integrate self -\nattention and convolution. By handling (FFN) for feature \noutput, it tries to gather information within and across initial \nreceptive fields.  Initially, following the conventional CNN \nbackbone model, the input data is passed through an FPN \nassembly to derive multi -level features. This structure \nconsists of four stages, each with a different downsampling \nrate: f4, 8, 16, 32g, and so on. The second step is to use multi-\nhead attention to extract after upsa mpling the extracted \nfeatures to the same scale. Two maps ‚Äîone for likelihood \nand one for threshold‚Äîwill be created using the feature. \n \nFIGURE 2. The Block for the Hybrid Transformer.  \nB. MATHEMATICAL DESIGN OF THE PROPOSED \nNETWORK \n1) TRANSFORMER-ENHANCED BACKBONE \nThe study can logically make CNN and Transformer operate \nin tandem to achieve greater performance since they each \nhave their own limits and complementary advantages. \nConvolutional layers are able to model local relations due to \ntheir inductive bias. Follow ing prior research [42], a 4 √ó 4 \nwindow is utilized, as shown in Figure 2. Our goal is to \nmaximize efficiency in the CNN path by applying depth -\nwise size. Also, we need to change the number of channels \nso they align with the merged branch, allowing them to  be \nseamlessly integrated because the two paths are \narchitecturally different. Their outputs are combined after \nthe channel adjustment and normalized using separate \nnormalization layers. By optimizing both parallel branches \nat the same time during training, we can achieve stronger \nfeature representation learning by weaving features across \nthe two branches. Then, the final output features are \ngenerated by fusing the learned relations in both routes using \na consecutive Feed Forward Network (FFN). \n2) FEATURE INTERACTION MODULE \nFigure 2 shows that there are bidirectional interactions \nbetween the output features of dual -path branches, and that \nthese features are not merely concentrated. Better \nrepresentation learning in both branches can be achieved \nwith the help of the complement ary clues provided by the \nparallel branches. As one route uses attention to extract \nchannel/spatial context, the other route uses channel/spatial \ninteraction to do the same. The channel and spatial \ninteractions make up the bidirectional interactions within the \nsame block. To begin with, the research utilizes a layout that \nis comparable to the SE layer [43], meaning that data from \nthe convolutional branch is transferred to the other branch \nvia the channel interaction. This structure includes a single \nGAP layer, two layers that are connected with normalization \nand activation, and provide a dynamic weight to various \nchannels. The channel sum is reduced to 1 using info from \nthe branch by means of two consecutive 1 √ó 1 layers that \ninterconnect with BN and GELU. A dditionally, a sigmoid \nlayer is utilized for the distribution of spatial weights. \n3) HYBRID TRANSFORMER BLOCK \nAn efficient parallel architecture, a FFN network make up \nthe hybrid Transformer block [44]. We also present residual \nconnections, which are defined in the following way: \nùëãÃÇùëñ = ùê∂ùëúùëõùëêùëéùë°(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùëãùëñ), ùëÄùëÜùê¥(ùëãùëñ), ùê∂ùëÇùëÅùëâ(ùëãùëñ) +\nùëãùëñ)       (2) \nùëãùëñ+1 = ùêπùêπùëÅ(ùêøùëéùë¶ùëíùëüùëÅùëúùëüùëö(ùëãÃÇùëñ) + ùëãÃÇùëñ) (3) \nwhere ùëãi where Conv is the depth-wise convolution process, \nMSA denotes the attention branch, and represents the \nfeatures at the ith block. \n4) MULTI-HEAD ATTENTION DECODER \nIn order to combine data from several scales, the majority of \nsemantic segmentation algorithms employ summing up or \ncascading techniques. But paying little attention means that \nsuch a basic fusion paradigm would necessarily miss some \ncrucial elements. Thus , in order to re -attend to the cases \nwhile maintaining the honesty of the spatially hidden text \nregion, the study employs a multi-head attention decoder. \nThe study scales the features from diverse stages into the \nsimilar resolve first so that they can be treated at the similar \nscale. The study denotes the feature maps made from the four \nphases in our backbone as ùëã ‚àà ùëÖùëÅ√óùê∂√óùêª√óùëä = {ùëãùëñ}ùëñ=0\nùëÅ‚àí1, where \nN is equal to 4. The study uses intermediate feature  ùëÜ ‚àà\nùëÖùê∂√óùêª√óùëä. The features are then divided into N pieces along \nthe channel dimension using a basic but effective multi-head \nattention module [45]. In this case, N is 4. In order to obtain \nthe final fused feature F, the relevant scaled features will be \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   9 \n \ngiven the data dependent dynamic weights. One way to \nexpress the aforementioned procedures is as: \nùêπÃÇ = ùê∂ùëúùëõùë£(ùê∂ùëúùëõùëêùëéùë°([ùëã0, ùëã1, ùëãùëÅ‚àí1]))  (4) \nThe research uses a dual -path approach to integrate self -\nattention and convolution. By handling input sending signals \nto the Feed -Forward Network (FFN) for feature output, it \ntries to gather information within and across initial receptive \nfields. \nùêπ = ùëÄùëÜùê¥(ùêπÃÇ)   (5) \nHere ùê∂ùëúùëõùëêùëéùë°( )stands for the 3 x 3 convolutional layers; \nùê∂ùëúùëõùë£ () signifies the concatenation operator. A multi -head \nself-attention system goes by the acronym MSA. According \nto the results, the study's head number is 4, which is also the \nstage number. In this case, \"i\" denotes the head's index. , ùëñ ‚àà\n{0,1,2,3}. \n5) POST-PROCESSING \nThe text areas we get can be made more expressive with the \nhelp of suitable post -processing algorithms applied to the \nobtained features. The research must execute binarization \nand label creation activities on the features before they can \nbe parsed into the text areas. \nDifferentiable Binarization : The binarization for the \nlikelihood map was used by [46], which binarization \ntechnique to make probability map ‚àà ùëÖùêª√óùëä , where \nB‚ààR^(H√óW) that is obligatory for a text or-not judging task. \nThe following is a typical formulation of the conventional \nbinarization process: \nùêµùëñ,ùëó = {1        ùëñùëì ùëÉùëñ,ùëó ‚â• ùë°\n0      ùëúùë°‚Ñéùëíùëüùë§ùëñùë†ùëí   (6) \nPixel values (x, y) in the probability map are denoted by \n(i,j), and t is a predetermined threshold number. On the other \nhand, differentiable binarization shines when it comes to \nisolating closely jointed text instances and text regions from \nthe background. Hence, the following is a  proposed \napproximate step function: \nùêµÃÇùëñ,ùëó =\n1\n1+ùëí‚àíùëò(ùëÉùëñ,ùëó‚àíùëáùëñ,ùëó)   (7) \nThe above ùêµÃÇ is a learnt adaptive threshold map, and T is \nan approximate binary map. The hyperparameter k is \nempirically adjusted to 30 using the GSK optimisation model \n(discussed in Section 4.2.6). This model mimics the \nbehaviour of normal binarization but is differen tiable, \nallowing it to be optimised during training alongside the \nproposed network. Here, the probability map for the \nbinarization is used to determine the threshold for each pixel \nin the text border map. \nLabel Generation : It was PSENet that served as an \ninspiration for the probability map's label generation [47]. \nTypically, post -processing methods display results as a \npolygonal cluster of vertices.: \nùê∫ = {ùëÜùëò}ùëò=1\nùëõ    (8) \nThe labelling rule in different datasets typically \ndetermines the number of vertices, denoted as n, and the \nreview findings in each data set are represented by the \nsymbol S. In order to speech the challenge of defining the \nboundaries of nearby texts, the Vatti clipping technique was \nsuggested as a means to efficiently produce an offset for \nreducing the size of the initial polygon. One can \nmathematically determine the offset D by using the formula:  \nùê∑ =\nùê¥ùëüùëíùëé(ùëÉ)√ó(1‚àíùëü2)\nùëÉùëíùëüùëñùëöùëíùë°ùëíùëü(ùëÉ)    (9) \nLocated here the computation of the polygon area is \ndenoted as Area (), the computation of the polygon perimeter \nis denoted as Perimeter ( ), and the shrink ratio, r, is \nanalytically fixed to 0.4. The kernel for each text section is \nproduced readily from the original ground truth using \ngraphics-related processes, which are applied to the \noutcomes of reduced polygons. \nThe first step in obtaining the binary map is to binarize the \nprobability map using a predetermined threshold, which is \ntypically chosen as 0.2 in this case. Second, using the binary \nmap, the research will group the text pixels into smaller \nsections. By applying an offset D' to the shrunken regions, \nthe final text prediction results are enlarged. \nùê∑‚Ä≤ =\nùê¥ùëüùëíùëé(ùëÉ)√óùëü‚Ä≤\nùëÉùëíùëüùëñùëöùëíùë°ùëíùëü(ùëÉ)    (10) \n6) HYPER-PARAMETER TUNING USING GSK \nEvery year, new optimisation methods are created and put \ninto use to address practical issues. Thus, the gaining-sharing \nknowledge optimisation method (GSK) was suggested as a \nnew optimisation algorithm not long ago [48]. GSK is an \nprocedure that mimics the procedure of learning and sharing \ninformation throughout the course of a person's lifetime. \nTwo crucial steps are junior gaining - knowledge, which are \nthe fundamental mechanisms that GSK relies on. \n‚ùñ Junior gaining and sharing knowledge:  At this \npoint, the person is attempting to learn as much as they can \nfrom their immediate social network, which consists of \nfriends, family, and neighbours, rather than from more \nextensive online resources like social media. Because they \nare still developi ng their capacity to classify individuals as \ngood or bad, juniors share out of genuine interest and a want \nto learn from those around them, regardless of their level of \nexpertise. \n‚ùñ Senior gaining and sharing knowledge:  At this \npoint in life, one has accumulated a wealth of life experience \nand has established relationships with a larger group of \nindividuals, including friends, coworkers, and social \nnetworks. This is how they learn: by observing those closest \nto them. They also have a great knack for sorting people into \ncategories like \"best,\" \"better,\" and \"worst\" at this stage. This \nleads them to hone their abilities and share what they've \nlearned with the right people. There are multiple stages to the \nexact description of the aforementioned GSK procedure.: \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n10                                                            VOLUME XX, 2017 \n‚ùñ Booting of the essential factors, such as N ‚Äîthe \npopulace size, which agrees to the sum of people. Loading \nof the starting populace is random while regarding the \nboundary restraints, where ùë•ùëñ (ùëñ =  1, 2, 3, . . . , ùëÅ) \ncharacterize the persons; each ùë•i corresponds to ùë•ih with \nùë•ùëñ‚Ñé(ùë•ùëñ1, ùë•ùëñ2, ùë•ùëñ3, . . . , ùë•ùëñùëë ), deferring to the potential array of \nacademic specialisations. Put another way, it's like a \nspecialisation that someone is given. A review of the fitness \nlevel of the populace renowned as ùëìùëó(ùëó =  1, 2, 3, . . . , ùëÅ) is \nalso conducted. \n‚ùñ The following nonlinear equation is used to \ndetermine the dimension separating junior and senior.: \nùëë(ùëóùë¢ùëõùëñùëúùëü) = ùëÉùëüùëúùëèùëôùëíùëöùë†ùëñùëßùëí ‚àó (1 ‚àí\nùê∫\nùê∫ùëíùëõ)\nùëò\n  (11) \nùëë(ùë†ùëíùëõùëñùëúùëü) = ùëÉùëüùëúùëèùëôùëíùëöùë†ùëñùëßùëí ‚àí ùëë(ùëóùë¢ùëõùëñùëúùëü)  (12) \nThe junior phase's dimension is d(junior), and the senior \nphase's dimension is d(senior). k is the knowledge rate, \nwhich is greater than zero. The sum of generations is denoted \nby G, and the extreme sum of generations is represented by \nGen. \n‚ùñ The time where juniors gain and share knowledge \nstarts in this step. At this point, everyone is they can from \ntheir small network while also doing their best to pass on \nwhat they've learned. At this stage, their curiosity drives their \ninteractions, thus it doesn't matter if the people they meet are \nfrom their network or not. \nAlgorithm 1 Phase 1: junior gaining and sharing facts \n[48]. \nfor i = 1: (N) do \nfor h = 1: (d) do \n      if ùëüùëéùëõùëë ‚â§  ùëòùëü(knowledge ratio) then \n       if ùëì(ùë•ùëñ ) >\nùëì(ùë•ùëüùëéùëõ)(ùë•ùëüùëéùëõ ùëñùë† ùëé ùëüùëéùëõùëëùëúùëöùëôùë¶ ùë†ùëíùëôùëíùëêùë°ùëíùëë ùëñùëõùëëùëñùë£ùëñùëëùë¢ùëéùëô) \nthen \n             ùë•ùëñ‚Ñé\nùëõùëíùë§ =  ùë•‚Ñé ‚àó ùêæùëì[(ùë•(ùëñ‚àí1)  ‚àí  ùë•(ùëñ+1))  + (ùë•ùëüùëéùëõ ‚àí\nùë•ùëñ)] \n      else \n          ùë•ùëñ‚Ñé\nùëõùëíùë§ =  ùë•‚Ñé ‚àó ùêæùëì[(ùë•(ùëñ‚àí1)  ‚àí ùë•(ùëñ+1))  +  (ùë•ùëñ ‚àí\nùë•ùëüùëéùëõ)] \n      end \n    else \n      ùë•ùëñ‚Ñé\nùëõùëíùë§ = ùë•ùëñ‚Ñé\nùëúùëôùëë \nend \nend \nend \n‚ùñ The present stage's persons are now updated in \naccordance with the junior plan. \n‚Äì The participants are decided in climbing order according \nto the standards of the goal function. - In order to learn about \neach person, we pick the best and worst options that are \nclosest to them. Furthermore, one is chosen at random to \nimpart wisdom. \n‚ùñ This step procedure is exposed in Procedure 1. ùêæf is \nthe facts factor, where ùêæùëì > 0; this limit controls the amount \nof information that is successful to individual. ùëòr is the \nknowledge ratio, where ùëòùëü ‚àà [0, 1]; How much information \ncan be transferred from one person to another is controlled \nby this limit. \n‚ùñ The senior knowledge phase is at this phase. A \nperson's ability to be classified is taken into consideration at \nthis level. At this point, the setup looks like this: \n‚Äì First, the standards of the objective purpose sort the \npersons in dominant order. \n‚Äì Then, those entities are divided into three collections: \nbest, for example: ùêµùëíùë†ùë° =  100ùëù%(ùë•ùëèùëíùë†ùë° ), ùëÄùëñùëëùëëùëôùëí =\n ùëÅ ‚àí  (2 ‚àó  100ùëù%)(ùë•ùëöùëñùëëùëëùëôùëí ), and ùëäùëúùëüùë†ùë° =\n 100ùëù%(ùë•ùë§ùëúùëüùë†ùë°). \n‚Äì Now, two vectors are selected from gaining (100ùëù%), \nwhile central is chosen for sharing ùëÅ ‚àí  (2 ‚àó 100ùëù%). p \nhere entities, where ùëù ‚àà [0, 1]. This phase procedure is \nexposed in Algorithm 2. \nAlgorithm 2 Phase 2: senior sharing knowledge [48]. \nfor i = 1: (N) do \n  for h = 1: (d) do \n        if ùëüùëéùëõùëë ‚â§  ùëòùëü then \n           if ùëì(ùë•ùëñ) > ùëì(ùë•ùëüùëéùëõ ) then \n               ùë•ùëñ‚Ñé\nùëõùëíùë§ = ùë•‚Ñé ‚àó  ùêæùëì[(ùë•ùëèùëíùë†ùë° ‚àí ùë•ùë§ùëúùëüùë†ùë° ) +\n (ùë•ùëöùëñùëëùëëùëôùëí  ‚àí  ùë•ùëñ )] \n            else \n              ùë•ùëñ‚Ñé\nùëõùëíùë§ = ùë•‚Ñé ‚àó ùêæùëì[(ùë•ùëèùëíùë†ùë° ‚àí ùë•ùë§ùëúùëüùë†ùë°) +  (ùë•ùëñ ‚àí\nùë•ùëöùëñùëëùëëùëôùëí  )] \n         end \n       else \n          ùë•ùëñ‚Ñé\nùëõùëíùë§ = ùë•ùëñ‚Ñé\nùëúùëôùëë \n     end \n   end \nend \n7) LOSS FUNCTION \nFor training the projected system, function can be expressed \nas: \nùêø = ùêøùë† + ùëé √ó ùêøùëè + ùõΩ √ó ùêøùë°  (13) \nThe scheme assigns diverse weights to the likelihood map \nloss Ls, the loss ùêøb, and the threshold map Lt accordingly. In \nrepetition, we set Œ± as 1.0 and Œ≤ as 10, correspondingly. For \nùêøs and ùêøb, the function used here is [49-50]. Aiming to \nnegatives, which happens in text finding approaches, hard \nnegative removal is functional in the BCE hard negatives and \nexpressed as shadows: \nùêøùë† = ùêøùëè = ‚àë ùë¶ùëñ ùëôùëúùëîùë•ùëñ + (1 ‚àí ùë¶ùëñ )ùëôùëúùëî(1 ‚àí ùë•ùëñ)ùëñ‚ààùëÜùëô   (14) \nwhere ùëÜl is the set tested by a ratio at 1 : 3, which is Mining \n(OHEM). To  calculate ùêøt, the study uses ùêø1 distances to \ndegree the resemblance among the forecast and label \nconfidential the polygon: \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   11 \n \nùêøùë° = ‚àë |ùë¶ùëñ\n‚àó ‚àí ùë•ùëñ\n‚àó|ùëñ‚ààùëÖùëë      (15) \nAt this time, ùëÖd is a pixel confidential the opened polygon; \nùë¶‚àó is the tag for x‚àó is the forecast likelihood map. \nV. EXPERIMENTATION, RESULTS AND DISCUSSION \nA. EXPERIMENTAL SETUP  \nThe following features are present in the system that is \nutilised to implement this concept. The manufacturer and \nmodel of the computer are HP, and the operating system is \nWindows 10 Home. The system's CPU is an Intel(R) \nPentium(R) CPU A1020 with 4 logical  processors and 2.41 \nGHz of speed and 2408 MHz of memory.  \nB. EVALUATION METRICS \nReal values that are known and can be found using a \nconfusion matrix are used to make the number of predictions, \nwhich are then classified as correct or erroneous. Results for \nùëáùëüùë¢ùëí ùëÅùëíùëîùëéùë°ùëñùë£ùëí (ùëáùëÅ), ùêπùëéùëôùë†ùëí ùëÉùëúùë†ùëñùë°ùëñùë£ùëí (ùêπùëÉ), ùëéùëõùëë ùëáùëüùë¢ùëí ùëÅùëíùëîùëéùë°ùëñùë£ùëí (ùëáùëÉ) \nare displayed in the confusion matrix for data fitting based \non classes of positive and negative. The model was assessed \nusing a variety of criteria, including the F1 score, recall, \naccuracy, and precision.  \nAccuracy: It evaluates the model's generalizability in \nterms of its ability to improve performance across different \nkinds of classes. Estimation is at its best when every choice \nis useful and significant. The formula is calculated by in-\nbetween the sum of precise judgements by the total sum of \njudgements. \nùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ =  ((ùëáùëÉ +  ùëáùëÅ))/((ùëáùëÉ +  ùëáùëÅ +  ùêπùëÉ +  ùêπùëÅ)) \n(16) \nPrecision: It establishes the model's positive trial \nclassification accuracy [34]. The sum of all positive cases \ndivided by the sum of positive samples that were correctly or \nerroneously identified yields this value. \n ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ =  ùëáùëÉ/(ùëáùëÉ +  ùêπùëÉ)  (17) \nRecall: The model's ability to classify positive samples is \nshown by this score. The volume of positive samples divided \nby the sum of positive trials that were precisely identified as \npositive is the formula for the percentage. \nùëÖùëíùëêùëéùëôùëô =\nùëáùëÉ\n(ùëáùëÉ+ùêπùëÅ)  (18) \nF1-measure: A harmonic mean of accuracy is used to \nestimate the F1-measures. \nùêπ1 ‚àí ùëöùëíùëéùë†ùë¢ùëüùëí =\n(2√óùëùùëüùëíùëêùëñùë†ùëñùëúùëõ√óùëüùëíùëêùëéùëôùëô)\n(ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëüùëíùëêùëéùëôùëô)   (19) \nFigure 3 presents the accuracy and loss for data , where \nFigure 4 provides the ROC curve of the proposed model. \n \nFIGURE 3. Accuracy and Loss of projected perfect \n \nFIGURE 4. ROC Curve of the projected model \nC. VALIDATION ANALYSIS OF PROPOSED MODEL \nTable 2 to 5 presents the comparative investigation of \nvarious models in terms of different metrics for different \nreview data.  \nTABLE II \nACCURACY EVALUATION \nDatabase Name Accuracy (%) \nMLP \n[23] \nSVM \n[23] \nDBN \n[23] \nCNN \n[21] \nLSTM Prop\nosed \ncds_and_vinyl 85 84 86 80 84 95 \nbooks 84 82 85 81 85 94 \nelectronics 82 81 84 82 86 96 \noffice_products 83 83 84 84 86 95 \ncell_phones_and_accessories 81 84 86 84 84 94 \nmusical_ instruments 82 82 86 82 84 96 \npatio_lawn_and_ garden 83 83 85 85 86 94 \nsports_and_ outdoors 84 81 86 84 85 95 \ngrocery_and_ gourmet_food 82 82 86 82 85 94 \narts_crafts_and_ sewing 81 84 85 86 84 93 \nclothing_shoes_ and_ \njewelry \n82 85 84 85 85 96 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n12                                                            VOLUME XX, 2017 \nTable 2 presents the Accuracy Evaluation. The projected \nmodel achieved an accuracy of 95, while the MLP [23] \nmodel realized 85, the SVM [23] model achieved 84, the \nDBN [23] model reached 86, the CNN [21] model \naccomplished 80, and the LSTM model achieved 84. \nSubsequently, the MLP [23] model accomplished 84, the \nSVM [23] model at tained 82, the DBN [23] model reached \n86, the CNN [21] model achieved 81, the LSTM model \naccomplished 85, and the proposed model achieved 94.  \nMoving to the electronics database, the MLP [23] model \nrealized an accuracy of 82, while the SVM [23] model \nachieved 81, the DBN [23] model reached 86, and the CNN \n[21] model achieved 82. In contrast, the projected model \nachieved an accuracy of 96. Afterwards, the MLP [23] model \nachieved an accuracy of 83, the SVM [23] model attained 83, \nthe LSTM model reached 86, and the proposed model \nultimately achieved an accuracy of 95. \nFollowing that, the MLP [23] model achieved an accuracy \nof 81, the SVM [23] model attained 84, the DBN [23] model \nreached 86, the CNN [21] model achieved 86, and the LSTM \nmodel accomplished 84. Concurrently, the proposed model \nrealized an accuracy of 94.  Subsequently, the MLP [23] \nmodel achieved an accuracy of 82, the SVM [23] model \nattained 82, the DBN [23] model reached 86, the CNN [21] \nmodel succeeded in achieving an accuracy of 82, the LSTM \nmodel achieved 84, and the proposed model achieved 96. \nAfterwards, the MLP [23] model attained an accuracy of \n83, the SVM [23] model attained 83, the DBN [23] model \nreached 86, the CNN [21] model accomplished 85, and the \nLSTM model achieved 86. The projected model \naccomplished an accuracy of 94, corresponding to the \nprojected model's accuracy. Next, the MLP [23] model found \nan accuracy of 84, the SVM [23] model found 81, the DBN \n[23] model found 86, and the CNN [21] model found 86. The \nproposed model, on the other hand, achieved an accuracy of \n95. Following tha t, the MLP [23] model achieved an \naccuracy of 82, the SVM [23] model accomplished 82, the \nDBN [23] model reached 86, the CNN [21] model \naccomplished 86, the LSTM perfect managed to achieve an \naccuracy of 85, and the proposed model achieved 94.  \nAfterwards, the MLP [23] model found an accuracy of 81, \nthe SVM [23] model found 84, the DBN [23] model found \n86, the CNN [21] model found 85, and the LSTM model \nfound 86. The proposed model, in contrast, found an \naccuracy of 93.  Following that, the MLP [23] model \naccomplished an accuracy of 82, the SVM [23] model \nattained 85, the DBN [23] model accomplished 86, the CNN \n[21] perfect achieved 84, and the LSTM perfect \naccomplished 85. Concurrently, the proposed perfect \nachieved an accuracy of 96. \n \n \n \nTABLE III \nVALIDATION ANALYSIS IN TERMS OF PRECISION \nDatabase Name \nPrecision (%) \nMLP \n[23] \nSVM \n[23] \nDBN \n[23] \nCNN \n[21] \nLSTM Propos\ned \ncds_and_vinyl 85 86 85 86 86 94 \nbooks 86 88 87 87 86 95 \nelectronics 84 84 87 86 84 95 \noffice_products 89 85 86 87 85 96 \ncell_phones_and_ac\ncessories \n87 86 85 88 87 94 \nmusical_ \ninstruments \n88 87 89 84 88 95 \npatio_lawn_and_ \ngarden \n86 82 87 85 85 93 \nsports_and_ \noutdoors \n85 86 88 86 86 94 \ngrocery_and_ \ngourmet_food \n85 89 85 87 87 95 \narts_crafts_and_ \nsewing \n86 87 86 86 85 96 \nclothing_shoes_ \nand_ jewelry \n87 86 87 85 82 95 \nThe Validation Analysis, characterized in terms of \nprecision in Table 3, is noteworthy. In the analysis of the \ncds_and_vinyl database, the MLP [23] model achieved a \nprecision of 85, the SVM [23] model attained 86, the DBN \n[23] model reached 85, the CNN [21 ] model achieved 86, \nand the LSTM model accomplished 86. The proposed model \nachieved 94, corresponding to the precision of the proposed \nmodel. \nAdditionally, in the books database, the MLP [23] model \nattained a precision of 86, the DBN [23] LSTM model \nachieved 87, and the proposed model accomplished a \nprecision of 95. Moving to the electronics database, the MLP \n[23] model achieved a precision of 84, the SVM [23] reached \n84, the DBN [23] attained 87, the CNN [21] achieved 86, the \nLSTM reached 84, and the Proposed model attained a \nprecision of 95.  In the cell_phones_and_accessories \ndatabase, the MLP [23] model reached a precision of 87, the \nSVM [23] achieved 86, the DBN [23] attained 85, the CNN \n[21] achieved 88, the LSTM reached 87, and the Proposed \nmodel attained a precision of 94.  Following that, in the \nsports_and_outdoors database, the MLP [23] model \nachieved a precision of 85, the DBN [23] model attained 86, \nthe LSTM model achieved 88, and the proposed model \nreached 94. For the patio_lawn_and_garden database, the \nMLP [23] model ac hieved a precision of 86, the SVM [23] \nreached 82, the DBN [23] attained 87, the CNN [21] \nachieved 85, the LSTM reached 85, and the proposed model \nreached 93. For the grocery and gourmet food database, the \nMLP [23] model achieved a precision of 85, the DBN [23] \nmodel attained 89, and the CNN [21] model achieved 85. \nAdditionally, the LSTM achieved 87, and the Proposed \nmodel reached a precision of 95.  In the \narts_crafts_and_sewing database, the MLP [23] model \nachieved a precision of 86, the SVM [23] reached 87, the \nDBN [23] attained 86, the CNN [21] achieved 86, the LSTM \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   13 \n \nreached 85, and the proposed model reached 96.  Finally, in \nthe clothing, shoes, and jewelry database, the MLP [23] \nmodel achieved a precision of 87, the SVM [23] reached 86, \nthe DBN [23] attained 87, the CNN [21] achieved 85, the \nLSTM reached 82, and the proposed model achieved 95. \nFurthermore, the proposed model attained a precision of 95. \nTable 4 presents a comparative analysis of various models \nin terms of recall. In the analysis of the cds_and_vinyl \ndatabase, the MLP [23] model achieved a recall of 85, SVM \n[23] reached 88, DBN [23] reached 84, CNN [21] reached \n85, and LSTM reached 85. The  proposed model achieved a \nrecall of 94 accordingly.  Moving to the books database, the \nMLP [23] model, SVM [23], DBN [23], and CNN [21] all \nreached recall values of 85, 82, and 94, respectively. LSTM \nalso reached recall values of 85, and CNN [21] achieved 94. \nIn the electronics database, the MLP [23] model, SVM \n[23], DBN [23], CNN [21], LSTM, and the proposed model \nachieved recall values of 96, 84, 85, 86, and 86, respectively. \nTABLE IV \nCOMPARISON ANALYSIS OF DIFFERENT MODELS IN TERMS OF  \nRECALL \n \nUsing the office_products  database, the proposed model \nachieved a recall of 92, while the MLP [23] model reached \n86, SVM [23] reached 83, CNN [21] reached 84, and LSTM \nreached 86. For the cell_phones_and_accessories database, \nthe MLP [23] model, LSTM, DBN [23], CNN [21], LSTM, \nand the proposed model all achieved recall values of 94, 85, \nand 82, respectively.  In the musical_instruments database, \nthe MLP [23] model achieved a recall of 85, SVM [23] \nreached 84, DBN [23] reached 88, CNN [21] reached 84, \nLSTM reached 85, and the propose d model attained a recall \nof 95.  Similarly, for the patio_lawn_and_garden database, \nthe MLP [23] model reached a recall of 84, SVM [23] \nreached 85, DBN [23] reached 86, CNN [21] reached 81, \nLSTM reached 86, and the proposed model attained a recall \nof 96. Next, in the grocery_and_gourmet_food database, the \nMLP [23] model reached recall as 85, SVM [23] reached 84, \nCNN [21] reached 87, CNN [21] reached 84, CNN [21] \nreached 86, DBN [23] reached 87, CNN [21] reached 82, and \nLSTM reached 85. The proposed model at tained a recall of \n94 accordingly. Using the arts_crafts_and_sewing database, \nthe proposed model achieved a recall of 94, while the MLP \n[23] model reached a recall of 82, DBN [23] reached a recall \nof 85, and CNN [21] reached a recall of 84.  Finally, in the \nclothing, shoes, and jewelry database, the proposed model \nachieved a recall of 95, while the MLP [23] model reached a \nrecall of 83, DBN [23] reached a recall of 86, CNN [21] \nreached a recall of 85, and LSTM reached a recall of 87.  \nTABLE V \nVERIFICATION OF PROPOSED MODEL ON F1-MEASURE \nDatabase Name \nF1-Measure \nML\nP \n[23] \nSV\nM \n[23] \nDB\nN \n[23] \nCN\nN \n[21] \nLST\nM \nProp\nosed \ncds_and_vinyl 86 86 87 85 87 94 \nbooks 85 85 82 84 86 95 \nelectronics 84 84 84 84 85 94 \noffice_products 86 86 86 85 86 93 \ncell_phones_and_accessories 87 85 89 86 88 96 \nmusical_ instruments 89 87 87 83 87 95 \npatio_lawn_and_ garden 85 86 85 86 85 93 \nsports_and_ outdoors 84 85 86 85 84 94 \ngrocery_and_ gourmet_food 87 84 84 84 89 92 \narts_crafts_and_ sewing 88 87 86 85 84 91 \nclothing_shoes_ and_ jewelry 86 86 85 84 85 94 \nTable 5 illustrates the Verification of the projected model \nbased on F1 -measure. In the analysis of the cds_and_vinyl \ndatabase, the MLP [23] model achieved an F1 -measure of \n86, DBN [23] reached 87, CNN [21] reached 85, LSTM \nreached 87, and the Proposed model attained an F1-measure \nof 94. Moving to the books database, the MLP [23] model \nachieved an F1-measure of 85, DBN [23] reached 82, LSTM \nreached 84, and the Proposed model attained an F1-measure \nof 95.  In the electronics database, the MLP [23] model \nreached an F1 -measure of 84, SVM [23] reached 84, DBN \n[23] reached 84, CNN [21] reached 84, LSTM reached 85, \nand the Proposed model attained an F1-measure of 94. Using \nthe office_products database, the MLP [23] model achieved \nan F1 -measure of 86, DBN [23] reached 86, CNN [21] \nreached 85, LSTM reached 86, and the Proposed model \nattained an F1-measure of 93.  For the \ncell_phones_and_accessories database, the MLP [23] model \nreached an F1 -measure of 87, DBN [23] reached 85, CNN \n[21] reached 89, LSTM reached 88, and the Proposed model \nattained an F1 -measure of 96.  In the musical_instruments  \ndatabase, the MLP [23] model achieved an F1 -measure of \n89, DBN [23] reached 87, CNN [21] reached 83, LSTM \nreached 87, and the Proposed model attained an F1-measure \nof 95. Similarly, in the patio_lawn_and_garden database, the \nMLP [23] model reached an F1 -measure of 85, DBN [23] \nDatabase Name \nRecall (%) \nMLP \n[23] \nSV\nM \n[23] \nDBN \n[23] \nCN\nN \n[21] \nLST\nM \nPropos\ned \ncds_and_vinyl 88 85 86 84 85 95 \nbooks 85 82 85 82 85 94 \nelectronics 84 84 84 83 87 96 \noffice_products 86 83 86 84 86 92 \ncell_phones_and_accessori\nes 84 82 82 85 84 94 \nmusical_ instruments 85 84 88 84 85 95 \npatio_lawn_and_ garden 84 85 86 81 86 96 \nsports_and_ outdoors 86 86 87 82 85 94 \ngrocery_and_ \ngourmet_food 85 84 87 83 84 91 \narts_crafts_and_ sewing 82 85 84 84 86 94 \nclothing_shoes_ and_ \njewelry 83 86 85 85 87 95 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n14                                                            VOLUME XX, 2017 \nreached 86, CNN [21] reached 85, LSTM reached 86, and \nthe Proposed model attained an F1 -measure of 93.  For the \nsports_and_outdoors database, the MLP [23] model reached \nan F1 -measure of 84, DBN [23] reached 85, CNN [21] \nreached 86, LSTM reached 85, and the Proposed model \nattained an F1 -measure of  94. In the \ngrocery_and_gourmet_food database, the MLP [23] model \nreached an F1-measure of 87, CNN [21] reached 84, LSTM \nreached 89, and the Proposed model attained an F1-measure \nof 92. Using the arts_crafts_and_sewing database, the MLP \n[23] model reached an F1-measure of 88, DBN [23] reached \n87, CNN [21] reached 85, LSTM reached 84, and the \nProposed model attained an F1 -measure of 91.  In the \nclothing_shoes_and_jewelry database, the MLP [23] model \nreached an F1 -measure of 86, DBN [23] reached 85, CNN \n[21] reached 85, LSTM reached 85, and the Proposed model \nattained an F1-measure of 94.  \nVI. CONCLUSION AND FUTURE WORK \nOur aim in composing this article was to contribute to the \nfield of sentiment analysis by elucidating the methods, tools, \nand datasets employed to train our model for accurately \npredicting customer reviews on Amazon. The primary focus \nof this investigation  was on product and customer review \nsentiment analysis across social media and product websites. \nTo tackle the limited receptive fields and inadequate \nmodeling capabilities of the initial CNN module for review \ncategorization, this study proposes a DPTN, or  double-path \ntransformer network. The suggested bidirectional \ninteractions between two parallel branches in DPTN \naugment both local and global perspectives. Additionally, by \nadjusting the hyper -parameters using the GSK optimization \nmethod, the proposed DPT N model enhances the \nclassification performance metrics used to evaluate the \nefficacy of these models on the aforementioned datasets. A \ncomparison of the final results with those of previously \nestablished methods was conducted, and the suggested \noutcomes proved to be satisfactory, either comparable to or \neven surpassing prior methods.  \n \nSome potential limitations of the research paper include: \n‚Ä¢ The study focuses specifically on customer reviews \non Amazon, which may limit the generalizability of \nthe findings to other platforms or domains. The \neffectiveness of the proposed DPTN model may \nvary when applied to datasets from different \nsources or industries. \n‚Ä¢ Although the GSK optimization method is utilized \nto adjust hyperparameters and enhance model \nperformance, the sensitivity of the proposed DPTN \nmodel to hyperparameter settings might present \nchallenges in practical deployment or fine-tuning. \nLooking ahead, there is scope for further exploration in \nenhancing sentiment analysis techniques. Future studies can \ndelve into integrating self -attention representations with \ndiverse word embedding techniques within deep learning \nframeworks to better unde rstand user interests and provide \nrecommendations. Addressing these areas will build upon \nour work and contribute to advancing sentiment analysis \nmethodologies. \n \nAUTHOR CONTRIBUTIONS \nAll authors contributed equally. \nDATA AVAILABILITY STATEMENT \nThe data that support the findings of this study are available \nupon reasonable request from the authors. \nETHICS APPROVAL \nThe submitted work is original and has not been published \nelsewhere in any form or language.  \nDISCLOSURE OF POTENTIAL CONFLICTS OF \nINTEREST \n There is no potential conflict of interest.  \nRESEARCH INVOLVING HUMAN PARTICIPANTS \nAND/OR ANIMALS \nNot Applicable  \nFUNDING \nThe authors declare that no funds, grants, or other support \nwere received during the preparation of this manuscript.  \nCOMPETING INTERESTS \nThe authors have no relevant financial or non -financial \ninterests to disclose. \nREFERENCES \n[1] A. S. AlQahtani, ‚ÄúProduct sentim ent analysis for amazon \nreviews,‚Äù International Journal of Computer Science & I nformation \nTechnology (IJCSIT), vol. 13, 2021.  \n[2] A. Rashid and Huang, C. Y, ‚ÄúSentiment Analysis on Consumer \nReviews of Amazon Products,‚Äù  International Journal of Computer \nTheory and Engineering , vol. 13, no. 2, pp.  7, 2021.  DOI: \n10.7763/IJCTE.2021.V13.1287. \n[3] S.Wassan, X. Chen, T. Shen, M. Waqar and N. Z. Jhanjhi, ‚ÄúAmazon \nproduct sentiment analysis using machine learning \ntechniques,‚Äù Revista Argentina de Cl√≠nica Psicol√≥gica, vol. 30, no. 1, \npp.  695, 2021. DOI: 10.24205/03276716.2020.2065. \n[4] M. Y. Salmony and A. R. Faridi, ‚ÄúSupervised Sentiment Analysis on \nAmazon Product Reviews: A survey,‚Äù In 2021 2nd International \nConference on Intelligent Engineering and Management (ICIEM)  \nIEEE, pp. 132-138, April 2021. \nDOI: 10.1109/ICIEM51511.2021.9445303.  \n[5] A. Dadhich and B. Thankachan, ‚ÄúSentiment analysis of amazon \nproduct reviews using hybrid rule-based approach,‚Äù In Smart Systems: \nInnovations in Computing: Proceedings of SSIC 2021  Springer \nSingapore., pp. 173-193, 2022. DOI: https://doi.org/10.1007/978.  \n[6] N. M. Alharbi, N. S. Alghamdi, E. H. Alkhammash and J. F. Al Amri, \n‚ÄúEvaluation of sentiment analysis via word embedding and RNN \nvariants for Amazon online reviews,‚Äù  Mathematical Problems in \nEngineering, pp. 1-10, 2021.   https://doi.org/10.1155/2021/5536560.  \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n  Author Name: Preparation of Papers for IEEE Access (February 2017)  \n \nVOLUME XX, 2017   15 \n \n[7] R. Rajat, P. Jaroli, N. Kumar and R. K. Kaushal, ‚ÄúA Sentiment \nAnalysis of Amazon Review Data Using Machine Learning Model,‚Äù \nIn 2021 6th International Conference on Innovative Technology in \nIntelligent System and Industrial Applications (CITISIA) IEEE, pp. 1-\n6, November 2021. DOI: 10.1109/CITISIA53721.2021.9719909.  \n[8] Vatambeti, R., Mantena, S.V., Kiran, K.V.D. et al. Twitter sentiment \nanalysis on online food services based on elephant herd optimization \nwith hybrid deep learning technique. Cluster Comput (2023). \nhttps://doi.org/10.1007/s10586-023-03970-7 \n[9] J. Budhwar and S. Singh, ‚ÄúSentiment analysis based method for \nAmazon product reviews,‚Äù  International Journal Of Engineering \nResearch & Technology (Ijert) Icact √¢, vol.  9, 2021.  \n[10] Z. Desai, K. Anklesaria and H. Balasubramaniam, ‚ÄúBusiness \nintelligence visualization using deep learning based sentiment analysis \non amazon review data,‚Äù In 2021 12th International Conference on \nComputing Communication and Networking Technologies \n(ICCCNT) IEEE., July, pp. 1 -7, 2021. \nDOI: 10.1109/ICCCNT51525.2021.9579786.  \n[11] Y. Xiao, C. Qi and H. Leng, ‚ÄúSentiment analysis of Amazon product \nreviews based on NLP,‚Äù In 2021 4th International Conference on \nAdvanced Electronic Materials, Computers and Software Engineering \n(AEMCSE) IEEE., pp. 1218 -1221, March 2021. \nDOI: 10.1109/AEMCSE51986.2021.00249.  \n[12] M. Hawlader, A. Ghosh, Z. K. Raad, W. A. Chowdhury, M. S. H. \nShehan and F. B Ashraf, ‚ÄúAmazon product reviews: Sentiment \nanalysis using supervised learning algorithms,‚Äù In 2021 International \nConference on Electronics, Communications and Information \nTechnology (ICECIT)  IEEE., pp. 1 -6, September 2021. \nDOI: 10.1109/ICECIT54077.2021.9641243.  \n[13] B. K. Shah, A. K. Jaiswal, A. Shroff, A. K. Dixit, O. N. Kushwaha and \nN. K. Shah, ‚ÄúSentiments detection for amazon product review,‚Äù \nIn 2021 International conference on computer communication and \ninformatics (ICCCI ), pp. 1 -6, IEEE.  January 2021.  \nDOI: 10.1109/ICCCI50826.2021.9402414.  \n[14] A. Falasari and M. A. Muslim, ‚ÄúOptimize na√Øve bayes classifier using \nchi square and term frequency inverse document frequency for \namazon review sentiment analysis,‚Äù  Journal of Soft Computing \nExploration, vol. 3, no. 1, pp. 31 -36, 2022. \nDOI: https://doi.org/10.52465/joscex.v3i1.68.  \n[15] M. Hawladar, A. Ghosh, Z. K. Raad, W. A. Chowdhury and M. S. H. \nShehan, ‚ÄúAmazon product reviews sentiment analysis using \nsupervised learning algorithms  (Doctoral dissertation, Brac \nUniversity),‚Äù 2021.  \n[16] S. Choudhary and C. Chhabra, ‚ÄúSentiment analysis of Amazon food \nreview data,‚Äù In 2021 Fourth International Conference on \nComputational Intelligence and Communication Technologies \n(CCICT), pp. 116 -120, IEEE.  2021 July.  \nDOI: 10.1109/CCICT53244.2021.00033.  \n[17] M. V. Rao and C. Sindhu, ‚ÄúDetection of Sarcasm on Amazon Product \nReviews using Machine Learning Algorithms under Sentiment \nAnalysis,‚Äù In 2021 Sixth International Conference on Wireless \nCommunications, Signal Processing and Networking (WiSPNET), pp. \n196-199, IEEE.  March 2021 . \nDOI: 10.1109/WiSPNET51692.2021.9419432. \n[18] D. A. J. Daniel and M. J. Meena, ‚ÄúA novel sentiment analysis for \namazon data with TSA based feature selection,‚Äù Scalable Computing: \nPractice and Experience , vol. 22, no. 1, pp. 53 -66, 2021. \nDOI: https://doi.org/10.12694/scpe.v22i1.1839. \n[19] N. N. A. Sjaif, ‚ÄúSentiment Analysis using Term based Method for \nCustomers‚Äô Reviews in Amazon Product,‚Äù  International Journal of \nAdvanced Computer Science and Applications , vol. 13, no. 7, 2022. \nDOI:10.14569/IJACSA.2022.0130780. \n[20] G. S. Budhi, R. Chiong, I. Pranata, & Z. Hu, ‚ÄúUsing machine learning \nto predict the sentiment of online reviews: a new framework for \ncomparative analysis,‚Äù  Archives of Computational Methods in \nEngineering, vol. 28, pp. 2543 -2566, 2021. DOI: \nhttps://doi.org/10.1007/s11831-020-09464-8. \n[21] M. Qorich  and R. El Ouazzani, ‚ÄúText sentiment classification of \nAmazon reviews using word embeddings and convolutional neural \nnetworks,‚Äù The Journal of Supercomputing , pp. 1 -26, 2023. DOI: \nhttps://doi.org/10.1007/s11227-023-05094-6. \n[22] A. Vollero, D. Sardanelli and A. Siano, ‚ÄúExploring the role of the \nAmazon effect on customer expectations: An analysis of user‚Äê\ngenerated content in consumer electronics retailing,‚Äù  Journal of \nConsumer Behaviour , vol. 22, no. 5, pp. 1062 -1073,  2023. \nhttps://doi.org/10.1002/cb.1969. \n[23] T. K. Venkataraman and A. Jadhav, ‚ÄúClassifying the sentiment \npolarity of Amazon mobile phone reviews and their ratings ,‚Äù In AIP \nConference Proceedings , vol. 2523,  no. 1, AIP Publishing.  2023 \nJanuary.  https://doi.org/10.1063/5.0110605. \n[24] T. K. Venkataraman and A. Jadhav, ‚ÄúClassifying the sentiment \npolarity of Amazon mobile phone reviews and their ratings ,‚Äù In AIP \nConference Proceedings, vol. 2523, no. 1, AIP Publishing.  January \n2023. https://doi.org/10.1063/5.0110605. \n[25] N. Liu and J. Zhao, ‚ÄúRecommendation system based on deep \nsentiment analysis and matrix factorization,‚Äù  IEEE Access , vol. 11, \npp. 16994-17001. 2023. DOI: 10.1109/ACCESS.2023.3246060.  \n[26] W. Jin, H. Mao, Z. Li, H. Jiang, C. Luo, H. Wen and X. Tang, \n‚ÄúAmazon-m2: A multilingual multi -locale shopping session dataset \nfor recommendation and text generation,‚Äù   arXiv preprint \narXiv:2307.09688, 2023. \n[27] T. J. Lakshmi and S. D. Bhavani, ‚ÄúLink prediction approach to \nrecommender systems,‚Äù  Computing, pp. 1 -27, 2023. DOI: \nhttps://doi.org/10.1007/s00607-023-01227-0. \n[28] T. L. Ho, A. C. Le and D. H. Vu, ‚ÄúMultiview Fusion Using \nTransformer Model for Recommender Systems: Integrating the Utility \nMatrix and Textual Sources,‚Äù  Applied Sciences, vol. 13, no. 10, pp. \n6324, 2023. https://doi.org/10.3390/app13106324.  \n[29] W. Yuan, C. Yang, Q. V. H. Nguyen, L. Cui, T. He and H. Yin, \n‚ÄúInteraction-level membership inference attack against federated \nrecommender systems,‚Äù arXiv preprint arXiv: pp. 2301.10964, 2023. \nhttps://doi.org/10.48550/arXiv.2301.10964.  \n[30]  S. Hiriyannaiah, S. GM and K. G. Srinivasa, ‚ÄúDeepLSGR: Neural \ncollaborative filtering for recommendation systems in smart \ncommunity,‚Äù Multimedia Tools and Applications , vol. 82, no. 6, pp. \n8709-8728. 2023. DOI: https://doi.org/10.1007/s11042-021-11551-2. \n[31] R. J. Kuo and S. S. Li, ‚ÄúApplying particle swarm optimization \nalgorithm-based collaborative filtering recommender system \nconsidering rating and review,‚Äù Applied Soft Computing, pp. 110038, \n2023.  https://doi.org/10.1016/j.asoc.2023.110038.  \n[32] Y. Zhang, Y. Zhang, D. Yan, S. Deng and Y. Yang, ‚ÄúRevisiting graph-\nbased recommender systems from the perspective of variational auto-\nencoder,‚Äù ACM Transactions on Information Systems , vol. 41, no. 3, \npp. 1-28, 2023. https://doi.org/10.1145/3573385. \n[33] Y. Wang, Z. Chu, X. Ouyang, S. Wang, H. Hao, Y. Shen and S. Li, \n‚ÄúEnhancing recommender systems with large language model \nreasoning graphs,‚Äù  arXiv preprint arXiv: pp. 2308.10835 , 2023. \nhttps://doi.org/10.48550/arXiv.2308.10835. \n[34] P. Pham, L. T. Nguyen, N. T. Nguyen, W. Pedrycz, U. Yun, J. C. W. \nLin and  B. Vo, ‚ÄúAn approach to semantic -aware heterogeneous \nnetwork em bedding for recommender systems,‚Äù  IEEE Transactions \non Cybernetics, 2023. DOI: 10.1109/TCYB.2022.3233819. \n[35] U. Norinder and P. Norinder, ‚ÄúPredicting Amazon customer reviews \nwith deep confidence using deep learning and conformal \nprediction,‚Äù Journal of Management Analytics, vol. 9, no. 1, pp. 1-16, \n2022.  https://doi.org/10.1080/23270012.2022.2031324. \n[36] https://nijianmo.github.io/amazon/index.html, accessed 23 April \n2020. \n[37] N. Garg and K. Sharma, ‚ÄúText pre -processing of multilingual for \nsentiment analysis based on social network data,‚Äù International \nJournal of Electrical & Computer Engineering , vol. 12, no. 1, pp. \n2088-8708, 2022. DOI: 10.11591/ijece.v12i1. \n[38] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado and J. Dean, \n‚ÄúDistributed representations of words and phrases and their \ncompositionality,‚Äù Advances in neural information processing \nsystems, vol. 26, 2013. \n[39] J. Pennington, R. Socher and C. D. Manning, ‚ÄúGlove: Global vectors \nfor word representation,‚Äù In Proceedings of the 2014 conference on \nempirical methods in natural language processing (EMNLP) , pp. \n1532-1543, October  2014.  \n[40] P. Bojanowski, E. Grave, A. Joulin and T. Mikolov, ‚ÄúEnriching word \nvectors with subword information,‚Äù  Transactions of the association \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                              Lella Kranthi Kumar : A GSK -based Double Path Transformer Network Approach for \nSentiment Analysis \n16                                                            VOLUME XX, 2017 \nfor computational linguistics , vol. 5, 135 -146, 2017.  \nhttps://doi.org/10.1162/tacl_a_00051.  \n[41] J. Devlin, M. W. Chang, K. Lee and K. Toutanova, ‚ÄúBert: Pre-training \nof deep bidirectional transformers for language understanding,‚Äù arXiv \npreprint arXiv:1810.04805 . 2018.  \nhttps://doi.org/10.48550/arXiv.1810.04805. \n[42] S. Baswaraju, V. U. Maheswari, K. K. Chennam, A. Thirumalraj, M. \nP. Kantipudi and R. Aluvalu, ‚ÄúFuture Food Production Prediction \nUsing AROA Based Hybrid Deep Learning Model in Agri -\nSector,‚Äù Human-Centric Intelligent Systems , pp. 1 -16, 2023. DOI: \nhttps://doi.org/10.1007/s44230-023-00046-y. \n[43] J. Hu, L. Shen and G.  Sun, ‚ÄúSqueeze -and-excitation networks,‚Äù \nIn Proceedings of the IEEE conference on computer vision and \npattern recognition, pp. 7132-7141, 2018.   \n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. \nGomez and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù  Advances in \nneural information processing systems, vol. 30, 2017.   \n[45] A. Thirumalraja and T. Rajesh, ‚ÄúAn Improved ARO Model for Task \nOffloading in Vehicular Cloud Computing in VANET,‚Äù  2023, \nhttps://doi.org/10.21203/rs.3.rs-3291507/v1.  \n[46] M. Liao, Z. Wan, C. Yao, K. Chen and X. Bai, ‚ÄúReal -time scene text \ndetection with differentiable binarization,‚Äù In Proceedings of the AAAI \nconference on artificial intelligence, vol. 34, no. 07, pp. 11474-11481, \n2020, April. DOI: https://doi.org/10.1609/aaai.v34i07.6812 . \n[47] W. Wang, E. Xie, X. Li, W. Hou, T. Lu, G. Yu and S. Shao ‚ÄúShape \nrobust text detection with progressive scale expansion network,‚Äù \nIn Proceedings of the IEEE/CVF conference on computer vision and \npattern recognition, pp. 9336-9345, 2019.   \n[48] A. W. Mohamed, A. A. Hadi and A. K. Mohamed, ‚ÄúGaining -sharing \nknowledge based algorithm for solving optimization problems: a \nnovel nature -inspired algorithm,‚Äù  International Journal of Machine \nLearning and Cybernetics, vol. 11, no.7, pp. 1501 -1529, 2020. DOI: \nhttps://doi.org/10.1007/s13042-019-01053-x. \n[49] Q. Guo, C. Wang, D. Xiao and Q. Huang, ‚ÄúA novel multi -label pest \nimage classifier using the modified Swin Transformer and soft binary \ncross entropy loss,‚Äù  Engineering Applications of Artificial \nIntelligence, vol. 126, pp. 107060, 2023. \nhttps://doi.org/10.1016/j.engappai.2023.107060. \n[50] Z. Bai, J. Wang, X. L. Zhang and J. Chen, ‚ÄúEnd -to-end speaker \nverification via curriculum bipartite ranking weighted binary cross -\nentropy,‚Äù IEEE/ACM Transactions on Audio, Speech, and Language \nProcessing, vol. 30, pp. 1330 -1344, 2022.  \nDOI: 10.1109/TASLP.2022.3161155. \nDR. LELLA KRANTHI KUMAR  is presently \nemployed as an Assistant Professor in the School \nof Computer Science and Engineering at VIT -AP \nUniversity, located in Amaravati, Andhra Pradesh, \nIndia. He possesses a strong passion for acquiring \nnew knowledge and applying it to address real -\nworld challenges. Driven by a keen interest in \nglobal technological advancements, he keeps a \nclose eye on all the latest developments in the tech \nworld. While he may not fit the stereotype of a nerd, he is socially conscious \nand attuned to everything happening around him. This research effectively \nleverages the current network infrastructure, making the learning process \naccessible to people of diverse backgrounds around the world. Additionally, \nhis research pursuits encompass Data Analytics, Speech Processing, Audio \nAI, IoT, and Medical Image Processing. \n \n \nDr. T. V. Nagaraju is currently working as an \nAssociate Professor in the Department of \nInformation Technology in MLR Institute of \nTechnology, Hyderabad, Telangana, India. He is a \nyoung and dynamic individual with a strong \nacademic background in computer science and \nengineering. He is received his Ph.D. in Machine \nlearning from JNTUK. His research interests \ninclude Machine earning, Natural Language Processing, blockchain \ntechnology. He has 12+ years of experience in the research and academics. \nHis research findings have been published in SCIE and Scopus indexed \ninternational Journals. He has attended and presented many research papers \nin IEEE and springer international conferences. He also published 12 \npatents. \n \n \nPAMULA UDAYARAJU  obtained his \nBachelor's degree in Computer Science and \nEngineering in 2010 from Velagapudi Siddhartha \nEngineering College, Vijayawada, Affiliated to \nAcharya Nagarjuna University, Guntur. He \ncontinued his studies at Sagi Ramakrishnam Raju \nEngineering Colleg e, Bhimavaram, Affiliated to \nJNTU Kakinada and obtained a M. Tech (CST) in \n2014 and Pursuing PhD in the field of Image Processing and Deep learning \nat Sathyabama University, Chennai from 2020. He is currently an Assistant \nProfessor in the Department of Computer Science and Engineering at Sagi \nRamakrishnam Raju Engineering College, Andhra Pradesh, India. His \nresearch interests include Genome Sequencing, Image Processing, Machine \nLearning, Deep Learning, 5G Networks and IOT Applications. \n \n \n \nDr. D. Siri Currently working as Assistant  \nProfessor in Department of CSE, Gokaraju \nRangaraju Institute of Engineering and \nTechnology, Bachupally, TS, India and obtained \nher B.Tech in Information Technology from \nJNTU, Hyderabad, M.Tech in Computer Science \nand Engineering from JNTUH, Hyderabad and \nPh.D from JJT University Rajasthan. She \npublished 2 papers in international journals and 1 \npaper in national and international conferences. Her area of interest are \nMachine Learning, Software Engineering and IOT. \n \n \n \nDr. G. Uday Kiran  received Ph.D from \nJawaharlal Nehru Technological University \nHyderabad in the Domain of Data Mining and \nM.Tech (Neural Networks) from Jawaharlal Nehru \nTechnological University Kakinada. Currently, he \nis working as Associate Professor in Department of \nComputer Science and Engineering (Artificial \nIntelligence and Machine Learning), B V Raju \nInstitute of Technology, has 15 Years of Teaching Experience. His Areas of \nInterests Include: Deep Learning, Natural Language Processing, Computer \nVision, Algorithms and Analysis. \n \n \nDR. B. N. JAGADESH is working as Associate \nProfessor in the School of Computer Science and \nEngineering, VIT -AP University, Amaravathi, \nIndia. He published more than 30 research articles \nin national and international journals with high \nreputation. He is a senior member -IEEE. His \nresearch interests are Image Processing, medical \nimage analysis, multi -modal biometrics and \nspeech recognition etc. \n \n \n \nRAMESH VATAMBETI  received his B.Tech \nfrom Sri Venkateswara University, Tirupati in \nComputer Science and Engineering and M.Tech in \nIT and PhD in CSE from Sathyabama University, \nChennai. He has around 18 years of teaching \nexperience from reputed Engineering Institutions. \nHe works as Professor in the School of Computer \nScience and Engineering at VIT -AP University, \nAmaravati, India. He has successfully guided 3 Ph.D. scholars. His research \ninterests include Computer Networks, Mobile Ad-Hoc and Sensor Networks \nand Machine Learning. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3368441\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Sentiment analysis",
  "concepts": [
    {
      "name": "Sentiment analysis",
      "score": 0.7165952920913696
    },
    {
      "name": "Computer science",
      "score": 0.6433548927307129
    },
    {
      "name": "Amazon rainforest",
      "score": 0.5927492380142212
    },
    {
      "name": "Transformer",
      "score": 0.46245455741882324
    },
    {
      "name": "Data mining",
      "score": 0.37732622027397156
    },
    {
      "name": "World Wide Web",
      "score": 0.34922343492507935
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3186681866645813
    },
    {
      "name": "Electrical engineering",
      "score": 0.10391420125961304
    },
    {
      "name": "Engineering",
      "score": 0.09335514903068542
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ]
}