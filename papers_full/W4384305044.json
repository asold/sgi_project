{
    "title": "GPT-4, an artificial intelligence large language model, exhibits high levels of accuracy on dermatology specialty certificate exam questions",
    "url": "https://openalex.org/W4384305044",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4382101891",
            "name": "Meghna Shetty",
            "affiliations": [
                "Guy's Hospital",
                "St. John's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2188185652",
            "name": "Michael Ettlinger",
            "affiliations": [
                "King's College London",
                "Cell and Gene Therapy Catapult"
            ]
        },
        {
            "id": "https://openalex.org/A4212059923",
            "name": "Magnus Lynch",
            "affiliations": [
                "King's College London",
                "St. John's Hospital",
                "Cell and Gene Therapy Catapult",
                "Guy's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A4382101891",
            "name": "Meghna Shetty",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2188185652",
            "name": "Michael Ettlinger",
            "affiliations": [
                "King's College London"
            ]
        },
        {
            "id": "https://openalex.org/A4212059923",
            "name": "Magnus Lynch",
            "affiliations": [
                "King's College London"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3086023397",
        "https://openalex.org/W3001446745"
    ],
    "abstract": "Abstract Artificial Intelligence (AI) has shown considerable potential within medical fields including dermatology. In recent years a new form of AI, large language models, has shown impressive performance in complex textual reasoning across a wide range of domains including standardised medical licensing exam questions. Here, we compare the performance of different models within the GPT family (GPT-3, GPT-3.5, and GPT-4) on 89 publicly available sample questions from the Dermatology specialty certificate examination. We find that despite no specific training on dermatological text, GPT-4, the most advanced large language model, exhibits remarkable accuracy - answering in excess of 85% of questions correctly, at a level that would likely be sufficient to pass the SCE exam.",
    "full_text": "GPT-4, an artificial intelligence large language model, exhibits high levels of \naccuracy on dermatology specialty certificate exam questions. \nMeghna Shetty1 Michael Ettlinger2 and Magnus Lynch1,2  \n1St John’s Institute of Dermatology, Guy’s Hospital, London SE1 9RT, U.K.; 2Centre for Gene \nTherapy and Regenerative Medicine, King’s College London, SE1 9RT, U.K  \nCorrespondence: Meghna Shetty \nE-mail: meghna.shetty@gstt.nhs.uk \nhttps://orcid.org/0000-0002-2648-0941 \nhttps://orcid.org/0000-0001-7586-4338 \n \nAbstract: \nArtificial Intelligence (AI) has shown considerable potential within medical fields including \ndermatology. In recent years a new form of AI, large language models, has shown \nimpressive performance in complex textual reasoning across a wide range of domains \nincluding standardised medical licensing exam questions. Here, we compare the \nperformance of different models within the GPT family (GPT-3, GPT-3.5, and GPT-4) on 89 \npublicly available sample questions from the Dermatology specialty certificate examination. \nWe find that despite no specific training on dermatological text, GPT-4, the most advanced \nlarge language model, exhibits remarkable accuracy - answering in excess of 85% of \nquestions correctly, at a level that would likely be sufficient to pass the SCE exam. \n \n   \nArtificial Intelligence (AI) has shown considerable potential within a wide range of medical \nfields including dermatology \n1,2. Recent years have seen the rapid advancement of “large \nlanguage models”. These models are trained on very large quantities of textual data \npermitting complex reasoning to emerge. The best-performing models at the present time \nare the GPT family of models developed by openAI. In recent years GPT-3, GPT-3.5, and \nGPT-4 have been released each of which contains a larger number of parameters and better \nperformance.  \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292418doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nThe largest, most advanced, large language model, GPT4, was released recently 3 and has \nshown remarkable performance across a wide range of domains including standardised \ngeneral medical exam questions 4. For specialist areas within medicine such as dermatology, \nthere is far less publicly available information, and it is unclear whether it will perform at such \na high level. Here, we compare the performance of different models within the GPT family \n(GPT-3, GPT-3.5, and GPT-4) on 89 publicly available sample questions from the \nDermatology specialty certificate (SCE) examination \n5. \n \nWe downloaded the text of 89 questions from the MRCP UK Dermatology sample question \ndataset and their corresponding answers. The AI language model is “prompted” with \narbitrary text and returns a textual answer in response.  Using the OpenAI GPT application \nprogramming interface (API) developer preview we developed a script to automatically \nprompt the GPT API with the unedited full text of the multiple choice question followed \nimmediately by the phrase \"Only answer with the letter of the answer; do not elaborate \nfurther\\n\\nAnswer Letter:” This ensured the API only returned a letter per answer and not an \nexplanation. We used the publicly available versions of the models and did not perform any \nfine-tuning training with medical or dermatological text. The answer was scored as correct \nwhere the letter response output by the AI matched the correct response. The API \nparameters were temperature=0, max_tokens=1, top_p=1, frequency_penalty=0.0, \npresence_penalty=0.0. For GPT-3, we used the model text-davinci-003; for GPT-3.5, we \nused gpt-3.5-turbo, and for GPT-4, we used “gpt-4.”.  Text from tables in the questions was \nincluded in the prompt but images were not included.  \n \nWe first compared the relative performance of the different large language models (Table 1). \nGPT-3 answered 43 questions correctly (48.31%), GPT3.5 answered 54 questions correctly \n(60.57%), and GPT-4 outperformed both by answering 76 questions correctly (85.39%). \nGPT4 accurately answered 75% of the image questions and 66.67% of the table questions \namong the ten questions that included an image or table. For the remaining 79 questions \nwithout images or tables, GPT-4 achieved an impressive 87.34% accuracy (Table 1). To \nexamine reasoning capability in more detail, we executed the script again with the question \nand the prompt “Elaborate on your answer:” for both correct and incorrect responses \n(www.github.com/thelynchlab/gpt4). These results highlight the incredible advances that \nlarge language models have made in answering questions relating to specialist \ndermatological conditions.  \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292418doi: medRxiv preprint \nImages in the questions were not included in the information supplied to the model, yet \ndespite this 75% of the image questions were answered correctly, however, GPT-4 is \ncapable of reading and interpreting tables. When we examined the questions that were \nincorrectly answered, we observed that GPT often provided the correct diagnosis but did not \nselect the most appropriate treatment options. For example, GPT-4 correctly made the \ndiagnosis of pyoderma gangrenosum but suggested dapsone as the appropriate initial \nintervention, whereas the correct answer is prednisolone.  \n \nIn summary, with no specific training for medical or dermatology questions, GPT-4 exhibited \nremarkable accuracy in answering text-based dermatology multiple-choice scenarios at a \nlevel that would likely be sufficient to pass the SCE exam. In future studies, it will be \nimportant to assess the performance of the model on more realistic clinical scenarios in \ncombination with medical images. It will also be of value to assess whether “fine-tuning” the \nmodel with clinical images and non-publicly available text relating to dermatological \ndiagnosis leads to superior performance, particularly for rarer conditions.  \n \nLarge language models, particularly in combination with clinical image processing have the \npotential to improve the provision of dermatological services. Obvious applications would \ninclude the descriptive interpretation of dermoscopy and dermatopathology images and aid \nin the diagnosis of rare conditions. These capabilities are likely to be of particular value in \nresource poor environments where access to highly experienced specialists is limited.  \n \nReferences: \n1. Du-Harpur X, Arthurs C, Ganier C et al . Clinically relevant vulnerabilities of deep \nmachine learning systems for skin cancer diagnosis. The Journal of investigative \ndermatology. 2021 Apr;141(4):916. \n2. Du ‐ Harpur X, Watt FM, Luscombe NM, Lynch MD. What is AI? Applications of \nartificial intelligence to dermatology. British Journal of Dermatology. 2020 Sep \n1;183(3):423-30. \n3. OpenAI. GPT-4 Technical Report. Last revised. 27 Mar 2023 . Available at: \nhttps://arxiv.org/abs/2303.08774 (last accessed 8 April 2023). \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292418doi: medRxiv preprint \n4. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of gpt-4 on \nmedical challenge problems. 2023 Mar 30. Available at: \nhttps://arxiv.org/abs/2303.13375 (last accessed 8 April 2023). \n5. MRCP UK. Dermatology sample questions [Internet]. London: MRCP UK; Available \nat: https://www.mrcpuk.org/mrcpuk-examinations/specialty-certificate-\nexaminations/specialties/dermatology/dermatology-sample (last accessed 8 April \n2023). \n \nAcknowledgments: We would like to thank Open AI for allowing us to use its resources in \nthe field of research. \nFunding sources: None \nConflicts of interest: none to declare. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292418doi: medRxiv preprint \nTable 1: Comparison of responses of versions of Chat GPT 3 versus 3.5 versus 4 in its \nperformance in Dermatology SCE questions: \n \n \nQuestion type GPT 3 GPT 3.5 GPT 4 \nTotal Questions \nAll questions \n89 89 89 \nCorrect answers 43 54 76 \nPercentage correct 48.31% 60.67% 85.39% \nTotal Questions \nQuestions with images \n4 4 4 \nCorrect answers 2 3 3 \nPercentage correct 50.00% 75.00% 75.00% \nTotal Questions \nQuestions with tables \n6 6 6 \nCorrect answers 0 3 4 \nPercentage correct 0.00% 50.00% 66.67% \nTotal Questions \nQuestions without tables or images \n79 79 79 \nCorrect answers 41 48 69 \nPercentage correct 51.90% 60.76% 87.34% \n \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted July 14, 2023. ; https://doi.org/10.1101/2023.07.13.23292418doi: medRxiv preprint "
}