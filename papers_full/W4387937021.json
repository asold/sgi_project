{
  "title": "Large language models for chemistry robotics",
  "url": "https://openalex.org/W4387937021",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5019451422",
      "name": "Naruki Yoshikawa",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5061244637",
      "name": "Marta Skreta",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5072822077",
      "name": "Kourosh Darvish",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5086145116",
      "name": "Sebastian Arellano-Rubach",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5103942596",
      "name": "Zhi Ji",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5017759578",
      "name": "Lasse Bjørn Kristensen",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5042918448",
      "name": "Andrew Zou Li",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5102115247",
      "name": "Yuchi Zhao",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A5067218634",
      "name": "Haoping Xu",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5093124382",
      "name": "Artur Kuramshin",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5071495561",
      "name": "Alán Aspuru‐Guzik",
      "affiliations": [
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5010648258",
      "name": "Florian Shkurti",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5061193324",
      "name": "Animesh Garg",
      "affiliations": [
        "Vector Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4318486595",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W1978892650",
    "https://openalex.org/W2205340216",
    "https://openalex.org/W2116341587",
    "https://openalex.org/W4365211632",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3042021489",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W1630678085",
    "https://openalex.org/W2794922736",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4312054168",
    "https://openalex.org/W4287759575",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4225000967",
    "https://openalex.org/W3084483561",
    "https://openalex.org/W3031157367",
    "https://openalex.org/W2950885698",
    "https://openalex.org/W4225154729",
    "https://openalex.org/W2023118067",
    "https://openalex.org/W3128894241",
    "https://openalex.org/W3001865277",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2921963218",
    "https://openalex.org/W2161414194",
    "https://openalex.org/W3213688553",
    "https://openalex.org/W2979490629",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W3101287459",
    "https://openalex.org/W4366734321",
    "https://openalex.org/W3178976598",
    "https://openalex.org/W4320727261",
    "https://openalex.org/W2135639338",
    "https://openalex.org/W1971086298",
    "https://openalex.org/W2128990851",
    "https://openalex.org/W2918619665",
    "https://openalex.org/W3215761265",
    "https://openalex.org/W4307411279",
    "https://openalex.org/W3197331740",
    "https://openalex.org/W2786893837",
    "https://openalex.org/W2971244547",
    "https://openalex.org/W2750411879",
    "https://openalex.org/W4312589189",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W3018637171",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4296414573",
    "https://openalex.org/W3110324412",
    "https://openalex.org/W4360819401",
    "https://openalex.org/W4305028650",
    "https://openalex.org/W2031441006",
    "https://openalex.org/W3024236153",
    "https://openalex.org/W3091684735",
    "https://openalex.org/W2986905106",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W3134840027",
    "https://openalex.org/W4287207918",
    "https://openalex.org/W4312334342",
    "https://openalex.org/W4200459772",
    "https://openalex.org/W4321277276",
    "https://openalex.org/W2125188427",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W2037849836",
    "https://openalex.org/W4206480062",
    "https://openalex.org/W4298186450",
    "https://openalex.org/W4365211638",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4292639835",
    "https://openalex.org/W4226403986",
    "https://openalex.org/W4286911712",
    "https://openalex.org/W3129746225",
    "https://openalex.org/W4297161808",
    "https://openalex.org/W2902762889",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W2236233024",
    "https://openalex.org/W2403069916",
    "https://openalex.org/W2805883505",
    "https://openalex.org/W4205737716",
    "https://openalex.org/W3212375327",
    "https://openalex.org/W4321854081",
    "https://openalex.org/W2028098565",
    "https://openalex.org/W3208860256",
    "https://openalex.org/W6767372934",
    "https://openalex.org/W2964055695",
    "https://openalex.org/W3203218874",
    "https://openalex.org/W4387349118",
    "https://openalex.org/W2049410986",
    "https://openalex.org/W3096471888",
    "https://openalex.org/W2999905431"
  ],
  "abstract": "Abstract This paper proposes an approach to automate chemistry experiments using robots by translating natural language instructions into robot-executable plans, using large language models together with task and motion planning. Adding natural language interfaces to autonomous chemistry experiment systems lowers the barrier to using complicated robotics systems and increases utility for non-expert users, but translating natural language experiment descriptions from users into low-level robotics languages is nontrivial. Furthermore, while recent advances have used large language models to generate task plans, reliably executing those plans in the real world by an embodied agent remains challenging. To enable autonomous chemistry experiments and alleviate the workload of chemists, robots must interpret natural language commands, perceive the workspace, autonomously plan multi-step actions and motions, consider safety precautions, and interact with various laboratory equipment. Our approach, CLAIRify , combines automatic iterative prompting with program verification to ensure syntactically valid programs in a data-scarce domain-specific language that incorporates environmental constraints. The generated plan is executed through solving a constrained task and motion planning problem using PDDLStream solvers to prevent spillages of liquids as well as collisions in chemistry labs. We demonstrate the effectiveness of our approach in planning chemistry experiments, with plans successfully executed on a real robot using a repertoire of robot skills and lab tools. Specifically, we showcase the utility of our framework in pouring skills for various materials and two fundamental chemical experiments for materials synthesis: solubility and recrystallization. Further details about CLAIRify can be found at https://ac-rad.github.io/clairify/ .",
  "full_text": "Autonomous Robots (2023) 47:1057–1086\nhttps://doi.org/10.1007/s10514-023-10136-2\nLarge language models for chemistry robotics\nNaruki Yoshikawa 1,2 · Marta Skreta 1,2 · Kourosh Darvish 1,2 · Sebastian Arellano-Rubach 3 · Zhi Ji 1 ·\nLasse Bjørn Kristensen 1 · Andrew Zou Li 1 · Yuchi Zhao 4 · Haoping Xu 1,2 · Artur Kuramshin 1 ·\nAlán Aspuru-Guzik 1,2,5 · Florian Shkurti 1,2 · Animesh Garg 1,2,6\nPublished online: 25 October 2023\n© The Author(s) 2023\nAbstract\nThis paper proposes an approach to automate chemistry experiments using robots by translating natural language instructions\ninto robot-executable plans, using large language models together with task and motion planning. Adding natural language\ninterfaces to autonomous chemistry experiment systems lowers the barrier to using complicated robotics systems and increases\nutility for non-expert users, but translating natural language experiment descriptions from users into low-level robotics lan-\nguages is nontrivial. Furthermore, while recent advances have used large language models to generate task plans, reliably\nexecuting those plans in the real world by an embodied agent remains challenging. To enable autonomous chemistry exper-\niments and alleviate the workload of chemists, robots must interpret natural language commands, perceive the workspace,\nautonomously plan multi-step actions and motions, consider safety precautions, and interact with various laboratory equip-\nment. Our approach, CLAIRify, combines automatic iterative prompting with program veriﬁcation to ensure syntactically\nvalid programs in a data-scarce domain-speciﬁc language that incorporates environmental constraints. The generated plan is\nexecuted through solving a constrained task and motion planning problem using PDDLStream solvers to prevent spillages of\nliquids as well as collisions in chemistry labs. We demonstrate the effectiveness of our approach in planning chemistry experi-\nments, with plans successfully executed on a real robot using a repertoire of robot skills and lab tools. Speciﬁcally, we showcase\nthe utility of our framework in pouring skills for various materials and two fundamental chemical experiments for materials\nsynthesis: solubility and recrystallization. Further details about CLAIRify can be found at https://ac-rad.github.io/clairify/.\nKeywords Large language models · Constrained task and motion planning · Plan generation veriﬁcation · Self-driving labs ·\nChemistry lab automation\nNaruki Y oshikawa, Marta Skreta and Kourosh Darvish have contributed\nequally to this work.\nB Kourosh Darvish\nkdarvish@cs.toronto.edu\n1 University of Toronto, Toronto, ON, Canada\n2 V ector Institute for Artiﬁcial Intelligence, Toronto, ON,\nCanada\n3 University of Toronto Schools, Toronto, ON, Canada\n4 University of Waterloo, Waterloo, ON, Canada\n5 CIFAR Artiﬁcial Intelligence Research Chair, Toronto, ON,\nCanada\n6 NVIDIA, Santa Clara, CA, USA\n1 Introduction\nThe execution of chemistry experiments, which represents a\ncrucial stage in the process of material discovery, typically\nrelies on human experts. This manual experimentation poses\na number of signiﬁcant challenges, such as difﬁculties in\nreproducibility, high resource requirements, and limited scal-\nability. To address these obstacles, the concept of self-driving\nlabs (SDLs) has emerged (Seifrid et al., 2022). Although\nspecialized hardware for chemistry experiments has been\nproposed and used in modern labs, we argue that using\ngeneral-purpose robots is beneﬁcial in developing SDLs that\nmaximize the use of existing resources and are more con-\nﬁgurable. The functionality of general-purpose robots can\nbe expanded by programming them to interact with exist-\ning chemistry instruments designed for humans. This feature\ncontributes to reducing the cost related to introducing addi-\n123\n1058 Autonomous Robots (2023) 47:1057–1086\nFig. 1 Task plans generated by LLMs may contain syntactical errors in\ndomain-speciﬁc languages. By using veriﬁer-assisted iterative prompt-\ning, CLAIRify can generate a valid program. Once the program has\nbeen veriﬁed, it is passed on to a task and motion planner (TAMP) for\nexecution by a robot\ntional hardware for speciﬁc purposes. Therefore, this study\ndiscusses the use of general-purpose robots for building\nchemistry SDLs.\nOne of the principal obstacles in effectively using robots in\nchemistry labs is to ensure that they are natural and intuitive\nfor chemists to operate. An approach to achieving a natural\nand intuitive interface between chemists and robots is through\nthe use of natural language as a communication medium. This\napproach enables users to instruct robots in an efﬁcient and\neffective manner.\nThis work aims to facilitate autonomous and safe execu-\ntion of chemistry experiments using general-purpose robot\nmanipulators. This is accomplished through natural language\ninstructions to generate plans. Several challenges must be\naddressed at both the natural language processing (NLP) and\nrobotic planning levels for this purpose. At the NLP level, the\nrobot must be capable of converting natural language instruc-\ntions into executable robot instructions (Fig. 1). At the robotic\nplanning level, the robotic system should be capable of plan-\nning robot tasks and motions that take safety considerations\ninto account, using intermediate goals identiﬁed by NLP and\nperceptual information of the robot workspace.\nNatural language has been used in the literature to over-\ncome the communication barrier between humans and robots,\nfor example in navigation tasks (Tellex et al., 2011). More\nrecently, numerous studies have demonstrated that large\nlanguage models (LLMs) can assist robots to reason with\ncommon sense (Huang et al., 2022; Singh et al., 2022). LLMs\nhave been used to generate structured outputs (Devlin et al.,\n2019; Brown et al., 2020; Chowdhery et al., 2022), includ-\ning code generation (Chen et al., 2021; Wang et al., 2021;\nLi et al., 2022) and robot programming (Liang et al., 2022).\nNevertheless, the application of LLMs to task-plan genera-\ntion for chemistry robots presents two key challenges. Just\nlike with all machine-executable code, generated plans must\nadhere to strict syntax rules in order to be executed by a robot.\nIn robotics, task-plan veriﬁcation is often desired, because it\nincreases the likelihood that a robot can reach the desired\ngoal (Garrett et al., 2020; Ahn et al., 2022). Furthermore,\nLLMs may perform poorly in generating task plans in a zero-\nshot manner for data-scarce, domain-speciﬁc languages, such\nas the ones in robot planning, and thus require ﬁne-tuning,\nor in technical scientiﬁc domains (Gu et al., 2021), or scien-\ntiﬁc coding (Liu et al., 2023). Several approaches have been\nproposed in the literature to address these issues. One promis-\ning technique is iterative prompting, which has an advantage\nover ﬁne-tuning LLMs, as the latter requires access to the\nmodel weights and training datasets to learn domain-speciﬁc\nlanguages reasonably well and incurs high computational\ncosts (Mishra et al., 2021; Wang et al., 2011; Wu et al.,\n2022). Iterative prompting enables the LLM to verify can-\ndidate plans while providing the rules of structured language\nas input, thereby leveraging in-context learning.\nThe planning component of the robotic system takes\nas input perception, vision-based outcome evaluation of\nexperiments and natural language instructions, and solves\na constrained task and motion planning (TAMP) problem.\nTo do so, the robot must possess both general and chemistry\ndomain-speciﬁc perception and manipulation skills, includ-\ning recognizing transparent and opaque objects (Xu et al.,\n2021; Wang et al., 2023), estimating object poses, and mon-\nitoring the state of materials synthesis, such as detecting if a\nsolution is dissolved (Shiri et al., 2021). Dexterous manipu-\nlation and handling are also necessary, such as constrained\nmotion for picking and transporting objects without spilling\ntheir contents, pouring skills, and manipulation of tools and\nobjects. Additionally, high precision and repeatability are\ncrucial for reproducible and reliable results in robot-executed\nchemistry experiments.\nEnsuring safety during experiments and interactions is\nanother challenge (Ménard & Trant, 2020). Multi-layered\nsafety requirements are necessary, including high-level con-\nstraints on material synthesis order in experiment description\nand task planning, and low-level manipulation and perception\nskills to prevent spilling during transportation of chemistry\nvials and beakers.\n123\nAutonomous Robots (2023) 47:1057–1086 1059\nContributions We introduce an autonomous robotic sys-\ntem for chemistry lab automation, an end-to-end closed-loop\nrobotic framework that translates natural language into struc-\ntured long-horizon task plans and then executes them by a\nconstrained task and motion planning solver, integrated with\nperception and manipulation skills, and vision-based evalua-\ntion of experiment outcomes (Fig. 1). Our approach consists\nof two modules. The ﬁrst is CLAIRify, which translates a\nnatural language input into a structured plan. The second\nis the translation of the plan to low-level robot actions. To\nevaluate the framework, we use a domain-speciﬁc language\ncalled Chemical Description Language (XDL) (Mehr et al.,\n2020). XDL is an XML-based domain-speciﬁc language\nused to describe action plans for chemistry experiments in\na structured format, and is suitable for commanding robots\nin self-driving laboratories (Seifrid et al., 2022). XDL is\nhardware-agnostic, meaning that it can be executed on many\nrobot platforms and lab setups. We showcase an example\nof how XDL can be executed on a Franka robot in a chem-\nistry lab setting by converting XDL to low-level robot actions\nusing TAMP . Our method includes (I) a rule-based iterative\nveriﬁer to check for syntax correctness and environment con-\nstraints, which improves zero-shot task plan generation in a\ndata-scarce domain. (II) At the TAMP level, constrained task\nand motion planning is incorporated via a PDDLStream (Gar-\nrett et al., 2020) solver to avoid spillage when transporting\nliquids and powders. When adding constraints to the robot’s\ndegrees of freedom, the motion planning success rate was\nlower compared to unconstrained planning. To overcome\nthis difﬁculty, we introduced a new degree of freedom to\nour robot. We demonstrate that an 8-DoF robot has 97%\nsuccess rate in constrained TAMP compared to 84% of a\n7-DoF robot. Moreover, we present accurate and efﬁcient\npouring skills inspired by human motions with an average\nrelative error of 8.1% and 8.8% for pouring water and salt,\nrespectively, compared to a baseline method with 81.4% and\n24.1% errors. Our method achieves comparable results to\nrecent studies (Kennedy et al., 2019; Huang et al., 2021),\nyet it stands out for its simplicity and the reduced need for\ncomplex sensor feedback.\nOur evaluation results demonstrate that CLAIRify outper-\nforms the current state-of-the-art model for XDL generation\npresented in (Mehr et al., 2020). Additionally, our frame-\nwork represents an advancement from the approach in\n(Fakhruldeen et al., 2022), which relied on a ﬁnite state\nmachine with ﬁxed objects in a static workspace. Our frame-\nwork perceives the environment and plans long-horizon\ntrajectories to perform multistep chemistry experiments. It\ncan close the loop at two levels. The ﬁrst is at the chemistry\ntask level, where online visual feedback is used to estimate\nthe progress of task execution. The second is at the robot\nmotion planning and skill level, where the robot is able to\nadapt to uncertainties, for example, by reﬁning its plan at\nexecution time using visual feedback. As a proof of concept\nfor chemistry lab automation, we achieved results that are\ncomparable to the literature ground truth for the solubility\nexperiment, with a 7.2% error rate for the solubility of salt,\nand successful recrystallization of alum.\nThe paper is organized as follows: Section 2 reviews the\nstate of the art. Section 3 deﬁnes the problem and presents\nthe proposed end-to-end approach, covering natural language\nand perceptual inputs to robot task and motion planning and\nskill execution. Experiments and results are presented in Sec-\ntion 4. Discussion of the results is provided in Section 5, and\nconclusions are drawn in Section 6.\n2 Related work\nThis section describes recent advancements in lab automa-\ntion, speciﬁcally focusing on robotics and the methods\nthrough which large language models (LLMs) can be incor-\nporated into these systems. Furthermore, the section high-\nlights the challenges associated with generating veriﬁable\ntask plans from LLMs, which are necessary to generate robot\ntasks and motion plans. Lastly, the section outlines recent\nefforts that focus on identifying the essential robot skills\nrequired to execute lab automation tasks effectively.\n2.1 Lab automation\nLab automation aims to introduce automated hardware in a\nlaboratory to improve the efﬁciency of scientiﬁc discovery.\nIt has been applied to high-throughput screening (Pereira\n& Williams, 2007) (Macarron et al., 2011) to discover\ndesired materials from a pool of candidates. With the rise\nof AI technologies, the concept of a self-driving laboratory\n(SDL) (Häse et al., 2019; Seifrid et al., 2022) that combines\nexperiment planning by AI and automated experimentation\nplatforms has emerged. A review on SDL can be found in\n(Abolhasani & Kumacheva, 2023). Different hardware for\nlab automation has been utilized in SDLs to meet the needs\nof individual laboratories, such as pipetting robots (Higgins\net al., 2021) or ﬂow reactors (Epps et al., 2020;L ie ta l . ,2020).\nThe Chemputer (Mehr et al., 2020) is an example of a special-\nized machine for automated chemistry experiments. In the\ncorresponding paper, the authors demonstrated an automated\nworkﬂow that translates organic chemistry literature into a\nstructured language called XDL and synthesized the speci-\nﬁed molecules. While specialized hardware for chemistry is\nwidely used in lab automation, general-purpose robots have\nalso been applied to chemistry because of their ﬂexibility,\nmobility, and dexterous manipulation capabilities. The utility\nof mobile general-purpose robots for discovering improved\nphotocatalysts for hydrogen production was demonstrated in\n(Burger et al., 2020). ARChemist (Fakhruldeen et al., 2022),\n123\n1060 Autonomous Robots (2023) 47:1057–1086\na lab automation system, was developed to conduct exper-\niments, including solubility screening and crystallization,\nwithout human intervention. Another work (Knobbe et al.,\n2022) endowed a collaborative robot with a force-sensitive\npipetting skill by extending the robot with a pipetting ﬁn-\nger system. Differently from other works where pick, place,\ninsertion, or liquid handling, were the focus of robot skills,\nin (Pizzuto et al., 2022), the robot learns to scrape pow-\nders from vials for crystallization experiments. During the\nexperiment, crystallized materials tend to adhere to the inner\nwall of the vial. Acquiring these crystals necessitates physi-\ncal interaction between the crystals and the vial using a robot\narm, which is unattainable with a valve-pump-based system.\nThese examples demonstrate the potential utilization of a\nversatile robot in expanding the scope of tasks automated\nwithin chemistry laboratories. Similarly, an automated chem-\nistry experiment system that mimics the motions of human\nchemists has been proposed in (Lim et al., 2020). Addition-\nally, The properties of thin-ﬁlm materials were optimized\nusing a robot arm in (MacLeod et al., 2020). Although these\nmajor steps towards chemistry lab automation have been\nmade, their dependence on predeﬁned tasks and on motion\nplans without constraint satisfaction guarantees limits their\nﬂexibility in new and dynamic workspaces. In those works,\npick & place was the primary task that the manipulators\nwere carrying out. Those works were tested in hand-tuned\nand static environments to avoid occurrences of unsatisﬁed\ntask constraints and the associated problems, such as chem-\nical spills during the transfer of vessels ﬁlled with liquid.\nOur framework resolves these gaps through using large lan-\nguage models to generate long-horizon machine-readable\ninstructions and passing them to a constraint satisfaction and\nscene-aware planning system with a variety of skills.\n2.2 Large language models for chemistry\nSeveral language models specialized for the chemistry or sci-\nence domain have been proposed, such as MolT5 (Edwards\net al., 2022), Chemformer (Irwin et al., 2022), and Galactica\n(Taylor et al., 2022). After the release of GPT-3, chem-\nistry applications were attempted without further training\n(Jablonka et al., 2023). The abilities to do Bayesian opti-\nmization (Ramos et al., 2023), to use external chemistry tools\n(Bran et al., 2023), and to synthesize molecules by reading\ndocumentation (Boiko et al., 2023) were explored. Our work\nfocuses on increasing the reliability of the output of LLMs\nwithout further training by introducing iterative prompting\nand low-level planning through a task and motion planning\nframework.\n2.3 Leveraging language models with external\nknowledge\nA challenge with LLMs generating code is that the cor-\nrectness of the code is not assured. There have been many\ninteresting works on combining language models with exter-\nnal tools to improve the reliability of the output. Mind’s\nEye (Liu et al., 2023) attempts to ground large language\nmodel’s reasoning with physical simulation. They trained\nLLM with pairs of language and codes and used the simula-\ntion results to prompt an LLM to answer general reasoning\nquestions.\nToolformer (Schick et al., 2023) incorporates API calls\ninto the language model to improve a downstream task, such\nas question answering, by ﬁne-tuning the model to learn how\nto call the API. LEVER (Ni et al., 2023) improves LLM\nprompting for SQL generation by using a model-based ver-\niﬁer trained to verify the generated programs. As SQL is a\ncommon language, the language model is expected to under-\nstand its grammar. However, for domain-speciﬁc languages,\nit is difﬁcult to acquire training datasets and expensive to\nexecute the plans to verify their correctness. Our method\ndoes not require ﬁne-tuning any models. Furthermore, there\nis no need for prior knowledge of the target language within\nthe language model obtained during the training phase. Our\nidea is perhaps closest to LLM-A ugmenter (Peng et al.,\n2023), which improves LLM outputs by giving it access\nto external knowledge and automatically revising prompts\nin natural language question-answering tasks. Our method\nsimilarly encodes external knowledge in the structure of the\nveriﬁer and prompts, but for a structured and formally ver-\niﬁable domain-speciﬁc language. A review on augmenting\nLLMs with external tools is found in (Mialon et al., 2023).\n2.4 Task planning with large language models\nHigh-level task plans are often generated from a limited set of\nactions (Garrett et al., 2020), because task planning becomes\nintractable as the number of actions and time horizon grows\n(Kaelbling & Lozano-Pérez, 2011). One approach to do task\nplanning is using rule-based methods (Mehr et al., 2020;\nBaier et al., 2009). More recently, it has been shown that\nmodels can learn task plans from input task speciﬁcations\n(Sharma et al., 2021; Mirchandani et al., 2021; Shah et al.,\n2021), for example using hierarchical learning (Xu et al.,\n2018; Huang et al., 2019), regression based planning (Xu et\nal., 2019), or reinforcement learning (Eysenbach et al., 2019).\nHowever, to effectively plan tasks using learning-based tech-\nniques, large datasets are required that are hard to collect in\nmany real-world domains.\nRecently, many works have used LLMs to translate natu-\nral language prompts to robot task plans (Ahn et al., 2022;\nHuang et al., 2022; Liang et al., 2022; Singh et al., 2022). For\n123\nAutonomous Robots (2023) 47:1057–1086 1061\nexample, Inner Monologue (Huang et al., 2022) uses LLMs\nin conjunction with environment feedback from various per-\nception models and state monitoring. However, because the\nsystem has no constraints, it can propose plans that are non-\nsensical. SayCan (Ahn et al., 2022), on the other hand,\ngrounds task plans generated by LLMs in the real world\nby providing a set of low-level skills the robot can choose\nfrom. A natural way of generating task plans is using code-\nwriting LLMs because they are not open-ended (i.e., they\nhave to generate code in a speciﬁc manner in order for it to\nbe executable) and are able to generate policy logic. Sev-\neral LLMs trained on public code are available, such as\nCodex (Chen et al., 2021), CodeT5 (Wang et al., 2021),\nAlphaCode (Li et al., 2022) and CodeRL (Le et al., 2022).\nLLMs can be prompted in a zero-shot way to generate\ntask plans. For example, Huang et al. ( 2022) analyzed the\nplanning ability of LLM in virtual environment, Code as\nPolicies (Liang et al., 2022) repurposes code-writing LLMs\nto write robot policy code, and ProgPrompt (Singh et al.,\n2022) generates plans that take into account the robot’s cur-\nrent state and the task objectives. PaLM-E is an embodied\nLLM that translates visual, state estimates, sensory data, and\nlanguage domains into embodied reasoning for robot plan-\nning (Driess et al., 2023). Text2Motion (Lin et al., 2023)\ncombines LLM with skill feasibility heuristics to guide task\nplanning. LLM-GROP (Ding et al., 2023) demonstrated\nhuman-aligned object rearrangement from natural-language\ncommands combined with TAMP . Inagaki et al. ( 2023) gen-\nerated Python code for an automated liquid-handling robot\nfrom natural language instructions. However, these methods\ngenerate Pythonic code, which is abundant on the Internet.\nFor domain-speciﬁc languages, naive zero-shot prompting is\nnot enough; the prompt has to incorporate information about\nthe target language so that the LLM can produce outputs\naccording to its rules.\nOur approach, on the other hand, generates a task plan\ndirectly from an LLM in a zero-shot way on a constrained\nset of tasks that are directly translatable to robot actions. We\nensure that the plan is syntactically valid and meets envi-\nronment constraints using iterative error checking. However,\nwhile the generated plan is veriﬁed for syntax and constraint\nsatisfaction, it does not consider the robot embodiment and\nworkspace scene, making its execution on a robot uncerti-\nﬁed. To address this issue, we integrate the generated task\nplans as intermediate goals into a certiﬁable task and motion\nplanner framework, which produces executable trajectories\nfor the robot.\n2.5 Task and motion planning with constraints\nTask and motion planning (TAMP) (Garrett et al., 2021)\nsimultaneously determines the sequence of high-level sym-\nbolic actions, such as picking and placing, and low-level\nmotions for the action, such as trajectory generation. Dif-\nferent approaches have been proposed in the literature to\nsolve the TAMP problem. For example, (Toussaint, 2015)\nproposed a non-linear constrained programming formula-\ntion for TAMP problems. In another work from the same\ngroup, (Toussaint et al., 2018) integrated TAMP with kine-\nmatic or dynamic constraints for tool-use and manipulation.\nAnother TAMP solver, PDDLStream (Garrett et al., 2020),\nextends PDDL (Ghallab et al., 1998), a common language to\ndescribe a planning problem mainly targeting discrete actions\nand states, by introducing streams, a declarative procedure\nvia sampling procedures. PDDLStream reduces a continuous\nproblem to a ﬁnite PDDL problem and invokes a classi-\ncal PDDL solver as a subroutine. Although pure-planning\napproaches to TAMP is general, it is computationally inefﬁ-\ncient. To accelerate the planning, an approach to incorporate\ngeometric information has been proposed (Dantam et al.,\n2018). Recently, geometric information also has been used\nwith learning-based approaches to improve planning efﬁ-\nciency (Khodeir et al., 2023; Kim et al., 2022). In (Driess\net al., 2020), an initial scene image is inputted to a neural\nnetwork that predicts the robot’s discrete action, and subse-\nquently, a motion planning problem is solved.\nAnother important aspect to take into account when solv-\ning a TAMP problem is the incorporation of safety measures\nand constraints. Since PDDLStream veriﬁes the feasibility\nof action execution during planning time, it can inherently\nenhance safety by avoiding unfeasible plans or plans that\nmay lead to unsafe situations. Nonetheless, PDDLStream\ndoes not yet account for constraints in the planning process,\nfor example, to avoid material spillage from beakers during\ntransportation, which impedes its deployment in real-world\nlab environments. For this purpose, sampling-based motion\nconstraint adherence (Berenson et al., 2011) or model-based\nmotion planning (Muchacho et al., 2022) are possible stream\nchoices. To overcome this shortcoming, our work extends\nPDDLStream with a projection-based sampling technique\n(Kingston et al., 2019) to provide constraint satisfaction,\ncompleteness, and global optimality. The proposed PDDL-\nStream takes intermediate goals generated by LLMs in a\nstructured language as its input.\n2.6 Skills and integration of chemistry lab tools\nIn the process of lab automation, robots interact with tools\nand objects within the workspace and require a repertoire of\nmany laboratory skills. Some skills can be completed with\nexisting heterogeneous instruments and sensors in chem-\nistry labs, such as scales, stir plates, pH sensors, and heating\ninstruments. Other skills are currently done either manually\nby humans in the lab or with expensive special instru-\nments. In a self-driving lab, robots should acquire those\nskills by effectively using different sensory inputs to compute\n123\n1062 Autonomous Robots (2023) 47:1057–1086\nappropriate robot commands. Pouring is a common skill in\nchemistry labs. Recent work (Kennedy et al., 2019; Huang\net al., 2021) used vision and weight feedback to pour liquid\nwith manipulators. (Kennedy et al., 2019) proposed using\noptimal trajectory generation combined with system identi-\nﬁcation and model priors. To achieve milliliter accuracy in\nwater pouring tasks with a variety of vessels at human-like\nspeeds, (Huang et al., 2021) used self-supervised learning\nfrom human demonstrations. In this work, we have reached\nsimilar results for pouring, using commercial scales that have\ndelayed feedback. Our approach is model-free, and it can\npour granular solids as well. Granular solids have different\ndynamics from liquids, similar to the avalanche phenomenon.\nLastly, while executing a chemistry experiment, the robot\nshould possess perception skills to measure progress toward\ncompleting the task. For example, in solubility experiments,\nthe robot should perceive when the solution is fully dissolved,\nand therefore stop pouring the solvent into the solution. There\nare different ways to measure solubility. In our work, we use\nthe turbidity measure (Shiri et al., 2021), which is based on\noptical properties of light scattering and absorption by sus-\npended sediment (Kitchener et al., 2017).\n3 Methods\nWe propose an automated robotic experiment platform that\ntakes instructions from a human in natural language and exe-\ncutes the corresponding experiment. The natural language\ninput is converted into a sequence of robot plans written in\na structured language by an LLM-based system, CLAIRify.\nXDL (Steiner et al., 2019) was used as the robot programming\nlanguage. The task and motion planning module generates\nthe robot motion from the generated XDL. The overview of\nthe proposed method is shown in Fig. 2.\n3.1 CLAIR IFY : natural language to structured\nprograms\nCLAIRify takes a chemistry experiment description in nat-\nural language and generates a structured experiment plan in\nXDL format, which will be fed into the subsequent module to\ngenerate robot motions. A general overview of the CLAIRify\npipeline is given in Fig. 2a.\nCLAIRify generates XDL with an automated iterative\nprompting between a generator and a veriﬁer. The generator\noutputs XDL from a prompt that combines the experiment\ndescription and the target language format description. How-\never, we cannot guarantee the output from the generator is\nsyntactically valid, meaning that it would deﬁnitely fail to\ncompile into lower-level robot actions. To generate syntac-\ntically valid programs, we pass the output of the generator\nthrough a veriﬁer. The veriﬁer determines whether the gener-\nator output follows all the rules and speciﬁcations of the target\nstructured language and can be compiled without errors. If\nit cannot, the veriﬁer returns error messages stating where\nthe errors were found and what they were. These are then\nappended to the generator output and added to the prompt\nfor the next iteration. This process is repeated until a valid\nprogram is obtained, or until the timeout condition is reached.\nAlgorithm 1 describes our proposed method.\nOnce the generator output passes through the veriﬁer with\nno errors, we are guaranteed that it is a syntactically valid\nstructured language. This output will then be translated into\nlower-level robot actions by passing it through TAMP for\nrobot execution. Each component of the pipeline is described\nin more detail below.\n3.1.1 Generator\nThe generator takes a user’s instruction and generates unver-\niﬁed structured language using an LLM. The input prompt\nto the LLM is composed of a description of the target lan-\nguage, a sentence specifying what the LLM should do (i.e.\n“Convert to XDL”), the command to the LLM, and the nat-\nural language instruction for which the task plan should be\ncreated. The description of the XDL language includes its\nﬁle structure and lists of the available actions (which can\nbe thought of as functions), their allowed parameters, and\ntheir documentation. The input prompt skeleton is shown in\nSnippet 1, Fig. 3.\nAlthough the description of the target structured language\nis provided, the output may contain syntactic errors. To\nensure syntactical correctness, the generator is iteratively\nprompted by the automated interaction with the veriﬁer. The\ngenerated code is passed through the veriﬁer, and if no errors\nare generated, then the code is syntactically correct. If errors\nare generated, we re-prompt the LLM with the incorrect task\nplan from the previous iteration along with the list of errors\nindicating why the generated steps were incorrect. The skele-\nton of the iterative prompt is shown in Snippet 2, Fig. 3.T h e\nfeedback from the veriﬁer is used by the LLM to correct the\nerrors from the previous iteration. This process is continued\nuntil the generated code is error-free or a timeout condition\nis reached, in which case the system reports not being able\nto generate a task plan.\n3.1.2 Veriﬁer\nThe veriﬁer works as a syntax checker and static analyzer to\ncheck the output of the generator and send feedback to the\ngenerator. It ﬁrst checks whether the input can be parsed as\ncorrect XML and then checks the allowance of action tags,\nthe existence of mandatory properties, and the correctness\nof optional properties using a rule-based method that checks\nfor permissible functions and parameters in the XDL docu-\n123\nAutonomous Robots (2023) 47:1057–1086 1063\nFig. 2 Our framework. a CLAIRify: LLM-based natural language pro-\ncessing module. The LLM takes the input (1), structured language\ndeﬁnition, and (optionally) resource constraints and generates unver-\niﬁed structured language (2). The output is examined by the veriﬁer\nand is passed to LLM with feedback (3). The LLM-generated outputs\npass through the veriﬁer (4). The correct output (5) is passed to the task\nand motion planning module (6) to generate robot trajectories. b Robot\nplanning module, which is composed of Perception, Task and Motion\nPlanning,a n d Skills blocks. Our framework enables the robot to lever-\nage available chemistry lab devices (including sensors and actuators) by\nadding them to the robot network through ROS. The robot is equipped\nwith an additional DoF at the end-effector, allowing it to perform con-\nstrained motions. Our framework receives the chemical synthesis goal\nin XDL format. The procedurecomponent is converted into correspond-\ning PDDL goals, and hardware and reagents components identify the\nrequired initial condition for synthesis. Perception detects objects and\nestimates their positions, contents in the workspace, and task progress.\nPDDLStream generates a sequence of actions for the robot execution\n(7)\n123\n1064 Autonomous Robots (2023) 47:1057–1086\nAlgorithm 1 CLAIRify: V eriﬁer-Assisted Iterative Prompts\nInput: Structured language description L, instruction x\nOutput: Structured language task plan, ySL\nprocedure IterativePrompting(L, x)\nySL ′ = Generator(L, x)\nerrors = Ve r i ﬁ e r(ySL ′)\nwhile len(errors) > 0 and timeout condition != True do\nySL ′ = Generator(L, x, ySL ′,errors)\nerrors = Ve r i ﬁ e r(ySL ′)\nend while\nySL = ySL ′\nend procedure\nFig. 3 Prompt skeleton: (1) At the initial generation, we prompt the\nLLM with a description of XDL and the natural language instruction.\n(2) After the LLM generates structured-language-like output, we pass\nit through our veriﬁer. If there are errors in the generated program, we\nconcatenate the initial prompt with the XDL from the previous iteration\nand a list of the errors. The full prompt can be viewed in “Appendix B”\nmentation. This evaluates if the input is syntactically correct\nXDL. The veriﬁer also checks the existence of deﬁnitions of\nhardware and reagents used in the procedure or provided as\nenvironment constraints, which works as a basic static anal-\nysis of necessary conditions for executability. If the veriﬁer\ncatches any errors, the candidate task plan is considered to\nbe invalid. In this case, the veriﬁer returns a list of errors it\nfound, which is then fed back to the generator. The role of the\nveriﬁer is limited to pointing out the errors, and it does not\npropose how to ﬁx them. To propose a correct modiﬁcation,\nthe veriﬁer requires an understanding of the meaning of the\ninput. However, semantic understanding is beyond the ability\nof a rule-based system. Therefore, in CLAIRify, the LLM-\nbased generator ﬁxes the errors using the feedback messages\nFig. 4 Web interface for CLAIRify. Users input natural language\ndescriptions of the experiment in the left column. XDL is generated\nin the right column when the user pushes the Translate button\nfrom the veriﬁer. The error message is designed to be concise\nto save the context length of LLM.\n3.1.3 Incorporating environment constraints\nBecause resources in a robot workspace are limited, we need\nto consider those constraints when generating task plans. If\nspeciﬁed, we include the available resources in the gener-\nator prompt. The veriﬁer also catches if the candidate plan\nuses any resources aside from those mentioned among the\navailable robot resources. Those errors are included in the\ngenerator prompt for the next iteration. If a constraint list is\nnot provided, we assume the robot has access to all resources.\nIn the case of chemistry lab automation, those resources\ninclude experiment hardware and reagents.\n3.1.4 User interface\nWe provide a graphical user interface for CLAIRify to\nincrease accessibility. Users can access it via a web browser\nand CLAIRify is called by the Python backend implemented\nin Flask (Grinberg, 2018). In Fig. 4, we show the user inter-\nface. The user enters an experiment (in the ﬁgure, we show\nExperiment 0 from the Chem-EDU dataset). After pressing\nthe Translate button, the interface shows the execution log\nand generated XDL in the right panel with syntax highlight-\ning. The time taken to generate XDL from a natural language\nusing CLAIRify is mainly dependent on the OpenAI server\nresponse time and the number of times the generator is called.\nFor the experiment in Fig. 4, we measured the translation\ntime. These measurements were performed on three separate\noccasions, spanning different days and times. On average, the\ngeneration process took approximately 33 ± 3 s per iteration\nand required two generator calls.\n123\nAutonomous Robots (2023) 47:1057–1086 1065\nAlgorithm 2 TampForLabAutomation()\nInput: A XDL recipe χ, sensory input H, PDDLStream domain D\nOutput: Reference plan to execute\n1: Goals ,O ←xdlParser(χ) ⊿ Objects\n2: I ← perception(H,O) ⊿ Initial conditions\n3: if not passConditions(I, O) then return\n4: plan =∅ , P =∅\n5: for all G ∈ Goals do\n6: while time () ≤ tmax do\n7: P = optimisticPddlStreamPlan(I, G, D)\n8: if P ̸=∅ and isStreamFeasible(P) then break\n9: end while\n10: if P =∅ then return\n11: plan ← plan ∪ P\n12: I = updateSceneRepresentation(I,P)\n13: end for\n14: return plan\n3.2 Task and motion planning for chemistry\nexperiments\nOur task plan execution framework consists of three com-\nponents: perception, task and motion planning (TAMP), and\na set of manipulation skills, as shown in Fig. 2b. XDL input\ncoming from CLAIRify provides a high-level description of\nexperiment instructions to the TAMP module. The percep-\ntion module updates the scene description by detecting the\nobjects and estimating their positions using ﬁducial markers.\nWe used AprilTag (Olson, 2011). Currently, we assume prior\nknowledge of vessel contents and sizes, and each vessel is\nmapped to a unique marker ID. Given the instructions from\nXDL and the instantiated workspace state information from\nperception, a sequence of high-level actions and robot tra-\njectories are simultaneously generated by our PDDLStream\nTAMP solver (Y oshikawa et al., 2023). The resulting plan\nis then realized by the manipulation module and robot con-\ntroller, while closing the loop with perception feedback, such\nas updated object positions and status of the solution.\nThe TAMP module converts experiment instructions given\nby XDL into PDDLStream goals and generates a motion plan.\nThe TAMP algorithm is shown in Algorithm 2.\n3.2.1 PDDLStream\nA PDDLStream problem described by a tuple (P,A,S,O,I,\nG) is deﬁned by a set of predicates P, actions A, streams\nS, initial objects O, an initial state I, and a goal state G.\nA predicate is a boolean function that describes the logical\nrelationship of objects. A logical action a ∈ A has a set\nof preconditions and effects. The action a can be executed\nwhen all the preconditions are satisﬁed. After execution, the\ncurrent state changes according to the effects. The set of\nstreams, S, distinguishes a PDDLStream problem from tra-\nditional PDDL. Streams are conditional samplers that yield\nobjects that satisfy speciﬁc constraints. The goal of PDDL-\nStream planning is to ﬁnd a sequence of logical actions and\na continuous motion trajectory starting from the initial state\nuntil all goals are satisﬁed, ensuring that the returned plan is\nvalid and executable by the robot. We deﬁne four types of\nactions in our PDDLStream domain: pick, move, place, and\npour. For example, the move action translates the robot end-\neffector from a grasping pose to a placing or pouring pose\nusing constrained motion planning. PDDLStream handles\ncontinuous motion using streams. Streams generate objects\nfrom continuous variables that satisfy speciﬁed conditions,\nsuch as feasible grasping pose and collision-free motion. An\ninstance of a stream has a set of certiﬁed predicates that\nexpands I and functions as preconditions for other actions.\nA PDDLStream problem is solved by invoking a classical\nPDDL planner, such as Fast Downward (Helmert, 2006), with\noptimistic instantiation of streams (line 7, Algorithm 2). If a\nplan for the PDDL problem is found, the optimistic stream\ninstances s ∈ S in the plan are evaluated to determine the\nactual feasibility (line 8). If no plan was found or the streams\nare not feasible, other plans are explored with a larger set of\noptimistic stream instances.\nChemical description language (XDL) XDL is based on\nXML syntax and is mainly composed of three mandatory\nsections: Hardware, Reagents, and Procedure. We parse XDL\ninstructions and pass them to the TAMP module. The Hard-\nware and Reagents sections are parsed as initial objects O.\nProcedure is translated into a set of goals Goals (line 1, Algo-\nrithm 2). I is generated from O and sensory inputs (line 2).\nEach intermediate goal G ∈ Goals is processed by PDDL-\nStream (line 5). If a plan to attain G is found, it is stored\n(line 10) and I is updated according to the plan (line 11).\nAfter a set of plans to attain all goals is found, we obtain a\ncomplete motion plan (line 12).\nPlan reﬁnement at execution time We adopt two consid-\nerations for the dynamic nature of chemistry experiments:\nmotion plan reﬁnement and task plan reﬁnement.\nThe generated motion plan is reﬁned to reﬂect the updated\nstatus of the scene and to overcome perception errors. The\ninitial object pose detection may contain errors, therefore,\nthe object may not be present in the expected position during\nexecution. This error arises for two reasons. First, when the\nrobot interacts with the objects in the workspace, their posi-\ntion changes, for example when regrasping an object after\nplacing it in the workspace. This change is not always fore-\nseeable by the planner ahead of time. Second, the perception\nerror is lower when the grasping pose is estimated when the\nrobot in-hand camera is closer to the target object, consider-\ning the hand-eye calibration error. Lowering the perception\nerror makes the execution more robust to grasping failures.\nTherefore, to improve the success rate, the object pose is\nestimated just before grasping, and the trajectory is reﬁned.\nWe assume that the perturbation of the perceived state of\nthe objects is bounded so that it does not cause a change\n123\n1066 Autonomous Robots (2023) 47:1057–1086\nin the logical state of the system, which would necessitate\ntask-level replanning.\nIn addition to motion reﬁnement, we consider task plan\nreﬁnement. Task execution can be repeated using the feed-\nback from perception modules at execution time to support\nconditional operations in chemistry experiments, such as\nadding acid until pH reaches 7. The number of repetitions\nrequired to satisfy conditions is unknown at planning time,\nso the task plan is reﬁned at execution time.\n3.2.2 Motion constraints for spillage prevention\nUnlike pick-and-place of solid objects, robots in a chemistry\nlab need to carry beakers that contain liquids, powders, or\ngranular materials. These chemicals are sometimes harmful,\nso the robot motion planner should incorporate constraints\nto prevent spillage. To this end, an important requirement for\nrobot motion is the orientation constraints of the end-effector.\nTo avoid spillage, the end-effector orientation should be kept\nin a limited range while beakers are grasped. We incorpo-\nrated constrained motion planning in the framework to meet\nthese safety requirements, under the assumption of velocity\nand acceleration upper bounds. Moreover, we introduced an\nadditional (8th) degree of freedom to the robot arm, in order\nto increase the success rate of constrained motion planning.\nWe empirically observed no spillage as long as orientation\nconstraints are satisﬁed in the regular acceleration and veloc-\nity of the robot end-effector, particularly since beakers are\ntypically not ﬁlled to their full capacity in a chemistry lab.\nAlgorithm 3 ConstrainedMotionPlanning()\n1: for all i ∈ trials do\n2: qg ← solveIK((I pB ,I RB ))\n3: pathPlanner← init(q0, qg )\n4: while path is ∅ do\n5: q ← sample()\n6: while ∥F (q)∥ >ϵ do\n7: δq ← J † (q)F (q)\n8: q ← q − δq\n9: end while\n10: path ← pathPlanner(q)\n11: end while\n12: if path ̸=∅ then return path\n13: end for\n14: return path\nConstrained motion planning Given a robot with n degrees\nof freedom in the workspace Q ∈ Rn with obstacle regions\nQobs ∈ Rn, the constrained planning problem can be\ndescribed as ﬁnding a path in the manipulator’s free con-\nﬁguration space Q fre e = Q − Qobs that satisﬁes initial\nconﬁguration q0 ∈ Rn, end-effector goal pose (I pB ∈\nR3,I RB ∈ SO (3)), and equality path constraints F (q) :\nQ → Rk . The constrained conﬁguration space can be repre-\nsented by the implicit manifold M ={ q ∈ Q | F (q) = 0}.\nThe implicit nature of the manifold prevents planners from\ndirectly sampling since the distribution of valid states is\nunknown. Further, since the constraint manifold resides in\na lower dimension than the conﬁguration space, sampling\nvalid states in the conﬁguration space is highly improbable\nand thus impractical. Following the constrained motion plan-\nning framework developed in (Kingston et al., 2019, 2018),\nour framework integrates the projection-based method for\nﬁnding constraint-satisfying conﬁgurations during sampling\nas described in Algorithm 3. In this work, the constraints are\nset to the robot end-effector, hence they can be described with\ngeometric forward kinematics, with its Jacobian deﬁned as\nJ (q) =\nδF\nδq . After sampling from Q fre e in line 5, projected\nconﬁgurations q are found by minimizing F (q) iteratively\nusing Newton’s method (highlighted in grey). We use prob-\nabilistic roadmap methods (PRM\n⋆ ) to plan efﬁciently in the\n8-DoF conﬁguration space found in our chemistry laboratory\ndomain (Karaman & Frazzoli, 2011; Kavraki et al., 1996).\nThe constrained path planning problem is sensitive to\nthe start and end states of the requested path, since paths\nbetween joint states may not be possible under strict or mul-\ntiple constraints. If constrained planning is executed with\nany arbitrary valid solution from the IK solver, the planner\ntypically fails. To address this shortcoming, three consider-\nations are made. First, a multi-threaded IK solver with both\niterative and random-based techniques is executed, and the\nsolution that minimizes an objective function φ is returned\nwith TRAC-IK, proposed in (Beeson & Ames, 2015). Dur-\ning grasping and placing, precision is paramount, and we\nonly seek to minimize the sum-of-squares error between\nthe start and goal Cartesian poses. Second, depending on\nthe robot task, the objective function is extended to maxi-\nmize the manipulability ellipsoid as described in (Y oshikawa,\n1985), which is applied for more complicated maneuvers,\nsuch as transferring liquids across the workspace. Finally,\nnote that conﬁguration sampling must account for the fact\nthat multiple goal conﬁgurations are possible. For this pur-\npose, Algorithm 3 can iterate several times to ﬁnd various\ngoal conﬁgurations in line 2.\n8-DoF robot arm To increase the success rate of planning\nand grasping under non-spillage constraints, we introduced\nan additional degree of freedom to the 7-DoF Franka robot\nand mounted the end-effector parallel to the ﬂoor as shown\nin the bottom right of Fig. 2b. The parallel grasping and\nintroduction of a revolute joint facilitate the manipulation\nof liquids within beakers. Parallel grasping, where the grip-\nper is horizontal, is advantageous over top-down grasping to\npour liquids because the robot hand does not touch the rim\nof a beaker and does not block the ﬂow of liquid. Frequently,\nwhen planning constrained motion, the initial 7-DoF robot\narm would encounter joint limit issues with parallel grasping\nposes, rendering the subsequent pouring action impractical.\n123\nAutonomous Robots (2023) 47:1057–1086 1067\nIt is possible to achieve parallel grasping by adjusting the\nangle of the end-effector without adding an extra revolute\njoint by connecting the end-effector with a rigid link to the\nrobot. However, the integration of a servo motor directly\nbefore the gripper facilitates pouring control, as it enables\nthe accomplishment of pouring actions using a single motor.\nLimiting the pouring motion to the last joint is also advan-\ntageous for the safety of humans near the robot because\npouring motion sometimes entails quick rotations. Mount-\ning the robot base on the wall instead of the default tabletop\nplacement is another possible solution to parallel grasping,\nbut it is not supported by the manufacturer. In addition to the\nadvantage in pouring, the 8-DoF robot has a higher empiri-\ncal success rate in constrained motion planning in the parallel\ngrasping pose, as shown in Sect. 4.4.2, which leads to a higher\nsuccess rate in total task and motion planning. As a result,\nadding one degree of freedom was the most suitable solution\nfor our purpose.\n3.2.3 Manipulation and perception skills\nChemistry lab skills require a particular suite of sensors,\nalgorithms, and hardware. We provide an interface for\ninstantiating different skill instances through ROS and simul-\ntaneously commanding them. For instance, recrystallization\nexperiments in chemistry require pouring, heating, and stir-\nring, which uses both weight feedback for volume estimation\nand skills for interacting with the liquid using available hard-\nware.\nPouring controller In chemistry labs, a frequently used skill\nin chemical experiments is pouring. Pouring involves high\nintra-class variations depending on the underlying objective\n(e.g., reaching a desired weight or pH value); the substances\nand material types being handled (e.g., granular solids or\nliquids); the glassware being used (e.g., beakers and vials);\nthe overall required precision; and the availability of accu-\nrate and fast feedback. Pouring is a closed-loop process, in\nwhich feedback should be continuously monitored. Among\nthese pouring actions, in our work, we consider the follow-\ning variations: pouring of liquids and pouring of granular\nsolids. Note that, in contrast to many control problems, pour-\ning is a non-reversible process where we cannot compensate\nfor overshoot (as the poured material cannot go back to the\npouring beaker if mixed with another material).\nInspired by observations of chemists pouring reagents, we\npropose a controller that allows the robot to perform differ-\nent pouring actions. As shown in Fig. 5, the proposed method\ntakes sensor measurements (e.g., weight feedback from the\nscale) as feedback and a reference pouring target. The algo-\nrithm outputs a robot end-effector joint velocity describing\noscillations of the arm’s wrist. Since sensors are character-\nized by measurement delays, chemical reactions require time\nFig. 5 Pouring skill controller: given the XDL and TAMP reference\nvalues and sensor feedback, the pouring controller computes the end-\neffector velocity for the robot by blending a shaping function s(t) and\na PD control output v\nPD (t)\nFig. 6 An example of pouring control. The velocity of the end-effector\nis controlled based on the feedback error and shaping function\nto stabilize, and pouring is a non-reversible action, chemists\ntend to conservatively pour a small amount of content from\nthe pouring vessel into the target vessel. They periodically\nwait for some time to observe any effects and then pour\nmicro-amounts again. In our approach, we use a shaping\nfunction s(t) to guide the direction and frequency of this\noscillatory pouring behavior, while a PD controller lowers the\npouring error. The end-effector velocity vector is computed\nby blending the shaping function s(t)over the PD control sig-\nnal, v\nPD(t) = k pe + kd ˙e, where e(t) = xref − x fb . Figure 6\nshows an example of the angular velocity of the end-effector\nand the error during actual pouring. More information about\nthe pouring skill method can be found in “Appendix A ”.\nTurbidity-based solubility measurement The solubility of a\nsolute is measured by determining the minimum amount of\nsolvent (water) required to dissolve all solutes at a given tem-\nperature when the overall system is in equilibrium (Shiri et\nal., 2021). Since the solutions get transparent when all solutes\ndissolve into water, turbidity, or opaqueness of the solution,\nis used as the metric to determine the completion of the exper-\n123\n1068 Autonomous Robots (2023) 47:1057–1086\nFig. 7 An example of automated turbidity measurement. The cam-\nera detects the Petri dish using Hough Circle Transform. The average\nbrightness of the detected area (red square) is used as a proxy of turbidity\n(Color ﬁgure online)\niment. The average brightness of the solution was used as a\nproxy for the relative turbidity, inspired by HeinSight (Shiri\net al., 2021). That work compared the current measured\nturbidity value with a reference value (coming from pure sol-\nvent) to determine when the solute was dissolved. Differently\nfrom them, we use the relative turbidity changes between\nthe current and previous measurement values to detect when\nthe solution is dissolved. Moreover, to make the perception\npipeline autonomous, when the robot with an in-hand camera\nobserves the dish containing the solution, it detects the largest\ncircular shape as the dish using a Hough Circle Transform\nimplemented in OpenCV . The square region containing the\ndish is converted into the HSV color space, and the average\nV alue (brightness) of the region is used as a turbidity value.\nFigure 7 shows an example of the automated turbidity mea-\nsurement. Although the detected area contains the dish and\nstir bar, they do not affect the relative value because these are\na constant bias in all measurements.\n4 Experiments\n4.1 XDL generation\nWe conducted experiments to evaluate the following hypothe-\nses: i) Automated iterative prompting increases the success\nrate of unfamiliar language generation, ii) The quality of\ngenerated task plans is better than existing methods. To gen-\nerate XDL plans, we use text-davinci-003,t h em o s t\ncapable GPT −3.5 model accessible using the OpenAI API\nat the time of performing experiments (February 2023). We\nchose to use this instead of code-davinci-002 due to\nquery and token limits. Additionally, ChatGPT was not yet\navailable through the API at that time.\n4.1.1 Datasets\nWe evaluated our method on two different datasets (Table 1):\nChem-RnD (chemistry research and development) This\ndataset consists of 108 detailed chemistry-protocols for\nsynthesizing different organic compounds in real-world\nchemistry labs, sourced from the Organic Syntheses dataset\n(volume 77) (Mehr et al., 2020a). Due to GPT-3 token limits,\nwe only use experiments with less than 1000 characters. We\nuse Chem-RnD as a proof-of-concept that our method can\ngenerate task plans for complex chemistry methods. We do\nnot aim to execute the plans in the real world, and so we do\nnot include any constraints.\nChem-EDU (everyday educational chemistry) We evaluate\nthe integration of CLAIRify with real-world robots through\na dataset of 42 natural language instructions containing\nonly safe (edible) chemicals and that are, in principle, exe-\ncutable by our robot. The dataset consists of basic chemistry\nexperiments involving edible household chemicals, includ-\ning acid–base reactions and food preparation procedures.\n1\nWe show some data samples in “Appendix C”. When gen-\nerating the XDL, we also included environment constraints\nbased on what equipment our robot had access to (for exam-\nple, our robot only had access to a mixing container called\n“beaker”).\n4.1.2 Metrics and results\nThe results section is organized based on the four perfor-\nmance metrics that we will consider, namely: Ability to\ngenerate structured-language output, Quality of the gener-\nated plans, Number of interventions required by the veriﬁer,\nand Robotic validation capability. We compared the perfor-\nmance of our method with SynthReader, a state-of-the-art\nXDL generation algorithm which is based on rule-based tag-\nging and grammar parsing of chemical procedures (Mehr et\nal., 2020).\n1. Ability to generate a structured language plan. First,\nwe investigate the success probability for generating plans.\nFor CLAIRify, if it is in the iteration loop for more than\nx steps (here, we use x = 10), we say that it is unable to\ngenerate a plan and we exit the program. When compar-\ning with SynthReader, we consider that approach unable to\ngenerate a structured plan if the SynthReader IDE (called\nChemIDE) throws a fatal error when asked to create a plan.\n2\nFor both models, we also consider them unable to generate\na plan if the generated plan only consists of empty XDL\n1 CLAIRify Data & code: https://github.com/ac-rad/xdl-generation/.\n2 ChemIDE using XDL: https://croningroup.gitlab.io/chemputer/xdlapp/.\n123\nAutonomous Robots (2023) 47:1057–1086 1069\nTable 1 Comparison of our\nmethod with existing methods\non the number of successfully\ngenerated valid XDL plans and\ntheir quality on 108 organic\nchemistry experiments from\n(Mehr et al., 2020a)b y1 0\nexpert chemists\nDataset Method Number generated ↑ Expert preference ↑\nChem-RnD SynthReader (Mehr et al., 2020) 92/108 13/108\nCLAIRify [ours] 105/108 75/108\nChem-EDU SynthReader (Mehr et al., 2020)0 / 4 2 –\nCLAIRify [ours] 42/42 –\nResults of the best-performing models for a given metric and dataset are given in bold\nFig. 8 Violin plots showing distributions of different error categories\nin XDL plans generated for experiments for the Chem-RnD (left) and\nChem-EDU (right) datasets. The x-axis shows the error categories and\nthe y-axis shows the number of errors for that category (lower is bet-\nter). For the Chem-RnD dataset, we show the error distributions for both\nCLAIRify and SynthReader. Each violin is split in two, with the left\nhalf showing the number of errors in plans generated from CLAIRify\n(teal) and the right half showing those from SynthReader (navy). For\nthe Chem-EDU dataset, we only show the distributions for CLAIRify.\nIn both plots, we show the mean of the distribution with a gold dot (and\nthe number beside in gold) and the median with a grey dot (Color ﬁgure\nonline)\ntags (i.e., no experimental protocol). For all experiments, we\ncount the total number of successfully generated language\nplans divided by the total number of experiments. Using\nthis methodology, we tested the ability of the two models\nto generate output on both the Chem-RnD and Chem-EDU\ndatasets. The results for both models and both datasets are\nshown in Table 1. We ﬁnd that out of 108 Chem-RnD exper-\niments, CLAIRify successfully returned a plan 97% of the\ntime, while SynthReader returned a plan 85% of the time. For\nthe Chem-EDU dataset, CLAIRify generated a plan for all\ninstructions. SynthReader was unable to generate any plans\nfor that dataset, likely because the procedures are different\nfrom typical chemical procedures (they use simple action\nstatements). This demonstrates the generalizability of our\nmethod: we can apply it to different language styles and\ndomains and still obtain coherent plans.\n2. Quality of the predicted plan (without executing the\nplan). To determine if the predicted task plans actually\naccomplish every step of their original instructions, we\nreport the number of actions and parameters that do not\nalign between the original and generated plan, as anno-\ntated by expert experimental chemists. To compare the\nquality of the generated plans between CLAIRify and\nSynthReader, we ask expert experimental chemists to,\ngiven two anonymized plans, either pick a preferred plan\namong them or classify them as equally good. We also\nask them to annotate errors in the plans in the follow-\ning categories: Missing action, Missing parameter, Wrong\naction, Wrong parameter, Ambiguous value, Other error.\nHere, actions refer to high-level steps in the procedure\n(e.g., <Add reagent=“acetic acid”> is an action)\nand parameters refer to reagents, hardware, quantities and\nexperiment descriptors (e.g., in <HeatChill vessel\n=“beaker” temp=“100C”> , vessel and temp are both\nparameters).\nThe chemists were 10 graduate students and postdoctoral\nfellows from the University of Toronto and ETH Zürich.\nFor the ﬁrst 20 experiments, the chemists labelled them\ntogether as a group to resolve any labelling ambiguities and\nthen labelled the rest individually (approximately 9 experi-\nments per chemist). The annotations were performed using\nthe LightTag Text Annotation Tool (Perry, 2021).\nChem-RnD dataset The results for the Chem-RnD dataset\nwith respect to expert preference are reported in the last col-\numn of Table 1. We found that out of 108 experiments,\nexperts preferred the XDL plan generated from CLAIRify\n75 times and the one from SynthReader 13 times (the remain-\ning 20 were considered to be of similar quality).\nThe distributions of the annotated errors are shown in\nFig. 8. We ﬁnd that for 4 out of 6 error categories, our model\ndoes at least as well as or better than the baseline method\nwhen considering the mean and median of the distributions.\nWe also ﬁnd that for those categories, our method produces\nmore experiments with 0 errors.\n123\n1070 Autonomous Robots (2023) 47:1057–1086\nOne advantage of our method is that it generates less plans\nwith missing actions compared with the baseline. As XDL\ngeneration in SynthReader is implemented by rule-based\npattern-matching techniques, any actions that do not match\nthose templates would not appear in the ﬁnal XDL. For exam-\nple, for the protocol:\nTo a solution of m- CPBA (200 mg, 0.8 mmol)\nin dichloromethane (10 m L), cooled to 0 ◦C,\nwas added dr opwise a solution of 5- chloro\n-10-oxa-3-thia - tricyclo [5.2.1.01, 5] dec-8-\nene (150 mg, 0.8 mmol) in dichloromethane\n(10 mL).\nthe plan generated by CLAIRify was\n<Add vessel= \"V1\" reagent= \"m-CPBA\" amount= \"\n200 mg\" />\n<Add vessel= \"V1\" reagent= \"dichloromethane\"\nvolume= \"10 mL\" />\n<HeatChill vessel= \"V1\" temp= \"0 ◦ C\" time= \"3\nmin\" />\n<Add vessel= \"V2\" reagent= \"5-chloro -10-oxa-3-\nthia - tricyclo[5,2,1 ,0*1,5*]dec-8-ene\" amount\n=\"150 mg\" />\n<Add vessel= \"V2\" reagent= \"dichloromethane\"\nvolume= \"10 mL\" />\n<Transfer from_vessel= \"V2\" to_vessel= \"V1\" />\nwhile the plan generated from SynthReader was\n<Add vessel= \"reactor\" reagent= \"5-chloro -10-\noxa-3-thia - tricyclo\" volume= \"0\" speed= \"40.0\"\n/>\nOur model is able to decompose a complicated procedure\ninto simpler actions by making two solutions in separate\nbeakers and combining them with a Transfer procedure. It\nalso assumes that the solutions don’t already exist as mix-\ntures and creates them from scratch. This is another beneﬁt\nof our model, as it is able to understand implicit actions. For\nexample, given the prompt\nL-Ornithine (31.92 g, 120 mmol) was added to\na mixture of KOH (6.72 g, 120 mmol), water\n(200 ml) and THF (100 ml)\nSynthReader considers a mixture of three chemicals as a sin-\ngle solution and creates the action:\n<Add vessel= \"reactor\" reagent= \"a mixture of\nKOH (6.72 g, 120 mmol), water (200 ml) and\nTHF (100 ml)\" volume= \"0\" speed= \"40.0\" />\n<AddSolid vessel= \"reactor\" reagent= \"L-\nOrnithine\" mass= \"31.92 g\" />\nOn the other hand, CLAIRify correctly understand the\nimplicit action to mix them beforehand and generates an\nappropriate XDL:\n<Add vessel= \"V1\" reagent= \"L-Ornithine\"\namount= \"31.92 g\" />\n<Add vessel= \"V1\" reagent= \"KOH\" amount= \"6.72\ng\" />\n<Add vessel= \"V1\" reagent= \"Water\" amount= \"200\nml\" />\n<Add vessel= \"V1\" reagent= \"THF\" amount= \"100\nml\" />\nHowever, our model produced plans with a greater num-\nber of wrong actions than SynthReader. This is likely because\nour model is missing domain knowledge on certain actions\nthat would need to be included in the prompt or veriﬁer.\nFor example, given the instruction “Dry solution over mag-\nnesium sulfate” , our model inserts a <Dry.../> into the\nXDL plan, dbut the instruction is actually referring to a\nprocedure where one passes the solution through a short car-\ntridge containing magnesium sulphate, a procedure which\nseems to be encoded in SynthReader. Another wrong action\nour model performs is reusing vessels. In chemistry, one\nneeds to ensure a vessel is uncontaminated before using it.\nHowever, our model generates plans that can use the same\nvessel in two different steps without washing it in between.\nOur model also sometimes generates plans with ambigu-\nous values. For example, many experiment descriptions\ninclude conditional statements such as “Heat the solution\nat the boiling point until it becomes white” . Conditions\nin XDL need a numerical condition as a parameter. Our\nmodel tries to incorporate them by including actions such\nas <HeatChill temp=“boiling point” time =\n“until it becomes white”/> , but they are ambigu-\nous. We can make our model better in the future by\nincorporating more domain knowledge into our structured\nlanguage description and improving our veriﬁer with real-\nworld constraints. For example, we can incorporate visual\nfeedback from the environment, include look-up tables for\ncommon boiling points, and ensure vessels are not reused\nbefore cleaning. Other errors include misunderstanding nota-\ntions commonly used in chemistry experiments. For instance,\ninstructions such as “Wash with EtOAc (2 × 10 mL)” indi-\ncate the need for two separate WashSolid() actions in\nXDL. However, the large language model often struggles to\nparse the (2 × 10 mL) notation correctly, resulting in either\nperforming a single WashSolid() action with 10 mL (or\noccasionally 20 mL if a multiplication action is inferred) or\n(more rarely) omitting the action entirely.\nAnother cause of errors occurs when a speciﬁc value is\nnot provided in the instruction. For example, if an instruc-\ntion states “wash product in EtOAc” without specifying a\nvolume, the model is unable to generate a reasonable value\nand defaults to writing 0 mL in the generated plan.\nDespite the XDL plans generated by our method contain-\ning errors, we found that the experts placed greater emphasis\non missing actions than ambiguous or wrong actions when\npicking the preferred output, indicating larger severity of this\nclass of error for the tasks and outputs investigated here.\nChem-EDU dataset We annotated the errors in the Chem-\nEDU datasets using the same annotation labels as for the\nChem-RnD dataset. The breakdown of the errors is in the\nright plot of Fig. 8. Note that we did not perform a comparison\nwith SynthReader as no plans were generated from it. We ﬁnd\nthat the error breakdown is similar to that from Chem-RnD,\nwhere we see amibiguous values in experiments that have\nconditionals instead of precise values. We also encounter\na few wrong parameter errors, where the model does not\n123\nAutonomous Robots (2023) 47:1057–1086 1071\nTable 2 V eriﬁer analysis Dataset Average num. Max/min Error type caught by veriﬁer [count]\nveriﬁer calls veriﬁer calls\nChem-RnD 2 .58 ± 2.00 10/1 Missing property in action [306]\nProperty not allowed [174]\nWrong tag [120]\nAction does not exist [21]\nItem not deﬁned in hardware or reagents list [15]\nPlan cannot be parsed as XML [6]\nChem-EDU 1 .14 ± 0.47 3/1 Item not deﬁned in hardware or reagents list [47]\nProperty not allowed [26]\nWrong tag [40]\nMissing property in action [3]\nWe report the average number of times CLAIRify calls the veriﬁer for the experiments in a given dataset, as\nwell as the minimum and maximum number of times. We also report the type of error encountered by the\nveriﬁer and the number of times it caught that type\ninclude units for measurements. This can be ﬁxed in future\nwork by improving the veriﬁer to check for these constraints.\n3. Number of interventions required by the veriﬁer. To\nbetter understand the interactions between the generator and\nveriﬁer in CLAIRify, we analyzed the number of interac-\ntions that occur between the veriﬁer and generator for each\ndataset to understand the usefulness of the veriﬁer. In Table 2,\nwe show that each experiment in the Chem-RnD dataset\nruns through the veriﬁer on average 2.6 times, while the\nChem-EDU dataset experiments runs through it 1.15 times\non average. The difference between the two datasets likely\nexists because the Chem-EDU experiments are shorter and\nless complicated. The top Chem-EDU error encountered by\nthe veriﬁer was that an item in the plan was not deﬁned in\nthe Hardware or Reagents list, mainly because we included\nhardware constraints for this dataset that we needed to match\nin our plan. In Fig. 9, we show a sample loop series between\nthe generator and veriﬁer.\n4.2 Robot execution\nTo analyze how well our system performs in the real world,\nwe execute a few experiments from the Chem-EDU dataset\non our robot. Three experiments from the Chem-EDU dataset\nwere selected to be executed.\n4.2.1 Experiment setup\nHardware The proposed lab automation framework has\nbeen evaluated using the Franka Emika Panda arm robot,\nequipped with a Robotiq 2F-85 gripper and an Intel RealSense\nD435i stereo camera mounted on the gripper to allow for\nactive vision. The robot’s DoF has been extended by one\ndegree (in total 8-DoF) at its end-effector using a Dynamixel\nXM540-W150 servo motor. Figure 2 shows the hardware\nsetup.\nLab tools integration The robot framework is expanded\nby incorporating lab tools. We used an IKA RET control-\nvisc device, which works as a scale, hotplate, and stir plate,\nand a Sartorius BCA2202-1 S Entris, which works as a high-\nprecision weighing scale. The devices communicate with the\nTAMP solver to execute chemistry speciﬁc skills.\nSoftware The robot is controlled using FrankaPy (Zhang\net al., 2020). We implemented a ROS wrapper for the\nservo motor (8th DoF). To detect ﬁducial markers, we use\nthe AprilTag library (Olson, 2011). We use the MoveIt\nmotion planning framework (Coleman et al., 2014) for our\nTAMP solver and its streams. The constrained planning func-\ntion (Kingston et al., 2019) is an extension of elion.\n3\n4.2.2 Solution color change based on pH\nAs a basic chemistry experiment, we demonstrated the color\nchange of a solution containing red cabbage juice. This is a\npopular introductory demonstration in chemistry education,\nas the anthocyanin pigment in red cabbage can be used as a\npH indicator (Fortman & Stubbs, 1992). We prepared red cab-\nbage solution by boiling red cabbage leaves in hot water. The\ncolour of the solution is dark purple/red. Red cabbage juice\nchanges its color to bright pink if we add an acid and to blue\nif we add a base, and so we acquired commercially-available\nvinegar (acetic acid, an acid) and baking soda (sodium bicar-\nbonate, a base).\n3 https://github.com/JeroenDM/elion.\n123\n1072 Autonomous Robots (2023) 47:1057–1086\nFig. 9 Feedback loop between the Generator and V eriﬁer. The input\ntext is converted to structured-like language via the generator and is\nthen passed through the veriﬁer. The veriﬁer returns a list of errors\n(marked with a yellow 1). The feedback is passed back to the generator\nalong with the erroneous task plan, generating a new task plan. Now\nthat previous errors were ﬁxed and the tags could be processed, new\nerrors were found (including a constraint error that the plan uses a ves-\nsel not in the environment). These errors are denoted with a blue 2. This\nfeedback loop is repeated until no more errors are caught, which in this\ncase required 3 iterations (Color ﬁgure online)\nIn this experiment, we generated XDL plans using\nCLAIRify from two language inputs:\n[1] Add 40 g of red cabbage solution into a\nbeaker. Add 10 g of acetic acid into the\nbeaker , then stir the solution for 10\nseconds.\n[2] Add 40 g of red cabbage solution into a\nbeaker. Add 10 g of baking soda into the\nbeaker , then stir the solution for 10\nseconds.\nFigure 10 shows the ﬂow of the experiment. Our system\ngenerated a XDL plan that correctly captured the experiment;\nthe plan was then passed through TAMP to generate a low-\nlevel action plan and was then executed by the robot.\n4.2.3 Kitchen chemistry\nWe then tested whether our robot could execute a plan\ngenerated by our model for a different application of house-\nhold chemistry: food preparation. We generated a plan using\nCLAIRify for the following lemonade beverage, which can\nbe viewed on our website:\nAdd 15 g of lemon juice and sugar mixture to\na cup containing 30 g o f sparkling water.\nStir vigorously for 20 sec.\nFigure 11 shows the ﬂow of an experiment. Since we\ndeal with edible material, we implemented a different stir-\nring motion that does not touch the content of the container.\n4.2.4 Solubility measurement\nWe ﬁnally measured the solubility of household solutes as\nan example of basic educational chemistry experiments for\nstudents (Wolthuis et al., 1960). Measuring solubility has\ndesirable characteristics as a benchmark for automated chem-\nistry experiments: (i) it requires basic chemistry operations,\nsuch as pouring, solid dispensing, and observation of the\nsolution status, (ii) solubility can be measured using ubiqui-\ntous food-safe materials, such as water, salt, sugar, and (iii)\nthe accuracy of the measurement can be evaluated quantita-\ntively by comparing with literature values. We measured the\nsolubility of three solutes: table salt (sodium chloride), sugar\n(sucrose), and alum (aluminum potassium sulfate).\nThe robot estimates the amount of water to make a satu-\nrated solution by repeatedly pouring a small amount of water.\nAfter pouring, the solution is stirred and the turbidity before\nand after stirring was compared. The turbidity decreases by\nstirring if the remaining solutes dissolved into water, whereas\nit stays at a constant value if there are no residues. If the tur-\nbidity decrease after the N -th pouring is smaller than 5%,\nwe assume there were no residues at the beginning of N -th\npouring and that the amount of water required to dissolve all\nsolutes is between the volume of water added at the (N −2)-\nth and (N − 1)-th pouring. We use the average of the two\nfor simplicity of presentation. Figure 13 shows an example\nof turbidity change during the experiment.\nA natural language explanation for the above solubility\nmeasurement protocol is as follows:\n123\nAutonomous Robots (2023) 47:1057–1086 1073\nFig. 10 Robot execution: The robot executes the motion plan generated from the XDL for given natural language input. a CLAIRify converts the\nnatural language input from the user into XDL. b The robot interprets XDL and performs the experiment. Stirring is done by a rotating stir bar\ninside the beaker\nFig. 11 Kitchen chemistry: The robot executes the motion plan generated from the XDL for given natural language input. a CLAIRify converts\nthe natural language input from the user into XDL. b The robot interprets XDL and performs the experiment. Stirring is done by shaking a cup with\nthe robot arm\nAdd 10 g of salt to the beaker.\nRepeat the following steps for five times.\nAdd 10 g of water into the beaker , and\nmeasure the turbidity.\nAfter stirring for 90 seconds , measure the\nturbidity again.\nNote that we extended the XDL to allow turbidity as a mea-\nsurable quantity of <Monitor> since the XDL standard at\nthe time of writing (XDL 2.0.1) only supports temperature\nand pH. We added this skill to our deﬁntion of XDL that we\ninput to the LLM in CLAIRify. The amount of solute and\nstirring time were changed for different solutes. The work-\nﬂow of the solubility experiments is shown in Fig. 12.\nThe measured solubility for three solutes is shown in\nTable 3. The robot framework managed to measure the sol-\nubility with sufﬁcient accuracy that they are comparable to\nsolubility values found in the literature (NAOJ, 2022).\nThe primary reason for the difference from the literature\nvalue is the range of minimum amount of water required\nfor dissolving. In an example of turbidity change shown in\nFig. 13, the robot can only tell the second pouring is insufﬁ-\ncient and the third pouring is sufﬁcient to dissolve all solutes,\nbut it cannot tell the exact required amount. As a result, the\nsolubility measurement inherently includes error caused by\nthe resolution of pouring. We can reduce the error by pouring\na smaller amount of water at once, but pouring less than 10 g\nis difﬁcult because of the delayed feedback of the scale and\nthe scale minimum resolution. We can improve the accuracy\nof solubility measurements by developing a pipette designed\nfor a robot.\n123\n1074 Autonomous Robots (2023) 47:1057–1086\nFig. 12 Workﬂow of solubility experiment. a We translate a natural\nlanguage input to XDL using CLAIRify. b We then execute the plan\non a robot using TAMP . First, a ﬁxed amount of the solute is added to\nthe dish on the weighing scale and stirrer. The robot pours 10 g of water\ninto the dish. The solution is mixed with the magnetic stirrer. After stir-\nring, the turbidity of the solution is measured to check dissolvement.\nIf undissolved, another 10 g of water is added until no solutes remain.\nThe experiment was conducted at room temperature (25\n◦C)\nTable 3 Results of the solubility experiments\nSolute Solute (g) Water (g) Solubility Lit. % error\nSalt 13.9 41.8 33.2 35.8 7.2\nSugar 60.00 26.46 226.8 203.9 11.2\nAlum 3.00 29.87 10.0 11.4 12.3\nAmount of solute in the beaker, amount of water to dissolve all solute,\ncalculated solubility (the amount of solute dissolved per 100 g of water),\nand literature data (lit.) for solubility at 20\n◦C is shown. Literature data\nare taken or calculated from (NAOJ, 2022)\n4.2.5 Recrystallization experiment\nRecrystallization is a purifying technique to obtain crystals\nof a solute by using the difference in solubility at different\ntemperatures. Typically, solutes have higher solubility at high\ntemperatures, meaning hot solvents will dissolve more solute\nthan cool solvents. The excess amount of solute that cannot\nbe dissolved anymore while cooling the solvent precipitates\nand forms crystals. We tested the recrystallization of alum by\nchanging the temperature of the water. Alum was chosen as\nthe target solute since its solubility greatly changes accord-\ning to water temperature. The recrystallization experiment\nsetup extends the solubility test by pre-heating the solvent.\nA natural language explanation for the above recrystalliza-\ntion experiment protocol is as follows:\n123\nAutonomous Robots (2023) 47:1057–1086 1075\nFig. 13 Turbidity change during experiment. Water is poured into the\ndish during pouring (grey) and turbidity is measured during observation\n(blue). The end of the experiment is determined by turbidity comparison.\nIn this example, all solutes are dissolved at the third pouring because\nthe turbidity change after the fourth pouring is below the threshold. The\naverage weight of the second and third pouring is used to calculate the\nsolubility (Color ﬁgure online)\nFig. 14 Recrystallization of\nalum inside the water. After\nheating water by putting a\nbeaker with water on a hotplate,\nthe robot poured alum into a\ndish. The robot then poured hot\nwater, and the solution was\nheated and stirred. The\nformation of a precipitate is\nobserved after the dish is cooled\ndown. The dried crystals in a\nvial are shown\nAdd 50g of water to beaker.\nHeat the beaker filled with water for 1 min\nto 60 C.\nAdd 20 g of alum into an empty beaker , and\nadd 50 g of the heated water into the beaker.\nCool the beaker for 30 min to 20 C.\nFigure 14 shows the result of the experiment.\n4.3 Ablation studies\nWe assess the impact of various components in our prompt\ndesigns and feedback messaging from the veriﬁer. We per-\nformed these tests on a small validation set of 10 chemistry\nexperiments from Chem-RnD (not used in the test set) and\nreport the number of XDL plans successfully generated (i.e.,\nwas not in the iteration loop for x = 10 steps).\n4.3.1 Prompt design\nTo evaluate the prior knowledge of the GPT-3 on XDL, we\nﬁrst tried prompting the generator without a XDL descrip-\ntion, i.e., with the input:\ninitial_prompt = \"\"\"\nConvert to XDL:\n# <Natural langua ge instruction >\"\"\"\nThe LLM was unable to generate XDL for any of the\ninputs from the small validation set that contains 10 chem-\nistry experiments. For most experiments, when asked to\ngenerated XDL, the model output a rephrased version of\nthe natural language input. In the best case, it output some\nnotion of structure in the form of S-expressions or XML tags,\nbut the outputs were very far away from correct XDL and\nwere not related to chemistry. We tried the same experiment\nwith code-davinci-002; the outputs generally had more\nstructure but were still nonsensical. This result suggests the\nLLM does not have the knowledge of the target language and\nincluding the language description in the prompt is essential\nto generate an unfamiliar language.\n4.3.2 Feedback design\nWe experimented with prompts in our iterative prompting\nscheme containing various levels of detail about the errors.\nThe baseline prompt contains a description as well as the\nnatural language instruction. We wanted to investigate how\nmuch detail is needed in the error message for the generator\nto be able to ﬁx the errors in the next iteration. For example,\nis it sufﬁcient to write “There was an error in the generated\nXDL\", and do we need to include a list of errors from the\nveriﬁer (such as “Quantity is not a permissible attribute for\nthe Add tag\"), or do we also need to include the erroneous\nXDL from the previous iteration? We also wanted to keep any\nfeedback messages as general as possible to reduce prompt\nlengths, considering there is a limit for how many tokens we\ncan query OpenAI models with.\nThe XDL generation success rate for different error mes-\nsage designs is shown in Table 4. We ﬁnd that including the\nerroneous XDL from the previous iteration and specifying\nwhy it was wrong resulted in the highest number of success-\nfully generated XDL plans. Including a list of errors was\nbetter than only writing “This XDL was not correct. Please\nﬁx the errors”, which was not informative enough to ﬁx any\nerrors. Including the erroneous XDL from the previous iter-\nation is also important; we found that including only a list\nof the errors without the context of the XDL plan resulted in\nlow success rates.\n4.4 Component analysis for robot execution\n4.4.1 Pouring skill evaluation\nWe evaluated the accuracy and efﬁciency of the pouring skill\nfor liquid and powder. To evaluate the effect of our pro-\nposed pouring method, we implemented a PD control pouring\nmethod where end-effector angular velocity is proportional to\n123\n1076 Autonomous Robots (2023) 47:1057–1086\nTable 4 Number of XDL plans successfully generated for different error message designs in the iterative prompting scheme on a validation set\nfrom Chem-RnD consisting of 10 experiments\nV ariations of iterative prompt design using veriﬁer error messages Plan’s generated success rate (%) ↑\nNaive: XDL from previous iteration and string “This XDL was not correct. Please ﬁx the errors.” 0\nLast error : Error list from veriﬁer from previous iteration 30\nAll errors cumulative : Accumulated error list from all previous iterations 50\nXDL + Last error : XDL and error list from veriﬁer from previous iteration 100\nthe difference between target and feedback weight as a base-\nline. Figure 15 shows the pouring experiment results. The\nresults show that the shaping function contributed to reduc-\ning the overshooting compared to PD control pouring. The\novershoot of the PD control pouring is mainly because of\nthe scale’s delayed feedback ( ∼3 s). The intermittent pour-\ning caused by the shaping function compensated for the delay\nand improved the overall pouring accuracy. On average, the\npouring errors using the shaping approach for water and salt\nwere 2.2 ±1.5g( 8 .1 ±4.8 %) and 2 .1 ±1.4g( 8 .8 ±6.2% ) .\nThe errors for PD control were 24 .5 ±12.0g( 8 1 .4 ±4.5% )\nand 7.7 ±4.3g( 2 4 .1 ±5.2 %). Moreover, as we can see both\nthe error and relative error stay approximately constant with\nrespect to the target amount when using the shaping method,\nin contrast to the PD controller. The average pouring times\nwith the shaping function for 50 g water and salt were 25.1\ns and 36.8 s, respectively. Our results are comparable with\nprevious work (Kennedy et al., 2019; Huang et al., 2021)\nin terms of pouring error and time, without using a learned,\nvision-based policy, or expensive equipment setup.\nIn order to assess the pouring skill more effectively, a\nseries of experiments were carried out involving three human\nsubjects. The experiments aimed to measure both the accu-\nracy and speed of pouring, with each experiment being\nrepeated ﬁve times. The average errors for 50 g of water and\nsalt were 0.8 g and 1.0 g, and the average pouring times were\n16.4 s and 18.6 s respectively. The main factor causing the dis-\nparity in pouring time between human subjects and the robot\nstems from their distinct behaviors during the pre-pouring\nphase. Humans have the ability to quickly rotate a beaker\nduring the pre-pouring stage by relying on visual cues to\ndetermine when the pouring begins. Conversely, the robot’s\nmovements during the pre-pouring stage were deliberately\nslow to prevent overshooting. On average, the pre-pouring\nstage of the robot took approximately 6 s, whereas it was\nnegligible for humans.\n4.4.2 Constrained motion planning in 7/8-DoF robot\nThe constrained motion planning performance of 7-DoF and\n8-DoF robots is evaluated in two scenarios: (1) single step,\n(2) two steps. In scenario (1), robots ﬁnd a constrained path\nwith a ﬁxed orientation from initial to ﬁnal positions that\nare randomly sampled. Scenario (2) extends the ﬁrst with\nan additional intermediate sampled waypoint. For each sce-\nnario, we run 50 trials in Algorithm 3 with random seeding\nof the IK solver.\nFig. 15 Evaluation of pouring error. The pouring errors of our shaping\npouring and PD control baseline pouring are compared using a water and\nb salt. The bar plot shows the error (poured amount—target amount) and\nthe line plot shows the relative error. The error bars show the standard\ndeviation (Color ﬁgure online)\n123\nAutonomous Robots (2023) 47:1057–1086 1077\nTable 5 Success rate comparison of 7 and 8-DoF robot\nScenario 1 (%) Scenario 2 (%)\nIK Plan IK Plan\n7 - D o F 9 98 4 9 97 0\n8-DoF 100 97 100 84\nIn scenario (2), we restart the sequence planning from the\nﬁrst step if a step fails. Constraints are set to the robot end-\neffector pitch and roll ( ∥θ,φ ∥≤ 0.1 rad).\nThe performance of the 7-DoF and 8-DoF robot arms for\nthe two scenarios are shown in Table 5. The results show that\nthe IK and constrained motion planning have higher success\nrates in 8-DoF compared with the 7-DoF robot.\n5 Discussion\nIn this paper, we demonstrate how LLMs are effective tools\nfor translating natural language inputs into domain-speciﬁc\ntarget languages without any ﬁne-tuning. We ﬁnd that by\nprompting an LLM with errors that it makes, it is able\nto correct its own output and generate syntactically valid\nplans. We also tested the ability of CLAIRify to generate\nplans for the same experiment written in different ways (see\n“Appendix D”), which is important for generalizability since\nhumans have different writing styles. We ﬁnd that LLMs are\nrobust to variations in natural language, which is important\nfor lowering the barrier to successful user interaction. The\nXDL plans generated by CLAIRify can then be combined\nwith our TAMP pipeline to effectively perform multistep\nchemistry experiments in the real world. However, the cur-\nrent study is limited to a few types of chemistry experiments\nbecause the number of skills incorporated in the framework is\nlimited. Increasing the repertoire of skills, such as glassware\nperception in 3D and clutter, without ﬁducial markers (Eppel\net al., 2020), can improve the framework scalability. As we\ndevelop more and more skills, we can append their descrip-\ntions to the language model input. We demonstrated this by\nappending a new skill, <Monitor>, to the XDL description,\nand the LLM was able to accurately incorporate it into the\nplan. Another issue is that the inefﬁciency of PDDLStream\ninhibits the framework from being reactive in a dynamic envi-\nronment. Incorporating the learning-based search heuristics\nfor PDDLStream (Khodeir et al., 2023, 2022) may overcome\nthis limitation. Constrained motion planning was shown to\neffectively avoid spillage of the beaker contents during trans-\nfer in our experiments. We have also shown that adding an\nextra 8th DoF to the robot enabled more ﬂexibility and a\nhigher success rate for constrained motion planning. How-\never, the proposed constrained motion planning embedded in\nTAMP cannot run in real-time. Considering the dynamics of\nthe beaker content may help to have higher ﬂexibility in robot\nmanipulation (Muchacho et al., 2022). Although our skill\nhas currently attained 8% error for liquid and powder pour-\ning, higher accuracy is desirable for precise experiments in a\nchemistry lab. We used a scale with integrated functionality\nfor stirring and heating, but its measurement is delayed for 3 s.\nHigher precision pouring can be attained using a scale with a\nshorter response time; also, it can be achieved by specialized\ntools, such as a pipette (Y oshikawa et al., 2023). In addition,\nvisual feedback during pouring may lead to faster and more\naccurate pouring and be helpful in avoiding spillage.\nMoreover, CLAIRify was successful in generating plans\nbeyond the state-of-the-art method for the chemistry domain-\nspeciﬁc structured language XDL. Although the generated\nplans were syntactically correct and satisﬁed the constraints,\nthey contained errors. However, experts placed greater\nemphasis on missing actions than on ambiguous or incor-\nrect actions when selecting the preferred output, indicating\nthat this class of error is more severe for the tasks and\noutputs investigated here. These results demonstrate the gen-\neralizability of our method, which uses zero-shot iterative\nprompting veriﬁcation. We can apply it to different language\nstyles and domains and still obtain coherent plans. While\nour approach, which combines LLMs and TAMP , showed\npromising results in generating feasible and executable plans,\nas evidenced by our evaluation, the capabilities of pure LLMs\nin generating semantically correct plans remain limited. The\nlimitation in task planning abilities has been highlighted in\na recent study (Bubeck et al., 2023) as well. To address this\nshortcoming, an alternative approach could be to incorpo-\nrate human-in-the-loop planning or to utilize multi-modal\nfoundation model that consider the surrounding scene of the\nrobot.\nAnother important consideration to address is the tradeoff\nbetween the human interpretability and expressive power of\nthe target structured language. Our approach to using inter-\nmediate language enables users to ensure the LLM’s natural\nlanguage interpretation is reasonable; however, the expres-\nsive power of XDL imposed limitations on the framework’s\nabilities. The robot framework can conduct more diverse\nactions than XDL can express, but the use of XDL limits the\navailable actions. This problem may be alleviated by gener-\nating the robot program directly, but human interpretability\nmay be decreased as a result.\n6 Conclusions\nIn this paper, we presented a framework for automat-\ning chemistry lab experiments using general-purpose robot\nmanipulators and natural language commands. In order to\nfacilitate the closed-loop execution of long-horizon chem-\nistry experiments, CLAIRify maps natural language com-\n123\n1078 Autonomous Robots (2023) 47:1057–1086\nmands to XDL, a human-interpretable intermediate language\nthat standardizes chemistry experiment descriptions. Sub-\nsequently, XDL instructions are converted into a sequence\nof subgoals for a constrained task and motion plan solver,\nand the robot executes those plans using its diverse set of\nskills. Finally, the robot visually monitors the progress of the\ntasks. We demonstrated that our approach lowers the bar-\nriers to instructing robots by non-experts to execute robot\ntask plans. The robot handles solubility and recrystallization\nexperiments autonomously when provided with natural lan-\nguage inputs.\nAcknowledgements We thank members of the Matter Lab for annotat-\ning task plans. We would also like to thank the Acceleration Consortium\nand Google Inc. for their generous support (NSERC-Google Industrial\nResearch Chair Award). L.B.K. acknowledges generous support from\nthe Carlsberg Foundation.\nAuthor Contributions NY , MS, KD, SA-R, ZJ, LBK worked on\nCLAIRify; NY , KD, AZL, YZ, HX, AK worked on robotic experiments.\nNY , MS, KD wrote the initial draft. LBK, AA-G, FS, AG proof-read\nthe manuscript, and AA-G, FS, AG supervised the project.\nFunding L.B.K. has received funding from the Carlsberg Foundation\nunder grant ID CF21-0669.\nDeclarations\nConﬂict of interest The authors have no relevant ﬁnancial or non-\nﬁnancial interest to disclose.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nAppendix A: Pouring policy\nThe pouring policy is the blending of a shaping function\ns(t) and a model-free PD controller vPD(t), expressed as\nvPD(t) × s(t). The PD controller is deﬁned as:\nvPD(t) = k pe + kd ˙e, (1)\nwhere e(t) = xref − x fb . The shaping function is imple-\nmented via the summation of several unit functions u(t) as\nfollows:\ns(t) = u(t) − 2 u(t − t0)\n+∑ N\nk=1{u(t − (t0 + kT deacti ve))\n+u(t − (t0 + k (Tdeacti ve + Tidle )))\n−2 u(t − (t0 + k (Tdeacti ve + Tidle + Tacti vate )))}\n(2)\nwhere t0 is the moment in which e(t) starts to change, mean-\ning that the material is getting added to the target dish.\nTdeacti ve, Tidle , and Tacti ve are the parameters set by the user\nto describe the periodic motion of the robot end-effector. This\nmotion continues till the material transferred to the target dish\nreaches the desired amount.\nAppendix B: Full prompt\nThe full XDL description and an experiment description from\nthe Chem-EDU dataset are shown as an example of a full\nprompt to the LLM.\ninitial_prompt = \"\"\"\nXDL files will follow XML syntax and consist\nof three mandatory sections: Hardware ,\nwhere virtual vessels that the reaction\nmixture can reside in are declared. Reagents\n, where all reagents that are used in the\nprocedure are declared , and Procedure , where\nthe synthetic actions involved in the\nprocedure are linearly declared.\nXDL File Stub:\n<XDL>\n<Synthesis >\n<Hardware >\n<!-- ... -->\n</Hardware >\n<Reagents >\n<!-- ... -->\n</Reagents >\n<Procedure >\n<!-- ... -->\n</Procedure >\n</Synthesis >\n</XDL>\nHardware:\nEach individual reagent , unless otherwise\nstated should be contained within their own\nvessels.\n(format is (Property , Type , Description))\nid, str, Name of hardware\nReagents:\nThe Reagents section contains Reagent\nelements with the props below.\nAny reagents which were c ombined before the\nexperiment should be combined as one reagent\nbefore the procedure. (i.e. ’ lime juice\nmixed with sugar’ = < Reagent name=’lime\njuice mixed with sugar’)\nReagent:\nReagent used by procedure.\n(format is (Property , Type , Description))\nname , str, Name of reagent\n123\nAutonomous Robots (2023) 47:1057–1086 1079\nProcedure:\nAll steps included in the Full Steps\nSpecification may be given within the\nProcedure block of a XDL file. Additionally ,\nthe Procedure block may be, but does not\nhave to be, divided up into Prep , Reaction ,\nWorkup and Purification blocks , each of\nwhich can contain any of the steps in the\nspecification.\nHere is a list of tags that can be used in\nthis language:\nLiquid Handling: Add, Separate , Transfer ,\nStirring: StartStir , Stir , StopStir ,\nTemperature Control: HeatChill ,\nHeatChillToTemp , StartHeatChill ,\nStopHeatChill\nInert Gas: EvacuateAndRefill , Purge ,\nStartPurge , StopPurge\nFiltration: Filter , FilterThrough , WashSolid\nSpecial: Wait , Repeat ,\nOther: CleanVessel , Crystallize , Dissolve ,\nDry, Evaporate , Irradiate , Precipitate ,\nResetHandling , RunColumn\nSteps:\nLiquid Handling:\nAdd liquid or solid reagent. Reagent\nidentity (ie liquid or solid) i s determined\nby the solid property of a reagent in the\nReagent section.\nThe quantity of the reagent can be specified\nusing either volume (liquid units) or\namount (all accepted units e.g. ’g’, ’mL’, ’\neq’, ’mmol’).\nformat(Property Type Description)\nvessel vessel Vessel to add reagent to.\nreagent reagent Reagent to add.\nSeparate:\nPerform separation.\nformat(Property Type Description)\nProperty Type Description\npurpose str ’ wash’ or ’ extract’. ’ wash’\nmeans that product phase will not be the\nadded solvent phase , ’ extract’ means product\nphase will be the added solvent phase. If\nno solvent is added just use ’extract’.\nproduct_phase str ’top ’ or ’bottom’. Phase\nthat product will be in.\nfrom_vessel vessel Contents of from_vessel\nare transferred to separation_vessel and\nseparation is performed.\nseparation_vessel vessel Vessel in which\nseparation of phases will be carried out.\nto_vessel vessel Vessel to send product\nphase to.\nTransfer:\nTransfer liquid from one vessel to another.\nThe quantity to transfer can be specified\nusing either volume (liquid units) or amount\n(all accepted units e.g. ’g’, ’mL’, ’eq’, ’\nmmol’).\nformat(Property Type Description)\nfrom_vessel vessel Vessel to transfer\nliquid from.\nto_vessel vessel Vessel to transfer\nliquid to.\nStirring:\nStartStir:\nStart stirring vessel.\nformat(Property Type Description)\nvessel vessel Vessel to start stirring.\nStir:\nStir vessel for given time.\nformat(Property Type Description)\nvessel vessel Vessel to stir.\ntime float Time to stir vessel for.\nStopStir:\nStop stirring given vessel.\nformat(Property Type Description)\nvessel vessel Vessel to stop stirring.\nTemperature Control:\nHeatChill:\nHeat or chill vessel to given temp for given\ntime.\nformat(Property Type Description)\nvessel vessel Vessel to heat or chill.\ntemp float Temperature to heat or chill\nvessel to.\ntime float Time to heat or chill vessel\nfor.\nConvert to XDL:\nAdd 30 g of red cabbage soup into a beaker.\nAdd 10 g of acetic acid into the beaker ,\nthen stir the soluti on for 10 seconds.\n\"\"\"\nAppendix C: Chem-EDU dataset\nWe show some examples of experiments in the Chem-EDU\ndataset.\nAdd 30 g of red cabbage soup into a beaker.\nAdd 10 g of acetic acid into the beaker ,\nthen stir the soluti on for 10 seconds.\nExperiment 0\nAcacia Honey Syrup\nAdd 200g of hot water to a beaker. Add 430g\nacacia honey to the beaker. Stir for 10\nminutes while heating.\nExperiment 13\nAdd 250g water to beaker. Heat until it\nreaches 100 degrees C. Add 40 g of pasta.\nHeat for 6 minutes at 100 degrees C.\nExperiment 30\nAppendix D: Variations in natural language\nSince humans have different writing styles, we wanted to\ndetermine if our model is able to generate consistent XDL\nplans from natural language descriptions written in differ-\nent ways. We took two experiments from Chem-EDU and\nprompted ChatGPT to rewrite the experiment. We ran all\nexperiments through CLAIRify and examined the outputs.\n4\nWe ﬁnd that CLAIRify can generate the same XDL plans\nwith different natural language descriptions.\n4 https://beta.openai.com.\n123\n1080 Autonomous Robots (2023) 47:1057–1086\nFig. 16 XDL generated for different writing styles for Experiment\n0 from Chem-EDU. The ﬁrst writing style (writing style 1) is taken\ndirectly from Chem-EDU. For the second writing style (writing style\n2), we asked ChatGPT to rewrite it. For both prompts, we generate a\nXDL plan. The output is semantically the same for both (note that the\nextra\n<StartSir> and <StopStir> actions in (d) are redun-\ndant, as the presence of the <Stir> action automatically does a\n<StartSir> and <StopStir> action)\n123\nAutonomous Robots (2023) 47:1057–1086 1081\nFig. 17 XDL generated for different writing styles for Experiment 30 from Chem-EDU. The ﬁrst writing style (writing style 1) is taken directly\nfrom Chem-EDU. For the second writing style (writing style 2), we asked ChatGPT to rewrite it. For both prompts, we generate a XDL plan. The\noutput is identical for both\n123\n1082 Autonomous Robots (2023) 47:1057–1086\nReferences\nAbolhasani, M., & Kumacheva, E. (2023). The rise of self-driving labs\nin chemical and materials sciences. Nature Synthesis 1–10.\nAhn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O., David, B.,\nFinn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al.\n(2022). Do As I Can, Not As I Say: Grounding language in robotic\naffordances. arXiv preprint. https://doi.org/10.48550/arXiv.2204.\n01691 .\nBaier, J. A., Bacchus, F., & McIlraith, S. A. (2009). A heuristic search\napproach to planning with temporally extended preferences. Arti-\nﬁcial Intelligence, 173 (5–6), 593–618.\nBeeson, P . & Ames, B. (2015) TRAC-IK: An open-source library\nfor improved solving of generic inverse kinematics. In 2015\nIEEE-RAS 15th international conference on humanoid robots\n(humanoids).\nBerenson, D., Srinivasa, S., & Kuffner, J. (2011). Task space regions:\nA framework for pose-constrained manipulation planning. The\nInternational Journal of Robotics Research, 30 (12), 1435–1460.\nhttps://doi.org/10.1177/0278364910396389\nBoiko, D. A., MacKnight, R., & Gomes, G. (2023). Emergent\nautonomous scientiﬁc research capabilities of large language mod-\nels. arXiv preprint. https://doi.org/10.48550/arXiv.2304.05332\nBran, A. M., Cox, S., White, A. D., & Schwaller, P . (2023). ChemCrow:\nAugmenting large-language models with chemistry tools. arXiv\npreprint. https://doi.org/10.48550/arXiv.2304.05376\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP ., Neelakantan, A., Shyam, P ., Sastry, G., Askell, A., et al. (2020).\nLanguage models are few-shot learners. Advances in Neural Infor-\nmation Processing Systems, 33 , 1877–1901.\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E.,\nKamar, E., Lee, P ., Lee, Y . T., Li, Y ., Lundberg, S., et al. (2023).\nSparks of artiﬁcial general intelligence: Early experiments with\ngpt-4. arXiv preprint. https://doi.org/10.48550/arXiv.2303.12712\nBurger, B., Maffettone, P . M., Gusev, V . V ., Aitchison, C. M., Bai, Y .,\nWang, X., Li, X., Alston, B. M., Li, B., Clowes, R., et al. (2020).\nA mobile robotic chemist. Nature, 583(7815), 237–241.\nChen, M., Tworek, J., Jun, H., Y uan, Q., Pinto, H. P . O., Kaplan, J.,\nEdwards, H., Burda, Y ., Joseph, N., Brockman, G., et al.(2021).\nEvaluating large language models trained on code. arXiv preprint.\nhttps://doi.org/10.48550/arXiv.2107.03374\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts,\nA., Barham, P ., Chung, H. W., Sutton, C., Gehrmann, S., et al.\n(2022). PaLM: Scaling language modeling with pathways. arXiv\npreprint. https://doi.org/10.48550/arXiv.2204.02311\nColeman, D., Sucan, I., Chitta, S., & Correll, N. (2014). Reducing the\nbarrier to entry of complex robotic software: A MoveIt! case study.\narXiv preprint. https://doi.org/10.48550/arXiv.1404.3785\nDantam, N. T., Kingston, Z. K., Chaudhuri, S., & Kavraki, L. E. (2018).\nAn incremental constraint-based framework for task and motion\nplanning. The International Journal of Robotics Research, 37 (10),\n1134–1151.\nDevlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT:\nPre-training of deep bidirectional transformers for language under-\nstanding. In North American chapter of the association for\ncomputational linguistics .\nDing, Y ., Zhang, X., Paxton, C., & Zhang, S. (2023). Task and motion\nplanning with large language models for object rearrangement.\narXiv preprint. https://doi.org/10.48550/arXiv.2212.09672\nDriess, D., Ha, J. S., & Toussaint, M. (2020). Deep visual reasoning:\nLearning to predict action sequences for task and motion plan-\nning from an initial scene image. arXiv preprint. https://doi.org/\n10.48550/arXiv.2006.05398\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B.,\nWahid, A., Tompson, J., Vuong, Q., Y u, T., et al. (2023). Palm-e:\nAn embodied multimodal language model. arXiv preprint. https://\ndoi.org/10.48550/arXiv.2303.03378\nEdwards, C., Lai, T., Ros, K., Honke, G., & Ji, H. (2022). Translation\nbetween molecules and natural language. arXiv preprint. https://\ndoi.org/10.48550/arXiv.2204.11817\nEppel, S., Xu, H., Bismuth, M., & Aspuru-Guzik, A. (2020). Computer\nvision for recognition of materials and vessels in chemistry lab set-\ntings and the vector-labpics data set. ACS Central Science, 6 (10),\n1743–1752.\nEpps, R. W., Bowen, M. S., V olk, A. A., Abdel-Latif, K., Han, S., Reyes,\nK. G., Amassian, A., & Abolhasani, M. (2020). Artiﬁcial chemist:\nAn autonomous quantum dot synthesis bot. Advanced Materials,\n32(30), 2001626.\nEysenbach, B., Salakhutdinov, R. R., & Levine, S. (2019). Search on\nthe replay buffer: Bridging planning and reinforcement learning.\nAdvances in Neural Information Processing Systems , 32.\nFakhruldeen, H., Pizzuto, G., Glowacki, J., & Cooper, A. I. (2022).\nARChemist: Autonomous robotic chemistry system architecture.\narXiv preprint. https://doi.org/10.48550/arXiv.2204.13571\nFortman, J. J., & Stubbs, K. M. (1992). Demonstrations with red cab-\nbage indicator. Journal of Chemical Education, 69 (1), 66.\nGarrett, C. R., Chitnis, R., Holladay, R., Kim, B., Silver, T., Kael-\nbling, L. P ., & Lozano-Pérez, T. (2021). Integrated task and motion\nplanning. Annual Review of Control, Robotics, and Autonomous\nSystems, 4 , 265–293.\nGarrett, C. R., Lozano-Pérez, T., & Kaelbling, L. P . (2020). PDDL-\nStream: Integrating symbolic planners and blackbox samplers via\noptimistic adaptive planning. In Proceedings of the 30th interna-\ntional conference on automated planning and scheduling (ICAPS) ,\n(pp. 440–448). AAAI Press.\nGhallab, M., Howe, A., Knoblock, C, McDermott, D. Ram, A., V eloso,\nM., Weld, D., Wilkins, D. (1998). PDDL - The Planning Domain\nDeﬁnition Language. Technical Report CVC TR98003/DCS\nTR1165. New Haven, CT: Yale Center for Computational Vision\nand Control.\nGrinberg, M. (2018). Flask web development: Developing web appli-\ncations with python. “ O’Reilly Media, Inc.”.\nGu, Y ., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Nau-\nmann, T., Gao, J., & Poon, H. (2021). Domain-speciﬁc language\nmodel pretraining for biomedical natural language processing.\nACM Transactions on Computing for Healthcare, 3 (1), 1–23.\nhttps://doi.org/10.1145/3458754\nHäse, F., Roch, L. M., & Aspuru-Guzik, A. (2019). Next-generation\nexperimentation with self-driving laboratories. Trends in Chem-\nistry, 1(3), 282–291.\nHelmert, M. (2006). The fast downward planning system. Journal of\nArtiﬁcial Intelligence Research, 26 , 191–246.\nHiggins, K., Ziatdinov, M., Kalinin, S. V ., & Ahmadi, M. (2021).\nHigh-throughput study of antisolvents on the stability of multicom-\nponent metal halide perovskites through robotics-based synthesis\nand machine learning approaches. Journal of the American Chem-\nical Society, 143 (47), 19945–19955.\nHuang, D. A., Nair, S., Xu, D., Zhu, Y ., Garg, A., Fei-Fei, L., Savarese,\nS., & Niebles, J. C. (2019). Neural task graphs: Generalizing to\nunseen tasks from a single video demonstration. In IEEE Computer\nVision and Pattern Recognition .\nHuang, W., Abbeel, P ., Pathak, D., & Mordatch, I. (2022). Language\nmodels as zero-shot planners: Extracting actionable knowledge for\nembodied agents. In International Conference on Machine Learn-\ning, (pp. 9118–9147). PMLR.\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P .,\nZeng, A., Tompson, J., Mordatch, I., Chebotar, Y ., et al. (2022).\nInner monologue: Embodied reasoning through planning with\nlanguage models. arXiv preprint. https://doi.org/10.48550/arXiv.\n2207.05608\n123\nAutonomous Robots (2023) 47:1057–1086 1083\nHuang, Y ., Wilches, J., & Sun, Y . (2021). Robot gaining accurate\npouring skills through self-supervised learning and generalization.\nRobotics and Autonomous Systems, 136 , 103692. https://doi.org/\n10.1016/j.robot.2020.103692\nInagaki, T., Kato, A., Takahashi, K., Ozaki, H., & Kanda, G. N. (2023).\nLLMs can generate robotic scripts from goal-oriented instructions\nin biological laboratory automation. arXiv preprint. https://doi.\norg/10.48550/arXiv.2304.10267\nIrwin, R., Dimitriadis, S., He, J., & Bjerrum, E. J. (2022). Chemformer:\nA pre-trained transformer for computational chemistry. Machine\nLearning: Science and Technology, 3 (1), 015022.\nJablonka, K. M., Schwaller, P ., Ortega-Guerrero, A., & Smit, B. (2023).\nIs gpt-3 all you need for low-data discovery in chemistry? Chem-\nRxiv. https://doi.org/10.26434/chemrxiv-2023-fw8n4\nKaelbling, L. P ., & Lozano-Pérez, T. (2011). Hierarchical task and\nmotion planning in the now. In IEEE International Conference\non Robotics and Automation (pp. 1470–1477). IEEE.\nKaraman, S., & Frazzoli, E. (2011). Sampling-based algorithms for\noptimal motion planning. The International Journal of Robotics\nResearch, 30(7), 846–894.\nKavraki, L. E., Svestka, P ., Latombe, J. C., & Overmars, M. H. (1996).\nProbabilistic roadmaps for path planning in high-dimensional con-\nﬁguration spaces. IEEE Transactions on Robotics and Automation,\n12(4), 566–580.\nKennedy, M., Schmeckpeper, K., Thakur, D., Jiang, C., Kumar, V .,\n& Daniilidis, K. (2019). Autonomous precision pouring from\nunknown containers. IEEE Robotics and Automation Letters, 4 (3),\n2317–2324. https://doi.org/10.1109/LRA.2019.2902075\nKhodeir, M., Agro, B., & Shkurti, F. (2023). Learning to search in task\nand motion planning with streams. IEEE Robotics and Automation\nLetters, 8 (4), 1983–1990.\nKhodeir, M., Sonwane, A., & Shkurti, F. (2022). Policy-guided lazy\nsearch with feedback for task and motion planning. arXiv preprint.\nhttps://doi.org/10.48550/arXiv.2210.14055\nKim, B., Shimanuki, L., Kaelbling, L. P ., & Lozano-Pérez, T. (2022).\nRepresentation, learning, and planning algorithms for geometric\ntask and motion planning. The International Journal of Robotics\nResearch, 41(2), 210–231.\nKingston, Z., Moll, M., & Kavraki, L. E. (2018). Sampling-based\nmethods for motion planning with constraints. Annual Review of\nControl, Robotics, and Autonomous Systems, 1 , 159–185.\nKingston, Z., Moll, M., & Kavraki, L. E. (2019). Exploring implicit\nspaces for constrained sampling-based planning. The International\nJournal of Robotics Research, 38 (10–11), 1151–1178. https://doi.\norg/10.1177/0278364919868530\nKitchener, B. G., Wainwright, J., & Parsons, A. J. (2017). A review\nof the principles of turbidity measurement. Progress in Physical\nGeography, 41(5), 620–642.\nKnobbe, D., Zwirnmann, H., Eckhoff, M., & Haddadin, S. (2022). Core\nprocesses in intelligent robotic lab assistants: Flexible liquid han-\ndling. In 2022 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS) , pp. 2335–2342.\nLe, H., Wang, Y ., Gotmare, A. D., Savarese, S., & Hoi, S. C. H. (2022).\nCodeRL: Mastering code generation through pretrained models\nand deep reinforcement learning. Advances in Neural Information\nProcessing Systems, 35 , 21314–21328.\nLi, J., Li, J., Liu, R., Tu, Y ., Li, Y ., Cheng, J., He, T., & Zhu, X.\n(2020). Autonomous discovery of optically active chiral inorganic\nperovskite nanocrystals through an intelligent cloud lab. Nature\nCommunications, 11 (1), 2046.\nLi, Y ., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond,\nR., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al.\n(2022). Competition-level code generation with alphacode. Sci-\nence, 378 (6624), 1092–1097.\nLiang, J., Huang, W., Xia, F., Xu, P ., Hausman, K., Ichter, B., Flo-\nrence, P ., & Zeng, A. (2022). Code as policies: Language model\nprograms for embodied control. arXiv preprint. https://doi.org/10.\n48550/arXiv.2209.07753\nLim, J. X. Y ., Leow, D., Pham, Q. C., & Tan, C. H. (2020). Development\nof a robotic system for automatic organic chemistry synthesis.\nEEE Transactions on Automation Science and Engineering, 18 (4),\n2185–2190.\nLin, K., Agia, C., Migimatsu, T., Pavone, M., & Bohg, J. (2023).\nText2Motion: From natural language instructions to feasible plans.\narXiv preprint. https://doi.org/10.48550/arXiv.2303.12153\nLiu, R., Wei, J., Gu, S.S., Wu, T.Y ., V osoughi, S., Cui, C., Zhou, D., &\nDai, A.M. (2023). Mind’s eye: Grounded language model reason-\ning through simulation. In The eleventh international conference\non learning representations .\nMacarron, R., Banks, M. N., Bojanic, D., Burns, D. J., Cirovic, D.\nA., Garyantes, T., Green, D. V ., Hertzberg, R. P ., Janzen, W. P .,\nPaslay, J. W., et al. (2011). Impact of high-throughput screening\nin biomedical research. Nature Reviews Drug discovery, 10 (3),\n188–195.\nMacLeod, B. P ., Parlane, F. G., Morrissey, T. D., Häse, F., Roch, L. M.,\nDettelbach, K. E., Moreira, R., Y unker, L. P ., Rooney, M. B., Deeth,\nJ. R., et al. (2020). Self-driving laboratory for accelerated discov-\nery of thin-ﬁlm materials. Science Advances, 6 (20), eaaz8867.\nMehr, H., Craven, M., Leonov, A., Keenan, G., & Cronin, L. (2020a).\nBenchmarking results and the XDL XML schema. https://zenodo.\norg/record/3955107\nMehr, S. H. M., Craven, M., Leonov, A. I., Keenan, G., & Cronin,\nL. (2020). A universal system for digitization and automatic exe-\ncution of the chemical synthesis literature. Science, 370 (6512),\n101–108.\nMénard, A. D., & Trant, J. F. (2020). A review and critique of academic\nlab safety research. Nature Chemistry, 12 (1), 17–25.\nMialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R.,\nRaileanu, R., Rozière, B., Schick, T., Dwivedi-Y u, J., Celikyil-\nmaz, A., et al. (2023). Augmented language models: a survey.\narXiv preprint. https://doi.org/10.48550/arXiv.2302.07842\nMirchandani, S., Karamcheti, S., & Sadigh, D. (2021). ELLA: Explo-\nration through learned language abstraction. Advances in Neural\nInformation Processing Systems, 34 , 29529–29540.\nMishra, S., Khashabi, D., Baral, C., & Hajishirzi, H. (2021). Cross-task\ngeneralization via natural language crowdsourcing instructions.\narXiv preprint. https://doi.org/10.48550/arXiv.2104.08773\nMuchacho, R. I. C., Laha, R., Figueredo, L. F., & Haddadin, S. (2022).\nA solution to slosh-free robot trajectory optimization. In 2022\nIEEE/RSJ international conference on intelligent robots and sys-\ntems (IROS), (pp. 223–230). IEEE.\nNational Astronomical Observatory of Japan. (2022). Handbook of sci-\nentiﬁc tables . World Scientiﬁc.\nNi, A., Iyer, S., Radev, D., Stoyanov, V ., Yih, W. T., Wang, S. I., &\nLin, X. V . (2023). Lever: Learning to verify language-to-code gen-\neration with execution. arXiv preprint. https://doi.org/10.48550/\narXiv.2302.08468\nOlson, E. (2011). Apriltag: A robust and ﬂexible visual ﬁducial sys-\ntem. In 2011 IEEE the international conference on robotics and\nautomation.\nPeng, B., Galley, M., He, P ., Cheng, H., Xie, Y ., Hu, Y ., Huang, Q.,\nLiden, L., Y u, Z., Chen, W., & Gao, J. (2023). Check your facts and\ntry again: Improving large language models with external knowl-\nedge and automated feedback. arXiv preprint. https://doi.org/10.\n48550/arXiv.2302.12813\nPereira, D., & Williams, J. (2007). Origin and evolution of high through-\nput screening. British Journal of Pharmacology, 152 (1), 53–61.\nPerry, T. (2021). LightTag: Text annotation platform. In Proceedings of\nthe EMNLP conference , (pp. 20–27).\nPizzuto, G., Wang, H., Fakhruldeen, H., Peng, B., Luck, K. S., & Cooper,\nA.I . (2022). Accelerating laboratory automation through robot\n123\n1084 Autonomous Robots (2023) 47:1057–1086\nskill learning for sample scraping. arXiv preprint. https://doi.org/\n10.48550/arXiv.2209.14875\nRamos, M. C., Michtavy, S. S., Porosoff, M. D., & White, A. D. (2023).\nBayesian optimization of catalysts with in-context learning. arXiv\npreprint. https://doi.org/10.48550/arXiv.2304.05341\nSchick, T., Dwivedi-Y u, J., Dessì, R., Raileanu, R., Lomeli, M., Zettle-\nmoyer, L., Cancedda, N., & Scialom, T. (2023). Toolformer:\nLanguage models can teach themselves to use tools. arXiv preprint.\nhttps://doi.org/10.48550/arXiv.2302.04761\nSeifrid, M., Pollice, R., Aguilar-Granda, A., Morgan Chan, Z., Hotta,\nK., Ser, C. T., V estfrid, J., Wu, T. C., & Aspuru-Guzik, A. (2022).\nAutonomous chemical experiments: Challenges and perspectives\non establishing a self-driving lab. Accounts of Chemical Research,\n55(17), 2454–2466.\nShah, D., Xu, P ., Lu, Y ., Xiao, T., Toshev, A., Levine, S., & Ichter, B.\n(2021). V alue function spaces: Skill-centric state abstractions for\nlong-horizon reasoning. arXiv preprint. https://doi.org/10.48550/\narXiv.2111.03189\nSharma, P ., Torralba, A., & Andreas, J. (2021). Skill induction and\nplanning with latent language. arXiv preprint. https://doi.org/10.\n48550/arXiv.2110.01517\nShiri, P ., Lai, V ., Zepel, T., Grifﬁn, D., Reifman, J., Clark, S., Grunert,\nS., Y unker, L. P ., Steiner, S., Situ, H., et al. (2021). Automated sol-\nubility screening platform using computer vision. iscience, 24 (3),\n102176.\nSingh, I., Blukis, V ., Mousavian, A., Goyal, A., Xu, D., Tremblay, J.,\nFox, D., Thomason, J., & Garg, A. (2022). Progprompt: Generat-\ning situated robot task plans using large language models. arXiv\npreprint. https://doi.org/10.48550/arXiv.2209.11302\nSteiner, S., Wolf, J., Glatzel, S., Andreou, A., Granda, J. M., Keenan,\nG., Hinkley, T., Aragon-Camarasa, G., Kitson, P . J., Angelone,\nD., et al. (2019). Organic synthesis in a modular robotic system\ndriven by a chemical programming language. Science, 363(6423),\neaav2211.\nT a y l o r ,R . ,K a r d a s ,M . ,C u c u r u l l ,G . ,S c i a l o m ,T . ,H a r t s h o r n ,A . ,S a r a v i a ,\nE., Poulton, A., Kerkez, V ., & Stojnic, R. (2022). Galactica: A\nlarge language model for science. arXiv preprint. https://doi.org/\n10.48550/arXiv.2211.09085\nTellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S.,\n& Roy, N. (2011). Understanding natural language commands for\nrobotic navigation and mobile manipulation. In Proceedings of the\nAAAI conference on artiﬁcial intelligence ,v o l .25, pp. 1507–1514.\nToussaint, M. (2015). Logic-geometric programming: An optimization-\nbased approach to combined task and motion planning. In IJCAI,\npp. 1930–1936.\nToussaint, M. A., Allen, K. R., Smith, K. A., & Tenenbaum, J. B. (2018).\nDifferentiable physics and stable modes for tool-use and manipu-\nlation planning.\nWang, S., Liu, Y ., Xu, Y ., Zhu, C., & Zeng, M. (2021). Want to reduce\nlabeling cost? GPT-3 can help. In Proceedings of the EMNLP Con-\nference, pp. 4195–4205.\nWang, Y ., Wang, W., Joty, S., & Hoi, S. C. (2021). CodeT5: Identiﬁer-\naware uniﬁed pre-trained encoder-decoder models for code under-\nstanding and generation. arXiv preprint. https://doi.org/10.48550/\narXiv.2109.00859\nWang, Y . R., Zhao, Y ., Xu, H., Eppel, S., Aspuru-Guzik, A., Shkurti, F.,\n& Garg, A. (2023). MVTrans: Multi-view perception of transpar-\nent objects. arXiv preprint. https://doi.org/10.48550/arXiv.2302.\n11683\nWolthuis, E., Pruiksma, A. B., & Heerema, R. P . (1960). Determina-\ntion of solubility: A laboratory experiment. Journal of Chemical\nEducation, 37 (3), 137.\nWu, C. J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng,\nK., Chang, G., Aga, F., Huang, J., Bai, C., et al. (2022). Sustainable\nAI: Environmental implications, challenges and opportunities.\nProceedings of Machine Learning and Systems, 4 , 795–813.\nXu, D., Martín-Martín, R., Huang, D. A., Zhu, Y ., Savarese, S., &\nFei-Fei, L. F. (2019). Regression planning networks. Advances\nin Neural Information Processing Systems , 32.\nXu, D., Nair, S., Zhu, Y ., Gao, J., Garg, A., Fei-Fei, L., & Savarese, S.\n(2018). Neural task programming: Learning to generalize across\nhierarchical tasks. In 2018 IEEE international conference on\nrobotics and automation (ICRA) (pp. 3795–3802). IEEE.\nXu, H., Wang, Y . R., Eppel, S., Aspuru-Guzik, A., Shkurti, F., & Garg,\nA. (2021). Seeing glass: Joint point-cloud and depth completion\nfor transparent objects. In Annual conference on robot learning .\nY oshikawa, N., Darvish, K., Garg, A., & Aspuru-Guzik, A. (2023). Dig-\nital pipette: Open hardware for liquid transfer in self-driving lab-\noratories. Digital Discovery. https://doi.org/10.1039/d3dd00115f\nY o s h i k a w a ,N . ,L i ,A .Z . ,D a r v i s h ,K . ,Z h a o ,Y . ,X u ,H . ,K u r a m s h i n ,\nA., Aspuru-Guzik, A., Garg, A., & Shkurti, F. (2023). Chemistry\nlab automation via constrained task and motion planning. arXiv\npreprint. https://doi.org/10.48550/arXiv.2212.09672\nY oshikawa, T. (1985). Manipulability of robotic mechanisms. The Inter-\nnational Journal of Robotics Research, 4 (2), 3–9.\nhttps://doi.org/\n10.1177/027836498500400201\nZhang, K., Sharma, M., Liang, J., & Kroemer, O. (2020). A modu-\nlar robotic arm control stack for research: Franka-Interface and\nFrankaPy. arXiv preprint. https://doi.org/10.48550/arXiv.2011.\n02398\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nNaruki Yoshikawa is a Ph.D. stu-\ndent at the Department of Com-\nputer Science of the University\nof Toronto. He is working on\nautomation of chemistry experi-\nments under the supervision of\nAlán Aspuru-Guzik. He received\nhis master’s degree in 2020 at The\nUniversity of Tokyo and his bach-\nelor’s degree in 2018 at the same\ninstitution.\nMarta Skreta is a Computer Sci-\nence PhD student under the super-\nvision of Alán Aspuru-Guzik at\nthe University of Toronto work-\ning at the intersection of machine\nlearning, chemistry, and self-driving\nlabs. Previously, she completed a\nMSc in Computer Science at the\nUniversity of Toronto and a HBSc\nin Chemical Biology at McMaster\nUniversity.\n123\nAutonomous Robots (2023) 47:1057–1086 1085\nKourosh Darvish is a postdoc-\ntoral researcher at the Computer\nScience and Robotics Institute of\nthe University of Toronto (UofT)\nand a member of the V ector Insti-\ntute. Before joining UofT in 2022,\nhe was a postdoctoral researcher\nat the Italian Institute of Tech-\nnology (IIT). In 2019, he com-\npleted his PhD in Bioengineer-\ning and Robotics from the Univer-\nsity of Genoa, Italy. Previously,\nhe received his B.Sc. and M.Sc.\ndegrees in Aerospace Engineer-\ning from K.N. Toosi University of\nTechnology and Sharif University of Technology (Tehran, Iran) in\n2012 and 2014, respectively. His research focuses on robotics for sci-\nentiﬁc discoveries, shared autonomy, and humanoid robotics.\nSebastian Arellano-Rubach is in\ngrade 11 at the University of\nToronto Schools. In 2023, he spent\nthe summer working for the Com-\nputer Science Department at UofT.\nHis work focuses mostly on web-\nsite design, front-end development,\nback-end development, develop-\nment tools, performance optimiza-\ntion, responsiveness, and SEO.\nZhi Ji is a dedicated Engineer-\ning Science student at the Uni-\nversity of Toronto, specializing in\nMachine Intelligence (AI) and minor-\ni n gi nE n g i n e e r i n gB u s i n e s s .S h e\nis very passionate about AIMLDL.\nLasse Bjørn Kristensen is a\npostdoctoral researcher working in\nAlán Aspuru-Guzik’s lab on quan-\ntum computing, machine learning,\nand algorithms at the intersection\nof the two. He obtained his PhD\nfrom Aarhus University in 2020,\nwhere his research focus was on\nthe use of superconducting cir-\ncuits and their quantum dynamics\nfor quantum computing. Before\nthis, he received his MSc in 2017\nand his BSc in 2015 at the same\ninstitution, both within the ﬁeld of\nPhysics.\nAndrew Zou Li is a senior under-\ngraduate student studying robotics\nin the Division of Engineering\nScience at the University of\nToronto. His research experience\nincludes state estimation for\nautonomous driving, task and\nmotion planning, and machine\nlearning compiler optimization.\nYuchi Zhao is a research assistant\nin the Computer Science Depart-\nment of the University of Toronto.\nHis research mainly focuses on\nrobot manipulation, perception and\ntactile sensing for self-driving labs.\nHe received his B.A.Sc degree\nin Mechatronics Engineering from\nthe University of Waterloo in 2023.\nHaoping Xu is a senior Ph.D.\nstudent from RA 2D : Robotics-\nassisted Accelerated Discovery lab\nat the Acceleration Consortium.\nHe is supervised by Prof Alán\nAsupuru-Guizk, Prof Animesh Grag\nand Prof Florian Shkurti. RA\n2 D\nlab is focused on building univer-\nsal robot for lab automation and\naccelerating science discoveries.\nHaoping’s research focus is com-\nputer vision and robotics control\nrelated to lab automation. In par-\nticular, he is interested in percep-\ntion and vision guided robot plan-\nning and control for transparent objects. He has publications in top-\nics of 2D perception, depth completion, multiview 3D perception and\nrobot manipulations for transparent objects in lab settings.\nArtur Kuramshin recieved the\nHBSc degree in Computer Sci-\nence with a specialization in Com-\nputer Vision from the University\nof Toronto (UofT) in 2023. Dur-\ning his time at UofT, he was part\nof the Robot Vision and Learning\n(RVL) lab.\n123\n1086 Autonomous Robots (2023) 47:1057–1086\nAlán Aspuru-Guzik research lies\nat the interface of computer sci-\nence with chemistry and physics.\nHe integrates robotics, machine\nlearning and quantum chemistry\nto develop “self-driving laborato-\nries”, accelerating rates of scien-\ntiﬁc discovery. He develops quan-\ntum computer algorithms and has\npioneered quantum algorithms for\nthe simulation of matter. He is a\nProfessor of Chemistry and Com-\nputer Science at the University of\nToronto, faculty member at the\nV ector Institute for Artiﬁcial Intel-\nligence and Director of the Acceleration Consortium, a University of\nToronto-based strategic initiative that aims to gather researchers from\nindustry, government and academia around topics related to the labs\nof the future. He was previously a full professor at Harvard Univer-\nsity where he started his career in 2006. He is currently the Canada\n150 Research Chair in Theoretical Chemistry, CIFAR AI Chair at the\nV ector Institute and co-founder of Zapata Computing and Kebotix, two\nearly-stage ventures in quantum computing and self-driving laborato-\nries.\nFlorian Shkurti is an Assistant Pro-\nfessor in the Department of Com-\nputer Science at the University of\nToronto, where he also serves as\na Faculty Member at the UofT\nRobotics Institute and a Faculty\nAfﬁliate at the V ector Institute. He\nearned his Ph.D. in Computer Sci-\nence and Robotics from McGill\nUniversity in 2018. He leads the\nRobot Vision and Learning (RVL)\nlab, focusing on robotics research\nencompassing machine learning,\nperception, planning, and control.\nFlorian’s research has been applied\nto environmental monitoring by autonomous robots, visual navigation\nfor autonomous vehicles, and mobile manipulation.\nAnimesh Garg received the PhD\ndegree from University of Califor-\nnia, Berkeley and was a postdoc\nat Stanford AI lab. He is a CIFAR\nchair Assistant Professor of com-\nputer science with the University\nof Toronto, a faculty member with\nthe V ector Institute, and a senior\nresearch scientist with Nvidia. He\nworks on the Algorithmic Foun-\ndations for Generalizable Auton-\nomy, to enable AI-based robots to\nwork alongside humans.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8206284046173096
    },
    {
      "name": "Robotics",
      "score": 0.6790751814842224
    },
    {
      "name": "Executable",
      "score": 0.6449483036994934
    },
    {
      "name": "Robot",
      "score": 0.6335448622703552
    },
    {
      "name": "Task (project management)",
      "score": 0.5974984765052795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5576030015945435
    },
    {
      "name": "Natural language",
      "score": 0.5399678945541382
    },
    {
      "name": "Workspace",
      "score": 0.49962902069091797
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4726152718067169
    },
    {
      "name": "Software engineering",
      "score": 0.36423876881599426
    },
    {
      "name": "Programming language",
      "score": 0.29396432638168335
    },
    {
      "name": "Systems engineering",
      "score": 0.17211276292800903
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    }
  ]
}