{
  "title": "Micro Expression Recognition Using Convolution Patch in Vision Transformer",
  "url": "https://openalex.org/W4386703038",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5019851641",
      "name": "Sakshi Indolia",
      "affiliations": [
        "Banasthali University"
      ]
    },
    {
      "id": "https://openalex.org/A5009187703",
      "name": "Swati Nigam",
      "affiliations": [
        "Banasthali University"
      ]
    },
    {
      "id": "https://openalex.org/A5071333197",
      "name": "Rajiv Singh",
      "affiliations": [
        "Banasthali University"
      ]
    },
    {
      "id": "https://openalex.org/A5026129552",
      "name": "Vivek Kumar Singh",
      "affiliations": [
        "Banaras Hindu University"
      ]
    },
    {
      "id": "https://openalex.org/A5024104784",
      "name": "Manoj Kumar Singh",
      "affiliations": [
        "Banaras Hindu University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3139546544",
    "https://openalex.org/W4322731888",
    "https://openalex.org/W2963230974",
    "https://openalex.org/W2938266529",
    "https://openalex.org/W4281645557",
    "https://openalex.org/W2900180747",
    "https://openalex.org/W4283587322",
    "https://openalex.org/W1217048403",
    "https://openalex.org/W2526853616",
    "https://openalex.org/W4211030889",
    "https://openalex.org/W2803393170",
    "https://openalex.org/W4304127880",
    "https://openalex.org/W2779656294",
    "https://openalex.org/W3094954011",
    "https://openalex.org/W4213264823",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W4306759110",
    "https://openalex.org/W2964606879",
    "https://openalex.org/W6616011953",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4376876473",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W4292862449",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W1753905863",
    "https://openalex.org/W4361792478",
    "https://openalex.org/W2898716161",
    "https://openalex.org/W1854318472",
    "https://openalex.org/W2074027098",
    "https://openalex.org/W2801973746",
    "https://openalex.org/W6805559843",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2962162344",
    "https://openalex.org/W4206941309",
    "https://openalex.org/W4310553896",
    "https://openalex.org/W2536459558",
    "https://openalex.org/W2959639774",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W4283372964",
    "https://openalex.org/W4313146694",
    "https://openalex.org/W4224951303",
    "https://openalex.org/W2044106642",
    "https://openalex.org/W2426188534",
    "https://openalex.org/W2538953432",
    "https://openalex.org/W2056591378",
    "https://openalex.org/W2041616772",
    "https://openalex.org/W3017006601",
    "https://openalex.org/W3092753574",
    "https://openalex.org/W2006426145",
    "https://openalex.org/W3204340584",
    "https://openalex.org/W6645733745",
    "https://openalex.org/W3092956019",
    "https://openalex.org/W4205268334",
    "https://openalex.org/W4221021599",
    "https://openalex.org/W6736917304",
    "https://openalex.org/W3040018512",
    "https://openalex.org/W2970981431",
    "https://openalex.org/W3098002894",
    "https://openalex.org/W3122081138",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W1983014650",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4225934279",
    "https://openalex.org/W1809872410",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W565148957",
    "https://openalex.org/W2608034484",
    "https://openalex.org/W2963703618"
  ],
  "abstract": "Humans possess an intrinsic ability to hide their true emotions. Micro-expressions are subtle changes in facial muscles that are involuntary by nature and easy to hide. To address these issues, several machine and deep learning models have been proposed in the past few years. Convolution neural network (CNN) is a deep learning method that has widely been adopted in vision-related tasks due to its remarkable performance. However, CNN suffers from overfitting due to a large number of trainable parameters. Additionally, CNN cannot capture global information with respect to an input image. Furthermore, the identification of important regions for the classification of micro-expressions is a challenging task. Self-attention mechanism addresses these issues by focusing on key areas. Furthermore, specific transformers, known as vision transformers are widely explored in vision-related applications. However, existing vision transformers divide an input image into a fixed number of patches due to which local correlation of image pixels is lost. Further, a vision transformer relies on self-attention mechanism which effectively captures global dependencies but does not exploit the local spatial relationships in an image. In this work, we propose a vision transformer based on convolution patches to overcome this problem. The proposed algorithm generates <inline-formula> <tex-math notation=\"LaTeX\">$c $ </tex-math></inline-formula> number of feature maps from input images using <inline-formula> <tex-math notation=\"LaTeX\">$c $ </tex-math></inline-formula> filters through convolution operation. These feature maps are then applied to a transformer model as fixed-size image patches to perform classification. Thus, the proposed architecture leverages advantages of both convolutional layers and transformer, and captures both spatial information and global dependencies respectively, leading to improved performance. The performance of the proposed model is evaluated on three benchmark datasets: CASME-I, CASME-II, and SAMM and compared with state-of-the-art machine and deep learning models, which generated classification accuracy of 95.97&#x0025;, 98.59&#x0025;, and 100&#x0025;, respectively.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nMicro Expression Recognition using\nConvolution Patch in Vision Transformer\nSAKSHI INDOLIA1, SWATI NIGAM2, RAJIV SINGH3, VIVEK KUMAR SINGH4, and MANOJ\nKUMAR SINGH.5\n1Department of Computer Science & Centre for Artificial Intelligence, Banasthali Vidyapith, Rajasthan, 304022, India (e-mail: sakshiindolia95@gmail.com)\n2Department of Computer Science & Centre for Artificial Intelligence, Banasthali Vidyapith, Rajasthan, 304022, India (e-mail: swatinigam.au@gmail.com)\n3Department of Computer Science & Centre for Artificial Intelligence, Banasthali Vidyapith, Rajasthan, 304022, India (e-mail: jkrajivsingh@gmail.com)\n4Department of Computer Science, Banaras Hindu University, Varanasi 221005, India (e-mail: vivekks12@gmail.com)\n5Department of Computer Science, Banaras Hindu University, Varanasi 221005, India (e-mail: manoj.dstcims@bhu.ac.in)\nCorresponding authors: Rajiv Singh and Vivek Kumar Singh (e-mail: jkrajivsingh@gmail.com, vivekks12@gmail.com).\nAcknowledgement. This work is supported by the extramural research grant no: 3(9)/2021-EG-II from Ministry of Electronics &\nInformation Technology (MeITY), Government of India, and by HPE Aruba Centre for Research in Information Systems at BHU (No.\nM-22-69 of BHU).\nABSTRACT Humans possess an intrinsic ability to hide their true emotions. Micro-expressions are subtle\nchanges in facial muscles that are involuntary by nature and easy to hide. To address these issues, several\nmachine and deep learning models have been proposed in the past few years. Convolution neural network\n(CNN) is a deep learning method that has widely been adopted in vision-related tasks due to its remarkable\nperformance. However, CNN suffers from overfitting due to a large number of trainable parameters.\nAdditionally, CNN cannot capture global information with respect to an input image. Furthermore, the\nidentification of important regions for the classification of micro-expressions is a challenging task. Self-\nattention mechanism addresses these issues by focusing on key areas. Furthermore, specific transformers,\nknown as vision transformers are widely explored in vision-related applications. However, existing vision\ntransformers divide an input image into a fixed number of patches due to which local correlation of image\npixels is lost. Further, a vision transformer relies on self-attention mechanism which effectively captures\nglobal dependencies but does not exploit the local spatial relationships in an image. In this work, we propose\na vision transformer based on convolution patches to overcome this problem. The proposed algorithm\ngenerates c number of feature maps from input images using c filters through convolution operation. These\nfeature maps are then applied to a transformer model as fixed-size image patches to perform classification.\nThus, the proposed architecture leverage advantages of both convolutional layers and transformer, and\ncaptures both spatial information and global dependencies respectively, leading to improved performance.\nThe performance of the proposed model is evaluated on three benchmark datasets: CASME-I, CASME-\nII, and SAMM and compared with state-of-the-art machine and deep learning models, which generated\nclassification accuracy of 95.97%, 98.59%, and 100%, respectively.\nINDEX TERMSFacial expression recognition, deep learning, micro-expression recognition, self-attention,\nvision transformer.\nI. INTRODUCTION\nMicro-expressions (ME) are involuntary subtle facial mus-\ncle movements which represent true emotions of a person\n[1]. There are a variety of possible applications for micro-\nexpression recognition (MER), including forensics, security,\nsurveillance, education, entertainment, and healthcare sys-\ntems [2]. However, identification and classification of ME is\na challenging task due to a variety of reasons. Typically, ME\nappear for a very short duration of time, i.e., 0:04 to 0:50\nseconds [3]. Furthermore, ME show very subtle change in\nfacial muscles, due to which identification and spotting of\nME become difficult.\nTraditional machine learning methods such as local binary\npatterns [4] and histogram of oriented gradients (HOG) [5],\n[6], [7], depend on handcrafted features for classification.\nThis dependency has been avoided by the use of deep\nlearning models. Convolutional neural network (CNN) is\na deep learning method which has recently demonstrated\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nremarkable performance in several vision based applications\nand outperformed both handcrafted features and shallow\nclassifiers [8]. A deep fusion-based CNN model proposed by\n[9] has been implemented for facial expression recognition,\nwhich shows the impact of transfer learning and feature\nfusion on the performance of the model. Similarly, CNN\nand transfer learning are incorporated by [10] to determine\nthe level of engagement of deaf and hard-of-hearing students\nby analyzing their facial expressions and categorizing these\nexpressions as highly engaged, nominally engaged or not\nengaged.\nCNN requires large training dataset; however, most of\nthe publicly available MER datassets are small in size, [11]\nused a data augmentation technique for CNN to increase\nthe size of the facial expression datasets. Similarly, a CNN-\nbased MER model proposed by [12], exploits optical flow\ninformation related to subtle muscle movements through\napex and reference frame. Then, this information is passed\nto a CNN model for classification of an emotion. In the past\nfew years, performance of CNN has been elevated by using\nit in stream or branch based networks.\nHowever, implementation of CNN models in MER is lim-\nited due to variety of reasons: (i) CNN requires large number\nof trainable parameters (ii) CNN based models often suffer\nfrom overfitting (iii) convolution operation only captures\nlocal receptive field of a pixel and it is incapable of handling\nglobal receptive field, (iv) CNN does not effectively handle\nsparse spatio-temporal information, and, (v) ME consist of\nsubtle movements of facial muscles which are difficult to\nhandle.\nAs mentioned above, CNN is incapable of handling spatio-\ntemporal information. Hence, 3D CNN has been explored\nby [13], [14], and [15] to address this issue for MER. A\nsiamese 3D CNN (MERSiamC3D) proposed by [13] is based\non two-stage learning. The first stage applies an optical flow\nestimation technique to explain the spatio-temporal infor-\nmation, followed by a siamese CNN model. The second\nstage adjusts the network parameters obtained from the first\nstage. Similarly, [15] also exploits a 3D CNN in combi-\nnation with SqueezeNet. Another work proposed by [16],\nincorporates Squeeze-and-Excitation Networks with a 3D\nDenseNet to exploit spatio-temporal features. The ability\nof attention mechanism to concentrate on certain locations\nmakes it effective. Attention mechanism is either employed\nin conjunction with CNN or it replaces certain components\nof CNN. Accurate detection of ME plays a vital role in\nimproving performance of the MER model. Attention mech-\nanism can be used to effectively detect the presence of micro-\nexpression in a video frame. A dual attention network known\nas LGAttNet, was proposed by [17] for automatic detection\nof micro-expression. Similarly, micro-expression analysis\nnetwork (MEAN) proposed by [18], is used for simultaneous\nspotting and recognition of ME.\nIn this work, effective and accurate classification is per-\nformed by exploiting vision transformer which depends on\nself-attention mechanism. In the past few years, vision trans-\nformers have attained remarkable results on vision-related\nclassification tasks with substantially fewer computational\nresources. A simple vision transformer typically divides an\nimage into fixed size patches. These non-overlapping patches\nform a linear embedding which is provided to the vision\ntransformer. This architecture captures global dependencies\nbut cannot capture spatial information. On the other hand, the\nproposed vision transformer architecture takes convolution\nfeature maps as input patches; these feature maps contain\nspatial information. These feature maps are then provided\nas the input patches for the subsequent transformer layers.\nThus, the proposed architecture leverage advantages of both\nconvolutional layers and transformer and captures both spa-\ntial information and global dependencies for improved per-\nformance. Due to its remarkable performance, the proposed\nmodel can have a wide variety of real-life applications across\ndifferent domains. For instance, the proposed model can\nbe used for early detection and diagnosis of mental health\nissues such as anxiety and depression. It can also be used\nin security and law enforcement, where, security personnel\ncan improve their ability to recognize possible threats by\nidentifying ME associated with suspicious behavior. Further,\nMER can also play a very vital role for applications based on\nhuman-computer interaction and cross-cultural studies.\nThe major contributions of this paper are as follows:\n1) We propose a deep learning framework for MER\nthrough a vision transformer with low computational\ncost.\n2) Conventional vision transformers divide input image\ninto fixed-sized patches; due to which it becomes dif-\nficult for the model to exploit the local correlation of\npixels. The proposed model addresses this issue by\nmaintaining the correlation of the target pixel with\nits neighbors through a local receptive field by using\nconvolution patches.\n3) We exploit global as well as local correlation in an im-\nage through a vision transformer and convolution patch\nrespectively, which improves the overall classification\nperformance of the model.\n4) Extensive experiments have been performed on three\nbenchmark datasets and comparison with existing\nstate-of-the-art models validates the effectiveness of\nthe proposed model.\nThe remaining sections of the paper are arranged as follows.\nSection II discusses the related work. Section III presents the\nproposed vision transformer for MER. Section IV provides\na description of the datasets, experimental setup, and hyper-\nparameters for training the model, results, and comparison\nof the proposed model with current state-of-the-art models.\nConclusions are provided in Section V .\nII. RELATED WORKS\nA. MICRO-EXPRESSION RECOGNITION\nBased on input data, MER models can be broadly cate-\ngorized into single-image-based and sequence-image-based\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1. Flattening of image patches in conventional vision transformer.\nsystems. Datasets such as AffectNet [19] and FER2013\n[20] are single-image-based datasets, whereas, CASME-\nI [21], CASME II [22], SAMM [23], and SMIC [24]\nare sequence-image-based datasets. Sequence-image-based\ndatasets are widely adopted for spotting and recognition\nof micro-expressions because they provide better insight\ninto data. However, primitive sequence-image-based datasets\nsuch as USF-HD [25] and Polikovsky’s [26] are not adopted\nat present because such datasets contain image sequences\nof posed expressions, and hence they cannot be used\nfor practical implementations. Whereas, state-of-the-art ME\ndatasets contain spontaneous image sequences captured in a\nlaboratory-controlled environment. Due to the availability of\nthese datasets, research in the MER domain has significantly\naccelerated.\nPrimitive approaches for MER rely on hand-crafted and\nlow-level features such as local binary pattern (LBP) [4],\ngradient features and optical flow. Local binary pattern from\nthree orthogonal planes (LBP-TOP) is a commonly used\nfeature for MER which considers horizontal and vertical\ndirections. However, LBP-TOP cannot capture muscle move-\nments in oblique direction which is essential for MER. To\naddress this problem, [27] proposed a new feature called\nLBP-FIP which could easily capture dynamic textures from\nimages calculated through five intersecting planes. Simi-\nlarly, [28] proposed an invisible emotion magnification al-\ngorithm (IEMA) which effectively magnifies the strength of\nfacial muscle movement for better classification of micro-\nexpression.\nHowever, it is difficult to accurately interpret and repre-\nsent ME through low-level features. Thus, a combination\nof several low-level features forming high-level features can\nbe exploited for a better representation of ME. High-level\nfeature representations can be obtained by deep learning\nmodels such as CNN. At an early stage, researchers exploited\nonly spatial features [29] [30] through CNN, however, studies\ndemonstrate that MER involves facial movement which can\nbe captured through long image sequences. Thus, state-of-\nthe-art MER models exploit both spatial as well as temporal\ninformation. A Deep 3DCNN-ANN model proposed by [31]\nperforms micro-expression recognition by learning spatio-\ntemporal features from the image sequences by combining\ndeep 3DCNN and ANN through a feature called visual\nassociations. However, it has been observed that CNN can-\nnot capture the relationship of an entity with its parent as\nan image. To address this issue, [32] proposed CapsuleNet\nbased on agreement routing mechanism for MNIST dataset.\nInspired by its success, [33] experimented CapsuleNet for\nMER model on SMIC, CASME-II and SAMM datasets.\nIt has been observed that training a model on a particular\ndataset may not necessarily perform well on other dataset.\nThus to experiment with cross-dataset MER, [34] proposed a\ndual-inception network which exploits horizontal and vertical\ncomponents extracted throgh optical flow.\nB. TRANSFORMERS\nTransformer model was originally designed for text based\napplications [35], where it has exhibited remarkable results.\nInspired by its success, it has also been experimented in\nvision tasks [36]. Vision transformers (ViT) take image as an\ninput and represent it as a series of fixed size image patches as\nshown in Figure 1. The obtained image patches are flattened\nand subjected to lower dimensional linear embedding. Due to\nflattening of patches, the correlation between adjacent patch\nmight be lost. Therefore, positional embedding is added to\nkeep the correlation information intact. Furthermore, vision\ntransformers rely on self-attention mechanism that provides\nglobal receptive field, unlike CNN, which yields local recep-\ntive field.\nConsidering the limitations of CNN models, vision trans-\nformer has been widely adopted for MER models. A late-\nfusion based vision transformer proposed by [37], exploits\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmotion features through optical flow. Late-fusion and optical\nflow mechanisms allow the model to deal with small ME\ndatasets. Similarly, a muscle motion-guided network pro-\nposed by [38], exploits the subtle muscle motion features for\naccurate classification of ME through a two branch model.\nThe first branch comprises of a continuous attention block,\nwhich focuses on modeling muscle movement, whereas, the\nsecond branch comprises of a position calibration module\nwhich consists of a vision transformer.\nStudies show that MER is difficult due to the fact that\nthey are highly dynamic in nature and appear on localized\nfacial regions. To solve this problem, [39] proposed a sparse\ntransformer which exploits multi-head attention for sparse\nrepresentation of emotions appearing in localized facial re-\ngions, whereas, temporal attentional fusion is employed to\ndeal with dynamic nature of ME. Furthermore, studies [40]\nshow that combination of local and global spatio-temporal\npattern can improve classification accuracy of MER. To\naddress the spatial patterns, a spatial encoder is employed,\nwhereas, a temporal aggregator models the temporal patterns.\nAnother work proposed by [41], exploits two swin vision\ntransformers F_transformer and S_transformer placed in two\nparallel streams. F_transformer exploits short term motion\ndynamics through optical flow sequences, whereas, long-\nterm motion dynamics are utilized through S_transformer.\nLater, feature fusion is performed on features obtained from\nthese two streams for classification of emotions.\nHowever, the existing vision transformer models for MER\ndivides the input image into n patches, due to which the\nlocal correlation of pixels with its neighboring pixels is lost.\nTo address this issue, in this work, we exploit feature maps\ngenerated by convolution operation. Furthermore, convolu-\ntion operation helps to capture local receptive field, and self-\nattention mechanism in vision transformer allows the model\nto capture global receptive field.\nIII. PROPOSED METHODOLOGY\nExisting vision transformer models [36], [42] create fixed\nsize patches from input image, which are flattened and pro-\nvided to transformer for classification. However, this tech-\nnique limits the performance of vision based algorithms, be-\ncause, image pixels exhibit correlation with their neighboring\npixels. Dividing images into fixed size patch deteriorates the\ncorrelation with neighboring pixels. Thus, a major limitation\nof this technique is that it cannot handle correlation among\npixels in an image. To address this issue, the proposed\nalgorithm generates c feature maps by applying c filters on\nan input image. These feature maps are considered as fixed\nsize image patches and passed to transformer model for\nclassification.\nA. PRE-PROCESSING AND CONVOLUTION PATCH\nFigure 2 presents detailed network architecture of the pro-\nposed model. First, the input sequence frames are provided to\nthe network through a pre-processing stage. The input frames\nare subjected to pre-processing operations such as horizontal\nflip, normalization and resize to 256 × 256 pixels. After pre-\nprocessing, the images of 3× 256 × 256 pixel dimension are\ngenerated. Next, to exploit local correlation, two subsequent\nconvolution operations are applied. First convolution opera-\ntion takes images of 16 × 3 × 256 × 256 pixel dimension,\nwhere, 16 is the batch size and applies 64 filters with stride\nequivalent to patch size i.e., 16. Then, gaussian error linear\nunit (GELU) activation function proposed by [43] is applied,\nwhere GELU is computed by Equation 1.\nGaussianErrorLinearUnit (z) =\n0.5 × z × (1 +Tanh(\nr\n2\nπ × (z + 0.44715 × z3)))\n(1)\nThereafter, another convolution operation is applied which\ntakes 64 feature maps and applies 3 filters with stride 1. Next,\nGELU activation function is applied to the obtained feature\nmaps of dimension 16 × 3 × 256 × 256, which are reshaped\nto obtain 16 × 256 × 256 × 3 feature maps.\nB. VISION TRANSFORMER\nConventional vision transformer models divide an image of\ndimension h × w pixels into n × m number of fixed size\npatches (as shown in Figure 1), where each patch is of h/n ×\nw/m pixel dimension. Thereafter, these patches are flattened\nand passed through linear projection.\nHowever, in the proposed work, we exploit local corre-\nlation of images through convolution operation, shown in\nFigure 2. Here, feature maps of 16 × 256 × 256 × 3\ndimension are flattened to form 16 × 256 × 768 feature\nvector. To maintain the order of sequence, we add positional\nembedding and perform reshape operation to generate fea-\nture vector of shape 257 × 16 × 256. It is further passed\nto six subsequent transformer encoders. The final feature\nvector is of shape 257 × 16 × 256, passed to multi-layer\nperceptron (MLP) head for classification of emotions. Figure\n3 illustrates a single transformer encoder, which incorporates\nMulti-head attention, which is further based on self-attention\nmechanism.\nAttention mechanism was introduced in encoder-decoder\nblock of a neural sequence transduction model by [44]. It\nenable content-based summary of data from a variable length\nsentence. Attention mechanism is widely adopted because it\nhas the ability to learn to focus on key areas. Self-attention\nmechanism also called intra-attention [35], allows the model\nto identify the inputs we should pay more attention to. It is\nused by [45] for facial expression recognition to deal with\nintra-class variation and inter-class similarity. It computes a\nweighted average of sequence elements where the weights\nare dynamically determined using the element keys and\nan input query. Attention mechanism rely on three feature\nvectors, key, query and value. In Figure 3, Query feature\nvector (represented by Q) attempts to identify the sequence-\nspecific information the model is searching for. Key vector\n(represented by K), describes what the input element is\noffering. The V aluevector (represented by V ) is the one\nthat we intend to average over. In this work, we exploit\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Detailed architecture of proposed model for MER using vision transformer.\nFIGURE 3. Transformer Encoder.\nscaled dot product attention which takes Query ∈ RSL×dk ,\nKey ∈ RSL×dk and V alue ∈ RSL×dv , where SL is\nsequence length, dk and dv are hidden dimensionalities. The\nscaled dot product attention is computed by Equation 2.\nAttention(Query, Key, V alue) =\nSoftmax ( QueryKey SL\n√dk\n)V alue (2)\nwhere, 1√dk\nis the scaling factor, used to monitor the\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4. Sample images of (a) CASME-I (b) CASME-II (c) SAMM datasets.\nFIGURE 5. Unbalanced nature of emotion samples in datasets .\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 1. Description of datasets (a) CASME-I, (b) CASME-II, and (c) SAMM\nCharacteristics CASME-I CASME-II SAMM\nSamples 96 255 159\nSubjects 35 26 29\nEthnicity 1 1 13\nFPS 60 200 200\nResolutions (in pixels) 1200 × 720 640 × 480 2040 × 1088\nClass Labels 8 7 8\nvariance of attention values. In Equation 2, Query and Key\nare two vectors with σ2 variance, when a product operation\nis applied on Query and Key, it generates a scalar with dk\ntimes higher variance. Thus, there is a need to scale down\nthe variance back to σ2, otherwise, softmax will make one\nrandom element saturate to 1 and other elements saturate to\n0. Therefore, we use dk for scaling, to maintain the optimal\nvariance of attention values.\nA network can pay attention to a particular sequence with\nscaled dot product attention. However, it does not allow\nsequence elements to attend to different features. This can be\nachieved through multi-head attention. Here, key, query, and\nvalue matrices are converted into h sub-keys, sub-queries,\nand sub-values respectively. Each of these sub-components\nis then independently passed through a hi scaled dot product\nattention with weight matrices WQ\ni and WK\ni . Thereafter,\nthese h heads are concatenated and it generates final weight\nmatrix WO.\nMulti −head(Q, K, V) =Concat(h1, h2, ..., hh)WO (3)\nwhere, hi=Attention(QWQ\ni , KWK\ni , V WK\ni )\nWQ\n1...h ∈ RD×dk (4)\nWK\n1...h ∈ RD×dk (5)\nWV\n1...h ∈ RD×dv (6)\nWO ∈ Rh·dk×dout (7)\nwhere, D is the input dimensionality.\nIV. EXPERIMENT AND RESULTS\nA. DATASETS\nPerformance of the proposed model has been tested on three\nbenchmark datasets CASME-I [21], CASME-II [22], and\nSAMM [23]. Sample images of these datasets are shown Fig-\nure 4. Table 1 describes detail of these datasets on the basis\nof number of video samples, subjects, ethnicity, frames per\nsecond (FPS), resolutions (in pixels) and number of emotion\nlabels. Figure 5 illustrates unbalanced nature of emotion sam-\nples in datasets. Furthermore, class-wise sample distribution\nis illustrated in Table 2. Video sequences containing the onset\nframe, progressing toward the apex emotion, and then ending\nwith the offset frame are used to train the model.\nTABLE 2. Number of frames against each emotion for (a) CASME-I, (b)\nCASME-II, and (c) SAMM datasets, used for training the proposed model.\nEmotions CASME-I CASME-II SAMM\nAnger - - 4165\nContempt 52 - 896\nDisgust 802 373 666\nFear 63 121 534\nHappiness 234 266 1937\nOthers - 298 2071\nRepression 777 251 -\nSadness 79 108 391\nSurprise 393 241 1062\nTense 1495 - -\nTotal 3895 1858 11723\nTABLE 3. Comparison of number of heads in transformer encoder.\nNumber of Heads Classification Accuracy\n1 96.31%\n2 95.62%\n4 96.31%\n8 97.08%\n16 96.74%\nB. EXPERIMENTAL SETUP AND TRAINING\nHYPERPARAMETERS\nThe proposed model is trained using Nvidia A100 provided\nby Google Colab Pro+. Adam optimizer is used for opti-\nmization of model weights, learning rate is set to 0.0003 and\nbatch size is 16. We initially tuned the number of heads for\ntraining the proposed vision transformer model; to ensure a\nfair comparison, same number of heads is used for ablation\nexperiments. We have investigated the model based on 1, 2,\n4, 8, and 16 heads. As shown in Table 3, it can be observed\nthat the selection of 8 heads outperformed other variants.\nThus, 8 heads are selected in multihead attention module\nof the transformer encoder for all the experiments. Other\nparameters used in the proposed transformer encoder are\nlisted in Table 4. To avoid overfitting of our model, we have\nexploited dropout regularization technique and layer normal-\nization. In our proposed model, we have chosen layer nor-\nmalization technique over batch normalization. The reason is\nthat, in batch normalization, each feature in the mini-batch\nis independently normalised, whereas, layer normalisation\nnormalises each input in the batch across all features. Further,\nwe compare our proposed model on the basis of number of\ntrainable parameters and GFLOPS as shown in Table 5. It can\nbe observed that the proposed model outperforms existing\nstate-of-the-art transformer and CNN based models.\nC. RESULTS AND DISCUSSION\n1) Performance Analysis\nThe proposed model is trained and tested on three bench-\nmarks datasets i.e., CASME-I, CASME-II and SAMM. The\nmodel is evaluated in terms of classification accuracy, pre-\ncision, recall and F1-score. The training and validation ac-\ncuracy of CASME-I, CASME-II and SAMM datasets are\nshown in Table 6. The validation accuracy of SAMM dataset\nis 100%, which raises concerns about potential overfitting.\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 6. Training loss curve for proposed convolution patch based vision transformer.\nFIGURE 7. Validation accuracy curve for proposed convolution patch based vision transformer.\nFIGURE 8. Validation loss curve for proposed convolution patch based vision transformer.\nFIGURE 9. Confusion Matrices obtained using proposed vision transformer for CASME-I, CASME-II and SAMM datasets respectively.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. Parameter values for Transformer encoder.\nParameters Value\nEmbedding Dimension 256\nHidden Dimension 512\nHeads 8\nTransformer Encoder Layers 6\nPatch Size 16\nInput Channels 3\nNumber of Patches 257\nDropout 0.2\nTABLE 5. Comparison of different models on the basis of number of\nparameters and GFLOPS.\nMethod Image Size Number of Parame-\nters\nGFLOPS\nSwin-B [46] 384 × 384 88M 47.0G\nCvT-21 [47] 384 × 384 32M 24.9G\nDeiT-B [48] 384 × 384 86M 55.4G\nEff-B6 [49] 528× 528 43M 19.0G\nProposed\nModel\n640 × 480 13M 17.2G\nFIGURE 10. Training and Validation curve on SAMM dataset.\nIn order to rule out this possibility, we have used layer\nnormalization (LayerNorm) as a regularization technique, as\nshown in Figure 3, and also applied dropout technique with\nvalue 0.2 to mitigate overfitting in our proposed model. For\nfurther analysis, we have plotted the training and validation\ncurves as mentioned in Figure 10 (where, x-axis represents\nepochs and y-axis represents accuracy), to closely monitor\nthe model’s performance. Overfitting can be measured by\nobserving a widening gap between the obtained training\nand validation curve. However, in our case, the training and\nvalidation curves exhibit a consistent alignment without a\nnoticeable gap between them. Hence, it can be inferred that\nTABLE 6. Training and Validation accuracy for CASME-I, CASME-II, and\nSAMM datasets.\nDatasets Training Accuracy\n(%)\nValidation Accuracy\n(%)\nCASME-I 96.95 96.00\nCASME-II 98.87 99.00\nSAMM 99.10 100\nthe model does not suffer from overfitting.\nThe obtained evaluation metrics are shown in Table 7, 8,\nand 9 for CASME-I, CASME-II and SAMM datasets, respec-\ntively. Because of the severe class imbalances in CASME\nI and SAMM datasets, the F1-Score is more reliable while\ncomparing performance of the proposed model. Figures 6\n- 9 depict training loss curve, validation accuracy curve,\nvalidation loss curves and confusion matrix, respectively,\nwhere, number of iterations during training or validation\nare represented by x-axis, whereas, y-axis represent loss in\nFigures 6, 8 and accuracy in Figure 7. Validation accuracy\nand validation loss curves of CASME-II datasets in Figures\n7 (ii) and 8 (ii) depict higher fluctuations as compared to\nother datasets. This might be due to lower number of training\nsamples in CASME-II dataset. Figures 7 (iii) and 8 (iii) show\nless fluctuations for SAMM dataset as compared to CASME-\nII. However, despite of large number of samples, fluctuations\nin SAMM are higher than CASME-I dataset which is due to\nunbalanced training samples in SAMM dataset.\nTable 7 shows evaluation metrics for CASME-I dataset.\nBased on evaluation of F1-Score, it can be inferred that\nthe proposed model correctly classifies contempt and fear\nemotions, which contain least number of training samples\ni.e., 52 and 63 respectively as compared to other emotions\n(shown in Table 2). Thus, it can be concluded that the pro-\nposed model addresses the issue of smaller training samples\nrequired by state-of-the-art deep learning models. However,\nsadness emotion also contain fewer number of training sam-\nples i.e., 79, but the model could correctly classify only 75%\nsamples. Figure 9 (a) shows confusion matrix obtained for\nthe proposed model on CASME-I dataset. It can be observed\nthat 11 samples of sadness emotion are wrongly classified as\ndisgust. This is due to low inter-class variation among these\ntwo classes. Emotions such as disgust (802), happiness (234),\nrepression (777), surprise (393) and tense (1495) generate\nF1-scores: 93%, 94%, 99%, 91%, and 98% respectively. The\nlower recognition rate might be because of overfitting of the\nmodel for emotions with higher number of training samples.\nThe overall classification accuracy of the proposed model on\nCASME-I dataset is 95.97%.\nTable 8 shows evaluation metrics for CASME-II dataset. It\ncan be observed, that the proposed model generates 98.59%\nclassification accuracy for CASME-II dataset. The model\ncorrectly classifies fear (121), and sadness (108) emotion.\nF1-score for repression (251), disgust (373), happiness (266),\nother (298), and surprise (241) are 99%, 98%, 98%, 98%,\nand 98% respectively. It can be observed that as the number\nof samples increases, the performance of the model drops for\nspecific emotions. The reason behind this might be overfitting\nof the model.\nTable 9 shows evaluation metrics for SAMM dataset. It\ncan be observed, that the proposed model generate highest\npossible accuracy i.e., 100% for SAMM dataset. It is because\nof the availability of large number of training samples. More-\nover, Table 2 shows that SAMM dataset is highly unbalanced,\nstill the proposed model outperforms existing state-of-the-art\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmodels. Thus, it can be inferred that our model can easily\nhandle unbalanced nature of the training datasets.\n2) Comparative Analysis\nWe contrast our proposed vision transformer model based\non convolution patches with a number of state-of-the-art\nmethods. We have compared the proposed transformer model\nwith various machine and deep learning algorithms such as\nprincipal component analysis (PCA), CNN, CNN-LSTM,\ngraph-CNN, and transformer models. From Tables 10-12,\nit can be observed that the proposed model outperforms\nseveral advance deep learning models and generates 95.97%,\n98.59%, and 100% classification accuracy for CASME-I,\nCASME-II, and SAMM datasets respectively.\nA machine learning method proposed by [50], address\ntwo important characteristics of ME: low facial movement\nintensity and short duration of ME. The first issue is dealt\nby exploiting robust PCA and the sparse nature of ME in\ntemporal domain is addressed by using local spatio-temporal\ndirectional features. This method generates 63.41% classifi-\ncation accuracy on CASME-II dataset. However, deep learn-\ning models such as CNN and LSTM generate remarkable\nperformance as compared to machine learning models. Thus,\nto show a fair comparison we have compared our proposed\nmodel with state-of-the-art CNN models also. A 3D flow\nCNN proposed by [14], exploits a 3D convolution operation\nto extract spatio-temporal feature information along with\noptical flow. In this method, overfitting is avoided by us-\ning dropout mechanism and batch normalization technique.\nThis method generates 59.11% classification accuracy on\nCASME-II dataset. To identify and analyse spatio-temporal\ndeformations of ME, a recurrent CNN was proposed by [51]\nwhich generates 80.30% and 78.60% classification accuracy\non CASME-II and SAMM datasets, respectively. Another\ncategory of recurrent neural network, known as long short\nterm memory in conjunction with CNN was proposed by\n[29], generates 47.30% classification accuracy on CASME-\nII.\nA vision transformer based model, muscle motion-guided\nnetwork (MMNet), proposed by [38], exploits a two-branch\nnetwork. The main branch of MMNet extracts motion-\npattern related features through a continuous attention block,\nwhereas a transformer encoder is exploited as a sub-branch\nof the model to generate positional embedding. Thereafter,\nthe positionl embedding are added to motion-pattern features\nto generate 88.35% and 80.14% classification accuracy for\nCASME-II and SAMM datasets, respectively. Another vision\ntransformer model based on optical flow and late fusion, pro-\nposed by [37], generates classification accuracy of 70.68%\non CASME-II dataset.\nV. CONCLUSIONS AND FUTURE WORKS\nWhen an existing vision transformer is exploited for micro-\nexpression recognition, it divides the input image into small\npatches and a sequence of patch embedding is created by\nlinearly embedding each patch. Due to this approach, the\nTABLE 7. Classification report over CASME-I dataset for 8 Classes\nEmotions Precision Recall F1-Score\nContempt 1.00 1.00 1.00\nDisgust 0.88 0.99 0.93\nFear 1.00 1.00 1.00\nHappiness 0.97 0.91 0.94\nRepression 0.97 1.00 0.99\nSadness 1.00 0.60 0.75\nSurprise 0.97 0.85 0.91\nTense 0.99 0.98 0.98\nAccuracy 0.96\nTABLE 8. Classification report over CASME-II dataset for 7 Classes\nEmotions Precision Recall F1-Score\nDisgust 0.98 0.98 0.98\nFear 1.00 1.00 1.00\nHappiness 0.98 0.99 0.98\nOther 0.98 0.98 0.98\nRepression 1.00 0.99 0.99\nSadness 1.00 1.00 1.00\nSurprise 0.98 0.98 0.98\nAccuracy 0.99\nTABLE 9. Classification report over SAMM dataset for 8 Classes\nEmotions Precision Recall F1-Score\nAnger 1.00 1.00 1.00\nContempt 1.00 1.00 1.00\nDisgust 1.00 1.00 1.00\nFear 1.00 1.00 1.00\nHappiness 1.00 1.00 1.00\nOthers 1.00 1.00 1.00\nSadness 1.00 1.00 1.00\nSurprise 1.00 1.00 1.00\nAccuracy 1.00\nTABLE 10. Comparison of the proposed method with existing models for\nCASME-I dataset in terms of classification accuracy\nYear Method Classification\nAccuracy (%)\n2015 MDMO-SVM [52] 68.86\n2015 LBP-TOP-ELM [53] 73.82\n2017 CNN [54] 74.25\n2018 Fusion motion boundary histograms [55] 61.33\n2019 3D optical flow-based CNN [14] 54.44\n2019 ResNet [56] 76.39\n2019 Lateral Accretive Hybrid Network [56] 80.62\n2022 Transfer learning with self-attention [57] 90.34\n2023 Dual-stream incorporating optical flow\nand CNN [58]\n61.20\n2023 Two-stream 3D deep learning with iris\nbiometric [59]\n99.99\n2023 Deep3DCANN [31] 87.00\nProposed Transformer 95.97\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 11. Comparison of the proposed method with existing models for\nCASME-II dataset in terms of classification accuracy\nYear Method Classification\nAccuracy (%)\n2014 LSTD [50] 63.41\n2016 CNN-LSTM [29] 47.30\n2019 3D optical flow-based CNN [14] 59.11\n2019 STRCN [51] 80.30\n2019 Lateral Accretive Hybrid [56] 76.57\n2020 LFM [60] 73.98\n2021 GEME [61] 75.20\n2021 Transformer [37] 70.68\n2022 MMNet [38] 88.35\n2023 Deep3DCANN [31] 86.00\nProposed Transformer 98.59\nTABLE 12. Comparison of the proposed method with existing models for\nSAMM dataset in terms of classification accuracy\nYear Method Classification\nAccuracy (%)\n2019 Dual-Stream Shallow Network [62] 63.41\n2020 Graph-TCN [63] 75.00\n2020 Knowledge distillation [64] 86.74\n2021 MERSiamC3D [13] 68.75\n2021 AU-GCN [65] 74.26\n2022 SqueezeNet and 3D CNN [15] 81.33\n2022 MMNet [38] 80.14\n2023 Deep3DCANN [31] 93.00\nProposed Transformer 100.00\nmodel may not exploit the local spatial relationships present\nin an image. To address this issue, in this work, a novel\nvision transformer based on convolution patches for micro-\nexpression is proposed, which captures local receptive field\nthrough patches generated by convolution operation, and\nglobal receptive field is captured through a vision transformer\nbased on self-attention mechanism.\nWhile implementing the proposed network architecture,\nthe following problems were handled: (i) Due to a large num-\nber of trainable parameters, self-attention-based operations,\nand long training time, high-performance computational re-\nsources are needed for the training of a vision transformer,\nthus, Nvidia A100 is utilized for training of the model\nwhich was provided by Google Colab Pro+, (ii) existing\ndeep learning models are prone to over-fitting, thus we have\nemployed layer normalization and dropout mechanism to\navoid overfitting which is usually caused by limited training\ndata. The performance of the model is evaluated in terms\nof standard evaluation metrics such as precision, recall, F1-\nscore, and classification accuracy. It has been demonstrated\nthat the proposed model outperforms several state-of-the-\nart machine and deep learning models on three benchmark\ndatasets i.e., CASME-I, CASME-II, and SAMM.\nHowever, experiments show that the performance is still\nlimited due to the following factors: (i) the existing micro-\nexpression datasets are highly unbalanced in nature. It is\nevident from Table 2, CASME-II dataset is fairly balanced\nwhen compared to CASME-I dataset, thus, CASME-II gener-\nates better classification accuracy of 98.59% as compared to\nCASME-I i.e., 95.97%. Hence, it can be inferred that sample\ndistribution plays a significant role in the performance of\nthe model. It is to be noted that SAMM dataset is also not\nbalanced (as shown in Table 2), but it contains large number\nof image samples for training, as compared to CASME-I and\nCASME-II, leading to the best possible classification accu-\nracy i.e., 100%. Therefore, it is implied that large number of\ntraining samples can improve the performance of the model\nand help the model to overlook the unbalanced nature of a\ndataset. Thus, in future, we will address this issue by using\ndata augmentation technique to generate a large number of\nsamples for CASME-I and CASME-II datasets. Most of the\nexisting MER datasets are laboratory controlled which limits\nthe implementation of MER in real-life applications, thus,\nthere is a need of in-the-wild datasets which contain a wide\nvariety of images of individuals belonging to different age\ngroups, gender, races, and cultural background. The existing\ndeep learning models can only perform emotion classifica-\ntion based on pre-defined classes, to address this issue, deep\ncontinual learning can be explored which can identify an\nunknown emotion category [66].\nREFERENCES\n[1] J. Weiss, “Ekman, p.(2009) telling lies: Clues to deceit in the marketplace,\npolitics, and marriage. new york: Norton,” 2011.\n[2] S. Zhao, H. Tang, S. Liu, Y . Zhang, H. Wang, T. Xu, E. Chen, and\nC. Guan, “Me-plan: A deep prototypical learning with local attention\nnetwork for dynamic micro-expression recognition,” Neural Networks,\nvol. 153, pp. 427–443, 2022.\n[3] P. Ekman and W. V . Friesen, “Nonverbal leakage and clues to deception,”\nPsychiatry, vol. 32, no. 1, pp. 88–106, 1969.\n[4] S. Nigam, R. Singh, and A. K. Misra, “Local binary patterns based\nfacial expression recognition for efficient smart applications,” in Security\nin Smart Cities: Models, Applications, and Challenges, pp. 297–322,\nSpringer, 2019.\n[5] S. Nigam, R. Singh, and A. K. Misra, “Efficient facial expression recogni-\ntion using histogram of oriented gradients in wavelet domain,” Multimedia\ntools and applications, vol. 77, no. 21, pp. 28725–28747, 2018.\n[6] Y . Guo, Y . Tian, X. Gao, and X. Zhang, “Micro-expression recognition\nbased on local binary patterns from three orthogonal planes and nearest\nneighbor method,” in 2014 international joint conference on neural net-\nworks (IJCNN), pp. 3473–3479, IEEE, 2014.\n[7] X. Ben, P. Zhang, R. Yan, M. Yang, and G. Ge, “Gait recognition and\nmicro-expression recognition based on maximum margin projection with\ntensor representation,” Neural Computing and Applications, vol. 27, no. 8,\npp. 2629–2646, 2016.\n[8] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,\nno. 7553, pp. 436–444, 2015.\n[9] S. Indolia, S. Nigam, and R. Singh, “Deep feature fusion for facial\nexpression recognition,” in 2022 Second International Conference on Next\nGeneration Intelligent Systems (ICNGIS), pp. 1–6, IEEE, 2022.\n[10] I. Lasri, A. Riadsolh, and M. Elbelkacemi, “Facial emotion recognition\nof deaf and hard-of-hearing students for engagement detection using\ndeep learning,” Education and Information Technologies, vol. 28, no. 4,\npp. 4069–4092, 2023.\n[11] S. Indolia, S. Nigam, and R. Singh, “An optimized convolution neural\nnetwork framework for facial expression recognition,” in 2021 Sixth\nInternational Conference on Image Information Processing (ICIIP), vol. 6,\npp. 93–98, IEEE, 2021.\n[12] Y . S. Gan, S.-T. Liong, W.-C. Yau, Y .-C. Huang, and L.-K. Tan, “Off-\napexnet on micro-expression recognition system,” Signal Processing: Im-\nage Communication, vol. 74, pp. 129–139, 2019.\n[13] S. Zhao, H. Tao, Y . Zhang, T. Xu, K. Zhang, Z. Hao, and E. Chen, “A\ntwo-stage 3d cnn based learning method for spontaneous micro-expression\nrecognition,” Neurocomputing, vol. 448, pp. 276–289, 2021.\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[14] J. Li, Y . Wang, J. See, and W. Liu, “Micro-expression recognition based on\n3d flow convolutional neural network,” Pattern Analysis and Applications,\nvol. 22, no. 4, pp. 1331–1339, 2019.\n[15] S. Liu, Y . Ren, L. Li, X. Sun, Y . Song, and C.-C. Hung, “Micro-expression\nrecognition based on squeezenet and c3d,” Multimedia Systems, pp. 1–10,\n2022.\n[16] L. Cai, H. Li, W. Dong, and H. Fang, “Micro-expression recognition\nusing 3d densenet fused squeeze-and-excitation networks,” Applied Soft\nComputing, vol. 119, p. 108594, 2022.\n[17] M. A. Takalkar, S. Thuseethan, S. Rajasegarar, Z. Chaczko, M. Xu, and\nJ. Yearwood, “Lgattnet: Automatic micro-expression detection using dual-\nstream local and global attentions,” Knowledge-Based Systems, vol. 212,\np. 106566, 2021.\n[18] G.-B. Liong, J. See, and C.-S. Chan, “Spot-then-recognize: A micro-\nexpression analysis network for seamless evaluation of long videos,”\nSignal Processing: Image Communication, vol. 110, p. 116875, 2023.\n[19] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database\nfor facial expression, valence, and arousal computing in the wild,” IEEE\nTransactions on Affective Computing, vol. 10, no. 1, pp. 18–31, 2017.\n[20] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Ham-\nner, W. Cukierski, Y . Tang, D. Thaler, D.-H. Lee, et al., “Challenges in\nrepresentation learning: A report on three machine learning contests,” in\nInternational conference on neural information processing, pp. 117–124,\nSpringer, 2013.\n[21] W.-J. Yan, Q. Wu, Y .-J. Liu, S.-J. Wang, and X. Fu, “Casme database:\nA dataset of spontaneous micro-expressions collected from neutralized\nfaces,” in 2013 10th IEEE international conference and workshops on\nautomatic face and gesture recognition (FG), pp. 1–7, IEEE, 2013.\n[22] W.-J. Yan, X. Li, S.-J. Wang, G. Zhao, Y .-J. Liu, Y .-H. Chen, and X. Fu,\n“Casme ii: An improved spontaneous micro-expression database and the\nbaseline evaluation,” PloS one, vol. 9, no. 1, p. e86041, 2014.\n[23] A. K. Davison, C. Lansley, N. Costen, K. Tan, and M. H. Yap, “Samm:\nA spontaneous micro-facial movement dataset,” IEEE transactions on\naffective computing, vol. 9, no. 1, pp. 116–129, 2016.\n[24] X. Li, T. Pfister, X. Huang, G. Zhao, and M. Pietikäinen, “A spontaneous\nmicro-expression database: Inducement, collection and baseline,” in 2013\n10th IEEE International Conference and Workshops on Automatic face\nand gesture recognition (fg), pp. 1–6, IEEE, 2013.\n[25] M. Shreve, S. Godavarthy, D. Goldgof, and S. Sarkar, “Macro-and micro-\nexpression spotting in long videos using spatio-temporal strain,” in 2011\nIEEE International Conference on Automatic Face & Gesture Recognition\n(FG), pp. 51–56, IEEE, 2011.\n[26] S. Polikovsky, Y . Kameda, and Y . Ohta, “Facial micro-expressions recog-\nnition using high speed camera and 3d-gradient descriptor,” 2009.\n[27] J. Wei, G. Lu, J. Yan, and H. Liu, “Micro-expression recognition using\nlocal binary pattern from five intersecting planes,” Multimedia Tools and\nApplications, pp. 1–26, 2022.\n[28] A. M. Buhari, C.-P. Ooi, V . M. Baskaran, R. C. Phan, K. Wong, and\nW.-H. Tan, “Invisible emotion magnification algorithm (iema) for real-\ntime micro-expression recognition with graph-based features,” Multimedia\nTools and Applications, vol. 81, no. 7, pp. 9151–9176, 2022.\n[29] D. Patel, X. Hong, and G. Zhao, “Selective deep features for micro-\nexpression recognition,” in 2016 23rd international conference on pattern\nrecognition (ICPR), pp. 2258–2263, IEEE, 2016.\n[30] V . Mayya, R. M. Pai, and M. M. Pai, “Combining temporal interpolation\nand dcnn for faster recognition of micro-expressions in video sequences,”\nin 2016 International Conference on Advances in Computing, Communi-\ncations and Informatics (ICACCI), pp. 699–703, IEEE, 2016.\n[31] S. Thuseethan, S. Rajasegarar, and J. Yearwood, “Deep3dcann: A deep\n3dcnn-ann framework for spontaneous micro-expression recognition,” In-\nformation Sciences, vol. 630, pp. 341–355, 2023.\n[32] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between\ncapsules,” Advances in neural information processing systems, vol. 30,\n2017.\n[33] N. Van Quang, J. Chun, and T. Tokuyama, “Capsulenet for micro-\nexpression recognition,” in 2019 14th IEEE International Conference on\nAutomatic Face & Gesture Recognition (FG 2019), pp. 1–7, IEEE, 2019.\n[34] L. Zhou, Q. Mao, and L. Xue, “Dual-inception network for cross-database\nmicro-expression recognition,” in 2019 14th IEEE International Confer-\nence on Automatic Face & Gesture Recognition (FG 2019), pp. 1–5, IEEE,\n2019.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[36] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., “An\nimage is worth 16x16 words: Transformers for image recognition at scale,”\narXiv preprint arXiv:2010.11929, 2020.\n[37] J. Hong, C. Lee, and H. Jung, “Late fusion-based video transformer for\nfacial micro-expression recognition,” Applied Sciences, vol. 12, no. 3,\np. 1169, 2022.\n[38] H. Li, M. Sui, Z. Zhu, and F. Zhao, “Mmnet: Muscle motion-guided net-\nwork for micro-expression recognition,” arXiv preprint arXiv:2201.05297,\n2022.\n[39] J. Zhu, Y . Zong, H. Chang, Y . Xiao, and L. Zhao, “A sparse-\nbased transformer network with associated spatiotemporal feature for\nmicro-expression recognition,” IEEE Signal Processing Letters, vol. 29,\npp. 2073–2077, 2022.\n[40] L. Zhang, X. Hong, O. Arandjelovic, and G. Zhao, “Short and long\nrange relation based spatio-temporal transformer for micro-expression\nrecognition,” arXiv preprint arXiv:2112.05851, 2021.\n[41] X. Zhao, Y . Lv, and Z. Huang, “Multimodal fusion-based swin transformer\nfor facial recognition micro-expression recognition,” in 2022 IEEE Inter-\nnational Conference on Mechatronics and Automation (ICMA), pp. 780–\n785, IEEE, 2022.\n[42] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao, “Multi-\nscale vision longformer: A new vision transformer for high-resolution im-\nage encoding,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 2998–3008, 2021.\n[43] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),” arXiv\npreprint arXiv:1606.08415, 2016.\n[44] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\n2014.\n[45] S. Indolia, S. Nigam, and R. Singh, “A framework for facial expression\nrecognition using deep self-attention network,” Journal of Ambient Intel-\nligence and Humanized Computing, pp. 1–20, 2023.\n[46] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin\ntransformer: Hierarchical vision transformer using shifted windows,” in\nProceedings of the IEEE/CVF international conference on computer vi-\nsion, pp. 10012–10022, 2021.\n[47] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“Cvt: Introducing convolutions to vision transformers,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pp. 22–31,\n2021.\n[48] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n“Training data-efficient image transformers & distillation through atten-\ntion,” in International conference on machine learning, pp. 10347–10357,\nPMLR, 2021.\n[49] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolu-\ntional neural networks,” in International conference on machine learning,\npp. 6105–6114, PMLR, 2019.\n[50] S.-J. Wang, W.-J. Yan, G. Zhao, X. Fu, and C.-G. Zhou, “Micro-expression\nrecognition using robust principal component analysis and local spatiotem-\nporal directional features,” in European Conference on computer vision,\npp. 325–338, Springer, 2014.\n[51] Z. Xia, X. Hong, X. Gao, X. Feng, and G. Zhao, “Spatiotemporal recurrent\nconvolutional networks for recognizing spontaneous micro-expressions,”\nIEEE Transactions on Multimedia, vol. 22, no. 3, pp. 626–640, 2019.\n[52] Y .-J. Liu, J.-K. Zhang, W.-J. Yan, S.-J. Wang, G. Zhao, and X. Fu, “A main\ndirectional mean optical flow feature for spontaneous micro-expression\nrecognition,” IEEE Transactions on Affective Computing, vol. 7, no. 4,\npp. 299–310, 2015.\n[53] Y . Guo, C. Xue, Y . Wang, and M. Yu, “Micro-expression recognition based\non cbp-top feature with elm,” Optik, vol. 126, no. 23, pp. 4446–4451, 2015.\n[54] M. A. Takalkar and M. Xu, “Image based facial micro-expression recog-\nnition using deep learning on small datasets,” in 2017 international confer-\nence on digital image computing: techniques and applications (DICTA),\npp. 1–7, IEEE, 2017.\n[55] H. Lu, K. Kpalma, and J. Ronsin, “Motion descriptors for micro-\nexpression recognition,” Signal Processing: Image Communication,\nvol. 67, pp. 108–117, 2018.\n[56] M. Verma, S. K. Vipparthi, G. Singh, and S. Murala, “Learnet: Dynamic\nimaging network for micro expression recognition,” IEEE Transactions on\nImage Processing, vol. 29, pp. 1618–1627, 2019.\n[57] S. Indolia, S. Nigam, and R. Singh, “Integration of transfer learning and\nself-attention for spontaneous micro-expression recognition,” in 2022 Sev-\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nenth International Conference on Parallel, Distributed and Grid Computing\n(PDGC), pp. 325–330, IEEE, 2022.\n[58] J. Tang, L. Li, M. Tang, and J. Xie, “A novel micro-expression recognition\nalgorithm using dual-stream combining optical flow and dynamic image\nconvolutional neural networks,” Signal, Image and Video Processing,\npp. 1–8, 2022.\n[59] V . Esmaeili and M. Mohassel Feghhi, “Real-time authentication for elec-\ntronic service applicants using a method based on two-stream 3d deep\nlearning,” Soft Computing Journal, 2023.\n[60] D. Y . Choi and B. C. Song, “Facial micro-expression recognition us-\ning two-dimensional landmark feature maps,” IEEE Access, vol. 8,\npp. 121549–121563, 2020.\n[61] X. Nie, M. A. Takalkar, M. Duan, H. Zhang, and M. Xu, “Geme: Dual-\nstream multi-task gender-based micro-expression recognition,” Neuro-\ncomputing, vol. 427, pp. 13–28, 2021.\n[62] H.-Q. Khor, J. See, S.-T. Liong, R. C. Phan, and W. Lin, “Dual-stream\nshallow networks for facial micro-expression recognition,” in 2019 IEEE\ninternational conference on image processing (ICIP), pp. 36–40, IEEE,\n2019.\n[63] L. Lei, J. Li, T. Chen, and S. Li, “A novel graph-tcn with a graph structured\nrepresentation for micro-expression recognition,” in Proceedings of the\n28th ACM International Conference on Multimedia, pp. 2237–2245, 2020.\n[64] B. Sun, S. Cao, D. Li, J. He, and L. Yu, “Dynamic micro-expression\nrecognition using knowledge distillation,” IEEE Transactions on Affective\nComputing, 2020.\n[65] H.-X. Xie, L. Lo, H.-H. Shuai, and W.-H. Cheng, “Au-assisted graph\nattention convolutional network for micro-expression recognition,” in\nProceedings of the 28th ACM International Conference on Multimedia,\npp. 2871–2880, 2020.\n[66] S. Thuseethan, S. Rajasegarar, and J. Yearwood, “Deep continual learning\nfor emerging emotion recognition,” IEEE Transactions on Multimedia,\nvol. 24, pp. 4367–4380, 2021.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3314797\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7658979892730713
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7563815116882324
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7066781520843506
    },
    {
      "name": "Overfitting",
      "score": 0.6968531608581543
    },
    {
      "name": "Transformer",
      "score": 0.5713822245597839
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5059542059898376
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4973001778125763
    },
    {
      "name": "Computer vision",
      "score": 0.4862014651298523
    },
    {
      "name": "Pixel",
      "score": 0.48544055223464966
    },
    {
      "name": "Deep learning",
      "score": 0.4480658769607544
    },
    {
      "name": "Artificial neural network",
      "score": 0.2918930649757385
    },
    {
      "name": "Engineering",
      "score": 0.08895319700241089
    },
    {
      "name": "Voltage",
      "score": 0.07784044742584229
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102117144",
      "name": "Banasthali University",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I91357014",
      "name": "Banaras Hindu University",
      "country": "IN"
    }
  ]
}