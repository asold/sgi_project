{
    "title": "COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter",
    "url": "https://openalex.org/W4324148013",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5015525397",
            "name": "Martin Müller",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5010649096",
            "name": "Marcel Salathé",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5079331687",
            "name": "Per Egil Kummervold",
            "affiliations": [
                "Fundación para el Fomento de la Investigación Sanitaria y Biomédica de la Comunitat Valenciana"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288407534",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3186243414",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3048276999",
        "https://openalex.org/W2962707339",
        "https://openalex.org/W2752201871",
        "https://openalex.org/W2773085311",
        "https://openalex.org/W6764528812",
        "https://openalex.org/W6691459498",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2913539360",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Introduction This study presents COVID-Twitter-BERT (CT-BERT), a transformer-based model that is pre-trained on a large corpus of COVID-19 related Twitter messages. CT-BERT is specifically designed to be used on COVID-19 content, particularly from social media, and can be utilized for various natural language processing tasks such as classification, question-answering, and chatbots. This paper aims to evaluate the performance of CT-BERT on different classification datasets and compare it with BERT-LARGE, its base model. Methods The study utilizes CT-BERT, which is pre-trained on a large corpus of COVID-19 related Twitter messages. The authors evaluated the performance of CT-BERT on five different classification datasets, including one in the target domain. The model's performance is compared to its base model, BERT-LARGE, to measure the marginal improvement. The authors also provide detailed information on the training process and the technical specifications of the model. Results The results indicate that CT-BERT outperforms BERT-LARGE with a marginal improvement of 10-30% on all five classification datasets. The largest improvements are observed in the target domain. The authors provide detailed performance metrics and discuss the significance of these results. Discussion The study demonstrates the potential of pre-trained transformer models, such as CT-BERT, for COVID-19 related natural language processing tasks. The results indicate that CT-BERT can improve the classification performance on COVID-19 related content, especially on social media. These findings have important implications for various applications, such as monitoring public sentiment and developing chatbots to provide COVID-19 related information. The study also highlights the importance of using domain-specific pre-trained models for specific natural language processing tasks. Overall, this work provides a valuable contribution to the development of COVID-19 related NLP models.",
    "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/four.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\nOPEN ACCESS\nEDITED BY\nWajdi Zaghouani,\nHamad Bin Khalifa University, Qatar\nREVIEWED BY\nIvan Vladimir Meza Ruiz,\nUniversidad Nacional Autónoma de México,\nMexico\nAnna Glazkova,\nTyumen State University, Russia\n*CORRESPONDENCE\nPer E. Kummervold\nper@capia.no\n†These authors have contributed equally to this\nwork and share ﬁrst authorship\nSPECIALTY SECTION\nThis article was submitted to\nNatural Language Processing,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nRECEIVED /one.tnum/nine.tnum August /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /three.tnum/one.tnum January /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/four.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nMüller M, Salathé M and Kummervold PE (/two.tnum/zero.tnum/two.tnum/three.tnum)\nCOVID-Twitter-BERT: A natural language\nprocessing model to analyse COVID-/one.tnum/nine.tnum\ncontent on Twitter.\nFront. Artif. Intell./six.tnum:/one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Müller, Salathé and Kummervold. This\nis an open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nCOVID-Twitter-BERT: A natural\nlanguage processing model to\nanalyse COVID-/one.tnum/nine.tnum content on\nTwitter\nMartin Müller /one.tnum†, Marcel Salathé /one.tnumand Per E. Kummervold /two.tnum*†\n/one.tnumDigital Epidemiology Lab, EPFL, Geneva, Switzerland, /two.tnumFISABIO-Public Health, Vaccine Research\nDepartment, Valencia, Spain\nIntroduction: This study presents COVID-Twitter-BERT (CT-BERT), a transfor mer-\nbased model that is pre-trained on a large corpus of COVID-/one.tnum/nine.tnum related Twitter\nmessages. CT-BERT is speciﬁcally designed to be used on COVID-/one.tnum/nine.tnum content,\nparticularly from social media, and can be utilized for various natural language\nprocessing tasks such as classiﬁcation, question-answering, and ch atbots. This\npaper aims to evaluate the performance of CT-BERT on diﬀerent classiﬁcation\ndatasets and compare it with BERT-LARGE, its base model.\nMethods: The study utilizes CT-BERT, which is pre-trained on a large corp us\nof COVID-/one.tnum/nine.tnum related Twitter messages. The authors evaluated the performance\nof CT-BERT on ﬁve diﬀerent classiﬁcation datasets, including on e in the target\ndomain. The model’s performance is compared to its base model, B ERT-LARGE, to\nmeasure the marginal improvement. The authors also provide d etailed information\non the training process and the technical speciﬁcations of the mod el.\nResults: The results indicate that CT-BERT outperforms BERT-LARGE wit h a\nmarginal improvement of /one.tnum/zero.tnum-/three.tnum/zero.tnum% on all ﬁve classiﬁcation datasets. The largest\nimprovements are observed in the target domain. The authors p rovide detailed\nperformance metrics and discuss the signiﬁcance of these results .\nDiscussion: The study demonstrates the potential of pre-trained transfor mer\nmodels, such as CT-BERT, for COVID-/one.tnum/nine.tnum related natural languageprocessing\ntasks. The results indicate that CT-BERT can improve the classiﬁca tion\nperformance on COVID-/one.tnum/nine.tnum related content, especially on social media. These\nﬁndings have important implications for various applications , such as monitoring\npublic sentiment and developing chatbots to provide COVID-/one.tnum/nine.tnumrelated\ninformation. The study also highlights the importance of usin g domain-speciﬁc\npre-trained models for speciﬁc natural language processing t asks. Overall, this\nwork provides a valuable contribution to the development of CO VID-/one.tnum/nine.tnum related\nNLP models.\nKEYWORDS\nnatural language processing (NLP), COVID-/one.tnum/nine.tnum, language model (LM), BERT, text\nclassiﬁcation\n/one.tnum. Introduction\nTwitter has been a valuable source of news and a public medium for expression during\nthe COVID-19 pandemic. However, manually classifying, ﬁltering, and summarizing the\nlarge amount of information available on COVID-19 on Twitter is impossible and has also\nbeen a challenging task to solve with tools from the ﬁeld of machine learning and natural\nlanguage processing (NLP). To improve our understanding of Twitter messages related to\nCOVID-19 content as well as the analysis of this content, we have therefore developed a\nmodel called COVID-Twitter-BERT (CT-BERT).\n/one.tnum\n/one.tnumhttps://github.com/digitalepidemiologylab/covid-twitter-bert\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nMüller et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\nTransformer-based models have changed the landscape of NLP.\nModels such as BERT, RoBERTa, and ALBERT are all based on\nthe same principle – training bi-directional transformer models\non huge unlabeled text corpuses (\nVaswani et al., 2017 ; Devlin\net al., 2018 ; Lan et al., 2019 ; Liu et al., 2019 ). This process is\ndone using methods such as mask language modeling (MLM),\nnext sentence prediction (NSP), and sentence order prediction\n(SOP). Diﬀerent models vary slightly in how these methods are\napplied, but in general, all training is done in a fully unsupervised\nmanner. This process generates a general language model that is\nthen used as input for a supervised ﬁnetuning for speciﬁc language\nprocessing tasks, such as classiﬁcation, question-answering models,\nand chatbots.\nOur model is based on the BERT-L ARGE (English, uncased,\nwhole word masking) model. BERT-L ARGE is trained mainly on\nraw text data from Wikipedia (3.5B words) and a free book corpus\n(0.8B words) (\nDevlin et al., 2018 ). Whilst this is an impressive\namount of text, it still contains little information about any speciﬁc\nsubdomain. To improve performance in subdomains, we have\nseen numerous transformer-based models trained on specialized\ncorpuses. Some of the most popular ones are B IOBERT (\nLee et al.,\n2020) and S CIBERT (Beltagy et al., 2019 ). These models are trained\nusing the exact same unsupervised training techniques as the main\nmodels (MLM/NSP/SOP). They can be trained from scratch, but\nthis requires a very large corpus, so a more common approach is to\nstart with the trained weights from a general model. In this study,\nthis process is called domain-speciﬁc pretraining. When trained,\nsuch models can be used as replacements for general language\nmodels and be trained for downstream tasks.\n/two.tnum. Method\nThe CT-BERT model is trained on a corpus of 160M\ntweets about the coronavirus collected through the Crowdbreaks\nplatform (\nMüller and Salathé, 2019 ) during the period from\nJanuary 12 to April 16, 2020. Crowdbreaks uses the Twitter ﬁlter\nstream API to listen to a set of COVID-19-related keywords\n/two.tnum\nin the English language. These downloaded tweets are the basis\nfor creating the training corpus. Prior to training, the original\ncorpus was cleaned for retweet tags. Each tweet was pseudonymised\nby replacing all Twitter usernames with a common text token.\nA similar procedure was performed on all URLs to web pages.\nWe also replaced all unicode emoticons with textual ASCII\nrepresentations (e.g., :smile:for /smiley) using the Python emoji\nlibrary.\n/three.tnumStarting from the 160M tweets, all retweets, duplicates and\nclose duplicates were removed. This resulted in a ﬁnal corpus of\n22.5M tweets that comprise a total of 0.6B words. The domain-\nspeciﬁc pretraining dataset therefore consists of 1/7th the size of\nwhat is used for training the main base model. Tweets were treated\nas individual documents and segmented into sentences using the\nspaCy library (\nHonnibal and Montani, 2017 ).\nAll input sequences to the BERT models are converted to a set\nof tokens from a 30.000-word vocabulary. As all Twitter messages\n/two.tnum wuhan, ncov, coronavirus, covid, SARS-CoV-/two.tnum.\n/three.tnum\nhttps://pypi.org/project/emoji/\nare limited to 280 characters, this allows us to reduce the sequence\nlength to 96 tokens, thereby increasing the training batch sizes\nto 1.024 examples. We use a dupe factor of 10 on the dataset,\nresulting in 285M training examples and 2.5M validation examples.\nA constant learning rate of 2e-5, as recommended on the oﬃcial\nBERT GitHub\n/four.tnumwhen doing domain-speciﬁc pretraining.\nLoss and accuracy was calculated through the pretraining\nprocedure. For every 100.000 training steps, we therefore save\na checkpoint and ﬁnetune this toward a variety of downstream\nclassiﬁcation tasks. Distributed training was performed using\nTensorﬂow 2.2 on a TPU v3-8 (128GB of RAM) for 120 h.\nAll scores in this work have been computed using the scikit-\nlearn Python package. Throughout this work, F1 scores are using\nmacro averaging.\n/two.tnum./one.tnum. Evaluation\nTo assess the performance of our model on downstream\nclassiﬁcation tasks, we selected ﬁve independent training sets (see\nTable 1). Three of them are publicly available datasets, and two\nare from internal projects not yet published. All datasets consist of\nTwitter-related data.\n/two.tnum./one.tnum./one.tnum. COVID-/one.tnum/nine.tnum category (CC)\nThis is an internal dataset of tweets that are sampled from the\ntraining dataset for CT-BERT, speciﬁcally for the period between\nJanuary 12 and February 24, 2020. Annotators on Amazon Turk\n(MTurk) were asked to categorize a given tweet text into either\nbeing a personal narrative (33.3%) or news (66.7%). The annotation\nwas performed using the Crowdbreaks platform (\nMüller and\nSalathé, 2019).\n/two.tnum./one.tnum./two.tnum. Vaccine sentiment (VS)\nThis dataset contains a collection of measles- and vaccination-\nrelated US-geolocated tweets collected between March 2, 2011 and\nOctober 9, 2016. The dataset was ﬁrst used by\nPananos et al.\n(2017), but a modiﬁed version from Müller and Salathé (2019)\nwas used here. The dataset contains three classes: positive (toward\nvaccinations) (51.9%), negative (7.1%), and neutral/others (41.0%).\nThe neutral category was used for tweets which are either irrelevant\nor ambiguous. Annotation was performed on MTurk.\n/two.tnum./one.tnum./three.tnum. Maternal vaccine stance (MVS)\nThe dataset is from a project related to the stance toward\nthe use of maternal vaccines (\nKummervold et al., 2021 ). The\nmethodology for creating the dataset is also explained in Martin\net al. (2020). Experts in the ﬁeld annotated the data into four\ncategories: neutral (41.0%), discouraging (25.3%), promotional\n(43.9%), and ambiguous (14.3%). Each tweet was annotated 3-fold,\nand disagreement amongst the experts was resolved in each case by\nusing a common scoring criterion.\n/four.tnumhttps://github.com/google-research/bert\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nMüller et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\nTABLE /one.tnum Overview of the evaluation datasets.\nDataset Classes Train Dev Labels\nCOVID-19 Category (CC) 2 3,094 1.031 Personal News\nVaccine Sentiment (VC) 3 5.000 3.000 N Neutral Positive\nMaternal Vaccine Stance (MVS) 4 1.361 817 Disc A N Promotional\nStanford Sentiment Treebank 2 (SST-2) 2 67.349 872 Negative Positive\nTwitter Sentiment SemEval (SE) 3 6.000 817 Neg Neutral Positive\nAll ﬁve evaluation datasets are multi-class datasets with sometime s strong label imbalance, visualized by the proportional bar width in t he label column. N and Neg stand for negative; Disc and\nA stand for discouraging and ambiguous, respectively.\n/two.tnum./one.tnum./four.tnum. Twitter sentiment SemEval (SE)\nThis is an open dataset from SemEval-2016 Task 4: Sentiment\nAnalysis in Twitter (\nNakov et al., 2019 ). In particular, we used\nthe dataset for subtask A, a dataset annotated 5-fold into three\ncategories: negative (15.7%), neutral (45.9%), and positive (38.4%).\nWe make a small adjustment to this dataset by fully anonymizing\nlinks and usernames.\n/two.tnum./one.tnum./five.tnum. Stanford sentiment treebank /two.tnum (SST-/two.tnum)\nSST-2 is a public dataset consisting of binary sentiment\nlabels, negative (44.3%) and positive (55.7%), within sentences\n(\nSocher et al., 2013 ). Sentences were extracted from a dataset of\nmovie reviews ( Pang and Lee, 2005 ) and did not originate from\nTwitter, making SST-2 our only non-Twitter dataset.\nThe division of the dataset into subsets for the SST-2 and SE\ndatasets was pre-determined. In the case of the other datasets, our\naim was to divide the data roughly 30–50% for the training and\nvalidation sets, respectively, with the remaining 20% designated as\nthe test set, which was not used in this study. The validation set was\nthe only subset used in this research.\nOur intention was not to optimize the ﬁnetuned models but to\nthoroughly evaluate the performance of the domain-speciﬁc CT-\nBERT-model. We experimented with diﬀerent numbers of epochs\nfor each training dataset for BERT-L ARGE (i.e., checkpoint 0 of CT-\nBERT) and selected the optimal one. We then used this number\nin subsequent experiments on the respective dataset. We ended\nwith three epochs for SST-2 (291 steps), CC (291 steps) and SE\n(564 steps), ﬁve epochs for VC (780 steps), and 10 epochs for\nMVS (430 steps), all with a learning rate of 2e-05 and a batch\nsize of 32. The number of epochs was dependent on both the size\nand balance of the categories. Larger and unbalanced sets require\nmore epochs.\n/three.tnum. Results\n/three.tnum./one.tnum. Domain-sepciﬁc pretraining\nFigure 1 shows the progress of pretraining CT-BERT at\nintervals of 25k training steps and the evaluation of 1k steps\non a held-out validation dataset. All metrics considered improve\nthroughout the training process. The improvement on the MLM\nloss task is most notable and yields a ﬁnal value of 1.48. The\nNSP task improves only marginally, as it already performs very\nwell initially. Training was stopped at 500.000, an equivalent of\nFIGURE /one.tnum\nEvaluation metrics for the domain-speciﬁc pretraining of CT -BERT.\nShown are the loss and accuracy of masked language modeling\n(MLM) and next sentence prediction (NSP) tasks.\n512M training examples, which we consider as our ﬁnal model.\nThis corresponds to roughly 1.8 training epochs. All metrics for\nthe MLM and NLM tasks improve steadily throughout training.\nHowever, using loss/metrics for these tasks to evaluate the correct\ntime to stop training is diﬃcult.\n/three.tnum./two.tnum. Evaluation on classiﬁcation datasets\nTo assess the performance of our model properly, we compared\nthe mean F1 score of CT-BERT with that of BERT-L ARGE on\nﬁve diﬀerent classiﬁcation datasets. We adapted the number of\ntraining epochs for each dataset according to its size in order\nto have a similar number of training steps for each dataset.\nOur ﬁnal model shows higher performance on all datasets\n(a mean F1 score of 0.833) compared with BERT-L ARGE (a\nmean F1 score of 0.802). As the initial performance varies\nwidely across datasets, we compute the relative improvement in\nmarginal performance ( /Delta1MP) for each dataset. /Delta1MP is calculated\nas follows:\n/Delta1MP = F1, CT-BERT − F1, BERT-LARGE\n1 − F1, BERT-LARGE\nFrom Table 2 we observe that the largest improvement of our\nmodel on the COVID-19-speciﬁc dataset (CC), with a /Delta1MP value\nof 25.88%. The marginal improvement is also high on the Twitter\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nMüller et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\nTABLE /two.tnum Comparison of the ﬁnal model performance with BERT-LARGE.\nDataset BERT-LARGE CT-BERT /Delta1MP\nCOVID-19 Category (CC) 0.931 0.949 25.88%\nVaccine Sentiment (VC) 0.824 0.869 25.27%\nMaternal Vaccine Stance\n(MVS)\n0.696 0.748 17.07%\nStanford Sentiment Treebank\n2 (SST-2)\n0.937 0.944 10.67%\nTwitter Sentiment SemEval\n(SE)\n0.620 0.654 8.97%\nAverage 0.802 0.833 17.57%\nCT-BERT shows improvements on all datasets. The marginal improveme nt is the highest on\nthe COVID-19-related dataset (CC) and lowest on the SST-2 and SemEv al datasets.\ndatasets related to vaccine sentiment (MVS). Our model likewise\nshows some improvements on the SST-2 and SemEval datasets, but\nto a smaller extent.\n/three.tnum./three.tnum. Evaluation on intermediary pretraining\ncheckpoints\nSo far, we have seen improvements in the ﬁnal CT-BERT\nmodel on all evaluated datasets. To understand whether the\nobserved decrease in loss during pretraining linearly translates\ninto performance on downstream classiﬁcation tasks, we evaluated\nCT-BERT on ﬁve intermediary versions (checkpoints) of the\nmodel and on the zero checkpoint, which corresponds to the\noriginal BERT-L ARGE model. At each intermediary checkpoint,\n10 repeated training runs (ﬁnetunings) for each of the ﬁve\ndatasets were performed, and the mean F1 score was recorded.\nFigure 2 shows the marginal performance increase ( /Delta1MP) at\nspeciﬁc pretraining steps. Our experiments show that downstream\nperformance increases fast up to step 200k in the pretraining\nand only demonstrates marginal improvement afterwards. The\nloss curve, on the other hand, shows a gradual increase even\nafter step 200k. We also note that for the COVID-19-related\ndataset, most of the marginal improvement occurred after 100k\npretraining steps. SST-2, the only non-Twitter dataset, improves\nmuch more slowly and reaches its ﬁnal performance only after 200k\npretraining steps.\nAmongst runs on the same model and dataset, some degree\nof variance in performance was observed. This variance is\nmostly driven by runs with a particularly low performance.\nWe observe that the variance is dataset dependent, but it does\nnot increase throughout diﬀerent pretraining checkpoints\nand is comparable to the variance observed on BERT-\nLARGE (pretraining step zero). The most stable training\nseems to be on the SemEval training set, and the least stable\none is on SST-2, but most of this diﬀerence is within the\nerror margins.\n/four.tnum. Discussion\nThe most accurate way to evaluate the performance of a\ndomain-speciﬁc model is to apply it on speciﬁc downstream tasks.\nCT-BERT is evaluated on ﬁve diﬀerent Twitter-based datasets.\nCompared to BERT-L ARGE , it improves signiﬁcantly on all\ndatasets. However, the improvement is largest in datasets related to\nhealth, particularly in datasets related to COVID-19. We therefore\nexpect CT-BERT to perform similarly well on other classiﬁcation\nproblems on COVID-19-related data sources, but particularly on\ntext derived from social media platforms.\nWhilst it is expected that the beneﬁt of using CT-BERT instead\nof BERT-LARGE is greatest when working with Twitter COVID-19\ntext, it is reasonable to expect some performance gains even when\nworking with general Twitter messages (SemEval dataset) or with a\nnon-Twitter dataset (SST-2).\nOur results show that the MLM and NSP metrics during the\npretraining align to some degree with downstream performance\nFIGURE /two.tnum\nMarginal performance increase in the F/one.tnum score (/Delta1MP) on ﬁnetuning on various classiﬁcation tasks at increasing st eps of pretraining. Zero on the\nx-axis corresponds to the base model, which is BERT -LARGEin this case. Our model improves on all evaluated datasets, with the biggest relative\nimprovement being in the COVID-/one.tnum/nine.tnum category dataset. The bands show the standard error of the mean (SEM) out of /one.tnum/zero.tnum repeats. From this we can\nalso estimate the standard deviation SD = SEM ∗\n√\n/one.tnum/zero.tnum.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nMüller et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\non classiﬁcation tasks. However, compared with COVID-19 or\nhealth-related content, out-of-domain text might require longer\npretraining to achieve a similar performance boost.\nWhilst we have observed an improvement in performance\non classiﬁcation tasks, we did not test our model on other\nnatural language understanding tasks. Furthermore, at the time\nof this paper’s writing, we only had access to one COVID-19-\nrelated dataset. The general performance of our model might\nbe improved further by considering pretraining under diﬀerent\nhyperparameters, particularly modiﬁcations to the learning rate\nschedules, training batch sizes and optimizers. Future work might\ninclude evaluation on other datasets and the inclusion of more\nrecent training data.\nThe best way to evaluate pretrained transformer models is to\nﬁnetune them on downstream tasks. Finetuning a classiﬁer on\na pre-trained model is considered computationally cheap. The\ntraining time is usually done in an hour or two on a GPU. Using this\nmethod for evaluation is more expensive, as it requires evaluating\nmultiple checkpoints to monitor improvement and on several\nvaried datasets to show robustness. As ﬁnetuning results vary\nbetween each run, each experiment must be performed multiple\ntimes when the goal is to study the pretrained model. In this case,\nwe repeated the training for six checkpoints, 10 runs for each\ncheckpoint on all the ﬁve datasets. A total of 300 evaluation runs\nwere performed. The computational cost for evaluation is therefore\non par with the pretraining. Large and reliable validation sets make\nthis task easier, as the number of repetitions can be reduced.\nAll the tests are done on categorization tasks, as this task\nis easier in terms of both data access and evaluation. However,\ntransformer-based models can be used for a wide range of tasks,\nsuch as named entity recognition and question answering. It is\nexpected that CT-BERT can also be used for these kinds of tasks\nwithin our target domain.\nOur primary goal in this work was to obtain stable results\non the ﬁnetuning in order to evaluate the pre-trained model,\nnot to necessarily optimize the ﬁnetuning. The number of\nﬁnetuning epochs and the learning rate are, for instance,\nhave been optimized for BERT-L ARGE , not for CT-BERT. This\nmeans that there is still great room for optimization on the\ndownstream task.\n/five.tnum. Limitations\nIn this study, we demonstrate that our model outperforms\nBERT Large, which at the time of the study held the state-of-\nthe-art performance for many NLP tasks. However, it should be\nnoted that since the completion of our study, several other language\nmodels have been introduced that may surpass the performance\nof BERT Large and potentially also CT-BERT. Nevertheless, the\nmethodology used to improve BERT Large in the development of\nCT-BERT could potentially be applied to other models in order to\nachieve similar performance improvements.\nData availability statement\nThe model, code, and public datasets are available in\nour GitHub repository: https://github.com/digitalepidemiologylab/\ncovid-twitter-bert.\nAuthor contributions\nAll authors listed have made a substantial, direct, and\nintellectual contribution to the work and approved it\nfor publication.\nFunding\nPK received funding from the European Commission for the\ncall H2020-MSCA-IF-2017 and the funding scheme MSCA-IF-EF-\nST for the V ACMA project (grant agreement ID: 797876). MM\nand MS received funding through the Versatile Emerging infectious\ndisease Observatory grant as a part of the European Commission’s\nHorizon 2020 framework programme (grant agreement ID:\n874735). The research was supported with Cloud TPUs from\nGoogle’s TensorFlow Research Cloud and Google Cloud credits in\nthe context of COVID-19-related research.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nBeltagy, I., Cohan, A., and Lo, K. (2019). Scibert: pretrained\ncontextualized embeddings for scientiﬁc text. arXiv preprint arXiv:1903.10676.\ndoi: 10.48550/arXiv.1903.10676\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ert: pre-training\nof deep bidirectional transformers for language understand ing. arXiv preprint\narXiv:1810.04805. doi: 10.48550/arXiv.1810.04805\nHonnibal, M., and Montani, I. (2017). spacy 2: natural language understanding with\nbloom embeddings, convolutional neural networks and increme ntal parsing. To appear\n7(1):411–420.\nKummervold, P. E., Martin, S., Dada, S., Kilich, E., Denny, C., Paterson, P., et al.\n(2021). Categorizing vaccine conﬁdence with a transformer -based machine learning\nmodel: analysis of nuances of vaccine sentiment in twitter dis course. JMIR Med. Inform.\n9:e29584. doi: 10.2196/29584\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricu t, R. (2019).\nAlbert: a lite bert for self-supervised learning of language repres entations. arXiv\npreprint arXiv:1909.11942. doi: 10.48550/arXiv.1909.11942\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., et al. (20 20).\nBiobert: a pre-trained biomedical language representation mo del for biomedical\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nMüller et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/two.tnum/eight.tnum/one.tnum\ntext mining. Bioinformatics 36, 1234–1240. doi: 10.1093/bioinformatics/\nbtz682\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (20 19). Roberta:\na robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\ndoi: 10.48550/arXiv.1907.11692\nMartin, S., Kilich, E., Dada, S., Kummervold, P. E., Denny, C., Paterson, P., et al.\n(2020). “vaccines for pregnant women...?! absurd”–mapping ma ternal vaccination\ndiscourse and stance on social media over six months. Vaccine 38, 6627–6637.\ndoi: 10.1016/j.vaccine.2020.07.072\nMüller, M. M., and Salathé, M. (2019). Crowdbreaks: tracking he alth trends\nusing public social media data and crowdsourcing. Front. Public Health 7, 81.\ndoi: 10.3389/fpubh.2019.00081\nNakov, P., Ritter, A., Rosenthal, S., Sebastiani, F., and Sto yanov, V. (2019).\nSemeval-2016 task 4: sentiment analysis in twitter. arXiv preprint arXiv:1912.01973.\ndoi: 10.18653/v1/S16-1001\nPananos, A. D., Bury, T. M., Wang, C., Schonfeld, J., Mohanty, S. P.,\nNyhan, B., et al. (2017). Critical dynamics in population vaccin ating behavior.\nProc. Natl. Acad. Sci. U.S.A. 114, 13762–13767. doi: 10.1073/pnas.170409\n3114\nPang, B., and Lee, L. (2005). “Seeing stars: exploiting class re lationships for\nsentiment categorization with respect to rating scales, ” in Proceedings of the 43rd\nAnnual Meeting on Association for Computational Linguistics (Association for\nComputational Linguistics), Ann Arbor, Michigan, 115–124.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., et al. (2013).\n“Recursive deep models for semantic compositionality over a se ntiment treebank, ”\nin Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, Seattle, Washington, USA, 1631–1642.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems, Long Beach, CA, USA, 5998–6008.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org"
}