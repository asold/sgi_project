{
    "title": "Comparing Transformers and RNNs on predicting human sentence processing data.",
    "url": "https://openalex.org/W3027353876",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2555382672",
            "name": "Danny Merkx",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2337662889",
            "name": "Stefan L. Frank",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2972818416",
        "https://openalex.org/W2170167574",
        "https://openalex.org/W138474712",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2394729563",
        "https://openalex.org/W2000387713",
        "https://openalex.org/W2888625409",
        "https://openalex.org/W2998527566",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W2973047874",
        "https://openalex.org/W2222142803",
        "https://openalex.org/W2561299349",
        "https://openalex.org/W2119728020",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W2031160462",
        "https://openalex.org/W1996359725",
        "https://openalex.org/W2471933213",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2977268464",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2896991425",
        "https://openalex.org/W2013112874",
        "https://openalex.org/W2987188351",
        "https://openalex.org/W1997161938",
        "https://openalex.org/W2064304610",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2964247056",
        "https://openalex.org/W2009499611",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W2964222268",
        "https://openalex.org/W3134226059",
        "https://openalex.org/W2950178297",
        "https://openalex.org/W2130914914",
        "https://openalex.org/W2064675550"
    ],
    "abstract": "Recurrent neural networks (RNNs) have long been an architecture of interest for computational models of human sentence processing. The more recently introduced Transformer architecture has been shown to outperform recurrent neural networks on many natural language processing tasks but little is known about their ability to model human language processing. It has long been thought that human sentence reading involves something akin to recurrence and so RNNs may still have an advantage over the Transformer as a cognitive model. In this paper we train both Transformer and RNN based language models and compare their performance as a model of human sentence processing. We use the trained language models to compute surprisal values for the stimuli used in several reading experiments and use mixed linear modelling to measure how well the surprisal explains measures of human reading effort. Our analysis shows that the Transformers outperform the RNNs as cognitive models in explaining self-paced reading times and N400 strength but not gaze durations from an eye-tracking experiment.",
    "full_text": "Human Sentence Processing: Recurrence or Attention?\nDanny Merkx\nRadboud University\nNijmegen, The Netherlands\nd.merkx@let.ru.nl\nStefan L. Frank\nRadboud University\nNijmegen, The Netherlands\ns.frank@let.ru.nl\nAbstract\nRecurrent neural networks (RNNs) have long\nbeen an architecture of interest for compu-\ntational models of human sentence process-\ning. The recently introduced Transformer ar-\nchitecture outperforms RNNs on many natural\nlanguage processing tasks but little is known\nabout its ability to model human language pro-\ncessing. We compare Transformer- and RNN-\nbased language models’ ability to account for\nmeasures of human reading effort. Our anal-\nysis shows Transformers to outperform RNNs\nin explaining self-paced reading times and neu-\nral activity during reading English sentences,\nchallenging the widely held idea that human\nsentence processing involves recurrent and im-\nmediate processing and provides evidence for\ncue-based retrieval.\n1 Introduction\nRecurrent Neural Networks (RNNs) are widely\nused in psycholinguistics and Natural Language\nProcessing (NLP). Psycholinguists have looked to\nRNNs as an architecture for modelling human sen-\ntence processing (for a recent review, see Frank\net al., 2019). RNNs have been used to account\nfor the time it takes humans to read the words of a\ntext (Monsalve et al., 2012; Goodkind and Bicknell,\n2018) and the size of the N400 event-related brain\npotential as measured by electroencephalography\n(EEG) during reading (Frank et al., 2015; Rabovsky\net al., 2018; Brouwer et al., 2017; Schwartz and\nMitchell, 2019).\nSimple Recurrent Networks (SRNs; Elman,\n1990) have difﬁculties capturing long-term pat-\nterns. Alternative RNN architectures have been\nproposed that address this issue by adding gating\nmechanisms that control the ﬂow of information\nover time; allowing the networks to weigh old and\nnew inputs and memorise or forget information\nwhen appropriate. The best known of these are\nthe Long Short-Term Memory (LSTM; Hochreiter\nand Schmidhuber, 1997) and Gated Recurrent Unit\n(GRU; Cho et al., 2014) models.\nIn essence, all RNN types process sequential\ninformation by recurrence: Each new input is pro-\ncessed and combined with the current hidden state.\nWhile gated RNNs achieved state-of-the-art results\non NLP tasks such as translation, caption genera-\ntion and speech recognition (Bahdanau et al., 2015;\nXu et al., 2015; Zeyer et al., 2017; Michel and Neu-\nbig, 2018), a recent study comparing SRN, GRU\nand LSTM models’ ability to predict human read-\ning times and N400 amplitudes found no signiﬁcant\ndifferences (Aurnhammer and Frank, 2019).\nUnlike the LSTM and GRU, the recently intro-\nduced Transformer architecture is not simply an\nimproved type of RNN because it does not use re-\ncurrence at all. A Transformer cell as originally\nproposed (Vaswani et al., 2017) consists of self-\nattention layers (Luong et al., 2015) followed by\na linear feed forward layer. In contrast to recur-\nrent processing, self-attention layers are allowed to\n‘attend’ to parts of previous input directly.\nAlthough the Transformer has achieved state-of-\nthe art results on several NLP tasks (Devlin et al.,\n2019; Hayashi et al., 2019; Karita et al., 2019),\nnot much is known about how it fares as a model\nof human sentence processing. The success of\nRNNs in explaining behavioural and neurophysi-\nological data suggests that something akin to re-\ncurrent processing is involved in human sentence\nprocessing. In contrast, the attention operations’\ndirect access to past input, regardless of temporal\ndistance, seems cognitively implausible.\nWe compare how accurately the word sur-\nprisal estimates by Transformer- and GRU-based\nlanguage models (LMs) predict human process-\ning effort as measured by self-paced reading,\neye tracking and EEG. The same human read-\ning data was used by Aurnhammer and Frank\n(2019) to compare RNN types. We believe the\nintroduction of the Transformer merits a simi-\narXiv:2005.09471v2  [cs.CL]  4 May 2021\nlar comparison because the differences between\nTransformers and RNNs are more fundamental\nthan among RNN types. All code used for the\ntraining of the neural networks and the anal-\nysis is available at https://github.com/\nDannyMerkx/next_word_prediction\n2 Background\n2.1 Human Sentence Processing\nWhy are some words more difﬁcult to process than\nothers? It has long been known that more pre-\ndictable words are generally read faster and are\nmore likely to be skipped than less predictable\nwords (Ehrlich and Rayner, 1981). Predictabil-\nity has been formalised as surprisal, which can\nbe derived from LMs. Neural network LMs are\ntrained to predict the next word given all previous\nwords in the sequence. After training, the LM can\nassign a probability to a word: it has an expecta-\ntion of a word w at position t given the preced-\ning words w1, ..., wt−1. The word’s surprisal then\nequals −log P(wt|w1, ..., wt−1).\nHale (2001) and Levy (2008) related surprisal\nto human word processing effort in sentence com-\nprehension. In psycholinguistics, reading times are\ncommonly taken as a measure of word process-\ning difﬁculty, and the positive correlation between\nreading time and surprisal has been ﬁrmly estab-\nlished (Mitchell et al., 2010; Monsalve et al., 2012;\nSmith and Levy, 2013). The N400, a brain poten-\ntial peaking around 400 ms after stimulus onset\nand associated with semantic incongruity (Kutas\nand Hillyard, 1980), has been shown to correlate\nwith word surprisal in both EEG and MEG studies\n(Frank et al., 2015; Wehbe et al., 2014).\nIn this paper, we compare RNN and Transformer-\nbased LMs on their ability to predict reading time\nand N400 amplitude. Likewise, Aurnhammer and\nFrank (2019) compared SRNs, LSTMs and GRUs\non human reading data from three psycholinguistic\nexperiments. Despite the GRU and LSTM gener-\nally outperforming the SRN on NLP tasks, they\nfound no difference in how well the models’ sur-\nprisal predicted self-paced reading, eye-tracking\nand EEG data.\n2.2 Comparing RNN and Transformer\nAccording to (Levy, 2008), surprisal acts as a\n‘causal bottleneck’ in the comprehension process,\nwhich implies that predictions of human processing\ndifﬁculty only depend on the model architecture\nFigure 1: Comparison of sequential information ﬂow\nthrough the Transformer and RNN, trained on next-\nword prediction.\nthrough the estimated word probabilities. Here we\nbrieﬂy highlight the difference in how RNNs and\nTransformers process sequential information. The\nactivation ﬂow through the networks is represented\nin Figure 1.1\nIn an RNN, incoming information is immedi-\nately processed and represented as a hidden state.\nThe next token in the sequence is again immedi-\nately processed and combined with the previous\nhidden state to form a new hidden state. Across\nlayers, each time-step only sees the corresponding\nhidden state from the previous layer in addition\nto the hidden state of the previous time-step, so\nprocessing is immediate and incremental. Infor-\nmation from previous time-steps is encoded in the\nhidden state, which is limited in how much it can\nencode so decay of previous time-steps is implicit\nand difﬁcult to avoid. In contrast, the Transformer’s\nattention layer allows each input to directly receive\ninformation from all previous time-steps.2 This ba-\nsically unlimited memory is a major conceptual dif-\nference with RNNs. Processing is not incremental\nover time: Processing of word wt is not dependent\non hidden states H1 through Ht−1 but on the unpro-\ncessed inputs w1 through wt−1. Consequently, the\nTransformer cannot use implicit order information,\nrather, explicit order information is added to the\ninput.\nHowever, a uni-directional Transformer can also\nuse implicit order information as long as it has\nmultiple layers. Consider H1,3 in the ﬁrst layer\nwhich is based on w1, w2 and w3. Hidden state\n1Note that the ﬁgure only shows how activation is propa-\ngated through time and across layers, not how speciﬁc architec-\ntures compute the hidden states (see Elman (1990); Hochreiter\nand Schmidhuber (1997); Cho et al. (2014); Vaswani et al.\n(2017) for speciﬁcs on the SRN, LSTM, GRU and Trans-\nformer, respectively).\n2Language modelling is trivial if the model also receives\ninformation from future time-steps, as is commonly allowed in\nTransformers. Our Transformer is thus uni-directional, which\nis achieved by applying a simple mask to the input.\nH1,3 does not depend on the order of the previous\ninputs (any order will result in the same hidden\nstate). However, H2,3 depends on H1,1, H1,2 and\nH1,3. If the order of the inputs w1, w2, w3 is dif-\nferent, H1,3 will be the same hidden state but H1,1\nand H1,2 will not, resulting in a different hidden\nstate at H2,3.\nUnlike Transformers, RNNs are inherently se-\nquential, making them seemingly more plausible as\na cognitive model. Christiansen and Chater (2016)\nargue for a ‘now-or-never’ bottleneck in language\nprocessing; incoming inputs need to be rapidly re-\ncoded and passed on for further processing to pre-\nvent interference from the rapidly incoming stream\nof new material. In line with this theory, Futrell\net al. (2020) proposed a model of lossy-context sur-\nprisal based on a lossy representation of memory.\nRecurrent processing, where input is forgotten as\nsoon as it is processed and only available for sub-\nsequent processing through a bounded-size hidden\nstate, is more compatible with these theories than\nthe Transformer’s attention operation.\n3 Methods\nWe train LMs with Transformer and GRU architec-\ntures and compare how well their surprisal explains\nhuman behavioural and neural data. Although\na state-of-the-art pre-trained model can achieve\nhigher LM quality, we opt to train our own models\nfor several reasons. Firstly, the predictive power\nof surprisal increases with language model quality\n(Goodkind and Bicknell, 2018), so to separate the\neffects of LM quality from those of the architectural\ndifferences, the architectures must be compared at\nequal LM capability. We also need to make sure\nboth models have seen the same sentences. Train-\ning our own models gives us control over training\nmaterial, hyper-parameters and LM quality to make\na fair comparison.\nPerhaps most importantly, we test our models\non previously collected human sentence processing\ndata. Most popular large-scale pre-trained models\nuse efﬁcient byte pair encodings as input rather\nthan raw word tokens. This is a useful technique\nfor creating the best possible LM, but also a crucial\nmismatch with how our test material was presented\nto the human subjects. It is not possible to directly\ncompare the surprisal generated on BPEs to whole-\nword measures such as gaze durations and reading\ntimes.\n3.1 Language Model Architectures\nWe ﬁrst trained a GRU model using the same ar-\nchitecture as Aurnhammer and Frank (2019): an\nembedding layer with 400 dimensions per word, a\n500-unit GRU layer, followed by a 400-unit linear\nlayer with a tanh activation function, and ﬁnally\nan output layer with log-softmax activation func-\ntion. All LMs used in this experiment use randomly\ninitialised (i.e., not pre-trained) embedding layers.\nWe implement the Transformer in PyTorch fol-\nlowing Vaswani et al. (2017). To minimise the\ndifferences between the LMs, we picked parame-\nters for the Transformer such that the total number\nof weights is as close as possible to the GRU model.\nWe also make sure the embedding layers for the\nmodels share the same initial weights. The Trans-\nformer model has an embedding layer with 400\ndimensions per word, followed by a single Trans-\nformer layer with 8 attention heads and a fully\nconnected layer with 1024 units, and ﬁnally an\noutput layer with log-softmax activation function.\nThe total number of parameters for our single-layer\nGRU and Transformer models are 9,673,137 and\n9,581,961 respectively.\nWe also train two-layer GRU and Transformer\nmodels. Neural networks tend to increase in ex-\npressiveness with depth (Abnar et al., 2019; Giu-\nlianelli et al., 2018) and a second layer allows\nthe Transformer to use implicit order information,\nas explained above. While results (see Section\n4.2) showed that the two-layer Transformer outper-\nformed the single-layer Transformer in explaining\nthe human reading data, the Transformer did not\nfurther beneﬁt from an increase to four layers so we\ninclude only the single and two layer models. We\ndid not see a performance increase in the two-layer\nGRU over the the single-layer GRU and therefore\ndid not try to further increase its layer depth.\n3.2 Language Model Training\nWe train our LMs on Section 1 of the English Cor-\npora from the Web (ENCOW 2014; Schäfer, 2015),\nconsisting of sentences randomly selected from the\nweb. We ﬁrst exclude word tokens containing nu-\nmerical values or punctuation other than hyphens\nand apostrophes, and treat common contractions\nsuch as ‘don’t’ as a single token. Following Au-\nrnhammer and Frank (2019) we then select the\n10,000 most frequent word types from ENCOW.\n134 word types from the test data (see Section 3.3)\nthat were not covered by these most frequent words\nare added for a ﬁnal vocabulary of 10,134 words.\nWe select the sentences from ENCOW that con-\nsist only of words in the vocabulary and limit the\nsentence length to 39 tokens (the longest sentence\nin the test data). Our training data contains 5.9M\nsentences with a total of 85M tokens.\nThe LMs are trained on a standard next-word pre-\ndiction task with cross-entropy loss. In the Trans-\nformer, we apply a mask to the upper diagonal of\nthe attention matrix such that each position can\nonly attend to itself and previous positions. To ac-\ncount for random effects of weight initialisation\nand data presentation order we train eight LMs of\neach type and share the random seeds between LM\ntypes so each random presentation order and em-\nbedding layer initialisation is present in both LM\ntypes. The LMs were trained for two epochs using\nstochastic gradient descent with a momentum of\n0.9. Initial learning rates (0.02 for the GRU and\n0.005 for the Transformer) were chosen such that\nthe language modelling performance of the GRU\nand Transformer models are as similar as possi-\nble. The learning rate was halved after 1\n3 , 2\n3 , and\nall sentences during the ﬁrst epoch and then kept\nconstant over the second epoch. LMs were trained\non minibatches of ten sentences.\n3.3 Human Reading Data\nWe use the self paced reading (SPR, 54 partici-\npants) and eye-tracking (ET, 35 participants) data\nfrom Frank et al. (2013) and the EEG data (24\nparticipants) from Frank et al. (2015). In these\nexperiments, participants read English sentences\nfrom unpublished novels. In the SPR and EEG\nexperiments, the participants were presented sen-\ntences one word at a time. In the SPR experiment\nthe reading was self paced while in the EEG ex-\nperiment words had a ﬁxed presentation time. In\nthe ET experiment, participants were shown full\nsentences while an eye tracking device monitored\nwhich word was ﬁxated. The SPR stimuli consist\nof 361 sentences, with the EEG and ET stimuli\nbeing a subset of the 205 shortest SPR stimuli. The\nexperimental measures representing processing ef-\nfort of a word are reading time for the SPR data\n(time between key presses), gaze duration for the\nET data (time a word is ﬁxated before the ﬁrst ﬁxa-\ntion on a different word) and N400 amplitude for\nthe EEG data (average amplitude at the centropari-\netal electrodes between 300 and 500 ms after word\nonset; Frank et al., 2015).\nWe exclude from analysis sentence-initial and\n-ﬁnal words, and words directly followed by a\ncomma. From the SPR and ET data we also ex-\nclude the word following a comma, and words\nwith a reading time under 50 ms or over 3500\nms. From the EEG data we exclude datapoints\nthat were marked by Frank et al. (2015) as contain-\ning artifacts. The numbers of data points for SPR,\nET, and EEG were 136,727, 33,001, and 32,417,\nrespectively.\n3.4 Analysis Procedure\nAt 10 different points during training (1K, 3K, 10K,\n30K, 100K, 300K, 1M, 3M sentences and after\nthe ﬁrst and second epoch) we save each LM’s\nparameters and estimate a surprisal value on each\nword of the 361 test sentences.\n3.4.1 Linear Mixed Effects Regression\nFollowing Aurnhammer and Frank (2019), we anal-\nyse how well each set of surprisal values predicts\nthe human sentence processing data using linear\nmixed effects regression (LMER) models with the\nMixedModels package in Julia (Bates et al., 2019).\nFor each datasets (SPR, ET, and EEG) we ﬁt a\nbaseline LMER model which takes into account\nseveral factors known to inﬂuence processing ef-\nfort. The dependent variables for the SPR and ET\nmodels are log-transformed reading time and gaze\nduration, respectively; for the EEG model it is the\nsize of the N400 response. All LMER models in-\nclude log-transformed word frequency (taken from\nSUBTLEXus; Brysbaert and New, 2009), word\nlength (in characters) and the word’s position in the\nsentence as ﬁxed effects.\nSpill-over occurs when processing a word is not\nyet completed when the next word is read (Rayner,\n1998).To account for spill-over in the SPR and ET\ndata we include the previous word’s frequency and\nlength. For the SPR data, we include the previous\nword’s reading time to account for the high corre-\nlation between consecutive words’ reading times.\nFor the EEG data, we include the baseline activ-\nity (average amplitude in the 100 ms before word\nonset). All ﬁxed effects were standardised, and all\nLMER models include two-way interaction effects\nbetween all ﬁxed effects, by-subject and by-item\n(word token) random intercepts, and by-subject\nrandom slopes for all ﬁxed effects.\nAfter ﬁtting the baseline models, we include the\nsurprisal values (for SPR and ET also the previous\nword’s surprisal) as ﬁxed effects, but no new in-\nteractions. For each LMER model with surprisal,\nwe calculate the log-likelihood ratio with its corre-\nsponding baseline model, indicating the decrease\nin model deviance due to adding the surprisal mea-\nsures. The more the surprisal values decrease the\nmodel deviance, the better they predict the human\nreading data. We call this log-likelihood ratio the\ngoodness-of-ﬁt between the surprisal and the data.\nSurprisal from the early stages of training often\nreceived a negative coefﬁcient, contrary to the ex-\npected longer reading times and higher N400 am-\nplitude for higher surprisal. This could be caused\nby collinearity, most likely between surprisal and\nthe log-frequency, which was conﬁrmed by their\nvery high correlation ( > .9) and Variance Inﬂa-\ntion Factors (> 15) (Tomaschek et al., 2018). Ap-\nparently, the neural networks are very sensitive to\nword frequency before they learn to pick up on\nmore complex relations in the data. We indicate af-\nfected goodness-of-ﬁt scores by adding a negative\nsign and excluded these scores from the next stage\nof analysis.\n3.4.2 Generalised Additive Modelling\nAs said before, it is well known that surprisal val-\nues derived from better LMs are a better ﬁt to hu-\nman reading data (Monsalve et al., 2012; Frank\net al., 2015; Goodkind and Bicknell, 2018). We use\ngeneralised additive modelling (GAM) to assess\nwhether the LMs differ in their ability to explain\nthe human reading data at equal language mod-\nelling capability, that is, because of their architec-\ntural differences and not due to being a better LM.\nThe log-likelihood ratios obtained in the LMER\nanalyses are a measure of how well each LM ex-\nplains the human reading data. We use each LM’s\naverage log probability over the datapoints used\nin the LMER analyses as a measure of the LM’s\nlanguage modelling capability. Separate GAMs\nare ﬁt for each of the three datasets, using the R\npackage mgcv by (Wood, 2004). LM type (single-\nlayer GRU, two-layer GRU, etc.) is used as an\nunordered factor so that separate smooths are ﬁt for\neach LM type. Furthermore, we add training repeti-\ntion (i.e., the random training order and embedding\ninitialisation) as a random smooth effect.\n4 Results\n4.1 LM Quality and Goodness-of-Fit\nFigure 2 shows the goodness-of-ﬁt values from the\nLMER models and the smooths ﬁt by the GAMs.\nOverall we see the expected relationship where\nhigher LM quality results in higher goodness of\nﬁt. The LM quality increases monotonically during\ntraining, meaning the clusters seen in the scatter-\nplots correspond to the points during training where\nthe network parameters were stored. The models\ndo seem to reach similar levels of LM quality at\nthe end of training: The average log probability of\nthe best LM (two-layer Transformer) is only 0.17\nhigher than the worst LM (two-layer GRU).\n4.2 GAM Comparisons\nThe bottom row of Figure 2 shows the estimated\ndifferences between the GAM curves in the mid-\ndle row. The two-layer GRU does not seem to im-\nprove over the single-layer GRU. It outperforms the\nsingle-layer GRU only in the early stages of train-\ning on the EEG data, with the single-layer GRU\noutperforming it in the later stages and on the SPR\ndata. The two-layer GRU also reaches lower LM\nquality on all datasets. For the Transformers we\nsee the opposite, with the two-layer Transformer\noutperforming the single-layer Transformer on the\nN400 data at the end of training and never being\noutperformed by its shallower counterpart. The\ntwo-layer Transformer reaches a higher maximum\nLM quality on all datasets.\nFor the comparison between architectures, we\nonly compare the best model of each type, i.e.,\nthe single-layer GRU and two-layer Transformer.\nThe GRU outperforms the Transformer in the early\nstages of training (3K-300K sentences) on the\nN400 data, but the Transformer outperforms the\nGRU at the end of training on both the SPR and\nN400 data. On the gaze duration data, there are\nsome performance differences with the Transform-\ners and GRUs outperforming each other at different\npoints during training but there are no differences\nin the later stages of training.\n4.3 Shorter and Longer Sentences in SPR\nThe SPR data contains a subset of sentences longer\n(in number of characters) than those in the EEG/ET\ndata. As the Transformer has unlimited memory\nof past inputs, the presence of longer sentences\ncould explain why it outperformed the GRU on the\nSPR data. We repeated the analysis of the single-\nand two-layer GRUs and Transformers but only\non those sentences from the SPR data that also\noccurred in the EEG/ET data. On these shorter\nsentences, there are no notable performance differ-\nences between any of the LM architectures (Figure\nFigure 2: Top row: results of the linear mixed effects regression analysis grouped by LM type. These scatter-plots\nshow the resulting goodness-of-ﬁt values plotted against the average log-probability over the included test data.\nNegative goodness-of-ﬁt indicates effects in the unexpected direction. Middle row: smooths resulting from the\nGAMs ﬁtted on the goodness-of-ﬁt data (excluding negative values), with their 95% conﬁdence intervals. Bottom\nrow: estimated differences in goodness-of-ﬁt score. The markings on the x-axis and the vertical lines indicate\nintervals where zero is not within the 95% conﬁdence interval. Each curve represents a comparison between\ntwo models, with an estimated difference above zero meaning the ﬁrst model performed better and vice versa for\ndifferences below zero.\n3). When we test on only those sentences that were\nnot included in the EEG/ET experiments (i.e., the\nlonger sentences), the Transformers outperform the\nGRUs as they did on the complete SPR dataset.\n5 Discussion\nWe trained several language models based on Trans-\nformer and GRU architectures to investigate how\nwell these neural networks account for human read-\ning data. At equal LM quality, the Transformers\ngenerally outperform the GRUs. It seems that their\nattention-based computation allows them to better\nﬁt the self-paced reading and EEG data. This is\nan unexpected result, as we considered the Trans-\nformer’s unlimited memory and access to past in-\nputs implausible given current theories on human\nlanguage processing.\nNotably, the Transformer outperformed the GRU\non the two datasets where sentences were presented\nto participants word by word (SPR and EEG).\nNeurophysiological evidence suggests that natu-\nral (whole sentence) reading places different de-\nmands on the reader than word-by-word reading,\nleading to different encoding and reading strategies\n(Metzner et al., 2015). Metzner et al. speculate\nthat word-by-word reading places greater demand\non working memory, leading to faster retrieval of\npreviously processed material. This seems to be\nsupported by our results; the Transformer has di-\nrect access to previous inputs and hidden states and\nis better at explaining the RT and N400 data from\nthe word-by-word reading experiments. However,\nwhen we split the SPR data by sentence length, the\nresults suggest that the Transformers’ advantage\nis mainly due to performing better on the longer\nSPR sentences. On the other hand, the Transformer\ndid outperform the GRU on the EEG dataset which\ncontains only the shorter subset of sentences. The\nquestion remains whether the Transformer’s unlim-\nited memory is an advantage on longer sentences\nonly, or if it could also explain why it performs bet-\nter on data presented word-by-word. This question\ncould be resolved with new data gathered in experi-\nments where the same set of stimuli is used in SPR\nand EEG. Furthermore, future research could do a\nmore speciﬁc error analysis to identify on which\nsentences the Transformer performs better, and per-\nhaps even on which sentences the GRU performs\nbetter. Such an analysis may reveal the models are\nsensitive to certain linguistic properties allowing\nus to form testable hypotheses.\nSurprisingly, adding a GRU layer did not im-\nprove performance, and even hurt it on reading\ntime data, despite previous research showing that\nincreasing layer depth in RNNs allows them to\ncapture more complex patterns in linguistic data\n(Abnar et al., 2019; Giulianelli et al., 2018). The\nTransformers did show improvement when adding\na second layer but did not improve much with four\nlayers. As explained in Section 2, a single-layer\nTransformer cannot make use of implicit order in-\nformation in the sequence. When adding a single\nlayer to our Transformer, the second layer operates\nno longer on raw input embeddings but on contex-\ntualised hidden states allowing the model to utilise\nimplicit input order information. Further layers in-\ncrease the complexity of the model but do not make\nsuch a fundamental difference in how input is pro-\ncessed. In future work we could investigate how\npowerful this implicit order information is, and\nwhether multi-layer Transformer LMs no longer\nrequire the additional explicit order information.\nOur results raise the question how good recur-\nrent models are as models of human sentence pro-\ncessing if they are outperformed by a cognitively\nimplausible model. However, one could also inter-\npret the results in favour of Transformers (and the\nattention mechanism) being plausible as a cogni-\ntive model. While unlimited working memory is\ncertainly implausible, some argue that the capac-\nity of working memory is even smaller than often\nthought (only 2 or 3 items) and that language pro-\ncessing depends on rapid direct-access retrieval of\nitems from storage (McElree, 1998; Lewis et al.,\n2006). Cue based retrieval theory posits that items\nare rapidly retrieved based on how well they match\nthe cue (Parker et al., 2017). This is compatible\nwith the attention mechanism used in Transformers\nwhich, simply put, weighs previous inputs based\non their similarity to the current input. Cue-based\nretrieval models due have a recency bias due to\ndecaying activation of memory representations but\nit is possible to implement a similar mechanism in\nTransformers (Peng et al., 2021).\nInterestingly, Lewis et al. (2006) claims that se-\nrial order information is retrieved too slowly to\nsupport sentence comprehension. However, our\ntwo-layer Transformer outperforms the single layer\nTransformer, presumably due to order information\nimplicitly arising as a natural result from the atten-\ntion operation being performed. The use of serial\norder information could be compatible with cue-\nFigure 3: Top row: the results of the linear mixed effects regression analysis on the SPR data, where the data is split\nby whether the sentences were present in the ET/EEG experiment or not. These scatter-plots show the resulting\ngoodness-of-ﬁt values plotted against the average surprisal over the included test data. Middle row: the smooths\nresulting from the GAMs ﬁtted on the goodness-of-ﬁt data, with their 95% conﬁdence intervals. Bottom row: the\nestimated differences in goodness-of-ﬁt score with intervals where 0 is not withing the 95% conﬁdence interval\nmarked by vertical lines and markers on the x-axis. Each curve represents a comparison between two models, with\nan estimated difference above zero meaning the ﬁrst model performed better and vice versa for differences below\nzero.\nbased retrieval models if the order information can\nnaturally arise from the retrieval operations.\nIn conclusion, we investigated how the Trans-\nformer architecture holds up as a model of human\nsentence processing compared to the GRU. Our\nTransformer LMs are better at explaining the EEG\nand SPR data which contradicts the widely held\nidea that human sentence processing involves recur-\nrent and immediate processing with lossy retrieval\nof previous input and provides evidence for cue-\nbased retrieval in sentence processing.\nAcknowledgements\nThe research presented here was funded by the\nNetherlands Organisation for Scientiﬁc Research\n(NWO) Gravitation Grant 024.001.006 to the Lan-\nguage in Interaction Consortium.\nReferences\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nWillem Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity and stability analysis of\nneural language models and brains. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n191–203, Florence, Italy. Association for Computa-\ntional Linguistics.\nChristoph Aurnhammer and Stefan L Frank. 2019.\nComparing gated and simple recurrent neural net-\nwork architectures as models of human sentence pro-\ncessing. In Proceedings of the 41st Annual Confer-\nence of the Cognitive Science Society, pages 112–\n118.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe 3rd International Conference on Learning Rep-\nresentations, ICLR 2015.\nDouglas Bates, Phillip Alday, Dave Kleinschmidt, José\nBayoán Santiago Calderón, Andreas Noack, Tony\nKelman, Milan Bouchet-Valat, Yakir Luc Gagnon,\nSimon Babayan, Patrick Kofod Mogensen, Morten\nPiibeleht, Michael Hatherly, Elliot Saba, and An-\ntoine Baldassari. 2019. Juliastats/mixedmodels.jl:\nv2.2.0.\nHarm Brouwer, Matthew W. Crocker, Noortje J. Ven-\nhuizen, and John C. J. Hoeks. 2017. A neurocompu-\ntational model of the N400 and the P600 in language\nprocessing. Cognitive Science, 41:1318–1352.\nMarc Brysbaert and Boris New. 2009. Moving be-\nyond Ku ˇcera and Francis: A critical evaluation of\ncurrent word frequency norms and the introduction\nof a new and improved word frequency measure\nfor American English. Behavior Research Methods,\n41(4):977–990.\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1724–\n1734.\nMorten H. Christiansen and Nick Chater. 2016. The\nNow-or-Never bottleneck: A fundamental constraint\non language. Behavioral and Brain Sciences ,\n39:E62.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 17th Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics, pages 4171–4186.\nSusan F. Ehrlich and Keith Rayner. 1981. Contextual\neffects on word perception and eye movements dur-\ning reading. Journal of Verbal Learning and Verbal\nBehavior, 20:641–655.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nStefan L. Frank, Padraic Monaghan, and Chara\nTsoukala. 2019. Neural network models of language\nacquisition and processing. In Peter Hagoort, editor,\nHuman Language: from Genes and Brains to Be-\nhavior, pages 277–291. Cambridge, MA: The MIT\nPress.\nStefan L. Frank, Irene F. Monsalve, Robin L. Thomp-\nson, and Gabriella Vigliocco. 2013. Reading time\ndata for evaluating broad-coverage models of En-\nglish sentence processing. Behavior Research Meth-\nods, 45(4):1182–1190.\nStefan L. Frank, Leun J. Otten, Giulia Galli, and\nGabriella Vigliocco. 2015. The ERP response to the\namount of information conveyed by words in sen-\ntences. Brain and Language, 140:1–11.\nRichard Futrell, Edward Gibson, and Roger P Levy.\n2020. Lossy-Context Surprisal: An Information-\nTheoretic Model of Memory Effects in Sentence Pro-\ncessing. Cognitive Science, 44(3):e12814.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to in-\nvestigate and improve how language models track\nagreement information. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 240–248,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive power of word surprisal for reading times is a\nlinear function of language model quality. In Pro-\nceedings of the 8th Workshop on Cognitive Modeling\nand Computational Linguistics (CMCL 2018), pages\n10–18.\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Proceedings of the Second\nMeeting of the North American Chapter of the Asso-\nciation for Computational Linguistics, pages 1–8.\nHiroaki Hayashi, Yusuke Oda, Alexandra Birch, Ioan-\nnis Konstas, Andrew Finch, Minh-Thang Luong,\nGraham Neubig, and Katsuhito Sudoh. 2019. Find-\nings of the third workshop on neural generation and\ntranslation. In Proceedings of the 3rd Workshop on\nNeural Generation and Translation, pages 1–14.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8).\nShigeki Karita, Nanxin Chen, Tomoki Hayashi,\nTakaaki Hori, Hirofumi Inaguma, Ziyan Jiang,\nMasao Someki, Nelson E. Y . Soplin, Ryuichi Ya-\nmamoto, Xiaofei Wang, Shinji Watanabe, Takenori\nYoshimura, and Wangyou Zhang. 2019. A compar-\native study on transformer vs rnn in speech applica-\ntions. In 2019 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU), pages 449–\n456.\nMarta Kutas and Steven A. Hillyard. 1980. Reading\nsenseless sentences: brain potentials reﬂect seman-\ntic incongruity. Science, 207(11):203–206.\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nR.L. Lewis, S. Vasishth, and J. A. Van Dyke. 2006.\nComputational principles of working memory in sen-\ntence comprehension. Trends in Cognitive Science,\n10:447–454.\nMinh-Thang Luong, Hieu Pham, and Christopher D.\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proceedings of\nthe 2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1412–1421.\nBrian McElree. 1998. Attended and non-attended\nstates in working memory: Accessing categorized\nstructures. Journal of Memory and Language ,\n38(2):225–252.\nPaul Metzner, Titus von der Malsburg, Shravan Va-\nsishth, and Frank Rösler. 2015. Brain responses\nto world knowledge violations: A comparison of\nstimulus- and ﬁxation-triggered event-related poten-\ntials and neural oscillations. Journal of Cognitive\nNeuroscience, 27(5):1–10.\nPaul Michel and Graham Neubig. 2018. MTNT: A\ntestbed for machine translation of noisy text. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 543–\n553. Association for Computational Linguistics.\nJ. Mitchell, M. Lapata, V . Demberg, and F. Keller. 2010.\nSyntactic and semantic factors in processing difﬁ-\nculty: An integrated measure. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, page 196–206.\nIrene F. Monsalve, Stefan L. Frank, and Gabriella\nVigliocco. 2012. Lexical surprisal as a general pre-\ndictor of reading time. In Proceedings of the 13th\nConference of the European Chapter of the Asso-\nciation for Computational Linguistics, Proceedings,\npages 398–408.\nD. Parker, M. Shvartsman, and J. A. Van Dyke. 2017.\nLanguage processing and disorders, chapter The\ncue-based retrieval theory of sentence comprehen-\nsion: New ﬁndings and new challenges. Cambridge\nScholars Publishing.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In The Ninth Interna-\ntional Conference on Learning Representations.\nMilena Rabovsky, Steven S. Hansen, and James L. Mc-\nClelland. 2018. Modelling the n400 brain potential\nas change in a probabilistic representation of mean-\ning. Nature Human Behaviour, 2:693–705.\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological Bulletin, 124(3):372–422.\nRoland Schäfer. 2015. Processing and querying large\nweb corpora with the COW14 architecture. In Pro-\nceedings of the 3rd Workshop on the Challenges in\nthe Management of Large Corpora, pages 28–34.\nDan Schwartz and Tom Mitchell. 2019. Understand-\ning language-elicited EEG data by predicting it from\na ﬁne-tuned language model. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nand Short Papers), pages 43–57, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nNathaniel J. Smith and Roger Levy. 2013. The effect\nof word predictability on reading time is logarithmic.\nCognition, 128:302–319.\nFabian Tomaschek, Peter Hendrix, and R. Harald\nBaayen. 2018. Strategies for addressing collinearity\nin multivariate linguistic data. Journal of Phonetics,\n71:249–267.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aiden N. Gomes, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st Conference\non Neural Information Processing Systems, pages\n6000–6010.\nL. Wehbe, A. Vaswani, K. Knight, and T. Mitchell.\n2014. Aligning context-based statistical models of\nlanguage with brain activity during reading. In Pro-\nceedings of the 2014 Conference on Empirical Meth-\nods in Natural Language Processing, pages 233–\n243.\nS. N. Wood. 2004. Stable and efﬁcient multiple\nsmoothing parameter estimation for generalized ad-\nditive models. Journal of the American Statistical\nAssociation, 99(467):673–686.\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun\nCho, Aaron Courville, Ruslan Salakhutdinov,\nRichard S. Zemel, and Yoshua Bengio. 2015. Show,\nattend and tell: Neural image caption generation\nwith visual attention. In Proceedings of the 32nd\nInternational Conference on Machine Learning, vol-\nume 37, pages 169–176.\nAlbert Zeyer, Patrick Doetsch, Paul V oigtlaender, Ralf\nSchluter, and Hermann Ney. 2017. A comprehen-\nsive study of deep bidirectional lstm rnns for acous-\ntic modeling in speech recognition. In Proceed-\nings of the 2017 IEEE International Conference\non Acoustics, Speech and Signal Processing, pages\n2462–2466."
}