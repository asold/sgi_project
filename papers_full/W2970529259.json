{
    "title": "75 Languages, 1 Model: Parsing Universal Dependencies Universally",
    "url": "https://openalex.org/W2970529259",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2886203835",
            "name": "Dan Kondratyuk",
            "affiliations": [
                "Saarland University",
                "Charles University"
            ]
        },
        {
            "id": "https://openalex.org/A2280391721",
            "name": "Milan Straka",
            "affiliations": [
                "Charles University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963571341",
        "https://openalex.org/W1970026646",
        "https://openalex.org/W2251386579",
        "https://openalex.org/W2964084097",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2950342398",
        "https://openalex.org/W2973088264",
        "https://openalex.org/W2964105617",
        "https://openalex.org/W2934032625",
        "https://openalex.org/W2152691628",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4288631803",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2969873034",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2889077436",
        "https://openalex.org/W2952768586",
        "https://openalex.org/W2988304195",
        "https://openalex.org/W2804387108",
        "https://openalex.org/W2552110825",
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2101609803",
        "https://openalex.org/W2250473257",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2550821151",
        "https://openalex.org/W2963967800",
        "https://openalex.org/W2115774663",
        "https://openalex.org/W2089722833",
        "https://openalex.org/W2963714641",
        "https://openalex.org/W1595487059",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963223306",
        "https://openalex.org/W2739827909",
        "https://openalex.org/W2915774325",
        "https://openalex.org/W2970854433",
        "https://openalex.org/W2898879711",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2891602716",
        "https://openalex.org/W2251324968",
        "https://openalex.org/W2740840489",
        "https://openalex.org/W1860935423",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2963643701",
        "https://openalex.org/W2915128308",
        "https://openalex.org/W2962739339"
    ],
    "abstract": "Dan Kondratyuk, Milan Straka. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
    "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 2779–2795,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n2779\n75 Languages, 1 Model: Parsing Universal Dependencies Universally\nDan Kondratyuk1,2 and Milan Straka1\n1Charles University, Institute of Formal and Applied Linguistics\n2Saarland University, Department of Computational Linguistics\ndankondratyuk@gmail.com, straka@ufal.mff.cuni.cz\nAbstract\nWe present UDify, a multilingual multi-task\nmodel capable of accurately predicting uni-\nversal part-of-speech, morphological features,\nlemmas, and dependency trees simultaneously\nfor all 124 Universal Dependencies treebanks\nacross 75 languages. By leveraging a multi-\nlingual BERT self-attention model pretrained\non 104 languages, we found that ﬁne-tuning\nit on all datasets concatenated together with\nsimple softmax classiﬁers for each UD task\ncan meet or exceed state-of-the-art UPOS,\nUFeats, Lemmas, (and especially) UAS, and\nLAS scores, without requiring any recurrent\nor language-speciﬁc components. We evaluate\nUDify for multilingual learning, showing that\nlow-resource languages beneﬁt the most from\ncross-linguistic annotations. We also evalu-\nate for zero-shot learning, with results suggest-\ning that multilingual training provides strong\nUD predictions even for languages that nei-\nther UDify nor BERT have ever been trained\non. Code for UDify is available athttps://\ngithub.com/hyperparticle/udify.\n1 Introduction\nIn the absence of annotated data for a given lan-\nguage, it can be considerably difﬁcult to create\nmodels that can parse the language’s text accu-\nrately. Multilingual modeling presents an attrac-\ntive way to circumvent this low-resource limita-\ntion. In a similar way learning a new language\ncan enhance the proﬁciency of a speaker’s previ-\nous languages (Abu-Rabia and Sanitsky, 2010), a\nmodel which has access to multilingual informa-\ntion can begin to learn generalizations across lan-\nguages that would not have been possible through\nmonolingual data alone. Works such as McDonald\net al. (2011); Naseem et al. (2012); Duong et al.\n(2015); Ammar et al. (2016); de Lhoneux et al.\n(2018); Kitaev and Klein (2018); Mulcaire et al.\n(2019) consistently demonstrate how pairing the\n“The best optimizer is grad student descent”\nThe best op ##timi ##zer is grad studentdescent\n...\nBERT\nx→x (optimizer)\nLayer Attention\n4 (is)\nnsubj\nNumber=Sing\nNOUN\nDependency Head\nDependency Tag\nLemma\nUFeats\nUPOS\nFigure 1: An illustration of the UDify network ar-\nchitecture with task-speciﬁc layer attention, inputting\nword tokens and outputting UD annotations for each\ntoken.\ntraining data of similar languages can boost eval-\nuation scores of models predicting syntactic infor-\nmation like part-of-speech and dependency trees.\nMultilinguality not only can improve a model’s\nevaluation performance, but can also reduce the\ncost of training multiple models for a collection\nof languages (Johnson et al., 2017; Smith et al.,\n2018).\nHowever, scaling to a higher number of lan-\nguages can often be problematic. Without an\nample supply of training data for the considered\nlanguages, it can be difﬁcult to form appropri-\nate generalizations and especially difﬁcult if those\n2780\nlanguages are distant from each other. But re-\ncent techniques in language model pretraining\ncan proﬁt from a drastically larger supply of un-\nsupervised text, demonstrating the capability of\ntransferring contextual sentence-level knowledge\nto boost the parsing accuracy of existing NLP\nmodels (Howard and Ruder, 2018; Peters et al.,\n2018; Devlin et al., 2018).\nOne such model, BERT (Devlin et al., 2018),\nintroduces a self-attention (Transformer) network\nthat results in state-of-the-art parsing performance\nwhen ﬁne-tuning its contextual embeddings. And\nwith the release of a multilingual version pre-\ntrained on the entirety of the top 104 resourced\nlanguages of Wikipedia, BERT is remarkably ca-\npable of capturing an enormous collection of\ncross-lingual syntactic information. Conveniently,\nthese languages nearly completely overlap with\nlanguages supported by the Universal Dependen-\ncies treebanks, which we will use to demonstrate\nthe ability to scale syntactic parsing up to 75 lan-\nguages and beyond.\nThe Universal Dependencies (UD) framework\nprovides syntactic annotations consistent across a\nlarge collection of languages (Nivre et al., 2018;\nZeman et al., 2018). This makes it an excel-\nlent candidate for analyzing syntactic knowledge\ntransfer across multiple languages. UD offers\ntokenized sentences with annotations ideal for\nmulti-task learning, including lemmas (LEMMAS ),\ntreebank-speciﬁc part-of-speech tags (XPOS),\nuniversal part-of-speech tags (UPOS), morpho-\nlogical features (UFEATS), and dependency edges\nand labels (DEPS ) for each sentence.\nWe propose UDify, a semi-supervised multi-\ntask self-attention model automatically producing\nUD annotations in any of the supported UD lan-\nguages. To accomplish this, we perform the fol-\nlowing:\n1. We input all sentences into a pretrained\nmultilingual BERT network to produce con-\ntextual embeddings, introduce task-speciﬁc\nlayer-wise attention similar to ELMo (Peters\net al., 2018), and decode each UD task simul-\ntaneously using softmax classiﬁers.\n2. We apply a heavy amount of regularization\nto BERT, including input masking, increased\ndropout, weight freezing, discriminative ﬁne-\ntuning, and layer dropout.\n3. We train and ﬁne-tune the model on the en-\ntirety of UD by concatenating all available\ntraining sets together.\nWe evaluate our model with respect to UDPipe\nFuture, one of the winners of the CoNLL 2018\nShared Task on Multilingual Parsing from Raw\nText to Universal Dependencies (Straka, 2018; Ze-\nman et al., 2018). In addition, we analyze lan-\nguages that multilingual training beneﬁts predic-\ntion the most, and evaluate the model for zero-\nshot learning, i.e., treebanks which do not have\na training set. Finally, we provide evidence from\nour experiments and other related work to help ex-\nplain why pretrained self-attention networks excel\nin multilingual dependency parsing.\nOur work uses the AllenNLP library built\nfor the PyTorch framework. Code for UD-\nify and a release of the ﬁne-tuned BERT\nweights are available at https://github.\ncom/hyperparticle/udify.\n2 Multilingual Multi-Task Learning\nIn this section, we detail the multilingual training\nsetup and the UDify multi-task model architecture.\nSee Figure 1 for an architecture diagram.\n2.1 Multilingual Pretraining with BERT\nWe leverage the provided BERT base multilingual\ncased pretrained model1, with a self-attention net-\nwork of 12 layers, 12 attention heads per layer, and\nhidden dimensions of 768 (Devlin et al., 2018).\nThe model was trained by predicting randomly\nmasked input words on the entirety of the top 104\nlanguages with the largest Wikipedias. BERT uses\na wordpiece tokenizer (Wu et al., 2016), which\nsegments all text into (unnormalized) sub-word\nunits.\n2.2 Cross-Linguistic Training Issues\nTable 1 displays a list of vocabulary sizes, indicat-\ning that UD treebanks possess nearly 1.6M unique\ntokens combined. To sidestep the problem of a\nballooning vocabulary, we use BERT’s wordpiece\ntokenizer directly for all inputs. UD expects pre-\ndictions to be along word boundaries, so we take\nthe simple approach of applying the tokenizer to\neach word using UD’s provided segmentation. For\nprediction, we use the outputs of BERT corre-\nsponding to the ﬁrst wordpiece per word, ignoring\n1https://github.com/google-research/\nbert/blob/master/multilingual.md\n2781\nTOKEN VOCAB SIZE\nWord Form 1,588,655\nBERT Wordpieces 119,547\nUPOS 17\nXPOS 19,126\nUFeats 23,974\nLemmas (tags) 109,639\nDeps 251\nTable 1: V ocabulary sizes of words and tags over all of\nUD v2.3, with a total of 12,032,309 word tokens and\n668,939 sentences.\nthe rest2.\nIn addition, the XPOS annotations are not uni-\nversal across languages, or even across treebanks.\nBecause each treebank can possess a different an-\nnotation scheme for XPOS which can slow down\ninference, we omit training and evaluation of\nXPOS from our experiments.\n2.3 Multi-Task Learning with UD\nFor predicting UD annotations, we employ a\nmulti-task network based on UDPipe Future\n(Straka, 2018), but with all embedding, encoder,\nand projection layers replaced with BERT. The re-\nmaining components include the prediction layers\nfor each task detailed below, and layer attention\n(see Section 3.1). Then we compute softmax cross\nentropy loss on the output logits to train the net-\nwork. For more details on reasons behind archi-\ntecture choices, see Appendix A.\nUPOS As is standard for neural sequence tag-\nging, we apply a softmax layer along each word\ninput, computing a probability distribution over\nthe tag vocabulary to predict the annotation string.\nUFeats Identical to UPOS prediction, we\ntreat each UFeats string as a separate token in\nthe vocabulary. We found this to produce higher\nevaluation accuracy than predicting each morpho-\nlogical feature separately. Only a small subset\nof the full Cartesian product of morphological\nfeatures is valid, eliminating invalid combinations.\nLemmas Similar to Chrupała (2006); M ¨uller\net al. (2015), we reduce the problem of lemmatiza-\ntion to a sequence tagging problem by predicting a\nclass representing an edit script, i.e., the sequence\nof character operations to transform the word form\nto the lemma. To precompute the tags, we ﬁrst ﬁnd\n2We found last, max, or average pooling of the word-\npieces were not any better or worse for evaluation. Kitaev\nand Klein (2018) report similar results.\nthe longest common substring between the form\nand the lemma, and then compute the shortest\nedit script converting the preﬁx and sufﬁx of the\nform into the preﬁx and sufﬁx of the lemma using\nthe Wagner–Fischer algorithm (Wagner and Fis-\ncher, 1974). Upon predicting a lemma edit script,\nwe apply the edit operations to the word form to\nproduce the ﬁnal lemma. See also Straka (2018)\nfor more details. We chose this approach over a\nsequence-to-sequence architecture like Bergmanis\nand Goldwater (2018) or Kondratyuk et al. (2018),\nas this signiﬁcantly reduces training efﬁciency.\nDeps We use the graph-based biafﬁne attention\nparser developed by Dozat and Manning (2016);\nDozat et al. (2017), replacing the bidirectional\nLSTM layers with BERT. The ﬁnal embeddings\nare projected through arc-head and arc-dep feed-\nforward layers, which are combined using biafﬁne\nattention to produce a probability distribution of\narc heads for each word. We then decode each tree\nwith the Chu-Liu/Edmonds algorithm (Chu, 1965;\nEdmonds, 1967).\n3 Fine-Tuning BERT on UD Annotations\nWe employ several strategies for ﬁne-tuning\nBERT for UD prediction, ﬁnding that regulariza-\ntion is absolutely crucial for producing a high-\nscoring network.\n3.1 Layer Attention\nEmpirical results suggest that when ﬁne-tuning\nBERT, combining the output of the last several\nlayers is more beneﬁcial for the downstream tasks\nthan just using the last layer (Devlin et al., 2018).\nInstead of restricting the model to any subset of\nlayers, we devise a simple layer-wise dot-product\nattention where the network computes a weighted\nsum of all intermediate outputs of the 12 BERT\nlayers using the same weights for each token. This\nis similar to how ELMo mixes the output of mul-\ntiple recurrent layers (Peters et al., 2018).\nMore formally, let wi be a trainable scalar for\nBERT embeddings BERTij at layer iwith a to-\nken at positionj, and letcbe a trainable scalar. We\ncompute contextual embeddings e(task) such that\ne(task)\nj = c\n∑\ni\nBERTij ·softmax(w)i (1)\nTo prevent the UD classiﬁers from overﬁtting\nto the information in any single layer, we devise\n2782\nlayer dropout, where at each training step, we set\neach parameter wi to −∞with probability 0.1.\nThis effectively redistributes probability mass to\nall other layers, forcing the network to incorporate\nthe information content of all BERT layers. We\ncompute layer attention per task, using one set of\nc,w parameters for each of UPOS, UFeats, Lem-\nmas, and Deps.\n3.2 Transfer Learning with ULMFiT\nThe ULMFiT strategy deﬁnes several useful meth-\nods for ﬁne-tuning a network on a pretrained lan-\nguage model (Howard and Ruder, 2018). We ap-\nply the same methods, with a few minor modiﬁca-\ntions.\nWe split the network into two parameter groups,\ni.e., the parameters of BERT and all other param-\neters. We apply discriminative ﬁne-tuning, setting\nthe base learning rate of BERT to be 5e−5 and\n1e−3 everywhere else. We also freeze the BERT\nparameters for the ﬁrst epoch to increase training\nstability.\nWhile ULMFiT recommends decaying the\nlearning rate linearly after a linear warmup, we\nfound that this is prone to training divergence\nin self-attention networks, introducing vanishing\ngradients and underﬁtting. Instead, we apply an\ninverse square root learning rate decay with lin-\near warmup (Noam) seen in training Transformer\nnetworks for machine translation (Vaswani et al.,\n2017).\n3.3 Input Masking\nThe authors of BERT recommend not to mask\nwords randomly with [MASK] when ﬁne-tuning\nthe network. However, we discovered that mask-\ning often reduces the tendency of the classiﬁers\nto overﬁt to BERT by forcing the network to rely\non the context of surrounding words. This word\ndropout strategy has been observed in other works\nshowing improved test performance on a variety\nof NLP tasks (Iyyer et al., 2015; Bowman et al.,\n2016; Clark et al., 2018; Straka, 2018).\n4 Experiments\nWe evaluate UDify with respect to every test set\nin each treebank. As there are too many results\nto ﬁt within one page, we display a salient subset\nof scores and compare them with UDPipe Future.\nThe full results are listed in Appendix A.\nWe do not directly reference metrics from other\nmodels in the CoNLL 2018 Shared Task, as the\ntables of results do not assume gold word segmen-\ntation and may not provide a fair comparison. In-\nstead, we retrained the open source UDPipe Fu-\nture model using gold segmentation and report re-\nsults here due to its architectural similarity to UD-\nify and its strong performance.\nNote that the UDPipe Future baseline does not\nitself use BERT. Evaluation of BERT utilization\nin UDPipe Future can be found in Straka et al.\n(2019).\n4.1 Datasets\nFor all experiments, we use the full Universal\nDependencies v2.3 corpus available on LINDAT\n(Nivre et al., 2018). We omit the evaluation of\ndatasets that do not have their training annotations\nfreely available, i.e., Arabic NYUAD (ar nyuad),\nEnglish ESL (en esl), French FTB (fr ftb), Hindi\nEnglish HEINCS (qhe heincs), and Japanese BC-\nCWJ (ja bccwj).\nTo train the multilingual model, we concatenate\nall available training sets together, similar to Mc-\nDonald et al. (2011). Before each epoch, we shuf-\nﬂe all sentences and feed mixed batches of sen-\ntences to the network, where each batch may con-\ntain sentences from any language or treebank, for\na total of 80 epochs3.\n4.2 Hyperparameters\nA summary of hyperparameters can be found in\nTable 6 in Appendix A.1.\n4.3 Probing for Syntax\nHewitt and Manning (2019) introduce a structural\nprobe for identifying dependency structures in\ncontextualized word embeddings. This probe eval-\nuates whether syntax trees (i.e., unlabeled undi-\nrected dependency trees) can be easily extracted as\na global property of the embedding space using a\nlinear transformation of the network’s contextual\nword embeddings. The probe trains a weighted\nadjacency matrix on the layers of contextual em-\nbeddings produced by BERT, identifying a lin-\near transformation where squared L2 distance be-\ntween embedding vectors encodes the distance be-\ntween words in the parse tree. Edges are decoded\nby computing the minimum spanning tree on the\nweight matrix (the lowest sum of edge distances).\n3 We train on a GTX 1080 Ti for approximately 25 days.\nSee Appendix A.1 for more details\n2783\nTREEBANK MODEL UPOS F EATS LEM UAS LAS\nCzech PDT\n(cs pdt)\nUDPipe 99.18 97.23 99.02 93.33 91.31\nLang 99.18 96.87 98.72 94.35 92.41\nUDify 99.18 96.85 98.56 94.73 92.88\nUDify+Lang 99.24 97.44 98.93 95.07 93.38\nGerman GSD\n(de gsd)\nUDPipe 94.48 90.68 96.80 85.53 81.07\nLang 94.77 91.73 96.34 87.54 83.39\nUDify 94.55 90.65 94.82 87.81 83.59\nUDify+Lang 95.29 91.94 96.74 88.11 84.13\nEnglish EWT\n(en ewt )\nUDPipe 96.29 97.10 98.25 89.63 86.97\nLang 96.82 97.27 97.97 91.70 89.38\nUDify 96.21 96.17 97.35 90.96 88.50\nUDify+Lang 96.57 96.96 97.90 91.55 89.06\nSpanish AnCora\n(es ancora)\nUDPipe 98.91 98.49 99.17 92.34 90.26\nLang 98.60 98.14 98.52 92.82 90.52\nUDify 98.53 97.84 98.09 92.99 90.50\nUDify+Lang 98.68 98.25 98.68 93.35 91.28\nFrench GSD\n(fr gsd)\nUDPipe 97.63 97.13 98.35 90.65 88.06\nLang 98.05 96.26 97.96 92.77 90.61\nUDify 97.83 96.59 97.48 93.60 91.45\nUDify+Lang 97.96 96.73 98.17 93.56 91.45\nRussian\nSynTagRus\n(ru syntagrus)\nUDPipe 99.12 97.57 98.53 93.80 92.32\nLang 98.90 96.58 95.16 94.40 92.72\nUDify 98.97 96.35 94.43 94.83 93.13\nUDify+Lang 99.08 97.22 96.58 95.13 93.70\nBelarusian HSE\n(be hse)\nUDPipe 93.63 73.30 87.34 78.58 72.72\nLang 95.88 76.12 84.52 83.94 79.02\nUDify 97.54 89.36 85.46 91.82 87.19\nUDify+Lang 97.25 85.02 88.71 90.67 86.98\nBuryat BDT\n(bxr bdt)\nUDPipe 40.34 32.40 58.17 32.60 18.83\nLang 52.54 37.03 54.64 29.63 15.82\nUDify 61.73 47.86 61.06 48.43 26.28\nUDify+Lang 61.73 42.79 58.20 33.06 18.65\nUpper Sorbian\nUFAL\n(hsb ufal)\nUDPipe 62.93 41.10 68.68 45.58 34.54\nLang 73.70 46.28 58.02 39.02 28.70\nUDify 84.87 48.63 72.73 71.55 62.82\nUDify+Lang 87.58 53.19 71.88 71.40 60.65\nKazakh KTB\n(kk ktb)\nUDPipe 55.84 40.40 63.96 53.30 33.38\nLang 73.52 46.60 57.84 50.38 32.61\nUDify 85.59 65.14 77.40 74.77 63.66\nUDify+Lang 81.32 60.50 67.30 69.16 53.14\nLithuanian HSE\n(lt hse)\nUDPipe 81.70 60.47 76.89 51.98 42.17\nLang 83.40 54.34 58.77 51.23 38.96\nUDify 90.47 68.96 67.83 79.06 69.34\nUDify+Lang 84.53 56.98 58.21 58.40 39.91\nTable 2: Test set scores for a subset of high-\nresource (top) and low-resource (bottom) languages\nin comparison to UDPipe Future without BERT, with\n3 UDify conﬁgurations: Lang, ﬁne-tune on the tree-\nbank. UDify, ﬁne-tune on all UD treebanks combined.\nUDify+Lang, ﬁne-tune on the treebank using BERT\nweights saved from ﬁne-tuning on all UD treebanks\ncombined.\nWe train the structural probe on unmodiﬁed\nand ﬁne-tuned BERT using the default hyperpa-\nrameters of Hewitt and Manning (2019) to evalu-\nMODEL CONFIGURATION UPOS F EATS LEM UAS LAS\nUDPipe w/o BERT 93.76 91.04 94.63 84.37 79.76\nUDify Task Layer Attn 93.40 88.72 90.41 85.69 80.43\nUDify Global Layer Attn 93.12 87.53 89.03 85.07 79.49\nUDify Sum Layers 93.02 87.20 88.70 84.97 79.33\nTable 3: Ablation comparing the average of scores\nover all treebanks: task-speciﬁc layer attention (4 sets\nof c,w computed for the 4 UD tasks), global layer at-\ntention (one set of c,w for all tasks), and simple sum\nof layers (c= 1and w = ).\nTREEBANK UPOS F EATS LEM UAS LAS\nBreton KEB br keb 63.67 46.75 53.15 63.97 40.19\nTagalog TRG tl trg 61.64 35.27 75.00 64.73 39.38\nFaroese OFT fo oft 77.86 35.71 53.82 69.28 61.03\nNaija NSC pcm nsc 56.59 52.75 97.52 47.13 33.43\nSanskrit UFAL sa ufal 40.21 18.45 37.60 41.73 19.80\nTable 4: Test set results for zero-shot learning, i.e., no\nUD training annotations available. Languages that are\npretrained with BERT are bolded.\nTREEBANK MODEL UUAS\nEnglish EWT (en ewt) BERT 65.48\nBERT+ﬁnetune en ewt 79.67\nTable 5: UUAS test scores calculated on the predic-\ntions produced by the syntactic structural probe (Hewitt\nand Manning, 2019) using the English EWT treebank,\non the unmodiﬁed multilingual cased BERT model and\nthe same BERT model ﬁne-tuned on the treebank.\nate whether the representations affected by ﬁne-\ntuning BERT on dependency trees would more\nclosely match the structure of these trees.\n5 Results\nWe show scores of UPOS, UFeats (F EATS), and\nLemma (L EM) accuracies, along with unlabeled\nand labeled attachment scores (UAS, LAS) eval-\nuated using the ofﬁcal CoNLL 2018 Shared Task\nevaluation script. 4 Results for a salient subset\nof high-resource and low-resource languages are\nshown in Table 2, with a comparison between\nUDPipe Future and UDify ﬁne-tuning on all lan-\nguages. In addition, the table compares UDify\nwith ﬁne-tuning on either a single language or\nboth languages (ﬁne-tuning multilingually, then\nﬁne-tuning on the language with the saved multi-\nlingual weights) to provide a reference point for\nmultilingual inﬂuences on UDify. We provide\n4https://universaldependencies.org/\nconll18/evaluation.html\n2784\n1 2 3 4 5 6 7 8 9 10 11 12\nBERT Layer #\n#4\n#3\n#2\n#1\n0\n1\nWeight UPOS\nUFeat \nLemma \nDep \nFigure 2: The unnormalized BERT layer attention\nweights wi contributing to layer i for each task after\ntraining. A linear change in weight scales each BERT\nlayer exponentially due to the softmax in Equation 1\na full table of scores for all treebanks in Ap-\npendix A.4.\nA more comprehensive overview is shown in\nTable 3, comparing different attention strategies\napplied to UDify. We display an average of scores\nover all (89) treebanks with a training set. For\nzero-shot learning evaluation, Table 4 displays a\nsubset of test set evaluations of treebanks that\ndo not have a training set, i.e., Breton, Tagalog,\nFaroese, Naija, and Sanskrit. We plot the layer at-\ntention weights w after ﬁne-tuning BERT in Fig-\nure 2, showing a set of weights per task. And Ta-\nble 5 compares the unlabeled undirected attach-\nment scores (UUAS) of dependency trees pro-\nduced using a structural probe on both the un-\nmodiﬁed multilingual cased BERT model and the\nextracted BERT model ﬁne-tuned on the English\nEWT treebank.\n6 Discussion\nIn this section, we discuss the most notable fea-\ntures of the results.\n6.1 Model Performance\nOn average, UDify reveals a strong set of re-\nsults that are comparable in performance with the\nstate-of-the-art in parsing UD annotations. UD-\nify excels in dependency parsing, exceeding UD-\nPipe Future by a large margin especially for low-\nresource languages. UDify slightly underperforms\nwith respect to Lemmas and Universal Features,\nlikely due to UDPipe Future additionally using\ncharacter-level embeddings (Santos and Zadrozny,\n2014; Ling et al., 2015; Ballesteros et al., 2015;\nKim et al., 2016), while (for simplicity) UDify\ndoes not. Additionally, UDify severely under-\nperforms the baseline on a few low-resource lan-\nguages, e.g., cop scriptorum. We surmise that this\nis due to using mixed batches on an unbalanced\ntraining set, which skews the model towards pre-\ndicting larger treebanks more accurately. How-\never, we ﬁnd that ﬁne-tuning on the treebank in-\ndividually with BERT weights saved from UDify\neliminates most of these gaps in performance.\nEchoing results seen in Smith et al. (2018),\nUDify also shows strong improvement leveraging\nmultilingual data from other UD treebanks. In\nlow-resource cases, ﬁne-tuning BERT on all tree-\nbanks can be far superior to ﬁne-tuning mono-\nlingually. A second round of ﬁne-tuning on an\nindividual treebank using UDify’s BERT weights\ncan improve this further, especially for treebanks\nthat underperform the baseline. However, for\nlanguages that are already display strong results,\nwe typically notice worse evaluation performance\nacross all the evaluation metrics. This indi-\ncates that multilingual ﬁne-tuning really is supe-\nrior to single language ﬁne-tuning with respect\nto these high-performing languages, showing im-\nprovements of up to 20% reduction in error.\nInterestingly, Slavic languages tend to perform\nthe best with multilingual training. While lan-\nguages like Czech and Russian possess the largest\nUD treebanks and do not differ as much in perfor-\nmance from monolingual ﬁne-tuning, evidenced\nby the improvements over single-language ﬁne-\ntuning, we can see a large degree of morpho-\nlogical and syntactic structure has transferred to\nlow-resource Slavic languages like Upper Sorbian,\nwhose treebank contains only 646 sentences. But\nthis is not only true of Slavic languages, as the Tur-\nkic language Kazakh (with less than 1,000 training\nsentences) has also improved signiﬁcantly.\nThe zero-shot results indicate that ﬁne-tuning\non BERT can result in reasonably high scores on\nlanguages that do not have a training set. It can\nbe seen that a combination of BERT pretraining\nand multilingual learning can improve predictions\nfor Breton and Tagalog, which implies that the\nnetwork has learned representations of syntax that\ncross lingual boundaries. Furthermore, despite the\nfact that neither BERT nor UDify have directly\nobserved Faroese, Naija, or Sanskrit, we see un-\nusually high performance in these languages. This\ncan be partially attributed to each language closely\nresembling another: Faroese is very close to Ice-\n2785\nMoving ﬁelds to the category and series areas\n.\n. . .. ..\n.. ..\n.\n. .\nMoving ﬁelds to the category and series areas\n.\n. . .. ..\n.. ...\n. .\nWe land and spill out and go our separate ways .\n. . .\n.\n. . . . . .\n.. ... ..\n. .\n.\nBERT\nWe land and spill out and go our separate ways .\n. . .\n.\n. . . . . .\n. .\n.\n. ..\n.\n.. .\nUDify\nFigure 3: Examples of minimum spanning trees produced by the syntactic probe are shown below each sentence,\nevaluated on BERT (left) and on UDify (right). Gold dependency trees are shown above each sentence in black.\nMatched and unmatched spanning tree edges are shown in blue and red respectively.\nlandic, Naija (Nigerian Pidgin) is a variant of En-\nglish, and Sanskrit is an ancient Indian language\nrelated to Greek, Latin, and Hindi.\nTable 3 shows that layer attention on BERT for\neach task is beneﬁcial for test performance, much\nmore than using a global weighted average. In\nfact, Figure 2 shows that each task prefers the lay-\ners of BERT differently, uniquely extracting the\noptimal information for a task. All tasks favor\nthe information content in the last 3 layers, with\na tendency to disprefer layers closer to the in-\nput. However, an interesting observation is that for\nLemmas and UFeats, the classiﬁer prefers to also\nincorporate the information of the ﬁrst 3 layers.\nThis meshes well with the linguistic intuition that\nmorphological features are more closely related to\nthe surface form of a word and rely less on con-\ntext than other syntactic tasks. Curiously enough,\nthe middle layers are highly dispreferred, mean-\ning that the most useful processing for multilin-\ngual syntax (tagging, dependency parsing) occurs\nin the last 3-4 layers. The results released by Ten-\nney et al. (2019) also agree with the intuition be-\nhind the weight distribution above, showing how\nthe different layers of BERT generate hierarchical\ninformation like a traditional NLP pipeline, start-\ning with low-level syntax (e.g., POS tagging) and\nbuilding up to high-level syntactic and semantic\ndependency parsing.\n6.2 Effect of Syntactic Fine-Tuning on BERT\nEven without any supervised training, BERT en-\ncodes its syntax in the embedding’s distance close\nto human-annotated dependencies. But more no-\ntably, the results in Table 5 show that ﬁne-tuning\nBERT on Universal Dependencies signiﬁcantly\nboosts UUAS scores when compared to the gold\ndependency trees, an error reduction of 41%.\nThis indicates that the self-attention weights have\nlearned a linearly-transformable representation of\nits vectors more closely resembling annotated de-\npendency trees deﬁned by linguists. Even with just\nunsupervised pretraining, a global structural prop-\nerty of the vector space of the BERT weights al-\nready produces a decent representation of the de-\npendency tree in the squared L2 distance. Fol-\nlowing this, it should be no surprise that train-\ning with a non-linear graph-based dependency de-\ncoder would produce even higher quality depen-\ndency trees.\n6.3 Attention Visualization\nWe performed a high-level visual analysis of\nthe BERT attention weights to see if they have\nchanged on any discernible level. Our obser-\nvations reveal something notable: the attention\nweights tend to be more sparse, and are more of-\nten sensitive to constituent boundaries like clauses\nand prepositions. Figure 4 illustrates this point,\nshowing the attention weights of a particular atten-\ntion head on an example sentence. We ﬁnd similar\nbehavior in 13 additional attention heads for the\nprovided example sentence.\nWe see that some of the attention structure re-\nmains after ﬁne-tuning. Previously, the atten-\ntion head was mostly sensitive to previous words\nand punctuation. But after ﬁne-tuning, it demon-\nstrates more ﬁne-grained attention towards imme-\ndiate wordpieces, prepositions, articles, and adjec-\ntives. We found similar evidence in other atten-\ntion heads, which implies that ﬁne-tuning on UD\nproduces attention that more closely resembles lo-\ncalized dependencies within constituents. We also\nﬁnd that BERT base heavily preferred to attend to\npunctuation, while UDify BERT does to a much\nlesser degree.\n2786\nFigure 4: Visualization of BERT attention head 4 at layer 11, comparing the attended words on an English sentence\nbetween BERT base and UDify BERT after ﬁne-tuning. The right column indicates the attended words (keys) with\nrespect to the words in the left column (queries). Darker lines indicate stronger attention weights.\n6.4 Factors that Enable BERT to Excel at\nDependency Parsing and Multilinguality\nGoldberg (2019) assesses the syntactic capabili-\nties of BERT and concludes that BERT is remark-\nably capable of processing syntactic tasks despite\nnot being trained on any supervised data. Con-\nducting similar experiments, Vig (2019) and Sileo\n(2019) visualize the attention heads within each\nBERT layer, showing a number of distinct atten-\ntion patterns, including attending to previous/next\nwords, related words, punctuation, verbs/nouns,\nand coreference dependencies.\nThis neat delegation of certain low-level in-\nformation processing tasks to the attention heads\nhints at why BERT might excel at processing syn-\ntax. We see that from the analysis on BERT ﬁne-\ntuned with syntax using the syntactic probe and at-\ntention visualization, BERT produces a represen-\ntation that keeps constituents close in its vector\nspace, and improves this representation to more\nclosely resemble human annotated dependency\ntrees when ﬁne-tuned on UD as seen in Figure 3.\nFurthermore, Ahmad et al. (2018) provide results\nconsistent with their claim that self-attention net-\nworks can be more robust than recurrent networks\nto the change of word order, observing that self-\nattention networks capture less word order infor-\nmation in their architecture, which is what allows\nthem to generally perform better at cross-lingual\nparsing. Wu and Dredze (2019) also analyze mul-\ntilingual BERT and report that the model retains\nboth language-independent as well as language-\nspeciﬁc information related to each input sentence,\nand that the shared embedding space with the\ninput wordpieces correlates strongly with cross-\nlingual generalization.\nFrom the evidence above, we can see that the\ncombination of strong regularization paired with\nthe ability to capture long-range dependencies\nwith self-attention and contextual pretraining on\nan enormous corpus of raw text are large con-\ntributors that enable robust multilingual modeling\nwith respect to dependency parsing. Pretraining\nself-attention networks introduces a strong syntac-\ntic bias that is capable of generalizing across lan-\nguages. The dependencies seen in the output de-\npendency trees are highly correlated with the im-\nplicit dependencies learned by the self-attention,\nshowing that self-attention is remarkably capable\nof modeling syntax by picking up on common syn-\ntactic patterns in text. The introduction of multi-\nlingual data also shows that these attention heads\nprovide a surprising amount of capacity that do\nnot degrade the performance considerably when\ncompared to monolingual training. E.g., Devlin\net al. (2018) report that the ﬁne-tuning on the mul-\ntilingual BERT model results in a small degrada-\ntion in English ﬁne-tune performance with 104\n2787\npretrained languages compared to an equivalent\nmodel pretrained only on English. This also hints\nthat the BERT model can be compressed signif-\nicantly without compromising heavily on evalua-\ntion performance.\n7 Related Work\nThis work’s main contribution in combining tree-\nbanks for multilingual UD parsing is most simi-\nlar to the Uppsala system for the CoNLL 2018\nShared Task (Smith et al., 2018). Uppsala com-\nbines treebanks of one language or closely related\nlanguages together over 82 treebanks and parses\nall UD annotations in a multi-task pipeline archi-\ntecture for a total of 34 models. This approach\nreduces the number of models required to parse\neach language while also showing results that are\nno worse than training on each treebank individu-\nally, and in especially low-resource cases, signif-\nicantly improved. Combining UD treebanks in a\nlanguage-agnostic way was ﬁrst introduced in Vi-\nlares et al. (2016), which train bilingual parsers on\npairs of UD treebanks, showing similar improve-\nments.\nOther efforts in training multilingual models in-\nclude Johnson et al. (2017), which demonstrate a\nmachine translation model capable of supporting\ntranslation between 12 languages. Recurrent mod-\nels have also shown to be capable of scaling to\na larger number of languages as seen in Artetxe\nand Schwenk (2018), which deﬁne a scalable ap-\nproach to train massively multilingual embeddings\nusing recurrent networks on an auxiliary task, e.g.,\nnatural language inference. Schuster et al. (2019)\nproduce context-independent multilingual embed-\ndings using a novel embedding alignment strat-\negy to allow models to improve the use of cross-\nlingual information, showing improved results in\ndependency parsing.\n8 Conclusion\nWe have proposed and evaluated UDify, a mul-\ntilingual multi-task self-attention network ﬁne-\ntuned on BERT pretrained embeddings, capable of\nproducing annotations for any UD treebank, and\nexceeding the state-of-the-art in UD dependency\nparsing in a large subset of languages while being\ncomparable in tagging and lemmatization accu-\nracy. Strong regularization and task-speciﬁc layer\nattention are highly beneﬁcial for ﬁne-tuning, and\ncoupled with training multilingually, also reduce\nthe number of required models to train down to\none. Multilingual learning is most beneﬁcial for\nlow-resource languages, even ones that do not pos-\nsess a training set, and can be further improved\nby ﬁne-tuning monolingually using BERT weights\nsaved from UDify’s multilingual training. All\nthese results indicate that self-attention networks\nare remarkably capable of capturing syntactic pat-\nterns, and coupled with unsupervised pretraining\nare able to scale to a large number of languages\nwithout degrading performance.\nAcknowledgments\nThe work described herein has been sup-\nported by OP VVV VI LINDAT/CLARIN\nproject of the Ministry of Education, Youth\nand Sports of the Czech Republic (project\nCZ.02.1.01/0.0/0.0/16 013/0001781) and it has\nbeen supported and has been using language\nresources developed by the LINDAT/CLARIN\nproject of the Ministry of Education, Youth\nand Sports of the Czech Republic (project\nLM2015071).\nDan Kondratyuk has been supported by the\nErasmus Mundus program in Language & Com-\nmunication Technologies (LCT), and by the Ger-\nman Federal Ministry of Education and Re-\nsearch (BMBF) through the project DEEPLEE\n(01IW17001).\nReferences\nSalim Abu-Rabia and Ekaterina Sanitsky. 2010. Ad-\nvantages of bilinguals over monolinguals in learn-\ning a third language. Bilingual Research Journal ,\n33(2):173–199.\nWasi Uddin Ahmad, Zhisong Zhang, Xuezhe Ma,\nEduard Hovy, Kai-Wei Chang, and Nanyun Peng.\n2018. On difﬁculties of cross-lingual transfer with\norder differences: A case study on dependency pars-\ning. arXiv preprint arXiv:1811.00570.\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A Smith. 2016. Many lan-\nguages, one parser. Transactions of the Association\nfor Computational Linguistics, 4:431–444.\nMikel Artetxe and Holger Schwenk. 2018. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. arXiv\npreprint arXiv:1812.10464.\nMiguel Ballesteros, Chris Dyer, and Noah A Smith.\n2015. Improved transition-based parsing by model-\ning characters instead of words with lstms. In Pro-\nceedings of the 2015 Conference on Empirical Meth-\n2788\nods in Natural Language Processing , pages 349–\n359.\nToms Bergmanis and Sharon Goldwater. 2018. Con-\ntext sensitive neural lemmatization with lematus. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , volume 1, pages 1391–\n1400.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M Dai, Rafal Jozefowicz, and Samy Ben-\ngio. 2016. Generating sentences from a continuous\nspace. CoNLL 2016, page 10.\nGrzegorz Chrupała. 2006. Simple data-driven\ncontext-sensitive lemmatization. Procesamiento del\nLenguaje Natural, 37.\nYoeng-Jin Chu. 1965. On the shortest arborescence of\na directed graph. Scientia Sinica, 14:1396–1400.\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc V Le. 2018. Semi-supervised se-\nquence modeling with cross-view training. arXiv\npreprint arXiv:1809.08370.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nTimothy Dozat and Christopher D Manning. 2016.\nDeep biafﬁne attention for neural dependency pars-\ning. arXiv preprint arXiv:1611.01734.\nTimothy Dozat, Peng Qi, and Christopher D Manning.\n2017. Stanford’s graph-based neural dependency\nparser at the conll 2017 shared task. Proceedings\nof the CoNLL 2017 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies, pages\n20–30.\nLong Duong, Trevor Cohn, Steven Bird, and Paul\nCook. 2015. Low resource dependency parsing:\nCross-lingual parameter sharing in a neural network\nparser. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\nvolume 2, pages 845–850.\nJack Edmonds. 1967. Optimum branchings. Journal\nof Research of the national Bureau of Standards B ,\n71(4):233–240.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 328–339.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daum ´e III. 2015. Deep unordered com-\nposition rivals syntactic methods for text classiﬁca-\ntion. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), vol-\nume 1, pages 1681–1691.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\net al. 2017. Googles multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence.\nNikita Kitaev and Dan Klein. 2018. Multilingual\nconstituency parsing with self-attention and pre-\ntraining. arXiv preprint arXiv:1812.11760.\nDaniel Kondratyuk, Tom ´aˇs Gaven ˇciak, Milan Straka,\nand Jan Haji ˇc. 2018. Lemmatag: Jointly tagging\nand lemmatizing for morphologically rich languages\nwith brnns. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 4921–4928.\nMiryam de Lhoneux, Johannes Bjerva, Isabelle Augen-\nstein, and Anders Søgaard. 2018. Parameter sharing\nbetween dependency parsers for related languages.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4992–4997.\nWang Ling, Chris Dyer, Alan W Black, Isabel Tran-\ncoso, Ramon Fermandez, Silvio Amir, Luis Marujo,\nand Tiago Luis. 2015. Finding function in form:\nCompositional character models for open vocabu-\nlary word representation. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1520–1530.\nRyan McDonald, Slav Petrov, and Keith Hall. 2011.\nMulti-source transfer of delexicalized dependency\nparsers. In Proceedings of the conference on empir-\nical methods in natural language processing, pages\n62–72. Association for Computational Linguistics.\nPhoebe Mulcaire, Jungo Kasai, and Noah Smith.\n2019. Polyglot contextual representations im-\nprove crosslingual transfer. arXiv preprint\narXiv:1902.09697.\n2789\nThomas M¨uller, Ryan Cotterell, Alexander Fraser, and\nHinrich Sch ¨utze. 2015. Joint lemmatization and\nmorphological tagging with lemming. In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing, pages 2268–2274.\nTahira Naseem, Regina Barzilay, and Amir Globerson.\n2012. Selective sharing for multilingual dependency\nparsing. In Proceedings of the 50th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Long Papers-Volume 1, pages 629–637. Asso-\nciation for Computational Linguistics.\nJoakim Nivre, Mitchell Abrams, ˇZeljko Agi ´c, and\nAhrenberg. 2018. Universal dependencies 2.3. LIN-\nDAT/CLARIN digital library at the Institute of For-\nmal and Applied Linguistics ( ´UFAL), Faculty of\nMathematics and Physics, Charles University.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proc. of NAACL.\nCicero D Santos and Bianca Zadrozny. 2014. Learning\ncharacter-level representations for part-of-speech\ntagging. In Proceedings of the 31st International\nConference on Machine Learning (ICML-14), pages\n1818–1826.\nTal Schuster, Ori Ram, Regina Barzilay, and Amir\nGloberson. 2019. Cross-lingual alignment of con-\ntextual word embeddings, with applications to\nzero-shot dependency parsing. arXiv preprint\narXiv:1902.09492.\nDamien Sileo. 2019. Understanding bert transformer:\nAttention isnt all you need. Towards Data Science.\nAaron Smith, Bernd Bohnet, Miryam de Lhoneux,\nJoakim Nivre, Yan Shao, and Sara Stymne. 2018. 82\ntreebanks, 34 models: Universal dependency pars-\ning with multi-treebank models. In Proceedings of\nthe CoNLL 2018 Shared Task: Multilingual Pars-\ning from Raw Text to Universal Dependencies, pages\n113–123, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL\n2018 UD shared task. In Proceedings of the CoNLL\n2018 Shared Task: Multilingual Parsing from Raw\nText to Universal Dependencies , pages 197–207,\nBrussels, Belgium. Association for Computational\nLinguistics.\nMilan Straka, Jana Strakov ´a, and Jan Haji ˇc. 2019.\nEvaluating Contextualized Embeddings on 54 Lan-\nguages in POS Tagging, Lemmatization and Depen-\ndency Parsing. arXiv preprint arXiv:1908.07448.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language models. arXiv preprint\narXiv:1904.02679.\nDavid Vilares, Carlos G ´omez-Rodr´ıguez, and\nMiguel A Alonso. 2016. One model, two lan-\nguages: training bilingual parsers with harmonized\ntreebanks. In Proceedings of the 54th Annual\nMeeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , volume 2,\npages 425–431.\nRobert A Wagner and Michael J Fischer. 1974. The\nstring-to-string correction problem. Journal of the\nACM (JACM), 21(1):168–173.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nbert. arXiv preprint arXiv:1904.09077.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nDaniel Zeman, Jan Haji ˇc, Martin Popel, Martin Pot-\nthast, Milan Straka, Filip Ginter, Joakim Nivre, and\nSlav Petrov. 2018. CoNLL 2018 shared task: Mul-\ntilingual parsing from raw text to universal depen-\ndencies. In Proceedings of the CoNLL 2018 Shared\nTask: Multilingual Parsing from Raw Text to Univer-\nsal Dependencies, pages 1–21, Brussels, Belgium.\nAssociation for Computational Linguistics.\nA Appendix\nIn this section, we detail and explain hyperparam-\neter choices and miscellaneous details related to\nmodel training and display the full tables of eval-\nuation results of UDify across all UD languages.\nA.1 Hyperparameters\nUpon concatenating all training sets, we shufﬂe\nall the sentences, bundle them into batches of 32\nsentences each, and train UDify for a total of 80\nepochs before stopping. We hold the learning rate\nconstant until we unfreeze BERT in the second\nepoch, where we and linearly warm up the learn-\ning rate for the next 8,000 batches and then apply\ninverse square root learning rate decay for the re-\nmaining epochs. For the dependency parser, we\nuse feedforward tag and arc dimensions of 300\n2790\nHYPERPARAMETER VALUE\nDependency tag dimension 256\nDependency arc dimension 768\nOptimizer Adam\nβ1,β2 0.9, 0.99\nWeight decay 0.01\nLabel Smoothing 0.03\nDropout 0.5\nBERT dropout 0.2\nMask probability 0.2\nLayer dropout 0.1\nBatch size 32\nEpochs 80\nBase learning rate 1e−3\nBERT learning rate 5e−5\nLearning rate warmup steps 8000\nGradient clipping 5.0\nTable 6: A summary of model hyperparameters.\nand 800 respectively. We apply a small weight de-\ncay penalty of 0.01 to ensure that the weights re-\nmain small after each update. For optimization we\nuse the Adam optimizer and we compute softmax\ncross entropy loss to train the network. We use\na default β1 value of 0.9 and lower the β2 value\nfrom the typical 0.999 to 0.99. The reasoning is\nto increase the decay rate of the second moment\nin the Adam optimizer to reduce the chance of the\noptimizer being too optimistic with respect to the\ngradient history. We clip the gradient updates to\na maximum L2 magnitude of 5.0. A summary of\nhyperparameters can be found in Table 6.\nTo speed up training, we employ bucketed\nbatching, sorting all sentences by their length and\ngrouping similar length sentences into each batch.\nHowever, to ensure that most sentences do not\nget grouped within the same batch, we fuzz the\nlengths of each sentence by a maximum of 10% of\nits true length when grouping sentences together.\nDespite using all the regularization strategies\nshown previously, we still observe overﬁtting and\nmust apply more aggressive techniques. To fur-\nther regularize the network, we also increase the\nattention and hidden dropout rates of BERT from\n0.1 to 0.2, and we also apply a dropout rate of\n0.5 to all BERT layers before computing layer at-\ntention for each of the four tasks and applying a\nlayer dropout with probability 0.1. We increase\nthe masking probability of each wordpiece from\n0.15 to 0.2.\nWith all these regularization strategies and hy-\nperparameter choices combined, we are able to\nﬁne-tune BERT for far more epochs before the net-\nwork starts to overﬁt, i.e., 80 as opposed to around\n102 103 104 105\nTrain Sentences\n0.3\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\nLAS Difference\nFigure 5: A plot of the difference in LAS between\nUDify and UDPipe Future with respect to the number\nof training sentences in each treebank.\n10. Even so, we believe even more regularization\ncan improve test performance.\nThe ﬁnal multilingual UDify model was trained\nover approximately 25 days on an NVIDIA GTX\n1080 Ti taking an average of 8 hours per epoch.\nWe use half-precision (fp16) training to be able to\nkeep the BERT model in memory. One notable\naspect of training is that while we observed the\nmodel start to level out in validation performance\nat around epoch 30, the model continually made\nsmall, incremental improvements over each sub-\nsequent epoch, resulting in far higher scores than\nif the model training was terminated early. This\ncan be partially attributed to the decaying inverse\nsquare root learning rate.\nDue to the high training times, we are only\nable to report on a small number of training ex-\nperiments for the most relevant and useful results.\nPrior to developing the ﬁnal model, we conducted\nﬁne-tuning experiments on pairs of languages to\nﬁnd a set of hyperparameters that worked best for\nmultilingual learning. After this, we gradually\nscaled up training to 3 languages, 5 languages, 15\nlanguages, and then ﬁnally the model presented\nabove. We had high doubts, and wanted to see\nwhere the limit was in multilingual training. We\nwere pleasantly surprised to ﬁnd that this simple\ntraining scheme was able to scale up so well to all\nUD treebanks.\nA.2 Training Size Effect on Performance\nTo gain a better understanding of where the largest\nscore improvements in UDify occur, we plot the\nLAS improvement UDify provides over UDPipe\nFuture for each treebank, ordered by the size\n(number of sentences) of the training set, see Fig-\n2791\n102 103 104 105\nTrain Sentences\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0LAS\nUDPipe Future\nUDify\nFigure 6: A plot of LAS between with respect to the\nnumber of training sentences in each treebank.\nure 5. The results show that the largest improve-\nments tend to occur on small treebanks with less\nthan 3,000 training examples. For absolute LAS\nvalues, see Figure 6, which indicates that more\ntraining resources tend to improve evaluation per-\nformance overall.\nA.3 Miscellaneous Details\nOur results show that modeling language-speciﬁc\nproperties is not strictly necessary to achieve high-\nperforming cross-lingual representations for de-\npendency parsing, though we caution that the\nmodel can also likely be improved by these tech-\nniques.\nFine-tuning BERT on UD introduces a syntac-\ntic bias in the network, and we are interested in\nobserving any differences in transfer learning by\nﬁne-tuning this new “UD-BERT” on other tasks.\nWe leave a comprehensive evaluation of injecting\nsyntactic bias into language models with respect\nto knowledge transfer for future work.\nWe note that saving the weights of BERT and\nﬁne-tuning a second round can improve perfor-\nmance as demonstrated in Stickland et al. (2019).\nThe improvements of UDify+Lang over just UD-\nify can be partially attributed to this, but we can\nsee that even these improvements can be inferior\nto ﬁne-tuning on all UD treebanks.\nBERT limits its positional encoding to 512\nwordpieces, causing some sentences in UD to be\ntoo long to ﬁt into the model. We use a slid-\ning window approach to break up long sentences\ninto windows of 512 wordpieces, overlapping each\nwindow by 256 wordpieces. After feeding the\nwindows into BERT, we select the ﬁrst 256 word-\npieces of each window and any remaining word-\npieces in the last window to represent the contex-\ntual embeddings of each word in the original sen-\ntence.\nA.4 Full Results of UD Scores\nWe show in Tables 7, 8, 9, and 10 UDify scores\nevaluated on all 124 treebanks with the ofﬁcial\nCoNLL 2018 Shared Task evaluation script. For\ncomparison, we also include the full test evalua-\ntion of UDPipe Future on the subset of 89 tree-\nbanks with a training set. We also add a column\nindicating the size of each treebank, i.e., the num-\nber of sentences in the training set.\n2792\nTREEBANK MODEL UPOS UF EATS LEMMAS UAS LAS CLAS MLAS BLEX S IZE\nAfrikaans AfriBooms UDPipe 98.25 97.66 97.46 89.38 86.58 81.44 77.66 77.82 1.3k\nUDify 97.48 96.63 95.23 86.97 83.48 77.42 70.57 70.93 1.3k\nAkkadian PISANDUB UDify 19.92 99.51 2.32 27.65 4.54 3.27 1.04 0.30 0\nAmharic ATT UDify 15.25 43.95 58.04 17.38 3.49 4.88 0.23 2.53 0\nAncient Greek PROIEL UDPipe 97.86 92.44 93.51 85.93 82.11 77.70 67.16 71.22 15.0k\nUDify 91.20 82.29 76.16 78.91 72.66 66.07 50.79 47.27 15.0k\nAncient Greek Perseus UDPipe 93.27 91.39 85.02 78.85 73.54 67.60 53.87 53.19 11.5k\nUDify 85.67 81.67 70.51 70.51 62.64 55.60 39.15 35.05 11.5k\nArabic PADT UDPipe 96.83 94.11 95.28 87.54 82.94 79.77 73.92 75.87 6.1k\nUDify 96.58 91.77 73.55 87.72 82.88 79.47 70.52 50.26 6.1k\nArabic PUD UDify 79.98 40.32 0.00 76.17 67.07 65.10 10.67 0.00 0\nArmenian ArmTDP UDPipe 93.49 82.85 92.86 78.62 71.27 65.77 48.11 60.11 561\nUDify 94.42 76.90 85.63 85.63 78.61 73.72 46.80 59.14 561\nBambara CRB UDify 30.86 57.96 20.42 30.28 8.60 6.56 1.04 0.76 0\nBasque BDT UDPipe 96.11 92.48 96.29 86.11 82.86 81.79 72.33 78.54 5.4k\nUDify 95.45 86.80 90.53 84.94 80.97 79.52 63.60 71.56 5.4k\nBelarusian HSE UDPipe 93.63 73.30 87.34 78.58 72.72 69.14 46.20 58.28 261\nUDify 97.54 89.36 85.46 91.82 87.19 85.05 71.54 68.66 261\nBreton KEB UDify 62.78 47.12 51.31 63.52 39.84 35.14 4.64 16.34 0\nBulgarian BTB UDPipe 98.98 97.82 97.94 93.38 90.35 87.01 83.63 84.42 8.9k\nUDify 98.89 96.18 93.49 95.54 92.40 89.59 83.43 80.44 8.9k\nBuryat BDT UDPipe 40.34 32.40 58.17 32.60 18.83 12.36 1.26 6.49 20\nUDify 61.73 47.45 61.03 48.43 26.28 20.61 5.51 11.68 20\nCantonese HK UDify 67.11 91.01 96.01 46.82 32.01 33.35 14.29 31.26 0\nCatalan AnCora UDPipe 98.88 98.37 99.07 93.22 91.06 87.18 84.48 86.18 13.1k\nUDify 98.89 98.34 98.14 94.25 92.33 89.27 86.21 86.61 13.1k\nChinese CFL UDify 83.75 82.72 98.75 62.46 42.48 43.46 21.07 42.22 0\nChinese GSD UDPipe 94.88 99.22 99.99 84.64 80.50 76.79 71.04 76.78 4.0k\nUDify 95.35 99.35 99.97 87.93 83.75 80.33 74.36 80.28 4.0k\nChinese HK UDify 82.86 86.47 100.00 65.53 49.32 47.84 22.85 47.84 0\nChinese PUD UDify 92.68 98.40 100.00 79.08 56.51 55.22 40.92 55.22 0\nCoptic Scriptorium UDPipe 94.70 96.35 95.49 85.58 80.97 72.24 64.45 68.48 371\nUDify 27.17 52.85 55.71 27.58 10.82 6.50 0.19 1.44 371\nCroatian SET UDPipe 98.13 92.25 97.27 91.10 86.78 84.11 73.61 81.19 7.0k\nUDify 98.02 89.67 95.34 94.08 89.79 87.70 72.72 82.00 7.0k\nCzech CAC UDPipe 99.37 96.34 98.57 92.99 90.71 88.84 84.30 87.18 23.5k\nUDify 99.14 95.42 98.32 94.33 92.41 91.03 84.68 89.21 23.5k\nCzech CLTT UDPipe 98.88 91.59 98.25 86.90 84.03 80.55 71.63 79.20 861\nUDify 99.17 93.66 98.86 91.69 89.96 87.59 79.50 86.79 861\nCzech FicTree UDPipe 98.55 95.87 98.63 92.91 89.75 86.97 81.04 85.49 10.2k\nUDify 98.34 91.82 98.13 95.19 92.77 90.99 77.77 88.39 10.2k\nCzech PDT UDPipe 99.18 97.23 99.02 93.33 91.31 89.64 86.15 88.60 68.5k\nUDify 99.18 96.69 98.52 94.73 92.88 91.64 87.13 89.95 68.5k\nCzech PUD UDify 97.93 93.98 96.94 92.59 87.95 84.85 77.39 82.81 0\nDanish DDT UDPipe 97.78 97.33 97.52 86.88 84.31 81.20 76.29 78.51 4.4k\nUDify 97.50 95.41 94.60 87.76 84.50 81.60 73.76 75.15 4.4k\nDutch Alpino UDPipe 96.83 96.33 97.09 91.37 88.38 83.51 77.28 79.82 12.3k\nUDify 97.67 97.66 95.44 94.23 91.21 87.32 82.81 80.76 12.3k\nDutch LassySmall UDPipe 96.50 96.42 97.41 90.20 86.39 81.88 77.19 78.83 5.8k\nUDify 96.70 96.57 95.10 94.34 91.22 88.03 82.06 81.40 5.8k\nTable 7: The full test results of UDify on 124 treebanks (part 1 of 4). The S IZE column indicates the number of\ntraining sentences.\n2793\nTREEBANK MODEL UPOS UF EATS LEMMAS UAS LAS CLAS MLAS BLEX S IZE\nEnglish EWT UDPipe 96.29 97.10 98.25 89.63 86.97 84.02 79.00 82.36 12.5k\nUDify 96.21 96.02 97.28 90.96 88.50 86.25 79.80 83.39 12.5k\nEnglish GUM UDPipe 96.02 96.82 96.85 87.27 84.12 78.55 73.51 74.68 2.9k\nUDify 95.44 94.12 93.15 89.14 85.73 83.03 72.55 74.30 2.9k\nEnglish LinES UDPipe 96.91 96.31 96.45 84.15 79.71 77.44 71.38 73.22 2.7k\nUDify 95.31 91.34 94.50 87.33 83.71 82.95 68.62 76.23 2.7k\nEnglish PUD UDify 96.18 93.50 94.20 91.52 88.66 87.83 75.61 80.57 0\nEnglish ParTUT UDPipe 96.10 95.51 97.74 90.29 87.27 82.58 76.44 80.33 1.8k\nUDify 96.16 92.61 96.45 92.84 90.14 86.28 74.59 82.01 1.8k\nErzya JR UDify 46.66 31.82 45.73 31.90 16.38 10.83 0.58 2.83 0\nEstonian EDT UDPipe 97.64 96.23 95.30 88.00 85.18 83.62 78.72 78.51 24.4k\nUDify 97.44 95.13 86.56 89.53 86.67 85.17 79.20 69.31 24.4k\nFaroese OFT UDify 77.46 35.20 51.09 67.24 59.26 51.17 2.39 21.92 0\nFinnish FTB UDPipe 96.65 96.62 95.49 90.68 87.89 85.11 80.58 81.18 15.0k\nUDify 93.80 90.38 88.80 86.37 81.40 81.01 68.16 70.15 15.0k\nFinnish PUD UDify 96.48 93.84 84.64 89.76 86.58 86.64 77.83 69.12 0\nFinnish TDT UDPipe 97.45 95.43 91.45 89.88 87.46 85.87 80.43 76.64 12.2k\nUDify 94.43 90.48 82.89 86.42 82.03 82.62 70.89 63.66 12.2k\nFrench GSD UDPipe 97.63 97.13 98.35 90.65 88.06 84.35 79.76 82.39 14.5k\nUDify 97.83 96.17 97.34 93.60 91.45 88.54 81.61 84.51 14.5k\nFrench PUD UDify 91.67 59.65 100.00 88.36 82.76 81.74 25.24 81.74 0\nFrench ParTUT UDPipe 96.93 94.43 95.70 92.17 89.63 84.62 75.22 78.07 804\nUDify 96.12 88.36 93.97 90.55 88.06 83.19 63.03 74.03 804\nFrench Sequoia UDPipe 98.79 98.09 98.57 92.37 90.73 87.55 84.51 85.93 2.2k\nUDify 97.89 88.97 97.15 92.53 90.05 86.67 67.98 82.52 2.2k\nFrench Spoken UDPipe 95.91 100.00 96.92 82.90 77.53 71.82 68.24 69.47 1.2k\nUDify 96.23 98.67 96.59 85.24 80.01 75.40 69.74 72.77 1.2k\nGalician CTG UDPipe 97.84 99.83 98.58 86.44 83.82 78.58 72.46 77.21 2.3k\nUDify 96.51 97.10 97.08 84.75 80.89 74.62 65.86 72.17 2.3k\nGalician TreeGal UDPipe 95.82 93.96 97.06 82.72 77.69 71.69 63.73 68.89 601\nUDify 94.59 80.67 94.93 84.08 76.77 73.06 49.76 66.99 601\nGerman GSD UDPipe 94.48 90.68 96.80 85.53 81.07 76.26 58.82 72.13 13.8k\nUDify 94.55 90.43 94.42 87.81 83.59 80.03 61.27 72.48 13.8k\nGerman PUD UDify 89.49 30.66 94.77 89.86 84.46 80.50 2.10 72.95 0\nGothic PROIEL UDPipe 96.61 90.73 94.75 85.28 79.60 76.92 66.70 72.93 3.4k\nUDify 95.55 85.97 80.57 85.61 79.37 76.26 63.09 58.65 3.4k\nGreek GDT UDPipe 97.98 94.96 95.82 92.10 89.79 85.71 78.60 79.72 1.7k\nUDify 97.72 93.29 89.43 94.33 92.15 88.67 77.89 71.83 1.7k\nHebrew HTB UDPipe 97.02 95.87 97.12 89.70 86.86 81.45 75.52 78.14 5.2k\nUDify 96.94 93.41 94.15 91.63 88.11 83.04 72.55 74.87 5.2k\nHindi HDTB UDPipe 97.52 94.15 98.67 94.85 91.83 88.21 78.49 86.83 13.3k\nUDify 97.12 92.59 98.23 95.13 91.46 87.80 75.54 86.10 13.3k\nHindi PUD UDify 87.54 22.81 100.00 71.64 58.42 53.03 3.32 53.03 0\nHungarian Szeged UDPipe 95.76 91.75 95.05 84.04 79.73 78.65 67.63 73.63 911\nUDify 96.36 86.16 90.19 89.68 84.88 83.93 64.27 72.21 911\nIndonesian GSD UDPipe 93.69 95.58 99.64 85.31 78.99 76.76 67.74 76.38 4.5k\nUDify 93.36 93.32 98.37 86.45 80.10 78.05 66.93 76.31 4.5k\nIndonesian PUD UDify 76.10 44.23 100.00 77.47 56.90 54.88 7.41 54.88 0\nIrish IDT UDPipe 92.72 82.43 90.48 80.39 72.34 63.48 46.49 55.32 567\nUDify 90.49 71.84 81.27 80.05 69.28 60.02 34.39 43.07 567\nItalian ISDT UDPipe 98.39 98.11 98.66 93.49 91.54 87.34 84.28 85.49 13.1k\nUDify 98.51 98.01 97.72 95.54 93.69 90.40 86.54 86.70 13.1k\nItalian PUD UDify 94.73 58.16 96.08 94.18 91.76 90.05 25.55 83.74 0\nItalian ParTUT UDPipe 98.38 97.77 98.16 92.64 90.47 85.05 81.87 82.99 1.8k\nUDify 98.21 98.38 97.55 95.96 93.68 89.83 86.83 86.44 1.8k\nTable 8: The full test results of UDify on 124 treebanks (part 2 of 4).\n2794\nTREEBANK MODEL UPOS UF EATS LEMMAS UAS LAS CLAS MLAS BLEX S IZE\nJapanese GSD UDPipe 98.13 99.98 99.52 95.06 93.73 88.35 86.37 88.04 7.1k\nUDify 97.08 99.97 98.80 94.37 92.08 86.19 82.99 85.12 7.1k\nJapanese Modern UDify 74.94 96.14 79.70 74.99 55.62 42.67 30.89 35.47 0\nJapanese PUD UDify 97.89 99.98 99.31 94.89 93.62 87.92 84.86 87.15 0\nKazakh KTB UDPipe 55.84 40.40 63.96 53.30 33.38 27.06 4.82 15.10 32\nUDify 85.59 65.49 77.18 74.77 63.66 61.84 34.23 45.51 32\nKomi Zyrian IKDP UDify 59.92 39.32 57.56 36.01 22.12 17.45 1.54 6.80 0\nKomi Zyrian Lattice UDify 38.57 29.45 55.33 28.85 12.99 10.79 0.72 3.28 0\nKorean GSD UDPipe 96.29 99.77 93.40 87.70 84.24 82.05 79.74 76.35 4.4k\nUDify 90.56 99.63 82.84 82.74 74.26 71.72 65.94 57.58 4.4k\nKorean Kaist UDPipe 95.59 100.00 94.30 88.42 86.48 84.12 80.72 79.22 23.0k\nUDify 94.67 99.98 85.89 87.57 84.52 82.05 78.27 68.99 23.0k\nKorean PUD UDify 64.43 60.47 70.47 63.57 46.89 45.29 16.26 30.94 0\nKurmanji MG UDPipe 53.36 41.54 69.58 45.23 34.32 29.41 2.74 19.39 21\nUDify 60.23 37.78 58.08 35.86 20.40 14.75 1.42 7.28 21\nLatin ITTB UDPipe 98.34 96.97 98.99 91.06 88.80 86.40 82.35 85.71 16.8k\nUDify 98.48 95.81 98.08 92.43 90.12 87.93 82.24 85.97 16.8k\nLatin PROIEL UDPipe 97.01 91.53 96.32 83.34 78.66 76.20 67.40 73.65 15.9k\nUDify 96.79 89.49 91.79 84.85 80.52 77.96 67.18 71.00 15.9k\nLatin Perseus UDPipe 88.40 79.10 81.45 71.20 61.28 56.32 41.58 45.09 1.3k\nUDify 90.96 82.09 81.08 78.33 69.60 65.95 50.26 51.33 1.3k\nLatvian LVTB UDPipe 96.11 93.01 95.46 87.20 83.35 80.90 71.92 76.64 7.2k\nUDify 96.02 89.78 91.00 89.33 85.09 82.34 69.51 72.58 7.2k\nLithuanian HSE UDPipe 81.70 60.47 76.89 51.98 42.17 38.93 18.17 28.70 154\nUDify 90.47 70.00 67.17 79.06 69.34 66.00 36.21 36.35 154\nMaltese MUDT UDPipe 95.99 100.00 100.00 84.65 79.71 71.49 66.75 71.49 1.1k\nUDify 91.98 99.89 100.00 83.07 75.56 65.08 58.14 65.08 1.1k\nMarathi UFAL UDPipe 80.10 67.23 81.31 70.63 61.41 57.44 29.34 45.87 374\nUDify 88.59 59.22 72.82 79.37 67.72 60.13 21.71 39.25 374\nNaija NSC UDify 55.44 51.32 97.03 45.75 32.16 31.62 4.73 29.33 0\nNorth Sami Giella UDPipe 92.54 90.03 88.31 78.30 73.49 70.94 62.40 61.45 2.3k\nUDify 90.21 83.55 71.50 74.30 67.13 64.41 51.20 40.63 2.3k\nNorwegian Bokmaal UDPipe 98.31 97.14 98.64 92.39 90.49 88.18 84.06 86.53 15.7k\nUDify 98.18 96.36 97.33 93.97 92.18 90.40 85.02 87.13 15.7k\nNorwegian Nynorsk UDPipe 98.14 97.02 98.18 92.09 90.01 87.68 82.97 85.47 14.2k\nUDify 98.14 96.55 97.18 94.34 92.37 90.39 85.01 86.71 14.2k\nNorwegian NynorskLIA UDPipe 89.59 86.13 93.93 68.08 60.07 54.89 44.47 50.98 340\nUDify 95.01 93.36 96.13 75.40 69.60 65.33 56.90 62.27 340\nOld Church Slavonic PROIEL UDPipe 96.91 90.66 93.11 89.66 85.04 83.41 73.63 77.81 4.1k\nUDify 84.23 71.30 65.70 76.71 66.67 64.10 46.25 43.88 4.1k\nOld French SRCMF UDPipe 96.09 97.81 100.00 91.74 86.83 83.85 79.91 83.85 13.9k\nUDify 95.73 96.98 100.00 91.74 86.65 83.49 78.85 83.49 13.9k\nPersian Seraji UDPipe 97.75 97.78 97.44 90.05 86.66 83.26 81.23 80.93 4.8k\nUDify 96.22 94.73 92.55 89.59 85.84 81.98 76.65 74.74 4.8k\nPolish LFG UDPipe 98.80 95.49 97.54 96.58 94.76 93.01 87.04 90.26 13.8k\nUDify 98.80 87.71 94.04 96.67 94.58 93.03 76.50 85.15 13.8k\nPolish SZ UDPipe 98.34 93.04 97.16 93.39 91.24 89.39 81.06 85.99 6.1k\nUDify 98.36 67.11 93.92 93.67 89.20 87.31 48.47 80.24 6.1k\nPortuguese Bosque UDPipe 97.07 96.40 98.46 91.36 89.04 85.19 76.67 83.06 8.3k\nUDify 97.10 89.70 91.60 91.37 87.84 84.13 69.09 78.64 8.3k\nPortuguese GSD UDPipe 98.31 99.92 99.30 93.01 91.63 87.67 85.96 86.94 9.7k\nUDify 98.04 95.75 98.95 94.22 92.54 89.37 82.32 87.90 9.7k\nPortuguese PUD UDify 90.14 51.16 99.79 87.02 80.17 74.10 17.51 74.10 0\nRomanian Nonstandard UDPipe 96.68 90.88 94.78 89.12 84.20 78.91 65.93 73.44 8.0k\nUDify 96.83 88.89 89.33 90.36 85.26 80.41 64.68 68.11 8.0k\nTable 9: The full test results of UDify on 124 treebanks (part 3 of 4).\n2795\nTREEBANK MODEL UPOS UF EATS LEMMAS UAS LAS CLAS MLAS BLEX S IZE\nRomanian RRT UDPipe 97.96 97.53 98.41 91.31 86.74 82.57 79.02 81.09 8.0k\nUDify 97.73 96.12 95.84 93.16 88.56 84.87 79.20 79.92 8.0k\nRussian GSD UDPipe 97.10 92.66 97.37 88.15 84.37 82.66 74.07 80.03 3.9k\nUDify 96.91 87.45 77.73 90.71 86.03 84.51 67.24 62.08 3.9k\nRussian PUD UDify 93.06 63.60 77.93 93.51 87.14 83.96 37.25 61.86 0\nRussian SynTagRus UDPipe 99.12 97.57 98.53 93.80 92.32 90.85 87.91 89.17 48.8k\nUDify 98.97 96.29 94.47 94.83 93.13 91.87 86.91 85.44 48.8k\nRussian Taiga UDPipe 93.18 82.87 89.99 75.45 69.11 65.31 48.81 57.21 881\nUDify 95.39 88.47 90.19 84.02 77.80 75.12 59.71 65.15 881\nSanskrit UFAL UDify 37.33 17.63 37.38 40.21 18.56 15.38 0.85 4.12 0\nSerbian SET UDPipe 98.33 94.35 97.36 92.70 89.27 87.08 79.14 84.18 2.9k\nUDify 98.30 92.22 95.86 95.68 91.95 90.30 78.45 84.93 2.9k\nSlovak SNK UDPipe 96.83 90.82 96.40 89.82 86.90 84.81 74.00 81.37 8.5k\nUDify 97.46 89.30 93.80 95.92 93.87 92.86 77.33 85.12 8.5k\nSlovenian SSJ UDPipe 98.61 95.92 98.25 92.96 91.16 88.76 83.85 86.89 6.5k\nUDify 98.73 93.44 96.50 94.74 93.07 90.94 81.55 86.38 6.5k\nSlovenian SST UDPipe 93.79 86.28 95.17 73.51 67.51 63.46 52.67 60.32 2.1k\nUDify 95.40 89.81 95.15 80.37 75.03 71.19 61.32 67.24 2.1k\nSpanish AnCora UDPipe 98.91 98.49 99.17 92.34 90.26 86.39 83.97 85.51 14.3k\nUDify 98.53 97.89 98.07 92.99 90.50 87.26 83.43 84.85 14.3k\nSpanish GSD UDPipe 96.85 97.09 98.97 90.71 88.03 82.85 75.98 81.47 14.2k\nUDify 95.91 95.08 96.52 90.82 87.23 82.83 72.47 78.08 14.2k\nSpanish PUD UDify 88.98 54.58 100.00 90.45 83.08 77.42 18.06 77.42 0\nSwedish LinES UDPipe 96.78 89.43 97.03 86.07 81.86 80.32 66.48 77.38 2.7k\nUDify 96.85 87.24 92.70 88.77 85.49 85.61 66.99 77.62 2.7k\nSwedish PUD UDify 96.36 80.04 88.81 89.17 86.10 85.25 57.12 72.92 0\nSwedish Sign Language SSLC UDPipe 68.09 100.00 100.00 50.35 37.94 39.51 30.96 39.51 88\nUDify 63.48 96.10 100.00 40.43 26.95 30.12 23.29 30.12 88\nSwedish Talbanken UDPipe 97.94 96.86 98.01 89.63 86.61 84.45 79.67 82.26 4.3k\nUDify 98.11 95.92 95.50 91.91 89.03 87.26 80.72 81.31 4.3k\nTagalog TRG UDify 60.62 35.62 73.63 64.04 40.07 36.84 0.00 13.16 0\nTamil TTB UDPipe 91.05 87.28 93.92 74.11 66.37 63.71 55.31 59.58 401\nUDify 91.50 83.21 80.84 79.34 71.29 69.10 53.62 54.84 401\nTelugu MTG UDPipe 93.07 99.03 100.00 91.26 85.02 81.76 77.75 81.76 1.1k\nUDify 93.48 99.31 100.00 92.23 83.91 79.92 76.10 79.92 1.1k\nThai PUD UDify 56.78 62.48 100.00 49.05 26.06 18.42 3.77 18.42 0\nTurkish IMST UDPipe 96.01 92.55 96.01 74.19 67.56 63.83 56.96 61.37 3.7k\nUDify 94.29 84.49 87.71 74.56 67.44 63.87 49.42 54.10 3.7k\nTurkish PUD UDify 77.34 24.59 84.31 67.68 46.07 39.95 2.61 32.50 0\nUkrainian IU UDPipe 97.59 92.66 97.23 88.29 85.25 81.90 73.81 79.10 5.3k\nUDify 97.71 88.63 94.00 92.83 90.30 88.15 72.93 81.04 5.3k\nUpper Sorbian UFAL UDPipe 62.93 41.10 68.68 45.58 34.54 27.18 3.37 16.65 24\nUDify 84.87 48.84 72.68 71.55 62.82 56.04 16.19 37.89 24\nUrdu UDTB UDPipe 93.66 81.92 97.40 87.50 81.62 75.20 55.02 73.07 4.0k\nUDify 94.37 82.80 96.68 88.43 82.84 77.00 56.70 73.97 4.0k\nUyghur UDT UDPipe 89.87 88.30 95.31 78.46 67.09 60.85 47.84 57.08 1.7k\nUDify 75.88 70.80 79.70 65.89 48.80 38.95 21.75 31.31 1.7k\nVietnamese VTB UDPipe 89.68 99.72 99.55 70.38 62.56 60.03 55.56 59.54 1.4k\nUDify 91.29 99.58 99.21 74.11 66.00 63.34 58.71 62.61 1.4k\nWarlpiri UFAL UDify 33.44 18.15 39.17 21.66 7.96 7.49 0.00 0.88 0\nYoruba YTB UDify 50.86 78.32 85.56 37.62 19.09 16.56 6.30 12.15 0\nTable 10: The full test results of UDify on 124 treebanks (part 4 of 4)."
}