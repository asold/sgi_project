{
    "title": "TextBox 2.0: A Text Generation Library with Pre-trained Language Models",
    "url": "https://openalex.org/W4385572863",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2133263866",
            "name": "Tianyi Tang",
            "affiliations": [
                "Search (Poland)",
                "Next Generation Technology (United States)",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2129999472",
            "name": "Junyi Li",
            "affiliations": [
                "University of Monterrey",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2099607494",
            "name": "Zhipeng Chen",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2232932148",
            "name": "Yiwen Hu",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A3173862604",
            "name": "Zhuohao Yu",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5059422098",
            "name": "Wenxun Dai",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2307999729",
            "name": "Wayne Xin Zhao",
            "affiliations": [
                "Search (Poland)",
                "Renmin University of China",
                "Next Generation Technology (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2777189136",
            "name": "Jian-Yun Nie",
            "affiliations": [
                "University of Monterrey"
            ]
        },
        {
            "id": "https://openalex.org/A3212238123",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Next Generation Technology (United States)",
                "Search (Poland)",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2785896739",
        "https://openalex.org/W3176400576",
        "https://openalex.org/W4290742115",
        "https://openalex.org/W3040809437",
        "https://openalex.org/W3199824684",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W3176929804",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W2964321064",
        "https://openalex.org/W3100128199",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3173322864",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W3174264667",
        "https://openalex.org/W4287887117",
        "https://openalex.org/W3206427007",
        "https://openalex.org/W2964006684",
        "https://openalex.org/W3176175717",
        "https://openalex.org/W2970574558",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3176998540",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W3042185737",
        "https://openalex.org/W3205944346",
        "https://openalex.org/W2962996600",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3090350559",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3169113923",
        "https://openalex.org/W3094342783",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2113207845",
        "https://openalex.org/W4229005744",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3173273620",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3186655327",
        "https://openalex.org/W2805430026",
        "https://openalex.org/W2962974452",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4226474154",
        "https://openalex.org/W2739874095",
        "https://openalex.org/W4221144361",
        "https://openalex.org/W3187018546",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2997195635",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3211466672",
        "https://openalex.org/W2154652894"
    ],
    "abstract": "Tianyi Tang, Junyi Li, Zhipeng Chen, Yiwen Hu, Zhuohao Yu, Wenxun Dai, Wayne Xin Zhao, Jian-yun Nie, Ji-rong Wen. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2022.",
    "full_text": "Proceedings of the The 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 435 - 444\nDecember 7-11, 2022c⃝2022 Association for Computational Linguistics\nTextBox 2.0: A Text Generation Library with\nPre-trained Language Models\nTianyi Tang1,5†, Junyi Li 1,3†, Zhipeng Chen 2†, Yiwen Hu 2, Zhuohao Yu 2, Wenxun Dai 4,\nWayne Xin Zhao1,5∗\n, Jian-Yun Nie 3, and Ji-Rong Wen1,2,5\n1Gaoling School of Artiﬁcial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3DIRO, Université de Montréal 4Xidian University\n5Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE\nsteventianyitang@outlook.com lijunyi@ruc.edu.cn batmanfly@gmail.com\nAbstract\nTo facilitate research on text generation, this\npaper presents a comprehensive and uniﬁed\nlibrary, TextBox 2.0, focusing on the use of\npre-trained language models (PLMs). To be\ncomprehensive, our library covers 13 common\ntext generation tasks and their corresponding\n83 datasets and further incorporates 45 PLMs\ncovering general, translation, Chinese, dia-\nlogue, controllable, distilled, prompting, and\nlightweight PLMs. We also implement 4 ef-\nﬁcient training strategies and provide 4 gen-\neration objectives for pre-training new PLMs\nfrom scratch. To be uniﬁed, we design the in-\nterfaces to support the entire research pipeline\n(from data loading to training and evaluation),\nensuring that each step can be fulﬁlled in a\nuniﬁed way. Despite the rich functionality, it\nis easy to use our library, either through the\nfriendly Python API or command line. To val-\nidate the effectiveness of our library, we con-\nduct extensive experiments and exemplify four\ntypes of research scenarios. The project is re-\nleased at the link: https://github.com/\nRUCAIBox/TextBox#2.0.\n1 Introduction\nText generation, aiming to generate human-like\ntexts on demand, has been a fundamental technique\nin many text applications, such as machine trans-\nlation (Dabre et al., 2020), text summarization (El-\nKassas et al., 2021), and dialogue system (Chen\net al., 2017). Recently, pre-trained language mod-\nels (PLMs) such as BART (Lewis et al., 2020) have\nbeen the mainstream approach to developing ef-\nfective text generation models. With the great ad-\nvances in text generation, it has become increas-\ningly important to reproduce, develop, and com-\npare various text generation models in a reliable,\nﬂexible, and uniﬁed way.\n†Equal contribution.\n∗Corresponding author\nConsidering the rapid progress of PLMs on text\ngeneration, in this paper, we present a signiﬁcant\nextension of a previously released text generation li-\nbrary, TextBox 1.0 (Li et al., 2021), called TextBox\n2.0. Different from TextBox 1.0 and other text gen-\neration libraries (Miller et al., 2017; Klein et al.,\n2018; Zhu et al., 2018) (mostly including classical\nmodels based on recurrent neural networks or gen-\nerative adversarial networks), this extension mainly\nfocuses on building a comprehensive and uniﬁed\nframework for better supporting PLM-based text\ngeneration models. Although some libraries (e.g.,\nFairseq (Ott et al., 2019) and Hugging Face (Wolf\net al., 2020)) also include PLMs, they are designed\nfor performing myriad NLP tasks (only consider-\ning a few text generation tasks). Moreover, they\ndon’t maintain a complete evaluation pipeline (e.g.,\ndata loading, training, inference, and evaluation)\nspecially designed for text generation. Thus, it is\nnot fully suited for developing and evaluating text\ngeneration models in a uniﬁed way.\nIn order to better facilitate research on text gen-\neration, TextBox 2.0 introduces a series of new\nfeatures for supporting the use of PLMs, which can\nbe summarized into three major aspects:\n•Generation Tasks: Our library supports 13\ncommonly studied text generation tasks (e.g., trans-\nlation and story generation) and their correspond-\ning 83 datasets, including most of the existing main-\nstream tasks and datasets for research. We reorga-\nnize these datasets so that they are framed in a\nuniﬁed text-to-text format. Users can simply set\nthe dataset via the command line or conﬁguration\nﬁle without additional preprocessing efforts.\n•Generation Models: As a key contribution, our\nlibrary incorporates 45 PLMs, covering the cate-\ngories of general, translation, Chinese, dialogue,\ncontrollable, distilled, prompting, and lightweight\nPLMs. We unify the interface to use existing PLMs\nand incorporate new PLMs, and it is convenient\nto run different PLMs for a speciﬁed task in our\n435\nAspects TextBox 1.0 TextBox 2.0\nTasks\n6 v.s. 13\nSummarization, translation, dialogue,\nunconditional generation, attribute-\nto-text generation, poem generation\nSummarization, translation, dialogue, data-to-text, question genera-\ntion, question answering, story generation, commonsense generation,\nChinese generation, paraphrase, style transfer and simpliﬁcation\nModels\n6 v.s. 45\nV AE: LSTMV AE, CNNV AE, CV AE,\nHybridV AE\nGAN: SeqGAN, TextGAN, RankGAN,\nMaliGAN, LeakGAN, MaskGAN\nPLM: GPT-2, XLNet, BERT2BERT, T5,\nBART, ProphetNet\nSeq2Seq: RNN, Transformer, Attr2Seq,\nContext2Seq, HRED\nGeneral: GPT-2, BERT2BERT, BART, T5, ProphetNet, GPT, GPT-\nNeo, OPT, UniLM, MASS, PEGASUS, MVP, Bigbird, LED\nTranslation: mBART, mT5, Marian, M2M 100, NLLB, XLM\nChinese: CPM, CPT, Chinese-BART, Chinese-T5, Chinese-GPT2\nDialogue: Blenderbot and DialoGPT\nControllable: CTRL and PPLM\nDistilled: DistilGPT2 and DistilBART\nPrompting: PTG and Context-Tuning\nLightweight: Adapter, Preﬁx-tuning, Prompt tuning, LoRA, BitFit,\nP-Tuning v2\nTraining\nStrategies Distributed data parallel Distributed data parallel, efﬁcient decoding, hyper-parameter opti-\nmization, repeated experiments, pre-training objectives\nTable 1: Comparison of TextBox 1.0 and TextBox 2.0. We also present a comparison of the numbers of tasks and\npre-trained models between the two versions.\nlibrary. We also provide a standard way to compare\nthese models and analyze the generated results.\n•Training Strategies: To support the optimiza-\ntion of PLMs, we provide four efﬁcient and ro-\nbust training strategies ( e.g., efﬁcient decoding)\nand four pre-training objectives ( e.g., denoising\nauto-encoding) for text generation. These strate-\ngies make optimizing text generation models more\nefﬁcient and reliable. Users can either pre-train a\nnew model from scratch or ﬁne-tune a pre-trained\nmodel for research purposes.\nAs another merit, TextBox 2.0 has been largely\naligned with our previous survey on PLM-based\ntext generation (Li et al., 2022b) in terms of task,\nmodel, and training. It will be meaningful for be-\nginners to explore and learn text generation models\nwith the survey and supporting libraries.\nTo summarize, TextBox 2.0 has contributed a\nsigniﬁcant addition to the previous version (see Ta-\nble 1 for a detailed comparison) to better support\nthe use of PLMs for text generation. It implements\nand maintains a uniﬁed way to conduct research on\ntext generation with 45 included models, covering\n13 tasks, and 83 datasets. We also perform exten-\nsive test experiments, and these results show that\nTextBox 2.0 can produce very competitive perfor-\nmance compared to the original implementations.\n2 Library Design\nIn order to facilitate PLM-based text generation\nresearch, TextBox 2.0 has introduced various new\nfeatures, mainly from three aspects: generation\ntasks, generation models, and training strategies.\n2.1 Generation Tasks\nSince there are various text generation applications,\nwe include 13 widely studied tasks and collect the\ncorresponding 83 datasets.\nTasks. These 13 tasks in TextBox 2.0 include text\nsummarization, machine translation, open-ended\ndialogue system, data-to-text generation, question\ngeneration, question answering, story generation,\ntask-oriented dialogue system, commonsense gen-\neration, paraphrase generation, text style transfer,\nand text simpliﬁcation. Besides these English-\ncentric tasks, we also include Chinese generation\ntasks. Existing PLM-based libraries such as Hug-\nging Face (Wolf et al., 2020) are focused on per-\nforming extensive NLP tasks and only consider\na few text generation tasks (mainly text summa-\nrization and machine translation), which are not\ncomprehensive for text generation research.\nDatasets. For each task, we collect widely-used\ndatasets and reorganize them in a uniﬁed text-to-\ntext format. In total, we include 83 datasets, and re-\nport their details on the page1, including the dataset\ndescription, basic statistics, and training/valida-\ntion/testing samples. In addition, we build a leader-\nboard for each dataset by collecting the automatic\nresults and generated texts of the latest research.\nIt is convenient for users to quickly learn about\nthe baselines and their results. We also encourage\ncommunity users to collaboratively maintain the\nleaderboard and submit their model results.\n1https://github.com/RUCAIBox/TextBox#\ndataset\n436\nMetrics. To conduct evaluations with these tasks\nand datasets, TextBox 2.0 supports four categories\nof automatic metrics: (1) lexical metrics, such as\nBLEU (Papineni et al., 2002) and ROUGE (Lin,\n2004), to measure the n-gram overlap between gen-\nerated texts and golden texts; (2) semantic met-\nrics, such as BERTScore (Zhang et al., 2020b)\nand style strength (Lai et al., 2021), to com-\npare the texts at sentence level; (3) diversity met-\nrics, such as Distinct (Li et al., 2016) and Self-\nBLEU (Zhu et al., 2018), to evaluate the lexical\ndiversity of generated texts; (4) accuracy metrics,\nsuch as exact match (Rajpurkar et al., 2016) and in-\nform (Budzianowski et al., 2018a), to calculate the\nprecision of important phrases. In total, we include\n12 general metrics and 5 task-speciﬁc metrics2.\nBesides the analysis using automatic metrics,\nTextBox 2.0 provides several visualization tools to\nexplore and analyze the generated texts in various\ndimensions (Liu et al., 2021b; Tuckute et al., 2022).\nFor instance, Figure 2 shows how it offers new\ninsights to improve summarization tasks (details\ncan be found in Section 4.3).\n2.2 Generation Models\nTo support the rapid progress of PLMs on text gen-\neration, TextBox 2.0 incorporates 45 PLMs3 and\naims to build a uniﬁed and standardized framework\nbased on PLMs. We list some included models as\nfollows:\n•General PLMs: GPT-2 (Radford et al., 2019)\nand BART (Lewis et al., 2020);\n•Translation PLMs: mBART (Liu et al., 2020)\nand XLM (CONNEAU and Lample, 2019);\n•Chinese PLMs: CPM (Zhang et al., 2021) and\nCPT (Shao et al., 2021);\n•Dialogue PLMs : DialoGPT (Zhang et al.,\n2020c) and Blenderbot (Roller et al., 2021);\n•Controllable PLMs : CTRL (Keskar et al.,\n2019) and PPLM (Dathathri et al., 2020);\n•Distilled PLMs : DistilGPT2 (Sanh et al.,\n2019) and DistilBART (Shleifer and Rush, 2020).\n•Prompting PLMs: PTG (Li et al., 2022a) and\nContext-Tuning (Tang et al., 2022);\n• Lightweight modules : Adapter (Houlsby\net al., 2019), Preﬁx-tuning (Li and Liang, 2021).\nThe wide coverage of PLMs makes it possible\nto deal with different text generation tasks using\n2https://github.com/RUCAIBox/TextBox#\nevaluation\n3https://github.com/RUCAIBox/TextBox#\nmodel\nTextBox 2.0. For example, to perform speciﬁc\ntasks such as dialogue system, users can adopt\ntask-speciﬁc PLMs such as DialoGPT; to deal with\nChinese generation tasks, users can adopt CPT. In\nresource-constrained situations, lightweight PLMs\nsuch as preﬁx-tuning can be a good choice.\n2.3 Training Strategies\nTextBox 2.0 provides four pre-training objectives\nto help users pre-train a model from scratch, in-\ncluding language modeling (Radford et al., 2019),\nmasked sequence-to-sequence modeling (Song\net al., 2019), denoising auto-encoding (Lewis et al.,\n2020), and masked span prediction (Raffel et al.,\n2020). These pre-training tasks can also be utilized\nfor domain-adaptive pre-training and task-adaptive\npre-training (Gururangan et al., 2020) to tailor ex-\nisting PLM to the domain of a target task.\nAlso, TextBox 2.0 provides four useful training\nmethods for improving the optimization of PLMs.\nIt supports distributed data parallel to implement\nmodels on multiple GPUs and machines to improve\nthe efﬁciency of pre-training and ﬁne-tuning. We\nincorporate Accelerate4 to support distributed train-\ning with a simple API. To further accelerate the de-\ncoding efﬁciency, we integrate FastSeq (Yan et al.,\n2021) to optimize the decoding process by atten-\ntion cache optimization, repeated n-gram detection,\nand asynchronous parallel I/O.\nMoreover, TextBox 2.0 enables users to ad-\njust and select hyper-parameters automatically.\nBased on the library Hyperopt (Bergstra et al.,\n2013), users just need to set the parameter range\nand search methods, and then the optimal hyper-\nparameters and corresponding results will return. It\nis useful for PLMs to search for hyper-parameters\nsuch as batch size and learning rate. Our library\nalso supports performing repeat experiments us-\ning different random seeds in one command line,\nwhich is especially useful to alleviate randomness\nespecially under few-shot settings.\n3 Library Usage\nIn this section, we introduce how to use our library\nin four different kinds of research scenarios by\nshowing the example codes.\nReproducing existing models. TextBox 2.0 in-\ncludes various PLMs and supports many text gen-\neration tasks and datasets. It is convenient for users\n4https://github.com/huggingface/\naccelerate\n437\npython run_textbox.py \\\n--dataset=xsum --model=pegasus \\\n--model_path=google/pegasus-large\nclass new_model(AbstractModel):\ndef __init__(self):\n…\nself.t5 = T5_MODEL\nself.gnn = GNN_MODEL\ndef forward(self, input, label):\n…\nembeds = self.gnn(input)\noutput = self.t5(embeds)\nreturn loss_func(output, label)\ndef generate(self, input):\n…\nembeds = self.gnn(input)\nreturn self.t5.generate(embeds)\npython run_textbox.py \\\n--dataset=wudao --model=bart \\\n--pretrain_task=denoising\n# hyper.test\nLearning_rate choice [1e-5, 3e-5]\ntrain_batch_size choice [64, 256]\n# command line instruction\npython run_hyper.py \n\\\n--space=hyper.test \\\n--dataset=xsum --model=pegasus \\\n--model_path=google/pegasus-large\naccelerate config\naccelerate launch run_textbox.py \\\n--dataset=wudao --model=bart \\\n--pretrain_task=denoising\npython run_analysis.py \\\n--dataset=cnndm \\\nBART_output.txt T5_output.txt\n(a) Example for reproducing existing models\n(b) Example for hyper-parameter optimization (c) Example for implementing a new model (f) Example for analyzing\n(e) Example for multi-GPU pre-training\n(d) Example for pre-training a Chinese BART\nFigure 1: Example usage of our TextBox 2.0.\nto quickly run existing PLMs and reproduce results\nfor each dataset. In particular, users only need to\nspecify the dataset and model by setting the conﬁg-\nurations dataset, model, and model_path,\nwithin a simple command line.\nFigure 1(a) presents an example to ﬁne-tune PE-\nGASUS (Zhang et al., 2020a) on XSum (Narayan\net al., 2018) dataset. Moreover, TextBox 2.0 en-\nables users to conduct hyper-parameter optimiza-\ntion by only providing a list of possible values.\nFigure 1(b) shows an example that automatically\nadjusts the hyper-parameters learning_rate\nand batch_size from the ranges [1 ×10−5, 3 ×\n10−5] and [64, 256], respectively.\nImplementing a new model. Since TextBox 2.0\nbuilds a uniﬁed pipeline for text generation re-\nsearch, users only need to deﬁne a new model class\nwithout considering other procedures to implement\na new model. Specially, users should ﬁrst inherit\nfrom our base model class AbstractModel be-\nfore specifying three speciﬁc model functions:\n(1) __init__(): this function initializes the\narchitectures and parameters of the model; (2)\nforward(): this function is used to calcu-\nlate the loss for optimization during training; (3)\ngenerate(): this function generates texts based\non input during inference.\nFigure 1(c) presents an example of implementing\na new model for the KG-to-text generation task . In\nthis example, the model adopts a graph neural net-\nwork (GNN) to encode KG and then uses T5 (Raf-\nfel et al., 2020) to generate texts. We ﬁrst deﬁne the\nGNN and T5 models in the __init__() func-\ntion. Then, we use GNN to encode KG to em-\nbeddings as the input of T5 and compute the loss\naccording to target labels in the forward() func-\ntion. Finally, we use a similar process to generate\ntext in the generate() function.\nPre-training a new model. In TextBox 2.0, we\nprovide several pre-training objectives for users\nto pre-train new models from scratch. Speciﬁ-\ncally, users just need to specify the pre-training\ntask, pre-training corpus, and architecture by set-\nting pretrain_task, dataset, and model.\nFigure 1(d) shows an example that pre-trains a Chi-\nnese BART on the WuDaoCorpora (Yuan et al.,\n2021) using the denoising pre-training objective.\nTo improve the pre-training efﬁciency, TextBox\n2.0 supports distributed data parallel and efﬁcient\ndecoding (Section 2.3). Figure 1(e) shows an\nillustrative example of how users can use the\naccelerate command to set conﬁgurations of\nmultiple devices and launch the training code.\nAnalyzing generated results. Besides simply\nobtaining the evaluation results, our library pro-\nvides several visualization analysis mechanisms\nto perform deep analysis on the generated results\nof models. For example, we support the use of\nthe statistical chart to analyze the mean and stan-\ndard deviation scores for different sentence lengths.\nThese methods can help users learn about the ad-\nvantages and disadvantages of different models in a\ndetailed comparison. Figure 1(f) shows an example\nof how to run the analysis using a simple command\nline and the results can be found in Figure 2. This\nexample compares the generated texts of BART\nand T5 on the CNN/DailyMail dataset.\n4 Experiments\nIn this section, we conduct extensive experiments\nto verify the generation abilities of TextBox 2.0.\n4.1 Result Reproduction\nAs an open-source library, TextBox 2.0 should be\nable to reproduce the results of existing work ef-\nfectively. To verify this, we select a number of\n438\nText Summarization Text Simpliﬁcation Chinese Generation Translation\nR-1 R-2 R-L B- 4 ME R- 2 LCSTS CSL ADGEN En →Ro Ro→En\nBART 44.16a 21.28 40.90 88.30 b 55.60 86.10 40.60 c 64.20 10.00 37.70 d 37.80\nBART (ours) 44.470.10 21.500.14 41.350.08 90.810.24 57.580.19 83.360.07 42.960.18 64.340.63 10.200.15 37.200.17 37.480.31\nData-to-text Generation Commonsense Generation Question Generation QA\nB-4 ME R-L B- 4 CIDEr SPICE B- 4 ME R-L F1 EM\nBART 64.55e 46.51 75.13 27.50 f 14.12 30.00 22.00 g 26.40 50.30 91.56 h 84.23\nBART (ours) 67.330.06 47.780.07 76.830.04 28.180.45 12.980.13 33.000.40 25.080.13 26.730.18 52.550.07 93.040.08 86.440.21\nOpen-ended Dialogue System Task-oriented Dialogue System Story Generation\nB-1 B-2 D-1 D-2 B-4 Success Inform Comb. B- 1 B-2 D-4\nBART 49.90g 40.00 1.30 8.00 17.89 i 74.91 84.88 97.78 30.70 j 13.30 69.90\nBART (ours) 49.581.12 39.240.90 1.440.09 8.890.57 20.170.63 75.401.22 84.401.15 100.070.53 33.790.13 15.780.21 78.762.15\nParaphrase Generation Style Transfer (E&M) Style Transfer (F&R)\nB-4 ME R- 1 R-2 R-L B- 4 Acc. HM B- 4 Acc. HM\nBART 47.30k 49.70 73.30 54.10 75.10 76.50 l 92.90 83.90 79.30 92.00 85.20\nBART (Ours) 48.350.70 50.600.49 74.160.47 55.250.74 75.840.42 76.930.55 94.370.87 84.740.05 80.110.29 92.290.37 85.770.10\nTable 2: The results of BART on thirteen tasks from the original papers and our TextBox 2.0. QA is short for\nquestion answering. B, R, D, ME, EM, HM, Acc., and Comb. denote BLEU, ROUGE, Distinct, METEOR, exact\nmatch, harmonic mean, accuracy, and combined score, respectively. LCSTS, CSL, ADGEN, and En ↔Ro are\nevaluated using the R-L, R-L, B- 4, and B- 4 metrics, respectively. a(Lewis et al., 2020) b(Gehrmann et al.,\n2021) c(Shao et al., 2021) d(Liu et al., 2020) e(Ke et al., 2021) f (Lin et al., 2020a) g(Liu et al., 2021a)\nh(Xu et al., 2021) i(Lin et al., 2020b) j(Guan et al., 2021) k(Sun et al., 2021) l(Lai et al., 2021)\nwidely-used datasets for each task (introduced in\nSection 2.1) and compare the results conducted\nby TextBox 2.0 with those in the original papers.\nWe totally evaluate 13 tasks using 14 datasets, in-\ncluding CNN/DailyMail (See et al., 2017), Wiki-\nAuto + Turk (Liu et al., 2021a), LCSTS (Hu\net al., 2015), CSL 5, ADGEN (Shao et al., 2019),\nWMT 16 English-Romanian (En↔Ro) (Bojar et al.,\n2016), WebNLG 2.1 (Gardent et al., 2017), Com-\nmonGen (Lin et al., 2020a), SQuAD (Rajpurkar\net al., 2016), PersonaChat (Zhang et al., 2018),\nMultiWOZ 2.0 (Budzianowski et al., 2018b),\nROCStories (Mostafazadeh et al., 2016), GY AFC\n(E&M and F&R) (Rao and Tetreault, 2018), and\nQuora (Kumar et al., 2020).\nSince BART is the prevalent PLM for text gen-\neration, we endeavor to reproduce existing works\nwith BARTLARGE\n6. For all experiments, we em-\nploy the sequence-to-sequence cross-entropy loss\nwith a label smoothing factor of 0.1 as the ob-\njective function. We optimize the model using\nAdamW (Loshchilov and Hutter, 2019) with a con-\nstant learning rate of 3 ×10−5. The accumulated\nbatch size is set to 192. During inference, we apply\nbeam search with a beam size of 5 and no-repeat\n5https://github.com/CLUEbenchmark/CLGE\n6For translation tasks, we utilize mBART-CC25 (Liu et al.,\n2020). For Chinese generation tasks, we utilize Chinese\nBARTLARGE (Shao et al., 2021).\nLibrary Preparation Training Generation\n(minutes) (minutes) (minutes)\nFairseq 2.930.02 410.058.86 79.241.50\nHugging Face 4.020.12 416.254.47 75.692.53\nTextBox 2.0 3.810.14 393.995.09 27.051.03\nTable 3: Efﬁciency comparison of three libraries for\nBARTLARGE ﬁne-tuned on CNN/DailyMail. The prepa-\nration stage consists of conﬁguration loading, text to-\nkenization, and necessary initialization options. The\ntraining stage takes time for ﬁne-tuning on the training\nset in one epoch. The generation stage takes time to\ngenerate on the test set with a beam size of 5.\nn-gram size of 3. To reduce randomness, we re-\nport the mean and standard deviation of our results\nbased on three random seeds: 2020, 2021, and\n2022. All codes are implemented in PyTorch 1.11.0\non Ubuntu SMP 20.04.1 (Linux 5.15.0-46) with\none GPU (NVIDIA GeForce RTX 3090 24GB).\nTo conduct these experiments, we only need to\nrun the script shown in Figure 1 (a) with differ-\nent dataset names. As shown in Table 2, our\nTextBox 2.0 can faithfully reproduce the results\nreported in existing work. Remarkably, our library\nachieves better performances than original works\non 37 of the 44 metrics evaluated. It might be\nbecause we adopt optimization strategies such as\nlabel smoothing and large batch sizes.\n439\n(a) Leaderboard of CNN/DailyMail\n (b) ROUGE-L scores of BART and T5\nfor different input lengths\n(c) N-gram overlap of target and gener-\nated texts with input document\nFigure 2: The partial visualization analysis on CNN/DailyMail dataset. The whole one can be found at https:\n//github.com/RUCAIBox/TextBox/blob/2.0.0/asset/example-analysis.html.\n4.2 Efﬁciency Comparison\nIn addition to accurately reproducing results, we\nhave optimized TextBox 2.0 for computational ef-\nﬁciency. We streamline the training process and\nsupport efﬁcient decoding strategies. To compare\nthe efﬁciency, we choose the well-known PLM li-\nbraries Fairseq7 and Hugging Face8, and then test\nthe time consumption under identical settings de-\nscribed in Section 4.1.\nFrom the results in Table 3, we can see that our\nTextBox 2.0 is more efﬁcient than Fairseq and Hug-\nging Face. During training, TextBox 2.0 simpliﬁes\nthe training process and reduces the time spent on\nnon-essential functions such as trainer management\nand loss tracking. In the generation process, our\nlibrary is signiﬁcantly faster than the other two\nlibraries due to the incorporation of efﬁcient decod-\ning strategies introduced in Section 2.3.\n4.3 Visualization Analysis\nBesides reproducing a model, it is also important to\ncompare existing methods, analyze the generated\ntexts, and explore directions for improvement. Our\nlibrary sets a speciﬁc leaderboard for each dataset,\nincluding basic metric results, author repositories,\nand generated texts. Figure 2 (a) showcases the\nleaderboard for the CNN/DailyMail dataset.\nUsers can also utilize TextBox 2.0 to conduct\nvisualization analysis for speciﬁed models. For\nexample, our library can automatically plot the\nboxplot of the ROUGE-L score for different in-\nput lengths and the n-gram overlap of target and\ngenerated texts with the input document. From\nthe results in Figure 2 (b), we can ﬁnd that T5 ex-\n7We utilize the code from Fairseq 0.12.2.\n8We utilize the code from Transformers 4.20.1.\ncels at short document summarization while BART\nexcels at long document summarization. It is use-\nful to analyze and improve the deﬁciencies of text\ngeneration models or obtain better performance\nby combining their results. As another example,\nFigure 2 (c) illustrates that BART and T5 have a sig-\nniﬁcantly higher n-gram overlap ratio than golden\nsentences, indicating that they tend to “copy” the\ninput document rather than “summarize” it. From\nsuch analysis results, users can apply the methods\nproposed by Goyal et al. (2022) to alleviate it.\n5 Conclusion\nThis paper presented TextBox 2.0, a comprehen-\nsive and uniﬁed library for conducting research on\nPLM-based text generation. Our library makes sig-\nniﬁcant extensions in three major aspects, namely\ngeneration tasks (13 tasks and 83 datasets), genera-\ntion models (45 PLMs), and training strategies (e.g.,\ndistributed data parallel and efﬁcient decoding). Re-\nsults from extensive test experiments demonstrate\nthat our library can accurately reproduce existing\nmodels. Besides, we also provide a series of utility\ntools to better analyze and explore the generated re-\nsults. To summarize, our library can be very useful\nto facilitate text generation research, and our team\nwill improve this library with regular updates.\nAcknowledgement\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nand Beijing Outstanding Young Scientist Program\nunder Grant No. BJJWZYJH012019100020098.\nXin Zhao is the corresponding author.\n440\nReferences\nJames Bergstra, Daniel Yamins, and David Cox. 2013.\nMaking a science of model search: Hyperparameter\noptimization in hundreds of dimensions for vision ar-\nchitectures. In Proceedings of the 30th International\nConference on Machine Learning, volume 28, pages\n115–123.\nOndˇrej Bojar, Christian Buck, Rajen Chatterjee, Chris-\ntian Federmann, Liane Guillou, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Aurélie\nNévéol, Mariana Neves, Pavel Pecina, Martin Popel,\nPhilipp Koehn, Christof Monz, Matteo Negri, Matt\nPost, Lucia Specia, Karin Verspoor, Jörg Tiedemann,\nand Marco Turchi, editors. 2016. Proceedings of the\nFirst Conference on Machine Translation: Volume\n1, Research Papers. Association for Computational\nLinguistics, Berlin, Germany.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018a. MultiWOZ - a\nlarge-scale multi-domain Wizard-of-Oz dataset for\ntask-oriented dialogue modelling. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5026, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018b. MultiWOZ - a\nlarge-scale multi-domain Wizard-of-Oz dataset for\ntask-oriented dialogue modelling. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5026, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017. A survey on dialogue systems: Re-\ncent advances and new frontiers. SIGKDD Explor.\nNewsl., 19(2).\nAlexis CONNEAU and Guillaume Lample. 2019.\nCross-lingual language model pretraining. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc.\nRaj Dabre, Chenhui Chu, and Anoop Kunchukuttan.\n2020. A survey of multilingual neural machine\ntranslation. ACM Comput. Surv., 53(5).\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language mod-\nels: A simple approach to controlled text generation.\nIn International Conference on Learning Represen-\ntations.\nWafaa S. El-Kassas, Cherif R. Salama, Ahmed A.\nRafea, and Hoda K. Mohamed. 2021. Automatic\ntext summarization: A comprehensive survey. Ex-\npert Systems with Applications, 165:113679.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating train-\ning corpora for NLG micro-planners. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 179–188, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du,\nEsin Durmus, Ond ˇrej Dušek, Chris Chinenye\nEmezue, Varun Gangal, Cristina Garbacea, Tat-\nsunori Hashimoto, Yufang Hou, Yacine Jernite,\nHarsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mi-\nhir Kale, Dhruv Kumar, Faisal Ladhak, Aman\nMadaan, Mounica Maddela, Khyati Mahajan,\nSaad Mahamood, Bodhisattwa Prasad Majumder,\nPedro Henrique Martins, Angelina McMillan-\nMajor, Simon Mille, Emiel van Miltenburg, Moin\nNadeem, Shashi Narayan, Vitaly Nikolaev, Andre\nNiyongabo Rubungo, Salomey Osei, Ankur Parikh,\nLaura Perez-Beltrachini, Niranjan Ramesh Rao,\nVikas Raunak, Juan Diego Rodriguez, Sashank\nSanthanam, João Sedoc, Thibault Sellam, Samira\nShaikh, Anastasia Shimorina, Marco Antonio\nSobrevilla Cabezudo, Hendrik Strobelt, Nishant\nSubramani, Wei Xu, Diyi Yang, Akhila Yerukola,\nand Jiawei Zhou. 2021. The GEM benchmark: Nat-\nural language generation, its evaluation and metrics.\nIn Proceedings of the 1st Workshop on Natural\nLanguage Generation, Evaluation, and Metrics\n(GEM 2021), pages 96–120, Online. Association for\nComputational Linguistics.\nTanya Goyal, Jiacheng Xu, Junyi Jessy Li, and Greg\nDurrett. 2022. Training dynamics for text summa-\nrization models. In Findings of the Association\nfor Computational Linguistics: ACL 2022 , pages\n2061–2073, Dublin, Ireland. Association for Com-\nputational Linguistics.\nJian Guan, Xiaoxi Mao, Changjie Fan, Zitao Liu, Wen-\nbiao Ding, and Minlie Huang. 2021. Long text gen-\neration by modeling sentence-level and discourse-\nlevel coherence. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6379–6393, Online. Association for\nComputational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\n441\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn ICML.\nBaotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LC-\nSTS: A large scale Chinese short text summarization\ndataset. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1967–1972, Lisbon, Portugal. Association for\nComputational Linguistics.\nPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Lin-\nfeng Song, Xiaoyan Zhu, and Minlie Huang. 2021.\nJointGT: Graph-text joint representation learning for\ntext generation from knowledge graphs. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021 , pages 2526–2538, Online.\nAssociation for Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Vincent\nNguyen, Jean Senellart, and Alexander Rush. 2018.\nOpenNMT: Neural machine translation toolkit. In\nProceedings of the 13th Conference of the Associa-\ntion for Machine Translation in the Americas (Vol-\nume 1: Research Track) , pages 177–184, Boston,\nMA. Association for Machine Translation in the\nAmericas.\nAshutosh Kumar, Kabir Ahuja, Raghuram Vadapalli,\nand Partha Talukdar. 2020. Syntax-guided con-\ntrolled generation of paraphrases. Transactions\nof the Association for Computational Linguistics ,\n8:329–345.\nHuiyuan Lai, Antonio Toral, and Malvina Nissim.\n2021. Thank you BART! rewarding pre-trained\nmodels improves formality style transfer. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) , pages 484–\n494, Online. Association for Computational Linguis-\ntics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nJunyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaox-\nuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu,\nWayne Xin Zhao, and Ji-Rong Wen. 2021. TextBox:\nA uniﬁed, modularized, and extensible framework\nfor text generation. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Con-\nference on Natural Language Processing: System\nDemonstrations, pages 30–39, Online. Association\nfor Computational Linguistics.\nJunyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, and\nXin Zhao. 2022a. Learning to transfer prompts for\ntext generation. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 3506–3518, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,\nand Ji-Rong Wen. 2022b. A survey of pretrained lan-\nguage models based text generation. arXiv preprint\narXiv:2201.05273.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020a. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020b. MinTL: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3391–3405, Online. Association for Computa-\ntional Linguistics.\nDayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi,\nHang Zhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun\nShou, Ming Gong, Pengcheng Wang, Jiusheng Chen,\nDaxin Jiang, Jiancheng Lv, Ruofei Zhang, Winnie\nWu, Ming Zhou, and Nan Duan. 2021a. GLGE: A\n442\nnew general language generation evaluation bench-\nmark. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021 , pages 408–\n420, Online. Association for Computational Linguis-\ntics.\nPengfei Liu, Jinlan Fu, Yang Xiao, Weizhe Yuan,\nShuaichen Chang, Junqi Dai, Yixin Liu, Zihuiwen\nYe, and Graham Neubig. 2021b. ExplainaBoard:\nAn explainable leaderboard for NLP. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing: System Demonstrations , pages 280–289,\nOnline. Association for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nAlexander Miller, Will Feng, Dhruv Batra, Antoine\nBordes, Adam Fisch, Jiasen Lu, Devi Parikh, and\nJason Weston. 2017. ParlAI: A dialog research soft-\nware platform. In Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations , pages 79–84,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nSudha Rao and Joel Tetreault. 2018. Dear sir or\nmadam, may I introduce the GY AFC dataset: Cor-\npus, benchmarks and metrics for formality style\ntransfer. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers) , pages 129–140,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume , pages 300–325,\nOnline. Association for Computational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nYunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai,\nFei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu.\n2021. Cpt: A pre-trained unbalanced transformer\nfor both chinese language understanding and gener-\nation. arXiv preprint arXiv:2109.05729.\n443\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei\nXu, and Xiaoyan Zhu. 2019. Long and diverse text\ngeneration with planning-based hierarchical varia-\ntional model. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 3257–3268, Hong Kong, China. As-\nsociation for Computational Linguistics.\nSam Shleifer and Alexander M Rush. 2020. Pre-\ntrained summarization distillation. arXiv preprint\narXiv:2010.13002.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA , volume 97 of Pro-\nceedings of Machine Learning Research , pages\n5926–5936. PMLR.\nJiao Sun, Xuezhe Ma, and Nanyun Peng. 2021. AE-\nSOP: Paraphrase generation with adaptive syntactic\ncontrol. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5176–5189, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nTianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong\nWen. 2022. Context-tuning: Learning contextual-\nized prompts for natural language generation. In\nProceedings of the 29th International Conference\non Computational Linguistics , pages 6340–6354,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nGreta Tuckute, Aalok Sathe, Mingye Wang, Harley\nYoder, Cory Shain, and Evelina Fedorenko. 2022.\nSentSpace: Large-scale benchmarking and evalua-\ntion of text using cognitively motivated lexical, syn-\ntactic, and semantic features. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies: System Demonstra-\ntions, pages 99–113, Hybrid: Seattle, Washington +\nOnline. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nPeng Xu, Davis Liang, Zhiheng Huang, and Bing\nXiang. 2021. Attention-guided generative models\nfor extractive question answering. arXiv preprint\narXiv:2110.06393.\nYu Yan, Fei Hu, Jiusheng Chen, Nikhil Bhendawade,\nTing Ye, Yeyun Gong, Nan Duan, Desheng Cui,\nBingyu Chi, and Ruofei Zhang. 2021. FastSeq:\nMake sequence generation faster. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations, pages 218–226, Online. As-\nsociation for Computational Linguistics.\nSha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding,\nXiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie\nTang. 2021. Wudaocorpora: A super large-scale chi-\nnese corpora for pre-training language models. AI\nOpen, 2:65–68.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020a. PEGASUS: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37th International Conference\non Machine Learning, pages 11328–11339.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213, Melbourne, Australia. Association for Com-\nputational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. Bertscore:\nEvaluating text generation with bert. In Interna-\ntional Conference on Learning Representations.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020c. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270–\n278, Online. Association for Computational Linguis-\ntics.\nZhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian\nGu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji,\nJian Guan, et al. 2021. Cpm: A large-scale gener-\native chinese pre-trained language model. AI Open,\n2:93–99.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. In The 41st International ACM SIGIR Con-\nference on Research & Development in Information\nRetrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-\n12, 2018, pages 1097–1100. ACM.\n444"
}