{
    "title": "TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution",
    "url": "https://openalex.org/W4318953863",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2966853964",
            "name": "Armin Mehri",
            "affiliations": [
                "Universitat Autònoma de Barcelona"
            ]
        },
        {
            "id": "https://openalex.org/A3047016652",
            "name": "Parichehr Behjati",
            "affiliations": [
                "Universitat Autònoma de Barcelona"
            ]
        },
        {
            "id": "https://openalex.org/A4290043323",
            "name": "Angel Domingo Sappa",
            "affiliations": [
                "Universitat Autònoma de Barcelona"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1930824406",
        "https://openalex.org/W2150081556",
        "https://openalex.org/W2088254198",
        "https://openalex.org/W1950594372",
        "https://openalex.org/W54257720",
        "https://openalex.org/W2964101377",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W6845935626",
        "https://openalex.org/W2154184228",
        "https://openalex.org/W1975026017",
        "https://openalex.org/W2743197123",
        "https://openalex.org/W2130446802",
        "https://openalex.org/W2905727092",
        "https://openalex.org/W2332427732",
        "https://openalex.org/W4285527993",
        "https://openalex.org/W2503339013",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W2242218935",
        "https://openalex.org/W2964125708",
        "https://openalex.org/W2963372104",
        "https://openalex.org/W2963645458",
        "https://openalex.org/W6782245826",
        "https://openalex.org/W3104028135",
        "https://openalex.org/W3118519535",
        "https://openalex.org/W3129158555",
        "https://openalex.org/W6759278257",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W6780669243",
        "https://openalex.org/W3113067059",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W6800099329",
        "https://openalex.org/W6790749177",
        "https://openalex.org/W6796298902",
        "https://openalex.org/W6803518762",
        "https://openalex.org/W4241716071",
        "https://openalex.org/W2254462240",
        "https://openalex.org/W3215835382",
        "https://openalex.org/W2520322935",
        "https://openalex.org/W2914997670",
        "https://openalex.org/W2913533015",
        "https://openalex.org/W2575376020",
        "https://openalex.org/W2760586727",
        "https://openalex.org/W2990674770",
        "https://openalex.org/W2195231623",
        "https://openalex.org/W2794124273",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W6805224724",
        "https://openalex.org/W4312594135",
        "https://openalex.org/W2012893378",
        "https://openalex.org/W6653714346",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W4312820606",
        "https://openalex.org/W3194042166",
        "https://openalex.org/W3081557952",
        "https://openalex.org/W4312812783",
        "https://openalex.org/W3039153384",
        "https://openalex.org/W3209478434",
        "https://openalex.org/W2964007263",
        "https://openalex.org/W4225672218",
        "https://openalex.org/W4287330714",
        "https://openalex.org/W4200548007",
        "https://openalex.org/W2913541967"
    ],
    "abstract": "Image Super Resolution is a potential approach that can improve the image quality of low-resolution optical sensors, leading to improved performance in various industrial applications. It is important to emphasize that most state-of-the-art super resolution algorithms often use a single channel of input data for training and inference. However, this practice ignores the fact that the cost of acquiring high-resolution images in various spectral domains can differ a lot from one another. In this paper, we attempt to exploit complementary information from a low-cost channel (visible image) to increase the image quality of an expensive channel (infrared image). We propose a dual stream Transformer-based super resolution approach that uses the visible image as a guide to super-resolve another spectral band image. To this end, we introduce Transformer in Transformer network for Guidance super resolution, named TnTViT-G, an efficient and effective method that extracts the features of input images via different streams and fuses them together at various stages. In addition, unlike other guidance super resolution approaches, TnTViT-G is not limited to a fixed upsample size and it can generate super-resolved images of any size. Extensive experiments on various datasets show that the proposed model outperforms other state-of-the-art super resolution approaches. TnTViT-G surpasses state-of-the-art methods by up to <inline-formula> <tex-math notation=\"LaTeX\">$0.19 \\sim 2.3dB $ </tex-math></inline-formula>, while it is memory efficient.",
    "full_text": "Received 8 January 2023, accepted 29 January 2023, date of publication 2 February 2023, date of current version 8 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3241852\nTnTViT-G: Transformer in Transformer Network\nfor Guidance Super Resolution\nARMIN MEHRI\n 1, PARICHEHR BEHJATI1,\nAND ANGEL DOMINGO SAPPA1,2, (Senior Member, IEEE)\n1Computer Vision Center, Autonomous University of Barcelona, 08193 Barcelona, Spain\n2ESPOL Polytechnic University, Guayaquil EC090112, Ecuador\nCorresponding author: Armin Mehri (amehri@cvc.uab.es)\nThis work supported in part by the Air Force Office of Scientific Research under Award FA9550-22-1-0261, in part by the Grant\nPID2021-128945NB-I00 funded by MCIN/AEI/10.13039/501100011033 and by ‘‘ERDF A way of making Europe’’, in part by the\n‘‘CERCA Programme/Generalitat de Catalunya,’’ and in part by the ESPOL Project CIDIS-12-2022.\nABSTRACT Image Super Resolution is a potential approach that can improve the image quality of\nlow-resolution optical sensors, leading to improved performance in various industrial applications. It is\nimportant to emphasize that most state-of-the-art super resolution algorithms often use a single channel\nof input data for training and inference. However, this practice ignores the fact that the cost of acquiring\nhigh-resolution images in various spectral domains can differ a lot from one another. In this paper, we attempt\nto exploit complementary information from a low-cost channel (visible image) to increase the image quality\nof an expensive channel (infrared image). We propose a dual stream Transformer-based super resolution\napproach that uses the visible image as a guide to super-resolve another spectral band image. To this\nend, we introduce Transformer in Transformer network for Guidance super resolution, named TnTViT-G,\nan efficient and effective method that extracts the features of input images via different streams and fuses\nthem together at various stages. In addition, unlike other guidance super resolution approaches, TnTViT-G\nis not limited to a fixed upsample size and it can generate super-resolved images of any size. Extensive\nexperiments on various datasets show that the proposed model outperforms other state-of-the-art super\nresolution approaches. TnTViT-G surpasses state-of-the-art methods by up to 0.19 ∼ 2.3dB, while it is\nmemory efficient.\nINDEX TERMS Single image super resolution, guidance super resolution, transformers, convolutional\nneural network.\nI. INTRODUCTION\nIn recent years, image super resolution, has achieved signifi-\ncant interest from both academic and industrial communities.\nThe process of reconstructing a high-resolution (HR) image\nfrom its low-resolution (LR) counterpart is referred to as\nthe super resolution (SR) problem in the field of image\nprocessing. Due to the fact that a single LR image might\nhave numerous mappings from LR to SR, SR is an ill-posed\nproblem, which is also known as a one-to-many problem.\nThus, numerous SR methods have been introduced to\nreconstruct a high-resolution image from its low-resolution\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Andrea F. Abate\n.\nones, such as traditional approaches like the self-exemplars\napproach [1], anchoring neighborhood regression [2], sparse\nrepresentation [3] and random forest [4].\nMore recently, by advancing the deep learning approaches,\nseveral Convolutional Neural Networks (CNNs) and Trans-\nformer networks are being used as a solution for the\nill-posed SR problem. This is largely attributable to the\nrecent successes of deep learning approaches in a variety\nof vision tasks, such as object detection, image recognition,\nsemantic segmentation, image classification, and many\nothers. The first work in this direction has been presented\nby Dong et al. [5], who developed a three-layer CNN\nmodel to train a nonlinear LR-to-HR mapping function called\nSRCNN, which greatly outperforms the traditional machine\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 11529\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nFIGURE 1. An example of super-resolved results comparison on M3FD\ndataset for scale factor×4.\nlearning-based methods. The majority of later expansions\nof SRCNN enhance SR accuracy by employing more\ncomplicated network designs (such as RDN [6], EDSR [6],\nRCAN [7], among others) or by utilizing a training dataset\nwith better quality.\nHowever, in real-world applications, the environment\naround us is dynamic and changing all the time due to many\nknown and unknown reasons, which require dealing with var-\nious challenging conditions such as rain, fog, occlusions, poor\nlighting, low resolution, and many others. All these factors\nmake it difficult for an algorithm that uses only a visible-band\nsensor (RGB) to achieve high performance under these\nconditions [8]. Therefore, the visible image is found to\nbe insufficient for such cases, and cross-spectral images\nhave become increasingly necessary in many applications\nas they are robust against obstacles in visual environments\nand provide support to the RGB images. Cross-spectral\nimages (e.g., infrared images) have been used in many\nranges of specialized fields such as surveillance [9], military\naffairs [10], pedestrian tracking [11], firefighting [12] and\nmany others. However, the associated costs of having\nsuch images grow significantly with the increase in their\nresolution.\nVarious approaches and algorithms have been proposed\nto improve the resolution of different infrared images using\nhardware or software. Employing low-resolution cameras\nthat are less expensive than high-end cross-spectral domain\ncameras and using SR methods to increase the resolution\nof such images is one strategy to boost the consumer\napplicability of such cameras to deal with a challenging\nsituation at a lower cost. However, as previously stated, single\nimage SR (SISR) is a tricky operation that becomes even\nmore difficult when the input image has a very poor resolution\n(such as the ones produced by inexpensive cross/multi-\nspectral sensors), which SISR techniques may hallucinate\nmissing details from low-resolution inputs and therefore\nyielding to artifacts [13].\nTo address the aforementioned problems, a fundamental\nsolution is to take advantage of any additional information\nthat can be found with the low-resolution infrared images as\nmost cross-spectral cameras are accompanied by an inbuilt\nvisible RGB camera with higher resolution. As a result,\nit is permissible to use low-cost visible images as additional\ninformation to considerably improve the accuracy of the\nSR results obtained from the costly infrared images. For\nexample, long-wave infrared (LWIR) detectors, required to\ncapture thermal images, are sealed inside their own separate\nvacuum packages in order to carry out the high-precision\nthermal measurement, which is a procedure that is both\ntime-consuming and costly [14]. As a result, the cost of\nLWIR sensors is much higher than that of RGB ones with\ncomparable spatial resolutions. The majority of commercially\navailable LWIR cameras capture LR images (for example,\n160 × 120 or even 80 × 60 pixels) [15], in which significant\ninformation is severely lost.\nIn this paper, we attempt to boost the performance of\nimage restoration in the expensive channel by taking into\naccount the complementary information captured by an\nadditional low-cost visible sensor. The primary focus of\nthis work is to build a deep learning model that applies\nmultimodal sensor fusion using visible cross-spectral images.\nThe proposed approach is evaluated with two different\nschemes (i.e., thermal infrared (LWIR), and near infrared\n(NIR)), but is also valid for any other input data. The proposed\nmodel accepts two images as inputs and integrates them\nin such a way as to enhance the generated infrared image\nresolution with fine detail with the help of the corresponding\nvisible image. Thus, a guidance super resolution network\n(TnTViT − G) is proposed to enhance the LR infrared\nimage by integrating the rich information from the HR visual\nimage. We show that HR visual images can help the model\nfill in the missing values and generate higher frequency\ndetails in the reconstructed SR infrared image, as shown in\nFig. 1.\nThe main contributions can be summarized as follows:\n• We present TnTViT-G, an efficient dual-stream\nTransformer-based network for guidance super resolu-\ntion (GSR) task. The TnTViT-G Transformer blocks are\nbuilt on top of the idea of the Vision Transformer (ViT)\nby completely revising the self-attention layer.\n• We present a lightweight Dual Attention layer that\nsignificantly improves the reconstruction quality by\ngenerating a global attention map from two local\nattention weights, which are obtained individually by\ntwo branches in parallel while it is not memory hunger.\n• We present a high-quality arbitrary upsampling module,\nwhich is capable of producing SR images at any scale\nfactor.\n• Extensive experiments show that TnTViT-G outper-\nforms CNN and transformer-based networks on different\nbenchmark datasets for the GSR task.\nThe rest of the paper is organized as follows: Section II\ndiscusses the related work. Section III describes the proposed\nTnTViT-G and its core components in detail. Experimental\ncomparisons against several state-of-the-art methods are\npresented in Section IV. The model investigation is presented\nin section V. Section VI concludes the paper.\n11530 VOLUME 11, 2023\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nFIGURE 2. The overall network architecture of the proposed TnTViT-G.\nII. RELATED WORK\nIn this section, the most recent state-of-the-art SR deep learn-\ning CNN and Transformer-based approaches are detailed.\nA. DEEP LEARNING BASED SINGLE IMAGE\nSUPER-RESOLUTION\nSingle Image Super Resolution aims to restore the\nwell-detailed image from its low-quality counterpart. The\nfirst deep learning-based work has been introduced by\nDong et al. [5] (SRCNN), which uses a convolutional neural\nnetwork to tackle the SR task. The SRCNN presents a\nshallow neural network that receives an upsampled image as\nan input that costs extra computation. Later on, to address\nthis drawback, FSRCNN [16] and ESPCN [17] have been\nproposed by receiving the LR image as input to reduce the\nlarge computational and run-time cost and upsampling the\nfeatures near the output of the network by a single transposed\nconvolution layer. Even though the strength of deep learning\ncomes from deep layers, the above-mentioned methods are\nreferred to as shallow networks. Therefore, Kim et al. [18] use\nresidual learning to ease the training challenges and increase\nthe depth of their network by adding 20 convolutional\nlayers. Then, [19] proposes a memory block in MemNet\nfor deeper networks and solves the problem of long-term\ndependency with 84 layers. Lim et al. [20] introduces EDSR\nby expanding the network size and enhancing the residual\nblock by omitting the batch normalization from the residual\nblock. Zhang et al. [6] propose RDN with residual and dense\nskip connections to fully use hierarchical features.\nMoreover, in recent years, there has been a rise in interest\nin developing lightweight approaches for super resolution\ntasks in order to lower the high computing cost of SR\ntask. Ahn et al. [21] design an efficient network that is\nsuitable for the mobile scenario. Later, [22] introduces\nMAFFSRN by proposing multi-attention blocks to improve\nthe performance. LatticeNet [23] introduces an economical\nstructure to adaptively combine residual blocks. Recently,\nOverNet presented by [24], designs an efficient network\nstructure with a multi-loss function to boost the network’s\nperformance. Also, a neural architecture search (NAS)-\nbased strategy has been proposed in SISR to construct\nefficient networks. MoreMNA-S [25] and FALSR [26] are\ntwo examples of networks that use NAS strategies in their\nnetwork. However, due to the limitations of the NAS strategy,\nthe performance of these models is limited.\nB. VISION TRANSFORMER\nNatural Language Processing (NLP) is revolutionized by\nthe groundbreaking performance of Transformer networks.\nTransformer networks, unlike Convolutional Neural Network\napproaches, have the benefit of being able to capture\nlong-range dependencies of the input sequences by using a\nself-attention layer. The ‘‘self-attention’’ layer is the funda-\nmental concept of the Transformer network. The Computer\nVision community was inspired to modify the Transformer\nto use in Vision problems. The initial research in this field\nwas carried out by Alex et al., who proposed the Vision\nTransformer known as ViT [27], which replaces the conven-\ntional CNN with the Transformer. ViT is directly trained on\nthe medium-sized flattened patches with large-scale data pre-\ntraining.\nSince introducing the first work, many Transformer based\narchitectures have been proposed for the Vision tasks such\nas in image recognition [28], object detection [29], [30],\nsegmentation [31], [32], and action recognition [33], [34].\nIn addition, Transformer based models have been studied\nfor low-level vision problems such as super resolution\n[35], [36], image colorization [37], denoising [38], and image\nrestoration [39]. For instance, DETR [29] is a transformer\nnetwork designed for object detection, which can predict a\nset of objects and model their relationships. SwinIR [35],\nintroduced by Jingyun et al. for low-level vision tasks, uses\nSwin Transformer [30] by applying self-attention within local\nimage regions to solve the low-level vision problems.\nVOLUME 11, 2023 11531\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nC. GUIDANCE SUPER RESOLUTION\nGuidance Super Resolution (GSR) techniques have been used\nto upsample images from a different domain to generate\nmore accurate SR images by using the information of\nother domain images (e.g., visible images), while having\nsuch high-resolution images is expensive (e.g., thermal\nimages). Traditional GSR approaches, such as joint bilateral\nupsampling [40] and rapid bilateral filtering [41], have\nalready been studied for this task; however, these methods\nfrequently over-smooth the reconstructed image. Recently,\nby advancing deep learning methods, several approaches\nhave been introduced to boost the performance of the\nGSR task [42]. GSR techniques have been studied in\ndifferent super resolution domains, such as depth-map SR,\ninfrared SR, thermal SR, hyperspectral SR, and some others.\nMSG-Net [43], employs CNNs to accomplish guidance super\nresolution, which is the first CNN model that attempts to\nupsample depth images under multi-scale guidance from the\ncorresponding HR visible images.\nMost GSR methods are based on the Siamese algorithm,\nwhich lets the network accept two inputs and perform simul-\ntaneous feature extraction from both spectral images and\nvisible images at the same time. These images are then fused\nat different levels of the network and upsampled to provide\nhigh-resolution images. Furthermore, GSR approaches with\nsimilar structures used in guidance hyperspectral SR methods\ninclude [44], [45]. Also, some models were proposed for\nguidance infrared SR such as [46], [47]. Feras et al. [13]\npropose a multimodal sensor fusion model to enhance the\nthermal images with help of RGB images. Riccardo et al. [48]\npropose an alternative interpretation of guided super reso-\nlution, where the roles of the source and guide images are\nswapped. Honey et al. [42] propose a network for GSR from\nlow-resolution thermal images that do not require pixel-to-\npixel alignment between the thermal and the guide image.\nMoreover, some approaches for cross-modal guidance super\nresolution extract edges from the visible images in order\nto obtain high-frequency features. The use of edge-based\nguiding facilitates the reconstruction of higher-frequency\nfeatures such as [49], [50]. Despite that the aforementioned\napproaches achieve reasonable performance, these methods\nare limited to a fixed scale factor and not ideal for real-world\napplications due to the number of network parameters and\ntheir performance. Hence, in this paper, we propose a novel\ndual stream Transformer based network for GSR, which\narchives remarkable performance with a completely new\ndesign for an upsampling module to be able to reconstruct SR\nimages in any arbitrary size while reminding computationally\nefficient.\nIII. PROPOSED METHOD\nIn this section, the overall network architecture of the\nproposed TnTViT-G is described. Following that, more\ninformation about the Dual Attention layer is provided.\nTnTViT is designed for Single Image Super Resolution and\nTnTViT-G is a siamese-based network of TnTViT, which\ndesigns for Guidance Super Resolution.\nA. OVERALL PIPELINE\nThe main objective of the proposed model is to design an effi-\ncient Transformer-based network for Guidance Super Reso-\nlution that is capable of producing fine details high-quality\nimages with the help of the guided images (e.g., visible\nimages) to boost the performance of the network while\nstaying computationally low. Thus, we employ the original\nTransformer structure but modify it in such a way that\nthe model achieves a considerable performance advantage\nover existing CNN and Transformer networks. The overall\narchitecture of the TnTViT-G is illustrated in Fig. 2.\nIt consists of two streams to extract the features of LR\ninfrared input images and HR visible images. In particular,\nthe proposed TnTViT-G consists of three modules: Shallow\nFeature Extraction (SFE), Dense Feature Extraction (DFE),\nand Multi-Level Reconstruction Modules. We defined IIR\nLR,\nIVis\nHR, and IIR\nSR as the low-quality infrared, high-quality RGB\ninputs, and high-quality output of our network, respectively.\nB. SHALLOW FEATURE EXTRACTION\nGiven the input images to the network, we apply a single\n3 × 3 convolutional layer on each network’s streams to the\nprovided LR and HR visible inputs in order to map these\nimages space to a higher dimensional feature space and\nimprove the performance of the network [51]. Therefore,\nwe extract the shallow features as follows:\nFIR\n0 = Conv3×3(IIR\nLR), FVis\n0 = Conv3×3(IVis\nHR), (1)\nwhere FIR\n0 (.) and FVis\n0 (.) denotes the output of shallow feature\nextraction on both infrared and visible images.\nC. DENSE FEATURE EXTRACTION\nAfter mapping the inputs to a higher dimensional feature\nspace, the features pass through the Dense Feature Extraction\nFDFE to encode the information in order to understand\nthe context of the sequences. The feature encoders of the\nproposed approach (i.e, Dense Feature Extraction) is a\nTransformer based network, which shares between both input\nimages (I IR\nLR and IVis\nHR) to keep the network computationally\nefficient. However, each stream receives the same patch of\nthe input image with different sizes since LR images are\nrelatively smaller than visible images. Particularly, Dense\nFeature Extraction design by using several Transformer\nblocks to extract abstract features and spotlights the high-\nlevel information. Each Transformer block consists of several\nTransformer layers and a 1 × 1 Conv layer with the benefit\nof cascade connections to transfer the information from the\nprevious stage to the current stage and help the gradient flow\nof the network. Thus, we extract the feature as follows:\nFDFE = HDFE (FIR\n0 ; FVis\n0 ), (2)\n11532 VOLUME 11, 2023\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nwhere HDFE (.) is Dense Feature Extraction with several\nTransformer blocks which can be formulated as\nFi = Conv1×1(C[HDATB(Fi−1), Xi−1], i = 1, 2, . . . ,K, (3)\nwhere HDATB(.) denotes the ith Dual Attention Transformer\nBlocks. C stands for the concatenation operation between\nthe initial and output features of each DATB block. Conv\ndenotes the convolutional layer after concat operation within\neach DATB. Using a convolutional layer in the Transformer\nblock, help to transfer inductive bias from the convolution\noperation into the Transformer network and provide a more\nsolid foundation for the later aggregation with shallow\nfeatures.\nAfter encoding the features through several DATB, the\noutput feature maps of each DATB stage are concatenated\ntogether to highlight the positional information via the\nGMFF module, which stands for Gated multi-layer percep-\ntron (MLP) Feature Fusion, before reconstructing the SR\nimages. GMFF module is designed to generate a multi-stage\nrepresentation feature map of Transformer blocks. Later, the\nfeature map passes through a lightweight MLP network.\nHowever, unlike to standard MLP network, the GMFF’s\nMLP module is designed by using a 3 × 3 depthwise\nConv layer and gating mechanism technique to first, leak\nthe spatial information since highlighting such features is\nimportant in SR tasks to achieve high performance. Second,\nallowing useful information to pass through the network and\nsuppressing the less informative ones. The gating mechanism\nis used by applying the element-wise product of two parallel\nroutes of linear transformation layer that one of which is\nactivated with the GELU. Thus, Gated MLP Feature Fusion\ncan be seen as follows:\nFGMFF = MLP(GELU(Conv3×3(MLP(Fi)))) + F0, (4)\nwhere FGMFF is the output of DFE with aggregation of the\ninitial features, which is later used by the Multi-Stage Feature\nFusion Module.\nD. MULTI STAGE FEATURE FUSION MODULE\nAfter encoding the information of both LR infrared image\nIIR\nLR and HR visible image IVis\nHR with a dual stream shared\nnetwork, the LR features first scale up to the same spatial\nsize as the HR visible image before fusing the information\nwith a learnable bicubic upsampling that contains a conv layer\nbefore it; later, the aggregated features of all the stages are\nconcatenating together to enhance the LR infrared images\nbefore upsampling them to the desired output size.\nFMSFF\n= Conv1×1(C[HUP(FIR\nGMFF ), FVis\nGMFF , FFS1, . . . ,FFS4]),\n(5)\nwhere FMSFF (·) denotes the output of the multi-stage feature\nfusion module of both TnTViT streams and the feature fusion\nof each stage.\nE. MULTI LEVEL RECONSTRUCTION MODULE\nLater, to upsample the LR infrared image after fusing\nthe information, we propose a new inductive bias in\nGSR architectures to generate SR images more accurately\nwith fewer artifacts compared with the other methods or\nnaive interpolation techniques. To do so, we first pass the\ninformation through two-pixel shuffle layers and a conv layer\nbefore each of them. Second, the upsampled features with\npixel shuffle layers feed to a learnable bicubic interpolation\nto upscale the features to any arbitrary size. Later, the\ninformation aggregated with the shallow features of the\nHR-guided image. These features are also upscaled with\nlearnable bicubic interpolation.\nISR = H↑\nRec(HUP(FIR\n0 ) + FMSFF + HUP(ILR)), (6)\nwhere HRec(·) and ISR denote the up-sampling module and\nhigh-quality reconstructed image respectively. Hence, the\nproposed module can learn how to refine the pixels more\ncorrectly via different levels of upscaling to bring it closer to\nits actual high-resolution counterpart and beyond. Extensive\nexperiments have been detailed in the ablation study to show\nthe efficiency of the proposed reconstruction module over\nother approaches.\nF. LOSS FUNCTION\nTo keep the consistency with previous works, we use L1 loss\nas a cost function during training to optimize the parameters\nof the proposed TnTViT-G.\nL1(θ) = 1\nN\nN∑\ni=1\n∥ISR − IHR∥1 , (7)\nwhere ISR is obtained by taking a low-quality infrared image\nas the input of our model and IHR is the corresponding ground\ntruth.\nIn the next subsections, we provide more details about our\nTransformer layer.\nG. DUAL ATTENTION LAYER\nThis section presents the proposed Dual Attention layer,\nan architecture abstracted from the general multi-head Trans-\nformer layer [52] with revising the self-attention layer. As is\ngenerally known, self-attention is critical to achieve excellent\nperformance in Transformer-based networks. However, self-\nattention might be troublesome for a variety of reasons.\nFor example, the computational complexity of self-attention\ngrows quadratically with the number of tokens to mix. Also,\ndue to its nature, self-attention does not take into account the\nlocal contextual information and treats the images as flattened\nsequences that ignore the structure of the image. Thus,\nwe propose the Dual Attention layer to address the mentioned\nlimitations by constructing a global attention map at a lower\ncomputational cost. The dual Attention layer creates a global\nattention map by combining two local attention maps, which\nare obtained in parallel by using a CNN-based Attention\nModule and a Transformer self-attention layer. Unlike the\nVOLUME 11, 2023 11533\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nFIGURE 3. Illustration of dual attention layer (DAL).\nprior token mixer, Dual Attention is able to take into account\nboth long-range dependency and local contextual information\nwith less computing cost.\nAs shown in Fig 3, we design our Dual Attention such\nthat the channel information is distributed evenly across\nboth attention module branches (SpAM and SeAM). Both\nattention branches get half of the input tensor from the\nNorm layer tensor X to generate the local attention map\nindependently. The SeAM is a self-attention Transformer,\nwhich first generates the query (Q), key (K), and value (V)\nprojections, enriched with the local context. Inspired by [39],\nwe apply SeAM only across the channels rather than\nthe spatial dimensions. Our SeAM uses only depth-wise\nconvolutions to emphasize the channel-wise spatial context\nbefore computing feature covariance to produce the attention\nmap. Thus, Q, K, V are computed as:\nQ = W Q\nd Y , K = W K\nd Y , V = W V\nd Y , (8)\nwhere W (·)\nd is the 3 × 3 depth-wise convolution. Next,\nquery and key projections reshape in such a way that their\ndot-product interaction generates a transposed-attention map.\nThus, the attention map generates the following:\nAttention(Q, K, V ) = Wd (V .Softmax(K.Q/α)) + X, (9)\nwhere X is the input feature map and α is a learnable scaling\nparameter that is used to regulate the magnitude of the dot\nproduct of K and Q before applying the Softmax function.\nSimilar to previous works [35], [52], [53] we perform the\nattention function for h times to learn separate attention maps\nin parallel in our SeAM module.\nThe second branch of our Dual Attention layer is the\nSpatial Attention Module (SpAM), which is an almost\nparameter-free attention mechanism. SpAM receives the\nother half of the input tensor to generate the local attention\nmap. The goal of SpAM module is to encode the spatial\ninformation, which represents the importance of each pixel\nin the input feature with a negotiable cost. Given half of the\ninput tensor information, the channels of the input tensor are\nreduced by mean and max operations, of which the shape is\n1×H ×W . The obtained features are concatenated, then pass\nthrough a dilated convolution layer with a kernel size of 3 ×3.\nAfter, a sigmoid activation layer applies to the output feature\nto generate the attention weights of shape 1 × H × W , which\nare later multiplied with the input tensor to refined tensors of\nshape C × H × W . Thus, the SpAM can be formulated as\nfollows:\nX = Sigmoid(Conv3×3[FMean(X), FMax (X)]) ∗ X, (10)\nwhere FMean(·) and FMax (·) denotes for mean and max\noperations. Later, both locally generated attention maps from\nSpAM and SeAM are concatenated together to obtain a\nunified global attention map with less computational cost.\nThus, the generated attention map contains both long-range\ndependency and local context information with enrich of\nspatial features.\nFollowing that, a multi-layer perceptron (MLP) with two\nfully connected layers and a GELU non-linearity activation\nfunction between them is employed for further feature\nmodifications. The norm layer is also added before MLP, and\nboth modules contain the residual connection between them.\nAs a result, the entire procedure within our Dual Attention is\nas follows:\nX = (Norm(SpAM(X/2) + SeAM(X/2))) + X\nY = MLP(Norm(X)) + X (11)\nwhere Norm(·) stands for normalization layer and Y for the\noutput feature map.\nIV. EXPERIMENTAL RESULTS\nA. SETTING\n1) DATASETS\nTwo datasets have been used to perform the experiments,\nnamely M3FD [54] and RGB-NIR [55]. The first dataset is\nM3FD, which newly released by [54]. The M3FD dataset\ncontains pairs of visible and thermal images. The dataset\nwas built with a synchronized system of one binocular\noptical camera and one binocular thermal sensor to capture\ncorresponding two modality images. We use the M3FD\nFusion dataset, which consists of 300 aligned pair images\nfrom different scenarios in daytime, night, and overcast. Also,\nthe dataset consists of images from different scenes, such as\nroad, campus, street, forest, and many others.\nThe second dataset is the RGB-NIR Scene [56]\ndataset. The RGB-NIR Scene dataset contains aligned\npairs of 477 RGB and near-infrared images, divided into\n9 categories such as country, field, forest, indoor, mountain,\nold building, street, urban, and water. The images were\nacquired by utilizing different exposures from customized\nSLR cameras equipped with visible and near-infrared filters.\n2) EVALUATION PROTOCOL\nTwo widely used quantitative metrics have been considered to\nmeasure the performance of our TnTViT compared to other\napproaches. We used the Peak Signal-to-Noise Ratio (PSNR)\nand Structural Similarity Index Measure (SSIM) to measure\nreconstructed SR image accuracy. PSNR assesses the image\nby statistically calculating distortion levels between the\nreconstructed and ground-truth images. SSIM measures the\nstructural similarity between two images based on luminance,\ncontrast, and structure, which has a value range between\n[0-1]. Higher value, better for both PSNR and SSIM.\n11534 VOLUME 11, 2023\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nTABLE 1. Average PSNR, SSIM comparisons with SOTA CNN- and Transformer-based methods with the same range of network parameters on the Bicubic\n(BI) degradation for scale factors [×2,×4, ×8]. Best results arehighlighted.\n3) DEGRADATION MODELS\nDegradation models have been created to replicate LR images\nin order to demonstrate the effectiveness of our proposed\napproach. The degradation model is bicubic downsampling\n(BI), which simulates LR images with the scale factors\n[×2, ×4, ×8] by applying bicubic downsampling to HR\nimages.\n4) IMPLEMENTATION DETAILS\nWe randomly select 70%, 20%, and 10% of images of\neach dataset for the training, validation, and test phases\nrespectively. In the training phase, we provide the image\npatches as inputs with different sizes based on the size of each\ndataset from LR images and corresponding RGB images. The\nbatch size has been set to 32 for the training. Horizontal\nrandom flips and 90 degree rotation data augmentation were\napplied to patches of images. Adam optimizer has been used\nwith the initial learning rate 10 −3. L1 is used as a loss function\nto optimize the model and the network has been trained for\n150K iterations. Also, the configurations of our transformer\nencoder are as follows: we use 4 Transformer blocks within\n6 Transformer layers for each block, Embedding dimension\nset to 64, and MLP ratio to 2 for all Transformer blocks.\nTnTViT-G is developed using the PyTorch framework and\ntrained on a single NVIDIA RTX 3090 GPU to achieve its\nperformance.\nB. COMPARISON WITH STATE-OF-THE-ART METHODS\nIn this section, we compare our proposed GSR (TnTViT-G)\nand SISR (TnTViT) with other lightweight state-of-the-art\napproaches on different datasets with different scale factors.\n1) EXPERIMENTS ON BICUBIC DEGRADATION\nTable 1 shows comparisons between the proposed approaches\n(TnTViT and TnTViT-G) and SOTA CNN- and Transformer-\nbased models, CARN [21], SwinIR [35], PixTransform [48],\nand UGSR [42] on the Bicubic (BI) degradation model\nfor scale factors [×2, ×4, ×8]. It is worth mentioning\nthat these networks contain almost the same number of\nnetwork parameters, allowing for a fair comparison. As can\nbe observed, when the proposed method is compared to\nthe approaches mentioned above, TnTViT achieves better\nVOLUME 11, 2023 11535\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nFIGURE 4. Visual results for scale factors [×2,×4, ×8] on M3FD an RGB-NIR datasets respectively.\n11536 VOLUME 11, 2023\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nTABLE 2. Average PSNR results on RGB-NIR dataset for different upscaling methods with arbitrary scales. Best results arehighlighted, second best\nunderlined.\nresults without the help of any guided image (visible image).\nFurthermore, the proposed method (TnTViT-G) with the\nguidance of visible image information achieves superior\nresults in almost most of the cases with major margins. This\ndemonstrates that TnTViT-G continually accumulates this\nhierarchical information from different spectral images in\norder to construct more robust representative features that\nare well-focused on spatial context information since that\nis the key to an accurate SR image. This claim is validated\nby the derived SSIM scores, which are based on the visible\nstructures in the image and hence are more accurate. Figure 4\nshows some qualitative results on M3FD and RGB-NIR\ndatasets on different scale factors for SOTA methods of\nboth SISR and GSR. As can be seen, TnTViT produces\nimages better than the existing method in the SISR since\nthe network is able to focus better on spatial information.\nHowever, TnTViT-G is able to reconstruct high-frequency\ndetails significantly better than all the existing methods and\ngenerates more accurate SR infrared images, which are more\nsimilar to ground truth images.\nV. ABLATION STUDY\nThe proposed model is further studied in an extended ablation\ninvestigation to demonstrate its efficiency. The ablation study\nis intended to offer further information about the performance\nof the proposed approach.\nA. VISUALIZATION ON IMPACT OF GUIDED IMAGE\nFigure 5 shows the average feature maps of each stage of our\nDense Feature Extraction module to investigate the impact of\nthe guidance image (i.e., visible image) when it is stacked up\nwith the LR feature map in each stage of DFE. Each average\nfeature map reflects the output of the Transformer block\nat each stage in the Dense Feature Extraction module. The\naverage feature maps without guidance images are presented\non the top row, while those with guidance images are shown\non the bottom row. We can observe from the feature maps that\nFIGURE 5. Average feature maps of TnTViT (top) and TnTViT-G (bottom)\non different stages of dense feature extraction.\nTABLE 3. Avargae LPIPS comparison between proposed method and the\nother methods on benchmark datasets for scale factors [×2,×4]. The\nlower is better.\nusing a guidance image helps the network acquire sharper\nrepresentations. Second, as the network focuses more on\nhigh-level information, feature maps tend to include more\nnegative values at each stage, showing a greater influence of\nsuppressing the smooth area of the input image, which yields\na more accurate SR output.\nB. INFLUENCE OF MULTI-LEVEL RECONSTRUCTION\nMODULE\nWe investigate the advantages of using the proposed\nMulti-Level Reconstruction Module, as well as the impact\nVOLUME 11, 2023 11537\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nTABLE 4. Average running time (s), memory consumption (MB), number\nof parameters (K), FLOPs (G), and PSNR comparison on RGB-NIR dataset\nfor ×4.\nof two widely used upsampling and interpolation approaches\non reconstruction results. We carried out the following\nexperiments: i) Directly employing Pixel Shuffle layer to\nproduce images after fusing the information of both network’s\nstreams instead of our MLUP; ii) Using Pixel Shuffle layer\nfollowed by a conv layer and bicubic interpolation to scale\nthe generated SR image to arbitrary scales. As can be seen\nin Table 2, when the proposed MLUP module is used for\nupscaling, superior results are obtained by a large margin\ncompared to other upsampling techniques. These studies\ndemonstrate that, opposite to common practice, the MLUP\nsignificantly improves reconstruction accuracy since the\nmodule is able to generate the SR images in multi-level with\nthe access of both direct and indirect shallow and abstract\nfeatures and yields consistent improvements on benchmark\ndatasets.\nC. LEARNED PERCEPTUAL IMAGE PATCH SIMILARITY\nIn Table 3, we provide the Learned Perceptual Image Patch\nSimilarity (LPIPS) [57] evaluation metric to evaluate the\nquality of the generated super-resolved images. LPIPS has\nbeen demonstrated to correlate better with human perceptual\nsimilarity of image quality than other evaluation metrics\n(i.e., PSNR and SSIM). LPIPS is a deep-feature-based\nevaluation metric that calculates the perceptual distance\nbetween two images. As can be seen, the proposed model\nachieves a lower value than other approaches (lower is better).\nPixTransform [48] was excluded from the table since it could\nnot even outperform bicubic interpolation. This shows the\neffectiveness of the proposed TnTViT-G to generate more\naccurate super-resolved IR images with the help of HR visible\nimages.\nD. MODEL COMPLEXITY AND INFERENCE TIME ANALYSIS\nTable 4 compares the proposed TnTViT and TnTViT-G\narchitectures with existing CNN and Transformer-based\narchitecture approaches on RGB-NIR test images in terms of\nnetwork Parameters (M), FLOPs (G), Memory consumption\n(MB), and Running Time (s). To provide a fair comparison,\nall models are tested using the same setup, including their\npublic source code and default hyper-parameters, on an Intel\nCore i9-10900K CPU and an NVIDIA RTX 3090 GPU.\nAs can be seen, TnTViT generates the SR images faster than\nother Transformer methods. This comparison shows that our\nTABLE 5. Summary of abbreviations.\nproposed model properly balances performance and running\ntime requirements.\nVI. CONCLUSION AND FUTURE WORK\nThis paper introduces TnTViT-G, a novel approach for\nguidance super resolution based on Transformer architecture.\nTnTViT-G is designed to accept two images of different\ndomains, extract the information from each domain (infrared\nand corresponding visible image) with a separate stream,\nand fuse them efficiently at different stages while remaining\nmemory efficient. We propose a dense feature extraction,\nwhich contains both a transformer self-attention layer and\na convolutional attention module that can capture both\nglobal dependency and local context information at a lower\ncomputational cost while its well focusing on spatial features\ncompared to other Transformer models. Furthermore, unlike\nother GSR methods, TnTViT-G is able to generate SR\nimages in arbitrary sizes, while other methods only generate\nSR images in fixed sizes. Our experiments highlight that\na high-cost, low-resolution spectral image (IR image) can\nbe enhanced by a corresponding high-resolution, low-cost\nvisible image (visible image). We have demonstrated that\nour approach achieves superior performance compared to\nother lightweight state-of-the-art methods on all benchmark\ndatasets.\nIn the future, we will expand our approach for unsu-\npervised guidance super resolution when a paired dataset\nis not available. To do so, we will attempt to change the\nmethodology of our proposed architecture to use a Generative\nAdversarial Network. Finally, despite the fact that there has\nbeen experimental proof that a low-cost channel can be\n11538 VOLUME 11, 2023\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\nTABLE 6. List of datasets used for multimodel guided image\nsuper-resolution.\nused to increase the resolution of an expensive channel, this\nstrategy relies on a well-registered paired dataset, which is\ndifficult to obtain since there can be misalignment between\nmulti-model sensors, and a simple feed-forward network\ncannot deal with the mismatch problem. Thus, the image\nalignment technique is required as a pre-process to match the\ncounterparts before encoding features of both domains and\ngenerating a super-resolved image. Therefore, we will try to\nintegrate the feature alignment method into our forward pass\nnetwork to address the aforementioned problem.\nAPPENDIX\nIn Table 5, we provide a full list of the abbreviations and\nacronyms, which have been used in this paper. Table 6\ncontains a list of datasets which has been used to carry out\nthe experiments of this study. We provide the number of HR\nimages, image formats, and category keywords.\nREFERENCES\n[1] J.-B. Huang, A. Singh, and N. Ahuja, ‘‘Single image super-resolution from\ntransformed self-exemplars,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 5197–5206.\n[2] R. Timofte, V . De, and L. V . Gool, ‘‘Anchored neighborhood regression for\nfast example-based super-resolution,’’ in Proc. IEEE Int. Conf. Comput.\nVis., Dec. 2013, pp. 1920–1927.\n[3] J. Yang, Z. Wang, Z. Lin, S. Cohen, and T. Huang, ‘‘Coupled dictionary\ntraining for image super-resolution,’’ IEEE Trans. Image Process., vol. 21,\nno. 8, pp. 3467–3478, Aug. 2012.\n[4] S. Schulter, C. Leistner, and H. Bischof, ‘‘Fast and accurate image\nupscaling with super-resolution forests,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2015, pp. 3791–3799.\n[5] C. Dong, C. C. Loy, K. He, and X. Tang, ‘‘Learning a deep convolutional\nnetwork for image super-resolution,’’ in Proc. Eur. Conf. Comput. Vis.\nCham, Switzerland: Springer, 2014, pp. 184–199.\n[6] Y . Zhang, Y . Tian, Y . Kong, B. Zhong, and Y . Fu, ‘‘Residual dense\nnetwork for image super-resolution,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., Jun. 2018, pp. 2472–2481.\n[7] Y . Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y . Fu, ‘‘Image super-\nresolution using very deep residual channel attention networks,’’ in Proc.\nEur. Conf. Comput. Vis. (ECCV), Sep. 2018, pp. 286–301.\n[8] F. Qingyun, H. Dapeng, and W. Zhaokui, ‘‘Cross-modality fusion\ntransformer for multispectral object detection,’’ 2021, arXiv:2111.00273.\n[9] W. K. Wong, H. L. Lim, C. K. Loo, and W. S. Lim, ‘‘Home alone faint\ndetection surveillance system using thermal camera,’’ in Proc. 2nd Int.\nConf. Comput. Res. Develop., May 2010, pp. 747–751.\n[10] A. C. Goldberg, T. Fischer, and Z. I. Derzko, ‘‘Application of dual-band\ninfrared focal plane arrays to tactical and strategic military problems,’’\nProc. SPIE, vol. 4820, pp. 500–514, Jan. 2003.\n[11] J. Baek, S. Hong, J. Kim, and E. Kim, ‘‘Efficient pedestrian detection at\nnighttime using a thermal camera,’’ Sensors, vol. 17, p. 1850, Aug. 2017.\n[12] B. C. Arrue, A. Ollero, and J. R. M. de Dios, ‘‘An intelligent system for\nfalse alarm reduction in infrared forest-fire detection,’’ IEEE Intell. Syst.\nAppl., vol. 15, no. 3, pp. 64–73, May/Jun. 2000.\n[13] F. Almasri and O. Debeir, ‘‘Multimodal sensor fusion in single thermal\nimage super-resolution,’’ in Proc. Asian Conf. Comput. Vis. Cham,\nSwitzerland: Springer, 2018, pp. 418–433.\n[14] A. Rogalski, P. Martyniuk, and M. Kopytko, ‘‘Challenges of small-pixel\ninfrared detectors: A review,’’ Rep. Prog. Phys., vol. 79, no. 4, Apr. 2016,\nArt. no. 046501.\n[15] Y . Cao, F. Wang, Z. He, J. Yang, and Y . Cao, ‘‘Boosting image super-\nresolution via fusion of complementary information captured by multi-\nmodal sensors,’’IEEE Sensors J., vol. 22, no. 4, pp. 3405–3416, Feb. 2022.\n[16] C. Dong, C. C. Loy, and X. Tang, ‘‘Accelerating the super-resolution\nconvolutional neural network,’’ in Proc. Eur. Conf. Comput. Vis.Cham,\nSwitzerland: Springer, 2016, pp. 391–407.\n[17] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop,\nD. Rueckert, and Z. Wang, ‘‘Real-time single image and video super-\nresolution using an efficient sub-pixel convolutional neural network,’’\nin Proc. IEEE Conf. Comput. Vis. pattern Recognit., Sep. 2016,\npp. 1874–1883.\n[18] J. Kim, J. K. Lee, and K. M. Lee, ‘‘Accurate image super-resolution using\nvery deep convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2016, pp. 1646–1654.\n[19] Y . Tai, J. Yang, X. Liu, and C. Xu, ‘‘MemNet: A persistent memory network\nfor image restoration,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nOct. 2017, pp. 4539–4547.\n[20] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ‘‘Enhanced deep residual\nnetworks for single image super-resolution,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. Workshops (CVPRW), Jul. 2017, pp. 136–144.\n[21] N. Ahn, B. Kang, and K.-A. Sohn, ‘‘Fast, accurate, and lightweight super-\nresolution with cascading residual network,’’ in Proc. Eur. Conf. Comput.\nVis. (ECCV), Sep. 2018, pp. 252–268.\n[22] A. Muqeet, J. Hwang, S. Yang, J. H. Kang, Y . Kim, and S.-H. Bae,\n‘‘Ultra lightweight image super-resolution with multi-attention layers,’’\n2020, arXiv:2008.12912.\n[23] X. Luo, Y . Xie, Y . Zhang, Y . Qu, C. Li, and Y . Fu, ‘‘Latticenet: Towards\nlightweight image super-resolution with lattice block,’’ in Proc. Eur. Conf.\nComput. Vis., Glasgow, U.K.: Springer, Aug. 2020, pp. 272–289.\n[24] P. Behjati, P. Rodriguez, A. Mehri, I. Hupont, C. F. Tena, and J. Gonzalez,\n‘‘OverNet: Lightweight multi-scale super-resolution with overscaling\nnetwork,’’ in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV),\nJan. 2021, pp. 2694–2703.\n[25] X. Chu, B. Zhang, R. Xu, and H. Ma, ‘‘Multi-objective reinforced evolution\nin mobile neural architecture search,’’ 2019, arXiv:1901.01074.\n[26] X. Chu, B. Zhang, H. Ma, R. Xu, and Q. Li, ‘‘Fast, accurate and\nlightweight super-resolution with neural architecture search,’’ 2019,\narXiv:1901.07261.\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[28] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. H. Tay, J. Feng,\nand S. Yan, ‘‘Tokens-to-token ViT: Training vision transformers from\nscratch on ImageNet,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2021, pp. 558–567.\n[29] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nEur. Conf. Comput. Vis.Cham, Switzerland: Springer, 2020, pp. 213–229.\n[30] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B.\nGuo, ‘‘Swin transformer: Hierarchical vision transformer using shifted\nWindows,’’ inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 10012–10022.\n[31] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, ‘‘Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2021, pp. 568–578.\n[32] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n‘‘SegFormer: Simple and efficient design for semantic segmentation with\ntransformers,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 1–13.\n[33] L. Shi, Y . Zhang, J. Cheng, and H. Lu, ‘‘Decoupled spatial–temporal\nattention network for skeleton-based action recognition,’’ 2020,\narXiv:2007.03263.\n[34] C. Plizzari, M. Cannici, and M. Matteucci, ‘‘Skeleton-based action\nrecognition via spatial and temporal transformer networks,’’ Comput. Vis.\nImage Understand., vols. 208–209, Jul. 2021, Art. no. 103219.\nVOLUME 11, 2023 11539\nA. Mehri et al.: TnTViT-G: Transformer in Transformer Network for Guidance Super Resolution\n[35] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\nImage restoration using swin transformer,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis. Workshops (ICCVW), Oct. 2021, pp. 1833–1844.\n[36] Z. Lu, J. Li, H. Liu, C. Huang, L. Zhang, and T. Zeng, ‘‘Efficient\ntransformer for single image super-resolution,’’ 2021, arXiv:2108.11084.\n[37] M. Kumar, D. Weissenborn, and N. Kalchbrenner, ‘‘Colorization trans-\nformer,’’ 2021, arXiv:2102.04432.\n[38] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li, ‘‘Uformer: A general\nU-shaped transformer for image restoration,’’ 2021, arXiv:2106.03106.\n[39] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,\n‘‘Restormer: Efficient transformer for high-resolution image restoration,’’\n2021, arXiv:2111.09881.\n[40] J. Kopf, M. F. Cohen, D. Lischinski, and M. Uyttendaele, ‘‘Joint bilateral\nupsampling,’’ACM Trans. Graph., vol. 26, no. 3, p. 96, Jul. 2007.\n[41] J. T. Barron and B. Poole, ‘‘The fast bilateral solver,’’ in Proc. Eur. Conf.\nComput. Vis.Cham, Switzerland: Springer, 2016, pp. 617–632.\n[42] H. Gupta and K. Mitra, ‘‘Toward unaligned guided thermal superresolu-\ntion,’’IEEE Trans. Image Process., vol. 31, pp. 433–445, 2021.\n[43] T.-W. Hui, C. C. Loy, and X. Tang, ‘‘Depth map super-resolution by\ndeep multi-scale guidance,’’ in Proc. Eur. Conf. Comput. Vision (ECCV),\nSep. 2016, pp. 353–369.\n[44] Z. Shi, C. Chen, Z. Xiong, D. Liu, Z.-J. Zha, and F. Wu, ‘‘Deep residual\nattention network for spectral image super-resolution,’’ in Proc. Eur. Conf.\nComput. Vis. (ECCV) Workshops, Sep. 2018, pp. 214–229.\n[45] F. Lahoud, R. Zhou, and S. Susstrunk, ‘‘Multi-modal spectral image\nsuper-resolution,’’ in Proc. Eur. Conf. Comput. Vis. (ECCV) Workshops,\nSep. 2018, pp. 35–50.\n[46] X. Chen, G. Zhai, J. Wang, C. Hu, and Y . Chen, ‘‘Color guided thermal\nimage super resolution,’’ in Proc. Visual Commun. Image Process. (VCIP),\nNov. 2016, pp. 1–4.\n[47] P. Song, X. Deng, J. F. Mota, N. Deligiannis, P. L. Dragotti, and\nM. R. Rodrigues, ‘‘Multimodal image super-resolution via joint sparse\nrepresentations induced by coupled dictionaries,’’ IEEE Trans. Comput.\nImag., vol. 6, pp. 57–72, 2019.\n[48] R. d. Lutio, S. D’aronco, J. D. Wegner, and K. Schindler, ‘‘Guided super-\nresolution as pixel-to-pixel transformation,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis., Oct. 2019, pp. 8829–8837.\n[49] J. Xie, R. S. Feris, and M.-T. Sun, ‘‘Edge-guided single depth image super\nresolution,’’ IEEE Trans. Image Process., vol. 25, no. 1, pp. 428–438,\nJan. 2016.\n[50] D. Zhou, R. Wang, J. Lu, and Q. Zhang, ‘‘Depth image super resolution\nbased on edge-guided method,’’ Appl. Sci., vol. 8, no. 2, p. 298, 2018.\n[51] T. Xiao, P. Dollar, M. Singh, E. Mintun, T. Darrell, and R. Girshick,\n‘‘Early convolutions help transformers see better,’’ in Proc. Adv. Neural\nInf. Process. Syst., vol. 34, 2021, pp. 30392–30400.\n[52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 1–11.\n[53] W. Yu, M. Luo, P. Zhou, C. Si, Y . Zhou, X. Wang, J. Feng, and\nS. Yan, ‘‘Metaformer is actually what you need for vision,’’ 2021,\narXiv:2111.11418.\n[54] J. Liu, X. Fan, Z. Huang, G. Wu, R. Liu, W. Zhong, and Z. Luo,\n‘‘Target-aware dual adversarial learning and a multi-scenario multi-\nmodality benchmark to fuse infrared and visible for object detection,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2022,\npp. 5802–5811.\n[55] M. Brown and S. Süsstrunk, ‘‘Multi-spectral sift for scene category\nrecognition,’’ in Proc. CVPR, Jun. 2011, pp. 177–184.\n[56] M. Brown and S. Süsstrunk, ‘‘Multispectral SIFT for scene category\nrecognition,’’ in Proc. Comput. Vis. Pattern Recognit.(CVPR), Jun. 2011,\npp. 177–184.\n[57] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ‘‘The\nunreasonable effectiveness of deep features as a perceptual metric,’’ in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 586–595.\nARMIN MEHRI received the B.Sc. and M.Sc.\ndegrees in computer science from Eastern Mediter-\nranean University, in 2014 and 2017, respectively.\nHe is currently pursuing the Ph.D. degree in\ndeep learning and computer vision with the\nComputer Vision Center, Universitat Autònoma de\nBarcelona. He is also the Computer Vision and\nDeep Learning Lead with Camaleonic Analytics,\nworking on AI-based software for sponsorship in\nsports. His research interests include computer\nvision and image processing under cross-modal frameworks resulting in\ncross-spectral domains.\nPARICHEHR BEHJATI received the bachelor’s\nand master’s degrees in computer science from\nEastern Mediterranean University and the Ph.D.\ndegree (cum laude) in deep learning and com-\nputer vision from the Computer Vision Center,\nUniversitat Autònoma de Barcelona. She worked\nas a Research Assistant with the Computer Science\nDepartment, Eastern Mediterranean University,\nfrom 2014 to 2016. Her research interests include\ndeep learning and computer vision.\nANGEL DOMINGO SAPPA (Senior Member,\nIEEE) received the degree in electromechanical\nengineering from the National University of\nLa Pampa, General Pico, Argentina, in 1995,\nand the Ph.D. degree in industrial engineering\nfrom the Polytechnic University of Catalonia,\nBarcelona, Spain, in 1999. In 2003, after holding\nresearch positions in France, the U.K., and Greece,\nhe joined the Computer Vision Center, Barcelona,\nwhere he currently holding a senior scientist\nposition. Since 2016, he has been a Full Professor at ESPOL Polytechnic\nUniversity, Guayaquil, Ecuador, where he leads the computer vision team\nat the CIDIS Research Center. He is also the Director of the Electrical\nEngineering Ph.D. Program. His research interests include cross-spectral\nimage processing and representation, 3D data acquisition, processing,\nmodeling, and computer vision applications.\n11540 VOLUME 11, 2023"
}