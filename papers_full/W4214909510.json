{
  "title": "Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators",
  "url": "https://openalex.org/W4214909510",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4208472320",
      "name": "Byung-Doh Oh",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2680413581",
      "name": "Christian Clark",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2097769359",
      "name": "William R. Schuler",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A4208472320",
      "name": "Byung-Doh Oh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2680413581",
      "name": "Christian Clark",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097769359",
      "name": "William R. Schuler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6768686062",
    "https://openalex.org/W6782341749",
    "https://openalex.org/W2318908254",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6632248436",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2139450036",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6679893334",
    "https://openalex.org/W6696775231",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W3083146265",
    "https://openalex.org/W6750421422",
    "https://openalex.org/W6750153154",
    "https://openalex.org/W6663937750",
    "https://openalex.org/W6750791894",
    "https://openalex.org/W3100748148",
    "https://openalex.org/W6680768788",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1984685117",
    "https://openalex.org/W3096699748",
    "https://openalex.org/W3037544092",
    "https://openalex.org/W6692563993",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2151969869",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W3158758286",
    "https://openalex.org/W1989939224",
    "https://openalex.org/W6691751413",
    "https://openalex.org/W6797970485",
    "https://openalex.org/W6680385414",
    "https://openalex.org/W2066476588",
    "https://openalex.org/W6667400720",
    "https://openalex.org/W2222142803",
    "https://openalex.org/W2130300813",
    "https://openalex.org/W6765629225",
    "https://openalex.org/W2997938465",
    "https://openalex.org/W2891040556",
    "https://openalex.org/W3160285835",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2108010971",
    "https://openalex.org/W1975387939",
    "https://openalex.org/W2294966994",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6779016856",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W2593581739",
    "https://openalex.org/W2781528640",
    "https://openalex.org/W2032152873",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2916486311",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2792932412",
    "https://openalex.org/W3171953676",
    "https://openalex.org/W2541061885",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2605063145",
    "https://openalex.org/W4365799947",
    "https://openalex.org/W2915722758",
    "https://openalex.org/W2326588846",
    "https://openalex.org/W2620949368"
  ],
  "abstract": "Expectation-based theories of sentence processing posit that processing difficulty is determined by predictability in context. While predictability quantified via surprisal has gained empirical support, this representation-agnostic measure leaves open the question of how to best approximate the human comprehender's latent probability model. This article first describes an incremental left-corner parser that incorporates information about common linguistic abstractions such as syntactic categories, predicate-argument structure, and morphological rules as a computational-level model of sentence processing. The article then evaluates a variety of structural parsers and deep neural language models as cognitive models of sentence processing by comparing the predictive power of their surprisal estimates on self-paced reading, eye-tracking, and fMRI data collected during real-time language processing. The results show that surprisal estimates from the proposed left-corner processing model deliver comparable and often superior fits to self-paced reading and eye-tracking data when compared to those from neural language models trained on much more data. This may suggest that the strong linguistic generalizations made by the proposed processing model may help predict humanlike processing costs that manifest in latency-based measures, even when the amount of training data is limited. Additionally, experiments using Transformer-based language models sharing the same primary architecture and training data show a surprising negative correlation between parameter count and fit to self-paced reading and eye-tracking data. These findings suggest that large-scale neural language models are making weaker generalizations based on patterns of lexical items rather than stronger, more humanlike generalizations based on linguistic structure.",
  "full_text": "ORIGINAL RESEARCH\npublished: 03 March 2022\ndoi: 10.3389/frai.2022.777963\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 1 March 2022 | Volume 5 | Article 777963\nEdited by:\nSebastian Padó,\nUniversity of Stuttgart, Germany\nReviewed by:\nJoshua Waxman,\nYeshiva University, United States\nVera Demberg,\nSaarland University, Germany\n*Correspondence:\nByung-Doh Oh\noh.531@osu.edu\nSpecialty section:\nThis article was submitted to\nLanguage and Computation,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nReceived: 16 September 2021\nAccepted: 31 January 2022\nPublished: 03 March 2022\nCitation:\nOh B-D, Clark C and Schuler W\n(2022) Comparison of Structural\nParsers and Neural Language Models\nas Surprisal Estimators.\nFront. Artif. Intell. 5:777963.\ndoi: 10.3389/frai.2022.777963\nComparison of Structural Parsers\nand Neural Language Models as\nSurprisal Estimators\nByung-Doh Oh*, Christian Clark and William Schuler\nDepartment of Linguistics, The Ohio State University, Colu mbus, OH, United States\nExpectation-based theories of sentence processing posit t hat processing difﬁculty is\ndetermined by predictability in context. While predictabil ity quantiﬁed via surprisal has\ngained empirical support, this representation-agnostic m easure leaves open the question\nof how to best approximate the human comprehender’s latent p robability model. This\narticle ﬁrst describes an incremental left-corner parser t hat incorporates information\nabout common linguistic abstractions such as syntactic cat egories, predicate-argument\nstructure, and morphological rules as a computational-lev el model of sentence\nprocessing. The article then evaluates a variety of structu ral parsers and deep neural\nlanguage models as cognitive models of sentence processing by comparing the\npredictive power of their surprisal estimates on self-pace d reading, eye-tracking, and\nfMRI data collected during real-time language processing. The results show that surprisal\nestimates from the proposed left-corner processing model d eliver comparable and often\nsuperior ﬁts to self-paced reading and eye-tracking data wh en compared to those from\nneural language models trained on much more data. This may su ggest that the strong\nlinguistic generalizations made by the proposed processin g model may help predict\nhumanlike processing costs that manifest in latency-based measures, even when the\namount of training data is limited. Additionally, experime nts using Transformer-based\nlanguage models sharing the same primary architecture and t raining data show a\nsurprising negative correlation between parameter count a nd ﬁt to self-paced reading\nand eye-tracking data. These ﬁndings suggest that large-sc ale neural language models\nare making weaker generalizations based on patterns of lexi cal items rather than stronger,\nmore humanlike generalizations based on linguistic struct ure.\nKeywords: sentence processing, incremental parsers, language models, surprisal theory, self-paced reading,\neye-tracking, fMRI\n1. INTRODUCTION\nMuch work in sentence processing has been dedicated to studyin g diﬀerential patterns of\nprocessing diﬃculty in order to shed light on the latent mecha nism underlying incremental\nprocessing. Within this line of work, expectation-based theo ries of sentence processing (\nHale,\n2001; Levy, 2008 ) have posited that processing diﬃculty is mainly driven by pred ictability in\nOh et al. Comparison of Parser and LM Surprisal\ncontext, or how predictable upcoming linguistic material is\ngiven its context. In support of this position, predictability\nquantiﬁed through information-theoretic surprisal (\nShannon,\n1948) has been shown to strongly correlate with behavioral\nand neural measures of processing diﬃculty ( Hale, 2001;\nDemberg and Keller, 2008; Levy, 2008; Roark et al., 2009;\nSmith and Levy, 2013; van Schijndel and Schuler, 2015;\nHale et al., 2018; Shain, 2019; Shain et al., 2020,\ninter alia).\nHowever, as surprisal can be calculated from any probability\ndistribution deﬁned over words and therefore makes minimal\nassumptions about linguistic representations that are built d uring\nsentence processing, this leaves open the question of how\nto best estimate the human language comprehender’s latent\nprobability model.\nIn previous studies, two categories of natural language\nprocessing (NLP) systems have been evaluated as surprisal-\nbased cognitive models of sentence processing. The ﬁrst are\nlanguage models (LMs), which directly deﬁne and estimate a\nconditional probability distribution of a word given its con text.\nSurprisal estimates from several well-established types of L Ms,\nincluding n-gram models, Simple Recurrent Networks (SRN;\nElman, 1991 ), and Long Short-Term Memory networks (LSTM;\nHochreiter and Schmidhuber, 1997 ), have been compared against\nbehavioral measures of processing diﬃculty (e.g., Smith and Levy,\n2013; Goodkind and Bicknell, 2018; Aurnhammer and Frank,\n2019\n). More recently, Transformer-based ( Vaswani et al., 2017 )\nmodels trained on massive amounts of data have dominated\nmany NLP tasks (\nDevlin et al., 2018; Liu et al., 2019; Brown\net al., 2020 ), causing a surge of interest in evaluating whether\nthese models acquire a humanlike understanding of language . As\nsuch, both large pretrained and smaller “trained-from-scra tch”\nTransformer-based LMs have been evaluated as models of\nprocessing diﬃculty (\nHao et al., 2020; Wilcox et al., 2020; Merkx\nand Frank, 2021 ).\nThe second category of NLP systems are incremental parsers,\nwhich make explicit decisions and maintain multiple hypothese s\nabout the linguistic structure associated with the sentenc e.\nSurprisal can be calculated from preﬁx probabilities of the\nword sequences at consecutive time steps by marginalizing ov er\nthese hypotheses. In this case, surprisal can be derived from\nthe Kullback–Leibler divergence between the two probabilit y\ndistributions over hypotheses and can be interpreted as the\namount of “cognitive eﬀort” taken to readjust the hypotheses\nafter observing a word (\nLevy, 2008 ). Examples of incremental\nparsers that have been applied as models of sentence processing\ninclude Earley parsers (\nHale, 2001 ), top-down parsers ( Roark\net al., 2009 ), Recurrent Neural Network Grammars ( Dyer et al.,\n2016; Hale et al., 2018 ), and left-corner parsers ( van Schijndel\net al., 2013; Jin and Schuler, 2020 ).\nThis article aims to contribute to this line of research\nby ﬁrst presenting an incremental left-corner parser that\nincorporates information about common linguistic abstract ions\nas a computational-level (\nMarr, 1982 ) model of sentence\nprocessing. This parser makes explicit predictions about synta ctic\ntree nodes with rich category labels from a generalized\ncategorial grammar (\nAjdukiewicz, 1935; Bar-Hillel, 1953; Bach,\n1981; Nguyen et al., 2012 ) as well as their associated\npredicate-argument structure. Additionally, this parser inc ludes\na character-based word generation model which deﬁnes the\nprocess of generating a word from an underlying lemma\nand a morphological rule, allowing the processing model\nto capture the predictability of given word forms in a\nﬁne-grained manner.\nSubsequently, we evaluate this parser as well as a range of other\nLMs and incremental parsers from previous literature on their\nability to predict measures of processing diﬃculty from human\nsubjects, including self-paced reading times, eye-gaze dur ations,\nand blood oxygenation level-dependent (BOLD) signals colle cted\nthrough fMRI. Our experiments yield two main ﬁndings. First,\nwe ﬁnd that our structural processing model achieves a strong ﬁt\nto latency-based measures (i.e., self-paced reading times a nd eye-\ngaze durations) that is comparable and in many cases superior\nto large-scale LMs, despite the fact that the LMs are trained\non much more data and show lower perplexities on test data.\nSecond, experiments using Transformer-based GPT-2 models\n(\nRadford et al., 2019 ) of varying capacities that share the same\nprimary architecture and training data show a surprising nega tive\ncorrelation between parameter count and ﬁt to self-paced read ing\nand eye-tracking data. In other words, Transformer models w ith\nfewer parameters were able to make better predictions when the\ntraining data was held constant.\nThese results suggest that the strong linguistic generaliza tions\nmade by incremental parsers may be helpful for predicting\nhumanlike processing costs that manifest in latency-based\nmeasures, even when the amount of training data is limited.\nIn addition, they add a new nuance to the relationship\nbetween language model perplexity and psychometric predictive\npower noted in recent psycholinguistic studies. While the\ncomparison of neural LMs and incremental parsers mostly\nsupports the linear relationship ﬁrst reported by\nGoodkind\nand Bicknell (2018) , our structural parser and the diﬀerent\nvariants of GPT-2 models provide counterexamples to this\ntrend. This suggests that the relationship between perplexity\nand predictive power may be mostly driven by the diﬀerence\nin their primary architecture or the amount of data used\nfor training.\nThis article is an extended presentation of\nOh et al. (2021) ,\nwith additional algorithmic details of the left-corner pars er and\nevaluations of structural parsers and neural LMs as surprisal\nestimators. These additional evaluations include a quanti tative\nanalysis of the eﬀect of model capacity on predictive power for\nneural LMs, as well as a replication of the main experiments\nusing a diﬀerent regression method that is sensitive to tempora l\ndiﬀusion. Code used in this work can be found at https://github.\ncom/modelblocks/modelblocks-release and https://github.com/\nbyungdoh/acl21_semproc.\nThe remainder of this article is structured as follows: Sect ion\n2 reviews earlier literature on evaluating neural and struct ural\nmodels of sentence processing; Section 3 provides a formal\nbackground on surprisal and left-corner parsing; Section 4\nintroduces our structural processing model; Sections 5 to 8\noutline the regression experiments using data from human\nsubjects; and Section 9 concludes with a discussion of the\nmain ﬁndings.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 2 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\n2. RELATED WORK\nSeveral recent studies have examined the predictive ability\nof various neural and structural models on psycholinguistic\ndata using surprisal predictors.\nGoodkind and Bicknell (2018)\ncompare surprisal-based predictions from a set of n-gram, LSTM,\nand interpolated (LSTM + n-gram) LMs. Testing on the Dundee\neye-tracking corpus ( Kennedy et al., 2003 ), the authors report a\nlinear relationship between the LM’s linguistic quality (me asured\nby perplexity) and its psychometric predictive power (measured\nby regression model ﬁt).\nWilcox et al. (2020) perform a similar analysis with more\nmodel classes, evaluating n-gram, LSTM, Transformer, and\nRNNG models on self-paced reading and eye-tracking data.\nEach type of LM is trained from scratch on corpora of varying\nsizes. Their results partially support the linear relationshi p\nbetween perplexity and psychometric predictive power reported\nin\nGoodkind and Bicknell (2018) , although they note a more\nexponential relationship at certain intervals. In addition , Wilcox\net al. also ﬁnd that a model’s primary architecture aﬀects its\npsychometric predictive power. When perplexity is held roughly\nconstant, Transformer models tend to make the best reading t ime\nand eye-tracking predictions, followed by n-gram models, LSTM\nmodels, and RNNG models.\nHao et al. (2020) also examine psycholinguistic predictions\nfrom Transformer, n-gram, and LSTM models, evaluating each\non eye-tracking data. Large pretrained Transformers such as\nGPT-2 (\nRadford et al., 2019 ) are tested alongside smaller\nTransformers trained from scratch. When comparing perplexity\nand psycholinguistic performance,\nHao et al. observe a similar\ntrend across architectures to that reported by Wilcox et al. (2020) ,\nwith Transformers performing best and LSTMs performing\nworse compared to n-gram models. However,\nHao et al.\nargue that perplexity is ﬂawed as a predictor of psychometric\npredictive ability, given that perplexity is sensitive to a mod el’s\nvocabulary size. Instead, they introduce a new metric for\nevaluating LM performance, Predictability Norm Correlatio n\n(PNC), which is deﬁned as the Pearson correlation between\nsurprisal values from a language model and surprisal values\nmeasured from human subjects using the Cloze task. Their\nsubsequent evaluation shows a more robust relationship bet ween\nPNC and psycholinguistic performance than between perplexity\nand psycholinguistic performance.\nAurnhammer and Frank (2019) compare a set of SRN, LSTM,\nand Gated Recurrent Unit (GRU; Cho et al., 2014 ) models,\nall trained on Section 1 of the English Corpora from the\nWeb (ENCOW;\nSchäfer, 2015 ), on their ability to predict self-\npaced reading times, eye-gaze durations, and N400 measures\nfrom electroencephalography (EEG) experiments. They ﬁnd\nthat as long as the three types of models achieve a similar\nlevel of language modeling performance, there is no reliable\ndiﬀerence in their predictive power.\nMerkx and Frank (2021)\nextend this study by comparing Transformer models against\nGRU models following similar experimental methods. The\nTransformer models are found to outperform the GRU models\non explaining self-paced reading times and N400 measures but\nnot eye-gaze durations. The authors view this as evidence th at\nhuman sentence processing may involve cue-based retrieval\nrather than recurrent processing.\n3. BACKGROUND\nThe experiments presented in this article use surprisal predicto rs\n(\nShannon, 1948 ) calculated by an incremental processing model\nbased on a left-corner parser ( Johnson-Laird, 1983; van Schijndel\net al., 2013 ). This incremental processing model provides a\nprobabilistic account of sentence processing by making a sing le\nlexical attachment decision and a single grammatical attac hment\ndecision for each input word.\n3.1. Surprisal\nSurprisal can be deﬁned as the negative log ratio of preﬁx\nprobabilities of word sequences w1..t at consecutive time\nsteps t − 1 and t:\nS(wt) def= − log P(w1..t)\nP(w1..t−1) (1)\nThese preﬁx probabilities can be calculated by marginalizing over\nthe hidden states qt of the forward probabilities of an incremental\nprocessing model:\nP(w1..t) =\n∑\nqt\nP(w1..t qt) (2)\nThese forward probabilities are in turn deﬁned recursively u sing\na transition model:\nP(w1..t qt) def=\n∑\nqt−1\nP(wt qt | qt−1) ·P(w1..t−1 qt−1) (3)\n3.2. Left-Corner Parsing\nSome of the transition models presented in this article are\nbased on a probabilistic left-corner parser (\nJohnson-Laird, 1983;\nvan Schijndel et al., 2013 ). Left-corner parsers have been used\nto model human sentence processing because they deﬁne a\nﬁxed number of decisions at every time step and also require\nonly a bounded amount of working memory, in keeping with\nexperimental observations of human memory limits (\nMiller and\nIsard, 1963 ). The transition model maintains a distribution over\npossible working memory store states qt at every time step t,\neach of which consists of a bounded number D of nested\nderivation fragments ad\nt /bd\nt . Each derivation fragment spans a\npart of a derivation tree below some apex node ad\nt lacking a\nbase node bd\nt yet to come. Previous work has shown that large\nannotated corpora such as the Penn Treebank (\nMarcus et al.,\n1993) do not require more than D = 4 of such fragments\n(Schuler et al., 2010 ).\nAt each time step, a left-corner parsing model generates a new\nword wt and a new store state qt in two phases (see Figure 1).\nFirst, it makes a set of lexical decisions ℓt regarding whether to\nuse the word to complete the most recent derivation fragment\n(match; mℓt =1), or to use the word to create a new preterminal\nnode aℓt (no-match; mℓt =0). Subsequently, the model makes a set\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 3 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 1 |Left-corner parser operations: (A) lexical match ( mℓt =1) and no-match ( mℓt =0) operations, creating new apex aℓt , and (B) grammatical match ( mgt =1)\nand no-match ( mgt =0) operations, creating new apex agt and base bgt .\nof grammatical decisions gt regarding whether to use a predicted\ngrammar rule to combine the node constructed in the lexical\nphase aℓt with the next most recent derivation fragment ( match;\nmgt =1), or to use the grammar rule to convert this node into a\nnew derivation fragment agt /bgt (no-match; mgt =0)1:\nP(wt qt | qt−1) =\n∑\nℓt,gt\nP(ℓt | qt−1) ·\nP(wt | qt−1 ℓt) ·\nP(gt | qt−1 ℓt wt) ·\nP(qt | qt−1 ℓt wt gt) (4)\nThus, the parser creates a hierarchically organized sequenc e of\nderivation fragments and joins these fragments up whenever\nexpectations are satisﬁed.\nIn order to update the store state based on the lexical and\ngrammatical decisions, derivation fragments above the mos t\nrecent nonterminal node are carried forward, and derivatio n\nfragments below it are set to null ( ⊥):\nP(qt | . . . ) def=\nD∏\nd′=1\n\n\n\n\n\n\n\n\n\n[ [\nad′\nt , bd′\nt = ad′\nt−1, bd′\nt−1\n] ]\nif d′ < d\n[ [\nad′\nt , bd′\nt = agt , bgt\n] ]\nif d′ = d\n[ [\nad′\nt , bd′\nt = ⊥ , ⊥\n] ]\nif d′ > d\n(5)\nwhere the indicator function [ [ [ [ϕ] ] ] ]= 1 if ϕ is true and 0 otherwise,\nand d = argmaxd′ {ad′\nt−1̸=⊥} +1 − mℓt − mgt . Together, these\nprobabilistic decisions generate the n unary branches and n − 1\nbinary branches of a parse tree in Chomsky normal form for an\nn-word sentence.\n4. STRUCTURAL PROCESSING MODEL\nUnlike the large pretrained neural LMs used in these\nexperiments, the structural processing model is deﬁned in\nterms of a set of common linguistic abstractions, including\n• Syntax treeswith nodes labeled by syntactic categoriesdrawn\nfrom a generalized categorial grammar (\nAjdukiewicz, 1935;\nBar-Hillel, 1953; Bach, 1981; Nguyen et al., 2012 ),\n1Johnson-Laird (1983) refers to lexical and grammatical decisions as “shift” and\n“predict”, respectively.\n• Logical predicateswith arguments signiﬁed by associated nodes\nin the tree, and\n• Morphological rules which associate transformations in\nlexical orthography with transformations between syntacti c\ncategories of words.\nThese form the “strong generalizations” in the introductio n and\nconclusion of this article.\n4.1. Processing Model\nThe structural processing model extends the above left-\ncorner parser (Section 3.2) to maintain lemmatized predicate\ninformation by augmenting each preterminal, apex, and base\nnode to consist not only of a syntactic category label cpt , cad\nt\n, or\ncbd\nt\n, but also of a binary predicate context vectorhpt , had\nt\n, or hbd\nt\n∈\n{0, 1}K+VK+EK , where K is the size of the set of predicate contexts\nand V is the maximum valence of any syntactic category 2, and\nE is the maximum number of non-local arguments (e.g., gap\nﬁllers) expressed in any category. Each 0 or 1 element of this\nvector represents a unique predicate context, which consists of\na ⟨predicate, role⟩ pair that speciﬁes the content constraints\nof a node in a predicate-argument structure. These predicate\ncontexts are obtained by reannotating the training corpus usi ng\na generalized categorial grammar of English (\nNguyen et al.,\n2012)3, which is sensitive to syntactic valence and non-local\ndependencies. For example, in Figure 2, the variable e2 (signiﬁed\nby the word eat) would have the predicate context EAT0 because it\nis the zeroth (initial) participant of the predication ( eate2 x1 x3)4.\nSimilarly, the variable x3 would have both the predicate context\nPASTA1, because it is the ﬁrst participant (counting from zero)\nof the predication ( pastae3 x3), and the predicate context EAT2,\nbecause it is the second participant (counting from zero) of th e\npredication (eate2 x1 x3).\n2The valence of a category is the number of unsatisﬁed syntactic a rguments\nit has. Separate vectors for each syntactic argument are needed i n order to\ncorrectly model cases such as passives where syntactic arguments do not align with\npredicate arguments.\n3The predicates in this annotation scheme come from words that have b een\nlemmatized by a set of rules that have been manually written and corrected in order\nto account for common irregular inﬂections.\n4Participants of predications are numbered starting with zero so as to a lign loosely\nwith syntactic arguments in canonical form.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 4 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 2 |Lambda calculus expression for the propositional content o f the\nsentence. Many people eat pasta, using generalized quantiﬁers over discourse\nentities and eventualities.\n4.1.1. Lexical Decisions\nEach lexical decision of the parser includes a match decision mℓt\nand decisions about a syntactic category cℓt and a predicate\ncontext vector hℓt that together specify a preterminal\nnode pℓt . The probability of generating the match decision\nand the predicate context vector depends on the base\nnode bd\nt−1 of the previous derivation fragment (i.e., its\nsyntactic category and predicate context vector). The ﬁrst\nterm of Equation (4) can therefore be decomposed into\nthe following:\nP(ℓt | qt−1) = SOFT MAX\nmℓt hℓt\n( FFθL [δd⊤, [δ⊤\ncbd\nt−1\n, h⊤\nbd\nt−1\n] EL] )·\nP(cℓt | qt−1 mℓt hℓt ) (6)\nwhere FF is a feedforward neural network, and δi is a\nKronecker delta vector consisting of a one at element i\nand zeros elsewhere. Depth d = argmaxd′ {ad′\nt−1̸=⊥}is the\nnumber of non-null derivation fragments at the previous\ntime step, and EL is a matrix of jointly trained dense\nembeddings for each syntactic category and predicate contex t.\nThe syntactic category and predicate context vector togethe r\ndeﬁne a complete preterminal node pℓt for use in the word\ngeneration model:\npℓt\ndef=\n{\ncbd\nt−1\n, hbd\nt−1\n+ hℓt if mℓt = 1\ncℓt , hℓt if mℓt = 0\n(7)\nand a new apex node aℓt for use in the grammatical\ndecision model:\naℓt\ndef=\n{\ncad\nt−1\n, had\nt−1\n+ Zt−1 hpℓt if mℓt = 1\npℓt if mℓt = 0\n(8)\nwhere Zt propagates predicate contexts from right progeny back\nup to apex nodes (see Equation 12 below).\n4.1.2. Grammatical Decisions\nEach grammatical decision includes a match decision mgt and\ndecisions about a pair of syntactic category labels cgt and c′\ngt ,\nas well as a predicate context composition operator ogt , which\ngoverns how the newly generated predicate context vector hℓt\nis propagated through its new derivation fragment agt /bgt .\nThe probability of generating the match decision and the\ncomposition operators depends on the base node b\nd−mℓt\nt−1\nof the previous derivation fragment and the apex node\naℓt from the current lexical decision (i.e., their syntactic\ncategories and predicate context vectors). The third term\nof Equation (4) can accordingly be decomposed into\nthe following:\nP(gt | qt−1 ℓt wt)\n= SOFT MAX\nmgt ogt\n( FFθG [δd⊤, [δ⊤\nc\nb\nd−mℓt\nt−1\n, h⊤\nb\nd−mℓt\nt−1\n, δ⊤\ncaℓt\n, h⊤\naℓt\n] EG] ) ·\nP(cgt | qt−1 ℓt wt mgt ogt ) · P(c′\ngt | qt−1 ℓt wt mgt ogt cgt ) (9)\nwhere EG is a matrix of jointly trained dense embeddings\nfor each syntactic category and predicate context. The\ncomposition operators are associated with sparse composition\nmatrices Aogt , deﬁned in Appendix A, which can be used to\ncompose predicate context vectors associated with the apex\nnode agt :\nagt\ndef=\n\n\n\nc\na\nd−mℓt\nt−1\n, h\na\nd−mℓt\nt−1\n+ Zt−1 A⊤\nogt\nhaℓt if mgt = 1\ncgt , A⊤\nogt\nhaℓt if mgt = 0\n(10)\nand sparse composition matrices Bogt , also deﬁned in\nAppendix A, which can be used to compose predicate context\nvectors associated with the base node bgt :\nbgt\ndef=\n\n\n\nc′\ngt , Bogt [h\nb\nd−mℓt\nt−1\n⊤, haℓt\n⊤]⊤ if mgt =1\nc′\ngt , Bogt [0⊤, haℓt\n⊤]⊤ if mgt =0\n(11)\nMatrix Zt propagates predicate contexts from right progeny back\nup to apex nodes 5:\nZt\ndef=\n{\nZt−1 [0H×H, IH×H] Bogt\n⊤ if mgt = 1\n[0H×H, IH×H] Bogt\n⊤ if mgt = 0 (12)\n4.2. Character-Based Morphological Word\nModel\nA character-based morphological word model applies a\nmorphological rule rt to a lemma xt to generate an inﬂected form\nwt. The set of rules model aﬃxation through string substitutio n\nand are inverses of lemmatization rules that are used to deriv e\npredicates in the generalized categorial grammar annotation\n(\nNguyen et al., 2012 ). For example, the rule %ay→%aidcan\napply to the word say to derive its past tense form said. There\nare around 600 such rules that account for inﬂection in Secti ons\n02 to 21 of the Wall Street Journal corpus of the Penn Treebank\n(\nMarcus et al., 1993 ), which includes an identity rule for words\nin bare form and a “no semantics” rule for generating certain\nfunction words.\nFor an observed input word wt, the model ﬁrst generates a\nlist of ⟨xt, rt⟩pairs that deterministically generate wt. This allows\n5Only identity propagation is implemented in the experiments described in\nthis article.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 5 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nthe model to capture morphological regularity and estimate\nhow expected a word form is given its predicted syntactic\ncategory and predicate context, which have been generated as\npart of the preceding lexical decision. In addition, this lets the\nmodel hypothesize the underlying morphological structure of\nout-of-vocabulary words and assign probabilities to them. T he\nsecond term of Equation (4) can thus be decomposed into\nthe following:\nP(wt | qt−1 ℓt) =\n∑\nxt,rt\nP(xt | qt−1 ℓt) ·\nP(rt | qt−1 ℓt xt) ·\nP(wt | qt−1 ℓt xt rt) (13)\nThe probability of generating the lemma sequence depends on\nthe syntactic category cpℓt and predicate context hℓt resulting\nfrom the preceding lexical decision ℓt:\nP(xt | qt−1 ℓt) =\n∏\ni\nSOFT MAX\nxt,i\n( WX xt,i + bX ) (14)\nwhere xt,1, xt,2, . . . , xt,I is the character sequence of lemma\nxt, with xt,1 = ⟨s⟩ and xt,I = ⟨e⟩ as special start and end\ncharacters. WX and bX are, respectively, a weight matrix and bias\nvector of a softmax classiﬁer. A recurrent neural network (RN N)\ncalculates a hidden state xt,i for each character from an input\nvector at that time step and the hidden state after the previou s\ncharacter xt,i−1:\nxt,i = RNNθX ( [δ⊤\ncpℓt\n, h⊤\nℓt , δ⊤\nxt,i ] EX, x⊤\nt,i−1 ) (15)\nwhere EX is a matrix of jointly trained dense embeddings for each\nsyntactic category, predicate context, and character.\nSubsequently, the probability of applying a particular\nmorphological rule to the generated lemma depends on the\nsyntactic category cpℓt and predicate context hℓt from the\npreceding lexical decision as well as the character sequence of\nthe lemma:\nP(rt | qt−1 ℓt xt) = SOFT MAX\nrt\n( WR rt,I + bR ) (16)\nHere, WR and bR are, respectively, a weight matrix\nand bias vector of a softmax classiﬁer. rt,I is the last\nhidden state of an RNN that takes as input the syntactic\ncategory, predicate context, and character sequence of\nthe lemma xt,2, xt,3, . . . , xt,I−1 without the special start and\nend characters:\nrt,i = RNNθR ( [δ⊤\ncpℓt\n, h⊤\nℓt , δ⊤\nxt,i ] ER, r⊤\nt,i−1 ) (17)\nwhere ER is a matrix of jointly trained dense embeddings for each\nsyntactic category, predicate context, and character.\nFinally, as the model calculates probabilities only for ⟨xt, rt⟩\npairs that deterministically generate wt, the word probability\nconditioned on these variables P(wt | qt−1 ℓt xt rt) = 1.\n5. EXPERIMENT 1: PREDICTIVE POWER\nOF SURPRISAL ESTIMATES\nIn order to compare the predictive power of surprisal estimates\nfrom structural parsers and LMs, regression models containing\ncommon baseline predictors and a surprisal predictor were\nﬁtted to self-paced reading times, eye-gaze durations, and b lood\noxygenation level-dependent signals collected during natu ralistic\nlanguage processing. For self-paced reading times and eye-\ngaze durations that were measured at the word level, linear\nmixed-eﬀects models were ﬁtted to the response data. In\ncontrast, for blood oxygenation level-dependent signals th at were\nmeasured in ﬁxed-time intervals, the novel statistical frame work\nof continuous-time deconvolutional regression (CDR;\nShain\nand Schuler, 2021 ) was employed. As CDR allows the data-\ndriven estimation of continuous impulse response functions\nfrom variably spaced linguistic input, it is more appropriate\nfor modeling fMRI responses, which are typically measured\nin ﬁxed time intervals. To compare the predictive power of\nsurprisal estimates from diﬀerent models on equal footing,\nwe calculated the increase in log-likelihood ( /Delta1LL) to a\nbaseline regression model as a result of including a surprisa l\npredictor, following recent work (\nGoodkind and Bicknell,\n2018; Aurnhammer and Frank, 2019; Hao et al., 2020;\nWilcox et al., 2020\n).\n5.1. Response Data\n5.1.1. Self-Paced Reading Times\nThe ﬁrst experiment described in this article used the\nNatural Stories Corpus (\nFutrell et al., 2021 ), which contains\nself-paced reading times from 181 subjects that read 10\nnaturalistic stories consisting of 10,245 tokens. The data were\nﬁltered to exclude observations corresponding to sentence-\ninitial and sentence-ﬁnal words, observations from subjec ts\nwho answered fewer than four comprehension questions\ncorrectly, and observations with durations shorter than 10 0\nms or longer than 3,000 ms. This resulted in a total of\n770,102 observations, which were subsequently partitioned into\nan exploratory set of 384,905 observations and a held-out\nset of 385,197 observations 6. The partitioning allows model\nselection (e.g., making decisions about baseline predictors and\nrandom eﬀects structure) to be conducted on the exploratory\nset and a single hypothesis test to be conducted on the\nheld-out set, thus eliminating the need for multiple trials\ncorrection. All observations were log-transformed prior to\nmodel ﬁtting.\n5.1.2. Eye-Gaze Durations\nAdditionally, the set of go-past durations from the Dundee\nCorpus (\nKennedy et al., 2003 ) provided the response variable\nfor the regression models. The Dundee Corpus contains eye-\ngaze durations from 10 subjects that read 67 newspaper editori als\nconsisting of 51,501 tokens. The data were ﬁltered to exclude\n6The exploratory set contains data points whose summed subject and se ntence\nnumber have modulo four equal to zero or one, and the held-out set co ntains data\npoints whose summed subject and sentence number have modulo four eq ual to\ntwo or three.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 6 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nunﬁxated words, words following saccades longer than four\nwords, and words at starts and ends of sentences, screens,\ndocuments, and lines. This resulted in a total of 195,507\nobservations, which were subsequently partitioned into an\nexploratory set of 98,115 observations and a held-out set of\n97,392 observations 7. All observations were log-transformed\nprior to model ﬁtting.\n5.1.3. Blood Oxygenation Level-Dependent Signals\nFinally, the time series of blood oxygenation level-depende nt\n(BOLD) signals in the language network, which were identiﬁe d\nusing functional magnetic resonance imaging (fMRI), were\nanalyzed. This experiment used the same fMRI data used by\nShain et al. (2020) , which were collected at a ﬁxed-time interval\nof every 2 s from 78 subjects that listened to a recorded\nversion of the Natural Stories Corpus. The functional region s of\ninterest (fROI) corresponding to the domain-speciﬁc language\nnetwork were identiﬁed for each subject based on the results of\na localizer task that they conducted. This resulted in a tota l of\n194,859 observations, which were subsequently partitioned i nto\nan exploratory set of 98,115 observations and a held-out set o f\n96,744 observations 8.\n5.2. Predictors\n5.2.1. Baseline Predictors\nFor each dataset, a set of baseline predictors that capture low -level\ncognitive processing were included in all regression models.\n• Self-paced reading times (\nFutrell et al., 2021 ): word length\nmeasured in characters, index of word position within\neach sentence\n• Eye-gaze durations (\nKennedy et al., 2003 ): word length\nmeasured in characters, index of word position within each\nsentence, saccade length, whether or not the previous word\nwas ﬁxated\n• BOLD signals (\nShain et al., 2020 ): index of fMRI sample within\nthe current scan, the deconvolutional intercept which captur es\nthe inﬂuence of stimulus timing, whether or not the word is\nat the end of sentence, duration of pause between the current\nword and the next word.\n5.2.2. Surprisal Estimates\nFor regression modeling, surprisal estimates were also calc ulated\nfrom all models evaluated in this experiment. This includes t he\nstructural processing model described in Section 4, which wa s\ntrained on a generalized categorial grammar (GCG;\nNguyen et al.,\n2012) reannotation of Sections 02 to 21 of the Wall Street Journal\n(WSJ) corpus of the Penn Treebank ( Marcus et al., 1993 ). Beam\nsearch decoding with a beam size of 5,000 was used to estimate\npreﬁx probabilities and by-word surprisal for this model 9.\n7The partitioning for eye-gaze durations followed the same protocol a s the self-\npaced reading times.\n8For each participant, alternate 60-s intervals of BOLD series were as signed to the\ntwo partitions.\n9The most likely sequence of parsing decisions from beam search dec oding can\nalso be used to construct parse trees. This model achieves a bracket ing F1 score of\n84.76 on WSJ22, 82.64 on WSJ23, 71.86 on Natural Stories, and 69 .87 on Dundee.\nIt should be noted that this performance is lower than the state-of -the-art partly\nAdditionally, in order to assess the contribution of lingui stic\nabstractions, two ablated variants of the above structural\nprocessing model were trained and evaluated.\n• − cat: This variant ablates the contribution of syntactic\ncategory labels to the lexical and grammatical decisions by\nzeroing out their associated dense embeddings in Equations\n(6) and (9).\n• − morph: This variant ablates the contribution of the\ncharacter-based morphological word model by calculating the\nword generation probabilities (i.e., Equation 13) using rela tive\nfrequency estimation.\nFinally, various incremental parsers and pretrained LMs were\nused to calculate surprisal estimates at each word.\n• RNNG (\nDyer et al., 2016; Hale et al., 2018 ): An LSTM-based\nmodel with explicit phrase structure, trained on Sections 02 t o\n21 of the WSJ corpus.\n• vSLC (van Schijndel et al., 2013 ): A left-corner parser based\non a PCFG with subcategorized syntactic categories ( Petrov\net al., 2006 ), trained on a generalized categorial grammar\nreannotation of Sections 02 to 21 of the WSJ corpus.\n• JLC (Jin and Schuler, 2020 ): A neural left-corner parser based\non stack LSTMs ( Dyer et al., 2015 ), trained on Sections 02 to\n21 of the WSJ corpus.\n• 5-gram (Heaﬁeld et al., 2013 ): A 5-gram language model with\nmodiﬁed Kneser-Ney smoothing trained on ∼3B tokens of the\nEnglish Gigaword Corpus ( Parker et al., 2009 ).\n• GLSTM (Gulordava et al., 2018 ): A two-layer LSTM model\ntrained on ∼80M tokens of the English Wikipedia.\n• JLSTM (Jozefowicz et al., 2016 ): A two-layer LSTM model with\nCNN character inputs trained on ∼800M tokens of the One\nBillion Word Benchmark ( Chelba et al., 2014 ).\n• GPT2XL (Radford et al., 2019 ): GPT-2 XL, a 48-layer decoder-\nonly autoregressive Transformer model trained on ∼8B tokens\nof the WebText dataset.\n5.3. Procedures\nTo calculate the increase in log-likelihood ( /Delta1LL) attributable to\neach surprisal predictor, a baseline regression model containing\nonly the baseline predictors (Section 5.2.1) was ﬁrst ﬁtted t o\nthe held-out set of each dataset. For self-paced reading time s\nand eye-gaze durations which are by-word response measures,\nlinear mixed-eﬀects (LME) models were ﬁtted using lme4(\nBates\net al., 2015 ). All baseline predictors were centered and scaled\nprior to model ﬁtting, and the baseline LME models included by-\nsubject random slopes for all ﬁxed eﬀects and random intercepts\nfor each word and subject-sentence interaction. For BOLD\nsignals that were measured in ﬁxed-time intervals, there is a\ntemporal misalignment between the linguistic input (i.e., wor ds\nthat are variably spaced) and the response measures (i.e., BOL D\nsignals measured at ﬁxed-time intervals), making them less\nappropriate to model using LME regression. To overcome this\nbecause the model was trained on data with GCG-style annotation with hundreds\nof syntactic categories. For comparison, the parser from van Schijndel et al. (2013)\nachieves a bracketing F1 score of 85.20 on WSJ22, 84.08 on WSJ23 , 69.60 on\nNatural Stories, and 70.66 on Dundee.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 7 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nissue without arbitrarily coercing the data, the novel stat istical\nframework of continuous-time deconvolutional regression 10\n(CDR; Shain and Schuler, 2021 ) was employed to estimate\ncontinuous hemodynamic response functions (HRF). Following\nShain et al. (2020) , the baseline CDR model assumed the two-\nparameter HRF based on the double-gamma canonical HRF\n(\nLindquist et al., 2009 ). Furthermore, the two parameters of\nthe HRF were tied across predictors, modeling the assumption\nthat the shape of the blood oxygenation response to neural\nactivity is identical in a given region. However, to allow th e\nHRFs to have diﬀering amplitudes, a coeﬃcient that rescales\nthe HRF was estimated for each predictor. The “index of\nfMRI sample” and “duration of pause” baseline predictors were\nscaled, and the baseline CDR model also included a by-fROI\nrandom eﬀect for the amplitude coeﬃcient and a by-subject\nrandom intercept.\nSubsequently, full regression models that include one surprisal\npredictor (Section 5.2.2) on top of the baseline regression m odel\nwere ﬁtted to the held-out set of each dataset. For self-paced\nreading times and eye-gaze durations, the surprisal predicto r\nwas scaled and centered, and its by-subject random slopes\nwere included in the full LME model. Similarly, for BOLD\nsignals, the surprisal predictor was centered, and its by-fROI\nrandom eﬀect for the amplitude coeﬃcient was included in\nthe full CDR model. After all the regression models were\nﬁtted, /Delta1LL was calculated by subtracting the log-likelihood of\nthe baseline model from that of a full regression model. This\nresulted in /Delta1LL measures for all incremental parsers and LMs\non each dataset. Additionally, in order to examine whether\nany of the models fail to generalize across domains, their\nperplexity on the entire Natural Stories and Dundee corpora was\nalso calculated.\n5.4. Results\nThe results in Figure 3A show that surprisal from our structural\nmodel ( Structural) made the biggest contribution to regression\nmodel ﬁt compared to surprisal from other models on self-\npaced reading times. This ﬁnding, despite the fact that the\npretrained LMs were trained on much larger datasets and\nalso show lower perplexities on test data 11, suggests that this\nmodel may provide a more humanlike account of processing\ndiﬃculty. In other words, the strong generalizations that ar e\nmade by the structural model seem to help predict humanlike\nprocessing costs that manifest in self-paced reading times ev en\nwhen the amount of training data is limited. Performance of\nthe surprisal predictors from ablated variants of the Structural\nmodel shows that the character-based morphological word\nmodel makes an especially large contribution to regression\nmodel ﬁt, which may suggest a larger role of morphology\nand subword information in sentence processing. Additional ly,\nthe results show that although parsers like Structural and\nvSLC deviate from this pattern, there is generally a monotonic\nrelationship between the test perplexity and the predictive\n10https://github.com/coryshain/cdr\n11Perplexity of the parsers is higher partly because they optimize for a j oint\ndistribution over words and trees.\npower of the models ( Goodkind and Bicknell, 2018; Wilcox\net al., 2020 ). Most notably, the 5-gram model outperformed\nthe neural LMs in terms of both perplexity and /Delta1LL. This\nis most likely due to the fact that the model was trained\non much more data ( ∼3B tokens) compared to the LSTM\nmodels ( ∼80M and ∼800M tokens, respectively) and that it\nemploys modiﬁed Kneser-Ney smoothing, which allows lower\nperplexity to be achieved on words in the context of out-of-\nvocabulary words.\nResults from regression models ﬁtted on eye-gaze\ndurations ( Figure 3B) show a very similar trend to self-\npaced reading times in terms of both perplexity and\n/Delta1LL, although the contribution of surprisal predictors in\ncomparison to the baseline regression model is weaker. This\nprovides further support for the observation that the strong\nlinguistic generalizations that are not explicitly made by th e\nLMs do indeed help predict humanlike processing costs.\nMoreover, the similar trend across the two datasets may\nindicate that latency-based measures like self-paced readi ng\ntimes and eye-gaze durations capture similar aspects of\nprocessing diﬃculty.\nHowever, the regression models ﬁtted on BOLD signals\ndemonstrate a very diﬀerent trend ( Figure 3C), with surprisal\nfrom GPT2XL making the biggest contribution to model\nﬁt in comparison to surprisal from other models. Most\nnotably, in contrast to self-paced reading times and eye-\ngaze durations, surprisal estimates from Structural and 5-\ngram models did not contribute as much to model ﬁt on\nfMRI data, with a /Delta1LL lower than those of the LSTM\nmodels. This diﬀerential contribution of surprisal estimates\nacross datasets suggests that latency-based measures and b lood\noxygenation levels may be sensitive to diﬀerent aspects of onli ne\nprocessing diﬃculty.\n6. EXPERIMENT 2: INFLUENCE OF MODEL\nCAPACITY\nThe previous experiment revealed that at least for the\nneural LMs, there is a monotonic relationship between\nperplexity and predictive power on latency-based measures of\ncomprehension diﬃculty. Although evaluating “oﬀ-the-shelf ”\nLMs that have been shown to be eﬀective allows them to\nbe examined in their most authentic setting without the\nneed of expensive training procedures, this methodology leave s\nsome variables uncontrolled, such as the primary architectu re\n(e.g., Transformers or LSTMs), model capacity, or the training\ndata used. This experiment aims to bring under control the\nprimary architecture as well as the training data associated\nwith LMs by evaluating the perplexity and predictive power\nof diﬀerent variants of GPT-2 models, which diﬀer only in\nterms of model capacity (i.e., number of layers and parameters ).\nTo this end, following similar procedures as Experiment 1,\nsurprisal estimates from diﬀerent variants of GPT-2 models we re\nregressed to self-paced reading times, eye-gaze durations, and\nBOLD signals to examine their ability to predict behavioral a nd\nneural measures.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 8 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 3 |Perplexity measures from each model, and improvements in LM ER (A,B) and CDR (C) model log-likelihood from including each surprisal estima te on (A)\nNatural Stories self-paced reading data, (B) Dundee eye-tracking data, and (C) Natural Stories fMRI data. The difference in by-item square d error between the\nStructural and GPT2XL models is signiﬁcant at p < 0.05 level for all three datasets.\n6.1. Procedures\nTo calculate the /Delta1LL measure for each GPT-2 surprisal\npredictor, the same baseline regression models containing\nthe baseline predictors outlined in Section 5.2.1 were\nadapted from Experiment 1. Subsequently, in order to ﬁt\nfull regression models that include one surprisal predictor\non top of the baseline regression model, surprisal estimates\nfrom the following GPT-2 models (\nRadford et al., 2019 )\nthat were pretrained on ∼8B tokens of the WebText dataset\nwere calculated.\n• GPT-2 Small, with 12 layers and ∼124M parameters.\n• GPT-2 Medium, with 24 layers and ∼355M parameters.\n• GPT-2 Large, with 36 layers and ∼774M parameters.\n• GPT-2 XL, with 48 layers and ∼1558M parameters.\nSimilarly to Experiment 1, LME models that contain each\nof these surprisal predictors were ﬁtted to the held-out set\nof self-paced reading times and eye-gaze durations using\nlme4(\nBates et al., 2015 ). All predictors were centered and\nscaled prior to model ﬁtting, and the LME models included\nby-subject random slopes for all ﬁxed eﬀects and random\nintercepts for each word and subject-sentence interaction.\nAdditionally, CDR models assuming the two-parameter double -\ngamma canonical HRF were ﬁtted to the held-out set of\nBOLD signals. Again, the two parameters of the HRF were\ntied across predictors, but the HRFs were allowed to have\ndiﬀering amplitudes by jointly estimating a coeﬃcient that\nrescales the HRF for each predictor. The “index of fMRI sample, ”\n“duration of pause, ” and surprisal predictors were scaled, and\nthe CDR models also included a by-fROI random eﬀect for\nthe amplitude coeﬃcient and a by-subject random intercept.\nAfter all the regression models were ﬁtted, /Delta1LL for each GPT-\n2 model was calculated by subtracting the log-likelihood of\nthe baseline model from that of the full regression model\nwhich contains its surprisal estimates. To further examine t he\nrelationship between perplexity and predictive power, their\nperplexity on the entire Natural Stories and Dundee corpora was\nalso calculated.\n6.2. Results\nThe results in Figure 4A demonstrate that surprisal from GPT-2\nSmall (GPT2S), which has the least number of parameters, made\nthe biggest contribution to regression model ﬁt on self-pace d\nreading times compared to surprisal from larger GPT-2 models\nthat have more parameters. Contrary to the ﬁndings of the\nprevious experiment that showed a negative correlation betwe en\ntest perplexity and predictive power, a positive correlation is\nobserved between these two variables from the GPT-2 models\nthat were examined. This may indicate that the trend observe d in\nExperiment 1, where neural LMs with lower perplexity predicted\nlatency-based measures more accurately, may be driven more by\nthe diﬀerence in their primary architecture or the amount of da ta\nused for training, rather than their model capacity. Additio nally,\nthese results may suggest that when the training data is held\nconstant, neural LMs are able to make accurate predictions\nabout the upcoming word while relying less on humanlike\ngeneralizations as their capacity increases. In other words, the\nlarger LMs may be able to eﬀectively condition on a much larger\ncontext window to make their predictions, while human reading\ntimes may be inﬂuenced more by a smaller context window.\nAs with Experiment 1, the results from regression models ﬁtte d\non eye-gaze durations ( Figure 4B) show a very similar trend,\nproviding further evidence for the positive relationship betw een\nperplexity and predictive power observed on self-paced reading\ntimes. Again, the similar trend in perplexity and /Delta1LL across the\ntwo datasets may indicate that latency-based measures captu re\nsimilar aspects of processing diﬃculty.\nIn contrast, the regression models ﬁtted on BOLD signals\ndo not show a clear relationship between perplexity and /Delta1LL\n(Figure 4C), with surprisal from GPT2M making the biggest\ncontribution to model ﬁt and that from GPT2S making the\nsmallest contribution to model ﬁt. Such lack of the pattern\nobserved in latency-based measures could be attributed to th e\npossibility that latency-based measures and blood oxygenat ion\nlevels are sensitive to diﬀerent aspects of online processing\ndiﬃculty, as noted in Experiment 1. Additionally, the fMRI data\nseems to be noisier in general, as can be seen by the smaller ov erall\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 9 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 4 |Perplexity measures from each GPT -2 model, and improvement s in LMER (A,B) and CDR (C) model log-likelihood from including each surprisal estima te\non (A) Natural Stories self-paced reading data, (B) Dundee eye-tracking data, and (C) Natural Stories fMRI data. The difference in by-item square d error between the\nGPT2S and GPT2M models is signiﬁcant at p < 0.05 level for the self-paced reading and eye-tracking data , and the difference in by-item squared error between the\nGPT2M and GPT2XL models is signiﬁcant at p < 0.05 level for the fMRI data.\ncontribution of surprisal predictors in comparison to the base line\nlog-likelihood for the BOLD signals.\n7. EXPERIMENT 3: REPLICATION USING\nCONTINUOUS-TIME DECONVOLUTIONAL\nREGRESSION\nThe previous two experiments used LME regression to\ncompare the predictive quality of surprisal estimates from\nstructural parsers and LMs on latency-based measures of\ncomprehension diﬃculty (i.e., self-paced reading times and e ye-\ngaze durations). Although the use of LME regression is popular\nin psycholinguistic modeling, it is limited in that it is unab le\nto capture the lingering inﬂuence of the current predictor on\nfuture response measures (i.e., temporal diﬀusion). In the context\nof latency-based measures, this means that LME models cannot\nusually take into account the delay in processing that may be\ncaused after processing an unusually diﬃcult word. One common\napproach taken to address this issue is to include “spillover”\nvariants of predictors from preceding words (\nRayner et al.,\n1983; Vasishth, 2006 ). However, including multiple spillover\nvariants of the same predictor often leads to identiﬁability issues\nin LME regression ( Shain and Schuler, 2021 ). Additionally,\neven spillover predictors may not be able to capture the long-\nrange inﬂuence of the input if it falls out of the “spillover\nwindow.” This experiment aims to mitigate these drawbacks of\nLME regression used in the previous experiments by replicating\nthe analysis of latency-based measures using continuous-t ime\ndeconvolutional regression (CDR;\nShain and Schuler, 2021 ),\nwhich allows the data-driven estimation of continuous impuls e\nresponse functions. To this end, the LME regression analyses of\nExperiments 1 and 2 were replicated using CDR, following the\nsame protocol of ﬁtting baseline and full regression models an d\ncalculating the diﬀerence in their log-likelihoods ( /Delta1LL).\n7.1. Procedures\nFor both self-paced reading times and eye-gaze durations,\nbaseline CDR models were ﬁtted to the held-out set using\nthe baseline predictors described in Section 5.2.1. In addit ion,\nthe index of word position within each document 12 and the\ndeconvolutional intercept that captures the inﬂuence of stim ulus\ntiming were also included as a baseline predictors. Following\nShain and Schuler (2018) , the baseline CDR models assumed\nthe three-parameter ShiftedGamma IRF. The “index of word\nposition within each document” and “index of word position\nwithin each sentence” predictors were scaled, and the “word\nlength in characters” and “saccade length” predictors were b oth\ncentered and scaled. The baseline CDR models also included a\nby-subject random eﬀect for all predictors.\nIn order to ﬁt full models that include one surprisal predictor\non top of the baseline model, surprisal estimates from the\nparsers and LMs (Section 5.2.2) as well as diﬀerent variants\nof the pretrained GPT-2 models (Section 6.1) were calculated .\nSubsequently, CDR models that contain each of these surprisal\npredictors were ﬁtted to the held-out set of self-paced readin g\ntimes and eye-gaze durations. All surprisal predictors were s caled\nprior to model ﬁtting, and the full CDR models also included a\nby-subject random eﬀect for the surprisal predictor. After all the\nregression models were ﬁtted, /Delta1LL for each model was calculated\nby subtracting the log-likelihood of the baseline model from that\nof a full regression model that contains its surprisal estima tes.\n7.2. Results\nFigure 5 shows that on both self-paced reading times and eye-\ngaze durations, using CDR results in higher /Delta1LL measures for all\nevaluated models compared to the results using LME regression\nin Figure 3. This indicates the usefulness of CDR in capturing\n12This is analogous to the “index of fMRI sample” predictor for BOLD si gnals.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 10 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 5 |Perplexity measures from each model, and improvements in CD R model log-likelihood from including each surprisal estim ate on (A) Natural Stories\nself-paced reading data and (B) Dundee eye-tracking data. The difference in by-item square d error between the Structural and GPT2XL models is signiﬁcant at\np < 0.05 level for both datasets.\nFIGURE 6 |Perplexity measures from each GPT -2 model, and improvement s in CDR model log-likelihood from including each surprisal estimate on (A) Natural\nStories self-paced reading data and (B) Dundee eye-tracking data. The difference in by-item square d error between the GPT2S and GPT2L models is signiﬁcant at\np < 0.05 level for the self-paced reading data, and the differen ce in by-item squared error between the GPT2S and GPT2M models is signiﬁcant at p < 0.05 level for\nthe eye-tracking data.\nthe lingering inﬂuence of surprisal to better explain latency -\nbased measures.\nOn self-paced reading times, the /Delta1LL measures from\nindividual models in Figure 5A show a diﬀerent trend from\nthe LME regression results in Figure 3A. More speciﬁcally,\nsurprisal from the 5-gram model made the biggest contribution\nto regression model ﬁt, outperforming surprisal from other\nmodels in predicting self-paced reading times. Although the\nstrong predictive power of 5-gram surprisal is less expected, on e\nfundamental diﬀerence between the 5-gram model and other\nmodels is that it has the shortest context window (i.e., ≤ 4 words\ndue to Kneser-Ney smoothing) among all models. This would\nresult in by-word surprisal estimates that depend especially\nstrongly on the local context, which may provide orthogonal\ninformation to the CDR model that considers a sequence of\nsurprisal predictors to make its predictions. Among the neural\nLMs, the JLSTM model now outperforms the others, including\nthe largest GPT-2 model ( GPT2XL). Again, it may be that using\nCDR to explicitly condition on previous surprisal values is\nless beneﬁcial for the Transformer-based GPT-2 models, whic h\nmay already be incorporating lossless representations of the\nprevious context into their surprisal estimates through thei r\nself-attention mechanism.\nAmong the parsers, the biggest diﬀerence is observed for\nthe Str-cat variant, which shows predictive power close to\nthe Structural model when LME regression is utilized, but is\noutperformed by all other parsers when CDR is used instead.\nAlthough the exact reason behind this phenomenon is unclear,\nit may be that ablating syntactic category information lead s to\nsurprisal estimates that are more faithful to the current wor d,\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 11 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nmaking them more appropriate for LME regression. Parsers like\nthe Structural model and the JLC model still outperform neural\nLMs that were trained on much larger datasets, which further\nsuggests the importance of strong linguistic generalization s in\nproviding a humanlike account of processing diﬃculty.\nCDR models ﬁtted on eye-gaze durations ( Figure 5B) show\na very similar trend to the LME models ( Figure 3B) in terms of\nboth perplexity and /Delta1LL, although the JLSTM model now slightly\noutperforms the JLC model. This similarity between CDR and\nLME modeling suggests that the lingering inﬂuence of previous\nwords may not be as strong as it is on self-paced reading times.\nAnother possibility for this is that useful information abou t the\npreceding words is already being captured by the two baseline\npredictors, “saccade length” and “previous word was ﬁxated, ”\nwhich are included in both the CDR and LME models.\nThe CDR results from the diﬀerent variants of the GPT-2\nmodel in Figure 6 replicate the results from LME regression\nand show a positive correlation between test perplexity and\npredictive power on both self-paced reading times and eye-gaze\ndurations. This provides further support for the observation t hat\nthe trend in which neural LMs with lower perplexity predict\nlatency-based measures more accurately may be mostly drive n\nby the diﬀerence in their primary architecture or the amount\nof data used for training. The replication of these results ma y\nalso suggest that neural LMs with higher model capacity are abl e\nto make accurate predictions about the upcoming word while\nrelying less on humanlike generalizations given the same am ount\nof training data.\n8. EXPERIMENT 4: EFFECT OF\nPREDICTABILITY OVER WORD\nFREQUENCY\nIn all previous experiments, only predictors that capture\nlow-level cognitive processing were included in the baselin e\nregression models. Although this procedure allowed a clean\ncomparison of the predictive power of surprisal estimates from\ndiﬀerent models, this did not shed light on whether or not they\ncontribute a separable eﬀect from word frequency, which has\nlong been noted to inﬂuence processing diﬃculty (\nInhoﬀ and\nRayner, 1986 ). The goal of this experiment is to evaluate the\ncontribution of surprisal estimates on top of a stronger base line\nregression model that includes word frequency as a predictor .\nTo this end, the CDR analyses of the previous experiments\nwere replicated with a stronger baseline model, following the\nsame protocol of ﬁtting baseline and full regression models an d\ncalculating the diﬀerence in their log-likelihoods ( /Delta1LL).\n8.1. Procedures\nFor self-paced reading times, eye-gaze durations, and BOLD\nsignals, baseline CDR models were ﬁtted to the held-out set\nusing the baseline predictors described in Section 5.2.1, as well\nas unigram surprisal to incorporate word frequency. Unigram\nsurprisal was calculated using the KenLM toolkit (\nHeaﬁeld et al.,\n2013) with parameters trained on the English Gigaword Corpus\n(Parker et al., 2009 ) and was scaled prior to regression modeling.\nOther baseline model speciﬁcations were kept identical to tho se\nof the previous experiments.\nThe full models include one surprisal predictor on top of this\nbaseline model, which were calculated from the parsers and LMs\n(Section 5.2.2) as well as diﬀerent variants of the pretrained GPT-\n2 models (Section 6.1). Similarly, the speciﬁcations of the fu ll\nmodels were kept identical to those of the previous experiments.\nAfter all the regression models were ﬁtted, /Delta1LL for each model\nwas calculated by subtracting the log-likelihood of the bas eline\nmodel from that of a full regression model that contains its\nsurprisal estimates.\n8.2. Results\nFigures 7A,B show that for self-paced reading times and eye-\ngaze durations, the /Delta1LL measures for most models indicate a\nsubstantial contribution of model surprisal on top of unigra m\nsurprisal. These results are consistent with\nShain (2019) , who\nobserved that the eﬀect of predictability subsumes that of\nword frequency in the context of naturalistic reading. The\ncontribution of surprisal estimates are more subdued on fMRI\ndata (Figure 7C), especially for the 5-gram and RNNG models as\nwell as the ablated variants of the Structural model.\nOn self-paced reading times, the /Delta1LL measures from the\nmodels in Figure 7A generally show a similar trend to the\nCDR results in Figure 5A. One notable diﬀerence, however, is\nthat the /Delta1LL measures for the GPT2XL and Str-cat models are\nmore comparable with those of other models when unigram\nsurprisal is included in the baseline. This may be due to the\nfact that both the GPT2XL and Str-cat models incorporate\nsubword information into their surprisal estimates (throug h their\nsubword-level prediction and character-based word generat ion\nmodel, respectively) and therefore capture information that is\nmore orthogonal to word frequency. On both eye-tracking and\nfMRI data, the overall trend is very similar to the CDR results i n\nFigures 5B, 3C.\nThe CDR results from the diﬀerent variants of the GPT-\n2 model in Figure 8 closely replicate the CDR results without\nunigram surprisal on all three datasets ( Figures 4C, 6). This\nagain shows a positive correlation between test perplexity and\npredictive power on self-paced reading times and eye-gaze\ndurations. Additionally, this close replication across the t hree\ndatasets shows that diﬀerent model capacity does not result\nin surprisal estimates that are diﬀerentially sensitive to word\nfrequency for the GPT-2 models.\n9. DISCUSSION AND CONCLUSION\nThis article evaluates two kinds of NLP systems, namely\nincremental parsers and language models, as cognitive models\nof human sentence processing under the framework of\nexpectation-based surprisal theory. As an attempt to develop\na more cognitively plausible model of sentence processing,\nan incremental left-corner parser that explicitly incorporat es\ninformation about common linguistic abstractions is ﬁrst\npresented. The model is trained to make decisions about syntac tic\ncategories, predicate-argument structure, and morphologic al\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 12 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nFIGURE 7 |Perplexity measures from each model, and improvements in CD R model log-likelihood from including each surprisal estim ate on (A) Natural Stories\nself-paced reading data, (B) Dundee eye-tracking data, and (C) Natural Stories fMRI data. The difference in by-item square d error between the Structural and\nGPT2XL models is signiﬁcant at p < 0.05 level for the self-paced reading and fMRI data.\nFIGURE 8 |Perplexity measures from each GPT -2 model, and improvement s in CDR model log-likelihood from including each surprisal estimate on (A) Natural\nStories self-paced reading data, (B) Dundee eye-tracking data, and (C) Natural Stories fMRI data. The difference in by-item square d error between the GPT2S and\nGPT2L models is signiﬁcant at p < 0.05 level for the self-paced reading data, and the differen ce in by-item squared error between the GPT2S and GPT2M models is\nsigniﬁcant at p < 0.05 level for the eye-tracking and fMRI data.\nrules, which is expected to help it capture humanlike expectati ons\nfor the word that is being processed.\nThe ﬁrst experiment reveals that surprisal estimates from thi s\nstructural model make the biggest contribution to regressi on\nmodel ﬁt compared to those from other incremental parsers\nand LMs on self-paced reading times and eye-gaze durations.\nConsidering that this model was trained on much less data\nin comparison to the LMs, this suggests that the strong\nlinguistic generalizations made by the model help capture\nhumanlike processing costs. This highlights the value of\nincorporating linguistic abstractions into cognitive mode ls of\nsentence processing, which may not be explicit in LMs that\nare trained to predict the next word. Future work could\ninvestigate the contribution of discourse-level informati on in\nproviding an explanation of humanlike processing costs (e.g.,\ninformation about coreferential discourse entities;\nJaﬀe et al.,\n2020). Additionally, perplexity measures from the evaluated\nmodels on the Natural Stories and Dundee corpora mostly\nsupport the negative monotonic relationship between LM\nperplexity and predictive power noticed in recent studies\n(\nGoodkind and Bicknell, 2018; Hao et al., 2020; Wilcox et al.,\n2020), although some incremental parsers deviate from this\ntrend. The BOLD signals do not show a similar pattern to what\nwas observed on latency-based measures, which indicates th at\nthey may be capturing diﬀerent aspects of processing diﬃculty.\nThe second experiment compares the predictive power of\nsurprisal estimates from diﬀerent variants of GPT-2 models\n(\nRadford et al., 2019 ), which diﬀer only by model capacity\n(i.e., number of layers and parameters) while holding the prima ry\narchitecture (i.e., Transformers) and training data consta nt. The\nresults show a robust positive correlation between perplexity and\npredictive power, which directly contradicts the ﬁndings of r ecent\nwork. This indicates that the previously observed relations hip\nbetween perplexity and predictive power may be driven more\nby the diﬀerence in the models’ primary architecture or traini ng\ndata, rather than their capacity. Additionally, these result s may\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 13 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nsuggest that when the training data is held constant, high-\ncapacity LMs may be able to accurately predict the upcoming\nword while relying less on humanlike generalizations, unlik e their\nlower-capacity counterparts.\nThe third experiment is a replication of the previous two\nexperiments using continuous-time deconvolutional regres sion\n(CDR;\nShain and Schuler, 2021 ), which is able to bring temporal\ndiﬀusion under control by modeling the inﬂuence of a sequence\nof input predictors on the response. While there was no\nsigniﬁcant diﬀerence in the trend of predictive power among\nthe diﬀerent models for eye-gaze durations, the use of CDR\nmade a notable diﬀerence in the results for self-paced reading\ntimes. This diﬀerential eﬀect across datasets could be due to t he\nfact that the regression models for eye-gaze durations incl ude\nbaseline predictors about the previous word sequence (i.e.,\n“saccade length” and “previous word was ﬁxated”). Additiona lly,\nthe models that saw the biggest increase in /Delta1LL on self-paced\nreading times were LMs that are especially sensitive to the loc al\ncontext (i.e., n-gram models and LSTM models). Therefore, it can\nbe conjectured that each by-word surprisal estimate from the se\nmodels provides orthogonal information for the CDR model to\nmake accurate predictions with. The positive correlation bet ween\nperplexity and predictive power among the diﬀerent variants of\nthe GPT-2 model is still observed when CDR is used, providing\nfurther support for the robustness of this trend.\nThe ﬁnal experiment is a replication of CDR analysis with\na stronger baseline model, which included unigram surprisal\nas a predictor that reﬂects word frequency. For most models,\nthe surprisal estimates contributed substantially to regres sion\nmodel ﬁt on top of unigram surprisal, with their eﬀects being\nstronger on self-paced reading times and eye-gaze durations .\nOn self-paced reading times, the inclusion of unigram surpris al\nin the baseline resulted in more comparable /Delta1LL measures for\nthe GPT2XL and Str-cat models, which hints at their capability\nto capture subword information. The general trend in /Delta1LL on\neye-gaze durations and BOLD signals, as well as the positive\ncorrelation between perplexity and predictive power among the\ndiﬀerent variants of the GPT-2 model, was replicated.\nTaken together, the above experiments seem to provide\nconverging evidence that incremental parsers that embody st rong\ngeneralizations about linguistic structure are more appropriat e\nas computational-level models of human sentence processing.\nAlthough deep neural LMs have been shown to be successful\nat learning useful, domain-general language representatio ns by\nthe NLP community, they seem to require orders of magnitude\nmore training data and yet do not provide a better ﬁt to\nhuman reading behavior. In order for NLP to further inform\ncognitive modeling, future work should continue to focus on\nincorporating linguistic generalizations that are relevan t into\nconcrete models and evaluating their predictions on human\nsubject data.\nDATA AVAILABILITY STATEMENT\nPublicly available datasets were analyzed in this study. Thi s\ndata can be found here: https://github.com/languageMIT/\nnaturalstories (Natural Stories SPR), https://osf.io/eyp8q/\n(Natural Stories fMRI).\nAUTHOR CONTRIBUTIONS\nB-DO: conceptualization, formal analysis, methodology,\nsoftware, visualization, writing—original draft, and rev iew and\nediting. CC: conceptualization, formal analysis, methodolo gy,\nsoftware, writing—original draft, and review and editing.\nWS: conceptualization, formal analysis, funding acquisiti on,\nmethodology, project administration, resources, software ,\nsupervision, writing—original draft, and review and editin g.\nAll authors contributed to the article and approved the\nsubmitted version.\nFUNDING\nThis work was supported by the National Science Foundation\nGrant #1816891.\nREFERENCES\nAjdukiewicz, K. (1935). “Die syntaktische Konnexitat, ” in Polish Logic 1920-1939,\ned S. McCall (Oxford: Oxford University Press), 207–231.\nAurnhammer, C., and Frank, S. L. (2019). “Comparing gated and simple recu rrent\nneural network architectures as models of human sentence processing , ” in\nProceedings of the 41st Annual Meeting of the Cognitive Scienc e Society\n(Montreal, QC), 112–118. doi: 10.31234/osf.io/wec74\nBach, E. (1981). “Discontinuous constituents in generalized categorial grammars,\nin Proceedings of the Annual Meeting of the Northeast Linguist ic Society\n(Cambridge, MA), 1–12.\nBar-Hillel, Y. (1953). A quasi-arithmetical notation for syntact ic description.\nLanguage 29, 47–58. doi: 10.2307/410452\nBates, D., Mächler, M., Bolker, B., and Walker, S. (2015). Fitti ng linear mixed-\neﬀects models using lme4. J. Stat. Softw. 67, 1–48. doi: 10.18637/jss.v067.i01\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariw al, P., et al.\n(2020). “Language models are few-shot learners, ” in Advances in Neural\nInformation Processing Systems, Vol. 33, eds H. Larochelle, M. Ranzato, R.\nHadsell, M. F. Balcan, and H. Lin (Red Hook, NY: Curran Associates, In c.),\n1877–1901.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., and Ko ehn, P.\n(2014). “One billion word benchmark for measuring progress in statisti cal\nlanguage modeling, ” in Proceedings of Interspeech (Singapore), 2635–2639.\ndoi: 10.21437/Interspeech.2014-564\nCho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougare s, F.,\nSchwenk, H., et al. (2014). “Learning phrase representations using R NN\nencoder-decoder for statistical machine translation, ” in Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Proces sing\n(EMNLP), (Doha: Association for Computational Linguistics), 1724–1 734.\ndoi: 10.3115/v1/D14-1179\nDemberg, V., and Keller, F. (2008). Data from eye-tracking corpora as evidence\nfor theories of syntactic processing complexity. Cognition 109, 193–210.\ndoi: 10.1016/j.cognition.2008.07.008\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). B ERT: pre-training\nof deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805. doi: 10.18653/v1/N19-1423\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 14 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nDyer, C., Ballesteros, M., Ling, W., Matthews, A., and Smith, N. A . (2015).\n“Transition-based dependency parsing with stack long short-term memory, ” in\nProceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language\nProcessing (Beijing: Association for Computational Linguistics), 334– 343.\ndoi: 10.3115/v1/P15-1033\nDyer, C., Kuncoro, A., Ballesteros, M., and Smith, N. A. (2016). “R ecurrent\nneural network grammars, ” in Proceedings of the 2016 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies(San Diego, CA: Association for Computational\nLinguistics), 199–209. doi: 10.18653/v1/N16-1024\nElman, J. L. (1991). Distributed representations, simple recurrent netw orks,\nand grammatical structure. Mach. Learn. 7, 195–225. doi: 10.1007/BF001\n14844\nFutrell, R., Gibson, E., Tily, H. J., Blank, I., Vishnevetsky, A., P iantadosi, S.,\net al. (2021). The Natural Stories corpus: a reading-time corpus of Eng lish\ntexts containing rare syntactic constructions. Lang. Resour. Eval. 55, 63–77.\ndoi: 10.1007/s10579-020-09503-7\nGoodkind, A., and Bicknell, K. (2018). “Predictive power of word s urprisal for\nreading times is a linear function of language model quality, ” in Proceedings\nof the 8th Workshop on Cognitive Modeling and Computational Linguistics (Salt\nLake City, UT), 10–18. doi: 10.18653/v1/W18-0102\nGulordava, K., Bojanowski, P., Grave, E., Linzen, T., and Baroni , M. (2018).\n“Colorless green recurrent networks dream hierarchically, ” in Proceedings of\nthe 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies(New Orleans, LA),\n1195–1205. doi: 10.18653/v1/N18-1108\nHale, J. (2001). “A probabilistic Earley parser as a psycholinguistic model, ”\nin Proceedings of the Second Meeting of the North American Chap ter\nof the Association for Computational Linguistics on Language Technologies\n(Pittsburgh, PA), 1–8. doi: 10.3115/1073336.1073357\nHale, J., Dyer, C., Kuncoro, A., and Brennan, J. (2018). “Findin g syntax in human\nencephalography with beam search, ” in Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics(Melbourne, NSW: Association\nfor Computational Linguistics), 2727–2736. doi: 10.18653/v 1/P18-1254\nHao, Y., Mendelsohn, S., Sterneck, R., Martinez, R., and Frank, R . (2020).\n“Probabilistic predictions of people perusing: evaluating metrics of lan guage\nmodel performance for psycholinguistic modeling, ” in Proceedings of the 10th\nWorkshop on Cognitive Modeling and Computational Linguistics (Punta Cana),\n75–86. doi: 10.18653/v1/2020.cmcl-1.10\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P. (2013). “S calable modiﬁed\nKneser-Ney language model estimation, ” in Proceedings of the 51st Annual\nMeeting of the Association for Computational Linguistics(Soﬁa), 690–696.\nHochreiter, S., and Schmidhuber, J. (1997). Long short-term memory. Neural\nComput. 9, 1735–1780. doi: 10.1162/neco.1997.9.8.1735\nInhoﬀ, A. W., and Rayner, K. (1986). Parafoveal word processing d uring eye\nﬁxations in reading: eﬀects of word frequency. Percept. Psychophys. 40,\n431–439. doi: 10.3758/BF03208203\nJaﬀe, E., Shain, C., and Schuler, W. (2020). “Coreference informa tion guides\nhuman expectations during natural reading, ” in Proceedings of the 28th\nInternational Conference on Computational Linguistics(Barcelona), 4587–4599.\ndoi: 10.18653/v1/2020.coling-main.404\nJin, L., and Schuler, W. (2020). “Memory-bounded neural increment al parsing\nfor psycholinguistic prediction, ” in Proceedings of the 16th International\nConference on Parsing Technologies and the IWPT 2020 Shared Task\non Parsing into Enhanced Universal Dependencies (Seattle, WA), 48–61.\ndoi: 10.18653/v1/2020.iwpt-1.6\nJohnson-Laird, P. N. (1983). Mental Models: Towards a Cognitive Science of\nLanguage, Inference, and Consciousness. Cambridge, MA: Harvard University\nPress.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and W u, Y. (2016). Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410.\nKennedy, A., Hill, R., and Pynte, J. (2003). “The Dundee Corpus, ” i n Proceedings of\nthe 12th European Conference on Eye Movement(Dundee).\nLevy, R. (2008). Expectation-based syntactic comprehension. Cognition 106,\n1126–1177. doi: 10.1016/j.cognition.2007.05.006\nLindquist, M. A., Loh, J. M., Atlas, L. Y., and Wager, T. D. (200 9). Modeling the\nhemodynamic response function in fMRI: Eﬃciency, bias and mis-mod eling.\nNeuroimage 45(1 Suppl. 1), S187–S198. doi: 10.1016/j.neuroimage.2008.\n10.065\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (2 019).\nRoBERTa: a robustly optimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692.\nMarcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993) . Building a\nlarge annotated corpus of English: the Penn Treebank. Comput. Linguist. 19,\n313–330. doi: 10.21236/ADA273556\nMarr, D. (1982). Vision: A Computational Investigation into the Human\nRepresentation and Processing of Visual Information. New York, NY: W.H.\nFreeman and Company.\nMerkx, D., and Frank, S. L. (2021). “Human sentence processing: recu rrence\nor attention?” in Proceedings of the Workshop on Cognitive Modeling\nand Computational Linguistics (Mexico: Association for Computational\nLinguistics), 12–22. doi: 10.18653/v1/2021.cmcl-1.2\nMiller, G. A., and Isard, S. (1963). Some perceptual consequences o f linguistic rules.\nJ. Verb. Learn. Verb. Behav. 2, 217–228. doi: 10.1016/S0022-5371(63)80087-0\nNguyen, L., van Schijndel, M., and Schuler, W. (2012). “Accurat e unbounded\ndependency recovery using generalized categorial grammars, ” in Proceedings\nof the 24th International Conference on Computational Linguistics (Mumbai),\n2125–2140.\nOh, B.-D., Clark, C., and Schuler, W. (2021). “Surprisal estimators for human\nreading times need character models, ” in Proceedings of the Joint Conference\nof the 59th Annual Meeting of the Association for ComputationalLinguistics\nand the 11th International Joint Conference on Natural Language Processing\n(Bangkok), 3746–3757. doi: 10.18653/v1/2021.acl-long.290\nParker, R., Graﬀ, D., Kong, J., Chen, K., and Maeda, K. (2009). E nglish Gigaword\nLDC2009T13.\nPetrov, S., Barrett, L., Thibaux, R., and Klein, D. (2006). “Learn ing accurate,\ncompact, and interpretable tree annotation, ” in Proceedings of the 21st\nInternational Conference on Computational Linguistics and 44th Annual\nMeeting of the Association for Computational Linguistics (Sydney, NSW),\n433–440. doi: 10.3115/1220175.1220230\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskev er, I. (2019).\nLanguage Models Are Unsupervised Multitask Learners. OpenAI Technical\nReport. Available online at: https://d4mucfpksywv.cloudfront.net/ better-\nlanguage-models/language_models_are_unsupervised_multitask_learners.pdf\nRayner, K., Carlson, M., and Frazier, L. (1983). The interaction of syntax\nand semantics during sentence processing: eye movements in the ana lysis\nof semantically biased sentences. J. Verb. Learn. Verb. Behav. 22, 358–374.\ndoi: 10.1016/S0022-5371(83)90236-0\nRoark, B., Bachrach, A., Cardenas, C., and Pallier, C. (2009). “De riving lexical\nand syntactic expectation-based measures for psycholinguistic modeling via\nincremental top-down parsing, ” in Proceedings of the 2009 Conference on\nEmpirical Methods in Natural Language Processing (Singapore), 324–333.\ndoi: 10.3115/1699510.1699553\nSchäfer, R. (2015). “Processing and querying large web corpora wit h the COW14\narchitecture, ” inProceedings of Challenges in the Management of Large Corpora\n3 (CMLC-3)(Lancaster. UCREL, IDS).\nSchuler, W., AbdelRahman, S., Miller, T., and Schwartz, L. (2010). Broad-coverage\nincremental parsing using human-like memory constraints. Comput. Linguist.\n36, 1–30. doi: 10.1162/coli.2010.36.1.36100\nShain, C. (2019). “A large-scale study of the eﬀects of word frequ ency\nand predictability in naturalistic reading, ” in Proceedings of the Annual\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Minneapolis, MN), 4086–4094.\ndoi: 10.18653/v1/N19-1413\nShain, C., Blank, I. A., van Schijndel, M., Schuler, W., and Fedo renko,\nE. (2020). fMRI reveals language-speciﬁc predictive coding during\nnaturalistic sentence comprehension. Neuropsychologia 138, 107307.\ndoi: 10.1016/j.neuropsychologia.2019.107307\nShain, C., and Schuler, W. (2018). “Deconvolutional time series regression: a\ntechnique for modeling temporally diﬀuse eﬀects, ” in Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing (Brussels).\ndoi: 10.18653/v1/D18-1288\nShain, C., and Schuler, W. (2021). Continuous-time deconvolut ional\nregression for psycholinguistic modeling. Cognition 215, 104735.\ndoi: 10.1016/j.cognition.2021.104735\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 15 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nShannon, C. E. (1948). A mathematical theory of communication.\nBell Syst. Tech. J . 27, 379–423. doi: 10.1002/j.1538-7305.1948.tb\n01338.x\nSmith, N. J., and Levy, R. (2013). The eﬀect of word predictability on reading\ntime is logarithmic. Cognition 128, 302–319. doi: 10.1016/j.cognition.2013.\n02.013\nvan Schijndel, M., Exley, A., and Schuler, W. (2013). A model of lan guage\nprocessing as hierarchic sequential prediction. Top. Cogn. Sci. 5, 522–540.\ndoi: 10.1111/tops.12034\nvan Schijndel, M., and Schuler, W. (2015). “Hierarchic syntax impro ves\nreading time prediction, ” in Proceedings of NAACL-HLT 2015 (Denver,\nCO: Association for Computational Linguistics). doi: 10.311 5/v1/\nN15-1183\nVasishth, S. (2006). “On the proper treatment of spillover in real-\ntime reading studies: consequences for psycholinguistic theori es, ” in\nProceedings of the International Conference on Linguistic E vidence,\n96–100.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” Advances in Neural Information Processing\nSystems, eds U. von Luxburg, I. Guyon, S. Bengio, H. Wallach, and R. Fergus\n(Red Hook, NY: Curran Associates).\nWilcox, E. G., Gauthier, J., Hu, J., Qian, P., and Levy, R. P. (20 20). “On\nthe predictive power of neural language models for human real-time\ncomprehension behavior, ” in Proceedings of the 42nd Annual Meeting of the\nCognitive Science Society(Toronto, ON), 1707–1713.\nAuthor Disclaimer: All views expressed are those of the authors and do not\nnecessarily reﬂect the views of the National Science Foundatio n.\nConﬂict of Interest: The authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their aﬃliated organizat ions, or those of\nthe publisher, the editors and the reviewers. Any product that may b e evaluated in\nthis article, or claim that may be made by its manufacturer, is not gua ranteed or\nendorsed by the publisher.\nCopyright © 2022 Oh, Clark and Schuler. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License(CC BY). The use,\ndistribution or reproduction in other forums is permitted, provided the original\nauthor(s) and the copyright owner(s) are credited and that the original publication\nin this journal is cited, in accordance with accepted academic practice. No use,\ndistribution or reproduction is permitted which does not comply with these terms.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 16 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nAPPENDIX\n1. PREDICATE CONTEXT COMPOSITION\nMATRICES\nPredicate context vectors ha and hb for apex nodes a and\nbase nodes b consist of 1 + V + E concatenated vectors of\ndimension K; one for the referential state signiﬁed by the sign\nitself, one for each of its V syntactic arguments, and one for\neach of its E non-local arguments (such as gap ﬁllers or relative\npronoun antecedents). Composition operators ogt may consist\nof zero or more unary operations (like extraction or argument\nreordering, which do not involve more than one child) followe d\nby a binary operation. Composition matrices Ao1,o2,... and Bo1,o2,...\nwith sequences of unary and binary operators o1, o2, . . . can be\nrecursively decomposed:\nAo1,o2,... = Ao2,... Uo1\nBo1,o2,... = Bo2,...\n[ Uo1 0H×H\n0H×H IH×H\n]\nwhere H = K + KV + KE. Each matrix is tiled together\nfrom identity matrices over predicate contexts that specify w hich\nsyntactic or non-local arguments are associated between ch ildren\n(rows u) and parents (columns v).\nUnary operators model extraction and argument swapping\nbetween a parent and a single child 13:\nUEa-n =\nV+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\n\n\n\n\nIK×K if u = n and v = V + 1\nIK×K if u ̸=n and u ≤ V and v = u\nIK×K if u ̸=n and u > V and v = u + 1\n0K×K otherwise\nLeft-child operator matrices model left arguments (Aa), lef t\nmodiﬁers (Mb), gap ﬁller attachments (G), left and right\nconjuncts (Ca, Cb), and other compositions:\nAAa-n-e =\nV+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u = 0 and v = n\nIK×K if u > V and v = u and e[v−V] = 0\n0K×K otherwise\nAMa-e =\nV+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n13M ⊗ N is a Kronecker product which tiles N with weights of elements of M:\nM ⊗ N =\n\n\n\n\nM[1,1]N M[1,2)N . . .\nM[2,1]N M[2,2)N . . .\n.\n.\n.\n.\n.\n.\n...\n\n\n\n\n.\n\n\n\n\n\nIK×K if u = 1 and v = 0\nIK×K if u > V and v = u and e[v−V] = 0\n0K×K otherwise\nAG = 0H×H\nACa, ACb = IH×H\nAo-e =\n(V+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u ≤ V and v = u\nIK×K if u > V and v = u and e[v−V] = 0\n0K×K otherwise\n\n\n\nfor all other o.\nwhere n ∈ { 1..V} is an argument number and e ∈ { 0, 1}E is a bit\nsequence encoding whether each non-local argument propagate s\nto the left (0) or right (1) child.\nRight-child operator matrices model right arguments (Ab),\nright modiﬁers (Mb), right relative clause attachments (Rb;\nintroducing a non-local argument for a relative pronoun), lef t\nand right conjuncts (Ca, Cb), and other compositions:\nBAb-n-e =\n(V+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u = 0 and v = n\nIK×K if u > V and v = u and e[v−V] = 1\n0K×K otherwise\n\n\n\n[IH×H, AAb-n⊤]\nBMb-e =\n(V+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u = 1 and v = 0\nIK×K if u > V and v = u and e[v−V] = 1\n0K×K otherwise\n\n\n\n[IH×H, AMb⊤]\nBRb =\n[\n0H×H,\n(V+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u = V + 1 and v = 1\nIK×K if u > V + 1 and v = u − 1\n0K×K otherwise\n\n\n\n\n\n\nBCa, BCb = [IH×H, IH×H]\nBo-e =\n(V+E∑\nu=0\nV+E∑\nv=0\nδu δ⊤\nv ⊗\n\n\n\n\n\nIK×K if u ≤ V and v = u\nIK×K if u > V and v = u and e[v−V] = 1\n0K×K otherwise\n\n\n\n[IH×H, Ao-e⊤] for other o.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 17 March 2022 | Volume 5 | Article 777963\nOh et al. Comparison of Parser and LM Surprisal\nwhere n ∈ { 1..V} is an argument number and e ∈ { 0, 1}E\nis a bit sequence encoding whether each non-local argument\npropagates to the left (0) or right (1) child. Right-child matri ces\nare of dimension H × 2H in order to accommodate associations\nbetween syntactic and non-local arguments in left children as well\nas parents.\nFrontiers in Artiﬁcial Intelligence | www.frontiersin.org 18 March 2022 | Volume 5 | Article 777963",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8115813732147217
    },
    {
      "name": "Natural language processing",
      "score": 0.6051679849624634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6014090776443481
    },
    {
      "name": "Parsing",
      "score": 0.5905984044075012
    },
    {
      "name": "Sentence processing",
      "score": 0.5426296591758728
    },
    {
      "name": "Sentence",
      "score": 0.5299808979034424
    },
    {
      "name": "Language model",
      "score": 0.48990505933761597
    },
    {
      "name": "Textual entailment",
      "score": 0.4179227948188782
    },
    {
      "name": "Logical consequence",
      "score": 0.18744885921478271
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    }
  ]
}