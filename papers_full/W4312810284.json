{
  "title": "DeepAC – conditional transformer-based chemical language model for the prediction of activity cliffs formed by bioactive compounds",
  "url": "https://openalex.org/W4312810284",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2775508793",
      "name": "Hengwei Chen",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A2064165665",
      "name": "Martin Vogt",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A192274276",
      "name": "Jürgen Bajorath",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A2775508793",
      "name": "Hengwei Chen",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A2064165665",
      "name": "Martin Vogt",
      "affiliations": [
        "University of Bonn"
      ]
    },
    {
      "id": "https://openalex.org/A192274276",
      "name": "Jürgen Bajorath",
      "affiliations": [
        "University of Bonn"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2039609876",
    "https://openalex.org/W2410846838",
    "https://openalex.org/W2970175280",
    "https://openalex.org/W2160114756",
    "https://openalex.org/W2005518172",
    "https://openalex.org/W2925328360",
    "https://openalex.org/W2178493412",
    "https://openalex.org/W2072582769",
    "https://openalex.org/W2047603685",
    "https://openalex.org/W2517880718",
    "https://openalex.org/W3193397483",
    "https://openalex.org/W3080117545",
    "https://openalex.org/W2085526045",
    "https://openalex.org/W3139001571",
    "https://openalex.org/W4229060137",
    "https://openalex.org/W4220790374",
    "https://openalex.org/W2558999090",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2027376226",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W4230674625",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W1437335841",
    "https://openalex.org/W6610522820",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W4297683907",
    "https://openalex.org/W4248625073",
    "https://openalex.org/W4302408070",
    "https://openalex.org/W2914874661",
    "https://openalex.org/W4283808340"
  ],
  "abstract": "Activity cliffs (ACs) are formed by pairs of structurally similar or analogous active small molecules with large differences in potency.",
  "full_text": "DeepAC – conditional transformer-based chemical\nlanguage model for the prediction of activity cliﬀs\nformed by bioactive compounds\nHengwei Chen, Martin Vogt and J¨urgen Bajorath *\nActivity cliﬀs (ACs) are formed by pairs of structurally similar or analogous active small molecules with large\ndiﬀerences in potency. In medicinal chemistry, ACs are of high interest because they often reveal structure–\nactivity relationship (SAR) determinants for compound optimization. In molecular machine learning, ACs\nprovide test cases for predictive modeling of discontinuous (non-linear) SARs at the level of compound\npairs. Recently, deep neural networks have been used to predict ACs from molecular images or graphs\nvia representation learning. Herein, we report the development and evaluation of chemical language\nmodels for AC prediction. It is shown that chemical language models learn structural relationships and\nassociated potency di ﬀerences to reproduce ACs. A conditional transformer termed DeepAC is\nintroduced that accurately predicts ACs on the basis of small amounts of training data compared to\nother machine learning methods. DeepAC bridges between predictive modeling and compound design\nand should thus be of interest for practical applications.\n1 Introduction\nIn medicinal chemistry, compound optimization relies on the\nexploration of structure–activity relationships (SARs). There-\nfore, series of structural analogues are generated to probe\nsubstitution sites in speci cally active compounds with\ndiﬀerent functional groups and improve potency and other lead\noptimization-relevant molecular properties. For lead optimiza-\ntion, the activity cliﬀ (AC) concept plays an important role. ACs\nare dened as pairs or groups of structurally similar compounds\nor structural analogues that are active against a given target and\nhave large diﬀerences in potency.\n1–3 As such, ACs represent\nstrongly discontinuous SARs because small chemical modi-\ncations lead to large biological eﬀects. In medicinal chemistry,\nSAR discontinuity captured by ACs helps to identify substitu-\nents that are involved in critically important ligand –target\ninteractions. In compound activity prediction, the presence of\nSAR discontinuity prevents the derivation of quantitative SAR\n(QSAR) models relying on continuous SAR progression and\nrequires non-linear machine learning models.\n1,2\nFor a non-ambiguous and systematic assessment of ACs,\nsimilarity and potency di ﬀerence criteria must be clearly\ndened.2,3 Originally, molecular ngerprints (that is, bit string\nrepresentations of chemical structure) have been used as\nmolecular representations to calculate the Tanimoto coe ﬃ-\ncient,4 a whole-molecule similarity metric, for identifying\nsimilar compounds forming ACs.2 Alternatively, substructure-\nbased similarity measures have been adapted for de ning\nACs, which have become increasingly popular in medicinal\nchemistry, because they are oen chemically more intuitive\nthan calculated whole-molecule similarity.\n3 For example,\na widely used substructure-based similarity criterion for AC\nanalysis is the formation of a matched molecular pair (MMP),\nwhich is dened as a pair of compounds that are only distin-\nguished by a chemical modication at a single site.\n5 Thus,\nMMPs can be used to represent pairs of structural analogues,\nwhich explains their popularity in medicinal chemistry. More-\nover, MMPs can also be eﬃciently identied algorithmically.\n5\nAlthough statistically signicant potency diﬀerences for ACs\ncan be determined for individual compound activity classes,6\nfor the systematic assessment of ACs and computational\nmodeling, a potency diﬀerence threshold of at least two orders\nof magnitude (100-fold) has mostly been applied.\n2,3\nWhile medicinal chemistry campaigns encounter ACs on\na case-by-case basis, systematic compound database analysis\nhas identied ACs across diﬀerent compound activity classes,\nproviding a wealth of SAR information.\n2,7 Here, computational\nand medicinal chemistry meet. With rapidly increasing\nnumbers of publicly available bioactive compounds, AC pop-\nulations have also grown over time.\n3 However, the rate at which\nACs are formed across diﬀerent activity classes has essentially\nremained constant. Only∼5% of pairs of structural analogues\nsharing the same activity form ACs across diﬀerent activity\nclasses.3,7 Thus, as expected for compounds representing the\nDepartment of Life Science Informatics and Data Science, B-IT, LIMES Program Unit\nChemical Biology and Medicinal Chemistry, Rheinische\nFriedrich-Wilhelms-Universit¨at, Friedrich-Hirzebruch-Allee 5/6, D-53115 Bonn,\nGermany. E-mail: bajorath@bit.uni-bonn.de; Fax: +49-228-7369-100; Tel: +49-228-\n7369-100\nCite this:Digital Discovery, 2022,1,\n898\nReceived 19th July 2022\nAccepted 28th October 2022\nDOI: 10.1039/d2dd00077f\nrsc.li/digitaldiscovery\n898 | Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\npinnacle of SAR discontinuity, structural analogues rarely form\nACs.\nSystematic identication of ACs across activity classes has\nalso provided the basis for computational predictions of ACs.\nFor machine learning, AC predictions generally present a chal-\nlenge, for three reasons. First, as discussed, the underlying SARs\nthat need to be accounted for are highly discontinuous; second,\ndata sets of ACs and non-ACs are unbalanced; third, predictions\nneed to be made at the level of compound pairs, rather than\nindividual compounds, which is usually the case in compound\nclassication or molecular property prediction. Initial attempts\nto predict ACs were reported a decade ago.\n8,9 ACs were rst\naccurately predicted using support vector machine (SVM)\nmodeling on the basis of special kernel functions enabling\ncompound pair predictions.\n9 These ndings have also catalyzed\nfurther AC predictions using SVR variants 10–12 and other\nmethods,13–18 as discussed below. Recently, various deep neural\nnetwork architectures have been used to predict ACs from\nimages14,15 and molecular graphs using representation\nlearning16 or derive regression models for potency prediction of\nAC compounds.17,18\nIn this work, we further extend this methodological spec-\ntrum by introducing chemical language models for combined\nAC prediction and generative compound design. Compared to\nearlier studies predicting ACs using classication models, the\napproach presented herein was designed to extend AC predic-\ntions with the capacity to produce new AC compounds, thus\nintegrating predictive and generative modeling in the context of\nAC analysis and AC-based compound design.\n2 Methods\n2.1 Compounds and activity data\nBioactive compounds with high-condence activity data were\nassembled from ChEMBL (version 26).19 The following selection\ncriteria were applied. Only compounds involved in direct\ninteractions with human targets at the highest assay condence\nlevel (assay condence score 9) were selected and only numer-\nically specied equilibrium constants (K\ni values) were accepted\nas potency measurements. Equilibrium constants were recor-\nded as (negative logarithmic) pKi values. Multiple measure-\nments for the same compound were averaged, provided all\nvalues fell within the same order of magnitude; if not, the\ncompound was disregarded. Hence, in a given class, all\ncompounds were active against a specic target. Compounds\nwere represented using molecular-input line-entry system\n(SMILES) strings.\n20\n2.2 Matched molecular pairs\nFrom activity classes, all possible MMPs were generated by\nsystematically fragmenting individual exocyclic single bonds in\ncompounds and sampling core structures and substituents in\nindex tables.5 For substituents, size restrictions were applied to\nlimit MMP formation to structural analogues typical for\nmedicinal chemistry. Accordingly, a substituent was permitted\nto contain at most 13 non-hydrogen atoms and the core\nstructure was required to be at least twice as large as a substit-\nuent. In addition, for MMP compounds, the maximum diﬀer-\nence in non-hydrogen atoms between the substituents was set\nto eight, yielding transformation size-restricted MMPs.\n21 The\nsystematic search identi ed 357 343 transformation size-\nrestricted MMPs originating from a total of 600 activity classes.\n2.3 Data set for model derivation\nFrom the MMPs, a large general data set for model training was\nassembled by combining 338 748 MMPs from 596 activity\nclasses. The majority of MMPs captured only minor diﬀerences\nin potency. Importantly, model pre-training, as specied below,\ndid not require the inclusion of explicit target information\nbecause during this phase, the model must learn MMP-\nassociated potency di ﬀerences caused by given chemical\ntransformations. Each MMP represented a true SAR, which was\nof critical relevance in this context, while target information was\nnot required for pre-training. By contrast, subsequent ne-\ntuning then focused the model on target-speci c activity\nclasses for AC prediction and compound design.\nMMPs comprising the general data set were represented as\ntriples:\n(Compound\nA, CompoundB, PotencyB − PotencyA).\nCompoundA represented the source compound that was\nconcatenated with the potency diﬀerence (PotencyB − PotencyA)\nwhile CompoundB represented the target compound. Each\nMMP yielded two triples, in which each MMP compound was\nused once as the source and target compound, respectively. The\nsource and target compounds were then used as the input and\nassociated output for model training, respectively. Further-\nmore, for MMP-triples, data ambiguities could arise if an MMP\nwas associated with multiple potency values for diﬀerent targets\nor if a given source compound and potency diﬀerence was\nassociated with multiple target compounds from di ﬀerent\nactivity classes. Such MMPs were eliminated. Finally, for the\ngeneral data set, a total of 338 748 qualifying MMP-triples were\nobtained.\nFor modeling, MMP-triples were randomly divided into\ntraining (80%), validation (10%), and test (10%) sets. Source\nand target compounds from MMP-triples displayed nearly\nindistinguishable potency value distributions.\nFor the initial evaluation of chemical language models, three\ndiﬀerent test (sub)set versions were designed:\n(i) Test-general: complete test set of 33 875 MMP-triples\nexcluded from model training.\n(ii) Test-core: subset of 2576 test set MMP-triples with core\nstructures not present in training compounds.\n(iii) Test-sub: subset of 14 193 MMP-triples with substituents\n(R-groups) not contained in training compounds.\nFor the generation of the training subsets, compounds were\ndecomposed into core structures and substituents via MMP\nfragmentation.\n5\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 9 8–909 | 899\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n2.4 Activity cli ﬀs\nFor ACs, the MMP-Cliﬀ denition was applied.21 Accordingly,\na transformation size-restricted MMP from a given activity class\nrepresented an AC if the two MMP-forming compounds had\na potency diﬀerence of at least two orders of magnitude (100-\nfold; i.e., DpK\ni $ 2.0). MMP-Cliﬀs were distinguished from\n“MMP-nonCliﬀs”, that is, pairs of structural analogues not\nrepresenting an AC. To avoid potency boundary eﬀects in AC\nprediction, compounds forming an MMP-nonCli ﬀ were\nrestricted to a maximal potency diﬀerence of one order of\nmagnitude (10-fold;DpKi # 1). Hence, MMPs capturing potency\ndiﬀerences between 10- and 100-fold were not considered for AC\nprediction.\nMMP-Cliﬀs and MMP-nonCliﬀs were extracted from four\nlarge activity classes including inhibitors of thrombin (ChEMBL\nID 204) and tyrosine kinase Abl (1862) as well as antagonists of\nthe Mu opioid receptor (233) and corticotropin releasing factor\nreceptor 1 (1800). For MMP-Cliﬀs and MMP-nonCliﬀs, triples\nwere ordered such that Compound\nA had lower potency than (or\nequal potency to) Compound B. These activity classes were\nexcluded from the general data set and their MMP-Cliﬀs and\nMMP-nonCliﬀs thus formed an external/independent test set\nfor AC prediction (Table 1).\n2.5 Deep chemical language models\nChemical language models for AC prediction were designed to\nlearn the following mapping from MMP-triples:\n(Source compound, Potency difference)/(Target compound).\nThen, given a new (Source compound, Potency diﬀerence)\ntest instance, trained models were supposed to generate a set of\ntarget candidate compounds meeting the potency diﬀerence\ncondition.\nSequence-to-sequence (Seq2Seq) models represent an\nencoder–decoder architecture to convert an input sequence\n(such as a character string) into an output sequence.\n22 These\nmodels can be adapted for a variety of applications, especially\nfor neural machine translation.\n22 The encoder reads an input\nsequence and compresses it into a context vector as its last\nhidden state. The context vector serves as the input for the\ndecoder network component that interprets the vector to\npredict an output sequence. Because long input sequences\noen present challenges for generating context vectors,\n23 an\nattention mechanism24 was introduced that utilizes hidden\nstates from each time step of the encoder. As a further advance,\na transformer neural network architecture was introduced that\nonly relies on the attention mechanism. 25 The transformer\narchitecture comprises multiple encoder –decoder modules\n(Fig. 1). An encoder module consists of a stack of encoding\nlayers composed of two sub-layers including a multi-head self-\nattention sub-layer and a fully connected feed-forward\nnetwork (FFN) sub-layer. Multi-head attention has multiple,\nsingle attention functions acting in parallel such that diﬀerent\npositions in the input sequence can be processed simulta-\nneously. The attention mechanism is based upon the following\nfunction:\nAttentionðQ; K; VÞ¼ softmax\n/C18\nQKT\nﬃﬃﬃﬃﬃdk\np\n/C19\nV (1)\nThe input for the attention layer is received in the form of\nthree parameters including query (Q), keys (K), and values (V). In\naddition, a scaling factor dk (equal to the size of weight\nmatrices) prevents calculations of excessive dot products. 25\nMore details concerning the attention function are provided in\nthe original literature of the transformer model.\n25 The FFN sub-\nlayer employs recti ed linear unit (ReLU) activation. 26 The\nmulti-head self-attention and FFN sub-layers are then linkedvia\nlayer normalization27 and a residual skip-connection.28 Each\ndecoder layer contains three sub-layers including an FFN sub-\nlayer and two multi-head attention sub-layers. Therst atten-\ntion sub-layer was controlled by a mask function.\nIn this work, all source and target molecules were repre-\nsented as canonical SMILES strings generated using RDKit29\nand further tokenized to construct a chemical vocabulary con-\ntaining all the possible chemical tokens. The start and end of\na sequence were represented by two special“start” and “end”\ntokens, respectively. For AC prediction, models must be guided\ntowards the generation of compounds meeting potency diﬀer-\nence constraints. Therefore, potency diﬀerences captured by\nMMPs were tokenized by binning.\n23 The potency diﬀerence,\nranging from −8.02 to 9.53, was partitioned into 1755 bins of\nwidth 0.01 that were also added to the chemical vocabulary.\nEach bin was encoded by a single token and each potency\ndiﬀerence was assigned to the token of the corresponding bin\n(Fig. 1), e.g., a potency diﬀerence of 2.134 was encoded as\n‘pK\ni_change_(2.13, 2.14) ’. Accordingly, the tokenization\npreserved the quantitative relationship between bins. The\nSMILES representation of a source compound combined with\nits potency di ﬀerence token then represented the input\nsequence for the transformer encoder and was converted into\nTable 1 Compound activity classes for activity cliﬀ prediction\nTarget name ChEMBL ID Total MMPs MMP- Cli ﬀs MMP-nonCli ﬀs\nThrombin 204 4249 438 2976\nMu opioid receptor 233 5875 329 4319\nTyrosine kinase Abl 1862 5403 564 3093\nCorticotropin releasing factor receptor 1 1800 3068 317 1889\n900 | Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\na latent representation. Based on this representation, the\ntransformer decoder iteratively generated output SMILES\nsequences until the end token was obtained. During training,\nthe transformer model minimized the cross-entropy loss\nbetween the ground-truth target and output sequence.\n2.6 Model derivation and selection\nSeq2Seq and transformer models were implemented using\nPytorch.\n30 The Adam optimizer with learning rate 0.0001 and\na batch size of 64 was applied. For transformer models, default\nhyperparameter settings were used,25 except for the input and\noutput encoding dimension, which was reduced from 512 to\n256, and label smoothing, which was set to 0. On the basis of the\ngeneral training set, models were derived over 200 epochs. A\ncheckpoint was saved at each epoch and for the validation set,\nminimal loss was determined for selecting thenal model. For\nthe test set, generated candidate compounds were canon-\nicalized using RDkit and compared to the target compounds.\n2.7 Reference methods for activity prediction\nFor AC prediction, the chemical language models were\ncompared to models of diﬀerent machine learning methods\nincluding support vector machine (SVM),\n31 random forest\n(RF),32 and extreme gradient boosting (XGboost)33 that were\ngenerated using scikit-learn.34 As a molecular representation,\nthe extended connectivityngerprint with bond diameter of 4\nFig. 1 Architecture of a transformer encoder–decoder with attention mechanism.\nTable 2 Hyperparameter settings for optimization of diﬀerent models\nModel Hyperparameters Value space for optimization\nSVM Kernel function ‘Linear’, ‘sigmoid’, ‘poly’, ‘rbf’, ‘tanimoto’\nC 1, 10, 100, 1000, 10 000\nGamma 10 −6,1 0−5,1 0−4,1 0−3,1 0−2,1 0−1\nRF Max_depth 3, 4, 5, 6, 7, 8, 9, 10\nMax_features 32, 64, 128, 256, 512, 1024\nn_estimators 1, 2, 4, 8, 16, 32, 64, 100, 200\nXGboost Max_depth 3, 4, 5, 6, 7, 8, 9, 10\nn_estimators 1, 2, 4, 8, 16, 32, 64, 100, 200\nLearning_rate 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3\nSubsample 0.5, 0.6, 0.7, 0.8, 0.9, 1\nMin_child_weight 0, 1, 2, 3, 4, 5\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 9 8–909 | 901\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n(ECFP4) was used.35 For the common core of an MMP and the\ntwo substituents dening the chemical transformation,nger-\nprint vectors were generated. For use of the MMP kernel,9 these\nvectors were concatenated to yield a single vector9 as input for\nderiving SVM, RF, and XGboost models. Hyperparameters of all\nmodels were optimized using the Hyperopt\n36 package withve-\nfold cross-validation, as reported in Table 2.\n2.8 Evaluation metrics\nA reproducibility criterion was introduced to measure the ability\nof a chemical language model to reproduce a target compound\nfor a given source compound and potency diﬀerence. An MMP-\ntriple met this criterion if it was reproduced when generating\na pre-dened number of target candidate compounds. In our\ncalculations, up to 50 distinct molecules were generated for\neach source compound to determine the reproducibility of\na target compound, dened as:\nReproducibility ¼\nMMPrepro\nMMPtest\n(2)\nMMPtest and MMPrepro denote the number of MMP-triples\nthat were tested and reproduced by a model, respectively.\nNotably, this denition of reproducibility directly corresponds\nto the recall of labeled instances for classication models.\nAC predictions were also evaluated by determining the true\npositive rate (TPR), true negative rate (TNR), and balanced\naccuracy (BA),\n37 dened as:\nTPR ¼ TP\nTP þ FN (3)\nTNR ¼ TN\nTP þ FN (4)\nBA ¼ TPR þ TNR\n2 (5)\nTP, TN, FP, and FN denote true positives, true negatives, false\npositives, and false negatives respectively.\n3 Results and discussion\n3.1 Study concept\nThe basic idea underlying the use of chemical language models\nfor AC prediction was learning the following mapping based on\ntextual/string representations:\n(Source compound, Potency difference)/(Target compound).\nThen, given a new (Source compound, Potency diﬀerence)\ntest instance, the pre-trained models should generate target\ncompounds with appropriate potency. For deriving pairs of\nsource and target compounds, the MMP formalism was applied.\nFor AC prediction, pre-trained models were subjected tone-\ntuning on MMP-Cliﬀs and MMP-nonCliﬀs from given activity\nclasses, corresponding to the derivation of other supervised\nmachine learning models.\n3.2 Pre-trained chemical language models\nInitially, the ability of Seq2Seq and transformer models to\nreproduce target compounds for test (sub)sets was evaluated by\ncalculating the reproducibility measure. The results are\nsummarized in Table 3. Therefore, for each test set triple, the\nsource compound/potency diﬀerence concatenation was used\nas input and 50 target candidate compounds were sampled.\nNotably, the sampling procedure is an integral part of chemical\nlanguage models in order to generate new candidate\ncompounds, hence setting these models apart from standard\nclass label prediction/classication approaches.\nFor the entire test set, the Seq2Seq and transformer model\nachieved reproducibility of 0.719 and 0.818, respectively. Hence,\nthe models were able to regenerate more than 70% and 80% of\nthe target compounds from MMP-triples not used for training,\nrespectively. However, reproducibility was consistently higher\nfor the transformer and all training set versions than for the\nSeq2Seq model (Table 3). Hence, preference for AC prediction\nwas given to the transformer. The test-general reproducibility of\nmore than 80% was considered high. Attempting to further\nincrease this reproducibility might compromise the ability of\nthe model to generate novel compounds by strongly focusing on\nchemical space encountered during training. As expected, the\ntest-core reproducibility was generally lowest because in this\ncase, the core structures of MMPs were not available during\ntraining (limiting reproducibility much more than in the case of\ntest-sub, i.e., evaluating novel substituents).\n3.3 Fine-tuning for activity cliﬀ prediction\nThe transformer wasrst applied to reproduce MMP-Cliﬀs and\nMMP-nonCliﬀs from the four activity classes excluded from pre-\ntraining. Therefore, for each MMP-triple, the source compound/\npotency diﬀerence concatenation was used as input for gener-\nating target compounds. As expected for activity classes not\nencountered during model derivation, reproducibility of MMP-\nCliﬀs and MMP-nonCliﬀs was low, reaching maximally 5% for\nMMP-Cliﬀs and∼19% for MMP-nonCliﬀs (Table 4).\nTherefore, a transfer learning approach was applied byne-\ntuning the pre-trained transformer on these activity classes. For\nne-tuning, 5%, 25%, and 50% of MMP-Cli ﬀs and MMP-\nnonCliﬀs of each class were randomly selected. The resulting\nmodels were then tested on the remaining 50% of the MMP-\nCliﬀs and MMP-nonCliﬀs.\nOnly 5% of the training data were required forne-tuning to\nachieve reproducibility rates of 70% to greater than 80% for\nTable 3 Reproducibility of target compounds by chemical language\nmodels\nTest-general Test-core Test-sub\nSeq2Seq 0.719 0.370 0.759\nTransformer 0.818 0.528 0.850\n902 | Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nMMP-Cliﬀs from the diﬀerent activity classes (Fig. 2A, solid\nlines). For MMP-nonCli ﬀs, 25% of the training data were\nrequired to achieve reproducibility between 60% and 80% for\nthe diﬀerent classes (Fig. 2B, solid lines). For practical appli-\ncations, thesendings were encouraging because for any given\ntarget, there were many more MMP-nonCliﬀs available than\nMMP-Cliﬀs.\nFurthermore, to directly test whether high reproducibility\nachieved through ne-tuning only depended on learning\nstructural relationships encoded by MMPs or if potency diﬀer-\nences were also learned, a prerequisite for meaningful AC\nprediction, control calculations with inverted potency diﬀer-\nences were carried out. Therefore, for all MMP-Cliﬀs, potency\ndiﬀerences were set toDpKi = 0.1 and for all MMP-nonCliﬀs,\npotency diﬀerences were set toDpKi = 2.0. Using these hypo-\nthetical (SAR-nonsensical) data as test instances, reproducibility\nrates were determined again. In this case, reproducibility rates\nremained well below 50% for both MMP-Cliﬀs (Fig. 2A, dashed\nlines) and MMP-nonCliﬀs (Fig. 2B, dashes lines) and further\ndecreased with increasing amounts of training data used for\nne-tuning. These ndings conclusively showed that the\nconditional transformer associated structural relationships\nwith corresponding potency diﬀerences, thereby learning to\nreproduce and diﬀerentiate between MMP-Cliﬀs and MMP-\nnonCliﬀs.\nIn the following, the conditional transformer for AC predic-\ntion is referred to as DeepAC.\nWe also evaluated the capability of the model to reconstruct\nboth MMP-Cliﬀs and MMP-nonCliﬀs originating from the same\nsource compound. For each activity class, we compiled a set of\nsource compounds from the original test data. Then, models\nwere ne-tuned with varying amounts of data and applied to\nreproduce MMP-Cliﬀ and MMP-nonCliﬀ target compounds\nfrom the same source compound. As shown in Fig. 3, DeepAC\nreproduced more than 80% of the target compounds using 5%,\n25%, or 50% of ne-tuning data, depending on the activity\nclass.\n3.4 Performance comparison of unconditional and\nconditional DeepAC\nWe also compared model performance of conditional DeepAC\nand unconditional DeepAC generated by randomly shuﬄing\npotency diﬀerences of MMPs duringne-tuning. Accordingly,\nfor each activity class, potency di ﬀerences were randomly\nshuﬄed for the three training set sizes (5, 25, and 50%) for the\nne-tuning MMPs; then the pre-trained transformer wasne-\ntuned using these articial MMPs. As shown in Fig. 4A, for\nMMP-Cliﬀs, the reproducibility of conditional DeepAC was\nsignicantly higher than of unconditional DeepAC. However,\nfor the reproducibility of MMP-nonCliﬀs, conditional DeepAC\nonly yielded slight improvement than unconditional DeepAC\nTable 4 Reproducibility of MMP-Cliﬀs and MMP-nonCliﬀs by pre-\ntrained DeepAC\nReproducibility\nActivity classes\nChEMBL204 1862 233 1800\nMMP-Cliﬀs 0.050 0.007 0.049 0.006\nMMP-nonCliﬀs 0.185 0.081 0.188 0.035\nFig. 2 Reproducibility of MMP-Cliﬀs and MMP-nonCliﬀs after ﬁne-\ntuning. For (A) MMP-Cli ﬀs and (B) MMP-nonCli ﬀs from di ﬀerent\nactivity classes (identiﬁed by ChEMBL target IDs according to Table 1),\nreproducibility is reported as a function of transfer ratio accounting for\nthe percentage of training data used for ﬁne tuning. Solid lines\nrepresent results for true MMP-Cliﬀs and MMP-nonCliﬀs and dashed\nlines for control data obtained by inverting potency diﬀerences for\nMMP-Cliﬀs and MMP-nonCliﬀs.\nFig. 3 Reproducibility of MMP-Cliﬀs and MMP-nonCliﬀs originating\nfrom the same source compound.\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 9 8–909 | 903\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\n(Fig. 4B). This was principally expected because potency diﬀer-\nences of most MMP-nonCliﬀs remained similar (less than one\norder of magnitude). Thesendings further demonstrated that\npotency diﬀerence of ACs played a critical role for model\nderivation.\n3.5 Alternative ne-tuning\nAs an additional control, ne-tuning was carried out using\nMMP-nonCliﬀs( DpKi < 1.0) and MMPs with 1.0# DpKi < 2.0\nthat were initially excluded from the analysis to prevent\npotential bias due to boundary eﬀects. Then, the reproducibility\nof MMP-Cliﬀs of the ne-tuned models was determined and\ncompared to regularne-tuning. Fig. 5A shows thatne-tuning\nonly with MMP-nonCliﬀs yielded reproducibility of 0.306–0.576\nfor the activity classes, reecting a baseline learning eﬀect of\nMMPs and associated potency diﬀerences, even if these were\nonly small. However,ne-tuning with MMPs (1.0# DpK\ni < 2.0),\nsignicantly increased the reproducibility of MMP-Cli ﬀst o\n0.620 for thrombin inhibitors, 0.607 for Mu opioid receptor\nligands, 0.726 for corticotropin releasing factor receptor 1\nligands and 0.716 for tyrosine kinase Abl inhibitors. Fine-tuning\nusing increasing proportions of MMP-Cliﬀs further increased\nreproducibility. Taken together, thesendings clearly demon-\nstrated the inuence of MMP-associated potency diﬀerences for\nAC predictions. Furthermore, consistent with these observa-\ntions, Fig. 5B shows thatne-tuning with MMP-nonCliﬀs, led to\nvery high reproducibility of MMP-nonCli ﬀs, which was\nsubstantially reduced when ne-tuning was carried out with\nMMPs capturing larger potency diﬀerences.\n3.6 Global performance comparison\nThe performance of DeepAC in activity prediction was\ncompared to other machine learning methods including SVM,\nRF, and XGboost. First, the reproducibility/recall of MMP-Cliﬀs\nand MMP-nonCliﬀs from the four activity classes was compared\nfor unbalanced training and test sets according to Table 1. For\nFig. 4 Performance of conditionalvs. unconditional DeepAC. Reproducibility is reported for (A) MMP-Cliﬀs and (B) MMP-nonCliﬀs. Mean and\nstandard deviations (error bars) are provided for each activity class. Independent-samplest tests were conducted: 0.05 <p #1.00 (ns), 0.01 <p #\n0.05 (*), 0.001 <p # 0.01 (**), 0.0001 <p # 0.001 (***), p # 0.0001 (****).\n904 | Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nAC prediction, unbalanced sets were deliberately used to\naccount for the fact that ACs are rare compared to other pairs of\nstructural analogues with minor potency di ﬀerences, thus\nproviding a realistic prediction scenario.\nThe predictions using di ﬀerent methods were generally\nstable, yielding only low standard deviations over independent\ntrials (Fig. 6). Using 5% of training data forne-tuning or model\nderivation, the recall (TPR) of MMP-Cli ﬀs was consistently\nhigher for DeepAC than the reference methods, which failed on\ntwo activity classes (Fig. 6). For increasing amounts of training\ndata, recall performance of the reference methods further\nincreased and SVM reached the 80% or 90% recall level of\nDeepAC in two cases when 50% of available data were used for\ntraining (Fig. 6).\nFor MMP-nonCliﬀs, representing the majority class for the\npredictions, a diﬀerent picture was obtained. Here, the recall of\nreference methods for increasing amounts of training data was\nmostly greater than 90% and signi cantly higher than of\nDeepAC (Fig. 7). For DeepAC, recall/reproducibility increased\nwith increasing amounts of training data and reached highest\nperformance very similar to the reference methods for two\nactivity classes when 50% training data were used.\nCalculation of BA for the prediction of MMP-Cliﬀs and MMP-\nnonCliﬀs gave similar results for all methods (Fig. 8). The level\nof 80% BA was generally reached for 25% or 50% training data.\nFor largest training sets, all methods were comparably accurate\nfor two activity classes, SVM reached highest accuracy for one\nclass, and DeepAC for another (Fig. 8). Compared to the other\nmethods, DeepAC produced higher TPR and lower TNR values,\nFig. 5 Model performance comparison after alternativeﬁne-tuning with diﬀerent types of MMPs. Reproducibility of (A) MMP-Cliﬀs and (B) MMP-\nnonCliﬀs is reported. Mean and standard deviations (error bars) are provided for each activity class. Independent-samplest tests were conducted:\n0.05 <p # 1.00 (ns), 0.01 <p # 0.05 (*), 0.001 <p # 0.01 (**), 0.0001 <p # 0.001 (***), p # 0.0001 (****).\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 9 8–909 | 905\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nFig. 6 Recall of MMP-Cliﬀs. For four diﬀerent methods, recall/reproducibility of MMP-Cliﬀs is reported for (A) thrombin inhibitors, (B) Mu opioid\nreceptor ligands, (C) corticotropin releasing factor receptor 1 ligands, and (D) tyrosine kinase Abl inhibitors. Average recall overﬁve independent\ntrials is reported for increasing amounts of training data randomly selected from the complete data set (error bars indicate standard deviations).\nStatistical tests are shown according to Fig. 4.\nFig. 7 Reproducibility of MMP-nonCliﬀs. In (A)–(D), reproducibility of MMP-nonCliﬀs is reported using four diﬀerent methods. Statistical tests\nare shown according to Fig. 4.\n906 | Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nresulting in overall comparable BA. Clearly, a major strength of\nDeepAC was the ability to accurately predict MMP-Cliﬀs on the\nbasis of small training data sets.\n3.7 Activity cli ﬀ predictions in context\nAs discussed above, AC predictions have been reported previ-\nously in independent studies, which are summarized (and\nordered chronologically) in Table 5. In 2012, AC predictions\nwith SVM and newly designed MMP kernels yielded high\naccuracy,\n9 which was also achieved in several subsequent\nstudies using modied SVM approaches (Table 5). In addition,\nin our current study, we have investigated decision tree\nmethods for AC predictions using molecular representations\nadapted from SVM, which yielded comparably high accuracy.\nHence, although AC predictions are principally challenging, for\nreasons discussed above, diﬀerent machine learning methods\nhave produced high-quality models for diﬀerent compound\ndata sets. Accordingly, there would be little incentive to inves-\ntigate increasingly complex models for AC predictions. None-\ntheless, recent studies have investigated deep learning\napproaches for AC predictions, with diﬀerent specic aims.\nThese investigations included the use of convolutional neural\nnetworks for predicting ACs from image data\n14,15 and the use of\ngraph neural networks for AC representation learning.16 While\nthese studies provided proof-of-concept for the utility of novel\nmethodologies for AC predictions, improvements in prediction\naccuracy compared to SVM in earlier studies have been\nmarginal at best. Therst eight studies in Table 5 report clas-\nsication models of varying complexity for AC prediction. While\nmost of these studies applied the MMP-Cliﬀ formalism, their\nsystem set-ups, calculation conditions, and test cases diﬀered\nsuch that prediction accuracies can only be globally compared\nand put into perspective including our current study. Further-\nmore, the last two studies\n17,18 in Table 5 report regression\nmodels for potency prediction of individual AC compounds that\nare distinct from the others, precluding comparison of the\nresults (these studies also used di ﬀerent AC de nitions).\nHowever, they are included for completeness.\nWith DeepAC, we have introduced the use of conditional\nchemical language models for AC prediction. Given that most\nstudies in Table 5 reported F1 (ref. 38) and Matthews' correla-\ntion coeﬃcient (MCC)\n39 scores for evaluating prediction accu-\nracies, we also calculated these scores for the DeepAC\npredictions reported herein. With F1 of 0.50–0.78 and MCC of\n0.43–0.75, DeepAC also yielded state-of-the-art prediction\naccuracy (and higher accuracy than recent AC predictions using\nFig. 8 Prediction accuracy. Reported are mean BA values and standard deviation (error bars) for the prediction of MMP-Cliﬀs and MMP-\nnonCliﬀs. In (A)–(D), results are reported using four diﬀerent methods and statistical tests according to Fig. 4.\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery, 2022,1,8 9 8–909 | 907\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\ngraph neural networks 16). However, DeepAC is principally\ndistinguished from other AC predictions approaches by its\nability to generate new compounds meeting AC criteria, which\npartly motivated its development.\n4 Conclusion\nIn this work, we have investigated chemical language models\nfor predictive modeling of ACs, a topical issue in both chemical\ninformatics and medicinal chemistry, with high potential for\npractical applications. ACs are rich in SAR information and\nrepresent focal points of compound optimization eﬀorts. For\nchemical language models, an encoding strategy was devised to\npredict target compounds from source compounds and asso-\nciated potency diﬀerences. Seq2Seq and transformer models\nwere pre-trained on pairs of structural analogues with varying\npotency diﬀerences representing true SARs and compared,\nrevealing superior performance of the transformer architecture\nin reproducing test compound pairs. The pre-trained trans-\nformer was thenne-tuned on ACs and non-ACs from diﬀerent\nactivity classes. It was conclusively shown that the transformer\nlearned structural relationships in combination with associated\npotency diﬀerences and thus accounted for SARs. Compared to\nreference methods, the conditional transformer (DeepAC)\nreached state-of-the-art prediction accuracy but displayed\ndiﬀerent prediction characteristics. DeepAC was less eﬀective in\npredicting non-ACs, but predicted ACs with higher accuracy\nthan reference methods, especially on the basis of small\ntraining data sets. A unique feature of DeepAC is its ability to\ngenerate novel candidate compounds. This ability and the\nobserved prediction characteristics render DeepAC attractive for\npractical applications aiming to generate new highly-potent AC\ncompounds, which will be investigated in future studies.\nData availability\nAll calculations were carried out using publicly available\nprograms, computational tools, and compound data. Python\nscripts used for implementing chemical language models and\ncurated activity classes used for AC predictions are freely\navailable via the following link: https://doi.org/10.5281/\nzenodo.7153115\nAuthor contributions\nAll authors contributed to designing and conducting the study,\nanalyzing the results, and preparing the manuscript.\nTable 5 Activity cliﬀ predictionsa\nStudy\nAC criteria, similarity/\npotency diﬀerence Prediction task Methods Prediction accuracy\nHeikamp et al.9 MMP/100-fold ACs for 9 activity classes Fingerprint-based SVM with\nMMP kernels\nF1: 0.70–0.99\nHusby et al.13 Binding mode\nsimilarity (80%)/100-\nfold\n3D-ACs for 9 activity classes Docking/VLS AUC: 0.75 –0.97\nHorvath et al.10 MMP/100-fold ACs for 7 activity classes CGR and descriptor\nrecombination-based SVM/\nSVR\nF1: 0.61–0.92\nTamura et al.12 MMP/100-fold ACs for 9 activity classes Fingerprint-based SVM with\nTanimoto kernel\nMCC: ∼0.20–0.80\nIqbal et al.14 MMP/100-fold ACs from MMP images and\nR-groups (5 activity classes)\nImage-based CNN with\ntransfer learning\nF1: 0.28–0.76\nMCC: 0.24–0.73\nIqbal et al.15 MMP/100-fold ACs from MMP images (3\nactivity classes)\nImage-based CNN F1: 0.36 –0.85\nAUC: 0.92–0.97\nMCC: 0.39–0.83\nTamura et al.11 MMP/100-fold ACs for 2 activity classes Fingerprint-based SVM with\nMMP kernel\nAUC: 0.46–0.69\nMCC: 0.69–0.89\nPark et al.16 MMP/100-fold ACs for 3 activity classes GCN F1: 0.34 –0.49\nAUC: 0.91–0.94\nMCC: 0.40–0.49\nJim´enez-Luna\net al.17\nMCS/10-fold — RF/DNN/GRAPHNET/GCN/\nMPNN/GAT\nRMSE: 0.698–1.029\nTilborg et al.18 Scaﬀold SMILES\nsimilarity (90%)/10-fold\nACs for 30 activity classes KNN/RF/GBM/SVM/MPNN/\nGAT/GCN/AFP/LSTM/CNN/\nTransformer\nRMSE: 0.62–1.60\na Abbreviations: SVM/R (support vector machine/regression); F1 (mean F1 score); AUC (area under the ROC curve); MCC (Matthews' correlation\ncoeﬃcient); 3D-ACs (three-dimensional activity cliﬀs); VLS (virtual ligand screening); CGR (condensed graphs of reaction); CNN (convolutional\nneural network); MCS (maximum common substructure); RF (random forest); DNN (deep neural network); GCN (graph convolutional network);\nMPNN (message passing neural network); GAT (graph attention network); RMSE (root mean square error); KNN (K-nearest neighbor); GBM\n(gradient boosting machine); AFP (attentivengerprint); LSTM (long short-term memory network).\n908\n| Digital Discovery, 2022,1,8 9 8–909 © 2022 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online\nConﬂicts of interest\nThere are no conicts of interest to declare.\nAcknowledgements\nH. C. is supported by the China Scholarship Council.\nReferences\n1 G. M. Maggiora,J. Chem. Inf. Model., 2006,46, 1535.\n2 D. Stumpfe, Y. Hu, D. Dimova and J. Bajorath,J. Med. Chem.,\n2014, 57,1 8–28.\n3 D. Stumpfe, H. Hu and J. Bajorath,ACS Omega, 2019, 4,\n14360–14368.\n4 D. R. Flower,J. Chem. Inf. Comput. Sci., 1998,38, 379–386.\n5 J. Hussain and C. Rea,J. Chem. Inf. Model., 2010,50, 339–348.\n6 H. Hu, D. Stumpfe and J. Bajorath,Future Med. Chem., 2019,\n11, 379–394.\n7 D. Stumpfe and J. Bajorath, Future Med. Chem., 2015, 7,\n1565–1579.\n8 R. Guha,J. Chem. Inf. Model., 2012,2, 2181–2191.\n9 K. Heikamp, X. Hu, A. Yan and J. Bajorath,J. Chem. Inf.\nModel., 2012,52, 2354–2365.\n10 D. Horvath, G. Marcou, A. Varnek, S. Kayastha, A. de la Vega\nde Le´on and J. Bajorath,J. Chem. Inf. Model., 2016,56, 1631–\n1640.\n11 S. Tamura, S. Jasial, T. Miyao and K. Funatsu,Molecules,\n2021, 26, 4916.\n12 S. Tamura, T. Miyao and K. Funatsu,Mol. Inf., 2020, 39,\n2000103.\n13 J. Husby, G. Bottegoni, I. Kufareva, R. Abagyan and\nA. Cavalli,J. Chem. Inf. Model., 2015,55, 1062–1076.\n14 J. Iqbal, M. Vogt and J. Bajorath,Artif. Intell. Life Sci., 2021,1,\n100022.\n15 J. Iqbal, M. Vogt and J. Bajorath,J. Comput.-Aided Mol. Des.,\n2021, 35, 1157–1164.\n16 J. Park, G. Sung, S. Lee, S. Kang and C. Park,J. Chem. Inf.\nModel., 2022,62, 2341–2351.\n17 J. Jim´enez-Luna, M. Skalic and N. Weskamp,J. Chem. Inf.\nModel., 2022,62, 274–283.\n18 D. van Tilborg, A. Alenicheva and F. Grisoni,Exposing the\nlimitations of molecular machine learning with activity cliﬀs,\nChemRxiv preprint, 2022.\n19 A. Gaulton, A. Hersey, M. Nowotka, A. P. Bento, J. Chambers,\nD. Mendez, P. Mutowo, F. Atkinson, L. J. Bellis, E. Cibri´an-\nUhalte, M. Davies, N. Dedman, A. Karlsson,\nM. P. Magarinos, J. P. Overington, G. Papadatos, I. Smit\nand A. R. Leach,Nucleic Acids Res., 2017,45, D945–D954.\n20 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n21 X. Hu, Y. Hu, M. Vogt, D. Stumpfe and J. Bajorath,J. Chem.\nInf. Model., 2012,52, 1138–1145.\n22 I. Sutskever, O. Vinyals and Q. V. Le,Adv. Neural Inf. Process.\nSyst., 2014, pp. 3104–3112.\n23 J. He, H. You, E. Sandström, E. Nittinger, E. J. Bjerrum,\nC. Tyrchan, W. Czechtizky and O. Engkvist, J. Cheminf.,\n2021, 13,1 –7.\n24 M.-T. Luong, H. Pham and C. D. Manning,Proceedings of the\n2015 conference on empirical methods in natural language\nprocessing, 2015, pp. 1412–1421.\n25 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser and I. Polosukhin,Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n26 V. Nair and G. E. Hinton,ICML, 2010, pp. 807–814.\n27 J. Ba, J. R. Kiros and G. E. Hinton, arXiv preprint\narXiv:1607.06450, 2016.\n28 K. He, X. Zhang, S. Ren and J. Sun, IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR)\n, 2015, pp.\n770–778.\n29 G. Landrum,RDkit: Open-source cheminformatics, 2006.\n30 A. Aszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,\nA. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai and S. Chintala,\nAdv. Neural Inf. Process. Syst., 2019, vol. 32, pp. 8026–8037.\n31 V. N. Vapnik, The nature of statistical learning theory ,\nSpringer, New York, 2000.\n32 L. Breiman,Mach. Learn., 2001,45,5 –32.\n33 T. Chen and C. Guestrin, Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery\nand Data Mining, 2016, p. 785794.\n34 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot, E. Duchesnay and G. Louppe, J.\nMach. Learn. Res., 2011,12, 2825–2830.\n35 D. Rogers and M. Hahn,J. Chem. Inf. Model., 2010,50, 742–\n754.\n36 J. Bergstra, B. Komer, C. Eliasmith, D. Yamins and D. Cox,\nComput. Sci. Discovery, 2015,8, 014008.\n37 K. H. Brodersen, C. S. Ong, K. E. Stephan and\nJ. M. Buhmann, Proceedings of the 20th International\nConference on Pattern Recognition (ICPR), 2010, pp. 3121–\n3124.\n38 C. J. Van Rijsbergen, Information retrieval , Butterworth-\nHeinemann, Oxford, 1979.\n39 B. W. Matthews,Biochim. Biophys. Acta, Protein Struct., 1975,\n405, 442–451.\n© 2022 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 2 ,1,8 9 8–909 | 909\nPaper Digital Discovery\nOpen Access Article. Published on 28 October 2022. Downloaded on 11/5/2025 2:57:42 PM. \n This article is licensed under a \nCreative Commons Attribution-NonCommercial 3.0 Unported Licence.\nView Article Online",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5618769526481628
    },
    {
      "name": "Computer science",
      "score": 0.47419315576553345
    },
    {
      "name": "Natural language processing",
      "score": 0.41570544242858887
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3736079931259155
    },
    {
      "name": "Chemistry",
      "score": 0.33196285367012024
    },
    {
      "name": "Engineering",
      "score": 0.1829507052898407
    },
    {
      "name": "Electrical engineering",
      "score": 0.07815548777580261
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}