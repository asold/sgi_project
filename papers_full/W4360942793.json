{
  "title": "Arabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer",
  "url": "https://openalex.org/W4360942793",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5059575637",
      "name": "Mohanad Sameer ..",
      "affiliations": [
        "Middle Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A2154941211",
      "name": "Ahmed Talib",
      "affiliations": [
        "Middle Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5066682824",
      "name": "Alla Hussein",
      "affiliations": [
        "Middle Technical University"
      ]
    },
    {
      "id": "https://openalex.org/A5059575637",
      "name": "Mohanad Sameer ..",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2154941211",
      "name": "Ahmed Talib",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5066682824",
      "name": "Alla Hussein",
      "affiliations": [
        "Middle Technical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2160815625",
    "https://openalex.org/W2394932179",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W1995562189",
    "https://openalex.org/W2402146185",
    "https://openalex.org/W1920942766",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W2520160253",
    "https://openalex.org/W2754624720",
    "https://openalex.org/W2940606701",
    "https://openalex.org/W3185689807",
    "https://openalex.org/W2783169573",
    "https://openalex.org/W1642584752",
    "https://openalex.org/W2183236601",
    "https://openalex.org/W2147768505",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2012256675",
    "https://openalex.org/W3197472475",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6750121898",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2517196708",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W2541674938",
    "https://openalex.org/W6701947533",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2995929068",
    "https://openalex.org/W2981022124",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3030437843",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2916864519"
  ],
  "abstract": "Recognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been more interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR architecture have been time-delay neural networks, recurrent neural networks (RNN), and long short-term memory (LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations of training parallelization, and they require a large amount of data to achieve acceptable results in recognizing Arabic speech This research presents an Arabic speech recognition based on a transformer encoder-decoder architecture with self-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The proposed model exceeds the performance of previous end-to-end approaches when utilizing the Common Voice dataset from Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 hours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition systems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training requires a very large amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language model.",
  "full_text": "Journal of Techniques, ISSN: 2708-8383, Vol. 5, No. 1, 2023, Pages 176-183 \nDOI: https://doi.org/10.51173/jt.v5i1.749  \n176 \n  \n \n \n \n \nRESEARCH ARTICLE - MANAGEMENT \nArabic Speech Recognition Based on Encoder-Decoder Architecture of Transformer \nMohanad Sameer 1*, Ahmed Talib 1, Alla Hussein 2, Husniza Husni 3 \n1 Technical College of Management - Baghdad, Middle Technical University, Baghdad, Iraq \n2 Technical Institute / Kut, Middle Technical University, Baghdad, Iraq \n3 School of Computing, Universiti Utara Malaysia, 06010 Sintok, Kedah, Malaysia \n* Corresponding author E-mail: dac0014@mtu.edu.iq  \nArticle Info. Abstract \nArticle history: \n \nReceived \n20 July 2022 \n \nAccepted \n01 September 2022 \n \nPublishing \n31 March 2023 \nRecognizing and transcribing human speech has become an increasingly important task. Recently, researchers have been \nmore interested in automatic speech recognition (ASR) using End to End models. Previous choices for the Arabic ASR \narchitecture have been t ime-delay neural networks, recurrent neural networks (RNN), and long short -term memory \n(LSTM). Preview end-to-end approaches have suffered from slow training and inference speed because of the limitations \nof training parallelization, and they require a lar ge amount of data to achieve acceptable results in recognizing Arabic \nspeech. This research presents an Arabic speech recognition based on a transformer encoder -decoder architecture with \nself-attention to transcribe Arabic audio speech segments into text, which can be trained faster with more efficiency. The \nproposed model exceeds the performance of previous end -to-end approaches when utilizing the Common Voice dataset \nfrom Mozilla. In this research, we introduced a speech-transformer model that was trained over 110 epochs using only 112 \nhours of speech. Although Arabic is considered one of the languages that are difficult to interpret by speech recognition \nsystems, we achieved the best word error rate (WER) of 3.2 compared to other systems whose training re quires a very \nlarge amount of data. The proposed system was evaluated on the common voice 8.0 dataset without using the language \nmodel. \nThis is an open-access article under the CC BY 4.0 license (http://creativecommons.org/licenses/by/4.0/) \nPublisher: Middle Technical University \nKeywords: Sequence to Sequence ASR; Arabic ASR; Transformer-Speech Recognition; Arabic Speech to Text. \n \n1. Introduction \nAutomatic speech recognition for Arabic is a very challenging task because of the complex morphological nature of the language and the \nabsence of short vowels in written text, which leads to several potential vowelizations for each grapheme, which are often co nflicting. The \nprimary goal of ASR is the machine's ability to interpret human speech and translate it to text or readable form with the ability to implement an \noperation based on instructions set by a human.  Over the past decades, ASR technologies have played multiple roles in many fields. For \nexample, education, personal computers, robotics, mobile phones, armies, health, security systems...etc.  Since deep learning for ASR systems \nhas been available [1], multiple neural network model architectures for ASR systems have been proposed in [2-6], The architectures mentioned \nabove include recurrent neural networks (RNN), particularly LSTM [7]. neural ne tworks are frequently employed whether, in the traditional \nsystem [3, 8], sequence-to-sequence-based systems [9, 10]), or end-to-end systems based on tranducers  [11]. The RNNs on the other hand has \nsome famous restrictions:  \n1) RNNs cannot deal with long-term temporal dependencies well Because of the problem of exploding and vanishing gradients identified in \n[8]. \n2) The recurrence nature of RNNs makes the processing of speech signals in parallel difficult. \nTo solve these problems, different architectures have been presented to replace RNNs, including TDNN [5], feed -forward sequential memory \nnetworks FSMN [6], and CNN [4, 9], since just limited improvement has been reached in the results.  It is worth mentioning that there is little \nresearch on the recognition of spoken speech in the Arabic language compared to other languages [10] noting that the Arabic l anguage is one \nof the 5 most important languages [11] and its speakers are more than 40 0 million people, an d it is the language of the Holy Qur'an, which \nbelieved by more than 1.8 billion people around the world in 2015 and the number is expected to be 3 billion in 2060.[12] The Arabic language \nis one of the oldest languages in the world. It is a language with great diversity. The Arabic language can be divided into three types: \n1) Classical Arabic: It is the language of the Islamic religion (the language of the Hadith and Qur‚Äôan, and the language of anci ent Arabic \npoetry). \n2) Modern Standard Arabic (MSA): Standard Arabic is utilized in communication, official correspondence newspapers, news, and mod ern \nbooks. \n3) Dialectal Arabic: Dialectal Arabic is used for everyday informal communication and has many forms(accents). \n \n \n  \nJOURNAL OF TECHNIQUES \n \nJournal homepage: http://journal.mtu.edu.iq \nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n177 \n \n \nNomenclature & Symbols   \nCNN        Convolution Neural Network LSTM Long Short-Term Memory \nMSA       Modern Standard Arabic      GMM Gaussian Mixture Model \nRNN    Recurrent Neural Network BDRNN Bidirectional Neural Network \nURL Uniform Resource Locator DNN Deep Neural Network  \nWER Word Error Rate MFCC Mel-Frequency Cepstral Coefficients   \nTDNN Time Delay Neural Networks FSMN Feed-Forward Sequential Memory Networks \ndmodel Dimensional Output HMM Hidden Markov Model \n \nThis paper aims to introduce an ASR system for recognizing Arabic speech using a Transformer based on the attention mechanism  technique \nto recognize modern standard Arabic speech and show how well the proposed model performs in a truly low -resource language as \nmodern standard Arabic and tries to achieve the best possible WER in Speech recognition by using the available data.  \nOur contributions are as follows. First, we show that depth is an important factor to acquire competitive end -to-end ASR models \nwith the Transformer especially for recognizing Arabic speech. Second, obtain the state-of-the-art result among end-to-end ASR \nmodels for the Arabic Common voice 8.0. This result is achieved using a total of 5 encoder layers,3 decoders, and 460 feed -\nforward layers. Note that the best result was previously achieved at 40.6% of WER on the common voice dataset [13]. \n2. Related Works \nSpeech recognition enables machines to recognize and respond to human speech. ASR is a machine's ability to recognize (interpret) spoken \nlanguage and convert it into a textual form, as well as take actions based on human-defined instructions [14]. In [15] The authors reported their \nexperiences in building the Arabic version of CMU's Sphinx4 ASR system, where they presente d a system to recognize the speech of Arabic \ndigits. In [16]the authors proposed a dataset of Algerian Arabic speech, consisting of statements spoken by 300 Algerian speakers with different \naccents, picked from 11 areas of Algeria. \nIn [17] the authors introduced a technique of Arabic ASR based on Sphinx-Train and also introduced a corpus of 11.5 hours of labeled speech \ndata for system training, their voice recognition system performance was evaluated using three Arabic speakers only.  The results achieved \naround a 97% recognition rate for Arabic single words. [18] proposed a system for phoneme recognition as an ASR system. Many techniques \nare used in this paper Firstly, in the stage of processing, the algorithm of Gaussians' Low -Pass-Filtering was used with an artificial neural \nnetwork to improve the result. In the recognition of the phoneme stage, signal catching, sampling, quantization, and setting energy are done, \nthen, a neural network is utilized to improve the achieved result. Whe n using the Gaussians' Low -Pass-Filtering on voice signals, the results \nhave an enhanced impact due to noise reduction, after that, the neural network will be used in the training phase to recognize the speech signal. \nThe authors in [19] presented an architecture for Arabic ASR, which consists of four stages:  \n1) Pre-processing stage.  \n2) Feature extraction stage.  \n3) Pronunciation dictionary, language model, and Acoustic model (decoding).  \n4) Stage of Post-processing result in which the best hypotheses are produced. \nThe working mechanism of these stages is as follows: \nPre-processing stage inputs are utterance speech signals, then the output is processed voice signals used as input to the feature extraction stage, \nwhere the features are represented by vectors as output. After that, the vectors are used as input in the de coding stage, and it works along with \na pronunciation dictionary. Finally, the outputs from the pronunciation dictionary step are fed into the post -processing step. In the hypothesis-\nsearch component, acoustic and language models are combined, scores are g iven to the features vector sequence and the predicted word \nsequence, then the word sequence with the best score is produced. In [10], the authors proposed three approaches to improve t he ASR system. \nThe first approach in pronunciation modeling is the employment of a decision tree with pronunciation variant generation. Then, a hybrid method \nis presented to adapt the native acoustic model with another native acoustic model. Lastly, processed text was used to improv e the language \nmodel. In the result, the word error rate is reduced by 1%, 1.2%, and 1.9% for the pronunciation model, the acoustic modeling, and the language \nmodel, respectively. \nThe Hidden Markov Model and Gaussian Mixture Model have been widely used for a long time in large -vocabulary continuous speech \nrecognition (LVCSR).[20] introduced the first HMM‚ÄìDNN hybrid technique, in which the GMM model was replaced by a deep neural network \nmodel. LVCSR performance was significantly improved compared to the previous HMM -GMM systems. After that, for acoustic modeling, \nseveral DNN architectures were investigated, such as Recurrent Neural Networks (RNN), Bidirectional RNN(BDRNN), and deep co nditional \nrandom fields, all of which demonstrated a significant increase in performance [21, 22]. In [5], the researchers introduced a  time-delay neural \nnetwork and obtained higher performance in learning and broader temporal dependencies than DNN - RNN-based architectures. In   [11], the \nauthors presented the recognition of isolated Arabic spoken words that were used in two ASR applications(Television voice com mand \nrecognition and spoken digit recognition). System components include (signal acquisition, feature extraction using MFCCs, corpus construction, \nmodel training with RNN and gated RNN, and classification). In   [13], the authors attempted to build a sequence -to-sequence ASR system for \nthe Tunisian dialect and modern standard Arabic(Common Voice dataset) by using deep learning algorithms based on DeepSpeech2 \narchitecture, they built a Tunisian speech dialect corpu s named \"TunSpeech\" which consisted of 11 hours only and achieved 40.6% WER on \nthe common voice that they trained their system on , Whereas, in this research, we achieved an accuracy of performance that reached 3.2 of \nWER on the Common Voice dataset using our speech -transformer model. In [23], the authors proposed an ASR system by utilizing a deep \nneural network-based hybrid and a Transformer-based End to End model to build the first ASR system and releasing the first speech corpus for \nthe Egyptian Arabic‚ÄìEnglish language pair in which hypotheses of the two systems are merged at the sentence and word level. These techniques \nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n178 \nresult in a total WER relative enhancement of 4.7% over the original WER score of 32.1%. In the instance of intra -sentential CS sentences, \nthey obtain a word error rate significant enhancement of 4.8 %. and the best system performance achieved a word error  rate of 30.6% on the \nArzEn test set. \nFinally, we would like to clarify here that the proposed system using the (Transformer) is the first of its kind that has been trained \nand tested on a common voice dataset. In Fig. 1, we illustrate the historical development of speech recognition systems from 1950 to the \nrecent and important developments that have occurred over the years. \n \nFig. 1. Historical Development of Speech Recognition Systems \n3. Proposed Model \nIn this section, we will first explain the pre-processing operations on the dataset, followed by the Methodology that describes the Acoustic Input \nLayer of the system, and then explains in detail the Architecture of the Transformer Model. Finally, the most  important components of the \ntransformer-speech recognition system are stated.  \n3.1. Pre-Processing \nThe sources of train, dev, and test are all data that have been reviewed and deemed of good quality. We cleaned the data by u sing only the \nallowed set of characters Arabic language. All audio clips are in an original dataset in mp3 format, and then we conv erted the format of audio \nfrom mp3 to Wav with a 16000-sample rate, a mono audio channel, and a depth of 16 bits.  \nRegular expression rules and processes were used to correct the orthographic problems in the dataset. The following are some steps used to \nreduce ambiguity in spelling and pronunciation: \n‚ñ™ Remove all URLs, E-mail addresses, paths, and special characters (&, @, $,;,:, ., (), \",‚Ä¶.) pronunciation, and non-Arabic texts.                                          \n‚ñ™ All diacritics representing consonant stressing or short vowels are stripped. \n‚ñ™ All numbers are normalized and they are converted into literal words. \n‚ñ™ Remove any double space between the words \n‚ñ™ The stretched words are reduced to their original form. For example, ÿßŸÑÿ±ÿ¨ÿßÿßÿßÿßŸÑ is replaced by ÿßŸÑÿ±ÿ¨ÿßŸÑ men\" \n‚ñ™ If a ÿ©  ta marobuTa is connected to the next word, a space is added after each word. For example,   ŸÜŸáÿßŸäÿ©ÿßŸÑŸÇÿ±ŸÜ   is replaced by ŸÜŸáÿßŸäÿ© ÿßŸÑŸÇÿ±ŸÜ century \nend\".   \n‚ñ™ The time is expressed in a literal manner, as shown for instance 15:30 is replaced by ÿßŸÑÿ´ÿßŸÑÿ´ÿ© Ÿà ÿ´ŸÑÿßÿ´ŸàŸÜ ÿØŸÇŸäŸÇÿ© \n‚ñ™ Replace some abbreviations with their corresponding meaning (As shown in Table 1). \n3.2. Architecture of Transformer Model \nWe apply four convolutional layers to downsample acoustic features (through convolution strides) and process local relations and calculate \nthe sum of the token and position embeddings when processing past target tokens for the decoder. \nThe proposed speech recognition-Transformer model aims to transform the speech features sequence into the corresponding characters \nsequence. It can be represented by a two-dimensional spectrogram with time and frequency axes. \n\nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n179 \nTable 1. Abbreviations of Arabic words \nAbbreviation Arabic words English term \n % ŸÅŸä ÿßŸÑŸÖÿßÿ¶ÿ© percent \nÿ™ ÿ∫ ÿ™ŸàŸÇŸäÿ™ ÿ∫ÿ±ŸäŸÜÿ™ÿ¥  GMT \nÿ™ ÿ™ÿßÿ±ŸäÿÆ  Date \nŸáŸÄ \nÿØ. ŸÉ \nŸáÿ¨ÿ±Ÿä  \nÿØŸàŸÑÿßÿ± ŸÉŸÜÿØŸä \nIslamic Calendar \nCanadian dollar \nÿ≥ ÿ≥ÿßÿπÿ©  Hour \nÿØ ÿØŸÇŸäŸÇÿ©  Minute \nÿ´ ÿ´ÿßŸÜŸäÿ©  Second \n3.2.1. Encoder-Decoder Block \nThe model consists of two important parts. The encoder is the first, consuming the source sequence and producing a high-level representation; \nthe second is the decoder, which generates the target sequence.  \nEncoder and Decoder are both neural networks that consist of neural parts that can learn the relationship between the input a nd output time \nsteps. The decoder also includes the process to condition particular encoder representation components. Instead of r ecurrence (RNN), multi-\nhead attention or its attention mechanism is the heart of the Transformer model. \nInput audio spectrograms are used by our model to predict a character sequence. The target character sequence is presented to  the decoder \nduring training. The decoder predicts the following token during inference by using its prior predictions [24, 25], see Fig. 2. \n \nFig. 2. Speech-Transformer model architecture \n3.2.2. Attention \nAttention is the process of employing a content-based information extractor from a set of Q queries, K keys, and V values. Similarities between \nthe Q and the K, and in turn return the weighted sum of the values using Scaled Dot -product-attention are the ba sis for the retrieval function \n\nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n180 \n[26]. Scaled Dot Product Attention is an efficient method for self-attention proposed in [25] As illustrated in Fig. 3, Let Q ‚àà R√ódq are queries, \nK ‚àà Rtq√ódq are keys and V ‚àà  Rtq√ódq are valued, where t represents the element numbers in various inputs and d‚àó is the corresponding element \ndimension. Normally, tk = tv, dq= dk. The outputs of self-attention are calculated as follows: \nùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ, ùêæ, ùëâ) = ùë†ùëúùëìùë°ùëöùëéùë• (\nùëÑùêæùëá \n‚àöùëëùëò\n) ùëâ                                                                                                                                                      (1) \n \nFig. 3. Scaled Dot-Product Attention \nWhere: \nMatMul is a function that returns the matrix product of two arrays, SoftMax is an activation function, mask refers to  masking it is utilized to \nensure the predictions for position j can depend only on the known outputs at positions less than j, Scale is meaning scale d own where the \nscaler 1/‚àö dk is used to prevent SoftMax function into regions that have very small gradients. \n3.2.3. Multi-Head Attention \nThe transformer is consisting of multiple dot Attention layers [26], in [25] dot -product attention has recently been improved, which scales the \nqueries before and then introduces a sub -space projection for K, Q, and V into n parallel heads. Since n -attention operations are implemented \nwith corresponding heads for each operation. The results are the chain from the attention output of each head. Conspicuously,  different from \nrecurrent connections which use a single state with a gating structure for Data transmissions, or convolution connections that linearly combine \nlocal states constrained by a kernel size, self-attention is an aggregation of the information in all time steps without intermediate transformation. \nThe Scaled Dot-Product Attention is computed individually, and their outputs are sequenced and fed into other linear projections to produce \nthe final dimensional outputs dmodel , see Fig. 4. \n \nFig. 4. Multi-Head Attention architecture \n\nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n181 \n3.3. Speech Model and Layers Description  \nThe Speech recognition using Transformer is like the previous sequence-to-sequence models, the encoder transforms a speech features sequence \n(x1, . . . , x T) to a hidden representation h = (h 1, . . . , h L), then the decoder produces an output sequence (y 1,..., ys). the decoder uses the prior \nemitted characters at each step as additional inputs when emitting the next character [25]. \nAs illustrated in Fig. 1. In Transformers, the encoder -decoder layers are made up of Self Attentional sub -layers that are connected with feed -\nforward neural layers. To accommodate extended speech statements, we adopt the reshaping technique described in [6] by gathering consecutive \nframes into a single step, then, we integrate the acoustic features with sinusoidal positional encoding [25 , 27, 28]. The input features from the \nTransformer encoder are passed to a self -attention layer, then to a feed -forward neural network with one hidden layer that uses the activation \nfunction of ReLU. Before these sub-modules, we include residual connections that create shortcuts between the higher layer and the lower-level \nrepresentation. The normalization layers positioned after each residual con nection significantly reduce the amplitude of the neuron values \ncaused by the residual layer's presence [29]. Modern translation systems use the Transformer decoder as the original transformer decoder from \n[25]. To keep the model's auto -regressive nature, the decoder's Self Attention layer should be masked such that each state only has access to \nthe previous states. This is a significant distinction between the decoder and the encoder. Moreover, Between the self -attention and the feed-\nforward layers, there is an additional attention layer that uses the target hidden layers as queries and the encoder outputs as keys and values. \nComparing this particular Transformer design to previous proposed RNNs and CNNs networks, there are multiple advantages, such  that each \nlayer and sub-module can be efficiently parallelized over the mini -batch and time dimensions of input data.in addition to the combination of \nresidual and layer normalization are the keys to allowing more depth configurations to be trainable. This is the primary factor behind the result \nperformance improvement in current research in both machine translation and NLP [30, 31]. \n4. Experimental Result \nMozilla's Common Voice is an audio collection that intends to make the recognition of human speech accessible to everyone. Co mmon Voice \ndataset includes English, Arabic, French, and German, in addition to more than ten other languages. The corpora were cre ated outside of a \ncontrolled environment and setting. The speech may contain noise in the background, and users may have diverse accents. Commo n Voice \nconsists of a unique MP3 and corresponding text file. The Arabic Common Voice version 8.0 contained 139 r ecorded hours in the dataset \n(before preprocessing). Afterward, we trained the model for 112 hours (after preprocessing the dataset and skipping the trans cription that \ncontained vocabulary words and characters). The dataset consists of demographic metadata  such as gender, accent, and age. May assist in \ntraining speech recognition engines [36]. \nTextual data is data from texts utilized to create the language model and to improve Deep Speech results. We use one of the l argest available \ntextual corpus is a raw text from Arabic daily newspapers collected over a year between 2004 and 2005 by Ahmed Abd elali that contains \n250354 lines of statements and 2941404 words. \nThe Python language was used to program the system based on the libraries of TensorFlow, Keras, NumPy, Glob, Os, Pydub, Wave, Audioop, \nand Jiwer which were used to obtain the Word Error Rate of the predictions, pyyaml h5py  that required for saving models and checkpoints due \nto training in HDF5 format. The Google Colab Pro was used to train the model as illustrated in Fig. 5. It offers GPU (NVIDIA-SMI 460.32.03, \nGPU name is Persistence-M) between 50 and 120 epochs using a custom learning rate schedule, and an early stop is used where the validation \nloss is not improved for 6 epochs. MSA common voice 8.0 dataset was utilized for train validation, and testing with splitting rates of 90%, 5%, \nand 5%. To train the system with the best combination and achieve the best result, we have conducted several experiments to t rain the system \nby using different variables. The best results were achieved when using frame-length=200, frame-step=80, and a batch size of training is 128, \nFinally, the duration of the training and testing of the model was about 34 hours.  \n \nFig. 5. Speech recognition training execution on Colab \nThe word error rate, which is a widely used measurement in automatic speech recognition evaluation, is defined as follows: \nùëäùê∏ùëÖ =\nS+D+I\nN                                                                                                                                                                                                         (7) \nN: number of words in the source sentence. \n\nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n182 \nS: substituted words  \nD: Deleted word  \nI: Inserted word  \nNote that increasing insertion may cause WER to be greater than 1. \n5.  Results and Discussion \nWe have used different hyperparameters to train the Arabic speech recognition model to achieve the best-desired results by changing the number \nof encoders Ne and the number of decoders Nd as we mention in Tables 2 and 3, ignoring experiments that led to the overfitting or underfitting. \nTable 2. Results Summary \nWER % Feedforward layer Nd Ne Model No. \n3.9 400 2 5 1 \n3.7 400 3 5 2 \n5.8 400 3 6 3 \n3.2 460 3 5 4 \n6.2 460 5 6 5 \n6.3 460 6 8 6 \nTable 3. Examples of Prediction \n              Target                          Prediction  \nÿßŸÜÿß ÿßÿπÿ±ŸÅ ÿπŸÜŸàÿßŸÜŸá ÿßŸÜÿß ÿßÿπÿ±ŸÅ ÿπŸÜŸàÿßŸÜŸá \nŸáŸÑ ÿ™ÿ∞ŸÉÿ± ÿßŸÑŸÑŸäŸÑÿ© ÿßŸÑÿ™Ÿä ÿ™ŸÇÿßÿ®ŸÑŸÜÿß ŸÅŸäŸáÿß ÿßŸàŸÑ ŸÖÿ±ÿ©  ŸáŸÑ ÿ™ÿ∞ŸÉÿ± ÿßŸÑŸÑŸäŸÑÿ© ÿßŸÑÿ™Ÿä ÿ™ŸÇÿßÿ®ŸÑŸÜÿß ŸÅŸäŸáÿß ÿßŸàŸÑ ŸÖÿ±ÿ©  \nŸáŸà ŸÖŸÜÿ≤ŸÑ ÿßŸÑŸÜÿ¨ŸàŸâ ÿ®ÿÆÿßŸÑŸä ÿßŸÑÿ£ÿπÿµÿ± ŸàŸáŸà ŸÖŸÜÿ≤ŸÑ ÿßŸÑŸÜÿ¨ŸàŸâ ÿ®ÿÆÿßŸÑŸä ÿßŸÑÿ£ÿπÿµÿ±  \nŸÑÿß ÿßÿ≠ÿ® ÿßŸÜ ÿßŸÉŸàŸÜ Ÿàÿ≠ÿØŸä ŸÑÿß ÿßÿ≠ÿ® ÿßŸÜ ŸÉŸÑ Ÿàÿ≠ÿØŸä  \nŸäŸÇŸàÿØ ÿ£ÿ®Ÿä ÿßŸÑÿ≥Ÿäÿßÿ±ÿ© ÿ•ŸÑŸâ ÿπŸÖŸÑŸá  ŸäŸÇŸàÿØ ÿ£ÿ®Ÿä ÿßŸÑÿ≥Ÿäÿßÿ±ÿ© ÿ•ŸÑŸâ ÿπŸÖŸÑŸá  \nÿ£ÿ®ÿß ÿ≠ŸÅŸäÿµ ÿ±ŸàŸäÿØÿß ÿ£ÿ®ÿß ÿ≠ŸÅŸäÿ≥ ÿ±ŸàŸäÿØÿß  \n \nThe best results were achieved through the use of 5 encoders and 3 decoders, obtaining a word error rate of 3.2. We can compare this r esult \nwith the latest result previously achieved by Messaoudi et al. (2021)[13], where the word error rate was achieved at 4.0 by utilizing the common \nvoice dataset. \nWhen analyzing the errors made by the proposed model, one issue that stands out is a truncated output. Quite a lot of output texts are much \nshorter than the source transcripts. The   performance of the model will be   seriously impacted by the failure to produce long enough output \nsentences. Automatic speech recognition has a lot of challenges, specifically about spontaneous Arabic speech due to the prev alence of \"uh\" \nand \"um\" disfluencies, as well as other speech irregularities such as stuttering and coughing. Therefore, the system must be capable of dealing \nwith out-of-vocabulary words (OOV). Other issues are the variation of speakers, word speed, accent, pitch, etc. \n6. Conclusion \nIn this paper, we proposed an ASR system to recognize the Arabic language based on a Transformer that depends on Attention -Mechanism \nwith positional encoding to learn position dependencies. The proposed work achieved the best accuracy in recognizing Arabic s peech and \nreduced the heavy training cost compared to the preview recurrence models. The proposed Speech -Transformer system achieved the state-of-\nthe-art performance 3.2 of WER on Common Voice when evaluated on the Common Voice dataset using five encoders and three decoders. We \ntrained the ASR model on the Modern Standard Arabic language without using a language model and utilizing only 112 hours for training and \nevaluating the system. In future work, the system can be trained on more data to get better results, and it can also be trained on the Iraqi dialect \nor mixed MSA -Iraqi dialect to achieve the optimum benefit from speech recognition applications in Iraqi communities, organizations, \ninstitutions, and companies. \nAcknowledgment \nThis work is extracted from a thesis ; it was submitted as part of the requirements for obtaining a master‚Äôs degree in managerial informatics \ntechniques. I took this opportunity to thank everyone who helped and supported me to complete this work so that it would reflect the scientific \nstanding of my esteemed university. \nReferences \n[1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, et al., \"Deep neural networks for acoustic modeling in speech recognition: \nThe shared views of four research groups,\" IEEE Signal processing magazine, vol. 29, pp. 82-97, 2012. \n[2] F. Seide, G. Li, and D. Yu, \"Conversational speech transcription using context -dependent deep neural networks,\" in Twelfth annual \nconference of the international speech communication association, 2011. \n[3] H. Sak, A. W. Senior, and F. Beaufays, \"Long short-term memory recurrent neural network architectures for large scale acoustic modeling,\" \n2014. \nMohanad S. et. al, Journal of Techniques, Vol. 5, No. 1, 2023 \n \n183 \n[4] O. Abdel -Hamid, A. -r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, \"Convolutional neural networks for speech recognition,\" \nIEEE/ACM Transactions on audio, speech, and language processing, vol. 22, pp. 1533-1545, 2014. \n[5] V. Peddinti, D. Povey, and S. Khudanpur, \"A time delay neural network architecture for efficient modeling of long temporal contexts,\" in \nSixteenth annual conference of the international speech communication association, 2015. \n[6] S. Zhang, H. Jiang, S. Wei, and L. Dai, \"Feedforward sequential memory neural networks without recurrent feedback,\" arXiv preprint \narXiv:1510.02693, 2015. \n[7] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, pp. 1735-1780, 1997. \n[8] Y. Bengio, P. Simard, and P. Frasconi, \"Learning long-term dependencies with gradient descent is difficult,\" IEEE transactions on neural \nnetworks, vol. 5, pp. 157-166, 1994. \n[9] R. Collobert, C. Puhrsch, and G. Synnaeve, \"Wav2letter: an end -to-end convnet -based speech recognition system,\" arXiv preprint \narXiv:1609.03193, 2016. \n[10] B. H. Ahmed and A. S. Ghabayen, \"Arabic automatic speech recognition enhancement,\" in 2017 Palestinian International Conference on \nInformation and Communication Technology (PICICT), 2017, pp. 98-102. \n[11] N. Zerari, S. Abdelhamid, H. Bouzgou, and C. Raymond, \"Bidirectional deep architecture for Arabic speech recognition,\" Open Computer \nScience, vol. 9, pp. 92-102, 2019. \n[12] A. A. Abdelhamid, H. A. Alsayadi, I. Hegazy, and Z. T. Fayed, \"End -to-end arabic speech recognition: A review,\" in Proceedings of the \n19th Conference of Language Engineering (ESOLEC‚Äô19), Alexandria, Egypt, 2020, pp. 26-30. \n[13] A. Messaoudi, H. Haddad, C. Fourati, M. B. Hmida, A. B. E. Mabrouk, and M. Graiet, \"Tunisian Dialectal End-to-end Speech Recognition \nbased on DeepSpeech,\" Procedia Computer Science, vol. 189, pp. 183-190, 2021. \n[14] H. H. Nasereddin and A. A. R. Omari, \"Classification techniques for automatic speech recognition (ASR) algorithms used with r eal time \nspeech translation,\" in 2017 Computing Conference, 2017, pp. 200-207. \n[15] H. Satori, M. Harti, and N. Chenfour, \"Introduction to Arabic speech recognition using CMUSphinx system,\" arXiv preprint \narXiv:0704.2083, 2007. \n[16] S. A. Selouani and M. Boudraa, \"Algerian Arabic speech database (ALGASD): corpus design and automatic speech recognition \napplication,\" Arabian Journal for Science and Engineering, vol. 35, pp. 157-166, 2010. \n[17] M. Belgacem, A. Maatallaoui, and M. Zrigui, \"Arabic language learning assistance based on automatic speech recognition system ,\" in \nProceedings of the International Conference on e-Learning, e-Business, Enterprise Information Systems, and e-Government (EEE), 2011, \np. 1. \n[18] K. Khatatneh, \"A novel Arabic Speech Recognition method using neural networks and Gaussian Filtering,\" International Journal of \nElectrical, Electronics & Computer Systems, vol. 19, 2014. \n[19] D. Yu and L. Deng, Automatic speech recognition vol. 1: Springer, 2016. \n[20] G. E. Dahl, D. Yu, L. Deng, and A. Acero, \"Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\" \nIEEE Transactions on audio, speech, and language processing, vol. 20, pp. 30-42, 2011. \n[21] A. Graves, A. -r. Mohamed, and G. Hinton, \"Speech recognition with deep recurrent neural networks,\" in 2013 IEEE international \nconference on acoustics, speech, and signal processing, 2013, pp. 6645-6649. \n[22] Y. Hifny, \"Unified acoustic modeling using deep conditional random fields,\" Transactions on Machine Learning and Artificial Intelligence, \nvol. 3, p. 65, 2015. \n[23] I. Hamed, P. Denisov, C. -Y. Li, M. Elmahdy, S. Abdennadher, and N. T. Vu, \"Investigations on speech recognition systems for low -\nresource dialectal    Arabic‚ÄìEnglish code-switching speech,\" Computer Speech & Language, vol. 72, p. 101278, 2022/03/01/ 2022. \n[24] L. Dong, S. Xu, and B. Xu, \"Speech -transformer: a no -recurrence sequence-to-sequence model for speech recognition,\" in 2018 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5884-5888. \n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, et al., \"Attention is all you need,\" Advances in neural information \nprocessing systems, vol. 30, 2017. \n[26] M.-T. Luong, H. Pham, and C. D. Manning, \"Effective approaches to attention -based neural machine translation,\" arXiv preprint \narXiv:1508.04025, 2015. \n[27] M. Sperber, J. Niehues, G. Neubig, S. St√ºker, and A. Waibel, \"Self-attentional acoustic models,\" arXiv preprint arXiv:1803.09519, 2018. \n[28] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, \"Convolutional sequence to sequence learning,\" in International \nconference on machine learning, 2017, pp. 1243-1252. \n[29] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450, 2016. \n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre -training of deep bidirectional transformers for language understanding,\" \narXiv preprint arXiv:1810.04805, 2018. \n[31] N.-Q. Pham, J. Niehues, T. -L. Ha, E. Cho, M. Sperber, and A. Waibel, \"The karlsruhe institute of technology systems for the news \ntranslation task in wmt 2017,\" in Proceedings of the Second Conference on Machine Translation, 2017, pp. 366-373. \n[32] A. Bapna, M. X. Chen, O. Firat, Y. Cao, and Y. Wu, \"Training deeper neural machine translation models with transparent attention,\" arXiv \npreprint arXiv:1808.07561, 2018. \n[33] A. Veit, M. J. Wilber, and S. Belongie, \"Residual networks behave like ensembles of relatively shallow networks,\" Advances in neural \ninformation processing systems, vol. 29, 2016. \n[34] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \"Deep networks with stochastic depth,\" in European conference on computer \nvision, 2016, pp. 646-661. \n[35] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: a simple way to prevent neural network s from \noverfitting,\" The journal of machine learning research, vol. 15, pp. 1929-1958, 2014. \n[36] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer , et al. , \"Common voice: A massively -multilingual speech corpus,\" \narXiv preprint arXiv:1912.06670, 2019. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8359782695770264
    },
    {
      "name": "Speech recognition",
      "score": 0.758145809173584
    },
    {
      "name": "Transformer",
      "score": 0.703048586845398
    },
    {
      "name": "Encoder",
      "score": 0.5940858721733093
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5697788000106812
    },
    {
      "name": "Word error rate",
      "score": 0.560321033000946
    },
    {
      "name": "Language model",
      "score": 0.5486557483673096
    },
    {
      "name": "Inference",
      "score": 0.5129827857017517
    },
    {
      "name": "Arabic",
      "score": 0.5124025940895081
    },
    {
      "name": "Architecture",
      "score": 0.4664786458015442
    },
    {
      "name": "Acoustic model",
      "score": 0.45858287811279297
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4505905210971832
    },
    {
      "name": "End-to-end principle",
      "score": 0.448365181684494
    },
    {
      "name": "Artificial neural network",
      "score": 0.4382270574569702
    },
    {
      "name": "Natural language processing",
      "score": 0.4041746258735657
    },
    {
      "name": "Speech processing",
      "score": 0.3046053946018219
    },
    {
      "name": "Engineering",
      "score": 0.07644140720367432
    },
    {
      "name": "Voltage",
      "score": 0.06382167339324951
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210115078",
      "name": "Middle Technical University",
      "country": "IQ"
    }
  ],
  "cited_by": 8
}