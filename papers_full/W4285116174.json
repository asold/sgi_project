{
  "title": "Pretrained Biomedical Language Models for Clinical NLP in Spanish",
  "url": "https://openalex.org/W4285116174",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2970055708",
      "name": "Casimiro Pio Carrino",
      "affiliations": [
        "Barcelona Supercomputing Center",
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A4303316096",
      "name": "Joan Llop",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "Barcelona Supercomputing Center"
      ]
    },
    {
      "id": "https://openalex.org/A3046976067",
      "name": "Marc Pàmies",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3166085439",
      "name": "Asier Gutiérrez-Fandiño",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2979305992",
      "name": "Jordi Armengol-Estapé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4303316100",
      "name": "Joaquín Silveira-Ocampo",
      "affiliations": [
        "Barcelona Supercomputing Center",
        "Universitat Politècnica de Catalunya"
      ]
    },
    {
      "id": "https://openalex.org/A2236276917",
      "name": "Alfonso Valencia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2731970290",
      "name": "Aitor Gonzalez-Agirre",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "Barcelona Supercomputing Center"
      ]
    },
    {
      "id": "https://openalex.org/A2296216261",
      "name": "Marta Villegas",
      "affiliations": [
        "Universitat Politècnica de Catalunya",
        "Barcelona Supercomputing Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3099008231",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4394637965",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W3200907515",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2805211535",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4286895930",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2989464093",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2985294119",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W3101058639",
    "https://openalex.org/W2948740140",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3096590546",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3003266211",
    "https://openalex.org/W3161430317"
  ],
  "abstract": "This work presents the first large-scale biomedical Spanish language models trained from scratch, using large biomedical corpora consisting of a total of 1.1B tokens and an EHR corpus of 95M tokens. We compared them against general-domain and other domain-specific models for Spanish on three clinical NER tasks. As main results, our models are superior across the NER tasks, rendering them more convenient for clinical NLP applications. Furthermore, our findings indicate that when enough data is available, pre-training from scratch is better than continual pre-training when tested on clinical tasks, raising an exciting research question about which approach is optimal. Our models and fine-tuning scripts are publicly available at HuggingFace and GitHub.",
  "full_text": "Proceedings of the BioNLP 2022 workshop, Dublin, Ireland, pages 193–199\nMay 26, 2022. ©2022 Association for Computational Linguistics\n193\nPre-trained Biomedical Language Models for Clinical NLP in Spanish\nCasimiro Pio Carrino, Joan Llop, Marc Pàmies, Asier Gutiérrez-Fandiño\nJordi Armengol-Estapé, Joaquín Silveira-Ocampo, Alfonso Valencia\nAitor Gonzalez-Agirre and Marta Villegas\nBarcelona Supercomputing Center\nmarta.villegas@bsc.es\nAbstract\nThis work presents the first large-scale biomed-\nical Spanish language models trained from\nscratch, using large biomedical corpora con-\nsisting of a total of 1.1B tokens and an EHR\ncorpus of 95M tokens. We compared them\nagainst general-domain and other domain-\nspecific models for Spanish on three clinical\nNER tasks. As main results, our models are\nsuperior across the NER tasks, rendering them\nmore convenient for clinical NLP applications.\nFurthermore, our findings indicate that when\nenough data is available, pre-training from\nscratch is better than continual pre-training\nwhen tested on clinical tasks, raising an excit-\ning research question about which approach is\noptimal. Our models and fine-tuning scripts are\npublicly available at HuggingFace and GitHub.\n1 Introduction and Background\nThe success of Transformer-based models in the\ngeneral domain (Devlin et al., 2019) soon en-\ncouraged the development of language models for\ndomain-specific scenarios (Chalkidis et al., 2020;\nGutiérrez-Fandiño et al., 2021; Tai et al., 2020;\nAraci, 2019; Lee and Hsiang, 2019). Specifically,\nin the biomedical domain, there has been a prolif-\neration of models (Peng et al., 2019; Beltagy et al.,\n2019; Alsentzer et al., 2019; Gu et al., 2021) since\nthe first BioBERT (Lee et al., 2019) model was\npublished. Unfortunately, there is still a significant\nlack of biomedical and clinical models in languages\nother than English, despite the increasing efforts of\nthe NLP community (Névéol et al., 2014; Schnei-\nder et al., 2020). Consequently, general-domain\npre-trained language models supporting Spanish,\nsuch as mBERT (Devlin et al., 2019) and BETO\n(Cañete et al., 2020), have been often used as a\nproxy to build domain-specific systems in the ab-\nsence of genuine alternatives. For instance, Sun\nand Yang (2019) used mBERT and BioBERT on\nthe PharmaCoNER (Gonzalez-Agirre et al., 2019)\ndataset, using a fine-tuning strategy aimed to maxi-\nmize the results.\nVery recently, new pre-trained clinical language\nmodels for Spanish have been published (López-\nGarcía et al., 2021) by further pre-training the\nmBERT, BETO and XLM-RoBERTa (Conneau\net al., 2020) models with a corpus of Spanish clini-\ncal cases with about 64M tokens. In our work, we\ngo one step further to address the language gap for\nSpanish and train two Transformer-based language\nmodels from scratch. We employed biomedical and\nclinical corpora (including clinical texts) gathered\nby ourselves. We evaluated our models with three\ndifferent Named Entity Recognition (NER) tasks,\nsince NER constitutes a core task in many clinical\nNLP scenarios. They obtained significant gains\nover the general-domain models, and matched or\noutperformed the domain-specific models in all\ntasks.\n2 Corpora\nWe built two corpora of very different sizes and\nnature: an Electronic Health Record (EHR) corpus\nand a biomedical one. The EHR corpuscontains\n95M tokens from more than 514k clinical docu-\nments (including discharge reports, clinical course\nnotes and X-ray reports). The biomedical corpus\nincludes Spanish data from a variety of sources\nfor a total of 1.1B tokens across 2,5M documents,\nnamely:\n• Medical crawler:1 Crawler of more than\n3,000 URLs belonging to Spanish biomedi-\ncal and health domains (Carrino et al., 2021).\n• Clinical cases misc.: A miscellany of medical\ncontent, essentially clinical cases. Note that a\nclinical case report is different from a scien-\ntific publication where medical practitioners\nshare patient cases and it is also different from\na clinical note or document.\n1https://zenodo.org/record/4561970\n194\n• Scielo:2 Scientific publications written in\nSpanish crawled from the Spanish SciELO\nserver in 2017.\n• BARR2 Background:3 Biomedical Abbrevi-\nation Recognition and Resolution (BARR2)\ncontaining Spanish clinical case study sec-\ntions from a variety of clinical disciplines.\n• Wikipedia (Life Sciences): Wikipedia ar-\nticles crawled on 04/01/2021 with the\nWikipedia API python library4 starting from\nthe \"Ciencias_de_la_vida\" category up to a\nmaximum of 5 subcategories. Multiple links\nto the same article are discarded to avoid re-\npeated content.\n• Patents: Google Patent in Medical Domain\nfor Spain (Spanish). The accepted codes\n(Medical Domain) for JSON files of patents\nare: \"A61B\", \"A61C\",\"A61F\", \"A61H\",\n\"A61K\", \"A61L\",\"A61M\", and \"A61P\".\n• EMEA:5 Spanish-side documents extracted\nfrom parallel corpora made out of PDF docu-\nments from the European Medicines Agency.\n• Mespen (MedlinePlus):6 Spanish-side arti-\ncles extracted from a collection of Spanish-\nEnglish parallel corpus consisting of biomedi-\ncal scientific literature. The collection of par-\nallel resources are aggregated from the Med-\nlinePlus source.\n• PubMed: Open-access Spanish abstracts\nfrom the PubMed repository crawled in 2017.\nFor each biomedical resource, we applied a\ncleaning pipeline with customized operations de-\nsigned to read data in different formats, split it into\nsentences, detect the language, remove noisy and\nill-formed sentences, deduplicate and eventually\noutput the data with their original document bound-\naries. Finally, to remove repetitive content, we\nconcatenated the entire corpus and deduplicated it\nagain, obtaining about 1.1B words. These prepro-\ncessing steps were applied to all data except the\nEHR corpus, which was left in its original form.\nTable 1 shows detailed statistics of each component\nof the corpus.\n2https://zenodo.org/record/2541681\n3https://temu.bsc.es/BARR2/downloads/\nbackground_set.raw_text.tar.bz2\n4https://github.com/martin-majlis/\nWikipedia-API/\n5http://opus.nlpl.eu/download.php?f=\nEMEA/v3/moses/en-es.txt.zip\n6https://zenodo.org/record/3562536\nSource No. tokens\nMedical crawler 903,558,136\nClinical cases misc. 102,855,267\nEHRs documents∗ 95,267,204\nScielo 60,007,289\nBARR2 Background 24,516,442\nWikipedia (Life Sciences) 13,890,501\nPatents 13,463,387\nEMEA 5,377,448\nMespen (MedlinePlus) 4,166,077\nPubMed 1,858,966\nTable 1: List of individual sources in the training cor-\npora. The number of tokens refers to white-spaced\ntokens on cleaned untokenized text. Documents from\nthe EHR corpus are marked with an asterisk.\n3 Models Pre-training\nThe models presented in this work were pre-trained\nfrom scratch employing a RoBERTa (Liu et al.,\n2019) base model with 12 self-attention layers. Fol-\nlowing the original training, we only used Masked\nLanguage Modeling (MLM) as the pre-training ob-\njective with Subword Masking (SWM), as in (Liu\net al., 2019).\nWe tokenized the training corpus with the Byte-\nLevel BPE algorithm (Radford et al., 2019), em-\nployed in the original RoBERTa, and learned a\nvocabulary of 50,262 tokens.\nWe run the training for 48 hours on 16 NVIDIA\nV100 GPUs of 16GB VRAM, using Adam opti-\nmizer (Kingma and Ba, 2015) with a peak learn-\ning rate of 0.0005, 10,000 warm-up steps and an\neffective batch size of 2,048 sentences. 7 Other\nhyper-parameters were left in their default values\nas in the original RoBERTa training configuration.\nTraining was performed at the document level, pre-\nserving document boundaries. 8 We performed a\ntrain-validation split based on the number of doc-\numents, choosing a total of 2,000 documents for\nthe validation set, corresponding to less than 1% of\nthe entire corpus’ documents. We then select the\nmodel with the lowest perplexity on the validation\nset as the best model.\nWe used the corpora described in the pre-\nvious section to produce two RoBERTa mod-\nels: a biomedical language model training\n7Through gradient accumulation as implemented in\nFairseq (Ott et al., 2019)\n8We believe document-level training may be crucial to\npromote the modelling of long-range dependencies and push\nthe model towards the comprehension of entire documents.\n195\nonly with the so-called biomedical resources\n(bsc-bio-es),9 and a BIO-EHR language\nmodel that uses both the biomedical and EHR cor-\npus (bsc-bio-ehr-es).10 We trained the latter\nmodel, the biomedical-EHR, to assess if adding a\nrelatively small EHR data to a large-scale corpora\nhas a positive impact on real-world clinical NLP\ntasks.\n4 NER Fine-tuning\nWe tested and evaluated our models by fine-tuning\nthe NER task, a key component of information ex-\ntraction tasks in the clinical domain. Indeed, we\nused it as a testbed to evaluate the effectiveness of\nour pre-trained models. Following the usual fine-\ntuning method, employed both for general-domain\nmodels (Devlin et al., 2019; Liu et al., 2019) and\ndomain-specific ones (Lee et al., 2019), we added a\nstandard linear layer as a token classification head,\nand the BIO tagging schema (Sang and Buchholz,\n2000) to solve the NER tasks. During fine-tuning,\nboth the pre-trained model and the classification\nlayer’s parameters are learned with stochastic gra-\ndient descent. We used an Adam (Kingma and\nBa, 2015) optimizer and searched for an optimal\nlearning rate out of [8e-6, 1e-5, 2e-5, 3e-5, 5e-\n5] with linear decay and no warm-up steps. We\nused a batch size of 32 sequences with a maximum\nlength of 512 tokens and a gradient accumulation\nof 2 steps, resulting in a total batch size of 64.\nWe trained each configuration using three random\nseeds. The rest of hyper-parameters were left to the\ndefault values of HuggingFace’s codebase (Wolf\net al., 2019). The complete list of hyper-parameter\nvalues is displayed in Appendix B.\nWe applied this fine-tuning strategy to three dif-\nferent NER datasets. The first two use annotations\non curated medical data (clinical cases extracted\nfrom medical literature), whereas the last one\nuses medical records from the ICTUSnet project.11\nMore details are given below.\nPharmaCoNER (Gonzalez-Agirre et al., 2019)\nis a track on chemical and drug mention recognition\nfrom Spanish medical texts. The authors compiled\na manually classified collection of clinical case\nreport sections derived from open access Spanish\nmedical publications, named the Spanish Clinical\n9https://huggingface.co/PlanTL-GOB-ES/\nbsc-bio-es\n10https://huggingface.co/PlanTL-GOB-ES/\nbsc-bio-ehr-es\n11https://ictusnet-sudoe.eu/es/\nCase Corpus (SPACCC). The corpus contained a\ntotal of 1,000 clinical cases and 396,988 words and\nwas manually annotated, with a total of 7,624 entity\nmentions, corresponding to four different mention\ntypes.12\nCANTEMIST (Miranda-Escalada et al., 2020)\nis a shared task focused on named entity recogni-\ntion of tumor morphology, in Spanish. The CAN-\nTEMIST corpus13 is a collection of 1,301 oncolog-\nical case reports written in Spanish, with a total of\n63,016 sentences and 1,093,501 tokens.\nThe ICTUSnet dataset consists of 1,006 hospi-\ntal discharge reports of patients admitted for stroke\nfrom 18 different Spanish hospitals. It contains\nmore than 79,000 annotations for 51 different vari-\nables. The dataset is part of the ICTUSnet project,\nwhose main objective was the development of an\ninformation extraction system to support domain\nexperts when identifying relevant information in\ndischarge reports.\nFinally, we remark that our main goal is a head-\nto-head comparison between different models to\nassess the best model pre-training choice. We were\nnot aiming at maximizing results on the NER tasks\nand therefore we decided not to use sophisticated\nclassification layers that might improve the perfor-\nmances, such as Conditional Random Field (Laf-\nferty et al., 2001) layers on top of Bidirectional\nLong Short-Term Memory Recurrent Networks\n(Panchendrarajan and Amaresan, 2018). We ar-\ngue that a simpler token classification layer better\nevaluates the quality of model representations than\na task-specific layer. Unlike Sun and Yang (2019),\nwhere authors fine-tuned for 200 epochs (obtain-\ning the best results using 100 epochs), we limit\nthe fine-tuning to 20 epochs, and we do not merge\nthe train and development sets in order to improve\nthe results. We consider that fine-tuning for 200\nepochs goes against the pre-training/fine-tuning\nphilosophy that states that fine-tuning should be\na relatively inexpensive step (Devlin et al., 2019),\nand also that fine-tuning for less epochs evaluates\nbetter the pre-training strategy.\n5 Evaluation and Results\nEach fine-tuning was executed on 4 NVIDIA V100\nGPUs of 16GB VRAM. It took around 0.5, 1 and\n12For a detailed description, see https://temu.bsc.\nes/pharmaconer/\n13CANTEMIST corpus: https://doi.org/10.\n5281/zenodo.3878178\n196\nAverage of all configurations Best on development set\nTask Model F1 Precision Recall F1 LR Epoch\nPharmaCoNER\nbsc-bio-es* 0.89070.01 0.87360.01 0.90850.00 0.89390.01 5e-5 15\nbsc-bio-ehr-es* 0.89130.01 0.87580.01 0.90730.01 0.89540.01 3e-5 10\nXLM-R-Galén 0.87540.01 0.85910.02 0.89240.01 0.88830.00 5e-5 15\nBETO-Galén 0.85370.02 0.83990.02 0.86800.01 0.87410.01 5e-5 20\nmBERT-Galén 0.85940.01 0.84690.02 0.87220.01 0.87600.00 5e-5 15\nmBERT 0.86710.01 0.85400.02 0.88090.01 0.87290.00 3e-5 13\nBioBERT 0.85450.01 0.85020.01 0.85900.01 0.85330.01 2e-5 12\nroberta-base-bne 0.84740.02 0.84300.02 0.85200.02 0.86800.01 5e-5 13\nCANTEMIST\nbsc-bio-es* 0.82200.01 0.79390.02 0.85220.01 0.83510.00 5e-5 20\nbsc-bio-ehr-es* 0.83400.01 0.81410.01 0.85510.01 0.84490.00 5e-5 20\nXLM-R-Galén 0.80780.02 0.77550.02 0.84310.01 0.82590.00 5e-5 15\nBETO-Galén 0.81530.01 0.79330.02 0.83870.01 0.83320.01 5e-5 20\nmBERT-Galén 0.81680.01 0.79190.02 0.84350.01 0.83040.00 5e-5 20\nmBERT 0.81160.01 0.79230.02 0.83190.01 0.82570.00 5e-5 16\nBioBERT 0.80700.01 0.78480.02 0.83060.01 0.82190.00 5e-5 20\nroberta-base-bne 0.78750.03 0.77330.03 0.80230.02 0.81610.00 5e-5 15\nICTUSnet\nbsc-bio-es* 0.87270.01 0.83590.01 0.91310.01 0.88040.00 5e-5 19\nbsc-bio-ehr-es* 0.87560.00 0.84180.01 0.91220.00 0.87810.00 2e-5 18\nXLM-R-Galén 0.87160.01 0.83750.01 0.90870.01 0.88090.00 5e-5 17\nBETO-Galén 0.84980.01 0.82260.01 0.87910.01 0.85510.00 5e-5 20\nmBERT-Galén 0.85090.01 0.82190.01 0.88200.01 0.85760.00 5e-5 17\nmBERT 0.86310.01 0.83010.01 0.89890.01 0.86460.01 2e-5 20\nBioBERT 0.85210.00 0.81320.01 0.89500.01 0.85030.00 2e-5 16\nroberta-base-bne 0.86770.01 0.84560.01 0.89100.01 0.87690.00 5e-5 18\nTable 2: Fine-tuning results of the models for each dataset on the test set. In bold, the best results for metric and\ntask. Subscript numbers indicate the standard deviations. Our models are marked with an asterisk.\n2 hours to complete the PharmaCoNER, CAN-\nTEMIST and ICTUSnet tasks, respectively.\nWe then report the overall best scores on the\ntest set, obtained by using the best model’s hyper-\nparameters on the development set for each dataset\n(the standard deviation is computed using all the\nseeds for that configuration). Finally, we also report\nthe models’ average scores and standard deviations\nby computing statistics across all the seeds and the\nlearning rates used for each dataset. The average\nscores are helpful to indicate which model is more\nrobust to the variation of hyper-parameters, which\nare the learning rate and initial seed in our case. A\nhigher average score and a smaller standard devi-\nation minimizes the risk of obtaining poor results\nwhen performing an extensive hyper-parameter\nsearch is not feasible.\nWe compared our models with a general-\ndomain Spanish model ( roberta-base-bne)\n(Gutiérrez-Fandiño et al., 2022), a general-domain\nmultilingual model that supports Spanish (mBERT),\na domain-specific English model (BioBERT), and\nthree domain-specific models based on continual\npre-training: mBERT-Galén (based on mBERT),\nBETO-Galén (based on BETO, a general-domain\nSpanish model), and XLM-R-Galén (based on\nXLM-RoBERTa, a general-domain multilingual\nmodel supporting Spanish). The results are shown\nin Table 2. The last two columns report the learning\nrate and epoch in which the best configuration on\nthe development set was achieved\nOur models obtained significantly better perfor-\nmances than the general-domain models, namely\nmBERT and roberta-base-bne. Compared\nto the domain-specific Galén models, our aver-\nage models’ scores surpassed them on the clinical\nNER tasks. However, when looking at the best on\ndevelopment score on the ICTUSnet dataset, the\nXLM-R-Galén model outperformed our models.\nWe also highlight that our models exhibit smaller\nstandard deviations. This makes them more robust\nand a good option if not enough computational\n197\nresources are available to experiment with the dif-\nferent hyper-parameter configurations.\n6 Conclusions and Future Work\nThis work presents the first large-scale biomedical\nSpanish language models trained from scratch, us-\ning a large biomedical corpora for a total of 1.1B\ntokens and an EHR corpus of 95M tokens. We\nfine-tuned the models on three clinical NER tasks\nand compared them with both general-domain and\nother available Spanish clinical models. The re-\nsults show the superiority of our models across the\nNER tasks, making them competitive candidates\nfor clinical NLP applications. Our findings demon-\nstrate the benefits of pre-training from scratch, as\nseen in Gu et al. (2021). Regarding continual\npre-training, the benefits are not clear, especially\nwhen continual pre-training is performed with\nsmall data, as in the case of the mBERT-Galén,\nXLM-R-Galén, and BETO-Galén (note that\nmBERT outperforms mBERT-Galén in two out\nof three tasks). Our work raises exciting research\nquestions about which pre-training approach is op-\ntimal to tackle challenging clinical NLP tasks. We\nwill devote future efforts to address the previous\nquestion in detail by providing new models based\non continual pre-training and extending our evalua-\ntion setting to a diverse range of tasks.\n7 Data Availability\nOur work encourages the development of Clini-\ncal and Biomedical NLP applications for Spanish.\nTherefore, we released our pre-trained models and\nthe best on dev set fine-tuned models under the\nApache License 2.0 in the HuggingFace models\nhub under the following links:\nPre-trained models\n• bsc-bio-es\n• bsc-bio-ehr-es\nFine-tuned models\n• bsc-bio-ehr-es-pharmaconer\n• bsc-bio-ehr-es-cantemist\nMoreover, to guarantee reproducibility, we\nshare the script used to fine-tuned our pre-\ntrained model in the official GitHub repository:\nhttps://github.com/PlanTL-GOB-ES/\nlm-biomedical-clinical-es.\nAcknowledgements\nThis work was funded by the Spanish State Secre-\ntariat for Digitalization and Artificial Intelligence\n(SEDIA) within the framework of the Plan-TL.14\n14https://plantl.mineco.gob.es/Paginas/\nindex.aspx\n198\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nDogu Araci. 2019. Finbert: Financial sentiment\nanalysis with pre-trained language models. CoRR,\nabs/1908.10063.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nCasimiro Pio Carrino, Jordi Armengol-Estapé, Ona\nde Gibert Bonet, Asier Gutiérrez-Fandiño, Aitor\nGonzalez-Agirre, Martin Krallinger, and Marta Vil-\nlegas. 2021. Spanish biomedical crawled corpus:\nA large, diverse dataset for spanish biomedical lan-\nguage models.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAitor Gonzalez-Agirre, Montserrat Marimon, Ander In-\ntxaurrondo, Obdulia Rabal, Marta Villegas, and Mar-\ntin Krallinger. 2019. PharmaCoNER: Pharmacologi-\ncal substances, compounds and proteins named entity\nrecognition track. In Proceedings of The 5th Work-\nshop on BioNLP Open Shared Tasks , pages 1–10,\nHong Kong, China. Association for Computational\nLinguistics.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Trans. Comput. Healthcare,\n3(1).\nAsier Gutiérrez-Fandiño, Jordi Armengol-Estapé, Aitor\nGonzalez-Agirre, and Marta Villegas. 2021. Spanish\nlegalese language model and corpora.\nAsier Gutiérrez-Fandiño, Jordi Armengol-Estapé, Marc\nPàmies, Joan Llop-Palao, Joaquin Silveira-Ocampo,\nCasimiro Pio Carrino, Carme Armentano-Oller, Car-\nlos Rodriguez-Penagos, Aitor Gonzalez-Agirre, and\nMarta Villegas. 2022. Maria: Spanish language mod-\nels. Procesamiento del Lenguaje Natural, 68(0):39–\n60.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random fields:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth In-\nternational Conference on Machine Learning, ICML\n’01, page 282–289, San Francisco, CA, USA. Morgan\nKaufmann Publishers Inc.\nJieh-Sheng Lee and Jieh Hsiang. 2019. Patentbert:\nPatent classification with fine-tuning a pre-trained\nBERT model. CoRR, abs/1906.02124.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nGuillermo López-García, José M Jerez, Nuria Ribelles,\nEmilio Alba, and Francisco J Veredas. 2021. Trans-\nformers for clinical coding in spanish. IEEE Access,\n9:72387–72397.\nA Miranda-Escalada, E Farré, and M Krallinger. 2020.\nNamed entity recognition, concept normalization\nand clinical coding: Overview of the cantemist\ntrack for cancer text mining in spanish, corpus,\nguidelines, methods and results. In Proceedings of\nthe Iberian Languages Evaluation Forum (IberLEF\n2020), CEUR Workshop Proceedings.\n199\nAurélie Névéol, H. Dalianis, G. Savova, and Pierre\nZweigenbaum. 2014. Clinical natural language pro-\ncessing in languages other than english: opportunities\nand challenges. Journal of Biomedical Semantics, 9.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nRrubaa Panchendrarajan and Aravindh Amaresan. 2018.\nBidirectional LSTM-CRF for named entity recogni-\ntion. In Proceedings of the 32nd Pacific Asia Con-\nference on Language, Information and Computation,\nHong Kong. Association for Computational Linguis-\ntics.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65, Florence,\nItaly. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nErik F. Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the conll-2000 shared task: Chunking.\nCoRR, cs.CL/0009008.\nElisa Terumi Rubel Schneider, João Vitor Andrioli\nde Souza, Julien Knafou, Lucas Emanuel Silva e\nOliveira, Jenny Copara, Yohan Bonescki Gumiel,\nLucas Ferro Antunes de Oliveira, Emerson Cabr-\nera Paraiso, Douglas Teodoro, and Cláudia Maria\nCabral Moro Barra. 2020. BioBERTpt - a Por-\ntuguese neural language model for clinical named\nentity recognition. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n65–72, Online. Association for Computational Lin-\nguistics.\nCong Sun and Zhihao Yang. 2019. Transfer learning in\nbiomedical named entity recognition: An evaluation\nof BERT in the PharmaCoNER task. In Proceedings\nof The 5th Workshop on BioNLP Open Shared Tasks,\npages 100–104, Hong Kong, China. Association for\nComputational Linguistics.\nWen Tai, H. T. Kung, Xin Dong, Marcus Comiter,\nand Chang-Fu Kuo. 2020. exBERT: Extending pre-\ntrained models with domain-specific vocabulary un-\nder constrained training resources. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1433–1439, Online. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. CoRR,\nabs/1910.03771.\nA Pre-training Hyper-parameters\nThe hyper-parameters used for pre-training our\nmodels are shown in Table 3.\nHyper-parameter Value\nNumber of Layers 12\nHidden size 768\nFNN inner hidden size 3072\nAttention Heads 12\nAttention Head size 64\nDropout 0.1\nAttention Dropout 0.1\nWarmup Steps 10k\nPeak Learning Rate 5e-4\nBatch Size 2,048\nWeight Decay 0.01\nMax Steps 125k\nLearning Rate Decay Linear\nAdam ϵ 1e-6\nAdam β1 0.9\nAdam β2 0.98\nGradient Clipping 0.0\nTable 3: Hyper-parameters used for pre-training.\nB Fine-tuning Hyper-parameters\nThe hyper-parameters used for fine-tuning the mod-\nels on various tasks are shown in Table 4.\nHyper-parameter Value\nLearning Rates {0.8, 1, 2, 3, 5}e-5\nLearning Rate Decay Linear\nWarmup Steps 0\nBatch Size 64\nWeight Decay 0.0\nMax. Training Epochs 20\nAdam ϵ 1e-8\nAdam β1 0.9\nAdam β2 0.999\nGradient Clipping 1.0\nTable 4: Hyper-parameters used for fine-tuning.",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.6288671493530273
    },
    {
      "name": "Computer science",
      "score": 0.593366265296936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5227106809616089
    },
    {
      "name": "Linguistics",
      "score": 0.42921045422554016
    },
    {
      "name": "Library science",
      "score": 0.3590506911277771
    },
    {
      "name": "Philosophy",
      "score": 0.05807015299797058
    }
  ]
}