{
  "title": "Large language model trained on clinical oncology data predicts cancer progression",
  "url": "https://openalex.org/W4411919356",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2111355907",
      "name": "MengLei Zhu",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2101455882",
      "name": "Hui Lin",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2121146405",
      "name": "Jue Jiang",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2553113079",
      "name": "Abbas J. Jinia",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2137910022",
      "name": "Justin Jee",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A1850367186",
      "name": "Karl Pichotta",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2187465943",
      "name": "Michele Waters",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2789436514",
      "name": "Doori Rose",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2134859304",
      "name": "Nikolaus Schultz",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A3027568251",
      "name": "Sulov Chalise",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2780285288",
      "name": "Lohit Valleru",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A1843965135",
      "name": "Olivier Morin",
      "affiliations": [
        "University of California, San Francisco"
      ]
    },
    {
      "id": "https://openalex.org/A2109125796",
      "name": "Jean Moran",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2807456698",
      "name": "Joseph O. Deasy",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A5114104734",
      "name": "Shirin Pilai",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2294234863",
      "name": "Chelsea Nichols",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A3176200361",
      "name": "Gregory Riely",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2097863730",
      "name": "Lior Z. Braunstein",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2107298206",
      "name": "Anyi Li",
      "affiliations": [
        "Memorial Sloan Kettering Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2111355907",
      "name": "MengLei Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101455882",
      "name": "Hui Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121146405",
      "name": "Jue Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553113079",
      "name": "Abbas J. Jinia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137910022",
      "name": "Justin Jee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1850367186",
      "name": "Karl Pichotta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2187465943",
      "name": "Michele Waters",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2789436514",
      "name": "Doori Rose",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134859304",
      "name": "Nikolaus Schultz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3027568251",
      "name": "Sulov Chalise",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2780285288",
      "name": "Lohit Valleru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1843965135",
      "name": "Olivier Morin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109125796",
      "name": "Jean Moran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2807456698",
      "name": "Joseph O. Deasy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114104734",
      "name": "Shirin Pilai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294234863",
      "name": "Chelsea Nichols",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176200361",
      "name": "Gregory Riely",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097863730",
      "name": "Lior Z. Braunstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107298206",
      "name": "Anyi Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4324308091",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319341091",
    "https://openalex.org/W105927013",
    "https://openalex.org/W4386117070",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4391940656",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4386080541",
    "https://openalex.org/W4362541523",
    "https://openalex.org/W2116760308",
    "https://openalex.org/W4236275961",
    "https://openalex.org/W6629195898",
    "https://openalex.org/W3189476076",
    "https://openalex.org/W2595840341",
    "https://openalex.org/W4387596213",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2913352150",
    "https://openalex.org/W4213261164",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4400531953",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W151377110",
    "https://openalex.org/W4225816390",
    "https://openalex.org/W4321459182"
  ],
  "abstract": "Abstract Subspecialty knowledge barriers have limited the adoption of large language models (LLMs) in oncology. We introduce Woollie, an open-source, oncology-specific LLM trained on real-world data from Memorial Sloan Kettering Cancer Center (MSK) across lung, breast, prostate, pancreatic, and colorectal cancers, with external validation using University of California, San Francisco (UCSF) data. Woollie surpasses ChatGPT in medical benchmarks and excels in eight non-medical benchmarks. Analyzing 39,319 radiology impression notes from 4002 patients, it achieved an overall area under the receiver operating characteristic curve (AUROC) of 0.97 for cancer progression prediction on MSK data, including a notable 0.98 AUROC for pancreatic cancer. On UCSF data, it achieved an overall AUROC of 0.88, excelling in lung cancer detection with an AUROC of 0.95. As the first oncology specific LLM validated across institutions, Woollie demonstrates high accuracy and consistency across cancer types, underscoring its potential to enhance cancer progression analysis.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01780-2\nLarge language model trained on clinical\noncology data predicts cancer\nprogression\nCheck for updates\nMenglei Zhu1,3, Hui Lin 2,3, Jue Jiang1, Abbas J. Jinia1,J u s t i nJ e e1, Karl Pichotta1, Michele Waters 1,\nDoori Rose1,N i k o l a u sS c h u l t z1, Sulov Chalise1, Lohit Valleru1, Olivier Morin 2,J e a nM o r a n1,\nJoseph O. Deasy1, Shirin Pilai1, Chelsea Nichols1, Gregory Riely1,L i o rZ .B r a u n s t e i n1 & Anyi Li 1\nSubspecialty knowledge barriers have limited the adoption of large language models (LLMs) in\noncology. We introduce Woollie, an open-source, oncology-speciﬁc LLM trained on real-world data\nfrom Memorial Sloan Kettering Cancer Center (MSK) across lung, breast, prostate, pancreatic, and\ncolorectal cancers, with external validation using University of California, San Francisco (UCSF) data.\nWoollie surpasses ChatGPT in medical benchmarks and excels in eight non-medical benchmarks.\nAnalyzing 39,319 radiology impression notes from 4002 patients, it achieved an overall area under the\nreceiver operating characteristic curve (AUROC) of 0.97 for cancer progression prediction on MSK\ndata, including a notable 0.98 AUROC for pancreatic cancer. On UCSF data, it achieved an overall\nAUROC of 0.88, excelling in lung cancer detection with an AUROC of 0.95. As theﬁrst oncology\nspeciﬁc LLM validated across institutions, Woollie demonstrates high accuracy and consistency\nacross cancer types, underscoring its potential to enhance cancer progression analysis.\nThe advent of ChatGPT has sparkedwidespread excitement about the\npotential for Artiﬁcial Intelligence (AI) to augment various domains of\nhuman endeavor1–3. Initial explorations of large language models (LLMs)\nsuch as ChatGPT in healthcare4,5 have yielded promising results across\nnumerous medicalﬁelds6–8. Oncology, in particular, relies heavily on the\nnuanced understanding of clinical histories and disease progression. The\neffectiveness of treatments is closely linked to how cancer responds, as\nobserved through radiological or pathological assessments. These critical\ndata points are often stored as real-world data (RWD) in non-standardized\nand unstructured formats, making them challenging to access and interpret.\nThe capability of LLMs to decipher and analyze such complex, unstructured\ndata holds transformative potential for improving cancer care, beneﬁting\nboth clinicians, researchers, and patients.\nPrivacy concerns have hampered the adoption of closed-source LLMs\nin clinical settings, compounded by studies indicating notable performance\ndeﬁciencies in these models within healthcare environments\n9.M o r e o v e r ,t h e\nintegration of general domain LLMs into healthcare is challenged by the\nnecessity for domain-speciﬁck n o w l e d g e ,ﬁne-tuning with proprietary\nclinical data, and the absence of evaluations by domain experts. Conversely,\nthe rise of open-source versions of LLM, inspired by recent advancements\nlike Google’sF L A N - T 5\n10,M E T A’sL l a m a11,a n dL l a m a212,h a si g n i t e d\ninterest across variousﬁelds. These open-source models mitigate privacy\nconcerns but face hurdles in adapting andﬁne-tuning for oncology due to\nthe limited specialized domain background and knowledge from the open-\nsource community. Consequently, the feasibility of applying open-source\nLLMs directly in clinical oncology practice remains uncertain, indicating a\nneed for further exploration and development to bridge these gaps.\nIn the medical domain, numerous studies have demonstrated the\nimproved performance of domain-speciﬁc foundation LLMs, including\nBioMedLM\n13,B i o G P T14, GatorTron15,M e d P a L M16,M e d P a l M 217,a n d\nBioMistral18. These high performance claims are typically based on standard\nmedical benchmarks such as PubMedQA19 and USMLE6, whose training\ndatasets are often available during the model’s pre-training phase. However,\nachieving high scores on these benchmarks does not necessarily equate to\nincreased effectiveness when using RWD in healthcare practices, especially in\nthe oncology sector. Like their general domain counterparts, these specialized\nmedical models frequently lack in-depth expertise in oncology and have not\nbeen validated across multiple institutions. This absence of cross-institutional\nvalidation restricts their practical useand trust in oncological applications.\nAddressing patient privacy concerns, the need for specialized oncology\nknowledge, the complexity of using real-world data, and the requirement of\ncross-institutional validation, we created Woollie— a dedicated LLM for\noncological radiology reports, based on the open-source Llama (Llama 1)\n11\nmodels from META. We extensively evaluated Woollie in various\n1Memorial Sloan Kettering Cancer Center, New York, NY, USA.2University of California San Francisco, San Francisco, CA, USA.3These authors contributed\nequally: Menglei Zhu, Hui Lin. e-mail: braunstl@mskcc.org; lia5@mskcc.org\nnpj Digital Medicine|           (2025) 8:397 1\n1234567890():,;\n1234567890():,;\nconﬁgurations, from a smaller 7 billion (B) parameter model to larger, more\nresource-intensive 13B, 33B, and 65B parameter models. Our strategy\ninvolved a layered approach to integrating broad oncology knowledge by\nreﬁning Woollie’s analytical skills through a stacked alignment process. This\nin-depth investigation included the alignment and evaluation of fourteen\nunique Woollie models against established benchmarks, such as medical\ndomain tests, including PubMedQA\n19,M e d M C Q A20, and USMLE17.W e\nalso assessed how different types of prompts affected the model’st u n i n g .\nFurthermore, we delved into scaling laws to analyze how model size impacts\nits effectiveness, providing critical insights into how various Woollie models\nperform within the oncology domain.\nAn essential contribution of this work lies in leveraging an advanced,\nlarge-scale LLM to analyze real-world clinical oncology data. Our study\nutilized a speciﬁc RWD from Memorial Sloan Kettering Cancer Center\n(MSK), enabling us to test Woollie’sp r oﬁciency in oncology-related tasks\nrigorously. By tapping into MSK’s extensive oncologic domain knowledge,\nwe thoroughly evaluated Woollie’s ability to interpret radiology reports,\nexamining 38,719 radiology impressions curated by radiologists from 3402\npatients acrossﬁve types of cancer: lung, breast, pancreatic, prostate, and\ncolorectal. For oncologists, the impression section of radiology reports\nprovides comprehensive insights into tumor location, size, and changes over\ntime— critical factors for determining cancer staging and guiding treatment\nstrategies. Tracking tumor progression through radiology is essential for\nassessing the effectiveness of treatments and guiding further clinical man-\nagement. Leveraging radiology impression notes, we assessed Woollie’s\neffectiveness in monitoring cancer progression and extracting pivotal\ninformation of cancer biology, identifying patterns of metastatic spread\nacross a vast array of medical information.\nA crucial element of this work was demonstrating the model’s cross-\ninstitutional generalizability and applicability, highlighting its relevance in\nvarious healthcare contexts. We extended our validation to include an\nindependent dataset of 600 radiology impressions involving 600 unique\npatients from the University of California, San Francisco (UCSF), focusing\non lung, breast, and prostate cancers— different from those in the MSK\ndataset. This step was essential to conﬁrm Woollie’s ability to consistently\napply its oncology expertise across various settings, establishing its broad\nutility and effectiveness in diverse healthcare environments.\nResults\nPerformance assessment of Woollie models in standard non-\nmedical benchmarks with improved outcomes in medical\ndomains\nWe trained a family of fourteen distinct Woollie models using a stacked\nalignment and ﬁne-tuning process. This process began with pretrained\nopen-source Llama models (Fig.1a) and progressively built upon each\niteration using a variety of datasets (Fig.1b, c). The performance of Woollie\nmodels and baseline Llama models was assessed across 11 standard\nbenchmarks in four different sizes— 7B, 13B, 33B, and 65B parameters,\nresulting in a total of 176 tests. We assessed the models’performance by\ncalculating accuracy, precision, F1 scores, and Matthews Correlation\nCoefﬁcients (MCC) on the test datasets. For standard benchmarks involving\nmultiple classes, we computed the macro-average of all metrics except\naccuracy (Supplementary Table 1). Accuracy was chosen as the primary\nmetric for comparison, as most benchmarks feature balanced classes with\nequal importance, and the primary focus is on the model’s ability to predict\nthe correct classes. Additionally, accuracy is widely used in open leader-\nboards for open-source LLMs, makingit easier for performance compar-\nisons. Other metrics show consistent trends, aligning with the overall model\nperformance. When compared to Llama models, Woollie models exhibited\nsuperior capabilities in logic, conversation, and reasoning tasks. In non-\nmedical benchmarks such as OpenbookQA\n21,L o g i Q A22, and MMLU23,\nparticularly in conversation-centric tests like COQA24 and HellaSWAG25,\nthe larger Woollie models (13B, 33B, and 65B) outperformed their Llama\nequivalents, with the 65B model showing the most signiﬁcant advantage\n(Fig. 2a, Supplementary Fig. 1a). For the COQA benchmark, the Woollie\n65B model achieved an accuracy of 0.78 compared to 0.74 for the Llama 65B\nmodel, and an F1 score of 0.82 versus 0.77.\nCatastrophic forgetting26, a common issue where an LLM loses pre-\nviously learned knowledge duringﬁne-tuning, is mitigated in Woollie\nmodels through the stacked alignmentstrategy. This approach allows us to\nobserve performance improvements by progressively building on pre-\nviously trained models. A comparison among Llama, Woollie Foundation,\nWoollie Medicine, and Woollie models clearly demonstrates incremental\nperformance improvements across various benchmarks (Fig.2b, Supple-\nmentary Fig. 1b).\nTo better visualize the impact of stacked alignment, we also compared\nmodels trained by simply concatenating all training datasets, labeled as\nWoollie All. While Woollie All outperformed the baseline Llama models,\nsigniﬁcant performance gaps were evident when compared to Woollie\nmodels aligned with the stacked alignment strategy. These gaps were par-\nticularly pronounced in non-medical benchmarks and even more so in\nmedical benchmarks.\nFor instance, in the 33B model evaluations, Woollie 33B achieved an\naccuracy of 0.43 on MMLU, compared to 0.40 for Woollie All 33B. Similarly,\non PubMedQA, Woollie 33B achieved an accuracy of 0.79, signiﬁcantly\noutperforming Woollie All 33B at 0.70. These results suggest that the\nstacked alignment strategy effectively counteracts the problem of cata-\nstrophic forgetting. As the comparison between 7B, 13B, and 33B models\nclearly demonstrated the performance trends, we decided to conclude\ntraining at Woollie All 33B.\nIn the medical domain, the Woollie models signiﬁcantly outperformed\nthe Llama models in tests such as PubMedQA, MedMCQA, and USMLE.\nSpeciﬁcally, the Woollie 65B model marked substantial improvements,\nachieving accuracies of 0.81 on PubMedQA, 0.50 on MedMCQA, and 0.52\non USMLE— exceeding the Llama 65B model’s scores of 0.70, 0.37, and 0.42,\nrespectively (Fig.2a). Notably, the Woollie 65B model’sp e r f o r m a n c eo n\nPubMedQA matched GPT-4’s accuracy of 0.804 and signiﬁcantly surpassed\nChatGPT (3.5-turbo)’s accuracy of 0.716\n5, despite being considerably\nsmaller than the GPT-3 (175B parameters)27 and GPT-428, which is esti-\nmated to have over 1 trillion parameters. These results underscore Woollie’s\nefﬁcacy in general medical contexts and validate the effectiveness of our\nstacked alignment strategy, which adeptly incorporates domain-speciﬁc\nknowledge while maintaining robust performance across broader con-\nversational and reasoning tasks.\nScaling Woollie models and comprehensive comparisons across\nmodel variants\nWe performed a scaling study to evaluate the impact of size, comparing\nperformance across models ranging from 7B to 65B parameters. The study\nincluded Llama, Woollie Foundation, Woollie Medicine, and Woollie\nmodels. Llama showed that performance generally improved with size,\nexcept on the PubMedQA tests, where the accuracy was 0.71 for 7B, 0.67 for\n13B, 0.66 for 33B, and 0.70 for 65B (Supplementary Fig. 1c). After align-\nments, all models conﬁrmed the expected scaling beneﬁts. For example, in\nthe Woollie model, the accuracy on PubMedQA improved with size: 0.72 for\n7B, 0.74 for 13B, 0.80 for 33B, and 0.81 for 65B (Fig.2c).\nThis trend afﬁrmed that models with a more extensive parameter base\nbefore alignment yielded better outcomes, with the performance scaling\nalmost linearly with sizes between 7B and 33B. The trend suggested that the\nalignment’s effectiveness is contingent upon the quality of the baseline\nmodel’sp r e - t r a i n i n g— the more robust the baseline, the better the perfor-\nmance of the aligned models. Nevertheless, a modest performance\nimprovement was noted in this scaling trend when comparing the 33B and\n65B models, both pre-trained on datasets of identical size (1.4 trillion\ntokens). For instance, on the PubMedQA benchmark, the Woollie 33B\nmodel achieved an accuracy of 0.7980, while the 65B model reached 0.8092.\nThis observation indicates that beyond a certain point, the scaling law in\naligned models may be more inﬂuenced by data quality than quantity alone.\nOur assessments concluded with a detailed performance evaluation of\nLlama, Woollie Foundation, Woollie Medicine, and Woollie models,\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 2\nvisually presented through a heatmap ofaccuracies for precise comparative\nanalysis. This heatmap used varying shades of color to indicate performance\nlevels across different test categories, clearly separating results from non-\nmedical and medical domains (Fig.2d). As more medical datasets were\nintegrated into the alignment process, a distinct trend appeared: non-\nmedical performance slightly declined while medical domain proﬁciency\nincreased. This specialization trade-off is depicted in the heatmap through a\ngradient change, where color intensity fades in non-medical domains and\ndeepens in medical domains,illustrating the models’shifting focus and\ncapabilities. Despite the marginal decrease in non-medical domain per-\nformance among the aligned models, their capabilities remained robust and\ncompetitive. Our approach fortiﬁed the model’s foundation and optimized\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 3\nit for oncology-speciﬁc applications, demonstrating improved performance\nupon furtherﬁne-tuning with oncology datasets.\nEvaluation of Woollie models performance on oncologic radi-\nology datasets\nLeveraging Woollie’s strong performance in medical benchmarks, we aim to\nsupport oncologists by monitoring cancer progression and understanding\ndisease pathways. Weﬁne-tuned and tested Woollie’s capability using real-\nworld data of 38,719 radiology impressions from 3401 patients withﬁve\ndifferent cancers: breast, colorectal, lung, pancreatic, and prostate. We\nanalyzed and plotted the sociodemographic distribution of the patient\ncohort included in this dataset (Fig.3a), and listed number of radiology\nimpression notes and the associated number of patients in each cancer (Fig.\n3b). The sociodemographic distribution plot categorizes the data by“Age at\nProcedure,”“Birth Sex,”“Marital Status,”“Race,”“Religion,” and “Ethni-\ncity.” Notably, over 95% of the cohort is aged 40 or older. Approximately\n80% of the patients identify as White, which is slightly higher than the\nnational average of 75% as of 2024. In contrast, 6% of the cohort identify as\nAfrican American, below the nationalaverage of 13%. The representation of\nAsian/Indian individuals is 7.5%, slightly above the national average\n29.\nThese radiology impressions, gathered from MSK30, were thoroughly\nreviewed and labeled by radiologists withﬁve categories indicating cancer\nprogression: “Progressing/Worsening/Enlarging,”“ Stable/No change,”\n“Improving/Responding,”“Not stated/Indeterminate,” and “Mixed” (Fig.\n3c). We simpliﬁed the classiﬁcation into a binary system, grouping“Mixed”\nand “Progressing/Worsening/Enlarging” as positive indicators labeled\n“Progressing,” while all other cases were classiﬁed as negative indicators\nlabeled “Not Progressing.” The proportion of“Not Progressing” labels is\nroughly equal to that of“Progressing” labels, except in lung cancer cases,\nwhich show a slight skew toward“Not Progressing” (Fig. 3d).\nTo gauge the extent of knowledge transfer duringﬁne-tuning, we also\ncompared the performance of non-ﬁnetuned Woollie Foundation, Woollie\nMedicine, Woollie All, Woollie models, as well as baseline Llama models, on\nthis MSK radiology impression dataset, denoted as“rad_imp”(Fig.4a). The\nperformance comparison between the“stacked aligned” Woollie models,\n“not stacked aligned” Woollie All models, and baseline Llama models\nprovides further evidence of the signiﬁcant impact of the alignment strategy\non performance, particularly on proprietary datasets that were not exposed\nduring pretraining. The Woollie All models (7B, 13B, and 33B) demonstrate\nhigher accuracy compared to their corresponding Llama models. However,\ntheir performance remains signiﬁc a n t l yl o w e rt h a nt h a to ft h eW o o l l i e\nmodels. The binary classiﬁcation results indicated that larger models gen-\nerally yield better performance; however, interestingly, the Woollie 65B\nmodel’s accuracy slightly lagged behind the 33B model, with accuracy of\n0.77 versus 0.79 for the 33B model.\nConsidering our observations on the scaling effects, we focused our\nmodel selection for further oncologicﬁne-tuning on the Woollie 7B and 33B\nmodels only. This decision was reinforced by comparing the performance of\nthe 65B and 33B models on the“rad_imp” t e s ta n db yn o t i n gt h a tt h e\nperformance decrease of the 65B model over the 33B model (Fig.4a). In\naddition, when evaluating standard medical benchmarks such as Pub-\nMedQA, MedMCQA, and USMLE, theoverall accuracy difference is\nminimal, with the 33B model scoring 0.589 and the 65B model scoring\n0.592. Including the 7B model in our analysis serves a strategic purpose,\nallowing us to explore the impact of model size scaling and evaluate the\nfeasibility of deploying smaller models in clinical settings. The 7B model’s\nsmaller parameters makes it an attractive option for widespread clinical use,\nc a p a b l eo fr u n n i n go nas i n g l eG P Uf o rinference, thus eliminating the need\nfor specialized centralized computing resources. We evaluated the memory\nfootprint, inference time, and energy consumption of the 7B and 33B\nmodels (Supplementary Table 2), with detailedﬁndings discussed in the\nDiscussion section.\nPredictive analysis of cancer progression using real-world\noncologic aligned Woollie models\nBuilding upon the Woollie models, we enhanced their capabilities byﬁne-\ntuning them with the MSK radiology impression dataset. This reﬁnement\nled to the development of two specialized models: Woollie MSK 7B and\nWoollie MSK 33B, both of which exhibited substantial improved perfor-\nmance on this dataset. When evaluating cancer progression as a binary\nclassiﬁcation within the test dataset, Woollie MSK 7B outperformed all\ncompeting 65B models (Llama, Woolie Foundation, Woollie Medicine,\nWoollie), achieving an overall accuracyof 0.86 across all disease sites in the\n“rad_imp” test. Woollie MSK 33B showed even greater efﬁcacy, achieving\nan overall accuracy of 0.90 (Fig.4a).\nInterestingly, the modelsﬁne-tuned with the MSK dataset also sig-\nniﬁcantly improved general medical domain benchmarks. Compared to the\nbaseline Woollie models, the accuracy of Woollie MSK 33B increased from\n0.80 to 0.83 in PubMedQA, from 0.45 to 0.48 in MedMCQA, and from 0.49\nto 0.53 in USMLE. A similar trend was observed in the smaller Woollie MSK\n7B model, where accuracy improved from 0.72 to 0.79 in PubMedQA, from\n0.32 to 0.35 in MedMCQA, and from 0.33 to 0.38 in USMLE (Fig.4b). These\nimprovements were statistically signiﬁcant (p < 0.001). These enhance-\nments highlight Woollie’s capacity to integrate and apply medical knowl-\nedge effectively, emphasizing its adaptability and reinforcing its role as a\nversatile foundational model. This also illustrates that data quality is critical\nin enhancing model performance across various disease sites rather than\nmerely the quantity of data.\nWe examined the receiver operating characteristic (ROC) curve to\nfurther analyze cancer progression through model sensitivity and speciﬁcity.\nAs Woollie is a generative model, we calculated the ROC using the logits of\nthe generated tokens in its predictions (Supplementary Note 2). In addition\nto using accuracy as a performance indicator, we also calculated the Area\nUnder the Receiver Operating Characteristic Curve (AUROC). AUROC\nassesses the model’s performance independently of speciﬁc classiﬁcation\nthresholds, making it particularly valuable in scenarios where thresholds\nmay vary. Furthermore, it serves as a standard baseline for comparing\ndifferent models and training approaches, ensuring consistency across\nexperiments. Comparing the Woollie MSK models to the Llama models,\nWoollie MSK 7B achieved an impressive AUROC of 0.94 across all disease\nFig. 1 | Stacked reﬁnement of Woollie models, from pre-training to oncology\ndomain ﬁne-tuning. aOverview of the pre-training process for the baseline Llama\nmodels, highlighting unsupervised training with a dataset featuring 1.4 trillion\ntokens, including the Common Crawl dataset and knowledge from medicine,\nengineering, mathematics, biology, etc. This stage encodes foundational knowledge\nacross various domains into the models, available in four sizes: 7 billion (B), 13\nbillion, 33 billion, and 65 billion parameters.b Description of the domain knowledge\nalignment process for the baseline Llama models using the Chain of Thoughts\n(COT), Alpaca, OpenAssistant, and InstructionWild datasets, creating the Woollie\nFoundation models. This supervised learning step involves training models to\nanswer questions correctly, enhancing their reasoning, logic, and conversational\nabilities. c Further alignment with the general medical and oncology domain using\ndatasets of MedQuAD, PubMedQA, MedMCQA, and USMLE, leading to the\ndevelopment of Woollie Medicine and Woollie. Woollie Medicine is an intermediate\nmodel for evaluating the effectiveness of stacked alignment methods.d Final ﬁne-\ntuning within the oncology domain using a proprietary MSK dataset of 38,719\nradiology reports manually curated by radiologists from 3402 patients acrossﬁve\ncancer types: breast, colorectal, lung, pancreatic, and prostate. This step trains the\nWoollie MSK models (7B and 33B versions) to determine tumor progression, with\nmodels being benchmarked against a test dataset and cross-institutionally validated\nwith a UCSF dataset covering lung, breast, and prostate cancers.e The summary of\nfourteen Woollie models detailing the datasets used for alignment andﬁne-tuning.\nThe table categorizes the datasets into three groups: reasoning, logic, and con-\nversation; general medical domain; and oncology domain, including the proprietary\nMSK dataset of radiology impressions. asterisk notes the selective use of 10,000 high-\nquality examples from the OpenAssistant dataset (OASST1), which contains\n160,000 human-created and annotated conversations in various languages.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 4\nsites, signiﬁcantly outperforming the base Llama 7B model’sA U R O Co f\n0.50. Similarly, Woollie MSK 33B surpassed the baseline Llama 33B model\nwith an AUROC of 0.97, compared to 0.87 (Fig.4c). These models were\nparticularly effective in detecting pancreatic cancer, achieving an AUROC of\n0.98 and an accuracy of 0.92 (Supplementary Fig. 2a, b). The details of the\nmetric selection for cancer progression prediction are provided in the\nMethods section.\nBeyond binary classiﬁcation, we noticed notable enhancements in\nperformance when evaluating the models’capacity to differentiate between\nthe originalﬁve labels from the MSK dataset. The micro-averaged AUROC\nfor the 7B models saw a signiﬁcant increase from 0.63 to 0.93 comparing to\nLlama. For the 33B models, it escalated from 0.80 to 0.97 (Fig.4d). This\nsubstantial improvement highlights the models’reﬁned ability to discern\ncomplex clinical labels, which is vital for practical applications in oncology.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 5\nThese scores provide a precise quantitative evaluation of the model’s\naccuracy and capability to accurately classify clinical conditions and critical\nfactors in clinical environments.\nWoollie shows exceptional generality via cross-institutional\nvalidation\nTo further assess Woollie’s capability to apply its knowledge of cancer\nprogression beyond merely replicatinglearned language patterns, we utilized\nan independent dataset from UCSF. This dataset comprised 600 radiology\nimpressions covering breast, lung, and prostate cancers, with 200 impres-\nsions per cancer type from various patients. The UCSF dataset has a different\nsociodemographic distribution compared to the MSK dataset (Fig.5a). Each\nimpression was manually labeled as either“Progressing” or “Not Progres-\nsing” (Fig. 5b). The Woollie MSK 7B and 33B models were tested on this\ndataset and achieved AUROCs of 0.89 and 0.88, with accuracies of 0.80 and\n0.78, respectively, in detecting cancer progression (Fig.5c). These results are\nsigniﬁcant as the models successfully navigated unfamiliar formats and\nterminologies do not present in the MSK dataset. This success underscores\nthat specialized training enhances performance within a given dataset and\nequips the model to generalize to other datasets with varying characteristics.\nNotably, in detecting lung cancer progression, the models achieved an\nAUROC of 0.95 and an accuracy of 0.88 (Supplementary Fig. 3). Although\nthere was a slight drop in performance compared to the MSK data, the\nWoollie models demonstrated effective knowledge transfer, afﬁrming their\nutility in diverse oncological settings across institutions.\nROC curves for all models (Fig.5d) highlighted the superior perfor-\nm a n c eo ft h eW o o l l i eM S Km o d e l s. While the Woollie MSK 33B model\noutperformed the 7B model on the MSKdataset, it showed a slight decrease\nin AUROC on the UCSF dataset (0.88 compared to 0.89 for the 7B model).\nThis discrepancy suggests a common trade-off in larger models like the 33B,\nwhich typically exhibit low bias but high variance, making them adept at\nﬁtting training data closely. This trade-off highlights the need for careful\nmodel selection and evaluation, considering factors such as model size,\ncomputational resources, and the speciﬁc requirements of clinical settings.\nIn addition to using AUROC as a metric, this study was driven by the\npotential use case of deploying LLMs as real-time assistants for oncologists\nwhile they enter patient information into electronic health record systems.\nFor such clinical applications, the precision of the model’s predictions is\ncritical, as it ensures oncologists can conﬁdently asking the model to identify\ncancer progression precisely. A precision comparison across models,\nincluding the baseline Llama (Fig.5e), using both MSK and UCSF datasets\nrevealed that although the 33B model’s AUROC was slightly lower than the\n7B’s, its precision in detecting cancer progression was higher at 0.95 com-\npared to 0.78, with high precision in detecting lung cancer progression\nat 0.99.\nWoollie distilled the primary topics and disease trajectories from\nthe report\nDespite the complexity often associated with large language models, our\nstudy aimed to assess whether Woollie could identify and synthesize key\nthemes and disease trajectories frommedical records. We posited that LLM-\nbased topic modeling would outperform conventional methods like Latent\nDirichlet Allocation (LDA)31 due to the model’s capacity to navigate com-\nplex linguistic features, idiomatic expressions, and contextual nuances\ntypical of clinical narratives. Such advanced topic modeling proves espe-\ncially useful in oncology contexts. We applied radiology impression data to\nextract insights regarding the nature oftumor progression. Woollie distilled\nthe primary themes of each report (Fig.6a–f), exhibiting not just the pre-\nsence or absence of disease progression but further detailing the sites of\ndisease progression. The model deciphered the rationale behind each\nimaging study in this analysis— diagnosing a speciﬁc condition like a pul-\nmonary embolism or monitoring metastatic disease. Notably, Woollie\nplausibly identiﬁed the most frequent sites of metastatic involvement that\nare otherwise known to be associated with the evaluated cancers (e.g., brain\nor bone metastases among the cohort with breast cancer or hepatic\nmetastases for those with colorectal cancer\n32,33).\nWoollie’s ability to extract global features like topics related to cancer\nbiology is unsurprising given its pretraining, alignment, andﬁne-tuning\nprocesses. Additionally,w ee v a l u a t e dt h em o d e l’s understanding of the local\nfeatures using local interpretable model-agnostic explanations (LIME)34,\nrather than treating the model as a black box. The results demonstrate that\nthe model has effectively learned andutilizes local features to inform its\nclassiﬁcations (Supplementary Figs. 4, 5, Supplementary Note 3). Woollie\nhighlights terms like“FDG” and “Increase” in notes labeled as“Progres-\nsing,” while also recognizing the inﬂuence of words such as“small” and\n“mild” in notes classiﬁed as“Not Progressing.” These local features, which\nplay a key role in identifying cancer progression, demonstrate the model’s\nunderstanding of cancer biology and align with human interpretation.\nGiven the Woollie’s capability of topic modeling and local feature extraction,\nit provides conﬁdence in the model’s explainability and its potential for\nclinical applications.\nThe Sankey plot (Fig.6g) visually represents the disease trajectory\nacross different cancer sites withinpatients from diagnosis onward. This\nplot offers insightful visual analytics on how patients’cancer types may\nevolve or spread to other sites over time, highlighting potential metastasis\npathways or indicating typical trajectories in disease progression. By tracing\nthese trajectories, we can gain a deeper understanding of cancer biology,\nincluding patterns of metastasis, the evolution of cancerous lesions over\ntime, and how different types of cancer might share common pathways or\ndiverge into distinct evolutionary routes. Our approach, using a single\nmodel, contrasts with similar studieson metastatic disease that employ\nmultiple models\n35, offering potential for new insights into disease progres-\nsion pathways that have not been thoroughly explored yet.\nDiscussion\nThe ﬁndings of this study have several important implications for the\nintegration of LLMs into theﬁeld of oncology. (1) It is possible to align and\nﬁne-tune LLMs within the oncology domain without losing prior knowl-\nedge. This can be achieved by selecting high-quality datasets and employing\na stacked alignment methodology . By preserving general domain\nFig. 2 | Performance comparisons of Woollie models on benchmarks, the\ninﬂuence of model size, and improvements in the medical domain. aThe stacked\nalignment strategy mitigates catastrophic forgetting in LLM. When comparing the\nbaseline Llama 65B model to the Woollie 65B model, modest improvements are\nobserved in standard benchmarks testing reasoning and logic in non-medical\ndomains. In the medical domain, however, signiﬁcant improvements are evident.\nThe performance in the medical domain is accentuated in shaded sections, illus-\ntrating how stacked alignment enhances performance while preserving capabilities\nin general domains.b Comparison of all stacked aligned 33B models (Woollie\nFoundation, Woollie Medicine, and Woollie) with non-stacked aligned 33B models\n(Woollie All). The results clearly demonstrate that stacked alignment signiﬁcantly\nimproves model performance incrementally, whereas non-stacked alignment leads\nto catastrophic forgetting, resulting in poorer performance.c A scaling study plotted\nthe performance of Woollie models against model sizes ranging from 7B to 65B\nparameters across 11 tests. The results suggest that larger models generally achieve\nbetter performance, though there is a noticeable plateau in performance enhance-\nment between the 33B and 65B models. This informs the decision-making process\nfor model selection in clinical applications, considering the balance between per-\nformance and resource consumption.d A detailed performance comparison among\ntwelve Woollie and four Llama models across 11 tests is depicted in a heatmap. The\ncolor intensity in each cell reﬂects the mean relative performance in each test. The\nheatmap is divided into non-medical domains on the left and medical domains on\nthe right, categorizing the models into Llama, Woollie Foundation, Woollie Medi-\ncine, and Woollie. This visualization underscores the performance improvements\nachieved through stacked alignment, with a clear transition from left to right,\nhighlighting advancements in medical and oncology domains across the models.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 6\ncompetencies of reasoning, conversation, and information extraction while\nenhancing medical domain proﬁciency, the aligned models demonstrated a\nstrong foundation for further specialization. This approach has shown the\nability in preventing catastrophic forgetting, ensuring the model retains\ncomprehensive capabilities while specializing in medical knowledge.\nAdditionally, choosing a baseline model with adequate parameters is crucial\nfor successful alignment andﬁne-tuning, as the knowledge acquired during\npre-training plays a signiﬁcant role in performance on downstream tasks.\nOur scaling studies indicate that larger models signiﬁcantly outperform\nsmaller ones. (2) Baseline benchmarks on LLMs should not be the sole\nmetrics for evaluating model performance. Our results highlight the\nimportance of cross-institutional validation. While some performance\ndegradation is expected when encountering new datasets, the overall per-\nformance remains remarkable. The model’s generalizability is validated,\nwith performance metrics of AUROC illustrating the model’s capability to\nclassify tumor progression accurately. (3) Knowledge transfer between\nproprietary data and standard benchmarks is not evident. For example, the\nGuanaco 65B model\n36, an enhancement of the Llama 65B model through\nFig. 3 | Sociodemographic characteristics and patient cohort distribution in the\nMSK radiology impression dataset. aSociodemographic distribution of the MSK\nradiology impression dataset, categorized by“Age at Procedure,”“Birth Sex,”\n“Marital Status,”“Race,”“Religion,”and “Ethnicity.”b Tables provide the number of\nreports and unique patients for each cancer type.c The MSK radiology impression\ndataset, manually curated by radiologists, classiﬁes cancer progression intoﬁve\ncategories: Progressing/Worsening/Enlarging, Stable/No change, Improving/\nResponding, Not Stated/Indeterminate, and Mixed. For each cancer type— color-\nectal, pancreatic, breast, prostate, lung— we detail the number of reports and the\ndistribution percentage of theseﬁve labels within each type.d Distribution of the\nMSK radiology impression dataset, featuring labels for progression and non-\nprogression acrossﬁve cancers: breast, colorectal, lung, prostate, and pancreatic,\ncomprising 38,719 reports from 3402 patients.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 7\nﬁne-tuning, attained 99% of ChatGPT’s performance according to the\nVicuna benchmark37 and was at the forefront of the open-source LLM\nleaderboard. When we evaluated its performance within our speciﬁc ﬁeld of\noncology, despite its substantial 65 billion parameters— a ﬁgure nine times\ng r e a t e rt h a nt h a to fa7b i l l i o np a r a m e t e rm o d e l— its performance did not\nmeasure up to our Woollie MSK 7B model. It even fell short compared to the\nWoollie 33B model withoutﬁnetuning on the MSK dataset (Supplementary\nFig. 6). (4) The inclusion of oncology domain knowledge from radiology\nimpression has also enhanced the model’s performance on standard med-\nical benchmarks like PubMedQA (Fig.4b). This underscores the LLM’sr o l e\nas a foundational model that can adaptto various tasks, particularly in\nmedical contexts. (5) Considering the cost implications, using LLMs in\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 8\noncology for purposes like tracking cancer progression and analyzing\nreports proves viable and advantageous. The cross-institution validation of\nthe Woollie model, validated on UCSF data, demonstrates that the smaller\n7B model performs comparably and even slightly outperforms its larger 33B\ncounterparts. When comparing the resource usage of deploying 33B models\nversus smaller 7B models, we found that the 7B model is more economically\nviable for clinical applications due toits lower memory footprint, reduced\nenergy consumption, and less demanding CPU and GPU requirements.\nLarger models like the 33B, while powerful, are impacted by their com-\nplexity, resulting in slower inference times. Speciﬁcally, the 7B model is three\nt i m e sf a s t e rt h a nt h e3 3 Bm o d e l( S u p p l e m e n t a r yT a b l e2 ) .I nc l i n i c a l\nworkﬂows, classiﬁcation requests can arise from various scenarios, such as\nreal-time cancer progression classiﬁcation while a physician enters data into\nelectronic health records (EHR), or retrospective studies analyzing tumor\nprogression across patient cohorts. 7B model’se fﬁciency and faster infer-\nence times make it signiﬁcantly more practical and feasible for deployment\nin a clinical environment as oncologistassistant. This highlights a critical\nbalance between performance and practicality in clinical environments.\nThe datasets from MSK and UCSF differ in the sociodemographic\ncomposition of their patient populations, making it important to evaluate\nthe model’s generalizability across subgroups deﬁned by“Age”, “Birth sex”,\nand “Race”. To assess this, we computed performance metrics stratiﬁed by\nthese variables (Supplementary Table 5a) and conducted statistical com-\nparisons. No signiﬁcant differences in performance were observed across\nthese demographic groups (p-values > 0.1). However, smaller sample sizes\nwithin certain age and race subgroups limit our ability to fully exclude the\npossibility of performance variation. While radiology impression narratives\nappear largely independent of th ese demographic factors, under-\nrepresentation in speciﬁc subgroups may impact model robustness. Future\nstudies with larger and more balancedcohorts are needed to further clarify\nthese ﬁndings. In contrast, model performance differed signiﬁcantly by\ncancer type (Supplementary Table 5b), with all associated p-values below\n0.01. These variations likely reﬂect differences in reporting style and diag-\nnostic emphasis across anatomical sites, which inﬂuence the content and\nstructure of radiology impressions.\nWhen utilizing Woollie as a classiﬁer, we conﬁgured the inference\nsettings with a temperature of 0.01 and top_p of 1.0. This setup effectively\nminimizes randomness, ensuring deterministic model behavior. Concerns\nregarding model hallucination were addressed through performance\nmetrics such as AUROC. The high AUROC scores, alignment with onco-\nlogical knowledge, and localized explainability analyses using LIME strongly\nsuggest that hallucination has been signiﬁcantly minimized in our use cases.\nAlthough hallucination in generative models remains a compelling and\ncritical topic, further research is required to explore its implications in\nscenarios beyond those examined in this study.\nHowever, it is important to acknowledge that hedging language— such\nas “possibility,”“ cannot exclude,” or “likely”— frequently reﬂects the\ninherent uncertainty in radiology impressions. While the use of human-\ncurated labels helps transfer sentiment, contextual understanding, and\ndomain knowledge into the LLM, the speciﬁc impact of such uncertain\nphrasing has not been systematically studied and warrants dedicated\ninvestigation, which was beyond the scope of this work. Additionally, reli-\nance solely on free-text radiology impressions may introduce biases tied to\ninstitutional reporting styles. Although we illustrate stylistic differences\nbetween MSK and UCSF reports (Supplementary Note 6), future work\nshould consider incorporating imaging data into a multimodal LLM fra-\nmework to mitigate such limitations and enhance model robustness.\nWoollie’s capabilities extend beyond mere data interpretation to an in-\ndepth understanding of cancer biology, positioning it well beyond con-\nventional topic modeling methods. Woollie adeptly summarizes the salient\nthemes within medical radiology reports, pinpointing intricate details of\ndisease progression and the exact locations of metastatic spread amidst a\nwealth of medical data. This proﬁciency indicates Woollie’sp o t e n t i a ln o t\njust for comprehending the intent behind medical imaging but also for\nmapping out and succinctly encapsulating the complex pathways of disease\nevolution across diverse organ systems (Fig.6). Such insights are invaluable,\nas they provide a deeper understanding of the prevalent routes of metastasis\nassociated with various types of cancer, thereby enhancing the scope of\noncological assessment and treatment planning.\nOur examination of fourteen distinct Woollie models revealed a slight\ndecrease in general performance following domain-speciﬁct r a i n i n gt h a t\nwas not otherwise substantial while demonstrating marked improvement in\ndomain-speciﬁc knowledge. This contrasts with BERT (Bidirectional\nEncoder Representations from Transformers) based models, which have\nbeen extensively studied in the medicalﬁe l d .T h eL L Mh a sp r o v e nt ob ea n\nadept multitasking learner. Trainingtailored to one proprietary dataset\nimproved performance in the oncology domain and enhanced performance\nacross other general medical domains and oncology institutions. Onceﬁne-\ntuned, an LLM can serve multiple functions across various subspecialties,\nmaking it an invaluable tool for oncology clinics and research. Its application\nextends to knowledge extraction, report generation, and classiﬁcation,\nsupporting a broad spectrum of medical knowledge requirements.\nMethods\nHardware and software\nOur experimental setup leveraged the advanced resources of MSK’sh i g h -\nperformance computing cluster, utilizing 64 Nvidia A100 GPUs (80GB\nVRAM) and 30TB of GPFS storage. We employed the Distributed Data\nParallel (DDP) framework from PyTorch for model training, adapting our\nprocess to utilize varying numbers of GPUs - speciﬁcally 4, 8, 16, 32, or 64\nGPUs, in alignment with the cluster’sc o nﬁguration where each node houses\n4 GPUs. Resource managementand job submission were efﬁciently handled\nusing IBM Spectrum LSF. Theﬁne-tuning process and subsequent analyses\nwere conducted using Python, ensuring a streamlined and effective\nworkﬂow.\nDataset\nOur study categorized thetraining dataset (Fig.1e) for model alignment and\nﬁne-tuning into two primary groups. The non-medical domain aims to\nenhance the model’s reasoning, question-and-answer, and conversational\nFig. 4 | High-performance cancer progression prediction by Woollie MSK\nmodels ﬁne-tuned on MSK oncology data, including their performance metrics\nand comparisons with other models. aComparison of Llama, Woollie Foundation,\nWoollie Medicine, Woollie, Woollie MSK models, and non-stacked aligned Woollie\nAll models on the MSK radiology impression dataset (rad_imp) for binary classi-\nﬁcation of cancer progression. The Woollie 33B model achieves an accuracy of 0.79,\noutperforming the 65B model at 0.77. Fine-tuned Woollie MSK models achieve\nsuperior accuracies of 0.86 (7B) and 0.90 (33B) across allﬁve cancer types. Non-\nstacked aligned Woollie All models lag behind the stacked aligned Woollie models.\nb Woollie MSK models,ﬁne-tuned on top of existing Woollie models, show\nimprovement in the general medical domain on tests like PubMedQA, USMLE, and\nMedMCQA. Fine-tuning on the MSK oncology dataset enhances performance:\nWoollie MSK 33B’s accuracy increased to 0.83 from 0.80 in PubMedQA, 0.48 from\n0.45 in MedMCQA, and 0.53 from 0.49 in USMLE. The Woollie MSK 7B model\nsimilarly shows gains, with accuracies improving notably across all tests.c ROC plot\nillustrating the performance of Woollie MSK 7B on the MSK dataset, with a sig-\nniﬁcant increase from a 0.50 AUROC for the baseline Llama model to 0.94 for\nWoollie MSK 7B. The right panel shows the larger Woollie MSK 33B model reaching\nan AUROC of 0.97, compared to 0.87 for the Llama model.d Comparative per-\nformance analysis between the Llama 7B and Woollie MSK 7B modelsﬁve labels\nreveals a signiﬁcantly higher (p < 0.001) micro-average AUROC for Woollie MSK 7B\nat 0.93 on the right, compared to 0.63 for Llama 7B on the left. A comparison\nbetween Llama 33B and Woollie MSK 33B models on the same dataset and labels\nshows an AUROC of 0.97 for Woollie MSK 33B versus 0.8 for Llama 33B. Fur-\nthermore, the Woollie MSK 33B model demonstrates enhanced performance in the\nconfusion matrix with a higher accuracy of 0.82 compared to 0.76 for Woollie\nMSK 7B.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 9\nFig. 5 | Cross-institution validation of model performance in predicting cancer\nprogression on MSK and UCSF datasets. aSociodemographic distribution of the\nUCSF radiology impression dataset, used exclusively as an independent validation\ndataset. This dataset was not used for Woollie MSKﬁne-tuning. The UCSF dataset\nhas different sociodemographic distribution than MSK dataset.b UCSF dataset\nincludes 600 reports from 600 unique patients, covering prostate, lung, and breast\ncancers, distinct from the MSK data.c Fine-tuned with MSK oncology data, ROC\ncurves for Woollie MSK 7B and 33B models demonstrate performance on the UCSF\ndataset. The Woollie MSK 7B model achieves an AUROC of 0.89, slightly better than\nthe 0.88 for the Woollie MSK 33B, suggesting that smaller models may outperform\nlarger ones in this dataset due to less bias but increased variance.d Comparison of\nWoollie MSK models on both MSK and UCSF datasets shows superior performance\non MSK data, though the knowledge transfer to UCSF is clear. Despite lagging\nbehind the MSK performance, the trend is consistent, indicating effective cross-\ninstitutional validation in cancer progression on an open-source LLM.e Precision\nscores are visualized on a heatmap, with varying color intensities indicating effec-\ntiveness in detecting different types of cancer, noting the absence of data for col-\norectal and pancreatic cancers in the UCSF dataset. Precision scores are crucial for\nclosely monitoring progressive cases. While Woollie MSK 7B shows higher AUROC\nscores on the UCSF dataset, Woollie MSK 33B excels in precision, signiﬁcantly\nnotably with a score of 0.99 in detecting lung cancer. Theﬁne-tuned Woollie models\nsigniﬁcantly outperform Llama models in accurately tracking cancer progression,\nunderscoring their practical application of inter-institutionally transferred\nknowledge.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 10\ncapabilities to mimic human interaction. This category included: (1) An\nupdated Alpaca dataset (link), augmented with GPT-4-generated content\nand manually reﬁned to reduce inaccuracies38,39.( 2 )G o o g l e’sC h a i no f\nThought (COT) dataset (link)t h a tw a sm o d iﬁed for instruction-basedﬁne-\ntuning40. (3) An English rendition of the InstructionWild dataset41 (link),\nsourced from Twitter and vetted by human reviewers. (4) A subset of the\nOpenAssistant dataset (OASST1) (link), containing 160,000 human-\ncreated and annotated conversations, from which we selected 10,000\nFig. 6 | Model parsing of disease trajectories and biology among different\nmalignancies. Salient topics from radiology reports using Woollie MSK 33B on\nreports from MSK patients with cancers of thea breast, b lung, c colorectal,\nd prostate, ande pancreatic. f Summarization of the salient topics among all diseases,\nnotably enriched for sites of distant metastatic seeing.g Sankey plots demonstrating\ntrajectories of metastatic disease acrossﬁve cancer types.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 11\nhigh-quality examples in various languages42. The choice of datasets for\nnon-medical domain alignment was guided by the goal of enhancing the\nmodel’s general reasoning, contextual understanding, and task adaptability,\nwhich are foundational for oncological applications. The datasets like Chain\nof Thought and Alpaca emphasize reasoning and logical progression, which\nare crucial for interpreting complexradiology impressions. These skills\ntranslate directly into the ability to parse nuanced medical terminology, infer\nprogression or stability. OpenAssistant is the Q&A datasets that train the\nmodel to respond effectively to speciﬁc and often domain-speciﬁcq u e r i e s .\nThis capability is essential for oncological tasks where clinicians or\nresearchers require precise answers to questions about tumor progression,\ndiagnostic accuracy, or treatment efﬁcacy based on imaging data.\nFor the medical domain, our dataset selection was as follows: (1)\nPubMedQA\n43 (link), which challenges models to generate answers from\nNIH PubMed abstracts to be compared with expert-curated responses,\ncovering a broad spectrum of biomedical topics. (2) MedQuAD44 (link),\ndrawn from 12 NIH websites, including the National Cancer Institute\n(NCI), provides Q&A pairs on medical subjects such as treatment, diag-\nnosis, and side effects, along with the 2017 consumer health Q&A dataset\nfrom the National Library of Medicine (NLM). (3) MedMCQA (link), an\nassembly of multiple-choice questions from Indian medical school entrance\nexams\n20.( 4 )T h eM e d Q A - U S M L Ed a t a s e t17 (link), developed by Google\nResearch to train advanced models like Med-PaLM and Med-PaLM 2.\nFor the oncology-speciﬁc ﬁne-tuning process, we compiled 38,719\nradiology impression notes from 3402 patients acrossﬁve cancer types: lung,\nbreast, pancreatic, prostate, and colorectal. This collection is a subset of the\nAACR Project GENIE Biopharma Collaborative (BPC) dataset45,46.T h e s e\nrecords provided detailed annotations of cancer occurrences and progres-\nsion. Additionally, we gathered 600radiology impression notes from 600\npatients at UCSF, focusing on lung, breast, and prostate cancers. The UCSF\nradiology impression notes are distinct in format and context from those of\nMSK, it was only used for the cross-institutional validation to evaluate the\nmodel’s generalizability, we applied theﬁne-tuned oncology models trained\non the MSK dataset to analyze cancer progression data from the UCSF\ndataset (Fig.5a). All MSK patients included in this study were part of\ninstitutional review board (IRB)-approved research protocols (MSK IRB\nProtocols 12–245). Additionally, the study received independent approval\nfrom the UCSF (UCSF IRB Protocols 20–32527). Patients provided written,\ninformed consent and were enrolled ina continuous, non-random fashion.\nStacked alignment strategy\nIn this study, we trained fourteen distinct models, employing a stacked\nmulti-turn alignment andﬁne-tuning process, building upon each succes-\nsive model iteration. The initial phase involves aligning a foundational\nmodel, Woollie Foundation, based on the pre-trained Llama models (Fig.\n1a), we employed the updated datasets Alpaca, Google’s Chain of Thought\n(COT) dataset, the InstructionWild dataset, and a subset of the Open-\nAssistant dataset (OASST1). The Woollie Foundations models were\nenhanced on reasoning, the chain of thoughts, and Q&A.\nSubsequently, we reﬁned our focus to the medical domain, utilizing the\nPubMedQA and MedQuAD datasets collected before our 2021 cut-off. This\nallowed us to test knowledge transfer to models predating the data used to\ntrain the baseline Llama models. We aligned our Woollie Foundation model\nwith these datasets, resulting in the Woollie Medicine model. Further\nalignment was done with the recently released USMLE and MedMCQA\ndatasets after 2021, leading to the creation of the Woollie models This\nstacked alignment strategy effectively addressed the challenge of“cata-\nstrophic forgetting,” which often occurs whenLLMs transition between\ndomains. We compared the Woollie models aligned using the stacked\nalignment strategy to those trained on asimple concatenation of all datasets\n(Woollia All models). The results reveal a signiﬁcant performance gap, with\nthe stacked alignment models demonstrating superior performance (Fig.2b,\nSupplementary Fig. 1b). Theﬁnal step involvedﬁne-tuning Woollie with\nreal-world oncology datasets from MSK radiology impression notes, cul-\nminating in the Woollie MSK model (Fig.1d).\nThe underlying Llama models varied in size: 7B, 13B, 33B, and 65B\nparameters. Given the general trend that larger models perform better, we\ninvestigated this aspect by trainingWoollie models at 7B, 13B, 33B, and 65B.\nWe limited the Woollie Instruction to 7B, 13B, and 33B models, while\nWoollie MSK wasﬁne-tuned only at 7B and 33B sizes.\nBenchmarks\nWe evaluated our model using a range of benchmarks, including COQA24,\nLogiQA22, ARC, HellaSWAG 25, MMLU, OpenBookQA, SciQ,\nPubMedQA19,U S M L E ,a n dM e d M C Q A20 (Supplementary Table 4). Dur-\ning evaluations and testing, we set the model’s generation temperature to\n0.01 and top_p to 1.0 to minimize randomness and ensure deterministic\nbehavior, as the model is being used as a classiﬁer. Additionally, we repeated\nthe same tasks multiple times and observed no randomness, with the per-\nformance metrics remaining constants across runs. These evaluations were\nconducted using a zero-shot learningapproach, where the LLM is prompted\nwith questions that generate answers without prior speciﬁc training on those\nquestions. Typically, model performance comparisons utilize few-shot\nlearning, where the LLM is provided with a small set of correct question-\nanswer pairs before being asked to respond to new questions. This approach\noften enhances performance due to the model having relevant context.\nHowever, our model was alreadyﬁne-tuned, so we aimed to ascertain its\ncomparative improvement over the non-ﬁne-tuned baseline Llama model\nusing zero-shot learning. As comparisons, we incorporated 1-shot and\n3-shot learning to evaluate their pe r f o r m a n c eo nt h eW o o l l i em o d e l s\n(Supplementary Fig. 8). Due to the model’s context size limitation of 2048\ntokens, using more than 3 shots exceeded the maximum allowable context.\nW h i l ew eo b s e r v e dap e r f o r m a n c eimprovement from 0-shot to 1-shot\nlearning, further increases in the number of shots showed minimal\nimprovement or, in some cases, a decline in performance. This decline may\nbe attributed to the model nearing itsmaximum context size, causing it to\nlose focus on content in the middle of the sequence\n47. Further research is\nneeded to explore the impact of larger context windows on performance.\nPrivacy considerations and de-identiﬁcation of clinical data for\nﬁne-tuning\nThe radiology impression data from the MSK patient cohort does not\ninclude protected health information (PHI). However, the impressions may\nrefer scan events, as well as the names of referring or diagnostic physicians.\nTo address this, we applied named entity recognition (NER) using a pre-\ntrained BERT\n48 model with spaCy49. Out of 38,719 impression notes, 2467\ncontained physician names, which we masked as [[PERSON]]. Dates\nassociated with scan events, considered identiﬁable patient data, were also\nreplaced with [[DATE]]. Examples of de-identiﬁed radiology impressions\nare provided in the Supplementary Note 6.\nThe Woollie MSK models, trained on a non-PHI dataset, do not\ncontain any knowledge of individual patient private information. Once\ndownloaded to a local environment, the model can function without an\nexternal network connection, allowing it to be used on PHI data while\nensuring the dataset remains secure and does not transmit outside the\nsystem. Predictions are based solely on the knowledge acquired during the\nmodel’sa l i g n m e n ta n dﬁne-tuning processes.\nClassiﬁcation of cancer progression\nA critical aspect of our testing was to assess the model’s generalizability to\ndatasets it had not encountered during pre-training orﬁne-tuning. This\naspect of LLM performance, especiallyin real-world oncology data, has not\nbeen extensively studied. The detailed breakdown of cases and patient\nnumbers is presented (Fig.3b, c). Through these comparative studies, we\naimed to demonstrate Woollie’sp r oﬁciency in accurately interpreting and\nclassifying oncological data from diverse institutional sources.\nThe radiologist has manually annotated the dataset. For the MSK\ndataset, we have annotated the reportin two types: the detailed annotation\nwith ﬁve labels to reﬂect whether the tumor has progressed or not: (a)\nProgressing/Worsening/Enlarging, (b) Stable/No change, (c) Improving/\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 12\nResponding, (d) Not stated/Indeterminate, (e) Mixed., The binary anno-\ntation combines the (a) Progressing/Worsening/Enlarging and (e) Mixed as\npositive case - tumor progressed (“Progressing”) and combination of other\nlabels as tumor not progressed (“Not Progressing”). For the UCSF dataset,\nwe only have binary annotation, with tumor progression or not.\nWe divided the 38,719 reports into training, validation, and testing\ngroups for the MSK dataset, following an 80%, 10%, and 10% split,\nrespectively. We used an optimized prompting strategy (Supplementary\nNote 5, Supplementary Fig. 7) with amaximum input token window of 2048\ntokens, which was sufﬁcient to accommodate the full radiology impression\nduring training (Supplementary Fig. 9). Woollie is a generative model, and\nwe must reformat the testing to a multiple-choice one; we prompt the\nradiology impression using the templates (Supplementary Note 7). The\nresultingﬁne-tuned models were Woollie MSK 7B and Woollie MSK 33B.\nTo test their performance, we employed the Language Model Evaluation\nHarness framework\n50, a standard tool used across the open-source LLM\ncommunity for evaluating model effectiveness. This framework was adapted\nto include a radiology impressions validation dataset in the evaluation cri-\nteria. It provided accuracy metrics and calculated log-likelihood, which we\nconverted into prediction probabilities for each answer using the Softmax\nfunction. Since Woollie is a generative model, we utilized the approach\ndescribed in Supplementary Note 2 to generate the AUROC, ROC curve,\nand the confusion matrix.\nTo evaluate our model’s performance acrossﬁve classes, we employed\nthe one-vs-rest strategy to calculate the AUROC. This approach involves\ngenerating an ROC curve for each class by comparing it against all other\nclasses combined, computing the AUROC for each curve, and then aver-\naging these values to obtain a single representative score.\nIn addition to AUROC, we calculated precision, recall, F1 scores, and\nMCC to provide a comprehensive assessment of the model’s performance\n(see Supplementary Table 1). AUROC was selected as a primary perfor-\nmance metric due to its robustness and widespread use in evaluating a\nmodel’s ability to distinguish between classes across various decision\nthresholds. Notably, AUROC assesses performance independently of spe-\nciﬁc classiﬁcation thresholds, making it particularly useful in scenarios\nwhere these thresholds may vary. Furthermore, AUROC is effective in\nhandling imbalanced datasets by focusing on true positive and false positive\nrates rather than absolute counts, providing a balanced evaluation even\nwhen class distributions are uneven.\nWe used the same prompt format for the UCSF dataset, consisting of\n600 radiology impression notes labeled“Progression” as positive and“Not\nProgression”as negative. These notes underwent evaluation using the same\nframework with both Woollie MSK 7B and Woollie MSK 33B models. We\nreported on accuracy and log-likelihood measures. ROC curves were also\nplotted for this dataset and were directly compared with those of the MSK\ndataset within the exactﬁgures (Fig.5c).\nCancer trajectories analysis\nInstead of viewing LLM as a black box, we aim to delve into the model’s\nunderstanding of the contextual intricacies inherent in clinical narratives\nand oncology knowledge embedded within clinical notes. To achieve this,\nwe thus implemented topic modeling using Woollie MSK models on the\ndataset of radiology impressions to garner insights into tumor progression\nand the pathways of cancer development, facilitating these discoveries\nthrough clustering identiﬁe dt o p i c s .T h i sa p p r o a c hh e l p su sg a u g et h ee x t e n t\nof cancer biology knowledge the model has assimilated during itsﬁne-\ntuning phase. Employing such sophisticated topic modeling techniques is\nparticularly beneﬁcial in the clinical setting, where healthcare professionals\nare challenged to extract evolving disease patterns from a dense array of data\npoints.\nSpeciﬁcally, we employed a neural topic modeling approach outlined in\nthe study\n51. The initial step involved transforming radiology impression\nnotes into a vectorized format using FlagEmbedding52, resulting in vectors of\ndimension 1024 for each note. To facilitate analysis, we reduced these high-\ndimensional vectors to 5 dimensions using Uniform Manifold\nApproximation and Projection (UMAP)53,c o nﬁgured with 15 neighbors for\ndimensionality reduction. This approach deviates from traditional k-means\nclustering by employing Hierarchical Density-based Spatial Clustering of\nApplications with Noise (HDBSCAN)\n54.H D B S C A Ni sa d e p ta tc l u s t e r i n g\noutputs from LLMs due to its ability to manage clusters with varied den-\nsities, autonomously identify the number of clusters, andﬁlter out noise as\noutlier data. This hierarchical clustering technique is precious for delving\ninto data across different levels of detail, enabling the exploration of intricate\nrelationships in LLM outputs. HDBSCAN is a prime tool for analyzing\nLLMs’nuanced and semantically rich outputs thanks to its versatility with\nvarious distance metrics and effectiveness in handling high-dimensional\ndata. We set the minimum cluster size to 15.\nUsing the Woollie MSK 33B model for summarization and topic\nextraction, we focused on identifying the top 10 most relevant keywords for\neach note, restricting the analysis to 20 topics with the highest probabilities.\nThe topic extraction wasﬁrst applied to individual cancer types (Fig.6a–e),\nand then extended to encompass all notes, identifying all pertinent topics\n(Fig.6f). We visualized the topics using UMAP to reduce dimensions to two,\nwith each color representing the same topics from the Woollie model. The\nsize of the dots indicates the number of radiology impression notes within\nthat reduced dimensional representation.\nConsidering the complexity of the large language model and its\npotential clinical applications, we conducted explainability studies using\nLIME (Supplementary Note 3). Combining insights from LLM-based topic\nmodeling and LIME explanations highlights the knowledge acquired by the\nWoollie models and bolsters conﬁdence in their explainability for clinical\nuse. While LLMs excel in extracting high-level concepts through topic\nmodeling, LIME effectively emphasizes the importance of local features,\nproviding a complementary perspective on the model’si n t e r p r e t a b i l i t y .\nThe Sankey plot (Fig.6g) visually represents the disease trajectory\nacross different cancer sites withinpatients from diagnosis onward. This\nplot offers insightful visual analytics on how patients’cancer types may\nevolve or spread to other sites over time, highlighting potential metastasis\npathways or indicating typical trajectories in disease progression.\nData availability\nThe radiology impression data used in this study is part of our MSK-\nCHORD project (“J e e ,J . ,F o n g ,C . ,P i c h o t t a ,K .e ta l .A u t o m a t e dr e a l - w o r l d\ndata integration improves cancer outcome prediction.Nature636, 728–736\n(2024). https://doi.org/10.1038/s41586-024-08167-5”). The labeled data is\npublicly available via cBioPortal at MSK-CHORD 2024 and GENIE\ncBioPortal. Additionally, it is included in the GENIE BPC Cohort, with\nannotated labels accessible atAACR Project GENIE Data, which provides\ninstructions for downloading the data. The Woollie Foundation, Woollie\nMedicine, and Woollie Models are available on Hugging Face atHugging\nFace - Woollie Collection. The collection includesmodels with parameters\nranging from 7B to 65B. The original radiology impression notes cannot be\nshared with researchers outside of the study team approved by the IRB of the\nparticipating institution due to MSK legal policies. Access to anonymized\ndata can be shared through a data transfer agreement (DTA) managed by\nthe principal institution. Woollie MSK 7B and 33B models trained on MSK\nclinical datasets can be shared upon legal agreement, subject to appropriate\nreview and approval.\nCode availability\nAlignment,ﬁnetuning and measurement codes for this study are available at\nhttps://github.com/msk-mph/woollie.\nReceived: 23 July 2024; Accepted: 4 June 2025;\nReferences\n1. Ouyang, L. et al. Training language models to follow instructions with\nhuman feedback.Adv. Neural Inf. Process. Syst.35, 27730–27744\n(2022).\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 13\n2. OpenAi. GPT-4 technical report(OpenAi, 2023).\n3. Eloundou, T., Manning, S., Mishkin, P. & Rock, D. Gpts are gpts: an\nearly look at the labor market impact potential of large language\nmodels. Preprint athttps://arxiv.org/abs/2303.10130 (2023).\n4. Will ChatGPT transform healthcare?Nat. Med.29, 505–506 (2023)\n5. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E.\nCapabilities of gpt-4 on medical challenge problems. Preprint at\nhttps://arxiv.org/abs/2303.13375 (2023).\n6. Kung, T. H. et al. Performance of ChatGPT on USMLE: potential for AI-\nassisted medical education using large language models.PLOS\nDigital Health2, e0000198 (2023).\n7. Sinha, R. K., Deb Roy, A., Kumar, N. & Mondal, H. Applicability of\nChatGPT in assisting to solve higher order problems in pathology.\nCureus 15, e35237 (2023).\n8. Rao, A. et al. Evaluating ChatGPT as an adjunct for radiologic\ndecision-making. medRxiv https://doi.org/10.1101/2023.02.02.\n23285399 (2023)\n9. Chen, S. et al. Use of artiﬁcial intelligence chatbots for cancer\ntreatment information.JAMA Oncol.https://doi.org/10.1001/\njamaoncol.2023.2954 (2023)\n10. Chung, H. W. et al. Scaling instruction-ﬁnetuned language models.J.\nMach. Learn. Res25.70 1–53 (2024).\n11. Touvron, H. et al. Llama: open and efﬁcient foundation language\nmodels. Preprint athttps://arxiv.org/abs/2302.13971 (2023).\n12. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. Preprint athttps://arxiv.org/abs/2307.09288 (2023).\n13. Bolton, E. et al. Biomedlm: A 2.7 b parameter language model trained\non biomedical text. Preprint athttps://arxiv.org/abs/2403.18421\n(2024).\n14. Luo, R. et al. BioGPT: generative pre-trained transformer for\nbiomedical text generation and mining.Brief. Bioinformatics23,\nhttps://doi.org/10.1093/bib/bbac409 (2022).\n15. Yang, X. et al. A large language model for electronic health records.npj\nDigital Med.5, 194.https://doi.org/10.1038/s41746-022-00742-2(2022).\n16. Singhal, K. et al. Large language models encode clinical knowledge.\nPreprint athttps://arxiv.org/abs/2212.13138 (2022).\n17. Singhal, K. et al. Toward expert-level medical question answering with\nlarge language models.Nat. Med1–8 (2025).\n18. Labrak, Y. et al. Biomistral: a collection of open-source pretrained\nlarge language models for medical domains. Preprint athttps://arxiv.\norg/abs/2402.10373 (2024).\n19. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. & Lu, X. 2567–2577 (2019).\n20. Pal, A., Logesh, K. U. & Malaikannan, S. Medmcqa: A large-scale multi-\nsubject multi-choice dataset for medical domain question answering.\nConference on health, inference, and learning. (PMLR, 2022).\n21. Mihaylov, T., Clark, P., Khot, T. & Sabharwal, A. inConference on\nEmpirical Methods in Natural Language Processing\n.\n22. Liu, J. et al. LogiQA: a challenge dataset for machine reading\ncomprehension with logical reasoning. Preprint athttps://arxiv.org/\nabs/2007.08124 (2020).\n23. Hendrycks, D. et al. Measuring massive multitask language\nunderstanding. Preprint athttps://arxiv.org/abs/2009.03300 (2020).\n24. Reddy, S., Chen, D. & Manning, C. D. Coqa: a conversational question\nanswering challenge.Trans. Assoc. Computational Linguist.7,\n249–266 (2019).\n25. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. & Choi, Y. Hellaswag:\nCan a machine reallyﬁnish your sentence? Preprint athttps://arxiv.\norg/abs/1905.07830 (2019).\n26. Luo, Y. et al. An empirical study of catastrophic forgetting in large\nlanguage models during continualﬁne-tuning. Preprint athttps://\narxiv.org/html/2308.08747v3 (2023).\n27. Brown, T. et al. (eds H. Larochelle et al.) 1877-1901 (Curran\nAssociates, Inc., 2020).\n28. Achiam, J. et al. Gpt-4 technical report. Preprint athttps://arxiv.org/\nabs/2303.08774 (2023).\n29. Bureau, U. S. C. United States Census Bureau Quick Facts.https://\nwww.census.gov/quickfacts/fact/table/US/PST045224 (2024).\n30. Jee, J. et al. Automated annotation for large-scale clinicogenomic\nmodels of lung cancer treatment response and overall survival.\nCancer Res.83, 5721–5721 (2023).\n31. Blei, D. M., Ng, A. Y. & Jordan, M. I. Latent dirichlet allocation.J. Mach.\nLearn. Res.3, 993–1022 (2003).\n32. Kennecke, H. et al. Metastatic behavior of breast cancer subtypes.J. Clin.\nOncol.28,3 2 7 1–3277,https://doi.org/10.1200/jco.2009.25.9820(2010).\n33. Patanaphan, V. & Salazar, O. M. Colorectal cancer: metastatic\npatterns and prognosis.South Med. J.86,3 8–41 (1993).\n34. Ribeiro, M. T., Singh, S. & Guestrin, C. inProceedings of the 22nd\nACM SIGKDD international conference on knowledge discovery and\ndata mining. 1135–1144.\n35. Do, R. K. G. et al. Patterns of metastatic disease in patients with cancer\nderived from natural language processing of structured CT radiology\nreports over a 10-year period.Radiology 301,1 1 5–122(2021).\n36. Dettmers, T., Pagnoni, A., Holtzman, A. & Zettlemoyer, L. Qlora:\nEfﬁcient ﬁnetuning of quantized LLMs. InAdvances in neural\ninformation processing Systems36 (NIPS, 2024).\n37. Chiang, W.-L. et al.Vicuna: an open-source chatbot impressing GPT-\n4 with 90%* ChatGPT quality\n(2023).\n38. Wang, Y. et al. Self-instruct: aligning language model with self\ngenerated instructions. Preprint athttps://arxiv.org/abs/2212.10560\n(2022).\n39. Gene, R.Alpaca dataset from Stanford, cleaned and curated(https://\ngithub.com/gururise/AlpacaDataCleaned) 2023.\n40. Si, Q. et al. An empirical study of instruction-tuning large language\nmodels in Chinese. Preprint athttps://arxiv.org/abs/2310.07328(2023).\n41. Xue, F., Jain, K., Shah, M. H., Zheng, Z. & Yang, Y.Instruction in the\nwild: a user-based instruction dataset(https://github.com/\nXueFuzhao/InstructionWild) 2023.\n42. Dettmers, T. et al. Qlora: Efﬁcient ﬁnetuning of quantized llms.\nAdvances in neural information processing systems 36, 10088–10115\n(2023).\n43. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. Pubmedqa: a\ndataset for biomedical research question answering. Preprint at\nhttps://arxiv.org/abs/1909.06146 (2019).\n44. Ben Abacha, A. & Demner-Fushman, D. A question-entailment\napproach to question answering.BMC Bioinforma.20, 511(2019).\n45. Lavery, J. A. et al. A scalable quality assurance process for curating\noncology electronic health records: the project GENIE biopharma\ncollaborative approach.JCO Clin. Cancer Inf.6, e2100105(2022).\n46. Schrag, D. inASCO annual meeting.\n47. Liu, N. F. et al. Lost in the middle: how language models use long\ncontexts. Trans. Assoc. Computational Linguist.12, 157–173 (2024).\n48. Liu, Y. RoBERTa: a robustly optimized bert pretraining approach.\nPreprint athttps://arxiv.org/abs/1907.11692 (2019).\n49. Honnibal, M., Montani, I., Van Landeghem, S. & Boyd, A. spaCy:\nindustrial-strength natural language processing in python.https://\narxiv.org/abs/1907.11692 (2020).\n50. Gao, L. et al.A framework for few-shot language model evaluation\n(Zenodo, 2021).\n51. Grootendorst, M. BERTopic: Neural topic modeling with a class-\nbased TF-IDF procedure. Preprint athttps://arxiv.org/abs/2203.\n05794 (2022).\n52. Xiao, S. et al. C-pack: Packed resources for general chinese embeddings.\nProceedings of the 47th international ACM SIGIR conference on research\nand development in information retrieval. (2024).\n53. McInnes, L., Healy, J. & Melville, J. Umap: uniform manifold\napproximation and projection for dimension reduction. Preprint at\nhttps://arxiv.org/abs/1802.03426 (2018).\n54. Campello, R., Moulavi, D. & Sander, J. (2013). Density-Based\nClustering Based on Hierarchical Density Estimates. 7819, 160–172.\nhttps://doi.org/10.1007/978-3-642-37456-2_14.\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 14\nAcknowledgements\nWe thank the High-Performance Computing Group— especially M. Chak-\nradeo for her essential role in providing access to the latest GPU cluster and\nthe Department of Medical Physics Computer Service at MSK for their\ninvaluable support. We appreciate Q. Morris, P. Stetson, G. Kuperman,\nJ. Choy and A. Chatterjee for their early discussions, insightful contributions,\nand inspiration for this project. We thank the GENIE BPC Statistical Coor-\ndinating Center for implementing programmatic quality assurance pro-\ncesses and the derivation of outcome variables for the MSK radiology\nimpression data. Additionally, this research was partially supported by the\nNIH/NCI Cancer Center Support Grant P30 CA008748.\nAuthor contributions\nL.B. and A.L. conceived the study. A.L. secured funding and supervised the\nresearch. L.B. supervised the cancer progression and cancer trajectories\nanalysis. M.Z. and H.L. analyzed the clinical data, measured the model\nperformance, and drafted the manuscript. Jue J. preprocessed data, wrote\nthe training code, and prepared baseline Llama models. A.J. computed the\nAUROC, Justin J., K.P., M.W., D.R., N.S. prepared radiology impression\ndataset and cleaned up the labels. L.V. allocated resources from HPC and\nmonitored the model training procedures. S.C. worked on the Sankey plot.\nO.M. collected the UCSF dataset, J.D., J.M., S.P., C.N., G.R., collected the\nMSK dataset and provided clinical expertise. All authors critically revised the\nmanuscript for important intellectual content. All authors provided\ncomments and approved the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01780-2\n.\nCorrespondenceand requests for materials should be addressed to\nLior Z. Braunstein or Anyi Li.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01780-2 Article\nnpj Digital Medicine|           (2025) 8:397 15",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.7328941822052002
    },
    {
      "name": "Subspecialty",
      "score": 0.6788040995597839
    },
    {
      "name": "Oncology",
      "score": 0.6241595149040222
    },
    {
      "name": "Internal medicine",
      "score": 0.6180666089057922
    },
    {
      "name": "Lung cancer",
      "score": 0.5692548155784607
    },
    {
      "name": "Cancer",
      "score": 0.5596693754196167
    },
    {
      "name": "Pancreatic cancer",
      "score": 0.5473005771636963
    },
    {
      "name": "Prostate cancer",
      "score": 0.5248731970787048
    },
    {
      "name": "Colorectal cancer",
      "score": 0.47679829597473145
    },
    {
      "name": "Receiver operating characteristic",
      "score": 0.45017147064208984
    },
    {
      "name": "Clinical Oncology",
      "score": 0.4251776933670044
    },
    {
      "name": "Medical physics",
      "score": 0.4177379012107849
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4123515188694
    },
    {
      "name": "Family medicine",
      "score": 0.21748322248458862
    },
    {
      "name": "Artificial intelligence",
      "score": 0.12959825992584229
    },
    {
      "name": "Computer science",
      "score": 0.12373819947242737
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1334819555",
      "name": "Memorial Sloan Kettering Cancer Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I180670191",
      "name": "University of California, San Francisco",
      "country": "US"
    }
  ],
  "cited_by": 5
}