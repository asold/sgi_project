{
  "title": "Generalization bias in large language model summarization of scientific research",
  "url": "https://openalex.org/W4409963278",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2027349828",
      "name": "Uwe Peters",
      "affiliations": [
        "Utrecht University"
      ]
    },
    {
      "id": "https://openalex.org/A4202179152",
      "name": "Benjamin Chin-Yee",
      "affiliations": [
        "Western University",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4202179152",
      "name": "Benjamin Chin-Yee",
      "affiliations": [
        "University of Cambridge",
        "Western University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2740649455",
    "https://openalex.org/W4296624030",
    "https://openalex.org/W4406855515",
    "https://openalex.org/W4367852612",
    "https://openalex.org/W4376224599",
    "https://openalex.org/W4322008312",
    "https://openalex.org/W4400047099",
    "https://openalex.org/W4393054030",
    "https://openalex.org/W4367051110",
    "https://openalex.org/W4323035111",
    "https://openalex.org/W4367188881",
    "https://openalex.org/W4377010595",
    "https://openalex.org/W4392504765",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W2564659821",
    "https://openalex.org/W2970446905",
    "https://openalex.org/W4400350189",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4407648663",
    "https://openalex.org/W4387100712",
    "https://openalex.org/W4400248253",
    "https://openalex.org/W4321370298",
    "https://openalex.org/W4317911825",
    "https://openalex.org/W2486590973",
    "https://openalex.org/W4301003649",
    "https://openalex.org/W4389519354",
    "https://openalex.org/W4389519841",
    "https://openalex.org/W4391690790",
    "https://openalex.org/W3194765239",
    "https://openalex.org/W3157906320",
    "https://openalex.org/W4402827393",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4404782672",
    "https://openalex.org/W4404782622",
    "https://openalex.org/W4389519607",
    "https://openalex.org/W2062575785",
    "https://openalex.org/W4392681182",
    "https://openalex.org/W4389164161",
    "https://openalex.org/W4362673335",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4401729468",
    "https://openalex.org/W4221031488",
    "https://openalex.org/W4293835408",
    "https://openalex.org/W4391184004",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4397006720"
  ],
  "abstract": "Artificial intelligence chatbots driven by large language models (LLMs) have the potential to increase public science literacy and support scientific research, as they can quickly summarize complex scientific information in accessible terms. However, when summarizing scientific texts, LLMs may omit details that limit the scope of research conclusions, leading to generalizations of results broader than warranted by the original study. We tested 10 prominent LLMs, including ChatGPT-4o, ChatGPT-4.5, DeepSeek, LLaMA 3.3 70B, and Claude 3.7 Sonnet, comparing 4900 LLM-generated summaries to their original scientific texts. Even when explicitly prompted for accuracy, most LLMs produced broader generalizations of scientific results than those in the original texts, with DeepSeek, ChatGPT-4o, and LLaMA 3.3 70B overgeneralizing in 26â€“73% of cases. In a direct comparison of LLM-generated and human-authored science summaries, LLM summaries were nearly five times more likely to contain broad generalizations (odds ratio = 4.85, 95% CI [3.06, 7.70], p &lt; 0.001). Notably, newer models tended to perform worse in generalization accuracy than earlier ones. Our results indicate a strong bias in many widely used LLMs towards overgeneralizing scientific conclusions, posing a significant risk of large-scale misinterpretations of research findings. We highlight potential mitigation strategies, including lowering LLM temperature settings and benchmarking LLMs for generalization accuracy.",
  "full_text": null,
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8113256096839905
    },
    {
      "name": "Generalization",
      "score": 0.6812560558319092
    },
    {
      "name": "Computer science",
      "score": 0.603600800037384
    },
    {
      "name": "Natural language processing",
      "score": 0.5495783090591431
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45683830976486206
    },
    {
      "name": "Language model",
      "score": 0.4405035972595215
    },
    {
      "name": "Epistemology",
      "score": 0.14218860864639282
    },
    {
      "name": "Philosophy",
      "score": 0.09254181385040283
    }
  ]
}