{
    "title": "From Code Generation to Software Testing: AI Copilot With Context-Based Retrieval-Augmented Generation",
    "url": "https://openalex.org/W4408324167",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2098270760",
            "name": "Yuchen Wang",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2944178324",
            "name": "Shangxin Guo",
            "affiliations": [
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2127459816",
            "name": "Chee Wei Tan",
            "affiliations": [
                "Nanyang Technological University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6871524336",
        "https://openalex.org/W4393278146",
        "https://openalex.org/W6748138928",
        "https://openalex.org/W4378418580",
        "https://openalex.org/W4367185264",
        "https://openalex.org/W6798182279",
        "https://openalex.org/W4312438588",
        "https://openalex.org/W4405908042",
        "https://openalex.org/W4387711873",
        "https://openalex.org/W2092382400",
        "https://openalex.org/W6777615688",
        "https://openalex.org/W6802425810",
        "https://openalex.org/W6855159228"
    ],
    "abstract": "The rapid pace of large-scale software development places increasing demands on traditional testing methodologies, often leading to bottlenecks in efficiency, accuracy, and coverage. We propose a novel perspective on software testing by positing bug detection and coding with fewer bugs as two interconnected problems that share a common goal, which is reducing bugs with limited resources. We extend our previous work on AI-assisted programming, which supports code auto-completion and chatbot-powered Q&A, to the realm of software testing. We introduce Copilot for Testing, an automated testing system that synchronizes bug detection with codebase updates, leveraging context-based Retrieval Augmented Generation (RAG) to enhance the capabilities of large language models (LLMs). Our evaluation demonstrates a 31.2% improvement in bug detection accuracy, a 12.6% increase in critical test coverage, and a 10.5% higher user acceptance rate, highlighting the transformative potential of AI-driven technologies in modern software development practices.",
    "full_text": "FROM CODE GENERATION TO SOFTWARE TESTING :\nAI C OPILOT WITH CONTEXT -BASED RAG\nA PREPRINT\nYuchen Wang\nNanyang Technological University\nSingapore\nyuchen011@e.ntu.edu.sg\nShangxin Guo\nCity University of Hong Kong\nHong Kong\nsxguo2-c@my.cityu.edu.hk\nChee Wei Tan\nNanyang Technological University\nSingapore\ncheewei.tan@ntu.edu.sg\nABSTRACT\nThe rapid pace of large-scale software development places increasing demands on traditional testing\nmethodologies, often leading to bottlenecks in efficiency, accuracy, and coverage. We propose a\nnovel perspective on software testing by positing “bug detection\" and “coding with fewer bugs\"\nas two interconnected problems that share a common goal—reducing bugs with limited resources.\nWe extend our previous work on AI-assisted programming, which supports code auto-completion\nand chatbot-powered Q&A, to the realm of software testing. We introduce “Copilot for Testing,\"\nan automated testing system that synchronizes bug detection with codebase updates, leveraging\ncontext-based Retrieval Augmented Generation (RAG) to enhance the capabilities of large language\nmodels (LLMs). Our evaluation demonstrates a 31.2% improvement in bug detection accuracy, a\n12.6% increase in critical test coverage, and a 10.5% higher user acceptance rate, highlighting the\ntransformative potential of AI-driven technologies in modern software development practices.\nKeywords AI-assisted Testing · Retrieval-Augmented Generation (RAG) · Software Quality Assurance · Bug\nDetection · Automated Software Testing\n1 Introduction\nIn the evolving landscape of software development, the rapid pace of innovation and the increasing complexity of\nsystems demand equally sophisticated tools to ensure reliability and efficiency. Traditional testing methods, while often\nreliant on manual efforts supplemented by semi-automated tools, often fall short in addressing the dual challenges of\nidentifying bugs effectively and minimizing the bug rate in generated code. This gap not only impedes the development\nprocess but also affects the overall software quality, leading to potential oversights and inadequate test coverage Ricca\net al. [2024], Russo [2024], Feldt et al. [2018].\nThe industry’s shift towards continuous integration and deployment accentuates the need for more efficient testing\nprocesses. In this context, artificial intelligence (AI) emerges as a game-changer for enhancing software testing. Recent\nadvancements in computational models, particularly large language models (LLMs) empowered by retrieval-augmented\ngeneration (RAG), offer new avenues for improving testing methods. These AI-driven technologies can analyze\nextensive codebases and detect complex patterns that may escape traditional testing approaches, thereby enabling\nthe generation of precise and comprehensive test casesAlshahwan et al. [2023]. Given the progress of AI-assisted\nprogramming, there’s a significant opportunity to further apply these innovations to software testing and validation. By\naddressing the dual problems of increasing the bug detection rate and decreasing the bug rate in code generation, AI can\nplay a pivotal role in transforming traditional testing paradigms.\nWe have previously launched and open-sourcedCopilot for Xcode,1 an AI-assisted programming tool for code auto-\ncompletion and Q&A, which was later re-licensed and assimilated into GitHub. 2 Our initial system demonstrated\nthe effectiveness of using contextual information to enhance the capabilities of LLMs in real-time code generation\n1https://github.com/intitni/CopilotForXcode\n2https://github.blog/changelog/2024-10-29-github-copilot-code-completion-in-xcode-is-now-available-in-public-preview/\narXiv:2504.01866v2  [cs.SE]  5 Apr 2025\narXiv Template A PREPRINT\ncode updates\ngenerated\ncontent\nlocal coding environment\nCodebase\n Cloud-based LLMsUser\nacceptance\ndecison\nsignals of\nchangestesting results\nprompt constructorRAG retriever\nCopilot for Testing\nretrieved contextual\ninformation\ncontext-aware\nprompts\nFigure 1: Architecture of Copilot for Testing: It proactively observes the local coding environment, retrieves code\ncontext, and generates context-aware prompts to interact with cloud-based LLMs and provide bug fix suggestions\nsynchronized with codebase updates, aiming at more efficient and effective software testing with higher accuracy and\ncoverage.\nand problem-solving, which inspired us to extend the solution to software testing by transforming the challenge of\ngenerating code with fewer bugs into one of detecting and fixing bugs in the codebase.\nThis paper introduces Copilot for Testing, an AI-assisted testing system that provides bug detection, fix suggestions,\nand automated test case generation directly within the development environment, in sync with codebase updates. Our\nproposed testing methodology integrates a context-based RAG mechanism that interacts dynamically with local coding\nenvironments and retrieves contextual information as an enhancement to the LLMs prompts. This interaction not only\nallows the system to adapt and refine testing strategies in real-time, responding to ongoing changes within the codebase,\nbut also improves the performance of automated testing in efficiency, accuracy, and coverage. The main contributions\nof this work include:\n• We propose a software testing methodology powered by a context-based RAG module, achieving a 31.2%\nimprovement in bug detection accuracy and a 12.6% increase in critical test coverage compared to the baseline.\n• We develop Copilot for Testing, an automated software testing system that seamlessly integrates with the\ndevelopment environment to deliver bug detection, fix suggestions, and test case generation in sync with\ncodebase updates, achieving a 10.5% incremental acceptance rate for code suggestions in our user studies.\n2 RELATED WORK\n2.1 AI-Assisted Programming and Testing with Large Language Models\nThe integration of AI, particularly through Large Language Models (LLMs), has revolutionized both programming and\ntesting by automating and enhancing various tasksBeurer-Kellner et al. [2023]. AI-assisted programming tools like\nGitHub Copilot have demonstrated the potentials of LLMs in code completion and bug detection by understanding and\ngenerating codeChen et al. [2021], Nguyen and Nadi [2022], Wong and Tan [2024]. These dual problems—writing\ncorrect code and verifying code correctness—although distinct, share a common goal of reducing bugs and enhancing\ncode quality. Our work extends this paradigm by offering a unified solution to both.\n2.2 Automated Software Testing\nAutomated software testing plays a crucial role in modern software development practices. Recent advancements\ninclude test case generation with machine learning techniques which reduces manual design effortsGuilherme and\nVincenzi [2023], automated test execution which can be integrated with continuous integration pipelines, and automated\ntest results analysis which enhances decision-making processes regarding software qualitySumanth and Allam [2022].\nDespite these advancements, challenges such as flaky tests and maintenance issues persist, highlighting the need for\n2\narXiv Template A PREPRINT\nq2 q4\nq3\nq1\nq0\ns1\ns4s3\ns6s0\ns2 s5\nEmbedding\nt = ti t = ti+1\ngraph\nupdate\nemb(s2)\nUser input\nCodebase\ndetected bugs\ns1\ns4s3\ns6s0\ns2 s5\nq2 q4\nq3\nq1\nq0\nprompt\nconstruction\nLLMs\nprompt\nFigure 2: Overview of the context-based RAG module. The codebase is modeled as a graph, with individual nodes of\ncode context embeddings. These embeddings are dynamically updated based on code changes initiated by the user\nand the outputs from LLMs. The toy example shows that when a change is made to node s2, its embedding is updated,\nfollowed by updates to its neighboring nodes. The updated embeddings, which carry contextual information, are then\nutilized in prompt construction for the LLMs.\nmore robust testing methods and smarter maintenance strategies. Common metrics include accuracy, efficiency, and\ncoverageRicca et al. [2024], which also guide our evaluation strategies.\n2.3 Search-Based Software Engineering\nSearch-Based Software Engineering (SBSE) utilizes optimization algorithms to solve software engineering problems,\nwhere solutions are systematically searched based on a defined fitness function and problem representationHarman\nand Jones [2001]. It has been successfully applied to various software engineering tasks including test case generation,\nsoftware refactoring, and requirement engineering. SBSE’s flexibility in defining fitness functions and representations\nmakes it ideal for tailoring solutions to specific software engineering challenges, especially those involving large\ncomplex problem spaces. In our research, we model the problem within the SBSE framework and search for the optimal\ncontextual information for the prompt construction.\n2.4 Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) combines retrieval mechanisms with generative models to enhance output\naccuracy and contextual relevanceLewis et al. [2020]. Originally developed for natural language processing, RAG\nhas shown promise in software engineering for tasks like bug detectionPerera et al. [2020]. By integrating relevant\ncode snippets or bug reports, RAG provides essential context to Large Language Models, improving the precision\nof generated solutions. Our approach leverages RAG to refine both programming and testing processes, ensuring\nAI-generated outputs are highly informed and effective.\n3 PROBLEM STATEMENT\nWe position the research problem within the context of SBSE, where the goal is to find a solution that maximizes the\nfitness function based on the problem’s representation. In our work, we aim to enhance LLM-driven code generation\nby optimizing the contextual information provided. Here, the problem’s representation is defined as code context\nembeddings, while the fitness function reflects key metrics from automated testing, such as efficiency, accuracy, and\ncoverage. This research is thus guided by the following key questions:\nRQ1: How can we improve the performance of AI-assisted software testing by utilizing context-based RAG to enhance\nbug detection and code generation in real-time?\nRQ2: How can we scale AI-assisted software testing to handle larger software projects while maintaining testing\nefficiency, accuracy, and coverage?\n3\narXiv Template A PREPRINT\n4 METHODOLOGY\nWe propose a code context-based AI-assisted testing system that synchronizes bug detection, fix suggestion, and test\ncase generation with codebase updates, alongside the software engineer’s coding activities. The subsequent subsections\ndetail the architecture and system flow.\n4.1 Architecture\nFigure 1 illustrates the architecture of the proposed system, which includes three components: the local coding\nenvironment that involves the user and the codebase, cloud-based LLMs, andCopilot for Testing which includes the\nproposed RAG retriever and LLM prompt constructor. The local coding environment involves the user and the codebase\nwhich updates as the user edits code or accepts code suggestions. The external LLM API receives specific prompts and\ngenerates content accordingly. Copilot for Testing proactively monitors changes in the codebase, retrieves contextual\ninformation with the context-based RAG retriever, provides context-aware prompts with the prompt constructor, and\nprovides fix suggestions for detected bugs from LLMs’ outputs.\nImplemented as a plug-in for the coding IDE, Copilot for Testing typically requires access permissions to the local\ncodebase and user interactions. We demonstrate the implementation details of its application within Xcode in the later\nsection.\n4.2 Context-Based RAG Retriever\nRAG techniques generally enhance content generation by providing an additional knowledge base. The objective of\nthe proposed context-based RAG retriever is to deliver highly relevant content from the codebase to optimize code\ngeneration. It does so by retrieving contextual information that accurately reflects the current state of the local coding\nenvironment, thereby providing insightful context for the generator. In our approach, the retriever learns code context\nembeddings that capture multiple factors and dynamically adapts them based on real-time code changes and error\npatterns. These embeddings serve as structured representations of the evolving code context, helping to refine prompt\nconstruction for LLM-based testing and generation.\nTo model the codebase, we use graph-based representations, as shown in Figure 2. In this model, nodes represent code\ncontext embeddings, and edges denote connections. These embeddings incorporate several dynamically updated factors,\nincluding:\n• File Path—The relative and absolute paths in the filesystem, which provide spatial and organizational context.\n• Cursor Position—The user’s current focus within a file, indicating the most relevant areas for immediate\ncontext.\n• File Content—Semantic information derived from the code within each file.\n• Bug Logs—Historical bug reports and test outcomes, which indicate areas with higher defect frequency or\ncomplexity.\n• Graph Connectivity—Metrics such as the number of neighbors and centrality, which reflect the complexity\nand interdependence of the code.\nThe weights of these factors are assigned based on empirical evaluation where different combinations of factors were\ntested for their impact on testing performance. For instance, we found that bug logs and cursor position provided strong\nindicators of relevance for immediate context, while graph connectivity was particularly useful for identifying critical\nfile paths.\nThe embeddings evolve continuously through information propagation within the graph, triggered by code edits,\ndetected bugs, and test outcomes. Typically, this information flows from modified nodes to neighboring nodes along\nthe edges, with diminishing weight to focus updates on a localized neighborhood, to keep the computational overhead\nmanageable and the context learning accurate. Meanwhile, insights from LLM-generated test results and bug detections\nfurther refine the embeddings over time, creating a self-improving feedback loop that enhances testing precision.\nConsequently, we frame the AI-assisted testing problem as optimizing the contextual information included in the\nprompts for LLMs to enhance testing effectiveness.\n4\narXiv Template A PREPRINT\nEdit code\nRequest with context-\naware promps\nRespond\nPresent suggestions\nSignals Retrieve contextualinformation andconstruct prompts\nAccept suggestions\nUpdate the codebase\nUser Codebase Copilot for Testing\nCloud-based LLMs\nFigure 3: Sequence Diagram of Copilot for Testing. The sequence diagram illustrates the functionality of Copilot for\nTesting which enables real-time auto-synchronized testing through context-based RAG that leverages LLMs.Copilot for\nTesting receives notifications upon code updates, retrieves contextual information, and subsequently constructs prompts\nwith the proposed RAG module. Upon receiving the suggestions, the user has the option to adopt the recommendations\nand directly apply the changes to the code base.\n4.3 Context-Aware Prompt Constructor\nFollowing the RAG retriever, another key component of Copilot for Testing is the LLM prompt constructor, which\nintegrates retrieved content, user interaction history, and contextual information to generate effective prompts.\nTypically, a prompt is structured in JSON format and comprises four main components:\n1. Context System Prompt — retrieved from the local code context\n2. Message History — an aggregated collection of past interactions between the user and the copilot\n3. Current Question — the current task to be executed, such as “generate tests,\" “execute tests,\" and “analyze\ntest results\"\n4. Config System Prompt — overarching instructions, including model parameters, temperature, and mode\nsettings\nThe primary goal of the prompt constructor is to determine the optimal combination and ranking of these components\nto maximize prompt effectiveness.\n4.4 System Flow\nFigure 3 illustrates the system flow of Copilot for Testing in response to code changes made by the user. Upon detecting\nchanges, the context-based RAG module updates the codebase graph and retrieves contextually relevant information for\nprompt construction. Based on the retrieved context, it presents detected bugs or suggested fixes to the user. When\ninconsistencies, errors, or anomalous patterns are identified, it prioritizes bug detection, providing targeted fixes along\nwith contextual explanations. Otherwise, it emphasizes code completion and enhancement. The generated content is\nalso utilized to update the graph node embeddings. The sensitivity of the code change listeners can be configured to\nprevent overly frequent updates, thereby conserving computational resources.\nCopilot for Testing is designed to automatically synchronize with local codebase changes, enabling all steps in the\nworkflow to be executed in parallel as asynchronous jobs. This approach ensures scalable and efficient project-wide test\ncoverage while maintaining a self-healing testing environment. The dynamic interaction within the system not only\nstreamlines the testing process but also reduces the manual effort required from developers.\n5\narXiv Template A PREPRINT\n4.5 Implementation Details\nWe previously implementedCopilot for Xcode, an AI-assisted programming tool for XcodeTan et al. [2023]. The project\nwas unprecedented and challenging due to the restrictions in Apple’s development environment, which generally blocks\nplugins. We overcame these limitations by leveraging the Accessibility API and network services to accurately track\nuser focus points and detect real-time code edits, enabling a seamless user experience and contextually intelligent code\ngeneration. Building on this foundation, we extended the tool to implement Copilot for Testing for automated testing,\nutilizing the contextual RAG-powered LLM capabilities for software testing. These extensions include modeling and\nmaintaining embedded graphs for code repositories and customizing the context-aware prompts for bug detection, fix\nsuggestions, and test case generation.\nWe selected Xcode as our initial platform due to its highly restrictive sandbox environment, which presents significant\nchallenges for retrieving and learning local code context. By successfully overcoming these challenges in Xcode, we\ndemonstrate the robustness and adaptability of our approach. The framework’s design is inherently platform-agnostic,\nrelying on modular components such as the RAG module and graph-based embeddings, which can be easily adapted to\nother IDEs with fewer restrictions. This ensures that expanding to other platforms, such as Visual Studio, IntelliJ, or\nEclipse, will require minimal adjustments, primarily involving the integration of platform-specific APIs for context\nretrieval.\n5 Evaluation\nWe evaluate the performance ofCopilot for Testing via both objective and subjective experiments, in comparison with\nthe baseline model which does not leverage the context-based RAG module. Experiment results have proved the validity\nand usability of Copilot for Testing, demonstrating its outstanding performance compared to the baseline. As aligned\nwith common topline metrics in test automation15, we assess the effectiveness of Copilot for Testing from the following\naspects.\n• Accuracy—Evaluated by the bug detection rate, emphasizing the method’s ability to identify complex bugs\ninvolving multi-file interactions, thus demonstrating its precision in detecting subtle issues.\n• Efficiency—Evaluated by the acceptance rate of the suggested fix for detected bugs or generated test cases\nsince accepted generations directly imply saved engineering efforts. It is also understood from the subjective\nfeedback from participants in the user study regarding their experiences in real projects.\n• Coverage—Evaluated by the proportion of the codebase covered by automatically generated tests. We prioritize\ntest coverage on critical code paths to optimize efficiency, rather than maximizing overall coverage, which\ncould lead to unnecessary tests and increased costs, in order to focus on essential areas to the application’s\nfunctionality\nWe utilized a database of known issues and test cases derived from open-source Swift projects and adapted C++\nprojects from the Software-artifact Infrastructure Repository.3 This provided a diverse set of challenges representative\nof real-world software development. Our evaluation included both objective experiments, to quantitatively measure\nthe accuracy of the testing method, and subjective experiments, where software engineers evaluated the efficiency in\npractical scenarios.\n5.1 Objective Evaluation\nWe conducted objective evaluations using a curated database of Swift and C++ projects from SIR, each containing\nknown bugs (artificial faults called “mutants\") of varying complexity, to assess theaccuracy, overall coverage, and\ncritical coverage of the proposed testing methodology in comparison to the baseline model.\nFollowing the standardized workflow of experimenting with SIR programs,4 we executed the subject programs with\ntheir test cases and mutants, collecting fault-revealing and coverage data for tracking analysis. Accuracy was measured\nas the percentage of successfully detected bugs, while coverage was assessed as the percentage of code paths covered by\ngenerated tests. Additionally, we introduced “critical coverage\", which prioritizes high-impact code areas most relevant\nto system functionality. This ensures an efficiency-driven balance between exhaustive test coverage and practical\nmaintainability.\nThe results are summarized in Table 1. Our evaluation revealed that the proposed testing methodology achieved a31.2%\nhigher bug detection rate compared to the baseline model, excelling at identifying intricate bugs that involve cross-file\n3https://sir.csc.ncsu.edu/\n4https://github.com/jwlin/SIR-note/blob/master/workflow.md\n6\narXiv Template A PREPRINT\nTable 1: Evaluation Results: Proposed Model vs. Baseline Model\nMetric Proposed Model Baseline Model Change from Baseline\nBug Detection Accuracy 85.3% 54.1% +31.2%\nOverall Test Coverage 68.7% 70.0% -1.3%\nCritical Coverage 83.6% 71.0% +12.6%\nCross-File Bug Detection 81.2% 49.0% +32.2%\nExecution Time Per Bug 0.42 seconds 0.68 seconds -\nSuggestion Acceptance Rate 31.9% 21.4% +10.5%\ndependencies with a 32.2% higher detection rate. While the overall test coverage was slightly lower ( -1.3%) than\nthe baseline model, this reduction is a strategic trade-off, as the focus shifted to enhancing “critical coverage” which\nincreased by 12.6%. To compute the “critical coverage” rate, we leveraged graph node embeddings which encompass\nfactors like bug logs, change logs, and complexity of dependencies to represent the importance level of each code path\nand then measured the test coverage among the most important code paths.\nThe contrasting results between overall and critical test coverage highlight a strategic tradeoff: by prioritizing critical\ncode paths, the proposed system achieves higher efficiency and effectiveness in detecting complex, high-impact bugs,\neven with a marginal reduction in overall coverage. This tradeoff is justified by the significant improvements in bug\ndetection rate and execution efficiency, as the overall execution time per detected bug was reduced from 0.68 seconds to\n0.42 seconds. These results demonstrate that the slight reduction in overall coverage is outweighed by the substantial\ngains in critical coverage and system efficiency, making the approach highly valuable for developers who prioritize\nidentifying critical bugs over exhaustive but less impactful testing.\nThe advanced performance of the proposed model in accuracy can be attributed to its dynamic adaptation to code\nchanges and the deep contextual insights it provides. Compared to the baseline model, the proposed system’s use of\ncontextual information enables a more nuanced understanding of the code structure, making it more adept at capturing\nsubtle and complex bugs. The usage of graph-based embeddings offers higher information density, reducing the risk of\ndistracting the LLM with less relevant data.\n5.2 Subjective Evaluation\nTo evaluate the efficiency and usability ofCopilot for Testing, we conducted a user study with 12 iOS developers. The\nstudy aimed to assess key metrics such as the acceptance rate of suggestions and gather qualitative feedback on the\ntool’s practical applicability. Participants were divided into two groups: the test group using the proposed module and\nthe control group using the baseline version. They were asked to complete testing-related tasks, designed to reflect\ncommon testing workflows, including debugging, generating test cases, and verifying the functionality of algorithms,\nUI elements, and database operations. The acceptance and rejection of code suggestions from both models were logged,\nand participants were also asked to evaluate the ease of use, compatibility with existing development practices, and\noverall impact on their testing workflows.\nThe acceptance rate was computed as the ratio of accepted suggestions to total suggestions triggered. Results show a\n10.5% higher acceptance rate with the proposed model compared to the baseline. Additionally, participants reported\na significant reduction in manual testing efforts, highlighting the value of automated test case generation. They also\nappreciated the system’s ability to adapt quickly to code changes and provide timely feedback.\nOn the other hand, participants noted a steep learning curve during the initial setup of the testing module. They also\nobserved slower response times during bulk operations, such as mass file generation or removal, which we attributed\nto the graph structure update process. When asked about desired features, participants expressed a strong interest in\nautomated test execution with integrated results for bug detection and fixes, as well as support for multiple programming\nlanguages and development environments. These insights offer valuable directions for further enhancements and\nrefinement of the system.\n6 FUTURE RESEARCH DIRECTIONS\nAs we continue to refine and expand the capabilities of the proposed automated testing module, our research and\ndevelopment efforts will focus on several key areas:\n1. End-to-End Parameter Tuning: In the current testing system, many parameters, such as the weights assigned\nto different sources of information for node embedding and the ranking of components for prompt construction,\n7\narXiv Template A PREPRINT\nare configured based on optimal values determined through trial and error. In future studies, we aim to\nincorporate these parameter settings into an end-to-end dynamic training procedure, enabling the system to\nlearn and optimize these values automatically.\n2. User Experience Improvements: We plan to enhance the user experience by implementing algorithmic\noptimizations to reduce loading and pending times, ensuring smoother performance when working with large\ncodebases. For example, we will leverage caching mechanisms for executed test results to eliminate redundant\nwork and improve efficiency. Additionally, to reduce the learning curve, we will introduce in-UI tips that\nprovide step-by-step guidance, helping users navigate the system more effectively during setup and daily use.\n3. Expansion to Broader Platforms: We aim to extend the tool’s compatibility to multiple development\nenvironments and programming languages beyond Xcode and Swift. The proposed RAG module and graph-\nbased embeddings are language-agnostic and platform-independent, making them easily generalizable to other\nless restrictive platforms. For example, integrating the framework into another IDE would primarily involve\nadapting the context retrieval mechanism to leverage that platform’s local API, while the core RAG and testing\nlogic would remain unchanged. This expansion will enhance the proposed solution’s accessibility and appeal\nto a broader range of developers, thereby improving its usability and adoption.\n7 CONCLUSION\nIn this study, we proposed a software testing solution with context-based RAG to enable bug detection, fix suggestions,\nand test case generation synchronized with coding, as an effective extension from AI-assisted code generation to\nsoftware testing. We validated its improved performance in accuracy, efficiency, and coverage over the baseline through\nboth objective and subjective experiments. These results demonstrate the potential of leveraging code contextual\ninformation to enhance LLMs for code generation and project development, helping to keep pace with the increasing\ncomplexity and scale of modern software systems, while establishing a foundation for future innovations in AI-assisted\ndevelopment tools.\nAcknowledgment\nThis research was supported by the Singapore Ministry of Education Academic Research Fund under Grant RG91/22.\nReferences\nFilippo Ricca, Alessandro Marchetto, and Andrea Stocco. A multi-year grey literature review on AI-assisted test\nautomation. arXiv preprint arXiv:2408.06224, 2024.\nDaniel Russo. Navigating the complexity of generative ai adoption in software engineering. ACM Transactions on\nSoftware Engineering and Methodology, 33(5):1–50, 2024.\nRobert Feldt, Francisco G de Oliveira Neto, and Richard Torkar. Ways of applying artificial intelligence in software\nengineering. In Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in\nSoftware Engineering, pages 35–41, 2018.\nNadia Alshahwan, Mark Harman, and Alexandru Marginean. Software testing research challenges: An industrial\nperspective. In 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), pages 1–10. IEEE,\n2023.\nLuca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: A query language for large\nlanguage models. Proceedings of the ACM on Programming Languages, 7(PLDI):1946–1969, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\nNhan Nguyen and Sarah Nadi. An empirical evaluation of GitHub Copilot’s code suggestions. In Proceedings of the\n19th International Conference on Mining Software Repositories, pages 1–5, 2022.\nMan Fai Wong and Chee Wei Tan. Aligning crowd-sourced human feedback for reinforcement learning on code\ngeneration by large language models. IEEE Transactions on Big Data, 2024.\nVitor Guilherme and Auri Vincenzi. An initial investigation of ChatGPT unit test generation capability. In Proceedings\nof the 8th Brazilian Symposium on Systematic and Automated Software Testing, pages 15–24, 2023.\n8\narXiv Template A PREPRINT\nTatineni Sumanth and Karthik Allam. Implementing AI-enhanced continuous testing in devops pipelines: Strategies for\nautomated test generation execution and analysis. Blockchain Technology and Distributed Systems, 2:46–81, 2022.\nMark Harman and Bryan F Jones. Search-based software engineering. Information and software Technology, 43(14):\n833–839, 2001.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler,\nMike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP\ntasks. Advances in neural information processing systems, 33:9459–9474, 2020.\nAnjana Perera, Aldeida Aleti, Marcel Böhme, and Burak Turhan. Defect prediction guided search-based software\ntesting. In Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, pages\n448–460, 2020.\nChee Wei Tan, Shangxin Guo, Man Fai Wong, and Ching Nam Hang. Copilot for Xcode: exploring AI-assisted\nprogramming by prompting cloud-based large language models. arXiv preprint arXiv:2307.14349, 2023.\n9"
}