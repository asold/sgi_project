{
  "title": "Transformer Based Traffic Flow Forecasting in SDN-VANET",
  "url": "https://openalex.org/W4367280678",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4323197176",
      "name": "Ali Abir Shuvro",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4367407167",
      "name": "Mohammad Shian Khan",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2517444081",
      "name": "Monzur Rahman",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2292477322",
      "name": "Faisal Hussain",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1971523869",
      "name": "Md Moniruzzaman",
      "affiliations": [
        "Islamic University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2312268330",
      "name": "Md. Sakhawat Hossen",
      "affiliations": [
        "Islamic University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4220669265",
    "https://openalex.org/W3047685524",
    "https://openalex.org/W3091636705",
    "https://openalex.org/W6698177178",
    "https://openalex.org/W2787510452",
    "https://openalex.org/W2040340473",
    "https://openalex.org/W2901504064",
    "https://openalex.org/W4323060042",
    "https://openalex.org/W2980820804",
    "https://openalex.org/W2125432598",
    "https://openalex.org/W2891809631",
    "https://openalex.org/W3098623161",
    "https://openalex.org/W2906154449",
    "https://openalex.org/W2516812769",
    "https://openalex.org/W2916664939",
    "https://openalex.org/W2586472645",
    "https://openalex.org/W4225016315",
    "https://openalex.org/W2028926203",
    "https://openalex.org/W3045604743",
    "https://openalex.org/W2115377324",
    "https://openalex.org/W3049183817",
    "https://openalex.org/W1965968818",
    "https://openalex.org/W2790585829",
    "https://openalex.org/W7071271474",
    "https://openalex.org/W2915905517",
    "https://openalex.org/W2150093579",
    "https://openalex.org/W2010947830",
    "https://openalex.org/W3019523662",
    "https://openalex.org/W4206957930",
    "https://openalex.org/W2403292383",
    "https://openalex.org/W2789926203",
    "https://openalex.org/W2086984184",
    "https://openalex.org/W2145286644",
    "https://openalex.org/W2984228170",
    "https://openalex.org/W2084718899",
    "https://openalex.org/W6773017188",
    "https://openalex.org/W3034749137",
    "https://openalex.org/W3125634432",
    "https://openalex.org/W2307718269",
    "https://openalex.org/W3151329197",
    "https://openalex.org/W3000386982"
  ],
  "abstract": "Intelligent Transportation System (ITS) provides services for proper traffic assistance. ITS helps in creating a transportation system that is smart, safe and efficient. Vehicular Ad-hoc Network supplies internet connectivity to vehicles and helps in traffic guidance. This paper uses a modified transformer architecture for time-series vehicular data to predict traffic flow. Time-series sequences are generated from the dataset for capturing temporal dependencies. Our proposed transformer-based model has been engineered to capture inter-feature correlations along with inter-sample correlations. The 2D-Transformers model has a significant decrease in error compared with Transformers and LSTM-based models. The prediction generated from the model can be transmitted throughout a network of vehicles. So, a holistic networking model is proposed where the vehicles will be connected to Road-side Units (RSUs) and the backbone network will be Software Defined Network (SDN). The traditional design principles, that incorporate data, control and management planes together in a network device, are incapable to adapt with this much data growth, bandwidth, speed, security, and scalability compared to SDN as it provides with centralized programmable mechanism reliably. The trained parameters learned using the transformer model can be passed throughout the network for traffic guidance.",
  "full_text": "Received 22 March 2023, accepted 20 April 2023, date of publication 27 April 2023, date of current version 3 May 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3270889\nTransformer Based Traffic Flow Forecasting\nin SDN-VANET\nALI ABIR SHUVRO\n , MOHAMMAD SHIAN KHAN\n, MONZUR RAHMAN, FAISAL HUSSAIN,\nMD. MONIRUZZAMAN\n , AND MD. SAKHAWAT HOSSEN\nDepartment of Computer Science and Engineering, Islamic University of Technology, Gazipur 1704, Bangladesh\nCorresponding author: Md. Sakhawat Hossen (sakhawat@iut-dhaka.edu)\nABSTRACT Intelligent Transportation System (ITS) provides services for proper traffic assistance. ITS\nhelps in creating a transportation system that is smart, safe and efficient. Vehicular Ad-hoc Network supplies\ninternet connectivity to vehicles and helps in traffic guidance. This paper uses a modified transformer\narchitecture for time-series vehicular data to predict traffic flow. Time-series sequences are generated from\nthe dataset for capturing temporal dependencies. Our proposed transformer-based model has been engineered\nto capture inter-feature correlations along with inter-sample correlations. The 2D-Transformers model has\na significant decrease in error compared with Transformers and LSTM-based models. The prediction\ngenerated from the model can be transmitted throughout a network of vehicles. So, a holistic networking\nmodel is proposed where the vehicles will be connected to Road-side Units (RSUs) and the backbone network\nwill be Software Defined Network (SDN). The traditional design principles, that incorporate data, control\nand management planes together in a network device, are incapable to adapt with this much data growth,\nbandwidth, speed, security, and scalability compared to SDN as it provides with centralized programmable\nmechanism reliably. The trained parameters learned using the transformer model can be passed throughout\nthe network for traffic guidance.\nINDEX TERMSVehicular ad-hoc network, transformers, sequence length, encoders, attention, traffic flow,\nsoftware-defined network.\nI. INTRODUCTION\nTHE Intelligent Transportation Systems (ITS) technologies\nprovide services to better traffic and transportation informa-\ntion. It makes transportation smarter by providing meaning-\nful insights into traffic data which are captured by highly\nadvanced traffic detection sensors, and in essence, makes the\nroads safer and more coordinated. Traffic data is captured via\ndetectors/sensors, so traffic forecasting has been critical to the\ndevelopment of ITS.\nTraffic data can be spatial-temporal data, which means it\ncontains information about the time and space of the vehicles\nat a given time. Additional data might be present as the\naverage speed, occupancy, etc. which will help us forecast\nthe traffic flow. LSTM models have a higher performance\nin comparison to models which incorporate algorithms like\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Qingchun Chen\n.\nHTM [1]. Short-term prediction of traffic conditions is nec-\nessary for overcoming traffic congestion [2].\nTime-series traffic data is the new and improved way of\npredicting traffic as it can utilize sequential data to provide a\nproper prediction. Perfect use of sequential machine learning\nmodels can also be implemented. Transformers have been\nseen to produce remarkable results in this regard.\nVehicular Ad-hoc Networks or V ANETs play a key role\nin ITS. Safety and roadside equipment communication of\nvehicles is a key factor when it comes to V ANET. It ensures\nvehicle-to-vehicle and vehicle-to-infrastructure communica-\ntion which makes it easier for ITS to communicate and\ncollect more information. V ANET can be incorporated with\nSoftware Defined Network or SDN as in the architecture\nthere are SDN components (SDN controller, SDN wireless\nnodes, SDN RSU). Software-Defined V ANET or SDVN can\noperate differently based on the degree of control of the\nSDN controller. SDVN also provides some benefits which\n41816\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME 11, 2023\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\ncan be utilized to provide many services [3]. Sadio et al. have\nproposed a design of a complete SDVN prototype [4]. Such\ndesigns can be used with our work to incorporate the trained\nweights of the models into the SDN-enabled network.\nTraffic prediction is quite a challenge to tackle as spatial-\ntemporal data is needed to be handled [5]. Traffic prediction\ndata can be represented with a graph and Graph Neural\nNetwork along with RNN can be used for the prediction of\ndata [6].\nHowever, there are some limitations to it due to the con-\ntinuous nature of time and the adjacent behaviour of space.\nThe use of RNNs is limited due to temporal dependen-\ncies for exploding and vanishing node problems. GRU and\nLSTM overcome this shortcoming of RNN. Therefore, sev-\neral algorithms combining Traffic Transformer have emerged\nas it works well with sequential data [6]. A combination of\nGCN and transformer has outperformed several other models\nmainly consisting of RNN in the past [5].\nTraffic data is in time-series format, so through a\nTransformer-based learning model, traffic flow can be fore-\ncasted with better performance and a computationally effi-\ncient model. The memory of the Transformer has no bound\nwith available resources. Multidimensional time-series data\nis necessary for more accurate prediction. Incorporating mul-\ntidimensional data can open a door for handling a different\nkind of situation in case of prediction. Our Transformer is\ncapable of handling such multi-featured time-series data effi-\nciently. Furthermore, a holistic model can be suggested which\nincorporates gathering data from the RSUs, learning using the\nSDN controllers and finally passing those learned weights to\nthe RSUs and eventually to the vehicles.\nTraining a time-series dataset on a learning model is espe-\ncially challenging as it involves making some adjustments to\npredict long-term dependencies in case of time, and variable\nspatial dependencies. The core contributions of the work can\nbe summarized as follows:\n1) Using a state-of-the-art transformer model which pro-\nvides more accurate results for the time series data.\n2) Introducing the 2D-Transformers model with 2D\nMulti-head Attention Mechanism and using the\nmultivariate model, inter-feature dependencies were\nensured.\n3) Proposed a holistic model for traffic guidance in an up-\nand-coming SDN-based V ANET.\nII. RELATED WORKS\nVehicular Ad Hoc Networks are a potential field of research\nand have caught a lot of interest. It is this eye-catching\nbecause of its potential to provide vehicle road safety,\nincrease the efficiency of traffic and provide convenience\nand comfort for passengers and drivers. Mobile Vehicular\nCloud and review cloud applications can be considered as sig-\nnificant technology to develop intelligent transportation [7].\nVehicular ad hoc networks (V ANET) refers to the creation of\na mobile ad-hoc network created in the domain of vehicles\nwhich supports communications among vehicles, roadside\nunits and base stations to provide safe and efficient trans-\nportation. There are three types of architecture in V ANET\nand those are pure cellular wireless local area networks,\npure ad-hoc networks and hybrid networks [8]. There are\nbasic characteristics of Vehicular networks. There are many\napplications associated with this field. There are require-\nments which come with many challenges. Solutions are also\nthere which can be considered to overcome challenges [9].\nWith the advancement of technology and the establishment\nof smart cities, there is an increasing need for Intelligent\nTransport Systems (ITS). Safety, communication and traffic-\nrelated issues are one of the major concerns of ITS. There are\nmachine learning techniques which can address these issues\nin a feasible manner and overcome such challenges [10].\nSecurity is a concern in the case of routing of the V ANET\nnodes. Enhancing network robustness, defence mechanisms\nwith low-security overhead can be introduced [11]. As per\nsecurity is concerned, blockchain technology has been incor-\nporated with V ANET to secure ride-sharing applications. The\nProof of Driving (PoD) in a blockchain-based ride-sharing\nservice in V ANET, which consumes fewer resources than\nProof of Work (PoW), maintains a fair selection than Proof\nOf Stake (PoS) as well as the randomness of consensus nodes\nin a publicly distributed network of vehicles [12].\nThe widespread IP network is a very complex and difficult\ncase of management, to configure according to predefined\npolicies as well as reconfigure according to faults, loads\nand changes. These networks are vertically integrated which\nmeans the control and data plane are bundled together. Soft-\nware Defined Networking is such a field that breaks this\nvertical integration separating the control logic from routers,\nand switches and provides flexibility, dynamic, optimal and\ncentralized logical control over the system [13], [14]. Due\nto design limitations, the traditional network is unable to\nmake users interact with the traffic or shape their own traffic\npolicies. Once the flow management (forwarding policy) has\nbeen determined, the only way to adjust will be by changing\nthe configuration of the devices which has restricted the\nnetwork operator who wants to scale their networks on traffic\ndemands. This leads to SDN as it provides with central-\nized programmable mechanism [15]. Northbound API in the\nControl layer is used to program the network and request\nservices from the Application layer by the applications and\noverall management system. An OpenFlow protocol which\nis often used as a southbound API is used between the\nControl layer and Infrastructure layer [16]. Wireless sensor\nnetworks are low-rate networks with few resources and short\ncommunication ranges and when it expands it faces network\nmanagement and heterogeneous-node networks challenges.\nHere SDN can be incorporated with wireless sensor networks\nto bring efficiency and sustainability and it is called Software\nDefined Wireless Sensor Networks (SDWSN). This model\ncan also play a critical role in incorporating SDN with the\nInternet of Things (IoT) [17]. IoT is a network which is open,\nVOLUME 11, 2023 41817\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\ngeographically distributed, and consists of heterogeneous\nnetworking infrastructures. Managing these in dynamic sit-\nuations is an important challenge. This challenge can be\novercome by using SDN with IoT. SDN can be designed for\nIoT for dynamically achieving quality to different IoT tasks in\nheterogeneous wireless networking [18]. For smart cities an\nIoT-SDN structure can be incorporated to bring benefits like\nenergy savings, network scalability and load balancing [19].\nThe rapid growth of the internet and mobile communica-\ntion technology has led us to more complexity. Now, these\nneed to be efficiently organized, managed and optimally\nmaintained. This is why more intelligence is needed to be\nincorporated and traditional networks failed to do so. SDN\nbrings the chance to use machine and deep learning and\nprovide this much-needed intelligence in the networks in this\nBig-Data world for its logically centralized control, global\nview of the network, software-based traffic analysis and\ndynamic updating of forwarding rules [20], [21]. SDN can\nbe therefore used to tackle network management challenges\nwhich include erratic network conditions and states, high-\nlevel language support for network setup, improved visibility,\nnetwork diagnostics, troubleshooting and flow classification\n[22], [23], [24].\nFor the rapid growth of roadside accidents, to ensure pas-\nsengers’ safety, an ad-hoc network that is vehicular ad-hoc\nnetwork is being encouraged. Managing the whole network\nfrom a single remote controller, SDN-V ANET helps to over-\ncome the drawbacks and complexity of standard V ANET\narchitecture [25]. The adaptability, controllability and versa-\ntility of controlling the network as a whole of SDN help to\nbuild an effective and secure V ANET with simplified network\ncontrol. When V ANET is implemented with SDN, in the\narchitecture there are SDN components (SDN controller,\nSDN wireless nodes, SDN RSU) which are incorporated with\nV ANET and so Software Defined V ANET can operate differ-\nently based on the degree of control of the SDN controller [3],\n[26]. SDN is used in Multi-access Edge Computing for the\neffective management of networks and services. Integration\nof SDN can leverage several advantages which are also useful\nfor the incorporation of 5G [27], [28]. A global optimal route\ncan be found to efficiently propagate messages from source to\ndestination with dynamic network density in SDVN. Central-\nized Routing Protocol is the SDN-based routing framework\nthat uses modified Dijkstra’s algorithm for efficient message\npropagation in V ANET and outperforms some other tradi-\ntional protocols [29].\nSDVN can be used to detect and predict traffic collab-\norating with machine learning. K-means clustering can be\nused to detect and LSTM with RNN can be used to then\npredict the traffic condition [30]. HTM can be used along-\nside LSTM for short-term arterial traffic prediction. Several\nMachine learning techniques have been deployed to predict\ntraffic state and also vehicles [31]. LSTM models have a\nhigher performance in comparison to models which incorpo-\nrate algorithms like HTM [1]. Short-term prediction of traffic\nconditions is necessary for overcoming traffic congestion [2].\nResults by applying a self-adjusted Neural Network (NN)\nhave been satisfactory as well [32]. Vacant parking spots have\nbeen predicted using different models like Support Vector\nRegression (SVR), LSTM and dense convolutional Neural\nNetworks with LSTM which showed significant improve-\nments in the domain of time-series prediction [33], [34], [35].\nSeveral models revolving around graph convolutional net-\nwork has also been effective in predicting short-term traffic\ndata. Temporal Graph Convolutional network yields decent\nperformance when it comes to solving this particular problem\nof traffic prediction [36]. A long short-term memory (LSTM)\nbased regression model can be used to predict 24-hour traffic\ncounts data. The algorithm was not discussed along with the\ndataset used [37]. Newer and better models can be used in\nthis sector to improve performance and proper analysis would\nhelp understand the internals of the prediction models.\nTraffic prediction is quite a challenge to tackle as spatial-\ntemporal data is needed to be handled [5]. Traffic prediction\ndata can be represented with a graph and Graph Neural\nNetwork along with RNN can be used for the prediction\nof data [6]. However, there are limitations to it due to the\ncontinuous nature of time and the adjacent behaviour of\nspace. The use of RNNs is limited due to temporal dependen-\ncies for exploding and vanishing node problems. GRU and\nLSTM overcome this shortcoming of RNN. Therefore, sev-\neral algorithms combining Traffic Transformer have emerged\nas it works well with sequential data [6]. A combination of\nGCN and transformer has outperformed several other models\nmainly consisting of RNN in the past [5].\nTime series data with proper sequencing can be used to\nprovide a suitable solution to some of the existing problems.\nThe short-term traffic condition will be addressed in a more\nefficient manner if time series data is used to train the model.\nAgain, since multiple features are naturally present in these\ntime series data, using these features to find a prediction\naltogether would yield a much better result. Modification of\nthe classic transformer architecture to incorporate multiple\nvariables in each of the time sequences is necessary.\nIII. METHODOLOGY\nThe aim of this study is to forecast the average occupancy\nof a station at a certain time given an already observed time-\nseries sensor-generated vehicular dataset. Basically, given the\nstation, the number of samples, percent observed, total flow,\naverage speed and some other parameters, the prediction of\naverage occupancy can be obtained.\nTime-series analysis on sequential data was conducted\nfor effective prediction of traffic flow. By training a model\non time-series data, accurate predictions of the condition\nof traffic can be produced in the future. Again, predictions\nat specific time intervals are possible using this methodol-\nogy. After preprocessing the collected time-series data, the\nobtained sequential data can be used for accurate predictions.\nDifferent deep learning models were used which have been\n41818 VOLUME 11, 2023\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nFIGURE 1. Architecture overview.\nproved in the past to have worked phenomenally well in terms\nof time-series mode of data, especially in the case of traffic\nflow [37].\nA. MODEL ARCHITECTURE\nAs shown in Figure 1, the proposed system consists of three\nsegments. They are Data Collection, Traffic Flow Prediction\nModel and underlying SDVN Architecture.\nAfter the collection of the data, it was preprocessed; the\nsteps involved cleaning, reduction, and transformation of\ndata. For transforming the traffic time-series data into sequen-\ntial data, the data had to undergo sequence generation. The\nfinal sequential data is used in the training of the model.\nA separate controller or server is used to train the model.\nThe trained weights obtained from the model are fed to\nthe SDVN architecture. The SDN controller propagates these\nweights or model parameters to the RSUs which these units\nfinally use to find predictions. The predictions are transferred\nto the vehicles. The vehicles in turn also provide the live\ntraffic data which is further sent down by the RSUs to the\ncontroller for newer optimized model parameters.\nIV. IMPLEMENTATION\nThe segments mentioned in section III-A that need to be\nimplemented are described in this section. Along with that,\nwe also talk about the dataset that we used for our experi-\nment. Let us look at the environment used for conducting the\nexperiment.\nTABLE 1. Variables along with their unique frequencies existing in the\ndataset.\nA. DATASET\nThe dataset we used was obtained from Caltrans Performance\nMeasurement System (PeMS) [38]. This dataset is used as a\nstandard as it contains real-life data with a lot of unforeseen\noutcomes. The dataset seems to behave similarly compared\nto many other datasets [5]. Nearly 40,000 traffic sensors are\ndeployed in almost all notable streets in the state of California\nand traffic data is collected everyday. Hourly data for the\nwhole year of 2021 from PeMS district 7 was taken and\nfinally, an 80-20 train-test split was performed.\nMultivariate time-series data sequences are used. For that\nreason, the variables or features which was selected are:\nB. PREPROCESSING\nAt first, raw hourly data was collected from the PeMS web-\nsite. This included selecting a specific year and district where\nthere are adequate busy streets for variation. Some of the\nadditional features were trimmed for making the experiment\ncomputationally feasible. Next, mean normalization was used\non the dataset so that the values are in the range of [0, 100].\nThis ensures that the model is not biased towards only a few\nfeatures.\nC. GENERATING SEQUENCE\nIn order to convert a time-series dataset into sequential\ndata, it is necessary to make a sequential copy of a por-\ntion of the data and each time introduce 1 more sample.\nSequence generation is important so that the model gets\nan idea of past and present situations in different spa-\ntial and temporal perspectives in that area which has an\nimpact on future conditions. Each time, the model gets\nthe whole sequence of data and predicts just a single\nsample.\nWhile creating sequences, the data is arranged in such a\nway that data of (n-1) th sample of all other variables will\nbe along with n th data of the target variable. This is made\nlike this so that when any prediction algorithm is used,\nit genuinely picks the future target variable based on the\npresent values of the other variables. The process of creating\nsequences takes just a few minutes. The sequence number that\nis picked determines the time required for training. For a large\nsequence length, the training time becomes large as well, and\nVOLUME 11, 2023 41819\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nFIGURE 2. Processing sequences in the dataset.\nTABLE 2. Shape of train and test dataset with respect to sequence\nnumber.\nvice-versa. In fact, the dataset size will be exactly the multiple\nof the sequence number. The shapes of the datasets obtained\nare shown in the table below:\nD. TRAINING\nThe Transformer Architecture which is mainly used in NLP\nfor word generation and translation is similar to category\nprediction or logistic regression. For the regression of contin-\nuous data, the basic transformer architecture has been made\naligned to the regression model which can fit the regression\ndata properly.\nAfter setting every module of the architecture properly, the\ntraining data is passed to the Data Generator which generates\nbatch-wise data. This batch-wise data is at first fed to the\nEmbedding layer and then to the Positional Encoding layer.\nThe encoded data carries information about the relative order\nof a sequence sample in the input sequence. The encoded\ndata makes it possible for the Transformer to use the feature\nand positional information about the order of the sequence\nsample. The encoded data generated from this module is\npassed to the Encoder. The exact same process is used to\nrepresent the input to Decoder as well.\n1) THE ENCODER\nThe Encoder consists of N Encoder Layers which are iden-\ntical and stacked upon each other. Each of the encoder lay-\ners consists of 2 sub-layers namely Multi-Head Attention\nand Position-Wise Feed Forward Network. The Multi-Head\nAttention layer is based on the self-attention mechanism in\nFIGURE 3. 2D multi-head attention mechanism.\nmultiple heads and the Positional Feed Forward networks that\nconsist of fully connected Neural Networks which are applied\nto each of the positions.\n2) 2D MULTI-HEAD ATTENTION LAYER\nIn the case of traffic flow, there are several factors which\nneed to be considered. These factors are responsible for traffic\nconditions in a given time step. This is why sequences of\nmultiple features are needed in case of such time series traffic\nflow forecasting.\nThere are several attention heads inside the multi-head\nattention mechanism and each has a self-attention mecha-\nnism. The theory behind it is to give the model more rep-\nresentational power. That means it will be able to learn\nfrom different points of view generalizing more forms of\nrepresentation, thinking that each head will learn something\ndifferent. As multiple features are included in such situations,\nthe standard self-attention mechanism of the transformer is\nnot enough to focus on more than one feature along with\nfocusing on a sequence of samples because transformers are\nbuilt to work with word sequences which consist of a single\nword at a specific time instead of a group of features.\n41820 VOLUME 11, 2023\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nAs per Figure 3, this self-attention mechanism needs two\nkinds or dimensions of six linear layers. The first dimension\nof the linear layer is focused on features and the second is\non the sequence. The encoded data from the previous layer\nis used to calculate f _query, f _key and f _value. These are\ncalculated after the input is fed to feature focused linear layer.\nf _query = LinearQF (s) (1)\nf _key = LinearFK (s) (2)\nf _value = LinearFV (s) (3)\nThe f _query and f _key are multiplied to calculate the\nf _score matrix. It determines how much focus a feature\nshould put on every other feature. This is a feature-to-feature\ncalculation. Each feature will have a score that corresponds\nto other features in that time step. It is divided by the square\nroot of dk which represents the number of features per head\nto ensure stability mitigating exploding effect.\nThe f _score matrix is used to generate a new query, key\nand value with the help of a sequence-focused linear layer.\nThe f _score matrix is fed to sequence focused linear layer\nresulting in sf _query, sf _key, sf _value. These three outputs\nwill represent the sequence as well as implicitly keep atten-\ntion information among the features.\nsf _query = LinearQSF (f _score) (4)\nsf _key = LinearKSF (f _score) (5)\nsf _value = LinearVSF (f _score) (6)\nThen the sf _query and sf _key will be multiplied to cal-\nculate the sf _score matrix which will undergo a softmax\noperation generating the attention weights. These weights\ndenote the self-attention among each and every sequence as\nwell as features.\nAttention(QF , KF , VF ) = (softmax(QSF × KT\nSF\n√dk\n)VSF )VF\n(7)\nThen the attention weights are used to do a weighted sum\nof the sf _value followed by another weighted sum operation\nusing f _value to generate the output vector. Thus important\nvalues will get priority. Then the output vector is fed to the\nfinal linear layer with a residual connection to produce an\noutput which is ready to go to the next layer.\nThere are several attention heads inside the multi-head\nattention mechanism which has the self-attention mechanism\nindividually. The theory behind it is to give the model more\nrepresentational power thinking that each head will learn\nsomething different.\n3) POSITION-WISE FEED FORWARD NETWORK\nThis sub-layer of an encoder layer is nothing but a fully\nconnected feed-forward network with a residual connection.\nIf the input is x then:\nPFFN(x) = ReLU(xW1 + b1)W2 + b2 (8)\nThe fully-connected layer is applied which has weights\nW1 and biases b1. ReLU non-linearity, max with zero,\nis applied to it followed by another fully-connected layer with\nweights W2 and biases b2.\n4) THE DECODER\nThe concept is similar to the encoder. The decoder con-\nsists of N Decoder Layers which are identical and stacked\nupon each other. Each of the decoder layers consists of\n3 sub-layers namely Masked Multi-Head Attention, Encoder-\nDecoder Multi-Head Attention and Position-Wise Feed For-\nward Network.\n5) GENERATING FEATURE WEIGHTS\nThe output from the last decoder layer is passed on to the\nGenerator. The Generator basically has a Linear layer and a\nSoftmax layer. It produces the impact factor of each feature\nto produce the target ‘‘Average Occupancy’’. This denotes the\ncontribution of each feature to the target value. The weighted\nsum of the features will be the target output value. In the train-\ning phase, the prediction is compared with the actual value\nand loss are computed and the loss is backpropagated. There,\nthe parametric weights of the model are updated which are\nused later for inference and finally to calculate the predicted\nvalue.\nV. RESULT AND DISCUSSION\nA. ENVIRONMENT AND CONFIGURATION\nGoogle Colab and Google Colab Pro have been used for the\nexperiment. As the experiment is resource-heavy, in most\ncases, Google Colab Pro has been the go-to notebook envi-\nronment. Google Colab Pro proves a heavy RAM option\nwhich gives an average of 25 GB RAM to its users. A Tesla\nP100 GPU with 16 GB memory and a storage capacity of\naround 200 GB.\nB. EVALUATION METRICS\n1) MSE\nMSE or Mean Squared Error is an error calculating method.\nIt is mainly used to show how close a regression line is to a\nset of predictable points. If Y be the actual output and ˆY be\nthe predicted output, then we get:\nMSE = 1\nn\nn∑\ni=1\n(Yi − ˆYi)2 (9)\n2) RMSE\nRMSE or Root Mean Squared Error is an error calculating\nmethod. It is more popular in its use as it is measured in\nthe same unit as the response variable. RMSE basically tells\nus about the average deviation between the predicted points\nand the actual points. If Y be the actual output and ˆY be the\npredicted output, then we get:\nRMSE =\n√\n∑n\ni=1(Yi − ˆYi)2\nn (10)\nVOLUME 11, 2023 41821\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nTABLE 3. Hyper-parameters used for the experiment.\nTABLE 4. MSE, RMSE and MAE values obtained from the predictions.\n3) MAE\nMAE or Mean Absolute Error is another error calculating\nmethod. It is the absolute difference between the paired obser-\nvations. It is used to measure the accuracy of continuous\nvariables. If Y be the actual output and ˆY be the predicted\noutput, then we get:\nMAE = 1\nn\nn∑\ni=1\n| Yi − ˆYi | (11)\nC. EXPERIMENTAL RESULTS\nThe model was trained on the Google Colab environment.\nThe following hyper-parameters which were empirically\nobserved through several experiments were selected:\nThe batch size refers to the number of training examples\nused by the model in each iteration. Generally, larger batch\nsize yields better results. The batch size here has been picked\nto be 200 considering the memory constraint. The learning\nrate is the step size of each iteration. Basically, the percentage\nof change in the weight due to the error in the training sample\nis determined by the learning rate. The learning rate has been\npicked to be 0.001 by a trial-and-error method which properly\nworks for our dataset. Iterative processing of the input is done\nin each layer of the encoder. By increasing the number of\nencoders, the model will be able to catch more deviations and\nfeature contributions to the result. By keeping the memory\nconstraint in mind, the number of encoders has been chosen\nto be a maximum of 5.\nWe have used the target variable as Average Occupancy\nand with different sequence lengths. The model was measured\nin terms of MSE, RMSE and MAE. After the convergence of\nthe training, the results that were obtained are as follows:\nThis shows the variation of different types of errors on\nthe basis of the change in sequence lengths. The transformer\narchitecture considering time series data along a specific\nvariable has been considered in [5] and [39]. The transformer\nTABLE 5. MSE values for varying the number of encoders.\nFIGURE 4. Comparison between 2D-Transformers, Transformers and\nLSTM.\nmodel across a specific variable failed to capture deviations\nwhich are related to other features in the data. This is what\nis depicted in the table 4. Both the sequence lengths 4 and\n8 have produced better results compared to the Transformer\nand LSTM model of sequence length = 8. Our model was\ntrained for around 30 minutes for each of the variables of\nencoders. We ensured a proper learning period for the Trans-\nformer and LSTM models by giving them each more than\nan hour of training time. We have kept the batch size at\n200 and a learning rate of 0.001. The number of encoders\nwas a maximum of 5 which achieved minimum error.\nVarying the number of encoders, the errors also fluctuate.\nNow, let us look at how encoders have impacted learning\nby observing the variation in MSE for different numbers of\nencoders:\nIt is evident that the number of encoders has a direct impact\non the model’s learning. A similar pattern is observed for both\nTransformers architecture and the proposed 2-D Transformer\narchitecture. With the increase in the number of encoders, the\nmodel learns better and gives a lesser error.\nD. RESULT ANALYSIS\n1) IMPACT OF SEQUENCE LENGTH\nIt is seen that with the increase in sequence length, all the\nerrors (MSE, RMSE and MAE) seem to decrease. This is\nbecause the sequence length necessarily defines how much\nof the time-series data are taken together for correlation at\na time. So, if the sequence length increases, more data is\ntaken together as a sequence and correlation is much more\nevident by the model. Similar results are also shown by our\n2D-Transformer model where increasing the sequence length\n41822 VOLUME 11, 2023\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nFIGURE 5. Performance of the model over four days.\nFIGURE 6. Average error distance of transformers and 2D-transformers over four days.\ndecreased the error up to a certain extent. We have taken\nsequence lengths 2, 4 and 8 where 4 has given a better result\nthan 2, and 8, has given a better result than 4.\nIf we look at the radar chart in Figure 7, we will see that\nour model of sequence lengths 4 and 8 has outperformed the\nLSTM model of sequence length = 8 and Transformers model\nof sequence lengths 4 and 8 in all three metrics.\n2) TEMPORAL PERFORMANCE OF THE MODEL\nThe model makes near-accurate predictions of the traffic flow\nwhich can be observed from Figure 5. The figure shows the\nperformance of the model over a continuous time frame of\n4 days in an hourly manner. A similar pattern can be observed\nover 5-minutes data as well. One of the critical problems\nthat can be observed for the basic Transformer model is that\nthe sudden changes in traffic flow were not captured. This\nprediction is of utmost importance as unusual traffic flow\nprediction is one of its main practical uses. These abrupt\nchanges were successfully captured by our model. The model\ncan be used for traffic guidance and management systems and\nmake positive improvements in ITS.\nFigure 6 shows the Average Error Distance which is the\naverage error of each of the models with respect to the\nactual value over the 4 days of time. Each of the error dis-\ntances is taken by averaging over the 5 consecutive hours\nof hourly errors caused by both 2D-Transformers and Trans-\nformers models. This gives a comprehensive idea about\nthe improvements offered by 2D-Transformers over a long\nperiod of time. It is evident that 2D-Transformers have out-\nperformed the Transformer model in almost each of the\nhours.\n3) IMPACT OF NUMBER OF ENCODERS\nThe number of encoders directly helps in improving the per-\nformance of the model. As the number of encoders increases,\nthe model is able to learn much more correlation among the\nfeatures. Let us see how the number of encoders has made the\nmodel improve in performance:\nThe basic Transformer model starts to improve with\nthe increase in the number of encoders as expected. But\nthe error is still larger than that of 2D-Transformers.\nEven with 5 encoders, the model has a higher error than\n2D-Transformers with only 3 encoders.\nOur experiment shows that our 2D-Transformer increasing\nthe number of encoders from 2 to 5 has decreased the MSE\nby around 70%, which is for sequence length = 4 as shown in\nFigure 4. Similar results are seen in the case of sequence\nlength = 8. The MSE has decreased by around 66%. This\ngraph shows that the 2D-Transformer model has converged\nand has a steady error rate moving forward.\nVOLUME 11, 2023 41823\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nFIGURE 7. Impact of number of encoders in MSE.\nE. INTEGRATING INTO AN SDN ENVIRONMENT\nSoftware-Defined Network (SDN) architecture is being cho-\nsen for deploying our traffic prediction model and for control-\nling and managing vehicular network communication. It can\nbe deduced that SDN architecture ensures better connectivity\nand performance for vehicular communication over tradi-\ntional networks, and upon experimentation, similar results\nhave been achieved. In this section, we will put some light\non why SDN is being chosen over traditional networks for\nintegrating vehicular communication, the experimental setup\nthat is being conducted on the Mininet emulator, and will\ndiscuss the simulation results.\nOne of the key concepts of SDN is the decoupling of data\nand the control plane. In the case of traditional networks,\nrouting devices perform the duties of both the control plane\nand the data plane. In SDN, a controller-switch architecture\nis observed for the decoupling of the control and data plane.\nControllers act as the control layer and switch just forward\nthe data.\n1) SDN EXPERIMENTAL SETUP\nThe experiment was conducted in Mininet Emulator with\nFloodlight Controller. The Mininet emulator resided in a\nUbuntu 14.04 OS. The floodlight controller is used as the\nremote controller of the network. A custom topology that\nhas been built to emulate a real-world scenario consists of\n4 switches and 4 hosts.\nThrough the experimentation in the simulation, we have\nmeasured on how much time it usually takes for the model\ndata to be transmitted to the other devices. Various samples\nof data are being gathered as to how much time it takes for\nthe full data to go from the devices, in the SDN network.\n2) SDN SIMULATION AND RESULTS\nWe have found the results of the experimentation in terms of\nthe time needed for transmitting the data to the other devices.\nWe have sample data from 100kb to 500kb and transmitted\nTABLE 6. Time needed for data transmission in the topology based on\nfile size.\nthem over the network. The time that is required for the\ndata transmission is presented in the table below. We also\nhave been able to showcase the topology in the floodlight\ncontroller as shown in the figure.\nVI. FUTURE WORKS\nA. DATASET MODIFICATION\nEach of the stations in the dataset has a specific coverage area.\nThis coverage varies from station to station. Because of this\nreason, some of the stations which have a smaller coverage\nmight have less traffic present even though it is closer to some\nheavy traffic. This sort of case makes it hard for the model\nto recognize correlations among stations with their positions\nand neighbouring stations.\nIn order to make the model perform better, these smaller\nstations can be merged with their nearest larger stations. This\nmodification in the dataset will yield a much better result as\nthe model will be able to understand the spatial correlations.\nB. RESOURCE ALLOCATION\nLarger sequence lengths and batch sizes would gener-\nally yield a better result. More encoders would also help\nin improving performance. To sum up, bigger sequences,\nbatches and more encoders would yield a much better result.\nAgain, a larger dataset would mean that the model is learn-\ning from more samples, which generally also produces much\nbetter results and avoid bias.\nVII. CONCLUSION\nIt is clear from the experiments that our transformer model\nhas performed amazingly on the dataset. This demonstrates\nhow well-suited transformer-based models are for estimating\ntraffic flow. The transformer design can tolerate dependencies\nacross vast distances and protracted timespan since it accom-\nmodates massive volumes of data.\nV ANET’s ad-hoc nature meshes seamlessly with the SDN\narchitecture. The centralized decision-making process of\nSDN can guarantee a quicker transmission rate and appropri-\nate control mechanisms that are needed for fast-moving vehi-\ncles. The research work has some limitations. Rigorous data\nanalysis could be done to find dependencies across stations\nwhich could create an avenue for feature engineering. The\nmodel has not been simulated in a real-life SDN environment\nwhich could also provide challenges in its behaviour in a\npractical setting with dynamic constraints.\n41824 VOLUME 11, 2023\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nIt can be concluded a comprehensive model for traffic flow\nprediction and propagation inside ITS might be produced\nby combining V ANET and SDN with the precise prediction\nmade possible by a transformer model.\nREFERENCES\n[1] J. Mackenzie, J. F. Roddick, and R. Zito, ‘‘An evaluation of HTM and\nLSTM for short-term arterial traffic flow prediction,’’ IEEE Trans. Intell.\nTransp. Syst., vol. 20, no. 5, pp. 1847–1857, May 2019.\n[2] X. Yu, S. Xiong, Y . He, W. E. Wong, and Y . Zhao, ‘‘Research on campus\ntraffic congestion detection using BP neural network and Markov model,’’\nJ. Inf. Secur. Appl., vol. 31, pp. 54–60, Dec. 2016.\n[3] I. Ku, Y . Lu, M. Gerla, R. L. Gomes, F. Ongaro, and E. Cerqueira,\n‘‘Towards software-defined V ANET: Architecture and services,’’ in Proc.\n13th Annu. Medit. Ad Hoc Netw. Workshop (MED-HOC-NET) , Jun. 2014,\npp. 103–110.\n[4] O. Sadio, I. Ngom, and C. Lishou, ‘‘Design and prototyping of a software\ndefined vehicular networking,’’ IEEE Trans. Veh. Technol., vol. 69, no. 1,\npp. 842–850, Jan. 2020.\n[5] L. Cai, K. Janowicz, G. Mai, B. Yan, and R. Zhu, ‘‘Traffic transformer:\nCapturing the continuity and periodicity of time series for traffic forecast-\ning,’’Trans. GIS, vol. 24, no. 3, pp. 736–755, Jun. 2020.\n[6] M. Xu, W. Dai, C. Liu, X. Gao, W. Lin, G.-J. Qi, and H. Xiong,\n‘‘Spatial-temporal transformer networks for traffic flow forecasting,’’\n2020, arXiv:2001.02908.\n[7] M. Gerla, ‘‘Vehicular cloud computing,’’ in Proc. 11th Annu. Medit.\nAd Hoc Netw. Workshop (Med-Hoc-Net), Jun. 2012, pp. 152–155.\n[8] I. Abbasi and A. S. Khan, ‘‘A review of vehicle to vehicle communication\nprotocols for V ANETs in the urban environment,’’ Future Internet, vol. 10,\nno. 2, p. 14, Jan. 2018.\n[9] G. Karagiannis, O. Altintas, E. Ekici, G. Heijenk, B. Jarupan, K. Lin, and\nT. Weil, ‘‘Vehicular networking: A survey and tutorial on requirements,\narchitectures, challenges, standards and solutions,’’ IEEE Commun. Sur-\nveys Tuts., vol. 13, no. 4, pp. 584–616, 4th Quart., 2011.\n[10] S. Khatri, H. Vachhani, S. Shah, J. Bhatia, M. Chaturvedi, S. Tanwar, and\nN. Kumar, ‘‘Machine learning models and techniques for V ANET based\ntraffic management: Implementation issues and challenges,’’ Peer Peer\nNetw. Appl., vol. 14, no. 3, pp. 1778–1805, May 2021.\n[11] C. Harsch, A. Festag, and P. Papadimitratos, ‘‘Secure position-based rout-\ning for V ANETs,’’ in Proc. IEEE 66th Veh. Technol. Conf., Sep. 2007,\npp. 26–30.\n[12] S. Kudva, S. Badsha, S. Sengupta, I. Khalil, and A. Zomaya, ‘‘Towards\nsecure and practical consensus for blockchain based V ANET,’’ Inf. Sci.,\nvol. 545, pp. 170–187, Feb. 2021.\n[13] O. N. Fundation, ‘‘Software-defined networking: The new norm for net-\nworks,’’ONF White Paper, vol. 2, nos. 2–6, p. 11, 2012.\n[14] D. Kreutz, F. M. V . Ramos, P. E. Verissimo, C. E. Rothenberg,\nS. Azodolmolky, and S. Uhlig, ‘‘Software-defined networking: A compre-\nhensive survey,’’Proc. IEEE, vol. 103, no. 1, pp. 14–76, Jan. 2015.\n[15] F. Alam, I. Katib, and A. S. Alzahrani, ‘‘New networking era: Software\ndefined networking,’’ Int. J. Adv. Res. Comput. Sci. Softw. Eng., vol. 3,\nno. 11, 2013.\n[16] K. Kirkpatrick, ‘‘Software-defined networking,’’ Commun. ACM, vol. 56,\nno. 9, pp. 16–19, Sep. 2013.\n[17] H. I. Kobo, A. M. Abu-Mahfouz, and G. P. Hancke, ‘‘A survey on software-\ndefined wireless sensor networks: Challenges and design requirements,’’\nIEEE Access, vol. 5, pp. 1872–1899, 2017.\n[18] Z. Qin, G. Denker, C. Giannelli, P. Bellavista, and N. Venkatasubramanian,\n‘‘A software defined networking architecture for the Internet-of-Things,’’\nin Proc. IEEE Netw. Oper. Manage. Symp. (NOMS), May 2014, pp. 1–9.\n[19] A. Rahman, M. K. Nasir, Z. Rahman, A. Mosavi, S. S, and\nB. Minaei-Bidgoli, ‘‘DistBlockBuilding: A distributed blockchain-\nbased SDN-IoT network for smart building management,’’ IEEE Access,\nvol. 8, pp. 140008–140018, 2020.\n[20] J. Xie, F. R. Yu, T. Huang, R. Xie, J. Liu, C. Wang, and Y . Liu, ‘‘A survey\nof machine learning techniques applied to software defined networking\n(SDN): Research issues and challenges,’’ IEEE Commun. Surveys Tuts.,\nvol. 21, no. 1, pp. 393–430, 1st Quart., 2019.\n[21] S. Sezer, S. Scott-Hayward, P. Chouhan, B. Fraser, D. Lake, J. Finnegan,\nN. Viljoen, M. Miller, and N. Rao, ‘‘Are we ready for SDN? Implemen-\ntation challenges for software-defined networks,’’ IEEE Commun. Mag.,\nvol. 51, no. 7, pp. 36–43, Jul. 2013.\n[22] H. Kim and N. Feamster, ‘‘Improving network management with software\ndefined networking,’’ IEEE Commun. Mag., vol. 51, no. 2, pp. 114–119,\nFeb. 2013.\n[23] C. Bernardos, A. de la Oliva, P. Serrano, A. Banchs, L. Contreras, H. Jin,\nand J. Zuniga, ‘‘An architecture for software defined wireless networking,’’\nIEEE Wireless Commun., vol. 21, no. 3, pp. 52–61, Jun. 2014.\n[24] M. Abbasi, H. Rezaei, V . G. Menon, L. Qi, and M. R. Khosravi, ‘‘Enhanc-\ning the performance of flow classification in SDN-based intelligent\nvehicular networks,’’ IEEE Trans. Intell. Transp. Syst., vol. 22, no. 7,\npp. 4141–4150, Jul. 2021.\n[25] G. A. Qadir, ‘‘Software defined network based V ANET,’’ Int. J. Sci. Bus.,\nvol. 5, no. 3, pp. 83–91, 2021.\n[26] M. O. Kalinin, V . M. Krundyshev, and P. V . Semianov, ‘‘Architectures for\nbuilding secure vehicular networks based on SDN technology,’’ Autom.\nControl Comput. Sci., vol. 51, no. 8, pp. 907–914, Dec. 2017.\n[27] S. D. A. Shah, M. A. Gregory, S. Li, R. D. R. Fontes, and L. Hou,\n‘‘SDN-based service mobility management in MEC-enabled 5G and\nbeyond vehicular networks,’’ IEEE Internet Things J., vol. 9, no. 15,\npp. 13425–13442, Aug. 2022.\n[28] S. D. A. Shah, M. A. Gregory, S. Li, and R. D. R. Fontes, ‘‘SDN enhanced\nmulti-access edge computing (MEC) for E2E mobility and QoS manage-\nment,’’IEEE Access, vol. 8, pp. 77459–77469, 2020.\n[29] M. Zhu, J. Cao, D. Pang, Z. He, and M. Xu, ‘‘SDN-based routing for\nefficient message propagation in V ANET,’’ in Proc. Int. Conf. Wireless\nAlgorithms, Syst., Appl. Cham, Switzerland: Springer, 2015, pp. 788–797.\n[30] J. Bhatia, R. Dave, H. Bhayani, S. Tanwar, and A. Nayyar, ‘‘SDN-based\nreal-time urban traffic analysis in V ANET environment,’’ Comput. Com-\nmun., vol. 149, pp. 162–175, Jan. 2020.\n[31] R. M. Alamgir, A. A. Shuvro, M. Al Mushabbir, M. A. Raiyan, N. J. Rani,\nM. M. Rahman, M. H. Kabir, and S. Ahmed, ‘‘Performance analysis\nof YOLO-based architectures for vehicle detection from traffic images\nin Bangladesh,’’ in Proc. 25th Int. Conf. Comput. Inf. Technol. (ICCIT),\nDec. 2022, pp. 982–987.\n[32] S. Rahimipour, R. Moeinfar, and S. M. Hashemi, ‘‘Traffic prediction using\na self-adjusted evolutionary neural network,’’ J. Modern Transp., vol. 27,\nno. 4, pp. 306–316, Dec. 2019.\n[33] J. Fan, Q. Hu, and Z. Tang, ‘‘Predicting vacant parking space availability:\nAn SVR method with fruit fly optimisation,’’ IET Intell. Transp. Syst.,\nvol. 12, no. 10, pp. 1414–1420, Dec. 2018.\n[34] J. Fan, Q. Hu, Y . Xu, and Z. Tang, ‘‘Predicting vacant parking space\navailability: A long short-term memory approach,’’ IEEE Intell. Transp.\nSyst. Mag., vol. 14, no. 2, pp. 129–143, Mar. 2022.\n[35] Y . Feng, Y . Xu, Q. Hu, S. Krishnamoorthy, and Z. Tang, ‘‘Predicting vacant\nparking space availability zone-wisely: A hybrid deep learning approach,’’\nComplex Intell. Syst., vol. 8, no. 5, pp. 4145–4161, Oct. 2022.\n[36] L. Zhao, Y . Song, C. Zhang, Y . Liu, P. Wang, T. Lin, M. Deng, and H. Li,\n‘‘T-GCN: A temporal graph convolutional network for traffic prediction,’’\nIEEE Trans. Intell. Transp. Syst., vol. 21, no. 9, pp. 3848–3858, Sep. 2020.\n[37] X. Du, H. Zhang, H. V . Nguyen, and Z. Han, ‘‘Stacked LSTM deep learning\nmodel for traffic prediction in vehicle-to-vehicle communication,’’ in Proc.\nIEEE 86th Veh. Technol. Conf. (VTC-Fall), Sep. 2017, pp. 1–5.\n[38] C. Chen, Freeway Performance Measurement System (PeMS). Berkeley,\nCA, USA: Univ. California, Berkeley, 2002.\n[39] Y . Jin, L. Hou, and Y . Chen, ‘‘A time series transformer based method\nfor the rotating machinery fault diagnosis,’’ Neurocomputing, vol. 494,\npp. 379–395, Jul. 2022.\nALI ABIR SHUVRO was born in Dhaka,\nBangladesh, in 1999. He received the B.Sc. degree\nin computer science and engineering from the\nIslamic University of Technology (IUT), Gazipur,\nBangladesh.\nHe is currently a Lecturer with the Department\nof Computer Science and Engineering, IUT. He is\nenthusiastic about different aspects of computer\nscience. His research interests include computer\nnetworks, software-defined networks, machine\nlearning, deep learning, and computer vision. He was awarded the Presti-\ngious IUT Gold Medal, in 2022, for his outstanding academic excellence.\nVOLUME 11, 2023 41825\nA. A. Shuvro et al.: Transformer Based Traffic Flow Forecasting in SDN-VANET\nMOHAMMAD SHIAN KHANreceived the B.Sc.\ndegree in computer science and engineering (CSE)\nfrom the Islamic University of Technology (IUT),\nGazipur, Bangladesh, in 2022.\nHis research interests include computer vision,\nmachine learning, deep learning, and networking.\nHe has been actively involved in research projects\nduring his undergraduate studies. He is looking\nforward to continuing his research and making\nsignificant contributions in these fields.\nMONZUR RAHMAN received the B.Sc. degree\nin computer science and engineering from the\nIslamic University of Technology, in 2022. His\neducational background in computer science,\nalong with his passion for technology and inno-\nvation, has led him to pursue a career in the field\nof computer science research. He is particularly\ninterested in studying the latest advancements in\nsoftware development and artificial intelligence\nand has a strong desire to contribute to the ongoing\nresearch efforts in these areas. He is eager to continue his education and\nexpand his knowledge and skills in the field of computer science in order to\nbecome a leading researcher in the field.\nFAISAL HUSSAINreceived the B.Sc. and M.Sc.\ndegrees in computer science and engineering from\nthe Islamic University of Technology, Bangladesh.\nHe is currently an Assistant Professor with\nthe Department of Computer Science and Engi-\nneering, Islamic University of Technology. His\nresearch interests include D2D communication,\nSDN, V ANET, network optimization, and network\nsecurity.\nMD. MONIRUZZAMAN received the bache-\nlor’s and master’s degrees from the Department\nof Computer Science and Engineering (CSE),\nIslamic University of Technology (IUT), and\nthe Ph.D. degree from Federation University\nAustralia, in 2021. He is currently an Assistant\nProfessor with the Department of CSE, IUT.\nMD. SAKHAWAT HOSSEN received the B.Sc.\nand Ph.D. degrees in computer science and engi-\nneering (CSE) from the Islamic University of\nTechnology (IUT), Gazipur, Bangladesh, and the\nM.Sc. degree in internetworking from the Royal\nInstitute of Technology (KTH), Sweden. He is\ncurrently an Associate Professor with the Depart-\nment of CSE, IUT. His research interests include\nD2D communication, SDN, V ANET, the Internet\nof Things, RFID, cellular automata, combinatorial\nand evolutionary optimization problems, internet security, wireless sensor\nnetworks, IP networks, and V oIP.\n41826 VOLUME 11, 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6941571235656738
    },
    {
      "name": "Vehicular ad hoc network",
      "score": 0.6440691947937012
    },
    {
      "name": "Computer network",
      "score": 0.5681724548339844
    },
    {
      "name": "Wireless ad hoc network",
      "score": 0.31650274991989136
    },
    {
      "name": "Telecommunications",
      "score": 0.17867258191108704
    },
    {
      "name": "Wireless",
      "score": 0.11794933676719666
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59805279",
      "name": "Islamic University of Technology",
      "country": "BD"
    }
  ]
}