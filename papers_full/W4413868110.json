{
  "title": "Healthcare agent: eliciting the power of large language models for medical consultation",
  "url": "https://openalex.org/W4413868110",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2749162117",
      "name": "Zhiyao Ren",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2130251626",
      "name": "Yibing Zhan",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2297136732",
      "name": "Baosheng Yu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2072270871",
      "name": "Liang, Ding",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2112969836",
      "name": "Pingbo Xu",
      "affiliations": [
        "Zhejiang Cancer Hospital",
        "Cancer Hospital of Chinese Academy of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2749162117",
      "name": "Zhiyao Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130251626",
      "name": "Yibing Zhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2297136732",
      "name": "Baosheng Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2072270871",
      "name": "Liang, Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112969836",
      "name": "Pingbo Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104129307",
      "name": "Dacheng Tao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3047107545",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4362598952",
    "https://openalex.org/W4387947536",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4402670791",
    "https://openalex.org/W4412888016",
    "https://openalex.org/W4391759824",
    "https://openalex.org/W4396753423",
    "https://openalex.org/W4383313218",
    "https://openalex.org/W4385571668",
    "https://openalex.org/W4389520259",
    "https://openalex.org/W4409283601",
    "https://openalex.org/W2902116953",
    "https://openalex.org/W3101223450",
    "https://openalex.org/W4385734161",
    "https://openalex.org/W4389519254"
  ],
  "abstract": "Abstract Large language models (LLMs) have achieved remarkable success across many fields but face challenges in complex real-world scenarios like medical consultation, particularly regarding inquiry quality and safety concerns. In this paper, we introduce a healthcare agent designed to address these issues, which cannot be fully resolved through a vanilla one-time fine-tuning process. The healthcare agent includes three components: a dialogue component for planning safe and effective conversations, a memory component for storing patient conversations and medical history, and a processing component for report generation. To evaluate our healthcare agent’s performance in medical consultations, we employ both expert assessment from medical professionals and an automated evaluation system powered by ChatGPT for large-scale testing. Our results demonstrate that the healthcare agent significantly enhances the capabilities of general LLMs in medical consultations, particularly in inquiry quality, response quality, and safety. Through extensive ablation studies, we also analyze the impact of each component.",
  "full_text": "npj |artiﬁcial intelligence Article\nhttps://doi.org/10.1038/s44387-025-00021-x\nHealthcare agent: eliciting the power of\nlarge language models for medical\nconsultation\nCheck for updates\nZhiyao Ren1, Yibing Zhan2 , Baosheng Yu1, Liang Ding3, Pingbo Xu4 & Dacheng Tao1\nLarge language models (LLMs) have achieved remarkable success across manyﬁelds but face\nchallenges in complex real-world scenarios like medical consultation, particularly regarding inquiry\nquality and safety concerns. In this paper, we introduce a healthcare agent designed to address these\nissues, which cannot be fully resolved through a vanilla one-timeﬁne-tuning process. The healthcare\nagent includes three components: a dialogue component for planning safe and effective\nconversations, a memory component for storing patient conversations and medical history, and a\nprocessing component for report generation. To evaluate our healthcare agent’s performance in\nmedical consultations, we employ both expert assessment from medical professionals and an\nautomated evaluation system powered by ChatGPT for large-scale testing. Our results demonstrate\nthat the healthcare agent signiﬁcantly enhances the capabilities of general LLMs in medical\nconsultations, particularly in inquiry quality, response quality, and safety. Through extensive ablation\nstudies, we also analyze the impact of each component.\nIn recent years, online medical consultations have grown signiﬁcantly1.\nWith the impressive success of largelanguage models (LLMs), there has\nbeen increasing attention on their application in medical consultations2– 4.\nHowever, applying general LLMs to medical consultations in real-world\nscenarios presents several signiﬁcant challenges. First, they cannot effec-\ntively guide patients through the step-by-step process of articulating their\nmedical conditions and relevant information, a crucial element of real-\nworld doctor-patient dialogue. Second, they lack the necessary strategies and\nsafeguards to manage medical ethics and safety issues, putting patients at\nrisk of serious consequences. Third, they cannot store consultation con-\nversations and retrieve medical histories.\nOne approach to addressing the challenges in medical consultation is\nbuilding medical LLMs from scratch orﬁne-tuning general LLMs with\nspeciﬁcd a t a s e t s\n5– 12. Examples like GatorTron5 and GatorTronGPT6 are\ntypical models trained from scratch using medical datasets. With the rapid\ndevelopment of general LLMs2,3,11,13– 16, medical LLMs ﬁne-tuned from\ngeneral LLMs have demonstrated state-of-the-art performances. Typical\nexamples include Med-PaLM7 and Med-PaLM28,w h i c ha r eﬁne-tuned\nfrom PaLM14 and PaLM215, respectively. Other examples include thoseﬁne-\ntuned from open-source LLMs, e.g., ChatDoctor9 and MedAlpaca10 are\nﬁne-tuned from LLaMA16;D o c t o r G L M11 and BianQue12 are ﬁne-tuned\nfrom ChatGLM 17. However, this one-time process is not only\ncomputationally expensive but also lacks theﬂexibility and adaptability\nneeded for real-world scenarios.\nIn contrast, LLM agents can reason and break down tasks into man-\nageable components without requiring re-training, making them better\nsuited for complex tasks. Previous researches18– 25 have demonstrated that\nagent methods achieve excellent performance in medical question\nanswering and medical report generation. For question answering tasks,\nMEDAGENTS\n18 introduces a method that selects multiple expert models\nbased on the task and derives theﬁnal diagnosis through mutual discussion\namong these expert models, thereby increasing diagnostic reliability.\nBuilding on MEDAGENTS, MDAgent19 proposes advanced expert colla-\nboration strategies to enhance the ability to solve different problems. CoD20\nproposes the chain-of-diagnosis strategy thereby simulating the diagnostic\nthinking of a doctor, thus enhancing the interpretability of decisions.\nMedchain\n21, on the other hand, retrieves similar cases to aid decision-\nmaking by designing medical special RAG. For processing medical reports,\nLLM agents are employed to handle a wide variety of medical documents,\nalleviating the workload of doctors. Ma et al.\n22 and Sudarshan et al.23 utilize\ndynamic prompt and self-reﬂection to process radiology reports and patient\nletters. ChatCAD+24 and BioSignal Copilot25 extend this capability by\nmanaging reports across different modalities through the integration of\nexpert models.\n1Nanyang Technological University, Singapore, Singapore.2Wuhan University, Wuhan, China.3The University of Sydney, Sydney, Australia.4Zhejiang Cancer\nHospital and Institute of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, China.e-mail: zybjy@mail.ustc.edu.cn; dacheng.tao@gmail.com\nnpj Artiﬁcial Intelligence|            (2025) 1:24 1\n1234567890():,;\n1234567890():,;\nAlthough LLM agents are increasinglyutilized in healthcare scenarios,\nthere remains a signiﬁcant gap in patient-oriented agent systems capable of\nacting as personalized doctors for consultations. This presents heightened\nchallenges in terms of agent-patient interaction capabilities, accuracy, and\nsafety. In this paper, we introduce a healthcare agent that elicits the power of\ngeneral LLMs for medical consultation. As shown in Fig.1, the proposed\nhealthcare agent consists of three components:Dialogue, Memory,a n d\nProcessing.T h eDialogue component serves as the interface with\npatients, facilitating safe, effective, and informative medical conversations.\nUpon receiving an inquiry from the patient, theFunction module ﬁrst\nidentiﬁes the medical purpose or task, such as medical diagnosis, explana-\ntion, and recommendation. It then conducts either single or multi-round\nconversations with patients. TheSafety module automatically detects\nmedical ethics and safety concerns, enhancing the security and reliability of\nresponses. Lastly, theDoctormodule provides an opportunity for medical\nprofessionals to offer necessary interventions. TheMemory component is\ndesigned to enhance the accuracy of conversations by providing historical\nand current conversation information: theConversation Memoryfor\nongoing conversations and theHistory Memoryfor condensed sum-\nmaries of past dialogues. The Processing component implements\nfunctions that manage the information of the entire dialogue. In our current\nimplementation, we offer a contentsummarization function to summarize\nongoing consultations and generate summary reports.\nTo evaluate the healthcare agent, we implement a two-stage evaluation\nprocess. First, we simulate realistic medical consultations using a virtual\npatient powered by ChatGPT. This virtual patient draws from real con-\nversations in the MedDialog datasetto interact naturally with the LLM\ndoctor. The evaluation itself consists of two components: doctor evaluation\nand automated evaluation. For doctor evaluation, a panel of seven physi-\ncians reviews and scores the consultation dialogues. To enable more com-\nprehensive testing, we also develop an automated evaluation system using\nChatGPT as an evaluator. This automated approach allows us to assess a\nlarger volume of conversations while reducing the time burden on medical\nprofessionals. Figure2 illustrates the signiﬁcant improvements made by the\nproposed healthcare agent over general LLMs.\nIn this paper, weﬁrst propose an innovative healthcare agent that\nmaximizes the capabilities of LLMs formedical consultation, presenting a\nnew paradigm for applying LLMs in healthcare. Then we design the\nhealthcare agent with three key components: a dialogue component for\nplanning safe and effective conversations, a memory component for storing\npatient conversations and medical history, and a processing component for\nreport generation. Finally, we conduct extensive experiments, and the\nresults show that the proposed healthcare agent framework signiﬁcantly\nenhances the performance of LLMs in medical consultations.\nResults\nThis section is divided into four parts. Theﬁrst part showcases a practical\napplication case of the healthcare agent in real-world medical consultation.\nThe second part demonstrations the results of automated evaluation and the\nthird part shows the results of doctor evaluation. Finally, we conduct\nablation studies to validate the contributions of different module on\nhealthcare agent.\nCase Study\nThe proposed healthcare agent will be integrated into the clinical workﬂow\nto help reduce GPs’ workload during online consultations. The patient\ninitiates the interaction by describing symptoms and seeking assistance. The\nagent then responds according to three possible scenarios: if additional\ninformation is needed, it prompts the patient with follow-up questions; if the\ninformation is sufﬁcient, it provides a diagnosis, suggestions, or explana-\ntions; and if it detects ethical concerns,emergencies, or potential errors, the\nsafety module makes modiﬁcations and doctor module assesses whether a\ndoctor’s intervention is required. After completing the interaction, the agent\ngenerates a summary report, which is reviewed and veriﬁed by a doctor\nbefore being shared with the patient. To further illustrate the workﬂow of the\nhealthcare agent, we use an example of a patient consulting about anxiety\ndisorder to demonstrate the entire working process of our healthcare agent,\nand explain the roles played by different modules. A brief overview of the\nconsultation process is depicted in Fig.3, with a more detailed exposition of\nthe process to follow later. This case study is conducted by GPT-4 as\nbackbone.\nStart of Consultation. The healthcare agent introduces its functionality\nto the patient and starts the consultation with\"Hello! I’mD r .A I ,y o u rv i r t u a l\nhealth assistant. I’m here to help you with any health concerns you might\nDialogue\nMemory\nConversation Memory\nEnd of Consultation\nDoctor, I am\ngoing through\n……\nProcessing\nReport GenerationMemory Summary\nUpdate\nHistory Memory \nPlanner\nMedical Explanation\nMedical Diagnosis\nMedical Recommendation\nInquiry\nFunction Module\nDoctor Module\nDirect\nModification\nGuided\nModification\nDoctor\nVerification\nSafety Module\nEmergency Detection\nError Detection\nEthics Detection\nModification\nFig. 1 | The main proposed healthcare agent framework.The framework consists of three parts: Dialogue, Memory, and Processing, which are responsible for interacting\nwith patients, storing conversational information, and processing paperwork, respectively.\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 2\nh a v e .P l e a s ef e e lf r e et os h a r ew h a t’sb e e nb o t h e r i n gy o u ,a n dI’ll do my best to\nprovide guidance and answer any questions you may have. How can I assist\nyou today?\"\nPlanner in Function Module. When the patient initiates the con-\nsultation by describing his symptoms:\"Hello Doctor, I’ve been experiencing a\nrapid heartbeat, intense fear, and nervousness. I took medication for my\nanxiety two hours ago, but I’m still not feeling any relief.\", the Planner sub-\nmodule determines the task as a medical diagnosis task and needs more\npatient information, thus choose the Inquiry sub-module as the next step.\nInquiry Submodule. In the Inquiry sub-module, the doctor asks\nquestions based on the patient’s condition, Conversation memory and\nHistory memory. After each round of inquiries, the Planner sub-module will\nevaluate the available information and decide whether to proceed with the\ninquiry. In this sub-module, the healthcare agent obtains information\nthrough an inquiry about the patient’s past medical history, medication\nusage, possible triggers, and recent changes. These are crucial details that the\npatient did not describe initially but are very important in the diagnosis.\nPreliminary Response. Subsequently, when several rounds of inquiries\nhave taken place, the Planner determines that there is enough information to\nmake a diagnosis and chooses to perform the Medical Diagnosis sub-\nmodule. Healthcare agent generates preliminary diagnoses and suggestions\nthrough the Medical Diagnosis prompt. Guided by prompts, healthcare\nagent provides responses structured around initial diagnosis, explanation of\nthe diagnosis, and subsequent recommendations.\nSafety Module. The preliminary response is then modiﬁed in the Safety\nmodule to ensure that the responses are ethically compliant, safe, and\naccurate. The Safety moduleﬁrst discusses possible problems in ethics,\nemergency and errors respectively andgives suggestions for improvement.\nThe preliminary response is then modiﬁed based on the results of this\ndiscussion.\nDoctor Module. After that, the doctor can review and modify the\nresponse. In this case, the doctor believes it’s important to emphasize“Need\nto remind patients not to change their medication without authorization.”.\nDoctors can either modify responses themselves or have the LLM auto-\nmatically make modiﬁcations. With just a simple guidance from the doctor,\nhealthcare agent can appropriately add or modify information in the\nresponses.\nPlanner for Another Question. Next, the patient seeks further medical\nassistance from the doctor based on their own situation:“Considering my\ncurrent symptoms and the fact that I’m in a location without a psychiatric\ndepartment, what immediate steps can I take to manage this panic attack\nuntil I can get to a hospital for a face-to-face consultation?”. According to the\njudgment of the Planner sub-module in the Function Module, the health-\ncare agent switches to a medical recommendation task, using the prompt of\nMedical Recommendation to directly generate a response.\nHistory Summary and Processing. When the consultation is over,\nhealthcare agent will post-process the conversation of the current con-\nsultation for History Memory summary and report processing. In the\nMemory Summary module, the healthcare agent provides a brief summary\nof the symptoms, diagnosis, and recommendations. This will be used in the\nHistory Memory of the subsequent consultation to bring information from\nthe current consultation into the follow-up visit. On the other hand, the\nProcessing module is used for handling dialogue information and gen-\nerating reports for patients to conclude this consultation. Guided by the\nprompt, healthcare agent generate a report containing an overview of the\ncondition, diagnostic results, and recommendations.\nResults of automated evaluation\nFor automated evaluation, we employ three popular open-source LLMs\n(LLama-3, Mistral, and Gemma-2) and three closed-source LLMs (GPT-4,\nClaude-3.5, and Gemini-1.5) as backbone. We evaluate 50 data and the\nresults are present in Table1.\nFor the inquiry quality, we reﬁne the assessment of inquiry quality into\ninquiry proactivity, inquiry relevance, and conversationﬂuency, corre-\nsponding to the number and comprehensiveness of the inquiry questions,\nthe relevance of the questions to the patient’s symptoms, and the overall\nconversational experience, respectively. For vanilla models, the results for\ninquiry proactivity indicate that most models, such as Mixtral, GPT-4,\nCluade-1.5 and Gemini-1.5, tend to provide direct answers without addi-\ntional inquiries. Although LLaMA-3 and Gemma-2 demonstrate some\ninquiry capability, the inquiry relevance results reveal that their questions\nlack depth and are not strongly aligned with the patient’s situation. Addi-\ntionally, these models often ask multiple questions within a single sentence,\nleading to poor conversationalﬂuency. When integrated with the healthcare\nagent framework, the inquiry proactivity and relevance of all models\nimprove signiﬁcantly, resulting in more relevant and effective questions.\nNotably, the conversationalﬂuency of open-source models is inferior to that\nof closed-source models. This is likely due to a deﬁciency in the ability to\nexecute the instructions of the inquiry sub-module. For instance, open-\nsource models sometimes include irrelevant descriptions or explanations\nbefore questioning, such as“Based on the patient’s descriptions, historical\nconversations, and medical records, myﬁrst follow-up question would be:”.\nFig. 2 |Illustration of the improvements made by the\nproposed healthcare agent in medical consultation,\nparticularly in terms of inquiry quality (proactivity,\nrelevance &ﬂuency), response quality (accuracy &\nhelpfulness), and safety (harmfulness & self-\nawareness).\n0\n1\n2\n3\n4\n5\nInquiry Proactivity\nInquiry Relevance\nConversational Fluency\nAccuracyHelpfulness\nHarmfulness\nSelf-Awareness\nLLama3 Llama3 (Agent) GPT4 GPT4 (Agent)\nMixtral Mixtral (Agent) Claude Claude (Agent)\nGemma2 Gemma2 (Agent) Gemini Gemini (Agent)\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 3\nSuch responses are inconsistent with real healthcare scenarios and nega-\ntively affect the user experience.\nFor the response quality, we evaluate response quality by the accuracy\nof the diagnosis and the helpfulness of the recommendations and sugges-\ntions. As shown in Table1, the vanilla models responses demonstrate\nrelatively good accuracy and helpfulness, highlighting the excellent medical\nknowledge of general LLMs. We canﬁnd that the integration of healthcare\nagent yields noticeable enhancementsin response accuracy and helpfulness\nacross all models. We consider that one signiﬁcant factor contributing to this\nimprovement is the additional information gathered by the Inquiry sub-\nmodule and the rectiﬁcation of inaccuracies through the Safety module.\nFurthermore, while the performance of open-source models in their vanilla\nstate lags behind that of closed-source models, the performance gap narrows\nconsiderably with the useof the agent framework. This indicates that the\nhealthcare agent can effectively bridge the performance disparity between\ndifferent models.\nGeneral LLM\nI've been struggling with an acute anxiety disorder, and despite being on medication, I'm not finding any relief. Even two hours after taking the medication, I still experience a\nrapid heartbeat, a lot of fear, and general nervousness. These symptoms persist and are quite distressing.\nHello, it sounds like you’re …… Rapid heartbeat …… can be signs of an anxiety attack or panic attack, especially if they occur suddenly and without a clear trigger ……Here\nare a few steps to consider: 1. **Stay Safe:** If you're in a situation ……. 2. **Breathing Exercises:** …… Please remember, if you have ……, it is essential to call for\nemergency assistance right away.\nHealthcare Agent\nI've been struggling with an acute anxiety disorder, and despite being on medication, I'm not finding any relief. Even two\nhours after taking the medication, I still experience a rapid heartbeat, a lot of fear, and general nervousness. These\nsymptoms persist and are quite distressing.\nHave you noticed any specific triggers or situations that seem to worsen your anxiety symptoms?\nI haven't identified any specific triggers recently, but I generally feel more anxious and fearful at night. The symptoms\nseem to intensify when I try to sleep, and I become overwhelmed by a sense of impending doom, which makes it even\nharder to pinpoint a specific cause or trigger.\n……\nHave you had any recent stressful events or significant emotional disturbances that might be\ncontributing to your heightened anxiety and fear at night?\nNo, I haven't experienced any recent stressful events or significant emotional disturbances that could be contributing to\nmy heightened anxiety and fear at night. My life has been relatively stable in that regard, and there haven't been any\nmajor changes or stressors that I can pinpoint as a direct cause for these worsening symptoms...\n**Diagnosis**: Based on the historical conversation and the details provided, it appears that you are experiencing\nexacerbated symptoms of your diagnosed acute anxiety disorder ... **Explanation**: Your symptoms of rapid heartbeat,\nintense fear, and nervousness that worsen at night and do not seem to be triggered by any specific environmental changes\nor recent stressors suggest that your current medication may not be adequately managing your anxiety ... **Advice**:\nFirst, it's crucial to discuss these persistent and worsening symptoms with your prescribing doctor as soon as possible...\nEthic Detection: The AI doctor's\nresponse provides a detailed analysis\nand offers specific advice, but it does\nnot clearly state that the advice is\nprovided by an AI system...\nNeed to remind patients not to change their medication without authorization.\n**Safety Disclaimer**: Please be aware that this response has been generated by an AI-based system and is not a\nsubstitute for professional medical advice, diagnosis, or treatment.... **Potential Risks**: It is important to understand\nthat while the suggestions provided here are based on general knowledge of anxiety disorders, individual conditions vary\ngreatly, and the effectiveness of treatments can differ from person to person. ...\nDate:[2024/02/13 20:00]\nPatient's condition: The patient is experiencing an acute anxiety disorder with symptoms that include a rapid heartbeat...\nDiagnosis: The patient is experiencing exacerbated symptoms of their diagnosed acute anxiety disorder...\nDiagnostic explanation:The patient's symptoms, which worsen at night and are not linked to...\nSuggestions:\n1. The patient should discuss the persistent and worsening symptoms with their prescribing...\n2. Consider incorporating cognitive-behavioral therapy (CBT) or mindfulness practices...\nDoctors can supervise the responses of\nHealthcare Agent. They can directly modify the\nanswers or guide the LLM to modify the\nresponses on its own.\nHello! I'm Dr. AI, your virtual health assistant. I'm here to help you with any health concerns you might have. Please\nfeel free to share what's been bothering you, and I'll do my best to provide guidance and answer any questions you\nmay have. How can I assist you today?\nHealthcare Agent initiates an enquiry with the\npatient and starts the consultation.\nWhen sufficient information is available,\nPlannner selects the diagnostic module to\nprovide a professional diagnosis. This Agent\nresponse consists of three parts: 1) Providing a\ndiagnosis; 2) Explaining the diagnosis results;\nand 3) Offering suggestions to the patient.\nThe patient asks the doctor questions that\nindicate the purpose of the consultation\nIn the Planner sub-module, the patient's\nintent is determined to be a medical\ndiagnosis task and require more\ninformation. The Agent therefore chooses the\nInquiry sub-module to ask the patient follow-\nup questions based on the patient's\nsymptoms and Memory module. After each\ndialogue round, the Planner sub-module\ndetermines whether the information provided\nby the current patient is sufficient for\ndiagnosis. If there is still a lack of information,\nthe inquiry process will continue.\nEmergency Detection: The symptoms\nmentioned in the AI doctor's response,\nsuch as rapid heartbeat, intense fear,\nand nervousness worsening at night,\nare indeed serious...\nErrors Detection: The AI doctor's\nresponse appears to be well-\nconsidered ... However, there are a\nfew points that could be clarified or\nadjusted for accuracy and safety...\nThe Safety module of the Healthcare Agent\nfirst discuss the AI doctor's response in three\nkey areas: 1) Emphasis on AI Agent Medical\nEthics Guidelines; 2) Urgency of emergency\ntreatment, and 3) Accuracy of the response.\nAccording to the discussion results of ethic,\nemergency and errors in the Safety module,\nthe previous agent response is modified to\neliminate these risks.\n… It is critical not to change your medication on your own to avoid the risk of dependence, and other side effects.\nAlways seek medical supervision before making any changes to your medication.  ...\nConsidering my current symptoms and the fact that I'm in a location without a psychiatric department, what immediate\nsteps can I take to manage this panic attack until I can get to a hospital for a face-to-face consultation?\nAlright, let's take this one step at a time. First, find a quiet spot to sit or lie down. Focus on your breathing—inhale\nslowly through your nose, then exhale through your mouth ... If you can, take a short walk; it might help you feel better.\nAnd when you're feeling a bit steadier, give someone you trust a call.\nFor the follow-up questions proposed by the\npatient, the Planner sub-module detects that\nthey are medical recommendation tasks, so\nthere is no need for additional inquiries, and\ndirectly replies the patient according to the\nmedical recommendation tasks.\nAfter the consultation concludes, the\nProcessing part can perform post-processing\non the entire previous dialogue. For example, it\ncan generate a summary report, providing\npatients with a comprehensive review of the\nconsultation and offering doctors an effective\nbrief during the visit.\nDate: 2024/02/13 20:00 The patient reported experiencing acute anxiety disorder symptoms, including a rapid heartbeat,\nintense fear ... The doctor diagnosed the patient with exacerbated symptoms of acute anxiety disorder, particularly\nnocturnal anxiety, which may not be well-managed by the current medication. The doctor recommended discussing these\nsymptoms with the prescribing doctor to reassess the medication regimen ...\nAfter the current consultation is concluded, the\nMemory Summary module is used to simplify\nthe current Conversation Memory into History\nMemory for later consultation.\nFig. 3 | An example of using a General LLM and healthcare agent.The yellow parts provide explanations of the different modules in healthcare agent.\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 4\nRegarding safety, our experiments evaluate both harmfulness and self-\nawareness. The experimental results show that beneﬁting from the model’s\nexcellent safety alignment, its responses usually do not contain harmful\ncontent. However, a prevalent issue is that these models, in addition to\nGemini-1.5, lack of adequate emphasis on AI disclaimers, which may lead to\nsome ethical problems. The integration of healthcare agent with these\nmodels markedly enhances the safety of their responses. Beneﬁting from\nmore patient information through inquiry strategy and the emergency\ndetection and the error detection in the Safety module, the harmfulness of\nthe responses is further reduced. At the same time, thanks to the ethics\nd e t e c t i o ni nt h eS a f e t ym o d u l e ,s e l f - a w a r e n e s si ss i g n iﬁcantly improved by\nclearly indicating the identity and risks of the AI doctor in the response.\nResults of doctor evaluation\nWe utilize doctor evaluation to more accurately and professionally assess\nhealthcare agent and to illustrate the reliability of our automated evaluation\nmethods. In this experiment, considering the workload of doctors, we utilize\nan open-source model LLaMA-3 and a closed-source model GPT-4 and\nevaluate 15 data. We invite seven doctors to conduct the evaluation and the\naverage assessment time is 3300 seconds. The results are shown in Table2\nand we further calculate the Pearson correlation between doctor evaluation\nresults and the automated evaluation results. The high correlation results\nshow that there is a high alignment between the doctor evaluation results\nand the automated evaluation results, except for the fact that the doctor\nresults are higher for the score of conversationalﬂuency and lower for the\nscore of harmfulness in the evaluation. This indicates that our healthcare\nagent is accepted by doctors for its validity and safety, demonstrating its\npotential for future clinical applications, as well as showing that our\nautomated evaluation method has a high degree of accuracy, providing an\nautomated method for futurelarge-scale evaluations.\nAblation studies\nThis subsection validates the contributions of different modules on\nhealthcare agent. Considering the labor cost and the fact that automated\nevaluation is proven to have a high consistency with doctor evaluation in\nSection 2.3, we only conduct experiments with automated evaluation on\nGPT-4 model.\nFirstly, we evaluate thePlanner sub-module andInquiry sub-\nmodule in theFunction module. In Table3,t h el a b e lw/o Planner\nSub-Modulesigniﬁes the absence of thePlannersub-module, causing\nall tasks to share Inquiry sub-module and the response with the same\nprompt. As a result, we observe a signiﬁcant drop in inquiry relevance and\nconversationalﬂuency performance. Without thePlanner sub-module,\nthe healthcare agent struggles to provide appropriate responses based on the\ncurrent scenario. This limitation results in the generation of long, illogical\nresponses for medical explanation and recommendation tasks, which\nrequire concise and effective direct answers and ask unnecessary questions\nfor diagnosis tasks with enough information, causing tedious experience for\npatients and unnecessary waste of computational resources. Additional\ndetails can be found in the supplementary material.\nIn the absence of theInquirysub-module, we observe a signiﬁcant\ndecline in all metrics of inquiry quality and response quality. This is because,\nwithout theInquirysub-modules, the healthcare agent loses its ability to\nask questions and instead tends to provide direct answers, resulting in a\nnotable reduction in inquiry quality. Moreover, the absence of the inquiry\nalso leads to the failure to obtain more comprehensive patient information,\nTable 1 | Automated evaluation results\nModels Inquiry Quality Response Quality Safety\nInquiry Proactivity Inquiry Relevance Conversational Fluency Accuracy Helpfulness Harmfulness Self-awareness\nLLaMA-3 3.00 ± 0.87 3.38 ± 0.74 3.10 ± 0.57 3.38 ± 0.61 3.40 ± 0.67 3.84 ± 0.37 3.22 ± 0.64\nLLaMA-3 Agent 3.90 ± 0.36 4.04 ± 0.28 3.92 ± 0.34 3.78 ± 0.41 3.88 ± 0.45 3.88 ± 0.59 3.74 ± 0.44\nMixtral 1.90 ± 0.36 2.58 ± 0.40 2.28 ± 0.73 3.28 ± 0.34 3.10 ± 0.27 3.96 ± 0.27 3.34 ± 0.52\nMixtral Agent 3.48 ± 0.49 3.82 ± 0.48 3.60 ± 0.72 3.86 ± 0.44 3.92 ± 0.30 4.14 ± 0.44 4.14 ± 0.52\nGemma-2 2.82 ± 0.79 3.34 ± 0.63 2.7 ± 0.61 3.36 ± 0.49 3.38 ± 0.56 4.16 ± 0.46 3.26 ± 0.48\nGemma-2 Agent 3.92 ± 0.84 4.08 ± 0.73 3.3 ± 0.76 3.94 ± 0.66 3.92 ± 0.51 4.30 ± 0.54 4.32 ± 0.55\nGPT-4 2.50 ± 0.41 2.02 ± 0.44 2.76 ± 0.54 3.66 ± 0.24 3.60 ± 0.14 3.94 ± 0.34 3.38 ± 0.38\nGPT-4 Agent 3.96 ± 0.20 4.20 ± 0.24 4.12 ± 0.38 3.94 ± 0.55 3.98 ± 0.53 4.14 ± 0.42 4.12 ± 0.52\nClaude-3.5 2.52 ± 0.37 2.10 ± 0.47 2.10 ± 0.50 3.66 ± 0.42 3.56 ± 0.32 4.04 ± 0.57 3.38 ± 0.34\nClaude-3.5 Agent 4.00 ± 0.00 4.04 ± 0.19 4.04 ± 0.20 3.88 ± 0.54 3.96 ± 0.20 4.12 ± 0.43 4.10 ± 0.36\nGemini-1.5 2.26 ± 0.14 2.92 ± 0.31 2.68 ± 0.32 3.64 ± 0.42 3.54 ± 0.57 3.94 ± 0.50 4.06 ± 0.31\nGemini-1.5 Agent 3.98 ± 0.59 4.18 ± 0.51 4.18 ± 0.67 3.86 ± 0.43 3.96 ± 0.42 4.26 ± 0.31 4.30 ± 0.55\nThe mean and variance values are displayed as mean ± variance.\nTable 2 | Doctor evaluation results\nModels Inquiry Quality Response Quality Safety\nInquiry\nProactivity\nInquiry\nRelevance\nConversational\nFluency\nAccuracy Helpfulness Harmfulness Self-awareness\nLLaMA-3 3.63 ± 0.22 (0.57) 3.83 ± 0.16 (0.71) 4.06 ± 0.27 (0.45) 3.89 ± 0.24 (0.53) 3.90 ± 0.14 (0.46) 3.31 ± 0.22 (0.39) 3.94 ± 0.15 (0.61)\nLLaMA-\n3 Agent\n4.10 ± 0.17 (0.64) 4.36 ± 0.20 (0.75) 4.24 ± 0.18 (0.30) 4.12 ± 0.17 (0.76) 4.21 ± 0.16 (0.53) 3.83 ± 0.40 (0.44) 4.25 ± 0.13 (0.50)\nGPT-4 2.79 ± 0.20 (0.44) 3.02 ± 0.15 (0.68) 4.11 ± 0.12 (0.65) 4.02 ± 0.18 (0.75) 4.05 ± 0.22 (0.82) 3.39 ± 0.18 (0.44) 3.86 ± 0.19 (0.57)\nGPT-4 Agent 4.12 ± 0.16 (0.81) 4.29 ± 0.16 (0.71) 4.34 ± 0.19 (0.51) 4.24 ± 0.13 (0.71) 4.30 ± 0.22 (0.55) 3.73 ± 0.18 (0.40) 4.27 ± 0.46 (0.81)\nThe mean and variance values are displayed as mean ± variance. The Pearson correlation between doctor evaluation results and the automated evaluation results are shown in ().\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 5\nwhich also leads to lower accuracy. Further details about this experiment are\navailable in the supplementary material.\nFor theSafetymodule, the experimental results in Table3 show that\nthe lack of emergency detection and error detection in the safety module\nleads to a decrease in accuracy and harmfulness, while the absence of ethic\ndetection will lead to a signiﬁcant decrease in self-awareness. This further\nillustrates the role of the safety module in ensuring accuracy, safety and\nethics. Furthermore, in the safety module, we propose a discuss-then-\nmodiﬁcation strategy to enhance the detection capability of the safety\nmodule. The labelw/o Discuss-then-Modiﬁcation signiﬁes only\nusing one instruction to detect and modify the response. The experimental\nresults show that although the results of self-awareness do not decrease, the\nresults in accuracy and harmfulness get worse. This suggests that the safety\nmodule has difﬁculty in modifying different safety issues at the same time,\nhighlighting the importance of our discuss-then-modify strategy. Further\nexamples can be found in the supplementary material.\nThe Doctor module serves as a crucial interface for physician over-\nsight and response modiﬁcation. To validate the feasibility of doctor-guided\nmodiﬁcations, we utilize ChatGPT to simulate a doctor’s role, reviewing\nprevious diagnostic results and identifying potential issues. Out of 50 cases,\nChatGPT provides guidance on 17 cases. These guidelines are then input\ninto theDoctormodule as instructions, followed by an assessment of the\nmodiﬁed results to ensure successful incorporation and proper placement of\nthe guidelines. As illustrated in Fig.4, medical professionals need not possess\nknowledge of LLM prompts; simple guidance sufﬁces, allowing this module\nto effectively implement or modifyresponses accordingly. Additional\nexperimental details and case studies can be found in the supplementary\nmaterial.\nFinally, we analyze the Memory component. Table 3 shows the\nimpacts ofConversation Memoryon metrics like inquiry relevance,\nconversational ﬂuency, response accuracy and helpfulness. Several cases\nwithout usingConversation Memory, which causes generating repe-\ntitive questions, can be found in the supplementary material. Moreover, the\ninquiries overly focus on the patient’s most recent responses. This issue\nadversely affects the depth and breadth of questioning, resulting in dimin-\nished inquiry capability. Furthermore, the need for patients to repeatedly\nrespond to similar queries hampers conversationalﬂuency, while the\nreduced information gathered during the inquiry phase contributes to\ndecreased response accuracy.\nTo assess the impact ofHistory Memory, we conducted case studies\non both initial and follow-up visits. These studies involve a comparative\nanalysis of outcomes whenHistory Memoryis and is not utilized during\nfollow-up visits. An example case study is depicted in Fig.\n5.T h i sc a s es t u d y\nunderscores the pivotal role ofHistory Memory in enhancing the\nhealthcare agent’s performance. When History Memory is used, it\nenables the healthcare agent to understand previous symptoms and medi-\ncations, enhancing the generationof questions. Conversely, whenHis-\ntory Memoryis not employed, the healthcare agent tends to revisit issues\nalready established in previous consultations. This redundancy can\ndiminish the efﬁciency of gathering new information and adversely impact\nthe user experience.\nDiscussion\nBoth automated and doctor evaluations conﬁrm that our healthcare agent\nsigniﬁcantly enhances general LLMs across seven key metrics: inquiry\nproactivity, inquiry relevance, conversationalﬂuency, accuracy, helpfulness,\nharmfulness reduction, and self-awareness. Our ablation studies attribute\nthese improvements to the synergistic design of the agent’sm o d u l a r\narchitecture. The Function module enables versatile scenario handling, with\nits Inquiry sub-module substantially improving information-gathering\ncapabilities. The Safety module enhances response accuracy and self-\nawareness while minimizing potential harm. The Doctor module provides\nhuman-in-the-loop reﬁnement, while the Memory component leverages\nboth current and historical patientinformation to improve diagnostic\naccuracy and recommendation quality.\nFurthermore, our healthcare agent demonstrates robust general-\nizability, achieving signiﬁcant performance improvements across both\nopen-source (e.g., Mixtral) and closed-source (e.g., GPT-4) models. How-\never, in general, a performance gap persists between open-source models\nand closed-source models, which mayb ec a u s e db yt h em e d i c a lk n o w l e d g e\nand the ability to follow the instructions of the models. Furthermore, when\nwe extend the application of healthcare agent to open-source medical LLMs,\nsuch as MedAlpaca-7B\n10, Meditron-7B26, and OpenBioLLM-8B27,o u r\nanalysis reveals that these models encounter challenges in healthcare agent\nguidelines, as evidenced by several failure cases outlined in the supple-\nmentary material. One possible solution isﬁne-tuning general LLMs.\nHowever, this paper focuses on developing a training-free, data-free\nmethodology applicable to all general LLMs. Fine-tuning general LLMs is\nbeyond the scope of this paper andwill be explored in future work.\nTo minimize evaluation overhead, we developed two automated\napproaches: virtual patients and automated evaluation. The virtual patient\nsystem employs LLMs to simulate realistic patient interactions, while our\nautomated evaluation utilizes a chain-of-thought\n28 based LLM-as-judge\nmethod. For the automated evaluation method, based on the high\nTable 3 | Ablation studies using scores on important modules\nModels Inquiry Quality Response Quality Safety\nInquiry\nProactivity\nInquiry\nRelevance\nConversational\nFluency\nAccuracy Helpfulness Harmfulness Self-awareness\nhealthcare agent 3.96 ± 0.20 4.20 ± 0.24 4.12 ± 0.38 3.94 ± 0.55 3.98 ± 0.53 4.14 ± 0.42 4.12 ± 0.52\nw/o Planner Sub-Module 4.18 ± 0.23 3.55 ± 0.41 3.88 ± 0.10 3.91 ± 0.63 3.99 ± 0.20 4.10 ± 0.33 4.15 ± 0.14\nw/o Inquiry Sub-Module 2.55 ± 0.30 1.98 ± 0.28 2.99 ± 0.41 3.76 ± 0.19 3.66 ± 0.32 4.16 ± 0.36 4.12 ± 0.31\nw/o Safety Module 4.00 ± 0.22 4.25 ± 0.50 4.10 ± 0.19 3.81 ± 0.46 3.95 ± 0.34 3.90 ± 0.14 3.40 ± 0.34\nw/o Discuss-then-\nModiﬁcation\n3.94 ± 0.22 4.22 ± 0.38 4.16 ± 0.20 3.88 ± 0.44 4.00 ± 0.23 4.06 ± 0.42 4.14 ± 0.55\nw/o Conversation Memory 3.98 ± 0.22 3.88 ± 0.13 4.10 ± 0.29 3.74 ± 0.15 3.64 ± 0.37 4.00 ± 0.62 4.09 ± 0.50\n0% 20% 40% 60% 80% 100%\nCorrect\nPlacement\nSuccessful\nIncorporation\n% Modify Responses\nTRUE FALSE\nFig. 4 |The inﬂuence of theDoctor module.\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 6\ncorrelation shown between doctor evaluation results and automated eva-\nluation results, this indicates that our automated evaluation method can\neffectively substitute for the doctor evaluation method and be used in large-\nscale evaluations. For the virtual patient method, considering that we use the\nreal-world doctor-patient consultation data and doctors are positive about\nthe realism of the simulation results, we believe our evaluation method is an\neffective alternative to clinical testing during early development phases,\nsubstantially reducing the human resources and time required for evalua-\ntion. We will engage our healthcare agent in real-world clinical testing in our\nfuture work.\nIn conclusion, we present a novel healthcare agent for medical con-\nsultation, which enables safe, effectivemedical conversations with patients.\nWe utilize a virtual patient method tosimulate the consultation conversa-\ntions, which are then assessed using bothd o c t o re v a l u a t i o na n da u t o m a t e d\nevaluation. Experimental results demonstrate that the healthcare agent can\nsigniﬁcantly enhance the capabilities ofgeneral LLMs in inquiry quality,\nresponse quality, and safety. Furthermore, our healthcare agent framework\nexhibits remarkable generalizability across different general LLMs. Con-\nsidering that existing commercial medical consultation services usually lack\npublicly disclosed technical details, we believe that this healthcare agent\nrepresents a feasible attempt and hope this research could further facilitate\nthe research on LLMs for medical applications.\nMethods\nIn this section, we detail the construction of healthcare agent for medical\nconsultation and the details of our experiments.\nHealthcare agent\nThe healthcare agent consists of three key components:Dialogue,\nMemory,a n dProcessing.T h eDialogue component serves as the\ninterface for patients to ensure their conversations with safety and effec-\ntiveness. TheMemorycomponent stores both current dialogue information\nand historical consultation data, aiming to enhance the ability ofDialo-\ngue.T h eProcessingcomponent is responsible for managing all dia-\nlogue contents, including generating summary reports to offer concise\noverviews of the consultations. The subsequent section will sequentially\nexplain the three key components and illustrate how they collaborate. All\nprompts utilized in the proposed healthcare agent are provided in the\nsupplementary material.\nThe Dialogue component is designed with the following critical\nrequirements: Firstly, it can managevarious medical tasks with smooth\ndialogue and provideﬁne-grained answers. Secondly, it can proactively ask\nfollow-up questions to facilitate gathering relevant information, particularly\nbeneﬁcial for patients with limited medical knowledge. This represents a\nsigniﬁcant departure from traditional medical LLMs, which primarily rely\non passive patient descriptions. Thirdly, it can adhere to ethical and safety\nstandards, highlighting the nature of the AI and the potential risks to\npatients, while diligently checking forfactual errors. Lastly, it can involve\nd o c t o r si nr e v i e w i n ga n dm o d i f y i n gresponses, ensuring professional\noversight and intervention when necessary. Following the above require-\nments, theDialoguecomponent is structured with three interconnected\nmodules:Function, Safety,a n dDoctor.\nThe Functionmodule is designed to handle three primary medical\ntasks: diagnosis, explanation, and recommendation. These tasks are chosen\nbased on the most common questions patients ask, as analyzed from haodf\n(https://www.haodf.com/) .W ec a ne x p a n dt h en u m b e ro ft a s k sa sn e e d e d .\nFor different tasks, we design speciﬁc instructions to guide LLMs in gen-\nerating professional content and invoking the required memory and tools.\nWhen additional information of the patient is needed, such as for medical\ndiagnosis, we introduce an inquiry task inspired by\n29 to prompt LLMs to ask\nmore questions. This process effectively facilitates multi-round QA sessions\nwith patients and guides them in providing more comprehensive infor-\nmation. Through such a series of well-guided questions, patients are assisted\nin articulating their actual conditions. We propose a planner that enables the\nmodule to automatically determinet h et a s kt ob ep e r f o r m e db a s e do nt h e\npatient’s input and the conversation memory.\nThe Safety module is tasked with ensuring the safety of the entire\ndialogue. It evaluates and address three types of safety vulnerabilities: 1)\nEthical risks. Healthcare agents must adhere to societal ethical standards for\nAI in medicine. For example,this includes explicitly disclosing the AI’s\nidentity and limitations, while avoiding inappropriate language that could\nbe perceived as hostile or disrespectfultoward patients; 2) Emergency risks.\nUnlike physicians who can provide immediate medical intervention, LLMs\nare limited to offering advice. Therefore, when encountering potentially life-\nthreatening conditions, such as blood pressure exceeding 180/120 mmHg,\nthe system must emphasize urgency and direct patients to seek immediate\nmedical attention; 3) Error risks. LLMs may generate factually incorrect\ninformation, which poses signiﬁcant dangers in medical contexts. For\ninstance, incorrect medication dosages could have severe consequences.\nThus, systematic error detection in responses is essential. To conduct a more\ndetailed safety inspection and improvement, we propose a discuss-then-\nmodiﬁcation process. First, we use different instructions to guide LLMs in\ndiscussing whether the response exhibits the aforementioned security vul-\nnerabilities. Then, a modiﬁcation LLM is employed to revise the response\nbased on the discussion results.\nThe Doctor module facilitates potential doctor intervention during\nthe dialogue. Doctors can readily review and modify responses proactively,\neither by directly editing the response or by providing concise guidance/\ninstruction for LLMs to make necessaryadjustments. Moreover, in critical\nsituations, this module is tasked with alerting doctors and requesting their\nintervention or assistance as needed. Importantly, in such instances, doctors\nare only required to provide simple directives or instructions, after which\nFig. 5 | An example showing the impact ofHistory Memory.For simplicity, we only show a portion of the doctor’s dialogue. See more details in the supplementary\nmaterial.\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 7\nLLMs will autonomously implement adjustments or additions at relevant\npoints in the response.\nThe Memorycomponent plays a crucial role in dialogue by furnishing\nrich information about the current inquiry and the patient’sh i s t o r i c a l\nrecords. It effectively acts as a doctor, allowing for the examination of\ncorresponding medical records to facilitate more accurate medical diag-\nnoses. As such, this component comprises two modules:Conversation\nMemory and History Memory. Both modules contribute to theDia-\nlogue component by providing prompts containing essential contextual\ninformation about the patient, thus aiding LLMs in generating more precise\nresponses.\nThe Conversationmodule records all information relevant to the\nongoing dialogue, including the patient’s questions, the patient’si n t e r a c -\ntions with Agent, and Agent’s responses. Providing the context of current\nconversation as part of the prompt proves to be crucial in prompting LLMs\nto generate more relevant contents. Basically, this module mainly enables\ncoherent conversations with multiple queries.\nThe History module is designed to store the patient’sh i s t o r yo f\nusing the agent. The historical records of a patient enable LLMs to better\ncomprehend the patient’s situation, potentially reducing the number of\nqueries required. Unlike theConversation Memory, which keeps\ncomplete records of current interaction, maintaining such extensive records\nof all past interactions can lead to longer context lengths, slower executions,\nand increased costs. Since theHistory Memorytypically holds less\nimmediately relevant information than theConversation Memory,w e\nsummarize historical conversations to save space and ensure system efﬁ-\nciency. That is, after each medical consultation, weﬁrst summarize the\nentire dialogue to retain only key information, and then store it in the\nHistory Memory. Additionally, to maintain the efﬁciency of theHis-\ntory Memory, we may also consider removing historical information that\nexceeds a certain time threshold (e.g., six months).\nThe Processing component provides post-processing functions\nafter patient dialogues. Currently, it only includes a content summarization\nmodule driven by LLMs for creating medical reports. These reports give an\noverview of the condition, diagnostic results, and recommendations. They\nserve two purposes: providing patients with a comprehensive summary of\ntheir consultation and giving doctors an effective brieﬁng during patient\nvisits.\nExperimental details\nExisting evaluation metrics of medical LLMs have predominantly focused\non either medical QA accuracy7,8 or natural language generation metrics like\nBLEU and ROUGE12,30. However, these objective metrics often fall short in\nadequately assessing real-world medical scenarios from the perspectives of\nusers31. To comprehensively evaluate the proposed healthcare agent, after\ncollaborating with doctors, we focus on three aspects: 1) theinquiry\nquality, which evaluates the ability to ask relevant and effective ques-\ntions; 2) theresponse quality, which measures the accuracy and\nprecision of the model’s responses; and 3) thesafety, which examines the\nmodel’s adherence to safety and ethical standards in its outputs. We further\ndeﬁne detailed evaluation metrics foreach aspect, encompassing seven\ncriteria. Forinquiry quality, we proposeinquiry proactivity\nto evaluate the activeness and comprehensiveness of the questions,\ninquiry relevanceto measure the alignment of the questions with the\npatient’sc o n d i t i o n ,a n dconversationalﬂuency to assess the user’s\nconversational experience. Forresponse quality,w eu s eaccuracy\nto evaluate the correctness of the diagnosis andhelpfulnessto deter-\nmine the value of the recommendations provided to the patient. For\nsafety, we considerharmfulnessto assess errors in the response and\ntheir potential risks andself-awarenessto evaluate the model’sa b i l i t y\nto acknowledge ethical obligations and inform users of potential risks.\nFurther details and examples of evaluation process are provided in the\nsupplementary material.\nGiven the potential ethical and safety risks associated with clinical\ntesting, dialogue data is typically collected during the research phase by\nhaving doctors simulate the role of patients, which demands considerable\nmanpower and time. As delineated in Isaza-Restrepo et al.\n32 a n dT ue ta l .31,\nLLMs possess signiﬁcant potential for simulatingthe role of patients during\nmedical consultations. Motivated by this, we primarily employ ChatGPT to\nact as virtual patients, evaluating the capabilities of our healthcare agent. To\nenhance ChatGPT’s capabilities as virtual patients, we utilize real cases from\nthe MedDialog dataset33 as references, ensuring authenticity and relevance\nin the simulated medical scenarios. The MedDialog dataset is chosen for our\nexperiments because it includes three critical components: descriptions of\npatients’ medical conditions, dialogues between doctors and patients, and\nthe corresponding diagnoses and recommendations provided by doctors.\nThis dataset includes 20 different medical specialties, such as Oncology,\nPsychiatry, and Otolaryngology, to ensure the comprehensiveness of\nmedical scenarios. For testing, weﬁrst select data containing dialogues\nextending beyond 40 rounds to ensure the acquisition of sufﬁcient and\nrobust information. Then we create a patient vignette based on each dia-\nlogue, summarizing the patient’s information. During the simulated med-\nical consultation process, the virtual patient will engage in the conversation\nb a s e do nt h i sp a t i e n tv i g n e t t e .F u r t h e rd e t a i l sa r ea v a i l a b l ei nt h es u p p l e -\nmentary material.\nFor the consultation dialogue data generated using the virtual patient\nmethod, we evaluate and score them based on the predeﬁned evaluation\nmetrics. We apply bothdoctor evaluationand automated eva-\nluation. Doctor evaluationis the most professional and reliable\nassessment approach, wherein doctor scores the consultation conversation\nbased on their expertise and experience. In our experiments, we invite seven\ndoctors to conduct the evaluation. However,doctor evaluation\nrequires signiﬁcant time and effort from the doctors. To validate the per-\nformance of our healthcare agent with more data and models, we further\npropose theautomated evaluationm e t h o d .I nt h i sm e t h o d ,W eu t i l i z e\nChatGPT as the evaluator following the practices of research conducted by\nLin and Chen\n34 and Liu et al.35. ChatGPT performs an analysis based on\npredeﬁned criteria, followed by the execution of scoring and ranking pro-\ncesses using the chain-of-thought strategy28. More details of doctor and\nautomated evaluation are providedin the supplementary material.\nData Availability\nThe datasets generated and analysed during this study are included in its\nsupplementary informationﬁles.\nCode availability\nThe underlying code for this study is available in its supplementary infor-\nmation ﬁles.\nReceived: 12 March 2025; Accepted: 1 July 2025;\nReferences\n1. McCall, B. Could telemedicine solve the cancer backlog?Lancet\nDigital Health2, e456– e457 (2020).\n2. Brown, T. et al. Language models are few-shot learners. InNeurIPS\nhttps://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html (2020).\n3. Achiam, J. et al. Gpt-4 technical report.arXiv preprinthttps://doi.org/\n10.48550/arXiv.2303.08774 (2023).\n4. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels. arXiv preprinthttps://doi.org/10.48550/arXiv.2307.09288\n(2023)\n5. Yang, X. et al. Gatortron: A large clinical language model to unlock\npatient information from unstructured electronic health records.npj\nDigit. Med.5, 194.https://doi.org/10.1038/s41746-022-00742-2\n(2022).\n6. Peng, C. et al. A study of generative large language model for medical\nresearch and healthcare.npj Digit. Med.6, 210.https://doi.org/10.\n1038/s41746-023-00958-w (2023).\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 8\n7. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172– 180 (2023).\n8. Singhal, K. et al. Towards expert-level medical question answering\nwith large language models.Nat. Med.31, 943– 950. https://doi.org/\n10.1038/s41591-024-03423-7 (2025).\n9. Li, Y. et al. Chatdoctor: A medical chat modelﬁne-tuned on a large\nlanguage model meta-ai (llama) using medical domain knowledge.\nCureus 15 https://doi.org/10.7759/cureus.40895 (2023).\n10. Han, T. et al. Medalpaca– an open-source collection of medical\nconversational ai models and training data.arXiv preprinthttps://doi.\norg/10.48550/arXiv.2211.09085 (2023).\n11. Xiong, H. et al. Doctorglm: Fine-tuning your chinese doctor is not a\nherculean task.arXiv preprinthttps://doi.org/10.48550/arXiv.2304.\n01097 (2023).\n12. Chen, Y. et al. Bianque: Balancing the questioning and suggestion ability\nof health llms with multi-turn health conversations polished by chatgpt.\narXiv preprinthttps://doi.org/10.48550/arXiv.2310.15896(2023).\n13. Ouyang, L. et al. Training language models to follow instructions with\nhuman feedback. InNeurIPS https://papers.nips.cc/paper_ﬁles/\npaper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-\nConference.html (2022).\n14. Chowdhery, A. et al. Palm: Scaling language modeling with pathways.\nJ. Mach. Learn. Res.24,1 – 113 (2023).\n15. Anil, R. et al. Palm 2 technical report.arXiv preprinthttps://doi.org/10.\n48550/arXiv.2305.10403 (2023).\n16. Touvron, H. et al. Llama: Open and efﬁcient foundation language models.\narXiv preprinthttps://doi.org/10.48550/arXiv.2302.13971(2023).\n17. Du, Z. et al. Glm: General language model pretraining with\nautoregressive blank inﬁlling. InACL https://doi.org/10.18653/v1/\n2022.acl-long.26(2021).\n18. Tang, X. et al. Medagents: Large language models as collaborators for\nzero-shot medical reasoning. InFinding of ACL(2024).\n19. Kim, Y. et al. MDAgents: An Adaptive Collaboration of LLMs for\nMedical Decision-Making. InNeurIPS (2024).\n20. Chen, J. et al. Cod, towards an interpretable medical agent using\nchain of diagnosis.arXiv preprint(2024).\n21. Liu, J. et al. Medchain: Bridging the gap between llm agents and\nclinical practice through interactive sequential benchmarking.arXiv\npreprint5 (2024).\n22. Ma, C. et al. An iterative optimizing framework for radiology report\nsummarization with chatgpt.IEEE Transactions on Artiﬁcial\nIntelligence (2024).\n23. Sudarshan, M. et al. Agentic llm workﬂ\nows for generating patient-\nfriendly medical reports.arXiv preprint(2024).\n24. Zhao, Z. et al. Chatcad+: Towards a universal and reliable interactive\ncad using llms.IEEE Transactions on Medical Imaging(2024).\n2 5 . L i u ,C . ,M a ,Y . ,K o t h u r ,K . ,N i k p o u r ,A .&K a v e h e i ,O .B i o s i g n a lc o p i l o t :\nLeveraging the power of llms in drafting reports for biomedical signals.\nmedRxiv2023– 06https://doi.org/10.1101/2023.06.28.23291916(2023).\n26. Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large\nlanguage models.arXiv preprint(2023).\n27. Ankit Pal, M. S. Openbiollms: Advancing open-source large language\nmodels for healthcare and life sciences.https://huggingface.co/\naaditya/OpenBioLLM-Llama3-70B (2024).\n28. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models. InNeurIPS https://papers.nips.cc/paper_ﬁles/\npaper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-\nConference.html (2022).\n29. Mishra, S. & Nouri, E. Help me think: A simple prompting strategy for\nnon-experts to create customized content with models. InACL\nhttps://doi.org/10.18653/v1/2023.ﬁndings-acl.751 (2022).\n30. Zhang, H. et al. Huatuogpt, towards taming language model to be a\ndoctor. InEMNLP https://doi.org/10.18653/v1/2023.ﬁndings-emnlp.\n725 (2023).\n31. Tu, T. et al. Towards conversational diagnostic artiﬁcial intelligence.\nNature 642, 442– 450. https://doi.org/10.1038/s41586-025-08866-7\n(2025).\n32. Isaza-Restrepo, A., Gómez, M. T., Cifuentes, G. & Argüello, A. The\nvirtual patient as a learning tool: a mixed quantitative qualitative study.\nBMC Med. Educ.18,1 – 10 (2018).\n33. Zeng, G. et al. Meddialog: Large-scale medical dialogue datasets. In\nEMNLP, 9241– 9250. https://aclanthology.org/2020.emnlp-main.743\n(2020).\n34. Lin, Y.-T. & Chen, Y.-N. Llm-eval: Uniﬁed multi-dimensional automatic\nevaluation for open-domain conversations with large language\nmodels. InNLP4ConvAI (2023).\n35. Liu, Y. et al. G-eval: NLG Evaluation using Gpt-4 with Better Human\nAlignment. In EMNLP(2023).\nAcknowledgements\nThis project is supported by the National Research Foundation, Singapore,\nunder its NRF Professorship Award No. NRF-P2024-001.\nAuthor contributions\nZ.R. and Y.Z. conceived and designed this study. Z.R. developed code and\nconducted experiments. Z.R., Y.Z., B.Y., L.D., and D.T. contributed to write\nthe manuscript. P.X. organised and conducted doctor evaluation. Y.Z., B.Y.\nand D.T. supervised this work. All authors have read and approved the\nmanuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s44387-025-00021-x\n.\nCorrespondenceand requests for materials should be addressed to\nYibing Zhan or Dacheng Tao.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s44387-025-00021-x Article\nnpj Artiﬁcial Intelligence|            (2025) 1:24 9",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6378200650215149
    },
    {
      "name": "Power (physics)",
      "score": 0.41499701142311096
    },
    {
      "name": "Psychology",
      "score": 0.3836906850337982
    },
    {
      "name": "Medicine",
      "score": 0.33996647596359253
    },
    {
      "name": "Political science",
      "score": 0.16290083527565002
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}