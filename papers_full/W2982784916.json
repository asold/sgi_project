{
  "title": "Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models",
  "url": "https://openalex.org/W2982784916",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2987938111",
      "name": "Batsergelen Myagmar",
      "affiliations": [
        "University of Tsukuba"
      ]
    },
    {
      "id": "https://openalex.org/A2045262209",
      "name": "Jie Li",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2152401214",
      "name": "Shigetomo Kimura",
      "affiliations": [
        "University of Tsukuba"
      ]
    },
    {
      "id": "https://openalex.org/A2987938111",
      "name": "Batsergelen Myagmar",
      "affiliations": [
        "University of Tsukuba",
        "Tsukuba University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2045262209",
      "name": "Jie Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152401214",
      "name": "Shigetomo Kimura",
      "affiliations": [
        "Tsukuba University of Technology",
        "University of Tsukuba"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6890311436",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2167660864",
    "https://openalex.org/W2741989495",
    "https://openalex.org/W2142617193",
    "https://openalex.org/W2567698949",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2153353890",
    "https://openalex.org/W6637618735",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6732958910",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6691459498",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4211186029",
    "https://openalex.org/W4210984920",
    "https://openalex.org/W6736353073",
    "https://openalex.org/W6685415078",
    "https://openalex.org/W2964117661",
    "https://openalex.org/W6734335776",
    "https://openalex.org/W2918288435",
    "https://openalex.org/W2062118960",
    "https://openalex.org/W6793575156",
    "https://openalex.org/W6682778277",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4294375521",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949847915",
    "https://openalex.org/W2963275094",
    "https://openalex.org/W2962796276",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963217615",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3152368098",
    "https://openalex.org/W1924770834"
  ],
  "abstract": "Cross-domain sentiment classification is an important Natural Language Processing (NLP) task that aims at leveraging knowledge obtained from a source domain to train a high-performance learner for sentiment classification on a target domain. Existing transfer learning methods applied on cross-domain sentiment classification mostly focus on inducing a low-dimensional feature representation shared across domains based on pivots and non-pivots, which is still a low-level representation of sequence data. Recently, there have been great progress in the NLP literature in developing high-level representation language models based on Transformer architecture, which are pre-trained on large text corpus and fine-tuned for specific task with an additional layer on top. Among such language models, the bidirectional contextualized Transformer language models of BERT and XLNet have greatly impacted NLP research field. In this paper, we fine-tune BERT and XLNet for the cross-domain sentiment classification. We then explore their transferability in the context of cross-domain sentiment classification through in-depth analysis of two models' performances and update the state-of-the-arts with a significant margin of improvement. Our results show that such bidirectional contextualized language models outperform the previous state-of-the-arts methods for cross-domain sentiment classification while using up to 120 times less data.",
  "full_text": "Received September 30, 2019, accepted October 22, 2019, date of publication November 8, 2019,\ndate of current version November 20, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2952360\nCross-Domain Sentiment Classification With\nBidirectional Contextualized Transformer\nLanguage Models\nBATSERGELEN MYAGMAR\n 1, JIE LI2, (Senior Member, IEEE),\nAND SHIGETOMO KIMURA1, (Member, IEEE)\n1Department of Computer Science, University of Tsukuba, Tsukuba 305-8577, Japan\n2Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China\nCorresponding author: Jie Li (lijiecs@sjtu.edu.cn)\nThis work was supported by the JSPS KAKENHI under Grant JP26280027.\nABSTRACT Cross-domain sentiment classiﬁcation is an important Natural Language Processing (NLP)\ntask that aims at leveraging knowledge obtained from a source domain to train a high-performance learner\nfor sentiment classiﬁcation on a target domain. Existing transfer learning methods applied on cross-domain\nsentiment classiﬁcation mostly focus on inducing a low-dimensional feature representation shared across\ndomains based on pivots and non-pivots, which is still a low-level representation of sequence data. Recently,\nthere have been great progress in the NLP literature in developing high-level representation language models\nbased on Transformer architecture, which are pre-trained on large text corpus and ﬁne-tuned for speciﬁc task\nwith an additional layer on top. Among such language models, the bidirectional contextualized Transformer\nlanguage models of BERT and XLNet have greatly impacted NLP research ﬁeld. In this paper, we ﬁne-tune\nBERT and XLNet for the cross-domain sentiment classiﬁcation. We then explore their transferability in the\ncontext of cross-domain sentiment classiﬁcation through in-depth analysis of two models’ performances\nand update the state-of-the-arts with a signiﬁcant margin of improvement. Our results show that such\nbidirectional contextualized language models outperform the previous state-of-the-arts methods for cross-\ndomain sentiment classiﬁcation while using up to 120 times less data.\nINDEX TERMS Transfer learning, cross-domain sentiment classiﬁcation, pre-trained language model.\nI. INTRODUCTION\nWith the user sentiment and opinion expressions becoming\nwidespread throughout social and e-commerce platforms,\ncorrectly understanding these thoughts and views becomes\nimportant in facilitating various downstream applications [1].\nSentiment classiﬁcation, an important task of Natural Lan-\nguage Processing (NLP), aims to identify the emotional ten-\ndencies (positive or negative) of given text input [2] and has\nattracted great research attention in recent years.\nDeep neural networks have been successfully applied\nfor diverse machine learning problems, including various\nNLP tasks, with greatly improved prediction performance\nmetrics. The standard model training for a NLP task had\nfocused on initializing the ﬁrst layer of a neural network\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Ah Hwee Tan.\nwith pretrained word vectors such as word2vec [3] and\nGloVe [4], and the rest of the network is trained on the\ntask-speciﬁc data with convolutional and/or recurrent neu-\nral networks. Convolutional neural networks (CNN) [5] are\nable to learn the local response from the temporal or spa-\ntial data but lack the ability to learn sequential correlations.\nRecurrent Neural Networks (RNN) [6] are used because of\ntheir sequence modeling capabilities and dealing with short-\nterm dependencies in a sequence of data, but have trouble\nwhen dealing with long-term dependencies. Long Short-Term\nMemory networks (LSTM) [7], which is a variation of RNN\narchitecture, aims to solve the long-term dependency prob-\nlem by introducing a memory into the network. RNN-based\ndeep learning architectures has been the standard for vari-\nous NLP tasks, including sentiment classiﬁcation. However,\nthese approaches still processed context in one direction only,\ni.e., create dependencies only on the left or right side of the\nVOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 163219\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\ncurrent word. Therefore they cannot capture contexts in both\ndirections at the same time, i.e., consider words on both sides\nof the current word when capturing dependencies.\nMost of these performance improvements in NLP with\ndeep neural networks come only via supervised learning with\nmassive amounts of labeled data. However, in real world\napplications, there are many scenarios where it is difﬁcult to\ncollect sufﬁcient data for high-performing supervised learn-\ning model of a speciﬁc task due to factors of scarcity of\nreadily available data or the high expense of data collection.\nIn addition, statistical classiﬁers assume that both the train-\ning and test data come from a common underlying distribu-\ntion [8], but due to the high variability and sparsity of natural\nlanguage, oftentimes there is distribution differences in the\nreal world data and the specialized training data [9].\nTransfer learning allows us to deal with this scenario\nby borrowing information from a relevant source domain\nwith abundant labeled data to help improve the prediction\nperformance in the target domain [10]. Cross-domain sen-\ntiment classiﬁcation (CDSC) aims at leveraging knowledge\nobtained from a source domain to train a high-performance\nlearner for sentiment classiﬁcation on a target domain, e.g.,\nbook product review, to help classiﬁcation in the target\ndomain, e.g., electronics product review, with few or no\nlabeled data. In the literature, transfer learning techniques\nhave been applied to CDSC. Traditional pivot-based CDSC\nschemes in [11], [12] attempt to infer the correlation between\npivot words, i.e., the domain-shared sentiment words, and\nnon-pivot words, i.e., the domain-speciﬁc sentiment words,\nby utilizing multiple pivot prediction tasks. However, these\nschemes share a major limitation that manual selection of\npivots is required.\nAll of the above discussed schemes need to train a ded-\nicated NLP model from scratch for every new task with\nits own specialized training data, which could take days\nand weeks to converge to a stable, high-performance model.\nAlternatively, substantial work has shown that unsupervised\npre-trained language models on large text corpus are bene-\nﬁcial for text classiﬁcation and other NLP tasks, which can\navoid training a new model from scratch. Various approaches\nare proposed for training general purpose language repre-\nsentation models using an enormous amount of unanno-\ntated text, such as ELMo [13] and GPT [14]. Pre-trained\nmodels can be ﬁne-tuned on NLP tasks without requiring\nhuge amount of labeled data and have achieved signiﬁ-\ncant improvement over training on task-speciﬁc annotated\ndata. More recently, a pre-training technique, Bidirectional\nEncoder Representations from Transformers (BERT) [15],\nis proposed and has created state-of-the-art models for a\nwide variety of NLP tasks,including question answering\n(SQuAD v1.1), natural language inference, text classiﬁcation\nand others. The latest of such pre-trained language models is\nXLNet [16], a generalized autoregressive pretraining method\nthat enables learning bidirectional contexts by maximizing\nthe expected likelihood over all permutations of the factor-\nization order, and overcomes the limitations of BERT thanks\nto its autoregressive formulation. Furthermore, XLNet inte-\ngrates ideas from Transformer-XL [17], the state-of-the-art\nautoregressive model, into pretraining. In this paper, we ﬁne-\ntune BERT and XLNet for CDSC and compare them with\nthe current state-of-the-art methods. We also closely study\ntheir performances in comparison to each other with various\nexperimental settings.\nOur main contributions are summarized as follows:\n• This is the ﬁrst work to explore the usage of\nTransformer-based bidirectional contextualized lan-\nguage models for CDSC.\n• Compare and comprehensively analyze the performance\nof the two highest performing Transformer language\nmodels of XLNet and BERT in the context of CDSC.\n• Achieves new state-of-the-arts results with signiﬁcant\nimprovements over the previous approaches.\nII. RELATED WORKS\nOver the last decade, many methods have been pro-\nposed for cross-domain sentiment classiﬁcation. Structural\nCorrespondence Learning (SCL) method is proposed by\nBlitzer et al. [11] to learn a joint low-dimensional fea-\nture representation for the source and target domains.\nSimilarly, Pan et al. [18] propose a Spectral Feature Align-\nment (SFA) method to align the pivots with the non-pivots\nto build a bridge between the source and target domains.\nHowever, these methods need to manually select the pivots\nbased on criterions such as the frequency in both domains,\nthe mutual information between features and labels on the\nsource domain data, and the mutual information between\nfeatures and domains [18]. Domain-Adversarial training of\nNeural Networks (DANN) is proposed by Ganin et al. [19] for\ndomain adaptation using a gradient reversal layer to reverse\nthe gradient direction in order to produce representations such\nthat a domain classiﬁer cannot predict the domain of the\nencoded representation, and at the same time, a sentiment\nclassiﬁer is built on the representation shared by domains to\nreduce the domain discrepancy and achieves better perfor-\nmance for cross-domain sentiment classiﬁcation. Proposed\napproaches by Sun et al. [20], and Zellinger et al. [21]\nfocus on learning domain invariant features whose distribu-\ntion is similar in source and target domain. They attempt\nto minimize the discrepancy between domain-speciﬁc latent\nfeature representations. However, all the domain alignment\napproaches can only reduce, but not remove, the domain\ndiscrepancy. Therefore, the target samples distributed near\nthe edge of the clusters, or far from their corresponding class\ncenters are most likely to be misclassiﬁed by the hyperplane\nlearned from the source domain [22].\nTransfer learning has been successfully applied in com-\nputer vision where lower network layers are trained on high-\nresource supervised datasets like ImageNet to learn generic\nfeatures [5], and are then ﬁne-tuned on target tasks, lead-\ning to impressive results for image classiﬁcation and object\ndetection [23], [24]. Following the successful practice of\npre-trained models for computer vision tasks, high-level\n163220 VOLUME 7, 2019\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\ncontextualized language models pre-trained on unlabeled\nlarge text corpus and ﬁne-tuned for a given speciﬁc task\nhave recently been proposed in NLP with great results.\nHoward and Ruder [25] proposed ULMFiT, the ﬁrst to pro-\npose ﬁne-tuning with pre-trained language model, show-\ncasing the effectiveness of discriminative ﬁne-tuning, and\ngradual unfreezing for retaining prior knowledge and cir-\ncumventing catastrophic forgetting during ﬁne-tuning. There\nare two existing strategies for applying pre-trained language\nrepresentations to downstream tasks: feature-based and ﬁne-\ntuning. The feature-based approach, such as ELMo pro-\nposed by Peters et al. [13], uses tasks-speciﬁc architectures\nthat include the pre-trained representations as additional fea-\ntures. Many ﬁne-tuning approaches, such as the Generative\nPre-trained Transformer (OpenAI GPT) proposed by Rad-\nford et al. [14] and the Bidirectional Encoder Representations\nfrom Transformers (BERT) proposed by Devlin et al. [15]\nintroduce minimal task-speciﬁc parameters, and are trained\non the downstream tasks by simply ﬁne-tuning the pre-trained\nparameters. Among the unsupervised pre-training methods\nfor language models in the literature, the two most success-\nful pretraining objectives are autoregressive (AR) language\nmodeling that seeks to estimate the probability distribution\nof a text corpus with an autoregressive model [13], [14],\nand autoencoding (AE) language modeling that aims to\nreconstruct the original data from corrupted input [15].\nYang et al. [16] proposed the XLNet, a combination of AR\nand AE language modeling where it can capture dependen-\ncies beyond the input sequence limit and process bidirectional\ncontexts at the same time .\nIII. PROBLEM DESCRIPTION AND NOTATIONS\nWe use the most common notations for transfer learning as\ndeﬁned in [18] applied on cross-domain sentiment classiﬁ-\ncation. Transfer learning comprises of two main concepts:\na domain and a task. A domain D consists of a feature space\nX, and a marginal probability distribution P(X) over the\nfeature space, where X ={x1,..., xn}∈ X. For a binary bag-\nof-words representation of an input text document, the feature\nspace X would be the set of all possible binary term vectors,\nxi is the i-th term vector corresponding to input and X is the\nrandom variable associated with sampling input documents.\nGiven a domain D = {X,P(X)}, a task T deﬁnes a label\nspace Y and a conditional probability distribution P(Y |X) that\nis learned from the training data pairs of xi ∈X and yi ∈Y.\nIn the context of binary sentiment classiﬁcation task, Y is\nthe set of all possible labels {1,0}representing positive and\nnegative sentiments, yi has value of either 1 or 0, and Y is the\nrandom variable associated with input document’s label.\nGiven a source domain DS with its task TS and a target\ndomain DT with its task TT , the objective of cross-domain\nsentiment classiﬁcation is to learn the conditional probability\ndistribution PT (YT |XT ) in DT by utilizing the knowledge\nlearned from DS and TS , where DS ̸= DT and sufﬁcient\nlabeled training data are available in DS . Typically either only\nfew or no labeled data are available in the target domain DT .\nIn cross-domain sentiment classiﬁcation, the marginal proba-\nbility distributions in source and target domains are different\nPS (XS ) ̸=PT (XT ), i.e. the text documents in these domains\ndiscuss different topics. The task of the cross-domain senti-\nment classiﬁcation is to learn a robust classiﬁer PS (YS |XS )\ntrained on labeled data in the source domain to predict the\npolarity of unlabeled examples from the target domain using\nthe learned classiﬁer PS (YT |XT ), where YT =YS , i.e., both\ndomains have the same label space.\nIV. BIDIRECTIONAL TRANSFORMER LANGUAGE MODELS\nA. TRANSFORMER\nBefore the introduction of Transformers, previous state-of-\nthe-art sequence modeling approaches in NLP relied mostly\non recurrent neural networks (RNN), such as Long Short-\nTerm Memory (LSTM) [7] and gated RNN [26]. However,\nthe recurrent models’ inherent sequential nature stymies par-\nallelization during training and limits its ability to contex-\ntualize longer input sequences. Attention mechanisms have\nbecome an integral part of compelling sequence modeling\nand transduction models in various tasks, allowing model-\ning of dependencies without regard to their distance in the\ninput or output sequences [27].\nThe Transformer [28] is ﬁrst introduced to improve the\nspeed of training models for neural machine translations\nusing the attention mechanism. Its architecture reduces\nsequential computation with multiple self-attention heads.\nIn order to compute a representation of an input sequence,\nself-attention mechanism associates different positions of\nthe sequence. Multi-head attention allows the model to\njointly attend to information from different representation\nsubspaces at different positions. The original Transformer\nhas encoder-decoder structure, with the encoder mapping an\ninput sequence to a sequence of continuous representations,\nwhich is used by the decoder to generate an output sequence\none element at a time. Each of the encoder and the decoder\nconsists of 6 identical layers, with each containing two sub-\nlayers of 8 parallel self-attention heads and a fully connected\nfeed-forward neural network.\nThe input representation to the ﬁrst encoder layer is a\nconcatenation of WordPiece embeddings [29] and positional\nembeddings generated from the input sequence. An attention\nfunction can be described as mapping a query and a set of key-\nvalue pairs to an output, where the query, keys, values, and\noutput are all vectors. The output is computed as a weighted\nsum of the values, where the weight assigned to each value is\ncomputed by a compatibility function of the query with the\ncorresponding key. Speciﬁcally, given an embedded vector\nx for an input sequence, we create a Query, Key, and Value\nvector for each input embedding token by multiplying the\nembedding by three learned matrices W Q,W K ,W V respec-\ntively. For parallel computation, we stack the Query, Key and\nValue vectors into matrices Q,K,V . Then the self-attention\nfunction is given by:\nAttention(x)=Attention(Q,K,V )=softmax(QK⊤\n√dk\n)V , (1)\nVOLUME 7, 2019 163221\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nFIGURE 1. The encoder layers of the standard transformer architecture. Each encoder layer’s output is passed as the input to the next layer.\nwhere dk is the dimension of queries and keys. The Trans-\nformer performs such self-attention function in parallel with\nmultiple attention heads by projecting the queries, keys and\nvalues h times with different, learned linear projections to\ndk ,dk and dv dimensions, respectively. Attention function\nis performed in parallel on each of these projected versions\nof queries, keys and values, resulting dv-dimensional output\nvalues.\nMultiHead(x) =MultiHead(Q,K,V )\n=Concat(head1,..., headh)W O, (2)\nwhere headi =Attention(QW Q\ni ,KW K\ni ,VW V\ni ), Concat is the\nconcatenation function, the projections are parameter matri-\nces W Q\ni ∈Rdmodel ×dk , W K\ni ∈Rdmodel ×dk , W V\ni ∈Rdmodel ×dv\nand W O ∈Rhdv×dmodel with dmodel =dk h.\nEach Transformer layer consists of two sub-layers. The\nﬁrst sub-layer is the multi-head attention and its normalized\noutput is fed to the second sub-layer of fully connected feed\nforward network. The activation function for the feed forward\nnetworks is ReLU. Formally, the hidden states of Transformer\nwith M number of Transformer layers are calculated as\nfollows:\nTrm(x) =norm(Att(x) +FFN(att(x))), (3)\nwhere\nAtt(x) =norm(x +MultiHead(x))\nFFN(x) =max(0,xW1 +b1)W2 +b2,\nwhere norm is the normalization function with linear con-\nnection following [30], FFN is fully connected feed forward\nnetwork, W1 and W2 are the weights of the ﬁrst and second\nfully connected networks with b1, b2 as bias values, and\nm ∈M. These fully connected networks have separate weight\nparameters for each encoder layer. Each encoder layer passes\nits output as an input to the next encoder layer, with the ﬁnal\nencoder layer producing the ﬁnal encoded representation\nfor ﬁne-tuning. Fig. 1 shows the architecture of the Trans-\nformer’s layers. In the original Transformer [28], the layer\nsize M is 6 and the multi-head h is 8.\nB. BERT\nBERT, which stands for Bidirectional Encoder Represen-\ntations from Transformers, is built upon recent works in\npre-training contextual representations such as ELMo [13],\nand ULMFiT [25], but these models are either unidirec-\ntional or shallowly bidirectional, meaning contextualized rep-\nresentation of a word only considers the words to its left or to\nits right. BERT, on the other hand, has deeply bidirectional\ncontextualization that combines the representations of both\nleft-context and right-context models. Its model architecture\nis a multi-layer bidirectional Transformer encoder based on\nthe original Transformer model proposed in [28]. The BERT\nmodel retains only the encoder part of the original model,\nwithout any decoder. It has 12 identical encoder layers, with\neach having two sub-layers of 12 parallel attention head and\nalso a fully connected feed-forward network.\nFor pre-training, unlike ELMo [13] and OpenAI GPT [14]\nthat use left-to-right or right-to-left language models, BERT\nuses two unsupervised prediction tasks. First is next sentence\nprediction task, where two sentences ( A,B) are selected from\nthe text corpus and a classiﬁer is trained to predict whether\nB actually follows A. 50% of the time B is the actual next\nsentence that follows A, and 50% of the time it is a random\nsentence from the corpus. The second task is the Masked Lan-\nguage Model task, where they mask some percentage of the\ninput tokens at random, and then predict only those masked\ntokens. Speciﬁcally, given a text sequence x =[x1,..., xT ],\nBERT ﬁrst constructs a corrupted version ˆx by randomly\nsetting a 15% of tokens in x to a special symbol [MASK].\nIf denote the masked tokens as ¯x, then the training objective\nis to reconstruct ¯x from ˆx:\nmax\nθ\nlog pθ(¯x|ˆx) ≈\nT∑\nt=1\nmt log pθ(xt |ˆx)\n=\nT∑\nt=1\nmt log exp(Hθ(ˆx)⊤\nt e(xt ))∑\nx′exp(Hθ(ˆx)⊤t e(x′)), (4)\nwhere mt = 1 indicates token xt is masked, e(x) denotes\nthe embedding of x and Hθ is a Transformer that maps a\nlength-T text sequence x into a sequence of hidden vectors\nHθ(x) =[Hθ(x)1,Hθ(x)2,..., Hθ(x)T ]. Note that the ≈sign\nin (4) indicates that when calculating pθ(¯x|ˆx), BERT makes\nan independence assumption that all masked tokens ¯x are\nseparately constructed. The biggest advantage of this training\nobjective is it allows the model simultaneous access to the\n163222 VOLUME 7, 2019\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\ncontextual information on both sides of a token. BERT is\nthe ﬁrst ﬁne-tuning based representation model that achieves\nstate-of-the-art performance on a large suite of sentence-level\nand token-level tasks, outperforming many systems with task-\nspeciﬁc architectures and advances the state-of-the-art for\neleven NLP tasks [15].\nC. XLNET\nBERT has achieved strong performances across multiple\ntasks but it had the following major ﬂaws:\n• The original Transformer architecture can capture con-\ntext within the speciﬁed maximum input sequence\nlength. If a document is longer than the speciﬁed\nlength, it would be divided into segments with each of\nthem being processed by the model independently from\nscratch without any connection between them.\n• BERT is trained to predict tokens replaced with the\n[MASK] symbol. However, this [MASK] token never\nappears in downstream tasks, which creates a discrep-\nancy between pre-training and ﬁne-tuning.\n• BERT makes predictions for the masked tokens with\nassumption that there is no dependencies between these\nmasked tokens, which is bit over-simpliﬁcation and can\ncause reduced number of dependencies that BERT can\nlearn at once.\nXLNet [16] solves BERT’s ﬁrst ﬂaw of input length con-\ntext constraint with the architecture of Transformer-XL [17],\nwhich itself is a modiﬁcation upon the original Trans-\nformer [28]. Transformer-XL introduces Recurrence Mech-\nanism and Relative Positional Encoding to the Transformer\narchitecture to capture long-term dependencies for docu-\nments that are longer than the maximum allowed input length.\nWith Recurrence Mechanism, the hidden state sequence com-\nputed for the previous segment is ﬁxed and cached to be\nreused as an extended context when the model processes the\nnext new segment. Although the gradient still remains within\na segment, this additional input allows the network to exploit\ninformation in the history, leading to an ability of modeling\nlonger-term dependency and avoiding context fragmentation.\nRelative Positional Encoding encodes position of a context\nin relative distance from the current token at each attention\nmodule, as opposed to encoding position statically only at\nthe beginning like in BERT. This is done so to accommodate\nthe Recurrence Mechanism and avoid having tokens from\ndifferent segments having the same positional encoding.\nDespite its ability to capture long-term dependencies,\nTransformer-XL still only holds unidirectional context,\ni.e., predicts the current token based on the given sequen-\ntial context on its left or its right side only. XLNet solves\nthe issue of unidirectional context, without using [MASK]\nsymbol as in BERT, by introducing a language modeling\nobjective called Permutation language modeling that predicts\na current token based on the given preceding context just like\ntraditional language model. However, instead of predicting\ntokens in sequential order, tokens are predicted following\na random permutation order. One problem with this objective\nis the computational high expense and slow convergence if\nwe to go through every permutation. Hence to reduce the\noptimization difﬁculty, only the last tokens in a factorization\norder is chosen for training. Formally, let ZT be the set of\nall possible permutations of the length- T index sequence\n[1,2,..., T ] with zt and z<t denoting the t-th element and\nthe ﬁrst t −1 elements of a permutation z ∈ZT . To choose\nthe tokens in a factorization order, z is split into a non-target\nsubsequence z≤c and a target subsequence z>c, where c is\nthe cutting point. Then the permutation language modeling\nobjective is to maximize the log-likelihood of the target\nsubsequence conditioned on the non-target subsequence as\nfollows:\nmax\nθ\nEz∼Zt [log pθ(xz>c |xz≤c )]\n=Ez∼Zt [\n|z|∑\nt=c+1\nlog pθ(xzt |xz<t )]\n=Ez∼Zt [\n|z|∑\nt=c+1\nlog exp(e(x)⊤gθ(xz<t ,zt ))∑\nx′exp(e(x′)⊤gθ(xz<t ,zt )), (5)\nwhere e(x) denotes the embedding of x input sequence,\ngθ(xz<t ,zt ) denotes a new type of representations which\nadditionally take the target position zt as input. To compute\ngθ(xz<t ,zt ), XLNet introduces a scheme called Two-Stream\nSelf-Attention that uses two sets of hidden representations:\n• The content stream hθ(xz≤t ), or abbreviated as hzt ,\nis same as the hidden states in the original Transformer.\nThis representation encodes both the context and xzt .\n• The query stream gθ(xz<t ,zt ), or abbreviated as gzt , only\nhas the contextual information xzt and the position zt ,\nwithout any knowledge of the content xzt .\nThe language model is trained to predict each token in the sen-\ntence using only the query stream. The content stream is used\nas input to the query stream. During ﬁne-tuning, the query\nstream is thrown away and the input data is represented with\nthe content stream. Formally, for each self-attention layer\nm = 1,2,..., M, the two streams of representations are\nupdated with shared set of parameters as follows:\ngm\nzt ←Attention(Q =gm−1\nzt ,KV =hm−1\nz<t ;θ),\nhm\nzt ←Attention(Q =hm−1\nzt ,KV =hm−1\nz≤t ;θ),\nwhere Q,K,V denote the query, key, value in an attention\noperation. The update rule of the content stream is same as the\noriginal Transformer self-attention. The query representation\nin the last layer gM\nzt is used to compute (5).\nD. FINE-TUNING FOR CDSC\nGiven a source domain DS with its task TS and a target\ndomain DT with its task TT , the objective of CDSC is to learn\nthe conditional probability distribution PS (YS |XS ) in DS and\nthen apply the learned distribution model on the target source\ndomain DT and with its task TT , where DS ̸=DT . We have\nVOLUME 7, 2019 163223\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nsufﬁcient labeled training data available in DS but no labeled\ndata in the target domain DT .\nWe shall ﬁne-tune the pre-trained Transformer models,\nBERT and XLNet, with a labeled sentiment data from a\nselected source domain and measure its performance in pre-\ndicting the sentiment polarity of other domain’s sentiment\ndata. To measure and compare the effectiveness of BERT and\nXLNet for cross-domain sentiment classiﬁcation, on top of\nthe pre-trained models we will only add one fully connected\nfeed-forward network that consists of two linear transforma-\ntions with GELU activation [31] in between. Given source\ndomain labeled data XS , we calculate the probability distribu-\ntions of input sequences using a softmax activation function.\nf (xi) =GELU(TrM (xi)W1 +b1)W2 +b2\np(yi|xi) = ef (xi)\n∑\nj ef (xj) , (6)\nwhere TrM (xi) is the output from the last Transformer layer\nM of either BERT or XLNet for the input sequence xi ∈XS .\nW1 and W2 are the weights of the ﬁrst and second linear\ntransformations with b1, b2 as bias values. The cost function\nto minimize is the cross-entropy loss as follows:\nL =−\nN∑\ni=1\n(yi log p(yi|xi) +(1 −yi) log(1 −p(yi|xi)), (7)\nwhere N is the total number of samples in the current batch, yi\nis the given label of the input sequence (1 for positive review\nand 0 for negative review) and p(yi|xi) is the probability of the\ninput sequence being positive.\nAfter ﬁne-tune training, we apply the learned models on\nthe target domain and predict the sentiment binary values\nusing softmax function p(yi|xi) with the trained parameters\nwhere xi ∈XT .\nV. EXPERIMENTATION\nA. DATASET\nOur experiments are conducted on the Amazon reviews\ndataset [11] that has been widely used in the literature for\ncross-domain sentiment classiﬁcation. The dataset contains\nreviews from ﬁve product types ( i.e. domains): Books,\nDVD, Electronics, Kitchen and Video. There are 6000 labeled\nreview data for each domain with 3000 positive reviews\n(higher than 3 stars) and 3000 negative reviews (lower than\n3 stars). Following the convention in [18], we construct\n20 cross-domain sentiment classiﬁcation tasks. We ﬁne-tune\non the pre-trained BERT-Large [32] and XLNet-Large [33]\nlanguage models with differing number of labeled data from\nthe selected source domain and test the trained models on the\nother domain data.\nB. PRE-TRAINING\nFor our experiment we use the latest pre-trained cased BERT-\nLarge model, refered to simply as BERT henceforth, with\nnew pre-processing technique called Whole Word Masking\nwhere all of the tokens corresponding to a word are masked\nat once, instead of masking those tokens belonging to a word\nindividually. It has 24 Transformer layers with 4096 hidden\ndimensions, 16 attention heads and a total of 340M param-\neters. For the pre-training, BERT uses the concatenation\nof BookCorpus (800M words) [34] and English Wikipedia\n(2,500M words) as pre-training data. BERT is pre-trained\nwith batch size of 256 sequences with each sequence con-\ntaining maximum of 512 tokens for 1,000,000 steps, which is\napproximately 40 epochs over the 3.3 billions word corpus.\nFor pre-training data, in addition to the BookCorpus and\nEnglish Wikipedia datasets, cased XLNet-Large model, ref-\nered to simply as XLNet henceforth, uses Giga5 (16GB\ntext) [35], ClueWeb 2012-B [36] and Common Crawl [37] as\npart of its pre-training data. ClueWeb2012-B and Common\nCrawl articles are ﬁltered out and after tokenization wih\nSentencePiece [38], the total pre-training data for XLNet\namounts to 32.89B subword pieces, which is an order of\nmagnitude greater than the pre-training data used for BERT.\nXLNet’s architecture has, similar to BERT, 24 Transformer\nlayers with 4096 hidden dimensions and 16 attention heads.\nXLNet is pre-trained with batch size of 2048 and sequence\nlength of 512 for 500,000 steps.\nC. IMPLEMENTATION DETAILS\nFor ﬁne-tune training of the language models, the hidden\ndimensions of the fully connected networks following the last\nlayer of the Transformers is 1024. for cross-domain sentiment\nclassiﬁcation. The dropout probability is kept at 0.1. For\nthe input, the maximum sequence length is set to 256 with\nbatch size of 32. The learning rate is 2e-5 and optimization\nis done with Adam optimizer. Training and testing of Ten-\nsorFlow implementations of BERT [32] and XLNet [33] are\nperformed separately on a single Google Cloud TPU v2 and\nthe total experiment time was over 400 hours for each TPU.\nFor comparison with other state-of-the-arts CDSC meth-\nods, the BERT and XLNet models are trained on 6000 labeled\ndata from a source domain for 3000 steps and evaluate\nthe prediction accuracy on all 6000 data of the remaining\ndomains.\nIn addition, to show BERT and XLNet’s effectiveness in\nlow resource transfer learning scenarios, we train the models\non different amount of source domain labeled data and test\neach trained model on all of the other domains. We com-\npare the runtimes of these two language models in the same\nconﬁguration scenarios with varying number of steps for the\ntraining phase and also with different number of samples for\nthe testing phase.\nD. PERFORMANCE COMPARISON\nThe baseline methods included in the comparison are\nfollowing:\n• DAmSDA [19]: an adversarial network based domain\nadaptation method that utilizes representations encoded\nin a 30,000-dimensional feature vector.\n163224 VOLUME 7, 2019\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nTABLE 1. The CDSC accuracy of the state-of-the-arts methods on the Amazon reviews dataset.\n• CNN-aux [12]: a CNN model based on the approach\nproposed by Kim et al. [39]. It jointly trains the\ncross-domain sentence embedding and the sentiment\nclassiﬁer.\n• AMN [40]: an adversarial network based method that\nlearns domain-shared representations based on memory\nnetworks and adversarial training.\n• HATN[41]: an attention network with hierarchical posi-\ntional encoding that focuses on both the word and sen-\ntence level sentiments.\n• HANP [42]: a hierarchical attention network than can\nobtain both domain independent and domain speciﬁc\nfeatures at the same time by adding prior knowledge.\n• BERT: the proposed ﬁne-tuned auto-encoding bidirec-\ntional contextualized language model pre-trained on\nMasked language modeling and the Next sentence pre-\ndiction tasks. Its architecture is based on the standard\nTransformer model.\n• XLNet: the proposed ﬁne-tuned auto-regressive bidi-\nrectional contextualized language model pre-trained on\nPermutation language modeling task. Its architecture is\nbased on Transformer-XL model and has two-stream\nself-attention mechanism.\nWe use classiﬁcation accuracy as our performance metrics,\nwhich is deﬁned as follows:\nAccuracy =Number of correct predictions\nTotal number of predictions .\nTable 1 shows the classiﬁcation accuracy of various state-\nof-the-arts methods in comparison to the bidirectional con-\ntextualized language models on the cross-domain sentiment\nclassiﬁcation task. For BERT and XLNet, we report the\nmean accuracy rate from 10 separate runs using all of the\n6000 labeled data available in the source domain. It can be\nobserved that the bidirectional contextualized Transformer\nlanguage models of BERT and XLNet greatly outperforms\nthe previous state-of-the-arts methods. BERT outperforms\nprevious state-of-the-arts methods by at least 2% accuracy.\nHowever, XLNet produces results that further improves the\nCDSC accuracy by 2.5% in comparison to BERT. XLNet is\nthe only method where all of the prediction accuracy rates are\nwell above above 90%.\nThe most interesting results are observed in Fig. 2 and\nTable 2. For BERT and XLNet, we report the mean boot-\nstrapped results from predicting four target domain data with\n95% conﬁdence interval from 40 observations where source\ndomain labeled data are selected randomly with replacement.\nBERT outperforms the previous SOTA methods using around\n300 samples or around 20 times less data. XLNet outperforms\nprevious state-of-the-arts methods after ﬁne-tuning only with\n50 source domain training samples, i.e., around 120 times\nless data than the previous SOTA methods. These results\nproves that pre-trained Transformer language models are very\nadaptive at capturing context with only few samples and are\nhighly suitable for transfer learning. Also it can be observed\nthat XLNet is much more efﬁcient at capturing contextualized\nVOLUME 7, 2019 163225\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nFIGURE 2. CDSC accuracy of BERT and XLNet on Amazon review dataset, fine-tuned on different amounts of labeled data from\nthe source domain and tested on all available labeled data of the target domain.\nrepresentations than BERT that it can ﬁne-tune its pre-\ntrained parameters to very quickly pivot towards captur-\ning sentiment polarity in the given sequences. This higher\nefﬁciency performance is due to the combination of differ-\nent pre-training objective function, ability to capture depen-\ndencies longer than the sequence length and the larger\n163226 VOLUME 7, 2019\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nTABLE 2. CDSC accuracy rates of BERT and XLNet on Amazon review dataset with the corresponding margins of error.\npre-training datasets. Table 2 shows the CDSC accuracy rates\nwith the corresponding margins of error.\nIn Table 3 and In Table 4, we compare the runtimes\nof the two models during ﬁne-tune training and testing.\nThe reported results are the mean duration times from\n10 separate runs for each training step size and test data\nsize. The test data are identical for both models and are\nrandomly selected with replacement. We can see that XLNet\nVOLUME 7, 2019 163227\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nTABLE 3. Comparison of running time during fine-tuning.\nTABLE 4. Comparison of running time during testing.\nis more efﬁcient than BERT during testing, on average around\n10% less time spent on testing. However, XLNet has shown\nto be much more resource-hungry when it comes to train-\ning. In our case where the main SOTA results are reported\nfrom 3000 training steps, XLNet is almost 20% slower than\nBERT. XLNet’s runtime is higher than BERT in training due\nto its segment recurrence mechanism for capturing context\ndependencies in documents longer than the maximum input\nsequence length. However, during testing, this segment recur-\nrence mechanism actually decreases the runtime for XLNet\nto be less than BERT’s because the representations from the\nprevious segments can be reused instead of being computed\nfrom scratch as in the case of the standard Transformer.\nFIGURE 3. BERT’s accuracy rate fluctuation over longer fine-tune training\nsteps.\nIn Fig. 3 and 4, we evaluate the effect of different number of\ntraining steps (300, 1000, 3000, 9000, 30000) on the CDSC\naccuracy rate. BERT and XLNet models are ﬁne-tuned on\nvarying amounts of labeled data (60, 600, 6000) from a source\ndomain (’Books’) and tested on all 6000 data of a target\ndomain (’Video’). The results are the mean accuracy rate\nchange over 10 separate runs for each step size and ﬁne-\ntune training data size. We observe that in general and at\nleast in the context of CDSC, there is a noticeable trade-off\nFIGURE 4. XLNet’s accuracy rate fluctuation over longer fine-tune training\nsteps.\nbetween amount of training data and training step size. For\nboth models ﬁne-tuned with only few labeled data, e.g., 60,\nthe accuracy rate drops off immediately when trained for\nlonger than the baseline 300 steps, meaning it overﬁts the\nsource domain. For XLNet, there is recognizable decrease\nin performance after 1000 training steps for all models.\nWe believe that XLNet captures the necessary contextual\ndependencies earlier in the training steps, when compared to\nBERT, and longer it trains, the parameters more overﬁt the\nsource domain. Therefore even though XLNet runs slower\nthan BERT, it learns more quickly with fewer training steps.\nFIGURE 5. BERT (inner ring) and XLNet’s (outer ring) domain\ntransferability of a given domain (inner circle).\nIn Fig. 5, we show the transferability, i.e., ease of trans-\nfer learning, among the ﬁve domains from 640 separate\nexperimental observations for each domain. Upper half of\na doughnut graph indicates for a given target domain (inner\ncircle), which domains were the most suitable source domain.\nContrarily, the lower half indicates which domains were the\nleast suitable. Suitability is measured with CDSC accuracy\nrates, i.e., higher the rate, more suitable the source domain\nwas for the given target. Inner ring displays the results\nobtained with BERT and outer ring contains the results of\nXLNet. For example: for ’Books’ domain, ’DVD’ is the\nmost suitable source and ’Electronics’ is the least suitable.\n’Kitchen’ and ’Electronics’ have high level of transferability,\ndue to their contents being the most similar. For ’Electronics’,\n163228 VOLUME 7, 2019\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\nﬁne-tuned BERT indicates ’Video’ and ’DVD’ are equally\nleast suitable, but XLNet overwhelmingly points towards\n’Video’ as the least suitable domain.\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, we apply the bidirectional contextualized\nTransformer language models of BERT and XLNet on cross-\ndomain sentiment classiﬁcation task. Due to their unsuper-\nvised pre-training tasks utilizing large unlabeled datasets\nand their self-attention Transformer mechanisms, BERT\nand XLNet both greatly outperforms the previous state-of-\nthe-arts methods for CDSC task. When compared closely,\nXLNet outperforms BERT on all CDSC tasks. XLNet is\nis very efﬁcient in capturing context and achieves state-of-\nthe-arts results with only using 50 ﬁne-tune training sam-\nples, i.e., around 120 times fewer data than the previous\nhigh-performing CDSC methods trained on. XLNet’s better\nprediction accuracy is mostly due to its novel pre-training\nobjective, ability to capture long-term dependencies, and\nlarger pre-training dataset. XLNet is more resource-hungry\nthan BERT, but learns contextual data much quicker than\nBERT with fewer ﬁne-tuning steps. For future, it is interesting\nto explore how, or whether, BERT’s performance improves if\npre-trained on the similar amount of data as in XLNet. Also\nmaking both models lighter and more resource-efﬁcient can\nbe an interesting area to explore.\nREFERENCES\n[1] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, ‘‘Emotional chatting\nmachine: Emotional conversation generation with internal and external\nmemory,’’ in Proc. 32nd AAAI , 2018, pp. 1–9.\n[2] B. Liu, ‘‘Sentiment analysis and opinion mining,’’ Synth. Lect. Hum. Lang.\nTechnol., vol. 5, no. 1, pp. 1–167, 2012.\n[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Distributed\nrepresentations of words and phrases and their compositionality,’’ in Proc.\nAdv. Neural Inf. Process. Syst. , 2013, pp. 3111–3119.\n[4] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. EMNLP, 2014, pp. 1532–1543.\n[5] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classiﬁcation\nwith deep convolutional neural networks,’’ in Proc. Adv. Neural Inf. Pro-\ncess. Syst., 2012, pp. 1097–1105.\n[6] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and\nC. Potts, ‘‘Recursive deep models for semantic compositionality over a\nsentiment treebank,’’ in Proc. EMNLP, 2013, pp. 1631–1642.\n[7] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[8] Q. Li, ‘‘Literature survey: Domain adaptation algorithms for nat-\nural language processing,’’ Dept. Comput. Sci., Graduate Center,\nCity Univ. New York, New York, NY , USA, Tech. Rep., 2012,\npp. 8–10.\n[9] Y . Goldberg, ‘‘Neural network methods for natural language process-\ning,’’ Synth. Lect. Hum. Lang. Technol. , vol. 10, no. 1, pp. 1–309,\n2017.\n[10] X. Wan, ‘‘Co-training for cross-lingual sentiment classiﬁcation,’’ in Proc.\nJoint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang.\nProcess. (AFNLP), 2009, pp. 235–243.\n[11] J. Blitzer, M. Dredze, and F. Pereira, ‘‘Domain adaptation for sentiment\nclassiﬁcation,’’ in Proc. ACL, 2007, pp. 440–447.\n[12] J. Yu and J. Jiang, ‘‘Learning sentence embeddings with auxiliary\ntasks for cross-domain sentiment classiﬁcation,’’ in Proc. EMNLP, 2016,\npp. 236–246.\n[13] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365. [Online]. Available: https://arxiv.org/abs/1802.05365\n[14] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. (2018). Improv-\ning Language Understanding by Generative Pre-Training . [Online].\nAvailable: https://s3-us-west-2.amazonaws.com/openai-assets/research-\ncovers/language-unsupervised/language_understanding_paper.pdf\n[15] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: https://arxiv.org/abs/1810.04805\n[16] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pretraining for language under-\nstanding,’’ 2019, arXiv:1906.08237. [Online]. Available: https://arxiv.\norg/abs/1906.08237\n[17] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n‘‘Transformer-xl: Attentive language models beyond a ﬁxed-length con-\ntext,’’ 2019, arXiv:1901.02860. [Online]. Available: https://arxiv.org/abs/\n1901.02860\n[18] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, ‘‘Cross-domain sentiment\nclassiﬁcation via spectral feature alignment,’’ in Proc. 19th Int. Conf.\nWWW, 2010, pp. 751–760.\n[19] Y . Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,\nM. Marchand, and V . Lempitsky, ‘‘Domain-adversarial training of neural\nnetworks,’’J. Mach. Learn. Res. , vol. 17, no. 59, pp. 1–35, 2016.\n[20] B. Sun, J. Feng, and K. Saenko, ‘‘Return of frustratingly easy domain\nadaptation,’’ in Proc. 30th AAAI , 2016, pp. 1–8.\n[21] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschlger, and\nS. Saminger-Platz, ‘‘Central moment discrepancy (CMD) for domain-\ninvariant representation learning,’’ 2017, arXiv:1702.08811. [Online].\nAvailable: https://arxiv.org/abs/1702.08811\n[22] C. Chen, Z. Chen, B. Jiang, and X. Jin, ‘‘Joint domain alignment and\ndiscriminative feature learning for unsupervised deep domain adaptation,’’\nin Proc. 33rd AAAI , 2019, pp. 3296–3303.\n[23] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and\nT. Darrell, ‘‘DeCAF: A deep convolutional activation feature for generic\nvisual recognition,’’ in Proc. ICML, 2014, pp. 1–9.\n[24] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, ‘‘CNN features\noff-the-shelf: An astounding baseline for recognition,’’ in Proc. IEEE Conf.\nCVPR Workshops, Jun. 2014, pp. 806–813.\n[25] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning\nfor text classiﬁcation,’’ 2018, arXiv:1801.06146. [Online]. Available:\nhttps://arxiv.org/abs/1801.06146\n[26] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio, ‘‘Empirical evalua-\ntion of gated recurrent neural networks on sequence modeling,’’ 2014,\narXiv:1412.3555. [Online]. Available: https://arxiv.org/abs/1412.3555\n[27] Y . Kim, C. Denton, L. Hoang, and A. M. Rush, ‘‘Structured attention\nnetworks,’’ in Proc. ICLR, 2017, pp. 1–21.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst. , 2017, pp. 5998–6008.\n[29] Y . Wu et al. , ‘‘Google’s neural machine translation system: Bridging the\ngap between human and machine translation,’’ 2016, arXiv:1609.08144.\n[Online]. Available: https://arxiv.org/abs/1609.08144\n[30] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‘‘Layer normalization,’’\n2016, arXiv:1607.06450. [Online]. Available: https://arxiv.org/abs/\n1607.06450\n[31] D. Hendrycks and K. Gimpel, ‘‘Gaussian error linear units (GELUs),’’\n2016, arXiv:1606.08415. [Online]. Available: https://arxiv.org/abs/1606.\n08415\n[32] TensorFlow Code and Pre-Trained Models for BERT. Accessed: Jun. 2019.\n[Online]. Available: https://github.com/google-research/bert\n[33] XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. Accessed: Jun. 2019. [Online]. Available:\nhttps://github.com/zihangdai/xlnet\n[34] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler, ‘‘Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,’’ in Proc. ICCV ,\nDec. 2015, pp. 19–27.\n[35] R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda, ‘‘English giga-\nword ﬁfth edition,’’ Linguistic Data Consortium, Philadelphia, PA, USA,\nTech. Rep., 2011.\n[36] J. Callan, M. Hoy, C. Yoo, and L. Zhao. (2009). The ClueWeb09 Dataset .\n[Online]. Available: https://lemurproject.org/clueweb09.php/\n[37] Common Crawl. Common Crawl Corpus . Accessed: Jun. 2019. [Online].\nAvailable: https://commoncrawl.org/the-data/\nVOLUME 7, 2019 163229\nB. Myagmaret al.: Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models\n[38] T. Kudo and J. Richardson, ‘‘SentencePiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text process-\ning,’’ 2018, arXiv:1808.06226. [Online]. Available: https://arxiv.org/abs/\n1808.06226\n[39] Y . Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’\n2014, arXiv:1408.5882. [Online]. Available: https://arxiv.org/abs/1408.\n5882\n[40] Z. Li, Y . Zhang, Y . Wei, Y . Wu, and Q. Yang, ‘‘End-to-end adversarial\nmemory network for cross-domain sentiment classiﬁcation,’’ in Proc.\nIJCAI, 2017, pp. 2237–2243.\n[41] Z. Li, Y . Wei, Y . Zhang, and Q. Yang, ‘‘Hierarchical attention transfer\nnetwork for cross-domain sentiment classiﬁcation,’’ in Proc. 32nd AAAI ,\n2018, pp. 1–8.\n[42] T. Manshu and W. Bing, ‘‘Adding prior knowledge in hierarchical attention\nneural network for cross domain sentiment classiﬁcation,’’ IEEE Access ,\nvol. 7, pp. 32578–32588, 2019.\nBATSERGELEN MYAGMAR received the B.S.\ndegree in computer science from the Ramapo\nCollege of New Jersey, in 2006, and the M.S.\ndegree in computer engineering from the Uni-\nversity of Florida, in 2010. He is currently pur-\nsuing the Ph.D. degree with the Department of\nComputer Science, University of Tsukuba, Japan.\nHis research interests include machine learning,\ntransfer learning, domain adaptation, and natural\nlanguage processing.\nJIE LIreceived the B.Eng. degree in computer sci-\nence from Zhejiang University, Hangzhou, China,\nthe M.Eng. degree in electronic engineering and\ncommunication systems from the China Academy\nof Posts and Telecommunications, Beijing, China,\nand the Dr.Eng. degree from the University of\nElectro-Communications, Tokyo, Japan. He is\ncurrently with Department of Computer Science\nand Engineering, Shanghai Jiaotong University,\nShanghai, China, where he is also a Chair Pro-\nfessor. He was a full Professor with the Department of Computer Science,\nUniversity of Tsukuba, Japan. He was a Visiting Professor with Yale Univer-\nsity, USA, Inria Sophia Antipolis, and Inria Grenoble-Rhone-Alpes, France.\nHis current research interests include big data, cloud computing, machine\nlearning and networking, network security, OS, and modeling and perfor-\nmance evaluation of information systems. He has also served on the program\ncommittees for several international conferences. He is the Co-Chair of the\nIEEE Technical Community on Big Data and the IEEE Big Data Community,\nand the Founding Chair of the IEEE ComSoc Technical Committee on\nBig Data. He serves as an Associate Editor for many IEEE journals and\nTRANSACTIONS.\nSHIGETOMO KIMURA received the B.Eng.,\nM.Eng., and Dr.Info.Sc. degrees from Tohoku Uni-\nversity, Japan. He is an Associate Professor with\nthe Faculty of Engineering, Information and Sys-\ntems, University of Tsukuba. His research interests\ninclude network protocols and performance evalu-\nation of communication systems. He is a member\nof the ACM, IPSJ, IEICE, and JSSST.\n163230 VOLUME 7, 2019",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8605639934539795
    },
    {
      "name": "Sentiment analysis",
      "score": 0.7251715660095215
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6785703301429749
    },
    {
      "name": "Natural language processing",
      "score": 0.6238131523132324
    },
    {
      "name": "Transformer",
      "score": 0.5935268998146057
    },
    {
      "name": "Feature learning",
      "score": 0.5267485976219177
    },
    {
      "name": "Language model",
      "score": 0.4998507499694824
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.49347984790802
    },
    {
      "name": "Transfer of learning",
      "score": 0.489298552274704
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.47804099321365356
    },
    {
      "name": "Representation (politics)",
      "score": 0.44583016633987427
    },
    {
      "name": "Machine learning",
      "score": 0.31500542163848877
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I146399215",
      "name": "University of Tsukuba",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ],
  "cited_by": 54
}