{
  "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
  "url": "https://openalex.org/W4389520349",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099884982",
      "name": "Hoang Nguyen",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2110002789",
      "name": "Ye Liu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2228299633",
      "name": "Chenwei Zhang",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1973217115",
      "name": "Tao Zhang",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2147553045",
      "name": "Philip Yu",
      "affiliations": [
        "University of Illinois Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4311917449",
    "https://openalex.org/W2963033987",
    "https://openalex.org/W3035589854",
    "https://openalex.org/W3014333092",
    "https://openalex.org/W4385569882",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287887682",
    "https://openalex.org/W3154449322",
    "https://openalex.org/W3099757670",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W3173854146",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W3100460087",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4389010536",
    "https://openalex.org/W4385569981",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385572438",
    "https://openalex.org/W3102573502",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4224212267",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4285228888",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2952781527",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2963042492"
  ],
  "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12109–12119\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCoF-CoT: Enhancing Large Language Models with Coarse-to-Fine\nChain-of-Thought Prompting for Multi-domain NLU Tasks\nHoang H. Nguyen1, Ye Liu2, Chenwei Zhang3, Tao Zhang1, Philip S. Yu1\n1 Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA\n2 Salesforce Research, Palo Alto, CA, USA\n3 Amazon, Seattle, W A, USA\n{hnguy7,tzhang90,psyu}@uic.edu, yeliu@salesforce.com, cwzhang@amazon.com\nAbstract\nWhile Chain-of-Thought prompting is popu-\nlar in reasoning tasks, its application to Large\nLanguage Models (LLMs) in Natural Language\nUnderstanding (NLU) is under-explored. Moti-\nvated by multi-step reasoning of LLMs, we pro-\npose Coarse-to-Fine Chain-of-Thought (CoF-\nCoT) approach that breaks down NLU tasks\ninto multiple reasoning steps where LLMs can\nlearn to acquire and leverage essential con-\ncepts to solve tasks from different granularities.\nMoreover, we propose leveraging semantic-\nbased Abstract Meaning Representation (AMR)\nstructured knowledge as an intermediate step\nto capture the nuances and diverse structures\nof utterances, and to understand connections\nbetween their varying levels of granularity. Our\nproposed approach is demonstrated effective in\nassisting the LLMs adapt to the multi-grained\nNLU tasks under both zero-shot and few-shot\nmulti-domain settings 1.\n1 Introduction\nNatural Language Understanding (NLU) of Dia-\nlogue systems encompasses tasks from different\ngranularities. Specifically, while intent detection\nrequires understanding of coarse-grained sentence-\nlevel semantics, slot filling requires fine-grained\ntoken-level understanding. Moreover, Semantic\nParsing entails the comprehension of connections\nbetween both token-level and sentence-level tasks.\nLarge Language Models (LLMs) possess logi-\ncal reasoning capability and have yielded excep-\ntional performance (Zoph et al., 2022; Zhao et al.,\n2023b). However, they remain mostly restricted\nto reasoning tasks. On the other hand, mutli-\nstep reasoning can take place when solving multi-\nple interconnected tasks in a sequential order. In\npractical NLU systems, as coarse-grained tasks\nare less challenging, they can be solved first be-\nfore proceeding to fine-grained tasks. Therefore,\nthe coarse-grained tasks’ outcomes can provide\n1https://github.com/nhhoang96/CoF-CoT\n  \n Show me dates for music festivals in 2018\n(c/ show-01\n    :ARG1 (m /me)\n    :ARG2 (d/ date-entity\n       :mod (f/ music-festival)\n        :year 2018))\nPlease display to me the dates for music festivals that occurred in 2018\nshow-01\nm/me d/date-entity\nf/music-festival 2018\n:ARG1 :ARG2\n:mod :year\n Intent (Coarse-grained): \nGET_EVENT\nSlot (Fine-grained):\nCATEGORY_EVENT music festivals\nDATE_TIME in 2018\nFigure 1: Illustration of Abstract Meaning Represen-\ntation (AMR)of two structurally different but semanti-\ncally similar utterances with the same fine-grained and\ncoarse-grained labels. Each colored node represents\nan AMR concept matching the colored word or phrase\nexistent in the corresponding utterances.\nvaluable guidance towards subsequent fine-grained\ntasks, allowing for deeper semantic understand-\ning of diverse utterances across different domains\nwithin NLU systems (Firdaus et al., 2019; Weld\net al., 2022; Nguyen et al., 2023a). For instance,\nconsider the utterance “Remind John the meeting\ntime at 8am” under reminder domain, recogniz-\ning GET_REMINDER_DATE_TIME intent is\ncrucial for correctly understanding the existence\nof PERSON_REMINDED slot type rather than\nCONTACTor ATTENDEE slot type.\nChain-of-Thought (CoT) (Wei et al., 2022) pro-\nvides an intuitive approach to elicit multi-step rea-\nsoning from LLMs automatically. However, there\nremain two major challenges with the current CoT\napproach: (1) LLMs entirely rely on their uncon-\ntrollable pre-trained knowledge to generate step-\nby-step reasoning and could result in unexpected\nhallucinations (Yao et al., 2022; Zhao et al., 2023a),\n(2) Additional beneficial structured knowledge can-\nnot be injected into LLMs via the current CoT.\nOn the other hand, structured representation\ndemonstrates the effectiveness in enhancing the\ncapability of Pre-trained Language Models (PLMs)\n(Xu et al., 2021; Bai et al., 2021; Shou et al., 2022).\nIn Dialogue systems, the dependencies among dif-\nferent dialogue elements together with the existent\ndiversely structured utterances necessitate the inte-\n12109\n  \nStep 1: Generate AMR\nInput: Sentence, Domain\nOutput: AMR\nStep 2: Generate Intent\nInput: Sentence, Intent Vocabulary, AMR, Domain\nOutput: Intent\nStep 3: Generate Slot Values\nInput: Sentence, AMR, Intent, Domain\nOutput: Slot Values\nStep 4: Generate (Slot Value, Slot Type) pairs\nInput: Sentence, AMR, Slot vocabulary, \nSlot Values, Intent, Domain\nOutput: (Slot Value, Slot Type) pairs\nStep 5: Generate Logic Form\nInput: Sentence, Intent, Domain,\n(Slot Value, Slot Type) pairs\nOutput: Logic Form\n Show me dates for music festivals in 2018\n[IN:GET_EVENT \n[SL:CATEGORY_EVENT music festivals ] \n[SL:DATE_TIME in 2018 ] ]\n(c/ show-01\n    :ARG1 (m /me)\n    :ARG2 (d/ date-entity\n       :mod (f/ music-festival)\n        :year 2018))\nGET_EVENT\nShow me, dates, music festival, in 2018 \n(Show me, O),(dates, O), \n(music festival, CATEGORY_EVENT), \n(in 2018, DATE_TIME) \n \n[IN:GET_DATE_TIME_EVENT\n [SL:TITLE_EVENT music festivals] \n[SL:DATE_TIME 2018]]\nFigure 2: Illustration of CoF-CoT and its counterpart Direct Promptapproach. The left side illustrates the proposed\nCoF-CoT. The right side illustrates the naive Direct Prompt approach. Red and Green represent sentence-level and\ntoken-level annotations captured in the Logic Form respectively. For CoF-CoT, the prompt at each step starting\nfrom Step 2 is conditioned on the relevant output predicted from the previous step(s).\ngration of additional structured representation. For\ninstance, as observed in Figure 1, by leveraging Ab-\nstract Meaning Representation (AMR) (Banarescu\net al., 2013), it is possible to map multiple seman-\ntically similar but structurally different utterances\nwith similar coarse-grained and fine-grained labels\ninto the same structured representation, allowing\nfor effective extraction of intents, slots, and their\ninterconnections within the Dialogue systems.\nIn our work, we explore the capability of LLMs\nin NLU tasks from various granularities, namely\nmulti-grained NLU tasks. Motivated by CoT, we\npropose an adaptation of CoT in solving multi-\ngrained NLU tasks with an integration of structured\nknowledge from AMR Graph. Our contribution\ncan be summarized as follows:\n•To the best of our knowledge, we conduct\nthe first preliminary study of LLMs’ capability in\nmulti-grained NLU tasks of the Dialogue systems.\n•We propose leveraging a CoT-based approach\nto solve multi-grained NLU tasks in a coarse-to-\nfine-grained sequential reasoning order.\n•We propose integrating structured knowledge\nrepresented via AMR Graph in the multi-step rea-\nsoning to capture the shared semantics across di-\nverse utterances within the Dialogue systems.\n2 Related Work\nChain-of-Thought (CoT) CoT (Wei et al., 2022)\nproposes leveraging intermediate steps to extracts\nlogical reasoning of LLMs and succeeds in vari-\nous reasoning tasks. Wang et al. (2022) enhances\nCoT by selecting the most consistent output an-\nswers via majority voting. Additionally, Fu et al.\n(2022) argues majority consistency voting works\nbest among the most complex outputs. They pro-\npose complexity metrics and leverage them to se-\nlect demonstration samples and decoding outputs.\nUnlike previous CoT approaches, we leverage CoT\nto solve multi-grained NLU tasks.\nStructured Representation Structured Repre-\nsentation has been widely incorporated in language\nmodels to further enhance the capability across var-\nious NLP tasks (Bugliarello and Okazaki, 2020;\nZhang et al., 2020). Structured representation can\nbe either in the syntax-based structure (Bai et al.,\n2021; Xu et al., 2021) such as Dependency Parsing\n(DP) Graph, Constituency Parsing (CP) Graph or\nsemantic-based structure (Shou et al., 2022) such\nas Abstract Meaning Representation (AMR) Graph\n(Banarescu et al., 2013). Unlike previous works,\nwe aim at leveraging structured representation as\nan intermediate step in the multi-step reasoning\napproach to extract essential concepts from diverse\nutterances in the multi-domain Dialogue systems.\n3 Proposed Framework\nIn this section, we introduce our proposed Coarse-\nto-Fine Chain-of-thought (CoF-CoT) approach for\nNLU tasks as depicted in Figure 2. Specifically, we\npropose a breakdown of multi-grained NLU tasks\ninto 5 sequential steps from coarse-grained to fine-\ngrained tasks. At each step, LLMs leverage the\ninformation from the previous steps as a guidance\ntowards the current predictions. As domain name\ncould provide guidance to NLU tasks (Xie et al.,\n2022; Zhou et al., 2023), at each step, we condition\nthe domain name of the given utterance in the input\nprompt. The model’s output is in the format of\nLogic Form (Kamath and Das) which encapsulates\n12110\nTable 1: Experimental results on MTOP and MASSIVE under zero-shot and few-shot multi-domain settings.\nMTOP\nModel Zero-shot Few-shot\nNLU Semantic Parsing NLU Semantic Parsing\nIntent Acc Slot F1 Frame Acc Exact Match Intent Acc Slot F1 Frame Acc Exact Match\nDirect Prompt 31.50±1.80 21.84±2.83 8.33±1.44 6.00±1.32 51.33±3.40 28.35±3.24 11.00±1.80 8.33±1.00\nCoT 31.83 ±2.02 22.40±1.61 8.67±0.35 6.33±1.04 47.67±5.20 28.46±3.10 11.83±1.53 8.50±1.04\nSC-CoT 32.50 ±1.89 22.71±2.44 10.05±0.87 6.83±0.76 53.50±3.04 29.53±1.99 12.50±1.80 9.00±0.87\nComplexCoT 32.67±2.00 22.86±3.17 10.83±0.29 7.16±0.58 48.83±2.47 29.21±2.65 13.17±0.58 8.83±2.89\nLeast-to-Most 45.67±0.58 21.84±1.91 14.50±0.50 8.00±0.50 49.83±4.54 27.28±2.41 16.00±0.50 8.83±0.76\nPlan-and-Solve 45.00±4.00 22.45±2.28 9.50±1.61 8.25±2.25 – – – –\nCoF-CoT 57.67±2.75 23.47±4.09 14.33±1.52 9.00±1.00 61.50±4.93 30.12±3.93 15.00±1.32 11.00±1.61\nMASSIVE\nDirect Prompt 72.50±4.58 33.24±3.34 24.17±3.79 20.67±3.28 75.17±0.58 42.36±2.98 29.00±5.39 24.50±4.07\nCoT 71.83 ±2.57 36.32±1.94 24.50±2.29 21.66±3.40 76.83±3.82 44.89±2.50 31.33±0.87 25.83±2.25\nSC-CoT 73.05 ±1.27 37.06±2.54 27.16±3.21 22.50±2.65 77.33±2.89 47.02±4.60 34.00±3.21 27.16±3.50\nComplexCoT 73.66±3.65 37.64±3.51 25.83±2.25 22.16±2.51 77.83±1.83 46.59±2.43 36.50±2.89 28.00±3.69\nLeast-to-Most 72..83±4.65 37.62±1.69 31.50±1.53 26.50±1.26 77..00±3.28 45.93±3.99 32.50±4.09 29.00±5.11\nPlan-and-Solve 69.33±2.47 38.07±2.07 32.00±1.26 29.00±1.26 – – – –\nCoF-CoT 89.00±2.29 38.66±3.25 33.17±4.04 25.50±2.64 92.00±2.29 47.06±4.63 37.50±1.89 29.50±3.12\ncoarse-grained intent label, fine-grained slot labels\nand slot values. Further details of Logic Form’s\nstructure and its connections with multi-grained\nNLU tasks are provided in the Appendix A.\nOur multi-step reasoning is designed in the fol-\nlowing sequential order:\n1. Generate AMR: Given the input utterance,\nLLMs generate the AMR structured representation\n(Banarescu et al., 2013). The representation is pre-\nserved in the Neo-Davidsonian format as demon-\nstrated in Figure 1,2. Each node in AMR graph\nrefers to a concept, including entity, noun phrase,\npre-defined frameset or special keyword. Edges\nconnecting two nodes represent the relation types.\n2. Generate Intent: In this step, LLMs generate\ncoarse-grained intent label prediction when con-\nditioned on the given input and its corresponding\nAMR Graph. AMR concepts could provide addi-\ntional contexts to ambiguous utterances, leading to\nimproved ability to recognize the correct intents.\n3. Generate Slot Values: In this stage, to gen-\nerate the fine-grained slot values existent in the in-\nput utterance, besides the utterance itself, prompts\nfor LLMs are conditioned on the generated AMR\nstructure and predicted intent label. As AMR graph\ncaptures the essential concepts existent in the ut-\nterance while abstracting away syntactic idiosyn-\ncrasies of the utterance, it can help extract the im-\nportant concepts mentioned in the utterances. In\norder to further couple the connections between\nslots and intents (Zhang et al., 2019; Wu et al.,\n2020), predicted intents from the Step 2 are also\nconcatenated to construct input prompts for Step 3.\n4. Generate Slot Value, Slot Type pairs: After\nobtaining slot values, LLMs label each identified\nslot value when given the slot vocabulary. Simi-\nlar to Step 3, we condition the generated output\nwith the predictions from previous steps, including\nAMR and intent. Both AMR and intent provide\nadditional contexts for slot type predictions of the\ngiven slot values besides the input utterance.\n5. Generate Logic Form: The last step involves\naggregating the predicted intents together with se-\nquences of slot type and slot value pairs to construct\nthe final Logic Form predictions.\n4 Experiments\n4.1 Datasets & Preprocessing\nWe evaluate our proposed framework on two multi-\ndomain NLU datasets, namely MTOP (Li et al.,\n2021) and MASSIVE (Bastianelli et al., 2020;\nFitzGerald et al., 2022). As the innate capability of\nlanguage understanding is best represented via the\nrobustness across different domains, we evaluate\nthe frameworks under low-resource multi-domain\nsettings, including zero-shot and few-shot. Details\nof both datasets are provided in Appendix B.\nTo provide a comprehensive evaluation for\ncoarse-grained, fine-grained NLU tasks, as well\nas the interactions between the two, we conduct an\nextensive study on both NLU and Semantic Pars-\ning metrics, including: Slot F1-score, Intent Accu-\nracy, Frame Accuracy, Exact Match. Intent Accu-\nracy assesses the performance on coarse-grained\nsentence-level tasks, while Slot F1 metric evaluates\nthe performance on more fine-grained token-level\ntasks. The computation of Frame Accuracy and\nExact Match captures the ability to establish the\naccurate connections between sentence-level and\ntoken-level elements. For more details of individ-\nual metric computation from the Logic Form, we\nrefer readers to (Li et al., 2021).\nTo conduct the evaluation with efficient API\n12111\ncalls, following (Khattab et al., 2022), we construct\ntest sets by randomly sampling 200 examples cover-\ning a set of selected domains, namely test domains.\nWe repeat the process with 3 different seeds to gen-\nerate 3 corresponding test sets. Reported perfor-\nmance is the average across 3 different seed test sets\nwith standard deviations. For few-shot settings, we\nrandomly select a fixed k samples from a disjoint\nset of domains, namely train domains. These sam-\nples are manually annotated with individual step\nlabels as commonly conducted by other in-context\nlearning CoT approaches (Wei et al., 2022; Wang\net al., 2022). Additional implementation details as\nwell as the prompt design and sample outputs are\nprovided in Appendix C and D respectively.\n4.2 Baseline\nChain-of-Thought (CoT) Approach Compari-\nson We compare our proposed method with the\ncurrent relevant state-of-the-art CoT approaches:\n•Direct Prompt: Naive prompting to generate the\nLogic Form given the intent and slot vocabulary.\n•CoT (Wei et al., 2022): Automatic generation of\nseries of intermediate reasoning steps from LLMs\n•SC-CoT (Wang et al., 2022): Enhanced CoT via\nmajority voting among multiple reasoning paths.\n•Complex-CoT (Fu et al., 2022) : Enhanced CoT\nby selecting and measuring the consistency of the\nmost complex samples. In our case, we leverage\nthe longest output as the complexity measure.\n•Least-to-Most (Zhou et al., 2022) : Enhanced\nCoT by first automatically decomposing the in-\nhand problems into series of simpler sub-problems,\nand then solving each sub-problem sequentially.\n•Plan-and-Solve (Wang et al., 2023) : Enhanced\nCoT by guiding LLMs to devise the plan before\nsolving the problems by prompting “Let’s first un-\nderstand the problem and devise a plan to solve the\nproblem. Then, let’s carry out the plan and solve\nthe problem step by step. ”\nFine-tuning (FT) Approach Comparison As\none of the early studies in leveraging LLM for\nNLU tasks, we also conduct additional compar-\nisons with traditional FT approaches. Specifically,\nwe leverage RoBERTa PLM (Zhuang et al., 2021)\nwith joint Slot Filling and Intent Detection objec-\ntives (Li et al., 2021) as the FT model. Unlike LLM,\ntraditional FT operates under closed-world assump-\ntion which requires sufficient data to learn domain-\nspecific and domain-agnostic feature extraction in\nmulti-domain settings. For a fair comparison with\nLLM, we impose an essential constraint that there\nTable 2: Comparison between FT and LLM approaches\non MTOP dataset.\nMethod Assumption Intent Acc Slot F1 Frame Acc Exact MatchRoBERTa FT Supervised 67.19±2.90 75.17±1.08 43.57±4.18 36.10±1.08RoBERTa FT ZSL 0 12.68 ±1.25 0 0RoBERTa FT FSL 0 13.75 ±1.22 0 0CoF-CoT ZSL 57.67±2.75 23.47±4.09 14.33±1.52 9.00±1.00CoF-CoT FSL 61.50±4.93 30.12±3.93 15.00±1.32 11.00±1.61\nTable 3: Ablation study on the effectiveness of differ-\nent structured representations on MTOP dataset under\nzero-shot settings. CP , DP , AMRdenote Constituency\nParsing, Dependency Parsing and Abstract Meaning\nRepresentation respectively.Intent Acc Slot F1 Frame Acc Exact MatchCoT (w/o structure) 57.16±3.69 17.50±2.92 12.16±1.61 4.67±3.33CP-CoT 57.33±3.25 19.34±3.34 13.16±1.04 5.50±1.32DP-CoT 57.50±3.01 17.83±2.53 12.67±1.04 5.83±2.08AMR-CoT 57.67±2.75 23.47±4.09 14.33±1.52 9.00±1.00\nexist no overlapping domains between train and\ntest domains under ZSL and FSL setting for both\nFT and CoT approaches. This leads to 3 different\nscenarios for FT approaches, including:\n•Fully Supervised: Samples sharing similar do-\nmains with test sets are used for training.\n•ZSL: We utilize samples from domains different\nfrom test domains for training.\n•FSL: We leverage samples from domains differ-\nent from test domains in conjunction with a fixed\nnumber of k-shot test domain samples.\n5 Result & Discussion\nAs observed in Table 1, our proposed CoF-CoT\nachieves state-of-the-art performance across differ-\nent evaluation metrics on MASSIVE and MTOP\ndatasets under both zero-shot and few-shot settings.\nThe performance gain over the most competitive\nbaseline is more significant in terms of Intent Ac-\ncuracy (25% and 15.34% improvements on MTOP\nand MASSIVE respectively in zero-shot settings).\nAdditional case studies presented in Appendix E\nfurther demonstrate the effectiveness of CoF-CoT.\nIn addition, we observe consistent improvements\nof different CoT variants over the Direct Prompt.\nIt implies that CoT prompting allows the model\nto reason over multiple steps and learn the connec-\ntions between different NLU tasks more effectively.\nIn comparison with MASSIVE, performance of\nall methods is significantly lower on MTOP. It is\nmainly due to the more complex Logic Form struc-\ntures existent in MTOP. It is noticeable that MAS-\nSIVE datasets contain samples of fewer average\nnumber of slots, leading to significantly better per-\nformance on Semantic Parsing tasks (i.e. Frame\nAccuracy and Exact Match).\nOur CoF-CoT shares certain degrees of similari-\nties with Least-to-Most (Zhou et al., 2022), Plan-\nand-Solve prompting (Wang et al., 2023). How-\n12112\nTable 4: Ablation study of step ordering on MASSIVE dataset. CoF and FoC denote Coarse-to-Fine-grained and\nFine-to-Coarse-grained order respectively.\nMethod Assumption Intent Acc Slot F1 Frame Acc Exact Match\nRandom-CoT Random Order 80.67 ±3.60 27.14 ±2.47 26.50 ±1.80 16.50 ±1.04\nFoC-CoT FoC order 83.00 ±2.88 32.11±2.50 28.50±3.21 18.00±3.50\nCoF-CoT (w/o step 1) No AMR 81.50 ±4.36 33.68±2.40 27.50±2.65 18.00±0.76\nCoF-CoT (w/o step 2) No intent 78.17 ±4.80 27.66±1.93 23.50±2.78 14.50±2.25\nCoF-CoT (w/o step 3) No separate KP 82.33 ±1.04 34.63±3.10 32.83±2.47 23.00±1.80\nCoF-CoT (w/o step 4) No separate slot prediction for KP 79.17±4.01 32.92±5.02 31.50±3.50 21.83±3.17\nCoF-CoT (w/o step 3+4) No separate slot prediction 81.33±4.19 31.31 ±3.77 27.67 ±5.34 21.00±4.92\nCoF-CoT CoF order (Full) 89.00 ±2.29 38.66±3.25 33.17±4.04 25.50±2.64\n- Conditioning No domain 84.50 ±2.75 36.80±2.08 32.50±1.73 24.83±0.58\never, unlike the two aforementioned baselines that\nrely heavily on the existent pre-trained knowledge\nof LLMs, CoF-CoT provides a controllable num-\nber of sequential steps and conditioning inputs for\neach step, allowing for flexible adaptations and cus-\ntomizations to future downstream tasks that LLMs\nmight not be familiar with.\nComparison with FT Under ZSL and FSL set-\ntings, the FT model suffers from the aforemen-\ntioned domain gap issues. Specifically, as observed\nin Table 2, since there exist minimal overlapping\nintent labels between train and test domains, with-\nout sufficient data in ZSL and FSL settings, the FT\napproaches are unable to learn transferable multi-\ndomain features, leading to 0 performance in Intent\nAccuracy. This behavior also results in 0 perfor-\nmance for both Frame Acc and Exact Match as\nthe correct intents are the prerequisites for correct\nsemantic frame and exact match metrics. On the\nother hand, Fully supervised FT approach acquires\ndomain-specific knowledge of target domains from\ntraining data and performs the best across differ-\nent evaluation metrics. However, this assumption\ndoes not directly match ZSL/FSL settings in which\nLLMs are currently evaluated.\nImpact of Structured RepresentationBesides\nAMR Graph, there exist other structured represen-\ntations that directly link to semantic and syntactic\nunderstanding of utterances, including DP,CP. Our\nempirical study presented in Table 3 reveals that\nAMR-CoT unanimously achieves the best perfor-\nmance, demonstrating its effectiveness in capturing\nthe diversity of input utterances when compared\nwith other structured representations.\nImpact of Step Order To further understand\nthe importance of the designed CoF step or-\nder, we conduct additional ablation studies on\n3 different scenarios: (1) random ordering (step\n3→1→2→4→5), (2) Fine-to-Coarse (FoC) order-\ning (step 1→3→4→2→5), and (3) CoF ordering\nwith hypothetical individual step removal. Table 4\ndemonstrates CoF logical ordering yields the best\nperformance with significant improvements on the\nchallenging Exact Match metrics (9.00 and 7.50\npoints of improvement over random and FoC re-\nspectively). Random, FoC ordering together with\nCoF ordering with missing individual steps ne-\nglect the natural connections of problem-solving\nfrom high-level (coarse-grained) to low-level (fine-\ngrained) tasks, leading to worse performance across\ndifferent metrics. For CoF-CoT, when step 1 or step\n2 is removed (no AMR or intent information), we\nobserve the most significant performance decrease,\nimplying the essence of coarse-grained knowledge\nfor LLMs to solve the later sequential steps.\nImpact of Conditioning The major advantage\nof our multi-step reasoning is the ability to explic-\nitly condition the prior predictions in later steps.\nAs observed in Table 4, conditioning prior knowl-\nedge in multi-step reasoning improves the overall\nperformance of CoF-CoT across different metrics\nwith the most significant gain in Intent Accuracy\n(+4.50%). This observation implies the importance\nof conditioning the appropriate information on CoT\nfor an improved performance of LLMs under chal-\nlenging zero-shot multi-domain settings.\n6 Conclusion\nIn this work, we conduct a preliminary study of\nLLMs’ capability in multi-grained NLU tasks of Di-\nalogue systems. Moreover, motivated by CoT, we\npropose a novel CoF-CoT approach aiming to break\ndown NLU tasks into multiple reasoning steps\nwhere (1) LLMs can learn to acquire and lever-\nage concepts from different granularities of NLU\ntasks, (2) additional AMR structured representa-\ntion can be integrated and leveraged throughout the\nmulti-step reasoning. We empirically demonstrate\nthe effectiveness of CoF-CoT in improving LLMs\ncapability in multi-grained NLU tasks under both\nzero-shot and few-shot multi-domain settings.\n12113\nLimitations\nOur empirical study is restricted to English NLU\ndata. It is partially due to the existent English-\nbias of Abstract Meaning Representation (AMR)\nstructure (Banarescu et al., 2013). We leave the\nadaptation of the CoF-CoT to multilingual settings\n(Nguyen and Rohrbaugh, 2019; Qin et al., 2022;\nNguyen et al., 2023b) as future directions for our\nwork.\nOur work is empirically studied on the Flat Logic\nForm representation. In other words, Logic Form\nonly includes one intent followed by a set of slot\nsequences. There are two major rationales for our\nempirical scope. Firstly, as the early preliminary\nstudy on multi-grained NLU tasks which unify both\nSemantic Parsing and NLU perspectives, we de-\nsign a small and controllable scope for the experi-\nments. Secondly, as most NLU datasets including\nMASSIVE (FitzGerald et al., 2022) are restricted\nto single-intent utterances, Flat Logic Form is a\nviable candidate reconciliating between traditional\nNLU and Semantic Parsing evaluations. We leave\nexplorations on the more challenging Nested Logic\nForm where utterances might contain multiple in-\ntents for future work.\n7 Acknowledgement\nWe thank the anonymous reviewers for their con-\nstructive feedback which we incorporated in the\nfinal version of this manuscript.\nThis work is supported in part by NSF under\ngrant III-2106758.\nReferences\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang,\nJing Bai, Jing Yu, and Yunhai Tong. 2021. Syntax-\nBERT: Improving pre-trained transformers with syn-\ntax trees. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 3011–3020,\nOnline. Association for Computational Linguistics.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Griffitt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of the 7th linguis-\ntic annotation workshop and interoperability with\ndiscourse, pages 178–186.\nEmanuele Bastianelli, Andrea Vanzo, Pawel Swietojan-\nski, and Verena Rieser. 2020. SLURP: A spoken lan-\nguage understanding resource package. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7252–7262, Online. Association for Computational\nLinguistics.\nEmanuele Bugliarello and Naoaki Okazaki. 2020. En-\nhancing machine translation with dependency-aware\nself-attention. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1618–1627, Online. Association for\nComputational Linguistics.\nIñigo Casanueva, Ivan Vuli ´c, Georgios Spithourakis,\nand Paweł Budzianowski. 2022. Nlu++: A multi-\nlabel, slot-rich, generalisable dataset for natural lan-\nguage understanding in task-oriented dialogue. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1998–2013.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nMauajama Firdaus, Ankit Kumar, Asif Ekbal, and Push-\npak Bhattacharyya. 2019. A multi-task hierarchi-\ncal approach for intent detection and slot filling.\nKnowledge-Based Systems, 183:104846.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\nSwetha Ranganath, Laurie Crist, Misha Britan,\nWouter Leeuwis, Gokhan Tur, and Prem Natara-\njan. 2022. Massive: A 1m-example multilin-\ngual natural language understanding dataset with 51\ntypologically-diverse languages.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nAishwarya Kamath and Rajarshi Das. A survey on\nsemantic parsing. In Automated Knowledge Base\nConstruction (AKBC).\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950–2962, Online. Association for Computa-\ntional Linguistics.\n12114\nHoang Nguyen and Gene Rohrbaugh. 2019. Cross-\nlingual genre classification using linguistic groupings.\nJournal of Computing Sciences in Colleges, 34(3):91–\n96.\nHoang Nguyen, Chenwei Zhang, Ye Liu, and Philip\nYu. 2023a. Slot induction via pre-trained language\nmodel probing and multi-level contrastive learning.\nIn Proceedings of the 24th Meeting of the Special\nInterest Group on Discourse and Dialogue , pages\n470–481, Prague, Czechia. Association for Computa-\ntional Linguistics.\nHoang Nguyen, Chenwei Zhang, Congying Xia, and\nS Yu Philip. 2020. Dynamic semantic matching and\naggregation network for few-shot intent detection.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 1209–1218.\nHoang Nguyen, Chenwei Zhang, Tao Zhang, Eugene\nRohrbaugh, and Philip Yu. 2023b. Enhancing cross-\nlingual transfer via phonemic transcription integra-\ntion. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023 , pages 9163–9175,\nToronto, Canada. Association for Computational Lin-\nguistics.\nLibo Qin, Qiguang Chen, Tianbao Xie, Qixin Li, Jian-\nGuang Lou, Wanxiang Che, and Min-Yen Kan. 2022.\nGl-clef: A global–local contrastive learning frame-\nwork for cross-lingual spoken language understand-\ning. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2677–2686.\nZiyi Shou, Yuxin Jiang, and Fangzhen Lin. 2022. AMR-\nDA: Data augmentation by Abstract Meaning Rep-\nresentation. In Findings of the Association for Com-\nputational Linguistics: ACL 2022, pages 3082–3098,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 2609–2634, Toronto, Canada. Associ-\nation for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nHenry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon,\nand Soyeon Caren Han. 2022. A survey of joint intent\ndetection and slot filling models in natural language\nunderstanding. ACM Computing Surveys, 55(8):1–\n38.\nDi Wu, Liang Ding, Fan Lu, and Jian Xie. 2020. SlotRe-\nfine: A fast non-autoregressive model for joint intent\ndetection and slot filling. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1932–1937, On-\nline. Association for Computational Linguistics.\nCongying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei\nZhang, and Philip Yu. 2020. Cg-bert: Conditional\ntext generation with bert for generalized few-shot\nintent detection. arXiv preprint arXiv:2004.01881.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-\ntor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,\nAnsong Ni, Ziyu Yao, Dragomir Radev, Caiming\nXiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,\nLuke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG:\nUnifying and multi-tasking structured knowledge\ngrounding with text-to-text language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 602–631,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nZenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun\nShou, Ming Gong, Wanjun Zhong, Xiaojun Quan,\nDaxin Jiang, and Nan Duan. 2021. Syntax-enhanced\npre-trained model. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5412–5422.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nChenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and S Yu\nPhilip. 2019. Joint slot filling and intent detection via\ncapsule neural networks. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5259–5267.\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2020. Sg-net:\nSyntax-guided machine reading comprehension. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 9636–9643.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023a. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\narXiv preprint arXiv:2305.03268.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023b. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\n12115\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, et al. 2022.\nLeast-to-most prompting enables complex reasoning\nin large language models. In The Eleventh Interna-\ntional Conference on Learning Representations.\nHan Zhou, Ignacio Iacobacci, and Pasquale Minervini.\n2023. XQA-DST: Multi-domain and multi-lingual\ndialogue state tracking. In Findings of the Associ-\nation for Computational Linguistics: EACL 2023 ,\npages 999–1009, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021.\nA robustly optimized bert pre-training approach with\npost-training. In Proceedings of the 20th Chinese\nNational Conference on Computational Linguistics,\npages 1218–1227.\nBarret Zoph, Colin Raffel, Dale Schuurmans, Dani Yo-\ngatama, Denny Zhou, Don Metzler, Ed H. Chi, Jason\nWei, Jeff Dean, Liam B. Fedus, Maarten Paul Bosma,\nOriol Vinyals, Percy Liang, Sebastian Borgeaud, Tat-\nsunori B. Hashimoto, and Yi Tay. 2022. Emergent\nabilities of large language models. TMLR.\nTable 5: Details of MTOP and MASSIVE datasets\nDataset MTOP MASSIVE\n# Domains 11 18\n# Train Domains 8 14\n# Test Domains 3 4\n# Intents 117 60\n# Slots 78 55\nSentence Length 6.14±2.30 6.34±2.94\n# Slots per sample 1.87±0.81 0.73±0.63\nA Connections between Semantic Parsing\nand NLU Tasks via Logic Form\nLogic Form not only captures the coarse-grained\nintent labels and fine-grained slot labels of the ut-\nterances but also encapsulates the implicit connec-\ntions between slots and intents.\nAs observed in Table 7, Logic Form is con-\nstructed as the flattened representation of the depen-\ndency structure between intents and slot sequences.\nSemantic Frame constructed as intent type(s) fol-\nlowed by a sequence of slot types can be directly\nextracted from the Logic Form. In addition, via\nthe Logic Form, the coarse-grained intent label\nCREATE_REMINDER, fine-grained slot TODO,\nDATE_TIME labels together the respective slot\nvalues (message Mike, at 7pm tonight) can all be\nextracted and converted to appropriate format (i.e.\nBIO format as the traditional sequence labeling\nground truths (Zhang et al., 2019)). Therefore,\nLogic Form can be considered the unified label for-\nmat to bridge the gap between Semantic Parsing (Li\net al., 2021; Xie et al., 2022) and traditional Intent\nDetection and Slot Filling tasks in NLU systems\n(Xia et al., 2020; Nguyen et al., 2020; Casanueva\net al., 2022).\nB Dataset Details\nWe provide the details of MTOP and MASSIVE\ndatasets in Table 5. As compared to MASSIVE,\nMTOP dataset not only contains more slot types\nand intent types but also tends to cover more slot\ntypes per sample in the Logic Form. This chal-\nlenging characteristic explains the consistent lower\nperformance across all methods on MTOP when\ncompared to MASSIVE as observed in Section 5.\nC Implementation Details\nAs the proposed step-by-step reasoning can be ap-\nplied to any LLMs, our proposed method is LLM-\nagnostic which is empirically studied in Appendix\nF. For simplicity and consistency, in our main em-\npirical study, we leverage gpt-3.5-turbo from Ope-\n12116\nnAI as the base LLM model. Following (Wang\net al., 2022), we set the decoding temperature T=0.7\nand number of outputs n=10.\nAs domain names provide essential clues for lan-\nguage models in multi-domain settings for multi-\ngrained NLU tasks (Zhou et al., 2023), to safe-\nguard the fairness in baseline comparisons, we\nconsistently include the domain name in the input\nprompts for all baselines unless stated otherwise.\nSpecifically, the only exception is presented in Ta-\nble 4 for CoF-CoT(CoF order)-Conditioning.\nFor few-shot (i.e. k-shot) learning settings, we\nrandomly sample k examples and manually prepare\nthe necessary labels for different baseline variants.\nWe experiment with k=5 in our empirical study.\nDomains of Demonstration SamplesTo repli-\ncate a more realistic scenario where the domains\nof k-shot demonstration samples are generally un-\nknown, we assume that k-shot demonstration sam-\nples come from different domains from the test\nsamples. The relaxation of constraints on the as-\nsumption regarding the domain similarity between\ndemonstration samples and test samples allows for\nbroader applications and encourages LLMs to ac-\ncumulate and extract the true semantic knowledge\nfrom k-shot demonstrations and avoid overfitting\nany specific domains. For completeness, we also\nconduct additional empirical studies to compare\nthe FSL performance of CoF-CoT under both sce-\nnarios: (1) k demonstration samples are from the\nsame domain as test samples, (2) k demonstrations\nare drawn from different domains from the test\nsamples. As observed in Table 6, additional con-\nstraint of similar domains between k-shot demon-\nstration samples and test samples leads to improve-\nments in the evaluation performance across NLU\nand Semantic Parsing tasks. This might be intuitive\nsince LLMs can extract domain-relevant informa-\ntion from the given k domain-similar samples to\nassist with inference process on test samples.\nD Prompt Design\nPrompts for individual steps of our CoF-CoT are\npresented in Figure 3. Additional output samples\nare also provided in Figure 4.\nE Qualitative Case Study\nWe present additional Qualitative Case Study com-\nparing the outputs between different baseline meth-\nods and our proposed CoF-CoT in Figure 5.\nAs observed in Figure 5, our CoF-CoT provides\nthe predictions closest to the ground truth while\nother baselines struggle to (1) generate the correct\nintent type (i.e. GET_DATE_TIME_EVENT in-\ntent type from Direct Prompt in comparison with\nGET_EVENT intent from ground truth) (2) iden-\ntify the correct slot values (i.e. everything slot\nvalue generated from CoT), (3) generate the cor-\nrect slot type for the corresponding slot values.\n(i.e. EVENT_TYPE slot type for music festivals\nslot values from Complex-CoT instead of CATE-\nGORY_EVENT slot type).\nF LLM-Agnostic Capability\nOur proposed CoF-CoT is LLM-agnostic since the\nfocus of the work is on the prompt design, which\ncan be applied to any LLMs. As most LLMs rely\non the high quality of the designed prompts, our\nproposed CoF-CoT prompt design can be used as\ninput to any LLMs for zero-shot and in-context\nlearning settings. This is also similarly observed\nin CoT (Wei et al., 2022), SC-CoT (Wang et al.,\n2022) and other comparable CoT methods. For\nfurther clarification, we report additional empirical\nresults of our proposed CoF-CoT applied to both\nof the backbone PaLM (Chowdhery et al., 2022)\nand GPT3.5 LLMs on the MTOP dataset under\nboth ZSL and FSL settings in Table 8. As observed\nin Table 8, CoF-CoT prompting consistently out-\nperforms the two backbone LLMs across all NLU\nand Semantic Parsing tasks, demonstrating both\nthe effectiveness and LLM-agnostic capability of\nour proposed CoF-CoT.\n12117\nTable 6: FSL Results of CoF-CoT with k-shot demonstration samples selected from different and similar domains in\ncomparison with domains of test samples on MTOP dataset.\nMethod Assumption Intent Acc Slot F1 Frame Acc Exact Match\nCoF-CoT k domain-different samples 61.50 ±4.93 30.12 ±3.93 15.00 ±1.32 11.0 ±1.61\nCoF-CoT k domain-similar samples 70.00 ±1.33 38.16 ±5.42 20.50 ±2.00 15.00 ±1.00\nTable 7: Sample utterance with its Logic Form under both Semantic Parsing and NLU tasks’ metrics. // denotes the\nseparation between tokens of the given utterance.\nMetric Granularity Level Format Ground TruthInput Sentence – – – Set // up// a // reminder // to // message // mike // at // 7pm // tonightLogic Form – – – [IN:CREATE_REMINDER [SL:TODO: message mike] [SL:DATE_TIME: at 7pm tonight]]\nNLU TasksIntent Accuracy Coarse-grained Intent Label IN:CREATE_REMINDERSlot F1 Fine-grained BIO Slot Sequence O // O // O // O // O // B-TODO // I-TODO // B-DATE_TIME // I-DATE_TIME // I-DATE_TIME\nSemantic Parsing TasksFrame Accuracy Both Logic Form IN:CREATE_REMINDER-SL:TODO-SL:DATE_TIMEExact Match Both Logic Form [IN:CREATE_REMINDER [SL:TODO: message mike] [SL:DATE_TIME: at 7pm tonight]]\n  \nStep 1: Given the utterance and its domain, generate a single corresponding Abstract Meaning Representation (AMR) Graph in the Neo-Davidsonian format. The format involves :ARG and :op relations.\nUtterance: {utterance}\nDomain: {domain}\nAMR Graph: {AMR}\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStep 2: Given the utterance and its domain, and its AMR Graph, select one of the following in the Intent Vocabulary as the intent type for the utterance.  \nUtterance: {utterance}\nDomain: {domain}\nAMR Graph: {AMR}]\nIntent Vocabulary: {intent_vocab}\nIntent:{intent}\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStep 3: Based on the utterance, domain, its AMR Graph and its intent, generate key phrases for the utterance. Key phrases can be made up from multiple AMR concepts. \nEach word in key phrases must exist in the given utterance. Each word in the utterance appears in only one key phrase. Key phrases need to contain consecutive words in the given utterance. \nKey phrases do not need to cover all words in the utterance. Return a list of key phrases separated by commas.\nUtterance: {utterance}\nDomain: {domain}\nAMR Graph: {AMR}\nIntent: {intent}\nKey phrases: {key_phrase}\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStep 4:  Given the slot vocabulary, utterance, its domain, its AMR Graph and its intent, identify the corresponding slot type as one of the types in the slot vocabulary for each key phrase. \nReturn the list of key phrases and their corresponding slot types in the following format: (key_phrase, slot_type) separated by commas. \nIf none of the slot types in the vocabulary fits, return the slot type as O.\nSlot Vocabulary: {slot_vocab}\nUtterance: {utterance}\nDomain: {domain}\nAMR Graph: {AMR}\nIntent: {intent}\n(Key phrase, Slot Type) pairs: {slot_pair}\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStep 5:  Given the utterance, domain, its intent type, its slot type and slot value pairs in (slot_type, slot_value) format, \ngenerate logic form of the utterance in the format of [IN:___ [SL:____] [SL:___]] where IN: is followed by an intent type and SL: is followed by a slot type and slot value pair separated by white space. \nThe number of [SL: ] is unlimited. The number of [IN: ] is limited to 1.\nUtterance: {utterance}\nDomain: {domain}\nIntent: {intent}\n(Key phrase, Slot Type) pairs: {slot_pair}\nLogic Form: {lf}\nFigure 3: CoF-CoT Prompt Design Template. {.} denotes the placeholder argument.\nTable 8: Experimental results on MTOP dataset under zero-shot few-shot multi-domain settings with different LLM\nbackbone architectures (PaLM (Chowdhery et al., 2022) and GPT3.5).\nMTOP\nModel Zero-shot Few-shot\nNLU Semantic Parsing NLU Semantic Parsing\nIntent Acc Slot F1 Frame Acc Exact Match Intent Acc Slot F1 Frame Acc Exact Match\nPaLM 16.67 ±2.52 7.24±1.00 3.17±0.76 1.17±0.76 48.83±4.54 14.24±1.58 4.17±2.02 2.33±0.76\nPaLM + CoF-CoT 42.33±3.33 13.73±2.88 4.01±0.15 3.50±1.32 57.17±3.79 21.47±3.79 10.33±3.06 6.67±2.75\nGPT3.5 31.50 ±1.80 21.84±2.83 8.33±1.44 6.00±1.32 51.33±3.40 28.35±3.24 11.00±1.80 8.33±1.00\nGPT3.5 + CoF-CoT 57.67±2.75 23.47±4.09 14.33±1.52 9.00±1.00 61.50±4.93 30.12±3.93 15.00±1.32 11.00±1.61\n12118\n  \nSample 1:\n{utterance} =  show me dates for music festivals in 2018\n{ground_truth} = [IN:GET_EVENT [SL:CATEGORY_EVENT music festivals ] [SL:DATE_TIME in 2018 ]]\n{domain} = event\n{AMR} = (c/ show-01\n    :ARG1 (m /me)\n    :ARG2 (d/ date-entity\n       :mod (f/ music-festival)\n        :year 2018))\n{intent} = GET_EVENT\n{key phrase} = Show me, dates, music fesitval, in 2018\n{Key phrase, Slot Type} = (Show me: O), (dates: O), (music festival: CATEGORY_EVENT), (in 2018: DATE_TIME)\n{lf} = [IN:GET_EVENT [SL:CATEGORY_EVENT music festivals ] [SL:DATE_TIME in 2018 ]]\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\nSample 2:\n{utterance} =  Set my timer for my tabata workout.\n{ground_truth} = [IN:CREATE_TIMER [SL:METHOD_TIMER timer ] [SL:TIMER_NAME tabata workout ] ]\n{domain} = timer\n{AMR} = (set-01\n     :ARG0 (I)\n     :ARG1 (timer-02\n          :ARG0 (my-03)\n              :op1 (workout-05\n                   :ARG0 (my-04)\n                   :ARG1 (tabata-06))))\n{intent} = CREATE_TIMER\n{key phrase} = Set my timer, tabata workout\n{Key phrase, Slot Type} = (Set my timer: METHOD_TIMER), (tabata workout: TIMER_NAME)\n{lf} = [IN:CREATE_TIMER [SL:METHOD_TIMER Set my timer ] [SL:TIMER_NAME tabata workout ]]\nFigure 4: Sample output from our CoF-CoT. {.} denotes the placeholder corresponding to template in Figure 3.\n  \nUtterance 1: Show me dates for music festivals in 2018\nGround Truth:        [IN:GET_EVENT [SL:CATEGORY_EVENT music festivals ] [SL:DATE_TIME in 2018 ]]\n--------------------------------------------------------------------------------------------------------------------------------------------------\nDirect Prompt: [IN:GET_DATE_TIME_EVENT [SL:TITLE_EVENT music festivals ] [SL:DATE_TIME:2018 ]]\nCoT: [IN:GET_EVENT [SL:DATE_TIME 2018 ] [SL:TITLE_EVENT music festivals ]]\nSC-CoT: [IN:GET_EVENT [SL:DATE_TIME 2018 ] [SL:TITLE_EVENT music festivals ]]\nComplex-CoT: IN:GET_DATE_TIME_EVENT [SL:EVENT_TYPE music festivals ] [SL:EVENT_YEAR 2018 ]]\n--------------------------------------------------------------------------------------------------------------------------------------------------\nCoF-CoT: [IN:GET_EVENT [SL:CATEGORY_EVENT music festivals ] [SL:DATE_TIME in 2018 ]] \nUtterance 2: I want to know everything that breaks about the dam breaking in california\nGround Truth:        [IN:NEWS_QUERY [SL:PLACE_NAME california ]]\n--------------------------------------------------------------------------------------------------------------------------------------------------\nDirect Prompt: [IN: WEATHER_QUERY] [SL: PLACE_NAME california ] [SL: EVENT_NAME dam breaking ]]\nCoT: IN: NEWS_QUERY [SL: WEATHER_DESCRIPTOR everything ] [SL: PLACE_NAME california ]]\nSC-CoT: [IN:NEWS_QUERY [SL:NEWS_TOPIC dam breaking in california ]]\nComplex-CoT: [IN:NEWS_QUERY [SL:NEWS_TOPIC dam breaking in california ]]\n--------------------------------------------------------------------------------------------------------------------------------------------------\nCoF-CoT: [IN: NEWS_QUERY [SL: PLACE_NAME california ]] \nFigure 5: Qualitative Case Study among baseline variants and the proposed CoF-CoT. Ground Truth is shown in red.\n12119",
  "topic": "Natural language understanding",
  "concepts": [
    {
      "name": "Natural language understanding",
      "score": 0.7865796089172363
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7019355893135071
    },
    {
      "name": "Computer science",
      "score": 0.6532541513442993
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.48523736000061035
    },
    {
      "name": "Granularity",
      "score": 0.4759873151779175
    },
    {
      "name": "Representation (politics)",
      "score": 0.4275316298007965
    },
    {
      "name": "Natural language processing",
      "score": 0.4089861214160919
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34522467851638794
    },
    {
      "name": "Natural language",
      "score": 0.33147746324539185
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39422238",
      "name": "University of Illinois Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210155268",
      "name": "Salesforce (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ]
}