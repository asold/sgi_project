{
  "title": "PVT: Point-Voxel Transformer for 3D Deep Learning",
  "url": "https://openalex.org/W3201205318",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2101963880",
      "name": "Cheng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3193611737",
      "name": "Haocheng Wan",
      "affiliations": [
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2327112031",
      "name": "Shengqiang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131541911",
      "name": "Xinyi Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2168267730",
      "name": "Zizhao Wu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963158438",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W3034314779",
    "https://openalex.org/W2963517242",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W2786036844",
    "https://openalex.org/W3177251636",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3138291980",
    "https://openalex.org/W3174926460",
    "https://openalex.org/W2963226018",
    "https://openalex.org/W2981983525",
    "https://openalex.org/W2981199548",
    "https://openalex.org/W2895472109",
    "https://openalex.org/W2942498895",
    "https://openalex.org/W2769312834",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2902302021",
    "https://openalex.org/W3035002114",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W2964253930",
    "https://openalex.org/W2963830382",
    "https://openalex.org/W2553307952",
    "https://openalex.org/W2980535048",
    "https://openalex.org/W2971230666",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2996167479",
    "https://openalex.org/W2962928871",
    "https://openalex.org/W2948196881",
    "https://openalex.org/W2586114507",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2962731536",
    "https://openalex.org/W2798777114",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3034239841",
    "https://openalex.org/W3103830808",
    "https://openalex.org/W2797997528",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2963727135",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2963046128",
    "https://openalex.org/W3104141662",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2963123724",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2963281829",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3177450673",
    "https://openalex.org/W3205586691",
    "https://openalex.org/W2964062501",
    "https://openalex.org/W3119786062"
  ],
  "abstract": "In this paper, we present an efficient and high-performance neural architecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which deeply integrates both 3D voxel-based and point-based self-attention computation to learn more discriminative features from 3D data. Specifically, we conduct multi-head self-attention (MSA) computation in voxels to obtain the efficient learning pattern and the coarse-grained local features while performing self-attention in points to provide finer-grained information about the global context. In addition, to reduce the cost of MSA computation with high efficiency, we design a cyclic shifted boxing scheme by limiting the MSA computation to non-overlapping local box and also preserving cross-box connection. Evaluated on classification benchmark, our method not only achieves state-of-the-art accuracy of 94.0% (no voting) but outperforms previous Transformer-based models with 7x measured speedup on average. On part and semantic segmentation, our model also obtains strong performance(86.5% and 68.2% mIoU, respectively). For 3D object detection task, we replace the primitives in Frustrum PointNet with PVT block and achieve an improvement of 8.6% AP.",
  "full_text": "PVT: Point-Voxel Transformer for Point Cloud Learning\nCheng Zhang1*, Haocheng Wan1‚àó, Xinyi Shen2, Zizhao Wu1‚Ä†\n1Hangzhou Dianzi University, Hangzhou China\n2University College London, London UK\n{zhangcheng828,wanhaocheng2022,xinyishen2018,wuzizhao}@foxmail.com, @163.com, @hdu.edu.cn\nAbstract\nThe recently developed pure Transformer architectures\nhave attained promising accuracy on point cloud learning\nbenchmarks compared to convolutional neural networks.\nHowever, existing point cloud Transformers are computa-\ntionally expensive because they waste a signiÔ¨Åcant amount\nof time on structuring irregular data. To solve this short-\ncoming, we present the Sparse Window Attention (SWA)\nmodule to gather coarse-grained local features from non-\nempty voxels. The module not only bypasses the expensive\nirregular data structuring and invalid empty voxel computa-\ntion, but also obtains linear computational complexity with\nrespect to voxel resolution. Meanwhile, we leverage two\ndifferent self-attention variants to gather Ô¨Åne-grained fea-\ntures about the global shape according to different scale\nof point clouds. Finally, we construct our neural archi-\ntecture called Point-Voxel Transformer (PVT), which inte-\ngrates these modules into a joint framework for point cloud\nlearning. Compared with previous Transformer-based and\nattention-based models, our method attains a top accuracy\nof 94.1% on the classiÔ¨Åcation benchmark and 10 √óinfer-\nence speedup on average. Extensive experiments also val-\nidate the effectiveness of PVT on semantic segmentation\nbenchmarks. Our code and pretrained model are avaliable\nat https://github.com/HaochengWan/PVT.\nKeywords Point Cloud Learning, Transformer, Neural\nNetwork, 3D Vision.\n1. Introduction\nPoint cloud learning has been receiving increasing atten-\ntion from both industry and academia due to its wide ap-\nplications including autonomous driving, robotics, AR/VR,\netc. In these settings, sensors like LIDAR produce irregu-\nlar and unordered sets of points that correspond to object\nsurfaces. However, how to capture semantics directly and\n*These authors contributed equally.\n‚Ä†Corresponding author.\nquickly from these data remains a challenge for point cloud\nlearning.\nMost existing point cloud learning methods can be clas-\nsiÔ¨Åed into two categories in terms of data representations:\nvoxel-based models and point-based models. The Voxel-\nbased models generally rasterize point clouds onto regu-\nlar grids and apply 3D convolution for feature learning\n[67, 30, 46]. These models are computationally efÔ¨Åcient\ndue to their excellent memory locality, but the inevitable\ninformation loss degrades the Ô¨Åne-grained localization ac-\ncuracy [35]. By contrast, point-based models naturally pre-\nserve the accuracy of point location, but are generally com-\nputationally intensive [41, 4, 18].\nRecently, Transformer architecture has drawn great at-\ntention in natural language processing and 2D vision be-\ncause of its superior capability in capturing long-range de-\npendencies. Powered by Transformer [40] and its vari-\nants [26, 8], point-based models have applied self-attention\n(the core unit of Transformer) to extract features from\npoint clouds and improve performance signiÔ¨Åcantly [14,\n9, 65, 53]. However, most of them suffer from the time-\nconsuming process of sampling and aggregating features\nfrom irregular points, which becomes the efÔ¨Åciency bottle-\nneck [27].\nIn this paper, we study how to design an efÔ¨Åcient and\nhigh-performance Transformer architecture while avoiding\nthe shortcoming of previous point cloud Transformers. We\nobserve that voxel-based models have regular data locality\nand can efÔ¨Åciently encode coarse-grained features, while\npoint-based networks preserve the accuracy of location in-\nformation with the Ô¨Çexible Ô¨Åelds and can effectively aggre-\ngate Ô¨Åne-grained features. Inspired by this, we propose\na novel point cloud learning architecture, namely, Point-\nV oxel Transformer (PVT), which combines the ideas of\nvoxel-based and point-based models. Our network focuses\non how to fully exploit the potential of the two models men-\ntioned above in Transformer architecture, capturing useful\ndiscriminative features from 3D data.\nTo investigate this, we conduct self-attention (SA) com-\nputation in voxels to obtain efÔ¨Åcient learning pattern while\n1\narXiv:2108.06076v4  [cs.CV]  25 May 2022\nperforming SA in points to preserve the accuracy of loca-\ntion information with the Ô¨Çexible Ô¨Åelds. However, directly\nperforming SA computation to voxels is infeasible, mainly\nowing to two facts. First, non-empty voxels are sparsely\ndistributed in the voxel domain and only account for a small\nproportion of total voxels [54]. Second, the original SA\ncomputation leads to quadratic complexity with respect to\nthe number of voxels, making it unsuitable for various 3D\nvision tasks, particularly for the dense prediction tasks of\nobject detection and semantic segmentation. To tackle these\nissues, we design a sparse window attention (SW A) module\nwhich only has linear computational complexity by comput-\ning SA locally within the non-overlapping 3D window. A\nkey design element of SW A lies in its sparse attention com-\nputing, in which a GPU-based Rule Book stores the non-\nempty voxel indexes. On the basis of this strategy, we can\nbypass the invalid computation of empty voxels and retain\nthe original 3D shape structure. For cross-window infor-\nmation interaction, inspired by Swin Transformer [26], we\npropose to apply shifted window to enlarge the receptive\nÔ¨Åeld. In addition, we leverage two different SA variants to\ncapture the global information according to different scales\nof point clouds. For small-scale point clouds, we propose\nrelative-attention (RA), which extends the SA mechanism\nto consider representations of the relative position, or dis-\ntances between points. The advantage of RA is that the\nabsolute coordinates of the same object can be completely\ndifferent with rigid transformations; thus, injecting relative\nposition representations (RPR) in our structure is generally\nmore robust. We observe that our network obtains signif-\nicant improvements over not using this RPR term without\nadding extra training parameters. Nevertheless, with tens\nof thousands of points (e.g., SemanticKITTI [3]) as inputs,\ndirectly applying the RA module in points incurs unaccept-\nable O(N2) memory consumption, where N is the input\npoint number. Thus, for large-scale point clouds, we per-\nform External Attention (EA), a linear attention variant to\navoid the O(N2) computational complexity of RA module.\nBased on these modules, We propose a two-branch PVT\nthat mainly consists of thevoxel branch and thepoint branch\n(Figure 1). As illustrated in Figure 4, by behaving SW A\nin the voxel branch and performing RA (or EA) in the\npoint branch, our method disentangles the coarse-grained\nlocal feature aggregation and the Ô¨Åne-grained global con-\ntext transformation so that each branch can be implemented\nefÔ¨Åciently and accurately.\nBy using the PVT, we construct PVT networks for var-\nious 3D tasks. These networks can capably serve as a\ngeneral-purpose backbone for point cloud learning. In par-\nticular, we conduct the classiÔ¨Åcation experiment on the\nModelNet40 [47] dataset and obtain the state-of-the-art\naccuracy of 94.1% (no voting), while being on average\n10√ófaster than its Transformer baselines. On ShapeNet\nPart [24], S3DIS [1], and SemanticKITTI [3] datasets, our\nmodel also obtains strong performance (86.6%, 69.2%, and\n64.9% mIoU, respectively).\nThe main contributions are summarized as following:\n‚Ä¢ We propose PVT, the Ô¨Årst Transformer-based ap-\nproach, to deeply incorporate the advantages from both\npoint-based and voxel-based networks to our knowl-\nedge.\n‚Ä¢ We propose an efÔ¨Åcient local attention module, named\nSW A, which achieves linear computational complexity\nwith respect to the input voxel size and bypasses the\ninvalid empty voxel computation.\n‚Ä¢ Extensive experiments show that our PVT model at-\ntains competitive results on general point cloud learn-\ning tasks with 10√ólatency speed-up than its baselines.\n2. Related works\n2.1. Voxel-based models on points\nTo leverage the success of convolutional neural networks\non 2D images, many researchers have endeavored to project\n3D point clouds directly onto voxels and perform 3D con-\nvolution for information aggregation [6, 21]. However,\nthe memory and computational consumption of such full-\nvoxel-based models increases cubically with respect to the\nvoxel‚Äôs resolution. To tackle this shortcoming, O-CNN\n[43], OctNet [33], and kd-Net [17] constructed tree struc-\ntures for 3D voxels to bypass the invalid computation of the\nempty voxels. Although such methods are efÔ¨Åcient in data\nstructuring, geometric details are lost during the projection\nbecause multiple bucketing points into the same voxel leads\nto indistinguishable features for learning.\nCompared with most voxel-based 3D models, our model\nis more efÔ¨Åcient and effective for the following reasons: 1)\nInstead of using 3D convolutions, we propose SW A to han-\ndle the sparsity characteristic of voxels that have a linear\ncomputational complexity with respect to voxel resolution\nand bypasses the invalid computation of empty voxels. 2)\nAs we employ RA in the point domain, an inherent advan-\ntage is that we can avoid feature loss during voxelization,\nmaintaining the geometric details of a point cloud.\n2.2. Point-based models for point cloud learning\nInstead of voxelization, it is possible to make a neutral\nnetwork that consumes directly on point clouds. [29] pro-\nposed PointNet, the pioneering work that learns directly on\nsparse and unstructured point clouds. Inspired by PointNet,\nprevious works introduced sophisticated neural modules to\nlearn per-point features. These models can be generally\nclassiÔ¨Åed as: 1) neighbouring feature pooling [63, 16], 2)\ngraph message passing [45, 59, 42], and 3) attention-based\nor Transformer-based models [34, 60, 55, 14, 65, 9].\nS-SWA\nRS-SWA\n{R=30,W=4}{R=30,W=4} {R=15,W=4}\nFigure 1. Model Architecture: The model for part segmentation take as input N points and feeds them into 3 stacked PVT blocks to\nlearn a semantically rich and discriminative representation for each point, followed by a MLP layer to generate the output feature. After\nthat, we leverage the max-pooling and repeating operators to extract an effective global feature representing the entire point cloud. Note\nthat shortcut connections are used to extract multi-scale features and one MLP layer (1280) to aggregate multi-scale features, where we\nconcatenate features from previous layers to get a 64+64+128+1024=1280-dimensional point cloud. Finally, we predict the Ô¨Ånal point-wise\nsegmentation scores for the input point cloud and the part label of a point is also determined as the one with maximal score. PVT block\n(grey box): The PVT block is composed of two branches. The upper branch is voxel-based for aggregating local features and the lower is\npoint-based for capturing global features. We can effectively fuse two branches because they are providing complementary information.R\nand w denote the voxel resolution size and 3d window size, respectively. ‚®Å: addition, ‚®Ç: concatenation.\nOwing to the sparsity and irregularity of point clouds, the\nmethods that directly consume points have achieved state-\nof-the-art performance. However, the cost of data structur-\ning of such methods has become the computation burden,\nespecially on the large-scale point clouds dataset [27, 51].\nIn this study, we handle this shortcoming by using the SW A\nmodule.\n2.3. Self-attention and Transformer\n[2] proposed a neural machine translation method with\nan attention mechanism, in which attention weight is com-\nputed through the hidden state of an RNN. Then [25] further\nproposed self-attention to visualize and interpret sentence\nembeddings. Subsequent works employed self-attention\nlayers to replace some or all the spatial convolution layers,\nsuch as Transformer for machine translation [40]. [7] pro-\nposed the bidirectional transformers (BERT), which is one\nof the most powerful models in the NLP Ô¨Åeld.\nGiven the success of self-attention and Transformer ar-\nchitectures in NLP, researchers have applied them to vision\ntasks [15, 32, 62]. For instance, [8] proposed an image\nrecognition network, ViT, which directly applied a Trans-\nformer architecture on image patches and achieved better\nperformance than the traditional convolutional neural net-\nworks. [26] recently introduced Swin Transformer to in-\ncorporate inductive bias for spatial locality, hierarchy and\ntranslation invariance.\n2.4. Transformers on point cloud\nRecently, plenty of researchers have attempted to explore\nTransformer-based architectures for point cloud learning.\n[9] proposed PT 1 to extract global features by introduc-\ning the dot-product SA mechanism. [14] proposed offset-\nattention to calculate the offset difference between the SA\nfeatures and the input features by element-wise subtraction.\nMoreover, [19] proposed PT 2 to build local vector atten-\ntion in neighborhood point sets and achieved signiÔ¨Åcant\nprogress. However, they wasted a high percentage of the to-\ntal time on structuring the irregular data, which becomes the\nefÔ¨Åciency bottleneck. In this work, we study how to solve\nthis shortcoming, so as to design an efÔ¨Åcient Transformer-\nbased architecture for point cloud analysis.\nThere also exists some Transformer-based architecture\nfor various point cloud processing tasks. For instance,\n[11] proposed the SE(3)-Transformer, a variant of the self-\nattention module for 3D point clouds and graphs, which\nis equivariant under continuous 3D roto-translations. Late,\n[57] and [49] have attempted to explore Transformer-based\narchitectures for point cloud completion and achieved sig-\nniÔ¨Åcant performance on all completion tasks. Recently,\n[58] proposed Point-BERT to unleash the scalability and\ngeneralization of Transformers for 3D point cloud repre-\nsentation learning. Extensive experiments also demon-\nstrate that Point-BERT signiÔ¨Åcantly improves the perfor-\nmance of standard point cloud Transformers. Different\nfrom these Transformers, in this paper, we study how to de-\nsign an efÔ¨Åcient and high-performance Transformer archi-\ntecture for point cloud learning, making it suitable for edge\ndevices with limited computational resources or real-time\nautonomous driving scenarios.\n3. Method\nOverview\nIn contrast to the previous point cloud Transformers [14,\n9, 65], we design the network as efÔ¨Åcient as possible with\nhigh accuracy so that it can be widely used on various 3D\ntasks.\nWe introduce an efÔ¨Åcient neural architecture for point\ncloud learning, namely, PVT. Formally, given a point cloud\nembedding F ‚ààRN√óD, our PVT is designed to map the\ninput features F to a new set of point feature F‚Ä≤‚ààRN√óD.\nAs illustrated in Figure 1, the PVT consists of two main\nbranches: a voxel branch and a point branch. We leverage\nthe voxel branch to map the inputs F to Flocal ‚ààRN√óD,\nwhich aggregates local features in the voxel domain. How-\never, full voxel method will inevitably encounter informa-\ntion loss during voxelization. Thus, we utilize the point\nbranch to map the inputs F to Fglobal ‚àà RN√óD, which\ncan directly extract global features for each individual point.\nWith both local features and aggregated global context, we\ncan efÔ¨Åciently fuse two branches into an addition layer as\nboth provide complementary information.\nBelow we detail the two branches in Sections 3.1 and 3.2.\nSection 3.3 details our feature fusion module, and Section\n3.4 discusses the relationship between the proposed model\nand the prior works.\n3.1. Voxel branch\nThis branch aims to effectively capture local informa-\ntion, which can bypass expensive sampling and neighbor\npoints querying. SpeciÔ¨Åcally, it contains three steps: vox-\nelization, feature aggregation, and devoxelization.\nThe voxelized and devoxelized methods map the input\npoint cloud Pto a new set of voxel features V and trans-\nform the voxel-wise features back to the point-wise features\nFlocal (Readers can refer to [27] for more details). In this\nwork, we apply the standard Transformer architecture [40]\nto perform feature aggregation on regular 3D voxels, which\ncan improve accuracy signiÔ¨Åcantly. However, the standard\nTransformer architecture conducts global-SA which leads\nto quadratic complexity with respect to the number of vox-\nels, making it unsuitable for many 3D vision problems re-\nquiring dense prediction, such as semantic segmentation.\nWindow Attention: To obtain efÔ¨Åcient modeling power,\nwe propose to compute SA within local 3D windows, which\nare arranged to evenly partition the voxel space in a non-\noverlapping manner. Assume that each local 3D window\ncontains W √óW √óW voxels and R denotes the voxel res-\nolution. The computational complexity of a global-SA and\n: query vùëñ (0, 1) \ncalculate \nattention field\nHash table\nRule book\nvoxel index\n(0, 1)\n(0, 3)\n(2, 2)\n(3, 0)\nkey value\n1021\n1156\n1689\n2324\n1021\n1156\n1689\n2324\n59\n64\n99\n103\n‚Ä¶ ‚Ä¶\n‚Ä¶ ‚Ä¶\n‚Ä¶ ‚Ä¶\n‚Ä¶\nNon-empty \nvoxel array\nPerforming \nattention\nFigure 2. Sparse Window Attention: This is also a 2D example\nand can be easily extended to 3D cases. SW A is a GPU-based\nmethod and can efÔ¨Åciently index the non-empty voxels in the at-\ntention Ô¨Åeld.\na window-based one on R3 voxels are:\n‚Ñ¶(Global-SA) = 4R3D2 + 2(R3)2D (1)\n‚Ñ¶(Window -SA) = 4R3D2 + 2B3R3D (2)\nwhere the former is quadratic to the number of voxels R3,\nthe latter is linear when W is Ô¨Åxed, D is the dimension of\nfeatures. In summary, global-SA computation is generally\nunaffordable for a large voxel resolution, whereas the local\nwindow-SA is scalable.\nSparse Window Attention: Different from 2D pixels\ndensely placed on an image plane, non-empty voxels only\naccount for a small proportion of total voxels. Inspired by\n[13, 54], we design Sparse Window Attention to handle\nthe sparsity characteristic of voxels. As shown in Figure\n2, for each querying index vi, we Ô¨Årst use Window Atten-\ntion to determine all neighboring voxel indices in the at-\ntention Ô¨Åeld, and then adopt a Hash table to get the hashed\ninteger neighboring voxel indices. Last, a GPU-based Rule\nBook stores the hashed voxel indices as keys, and the corre-\nsponding indices for the non-empty voxel array as values.\nFinally, we can perform Window Attention to gather the\ncoarse-grained local features. Note that all the steps can be\nconducted in parallel on GPUs by assigning each querying\nvoxel vi a separate CUDA thread.\nThe SW A lacks information interaction across windows,\nwhich may limit the representation power of our model.\nThus, we extend the shifted 2D window mechanism of Swin\nTransformer [26] to 3D windows for the purpose of intro-\nducing cross-window information interaction while main-\ntaining the efÔ¨Åcient computation of non-overlapping SW A.\nReaders can refer to Swin Transformer for more details\nAs shown in Figure 1, with the cyclic shifted window\npartitioning approach, the step of feature aggregation of this\nbranch can be described as follows:\nAlgorithm 1 Pseudo-code for the V oxel Branch\nInput: P, an array with shape [B, N, C]. F, an array with\nshape [B, N, D].\nOutput: Flocal, an array with shape [B, N, D]\nF 1 = voxelize(P, F)#shape = [B, R3, D]\nF 1 = cyclic shift(F 1)\nF 2 = SW A(layernorm(F1)) + F 1\nF 2 = MLP(layernorm(F 2)) + F 2\nF 2 = reverse cyclic shift (F 2)\nF 3 = SW A(layernorm(F2)) + F 2\nFlocal = MLP(layernorm(F 3)) + F 3\n3.2. Point branch\nThe voxel branch gathers the neighborhood information\nwith low resolution. However, in order to capture long-\nrange dependencies, low-resolution voxel branch alone is\nlimited. To this end, we attempt to employ the following\nself-attention on the entire point cloud for global context\naggregation, which is computed as:\nFsa = softmax(QKT )V, F sa ‚ààRN√óD (3)\nFglobal = MLP (Fsa) + F, F global ‚ààRN√óD (4)\nwhere (Q, K, V) ‚àà RN√óD is generated by shared linear\ntransformations and the input features F and are all ordered\nindependent. Moreover, softmax and weighted sum are both\npermutation-independent operators. Thus, the self-attention\ncomputation is permutation-invariant, making it well-suited\nto handle the irregular, disordered 3D points.\nRelative Attention: Self-attention mentioned above\nfails to incorporate relative position representations in its\nstructure, whereas such ability is very important to 3D vi-\nsual tasks. For example, the absolute coordinates of the\nsame object can be completely different with rigid trans-\nformations. Therefore, injecting relative position represen-\ntations are generally more robust. In this study, we design\nour relative-attention to embed relative position representa-\ntions, which are not well studied in prior point cloud learn-\ning works.\nFirst of all, by embedding RPR into the scaled dot-\nproduct self-attention module, Eq 3 can be re-formulated\nas:\nFra = softmax(QKT + B)V (5)\nwhere B ‚ààRN√óN is the relative representations bias.\nSuppose that the original point cloud with N points is\ndenoted by P= {pi}N\ni=1 ‚äÜR3. We compute the relative\nposition B as follows:\nBpi,pj,m = pi,m ‚àípj,m, m ‚àà{x, y, z}. (6)\nTo map relative coordinates to the corresponding posi-\ntion encoding, we maintain three learnable look-up tables\ntx, ty, tz ‚ààRL√óN√óN corresponding to the x, y and z axis,\nrespectively. As the relative coordinates are continuous\nÔ¨Çoating-point numbers, we uniformly quantize the range of\nBpi,pj,m into L discrete parts and map the relative coordi-\nnates Bpi,pj,m to the indices of the tables as:\nidxi,j,m = ‚åäBpi,pj,m + smax\nsquad\n‚åã (7)\nwhere smax is the maximum size of point cloud coordinates\nand squad = 2¬∑smax\nL is the quantization size, and‚åä¬∑‚åãdenotes\nÔ¨Çoor rounding.\nWe look up the tables to retrieve corresponding embed-\nding with the index and sum them up to obtain the position\nencoding of\nBpi,pj =\n3‚àë\nm=1\ntm[idxi,j,m], (8)\nwhere t[idx] indicates the idx-th entry of the learnable\nlook-up table t, and Bpi,pj means the relative position en-\ncoding between pi and pj.\nExternal Attention: Although the RA module has\ndemonstrated signiÔ¨Åcant performance on small-scale point\nclouds, it is unsuitable for large-scale point clouds (e.g., Se-\nmanticKITTI [3]) due to its unacceptable O(N2) memory\nconsumption. Therefore, in this work, we perform Exter-\nnal Attention computation on large-scale point clouds. Ex-\nternal Attention, is a novel attention mechanism based on\ntwo external, small, learnable, shared memories, which can\nbe implemented easily by simply using two cascaded linear\nlayers and two normalization layers. It has linear complex-\nity and implicitly considers the correlations between all data\nsamples, making it suitable for large-scale point clouds.\n3.3. Feature fusion\nWe effectively fuse the outputs of two branches with an\naddition as they are providing complementary information:\nF‚Ä≤= Flocal + Fglobal, F ‚Ä≤‚ààRN√óD (9)\n3.4. Relationship to prior works\nThe proposed PVT is related to several prior works\nwhich includes PVCNN [27], PCT [14], PT1[9], PT2[65],.\nAlthough we are inspired by the idea of PVCNN [27],\nour PVT is different in several ways: 1) in thevoxel branch,\nPVCNN uses a 3D convolution to gather local information\nwhile we employ SW A computation within each local win-\ndow, which is highly efÔ¨Åcient. Per-layer complexity for dif-\nferent layer types are shown in Table 1, for large-scale point\nclouds, approximately 10% of the voxels are non-empty\nLayer Type Time Complexity per Layer\nThe- 3D Convolutions O(k ¬∑R3 ¬∑D2)\nvoxel- Window-attention O(v ¬∑R3 ¬∑D)\nbranch SW A O(r2 ¬∑v ¬∑R3 ¬∑D)\nThe- 1D Convolutions O(k ¬∑N ¬∑D2)\npoint- Relative-attention O(N2 ¬∑D)\nbranch External-attention O(N ¬∑D)\nTable 1. Per-layer complexity for different layer types. R is the\nresolution of voxels, v = W √óW √óW is the number of vox-\nels in a same 3D window, r denotes the proportion of non-empty\nvoxels in a local 3D window, N is the number of points, D is the\nrepresentation dimension, and k is the kernel size of convolutions.\n(r = 0.1) [51] which means that our SW A can save signif-\nicant computation power compared with window-attention.\n2) in the point branch, PVCNN uses 1D convolution, which\nis computation efÔ¨Åcient but lacks the global context model-\ning capability. By performing RA (or EA) computation on\nthe entire points, our method gathers the global modeling\npower.\nUnlike prior Transformer-based 3D models that need to\ngather the downsampled points and Ô¨Ånd the correspond-\ning neighbors by using expensive FPS and k-NN in point\ndomain, our approach does not require explicitly identify\nwhich point is the farthest and what are in the neighbor-\ning set. Instead, the voxel branch observes regular data and\nlearns to capture local features using SW A. Additionally,\nthe point branch only needs to perform RA (or EA) on the\nentire point cloud, which also does not require to Ô¨Ånd the\nneighboring points. Thus, our approach obtains highly efÔ¨Å-\nciency than PCT, PT1 and PT2 (see Table 4).\n4. Experiments\nWe now discuss the PVT that can be constructed from\nPVT blocks for different point cloud learning tasks: shape\nclassiÔ¨Åcation, and object and semantic segmentation. The\nperformance is quantitatively evaluated with four metrics,\nincluding overall accuracy (OA), average precision (AP),\nthe intersection over union (IoU), and mean IoU (mIoU).\nFor the sake of fair comparison with baselines, we report\nthe measured latency on a GTX 2080 GPU to reÔ¨Çect the ef-\nÔ¨Åciency but evaluate other indicators on a GTX 3090 GPU.\nModel. The architecture used for the segmentation task\nis shown in Figure 1. In our settings, dropout with keep\nprobability of 0.5 is used in the last two linear layers. All\nlayers include ReLU and batch normalization. In addition,\nfor other point cloud learning tasks, we use the similar ar-\nchitecture as in segmentation. Readers can refer to our\nsource code for more derails.\nBaselines. We select four models as the compari-\nson baselines, including the strong attention-based model\nPointASNL and the three powerful Transformer-based net-\nModel Input Points OA(%) Latency\nOA<92.5\nPointNet xyz 16 √ó1024 89.2 13.6ms\nPointNet++ [31] xyz,nor 16 √ó1024 91.9 35.3ms\nSO-Net [22] xyz,nor 8 √ó2048 90.9 ‚àí\nPointGrid [21] xyz,nor 16 √ó1021 92.0 ‚àí\nSpiderCNN [52] xyz,nor 8 √ó1024 92.4 82.6ms\nPointCNN [23] xyz 16 √ó1024 92.2 221.2ms\nPointWeb [64] xyz,nor 16 √ó1024 92.3 ‚àí\nPVCNN [27] xyz,nor 16 √ó1024 92.4 24.2ms\nOA>92.5\nKPConv [39] xyz 16 √ó6500 92.9 120.5ms\nDGCNN [45] xyz 16 √ó1024 92.9 85.8ms\nLDGCNN [59] xyz 16 √ó1024 92.7 ‚àí\nPointASNL [53] xyz,nor 16 √ó1024 93.2 923.6ms\nPT1 [9] xyz,nor 16 √ó1024 92.8 320.6ms\nPT2 [65] xyz,nor 8 √ó1024 93.7 530.2ms\nPCT [14] xyz 16 √ó1024 93.2 92.4ms\nPVT (Ours) xyz 16 √ó1024 93.7 45.5ms\nPVT (Ours) xyz,nor 16 √ó1024 94.1 48.6ms\nTable 2. Results on ModelNet40 [48]. Compared with previous\nTransformer-based models, our PVT achieves the state-of-the-art\naccuracy with 10√ómeasured speed-up on average.\nInput PT1 PT2 PCT Ours\n16√ó512 247.5 430.7 76.4 31.5\n16√ó1024 320.6 530.2 92.4 45.5\n8√ó2048 460.5 720.4 145.4 71.2\nTable 3. The speed comparison of different models when the num-\nber of input points varies.\nworks PT1, PT2, and PCT.\n4.1. Shape ClassiÔ¨Åcation\nData. We conduct the classiÔ¨Åcation experiment on the\nModelNet40 [47] dataset. ModelNet40 includes 12,311\nCAD models from 40 different object classes, in which\n9843 models are used for training and the rest for testing.\nWe follow the experimental conÔ¨Åguration of PointNet, i.e.,\nfor each model, we uniformly sample 1024 points with 3\nchannels (or 6) of spatial location (and normal) as input; the\npoint cloud is re-scaled to Ô¨Åt the unit sphere.\nSetting. We utilize random translation, random\nanisotropic scaling and random input dropout strategies to\naugment the input points data during training. During test-\ning, no data augmentation or voting methods were used. For\nclassiÔ¨Åcation on ModelNet40, the SGD optimizer was used\nfor 200 epochs with the batch size 32. We set the initial\nlearning rate to 0.01 and adopt a cosine annealing schedule\nto adjust the learning rate at every epoch.\nResults. The results are shown in Tables 2 and 3, we\nFigure 3. Accuracy of different Transformer-based methods over\ntime and epochs on ModelNet40. Our method performs the best\nin both equal-time and equal-epoch comparisons. The accuracy\nof 90% can be achieved not only after 8 epochs but also in the\nshortest training time.\nModel Params FLOPs SDA( %) OA( %)\nPointNet 3.47M 0.45G 0.0 89.2\nPointNet++(SSG) 1.48M 1.68G 43.5 90.7\nPointNet++(MSG) 1.74M 4.09G 47.6 91.9\nDGCNN 1.81M 2.43G 57.2 92.9\nPointASNL 3.98M 5.92G 39.8 93.1\nPT1 21.1M 5.05G 32.5 92.8\nPT2 9.14M 17.1G 65.4 93.7\nPCT 2.88M 2.17G 24.6 93.2\nPVT(Ours) 2.76M 1.93G 9.1 94.1\nTable 4. Computational resource requirements. SDA means the\nrate of total runtime on structuring the sparse data.\ncan see that our PVT outperforms most previous models.\nCompared with its baselines, such as PCT, PT 1 and PT2,\nPointASNL, our PVT not only achieves state-of-the-art ac-\ncuracy of 94.1%, but also has the best speed-accuracy trade-\noff (10√ófaster on average). Figure 3 also provides an ac-\ncuracy plot under equal-epoch setting. As can be seen, our\nmethod outperforms all Transformer-based methods, being\nthe fastest and most accurate towards convergence.\n4.2. Analysis of computational requirements\nNow, we analyze the computational requirements of\nPVT and several other baselines by comparing the Ô¨Çoat-\ning point operations required (FLOPs) and number of pa-\nrameters (Params) in Table 4. PVT has the lowest memory\nrequirements with only 2.45M parameters, and also puts a\nlow load on the processor of only 1.62G FLOPs, yet deliv-\ners highly accurate results of 94.1%. These characteristics\nmake it suitable for deployment on a mobile device. In ad-\ndition, PT2 has attained top accuracy on various point cloud\nlearning benchmarks, but its shortcomings of slow inference\ntime and high computing cost are also obvious. In the sum-\nmary in Table 4, PVT only spends 6.3% of the total runtime\non structuring the irregular data, which is much lower than\nprevious Transformer-based models. Overall, PVT has the\nbest accuracy and the lowest computational and memory re-\nquirements compared with its baselines.\n4.3. Object part segmentation\nData. The model is also evaluated on ShapeNet Parts\n[24]. It is composed of a total of 16,880 models (14,006\nmodels are used for training, the rest for testing), each of\nwhich has 2 to 6 parts and the whole dataset is labeled in\n50 different parts. A total 2048 points are sampled from\neach model as input and only few point sets have six labeled\nparts. We directly adopt the same train-test split as PointNet\nin our experiment.\nSetting. The same training setting as in our classiÔ¨Åca-\ntion task was adopted. For this task on ShapeNet Part, we\ntrain our model using the SGD optimizer for 200 epochs\nwith the batch size 16. The initial learning rate was set to\n0.1, with a cosine annealing schedule to adjust the learning\nrate at every epoch.\nResults. Table 5 lists the class-wise segmentation re-\nsults. Part-average IoU is used to evaluate our model, which\nis given both overall and for each object category. The re-\nsults show that our PVT makes an improvement of 2.9%\nover PointNet. Compared with all baselines, PVT attains\nthe top pIoU with 86.6%.\nVisualization. In Figure 4, we illustrate output features\nfrom the point and voxel branches respectively, where a\nwarmer color represents larger magnitude. As we can see,\nthe voxel branch captures large, continuous parts while the\npoint-based counterpart captures global shape details (e.g.,\ntable legs, airplane wings, and tail).\n4.4. Indoor Scene Segmentation\nData. To further assess our network, we conduct seman-\ntic segmentation task on S3DIS dataset [1], which includes\n273 million points from six indoor areas of three different\nbuildings. Each point is annotated with a semantic label\nfrom 13 classes‚Äîe.g., beam, bookcase, chair, column, and\nwindow. We follow [38] and [29] and uniformly sampled\npoints from blocks of area size 1m √ó1m, where each point\nis represented by a 9D vector (XYZ, RGB, and normalized\nspatial coordinates).\nSetting. During the training process, we generate\ntraining data by randomly sampling 4,096 points from each\nblock on-the-Ô¨Çy. Following [45], we utilize Area-5 as the\ntest scene and all the other areas for training. Note that the\nposition and RGB information of points are used to as the\ninput features. The same training setting as in our classiÔ¨Å-\ncation task is adopted, but requires 50 epochs with the batch\nsize 8 to fulÔ¨Ål the experiment.\nResults. The results are presented in Tables 6 and Table\n8. From Tables 6, we can see that our PVT attains mIoU of\n67.3%, which outperforms MLPs-based frameworks such\nas PointNet [29] and PointNet++ [31], graph-based meth-\nods such as DGCNN [45], attention-based models such as\nPointASNL [53]. Moreover, PVT attains mIoU of 69.2%\n(a) Top row: features aggregated from the voxel branch.\n(b) Bottom row: features extracted from the point branch.\nFigure 4. We demonstrate the output features extracted from two branches using Open3D [66]. The voxel branch focuses on the large,\ncontinuous parts, while the point-based captures the global shape details.\nModel pIoU Areo Bag Cap Car Chair Ear\nPhone\nGuitar Knife Lamp Laptop Motor Mug Pistol Rocket Skate\nBoard\nTable\n# Shapes 2690 76 55 898 3758 69 787 392 1547 451 202 184 283 66 152 5271\nPointNet 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\nP2Sequence 85.1 82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8\nPointASNL 86.1 84.1 84.7 87.9 79.7 92.2 73.7 91.0 87.2 84.2 95.8 74.4 95.2 81.0 63.0 76.3 83.2\nRS-CNN 86.2 83.5 84.8 88.8 79.6 91.2 81.1 91.6 88.4 86.0 96.1 73.7 94.1 83.4 60.5 77.7 83.6\nPT1 85.9 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí\nPCT 86.4 85.0 82.4 89.0 81.2 91.9 71.5 91.3 88.1 86.3 95.8 64.6 95.8 83.6 62.2 77.6 73.7\nPVT (Ours) 86.6 85.3 82.1 88.7 82.1 92.4 75.5 91.0 88.9 85.6 95.4 76.2 94.7 84.2 65.0 75.3 81.7\nTable 5. Results of part segmentation on ShapeNet. pIoU means part-average Intersection-over-Union.\nunder 6-fold cross-validation, substantially outperforming\nmost prior models.\nVisualization. Fig.5 shows the visualization of PVT‚Äôs\npredictions. The predictions of our network are very close\nto the ground truth. PVT captures detailed semantic struc-\nture in complex 3D scenes, which is important in our net-\nwork.\n4.5. Outdoor semantic Segmentation\nData. To further evaluate our network, we conduct out-\ndoor semantic segmentation task on SemanticKITTI [3]\ndataset, which includes 43,552 densely labeled LIDAR\nscans belonging to 21 sequences. Each scan is a large-\nscale point cloud with ‚àº 105 points and spanning up to\n160√ó160√ó20 m in 3D space. OfÔ¨Åcially, the sequences\n00 ‚àº07 and 09 ‚àº10 (19,130 scans) are used for train-\ning, the sequence 08 (4071 scans) for validation, and the\nsequences 11 ‚àº21 (20,351 scans) for online testing. The\nraw 3D points only have 3D coordinates without color in-\nformation.\nSetting. Based on UNet, we build our backbone network\nfor large-scale point clouds segmentation with a stem, four\ndown-sampling and four up-sampling stages, and the di-\nmensions of these nine stages are 32, 64, 64, 128, 256, 256,\n128, 64, and 64, respectively. As for the voxel branch, the\nvoxel resolution is 0.05 m for segmentation experiments.\nWe train PVT for 100 epochs on a single GeForce RTX\n3090 GPU with the batch size 8. In addition, the Adam opti-\nmizer is employed to minimize the overall loss; the learning\nrate starts from 0.01 and decays with a rate of 0.5 after every\n10 epochs.\nResults. As shown in Table 7, PVT outperforms the pre-\nvious state-of-the-art point-based model BAAF by 5.0% in\nmIoU with 48√ó measured speedup. Compared with strong\nattention-based PointASNL and point-based KPConv, our\nPVT achieves +18.1% and +6.1% mIoU improvements,\nwith 51√ó and 25√ó measured speedup respectively.\n5. Ablation Studies\nIn this section, we conduct extensive ablation study to\nanalyze the effectiveness of different components of PVT\nblock. The results of the ablation study are summarized in\nTables 9 and 10.\nImpact of the voxel-based and point-based branches.\nModel mIoU Ceiling Floor Wall Bean Column Window Door Chair Table Bookcase Sofa Board Clutter\nPointNet 41.09 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22\nPointNet++ 50.04 90.79 96.45 74.12 0.02 5.77 43.59 25.39 69.22 76.94 21.45 55.61 49.34 41.88\nPointNet++-CE 51.56 92.28 96.87 74.77 0.02 7.04 46.78 25.42 69.13 79.18 26.67 53.39 54.61 44.03\nDGCNN 47.08 92.42 97.46 76.03 0.37 12.00 51.59 27.01 64.85 68.58 7.67 43.76 29.44 40.83\nPVCNN 56.12 91.23 97.54 77.13 0.29 13.02 51.72 26.74 68.52 75.48 28.64 53.29 27.21 41.92\nBPM [12] 61.43 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí\nPointASNL [12]62.60 94.31 98.42 79.13 0.00 26.71 55.21 66.21 83.32 86.83 47.64 68.32 56.41 52.12\nIAF-Net [50] 64.60 91.41 98.60 81.80 0.00 34.90 62.00 54.70 79.70 86.90 49.90 72.40 74.80 52.10\nPVT(Ours) 68.21 91.18 98.76 86.23 0.31 34.21 49.90 61.45 81.62 89.85 48.20 79.96 76.45 54.67\nTable 6. Indoor scene segmentation results on the S3DIS dataset, evaluated on Area5. From this table, we can see that the proposed PVT\noutperforms most of previous 3D models in some categories signiÔ¨Åcantly,\nModel Latency mIoU\nCar\nBicycle\nMotorCycle\nTruck\nOther-vehicle\nPerson\nBicyclist\nMotorcyclist\nRoad\nParking\nSidewalk\nOther-ground\nBuilding\nFence\nvegetation\nTrunk\nTerrain\nPole\nTrafÔ¨Åc-sign\nPointNet‚àó 500 14.6 46.3 1.3 0.3 0.1 0.8 0.2 0.2 0.0 61.6 15.8 35.7 1.4 41.4 12.9 31.0 4.6 17.6 2.4 3.7\nPointNet++‚àó 5900 20.1 53.7 1.9 0.2 0.9 0.2 0.9 1.0 0.0 72.0 18.7 41.8 5.6 62.3 16.9 46.5 13.8 30.0 6.2 8.9\nPVCNN‚àó 146 39.0 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí\nTangentConv‚àó 3000 40.9 83.9 63.9 33.4 15.4 83.4 90.8 15.2 2.7 16.5 12.1 79.5 49.3 58.1 23.0 28.4 8.1 49.0 35.8 28.5\nPointASNL 7260 46.8 87.4 74.3 24.3 1.8 83.1 87.9 39.0 0.0 25.1 29.2 84.1 52.2 70.6 34.2 57.6 0.0 43.9 57.8 36.9\nRandLA-Net‚àó 880 53.9 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7\nKPConv‚àó 3560 58.8 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4\nBAAF 6880 59.9 90.9 74.4 62.2 23.6 89.8 95.4 48.7 31.8 35.5 46.7 82.7 63.4 67.9 49.5 55.7 53.0 60.8 53.7 52.0\nSPVCNN‚àó 110 63.7 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí\nPVT(Ours) 142 64.9 97.7 76.9 59.8 30.5 91.8 94.4 49.2 34.3 42.3 62.1 81.3 65.6 70.9 47.5 68.7 86.2 70.8 52.1 54.0\nTable 7. Results of outdoor scene segmentation on SemanticKITTI. ‚àó: results directly taken from [36]\nModel mIoU(%) OA(%) mAcc(%)\nPointNet 47.6 78.5 66.2\nG+RCU [10] 49.7 81.1 ‚àí\nTangentConv [37] 52.8 ‚àí ‚àí\nRSNet [28] 56.5 ‚àí 66.5\n3P-RNN [56] 56.3 86.9 ‚àí\nSGPN [44] 50.4 ‚àí ‚àí\nSPGraph [20] 62.1 85.5 73.0\nHAPGN [5] 62.9 85.8 ‚àí\nPVCNN 63.2 85.8 72.5\nShellNet [61] 66.8 87.1 ‚àí\nPVT(Ours) 69.2 88.3 76.2\nTable 8. Results of semantic segmentation of 3D indoor scenes on\nS3DIS, evaluated with 6-fold cross-validation. From this table, we\ncan see that the proposed PVT outperforms most of previous 3D\nmodels.\nWe set two baselines: A and B. Model A only encodes\nglobal context features by the point branch, and Model\nB only encodes local features by the voxel one. As re-\nported in Table 9, the Baseline model A obtains a low accu-\nracy of 92.8% on classiÔ¨Åcation benchmarks, and model B\ngets 92.3%. When we combine local and global features\n(PVTfull ), there is a notable improvement in both tasks.\nThis means our network can take advantage of the com-\nModel PB VB shifting NPB OA(%) Latency\nA % ! ! 3 92.8 33.5\nB ! % ! 3 92.3 25.2\nC ! ! % 3 93.0 47.9\nD ! ! ! 2 93.1 36.4\nE ! ! ! 4 93.6 72.5\nPVT full ! ! ! 3 94.1 48.6\nTable 9. Ablation study on ModelNet40. PB and VB mean the\npoint-based branch and the voxel-based branch; NPB denotes the\nnumber of PVT blocks; shifting means all self-attention modules\nadopt the cyclic shifted box partitioning method.\nbination of two branches, which provide richer information\nabout the points.\nEffect of the cyclic shifted windows scheme. Ablation\nof the shifted window method on classiÔ¨Åcation is reported\nin Table 9 (Model C). The network with the shifted win-\ndow partitioning (PVTfull ) outperforms the Model C with-\nout shifting at each layer by +1.0% OA on ModelNet40.\nThe results indicate the effectiveness and efÔ¨Åciency of us-\ning cyclic shifted window to build cross-window informa-\ntion interaction in the preceding layers.\nImpact of the number of PVT blocks. we validate the\nimpact of the PVT block by controlling the number of PVT\nFigure 5. Visualization of semantic segmentation results on the S3DIS dataset. The input is in the top row, PVT predictions on the middle,\nthe ground truth on the bottom.\nAblation ModelNet40(OA) ShapeNet(mIoU)\nMLP 92.6 85.3\nEdgeConv 93.1 85.8\nw/o rel. pos 93.2 86.3\nPVTfull 94.1 86.6\nTable 10. Ablation study on the relative-attention and relative po-\nsition bias on two benchmarks. MLP: replace relative-attention\nwith MLP layer in our architecture. EdgeConv: replace relative-\nattention with EdgeConv layer in our architecture. no rel. pos: the\nrelative attention without an additional relative position bias term\n(see Eq 5).\nblocks and report the results in Table 9. From this table,\nwe can conclude the following: on one hand, reducing the\nnumber of PVT blocks can save latency, for example, com-\npared with PVTfull , Model D saves 25% latency but incurs\na loss on accuracy; on the other hand, increasing the number\nof PVT blocks from PVT full can hardly support Model E\naccuracy beneÔ¨Åt but leads to an increase on latency. To bal-\nance between speed and accuracy, we adopt 3 PVT blocks\nas our full model.\nEffect of Relative-attention. We validate the impact of\nRA module used in our network. The results are shown in\nTable 10. We set two baselines. ‚ÄùMLP‚Äù is a no-attention\nbaseline that replaces RA with a MLP layer. ‚ÄùEdgeConv‚Äù\nis a more advanced no-attention baseline that replaces RA\nwith a EdgeConv layer. EdgeConv performs feature aggre-\ngating at each point and enables each point to exchange in-\nformation with its neighboring points, but does not lever-\nage attention mechanisms. We can see that the RA mod-\nule achieves better results than the no-attention baselines.\nThe performance gap between RA and MLP baselines is\nsigniÔ¨Åcant: 94.1% vs. 92.6% and 86.6% vs. 85.3%, an re-\nspectivel improvement of 1.5 and 1.3 absolute percentage\npoints. Compared with EdgeConv baseline, our RA module\nalso achieves improvements of 0.9 and 0.9 absolute percent-\nage points, respectively.\nEffect of relative position representations. Finally, we\nalso investigate the impact of RPR used in the RA mod-\nule. As demonstrated in Table 10, our PVT with RPR yields\n+0.8% OA/+0.4% mIoU on ModelNet40 and ShapeNet in\nrelation to those without this term respectively, indicating\nthe effectiveness of the RPR.\n6. Conclusion and Future Work\nIn this work, we present PVT for efÔ¨Åcient point cloud\nlearning. PVT is found to surpass previous Transformer-\nbased and attention-based models in efÔ¨Åciency. It achieves\nthis by deeply combining the advantages from both voxel-\nbased and point-based networks. To reduce the computation\ncost, we design a GPU-based SW A computing method that\nhas linear computational complexity with respect to voxel\nresolution and bypasses the invalid empty voxel computa-\ntions. In addition, we study two different self-attention vari-\nants to gather Ô¨Åne-grained features about the global shape\naccording to different scales of point clouds.\nIn the future, we expect to promote the primary struc-\nture for other research areas, such as point cloud pretrain-\ning, generation, and completion.\nReferences\n[1] I. Armeni, S. Sax, A. R. Zamir, and S. Savarese. Joint 2d-3d-\nsemantic data for indoor scene understanding.arXiv preprint\narXiv:1702.01105, 2017. 2, 7\n[2] Bahdanau, Dzmitry, K. Cho, and Y . Bengio. Neural machine\ntranslation by jointly learning to align and translate.in ICLR,\n2014. 3\n[3] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke,\nC. Stachniss, and J. Gall. Semantickitti: A dataset for se-\nmantic scene understanding of lidar sequences. In 2019\nIEEE/CVF International Conference on Computer Vision,\nICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 9296‚Äì9306. IEEE, 2019. 2, 5, 8\n[4] C. Chen, G. Li, R. Xu, T. Chen, M. Wang, and L. Lin.\nClusternet: Deep hierarchical cluster network with rigor-\nously rotation-invariant representation for point cloud analy-\nsis. Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020. 1\n[5] C. Chen, S. Qian, Q. Fang, and C. Xu. Hapgn: Hierarchi-\ncal attentive pooling graph network for point cloud segmen-\ntation. IEEE Transactions on Multimedia , 23:2335‚Äì2346,\n2021. 9\n[6] C. Choy, J. Y . Gwak, and S. Savarese. 4d spatio-temporal\nconvnets: Minkowski convolutional neural networks. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2019. 2\n[7] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova. Bert:\nPre-training of deep bidirectional transformers for language\nunderstanding. in NAACL-HLT, 2018. 3\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. in ICLR, 2021. 1, 3\n[9] N. Engel, V . Belagiannis, and K. Dietmayer. Point trans-\nformer. CoRR, abs/2011.00931, 2020. 1, 2, 3, 4, 5, 6\n[10] F. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe.\nExploring spatial context for 3d semantic segmentation of\npoint clouds. 2017 IEEE International Conference on Com-\nputer Vision Workshop (ICCVW), 2017. 9\n[11] F. Fuchs, D. E. Worrall, V . Fischer, and M. Welling. Se(3)-\ntransformers: 3d roto-translation equivariant attention net-\nworks. In Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, vir-\ntual, 2020. 3\n[12] J. Gong, J. Xu, X. Tan, J. Zhou, Y . Qu, Y . Xie, and L. Ma.\nBoundary-aware geometric encoding for semantic segmen-\ntation of point clouds. Proceedings of the AAAI Conference\non ArtiÔ¨Åcial Intelligence, 2021. 9\n[13] B. Graham, M. Engelcke, and L. van der Maaten. 3d se-\nmantic segmentation with submanifold sparse convolutional\nnetworks. In 2018 IEEE Conference on Computer Vision\nand Pattern Recognition, Salt Lake City, UT, USA, June 18-\n22, pages 9224‚Äì9232. Computer Vision Foundation / IEEE\nComputer Society, 2018. 4\n[14] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin,\nand S.-M. Hu. Pct: Point cloud transformer. Computational\nVisual Media, 7(2):187‚Äì199, Apr 2021. 1, 2, 3, 4, 5, 6\n[15] H. Hu, Z. Zhang, Z. Xie, and S. Lin. Local relation networks\nfor image recognition. IEEE/CVF International Conference\non Computer Vision (ICCV), 2019. 3\n[16] Q. Huang, W. Wang, and U. a. Neumann. Recurrent slice\nnetworks for 3d segmentation of point clouds. Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 2018. 2\n[17] R. Klokov and V . Lempitsky. Escape from cells: Deep kd-\nnetworks for the recognition of 3d point cloud models. Pro-\nceedings of the IEEE International Conference on Computer\nVision, pages 863‚Äì872, 2017. 2\n[18] S. Lan, R. Yu, G. Yu, and L. S. Davis. Modeling local geo-\nmetric structure of 3d point clouds using geo-cnn. Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019. 1\n[19] L. Landrieu and M. Boussaha. Point cloud oversegmentation\nwith graph-structured deep metric learning. Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020. 3\n[20] L. Landrieu and M. Simonovsky. Large-scale point cloud se-\nmantic segmentation with superpoint graphs. IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\n2018. 9\n[21] T. Le and D. Ye. Pointgrid: A deep network for 3d shape\nunderstanding. Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2018. 2, 6\n[22] J. Li, B. M. Chen, and G. H. Lee. So-net: Self-organizing\nnetwork for point cloud analysis. Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2018. 6\n[23] Y . Li, R. Bu, M. Sun, and B. Chen. Pointcnn: Convolution\non x-transformed points. Advances in Neural Information\nProcessing Systems (NeurIPS), 2018. 6\n[24] Y . Li, V . G. Kim, D. Ceylan, I. C. Shen, M. Yan, S. Hao,\nC. Lu, Q. Huang, A. Sheffer, and L. Guibas. A scalable ac-\ntive framework for region annotation in 3d shape collections.\nACM Transactions on Graphics (TOG) , 35(6cd):210.1‚Äì\n210.12, 2016. 2, 7\n[25] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou,\nand Y . Bengio. A structured self-attentive sentence embed-\nding. in ICLR, 2017. 3\n[26] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo. Swin transformer: Hierarchical vision transformer\nusing shifted windows. IEEE/CVF International Conference\non Computer Vision (ICCV), 2021. 1, 2, 3, 4\n[27] Z. Liu, H. Tang, Y . Lin, and S. Han. Point-voxel cnn for\nefÔ¨Åcient 3d deep learning. Advances in Neural Information\nProcessing Systems (NeurIPS), 2019. 1, 3, 4, 5, 6\n[28] R. Mehta and T. Arbel. Rs-net: Regression-segmentation 3d\ncnn for synthesis of full resolution missing brain mri in the\npresence of tumours. International Workshop on Simulation\nand Synthesis in Medical Imaging, pages 119‚Äì129, 2018. 9\n[29] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep\nlearning on point sets for 3d classiÔ¨Åcation and segmentation.\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 652‚Äì660, 2017. 2, 7\n[30] C. R. Qi, H. Su, M. Niebner, A. Dai, M. Yan, and L. J.\nGuibas. V olumetric and multi-view cnns for object classiÔ¨Å-\ncation on 3d data. Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2016. 1\n[31] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++:\nDeep hierarchical feature learning on point sets in a metric\nspace. Advances in Neural Information Processing Systems\n(NeurIPS), 2017. 6, 7\n[32] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Lev-\nskaya, and J. Shlens. Stand-alone self-attention in vision\nmodels. CoRR, abs/1906.05909, 2019. 3\n[33] G. Riegler, A. O. Ulusoys, and A. Geiger. Octnet: Learning\ndeep 3d representations at high resolutions. IEEE Computer\nSociety, 2016. 2\n[34] Q. Shi, S. Anwar, and N. Barnes. Semantic segmentation\nfor real point cloud scenes via bilateral augmentation and\nadaptive fusion. Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2021. 2\n[35] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and\nH. Li. Pv-rcnn: Point-voxel feature set abstraction for 3d\nobject detection. Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2019. 1\n[36] H. Tang, Z. Liu, S. Zhao, Y . Lin, J. Lin, H. Wang, and S. Han.\nSearching efÔ¨Åcient 3d architectures with sparse point-voxel\nconvolution. In European Conference on Computer Vision,\n2020. 9\n[37] M. Tatarchenko, J. Park, V . Koltun, and Q. Y . Zhou. Tan-\ngent convolutions for dense prediction in 3d. Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2018. 9\n[38] L. Tchapmi, C. Choy, I. Armeni, J. Y . Gwak, and S. Savarese.\nSegcloud: Semantic segmentation of 3d point clouds. 2017\nInternational Conference on 3D Vision (3DV), 2017. 7\n[39] H. Thomas, C. R. Qi, J. E. Deschaud, B. Marcotegui, and\nL. J. Guibas. Kpconv: Flexible and deformable convolu-\ntion for point clouds. IEEE/CVF International Conference\non Computer Vision (ICCV), 2019. 6\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2017. 1, 3, 4\n[41] C. Wang, B. Samari, and K. Siddiqi. Local spectral graph\nconvolution for point set feature learning. in ECCV, 2018. 1\n[42] L. Wang, Y . Huang, Y . Hou, S. Zhang, and J. Shan. Graph\nattention convolution for point cloud semantic segmentation.\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2019. 2\n[43] P. S. Wang, Y . Liu, Y . X. Guo, C. Y . Sun, and X. Tong.\nO-cnn: Octree-based convolutional neural networks for 3d\nshape analysis. Acm Transactions on Graphics , 36(4):72,\n2017. 2\n[44] W. Wang, R. Yu, Q. Huang, and U. Neumann. Sgpn: Sim-\nilarity group proposal network for 3d point cloud instance\nsegmentation. Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 2569‚Äì2578,\n2018. 9\n[45] Y . Wang, Y . Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and\nJ. M. Solomon. Dynamic graph cnn for learning on point\nclouds. ACM Transactions on Graphics, 38(5), 2018. 2, 6, 7\n[46] Z. Wang and F. Lu. V oxsegnet: V olumetric cnns for seman-\ntic part segmentation of 3d shapes. IEEE Transactions on\nVisualization and Computer Graphics, pages 1‚Äì1, 2019. 1\n[47] Z. Wu, S. Song, A. Khosla, X. Tang, and J. Xiao. 3d\nshapenets for 2.5d object recognition and next-best-view pre-\ndiction. Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2014. 2, 6\n[48] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and\nJ. Xiao. 3d shapenets: A deep representation for volumet-\nric shapes. 2015 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2015. 6\n[49] P. Xiang, X. Wen, Y . Liu, Y . Cao, P. Wan, W. Zheng,\nand Z. Han. SnowÔ¨Çakenet: Point cloud completion by\nsnowÔ¨Çake point deconvolution with skip-transformer. In\n2021 IEEE/CVF International Conference on Computer Vi-\nsion, ICCV 2021, Montreal, QC, Canada, October 10-17,\n2021, pages 5479‚Äì5489. IEEE, 2021. 3\n[50] M. Xu, Z. Zhou, J. Zhang, and Y . Qiao. Investigate in-\ndistinguishable points in semantic segmentation of 3d point\ncloud. Proceedings of the AAAI Conference on ArtiÔ¨Åcial In-\ntelligence, abs/2103.10339, 2021. 9\n[51] Q. Xu, X. Sun, C.-Y . Wu, P. Wang, and U. Neumann. Grid-\ngcn for fast and scalable point cloud learning.Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5661‚Äì5670, 2020. 3, 6\n[52] Y . Xu, T. Fan, M. Xu, Z. Long, and Q. Yu. Spidercnn: Deep\nlearning on point sets with parameterized convolutional Ô¨Ål-\nters. ECCV, 2018. 6\n[53] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui. Pointasnl: Ro-\nbust point clouds processing using nonlocal neural networks\nwith adaptive sampling. Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2020.\n1, 6, 7\n[54] Y . Yan, Y . Mao, and B. Li. Second: Sparsely embedded\nconvolutional detection. Sensors, 18(10), 2018. 2, 4\n[55] B. Yang, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham,\nand N. Trigoni. Learning object bounding boxes for 3d\ninstance segmentation on point clouds. Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019. 2\n[56] X. Ye, J. Li, H. Huang, L. Du, and X. Zhang. 3d recurrent\nneural networks with context fusion for point cloud semantic\nsegmentation. Proceedings of the European Conference on\nComputer Vision (ECCV), pages 403‚Äì417, 2018. 9\n[57] X. Yu, Y . Rao, Z. Wang, Z. Liu, J. Lu, and J. Zhou. Pointr:\nDiverse point cloud completion with geometry-aware trans-\nformers. In 2021 IEEE/CVF International Conference on\nComputer Vision, ICCV 2021, Montreal, QC, Canada, Octo-\nber 10-17, 2021, pages 12478‚Äì12487. IEEE, 2021. 3\n[58] X. Yu, L. Tang, Y . Rao, T. Huang, J. Zhou, and J. Lu. Point-\nbert: Pre-training 3d point cloud transformers with masked\npoint modeling. CoRR, abs/2111.14819, 2021. 3\n[59] K. Zhang, M. Hao, J. Wang, C. D. Silva, and C. Fu. Linked\ndynamic graph cnn: Learning on point cloud via linking hi-\nerarchical features. arXiv:1904.10014 [cs], 2019. 2, 6\n[60] W. Zhang and C. Xiao. Pcan: 3d attention map learning\nusing contextual information for point cloud based retrieval.\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2019. 2\n[61] Z. Zhang, B. S. Hua, and S. K. Yeung. Shellnet: EfÔ¨Åcient\npoint cloud convolutional neural networks using concentric\nshells statistics. 2019 IEEE/CVF International Conference\non Computer Vision (ICCV), 2019. 9\n[62] H. Zhao, J. Jia, and V . K. and. Exploring self-attention for\nimage recognition. Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2020. 3\n[63] H. Zhao, L. Jiang, C. W. Fu, and J. Jia. Pointweb: Enhancing\nlocal neighborhood features for point cloud processing. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2019. 2\n[64] H. Zhao, L. Jiang, C.-W. Fu, and J. Jia. Pointweb: Enhancing\nlocal neighborhood features for point cloud processing. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5565‚Äì5573, 2019. 6\n[65] H. Zhao, L. Jiang, J. Jia, P. Torr, and V . Koltun. Point trans-\nformer. IEEE/CVF International Conference on Computer\nVision (ICCV), 2021. 1, 2, 4, 5, 6\n[66] Q.-Y . Zhou, J. Park, and V . Koltun. Open3D: A modern li-\nbrary for 3D data processing. arXiv:1801.09847, 2018. 8\n[67] Y . Zhou and O. Tuzel. V oxelnet: End-to-end learning for\npoint cloud based 3d object detection. IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 4490‚Äì\n4499, 2018. 1",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7709136009216309
    },
    {
      "name": "Speedup",
      "score": 0.6722093820571899
    },
    {
      "name": "Computation",
      "score": 0.6693804264068604
    },
    {
      "name": "Voxel",
      "score": 0.6362451910972595
    },
    {
      "name": "Discriminative model",
      "score": 0.6348599195480347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6218268871307373
    },
    {
      "name": "Deep learning",
      "score": 0.5152931213378906
    },
    {
      "name": "Transformer",
      "score": 0.47561556100845337
    },
    {
      "name": "Segmentation",
      "score": 0.4722651243209839
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43116921186447144
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4291146397590637
    },
    {
      "name": "Machine learning",
      "score": 0.3477836847305298
    },
    {
      "name": "Algorithm",
      "score": 0.2945328950881958
    },
    {
      "name": "Parallel computing",
      "score": 0.1620272397994995
    },
    {
      "name": "Engineering",
      "score": 0.0868818461894989
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50760025",
      "name": "Hangzhou Dianzi University",
      "country": "CN"
    }
  ]
}