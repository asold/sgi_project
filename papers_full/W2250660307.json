{
  "title": "Deep Neural Language Models for Machine Translation",
  "url": "https://openalex.org/W2250660307",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A2251056627",
      "name": "Thang Luong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2497596190",
      "name": "Michael Kayser",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149153931",
      "name": "Christopher D. Manning",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2072128103",
    "https://openalex.org/W4231109964",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2083545877",
    "https://openalex.org/W932413789",
    "https://openalex.org/W2184045248",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2250612599",
    "https://openalex.org/W2118434577",
    "https://openalex.org/W30655990",
    "https://openalex.org/W2171421863",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2171928131"
  ],
  "abstract": "Neural language models (NLMs) have been able to improve machine translation (MT) thanks to their ability to generalize well to long contexts.Despite recent successes of deep neural networks in speech and vision, the general practice in MT is to incorporate NLMs with only one or two hidden layers and there have not been clear results on whether having more layers helps.In this paper, we demonstrate that deep NLMs with three or four layers outperform those with fewer layers in terms of both the perplexity and the translation quality.We combine various techniques to successfully train deep NLMs that jointly condition on both the source and target contexts.When reranking nbest lists of a strong web-forum baseline, our deep models yield an average boost of 0.5 TER / 0.5 BLEU points compared to using a shallow NLM.Additionally, we adapt our models to a new sms-chat domain and obtain a similar gain of 1.0 TER / 0.5 BLEU points. 1",
  "full_text": "Proceedings of the 19th Conference on Computational Language Learning, pages 305–309,\nBeijing, China, July 30-31, 2015.c⃝2015 Association for Computational Linguistics\nDeep Neural Language Models for Machine Translation\nMinh-Thang Luong Michael Kayser Christopher D. Manning\nComputer Science Department, Stanford University, Stanfo rd, CA, 94305\n{lmthang, mkayser, manning}@stanford.edu\nAbstract\nNeural language models (NLMs) have\nbeen able to improve machine translation\n(MT) thanks to their ability to generalize\nwell to long contexts. Despite recent suc-\ncesses of deep neural networks in speech\nand vision, the general practice in MT\nis to incorporate NLMs with only one or\ntwo hidden layers and there have not been\nclear results on whether having more lay-\ners helps. In this paper, we demonstrate\nthat deep NLMs with three or four lay-\ners outperform those with fewer layers in\nterms of both the perplexity and the trans-\nlation quality. We combine various tech-\nniques to successfully train deep NLMs\nthat jointly condition on both the source\nand target contexts. When reranking n-\nbest lists of a strong web-forum baseline,\nour deep models yield an average boost\nof 0.5 T ER / 0.5 B LEU points compared\nto using a shallow NLM. Additionally, we\nadapt our models to a new sms-chat do-\nmain and obtain a similar gain of 1.0 T ER\n/ 0.5 B LEU points.1\n1 Introduction\nDeep neural networks (DNNs) have been success-\nful in learning more complex functions than shal-\nlow ones (Bengio, 2009) and exceled in many\nchallenging tasks such as in speech (Hinton et al.,\n2012) and vision (Krizhevsky et al., 2012). These\nresults have sparked interest in applying DNNs\nto natural language processing problems as well.\nSpeciﬁcally, in machine translation (MT), there\n1Our code and related materials are publicly available at\nhttp://stanford.edu/˜lmthang/nlm.\nhas been an active body of work recently in uti-\nlizing neural language models (NLMs) to improve\ntranslation quality. However, to the best of our\nknowledge, work in this direction only makes use\nof NLMs with either one or two hidden layers. For\nexample, Schwenk (2010, 2012) and Son et al.\n(2012) used shallow NLMs with a single hidden\nlayer for reranking. Vaswani et al. (2013) consid-\nered two-layer NLMs for decoding but provided\nno comparison among models of various depths.\nDevlin et al. (2014) reported only a small gain\nwhen decoding with a two-layer NLM over a sin-\ngle layer one. There have not been clear results on\nwhether adding more layers to NLMs helps.\nIn this paper, we demonstrate that deep NLMs\nwith three or four layers are better than those\nwith fewer layers in terms of the perplexity and\nthe translation quality. We detail how we com-\nbine various techniques from past work to suc-\ncessfully train deep NLMs that condition on both\nthe source and target contexts. When reranking n-\nbest lists of a strong web-forum MT baseline, our\ndeep models achieve an additional improvement\nof 0.5 T ER / 0.5 B LEU compared to using a shal-\nlow NLM. Furthermore, by ﬁne-tuning general in-\ndomain NLMs with out-of-domain data, we obtain\na similar boost of 1.0 T ER / 0.5 B LEU points over\na strong domain-adapted sms-chat baseline com-\npared to utilizing a shallow NLM.\n2 Neural Language Models\nWe brieﬂy describe the NLM architecture and\ntraining objective used in this work as well as com-\npare our approach to other related work.\nArchitecture. Neural language models are fun-\ndamentally feed-forward networks as described in\n(Bengio et al., 2003), but not necessarily lim-\nited to only a single hidden layer. Like any\n305\nother language model, NLMs specify a distribu-\ntion, p(w|c), to predict the next word w given a\ncontext c. The ﬁrst step is to lookup embeddings\nfor words in the context and concatenate them to\nform an input, h(0), to the ﬁrst hidden layer. We\nthen repeatedly build up hidden representations as\nfollows, for l = 1, . . . , n :\nh(l) = f\n(\nW (l)h(l−1) + b(l)\n)\n(1)\nwhere f is a non-linear fuction such as tanh. The\npredictive distribution, p(w|c), is then derived us-\ning the standard softmax:\ns = W (sm)h(n) + b(sm)\np(w|c) = exp(sw)\n∑\nw∈V exp(sw)\n(2)\nObjective. The typical way of training NLMs is\nto maximize the training data likelihood, or equiv-\nalently, to minimize the cross-entropy objective of\nthe following form: ∑\n(c,w)∈T − log p(w|c).\nTraining NLMs can be prohibitively slow due\nto the computationally expensive softmax layer.\nAs a result, past works have tried to use a more\nefﬁcient version of the softmax such as the hi-\nerarchical softmax (Morin, 2005; Mnih and Hin-\nton, 2007; Mnih and Hinton, 2009) or the class-\nbased one (Mikolov et al., 2010; Mikolov et al.,\n2011). Recently, the noise-contrastive estimation\n(NCE) technique (Gutmann and Hyv¨ arinen, 2012)\nhas been applied to train NLMs in (Mnih and Teh,\n2012; Vaswani et al., 2013) to avoid explicitly\ncomputing the normalization factors.\nDevlin et al. (2014) used a modiﬁed version\nof the cross-entropy objective, the self-normalized\none. The idea is to not only improve the predic-\ntion, p(w|c), but also to push the normalization\nfactor per context, Zc, close to 1:\nJ =\n∑\n(c,w)∈T\n− log p(w|c) +α log2(Zc) (3)\nWhile self-normalization does not lead to speed up\nin training, it allows trained models to be applied\nefﬁciently at test time without computing the nor-\nmalization factors. This is similar in ﬂavor to NCE\nbut allows for ﬂexibility (through α) in how hard\nwe want to “squeeze” the normalization factors.\nTraining deep NLMs. We follow (Devlin et al.,\n2014) to train self-normalized NLMs, condition-\ning on both the source and target contexts. Unlike\n(Devlin et al., 2014), we found that using the recti-\nﬁed linear function, max{0, x }, proposed in (Nair\nand Hinton, 2010), works better than tanh. The\nrectiﬁed linear function was used in (Vaswani et\nal., 2013) as well. Furthermore, while these works\nuse a ﬁxed learning rate throughout, we found that\nhaving a simple learning rate schedule is useful\nin training well-performing deep NLMs. This has\nalso been demonstrated in (Sutskever et al., 2014;\nLuong et al., 2015) and is detailed in Section 3.\nWe do not perform any gradient clipping and no-\ntice that learning is more stable when short sen-\ntences of length less than or equal to 2 are re-\nmoved. Bias terms are used for all hidden layers\nas well as the softmax layer as described earlier,\nwhich is slightly different from other work such as\n(Vaswani et al., 2013). All these details contribute\nto our success in training deep NLMs.\nFor simplicity, the same vocabulary is used for\nboth the embedding and the softmax matrices. 2 In\naddition, we adopt the standard softmax to take ad-\nvantage of GPUs in performing large matrix mul-\ntiplications. All hyperparameters are given later.\n3 Experiments\n3.1 Data\nWe use the Chinese-English bitext in the DARPA\nBOLT (Broad Operational Language Translation)\nprogram, with 11.1M parallel sentences (281M\nChinese words and 307M English words). We re-\nserve 585 sentences for validation, i.e., choosing\nhyperparameters, and 1124 sentences for testing. 3\n3.2 NLM Training\nWe train our NLMs described in Section 2 with\nSGD, using: (a) a source window of size 5, i.e.,\n11-gram source context 4, (b) a 4-word target his-\ntory, i.e., 5-gram target LM, (c) a self-normalized\nweight α = 0.1, (d) a mini-batch of size 128, and\n(e) a learning rate of 0.1 (training costs are nor-\nmalized by the mini-batch size). All weights are\nuniformly initialized in [−0.01, 0.01]. We train\nour models for 4 epochs (after 2 epochs, the learn-\ning rate is halved every 0.5 epoch). The vocab-\nularies are limited to the top 40K frequent words\nfor both Chinese and English. All words not in\n2Some work (Schwenk, 2010; Schwenk et al., 2012) uti-\nlize a smaller softmax vocabulary, called short-list.\n3The test set is from BOLT and labeled as p1r6 dev.\n4We used an alignment heuristic similar to Devlin et al.\n(2014) but applicable to our phrase-based MT system.\n306\nModels Valid Test | log Z|\n1 layer 9.39 8.99 0.51\n2 layers 9.20 8.96 0.50\n3 layers 8.64 8.13 0.43\n4 layers 8.10 7.71 0.35\nTable 1: Training NLMs – validation and test\nperplexities achieved by self-normalized NLMs of\nvarious depths. We report the | log Z| value (base\ne), similar to Devlin et al. (2014), to indicate how\ngood each model is in pushing the log normaliza-\ntion factors towards 0. All perplexities are derived\nby explicitly computing the normalization factors.\nthese vocabularies are replaced by a universal un-\nknown symbol. Embeddings are of size 256 and\nall hidden layers have 512 units each. Our train-\ning speed on a single Tesla K40 GPU device is\nabout 1000 target words per second and it gener-\nally takes about 10-14 days to fully train a model.\nWe present the NLM training results in Table 1.\nWith more layers, the model succeeds in learning\nmore complex functions; the prediction, hence,\nbecomes more accurate as evidenced by smaller\nperplexities for both the validation and test sets.\nInterestingly, we observe that deeper nets can learn\nself-normalized NLMs better: the mean log nor-\nmalization factor, | log Z| in Eq. (3), is driven to-\nwards 0 as the depth increases. 5\n3.3 MT Reranking with NLMs\nOur MT models are built using the Phrasal MT\ntoolkit (Cer et al., 2010). In addition to the stan-\ndard dense feature set 6, we include a variety of\nsparse features for rules, word pairs, and word\nclasses, as described in (Green et al., 2014). Our\ndecoder uses three language models. 7 We use a\ntuning set of 396K words in the newswire and web\ndomains and tune our systems using online ex-\npected error rate training as in (Green et al., 2014).\nOur tuning metric is (B LEU -TER)/2.\nWe run a discriminative reranker on the 1000-\nbest output of a decoder with MERT. The features\nused in reranking include all the dense features,\n5As a reference point, though not directly comparable,\nDevlin et al. (2014) achieved 0.68 for | log Z| on a different\ntest set with the same self-normalized constant α= 0.1.\n6Consisting of forward and backward translation mod-\nels, lexical weighting, linear distortion, word penalty, p hrase\npenalty and language model.\n7One is trained on the English side of the bitext, one\nis trained on a 16.3-billion-word monolingual corpus taken\nfrom various domains, and one is a class-based language\nmodel trained on the same large monolingual corpus.\nSystem dev test1 test2\nT↓ B↑ T↓ B↑ T↓ B↑\nbaseline 53.7 33.1 55.1 31.3 63.5 16.5\nReranking\n1 layer 52.9 34.3 54.5 32.0 63.1 16.7\n2 layers 52.7 34.1 54.3 31.9 63.0 16.8\n3 layers 52.5 34.5 54.3 32.3 62.5 17.3\n4 layers 52.6 34.7 54.1 32.4 62.7 17.2\nvs. baseline +1.2 † +1.6† +1.0† +1.1† +1.0† +0.8†\nvs. 1 layer +0.4 † +0.4† +0.4† +0.4† +0.6† +0.6†\nTable 2: Web-forum Results – T ER (T)\nand B LEU (B) scores on both the dev set\n(dev10wb dev), used to tune reranking weights,\nand the test sets (dev10wb syscomtune and\np1r6 dev accordingly). Relative improvements\nbetween the best system and the baseline as well\nas the 1-layer model are bolded. † marks improve-\nments that are statistically signiﬁcant ( p< 0.05).\nan aggregate decoder score, and an NLM score.\nWe learn the reranker weights on a second tuning\nset, different from the decoder tuning set, to make\nthe reranker less biased towards the dense features.\nThis second tuning set consists of 33K words of\nweb-forum text and is important to obtain good\nimprovements with reranking.\n3.4 Results\nAs shown in Table 2, it is not obvious if the depth-\n2 model is better than the single layer one, both\nof which are what past work used. In contrast,\nreranking with deep NLMs of three or four lay-\ners are clearly better, yielding average improve-\nments of 1.0 T ER / 1.0 B LEU points over the base-\nline and 0.5 T ER / 0.5 B LEU points over the sys-\ntem reranked with the 1-layer model, all of which\nare statisﬁcally signiﬁcant according to the test de-\nscribed in (Riezler and Maxwell, 2005).\nThe fact that the improvements in terms of the\nintrinsic metrics listed in Table 1 do translate into\ngains in translation quality is interesting. It rein-\nforces the trend reported in (Luong et al., 2015)\nthat better source-conditioned perplexities lead to\nbetter translation scores. This phenomon is a use-\nful result as in the past, many intrinsic metrics,\ne.g., alignment error rate, do not necessarily cor-\nrelate with MT quality metrics.\n3.5 Domain Adaptation\nFor the sms-chat domain, we use a tune set of\n260K words in the newswire, web, and sms-chat\ndomains to tune the decoder weights and a sepa-\n307\nSystem dev test\nT↓ B↑ T↓ B↑\nbaseline 62.2 18.7 57.3 23.3\nReranking\n1 layer (5.42, 0.51) 60.1 22.0 56.2 25.9\n2 layers (5.50, 0.51) 60.7 21.5 56.3 26.0\n3 layers (5.34 0.43) 59.9 21.4 55.2 26.4\nvs. baseline +2.3 ‡ +3.3‡ +2.1‡ +3.1‡\nvs. 1 layer +0.2 -0.6 +1.0 ‡ +0.5\nTable 3: Domain-adaptation Results – transla-\ntion scores for the sms-chat domain similar to\nTable 2. We use p2r2smscht\ndev for dev and\np2r2smscht syscomtune for test. The test perplex-\nities and the | log Z| values of our domain-adapted\nNLMs are shown in italics. ‡ marks improvements\nthat are statistically signiﬁcant ( p< 0.01).\nrate small, 8K words set to tune reranking weights.\nTo train adapted NLMs, we use models previously\ntrained on general in-domain data and further ﬁne-\ntune with out-domain data for about 4 hours. 8\nSimilar to the web-forum domain, for sms-chat,\nTable 3 shows that on the test set, our deep NLM\nwith three layers yields a signiﬁcant gain of 2.1\nTER / 3.1 B LEU points over the baseline and 1.0\nTER / 0.5 B LEU points over the 1-layer reranked\nsystem. It is worth pointing out that for such a\nsmall amount of out-domain training data, depth\nbecomes less effective as exhibited through the in-\nsigniﬁcant BLEU gain in test and a drop in dev\nwhen comparing between the 1- and 3-layer mod-\nels. We exclude the 4-layer NLM as it seems to\nhave overﬁtted the training data. Nevertheless, we\nstill achieve decent gains in using NLMs for MT\ndomain adaptation.\n4 Analysis\n4.1 NLM Training\nWe show in Figure 1 the learning curves for vari-\nous NLMs, demonstrating that deep nets are better\nthan the shallow NLM with a single hidden layer.\nStarting from minibatch 20K, the ranking is gen-\nerally maintained that deeper NLMs have better\ncross-entropies. The gaps become less discernible\nfrom minibatch 30K onwards, but numerically, as\nthe model becomes deeper, the average gaps, in\nperplexities, are consistently 40.1, 1.1, and 2.0.\n8Our sms-chat corpus consists of 146K sentences (1.6M\nChinese and 1.9M English words). We randomly select 3000\nsentences for validation and 3000 sentences for test. Model s\nare trained for 8 iterations with the same hyperparameters.\n1 2 3 4 5 6 7 8 9 10\nx 10\n4\n3\n3.5\n4\n4.5\n5\n5.5\n6\n6.5\nMini−batches\nCross−Entropy\t\t\t\t\n \n \n1 layer\n2 layers\n3 layers\n4 layers\nFigure 1: NLM Learning Curve – test cross-\nentropies ( loge perplexities) for various NLMs.\n4.2 Reranking Settings\nIn Table 4, we compare reranking using all dense\nfeatures ( All) to conditions which use only dense\nLM features ( LM) and optionally, include a word\npenalty (WP) feature. All these settings include\nan NLM score and an aggregate decoder score. As\nshown, it is best to include all dense features at\nreranking time.\nAll LMs + WP LMs\n1 layer 11.3 11.3 11.4\n2 layers 11.2 11.4 11.5\n3 layers 11.0 11.1 11.4\n4 layers 10.9 11.2 11.3\nTable 4: Reranking Conditions– (T ER-BLEU )/2\nscores when reranking the web-forum baseline.\n5 Related Work\nIt is worth mentioning another active line of re-\nsearch in building end-to-end neural MT systems\n(Kalchbrenner and Blunsom, 2013; Sutskever et\nal., 2014; Bahdanau et al., 2015; Luong et al.,\n2015; Jean et al., 2015). These methods have\nnot yet demonstrated success on challenging lan-\nguage pairs such as English-Chinese. Arsoy et al.\n(2012) have preliminarily examined deep NLMs\nfor speech recognition, however, we believe, this\nis the ﬁrst work that puts deep NLMs into the con-\ntext of MT.\n6 Conclusion\nIn this paper, we have bridged the gap that past\nwork did not show, that is, neural language mod-\nels with more than two layers can help improve\ntranslation quality. Our results conﬁrm the trend\nreported in (Luong et al., 2015) that source-\nconditioned perplexity strongly correlates with\nMT performance. We have also demonstrated the\n308\nuse of deep NLMs to obtain decent gains in out-\nof-domain conditions.\nAcknowledgment\nWe gratefully acknowledge support from a gift\nfrom Bloomberg L.P. and from the Defense\nAdvanced Research Projects Agency (DARPA)\nBroad Operational Language Translation (BOLT)\nprogram under contract HR0011-12-C-0015\nthrough IBM. Any opinions, ﬁndings, and con-\nclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not\nnecessarily reﬂect the view of DARPA, or the US\ngovernment. We thank members of the Stanford\nNLP Group as well as the annonymous reviewers\nfor their valuable comments and feedbacks.\nReferences\nEbru Arsoy, Tara N. Sainath, Brian Kingsbury, and\nBhuvana Ramabhadran. 2012. Deep neural network\nlanguage models. In NAACL WLM Workshop.\nD. Bahdanau, K. Cho, and Y . Bengio. 2015. Neural\nmachine translation by jointly learning to align and\ntranslate. In ICLR.\nY . Bengio, R. Ducharme, P. Vincent, and C. Jan-\nvin. 2003. A neural probabilistic language model.\nJMLR, 3:1137–1155.\nYoshua Bengio. 2009. Learning deep architectures for\nai. Foundations and Trends in Machine Learning,\n2(1):1–127, January.\nD. Cer, M. Galley, D. Jurafsky, and C. D. Manning.\n2010. Phrasal: A statistical machine translation\ntoolkit for exploring new model features. In ACL,\nDemonstration Session.\nJ. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz,\nand J. Makhoul. 2014. Fast and robust neural net-\nwork joint models for statistical machine translation.\nIn ACL.\nS. Green, D. Cer, and C. D. Manning. 2014. An em-\npirical comparison of features and tuning for phrase-\nbased machine translation. In WMT.\nMichael Gutmann and Aapo Hyv¨ arinen. 2012. Noise-\ncontrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics.\nJMLR, 13:307–361.\nG. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury. 2012. Deep neural\nnetworks for acoustic modeling in speech recogni-\ntion. IEEE Signal Processing Magazine.\nS´ ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large tar-\nget vocabulary for neural machine translation. In\nACL.\nN. Kalchbrenner and P. Blunsom. 2013. Recurrent\ncontinuous translation models. In EMNLP.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. 2012.\nImageNet classiﬁcation with deep convolutional\nneural networks. In NIPS.\nM.-T. Luong, I. Sutskever, Q. V . Le, O. Vinyals, and\nW. Zaremba. 2015. Addressing the rare word prob-\nlem in neural machine translation. In ACL.\nTomas Mikolov, Martin Karaﬁ´ at, Lukas Burget, Jan\nCernock´ y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Inter-\nspeech.\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan\nCernock´ y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn ICASSP.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn ICML.\nAndriy Mnih and Geoffrey Hinton. 2009. A scalable\nhierarchical distributed language model. In NIPS.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic\nlanguage models. In ICML.\nFrederic Morin. 2005. Hierarchical probabilistic neu-\nral network language model. In AISTATS.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed\nlinear units improve restricted boltzmann machines.\nIn ICML.\nStefan Riezler and T. John Maxwell. 2005. On some\npitfalls in automatic evaluation and signiﬁcance test-\ning for MT. In ACL Workshop, Intrinsic/Extrinsic\nEvaluation Measures for MT and Summarization.\nH. Schwenk, A. Rousseau, and M. Attik. 2012. Large,\npruned or continuous space language models on a\ngpu for statistical machine translation. In NAACL\nWLM workshop.\nH. Schwenk. 2010. Continuous space language mod-\nels for statistical machine translation. The Prague\nBulletin of Mathematical Linguistics, (93):137–146.\nLe Hai Son, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012. Continuous space translation models with\nneural networks. In NAACL-HLT.\nI. Sutskever, O. Vinyals, and Q. V . Le. 2014. Sequence\nto sequence learning with neural networks. In NIPS.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale\nneural language models improves translation. In\nEMNLP.\n309",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9597510695457458
    },
    {
      "name": "Machine translation",
      "score": 0.8555278778076172
    },
    {
      "name": "Computer science",
      "score": 0.8252041935920715
    },
    {
      "name": "BLEU",
      "score": 0.7696372866630554
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7122671604156494
    },
    {
      "name": "Deep neural networks",
      "score": 0.6710488200187683
    },
    {
      "name": "Translation (biology)",
      "score": 0.635472297668457
    },
    {
      "name": "Language model",
      "score": 0.5952993631362915
    },
    {
      "name": "Deep learning",
      "score": 0.5422552227973938
    },
    {
      "name": "Artificial neural network",
      "score": 0.5157037377357483
    },
    {
      "name": "Natural language processing",
      "score": 0.4862784147262573
    },
    {
      "name": "Baseline (sea)",
      "score": 0.41489723324775696
    },
    {
      "name": "Speech recognition",
      "score": 0.3488966226577759
    },
    {
      "name": "Machine learning",
      "score": 0.3273889422416687
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}