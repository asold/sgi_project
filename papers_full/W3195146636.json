{
  "title": "A Transformer Architecture for Stress Detection from ECG",
  "url": "https://openalex.org/W3195146636",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4282408320",
      "name": "Behinaein, Behnam",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A4282408319",
      "name": "Bhatti, Anubhav",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A4287167921",
      "name": "Rodenburg, Dirk",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A4227576988",
      "name": "Hungler, Paul",
      "affiliations": [
        "Queen's University"
      ]
    },
    {
      "id": "https://openalex.org/A3179272068",
      "name": "Etemad, Ali",
      "affiliations": [
        "Queen's University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3080108059",
    "https://openalex.org/W2563836857",
    "https://openalex.org/W2562037482",
    "https://openalex.org/W2785419426",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W2522453581",
    "https://openalex.org/W2026891775",
    "https://openalex.org/W2984017204",
    "https://openalex.org/W3127637041",
    "https://openalex.org/W2584561145",
    "https://openalex.org/W2978697615",
    "https://openalex.org/W3005055041",
    "https://openalex.org/W2980927909",
    "https://openalex.org/W2894771803",
    "https://openalex.org/W2767018745",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Electrocardiogram (ECG) has been widely used for emotion recognition. This paper presents a deep neural network based on convolutional layers and a transformer mechanism to detect stress using ECG signals. We perform leave-one-subject-out experiments on two publicly available datasets, WESAD and SWELL-KW, to evaluate our method. Our experiments show that the proposed model achieves strong results, comparable or better than the state-of-the-art models for ECG-based stress detection on these two datasets. Moreover, our method is end-to-end, does not require handcrafted features, and can learn robust representations with only a few convolutional blocks and the transformer component.",
  "full_text": "A Transformer Architecture for Stress Detection from ECG\nBehnam Behinaein\nDepartment of Electrical and\nComputer Engineering,\nIngenuity Labs Research Institute\nQueenâ€™s University, Kingston, Canada\n9hbb@queensu.ca\nAnubhav Bhatti\nDepartment of Electrical and\nComputer Engineering,\nIngenuity Labs Research Institute\nQueenâ€™s University, Kingston, Canada\nanubhav.bhatti@queensu.ca\nDirk Rodenburg\nIngenuity Labs Research Institute\nQueenâ€™s University, Kingston, Canada\ndjr08@queensu.ca\nPaul Hungler\nIngenuity Labs Research Institute\nQueenâ€™s University, Kingston, Canada\npaul.hungler@queensu.ca\nAli Etemad\nDepartment of Electrical and\nComputer Engineering,\nIngenuity Labs Research Institute\nQueenâ€™s University, Kingston, Canada\nali.etemad@queensu.ca\nABSTRACT\nElectrocardiogram (ECG) has been widely used for emotion recog-\nnition. This paper presents a deep neural network based on convo-\nlutional layers and a transformer mechanism to detect stress using\nECG signals. We perform leave-one-subject-out experiments on\ntwo publicly available datasets, WESAD and SWELL-KW, to evalu-\nate our method. Our experiments show that the proposed model\nachieves strong results, comparable or better than the state-of-the-\nart models for ECG-based stress detection on these two datasets.\nMoreover, our method is end-to-end, does not require handcrafted\nfeatures, and can learn robust representations with only a few con-\nvolutional blocks and the transformer component.\nKEYWORDS\nAffective Computing, Stress, Transformers, ECG, Wearable\n1 INTRODUCTION\nAffective computing studies how machines can recognize, infer,\nprocess, and simulate human emotions [16], with applications in\neducation, health care, video games, and others [16â€“18]. The ubiq-\nuitous availability of consumer-grade wearable sensing devices that\ncollect biological signals (e.g., ECG) and the availability of deep\nlearning frameworks have facilitated affective computing technolo-\ngies [4, 11, 21]. Classical machine learning and feature engineering\nmethods have been used to extract handcraft features and classify\naffect states [1, 2, 6, 7, 21, 22]. Although handcrafted features per-\nform well on emotion recognition, extracting them requires field\nexpertise as such features are very application-specific. Convolu-\ntional layers have been employed to automate the feature extraction\nprocess [9, 19, 20]. In addition, Transformer architectures [23] have\nrecently emerged as a powerful solution and an alternative to recur-\nrent neural networks for processing sequential data, and have been\nwidely used in natural language processing [5, 25] and computer\nvision [8, 10].\nDue to the sequential nature of ECG time-series, transformers are\nviable candidates to learn spatio-temporal representations [3, 24].\nIn this paper, we propose an architecture that uses ECG to detect\nhttps://doi.org/10.1145/3460421.3480427\nPosiï¿½onal encoder\nConv\nReLU\nMaxPool\n+\nECG Seq\nLinear\nReLU\nLinear\nStress\nx 2\nSigmoid\nx 4\nMulï¿½-head\nAï¿½enï¿½on\nAdd & Norm\nFeed-fwd\nAdd & Norm\nLinear\nReLU\nFigure 1: Transformer architecture for stress detection.\nstress based on a combination of convolutional and transformer ar-\nchitectures. Our model uses only two convolutional blocks, which is\nconsiderably less compared to other works in the area [14, 19]. We\ntest our proposed model on two publicly available affective comput-\ning datasets, WESAD [21] and SWELL-KW [13], using leave-one-\nsubject-out (LOSO) scheme. Initial results using LOSO demonstrate\nthat a fine-tuning (calibration) step is required to yield competitive\nresults versus prior work. We demonstrate that by fine-tuning the\nmodel on only 10% of user-specific data, strong results are achieved.\n2 METHOD\nWe propose an end-to-end network comprising three subnetworks,\na convolutional subnetwork, a transformer encoder, and a fully\nconnected (FC) subnetwork. The model and architectural details\nare depicted in Figure 1. The convolutional front-end subnetwork\ncomprises two convolutional layers, each directly followed by a\nReLU activation and a maxpooling layer. The convolutional layers\nare followed by a reshape layer to flatten the last dimension. The\nrole of the convolutional subnetwork is to extract spatio-temporal\nfeatures from raw input ECG signals and feed them to the encoder.\nSince using a multi-head component (which will come later) results\nin loss of ordering in the input sequence, a piece of information\nneeds to be added to the embeddings to give the encoder some sense\nof order. Here, we use the positional encoder proposed by Vaswani\net al. [23] and add its output to the embeddings obtained from the re-\nshape layer before supplying them to the transformer encoder. Next,\nthe encoder consists of a multi-head, self-attention layer, followed\nby a dropout and a layer normalization, then a fully connected\narXiv:2108.09737v1  [eess.SP]  22 Aug 2021\nBehinaein, Bhatti, Rodenburg, Hungler, and Etemad\nfeed-forward network, and finally a dropout and a layer normaliza-\ntion. We use scaled dot-product attention [23] for our model where\na query, key, and value vectors are generated. These vectors are\ncreated for each input by multiplying the input by ğ‘Šğ‘, ğ‘Šğ‘˜, and ğ‘Šğ‘£,\nwhich are learned weight matrices for query, key, and value, re-\nspectively. The queries, keys, and values are individually stacked to\ncreate ğ‘„, ğ¾, and ğ‘‰, respectively. The attention values are then com-\nputed based on [ 23], as ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„,ğ¾,ğ‘‰ ) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)ğ‘‰,\nwhere ğ‘„ âˆˆğ‘…ğ‘›Ã—ğ‘‘ğ‘ , ğ¾ âˆˆğ‘…ğ‘›Ã—ğ‘‘ğ‘˜ , and ğ‘‰ âˆˆğ‘…ğ‘›Ã—ğ‘£ğ‘˜ . Here, ğ‘›is the length\nof the sequence, and ğ‘‘ğ‘, ğ‘‘ğ‘˜, and ğ‘‘ğ‘£ are the embedding dimensions\nof ğ‘„, ğ¾, and ğ‘‰, respectively. The scaling factor\nâˆšï¸\nğ‘‘ğ‘˜ is added to\nmitigate the softmaxâ€™s small gradient when its argument is very\nlarge. We use four of these components in the transformer encoder.\nEmbeddings generated by the transformer encoder are flattened\nand fed to three FC layers where each of the first two FC layers\nis followed by a Rectified Linear Unit (ReLU) activation function\nand the third one is followed by a sigmoid function for achieving\nbinary classification. The network specifications are as follows: first\nconvolution layer (filters = 64, kernel = 64, strides = 8), second con-\nvolution layer (filters = 128, kernel = 32, strides = 4), dğ‘šğ‘œğ‘‘ğ‘’ğ‘™ = 1024,\nK, V, Qâ€™s dimension = 1024, dğ‘“ğ‘“ = 512, heads = 4, and finally the\nthree FC layersâ€™ output dimensions = 512, 256, and 1, respectively.\nA dropout of 0.5 is used after each of the first two FC layers.\nWe implement the model in PyTorch on an NVIDIA TITAN RTX\nGPU. We use an Adam optimizer with an initial learning rate of\n0.0001 and exponential decay of 0.985. We use the weighted binary\ncross-entropy loss to deal with the class imbalance. We train the\nmodel for 70 epochs with a batch size of 256. Due to significant inter-\nsubject variability, we use a small portion of each test subjectâ€™s data\nfor calibrating (fining-tuning) the model. In this step, each model is\ntrained for 40 epochs and fine-tuned for 30 epochs.\n3 EXPERIMENTS AND RESULT\nDatasets. We use two publicly available datasets, Wearable Stress\nand Affect Detection (WESAD) [21] and SWELL knowledge work\n(SWELL-KW) [13], for evaluating our model. WESAD is a mul-\ntimodal dataset collected from 15 subjects using wrist-worn and\nchest-worn wearable devices. The affect status of the subjects is also\nrecorded in the dataset. We use the ECG data from the chest-worn\ndevice. In the SWELL-KW dataset, several modalities were recorded\nfrom 25 subjects. In this study, we only consider the ECG modality.\nData Pre-processing. We apply a 5th-order Butterworth high-\npass filter with a cutoff frequency of 0.5 Hz on ECG similar to [15].\nECG was originally sampled at 700 Hz and 2048 Hz in WESAD\nand SWELL-KW, respectively. We down-sample signals from both\ndatasets to 256 Hz for our study. ECG signals are normalized using\nuser-specific z-score normalization [ 19]. In terms of the output\nclasses, WESAD recorded three affective states, neutral, stress, and\namusement. For binary classification (stress vs. non-stress), we\nmerge the neutral and amusement states into a â€˜non-stressâ€™ state.\nFor SWELL-KW, we use the â€˜neutralâ€™ as the non-stress state and\nâ€˜time pressureâ€™ and â€˜interruptionsâ€™ as stress state.\nValidation Schemes. To evaluate our model, we perform LOSO\nvalidation. We segment the data with a window size of 30 seconds\nTable 1: Classification Results. TF: Transformer, SVM: Sup-\nport Vector Machine, LDA: Linear Discriminant Analysis,\nQDA: Quadratic Discriminant Analysis, FT: Fine-Tuned.\nDataset Ref Method Modality Approach Acc. F 1\nWESAD [2] QDA ECG LOSO 85.7 -\n[21] LDA ECG LOSO 85.4 81.3\nOurs TF ECG LOSO 80.4 69.7\nOurs TF ECG LOSO (FT) 91.1 83.3\nSWELL [12] SVM Multi LOSO 58.9 â€“\nOurs TF ECG LOSO 58.1 58.8\nOurs TF ECG LOSO (FT) 71.6 74.2\nTable 2: Fine-tuning results. Values are in Acc (F 1) format.\nDataset No Tuning 1% 5% 10%\nWESAD 80.4 (69.7) 81.6 (69.8) 89.9 (80.8) 91.1 (83.3)\nSWELL-KW 58.1 (58.8) 67.4 (69.7) 68.3 (70.8) 71.6 (74.2)\nand incremental steps of 1 second. In the fine-tuning step, we use\n1%, 5%, and 10% of data to calibrate the model.\nResults. The results for stress detection on WESAD is presented\nin Table 1. Our model obtains an accuracy of 80.4% and F1 score of\n69.7%, which are below the state-of-the-art results. By fine-tuning\nthe model with only 10% of the test data, the performance is con-\nsiderably boosted to an accuracy of 91.1%, outperforming other\nmethods in Table 1. For the SWELL-KW dataset, as can be seen\nfrom Table 1, we achieve an accuracy of 58.1 and F1 score of 58.8\nwhich are comparable to [ 12]. It should be noted, however, that\n[12] uses multi-modal data (facial, posture, computer interactions,\nECG, and EDA) as apposed to our uni-modal approach. Similar to\nWESAD, we observe that fine-tuning on only 10% of data results\nin a considerable performance boost and an accuracy of 71.6% (see\nTable 1). Table 2 shows the performance when different percentages\nof test data are used for calibration. As can be seen, in WESAD, we\nneed to use more than 1% of the data in the fine-tuning step to con-\nsiderably improve the result. However, for SWELL-KW, calibrating\nwith even 1% of data boosts the performance to outperform the\nbaselines. While our approach yields promising results and is end-\nto-end (does not require hand-crafted features), the results indicate\nthat to generalize to unseen subjects better than the state-of-the-\nart, it requires calibration with a small amount of data, which is\nconsidered a limitation of our work. Nonetheless, we believe our\nmethod demonstrates the potential for transformer architectures\nto be used in the area of affective computing.\n4 CONCLUSION\nWe presented a model based on convolutional and transformer ar-\nchitectures for detecting stress versus non-stress using ECG signals.\nTo test our model, we used two publicly available datasets, WESAD\nand SWELL-KW. We showed that our model can achieve competi-\ntive results by using transformers with few convolutional layers.\nThe results using LOSO validation showed that by fine-tuning the\nmodel with only a fraction of the test data (10%), the proposed\nmodel can outperform the baseline methods.\nA Transformer Architecture for Stress Detection from ECG\nREFERENCES\n[1] Dhananjai Bajpai and Lili He. 2020. Evaluating KNN Performance on WESAD\nDataset. In 2020 12th International Conference on Computational Intelligence and\nCommunication Networks (CICN) . IEEE, 60â€“62.\n[2] PatrÃ­cia Bota, Chen Wang, Ana Fred, and Hugo Silva. 2020. Emotion Assessment\nUsing Feature Fusion and Decision Fusion Classification Based on Physiological\nData: Are We There Yet? Sensors 20, 17 (2020), 4723.\n[3] Chao Che, Peiliang Zhang, Min Zhu, Yue Qu, and Bo Jin. 2021. Constrained\nTransformer Network for ECG Signal Processing and Arrhythmia Classification.\nBMC Medical Informatics and Decision Making 21 (2021).\n[4] Juan Abdon Miranda Correa, Mojtaba Khomami Abadi, Niculae Sebe, and Ioannis\nPatras. 2018. Amigos: A Dataset for Affect, Personality and Mood Research on\nIndividuals and Groups. IEEE Transactions on Affective Computing (2018).\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\narXiv preprint arXiv:1810.04805 (2018).\n[6] Hany Ferdinando, Tapio SeppÃ¤nen, and Esko Alasaarela. 2016. Comparing fea-\ntures from ECG pattern and HRV analysis for emotion recognition system. In\n2016 IEEE Conference on Computational Intelligence in Bioinformatics and Compu-\ntational Biology (CIBCB) . 1â€“6. https://doi.org/10.1109/CIBCB.2016.7758108\n[7] Han-Wen Guo, Yu-Shun Huang, Chien-Hung Lin, Jen-Chien Chien, Koichi\nHaraikawa, and Jiann-Shing Shieh. 2016. Heart rate variability signal features\nfor emotion recognition by using principal component analysis and support\nvectors machine. In 2016 IEEE 16th International Conference on Bioinformatics and\nBioengineering (BIBE) . IEEE, 274â€“277.\n[8] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua\nLiu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al . 2020. A Survey on\nVisual Transformer. arXiv preprint arXiv:2012.12556 (2020).\n[9] Bosun Hwang, Jiwoo You, Thomas Vaessen, Inez Myin-Germeys, Cheolsoo\nPark, and Byoung-Tak Zhang. 2018. Deep ECGNet: An optimal deep learn-\ning framework for monitoring mental stress using ultra short-term ECG signals.\nTELEMEDICINE and e-HEALTH 24, 10 (2018), 753â€“772.\n[10] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-\nhad Shahbaz Khan, and Mubarak Shah. 2021. Transformers in Vision: A Survey.\narXiv preprint arXiv:2101.01169 (2021).\n[11] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan\nYazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. 2011.\nDeap: A Database for Emotion Analysis; Using Physiological Signals. IEEE\nTransactions On Affective Computing 3, 1 (2011), 18â€“31.\n[12] Saskia Koldijk, Mark A Neerincx, and Wessel Kraaij. 2016. Detecting Work Stress\nin Offices by Combining Unobtrusive Sensors. IEEE Transactions on Affective\nComputing 9, 2 (2016), 227â€“239.\n[13] Saskia Koldijk, Maya Sappelli, Suzan Verberne, Mark A Neerincx, and Wessel\nKraaij. 2014. The Swell Knowledge Work Dataset for Stress and User Model-\ning Research. In Proceedings of the 16th International Conference on Multimodal\nInteraction. 291â€“298.\n[14] Jionghao Lin, Shirui Pan, Cheng Siong Lee, and Sharon Oviatt. 2019. An Explain-\nable Deep Fusion Network for Affect Recognition using Physiological Signals. In\nProceedings of the 28th ACM International Conference on Information and Knowl-\nedge Management . 2069â€“2072.\n[15] Dominique Makowski, Tam Pham, Zen J. Lau, Jan C. Brammer, FranÃ§ois\nLespinasse, Hung Pham, Christopher SchÃ¶lzel, and S. H. Annabel Chen. 2021.\nNeuroKit2: A Python toolbox for neurophysiological signal processing. Behavior\nResearch Methods (02 Feb 2021). https://doi.org/10.3758/s13428-020-01516-y\n[16] Rosalind W Picard. 2000. Affective computing . MIT press.\n[17] Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A Re-\nview Of Affective Computing: From Unimodal Analysis To Multimodal Fusion.\nInformation Fusion 37 (2017), 98â€“125.\n[18] Kyle Ross, Pritam Sarkar, Dirk Rodenburg, Aaron Ruberto, Paul Hungler, Adam\nSzulewski, Daniel Howes, and Ali Etemad. 2019. Toward Dynamically Adaptive\nSimulation: Multimodal Classification of User Expertise using Wearable Devices.\nSensors 19, 19 (2019), 4270.\n[19] Pritam Sarkar and Ali Etemad. 2020. Self-supervised ecg representation learning\nfor emotion recognition. IEEE Transactions on Affective Computing (2020).\n[20] Pritam Sarkar and Ali Etemad. 2020. Self-supervised learning for ecg-based\nemotion recognition. In 2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP) . 3217â€“3221.\n[21] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof\nVan Laerhoven. 2018. Introducing Wesad, A Multimodal Dataset for Wearable\nStress and Affect Detection. In Proceedings of the 20th ACM International Confer-\nence on Multimodal Interaction . 400â€“408.\n[22] Senthil Sriramprakash, Vadana D Prasanna, and OV Ramana Murthy. 2017. Stress\nDetection in Working People. Procedia Computer Science 115 (2017), 359â€“366.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll You Need. In Proceedings of the 31st International Conference on Neural Infor-\nmation Processing Systems (Long Beach, California, USA) (NIPSâ€™17). 6000â€“6010.\n[24] Bin Wang, Chang Liu, Chuanyan Hu, Xudong Liu, and Jun Cao. 2021. Arrhythmia\nClassification with Heartbeat-Aware Transformer. In ICASSP 2021-2021 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 1025â€“1029.\n[25] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language\nProcessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations . 38â€“45.",
  "topic": null,
  "concepts": []
}