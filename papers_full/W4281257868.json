{
  "title": "A Graph-Transformer for Whole Slide Image Classification",
  "url": "https://openalex.org/W4281257868",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2105699845",
      "name": "Zheng Yi",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A4281280909",
      "name": "Gindra, Rushin H.",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A4281280910",
      "name": "Green, Emily J.",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A4281280911",
      "name": "Burks, Eric J.",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A2540976806",
      "name": "Betke, Margrit",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A4281280913",
      "name": "Beane, Jennifer E.",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A4281280914",
      "name": "Kolachalama, Vijaya B.",
      "affiliations": [
        "Boston University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2126096721",
    "https://openalex.org/W2216754497",
    "https://openalex.org/W2964756323",
    "https://openalex.org/W2040414046",
    "https://openalex.org/W2971376088",
    "https://openalex.org/W2952846726",
    "https://openalex.org/W2760946358",
    "https://openalex.org/W2796409016",
    "https://openalex.org/W2302302587",
    "https://openalex.org/W2964345665",
    "https://openalex.org/W3108327761",
    "https://openalex.org/W3165497806",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2983288276",
    "https://openalex.org/W2988856610",
    "https://openalex.org/W4287503054",
    "https://openalex.org/W3042024476",
    "https://openalex.org/W3034203199",
    "https://openalex.org/W2890655214",
    "https://openalex.org/W3091402370",
    "https://openalex.org/W3203838058",
    "https://openalex.org/W4206402984",
    "https://openalex.org/W3034534840",
    "https://openalex.org/W3090852973",
    "https://openalex.org/W3093496925",
    "https://openalex.org/W3217658289",
    "https://openalex.org/W6803426362",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W2329674354",
    "https://openalex.org/W130099911",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6771497439",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W6779032261",
    "https://openalex.org/W6787329981",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2136655611",
    "https://openalex.org/W6677919164",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W3215921195",
    "https://openalex.org/W3099404130",
    "https://openalex.org/W2118858186",
    "https://openalex.org/W4206502244",
    "https://openalex.org/W4225604175"
  ],
  "abstract": "Deep learning is a powerful tool for whole slide image (WSI) analysis. Typically, when performing supervised deep learning, a WSI is divided into small patches, trained and the outcomes are aggregated to estimate disease grade. However, patch-based methods introduce label noise during training by assuming that each patch is independent with the same label as the WSI and neglect overall WSI-level information that is significant in disease grading. Here we present a Graph-Transformer (GT) that fuses a graph-based representation of an WSI and a vision transformer for processing pathology images, called GTP, to predict disease grade. We selected 4,818 WSIs from the Clinical Proteomic Tumor Analysis Consortium (CPTAC), the National Lung Screening Trial (NLST), and The Cancer Genome Atlas (TCGA), and used GTP to distinguish adenocarcinoma (LUAD) and squamous cell carcinoma (LSCC) from adjacent non-cancerous tissue (normal). First, using NLST data, we developed a contrastive learning framework to generate a feature extractor. This allowed us to compute feature vectors of individual WSI patches, which were used to represent the nodes of the graph followed by construction of the GTP framework. Our model trained on the CPTAC data achieved consistently high performance on three-label classification (normal versus LUAD versus LSCC: mean accuracy = 91.2 ± 2.5%) based on five-fold cross-validation, and mean accuracy = 82.3 ± 1.0% on external test data (TCGA). We also introduced a graph-based saliency mapping technique, called GraphCAM, that can identify regions that are highly associated with the class label. Our findings demonstrate GTP as an interpretable and effective deep learning framework for WSI-level classification.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022 3003\nA Graph-T ransformer for Whole Slide\nImage Classiﬁcation\nYi Zheng, Rushin H. Gindra, Emily J. Green, Eric J. Burks, Margrit Betke, Jennifer E. Beane,\nand Vijaya B. Kolachalama, Member, IEEE\nAbstract — Deep learning is a powerful tool for whole slide\nimage (WSI) analysis. Typically, when performing super-\nvised deep learning, a WSI is divided into small patches,\ntrained and the outcomes are aggregated to estimate dis-\nease grade. However, patch-based methods introduce label\nnoise during training by assuming that each patch is\nindependent with the same label as the WSI and neglect\noverall WSI-level information that is signiﬁcant in disease\ngrading. Here we present a Graph-Transformer (GT) that\nfuses a graph-based representation of an WSI and a vision\ntransformer for processing pathology images, called GTP,\nto predict disease grade. We selected 4,818 WSIs from the\nClinical Proteomic Tumor Analysis Consortium (CPTAC),\nthe National Lung Screening Trial (NLST), and The Cancer\nGenome Atlas (TCGA), and used GTP to distinguish ade-\nnocarcinoma (LUAD) and squamous cell carcinoma (LSCC)\nfrom adjacent non-cancerous tissue (normal). First, using\nNLST data, we developed a contrastive learning framework\nto generate a feature extractor. This allowed us to compute\nfeature vectors of individual WSI patches, which were used\nto represent the nodes of the graph followed by construction\nof the GTP framework. Our model trained on the CPTAC data\nachieved consistently high performance on three-label clas-\nsiﬁcation (normal versus LUAD versus LSCC: mean accu-\nracy = 91.2 ± 2.5%) based on ﬁve-fold cross-validation, and\nManuscript received 21 April 2022; accepted 12 May 2022. Date\nof publication 20 May 2022; date of current version 27 October\n2022. This work was supported in part by the Grants from the\nNational Institutes of Heath under Grant R21-CA253498 and Grant\nR01-HL159620, in part through a sponsored research agreement\nfrom Johnson & Johnson Enterprise Innovation, Inc., in part by\nthe American Heart Association under Grant 20SFRN35460031,\nin part by the Karen Tofﬂer Charitable Trust, and in part by\nthe National Science Foundation under Grant 1551572 and\nGrant 1838193.\n(Corresponding authors: Jennifer E. Beane;\nVijaya B. Kolachalama.)\nYi Zheng is with the Department of Computer Science, Boston Univer-\nsity, Boston, MA 02215 USA, and also with the Department of Medicine,\nBoston University School of Medicine, Boston, MA 02218 USA (e-mail:\nyizheng@bu.edu).\nRushin H. Gindra, Emily J. Green, and Jennifer E. Beane are with\nthe Department of Medicine, Boston University School of Medicine,\nBoston, MA 02218 USA (e-mail: rushing@bu.edu; egreen1@bu.edu;\njbeane@bu.edu).\nEric J. Burks is with the Department of Pathology and Laboratory\nMedicine, Boston University School of Medicine, Boston, MA 02218 USA\n(e-mail: eric.burks@bmc.org).\nMargrit Betke is with the Department of Computer Science, Fac-\nulty of Computing and Data Sciences, Boston University, Boston,\nMA 02215 USA (e-mail: betke@bu.edu).\nVijaya B. Kolachalama is with the Department of Medicine, Boston\nUniversity School of Medicine, Boston, MA 02218 USA, and also\nwith the Department of Computer Science, Faculty of Computing and\nData Sciences, Boston University, Boston, MA 02215 USA (e-mail:\nvkola@bu.edu).\nDigital Object Identiﬁer 10.1109/TMI.2022.3176598\nmean accuracy = 82.3 ± 1.0% on external test data (TCGA).\nWe also introduced a graph-based saliency mapping tech-\nnique, called GraphCAM, that can identify regions that are\nhighly associated with the class label. Our ﬁndings demon-\nstrate GTP as an interpretable and effective deep learning\nframework for WSI-level classiﬁcation.\nIndex Terms— Digital pathology, graph convolutional net-\nwork, vision transformer, deep learning, lung cancer.\nI. I NTRODUCTION\nC\nOMPUTATIONAL pathology [1]–[4], which entails\nthe analysis of digitized pathology slides, is gaining\nincreased attention over the past few years. The sheer size of\na single whole slide image (WSI) typically can exceed a giga-\nbyte, so traditional image analysis routines may not be able\nto fully process all this data in an efﬁcient fashion. Modern\nmachine learning methods such as deep learning have allowed\nus to make great progress in terms of analyzing WSIs includ-\ning disease classiﬁcation [5], tissue segmentation [6], mutation\nprediction [7], and spatial proﬁling of immune inﬁltration [8].\nMost of these methods rely on systematic breakdown of WSIs\ninto image patches, followe d by development of deep neural\nnetworks at patch-level and integration of outcomes on these\npatches to create overall WSI-level estimates [9], [10]. While\npatch-based approaches cataly zed research in the ﬁeld, the\ncommunity has begun to appreciate the conditions in which\nthey confer beneﬁt and in those where they cannot fully cap-\nture the underlying pathology. For example, methods focused\non identifying the presence or absence of a tumor on an WSI\ncan be developed on patches using computationally efﬁcient\ntechniques such as multiple instance learning [11]. On the\nother hand, if the goal is to identify the entire tumor region\nor capture the connectivity of the tumor microenvironment\ncharacterizing the stage of dis ease, then it becomes important\nto assess both regional and WSI-level information. There are\nseveral other scenarios where both the patch- and WSI-level\nfeatures need to be identiﬁed to assess the pathology [12], and\nmethods to perform such analysis are needed.\nThe success of patch-based deep learning methods can\nbe attributed to the availability of pre-trained deep neural\nnetworks on natural images fro m public databases (i.e., Ima-\ngeNet [13]). Since there are millions of parameters in a typical\ndeep neural network, de novotraining of this network requires\naccess to a large set of pathology data, and such resources are\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3004 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nnot necessarily available at all locations. To address this bottle-\nneck, researchers have leveraged transfer learning approaches\nthat are pre-trained on ImageN et to accomplish various tasks.\nRecently, transformer architect ures were applied directly to\nsequences of image patches for v arious classiﬁcation tasks.\nSpeciﬁcally, Vision Transformers (ViT) were shown to achieve\nexcellent results compared to stat e-of-the-art convolutional\nnetworks while requiring substantially fewer computational\nresources for model training [14]. Position embeddings were\nused in ViTs to retain spatial information and capture the\nassociation of different patches within the input image. The\nself-attention mechanism in ViT requires the calculation of\npairwise similarity scores on all the patches, resulting in mem-\nory efﬁciency and a simple time complexity that is quadratic\nin the number of patches. Lev eraging such approaches to\nperform pathology image analysis is not trivial because each\nWSI can contain thousands of pa tches. Additionally, some\napproximations are often made on these patches such as using\nthe WSI-level label on each patch during training, which\nis not ideal in all scenarios as there is a need to process\nboth the regional information as well as the WSI in its\nentirety to better understand the pathological correlates of\ndisease.\nSimilar to the regional and WSI-level examination, we argue\nthat an expert pathologist’s workﬂow also involves examina-\ntion of the entire slide under the microscope using manual\noperations such as panning and zooming in and out of speciﬁc\nregions of interest to assess various aspects of disease at\nmultiple scales. In the zoom-in assessment, the pathologists\nperform an in-depth, evaluation of regional manifestations of\ndisease whereas, the zoom-out assessment involves obtaining\na rational estimate of the overall disease on the entire WSI.\nBoth these assessments are critical as the pathologist obtains\na gestalt on various image features to comprehensively assess\nthe disease [12].\nA. Related Work\nRecent attempts to perform WSI-level analysis have shown\npromising results in terms of assessing the overall tissue\nmicroenvironment. In particular, graph-based approaches such\nas graph convolutional networks have gained a lot of trac-\ntion due to their ability to represent the entire WSI and\nanalyze patterns to predict various outcomes of interest.\nTo learn hierarchical representation for graph embedding,\nrecent approaches have proposed pooling strategies. For exam-\nple, in AttPool [15], Huang and colleagues devised an attention\npooling layer to select discriminative nodes and built the\ncoarser graph based on calculated attention values. AttPool\nsufﬁciently leveraged the hierarchical representation and facil-\nitated model learning on several graph classiﬁcation bench-\nmark datasets. Zhou and colleagues developed a cell-graph\nconvolutional neural network on WSIs to predict the grade\nof colorectal cancer (CRC) [16]. In this work, the WSI was\nconverted to a graph, where each nucleus was represented\nby a node and the cellular interactions were denoted as\nedges between these nodes to accurately predict CRC grade.\nAlso, Adnan and colleagues developed a two-stage framework\nfor WSI representation learning [17], where patches were\nsampled based on color and a graph neural network was\nconstructed to learn the inter-patch relationships to discrim-\ninate lung adenocarcinoma (LUAD) from lung squamous cell\ncarcinoma (LSCC). In another recent work, Lu and team\ndeveloped a graph representation of the cellular architecture\non the entire WSI to predict the status of human epidermal\ngrowth factor receptor 2 and progesterone receptor [18].\nTheir architecture attempted t o create a bottom-up approach\n(i.e., nuclei- to WSI-level) to construct the graph, and in so\ndoing, achieved a relatively efﬁcient framework for analyzing\nthe entire WSI.\nSeveral other researchers have leveraged graph-based\napproaches to process WSIs to address various questions\nfocused on survival analysis [19]–[22], prediction of lymph\nnode metastasis [23], mutational prediction [24], cell classi-\nﬁcation [25], and retrieval of relevant regions [26]. With the\nobjective of retaining the corre lation among different patches\nwithin an WSI, Shao and coll eagues leveraged the computa-\ntionally efﬁcient sampling approach of multiple instance learn-\ning and the self-attention mechanism of ViT in TransMIL [27].\nTo deal with the memory and quadratic time complexity issue\nin ViT, they adopted the Nystrom Method [28]. They expressed\nthe signiﬁcance of retaining these correlations among instances\nby showing good performance for tumor classiﬁcation in 3 dif-\nferent datasets: CAMELYON16, TCGA-NSCLC and TCGA-\nRCC. Motivated by these advances, we submit that integration\nof computationally efﬁcient approaches such as ViTs along\nwith graphs can lead to more efﬁcient representation learning\napproaches for the assessment of WSIs.\nB. Contributions\nThe main contributions of this paper are summarized below:\n• We developed a graph-based vision transformer for digital\npathology called GTP that leverages a graph representa-\ntion of pathology images and the computational efﬁciency\nof transformer architectures to perform WSI-level analy-\nsis. To build the GTP framework, we constructed a graph\nconvolutional network by embedding image patches in\nfeature vectors using contrastive learning, followed by the\napplication of a vision transformer to predict a WSI-level\nlabel.\n• Using WSI and clinical data from three publicly available\nnational cohorts (4 ,818 WSIs), we developed a model\nthat could distinguish between normal, LUAD, and LSCC\nWSIs.\n• We introduced graph-based class activation mapping\n(GraphCAM), a novel approach to generate WSI-level\nsaliency maps that can identify image regions that are\nhighly associated with the output class label. On a few\nWSIs, we also compared the performance of the Graph-\nCAMs with pathologist-driven annotations and showed\nthat our approach identiﬁes important disease-related\nregions of interest.\n• Over a series of ablation studi es and sensitivity analyses,\nwe showed that our GTP framework outperforms current\nstate-of-the-art methods used for WSI classiﬁcation.\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3005\nTABLE I\nSTUDY POPULATION .W HOLE SLIDE IMAGES AND CORRESPONDING\nCLINICAL INFORMATION FROM THREE DISTINCT COHORTS INCLUDING\nTHE CLINICAL PROTEOMIC TUMOR ANAL YSIS CONSORTIUM (CPTAC),\nTHE CANCER GENOME ATLAS (TCGA) AND THE NATI ONAL LUNG\nSCREENING TRIAL (NLST) W ERE USED\nII. M A TERIALS AND METHODS\nA. Study Population\nWe obtained access to WSIs as well as demographic and\nclinical data of lung tumors (LUAD and LSCC) and normal\ntissue from the Clinical Proteomic Tumor Analysis Consor-\ntium (CPTAC), the National Lung Screening Trial (NLST)\nand The Cancer Genome Atlas (TCGA) ( Table I). CPTAC is a\nnational effort to accelerate the understanding of the molecular\nbasis of cancer through the application of large-scale proteome\nand genome analysis [29]. NLST was a randomized controlled\ntrial to determine whether screening for lung cancer with\nlow-dose helical computed tomography reduces mortality from\nlung cancer in high-risk individuals relative to screening with\nchest radiography [30]. TCGA is a landmark cancer genomics\nprogram, which molecularly characterized thousands of pri-\nmary cancer and matched normal samples spanning 33 cancer\ntypes [31].\nB. Graph-Transformer\nOur proposed Graph-Transformer (GT) ( Fig. 1 (a) )f u s e s\na graph representation G of an WSI ( Fig. 1 (b) ), and a\ntransformer that can generate WSI-level predictions in a com-\nputationally efﬁcient fashion. Let G = (V, E) be an undirected\ngraph where V is the set of nodes representing the image\npatches and E is the set of edges between the nodes in V\nthat represent whether two image patches are adjacent to each\nother. We denote the adjacency matrix of G as A =[ Aij ]\nwhere Aij = 1 if there exists an edge (vi ,v j ) ∈ E and\nAij = 0 otherwise. An image patch must be connected to\nother patches and can be surrounded by at most 8 adjacent\npatches, so the sum of each row or column of A is at least\none and at most 8. A graph can be associated with a node\nfeature matrix F, F ∈ IR\nN×D , where each row contains the\nD-dimensional feature vector computed for an image patch,\ni.e., node, and N =| V |.\nUsing all the pixels within each image patch as features\ncan make model training computationally intractable. Instead,\nour framework applies a feature extractor to generate a vector\ncontaining features and uses it to deﬁne the information\ncontained in an image patch, which is a node in the graph. This\nstep reduces the node feature dimension from W\np × Hp × Cp\nto D,w h e r eWp, Hp, and Cp are width, height, and channel\nof the image patch, and D × 1 is the dimension of extracted\nfeature vector. The expectation is that the derived feature\nvector provides an efﬁcient representation of the node and\nalso serves as a robust means by which to deﬁne a uniform\nrepresentation of an image patch for graph-based classiﬁcation.\nAs described above, current methods that have been devel-\noped at patch-level impose WSI-level labels on all the patches\nor use weakly supervised learning to extract feature vectors\nthat are representative of the WSI. This strategy is not suit-\nable for all scenarios, especi ally when learning the overall\nWSI-level information. We leveraged a strategy based on\nself-supervised contrastive learning [32], to extract features\nfrom the WSIs. This framework enables robust representations\nthat can be learned without the need for manual labels.\nOur approach involves using contrastive learning to train a\nconvolutional neural network (CNN) that produces embed-\nding representations by maximi zing agreement between two\ndifferently augmented views of the same image patch via a\ncontrastive loss in the latent space ( Fig. 1(C) ). The training\nstarts with tiling the WSIs from the training set into patches\nand randomly sampling a mini-batch of K patches. Two\ndifferent data augmentation operations are applied to each\npatch ( p), resulting in two augmented patches ( p\ni and pj ).\nThe pair of two augmented patches from the same patch is\ndenoted as a positive pair. For a mini-batch of K patches, there\nare 2 K augmented patches in total. Given a positive pair, the\nother 2 K − 1 augmented patches are considered as negative\nsamples. Subsequently, the CNN is used to extract repre-\nsentative embedding vectors ( f\ni , f j ) from each augmented\npatch ( pi , pj ). The embedding vectors are then mapped by\na projection head to a latent space ( zi , z j ) where contrastive\nlearning loss is applied. The contrastive learning loss function\nfor a positive pair of augmented patches (i, j) is deﬁned as :\nli,j =− log exp(sim (zi ,zj )/τ)\n∑2K\nk=1\n/BD\n[k\u0003=i] exp(sim (zi ,zk)/τ)\n, (1)\nwhere /BD\n[k\u0003=i] ∈{ 0,1} is an indicator function evaluating to\n1 if and only if k \u0003= i and τ denotes a temperature parameter.\nAlso, sim (u,v) = uT v/\u0004u\u0004\u0004v\u0004 denotes the dot product\n3006 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nFig. 1. The GTP framework. (a) Each whole slide image (WSI) was divided into patches. Patches that predominantly contained the background\nwere removed, and the remaining patches were embedded in feature vectors by a contrastive learning-based patch embedding module. The feature\nvectors were then used to build the graph followed by a transformer that takes the graph as the input and predicts WSI-level class label.(b) Each\nselected patch was represented as a node and a graph was constructed on the entire WSI using the nodes with an 8-node adjacency matrix. Here,\ntwo sets of patches of an WSI and their corresponding subgraphs are shown. The subgraphs are connected within the graph representing the entire\nWSI. (c) We applied three distinct augmentation functions, including random color distortions, random Gaussian blur, and random cropping followed\nby resizing back to the original size, on the same sample in a mini-batch. If the mini-batch size is\nK, then we ended up with 2× K augmented\nobservations in the mini-batch. The ResNet received an augmented image leading to an embedding vector as the output. Subsequently, a projection\nhead was applied to the embedding vector which produced the inputs to contrastive learning. The projection head is a multilayer perceptron (MLP)\nwith 2 dense layers. In this example, we considered\nK = 3 samples in a minibatch (A, B & C). For sample A, the positive pairs are (A1, A2) and\n(A2, A1), and the negative pairs are (A1, B1), (A1, B2), (A1, C1), (A1, C2). All pairs were used for computing contrastive learning loss to train the\nResnet. After training, we used the embedding vectors (straight from the ResNet) for constructing the graph.\nbetween L2 normalized u and v (i.e., cosine similarity). For\nmodel training, the patches were densely cropped without\noverlap and treated as individual images. The ﬁnal loss was\ncomputed across all positive pairs, including both (i, j) and\n(j, i) in a mini-batch. After co nvergence, we kept the feature\nextractor and used it for our GTP model to compute the\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3007\nfeature vectors of the patches from the WSIs. GTP uses\nthese computed feature vectors as node features in the graph\nconstruction phase. Speciﬁcally, we obtained the node-speciﬁc\nfeature matrix F =[ f1; f2; ... ; fN ], F ∈ IRN×D ,w h e r efi\nis the D-dimensional embedding vector obtained from Resnet\ntrained using contrastive learning and N is the number of\npatches from one WSI. Note that N is variable since different\nWSIs contain different numbers of patches. As a result, each\nnode in F corresponds to one patch of the WSI. We deﬁned\nan edge between a pair of nodes in F b a s e do nt h es p a t i a l\nlocation of its corresponding patches on the WSI. If patch i\nis a neighbor of patch j on the WSI ( Fig. 1(B) ), then GTP\ncreates an edge between node i and node j as well as set\nAij = 1a n d Aji = 1, otherwise Aij = 0a n d Aji = 0. GTP\nuses feature node matrix F and adjacent matrix A to construct\na graph to represent each WSI.\nWe implemented the graph convolutional (GC) layer, intro-\nduced by Kipf & Welling [33], to handle the graph-structured\ndata. The GC layer operates m essage propagation and aggre-\ngation in the graph, and is deﬁned as :\nHm+1 = ReLU ( ˆAHmWm), m = 1,2, ..,M (2a)\nˆA = ˜D−1\n2 ˜A ˜D−1\n2 (2b)\nwhere ˆA is the symmetric normali zed adjacency matrix of A\nand M is the number of GC layers. Here, ˜A = A + I is the\nadjacency matrix with a self-loop added to each node, and ˜D\nis a diagonal matrix where ˜Dii = ∑\nj ˜Aij . Hm is the input of\nthe m-th GC layer and H1 is initialized with the node feature\nmatrix F. Additionally, Wm ∈ IRCm×Cm+1 is the matrix of\nlearnable ﬁlters in the GC layer, where Cm is the dimension\nof the input and Cm+1 is the dimension of the output.\nThe GC layer of GTP enables learning of node embeddings\nthrough propagating and aggregating needed information.\nHowever, it is not trivial for a model to learn hierarchical\nfeatures that are crucial for gra ph representation and classiﬁ-\ncation. To address this limitation, we introduced a transformer\nlayer that selects the most signiﬁcant nodes in the graph and\naggregates information via the attention mechanism. Trans-\nformers use a Self-Attentio n (SA) mechanism to model the\ninteractions between all tokens in a sequence [34], by allowing\nthe tokens to interact with each other (“self”) and ﬁnd out\nwho they should pay more attention to (“attention”), and the\naddition of positional information of tokens further increases\nthe use of sequential order information. Excitingly, the Vision\nTransformer (ViT) enables the application of transformers to\n2D images [14]. Inspired by these studies, we propose a\ntransformer layer to interpret our graph-structured data. While\nthe SA mechanism has been extensively used in the context\nof natural language processing, we extended the framework\nfor WSI data. Brieﬂy, the standard qkv self-attention [34] is a\nmechanism to ﬁnd the words of importance for a given query\nword in a sentence, and it receives as input a 1D sequence of\ntoken embeddings. For the graph, the feature nodes are treated\nas tokens in a sequence and the adjacency matrix is used to\ndenote the positional information. Given that x ∈ R\nN×D is the\nsequence of patches (feature nodes) in the graph, where N is\nthe number of patches and D is the embedding dimension\nof each patch, we compute q(query), k(key) and v(value)\n(Eq.3a). The attention weights Aij are based on the pairwise\nsimilarity between two patches of the sequence and their\nrespective query qi and key kj in Eq.3b. Multihead Self-\nAttention (MSA) is a mechanism that involves combining the\nknowledge explored by k number of SA operations, called\n‘heads’. It projects concaten ated outputs of SA in Eq.3c.\nDh (Eq.3a) is typically set to D/k to facilitate computa-\ntion and maintain the number of parameters constant when\nchanging k.\n[q,k,v]= xUqkv, Uqkv ∈ RD×3Dh (3a)\nA = softmax(qkT /\n√\nDh ), A ∈ RN×N (3b)\nSA(x) = Av, (3c)\nMSA(x) =[ SA1(x); SA2(x); ... SAk(x)] (3d)\n× Umsa , andUmsa ∈ Rk·Dh ×D.\nThe goal of the transformer layer is to learn the mapping:\nH → T,w h e r eH is the graph space, and T is the transformer\nspace. We deﬁne the mapping of H → T as:\nt0 =[ xclass; h(1); h(2); ... ; h(N)], h(i) ∈ H (4a)\nt\u0006\nl = MSA(LN(tl−1)) + tl−1, l = 1 ... L (4b)\ntl = MLP(LN(t\u0006\nl )) + t\u0006\nl , l = 1 ... L (4c)\nwhere MSA is the Multihead Self-Attention (Eq.3), MLP\nis a Multilayer Perceptron, and LN denotes Layer Norm.\nL is the number of MSA blocks [14]. Each block con-\nsists of an MSA layer (Eq.4b) and an MLP block (Eq.4c).\nIn order to learn the mapping T → Y from transformer\nspace T to label space Y, we prepared a learnable embedding\n(t\n(0\n0 ) = xclass) to the feature nodes (Eq.4a), whose state at\nthe output of the transformer layer ( z0\nL) serves as mapping\nof T → Y:\ny = LN (z(0)\nL ). (5)\nRecently, position embeddings were added to the patch\nembeddings to retain positional information [14]. Position\nembedding explores absolute position encoding (e.g., sinu-\nsoidal or learnable absolute encoding) as well as conditional\nposition encoding. However, the learnable absolute encoding\nis commonly used in problems with ﬁxed length sequences\nand does not meet the requirement for WSI analysis. This\nis because the number of pat ches that can be extracted\ncan vary among different WSIs. To address this, Islam and\ncolleagues showed that the addition of zero padding can\nprovide an absolute position information for convolution [35].\nIn our work, we do not leverage position embeddings in\nthe same fashion as ViTs or via inclusion of zero padding.\nRather, we used position embedding in the form of an\nadjacency matrix for the grap h convolutional layers. The\nadjacency matrix of the WSI graph accounts for the spatial\ninformation and inter-connectivity of the nodes, which is\npreserved when performing graph convolutions. To reduce the\nnumber of inputs to the transformer layer, a pooling layer\nwas added between the graph convolution and transformer\nlayer (see below). The positional information between the\npooled adjacency matrix and t he original patch adjacency\n3008 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nmatrix is preserved by a dense learned assignment used in\npooling the features. With this framework, we were able\nto avoid the need of adding an additional encoder in the\ntransformer and instead preserved positional information in\nthe GC layers of GTP, thus reducing the complexity of our\nmodel.\nThe softmax function is typically used as a row-by-\nrow normalization function in transformers for computer\nvision tasks [36], [37]. The standard self-attention mechanism\nrequires the calculation of similarity scores between each pair\nof nodes, resulting in both memory and time complexity that\nis quadratic in the number of nodes ( O(n\n2)). Since the WSI\ngraph contains several nodes (in thousands), it may lead to out-\nof-memory issues during model training and thus applying the\ntransformer layer directly to the convolved graphs is not trivial.\nWe therefore added a mincut pooling layer [38] between the\nGC and transformer layers that reduced the number of inputs\nfrom thousands to hundreds of nodes. Alternatively, mean\npooling can be used to reduce the number of nodes but it may\nlose information. As such, the mean pooling operation does\nnot contain learnable parameters like what we have with the\ncase of min-cut pooling. Min-cut pooling could preserve the\nlocal information of neighboring nodes and reduce the number\nof nodes at the same time. The idea behind min-cut pooling\nis to take a continuous relaxation of the min-cut problem and\nimplement it as a pooling layer with a custom loss function.\nBy minimizing the custom loss, the pooling layer learns to\nﬁnd min-cut clusters on any given graph and aggregates the\nclusters to reduce the graph size. At the same time, because\nthe pooling layer can be used as part of the entire GTP,\nthe classiﬁcation loss of GTP that is being minimized during\ntraining will inﬂuence the min-cut layer as well. In so doing,\nour GTP graph-transformer was able to accommodate several\npatches as input, underscoring the novelty of our approach and\nits application to WSI data.\nC. Class Activation Mapping\nTo understand how GTP processes WSI data and identi-\nﬁes regions that are highly associated with the class label,\nwe proposed a novel class activation mapping technique on\ngraphs (Fig. 2). In what follows, we use the term GraphCAM\nto refer to this technique. Our technique was inspired by\nthe recent work by Chefer and colleagues [39], who used\nthe deep Taylor decomposition principle to assign local rel-\nevance scores and propagated them through the layers by\nmaintaining the total relevancy across layers. In a similar\nfashion, our method computes the class activation map from\nthe output class to the input graph space, and reconstructs\nthe ﬁnal class activation map for the WSI from its graph\nrepresentation.\nLet A\n(l) represent the attention map of the MSA block l in\nEq.3b. Following the propagation procedure of relevance and\ngradients by Chefer and colleagues [39], GraphCAM computes\nthe gradient ∇A(l) and layer relevance R(nl ) with respect to\na target class for each attention map A(l),w h e r enl is the\nlayer that corresponds to the softmax operation in Eq.3b of\nblock l. The transformer relevance map Ct is then deﬁned as\nFig. 2. Schematic of the GraphCAM. Gradients and relevance are\npropagated through the network and integrated with an attention map to\nproduce the transformer relevancy maps. Transformer relevancy maps\nare then mapped to graph class activation maps via reverse pooling.\na weighted attention relevance :\nCt =\nL∏\nl=1\n¯A(l) (6a)\n¯A(l) = Eh(∇A(l) \b R(nl )) + I (6b)\nwhere \b is the Hadamard product, Eh is the mean across the\n‘heads’ dimension, and I is the identity matrix to avoid self\ninhibition for each node.\nThe pooled node features by the mincut pooling layer are\ncomputed as X pool = ST X,w h e r eS ∈ RNg ×Nt is the dense\nlearned assignment, and Nt and Ng are the number of nodes\nbefore and after the pooling layer. To yield the graph relevance\nmap C\ng from transformer relevance map Ct , our GraphCAM\nperforms mapping Ct to each node in the graph based on the\ndense learned assignments as Ct\nS\n−→ Cg. Finally, GraphCAM\nreconstructs the ﬁnal class activation map on the WSI using the\nadjacency matrix of the graph and coordinates of the patches.\nD. Data and Code Availability\nAll the WSIs and corresponding clinical data can be\ndownloaded freely from CPTAC, TCGA and NLST websites.\nPython scripts and manuals are made available on GitHub\n(https://github.com/vkola-lab/tmi2022).\nIII. E\nXPERIMENTS\nWe performed several experiments to evaluate our GTP\nframework. The NLST data ( ≈ 1.8 million patches) was\nexclusively used for contrastive learning to generate patch-\nspeciﬁc features, which were used to represent each node.\nThe GTP framework was trained on the CPTAC data\n(2,071 WSIs) using 5-fold cross validation, and the TCGA\ndata (2 ,082 WSIs) was used as an independent dataset for\nmodel testing using the same hyperparameters. We also con-\nducted ablation studies to evaluate the contribution of various\ncomponents on the overall GTP framework. By blocking out\nthe GTP components, we were left with a framework that is\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3009\nTABLE II\nPERFORMANCE METRICS FOR THE 3-L ABEL (NORMAL VS .L U A DVS. LSCC) C LASSIFICATION TASK . MEAN PERFORMANCE METRICS ARE\nREPORTED ALONG WITH THE CORRESPONDING VALUES OF STANDARD DEVIATION IN PARENTHESES\ncomparable to the state-of-the-art in the ﬁeld. Finally, we used\nGraphCAMs to identify salient regions on the WSIs, and\nexplored their validity in terms of highlighting the histopatho-\nlogic regions of interest.\nA. Experimental Settings\nEach WSI was cropped to create a bag of 512 × 512 non-\noverlapping patches at 20 × magniﬁcation, and background\npatches with non-tissue area > 50% were discarded. We used\nResnet18 as the CNN backbone for the feature extractor.\nWe adapted the Adam optimizer with an initial learning\nrate of 0 .0001, a cosine annealing scheme for learning rate\nscheduling, and a mini-batch size of 512. We kept the trained\nfeature extractor and used it to build the graphs. We used\none graph convolutional layer, and set the transformer layer\nconﬁgurations as L = 3, MLP size = 128, D = 64 and k\n= 8 (Eq.4, Eq.3). The GTP model was trained in batches of\n8 examples for 150 iterations. The learning rate was set to\n10\n−3 initially, and decayed to 10 −4 and 10 −5 at step 30 and\n100, respectively.\nB. Ablation Studies\nTo evaluate the effectiveness of our proposed GTP frame-\nwork, we performed several ablation studies as well as\ncompared the GTP model perfo rmance with other state-\nof-the-art methods ( Table II ). Since our framework lever-\nages graph-based learning and transformer architectures,\nwe selected a graph-based method called AttPool [15] and\na transformer-based framework called TransMIL [27] for\ncomparison, as they both retain spatial information to make\na WSI-level prediction. To make a fair comparison, we used\nthe same contrastive learning based model as the feature\nextractor for all methods. When implementing AttPool and\nTransMIL, we ﬁne-tuned the hyperparameters used in the\npreviously published original work to achieve the best perfor-\nmance on CPTAC and TCGA cohorts. Later, we removed the\ntransformer component and trained the graph and compared\nit with the full GTP framework to study the performance\nof the graph-based portion of our model. Due to computa-\ntional constraints, it was not feasible to remove the graph\ncomponent and train the transformer; there was an out-of-\nmemory issue when the number of patches exceeded 4,760 for\nbatch size of 1 on a GeForce RTX 2080 Ti, GPU memory:\n11GB workstation.\nWe also performed a detailed analysis to evaluate the effect\nof contrastive learning on the GTP model performance by\nconducting studies with and without it ( Table III). We adopted\nthe supervised learning-based model, Resnet [40], instead\nof contrastive learning-based model. Resnet\n⋆ represents the\nmodel trained on ImageNet [13]. Resnet † represents Resnet ⋆\nﬁne-tuned on the same NLST data which is used for con-\ntrastive learning. To train Resnet\n† via supervised learning,\nwe assigned the WSI label to all patches belonging to the\nsame WSI. We also added an unsupervised learning model,\na convolutional autoencoder (CAE) [41], for comparison.\nAdditionally, to make a fair comparison in terms of domain\nspeciﬁcity, we added the contrastive learning model trained on\nSTL-10 [42] to compare to the model trained on NLST. The\nSTL-10 dataset is an image recognition dataset for developing\nunsupervised feature learning, whose images were acquired\nfrom labeled examples on ImageNet, whereas the NLST\ndataset is a lung cancer imaging dataset. In essence, these\nablation studies allowed us to fully evaluate the power of the\nGTP framework for WSI-level classiﬁcation.\nC. Computing Infrastructure\nWe used PyTorch (v1.9.0) and a NVIDIA 2080Ti graphics\ncard with 11 GB memory on a GPU workstation to implement\n3010 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nTABLE III\nABLATION STUDIES ON THE FEATURE EXTRACTORS IN 3-L ABEL (NORMAL VS .L U A DVS. LSCC) C LASSIFICATION TASK . WE USED DIFFERENT\nFEATURE EXTRACTORS FOR GRAPH CONSTRUCTION AND EVALUATED THEIR ROLE ON THE OVERALL CLASSIFICATION\nTASK .H ERE ,R ESNET ⋆ INDICATES THE USE OF A PRE -T RAINED RESNET 18 NETWORK WITHOUT FINE -T UNING ,\nRESNET † INDICATES WITH FINE -T UNING .C A EISA CONVOLUTIONAL AUTO ENCODER .C LR EPRESENTS CONTRASTIVE\nLEARNING ON STL10 OR NLST BY OUR METHOD .M EAN PERFORMANCE METRICS ARE REPORTED\nALONG WITH THE CORRESPONDING VALUES OF STANDARD DEVIATION IN PARENTHESES\nthe model. The training speed was about 2 .4 iterations/s, and\nit took less than a day to reach convergence. The inference\nspeed was < 1 s per WSI with a batch size of 2.\nD. Performance Metrics\nFor the classiﬁcation task (LUAD vs. LSCC vs. normal),\nwe generated receiver operating characteristic (ROC) and\nprecision-recall (PR) curves b ased on model predictions on\nthe CPTAC testing and the full TCGA datasets. For each ROC\nand PR curve, we also computed the area under curve (AUC),\nprecision, recall, speciﬁcity, and accuracy. Since we used\n5-fold cross validation, we took all the curves from different\nfolds and calculated the mean AUCs and their variance.\nDelong’s statistical test was used to show whether the AUCs of\ntwo different models were signiﬁcantly different. GraphCAMs\nwere used to generate visualizations and gain a qualitative\nunderstanding on the model performance.\nE. Expert Annotations\nA randomly selected set of WSIs (n = 10 for LUAD\nand n = 10 for LSCC) were uploaded to a secure, web-\nbased software (PixelView; deepPath, Boston, MA) for expert\nreview. Using an Apple Pencil and an iPad, E.J.B. annotated\nseveral histologic features of LUAD and LSCC. Tumor regions\nof LUAD were annotated by their LUAD tumor subtypes\n(solid, micropapillary, cribiform, papillary, acinar, and lepidic).\nFor both LUAD and LSCC, other histologic features of the\ntumor were also annotated including necrosis, lymphatic inva-\nsion, and vascular invasion. Non-tumor regions were annotated\nas normal or premalignant epithelium, normal or inﬂammed\nlung, stroma, and cartilage. T he annotations of each region\nare reﬂective of the most pre dominant histologic feature.\nThe annotations were grouped into tumor and non-tumor\ncategories and exported as binary images and processed to\nquantify the extent of overlap between the model-derived\nGraphCAMs and the pathologist-driven annotations. Intersec-\ntion over union (IoU) was used as the metric to measure the\noverlap between the model and the expert.\nIV . R\nESULTS\nThe GTP framework that leveraged contrastive learning\nfollowed by fusion of a graph with a transformer provided\naccurate prediction of WSI-level class label ( Table II). High\nmodel performance was observed on the normal vs. LUAD\nvs. LSCC task on the CPTAC test dataset but dropped slightly\non the TCGA dataset. High model performance was also\nconﬁrmed via the ROC and PR curves generated on both\nthe CPTAC and TCGA datasets for all the classiﬁcation tasks\n(Fig. 3 ). On each task, the mean area under the ROC and\nPR curves was high (all > 0.9) on the CPTAC test data. For\nthe TCGA dataset, which was used for external testing, the\nmean area under the ROC and PR curves dropped slightly,\nespecially for the LUAD and LSCC classiﬁcation tasks. The\nmodel leaned towards incorrectly classifying a few LSCC and\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3011\nFig. 3. Model performance on the CPTAC and TCGA datasets. Mean ROC and PR curves along with standard deviations for the classiﬁcation\ntasks (normal vs. tumor; LUAD vs. others; LSCC vs. others) are shown.\nLUAD cases but correctly classiﬁed most of the WSIs with\nno tumor.\nThe GTP framework achieved the best performance com-\npared with other state-of-the-art methods such as Trans-\nMIL [27] or AttPool [15] ( Table II). The AttPool framework\nselects the most signiﬁcant nodes in the graph and aggre-\ngates information via the attention mechanism. The TransMIL\nframework uses a self-attention mechanism from the trans-\nformer to model the interactions between all patches from\nthe WSI for information aggregation. We submit that the\ngraph structure along with graph pooling enabled the GTP\nto capture the short-range associations while the transformer\nhelped capture the long-range associations. Also, since we\nwere able to perform batch training instead of just a single\nsample per iteration, the model achieved faster convergence\nand also processed any potential variabilities between multiple\nsamples in the training process.\nThe GT-based class activation maps (GraphCAMs) identi-\nﬁed WSI regions that were highly associated with the output\nclass label ( Fig. 4 ). We also observed a high degree of\noverlap between the expert-identiﬁed regions of interest with\nthe binarized GraphCAMs. Sp eciﬁcally, the maximum value\nof the intersection over union (IoU) was 0 .857 at threshold\nprobability of 0.6 for the LUAD case (Row 1 in Fig. 4), and an\nIoU of 0.733 at threshold probability of 0.2 for the LSCC case\n(Row 2 in Fig. 4). Additionally, the expert pathologist selected\n20 cases from the TCGA cohort (10 LUAD and 10 LSCC)\nand annotated the tumor regions on them. We then estimated\nthe overlap between the model-identiﬁed regions of interest\n(via GraphCAMs) and the pathol ogist annotations, resulting\nin a high degree of agreement (Mean of maximum values\nof IoU = 0.817 ± 0.165). Importantly, the degree of overlap\nremained fairly consistent as the threshold for binarization\nvaried, supporting that our approach to identifying impor-\ntant WSI regions has clinicopathologic signiﬁcance. We also\nobserved that the same set of WSI regions were highlighted by\nour method across the various cross-validation folds ( Fig. 5),\nthus indicating consistency with our technique in identify-\ning salient regions of interest. Also, since we can generate\nclass-speciﬁc probability for each GraphCAM, our approach\nallows for better appreciation o f the model performance and\nits interpretability in predicting an output class label. We must\nhowever note that in certain cases when the model fails to\npredict the class label, the GraphCAMs may not result in\ninterpretable ﬁndings ( Fig. 6).\nAblation studies revealed that our GTP framework that\nused contrastive learning and combined a graph with a trans-\nformer served as a superior mode l for WSI-level classiﬁcation\n(Table III ). When contrastive learning was replaced with\na pre-trained architecture (Resnet18 with and without ﬁne\n3012 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nFig. 4. GraphCAMs and their comparison with the expert annotations. For each WSI, we generated GraphCAMs and compared them with\nannotations from the pathologist. The ﬁrst column contains the originalWSIs, the second and third columns contain GraphCAMs and pathologist’s\nannotations, respectively and the fourth column contains the binarized GraphCAMs based on the threshold from the Intersection of Union (IoU) plot\nin the last column. The ﬁrst row shows an LUAD case and the second row denotes an LSCC case.\nFig. 5. Graph class activation map performance. GraphCAMs generated on WSIs across the runs performed via 5-fold cross validation are\nshown. The ﬁrst column shows the original WSIs and the other columns show the GraphCAMs with prediction probabilities on the cross-validated\nmodel runs. The ﬁrst row shows a sample WSI from the LUAD class and the second row shows an WSI from the LSCC class. The colormap\nrepresents the probability by which an WSI region is associated with the output label of interest.\ntuning), the model performance for the 3-label classiﬁcation\ntask dropped. The reduction in performance was evident on\nboth CPTAC and TCGA datasets. The model performance\nalso dropped when we trained a novel convolutional auto-\nencoder [41] in lieu of contrastive learning. These results\nimply that the feature maps generated via contrastive learning\nwere informative to encode a variety of visual information for\nGT-based classiﬁcation with a fair degree of generalizability.\nWe also studied the effect of the domain speciﬁcity of the\ndataset on the model performance, by comparing contrastive\nlearning models trained on STL-10 and NLST ( Table III). The\nSTL-10 [42] is an image recognition dataset for developing\nunsupervised feature learning. The images were acquired\nfrom labeled examples on ImageN et. The contrasive learning\nmodels trained on NLST performed signiﬁcantly better than\nSTL-10, indicating that the domain knowledge learned by\ncontrastive learning helps improve the generalizability of our\nGTP model. ( Table III).\nTo analyze the impact of different model hyperparameters\nand graph conﬁgurations on the classiﬁcation performance,\nwe performed a series of ablation studies ( Table IV). In gen-\neral, the consistency of the results generated using different\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3013\nFig. 6. GraphCAMs for failure cases. The ﬁrst row shows a sample\nWSI from the LUAD class where the model prediction was LSCC, and\nthe second row shows an WSI from the LSCC class where the model\nprediction was LUAD. The ﬁrst column shows the original WSI, and the\nsecond and third columns show the generated GraphCAMs along with\nprediction probabilities. The bold font underneath certain GraphCAMs\nwas used to indicate the model predicted class label for the respective\ncases. Since this is a 3-label classiﬁcation task (normal vs. LUAD vs.\nLSCC), the LUAD and LSCC probability values do not add up to 1.\nTABLE IV\nA\nBLATION STUDIES ON MODEL HYPERPARAMETERS . VARIOUS\nHYPERPARAMETERS WERE VARIED TO EVALUATE THEIR EFFECT\nON THE OVERALL MODEL PERFORMANCE .A LL THE MODELS WERE\nTRAINED AND EVALUATED USING A PORTION OF DATA FROM THE\nCPTAC COHORT , AND MODEL ACCURACY WAS REPORTED ON THE\nLEFT-OUT CPTAC CASES (LAST COLUMN ). THE MLP DIMENSION\nOF THE MODEL WAS 128. WE USED WSI S WITH 20× MAGNIFICATION\nFOR ALL CASES .W E USED NON OVERLAPPING PATCHES FOR ALL\nCASES EXCEPT FOR ⋆.T HE BATCH SIZE USED WAS 8E XCEPT\nFOR †.F OR ALL THESE STUDIES , THE CPTAC DATA\nWAS RANDOML YDIVIDED IN 7:3 RATI O,W HERE\n70/B1 DATA WAS USED FOR TRAINING AND THE\nREST FOR TESTING\nchoices of hyperparameters demonstrate robustness of our\nproposed model. When the dimension of the hidden state was\nreduced by half (i.e., 128 to 64), we noticed that the model\nperformance degraded. Similar r esults in model performance\nwere observed when the number of GCN layers reduced from\n3 to 1. In essence, additional GCN layers increase the receptive\nﬁeld of the nodes and could continue to provide long-term\ninformation during model training. Increasing the number of\nmin-cut nodes improved the performance, since typically rep-\nresenting the data using more tokens can contribute to higher\nprediction accuracy [14]. Also, we found that 3 transformer\nblocks were sufﬁcient to integrate information across the entire\npooled nodes from the graph. It must be noted that using more\nparameters for model training might alter the performance but\nwould be feasible only at a higher computational cost. For\ninstance, the cost can increase quadratically with respect to\nthe token number in the self-attention blocks. Nevertheless,\nto ﬁnd a trade-off between computational efﬁciency and model\neffectiveness, we tested various parameters including the effect\nof different forms of the graph construction. Increasing the\npatch size lowered the accuracy. Using 4-node connectivity\ninstead of 8-node connectivity reduced the receptive ﬁeld,\nresulting in lower accuracy. Choosing smaller patch sizes\nincreased the size of graph, which increased the computa-\ntional cost. Lastly, when a 10% overlap between patches\nwas considered and graph constructed, the model performance\nslightly reduced. These ﬁndings indicate that our proposed\nGTP framework along with the selected hyperparameters is\ncapable of reasonably integrating information across the entire\nWSI to predict WSI-level output of interest.\nV. D\nISCUSSION\nIn this work, we developed a deep learning framework\nthat integrates graphs with vision transformers to generate an\nefﬁcient classiﬁer to differentiate normal WSIs from those with\nLUAD or LSCC. Based on the standards of various model\nperformance metrics, our approach resulted in classiﬁcation\nperformance that exceeded othe r deep learning architectures\nthat incorporated various state-of-the-art conﬁgurations. Also,\nour novel class activation mapping technique allowed us to\nidentify salient WSI regions that were highly associated with\nthe output class label of interest. Finally, we found that\nour model-identiﬁed regions of interest closely matched with\npathologist-derived assessments on the WSIs. Thus, our ﬁnd-\nings represent novel contributions to the ﬁeld of interpretable\ndeep learning while also simultaneously advancing the ﬁelds\nof computer vision and digital pathology.\nThe ﬁeld of computational pathology has made great strides\nrecently due to advan cements in vision-based deep learning.\nStill, owing to the sheer size of pathology images generated\nat high resolution, WSI assessment that can integrate spa-\ntial signatures along with local, region-speciﬁc information\nfor prediction of tumor grade remains a challenge. A large\nbody of work has focused on patch-level models that may\naccurately predict tumor grad e but fail to capture spatial\nconnectivity information. As a result, identiﬁcation of impor-\ntant image-level features via such techniques may lead to\ninconsistent results. Our GTP framework precisely tackled\nthis scenario by integrating WSI-level information via a graph\nstructure and thus represents a key advancement in the ﬁeld.\n3014 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 11, NOVEMBER 2022\nOne of the novel contributions in our work is the generation\nof graph-based class activation maps (GraphCAM), which can\nhighlight the WSI regions that are associated with the output\nclass label. Unlike other saliency mapping techniques such as\nattention rollout [43] or layer-wise relevance propagation [44],\nGraphCAMs can generate class-speciﬁc heatmaps. This is a\nmajor advantage because an im age may contain information\npertaining to multiple classes, and for these scenarios, identiﬁ-\ncation of class-speciﬁc feature m aps becomes important. This\nis especially true in real-world scenarios such as pathology\nimages containing lung tumors . Class-speciﬁc features could\nbe useful, for example, in identifying rare mixed histology\nadenosquamous tumors that contain both LUAD and LSCC\npatterns [45]. In such cases, training well-known supervised\ndeep learning classiﬁers such as convolutional neural networks\nthat use the overall WSI label for classiﬁcation at patch-level\nor even at the WSI-level may not necessarily perform well\nand even misidentify the regions of interest associated with\nthe class label. By generating class-speciﬁc CAMs learned at\nthe WSI-level, our GTP approach provides a better way to\nidentify regions of interest on WSIs that are highly associated\nwith the corresponding class label.\nOur study has a few limitations. We leveraged contrastive\nlearning to generate patch-level feature vectors before con-\nstructing the graph, which turned out to be a computation-\nally intensive task. Future studies can explore other possible\ntechniques for feature extraction that can lead to improved\nmodel performance. Our graph was constructed by dividing the\nWSI into image patches, followed by creation of nodes using\nthe embedding features from these patches. Other alternative\nways can be explored to deﬁne the nodes and create graphs\nthat are more congruent and spatially connected. While we\nhave demonstrated the applicability of GTP to lung tumors,\nextension of this framework to other cancers is needed to fully\nappreciate its role in terms of assessing WSI-level correlates of\ndisease. In fact, our method is not speciﬁc to cancers and could\nbe adapted to other computational pathology applications.\nIn conclusion, our GTP framework produced an accurate,\ncomputationally efﬁcient model by capturing regional and\nWSI-level information to predi ct the output class label. GTP\ncan tackle high resolution WSIs and predict multiple class\nlabels, leading to generation o f interpretable ﬁndings that\nare class-speciﬁc. Our GTP framework could be scaled to\nWSI-level classiﬁcation tasks on other organ systems and has\nthe potential to predict response to therapy, cancer recurrence\nand patient survival.\nA\nCKNOWLEDGMENT\nThe authors thank the National Cancer Institute (NCI) for\naccess to NCI’s data collected by the National Lung Screening\nTrial (NLST). The statements contained herein are solely those\nof the authors and do not represent or imply concurrence or\nendorsement by the NCI.\nR\nEFERENCES\n[1] D. N. Louis et al., “Computational pathology: An emerging deﬁnition,”\nArch. Pathol. Lab. Med., vol. 138, no. 9, pp. 1133–1138, Sep. 2014.\n[2] D. N. Louis et al., “Computational pathology: A path ahead,” Arch.\nPathol. Lab. Med., vol. 140, no. 1, pp. 41–50, 2015.\n[3] E. Abels et al., “Computational pathology deﬁnitions, best practices,\nand recommendations for regulatory guidance: A white paper from the\ndigital pathology association,” J. Pathol., vol. 249, no. 3, pp. 286–294,\nNov. 2019.\n[4] T. J. Fuchs and J. M. Buhmann, “Com putational pathology: Challenges\nand promises for tissue analysis,” Computerized Med. Imag. Graph.,\nvol. 35, nos. 7–8, pp. 515–530, Oct./Dec. 2011.\n[5] X. Wang et al., “Weakly supervised deep learning for whole slide\nlung cancer image analysis,” IEEE Trans. Cybern., vol. 50, no. 9,\npp. 3950–3962, Sep. 2020.\n[6] S. Wang, D. M. Yang, R. Rong, X. Zhan, and G. Xiao, “Pathology image\nanalysis using segmentation deep learning algorithms,” Amer. J. Pathol.,\nvol. 189, no. 9, pp. 1686–1698, Sep. 2019.\n[7] N. Coudray et al., “Classiﬁcation and mutation prediction from non-\nsmall cell lung cancer histopathology images using deep learning,”\nNature Med., vol. 24, no. 10, pp. 1559–1567, 2018.\n[8] J. Saltz et al., “Spatial organization and molecular correlation of tumor-\ninﬁltrating lymphocytes using deep learning on pathology images,” Cell\nRep., vol. 23, no. 1, pp. 181–193, 2018.\n[9] L .H o u ,D .S a m a r a s ,T .M .K u r c ,Y .G a o ,J .E .D a v i s ,a n dJ .H .S a l t z ,\n“Patch-based convolutional neural network for whole slide tissue image\nclassiﬁcation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2016, pp. 2424–2433.\n[10] J. W. Wei, L. J. Tafe, Y . A. Linnik, L. J. Vaickus, N. Tomita, and\nS. Hassanpour, “Pathologist-level classiﬁcation of histologic patterns on\nresected lung adenocarcinoma slides with deep neural networks,” Sci.\nRep., vol. 9, no. 1, pp. 1–8, Dec. 2019.\n[11] K. Das, S. Conjeti, J. Chatterjee, and D. Sheet, “Detection of breast\ncancer from whole slide histopathological images using deep multiple\ninstance CNN,” IEEE Access, vol. 8, pp. 213502–213511, 2020.\n[12] Y. Z h e n get al., “Deep-learning-driven quantiﬁcation of interstitial\nﬁbrosis in digitized kidney biopsies,” Amer. J. Pathol., vol. 191, no. 8,\npp. 1442–1453, 2021.\n[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248–255.\n[14] A. Dosovitskiy et al., “An image is worth 16 ×16 words: Transform-\ners for image recognition at scale,” in Proc. ICLR, Vienna, Austria,\n2021.\n[15]\nJ. Huang, Z. Li, N. Li, S. Liu, and G. Li, “AttPool: Towards hierarchical\nfeature representation in graph conv olutional networks via attention\nmechanism,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV) ,\nOct. 2019, pp. 6480–6489.\n[16] Y . Zhou, S. Graham, N. A. Koohbanani , M. Shaban, P.-A. Heng, and\nN. Rajpoot, “CGC-Net: Cell graph c onvolutional network for grading\nof colorectal cancer histology images,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis. Workshop (ICCVW), Oct. 2019, pp. 388–398.\n[17] M. Adnan, S. Kalra, and H. R. Tizhoos h, “Representation learning of\nhistopathology images using graph neural networks,” inProc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2020,\npp. 4254–4261.\n[18] W. Lu, S. Graham, M. Bilal, N. Rajpoot, and F. Minhas, “Capturing cel-\nlular topology in multi-gigapixel pathology images,” in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2020,\npp. 1049–1058.\n[19] R. Li, J. Yao, X. Zhu, Y . Li, and J. Huang, “Graph CNN for survival\nanalysis on whole slide pathological images,” in Medical Image Com-\nputing and Computer Assisted Intervention MICCAI 2018, A. F. Frangi,\nJ. A. Schnabel, C. Davatzikos, C. Alb erola-López, and G. Fichtinger,\nEds. Cham, Switzerland: Springer, 2018, pp. 174–182.\n[20] D. Di, S. Li, J. Zhang, and Y . Gao, “Ra nking-based survival prediction\non histopathological whole-slide images,” in Medical Image Computing\nand Computer Assisted Intervention MICCAI 2020 , A. L. Martel,\nP. Abolmaesumi, D. Stoyanov, D. Mateus, M. A. Zuluaga, S. K. Zhou,\nD. Racoceanu, and L. Joskowicz, Eds. Cham, Switzerland: Springer,\n2020, pp. 428–438.\n[21] R. J. Chen et al., “Whole slide images are 2d point clouds: Context-\naware survival prediction using patc h-based graph convolutional net-\nworks,” in Medical Image Computing and Computer Assisted Interven-\ntion MICCAI 2021, M. de Bruijne, P. C. Cattin, S. Cotin, N. Padoy,\nS. Speidel, Y . Zheng, and C. Essert , Eds. Cham, Switzerland: Springer,\n2021, pp. 339–349.\n[22] D. Di, J. Zhang, F. Lei, Q. Tian, and Y . Gao, “Big-hypergraph\nfactorization neural network for survival prediction from whole\nslide image,” IEEE Trans. Image Process., vol. 31, pp. 1149–1160,\n2022.\nZHENG et al.: GRAPH-TRANSFORMER FOR WHOLE SLIDE IMAGE CLASSIFICATION 3015\n[23] Y. Z h a oet al., “Predicting lymph node metastasis using histopathological\nimages based on multiple instance l earning with deep graph convolu-\ntion,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2020, pp. 4837–4846.\n[24] K. Ding, Q. Liu, E. Lee, M. Zhou, A. Lu, and S. Zhang, “Feature-\nenhanced graph networks for gene tic mutational prediction using\nhistopathological images in colon cancer,” in Medical Image Computing\nand Computer Assisted Intervention MICCAI 2020 , A. L. Martel,\nP. Abolmaesumi, D. Stoyanov, D. Mateus, M. A. Zuluaga, S. K. Zhou,\nD. Racoceanu, and L. Joskowicz, Eds. Cham, Switzerland: Springer,\n2020, pp. 294–304.\n[25] J. Shi, R. Wang, Y . Zheng, Z. Jiang, H. Zhang, and L. Yu, “Cervical\ncell classiﬁcation with graph convolutional network,” Comput. Methods\nPrograms Biomed., vol. 198, Jan. 2021, Art. no. 105807.\n[26] Y. Z h e n get al., “Encoding histopathology whole slide images with\nlocation-aware graphs for diagnostically relevant regions retrieval,”Med.\nImage Anal., vol. 76, Feb. 2022, Art. no. 102308.\n[27] Z. Shao et al., “TransMIL: Transformer based correlated multiple\ninstance learning for whole slide image classiﬁcation,” in Proc. Adv.\nNeural Inf. Process. Syst., vol. 34, 2021, pp. 2136–2147.\n[28] Y . Xiong et al. , “Nyströmformer: A Nyström-based algorithm\nfor approximating self-attention,” in Proc. AAAI , vol. 35, 2021,\npp. 14138–14148.\n[29] N. J. Edwards et al., “The CPTAC data portal: A resource for cancer\nproteomics research,” J. Proteome Res., vol. 14, no. 6, pp. 2707–2713,\nJun. 2015.\n[30] The National Lung Screening Trial Research Team, “Reduced lung-\ncancer mortality with low-dose computed tomographic screening,” New\nEngland J. Med., vol. 365, pp. 395–409, 2011.\n[31] TCGA Research Network. The Cancer Genome Atlas Program .\nAccessed: Oct. 2021. [Online]. Available: https://portal.gdc.cancer.gov/\n[32] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple\nframework for contrastive learning of visual representations,” in Proc.\nICML, 2020, pp. 1597–1607.\n[33] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in Proc. ICLR, Toulon, France, 2017.\n[34] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30. Red Hook, NY , USA: Curran Associates, 2017,\npp. 1–11.\n[35] M. A. Islam, S. Jia, and N. D. B. Bruce, “How much position\ninformation do convolutional neural networks encode?” in Proc. ICLR,\n2020.\n[36] X. Zhu et al., “Deformable DETR: Deformable transformers for end-\nto-end object detection,” in\nProc. ICLR, 2021.\n[37] J. Chen et al. , “TransUNet: Transformers make strong\nencoders for medical image segmentation,” 2021, arXiv:2102.\n04306.\n[38] F. M. Bianchi, D. Grattarola, and C. Alippi, “Spectral clustering with\ngraph neural networks for graph pooling,” in Proc. ICML, 2020,\npp. 874–883.\n[39] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond\nattention visualization,” 2020, arXiv:2012.09838.\n[40] K. He, X. Zhang, S. Ren, and J. S un, “Deep residual learning for\nimage recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[41] J. Masci, U. Meier, D. Cire¸ san, and J. Schmidhuber, “Stacked\nconvolutional auto-encoders fo r hierarchical feature extrac-\ntion,” in Proc. ICANN . Berlin, Germany: Springer, 2011,\npp. 52–59.\n[42] A. Coates, H. Lee, and A. Y . Ng, “An analysis of single-layer net-\nworks in unsupervised feature learning,” J .M a c h .L e a r n .R e s ., vol. 15,\npp. 215–223, Jan. 2011.\n[43] S. Abnar and W. Zuidema, “Quantifying attention ﬂow in transform-\ners,” in Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, 2020,\npp. 4190–4197.\n[44] A. Binder, G. Montavon, S. Lapuschki n, K.-R. Müller, and W. Samek,\n“Layer-wise relevance propagation for neural networks with local renor-\nmalization layers,” in Proc. ICANN. Cham, Switzerland: Springer, 2016,\npp. 63–71.\n[45] A. G. Nicholson et al., “The 2021 WHO classiﬁcation of lung tumors:\nImpact of advances since 2015,” J. Thoracic Oncol., vol. 17, no. 3,\npp. 362–387, Mar. 2022.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6349059343338013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5897172689437866
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4973030388355255
    },
    {
      "name": "Graph",
      "score": 0.4684246778488159
    },
    {
      "name": "Theoretical computer science",
      "score": 0.10581779479980469
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111088046",
      "name": "Boston University",
      "country": "US"
    }
  ]
}