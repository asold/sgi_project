{
    "title": "Equivariant Transformer Networks",
    "url": "https://openalex.org/W2913651928",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4287530215",
            "name": "Tai, Kai Sheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3045217533",
            "name": "Bailis, Peter",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221688722",
            "name": "Valiant, Gregory",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2279221249",
        "https://openalex.org/W2569680626",
        "https://openalex.org/W2259303769",
        "https://openalex.org/W2962686123",
        "https://openalex.org/W2962697559",
        "https://openalex.org/W127298752",
        "https://openalex.org/W2963564809",
        "https://openalex.org/W2136026194",
        "https://openalex.org/W2078238424",
        "https://openalex.org/W2061543840",
        "https://openalex.org/W1994197834",
        "https://openalex.org/W2562066862",
        "https://openalex.org/W2335728318",
        "https://openalex.org/W2340427832",
        "https://openalex.org/W2576915720",
        "https://openalex.org/W2797275157",
        "https://openalex.org/W2963299736",
        "https://openalex.org/W2963022858",
        "https://openalex.org/W1963943638",
        "https://openalex.org/W2118877769",
        "https://openalex.org/W2963703618",
        "https://openalex.org/W2131372145",
        "https://openalex.org/W603908379",
        "https://openalex.org/W2069114279",
        "https://openalex.org/W2785523195",
        "https://openalex.org/W2964298108",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2621199038",
        "https://openalex.org/W2060510461",
        "https://openalex.org/W2769152880",
        "https://openalex.org/W1991605728",
        "https://openalex.org/W2090196588"
    ],
    "abstract": "How can prior knowledge on the transformation invariances of a domain be incorporated into the architecture of a neural network? We propose Equivariant Transformers (ETs), a family of differentiable image-to-image mappings that improve the robustness of models towards pre-defined continuous transformation groups. Through the use of specially-derived canonical coordinate systems, ETs incorporate functions that are equivariant by construction with respect to these transformations. We show empirically that ETs can be flexibly composed to improve model robustness towards more complicated transformation groups in several parameters. On a real-world image classification task, ETs improve the sample efficiency of ResNet classifiers, achieving relative improvements in error rate of up to 15% in the limited data regime while increasing model parameter count by less than 1%.",
    "full_text": "Equivariant Transformer Networks\nKai Sheng Tai1 Peter Bailis 1 Gregory Valiant1\nAbstract\nHow can prior knowledge on the transforma-\ntion invariances of a domain be incorporated\ninto the architecture of a neural network? We\npropose Equivariant Transformers (ETs), a fam-\nily of differentiable image-to-image mappings\nthat improve the robustness of models towards\npre-deﬁned continuous transformation groups.\nThrough the use of specially-derived canonical co-\nordinate systems, ETs incorporate functions that\nare equivariant by construction with respect to\nthese transformations. We show empirically that\nETs can be ﬂexibly composed to improve model\nrobustness towards more complicated transforma-\ntion groups in several parameters. On a real-world\nimage classiﬁcation task, ETs improve the sample\nefﬁciency of ResNet classiﬁers, achieving rela-\ntive improvements in error rate of up to 15% in\nthe limited data regime while increasing model\nparameter count by less than 1%.\n1. Introduction\nIn computer vision, we are often equipped with prior knowl-\nedge on the transformation invariances of a domain. Con-\nsider, for example, the problem of classifying street signs\nin real-world images. In this domain, we know that the\nappearance of a sign in an image is subject to various defor-\nmations: the sign may be rotated, its scale will depend on its\ndistance, and it may appear distorted due to perspective in\n3D space. Regardless, the identity of the street sign should\nremain invariant to these transformations.\nWith the exception of translation invariance, convolutional\nneural network (CNN) architectures typically do not take\nadvantage of such prior knowledge on the transformation\ninvariances of the domain. Instead, current standard practice\nheuristically incorporates these priors during training via\ndata augmentation (e.g., by applying a random rotation or\n1Stanford University, Stanford, CA, USA. Correspondence to:\nKai Sheng Tai <kst@cs.stanford.edu>.\nProceedings of the 36 th International Conference on Machine\nLearning, Long Beach, California, PMLR 97, 2019. Copyright\n2019 by the author(s).\nscaling to each training image). While data augmentation\ntypically helps reduce the test error of CNN-based models,\nthere is no guarantee that transformation invariance will be\nenforced for data not seen during training.\nIn contrast to training time approaches like data augmen-\ntation, recent work on group equivariant CNNs (Cohen &\nWelling, 2016; Dieleman et al., 2016; Marcos et al., 2017;\nWorrall et al., 2017; Henriques & Vedaldi, 2017; Cohen\net al., 2018) has explored new CNN architectures that are\nguaranteed to respond predictably to particular transforma-\ntions of the input. For example, the CNN model family\nmay be constrained such that a rotation of the input results\nin a corresponding rotation of its subsequent representa-\ntion, a property known as equivariance. However, these\ntechniques—most commonly designed for rotations and\ntranslations of the input (e.g., Dieleman et al. (2016); Mar-\ncos et al. (2017); Worrall et al. (2017))—fail to generalize\nto deeper compositions of continuous transformations. This\nlimits the applicability of these techniques in more compli-\ncated real-world scenarios involving continuous transforma-\ntions in several dimensions, such as the above example of\nstreet sign classiﬁcation.\nTo address these shortcomings of group equivariant CNNs,\nwe propose Equivariant Transformer (ET) layers, a ﬂexible\nclass of functions that improves robustness towards arbitrary\npre-deﬁned groups of continuous transformations. An ET\nlayer for a transformation group Gis an image-to-image\nmapping that satisﬁes the following local invariance prop-\nerty: for any input image φand transformation T ∈G, the\nimages φand Tφ are both mapped to the same output im-\nage. ET layers are differentiable with respect to both their\nparameters and input, and thus can be easily incorporated\ninto existing CNN architectures. Additionally, ET layers\ncan be ﬂexibly combined to achieve improved invariance\ntowards more complicated compositions of transformations\n(e.g., simultaneous rotation, scale, shear, and perspective\ntransformations).\nImportantly, the invariance property of ETs holds by con-\nstruction, without any dependence on additional heuristics\nduring training. We achieve this by using the method of\ncanonical coordinates for Lie groups (Rubinstein et al.,\n1991). The key property of canonical coordinates that we\nutilize is their ability to reduce arbitrary continuous transfor-\narXiv:1901.11399v2  [cs.CV]  24 May 2019\nEquivariant Transformer Networks\nmations to translation. For example, polar coordinates are\ncanonical coordinates for the rotation group, since a rotation\nreduces to a translation in the angular coordinate. These\nspecialized coordinates can be analytically derived for a\ngiven transformation and efﬁciently implemented within a\nneural network.\nWe evaluate the performance of ETs using both synthetic\nand real-world image classiﬁcation tasks. Empirically, ET\nlayers improve the sample efﬁciency of image classiﬁers rel-\native to standard Spatial Transformer layers (Jaderberg et al.,\n2015). In particular, we demonstrate that ET layers improve\nthe sample efﬁciency of modern ResNet classiﬁers on the\nStreet View House Numbers dataset, with relative improve-\nments in error rate of up to 15% in the limited data regime.\nMoreover, we show that a ResNet-10 classiﬁer augmented\nwith ET layers is able to exceed the accuracy achieved by\na more complicated ResNet-34 classiﬁer without ETs, thus\nreducing both memory usage and computational cost.\n2. Related Work\nEquivariant CNNs. There has been substantial recent in-\nterest in CNN architectures that are equivariant with respect\nto transformation groups other than translation. Equivari-\nance with respect to discrete transformation groups (e.g., re-\nﬂections and 90o rotations) can be achieved by transforming\nCNN ﬁlters or feature maps using the group action (Cohen\n& Welling, 2016; Dieleman et al., 2016; Laptev et al., 2016;\nMarcos et al., 2017; Zhou et al., 2017). Invariance can then\nbe achieved by pooling over this additional dimension in\nthe output of each layer. In practice, this technique supports\nonly relatively small discrete groups since its computational\ncost scales linearly with the cardinality of the group.\nMethods for achieving equivariance with respect to con-\ntinuous transformation groups fall into one of two classes:\nthose that expand the input in a steerable basis (Amari,\n1978; Freeman & Adelson, 1991; Teo, 1998; Worrall et al.,\n2017; Jacobsen et al., 2017; Weiler et al., 2018; Cohen\net al., 2018), and those that compute convolutions under\na specialized coordinate system (Rubinstein et al., 1991;\nSegman et al., 1992; Henriques & Vedaldi, 2017; Esteves\net al., 2018). The relationship between these two categories\nof methods is analogous to the duality between frequency\ndomain and time domain methods of signal analysis. Our\nwork falls under the latter category that uses coordinate\nsystems specialized to the transformation groups of interest.\nEquivariance via Canonical Coordinates. Henriques &\nVedaldi (2017) apply CNNs to images represented using\ncoordinate grids computed using a given pair of continuous,\ncommutative transformations. Closely related to this tech-\nnique are Polar Transformer Networks (Esteves et al., 2018),\na method that handles images deformed by translation, rota-\ntion, and dilation by ﬁrst predicting an origin for each image\nbefore applying a CNN over log-polar coordinates. Unlike\nthese methods, we handle higher-dimensional transforma-\ntion groups by passing an input image through a sequence\nof ET layers in series. In contrast to Henriques & Vedaldi\n(2017), where a pair of commutative transformations is\nassumed to be given as input, we show how canonical co-\nordinate systems can be analytically derived given only a\nsingle one-parameter transformation group using technical\ntools described by Rubinstein et al. (1991).\nSpatial Transformer Networks. As with Spatial Trans-\nformer (ST) layers (Jaderberg et al., 2015), our ET layers\naim to factor out nuisance modes of variation in images due\nto various geometric transformations. Unlike STs, ETs in-\ncorporate additional structure in the functions used to predict\ntransformations. We expand on the relationship between\nETs and STs in the following sections.\nLocally-Linear Approximations. Gens & Domingos\n(2014) use local search to approximately align ﬁlters to\nimage patches, in contrast to our use of a global change\nof coordinates. The sequential pose prediction process in\na stack of ET layers is also reminiscent of the iterative\nnature of the Lucas-Kanade (LK) algorithm and its descen-\ndants (Lucas & Kanade, 1981; Lin & Lucey, 2017).\nImage Registration and Canonicalization. ETs are re-\nlated to classic “phase correlation” techniques for image\nregistration that compare the Fourier or Fourier-Mellin trans-\nforms of an image pair (De Castro & Morandi, 1987; Reddy\n& Chatterji, 1996); these methods can be interpreted as\nFourier basis expansions under canonical coordinate sys-\ntems for the relevant transformations. Additionally, the\nnotion of image canonicalization relates to work on de-\nformable templates, where object instances are generated\nvia deformations of a prototypical object (Amit et al., 1991;\nYuille, 1991; Shu et al., 2018).\n3. Problem Statement\nIn this section, we begin by reviewing inﬂuential prior\nwork on image canonicalization with Spatial Transform-\ners (Jaderberg et al., 2015). We then argue that the lack of\nself-consistency in pose prediction is a key weakness with\nthe standard ST that results in poor sample efﬁciency.\n3.1. Image Canonicalization with Spatial Transformers\nSuppose that we observed a collection of images φ(x), each\nof which is a mapping from image coordinates x ∈R2 to\npixel intensities in each channel. Each image is a trans-\nformed version of some latent canonical image φ∗: φ =\nTθφ∗:= φ∗(Tθx), where the transformation Tθ : R2 →R2\nis modulated by pose parameters θ∈Rk.\nEquivariant Transformer Networks\nFigure 1. Sample complexity for predicting rotations. Predicted rotation angles vs. true angles for a rotated MNIST digit (left). The\npredictions of a self-consistent pose predictor will be parallel to the diagonal (dotted line). (a) After training with 10k rotated examples, a\npose prediction CNN is not self-consistent; (b) with 50k rotated examples, it is only self-consistent over a limited range of angles. In\ncontrast, (c) a rotationally-equivariant CNN outputs self-consistent predictions after 10k examples (with small error due to interpolation\nand boundary effects). There is a nonzero bias in ˆθsince the pose labels are latent and there is no preferred image orientation.\nIf the transformation family and the pose parameters θ\nfor each image φ are known, then the learning problem\nmay be greatly simpliﬁed. If Tθ is invertible, then ac-\ncess to θ implies that we can recover φ∗ from φ via\nT−1\nθ φ= T−1\nθ Tθφ∗= φ∗. This is advantageous for learning\nwhen φ∗is drawn from a small or even ﬁnite set ( e.g., φ∗\ncould be sampled from a ﬁnite set of digits, while φbelongs\nto an inﬁnite set of transformed images).\nWhen the pose parameters are latent, as is typical in practice,\nwe can attempt to predict an appropriate inverse transfor-\nmation from the observed input.1 Based on this intuition, a\nSpatial Transformer (ST) layer L: Φ →Φ (Jaderberg et al.,\n2015) transforms an input image φusing pose parameters\nˆθ= f(φ) that are predicted as a function of the input:\nL(φ) = T−1\nf(φ)φ,\nwhere the pose predictor f : Φ →Rk is typically parame-\nterized as a CNN or fully-connected network.\n3.2. Self-Consistent Pose Prediction\nA key weakness of standard STs is the pose predictor’s\nlack of robustness to transformations of its input. As a\nmotivating example, consider images in a domain that is\nknown to be rotationally invariant ( e.g., classiﬁcation of\nastronomical objects), and suppose that we train an ST-\naugmented CNN that aims to canonicalize the rotation angle\nof input images. For some input φ, let the output of the pose\npredictor be f(φ) = ˆθ for some ˆθ ∈[0,2π). Then given\nTθφ(i.e., the same image rotated by an additional angle θ),\nwe should expect the output of an ideal pose predictor to be\nf(Tθφ) = ˆθ+ θ+ 2πmfor some integer m. In other words,\nthe pose prediction for an input φshould constrain those for\nTθφover the entire orbit of the transformation.\nWe refer to this desired property of the pose prediction\nfunction as self-consistency (Figure 2). In general, we say\n1For example, the apparent convergence of parallel lines in the\nbackground of an image can provide information on the correct\ninverse projective transformation to be applied.\nφ∈Φ Tθφ\nˆθ ˆθ+ θ\nImages\nPredicted poses\nTθ\nf\n+θ\nf\nFigure 2. Self-consistent pose prediction. We call a function\nf : Φ →Rk self-consistent if the action of a transformation Tθ\non its input results in a corresponding increment of θin its output.\nSelf-consistency is desirable for functions that predict the pose\n(e.g., rotation angle) of an object in an image.\nthat a pose prediction functionf : Φ →Rkis self-consistent\nwith respect to a transformation group G parameterized\nby θ ∈Rk if f(Tθφ) = f(φ) + θ, for any image φ and\ntransformation Tθ ∈G. We note that self-consistency is a\nspecial case of group equivariance.2\nHowever, there is no guarantee that self-consistency should\nhold when pose prediction is performed using a standard\nCNN or fully-connected network: while standard CNNs are\nequivariant with respect to translation, they are not equivari-\nant with respect to other transformation groups (Cohen &\nWelling, 2016). In Figure 1, we illustrate a simple example\nof this limitation of standard CNNs. Using MNIST digits\nrotated by angles uniformly sampled in θ ∈[0,2π), we\ntrain a CNN classiﬁer with a ST layer that predicts the ro-\ntation angle of the input image. During training, the model\nreceives a rotated image as input along with the class la-\nbel y∈{0,..., 9}; the true rotation angle θis unobserved.\nIn this example task, we ﬁnd that the poses predicted by\nthe CNN are only approximately self-consistent within a\nsmall range of angles, even when the network is trained with\n50,000 examples. In contrast, a rotation-equivariant CNN\ncan achieve approximate self-consistency given only 10,000\ntraining examples.\n2A function f is equivariant with respect to the group Gif\nthere exist transformations Tg and T′\ng such that f(Tgφ) = T′\ngf(φ)\nfor all g∈Gand φ∈Φ.\nEquivariant Transformer Networks\n4. Equivariant Transformers\nDue to this weakness of standard CNN pose predictors,\nwe will instead use functions that are guaranteed by con-\nstruction to satisfy self-consistency. We achieve this by\nleveraging the translation equivariance of standard CNN\narchitectures in combination with specialized canonical co-\nordinate systems designed for the particular transformation\ngroups of interest. Canonical coordinates allow us to reduce\nthe problem of self-consistent prediction with respect to an\narbitrary continuous transformation group to that of self-\nconsistent prediction with respect to the translation group.\nWe begin with preliminaries on canonical coordinates sys-\ntems (§4.1). We then describe our proposed Equivariant\nTransformer architecture ( §4.2). Next, we describe how\ncanonical coordinates can be derived for a given transfor-\nmation (§4.3). Finally, we describe how ET layers can be\napplied sequentially to handle compositions of several trans-\nformations (§4.4) and cover implementation details (§4.5).\n4.1. Canonical Coordinate Systems for Lie Groups\nThe method of canonical coordinates was ﬁrst described\nby Rubinstein et al. (1991) and later developed in more\ngenerality by Segman et al. (1992) for the purpose of com-\nputing image descriptors that are invariant under the action\nof continuous transformation groups.\nA Lie group with parameters θ∈Rk is a group of transfor-\nmations of the form Tθ : Rd →Rd that are differentiable\nwith respect to θ. We let the parameter θ= 0 correspond to\nthe identity element, T0x = x. A canonical coordinate sys-\ntem for Gis deﬁned by an injective map ρfrom Cartesian\ncoordinates to the new coordinate system that satisﬁes\nρ(Tθx) = ρ(x) +\nk∑\ni=1\nθiek, (1)\nfor all Tθ ∈G, where ei denotes the ith standard basis\nvector. Thus, a transformation by Tθ appears as a translation\nby θunder the canonical coordinate system. To help build\nintuition, we give two examples of canonical coordinates:\nExample 1 (Rotation). For Tθx = ( x1 cos θ −\nx2 sin θ,x1 sin θ + x2 cos θ), a canonical coordinate\nsystem is the polar coordinate system, ρ(x) =\n(tan−1(x2/x1),\n√\nx2\n1 + x2\n2).\nExample 2 (Horizontal Dilation). For Tθx = (x1eθ,x2),\na canonical coordinate system is ρ(x) = (log x1,x2).\nReduction to Translation. The key property of canon-\nical coordinates is their ability to adapt translation self-\nconsistency to other transformation groups. Formally, this\nis captured in the following result (we defer the straightfor-\nward proof to the Appendix):\nf ˆθ\nT−1\nˆθ\n(a) Spatial Transformer (ST)\nρ\n f ˆθ\nT−1\nˆθ\n(b) Equivariant Transformer (ET)\nFigure 3. Spatial and Equivariant Transformer architectures.\nIn both cases, pose parameters ˆθestimated as a function f of the\ninput image are used to apply an inverse transformation to the\nimage. The ET predicts ˆθ in a self-consistent manner using a\ncanonical coordinate system ρ.\nProposition 1. Let f : Φ →Rk be self-consistent with\nrespect to translation and let ρbe a canonical coordinate\nsystem with respect to a transformation group Gparameter-\nized by θ∈Rk. Then fρ(φ) := f(φ◦ρ−1) is self-consistent\nwith respect to G.\nGiven a canonical coordinate system ρfor a group G, we\ncan thus immediately achieve self-consistency with respect\nto Gby ﬁrst performing a change of coordinates into ρ, and\nthen applying a function that is self-consistent with respect\nto translation.\n4.2. Equivariant Transformer Layers\nOur proposed Equivariant Transformer layer leverages\ncanonical coordinates to incorporate prior knowledge on\nthe invariances of a domain into the network architecture:\nAn Equivariant Transformer (ET) layer LG,ρ : Φ →Φ\nfor the group Gwith canonical coordinates ρis deﬁned\nas:\nLG,ρ(φ) := T−1\nfρ(φ)φ (2)\nwhere the self-consistent pose predictor fρ is a CNN\nwhose input is represented using the coordinates ρ.\nThe ET layer is an image-to-image mapping that applies the\ninverse transformation of the predicted input pose, where the\npose prediction is performed using a network that satisﬁes\nself-consistency with respect to a pre-deﬁned group G. A\nstandard Spatial Transformer layer can be viewed as an ET\nwhere ρis simply the identity map. Like the ST, the ET\nlayer is differentiable with respect to both its parameters\nand its input; thus, it is easily incorporated as a layer in\nexisting CNN architectures. We summarize the computation\nencapsulated in the ET layer in Figure 3.\nEquivariant Transformer Networks\nLocal Invariance. Unlike ST layers, ET layers are en-\ndowed with a form of local transformation invariance: for\nany input image φ, we have that LG,ρ(φ) = LG,ρ(Tθφ)\nfor all Tθ ∈G. In other words, an ET layer collapses the\norbit generated by the group action on an image to a single,\n“canonical” point. This property follows directly from the\nself-consistency of the pose predictor with respect to the\ngroup G. Importantly, local invariance holds for any setting\nof the parameters of the ET layer; thus, ETs are equipped\nwith a strong inductive bias towards invariance with respect\nto the transformation group G.\nImplementing Self-Consistency. We implement transla-\ntion self-consistency in f by ﬁrst predicting a spatial distri-\nbution by passing a 2D CNN feature map through a softmax\nfunction, and then outputting the coordinates of the centroid\nof this distribution. By the translation equivariance of CNNs,\na shift in the CNN input results in a corresponding shift in\nthe predicted spatial distribution, and hence the location of\nthe centroid. We rescale the centroid coordinates to match\nthe scale of the input coordinate grid.\n4.3. Constructing Canonical Coordinates (Algorithm 1)\nIn order to construct an ET layer, we derive a canonical coor-\ndinate system for the target transformation. Canonical coor-\ndinate systems exist for all one-parameter Lie groups (Seg-\nman et al., 1992; Theorem 1). For Lie groups with more\nthan one parameter, canonical coordinates exist for Abelian\ngroups of dimension k≤d: that is, groups whose transfor-\nmations are commutative.\nHere, we summarize the procedure described in Segman\net al. (1992). For clarity of exposition, we will focus on\nLie groups representing transformations on R2 with one\nparameter θ∈R. This corresponds to the practically useful\ncase of one-parameter deformations of 2D images. In this\nsetting, condition (1) reduces to:\nρ(Tθx) = ρ(x) + θe1.\nTaking the derivative with respect to θ, we can see that it\nsufﬁces for ρto satisfy the following ﬁrst-order PDEs:\n(∂(Tθx)1\n∂θ\n⏐⏐⏐⏐\nθ=0\n∂\n∂x1\n+ ∂(Tθx)2\n∂θ\n⏐⏐⏐⏐\nθ=0\n∂\n∂x2\n)\nρ1(x) = 1,\n(3)(∂(Tθx)1\n∂θ\n⏐⏐⏐⏐\nθ=0\n∂\n∂x1\n+ ∂(Tθx)2\n∂θ\n⏐⏐⏐⏐\nθ=0\n∂\n∂x2\n)\nρ2(x) = 0.\n(4)\nWe can solve these ﬁrst-order PDEs using the method of\ncharacteristics (e.g., Strauss, 2007). Observe that the homo-\ngeneous equation (4) admits an inﬁnite set of solutions ρ2;\neach solution is a different coordinate function that is invari-\nant to the transformation Tθ. Thus, there exists a degree of\nAlgorithm 1 Constructing a canonical coordinate system\nInput: Transformation group {Tθ}\nOutput: Canonical coordinates ρ(x)\nvi(x) ←(∂(Tθx)i/∂θ)|θ=0, i = 1,2\nDx ←(v1(x)∂/∂x1 + v2(x)∂/∂x2)\nρ1(x) ←a solution of Dxρ1(x) = 1\nρ2(x) ←a solution of Dxρ2(x) = 0\nReturn ρ(x) = (ρ1(x),ρ2(x))\nfreedom in choosing invariant coordinate functions; due to\nthe ﬁnite resolution of images in practice, we recommend\nchoosing coordinates that minimally distort the input image\nto mitigate the introduction of resampling artifacts.\nExample 3 (Hyperbolic Rotation). As a concrete exam-\nple, we will derive a set of canonical coordinates for hyper-\nbolic rotation, Tθx = (x1eθ,x2e−θ). This is a “squeeze”\ndistortion that dilates an image along one axis and com-\npresses it along the other. We obtain the following PDEs:\n(x1∂/∂x1 −x2∂/∂x2)ρ1(x) = 1,\n(x1∂/∂x1 −x2∂/∂x2)ρ2(x) = 0.\nIn the ﬁrst quadrant, the solution to the inhomogeneous\nequation is ρ1(x) = log\n√\nx1/x2 + c1, where c1 is an arbi-\ntrary constant, and the solution to the homogeneous equation\nis ρ2(x) = h(x1x2), where his an arbitrary differentiable\nfunction in one variable (the choice h(z) = √zis known as\nthe hyperbolic coordinate system). These coordinates can\nbe deﬁned analogously for the remaining quadrants to yield\na representation of the entire image plane, excluding the\nlines x1 = 0 and x2 = 0.\n4.4. Compositions of Transformations\nA single transformation group with one parameter is typ-\nically insufﬁcient to capture the full range of variation in\nobject pose in natural images. For example, an important\ntransformation group in practice is the 8-parameter pro-\njective linear group PGL(3,R) that represents perspective\ntransformations in 3D space.\nIn the special case of two-parameter Abelian Lie groups,\nwe can construct canonical coordinates that yield self-\nconsistency simultaneously for both parameters (Segman\net al., 1992; Theorem 1). For example, log-polar coordi-\nnates are canonical for both rotation and dilation. How-\never, for transformations on Rd, a canonical coordinate\nsystem can only satisfy condition (1) for up to dparameters.\nThus, a single canonical coordinate system is insufﬁcient\nfor higher-dimensional transformation groups on R2 such\nas PGL(3,R).\nStacked ETs. Since we cannot always achieve simulta-\nneous self-consistency with respect to all the parameters\nEquivariant Transformer Networks\nof the transformation group, we instead adopt the heuristic\napproach of using a sequence of ET layers, each of which\nimplements self-consistency with respect to a subgroup of\nthe full transformation group. Intuitively, each ET layer\naims to remove the effect of its corresponding subgroup.\nSpeciﬁcally, let Tθ be a k-parameter transformation that\nadmits a decomposition into one-parameter transformations:\nTθ = T(1)\nθ′\n1\n◦T(2)\nθ′\n2\n◦···◦ T(k)\nθ′\nk\n,\nwhere θ′\ni ∈R. For example, in the case of PGL(3,R), we\ncan decompose an arbitrary transformation into a composi-\ntion of one-parameter translation, dilation, rotation, shear,\nand perspective transformations. We then apply a sequence\nof ET layers in the reverse order of the transformations:\nL(φ) = LG(k),ρ(k) ◦LG(k−1),ρ(k−1) ◦···◦ LG(1),ρ(1) (φ),\nwhere ρ(i) are canonical coordinates for each one-parameter\nsubgroup G(i).\nWhile we can no longer guarantee self-consistency for a\ncomposition of ET layers, we show empirically (§5) that this\nstacking heuristic works well in practice for transformation\ngroups in several parameters.\n4.5. Implementation\nHere we highlight particularly salient details of our im-\nplementation of ETs. Our PyTorch implementation is\navailable at github.com/stanford-futuredata/\nequivariant-transformers.\nChange of Coordinates. We implement coordinate trans-\nformations by resampling the input image over a rectangular\ngrid in the new coordinate system. This grid consists of\nrows and columns that are equally spaced in the intervals\n[umin\n1 ,umax\n1 ] and [umin\n2 ,umax\n2 ], where the limits of these in-\ntervals are chosen to achieve good coverage of the input\nimage. These points u in the canonical coordinate system\ndeﬁne a set of sampling points ρ−1(u) in Cartesian coordi-\nnates. We use bilinear interpolation for points that do not\ncoincide with pixel locations in the original image, as is\ntypical with ST layers (Jaderberg et al., 2015).\nAvoiding Resampling. When using multiple ET layers,\niterated resampling of the input image will degrade image\nquality and amplify the effect of interpolation artifacts. In\nour implementation, we circumvent this issue by resampling\nthe image lazily. More speciﬁcally, letφ(i) denote the image\nobtained after itransformations, where φ(0) is the original\ninput image. At each iteration i, we represent φ(i) implic-\nitly using the sampling grid Gi :=\n(\nT(1)\nˆθ1\n◦···◦ T(i)\nˆθi\n)\nG0,\nwhere G0 represents the Cartesian grid over the original in-\nput. We materialize φ(i) (under the appropriate canonical\nFigure 4. Projective MNIST. Examples of transformed digits\nfrom each class (ﬁrst row: 0–4, second row: 5–9). Each base\nMNIST image is transformed using a transformation sampled from\na 6-parameter group (i.e., PGL(3,R) without translation).\ncoordinates) in order to predict ˆθi+1. By appending the next\npredicted transformation T(i+1)\nˆθi+1\nto the transformation stack,\nwe thus obtain the subsequent sampling grid, Gi+1.\n5. Experiments\nWe evaluate ETs on two image classiﬁcation datasets: an\nMNIST variant where the digits are distorted under ran-\ndom projective transformations (§5.1), and the real-world\nStreet View House Numbers (SVHN) dataset (§5.2). Using\nprojectively-transformed MNIST data, we evaluate the per-\nformance of ETs relative to STs in a setting where images\nare deformed by a known transformation group in several\nparameters. The SVHN task evaluates the utility of ET\nlayers when used in combination with modern CNN archi-\ntectures in a realistic image classiﬁcation task. In both cases,\nwe validate the sample efﬁciency beneﬁts conferred by ETs\nrelative to standard STs and baseline CNN architectures.3\n5.1. Projective MNIST\nWe introduce the Projective MNIST dataset, a variant of\nthe MNIST dataset where the digits are distorted using\nrandomly sampled projective transformations: namely ro-\ntation, shear, x- and y-dilation, and x- and y-perspective\ntransformations (i.e., 6 pose parameters in total). The Pro-\njective MNIST training set contains 10,000 base images\nsampled without replacement from the MNIST training set.\nEach image is resized to 64 ×64 and transformed using an\nindependently-sampled set of pose parameters.\nWe also generated three larger versions of the dataset for the\npurpose of controlled evaluation of the effect of (idealized)\ndata augmentation: these additional datasets respectively\ncontain 2, 4, and 8 copies of the base MNIST images, each\ntransformed under different sets of parameters.\nUnlike other MNIST variants such as Rotated\nMNIST (Larochelle et al., 2007), MNIST-RTS (Jaderberg\net al., 2015), and SIM2MNIST (Esteves et al., 2018), our\n3In the Appendix, we report additional experimental results on\nrobustness to transformations not seen at training time.\nEquivariant Transformer Networks\nTable 1.Classiﬁcation error rates on Projective MNIST (§5.1).\nAll methods use the same CNN architecture for classiﬁcation and\ndiffer in the transformations applied to the input images. We train\non up to 8 sampled transformations for each base MNIST image.\nLP: log-polar coordinates; shx: x-shear; hr: hyperbolic rotation;\npx: x-perspective; py: y-perspective.\nMethod Transformations # sampled transformations\n1 2 4 8\nCartesian - 11.91 9.67 7.64 6.93\nLog-polar - 6.55 5.05 4.48 3.83\nST-LP shx 5.77 4.27 3.97 3.47\nST-LP shx hr 4.92 3.87 3.22 3.03\nST-LP* shx hr px py – – – –\nET-LP shx 5.48 4.67 3.63 3.21\nET-LP shx hr 4.18 3.17 2.96 2.62\nET-LP shx hr px py 3.76 3.11 2.80 2.60\n*We omit this conﬁguration due to training instability.\nProjective MNIST dataset incorporates higher-dimensional\ncombinations of transformations, including projective trans-\nformations not considered in prior work (e.g., perspective\ntransforms). We provide further details on the construction\nof the dataset in the Appendix.\nNetwork Architectures. We used a CNN architecture\nbased on the Z2CNN from Cohen & Welling (2016), with 7\nlayers of 3×3 convolutions with 32 channels, batch normal-\nization after convolutional layers, and dropout after the 3rd\nand 6th layers. In addition to this baseline “Cartesian” CNN,\nwe also evaluated a more rotation- and dilation-robust net-\nwork where the inputs are ﬁrst transformed to log-polar co-\nordinates (Henriques & Vedaldi, 2017; Esteves et al., 2018).\nWe introduce a sequence of transformer layers before the\nlog-polar coordinate transformation to handle the remaining\ngeometric transformations applied to the input. For both the\nbaseline STs and ETs, we apply a sequence of transformer\nlayers, with each layer predicting a single pose parameter.\nThe pose predictor networks in both cases are 3-layer CNNs\nwith 32 channels in each layer. We selected the transforma-\ntion order, dropout rate, and learning rate schedule based on\nvalidation accuracy (see the Appendix for details).\nClassiﬁcation Accuracy (Table 1). We ﬁnd that the ET\nlayers consistently improve on test error rate over both the\nlog-polar and ST baselines. By accounting for additional\ntransformations, the ET improves on the error rate of the\nbaseline log-polar CNN by 2.79%—a relative improvement\nof 43%—when trained on a single pose per prototype. Note\nthat we omit the ST baseline with the full transformation\nsequence due to training instability, despite more extensive\nhyperparameter tuning than the ET. We ﬁnd that all methods\nimprove from augmentation with additional poses, with the\nFigure 5. Sensitivity to initial learning rate. For each learning\nrate setting, we plot the minimum, mean, and maximum validation\nerror rates over 10 runs for networks trained with ETs and STs.\nThe predicted transformations are x-shear and hyperbolic rotation.\nWe ﬁnd that ETs are signiﬁcantly more robust than STs to the\nlearning rate hyperparameter.\nET retaining its advantage but at a reduced margin.\nHyperparameter Sensitivity (Figure 5). We compared\nthe sensitivity of ET and ST networks to the initial learn-\ning rate by comparing validation error when training with\nlearning rate values ranging from 1 ×10−4 to 4 ×10−3.\nFor each setting, we trained 10 networks with independent\nrandom initializations on Projective MNIST with 10,000\nexamples, computing the validation error after each epoch\nand recording the minimum observed error in each run. We\nﬁnd that STs were signiﬁcantly more sensitive to learning\nrate than ET, with far higher variance in error rate between\nruns. This suggests that the self-consistency constraint im-\nposed on ETs helps improve the training-time stability of\nnetworks augmented with transformer layers.\n5.2. Street View House Numbers (SVHN)\nThe goal of the single-digit classiﬁcation task of the SVHN\ndataset (Netzer et al., 2011) is to classify the digit in the\ncenter of 32 ×32 RGB images of house numbers. SVHN\nis well-suited to evaluating the effect of transformer lay-\ners since there is a natural range of geometric variation\nin the data due to differences in camera position—unlike\nProjective MNIST, we do not artiﬁcially apply further trans-\nformations to the data. The training set consists of 73,257\nexamples; we use a randomly-chosen subset of 5,000 exam-\nples for validation and use the remaining 68,257 examples\nfor training. In order to evaluate the data efﬁciency of each\nmethod, we also trained models using smaller subsets of\n10,000 and 20,000 examples. The dataset also includes\n531,131 additional images that can be used as extra train-\ning data; we thus additionally evaluate our methods on the\nconcatenation of this set and the training set.\nNetwork Architectures. We use 10-, 18-, and 34-layer\nResNet architectures (He et al., 2016) as baseline networks.\nEach transformer layer uses a 3-layer CNN with 32 chan-\nEquivariant Transformer Networks\nTable 2. Classiﬁcation error rates on SVHN ( §5.2). For both\nSTs and ETs, we used the following transformations: x- and y-\ntranslation, rotation, and x-scaling. Error rates are each averaged\nover 3 runs. ETs achieve the largest accuracy gains relative to STs\nand the baseline CNNs in the limited data regime.\nNetwork Transformer # training examples\n10k 20k 68k 600k\nResNet-10 None 9.83 7.90 5.35 2.96\nSpatial 9.80 7.66 4.96 2.92\nEquivariant 8.24 6.71 4.84 2.70\nResNet-18 None 9.23 7.31 4.81 2.76\nSpatial 9.10 7.17 4.51 2.70\nEquivariant 7.81 6.37 4.50 2.57\nResNet-34 None 8.73 7.05 4.67 2.53\nSpatial 8.60 6.91 4.37 2.66\nEquivariant 7.72 5.98 4.23 2.47\nnels per layer for pose prediction. We applied x- and y-\ntranslation, rotation, and x- and y-scaling to the input im-\nages: these were selected from among the subgroups of the\nprojective group using the validation set.\nResults (Table 2). We ﬁnd that ETs improve on the error\nrate achieved by both STs and the baseline ResNets, with the\nlargest gains seen in the limited data regime: with 10,000\nexamples, ETs improve on the error rates of the baseline\nCNNs and ST-augmented CNNs by 0.9–1.6%, or a rela-\ntive improvement of 10–16%. We see smaller gains when\nmore training data is available: the relative improvement\nbetween ETs and the baseline CNNs is 11–13% with 20,000\nexamples, and 6.4–9.5% with 68,257 examples.\nWhen data is limited, we ﬁnd that a simpler classiﬁer where\nprior knowledge on geometric invariances has been encoded\nusing ETs can outperform more complex classiﬁers that\nare not equipped with this additional structure. In particu-\nlar, when trained on 10,000 examples, a ResNet-10 classi-\nﬁer with ET layers achieves lower error than the baseline\nResNet-34 classiﬁer. The baseline ResNet-34 has over 5.3M\nparameters; in contrast, the ResNet-10 has 1.2M parame-\nters, with the ET layers adding only 31k parameters in total.\nThe ET-augmented ResNet-10 therefore achieves improved\nerror rate with an architecture that incurs less memory and\ncomputational cost than a ResNet-34.\n6. Discussion and Conclusion\nLimitations of ETs. The self-consistency guarantee of\nETs can fail due to boundary effects that occur when image\ncontent is cropped after a transformation. This issue can be\nmitigated by padding the input such that the transformed\nimage does not fall “out of frame”. Even without a strict\nself-consistency guarantee, we still observe gains when ET\nlayers are used in practice (e.g., in our SVHN experiments).\ninput x-shear hyp. rot. x-persp. y-persp.\ninput translation rot./scale x-scale\nFigure 6.Predicted transformations. On Projective MNIST\n(top), ETs reverse the effect of distortions such as shear and per-\nspective, despite being provided no direct supervision on pose\nparameters (the ﬁnal images remain rotated and scaled since the\nclassiﬁcation CNN operates over their log-polar representation).\nOn SVHN (bottom), the ﬁnal x-scale transformation has a crop-\nping effect that removes distractor digits.\nAs discussed in §4.4, the method of stacking ET layers\nis ultimately a heuristic approach as it does not guaran-\ntee self-consistency with respect to the full transformation\ngroup. Moreover, higher-dimensional groups require the\nuse of long sequences of ET layers, resulting in high com-\nputational cost. In such cases, we could employ a hybrid\napproach where “difﬁcult” subgroups are handled by ET lay-\ners, while the remaining degrees of freedom are handled by\na standard ST layer. In general, enforcing equivariance guar-\nantees for higher-dimensional transformation groups in a\ncomputationally scalable fashion remains an open problem.\nIn contrast to the use of prior knowledge on transformation\ninvariances in this work, there is a separate line of research\nthat concerns learning various classes of transformations\nfrom data (Hashimoto et al., 2017; Thomas et al., 2018).\nExtending ETs to these more ﬂexible notions of invariance\nmay prove to be an interesting direction for future work.\nConclusion. We proposed a neural network layer that\nbuilds in prior knowledge on the continuous transformation\ninvariances of its input domain. By encapsulating equiv-\nariant functions within an image-to-image mapping, ETs\nexpose a convenient interface for ﬂexible composition of lay-\ners tailored to different transformation groups. Empirically,\nwe demonstrated that ETs improve the sample efﬁciency of\nCNNs on image classiﬁcation tasks with latent transforma-\ntion parameters. Using libraries of ET layers, practitioners\nare able to quickly experiment with multiple combinations\nof transformations to realize gains in predictive accuracy,\nparticularly in domains where labeled data is scarce.\nEquivariant Transformer Networks\nAcknowledgements\nWe thank Pratiksha Thaker, Kexin Rong, and our anony-\nmous reviewers for their valuable feedback on earlier ver-\nsions of this manuscript. This research was supported in\npart by afﬁliate members and other supporters of the Stan-\nford DAWN project—Ant Financial, Facebook, Google,\nIntel, Microsoft, NEC, SAP, Teradata, and VMware—as\nwell as Toyota Research Institute, Keysight Technologies,\nNorthrop Grumman, Hitachi, NSF awards AF-1813049 and\nCCF-1704417, an ONR Young Investigator Award N00014-\n18-1-2295, and Department of Energy award de-sc0019205.\nReferences\nAmari, S. Feature Spaces which Admit and Detect Invariant\nSignal Transformations. In International Joint Confer-\nence on Pattern Recognition, 1978.\nAmit, Y ., Grenander, U., and Piccioni, M. Structural image\nrestoration through deformable templates. Journal of the\nAmerican Statistical Association, 1991.\nCohen, T. S. and Welling, M. Group Equivariant Convolu-\ntional Networks. In International Conference on Machine\nLearning, 2016.\nCohen, T. S., Geiger, M., K¨ohler, J., and Welling, M. Spher-\nical CNNs. In International Conference on Learning\nRepresentations, 2018.\nDe Castro, E. and Morandi, C. Registration of translated\nand rotated images using ﬁnite Fourier transforms. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence, pp. 700–703, 1987.\nDieleman, S., De Fauw, J., and Kavukcuoglu, K. Exploit-\ning cyclic symmetry in convolutional neural networks.\nInternational Conference on Machine Learning, 2016.\nEsteves, C., Allen-Blanchette, C., Zhou, X., and Daniilidis,\nK. Polar Transformer Networks. In International Confer-\nence on Learning Representations, 2018.\nFreeman, W. T. and Adelson, E. H. The design and use of\nsteerable ﬁlters. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, pp. 891–906, 1991.\nGens, R. and Domingos, P. M. Deep Symmetry Networks.\nIn Advances in Neural Information Processing Systems,\n2014.\nHashimoto, T. B., Liang, P. S., and Duchi, J. C. Unsu-\npervised transformation learning via convex relaxations.\nIn Advances in Neural Information Processing Systems,\n2017.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), 2016.\nHenriques, J. F. and Vedaldi, A. Warped Convolutions:\nEfﬁcient Invariance to Spatial Transformations. In Inter-\nnational Conference on Machine Learning, 2017.\nJacobsen, J.-H., De Brabandere, B., and Smeulders, A. W.\nDynamic Steerable Blocks in Deep Residual Networks.\narXiv preprint arXiv:1706.00598, 2017.\nJaderberg, M., Simonyan, K., Zisserman, A., and\nKavukcuoglu, K. Spatial Transformer Networks. In Ad-\nvances in Neural Information Processing Systems, 2015.\nLaptev, D., Savinov, N., Buhmann, J. M., and Pollefeys,\nM. TI-POOLING: transformation-invariant pooling for\nfeature learning in convolutional neural networks. In\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2016.\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J., and\nBengio, Y . An empirical evaluation of deep architectures\non problems with many factors of variation. In Interna-\ntional Conference on Machine Learning, 2007.\nLin, C.-H. and Lucey, S. Inverse Compositional Spatial\nTransformer Networks. IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR), 2017.\nLucas, B. D. and Kanade, T. An iterative image registra-\ntion technique with an application to stereo vision. In\nInternational Joint Conference on Artiﬁcial intelligence,\n1981.\nMarcos, D., V olpi, M., Komodakis, N., and Tuia, D. Rota-\ntion Equivariant Vector Field Networks. In International\nConference on Computer Vision, 2017.\nNetzer, Y ., Wang, T., Coates, A., Bissacco, A., Wu, B.,\nand Ng, A. Y . Reading digits in natural images with\nunsupervised feature learning. In NIPS workshop on\ndeep learning and unsupervised feature learning, 2011.\nRawlinson, D., Ahmed, A., and Kowadlo, G. Sparse un-\nsupervised capsules generalize better. arXiv preprint\narXiv:1804.06094, 2018.\nReddi, S. J., Kale, S., and Kumar, S. On the Convergence\nof Adam and Beyond. In International Conference on\nLearning Representations, 2018.\nReddy, B. S. and Chatterji, B. N. An FFT-based technique\nfor translation, rotation, and scale-invariant image regis-\ntration. IEEE Transactions on Image Processing, 5(8):\n1266–1271, 1996.\nEquivariant Transformer Networks\nRubinstein, J., Segman, J., and Zeevi, Y . Recognition of dis-\ntorted patterns by invariance kernels.Pattern Recognition,\n24(10):959–967, 1991.\nSabour, S., Frosst, N., and Hinton, G. E. Dynamic routing\nbetween capsules. In Advances in Neural Information\nProcessing Systems, 2017.\nSegman, J., Rubinstein, J., and Zeevi, Y . Y . The canonical\ncoordinates method for pattern deformation: Theoretical\nand computational considerations. IEEE Transactions\non Pattern Analysis and Machine Intelligence, pp. 1171–\n1183, 1992.\nShu, Z., Sahasrabudhe, M., Alp Guler, R., Samaras, D.,\nParagios, N., and Kokkinos, I. Deforming Autoencoders:\nUnsupervised Disentangling of Shape and Appearance.\nIn European Conference on Computer Vision (ECCV) ,\n2018.\nStrauss, W. A. Partial Differential Equations: An Introduc-\ntion. Wiley, 2007.\nTeo, P. C. Theory and Applications of Steerable Functions.\nPhD thesis, Stanford University, 1998.\nThomas, A., Gu, A., Dao, T., Rudra, A., and R´e, C. Learn-\ning compressed transforms with low displacement rank.\nIn Advances in Neural Information Processing Systems,\n2018.\nWeiler, M., Hamprecht, F. A., and Storath, M. Learn-\ning Steerable Filters for Rotation Equivariant CNNs. In\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2018.\nWorrall, D. E., Garbin, S. J., Turmukhambetov, D., and\nBrostow, G. J. Harmonic Networks: Deep Translation\nand Rotation Equivariance. In IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), 2017.\nYuille, A. L. Deformable templates for face recognition.\nJournal of Cognitive Neuroscience, 1991.\nZhou, Y ., Ye, Q., Qiu, Q., and Jiao, J. Oriented response\nnetworks. In IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2017.\nEquivariant Transformer Networks\nA. Proof of Proposition 1\nProposition 2. Let f : Φ →Rk be self-consistent with respect to translation and let ρbe a canonical coordinate system\nwith respect to a transformation group Gparameterized by θ ∈Rk. Then fρ(φ) := f(φ◦ρ−1) is self-consistent with\nrespect to G.\nProof. By the deﬁnition of ρ,\n(ρ◦Tθ ◦ρ−1)(u) = ρ(ρ−1(u)) +\nk∑\ni=1\nθiei\n= u +\nk∑\ni=1\nθiei,\nand therefore (Tθ ◦ρ−1)(u) = ρ−1\n(\nu + ∑k\ni=1 θiei\n)\n. By this identity and translation self-consistency of f,\nfρ(Tθφ) = f((Tθφ◦ρ−1)(u))\n= f((φ◦Tθ ◦ρ−1)(u))\n= f\n(\n(φ◦ρ−1)\n(\nu +\nk∑\ni=1\nθiei\n))\n= f((φ◦ρ−1)(u)) + θ\n= fρ(φ) + θ,\nwhere in the second line we used the deﬁnition of Tθφ, in the third line we used the identity for (Tθ ◦ρ−1)(u), and in the\nfourth line we used the translation self-consistency of f. This establishes self-consistency with respect to G.\nB. Canonical Coordinate Systems\nIn Table 3, we list the set of canonical coordinates that we derived for our experiments along with their corresponding\ntransformation groups. As explained in the main text, these coordinates are not unique for one-parameter transformation\ngroups: in this case, there exists a degree of freedom in specifying the complementary set of coordinates.\nIn Figure 7, we plot some examples of canonical coordinate grids used in our experiments.\nC. Experimental Details\nC.1. Projective MNIST\nDataset. To construct the Projective MNIST dataset, we sampled 10,000 images without replacement from the MNIST\ntraining set (consisting of 60,000 examples). Each 28 ×28 base image is extended to 64 ×64 by symmetric zero padding.\nThe images are then distorted using transformations sampled independently from the projective group. The 6 pose parameters\nwere sampled uniformly from the ranges listed in Table 4. We excluded translation from this combination of transformations\nin order to avoid cropping issues due to the distorted digit exceeding the boundaries of the image. We selected these pose\nparameter ranges in order to evaluate performance on a more challenging set of transformations than those evaluated in\nJaderberg et al. (2015), using a smaller training set in line with the popular Rotated MNIST dataset (Larochelle et al., 2007).\nFor each base image, we independently sampled 8 sets of pose parameters, thus yielding 8 transformed versions of each\nbase image. We created 4 training sets with 10,000, 20,000, 40,000 and 80,000 examples respectively, each containing 1, 2,\n4, or 8 versions of each base image.\nIn addition to the training sets, we generated a validation set of size 5000 using examples from the MNIST training set that\nwere not used in the Projective MNIST training set. Each of the images in the validation set was transformed using an\nindependently sampled set of pose parameters sampled from the same range as the training set.\nEquivariant Transformer Networks\nFigure 7. Examples of canonical coordinate systems. The corresponding images of (x,y) for each transformation are (1a) rotation,\n(1b) dilation, (2) x-shear, (3) hyperbolic rotation, and (4) x-perspective.\nWe generated a test set using all 10,000 images in the MNIST test set. For each base image, we sampled 8 sets of pose\nparameters, thus yielding a test set of size 80,000 that contains 8 transformed versions of each base test\nPreprocessing. We preprocessed the data by subtracting the mean pixel value and dividing by the standard deviation. The\nmean and standard deviation were computed as scalar values, ignoring pixel locations.\nNetwork Architectures. We used a baseline CNN architecture similar to the Z2CNN used in the experimental evaluation\nof Cohen & Welling (2016). In all our experiments, each convolutional layer is followed by a spatial batch normalization\nlayer. This network has the following architecture, with output shapes listed in (channel ×height ×width format):\nLayer Output Shape\ninput 1 ×64 ×64\nconv1 32 ×62 ×62\navgpool 32 ×31 ×31\nconv2 32 ×29 ×29\navgpool 32 ×14 ×14\nconv3 32 ×12 ×12\ndropout 32 ×12 ×12\nconv4 32 ×10 ×10\nconv5 32 ×8 ×8\nconv6 32 ×6 ×6\ndropout 32 ×6 ×6\nconv7 10 ×4 ×4\nmaxpool 10\nFor self-consistent pose prediction within ET layers, we used the following architecture:\nEquivariant Transformer Networks\nLayer Output Shape\ninput 1 ×64 ×64\ncanon. coords. 1 ×64 ×64\nconv1 32 ×32 ×32\nconv2 32 ×32 ×32\nmaxpool 32 ×32\nconv3 1 ×32\nsoftmax 1 ×32\ncentroid 1\nIn this network, the max-pooling layer eliminates the extraneous dimension in the feature map. For example, when predicting\nrotation angle using polar coordinates, this operator pools over the radial dimension; the remaining feature map is then\nindexed by the angular coordinate. We then pass this feature map through a softmax operator to obtain a spatial distribution,\nand ﬁnally compute the centroid of this distribution to obtain the predicted pose. When a pair of pose predictions are needed\n(e.g., when predicting rotation and dilation parameters simultaneously), we use two output branches that each pools over a\ndifferent dimension.\nFor baseline, non-equivariant pose prediction within ST layers, we used the following architecture:\nLayer Output Shape\ninput 1 ×64 ×64\nconv1 32 ×32 ×32\nconv2 32 ×32 ×32\nmaxpool 32 ×3 ×3\nfc 1\nUnlike the self-consistent pose predictor, this network does not represent the input using a canonical coordinate system. The\npose is predicted using a fully-connected layer.\nHyperparameters. We tuned the set of ET layers, their order, the dropout probability, the initial learning rate and the\nlearning rate decay factor on the validation set. ET layers were selected from subgroups of the projective group: rotation,\ndilation, hyperbolic rotation, x-shear, x-perspective and y-perspective. The dropout probability was selected from the set\n{0.25,0.30,0.35,0.40}. We computed validation accuracy after each epoch of training and computed test accuracy using\nthe network that achieved the best validation accuracy. For optimization, we used the AMSGrad algorithm (Reddi et al.,\n2018) with a minibatch size of 128. For the baseline and ET networks, we used an initial learning rate of 2 ×10−3. For the\nST networks, we used an initial learning rate of 2 ×10−4: higher learning rates resulted in unstable training and hence\nreduced accuracy. We multiplicatively decayed the learning rate by1% after each epoch. We trained all our networks for\n300 epochs.\nC.2. Street View House Numbers\nPreprocessing. For each channel, we preprocessed the data by subtracting the mean pixel value and dividing by the\nstandard deviation. The mean and standard deviation were computed as scalar values, ignoring pixel locations.\nNetwork Architectures. We used standard ResNet architectures as baseline CNNs (He et al., 2016). For pose prediction,\nwe used the same CNN architectures as in the Projective MNIST task, but with 3 input channels.\nHyperparameters. As with the Projective MNIST experiments, we tuned the set of ET layers, their order, the dropout\nprobability, the initial learning rate and the learning rate decay factor on the validation set (a 5000-example subset of the\nSVHN training set). We tuned the transformation and dropout hyperparameters over the same set of possible values as for\nProjective MNIST. Again, we used the AMSGrad algorithm for optimization with a minibatch size of 128. Due to the large\nsize (≈600k examples) of the training set with the addition of the extra training images, we only trained our networks for\n150 epochs in this setting. For the remaining runs, we trained for 300 epochs.\nEquivariant Transformer Networks\nTable 3.Canonical coordinate systems implemented for our experiments with their corresponding transformation groups.\nTransformation Tθx ρ1(x) ρ2(x)\nx-translation (x1 + θ,x2) x1 x2y-translation (x1,x2 + θ)\nRotation (x1 cos θ−x2 sin θ,x1 sin θ+ x2 cos θ) tan−1(x2/x1) log\n√\nx2\n1 + x2\n2Dilation (x1eθ,x2eθ)\nx-scale (x1eθ,x2) log x1 log x2y-scale (x1,x2eθ)\nHyperbolic Rotation (x1eθ,x2e−θ) log\n√\nx1/x2\n√x1x2\nx-shear (x1 −θx2,x2) −x1/x2 x2\ny-shear (x1,x2 + θx1) x2/x1 x1\nx-perspective (x1/(θx1 + 1),x2/(θx1 + 1)) 1 /x1 tan−1(x2/x1)\ny-perspective (x1/(θx2 + 1),x2/(θx2 + 1)) 1 /x2 tan−1(x1/x2)\nTable 4.Ranges of sampled transformations for Projective MNIST.\nTransformation Tθx Range\nRotation (x1 cos θ−x2 sin θ,x1 sin θ+ x2 cos θ) [ −π,π]\nDilation (x1eθ,x2eθ) [0 ,log 2]\nHyperbolic Rotation (x1eθ,x2e−θ) [ −log 1.5,log 1.5]\nx-shear (x1 −θx2,x2) [ −1.5,1.5]\nx-perspective (x1/(θxx1 + 1),x2/(θxx1 + 1)) {(θx,θy) : |θx|+ |θy|≤ 0.8}y-perspective (x1/(θyx2 + 1),x2/(θyx2 + 1))\nD. Additional Experiments\nD.1. Robustness to Unseen Transformations\nIn this experiment, we evaluate the robustness of CNNs with ET layers to transformations not seen at training time. Following\nthe procedure of Sabour et al. (2017), we train on a variant of the MNIST training set where each digit is randomly placed\non a 40 ×40 black background. This network is then tested on the affNIST test set, a variant of the MNIST test set where\neach digit is transformed with a small afﬁne transformation.4 We perform model selection against a validation set of 5000\nheld-out MNIST training images, each randomly placed on the 40 ×40 background but subject to no further transformations.\nOur CNN baseline uses three convolutional layers with 256, 256 and 128 channels, each with 5 ×5 kernels and a stride\nof 1. Each convolutional layer is followed by a batch normalization layer and a ReLU nonlinearity. The output of the\nﬁnal convolutional layer is average pooled to obtain a 128-dimensional embedding which is mapped to 10 classes by a\nfully-connected layer. We use dropout before the ﬁnal classiﬁcation layer.\nWe evaluate this CNN architecture with three ET conﬁgurations: (1) x- and y-translation followed by a transformation to\nlog-polar coordinates,5 (2) x- and y-translation, followed by x-shear, followed by a transformation to log-polar coordinates,\nand ﬁnally (3) x- and y-translation followed by rotation/scale, without a further log-polar transformation.\nTable 5 summarizes our results. We ﬁnd that our baseline CNN already outperforms the Capsule Network architecture from\nSabour et al. (2017), while the baseline CNN over log-polar coordinates (without any ET layers) performs poorly due to the\n4The affNIST dataset can be found at https://www.cs.toronto.edu/˜tijmen/affNIST/. The transformation parame-\nters are sampled uniformly within the following ranges: rotation in [−20,20] degrees, shear in [−0.2,0.2], vertical and horizontal scaling\nin [0.8,1.2]. The transformed digit is placed uniformly at random on a 40 ×40 black background, subject to the constraint that no nonzero\npart of the digit image is cropped.\n5This ﬁrst ET conﬁguration is equivalent to the Polar Transformer architecture introduced by Esteves et al. (2018).\nEquivariant Transformer Networks\nTable 5.Test accuracies on affNIST. All models except (*) were trained only on randomly-translated MNIST training data and tested on\naffNIST images, which are MNIST test images distorted by random afﬁne transformations. The classiﬁcation layer for (*) was trained on\naffNIST data using a feature extractor that was trained on MNIST.\nMethod affNIST test accuracy (%)\nStandard CNN (Sabour et al., 2017) 66\nStandard CNN (ours) 88.3\nCapsule Network (Sabour et al., 2017) 79\nSparse Unsupervised Capsule features + SVM (Rawlinson et al., 2018) 90.1*\nLog-polar 76.6\nET-LP (translation) 98.1\nET-LP (translation + x-shear) 98.3\nET-Cartesian (translation + rotation/scale) 98.2\nloss of translation equivariance. The ET-augmented CNNs improve on these results, with both networks demonstrating\ncomparable robustness to afﬁne transformations not seen at training time. The higher test accuracies achieved by the ET\nnetworks relative to the Capsule Network baselines reﬂect the stronger priors that have been built into the ET architecture—in\ncontrast to the ET networks, the Capsule Network baselines have to learn invariances from the training data."
}