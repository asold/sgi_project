{
    "title": "Improved Swin Transformer-Based Semantic Segmentation of Postearthquake Dense Buildings in Urban Areas Using Remote Sensing Images",
    "url": "https://openalex.org/W4313415801",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2410802430",
            "name": "Liangyi Cui",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2098716461",
            "name": "Xin Jing",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2097172772",
            "name": "Yu Wang",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A4316008504",
            "name": "Yixuan Huan",
            "affiliations": [
                "Lanzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2022600991",
            "name": "Xu Yang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098472325",
            "name": "Qiangqiang Zhang",
            "affiliations": [
                "Lanzhou University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2884822772",
        "https://openalex.org/W6753412334",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3212386989",
        "https://openalex.org/W6804036380",
        "https://openalex.org/W6849773570",
        "https://openalex.org/W1980715395",
        "https://openalex.org/W4313124045",
        "https://openalex.org/W4281707765",
        "https://openalex.org/W6679055852",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W3126435384",
        "https://openalex.org/W2533566148",
        "https://openalex.org/W3000305214",
        "https://openalex.org/W2999491422",
        "https://openalex.org/W3167618040",
        "https://openalex.org/W4213068057",
        "https://openalex.org/W6729983426",
        "https://openalex.org/W4312549298",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2027000042",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2170535121",
        "https://openalex.org/W4210328379",
        "https://openalex.org/W4306955484",
        "https://openalex.org/W2937588654",
        "https://openalex.org/W2994342516",
        "https://openalex.org/W2793659031",
        "https://openalex.org/W2082699349",
        "https://openalex.org/W4213244977",
        "https://openalex.org/W2074823111",
        "https://openalex.org/W6838697126",
        "https://openalex.org/W6639824700",
        "https://openalex.org/W3145185940",
        "https://openalex.org/W6846279210",
        "https://openalex.org/W6798046796",
        "https://openalex.org/W2128254161",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W2110764733",
        "https://openalex.org/W6800217721",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W6783267081",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W2949846184",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4319300975",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W4287022992",
        "https://openalex.org/W3213712995",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W4308245819",
        "https://openalex.org/W4308558335",
        "https://openalex.org/W2128200012"
    ],
    "abstract": "Timely acquiring the earthquake-induced damage of buildings is crucial for emergency assessment and post-disaster rescue. Optical remote sensing is a typical method for obtaining seismic data due to its wide coverage and fast response speed. Convolutional neural networks (CNNs) are widely applied for remote sensing image recognition. However, insufficient extraction and expression ability of global correlations between local image patches limit the performance of dense building segmentation. This paper proposes an improved Swin Transformer to segment dense urban buildings from remote sensing images with complex backgrounds. The original Swin Transformer is used as a backbone of the encoder, and a convolutional block attention module is employed in the linear embedding and patch merging stages to focus on significant features. Hierarchical feature maps are then fused to strengthen the feature extraction process and fed into the UPerNet (as the decoder) to obtain the final segmentation map. Collapsed and non-collapsed buildings are labeled from remote sensing images of the Yushu and Beichuan earthquakes. Data augmentations of horizontal and vertical flipping, brightness adjustment, uniform fogging, and non-uniform fogging are performed to simulate actual situations. The effectiveness and superiority of the proposed method over the original Swin Transformer and several mature CNN-based segmentation models are validated by ablation experiments and comparative studies. The results show that the mean intersection-over-union of the improved Swin Transformer reaches 88.53&amp;#x0025;, achieving an improvement of 1.3&amp;#x0025; compared to the original model. The stability, robustness, and generalization ability of dense building recognition under complex weather disturbances are also validated.",
    "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023 369\nImproved Swin Transformer-Based Semantic\nSegmentation of Postearthquake Dense\nBuildings in Urban Areas Using\nRemote Sensing Images\nLiangyi Cui, Xin Jing, Yu Wang, Yixuan Huan, Yang Xu, and Qiangqiang Zhang\nAbstract—Timely acquiring the earthquake-induced damage of\nbuildings is crucial for emergency assessment and post-disaster\nrescue. Optical remote sensing is a typical method for obtaining\nseismic data due to its wide coverage and fast response speed. Con-\nvolutional neural networks (CNNs) are widely applied for remote\nsensing image recognition. However, insufﬁcient extraction and ex-\npression ability of global correlations between local image patches\nlimit the performance of dense building segmentation. This paper\nproposes an improved Swin Transformer to segment dense urban\nbuildings from remote sensing images with complex backgrounds.\nThe original Swin Transformer is used as a backbone of the encoder,\nand a convolutional block attention module is employed in the\nlinear embedding and patch merging stages to focus on signiﬁcant\nfeatures. Hierarchical feature maps are then fused to strengthen the\nfeature extraction process and fed into the UPerNet (as the decoder)\nto obtain the ﬁnal segmentation map. Collapsed and non-collapsed\nbuildings are labeled from remote sensing images of the Yushu\nand Beichuan earthquakes. Data augmentations of horizontal and\nvertical ﬂipping, brightness adjustment, uniform fogging, and non-\nuniform fogging are performed to simulate actual situations. The ef-\nfectiveness and superiority of the proposed method over the original\nSwin Transformer and several mature CNN-based segmentation\nmodels are validated by ablation experiments and comparative\nstudies. The results show that the mean intersection-over-union\nof the improved Swin Transformer reaches 88.53%, achieving an\nimprovement of 1.3% compared to the original model. The stability,\nrobustness, and generalization ability of dense building recognition\nunder complex weather disturbances are also validated.\nManuscript received 26 October 2022; revised 20 November 2022; accepted\n24 November 2022. Date of publication 28 November 2022; date of current\nversion 15 December 2022. This work was supported in part by the National Key\nResearch and Development Program under Grant 2019YFC1511005, in part by\nthe China Postdoctoral Science Foundation under Grant BX20190102 and Grant\n2019M661286, and in part by the Heilongjiang Province Postdoctoral Funding\nunder Grant LBH-TZ2016 and Grant LBH-Z19064 and in part by Heilongjiang\nProvince Natural Science Funding under Grant LH2022E070.(Liangyi Cui and\nXin Jing contributed equally to this work.) (Corresponding authors: Qiangqiang\nZhang; Yang Xu.)\nLiangyi Cui, Xin Jing, Yu Wang, Yixuan Huan, and Qiangqiang Zhang are\nwith the School of Civil Engineering and Mechanics, Lanzhou University,\nLanzhou 730000, China, and also with the Key Laboratory of Mechanics on\nDisaster and Environment in Western China, The Ministry of Education of\nChina, Beijing 100816, China (e-mail: cuily20@lzu.ed.cn; jingx21@lzu.edu.cn;\nwangyu16@lzu.edu.cn; huanyx21@lzu.edu.cn; zhangqq@lzu.edu.cn).\nYang Xu is with the School of Civil Engineering, Harbin Institute of Tech-\nnology, Harbin 150001, China (e-mail: xyce@hit.edu.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3225150\nIndex Terms —Attention mechanism, complex weather\ndisturbances, dense seismic building segmentation, feature\nfusion, improved Swin Transformer, remote sensing images.\nI. INTRODUCTION\nE\nARTHQUAKES are one of the most severe natural dis-\nasters, and due to the recent acceleration of urbanization\ndevelopment, earthquake-induced building damage has become\none of the most severe threats to human beings [1]. Therefore,\nafter an earthquake occurs, it is crucial to recognize the number,\nlocation, and damage level of urban buildings rapidly to en-\nsure postearthquake rescue and reconstruction [2]. The seismic\ndamage-related data have been mainly collected via ﬁeld investi-\ngation, which is labor-time-intensive and inefﬁcient. In addition,\nparticular circumstances, such as power facility destruction and\ncommunication system interruption caused by earthquakes, can\nbring additional challenges to conducting immediate ﬁeld in-\nvestigation. Therefore, an efﬁcient and effective method that\ncan meet the practical requirements of postearthquake rapid\nassessment and emergency rescue is urgently needed.\nIn recent years, with the development of satellite systems,\nremote sensing techniques have become increasingly popular\nin the ﬁeld of natural disaster assessment [3]. The commonly-\nused remote sensing data [4], [5], can be roughly divided into\nthree categories: synthetic aperture radar images [6], [7]; optical\nimages [8]; and light detection and ranging data [9]. Among\nthem, high-resolution optical images—which are easy to obtain\nand can provide rich information on postearthquake building\nattributes, such as color, texture, and shape—have been the most\nwidely used [10]. Remote sensing images are wide-ranging,\nall-weather, unaffected by earthquakes, and accessible without\nonsite human inspection. In early-stage research, remote sensing\nimage interpretation primarily relied on preset thresholds and\nhandcrafted parameters and thus was highly affected by a subjec-\ntive judgment in various application scenarios. In addition, the\nrecognition speed and reliability highly depended on engineer-\ning experience and prior knowledge of image analysts. However,\nautomatic extraction and autonomous recognition of seismic\ndamage from remote sensing images have rapidly developed\nwith advanced computer vision techniques, including image\nprocessing, machine learning, and deep learning.\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n370 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nCompared to image processing and machine learning, deep\nlearning has relatively better learning ability and stronger robust-\nness against interference and variations in object size, position,\nshape, and geometry and thus can provide more accurate lo-\ncalization and damage information on dense seismic buildings\n[11]. Convolutional neural networks (CNNs) have been the most\nwidely-used deep learning-based model for seismic damage data\nextraction from high-resolution remote sensing optical images.\nCurrently, CNNs are widely applied to seismic damage iden-\ntiﬁcation from postearthquake remote sensing images. Cooner\net al. [12] adopted CNNs to classify high-resolution seismic\nremote sensing imagery and quickly detect damaged buildings,\nachieving an accuracy of 55% for the 2010 Haiti earthquake\nwith the 7.0 magnitude. Ma et al. [13] combined remote sensing\nimages with block vector data and improved the Inception V3\narchitecture; a test accuracy of 90.07% on postearthquake aerial\nimagery of Yushu was achieved. Furthermore, Ji et al. [14] used\nthe pretrained VGG model to recognize collapsed buildings in\nremote sensing images before and after the 2010 Haiti earth-\nquake, concluding that the ﬁne-tuned VGGNet model outper-\nformed the original VGGNet model trained from scratch with\nan overall accuracy increasing from 83.38% to 85.19%. Xiao\net al. [15] proposed a dynamic cross-fusion network to enable\neach task to share features from different CNN layers adaptively\nand achieved state-of-the-art performance. Zhan et al. [16] used\nthe Mask R-CNN to extract information on damaged buildings\nfrom postearthquake remote sensing images and identify the\ndamage level. An improved feature pyramid network (FPN) was\ndesigned, and a detection accuracy of 92% was achieved for\nthe most severely damaged buildings (the overall classiﬁcation\naccuracy for four damage classes was 88%).\nHowever, conventional CNNs can focus only on a small range\nof pixel-level features, thereby providing insufﬁcient informa-\ntion on global correlations between local pixels and lack the\ncapacity to model global relationships between objects within\nan image and nonlocal relationships between pixels. In addition,\nthe limited receptive ﬁeld could not provide sufﬁcient contex-\ntual features, which might have a signiﬁcant impact on the\ndamage assessment accuracy of dense seismic buildings [17].\nTransformer-based models using global self-attentive mecha-\nnisms can compensate for the abovementioned shortcomings of\nconventional CNNs that focus only on local receptive ﬁelds with-\nout considering global features [18], [19], [20], [21], allowing\neach pixel to contain global correlations and thus improving\ngeneralization ability and interference robustness [22], [23],\n[24], [25], [26].\nDosovitskiy et al. [27] ﬁrst present the vision transformer\n(ViT) models and utilized the transformer as the backbone\nnetwork for image classiﬁcation tasks. The ViT models to-\nkenized the input image into ﬁxed-size patches, which were\nthen ﬂattened as vectors and fed to the transformer backbone.\nExperimental results demonstrated that the ViT models pre-\ntrained on large-scale datasets could achieve better performance\nthan the CNNs when migrated to the classiﬁcation tasks on\nsmall-size and medium-size datasets. In recent years, several\ntransformer-based vision models have been proposed for dif-\nferent computer vision tasks, such as target classiﬁcation [28],\nobject detection [29], and semantic segmentation [30], [31],\n[32]. Despite the successful application of the transformer in\nthe natural language processing ﬁeld, there are two main chal-\nlenges in its application to the visual domain from the original\nlanguage domain. These challenges are introduced by signiﬁcant\ndifferences in visual entity size among images and much higher\nresolutions of images compared to texts, which leads to an\nintensive computational cost.\nTo solve the above problem, Swin Transformer [33] is pro-\nposed with two principle improvements over conventional ViTs.\n1) A hierarchical structure similar to the CNN structure is\ndesigned. This structure is very ﬂexible in multiscale\nmodeling and reduces the increase in computational com-\nplexity with the image size from square to linear.\n2) The shifted window multihead self-attention (SW-MSA)\nblock is proposed to reduce the computational cost while\nconsidering the information transferred between different\nwindows.\nAlthough the transformer-based models made a splash in\ncomputer vision, they have still been in the infancy phase\nfor large-scale seismic disaster evaluation in urban areas.\nDa et al. [34] developed a two-stage damage assessment frame-\nwork named the SDAFormer, which feeds pre-disaster and\npostdisaster images to the network separately for damage as-\nsessment. The SDAFormer won ﬁrst place on the xBD (a\nlarge-scale building damage assessment dataset) and achieved\na mean intersection-over-union (mIoU) improvement of 1.5%\ncompared to the second-place method. Chen et al. [35] proposed\na transformer-based damage assessment architecture consisting\nof a Siamese transformer encoder and a lightweight dual-tasks\ndecoder, which outperformed traditional CNN models such as\nthe Mask R-CNN and Siamese-UNet.\nAlthough the CNN models have been extensively investigated\nfor computer vision tasks, the feature extraction process of\nconventional CNN is always performed at a local region, and\nmodelling the global correlation is challenging. Considering\nthe characteristics of the investigated remote sensing images\nfor postearthquake buildings in a city area, the buildings are\ndensely distributed, and the structure style and damage type\nare often similar, which suggests that the small-region features\nare closely related and the global correlations should be signiﬁ-\ncant for the recognition accuracy. Therefore, this article designs\nan integrated model using the improved Swin Transformer for\nglobal correlation modeling and CNN for local feature extraction\nto further enhance the recognition capacity of building damage\nstates and location semantics, respectively.\nMeanwhile, statistical analyses of previous studies have\ndemonstrated that clouds approximately cover 70% of the Earth,\nwhich suggests that weather interferences of cloud or fog ob-\nscuration and illumination variances inevitably exist in remote\nsensing optical images [36]. In addition, postearthquake remote\nsensing images can suffer from light overexposure and dark-\nness due to various illumination conditions. Therefore, accurate\nrecognition of dense seismic buildings in images collected under\nstrong weather disturbances represents a great challenge in\nsemantic segmentation. However, research on semantic segmen-\ntation of postearthquake remote sensing images of dense urban\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 371\nbuildings with complex backgrounds and strong interferences is\nrather limited.\nTo address the abovementioned limitations, this article pro-\nposes a semantic segmentation method for seismic damage of\nlarge-scale dense buildings in large-scale urban areas with com-\nplex backgrounds and strong weather interferences. In addition,\nthe opportunity of incorporating the transformer and CNN for\nseismic damage recognition from remote sensing images is\nanalyzed.\nThe main contributions of this article can be summarized as\nfollows.\n1) An effective semantic segmentation method is proposed\nfor high-resolution remote sensing optical images of dense\nbuildings with complex backgrounds and strong weather\ninterferences; this method can accurately and simulta-\nneously extract the building damage state and location\nsemantics.\n2) An improved Swin Transformer with the encoder-decoder\nstructure is proposed to simultaneously exploit multilevel\nlocal features and global correlations, which performs the\nmultilevel feature fusion at each stage of the encoder,\ninserts convolutional block attention module (CBAM) in\nthe linear embedding and patch merging modules, and uses\nthe UPerNet as a decoder.\n3) Two actual seismic scenarios of Yushu city and Beichuan\ncity with different weather disturbances are used to sim-\nulate possible light overexposure, darkness, and fog oc-\nclusions and validate the effectiveness of the proposed\nmethod.\n4) Ablation experiments are performed to demonstrate the\nefﬁcacy and necessity of the proposed modules in the im-\nproved Swin Transformer. In addition, comparative stud-\nies are conducted to verify the superiority of the improved\nSwin Transformer over the original Swin Transformer and\nvarious mature CNN-based segmentation models.\nThe rest of the article is organized as follows. Section II\ndescribes the architecture of the improved Swin Transformer.\nSection III introduces the dataset and implementation details.\nSection IV presents the test results under two real-world seis-\nmic scenarios, ablation experiments, and comparative studies.\nSection V concludes the article.\nII. PROPOSED METHOD\nA. Overall Architecture\nAn improved Swin Transformer based on the encoder-decoder\nframework is proposed to realize accurate semantic segmen-\ntation of postearthquake dense buildings from remote sensing\nimages with complex backgrounds and strong weather inter-\nferences. The overall architecture that uses the original Swin\nTransformer as a backbone of the encoder is presented in Fig. 1.\nAs shown in Fig. 1, a feature fusion module is added to the end\nof the encoder to fully exploit the extracted features at various\nlevels. In the proposed structure, hierarchical feature maps are\nconcatenated using convolutions to enrich the transferable local\nfeatures of different stages by multilevel feature fusion. In addi-\ntion, the CBAM is inserted into the linear embedding and patch\nmerging modules to alleviate feature leakage during the patch\nFig. 1. Overall architecture of improved Swin Transformer for dense building\nsegmentation.\ndownsampling process in the encoding stage. This enables the\nproposed model to distinguish different building damage states\nand location semantics, thus improving multiclass segmentation\naccuracy. Finally, the UperNet incorporating multilevel features\nis used as a decoder. Details on the feature fusion and CBAM\nmodules are described in the following sections.\nB. Swin Transformer Backbone\nThe Swin Transformer backbone includes an initial patch\npartition module and four different stages denoted by stages 1–4.\nStage 1 consists of a linear embedding layer and two consecutive\nSwin Transformer blocks. Stage 2 consists of a patch merging\nmodule and two Swin Transformer blocks. Stage 3 consists\nof a patch merging module and 18 Swin Transformer blocks.\nFinally, Stage 4 consists of a patch merging module and two\nSwin Transformer blocks.\nFor the patch partition module, the input image with a size\nof H × W × 3 is split four times in the spatial directions\nand ﬂatted in the channel direction, generating a patch size of\nH/4 × W/4 × 48. Then, the linear embedding layer projects the\nchannel dimension to an arbitrary number denoted byC (in this\narticle, C = 128) through the 1× 1 convolution, generating a\nfeature map with a size ofH/4 × W/4 × C. The feature map\nof each stage is input into the patch merging module, and a\nhalf-ﬂat-size downsampling process is performed by neighbor-\nhood sampling every two points, and thus the channel number\nquadruples. Then, a 1× 1 convolution is utilized to adjust the\n372 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 2. Schematic of the patch partition module.\nFig. 3. Schematic of the fundamental component for Swin Transformer block.\nchannel number to double. The overall schematic of the patch\npartition module is presented in Fig. 2.\nThe schematic diagram of the Swin Transformer block, which\nis the fundamental component of the Swin Transformer, is\npresented in Fig. 3. Each Swin Transformer block includes a\nregular window and a shift window. The regular window con-\nsists of a layer-normalization (LN) layer, a window multihead\nself-attention (W-MSA) module, a residual connection, an LN\nlayer, a multilayer perceptron (MLP), and a residual connection.\nThe shift window has a similar structure as the regular window;\nthe only difference is that an SW-MSA module is used instead of\nthe W-MSA. The mathematical formula of the Swin Transformer\nblock is expressed as follows:\nˆZl = W − MSA[LN(Zl−1)] +Zl−1\nZl = MLP[LN( ˆZl)] +ˆZl\nˆZl+1 =S W− MSA[E(Zl)] +Zl\nZl+1 = MLP[LN( ˆZl+1)] +ˆZl+1 (1)\nwhere Zl−1 and Zl+1 denote the input and output of the\nSwin Transformer block, respectively. A detailed description of\nW-MSA, MLP, and SW-MSA can be found in the study of Han\net al. [32].\nFig. 4. Flowchart of the feature fusion module.\nC. Feature Fusion Module\nCompared with the traditional semantic segmentation task,\nthe dataset investigated in this article consists of remote sensing\nimages with complex backgrounds, and its unique character-\nistics are reﬂected in two aspects: images contain complex\nbackgrounds, including several types of strong distractions, such\nas illumination variations and fog obscurations; and buildings in\nremote sensing images are in different geometries; particularly,\nshapes and sizes of collapsed and not-collapsed buildings are\ndifferent.\nA previous study has shown that using different convolution\noperators in the transformer architecture can provide infor-\nmation on both local and global features of the input image,\nsigniﬁcantly improving the semantic segmentation performance\n[37]. Inspired by this idea, a multilevel feature fusion module is\ndesigned after each Swin Transformer block to convolute the\nfeature maps output by the previous levels to further enhance the\nextraction capability of local features and global correlations.\nAlthough the Swin Transformer has a hierarchical structure,\nthere are no interactions between feature maps at any stages.\nTherefore, enriching the extracted features is essential con-\nsidering that remote sensing images of postearthquake dense\nbuildings contain various types of background distractions, in-\ncluding illumination overexposure, darkness, uniform fog, and\nnon-uniform fog, and have a high diversity of geometric shapes\nand sizes.\nThe schematic diagram of the feature fusion module is shown\nin Fig. 4, where four feature maps from the corresponding\nstage of the Swin Transformer backbone are illustrated. The ﬂat\ndimension of each stage is halved, and the channel dimension\nis doubled. The feature map of each stage is downsampled by\na2 × 2 convolutional kernel with a sliding stride of two and\nconcatenated with that of the next stage in the channel direction.\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 373\nFig. 5. Schematic of CBAM attention module.\nFig. 6. Flowchart of inserting CBAM in linear embedding module.\nThen, the channel number of the concatenated feature map is half\nreduced by the 1× 1 convolution. Finally, feature maps from all\nstages are fused in the channel direction, and the channel size is\nquartered using a 1× 1 convolutional kernel.\nD. Convolutional Block Attention Module\nThe attention mechanism is a typical way to achieve adaptive\nattention inside a neural network, and the commonly-used atten-\ntion mechanisms include channel attention and spatial attention.\nThe channel attention aims to enable the network to focus on the\ncategory information inside an image by keeping the channel di-\nmension unchanged and compressing the spatial dimension into\na scalar. Furthermore, the spatial attention assists the network\nin paying more attention to the location information of targets\ninside an image by keeping the spatial dimension unchanged\nand compressing the multiple-channel dimension into one single\nchannel. This article utilizes the CBAM by simultaneously com-\nbining channel attention and spatial attention and can distinguish\nsigniﬁcant feature maps of building damage states and location\nsemantics. The schematic diagram of the CBAM, which is a\nlightweight attention mechanism module consisting of a channel\nattention part and a spatial attention part by Woo et al. [38], is\npresented in Fig. 5. Details of CBAM have been presented in\n[38] and omitted here.\nThe process of inserting the CBAM into the linear embedding\nmodule is illustrated in Fig. 6. The dimension of the feature map\ngenerated by the patch partition module is transformed toC by\na1 × 1 convolution block, and the CBAM module is inserted\nbefore the LN layer.\nConventional downsampling operations often use convolu-\ntion, average pooling, and maximum pooling in a local region,\nwhich will inevitably cause feature leakage. Patch merging\nselects the neighborhood of every two pixels, reassembles them\ninto a series of patches (the spatial size of patches is halved), and\nconcatenates the patches in the channel dimension (the channel\ndimension is quadrupled), which is ﬁnally followed by a 1× 1\nconvolution to adjust the channel dimension. Therefore, all the\ninput information can be reserved, and no feature leakage occurs\nin patch merging. The process of inserting the CBAM into the\nFig. 7. Flowchart of inserting CBAM in patch merging module.\nFig. 8. UPerNet decoder architecture.\nFig. 9. Schematic of the PPM module in the decoder.\npatch merging module is presented in Fig. 7. In each channel,\nneighborhood areas of every two points are reassembled into a\npatch (i.e., the ﬂat size is halved). The reconstructed patches\nare fed into the CBAM module individually, and the output\nfeature maps of the CBAM module are fused in the channel\ndirection. The CBAM module is followed by an LN layer and a\nfully-connected linear layer.\nE. UPerNet Decoder\nFor remote sensing images with complex backgrounds and\nsmall dense buildings, a multilevel segmentation predictor, the\nUPerNet [39], is employed to achieve full-scale coverage from\nlow-level concrete features to high-level abstract features. The\ndesign of the UPerNet is based on the pyramid pooling module\n374 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 10. Representative postearthquake remote sensing images with data augmentation. (a) Original image of Yushu city. (b) Brightness transformations of light\noverexposure and darkness. (c) Uniform fogging in light and heavy degrees. (d) Nonuniform fogging in light and heavy degrees.\n(PPM) [40] and FPN, which fully integrates extracted features\nfrom different stages of the encoder. The architecture of the\nUPerNet decoder is shown in Fig. 8.\nThe PPM block utilizes pooling kernels covering different\nportions of the input feature map to generate multiscale corre-\nlations among different subregions. In this article, a four-level\npyramid pooling is designed to individually perform the pooling\noperation for the whole, half of, a third of, and a sixth of the input\nfeature map. Then, the channel dimensions are adjusted using\n1 × 1 convolution, and the spatial dimensions are uniﬁed by\nbilinear interpolation upsampling. Finally, they are fused as the\nglobal prior and concatenated with the original feature map at\nthe channel dimension, as shown in Fig. 9.\nIII. DATASET ANDIMPLEMENTATIONDETAILS\nA. Dataset\nIn this article, 24 remote sensing city-scale images of the\nYushu city and Beichuan city after Yushu and Wenchuan earth-\nquakes with a resolution of 4608× 2560 were used. The orig-\ninal images were downloaded from the Internet and manually\npixel-wise labeled using “labelme” [41] to classify buildings\ninto collapsed and non-collapsed buildings. Buildings with de-\nstructive shapes, severely-damaged roofs, columns, and beams\nwere classiﬁed as collapsed, and other buildings were labeled as\nnon-collapsed.\nData augmentation operations, including random ﬂipping in\nthe horizontal and vertical directions, brightness transformation,\nuniform fogging, and nonuniform fogging, were performed to\nexpand the dataset and simulate possible light overexposure and\ndarkness and fog occlusions in remote sensing images.\nThe brightness transformation is realized by rescaling the\npixel intensity as follows:\nˆI(h,w)= median [0,α × I(h,w),255] (2)\nwhere I(h,w) and ˆI(h,w) denote the image intensity at the\npixel location (h,w) before and after brightness transformation,\nrespectively; α denotes the rescaling coefﬁcient controlling the\nlight exposure and darkness; median operator ensures the trans-\nformed pixel intensity within the range of 0-255.\nBased on the dark channel prior theory [42], dark pixels have\nvery low intensity in at least one color channel of the RGB\nfor most local regions that do not cover the sky; therefore, the\nnon-uniform fogging operation is expressed by\nˆJ(h,w)= t(h,w)J(h,w)+[ 1− t(h,w)] × A\nt(h,w) = exp[−β × d(h,w)]\nd(h,w)= − γ ×\n√(\nh− H\n2\n)2\n+\n(\nw− W\n2\n)2\n+ W (3)\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 375\nFig. 11. Test results of 512× 512 patches for Yushu city. (a) Patch 1. (b) Patch 2.\nFig. 12. Test results of large-scale remote sensing image for Yushu city. (a) Input Image. (b) Ground-truth Annotation. (c) Original Swin Transformer.\n(d) Improved Swin Transformer.\n376 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 13. Test results of 512× 512 patches for Beichuan city. (a) Patch 1. (b) Patch 2.\nFig. 14. Test results of large-scale remote sensing image for Beichuan city. (a) Input Image. (b) Ground-truth Annotation. (c) Original Swin Transformer.\n(d) Improved Swin Transformer.\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 377\nFig. 15. Test results of negative objects for wild regions with trees, tents, and rivers. (a) Patch 1. (b) Patch 2.\nwhere J(h,w) and ˆJ(h,w) denote the image intensity before\nand after fogging transformation;hand w are the pixel indexes\nin the height and width directions, respectively;A denotes the\nfog brightness parameter, and its value is in the range of 0-255\ncorresponding to the grayscale intensity of fog changes from\nblack to white; t(h,w) represents the light transmittance; β\ndenotes the fogging concentration factor;γ denotes the constant\ninﬂuence factor, and in this articleγ =0 .04; d(h,w) denotes\nthe scene depth.H and W denote the height and width of the\ninput image.\nConsidering that remote sensing images could be completely\ncovered by a large area of clouds or fog, the uniform fogging\noperation is used to simulate possible scenarios and enhance the\ndataset as\nˆJ(h,w)= tJ(h,w)+( 1− t) × A. (4)\nEquation (4) is a particular case of (3) with a constant light\ntransmittance at all pixel locations, whereˆJ(h,w) denotes the\nimage intensity after uniform fogging transformation,J(h,w)\ndenotes the original image andL(h,w) denotes a new image\nwith the identical pixel value of 170 on three channels of RGB.\nFig. 10 shows some representative postearthquake remote\nsensing images with dense buildings after brightness, uniform,\nand non-uniform fogging transformations with different con-\nﬁgurations. After data augmentation, the original images were\ncropped to 512× 512 patches with an overlap ratio of 50%.\nFinally, 8262 patches were obtained, 80% of which were used\nfor training by random assignment, and the rest was used for\nvalidation.\nB. Implementation Settings\nThe proposed method was implemented in PyTorch 1.7.0 on\na workstation equipped with an i9-10900k CPU and a GeForce\nRTX 3090 GPU. The AdamW optimization algorithm was em-\nployed to update the model parameters under a learning rate\nof 0.0001, a batch size of 8, and a training epoch of 50. The\nmIoU between the predicted and ground-truth buildings was\nused as an evaluation metric of the proposed method and used the\nweights obtained from pre-trained on the ADE20K [43] dataset\nas pre-training weights for the model.\nIV . RESULTS ANDDISCUSSION\nA. Test Results of Yushu City\nRemote sensing seismic images of Yushu city, including\nvarious weather disturbances, were used to demonstrate the\n378 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nrecognition accuracy of the proposed method for postearthquake\ndense buildings. The test results obtained by the original and\nimproved Swin Transformers on 512× 512 patches of Yushu\ncity collected under different weather disturbances are presented\nin Fig. 11. The results show that the proposed improved Swin\nTransformer achieved higher accuracy and better robustness\nagainst light overexposure, darkness, and fog occlusions than the\noriginal Swin Transformer with an average mIoU improvement\nof 0.83% for 512× 512 patches. In Fig. 11, white circles in\nsub-ﬁgures present local details of predicted building corners\nand edges, indicating that the improved Swin Transformer could\nmaintain better recognition ability under various weather dis-\nturbances than the original Swin Transformer. In addition, the\nimproved Swin Transformer achieved better recognition on the\nfogging test images where the buildings were already difﬁcult to\ndistinguish, and the mIoU value improved by 1.18% compared to\nthe orginal Swin Transformer. The test results on the large-scale\nimage with a resolution of 4608×2560 are presented in Fig. 12,\nwhich shows that the improved Swin Transformer performed\nbetter than the original Swin Transformer.\nB. Test Results of Beichuan City\nRemote sensing seismic images of Beichuan city, which in-\ncluded various weather disturbances, were used to demonstrate\nthe recognition accuracy of postearthquake dense buildings fur-\nther. The test results obtained by the original Swin Transformer\nand improved Swin Transformer on the 512× 512 patches of\nBeichuan city are presented in Fig. 13. The results in Fig. 13\nshow that the improved Swin Transformer achieved higher accu-\nracy and better robustness against light overexposure, darkness,\nand fog occlusions than the original Swin Transformer with an\naverage mIoU improvement of 1.05% for 512× 512 patches. In\naddition, the improved Swin Transformer still achieved better\nrecognition on the fogging test images, and the mIoU value\nimproved by 1.77% compared to the orginal Swin Transformer.\nAdditional test results on the 512× 512 patches are given in\nFig. 20. The test results of the two transformers on the large-scale\nimage with a resolution of 4608×2560 are presented in Fig. 14,\nwhich shows that the improved Swin Transformer performed\nbetter than the original Swin Transformer.\nC. Discussion of Test Results\nFor all test images of Yushu city and Beichuan city, the\noriginal Swin Transformer had more local misrecognition and\nlarger prediction errors for building edges than the improved\nSwin Transformer, which resulted in the distinct shape variance\nof dense building regions. The original Swin Transformer tended\nto ignore unconnected pixels inside the building region and\nclassiﬁed them into the same class. Moreover, the improved\nSwin Transformer achieved higher recognition accuracy than\nthe original Swin Transformer for collapsed buildings with more\nirregular geometrical shapes. The recognition results of negative\nobjects for wild regions with trees, tents, and rivers are shown in\nFig. 15. The results show that negative objects are successfully\nclassiﬁed into the background, and misrecognition rarely occurs.\nFig. 16. Comparisons of category-wise IoU with different models and weather\ndisturbances. (a) IoU comparisons for each category using Original and the\nimproved Swin Transformer. (b) IoU contour plot for each category under\ndifferent weather disturbances use the improved Swin Transformer.\nIt further indicates that the proposed model possesses good\nstability against complex environmental disturbances.\nUnder weather disturbances of fogging and brightness trans-\nformation, the misrecognition of the background of collapsed\nbuildings and incomplete recognition of non-collapsed build-\nings often occurred. A possible reason may be that the fog-\nging and brightness transformation introduced severe occlu-\nsion in certain areas, thus increasing the difﬁculty of accurate\nsegmentation.\nThe comparison results of category-wise intersection-over-\nunion (IoU) of the two models for different weather distur-\nbances are presented in Fig. 16. As shown in Fig. 16(a), the\nproposed Swin Transformer improved the average segmentation\nIoU for each category with a lower volatility than the original\nSwin Transformer, suggesting the robustness and stability of\nthe proposed method. Fig. 16(b) shows that the model perfor-\nmance decreased for each category when weather disturbances\nexisted. Among the considered types of weather disturbances,\nthe non-uniform fogging affected the model performance of the\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 379\nTABLE I\nCOMPARISONS OFMODEL PERFORMANCE IN ABLATION EXPERIMENTS\nimproved Swin Transformer the most, and the proposed model\nwas less sensitive to brightness transformation than fogging\nocclusion.\nIt should be noted that the remote sensing images of Yushu\ncity and Beichuan city had unique characteristics. In Yushu city,\nbuildings were more densely distributed; intensities in the color\nspace were similar to the background and plenty of tents and\nvehicles existed in the images, which increased difﬁculty in\nrecognition. Although these factors could cause a slight decrease\nin average IoU, the improved Swin Transformer still achieved\ngood recognition accuracy for each category. The results also\nindicated that the proposed method efﬁciently addressed the\ndeﬁciencies of the original Swin Transformer and enhanced the\nedge smoothness and completeness of the results of geometri-\ncal shapes for postearthquake dense buildings. Therefore, the\nimproved Swin Transformer had stronger robustness and resis-\ntance to different types of severe interferences under real-world\nscenarios than the original Swin Transformer.\nD. Ablation Experiments and Comparative Studies\nAblation experiments were performed to demonstrate the\neffectiveness and necessity of the feature fusion and CBAM\nmodules in the improved Swin Transformer. Besides the pro-\nposed model (including both the feature fusion module and\nthe CBAM module), three additional models, namely the orig-\ninal Swin Transformer, the Swin Transformer+ feature fusion\nmodule, and the original Swin Transformer+ CBAM module,\nwere trained using the same dataset, optimization algorithm,\nand training hyperparameters. Table I gives the comparison\nresults of model performances in the ablation experiments. The\nresults showed that both the feature fusion and the CBAM had\ncertain contributions to the model performance improvement,\nbut the effect of the feature fusion module was more signiﬁcant.\nAccordingly, the feature fusion and CBAM modules improved\nTABLE II\nCOMPARISONS OFMODEL PERFORMANCE WITH CNN-BASED METHODS\nTABLE III\nCOMPARISONS OFDIFFERENT FEATURE FUSION MODULES\nthe segmentation accuracy of background, collapsed buildings,\nand noncollapsed buildings.\nThe full model achieved the highest improvements in seg-\nmentation IoU of background, collapsed buildings, and non-\ncollapsed buildings by 0.38%, 1.57%, and 1.95%, respectively.\nThe overall mIoU improvement was 1.3%, demonstrating that\nthe improved Swin Transformer successfully integrated the\nadvantages of feature fusion and CBAM modules. It further\nindicated the effectiveness of multilevel feature fusion in al-\nleviating feature leakage and CBAM in focusing on small dense\nobjects.\nTo verify the effectiveness of the improved Swin Transformer\nover conventional CNNs, several mature CNN-based semantic\nsegmentation models, including the PSPNet [43], DeepLabV3+\n[44], and UNet [45], were used for comparison. The dataset, opti-\nmization algorithm, and training hyperparameters were the same\nas those of the improved Swin Transformer. Table II gives a com-\nparison of the performances of the improved Swin Transformer\nand several CNN-based models. The results showed that the\nUNet performed the best among the three CNN-based segmen-\ntation models but worse than the proposed Swin Transformer.\nAlthough the background IoU, noncollapsed IoU, collapsed\nIoU, and mIoU of the UNet reached 93.86%, 80.85%, 79.05%,\nand 84.59%, the improved Swin Transformer performed better\nin terms of all metrics by 2.49%, 5.22%, 5.11%, and 3.94%,\n380 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 17. Test results under different fogging conditions for a representative image.\nrespectively. This indicated that the proposed method integrating\nthe Swin Transformer and CNN together enhanced the semantic\nsegmentation accuracy of dense buildings in postearthquake\nremote sensing images compared to conventional CNN-based\nmodels.\nThe feature fusion module is designed to alleviate the possible\nfeature leakage and enhance the multistage feature extraction.\nEven if some features at a particular stage are ignored, the feature\nfusion module can ensure that the information on missed features\nis retained and can be fed into the subsequent decoder. The\nauthors admit that it is indeed challenging to determine which\nfeature stage is essential and should be enhanced in the feature\nfusion module. Therefore, the feature fusion model is designed\nin a two-step manner: the adjacent stages are fused to alleviate\nthe feature leakage at the previous stage; and all the stages are\nfused at the ﬁnal stage to take full advantage of the multistage\nfeatures.\nIn addition, two comparative studies are performed to demon-\nstrate the effectiveness of the proposed feature fusion module.\nFirst, the feature fusion module is only adopted at the ﬁnal stage\nand ignored for the adjacent stages in the encoder, noted as\nfeature fusion-1 in Table III.\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 381\nFig. 18. Test results under different lightness conditions for a representative image.\nSecond, the feature fusion module is adopted both in the\nencoder and decoder, noted as feature fusion-2 in Table III. The\nencoder part is the same as Fig. 4; for the decoder part, feature\nmaps of the ﬁrst and second stages are downsampled by 2× 2\nconvolution and concatenated in the channel dimension with\nthose of the next stage. Afterward, the number of channels is\nhalved by 1× 1 convolution, and the residuals are ﬁnally added\ntogether. Table III gives the comparison results of these three\ndifferent feature fusion modules, indicating that both insufﬁcient\n(feature fusion-1) and excessive (feature fusion-2) feature fusion\nmodules have negative impacts on recognition accuracy.\nTo explore the applicable range of controlling parameters un-\nder each weather condition, more experiments are performed, as\nshown in Figs. 17 and 18. Fig. 17 shows representative test results\nunder various lightness conditions. It suggests that the control-\nling parameterαcould be recommended in the range of 0.4–1.3\nwith a high mIoU over 0.8. Whenαis set as 1.9, a signiﬁcant drop\nof about 19.85% in the prediction accuracy occurs. Fig. 18 shows\nrepresentative test results under various fogging conditions. It\nsuggests that the controlling parameterβcould be recommended\nin the range of 0–0.04 with a high mIoU over 0.8. Whenβis set as\n0.05, a signiﬁcant drop of about 20% in the prediction accuracy\noccurs.\nFig. 19. Comparison of segmentation mIoU for different insertion positions\nof CBAM in original Swin Transformer backbone (using the average results of\nthree independent experiments).\nThe mIoU increasing curves of the background, noncollapsed,\nand collapsed buildings for different CBAM insertion locations\nin the original Swin Transformer backbone are presented in\n382 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 20. Additional test results for 512× 512 patches in Beichuan city and Yushu city (white circles compare local details of predicted building corners and\nedges improved by the proposed method). (a) Patch 1. (b) Patch 2. (c) Patch 3. (d) Patch 4. (e) Patch 5.\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 383\nFig. 20. (Continued).\nFig. 19, where “CBAM before concat” represents that all fea-\nture maps were ﬁrst input into the CBAM module and then\nconcatenated in the proposed patch merging block; “CBAM\nafter concat” represents that all the related feature maps were\nconcatenated before being input into the CBAM module in the\nPatch Merging block. The results indicated that the insertion\nstrategy of CBAM before concatenation gained the higher train-\ning accuracy and lower diversity than inserting CBAM after\nconcatenation.\nV. CONCLUSION\nThis article proposed an improved Swin Transformer for\nremote sensing segmentation of postearthquake dense buildings\nin urban areas. The main contributions of this article are obtained\nas follows.\n1) An improved Swin Transformer following the encoder-\ndecoder framework was proposed to achieve accurate\nsemantic segmentation of postearthquake dense buildings\nfrom remote sensing images under complex backgrounds\n384 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nand strong weather interferences. The proposed struc-\nture performed multilevel feature fusion at each stage of\nthe encoder, inserted the CBAM into the linear embed-\nding and patch merging modules based on the original\nSwin Transformer backbone, and used the UPerNet as a\ndecoder.\n2) A total of 24 high-resolution remote sensing city-scale\nimages were used to train and validate the proposed model.\nDifferent weather disturbances were considered by per-\nforming brightness transformation, uniform fogging, and\nnonuniform fogging to expand the dataset and simulate\npossible light overexposure, darkness, and fog occlusions\nunder actual situations. The results showed that the im-\nproved Swin Transformer achieved higher recognition\naccuracy than the original Swin Transformer, especially\nfor collapsed buildings with highly irregular geometrical\nshapes.\n3) Ablation experiments were performed to demonstrate the\neffectiveness and necessity of the proposed modules in\nthe improved Swin Transformer. The comparison results\nshowed that the full model (i.e., the proposed model with\nfeature fusion and CBAM) obtained the best segmentation\nIoU result of background of collapsed and noncollapsed\nbuildings among all models, which further indicated the\nadvantages of the multilevel feature fusion in alleviating\nfeature leakage and the CBAM in focusing on small dense\nobjects.\n4) The comparison results showed that the improved Swin\nTransformer had distinct superiority over the original Swin\nTransformer and some mature CNN-based segmentation\nmodels, including the PSPNet, DeepLabV3+, and UNet.\nIt indicated that the proposed method could enhance\nthe semantic segmentation accuracy of dense buildings\nin postearthquake remote sensing images owing to the\ncomprehensive extraction capability of local features and\nglobal correlations by organically integrating transformer\nand CNN structures.\nIn future work, the multiscale recognition of seismic disasters\nis supposed to be investigated using multisource data based on\nViTs.\nACKNOWLEDGMENT\nThe authors highly appreciate Prof. Hui Li from Harbin Insti-\ntute of Technology for the insightful comments and suggestions.\nREFERENCES\n[1] Y . Wang, L. Y . Cui, C. Z. Zhang, W. L. Chen, Y . Xu, and Q. Q. Zhang,\n“A two-stage seismic damage assessment method for small, dense, and\nimbalanced buildings in remote sensing images,”Remote Sens., vol. 14,\nno. 4, Feb. 2022, Art. no. 1012.\n[2] C. Schweier and M. Markus, “Classiﬁcation of collapsed buildings for\nfast damage and loss assessment,” Bull. Earthq. Eng., vol. 4, no. 2,\npp. 177–192, Apr. 2006.\n[3] S. V oigt, T. Kemper, T. Riedlinger, R. Kieﬂ, K. Scholte, and H. Mehl,\n“Satellite image analysis for disaster and crisis-management support,”\nIEEE Trans. Geosci. Remote Sens. , vol. 45, no. 6, pp. 1520–1528,\nJun. 2007.\n[4] L. G. Dong and J. Shan, “A comprehensive review of earthquake-induced\nbuilding damage detection with remote sensing techniques,”ISPRS J.\nPhotogramm., vol. 84, pp. 85–99, Oct. 2013.\n[5] B. Adriano, J. Xia, G. Baier, N. Yokoya, and S. Koshimura, “Multi-source\ndata fusion based on ensemble learning for rapid building damage mapping\nduring the 2018 Sulawesi earthquake and tsunami in Palu, Indonesia,”\nRemote Sens., vol. 11, no. 7, pp. 886, Apr. 2019.\n[6] Y . S. Zhou, S. Zhang, X. K. Sun, F. Ma, and F. Zhang,\n“SAR target incremental recognition based on hybrid loss function\nand class-Bias correction,” Appl. Sci. , vol. 12, no. 3, Jan. 2022,\nArt. no. 1279.\n[7] L. L. Li, X. G. Liu, Q. H. Chen, and S. Yang, “Building damage assessment\nfrom PolSAR data using texture parameters of statistical model,”Comput.\nGeosci., vol. 113, pp. 115–126, Apr. 2018.\n[8] X. Wang and P. J. Li, “Extraction of urban building damage using spectral,\nheight and corner information from VHR satellite images and airborne\nLiDAR data,”ISPRS J. Photogramm. Remote Sens., vol. 159, pp. 322–336,\nJan. 2020.\n[9] K. Saito, R. J. S. Spence, C. Going, and M. Markus, “Using high-resolution\nsatellite images for post-earthquake building damage assessment: A study\nfollowing the 26 January 2001 Gujarat earthquake,” Earthq. Spectra,\nvol. 20, no. 1, pp. 145–169, Feb. 2004.\n[10] P. Gamba and F. Casciati, “GIS and image understanding for near-real-time\nearthquake damage assessment,”Photogramm. Eng. Remote Sens., vol. 64,\npp. 987–994, Oct. 1998.\n[11] W. J. Deng, Q. Shi, and J. Li, “Attention-gate-based encoder–decoder\nnetwork for automatical building extraction,”IEEE J. Sel. Topics Appl.\nEarth Observ. Remote Sens., vol. 14, pp. 2611–2620, 2021.\n[12] A. J. Cooner, Y . Shao, and J. B. Campbell, “Detection of urban damage\nusing remote sensing and machine learning algorithms: Revisiting the 2010\nHaiti earthquake,”Remote Sens., vol. 8, no. 10, Oct. 2016.\n[ 1 3 ] H .J .M a ,Y .L .L i u ,Y .H .R e n ,D .C .W a n g ,L .J .Y u ,a n dJ .X .Y u ,\n“Improved CNN classiﬁcation method for groups of buildings damaged\nby earthquake, based on high resolution remote sensing images,”Remote\nSens., vol. 12, no. 2, Jan. 2020.\n[14] M. Ji, L. F. Liu, R. C. Zhang, and M. F. Buchroithner, “Discrimination\nof earthquake-induced building destruction from space using a pretrained\nCNN model,”Appl. Sci., vol. 10, pp. 85–99, Jan. 2020.\n[15] H. Xiao, Y . Peng, H. Tan, and P. Li, “Dynamic cross fusion network for\nbuilding-based damage assessment,” inProc. IEEE Int. Conf. Multimedia\nExpo, 2021, pp. 1–6.\n[16] Y . H. Zhan, W. Liu, and Y . Maruyama, “Damaged building extraction\nusing modiﬁed Mask R-CNN model using post-event aerial images of the\n2016 Kumamoto earthquake,”Remote Sens., vol. 14, no. 4, Feb. 2022,\nArt. no. 1002.\n[17] W. J. Luo, Y . J. Li, R. Urtasun, and R. Zemel, “Understanding the effective\nreceptive ﬁeld in deep convolutional neural networks,”NIPS, vol. 29,\nDec. 2016, pp. 4905–4913.\n[18] W. G. C. Bandara and V . M. Patel, “A transformer-based siamese network\nfor change detection,” inProc. IEEE Int. Geosci. Remote Sens. Symp.,\n2022, pp. 207–210.\n[19] K. Zhang et al., “Practical blind denoising via swin-conv-unet and data\nsynthesis,” 2022,arXiv:2203.13278v2.\n[20] C. Y . Si, W. H. Yu, P. Zhou, Y . C. Zhou, X. C. Wang, and S. C. Yan,\n“Inception transformer,” 2022,arXiv:2205.12956v2.\n[21] R. L. Shao, Z. X. Shi, J. F. Yi, P. Y . Chen, and C. Hsieh, “On the adversarial\nrobustness of vision transformers,” 2021,arXiv:2103.15670v3.\n[22] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner, and A.\nVeit, “Understanding robustness of transformers for image classiﬁcation,”\nin Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 10231–10241.\n[23] S. Paul and P. Y . Chen, “Vision transformers are robust learners,”Proc.\nAAAI, vol. 36, no. 2, pp. 2071–2081, 2022.\n[24] J. Y . Guo et al., “CMT: Convolutional neural networks meet vision\ntransformers,” in Proc. Comput. Vis. Pattern Recognit. Conf., 2021,\npp. 12175–12185.\n[25] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient transformers:\nA survey,” Apr. 2022,arXiv:2009.06732.\n[26] M. Raghu, T. Unterthiner, S. Kornblith, C. Y . Zhang, and A. Dosovitskiy,\n“Do vision transformers see like convolutional neural networks?,”Mach.\nLearn., vol. 34, pp. 12116–12128, Aug. 2021.\n[27] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020,arXiv:2010.11929v2.\n[28] S. X. Zheng et al., “Rethinking semantic segmentation from a sequence-\nto-sequence perspective with transformers,” inProc. Comput. Vis. Pattern\nRecognit. Conf., 2021, pp. 6881–6890.\nCUI et al.: IMPROVED SWIN TRANSFORMER-BASED SEMANTIC SEGMENTATION OF POSTEARTHQUAKE DENSE 385\n[29] T. Y . Lin, Y . X. Wang, X. Y . Liu, and X. P. Qiu, “A survey of transformers,”\nAI Open, vol. 3, pp. 111–132, Jun. 2022.\n[30] Y . Liu et al., “A survey of visual transformers,” 2021,arXiv:2111.06091v3.\n[31] M. H. Guo et al., “Attention mechanisms in computer vision: A survey,”\nComput. Vis. Media, vol. 8, pp. 1–38, Mar. 2022.\n[32] K. Han et al., “A survey on vision transformer,”IEEE Trans. Pattern Anal.\nMach. Intell., to be published, doi:10.1109/TPAMI.2022.3152247.\n[33] Z. Liu et al., “Swin Transformer: Hierarchical vision transformer\nusing shifted windows,” in Proc. Int. Conf. Comput. Vis. , 2021,\npp. 10012–10022.\n[34] Y . F. Da, Z. Y . Ji, and Y . S. Zhou, “Building damage assessment based\non siamese hierarchical transformer framework,”Mathematics, vol. 10,\nno. 11, Jun. 2022, Art. no. 1898.\n[35] H. Chen, E. Nemni, S. Vallecorsa, X. Li, C. Wu, and L. Bromley, “Dual-\ntasks siamese transformer framework for building damage assessment,” in\nProc. IEEE Int. Geosci. Remote Sens. Symp., 2022, pp. 1600–1603.\n[36] I.-H. Lee and M. T. Mahmood, “Robust registration of cloudy satellite\nimages using two-step segmentation,”IEEE Geosci. Remote. Sens. Lett.,\nvol. 12, no. 5, pp. 1121–1125, May 2015.\n[37] M. Heidari et al., “Hiformer: Hierarchical multi-scale representa-\ntions using transformers for medical image segmentation,” 2022,\narXiv:2207.08518.\n[38] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional block\nattention module,” inProc. Eur. Conf. Comput. Vis., 2018, pp. 3–19.\n[39] T. T. Xiao, Y . Liu, B. L. Zhou, Y . N. Jiang, and J. Sun, “Uniﬁed perceptual\nparsing for scene understanding,” inProc. Eur. Conf. Comput. Vis., 2018,\npp. 418–434.\n[40] H. S. Zhao, J. P. Shi, X. J. Qi, X. G. Wang, and J. Y . Jia, “Pyramid\nscene parsing network,” inProc. IEEE/CVF Comput. Vis. Pattern Recognit.\nConf., 2017, pp. 2881–2890.\n[41] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman, “LabelMe:\nA database and web-based tool for image annotation,”Int. J. Comput. Vis.,\nvol. 77, no. 1, pp. 157–173, May 2008.\n[42] K. He, J. Sun, and X. O. Tang, “Single image haze removal using dark\nchannel prior,”IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12,\npp. 2341–2353, Dec. 2011.\n[43] B. L. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n“Scene parsing through ade20k dataset,” inProc. IEEE/CVF Comput. Vis.\nPattern Recognit. Conf., 2017, pp. 5122–5130.\n[44] L.-C. Chen, Y . K. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmenta-\ntion,” inProc. Eur. Conf. Comput. Vis., 2018, pp. 801–818.\n[45] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-\nworks for biomedical image segmentation,”LNIP, vol. 9351, pp. 234–241,\nJan. 2015.\nLiangyi Cui received the B.Sc. degree in civil en-\ngineering from Jilin University, Changchun, China,\nin 2020. He is currently working toward the M.Sc.\ndegree with Lanzhou University of Civil Engineering,\nLanzhou, China.\nHis research interests include computer vision in\nremote sensing.\nXin Jing received the B.Sc. degree in civil engi-\nneering from Lanzhou University, Lanzhou, China,\nin 2021. He is currently working toward the M.Sc.\ndegree with Lanzhou University of Civil Engineering,\nLanzhou, China.\nHis research interests include machine learning in\nseismic damage assessment.\nYu Wang received the B.Sc. degree in civil engi-\nneering from Jilin University, Changchun, China, in\n2016 and the M.Sc. degree in civil engineering from\nLanzhou University, Lanzhou, China, in 2019. He\nis currently working toward the Ph.D. degree with\nLanzhou University of Civil Engineering, Lanzhou,\nChina.\nHis research interests include deep learning and\ncomputer vision in disaster prevention.\nYixuan Huan received the B.Sc. degree in civil engi-\nneering from North China University of Technology,\nBeijing, China, in 2021. He is currently working\ntoward the M.Sc. degree with Lanzhou University of\nCivil Engineering, Lanzhou, China.\nHis research interests include multimodal learning.\nYang Xureceived the B.Sc., M.Sc., and Ph.D. degrees\nin civil engineering and engineering mechanics from\nHarbin Institute of Technology, Harbin, China, in\n2012, 2014, and 2019, respectively.\nHe is currently an Assistant Professor with the\nSchool of Civil Engineering, Harbin Institute of Tech-\nnology, China. His research topic is structural health\ndiagnosis with computer vision and deep learning.\nAs PI, he takes one Youth Project and one subproject\nof Major Program from National Natural Science\nFoundation of China, two subprojects of National Key\nR&D Program, one China National Postdoctoral Program for Innovative Talents,\none general funding of Postdoctoral Science Foundation of China, one open\nfunding of National Key Laboratory of China, one program from Natural Science\nFoundation of Heilongjiang Province, one special funding and one general\nfunding of Heilongjiang Postdoctoral Program, respectively. He has authored\nor coauthored more than 40 papers in international journals and conferences\n(more than 20 SCI indexed, 4 selected as ESI, and Wiley Top Cited Papers) and\nauthorized over 20 national patents and software copyrights.\nDr. Xu is currently a Member of the Early Career Researchers Committee\nof ISHMII, a Member of organizing committee in the international series\ncompetitions for structural health monitoring, the special session chair in the\n8th World Conference on Structural Control and Monitoring, and the special\nissue guest editor of SCI journals.\nQiangqiang Zhang received the B.Sc. and M.Sc.\ndegrees in civil engineering from Harbin Institute of\nTechnology, China, in 2010 and 2012, respectively,\nand the Ph.D. degree in engineering mechanics from\nHarbin Institute of Technology, China, in 2016.\nHe is a “CuiYing Scholar” Professor with The\ncollege of Civil Engineering and Mechanics, Lanzhou\nUniversity, China, the Dean of the Department of\nCivil Engineering, and a Doctoral Supervisor. He\nwas a Visiting Researcher with Purdue University\nof Materials Science, USA. He won the support of\nNational Special Support Plan for High-Level Talents in 2022. He has conducted\nlong-term research on the key scientiﬁc issues of smart structure and intelligent\nsensing techniques. His research work has been published in high-level journals\n(more than 40 SCI index) including Science and advanced materials and reported\nby nature, nature review materials, science, and other journal highlights."
}