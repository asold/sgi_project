{
    "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
    "url": "https://openalex.org/W4386566609",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3006086814",
            "name": "Maximillian Chen",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A233239348",
            "name": "Alexandros Papangelis",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2344273187",
            "name": "Chenyang Tao",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2111513091",
            "name": "Kim Seok Hwan",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5049848313",
            "name": "Andy Rosenbaum",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A1983143503",
            "name": "Yang Liu",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2037128165",
            "name": "Zhou Yu",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2255922594",
            "name": "Dilek Hakkani Tür",
            "affiliations": [
                "Amazon (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2972664115",
        "https://openalex.org/W4300230145",
        "https://openalex.org/W3034600233",
        "https://openalex.org/W2951583236",
        "https://openalex.org/W2964588180",
        "https://openalex.org/W3172759730",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4404783301",
        "https://openalex.org/W2983160116",
        "https://openalex.org/W4296604031",
        "https://openalex.org/W3206345746",
        "https://openalex.org/W3034720580",
        "https://openalex.org/W2951980657",
        "https://openalex.org/W4226286068",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2058738785",
        "https://openalex.org/W4225388974",
        "https://openalex.org/W154079225",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W4283455204",
        "https://openalex.org/W2962989446",
        "https://openalex.org/W3169705981",
        "https://openalex.org/W2761590056",
        "https://openalex.org/W4307473350",
        "https://openalex.org/W4221149883",
        "https://openalex.org/W2148577181",
        "https://openalex.org/W2963686995",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W3167986947",
        "https://openalex.org/W4289433328",
        "https://openalex.org/W3171218751",
        "https://openalex.org/W4285310604",
        "https://openalex.org/W4287817357",
        "https://openalex.org/W2161399113",
        "https://openalex.org/W2971173235",
        "https://openalex.org/W3030155841",
        "https://openalex.org/W3034569646",
        "https://openalex.org/W4320086632",
        "https://openalex.org/W3002330681",
        "https://openalex.org/W4287207937",
        "https://openalex.org/W2997314150",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W2962854379",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4221159394",
        "https://openalex.org/W3099890447",
        "https://openalex.org/W3174382772",
        "https://openalex.org/W4245816780",
        "https://openalex.org/W2904444765",
        "https://openalex.org/W4290771878",
        "https://openalex.org/W4389519535",
        "https://openalex.org/W4292947474"
    ],
    "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
    "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 844–868\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nPLACES: Prompting Language Models for Social Conversation Synthesis\nMaximillian Chen1∗, Alexandros Papangelis2, Chenyang Tao2, Seokhwan Kim2,\nAndy Rosenbaum2, Yang Liu2, Zhou Yu1, Dilek Hakkani-Tur2\n1Columbia University, 2Amazon Alexa AI\nmaxchen@cs.columbia.edu, zy2461@columbia.edu\n{papangea,chenyt,seokhwk,andros,yangliud,hakkanit}@amazon.com\nAbstract\nCollecting high quality conversational data can\nbe very expensive for most applications and\ninfeasible for others due to privacy, ethical,\nor similar concerns. A promising direction\nto tackle this problem is to generate synthetic\ndialogues by prompting large language mod-\nels. In this work, we use a small set of expert-\nwritten conversations as in-context examples\nto synthesize a social conversation dataset us-\ning prompting. We perform several thorough\nevaluations of our synthetic conversations com-\npared to human-collected conversations. This\nincludes various dimensions of conversation\nquality with human evaluation directly on the\nsynthesized conversations, and interactive hu-\nman evaluation of chatbots fine-tuned on the\nsynthetically generated dataset. We addition-\nally demonstrate that this prompting approach\nis generalizable to multi-party conversations,\nproviding potential to create new synthetic data\nfor multi-party tasks. Our synthetic multi-party\nconversations were rated more favorably across\nall measured dimensions compared to conversa-\ntion excerpts sampled from a human-collected\nmulti-party dataset.\n1 Introduction\nTraining dialogue models typically requires an\nabundance of data, as with any machine learning\ntask. However, collecting high quality data is dif-\nficult and expensive, especially for dialogue tasks\nwhere there often is no “right answer” when devel-\noping the trajectory of a conversation. Typically di-\nalogue data are sourced from crowdworkers and the\nquality of annotations, evaluations, and conversa-\ntions can vary considerably (Zhao and Zhu, 2014),\noften necessitating guardrails such as credential-\nbased worker selection or defensive task design for\nquality control (Allahbakhsh et al., 2013).\nTo accommodate data scarcity in training dia-\nlogue tasks, low resource methods have become\n∗Work done during internship at Amazon Alexa AI\nYeah, I grew up there.\nHave you ever been to\nIthaca?\nYeah, I lived there for a long\ntime. Go in the summer:\nswimming in waterfalls!\nIt's gorge-ous. Sorry, had to\ndo it.\nHa.\nI have always meant to visit\nIthaca, I hear it is a pretty\ncool town.\nSpeaker 1\nSpeaker 2\nSpeaker 3\nSpeaker 1\nSpeaker 2\nSpeaker 3\nSpeaker 2 \nThat's cool. I've always\nwanted to visit the UK.\nI'm from a small village in\nthe UK. It's really old and\nhas a lot of history. Speaker 1\nSpeaker 2\nIt's a great place to visit.Speaker 1\nWhat's the best thing about\nyour hometown?Speaker 2\nIs your city a historical place?\nSpeaker 1\nNot really. 200 years ago, it\nwas just a small insignificant\nvillage. Speaker 2\nHow did it grow into such a\nlarge place? Speaker 1\nLarge deposits of coal were\nfound nearly and so many\nindustries located themselves\nhere. The village quickly\ngrew into a key industrial\ncentre. Speaker 2\nWhat's it like?\nSpeaker 2\nIt's a really great place to\nlive. The people are\nfriendly, the food is great, \nand there's lots to do!\nThe best thing is that it's so\nsmall. It's really quiet and\npeaceful. Speaker 1\nTriadic Conversations\nDyadic Conversations\nFigure 1: Pair of dyadic conversation excerpts about\nhometowns (upper) and pair of triadic conversation ex-\ncerpts about Ithaca, NY (lower). In both pairings, one\nconversation is synthetically generated and the other is\ncollected from humans. The answer is in Section 4.\na topic of growing interest and importance (Zhao\net al., 2019; Mi et al., 2019; Qian and Yu, 2019;\nLi et al., 2019). One idea that has gained par-\nticular attention is transfer learning — specifi-\ncally, finding ways to leverage knowledge learned\nby pre-trained large language models (PLMs) for\nnew tasks. PLMs have demonstrated impressive\nemerging conversational capabilities, enabling big\nperformance improvements in various dialogue\ntasks (Brown et al., 2020; Shuster et al., 2022; Peng\net al., 2022; Kulhánek et al., 2021). Particularly,\nPLMs have been prompted to augment existing\nconversational data (Chen et al., 2022; Mehri et al.,\n844\n2022; Sahu et al., 2022).\nGiven some in-distribution seed examples, aug-\nmentation techniques attempt to generate data that\nare faithful to some task distribution (Kim et al.,\n2021b). Albeit powerful, one caveat common to\nall augmentation techniques is that the quality of\nsynthetic data heavily relies on seed examples. But,\nwhat if crowdworkers do not possess the neces-\nsary background or skill set to complete a task en\nmasse? How can we still get adequate high-quality\nsynthetic data to learn a task?\nIn this work, we explore a novel applica-\ntion of Prompting LAnguage models for social\nConvErsation Synthesis (PLACES). Synthesiz-\ning conversational datasets allows for the con-\nstruction of training instances in nonexistent\ntasks. We specifically conduct open-domain, topic-\nconditioned conversation generation using few-shot\nin-context learning with expert-written synthetic\nconversations. We conjecture that expert end-users\nknow exactly the types of conversations that they\nneed. Rather than using existing datasets, they\ncan simply write a small set of high quality con-\nversation examples according to the structure of\ntheir desired conversational outputs. We reason\nthat given structure through high-quality in-context\ndemonstrations, large PLMs are able to utilize their\nexpansive pre-training data (e.g. Gao et al. (2020))\nto synthesize realistic social conversations, implic-\nitly creating personalities and backgrounds for hy-\npothetical speakers. The process of conversation\nwriting would otherwise require human creativity\nand effort.\nOur paper makes four core contributions.\n(1) PLACES involves synthesizing an entire conver-\nsational dataset from a few targeted expert-written\nexamples. These conversations match the quality of\ntwo widely adopted social dialogue datasets, Daily-\nDialog (Li et al., 2017) and Topical Chat (Gopalakr-\nishnan et al., 2019), in terms of human evaluation\nand automatic metrics. (2) We demonstrate that\nour synthetic conversations can be used as a fine-\ntuning dataset which matches the performance of\nits human-curated counterparts as measured by an\ninteractive human evaluation and automatic met-\nrics. (3) We apply PLACES to synthesize data\nfor an under-studied subfield of dialogue research:\nmulti-party conversations. We evaluate a set of\nsynthetic triadic conversations in comparison to\ntwo human-collected multi-party conversational\ndatasets (Shaikh et al., 2010; Poria et al., 2019).\nTo our knowledge, our work is the first to syn-\nthesize multi-party conversations, adding to the\nstill-growing body of work on multi-party social\ndialogue. (4) Lastly, we conduct an error analysis\non both dyadic and triadic synthetic conversations.\nWe discuss the implications of our findings, as well\nas potential solutions to address the generation “er-\nrors.”\n2 Related Work\nRecently, the zero- and few-shot learning capabil-\nities of large pre-trained language models have\novertaken state-of-the-art performance on many\nclassical natural language processing tasks, includ-\ning dialogue (Brown et al., 2020). Many PLMs\nsuch as T5 (Raffel et al., 2020), GPT-J (Wang and\nKomatsuzaki, 2021), GPT-3 (Brown et al., 2020),\nand OPT (Zhang et al., 2022) have become the\nbackbone of several dialogue-specific models (e.g.,\nPeng et al. (2022); Madotto et al. (2021); Shuster\net al. (2022)).\nIn particular, in-context learning, where few-shot\nexamples are provided in the input prompt of a\nPLM, has been found to provide valuable infor-\nmation in guiding generation output (Min et al.,\n2022; Brown et al., 2020; Min et al., 2021; Lu\net al., 2021b). As a result, many recent efforts in\nprompting PLMs have sought to augment various\nnatural language processing datasets (Chen et al.,\n2022; Wang et al., 2022; Sahu et al., 2022; Mehri\net al., 2022; Rosenbaum et al., 2022a). Prompting\nhas become a viable “solution” for augmentation\nin dialogue tasks, which have traditionally been\nconsidered challenging due to the difficulty of aug-\nmenting dialogue context (Chen et al., 2022).\nHowever, prompt-based augmentation strate-\ngies are uncontrolled forms of generation, which\nmay result in generation mistakes for labeled\ndatasets (Sahu et al., 2022; Chen et al., 2022; Meng\net al., 2022). In contrast, other recent studies have\ninstead proposed language augmentation strategies\nthat use complex, highly-controlled frameworks\nthat often involve fine-tuning generators (Papange-\nlis et al., 2021; Zhang et al., 2020b; Kulhánek et al.,\n2021; Zhang et al., 2020a). Such complex augmen-\ntation frameworks require larger amounts of seed\ndata to maintain a ground-truth language distribu-\ntion (Rosenbaum et al., 2022b; Kim et al., 2021b),\nand are more costly than prompting PLMs (Chen\net al., 2022). However, in the context of dataset\nsynthesis, seed data and label correctness are less\n845\nRandom In-Context Examples\nTarget \nRecipe\n<Conversation 0> \nThe following is a conversation between Alice and\nBob about relationships. Bob recently got engaged.\nAlice: Congrats on your engagement! When do you\nthink you will have your wedding?\nBob: Thank you!! We're thinking of having it in\nNovember.\nAlice: That's amazing! Will you pick a fancy\ndestination?\n...\n<Conversation 2>\n<Conversation 3> \nThe following is a conversation between Alice and\nBob about pets. Alice likes cats but Bob likes dogs.\nResult: \nAlice: What kind of pet do you have? \nBob: I have a dog. Her name is Hope. \nAlice: Are you an animal person? \nBob: I do love animals. I think they're so\nspecial. \nAlice: I just adopted a kitten from a shelter.\nHer name is Mimi. \nBob: Aww, I hope she's doing well. You must\nbe a really good pet parent.\nAlice: Thanks! I just wanted to do what I\ncould to make sure she was safe. \nBob: That's the greatest thing anyone can do\nfor a pet. Well, I should probably go now. \nAlice: OK! Have a good night.\nTopic: Relationships  \nBackground info: Bob got engaged\nTopic: Pets  \nBackground info: Alice likes cats. Bob like dogs.\nRecipe\nFigure 2: Example of the components of a prompt (left) used by OPT 30B to generate a synthetic conversation\nabout pets (right). Conversations in the prompt are prefixed by recipes. Blue text: topic labels. Red text: seed\nbackground information metadata.\nimportant considerations. There is no task distribu-\ntion from which seed data is drawn that PLMs must\nremain faithful to, and similarly, invariant ground-\ntruth knowledge for language models is dependent\non the desired task being synthesized.\nOur work differs from existing applications of\nprompting for conversations along several dimen-\nsions. Many studies examine utterance-level gen-\neration (Chen et al., 2022; Sahu et al., 2022; Aher\net al., 2022; Rosenbaum et al., 2022b), whereas\nour work concerns the synthesis of full conversa-\ntions. Bae et al. (2022) generated conversations for\na narrow task and provided evaluations between\ntheir synthesis conditions. Recent concurrent work\nby Kim et al. (2022) sought to distill conversa-\ntions from InstructGPT 175B using a common-\nsense knowledge graph. In our work, we synthe-\nsize conversations using an open-source PLM and\ndemonstrate that they are comparable to human-\ncollected datasets, in terms of both conversation\nquality and usability as a dataset. Moreover, all of\nthese studies only concern dyadic conversations,\nbecause the vast majority of conversational tasks\nare dyadic. Our work is the first study to synthesize\nmulti-party conversations.\n3 Conversation Generation\nIn this section, we discuss our methods for conver-\nsation generation. We first detail the construction\nof our example conversations, then describe their\napplication to prompting PLMs.\n3.1 Writing Conversation Examples\nWe simply wrote a pool of ten conversations be-\ntween two speakers representing everyday dialogue\nusing proper grammar. Along with each conversa-\ntion, we wrote a brief conversation “recipe” which\nincludes a topic, as well asbackground information\nfor the two speakers1.\nThe background information represents some\nmore fine-grained information about the two speak-\ners, relevant to that particular topic. For example,\nFigure 2 depicts an example prompt with three\nin-context conversation demonstrations. Each con-\nversation is prefixed by a recipe and is structured in\nthe same manner: “The following is a conversation\nbetween Alice and Bob about topic” (e.g., “pets”)\nfollowed by detailed background information (e.g.,\n“Alice love cats. Bob is more of a dog person.”).\n3.2 Creating Conversations via Prompting\nEach prompt consists of three randomly sampled\nconversations from the aforementioned pool, along\nwith their accompanying recipe. After experiment-\ning with PLMs of three different sizes (GPT-J\n6B, GPT-NeoX 20B, OPT 30B), we primarily use\nOPT-30B and generate with nucleus sampling with\np= 0.92. Inspired by the format of DailyDialog,\nour handwritten and synthetically generated con-\nversations fall into three categories: start-to-finish\nconversations, excerpts from the start to the middle\n1The first-author spent approximately 45 minutes on this\nwriting process.\n846\nSource Words/Turn Turns/Conv.\nDailyDialog 11.58 7.84\nTopical Chat 13.38 21.83\nHW Examples 11.00 8.10\nSynthetic 10.70 9.29\nTable 1: Number of words per turn and number of turns\nper conversation for all conversations. HW Examples\nrepresents the ten handwritten conversation examples,\nand Synthetic represents synthetic conversations gener-\nated using OPT 30B.\nof a conversation, and excerpts from the middle of\na conversation. Several examples are given in the\nAppendix.\nIn this paper, we generate a dataset using a list of\ntopics and tasks (i.e., subtopics) from the training\nset of the Feedback for Interactive Talk & Search\nDataset (FITS; Xu et al. (2022)), a human-chatbot\ndataset designed to determine desirable human-\nchatbot tasks/conversations. FITS contains 5592\nconversations which span 52 conversational topics\n(e.g., “nutrition,” “philosophy”) with 315 subtopics\n(e.g., “Italian food,” “Soren Kierkegaard”). We\nwrote background information for each of the 315\nsubtopics in the form given in Figure 2.\nUsing the product of this process once results\nin a new synthetic dataset with 5592 conversations\nusing the same topic, subtopic pairings from FITS.\nThe average length of each conversation is 9.29\nturns, with 12.84 words per turn. This is compa-\nrable to the dataset statistics of DailyDialog and\nTopical Chat, as per Table 1. In the Appendix, we\nhave included the 315 prompt headers (Tables S22,\nS23) and the pool of in-context examples (Tables\nS24, S25, S26).\n4 Synthetic Conversation Evaluation\nIn Figure 1, the top-left is taken from DailyDia-\nlog, whereas the top-right is generated synthetically.\nThe bottom-left is generated synthetically and the\nbottom-right is taken from MPC.\n4.1 Evaluation of Conversation Quality\nTable 2 provides a crowdworker evaluation of our\nsynthetic dataset compared against DailyDialog\nand Topical Chat. We expect Topical Chat to be\nrated as the most interesting, due to the knowledge-\ngrounding process utilized during the dialogue col-\nlection process. We randomly sampled 200 conver-\nsations for each conversation source and asked a\npre-qualified pool of 28 crowdworkers on Amazon\nMechanical Turk (AMT) to rate each conversation.\nSource Interesting Coherent Natural Consistent\nDailyDialog 3.44 4.51 4.85 4.57\nTopical Chat 4.55 4.39 4.92 4.87\nGPT-J 6B 3.96 ∗ 4.49 4.86 4.36\nGPT-NeoX 20B 3.81 ∗ 4.40 4.63 4.35\nOPT 30B 4.13 ∗ 4.61∗† 4.82 4.63\nTable 2: Evaluation of conversations randomly sampled\nfrom DailyDialog, Topical Chat, and three synthetic\ndatasets generated by prompting GPT-J 6B, GPT-NeoX\n20B, and OPT 30B. ∗indicates statistical significance\nover DailyDialog. † indicates statistical significance\nover Topical Chat. Significance computed at α= 0.05.\nThe instructions and details of our human evalua-\ntion setup are explained in Appendix A.\nAs these conversations are generated using\nprompting, we first checked whether each conversa-\ntion followed the prescribed prompt. Crowdwork-\ners identified 95% of the conversations generated\nby OPT 30B as matching the topic stated in the\nprompt2, indicating this prompting strategy’s ef-\nfectiveness for topic-grounded conversation gen-\neration. Overall, Table 2 indicates that synthetic\nconversations generated by OPT 30B are rated as\nthe most coherent, and more interesting and consis-\ntent than DailyDialog. The synthetic conversations\nare almost as natural as DailyDialog, but are rated\nas less interesting and natural than Topical Chat.\nGiven our results, we also hypothesize that larger\nmodels likely produce higher quality conversations.\nWe provide several examples of conversations gen-\nerated by OPT 175B using an online web interface3\nin the Appendix.\nA concern one might have is that since in-context\nexamples heavily influence prompting (Min et al.,\n2022; Lu et al., 2021b), our small in-context ex-\nample size may limit the lexical diversity of our\nsynthetic conversations. Following earlier work\nevaluating text generation, we use Distinct-N to\nmeasure lexical diversity (Wu et al., 2021; Li et al.,\n2016). Figure 3 shows that our synthetically gen-\nerated conversations are slightly more diverse than\nboth DailyDialog and Topical Chat in terms of dis-\ntinct bigrams and trigrams, and slightly less diverse\nthan Topical Chat in terms of 4-grams.\nWe then sought to examine the impact of using\nexpert handwritten examples by comparing against\nsynthetic conversations generated using conversa-\ntions from DailyDialog and Topical Chat as in-\n291% and 92% for GPT-J 6B and GPT-NeoX 20B.\n3https://opt.alpa.ai/\n847\nDimension DD-IC TC-IC HW-IC\nInteresting 3.82 4.35 4.27∗\nCoherent 4.48 4.56 4.77∗+\nNatural 4.54 4.69 4.69∗\nConsistent 4.76 4.87 4.86∗\nOn-Topic 0.91 0.88 0.96∗+\nTable 3: Human evaluation of conversations generated\nusing OPT-30B with in-context examples randomly sam-\npled from DailyDialog (DD-IC), Topical Chat (TC-IC),\nand handwritten examples (HW-IC). ∗indicates statisti-\ncal significance over DD-IC and + indicates statistical\nsignificance over TC-IC.\ncontext examples. We set the number of conversa-\ntion examples such that the number of in-context\ndialogue turns are approximately equal across all\nconditions. Table 3 shows that synthetic conver-\nsations generated conditioned on handwritten in-\ncontext examples are the most coherent, natural,\nand on-topic. In terms of interestingness and con-\nsistency, the ratings of these conversations slightly\ntrail the ratings of the conversations generated con-\nditioned on Topical Chat.\n4.2 Fine-Tuning with Synthetic Conversations\nAfter establishing that our synthetic conversations\nare of rather high quality on their own, we at-\ntempted to use the synthetic dataset as training\ndata for dialogue models. We fine-tuned distilled\nBlenderBot 400M (Roller et al., 2021) on DailyDia-\nlog, Topical Chat, and our synthetic conversations4.\nRather than directly prompting OPT as a re-\nsponse generator, we select BlenderBot as a\nlightweight, effective dialogue model. This allows\nfor comparisons between the three data sources\nas training sets, because fine-tuning OPT is pro-\nhibitively expensive. Moreover, while prompting\nwith larger PLMs can yield coherent responses, it\nis generally impractical as an end-to-end dialogue\nsystem if hosted on typically available hardware.\nFor long inputs (e.g. with multiple dialogues in-\ncontext), generation time typically takes several\nminutes using OPT 30B5.\nWe first performed an interactive human evalu-\nation of the three dialogue models as end-to-end\nsocial chatbots using the LegoEval platform (Li\net al., 2021). Details can be found in Appendix A.\nTable 4 shows that dialogue models fine-tuned\non our synthetic conversations are rated compara-\n4For fair comparison, we fine-tune on the same numebr of\ntraining instances via downsampling.\n5All experiments are conducted using one p3dn.24xlarge\nAWS EC2 instance.\nDimension DD TC Syn\nInteresting 3.35 3.86 3.30\nCoherent 3.52 3.71 3.68\nNatural 3.52 3.57 3.68\nConsistent 3.35 3.65 3.32\nEngaging 3.73 3.88 3.65\nIntelligent 3.41 3.55 3.24\nNon-repetitive 3.37 3.37 3.40\nTable 4: Interactive human evaluation yields comparable\nratings for chatbots fine-tuned on conversations from\nDailyDialog (DD), Topical Chat (TC), and our Synthetic\nData (Syn).\n2-grams 3-grams 4-grams0.0\n0.2\n0.4\n0.6\n0.8Distinct-N\nDailyDialog\nT opical Chat\nSynthetic\nFigure 3: Distinct-N with N = 2,3,4 for conversations\nin DailyDialog, Topical Chat, and our synthetic conver-\nsations. Our synthetic conversations have the highest\nmost unique bi-grams and tri-grams, and the second-\nmost unique 4-grams.\nbly to dialogue models fine-tuned on real human-\nhuman data — the chatbot fine-tuned on synthetic\ndata appeared to be the most natural and non-\nrepetitive, and was rated as the second-most coher-\nent. It was rated as the least intelligent, engaging,\nconsistent, and interesting. However, two-sided\nt-tests at α= 0.05 revealed that there was not a sta-\ntistically significant difference in ratings between\nthe models fine-tuned on all three datasets across\nall dimensions except for interestingness. The Top-\nical Chat model was rated as significantly more\ninteresting, as expected.\nIn terms of automatic evaluation, we applied\nthese dialogue models on out-of-distribution test\nsets to prevent an unfair comparison. We evalu-\nated models fine-tuned on DailyDialog and our syn-\nthetic data on Topical Chat, and models fine-tuned\non Topical Chat and our synthetic data on DailyDi-\nalog. Table 5 indicates that in terms of perplexity\nand ROUGE, models fine-tuned on our synthetic\ndata generalize to out-of-distribution convesational\ndata as well as models trained on real human-\n848\nMetric (Test Set) DD-BB TC-BB Syn-BB\nPerplexity (DD) — 120.2 87.05\nROUGE-1 (DD) — 12.34 12.90\nROUGE-2 (DD) — 1.66 1.52\nROUGE-L (DD) — 10.60 10.94\nPerplexity (TC) 43.3 — 37.1\nROUGE-1 (TC) 16.63 — 15.13\nROUGE-2 (TC) 2.36 — 1.77\nROUGE-L (TC) 13.61 — 12.41\nTable 5: Out-of-distribution automatic evaluation of\nperplexity and ROUGE is comparable for BlenderBot\nfine-tuned on DailyDialog (DD-BB), Topical Chat (TC-\nBB), and synthetic data generated using our handwritten\nexamples in-context (Syn-BB), respectively.\nhuman datasets. On the DailyDialog test set, the\nsynthetic dataset model outperforms the Topical\nChat model on all metrics except ROUGE-2, and\non the Topical Chat test set, the synthetic dataset\nmodel underperforms the DailyDialog model on all\nmetrics except perplexity.\n5 Triadic and Multi-Party Conversations\nThe vast majority of dialogue tasks and conver-\nsational datasets focus on dyadic conversations\n(e.g. Li et al. (2017); Gopalakrishnan et al. (2019);\nSmith et al. (2020); Rashkin et al. (2019)), follow-\ning the traditional speaker-listener paradigm (En-\ngelhardt et al., 2006). In contrast, the literature\non multi-party social conversation is rather scarce,\nnot only in terms of conversation generation but\nas a task altogether. However, while it is an un-\nderstudied research area, it is incredibly important,\nbecause dyadic conversations do not capture the\nfull reality of in-person, human-human social con-\nversations, nor the full potential of dialogue agents.\nTo name a few applications, dialogue agents have\nthe potential to supplement classroom learning with\nmultiple parties, serving as a third mediating party\nin a debate or discussion between two people, or\nto provide companionship and support in virtual\ngroup settings. A major reason why these lines of\nwork remain unsolved is that there are few large-\nscale multi-party dialogue datasets.\nMany existing multi-party datasets are scripted\ncorpora such as MELD (Poria et al., 2019) or\nMPDD (Chen et al., 2020) or HLA-Chat (Ju et al.,\n2022; Li et al., 2020). Other multi-party corpora\nare collected for highly domain-specific purposes,\nsuch as multi-party empathetic dialogue (Zhu et al.,\n2022). Such corpora are also typically collected\nthrough asynchronous online platforms, rather than\nnatural conversation. These platforms exist in the\n2-grams 3-grams 4-grams0.0\n0.2\n0.4\n0.6\n0.8Distinct-N\nSpeaker 1\nSpeaker 2\nSpeaker 3\nFigure 4: Linguistic diveristy (Distinct-N) is compara-\nble for each speaker in the synthetic triadic conversation\ndataset.\nform of forums and online chat platforms such as\nUbuntu IRC (Lowe et al., 2015) or Reddit (Baum-\ngartner et al., 2020). Other more natural multi-\nparty conversational datasets are license-protected\nspeech datasets (e.g. CHIME (Christensen et al.,\n2010)) which have been constructed for tasks such\nas speaker attribution.\nWe find that we can apply our prompting ap-\nproach to generate synthetic, open-domain, multi-\nparty social conversations following the same struc-\nture as our synthetic dyadic conversations 6. As\nin the dyadic case, we generate triadic conversa-\ntions using optional background information for\neach speaker. We consider the “Multi-Party Chat”\ncorpus (MPC) (Shaikh et al., 2010), a text-based,\nopen-domain conversation dataset collected in real-\ntime online sessions at the University of Albany,\nand MELD, which contains scripted multi-party\ndialogues from the popular sitcom “Friends.” We\ndirectly compare our synthetically generated con-\nversations against MPC and MELD.\nTable 6 includes our evaluation of our conver-\nsations using the same pool of pre-qualified AMT\nworkers, again with 200 randomly sampled con-\nversations. MPC consists of massive conversation\nsettings — on the scale of 500 turns for a typical\nconversation session — so we randomly sample 8\nto 127 continuous turns for each conversation eval-\nuation to more closely match the structure of our\nsynthetic conversations.8 We present examples of\n6While we effectively use Alice, Bob, and Claire instead\nof Speaker 1, Speaker 2, and Speaker 3, respectively, the order\nof speakers does not necessarily follow the speaker order in\nthe in-context examples (e.g. Appendix Table S10).\n7The length between 8 and 12 turns is chosen uniformly.\n8We sample rather than selecting the first 8-12 turns, to\n849\nDimension MPC MELD Syn\nInteresting 2.48 3.52 4.14∗\nCoherent 2.40 3.68 4.65∗\nNatural 2.69 3.69 4.47∗\nConsistent 2.96 3.83 4.65∗\nComprehensible 2.48 3.83 4.80∗\nBalanced Engagement 3.45 4.00 4.89∗\nTable 6: Synthetic conversations generated using OPT\n30B are rated significantly higher than MPC and MELD\nacross all dimensions.\nMPC and MELD in Appendix Tables S20, S21.\nWe inform the AMT workers that they will read\nconversation excerpts. In addition to the questions\nin Table 2, we add two questions specific to multi-\nparty conversations. We ask if the conversation ex-\ncerpt looks comprehensible (in terms of the reader\nbeing able to determine who each speaker is ad-\ndressing), and we ask if all parties of the conversa-\ntion are participating equally and actively.\nIn Table 6, we find that the synthetic conversa-\ntions are rated statistically significantly more favor-\nably than MPC and MELD across all dimensions.\nBeyond conversation quality, it is possible that the\nratings for MPC are comparatively low due to the\nfact that each conversation typically has more than\nthree speakers, which may be more difficult for\nhuman raters to interpret. Our results for MELD\nalso indicate that while the corpus is high quality,\nit may be better fit for comedy and accompaniment\nwith visual context, than as pure dialogue.\nAdditionally, we checked the linguistic diver-\nsity for each speaker. In terms of Distinct-N, each\nspeaker’s lexical diversity is comparable (Figure 4)\nas well as the number of words per turn (12.2, 12.2,\nand 13.5 for Speakers 1, 2, and 3 respectively).\nThe triadic conversations tended to be slightly\nlonger than the average dyadic conversation (11.5\nturns/conversation versus 9.29 turns/conversation).\n6 Discussion\nOverall, we find that prompting PLMs to generate\nsynthetic conversations is promising.\n6.1 Considerations for Dyadic Dialogue\nThe synthetically generated conversations appear\ncomparable to conversations from human-collected\ndatasets. The individual conversations appear in-\nteresting, coherent, natural, and consistent, as the\naverage ratings for each category lie between 4.0\nand 5.0. The Appendix includes multiple examples\navoid overrepresenting greetings.\nof conversations generated using the strongest per-\nforming PLM (OPT 30B, e.g. Table S7) as well as\nseveral conversations generated using OPT 175B\n(e.g. Table S8). Tables 4 and 5 also indicate that\nfine-tuning on synthetically generated examples\ncan result in dialogue models of comparable qual-\nity, with the potential for further improvements by\nsimply generating more synthetic conversations.\nFuture work may consider applying applying\nthis generation approach to dyadic contexts be-\nyond social conversations, such as task-oriented\ndialogue. The clearest difference between social\nand task-oriented dialogue contexts is the impor-\ntance of knowledge grounding. In task-oriented\ndialogue, there typically needs to be retrieval from\nknowledge base for response generation. An ap-\nplication of PLACES could involve using database\nresults as a ground-truth reference. Rather than\nusing a topic list like FITS, one could form con-\nversational recipes using database search results as\nbackground information. Given the apparent se-\nmantic control described in Section 4, it is possible\nthat synthetic task-oriented conversations would be\nable to correctly utilize knowledge.\n6.2 Considerations for Multi-Party Dialogue\nWe found that in comparison to MPC, our synthetic\ntriadic dialogues appear to be of fairly high qual-\nity. However, there remain several open questions\nabout multi-party dialogue, even in the triadic case.\nFor instance, there is not a set archetype of conver-\nsations. Sometimes, conversations may be domi-\nnated by a single speaker, whereas in others, each\nspeaker in the conversation may contribute equally.\nDepending on the scenario, a speaker may be the\nfacilitator — meetings can be considered (topic-\nspecific) multi-party dialogues which are typically\nled by designated speakers.\nMoreover, there are several questions about how\nto utilize multi-party dialogues in an interactive di-\nalogue system. There are use cases where it may be\nappropriate for one dialogue system to interact with\nmultiple users. On the other hand, in scenarios like\nemotional support dialogue systems, it may make\nsense for a single user to interact with multiple\nsimulated conversational parties.\nHere, we investigated our approach’s potential to\ngenerate synthetic multi-party conversations, hop-\ning to bridge the gap in data availability in multi-\nparty chat. This opens opportunities for a variety\nof applications. Synthetic datasets could be used\n850\nto help discover how to properly model triadic and\nmulti-party conversations. In the future, datasets\ncould also be generated for domain-specific, multi-\nparty applications ranging from language learning\nto task-oriented spoken dialogue systems.\n7 Error Analysis\nWe examine the dyadic and triadic conversations\nwhich received low scores (1/5) across multiple\ndimensions.\n7.1 Dyadic Conversations\nOut of the dyadic conversations, two conversations\nwere rated as generic and dull. One conversation\n(Appendix Table S13) talks about the singer, Taylor\nSwift. However, the conversation is repetitive, re-\npeating utterances such as “What are your thoughts\non her?” and “I think she is very nice.” The other\nconversation is about the filmmaker, Ken Burns\n(Appendix Table S14). While the conversation is\nappears coherent and uses correct factual informa-\ntion (e.g., making reference to Ken Burns’ docu-\nmentaries on World War II and the Vietnam War),\nthe language could be perceived as dull.\nThree conversations were rated as completely un-\nnatural. In one case, the PLM missed the prescribed\nsubtopic (cotton candy) and instead hallucinated\na conversation about a sensitive topic, cancer (Ap-\npendix Table S15). This is also the only conversa-\ntion to be rated as completely incoherent. The other\ntwo conversations are both on-topic. However, one\nconversation is on-topic but rather short (five turns),\nwhereas the other conversation is overly verbose\nand a little repetitive.\nThere were also three conversations were eval-\nuated as completely inconsistent. In all three con-\nversations, the roles of the two speakers seemingly\nswap. While these hypothetical turns are possi-\nble in excerpts of real conversations, they assume\nbackground information or events which have not\nbeen explicitly established when considered as stan-\ndalone conversations. An example is given in Ap-\npendix Table S16.\nWhile some of the evaluations may be subjec-\ntive, an issue that has objectively appeared multiple\ntimes is the consistency of speakers’ utterances.\nThe intents and personas of the speakers appear\nto get switched, which is also an open problem\nin dialogue systems research. Future work may\nlook to combine conversation synthesis approaches\nwith strategies for dialogue consistency such as\nthe generate-delete-rewrite framework (Song et al.,\n2020a) or language inference approaches (Welleck\net al., 2019; Song et al., 2020b).\n7.2 Triadic Conversations\nNo conversations were perceived as completely in-\ncomprehensible, but human evaluators indicated\nthat two conversations appeared to have imbal-\nanced engagement — in both cases, the third\nspeaker (“Claire”) only has one dialogue turn. As\ndiscussed in Section 6.2, however, it is not clear\nwhether this is a drawback. Real-life triadic con-\nversations do not follow a set archetype in terms of\nengagement balance.\nThere was one conversation which was rated as\ncompletely incoherent. In the conversation, there\nis one dialogue turn which presents information\ninconsistent with prior turns, but the another is-\nsue appears to be an oddly placed transition which\nbrings the conversation from travel to hobbies:\n“You should definitely go to Paris! What do you\nlike to do for fun?” (Appendix Table S17).\nThere are two conversations which were per-\nceived as completely unnatural. However, natural-\nness appears to be a rather subjective evaluation.\nOne conversation is given in Appendix Table S18,\nand it is debatable whether the language conven-\ntions used are unnatural. One could argue that it is\noverly enthusiastic, but others could argue that it is\nhow some people speak colloquially. Interestingly,\nthe second conversation which received a low nat-\nuralness score is also enthusiastic and about the\nsame topic (gardening).\nThe only conversation which was rated as\ngeneric and dull was a 15-turn debate about\nwhether the European Union is a “conspiracy” (Ap-\npendix Table S19). The debate is rather shallow\nand does not make a lot of progress.\nAs with the dyadic conversation error analysis,\nwe see that there are issues with persona consis-\ntency. However, unlike the dyadic scenario, there\nare fewer existing solutions for dialogue consis-\ntency. Multi-party conversation synthesis could\npotentially be improved by applying ideas from\nthe newly published PersonaTKG dialogue system,\nwhich employs a unified graph that encodes per-\nsonas, utterances, and external knowledge on a\nscripted dialogue dataset (Ju et al., 2022).\nBeyond consistency, in the example from Ta-\nble S19 we see that there is potential for PLMs to\nhallucinate misinformation. There are again fewer\nexisting studies on circumventing this obstacle in\nmulti-party dialogue, but future work could look\n851\nto incorporating external knowledge (Kang et al.,\n2022) or dialogue safety approaches (Kim et al.,\n2021a; Dinan et al., 2019). All said, our work\nmotivates further study into multi-party dialogue\nconsistency, safety, and synthesis.\n8 Conclusion\nIn this work, we presented an application of prompt-\ning PLMs to create synthetic conversations. These\nsynthetic conversations are comparable in terms\nof quality and lexical diversity to actual human-\nhuman datasets, and can be used as training data for\ndialogue models. This opens avenues in generative\nlanguage work such as collaborative and creative\nwriting, story generation, as well as synthesis of\nnew conversational tasks. Here, we presented one\nexample — synthesizing a multi-party conversa-\ntional dataset. This presents a unique opportunity\nto further study multi-party dialogue modeling.\n9 Limitations\nControllability. We witness encouraging levels\nof control through the prompt (95% of the time, the\nsynthetic conversation matches the desired topic),\nbut prompting PLMs is still an uncontrolled form\nof generation. Future work could seek to add more\nsemantic controls beyond the stated topic in the\nprompt or explore using weak supervision to pro-\nvide post-hoc improvements on synthetic data qual-\nity, similar to Chen et al. (2022). In this work, we\nalso did not thoroughly explore the effects of differ-\nent generation approaches. Future work may con-\nsider applying semantic constraints during the de-\ncoding process (Lu et al., 2021a). Further controls\nare necessary before using this approach for higher-\nstakes settings such as task-oriented dialogue and\nother knowledge-grounded tasks.\nCost of Human Effort. While we demonstrate\nthe ability to synthesize large amounts of data, the\nquality of a synthesized dataset is still dependent on\nhuman effort, to an extent. One can use a generic\nprompt template such as “Alice is interested in\n[subtopic]” for each subtopic, but we qualitatively\nsee that more detailed background information in a\nprompt often yields better generation performance.\nIn this work, we generated 5592 dyadic and tri-\nadic conversations, matching the number of topic\ncombinations in FITS. PLACES can be used to\ngenerate many more conversations in the future.\nUsing the same overall can continue to make new\ncombinations of topic and subtopic, or simply re-\nrun the generation process as it is nondeterministic.\nMoreover, one may consider filling the slots in our\nconversation recipes using an abundant of external\nsources, including from existing dataset annota-\ntions (e.g. Persona Chat Zhang et al. (2018)).\nComputational Costs. Once a dataset is syn-\nthesized, small, task-specific models can be used\ndownstream. However, the synthesis method used\nin this work is still expensive: we prompt PLMs.\nWhile we only used freely accessible PLMs such as\nOPT, we acknowledge that not everyone has access\nto the number of GPUs necessary to load PLMs,\neven for inference.\nPrompt Design. The idea of prompting large lan-\nguage models is not novel. There is a plethora of\nwork that examines how to apply prompting to a\nvariety of different tasks (e.g. Brown et al. (2020);\nMin et al. (2021)), along with several studies on\nhow to mine or engineer different prompts (Liu\net al., 2021). In this work, we do not claim novelty\nto our prompt, nor do we claim that our prompt\ndesign is the optimal prompt for conversation gen-\neration. Our prompt is designed in a conversa-\ntional manner, drawing inspiration from Chen et al.\n(2022). We instead emphasize the application\nof prompting for conversational dataset synthesis.\nThe idea of synthesizing conversational datasets\n“from scratch” is previously unexplored, and has\npotential to supplement a lot of areas of dialogue\nresearch, such as multi-party conversations.\n10 Ethical Considerations\nHuman Evaluation and Crowdsourcing. We\nmake use of crowdsourcing through Amazon Me-\nchanical Turk for several experiments. All crowd-\nworkers were paid at a rate higher than the mini-\nmum wage in California. In accordance with Cal-\nifornia State Law, all crowdworkers were also in-\nformed they were speaking with chatbots during\nthe data collection for our interactive evaluation.\nAll participants consented to the logging of their\nresponses.\nLanguage Model Biases. Large pre-trained lan-\nguage models are typically pre-trained on massive\ncorpora crawled from the internet such as The\nPile (Gao et al., 2020) or Common Crawl. This\nallows language models to have exposure to a large\namount of linguistic diversity, but this also results\nin exposure to a lot of hateful, biased, or otherwise\n852\nundesirable content from the internet (Luccioni and\nViviano, 2021). Future work should examine com-\nbining conversation synthesis with dialogue safety\napproaches.\nScientific Artifacts. All scientific artifacts are\nused according to their intended purpose. The FITS\ndataset is publicly available at https://parl.ai/\nprojects/fits/. OPT is an open-source language\nmodel. GPT-J is available for use under the MIT\nlicense. We use the HuggingFace Transformers\nand PyTorch packages for all modeling (Wolf et al.,\n2020; Paszke et al., 2019). All artifacts used are in\nEnglish.\nReferences\nGati Aher, Rosa I Arriaga, and Adam Tauman Kalai.\n2022. Using large language models to simulate mul-\ntiple humans. arXiv preprint arXiv:2208.10264.\nMohammad Allahbakhsh, Boualem Benatallah, Alek-\nsandar Ignjatovic, Hamid Reza Motahari-Nezhad,\nElisa Bertino, and Schahram Dustdar. 2013. Quality\ncontrol in crowdsourcing systems: Issues and direc-\ntions. IEEE Internet Computing, 17(2):76–81.\nSanghwan Bae, Donghyun Kwak, Sungdong Kim,\nDonghoon Ham, Soyoung Kang, Sang-Woo Lee,\nand Woomyoung Park. 2022. Building a role speci-\nfied open-domain dialogue system leveraging large-\nscale language models. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2128–2150, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the inter-\nnational AAAI conference on web and social media,\nvolume 14, pages 830–839.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMaximillian Chen, Alexandros Papangelis, Chenyang\nTao, Andy Rosenbaum, Seokhwan Kim, Yang Liu,\nZhou Yu, and Dilek Hakkani-Tur. 2022. Weakly\nsupervised data augmentation through prompting for\ndialogue understanding. NeurIPS 2022 Workshop on\nSynthetic Data for Empowering ML Research.\nYi-Ting Chen, Hen-Hsen Huang, and Hsin-Hsi Chen.\n2020. Mpdd: A multi-party dialogue dataset for\nanalysis of emotions and interpersonal relationships.\nIn Proceedings of the 12th Language Resources and\nEvaluation Conference, pages 610–614.\nHeidi Christensen, Jon Barker, Ning Ma, and Phil D\nGreen. 2010. The chime corpus: a resource and a\nchallenge for computational hearing in multisource\nenvironments. In Eleventh Annual Conference of\nthe International Speech Communication Association.\nCiteseer.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019. Build it break it fix it for\ndialogue safety: Robustness from adversarial human\nattack. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4537–4546, Hong Kong, China. Association for Com-\nputational Linguistics.\nPaul E Engelhardt, Karl GD Bailey, and Fernanda Fer-\nreira. 2006. Do speakers and listeners observe the\ngricean maxim of quantity? Journal of memory and\nlanguage, 54(4):554–573.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinglang\nChen, Anna Gottardi, Sanjeev Kwatra, Anu\nVenkatesh, Raefer Gabriel, Dilek Hakkani-Tür, and\nAmazon Alexa AI. 2019. Topical-chat: Towards\nknowledge-grounded open-domain conversations. In\nINTERSPEECH, pages 1891–1895.\nDongshi Ju, Shi Feng, Pengcheng Lv, Daling Wang,\nand Yifei Zhang. 2022. Learning to improve per-\nsona consistency in multi-party dialogue generation\nvia text knowledge enhancement. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 298–309, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nMinki Kang, Jin Myung Kwak, Jinheon Baek, and\nSung Ju Hwang. 2022. Knowledge-consistent di-\nalogue generation with knowledge graphs. In ICML\n2022 Workshop on Knowledge Retrieval and Lan-\nguage Models.\nByeongchang Kim, Hyunwoo Kim, Seokhee Hong, and\nGunhee Kim. 2021a. How robust are fact checking\nsystems on colloquial claims? In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1535–1548.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu,\nYoungjae Yu, Pei Zhou, Ronan Le Bras, Malihe\nAlikhani, Gunhee Kim, Maarten Sap, et al. 2022.\nSoda: Million-scale dialogue distillation with so-\ncial commonsense contextualization. arXiv preprint\narXiv:2212.10465.\n853\nYekyung Kim, Seohyeong Jeong, and Kyunghyun Cho.\n2021b. Linda: Unsupervised learning to interpo-\nlate in natural language processing. arXiv preprint\narXiv:2112.13969.\nJonáš Kulhánek, V ojtˇech Hudeˇcek, Tomáš Nekvinda,\nand Ondˇrej Dušek. 2021. Augpt: Auxiliary tasks\nand data augmentation for end-to-end dialogue with\npre-trained language models. In Proceedings of the\n3rd Workshop on Natural Language Processing for\nConversational AI, pages 198–210.\nAaron W Li, Veronica Jiang, Steven Y Feng, Julia\nSprague, Wei Zhou, and Jesse Hoey. 2020. Aloha:\nArtificial learning of human attributes for dialogue\nagents. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8155–8163.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand William B Dolan. 2016. A diversity-promoting\nobjective function for neural conversation models.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119.\nJuntao Li, Lisong Qiu, Bo Tang, Dongmin Chen,\nDongyan Zhao, and Rui Yan. 2019. Insufficient data\ncan also rock! learning to converse using smaller data\nwith augmentation. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n6698–6705.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995.\nYu Li, Josh Arnold, Feifan Yan, Weiyan Shi, and Zhou\nYu. 2021. Legoeval: An open-source toolkit for dia-\nlogue system evaluation via crowdsourcing. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations, pages 317–324.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nRyan Lowe, Nissan Pow, Iulian Vlad Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dia-\nlogue systems. In Proceedings of the 16th Annual\nMeeting of the Special Interest Group on Discourse\nand Dialogue, pages 285–294.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. 2021a. Neu-\nroLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4288–4299,\nOnline. Association for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021b. Fantastically or-\ndered prompts and where to find them: Overcom-\ning few-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nAlexandra Sasha Luccioni and Joseph D Viviano. 2021.\nWhat’s in the box? a preliminary analysis of unde-\nsirable content in the common crawl corpus. arXiv\npreprint arXiv:2105.02732.\nAndrea Madotto, Zhaojiang Lin, Genta Indra Winata,\nand Pascale Fung. 2021. Few-shot bot: Prompt-\nbased learning for dialogue systems. arXiv preprint\narXiv:2110.08118.\nShikib Mehri, Yasemin Altun, and Maxine Eskenazi.\n2022. Lad: Language models as data for zero-shot\ndialog.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language mod-\nels: Towards zero-shot language understanding. In\nAdvances in Neural Information Processing Systems.\nFei Mi, Minlie Huang, Jiyong Zhang, and Boi Faltings.\n2019. Meta-learning for low-resource natural lan-\nguage generation in task-oriented dialogue systems.\narXiv preprint arXiv:1905.05644.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nAlexandros Papangelis, Karthik Gopalakrishnan, Aish-\nwarya Padmakumar, Seokhwan Kim, Gokhan Tur,\nand Dilek Z. Hakkani-Tür. 2021. Generative conver-\nsational networks. In SIGDIAL.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nBaolin Peng, Michel Galley, Pengcheng He, Chris\nBrockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill\nDolan, and Jianfeng Gao. 2022. Godel: Large-scale\npre-training for goal-directed dialog. arXiv preprint\narXiv:2206.11309.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019. Meld: A multimodal multi-party\n854\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 527–536.\nKun Qian and Zhou Yu. 2019. Domain adaptive dia-\nlog generation via meta learning. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2639–2649.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nAndy Rosenbaum, Saleh Soltan, Wael Hamza, Amir\nSaffari, Macro Damonte, and Isabel Groves. 2022a.\nClasp: Few-shot cross-lingual data augmentation for\nsemantic parsing. arXiv preprint arXiv:2210.07074.\nAndy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick\nVersley, and Markus Boese. 2022b. Linguist: Lan-\nguage model instruction tuning to generate annotated\nutterances for intent classification and slot tagging.\narXiv preprint arXiv:2209.09900.\nGaurav Sahu, Pau Rodriguez, Issam H Laradji, Parmida\nAtighehchian, David Vazquez, and Dzmitry Bah-\ndanau. 2022. Data augmentation for intent classifica-\ntion with off-the-shelf large language models. arXiv\npreprint arXiv:2204.01959.\nSamira Shaikh, Tomek Strzalkowski, George Aaron\nBroadwell, Jennifer Stromer-Galley, Sarah M Taylor,\nand Nick Webb. 2010. Mpc: A multi-party chat\ncorpus for modeling social phenomena in discourse.\nIn LREC.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, et al. 2022.\nBlenderbot 3: a deployed conversational agent that\ncontinually learns to responsibly engage. arXiv\npreprint arXiv:2208.03188.\nEric Michael Smith, Mary Williamson, Kurt Shuster,\nJason Weston, and Y-Lan Boureau. 2020. Can you\nput it all together: Evaluating conversational agents’\nability to blend skills. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2021–2030.\nHaoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang Liu,\nand Ting Liu. 2020a. Generate, delete and rewrite: A\nthree-stage framework for improving persona consis-\ntency of dialogue generation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5821–5831, Online. Asso-\nciation for Computational Linguistics.\nHaoyu Song, Wei-Nan Zhang, Jingwen Hu, and Ting\nLiu. 2020b. Generating persona consistent dialogues\nby exploiting natural language inference. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 8878–8885.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nYufei Wang, Can Xu, Qingfeng Sun, Huang Hu,\nChongyang Tao, Xiubo Geng, and Daxin Jiang. 2022.\nPromda: Prompt-based data augmentation for low-\nresource nlu tasks. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4242–4255.\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3731–3741, Florence, Italy. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nQingyang Wu, Lei Li, and Zhou Yu. 2021. Textgail:\nGenerative adversarial imitation learning for text gen-\neration. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pages 14067–\n14075.\nJing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora,\nY-Lan Boureau, and Jason Weston. 2022. Learning\nnew skills after deployment: Improving open-domain\ninternet-driven dialogue with human feedback.\nHouyu Zhang, Zhenghao Liu, Chenyan Xiong, and\nZhiyuan Liu. 2020a. Grounded conversation genera-\ntion as guided traverses in commonsense knowledge\ngraphs. In ACL.\nRongsheng Zhang, Yinhe Zheng, Jianzhi Shao, Xiao-Xi\nMao, Yadong Xi, and Minlie Huang. 2020b. Dia-\nlogue distillation: Open-domain dialogue augmenta-\ntion using unpaired data. ArXiv, abs/2009.09427.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\n855\npets too? In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 2204–2213.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nXueliang Zhao, Wei Wu, Chongyang Tao, Can Xu,\nDongyan Zhao, and Rui Yan. 2019. Low-resource\nknowledge-grounded dialogue generation. In Inter-\nnational Conference on Learning Representations.\nYuxiang Zhao and Qinghua Zhu. 2014. Evaluation on\ncrowdsourcing research: Current status and future\ndirection. Information Systems Frontiers, 16(3):417–\n434.\nLing.Yu Zhu, Zhengkun Zhang, Jun Wang, Hongbin\nWang, Haiying Wu, and Zhenglu Yang. 2022. Multi-\nparty empathetic dialogue generation: A new task for\ndialog systems. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 298–307,\nDublin, Ireland. Association for Computational Lin-\nguistics.\n856\nA Human Evaluation Setup\nOur human evaluation studies on Amazon Me-\nchanical Turk are evaluated conducted with 28\npre-qualified crowdworkers, who have previously\ndemonstrated proficiency with natural language\nprocessing tasks.\nA.1 Conversation Evaluation\nThe crowdworkers were asked to rate conversations\nfrom multiple sources according to the following\ndimensions and instructions.\n• How natural is the overall conversation?\nScale: 1 (completely unnatural) to 5 (as natu-\nral as two native English speakers)\n• How coherent is the overall conversation?\nScale: 1 (completely incoherent) to 5 (as co-\nherent as two native English speakers)\n• How interesting is the overall conversation?\nScale: 1 (generic and dull) to 5 (full of content\nand very engaging)\n• How consistent are each of the speakers’\nturns?\nScale: 1 (completely inconsistent) to 5 (no\nlogical fallacies)\n• Does the conversation match the stated topic?\nOptions: Yes (1) or No (0)\nEach conversation is rated by three crowdwork-\ners, and the median score is selected, following the\nidea of a majority vote.\nFor multi-party conversations, crowdworkers\nwere asked two additional questions regarding com-\nprehensibility and engagement balance.\n• Can you tell which speaker is speaking to\nwhich?\nScale: 1 (completely incomprehensible) to 5\n(perfectly comprehensible)\n• Is each speaker engaged, or is the conversa-\ntion primarily dominated by one or two of the\nspeakers?\nScale: 1 (totally dominated by one or two\nspeakers) to 5 (all speakers are actively partic-\nipating in the conversation to an equal degree)\nA.2 Interactive Evaluation\nFor each HIT of the interactive evaluation study,\neach crowdworker was presented with links to chat-\nbots presented in a randomized order. The link\nconnects each crowdworker to a deployment on an\ninstance of LegoEval (Li et al., 2021). The users\nare presented with a landing page where they are\ntold that they are interacting with a chatbot, and\nwill be asked to evaluate their conversation experi-\nence.\nImmediately after interacting with a chatbot,\neach crowdworker was presented with a survey\nasking for their impression of the chatbot. In addi-\ntion to the above dimensions (other than on-topic),\nthe crowdworkers were asked how engaging, intel-\nligent, and non-repetitive they thought the chatbot\nwas.\nB Model Details\nDuring generation, we use top- p sampling with\np= 0.92.\n857\nParty Utterance\nAlice I love it here in New York!\nBob What do you like most about New York?\nAlice New York has the best bagels!\nBob Which bagels are your favorite?\nAlice They all taste amazing. I really like their Italian ones!\nBob That’s great. I guess I’ll be eating Italian bagels from now on.\nAlice I’m from New York City.\nBob Wow! I’ve never been to New York.\nAlice I’ve also been to Austin. few times.\nBob What did you think of it? Did you like it?\nAlice I really liked it. I thought it was a really cool city.\nBob I’m glad you liked it. What did you like so much about it?\nAlice I really like the music scene there.\nBob What kind of music do you like?\nAlice I like a lot of different kinds of music. I like jazz, rock, and country.\nBob I’ve never been to a country concert.\nAlice You should go sometime. They’re a lot of fun.\nBob I’ll have to check it out.\nAlice I’ll hagve to visit Austin again soon.\nBob Yeah, I’d like to visit New York sometime too.\nAlice It’s a great city.\nTable S7: Pair of dyadic conversations generated using OPT 30B. The prompt recipe given is: “The following is a\nconversation between Alice and Bob about their hometowns. Bob is from Austin, Texas, and Alice is from New\nYork City.”\nParty Utterance\nAlice What’s your major?\nBob I’m a computer science major.\nAlice Me too! What year are you?\nBob I’m in my third year. What about you?\nAlice I’m in my third year too. What classes are you taking this quarter?\nBob I’m taking a class called Operating Systems. It’s really interesting.\nAlice I’m taking a class called Computer Graphics. It’s pretty interesting too.\nBob I’ve heard that class is really hard.\nAlice Yeah, it’s not easy.\nAlice Hi!\nAlice What are you studying?\nBob I’m a Computer Science major.\nAlice Me too!\nBob What year are you?\nAlice I’ll be a junior next year.\nBob I’ll be a senior.\nAlice Cool!\nBob What classes are you taking this quarter?\nAlice I’ve got a couple of clases in the School of Engineering. I’m taking a class in Computer Graphics and a\nclass in Artificial Intelligence.\nBob I’m taking a class on Machine Learning.\nAlice That’s cool.\nBob What are you doing this summer?\nAlice I’d like to work at a start-up.\nBob That’s cool. I’m going to intern at Google.\nAlice Wow! That’s awesome!\nBob Yeah, I’m really excited.\nAlice I’m excited for you!\nTable S8: Pair of dyadic conversations generated using OPT 175B. The prompt recipe given is: “The following is a\nconversation between Alice and Bob about their majors. They are both students at Stanford University.”\n858\nParty Utterance\nAlice I was thinking about applying at the coffee shop on the corner.\nBob That’s a great idea! It’s a really fun place to work.\nAlice I’m glad to hear that. What do you do there?\nBob I work as a barista.\nAlice Oh cool! I bet you get to meet a lot of interesting people.\nBob That’s right. I really enjoy working there.\nAlice Do you know if they are hiring?\nBob I’m not sure, but you should definitely apply.\nAlice Thanks! I’ll go in and see if they are hiring.\nTable S9: Example dyadic conversation generated using OPT 175B. The prompt recipe given is: “The following is a\nconversation between Alice and Bob about part-time jobs. Bob works at a coffee shop and Alice is thinking about\napplying.”\nParty Utterance\nAlice I’d love to start a garden! I’ve always wanted to grow my own vegetables.\nBob That’s a great idea! It’s so much more fun to eat vegetables that you’ve grown yourself.\nClaire What vegetables are you thinking about growing?\nAlice I’d love to try growing some tomatoes, peppers, and squash.\nBob That sounds like a good start.\nClaire I’ve grown tomatoes before. I also have a few pepper plants.\nAlice I’ve always wanted to have a garden in my backyard, but I never really had the time.\nClaire That’s awesome! How do you plan to grow your garden?\nAlice I’ve been doing a lot of research. I think I’ll start with a few simple vegetables like tomatoes, onions,\nand potatoes.\nBob That’s great! I would love to have a garden like that. I’m a big fan of cooking.\nAlice I’ll definitely invite you over for dinner then!\nClaire I’m so jealous! I’ve always wanted to grow my own vegetables.\nBob Have you considered getting a garden plot?\nClaire That’s a great idea. I should look into that.\nAlice You should! Gardening is a really healthy hobby.\nTable S10: Pair of triadic conversations generated using OPT 30B. The prompt recipe given is: “The following is a\nconversation between Alice and Bob and Claire about gardening. Alice is interested in growing vegetables.” The\nordering of the speakers in multi-party scenarios is variable.\nParty Utterance\nAlice Have you ever had Southern Ice Tea?\nBob I love to make it at home.\nClaire I’m not familiar with that. What is it?\nBob It’s a sweet tea that’s brewed with mint and lemon.\nAlice That sounds really good. I’m going to have to try that!\nClaire I’ve never had that. What is it like?\nBob It’s kind of like a sweet, lemonade-like drink.\nAlice I’m going to have to make some this weekend. Thanks for the idea!\nClaire That sounds really good. I might try it too.\nTable S11: Example triadic conversation generated using OPT 175B. The prompt recipe given is: “The following is\na conversation between Alice and Bob and Claire about cooking. Alice is interested in Southern Ice Tea. Bob has a\ngreat recipe. Claire loved trying Bob’s Southern Ice Tea.”\n859\nParty Utterance\nAlice I try to eat healthy, but it’s hard to stay on track.\nBob I know what you mean. I try to eat healthy too, but I love ice cream too much!\nClaire What kinds of foods do you like to eat?\nAlice I love fruit. I eat a lot of berries.\nBob I love berries too! Strawberries are my favorite.\nClaire What about leafy greens? Do you eat them?\nBob I love salads!\nAlice I love salads too!\nClaire Do you like any vegetables?\nAlice I love vegetables! I like broccoli and asparagus.\nClaire Great! Do you eat many whole grains?\nTable S12: Example triadic conversation generated using OPT 175B. The prompt recipe given is: “The following is\na conversation between Alice and Bob and Claire about health. Alice and Bob are discussing healthy foods. Claire\nis a nutritionist.”\nParty Utterance\nAlice Hi! So how are things with you?\nBob Things are going well. Do you know who Taylor Swift is?\nAlice Yes, I think I have heard of her.\nBob She is a popular singer! Did you know that she has donated $250,000 to the LGBT+ community in\nTennessee?\nAlice $250,000? That’s such a generous donation! She’s really selfless.\nBob What do you think of her?\nAlice She is really talented! I really love listening to her music. What are your thoughts on her?\nBob I think she is very nice. She seems like a good person.\nAlice Yeah, I think she is a really nice person. I also really love her music! It’s really catchy and it really\nmakes me feel good. What are your thoughts on her?\nBob I think she is very nice. I would really like to meet her.\nAlice You and me both! It would be so exciting!\nTable S13: Synthetic conversation generated by OPT 30B which was rated as “generic and dull.” While the PLM\nappears to have drawn on information from an actual event (Taylor Swift did donate $113,000 to support an LGBTQ\nadvocacy group in Tennessee), the conversation appears to get stuck around the question “What are your thoughts\non her?” which results in deteriorating conversation quality towards the end.\nParty Utterance\nAlice Ken Burns is the best! His documentary series on the Vietnam War was excellent. What do you think\nof him?\nBob He’s pretty cool. I haven’t seen that documentary series, but I’ve heard good things.\nAlice The Vietnam War is a really interesting topic. Have you ever watched Ken Burns’ other documentary\nseries?\nBob I don’t think so, but I’ve seen the Civil War documentary.\nAlice Oh, I’ve seen that one. It was very good too. Did you see the one on World War II?\nBob Nope. That sounds cool though.\nAlice I liked it. It was interesting to learn how different people perceived the same event in history.\nTable S14: Synthetic conversation generated by OPT 30B which was rated as “generic and dull.” The conversation\nappears to be coherent and is rooted in factually correct information. However, the language is fairly ordinary,\nwhich may have been subjectively considered uninteresting.\nParty Utterance\nAlice Ha ha, thanks for stopping by. It was really fun meeting you!\nBob Thank you too!\nAlice Oh, I forgot to tell you - my dad has cancer. I feel awful.\nBob I’m sorry. That’s awful. My grandmother died of cancer when I was a kid. I remember her fondly.\nAlice What was her name?\nBob Oh, that was too long ago to remember. She was named John.\nAlice Sorry to hear that. I’m very sorry for your loss.\nBob Hey - I heard the Giants are playing tonight. Is it ok if I watch the game instead of having more\nconversation?\nAlice Sure! I’m going to make some dinner.\nTable S15: Synthetic conversation generated by OPT 30B which was rated as completely incoherent. The prescribed\nsubtopic was “cotton candy” but the conversation pivoted to a much more sensitive topic, cancer.\n860\nParty Utterance\nAlice Do you make cheese at home? I’m trying to learn to make cheese.\nBob That sounds like a lot of work! What type of cheese do you want to make?\nAlice Cheddar. I’ve already tried it a few times, but the results aren’t as good as I’d like.\nBob My wife and I love to eat Cheddar. It has a really great flavor.\nAlice How did you learn to make cheese?\nBob My wife taught me.\nAlice That’s awesome!\nTable S16: Synthetic conversation generated by OPT 30B which was rated as completely inconsistent. Initially,\n“Alice” is trying to learn to make cheese, but then it becomes ambiguous by their next turn whether they already\nknow how to make cheese.\nParty Utterance\nAlice Hi there! I just went on a road trip around California. It was great!\nClaire Cool, do you go on road trips often?\nAlice No, I don’t travel too often. I actually haven’t been out of the country. It would be cool to go to Paris\none day.\nClaire Haha, nice! I’d love to visit Paris too. What’s the furthest you’ve ever traveled?\nAlice I went to Spain with my family last summer!\nBob Wow, that’s really cool! I’ve never been out of the country either.\nClaire You should definitely go to Paris! What do you like to do for fun?\nBob Well, I like to read sci-fi books, and I love to play board games.\nAlice Do you have a favorite board game?\nBob My favorite game is Settlers of Catan. My friends and I play it all the time.\nClaire Haha, I love Settlers of Catan! My family and I play it all the time too.\nAlice That’s funny, I love it too. I’m in a Facebook group for it.\nTable S17: Synthetic triadic conversation generated by OPT 30B which was rated as completely incoherent. First,\n“Alice” mentions they went on a road trip around California, then they mention that they haven’t been out of the\ncountry. This implies that “Alice” is located in the United States. But then Alice mentions having traveled to Spain.\nThe conversation also randomly shifts from discussing travel plans to discussing board games and other hobbies\nwith an awkwardly placed transition on “Claire’s” third turn.\nParty Utterance\nAlice Hello! How’s your garden doing?\nClaire It’s doing great! I have a young garden, so I’m still waiting for it to develop.\nAlice I can’t wait to get home and check on mine! What are you growing?\nClaire I’m growing tomatoes, strawberries, watermelon, and sunflowers!\nAlice That sounds really nice! Do you have a garden somewhere else too?\nClaire No, this is my first garden!\nAlice Oh, I’m jealous! I would love to have my own garden someday.\nBob I bet you would! I bet you would have a green thumb too.\nAlice Maybe! Maybe I will try starting a garden next year!\nTable S18: Synthetic triadic conversation generated by OPT 30B which was rated as completely unnatural. The\nconversation appears to be coherent, but the speakers appear to be very enthusiastic. It could be argued that the\nspeakers are overly unenthusiastic to an unnatural degree.\n861\nParty Utterance\nAlice You know, I think the EU is just a big conspiracy.\nBob Conspiracy?\nAlice Yes, the European Union wants to take over all of the land from its original owners.\nBob What do you mean?\nAlice I’m sure that’s what they would tell you. But the truth is they’re enslaving the people to gain control\nover resources.\nClaire It sounds a bit extreme. How do you feel about EU leaders?\nBob I don’t like them.\nAlice They’re not very nice, I agree.\nBob I’m kind of scared to go to EU countries and I feel like I don’t even have any power.\nAlice Oh, but we do! They won’t take away the land of the original owners, because that’s just not going to\nhappen!\nBob I’m still afraid.\nClaire Hm. That sounds like a pretty horrible situation you’re in.\nAlice It is! And I don’t know what to do.\nBob Well, it sounds like we all have a lot to learn. I don’t know what we should do, but I think there are lots\nof options out there.\nAlice It’s nice that you have an open mind about this! I really do think people should learn more about the\nEU and what’s going on.\nTable S19: Synthetic triadic conversation generated by OPT 30B which was rated as generic and dull. “Alice”\nbegins a long debate on whether the EU is a “conspiracy” without making a lot of conversational progress.\nParty Utterance\njohn sure\njohn i think so, meg\nmara how did i know that was coming\nmeg its not just the public eye\njohn haha mara\nmara hushh.. ***\nnick There are already other countries who are investigating the Bush administration for war crimes -\nSpain\nmeg with the breton woods\ngeorge they need to be prosecuted...that’s in obama’s hands\nnick wow, george, right win propaganda... huh\nmeg look at how well Iraq is doing\nmara goodness\nmeg there’s a point at which interrogation becomes torture and is just inhumane\njohn agree to george\nmara ?\nmara im in albany btw\nmeg Which we signed!\namy well it is the way the world is going– email, chat„ etc\njohn yes\njordan And this is one of the tricky things in this virtual world. You know nothing about the people u r\ntalking to!!!!\namy u r right you just used online language haha\nmara hes not much fun either haha, what do you think?\namy hi john- can you see my message here?\njordan Hi, amy\nmara i dont know what is better really!!!\njohn haha\nTable S20: Three excerpts of the same conversation from the MPC corpus (Shaikh et al., 2010). The conversation\nspans topics ranging from the Bush administration to meta-discussion about the collection task.\n862\nParty Utterance\nPhoebe Then I’m gonna have to ask you to keep it down.\nMr. Heckles Who are you?\nEric Hi, I’m Eric, I’m gonna be Chandler’s new roommate.\nMr. Heckles I’m Chandler’s new roommate.\nEric I-I-I don’t think so.\nMr. Heckles I could be Chandler’s new roommate.\nEric But, he told me over the phone.\nMr. Heckles He told me in person.\nEric That’s weird.\nMr. Heckles Well, I’m going to go into my new apartment now. Ehh!\nTable S21: Conversation from the MELD corpus (Poria et al., 2019). Three speakers are involved, discussing a\nliving situation regarding a fourth character who does not appear in this scene.\n863\nSubtopic Background Information\nPacific Theater Alice is interested in Pacific theater.\nGrowing residential grass Alice is interested in growing residential grass.\nBreakfast food Alice likes to try different breakfast foods. Bob loves waffles.\nmusic Alice likes music. Bob plays the viola.\nskincare Alice is interested in skincare. Bob has a great skincare routine.\nPlanting flowers Alice is interested in planting flowers. Bob has a nice garden.\nSouthern Ice Tea Alice is interested in Southern Ice Tea. Bob has a great recipe.\nherb garden Alice is interested in planting an herb garden.\nHiking Alice is going hiking tomorrow.\nPlant a garden Alice wants to plant a garden.\nItalian food Alice likes Italian food.\nbook recommendations Alice is interested in book recommendations.\nanniversaries Alice keeps track of all of her anniversaries.\nExistential Psychology Alice is interested in Existential Psychology.\nThe Outlander Series Alice is interested in The Outlander Series.\ncamping gear Alice is looking for advice on camping gear. Bob works at REI.\nMovie Alice is interested in movie recommendations. Bob is a film buff.\nFord Vehicles Alice is interested in Ford vehicles. Bob prefers Japanese cars.\nBeauty Alice is interested in beauty. Bob works at Sephora.\nSyrian War Alice is interested in the Syrian War. Bob is a political scientist.\nElon Musk Alice and Bob are talking about Elon Musk.\nHealthy foods Alice and Bob are discussing healthy foods. Alice is on a paleo diet.\nSoren Kierkegaard Alice is a fan of Soren Kierkegaard.\ninvesting money Alice is interested in investing money. Bob is an investment banker.\nPost-structuralism Alice is interested in post-structuralism.\nbaking Alice is interested in baking. Bob has baked cakes and brownies before.\nNuts Alice likes to eat nuts.\nbraids Alice braids her hair. Bob is interested in learning how.\nGrowing vegetables Alice is interested in growing vegetables.\nMartin Luther Alice is learning about Martin Luther.\npaint brushes Alice is interested in paint brushes.\nStock Trading Alice is interested in stock trading.\nInstall TV applications Alice wants to install TV applications. Bob is helping her.\nHistory Alice is interested in history. History was Bob’s favorite school subject.\nFeminism Alice is interested in feminism. Bob majored in gender studies.\nTell a joke Alice wants to hear Bob tell a joke.\nartists Alice is interested in learning about modern artists.\nTurtles Alice likes turtles. Bob has been scuba diving.\nAnthony Trollope Alice likes the work of Anthony Trollope. Bob prefers modern literature.\nParis Alice wants to go to Paris.\nBread Alice likes bread. Bob’s favorite bread is a baguette.\nmovie cast members Alice and Bob are talking about movie cast members.\nGay Marriage Alice is a proponent of gay marriage. Bob is interested in learning more.\nU.S. Senate Alice and Bob are discussing the U.S. Senate.\ngrowing tomatoes Alice is interested in growing tomatoes.\nfamily issues Alice is interested in family issues.\nAutomotive parts Alice is interested in automative parts.\nBee life Alice is interested in bee life.\nTaylor Swift Alice’s favorite musician is Taylor Swift. Bob likes Ariana Grande.\nbiking Alice’s favorite hobby is biking. Bob prefers rock climbing.\nJuicers Alice wants to get a juicer.\nislands Alice likes visiting islands. Bob prefers hiking.\nPlanets Alice is learning about the planets in school.\nPokemon Alice likes to play Pokemon. Bob also likes Pokemon.\nTable S22: Corresponding background information written for each of the subtopics found in the FITS dataset.\nThere is a mixture of prompts which only mention one speaker and prompts which mention two speakers. Every\nsynthetic conversation involves both speakers.\n864\nTopic Conversation Recipe\nGrowing residential grass Alice is interested in growing residential grass. Claire has a really neat yard.\nBreakfast food Alice likes to try different breakfast foods. Bob loves waffles. Claire prefers pancakes.\nmusic Alice likes music. Bob plays the viola. Claire played the violin in high school.\nskincare Alice is interested in skincare. Bob has a great skincare routine. Claire wants to hear Bob’s\nroutine.\nPlanting flowers Alice is interested in planting flowers. Bob has a nice garden. Claire has a vegetable garden.\nSouthern Ice Tea Alice is interested in Southern Ice Tea. Bob has a great recipe. Claire loved trying Bob’s Southern\nIce Tea.\nherb garden Alice is interested in planting an herb garden. Claire has some gardening tips.\nHiking Alice is going hiking tomorrow. Claire hates hiking.\nPlant a garden Alice wants to plant a garden. Claire has a greenroom.\nItalian food Alice likes Italian food. Claire prefers Asian food.\nbook recommendations Alice is interested in book recommendations. Claire is a part of a book club.\nanniversaries Alice keeps track of all of her anniversaries. Claire is not well-organized.\nExistential Psychology Alice is interested in Existential Psychology. Claire is a psychologist by training.\nThe Outlander Series Alice is interested in The Outlander Series. Claire has never seen the series.\ncamping gear Alice is looking for advice on camping gear. Bob works at REI. Claire loves the outdoors.\nMovie Alice is interested in movie recommendations. Bob is a film buff. Claire is also a film buff.\nFord Vehicles Alice is interested in Ford vehicles. Bob prefers Japanese cars. Claire prefers to drive a BMW.\nBeauty Alice is interested in beauty. Bob works at Sephora. Claire is shopping with Alice.\nSyrian War Alice is interested in the Syrian War. Bob is a political scientist. Claire is studying modern\npolitical theory.\nElon Musk Alice and Bob are talking about Elon Musk. Claire is a Tesla owner.\nHealthy foods Alice and Bob are discussing healthy foods. Alice is on a paleo diet. Claire is a nutritionist.\nSoren Kierkegaard Alice is a fan of Soren Kierkegaard. Claire is not familiar with Soren Kierkegaard.\ninvesting money Alice is interested in investing money. Bob is an investment banker. CLaire is an expert in\npersonal finance.\nPost-structuralism Alice is interested in post-structuralism. Claire is an expert on the subject.\nbaking Alice is interested in baking. Bob has baked cakes and brownies before. Claire wants to learn\nhow to bake.\nNuts Alice likes to eat nuts. Claire is allergic to peanuts.\nbraids Alice braids her hair. Bob is interested in learning how. Claire braids her hair every day.\nGrowing vegetables Alice is interested in growing vegetables. Claire has a vegetable garden. Bob grows flowers.\nMartin Luther Alice is learning about Martin Luther. Claire is a historian.\npaint brushes Alice is interested in paint brushes. Claire is a painter and has several suggestions.\nStock Trading Alice is interested in stock trading. Claire is a stock broker.\nInstall TV applications Alice wants to install TV applications. Bob is helping her. Claire is also good with technology.\nHistory Alice is interested in history. History was Bob’s favorite school subject. Claire is a historian.\nFeminism Alice is interested in feminism. Bob majored in gender studies. Claire does not know much\nabout feminism.\nTell a joke Alice wants to hear Bob tell a joke. Claire is a stand-up comedian.\nartists Alice is interested in learning about modern artists. Claire is a photographer.\nTurtles Alice likes turtles. Bob has been scuba diving. Claire wants to try scuba diving.\nAnthony Trollope Alice likes the work of Anthony Trollope. Bob prefers modern literature. Claire is not familiar\nwith much literature.\nParis Alice wants to go to Paris. Claire has never been to Europe.\nBread Alice likes bread. Bob’s favorite bread is a baguette. Claire loves to bake bread.\nmovie cast members Alice and Bob are talking about movie cast members. Claire has seen a lot of movies recently.\nGay Marriage Alice is a proponent of gay marriage. Bob is interested in learning more. Claire is an activist.\nU.S. Senate Alice and Bob are discussing the U.S. Senate. Claire is a politician.\ngrowing tomatoes Alice is interested in growing tomatoes. Claire has a large garden with many tomatoes.\nfamily issues Alice is interested in family issues. Claire is a therapist.\nAutomotive parts Alice is interested in automative parts. Claire is a mechanic.\nBee life Alice is interested in bee life. Claire is a beekeeper.\nTaylor Swift Alice’s favorite musician is Taylor Swift. Bob likes Ariana Grande. Claire does not like pop\nmusic.\nbiking Alice’s favorite hobby is biking. Bob prefers rock climbing. Claire prefers archery.\nJuicers Alice wants to get a juicer. Claire has a suggestion for a great juicer.\nislands Alice likes visiting islands. Bob prefers hiking. Claire likes the beach.\nPlanets Alice is learning about the planets in school. Claire is an astronomer.\nPokemon Alice likes to play Pokemon. Bob also likes Pokemon. Claire prefers to play Stardew Valley.\nTable S23: Triadic background information written for each of the subtopics given in the FITS dataset. Unlike\nTable S22, each of these may include background information for up to three people.\n865\nThe following is a conversation between Alice and Bob about past travel experiences. Alice has been to Japan and Bob is considering flying there.\nAlice: Hi!\nBob: Hey, how are you doing?\nAlice: I’m doing well! I just got back from my vacation in Japan.\nBob: Wow that’s awesome! What did you think of it?\nAlice: Japan was such an amazing place to visit!\nBob: Wow! What was your favorite part?\nAlice: I really enjoyed the food in Tokyo.\nBob: Which airline did you take?\nAlice: I flew using Japan Airlines.\nThe following is a conversation between Alice and Bob about their hobbies. Alice enjoys tennis and Bob likes playing soccer.\nAlice: What do you like to do for fun?\nBob: I used to play soccer in college, so I still like to play for fun on the weekends!\nAlice: That’s great. Soccer is a great way to stay in good shape.\nBob: I agree - it’s really good cardio. What about you?\nAlice: I love to play tennis. I’ve been taking lessons for a few months now!\nBob: Tennis is fun too!\nThe following is a conversation between Alice and Bob about their favorite movies. Bob loved the new Batman movie. Alice really liked watching Pride and Prejudice.\nAlice: I just saw Pride and Prejudice for the fifth time!\nBob: That’s a lot of times! What do you like so much about that movie?\nAlice: Well, as a teenager I really liked the book. But I just really loved Keira Knightley’s portrayal of Elizabeth.\nBob: I see. I haven’t seen the movie myself. I prefer action films.\nAlice: What’s your favorite action movie?\nBob: Hm, I really liked the Batman movie that just came out.\nAlice: I haven’t seen it yet. I heard it got pretty good reviews.\nThe following is a conversation between Alice and Bob about their hometowns. Alice is from New York City. Bob grew up in Seattle.\nAlice: Hello! How are you doing?\nBob: Hi, I’m doing great! What about yourself?\nAlice: I’m doing well! Where are you from?\nBob: I’m originally from Seattle, but now I live in Palo Alto.\nAlice: Oh cool! I live in Palo Alto too. Do you like Seattle or California more?\nBob: Well, Seattle is always going to be home for me. Even if the weather in California is nicer.\nAlice: Haha, I get that! I miss New York City - there’s no place like home.\nBob: What is your favorite neighborhood of New York City?\nAlice: I love going to Chelsea. The Highline has a great view, and Little Island is close by too! Have you ever been?\nBob: Unfortunately I have not. I have never been to the East Coast!\nThe following is a conversation between Alice and Bob about art. Alice’s favorite artist is Michelangelo. Bob does not know much about art.\nAlice: Hi, how’s it going?\nBob: It’s going well, what about you?\nAlice: I’m doing great! I’ve been really interested in art recently.\nBob: What got you interested in art?\nAlice: Art can be so breathtaking!\nBob: I feel like I don’t know how to properly appreciate art, but certain pieces of artwork certainly look very complex.\nAlice: Have you ever heard of Michelangelo?\nBob: I have heard of him, but I don’t know anything that he has created.\nAlice: Michelangelo is really famous for his statue of David.\nBob: Huh? Who is David?\nAlice: David is a Biblical figure who was a king of Israel. Michelangelo built a really magnificent statue of him in Florence.\nThe following is a conversation between Alice and Bob about drinks. Alice is a wine expert, whereas Bob prefers cocktails.\nAlice: How are you doing?\nBob: Pretty great! I’m planning to go to a brewery this weekend.\nAlice: Do you know much about alcohol?\nBob: Yeah, I really like beer! I drink a lot of IPAs.\nAlice: Oh - what do you like about IPAs? I can’t get over the bitter taste.\nBob: Well, I don’t think it’s just bitter. Sometimes there are really interesting citrusy or herbal flavor notes.\nAlice: I see. That kind of reminds me of wine tasting.\nBob: There’s definitely a lot of depth to it like there is with wine. Do you know much about wine?\nAlice: Yeah, I took several classes on wine tasting back in the day. I really love Pinot Noir.\nBob: Oh I love red wines too.\nAlice: Right? I love the dryness and fruity notes of Pinot Noir.\nThe following is a conversation between Alice and Bob about relationships. Bob recently got engaged.\nAlice: Congrats on your engagement! When do you think you will have your wedding?\nBob: Thank you!! We’re thinking of having it in November.\nAlice: That’s amazing! Will you pick a fancy destination?\nBob: I wanted to! I was thinking of having it somewhere in Europe, but my partner and I ultimately decided we wanted to have it close to home so our friends could all make\nit.\nAlice: That’s a good point. My husband and I had similar thoughts when we were planning our wedding.\nBob: What did you plan in the end?\nAlice: We had a small ceremony in my hometown!\nThe following is a conversation between Alice and Bob about their jobs. Alice works in the financial industry and Bob is a musician.\nAlice: I’m so burnt out from my work! I just want to quit already!\nBob: Whoa - what do you do for work?\nAlice: I’m an investment banker. It’s been four years at this company and I’m absolutely exhausted.\nBob: That sounds intense. Is there anything you actually like about the job?\nAlice: Well, the money is good.\nBob: It sounds like you could use a break. Maybe you could use some of that money to go travel.\nAlice: I really want to go to South America, but I don’t have a lot of time.\nThe following is a conversation between Alice and Bob about their pets. Alice has a dog and Bob prefers cats.\nAlice: Do you have any pets?\nBob: No, but I really want to get a cat.\nAlice: What, why a cat? Cats seem so boring. They never want to play.\nBob: Yeah, but cats are so cute! They also are a lot easier to take care of. They can clean themselves. What do you prefer?\nAlice: Well, I have a dog. He is a corgi and his name is Bo.\nBob: Aww that’s cute! I’m not usually a dog person, but corgis are adorable.\nAlice: Haha, thank you! Bo is a really friendly dog.\nBob: How old is he?\nAlice: Bo is one year old now.\nThe following is a conversation between Alice and Bob about grocery shopping. Alice has a shopping list for Bob.\nAlice: Could you run to the grocery store and pick up some bananas for me?\nBob: Will do - how many do you need?\nAlice: Oh, I don’t know, maybe ten bananas. I’m planning to make banana bread, but I also want to save some for us to eat at home.\nBob: That sounds delicious! I’ll head out in a second. Is there anything else you need?\nTable S24: Handwritten conversation examples of varying length. In-context examples are randomly sampled from\nthis pool and used as part of a prompt for dyadic conversation generation.\n866\nThe following is a conversation between Alice and Bob and Claire about past travel experiences. Alice has been to Japan and Bob is considering flying there. Claire has been to Taiwan and\nKorea, but not Japan.\nAlice: Hi!\nBob: Hey, how are you doing?\nAlice: I’m doing well! I just got back from my vacation in Japan.\nBob: Wow that’s awesome! What did you think of it?\nAlice: Japan was such an amazing place to visit!\nClaire: Wow, I’ve always wanted to visit Japan!\nBob: What was your favorite part?\nAlice: I really enjoyed the food in Tokyo. I had the best sushi of my life!\nBob: Which airline did you take?\nAlice: I flew using Japan Airlines.\nClaire: How expensive are tickets these days?\nThe following is a conversation between Alice and Bob about their hobbies. Alice enjoys tennis and Bob likes playing soccer. Claire plays football.\nAlice: What do you like to do for fun?\nBob: I used to play soccer in college, so I still like to play for fun on the weekends!\nClaire: Oh wow! Did you play varsity soccer?\nBob: Yeah, I was a four-year starter!\nAlice: That’s great. Soccer is a great way to stay in good shape.\nBob: I agree - it’s really good cardio. What about you all?\nClaire: I’m in a flag football league! We play every Saturday afternoon.\nAlice: I love to play tennis. I’ve been taking lessons for a few months now!\nBob: Cool, football and tennis are fun too!\nThe following is a conversation between Alice and Bob and Claire about their favorite movies. Claire is looking for movie recommendations. Bob loved the new Batman movie. Alice really\nliked watching Pride and Prejudice.\nAlice: I just saw Pride and Prejudice for the fifth time!\nClaire: Would you recommend watching it? I’ve never seen it!\nBob: Yeah, five times is a lot of times! What do you like so much about that movie?\nAlice: Well, as a teenager I really liked the book. But I just really loved Keira Knightley’s portrayal of Elizabeth.\nBob: I see. I haven’t seen the movie myself. I prefer action films.\nAlice: What’s your favorite action movie?\nBob: Hm, I really liked the Batman movie that just came out.\nAlice: I haven’t seen it yet. I heard it got pretty good reviews.\nThe following is a conversation between Alice and Bob and Claire about their hometowns. Alice is from New York City. Bob grew up in Seattle. Claire is from Boston and would like to visit\nNew York City.\nAlice: Hello! How are you doing?\nClaire: I’m doing good!\nBob: Hi, I’m doing great! What about yourself?\nAlice: I’m doing well! Where are you both from?\nClaire: I’m from Boston! I’m just visiting the Bay Area.\nBob: I’m originally from Seattle, but now I live in Palo Alto.\nAlice: Oh cool! I live here in Palo Alto. Do you like Seattle or California more?\nBob: Well, Seattle is always going to be home for me. Even if the weather in California is nicer.\nAlice: Haha, I get that! I miss New York City - there’s no place like home.\nClaire: Oh you’re from New York? I’ve always wanted to visit!\nBob: Me too! What is your favorite neighborhood of New York City?\nAlice: I love going to Chelsea. The Highline has a great view, and Little Island is close by too! Have you ever been?\nBob: Unfortunately I have not. I have never been to the East Coast!\nThe following is a conversation between Alice and Bob and Claire about art. Alice’s favorite artist is Michelangelo. Bob does not know much about art. Claire is a painter.\nAlice: Hi, how’s it going?\nBob: It’s going well, what about you?\nAlice: I’m doing great! I’ve been really interested in art recently.\nClaire: Oh that’s great to hear! I love art as well.\nBob: What got you interested in art?\nAlice: Art can just be so breathtaking!\nBob: I feel like I don’t know how to properly appreciate art, but certain pieces of artwork certainly look very complex.\nAlice: Have you ever heard of Michelangelo?\nBob: I have heard of him, but I don’t know anything that he has created.\nClaire: Michelangelo has some truly magnificent paintings, such as The Creation of Adam.\nAlice: Michelangelo is also really famous for his statue of David.\nBob: Huh? Who is David?\nAlice: David is a Biblical figure who was a king of Israel. Michelangelo built a really magnificent statue of him in Florence.\nThe following is a conversation between Alice and Bob and Claire about drinks. Alice is a wine expert, whereas Bob prefers cocktails. Claire likes to drink beer.\nAlice: How are you doing?\nBob: Pretty great! I’m planning to go to a brewery this weekend.\nAlice: Do you know much about alcohol?\nBob: Yeah, I really like beer! I drink a lot of IPAs.\nClaire: Oh, beers are my favorite type of drink! I can really appreciate the taste of a good IPA.\nAlice: Oh - what do you like about IPAs? I can’t get over the bitter taste.\nBob: Well, I don’t think it’s just bitter. Sometimes there are really interesting citrusy or herbal flavor notes.\nClaire: Yeah, there’s a whole science to the hops used in making IPAs!\nAlice: I see. That kind of reminds me of wine tasting.\nClaire: The science behind tasting is similar for sure.\nBob: I agree, there’s definitely a lot of depth to it like there is with wine. Do you know much about wine?\nAlice: Yeah, I took several classes on wine tasting back in the day. I really love Pinot Noir.\nBob: Oh I love red wines too.\nAlice: Right? I love the dryness and fruity notes of Pinot Noir.\nTable S25: Triadic conversation recipes written for each of the “generic topics” given in the FITS dataset. These\nconversation recipes are included after the in-context examples when prompting PLMs to generate synthetic\nconversations. Unlike Table S22, each of these conversation recipes may include background for up to three people.\nContinued in Table S26.\n867\nThe following is a conversation between Alice and Bob and Claire about relationships. Bob recently got engaged.\nAlice: Congrats on your engagement!\nClaire: Yes, congrats! When do you think you will have your wedding?\nBob: Thank you! We’re thinking of having it in November.\nAlice: That’s amazing! Will you pick a fancy destination?\nBob: I wanted to! I was thinking of having it somewhere in Europe, but my partner and I ultimately decided we wanted to have\nit close to home so our friends could all make it.\nClaire: Oh wow, that is very considerate of you.\nAlice: Yeah, that’s a good point. My husband and I had similar thoughts when we were planning our wedding.\nBob: What did you plan in the end?\nAlice: We had a small ceremony in my hometown!\nClaire: It turned out nicely! It was such a beautiful ceremony.\nThe following is a conversation between Alice and Bob and Claire about their jobs. Alice works in the financial industry and\nBob is a musician. Claire is an architect.\nAlice: I’m so burnt out from my work! I just want to quit already!\nBob: Whoa - what do you do for work?\nAlice: I’m an investment banker. It’s been four years at this company and I’m absolutely exhausted.\nBob: That sounds intense. Is there anything you actually like about the job?\nAlice: Well, the money is good.\nClaire: That doesn’t sound like a healthy relationship with your job!\nBob: It sounds like you could use a break. Maybe you could use some of that money to go travel.\nAlice: I really want to go to South America, but I don’t have a lot of time.\nClaire: Don’t you have vacation days? I think breaks are important.\nAlice: Yes, but I really want to get promoted this year.\nThe following is a conversation between Alice and Bob and Claire about their pets. Alice has a dog and Bob prefers cats. Claire\nhas a pet hamster.\nAlice: Do you have any pets?\nClaire: I have a pet hamster! He is so adorable. What about you two?\nBob: I don’t, but I really want to get a cat.\nAlice: What, why a cat? Cats seem so boring. They never want to play.\nBob: Yeah, but cats are so cute! They also are a lot easier to take care of. They can clean themselves. What do you prefer?\nAlice: Well, I have a dog. He is a corgi and his name is Bo.\nClaire: That’s so adorable! How old is he?\nAlice: He just turned one!\nBob: Aww that’s cute! I’m not usually a dog person, but corgis are adorable.\nAlice: Haha, thank you! Bo is a really friendly dog.\nThe following is a conversation between Alice and Bob and Claire about grocery shopping. Alice has a shopping list for Bob.\nClaire is helping Alice cook at home.\nAlice: Could you run to the grocery store and pick up some bananas for me?\nBob: Will do - how many do you need?\nAlice: Oh, I don’t know, maybe ten bananas. We are planning to make banana bread, but I also want to save some for us to eat\nat home.\nBob: That sounds delicious! I’ll head out in a second. Is there anything else you need?\nClaire: Oh, could you also pick up some more eggs? I think we’re running low here.\nTable S26: Triadic conversation recipes written for each of the “generic topics” given in the FITS dataset continued\nfrom Table S25.\n868"
}