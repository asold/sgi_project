{
  "title": "Boundary-Aware Transformers for Skin Lesion Segmentation",
  "url": "https://openalex.org/W3200640732",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A36216075",
      "name": "Wang, Jiacheng",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2095804213",
      "name": "Wei Lan",
      "affiliations": [
        "Xiamen University Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A1383091939",
      "name": "Wang Liansheng",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2222930782",
      "name": "Zhou Qi-chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099562854",
      "name": "Zhu, Lei",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2122621175",
      "name": "Qin Jing",
      "affiliations": [
        "Hong Kong Polytechnic University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2295519047",
    "https://openalex.org/W2623166637",
    "https://openalex.org/W2437694626",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W6600007113",
    "https://openalex.org/W2921406441",
    "https://openalex.org/W3035485193",
    "https://openalex.org/W3096678291",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3042980549",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W2129259959",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6602254124",
    "https://openalex.org/W2962850830",
    "https://openalex.org/W2564782580",
    "https://openalex.org/W2607363228",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2884436604"
  ],
  "abstract": null,
  "full_text": "Boundary-aware Transformers for\nSkin Lesion Segmentation\nJiacheng Wang1, Lan Wei2, Liansheng Wang1(\u0000 ), Qichao Zhou3(\u0000 ),\nLei Zhu4, Jing Qin5\n1 Department of Computer Science at School of Informatics, Xiamen University\njiachengw@stu.xmu.edu.cn,lswang@xmu.edu.cn\n2 School of Electrical and Computer Engineering, Xiamen University Malaysia\nweilanlan21@gmail.com\n3 Manteia Technologies Co.,Ltd zhouqc@manteiatech.com\n4 Department of Computer Science and Engineering, The Chinese University of Hong Kong\nlzhu@cse.cuhk.edu.hk\n5 Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University\nharry.qin@polyu.edu.hk\nAbstract. Skin lesion segmentation from dermoscopy images is of great impor-\ntance for improving the quantitative analysis of skin cancer. However, the auto-\nmatic segmentation of melanoma is a very challenging task owing to the large\nvariation of melanoma and ambiguous boundaries of lesion areas. While con-\nvolutional neutral networks (CNNs) have achieved remarkable progress in this\ntask, most of existing solutions are still incapable of effectively capturing global\ndependencies to counteract the inductive bias caused by limited receptive ﬁelds.\nRecently, transformers have been proposed as a promising tool for global con-\ntext modeling by employing a powerful global attention mechanism, but one of\ntheir main shortcomings when applied to segmentation tasks is that they can-\nnot effectively extract sufﬁcient local details to tackle ambiguous boundaries. We\npropose a novel boundary-aware transformer (BAT) to comprehensively address\nthe challenges of automatic skin lesion segmentation. Speciﬁcally, we integrate a\nnew boundary-wise attention gate (BAG) into transformers to enable the whole\nnetwork to not only effectively model global long-range dependencies via trans-\nformers but also, simultaneously, capture more local details by making full use of\nboundary-wise prior knowledge. Particularly, the auxiliary supervision of BAG\nis capable of assisting transformers to learn position embedding as it provides\nmuch spatial information. We conducted extensive experiments to evaluate the\nproposed BAT and experiments corroborate its effectiveness, consistently outper-\nforming state-of-the-art methods in two famous datasets.\nKeywords: Transformer · Medical Image Segmentation · Deep Learning.\n1 Introduction\nMelanoma is one of the most rapidly increasing cancers all over the world. According to\nthe American Cancer Society’s estimation, there are about 100,350 new cases and over\nJ. Wang and L. Wei—Contributed equally; L. Wang and Q. Zhou—Corresponding authors.\nCode is available at https://github.com/jcwang123/BA-Transformer\narXiv:2110.03864v1  [eess.IV]  8 Oct 2021\n2 Wang and Wei et al.\nFig. 1.The challenges of automatic skin lesion segmentation from dermoscopy images: (a)-(d)\nlarge skin lesion variations in size, shape, and color, (f)(g) partial occlusion by hair, and (f)-(h)\nambiguous boundaries.\n65,00 deaths in 2020 [14]. Segmenting skin lesions from dermoscopy images is a key\nstep in skin cancer diagnosis and treatment planning. In current clinical practice, derma-\ntologists usually need to manually delineate skin lesions for further analysis. However,\nmanual delineation is usually tedious, time-consuming, and error-prone. To the end, au-\ntomated segmentation methods are highly demanded in clinical practice to improve the\nsegmentation efﬁciency and accuracy. It remains, however, a very challenging task be-\ncause (1) skin lesions have large variations in size, shape,and color (see Fig. 1 (a-d)), (2)\npresent of hair will partially cover the lesions destroying local context, (3) the contrast\nbetween some lesions to normal skin are relatively low, resulting in ambiguous bound-\naries (see Fig. 1 (e-h)), and (4) the limited training data make the task even harder. A lot\nof effort has been dedicated to overcoming these challenges. Traditional methods based\non various hand-crafted features are usually not stable and robust, leading to poor seg-\nmentation performance when facing lesions with large variations [16]. The main reason\nis that these hand-crafted features are incapable of capturing distinctive representations\nof skin lesions. To solve the problem, deep learning models based on convolutional neu-\nral networks (CNN) have been proposed and achieved remarkable performance gains\ncompared with traditional methods such as some advanced version of the fully convolu-\ntional network (FCN) [22,21]. However, these models are still insufﬁcient to tackle the\nchallenges of skin lesion segmentation due to the inductive bias caused by the lack of\nglobal context. With regard to this, researchers propose various approaches to enlarging\nthe receptive ﬁelds inspired by the advancement of dilated convolution [20,19]. Lee et\nal [11] extensively incorporate the dilated attention module with boundary prior so that\nthe network predict boundary key-points maps to guide the attention module.\nNevertheless, most existing solutions are still incapable of effectively capturing suf-\nﬁcient global context to deal with above mentioned challenges. Recently, transformers\nhave been proposed to regard an image as a sequence of patches and aggregate feature\nin global context by self-attention mechanisms [4,15]. For example, TransUNet [6], a\nBoundary-aware Transformers for Skin Lesion Segmentation 3\nhybrid architecture of CNN and transformer, performs well on Synapse multi-organ\nsegmentation. Yet, it is difﬁcult for transformer based framework to achieve the same\nsuccess on skin lesion segmentation, which usually has only thousands of data not the\nsame as what they have done in the COCO 2017 Challenge [5] containing 118k training\nimages and 5k validation images. Limited images make it difﬁcult to encode position\nembedding, and hence will not always be able to accurately and effectively model long-\nrange interactions. Moreover, regions of lesion cover a relatively small area compared\nto normal tissues and generally has ambiguous boundary not as human organs, which\ninterferes with segmentation performance by a large margin.\nIn this paper, we propose boundary-aware transformer (BAT) to ably handle afore-\nmentioned problems, by holistically leveraging the advancement of boundary-wise prior\nknowledge and transformer-based network. In fact, this design is based on the intu-\nitions for human beings to perceive lesions in vision, i.e. considering global context to\ncoarsely locate lesion area and paying special attention to ambiguous area to specify the\nexact boundary. Concretely, we propose a boundary-wise attention gate (BAG) in trans-\nformer architecture to make full use of boundary-wise prior knowledge. Firstly, BAG\nwould learn which patches in the sequence belong to ambiguous boundary, thus provid-\ning a patch-wise attention map to guide this attention gate. Secondly, a novel key-patch\nmap generation algorithm is introduced for adeptly giving the ground-truth label that\ncan best represent the ambiguous boundary of target lesion. Thirdly, the auxiliary su-\npervision of BAG provides feedback to train transformers that can let it efﬁciently learn\nposition embedding on a relatively small dataset. We evaluate our model on different\npublicly available databases. One is the ISBI 2016 and PH2 dataset following the ex-\nperimental setting in most recent work [11], the other one is the latest ISIC 2018 dataset\nconsisting of 2594 labeled images in total. All the experiment results demonstrate the\nsigniﬁcant performance gains of our proposed framework.\n2 Method\nAn overview of our proposed model is illustrated in Fig. 2. We will ﬁrst introduce\nour basic transformer network that leverages the intrinsic local locality of CNN and in-\nnate long-range dependency to complement the skin lesion segmentation in Section 2.1.\nThen, the overall framework of our boundary-aware transformer for segmenting regions\nof ambiguous boundary will be elaborated in Section 2.2. In the end, Section 2.3 intro-\nduces the hybrid objective function to efﬁciently train our boundary-aware transformer.\n2.1 Basic Transformer for Segmentation\nOur basic transformer (BT) blends typical CNN’s and transformer’s architectures to per-\nform segmentation task, by three procedures: image sequentialization, sequence trans-\nformation, and atrous prediction. Image sequentialization is typically the primary step\nat which an image is converted into 1D sequential embedding ﬁtting the input format of\nsubsequent sequence transformation. Atrous prediction is designed to effectively rem-\nedy the local feature representation which has been ignored in sequence transformation\nand robustly segment lesions at multiple scales.\n4 Wang and Wei et al.\n…\n\u0000\n16 x \u0000\n16  x C\nPositional \nEmbedding\nTransformer \nEncoder\n…      \nTransformer \nDecoder\n…\nZe\n…\nLearn-able Boundary \nPoint Prototype\n…      \nEmbedded \nSequence Zi\nLMap\nPredicted\nSegmentation \nMap\nGround Truth\nLSeg\nPredicted\nKey-Patch Map\nGround-Truth \nKey-Patch Map\nBinary Key-Patch Map\n       Min/Max\nabs(overlap area - 0.5)\n\u0000\n16 x \u0000\n16  x C\nIf\nNMS\n…      …      \nAdd & \nNormMSA + MLPAdd & \nNorm +BAG\nFig. 2.An overview of the proposed Boundary-Aware Transformer framework.\nImage Sequentialization.Current literature processing sequentialization by typi-\ncal CNN encoder [4,12] or linear projection [8,23]. Despite its successful advancement\nachieved on some natural vision tasks, linear projection remains the shortcoming that\nis the high dependency on the amount of dataset, leading to inferior segmentation per-\nformance in medical domain [6]. To this end, we perform sequentialization through the\nformer. Given an input image I ∈RH×W×3 with width W and height H, the CNN\nbackbone (ResNet50 as default in this work) produces the corresponding image fea-\nture map If ∈R\nH\n16 ×W\n16 ×C, that the size of each patch is 16 ×16. The feature map\nis then ﬂattened into 1D patch embedding and added by a learnable positial embed-\nding [9] which is randomly initialized to compensate spatial information destroyed by\nsequentialization, resulting in ﬁnal sequential embedding as E ∈RL×C,L = HW\n256 .\nSequence Transformation.Transformer encoder composed of nstacked encoder\nlayers is applied to capture long-range context in a whole dermoscopic image. Each\nlayer in the encoder consists of a multi-head self-attention module (MSA) and a Mul-\ntilayer Perceptron (MLP) following typical design [18]. Assumed that the input of i-th\nlayer is Zi−1 (specially, Z0 ←E), the output can be written as follows:\nZi = MSA(Zi−1) ⊕MLP(MSA(Zi−1)), (1)\nEventually, transformed feature of the last layer Zn will be reshaped to 2D format as\nZ ∈R\nH\n16 ×W\n16 ×C, for dense prediction in the next.\nAtrous Prediction. For segmenting lesions at multiple scales, this module takes\ntransformed feature Z after self-attention mechanism as input and aims to produce a\ndense prediction. Aiming to enhance the local feature representation and handle the\nmulti-scale lesion context, an atrous prediction module is designed as follows:\nˆSpred = δ(d1\n1([d3\n1(Z),d3\n3(Z),d3\n6(Z)])). (2)\nBoundary-aware Transformers for Skin Lesion Segmentation 5\nHere, ds\nr(·) denotes dilated convolution function with a dilation rate rand ﬁlter size of\ns×s. δis a sigmoid function. The enhanced feature mapsds\nr(Z) with various receptive\nﬁelds are concatenated across channel-wise and projected into segmentation map space.\n2.2 Boundary-Aware Transformer\nEfforts to incorporate structural boundary information to CNNs have been made a lot\nthese years, but there is little literature investigating the effectiveness on transformer.\nWe argue that the equipment of boundary information can also let transformer obtain\nmore power in addressing lesions with ambiguous boundary. To this end, we devise the\nboundary-aware transformer (BAT), in which a boundary-wise attention gate (BAG) is\nadded at end of each transformer encoder layer to reﬁne transformed feature. BAG’s ar-\nchitecture is similar to conventional spatial attention gate including (1) a key-patch map\ngenerator which takes the transformed feature as input and output a binary patch-wise\nattention map ˆMpred = δ(d1\n1(Z)) ∈RL×1, where value1 indicates that the correspond-\ning patch is at ambiguous boundary. (2) and a residual attention scheme for preserving\nboundary-wise information. Hence, the boundary-aware transformed feature can be re-\nwritten as:\nVi−1 = MSA(Zi−1) ⊕MLP(MSA(Zi−1)),\nZi = Vi−1 ⊕(Vi−1 ⊗ ˆMi−1),\n(3)\nwhere ⊕and ⊗denote element-wise addition and channel-wise multiplication, respec-\ntively.\nIn addition to BAGs in transformer encoder layers, a query embedding based BAG\nis applied after encoder to reﬁne the feature Zn. It plays the same role in boundary-\nwise attention but comes true by a totally different way. Here, instead of learning the\nlinear projection as classiﬁer, we refer a learnable embedding Qb as context prototype\nfor regions among ambiguous boundary. It will be compared with all patch embedding\n(Z) after aforementioned blocks, to produce a similarity map Mn. Those patches with\nhigh similarity will be the regions of ambiguous boundary. Similar to other BAGs, a\nresidual attention scheme is also applied here as: Zn+1 = Zn ⊕(Zn ⊗ ˆMn).\nBy this design, BAT learns robust feature representation of ambiguous boundary\nin a variety of ways, which is of great signiﬁcance to handle segmentation of lesions\nwith ambiguous boundary. Following our basic design, feature Zn+1 is fed into atrous\nprediction module to produce the segmentation map ˆSpred.\nBoundary-Supervised Generator.As the generator doesn’t necessarily know on\nits own which patches can best represent structural boundary of target lesion, we intro-\nduce a novel algorithm to produce ground-truth key-patch map to train the generator\nwith full supervision. Besides the enhancement of boundary features, this design can\nalso help in accelerating training transformer thanks to the auxiliary constraints.\nSpeciﬁcally, boundary points set is produced using conventional edge detection al-\ngorithm at ﬁrst. For each point in this set, we draw a circle of radius r (set to 10 as\ndefault) and calculate the proportion p of lesion area in this circle. Larger or smaller\nproportion indicates that boundary is not smooth in this cricle. Thus we score each point\nas |p−0.5|, representing the assistance in segmenting ambiguous parts. Non-maximum\nsuppression is then utilized to ﬁlter points with larger proportion than neighbour k(set\n6 Wang and Wei et al.\nto 30 as default) points. Next, ﬁltered points’ 2D location (x,y) is mapped into 1D lo-\ncation as ⌊x/16⌋∗16+ ⌊y/16⌋, and patch labels at these location are set to1 and others\nare set to 0, leading to ﬁnal ground-truth MGT .\n2.3 Objective Function\nTo train the segmentation network including the proposed BAGs, we emply two types\nof loss functions. The ﬁrst one is a Dice loss function to minimize the difference be-\ntween the groud-truth segmentation map and the predicted segmentation map as LSeg.\nThe second one is a Cross-Entropy loss to reduce the predicted key-patch map and its\nground-truth as LMap . Total loss is deﬁned as:\nLTotal =LSeg +\nn+1∑\ni=1\nLi\nMap ,\nLSeg = φDICE (SGT , ˆSPred ),Li\nMap = φCE (MGT , ˆMi\nPred ),\n(4)\nwhere Mi\nPred denotes the predicted key-patch map at i-th transformer encoder layer.\nφDICE ,φCE denote Dice loss function and Cross-Entropy loss function, respectively.\nndenotes the number of transformer encoder layers and is set to 4 as default.\n3 Experimental Results\n3.1 Datasets\nWe conduct extensive experiments on the skin lesion segmentation datasets from In-\nternational Symposium on Biomedical Imaging (ISBI) of the years 2016 and 2018.\nThe datasets are collected from a variety of different treatment centers, archived by the\nInternational Skin Imaging Collaboration (ISIC), which hosted a challenge named skin\nlesion analysis toward melanoma detection to boost the performance of melanoma diag-\nnosis. ISIC 2016 contains a total number of 900 samples for training and a total number\nof 379 dermoscopy images for testing. We follow up the same experimental protocols in\nthe most recent work [11], in which we train our model on the training set of ISIC 2016\nand extensively evaluate it on PH2 dataset. ISIC 2018 contains 2594 training samples\nin total and annotation of its public test set is missing, therefore we perform ﬁve-fold\ncross-validation on its training set for fair comparison.\n3.2 Implementation Details\nOur network is implemented on a single NVIDIA RTX 3090Ti. All images are empir-\nically resized to (512 ×512) considering the efﬁciency, and we do data augmentation\nincluding vertical ﬂip, horizontal ﬂip and random scale change (limited 0.9-1.1). Each\nmini-batch includes 24 images and we utilize Adam with an initial learning rate of 0.001\nto optimize the network. Learning rate decrease in half when loss on the validation set\nhas not dropped by 10 epochs. The encoder of each network has been pre-trained on\nImageNet and all parameters are then ﬁne-tuned for 500 epochs in total.\nBoundary-aware Transformers for Skin Lesion Segmentation 7\nTable 1.Experimental results on different datasets.\nModel ISIC 2016 + PH2 Model ISIC 2018\nDice ↑ IoU ↑ Dice ↑ IoU ↑\nSSLS[1] 0.783 0 .681 DeepLabv3 [7] 0.884 0 .806\nMSCA[3] 0.815 0 .723 U-Net++ [24] 0.879 0 .805\nFCN [13] 0.894 0 .821 CE-Net [10] 0.891 0 .816\nBi et al[2] 0.906 0 .839 MedT [17] 0.859 0 .778\nLee et al[11] 0.918 0 .843 TransUNet [6] 0.894 0 .822\nBAT 0.921 0 .858 BAT 0.912 0 .843\n(a) Image (b) UNet++ (c) CE-Net (d) TransUNet (e) Ours (f) GT\nFig. 3.Visual comparison of lesion segmentation results produced by different methods.\n3.3 Comparison with State-of-the-Arts\nFor baseline comparisons, we run experiments on both convolutional and transformer-\nbased methods. With regard to the evaluation metrics, we employ a Dice coefﬁcient\n(Dice), and a Intersection over Union (IoU). Table 1 displayed the comparative study\nof our proposed boundary-aware transformer (BAT) with other methods on different\ndatasets. It’s obviously shown that our model achieves the best segmentation perfor-\nmance.\nOn the ISIC 2016 + PH2dataset, we compare our method with ﬁve state-of-the-\nart methods. Among them, Lee et al [11] is a 2D attention-based model with use\nof boundary-prior knowledge, achieving best segmentation performance on skin le-\nsion segmentation recently. As seen in the Table 1, our BAT achieves 0.920 in Dice\nand 0.858 in IoU, outperform Lee et al by 0.2% and 1.5% in Dice and IoU, respec-\ntively. We extensively conduct experiments with other SOTA segmentation networks on\nthe ISIC 2018dataset, including three famous convolutional models for segmentation\n(DeepLabv3 [7], UNet++ [24], CE-Net [10]) and two transformer-based network to ad-\ndress medical image segmentation (TransUNet [6], MedT [17]). Even compared with\n8 Wang and Wei et al.\nTable 2.Experimental results on different datasets.\nTrans. BAG ISIC 2016 + PH2 ISIC 2018\nDice ↑ IoU ↑ Dice ↑ IoU ↑\n0.884 0 .805 0 .879 0 .810\n✓ 0.900 0 .827 0 .890 0 .821\n✓ ✓ 0.921 0 .858 0 .912 0 .843\nother state-of-the-art segmentation models, our BAT still achieves the consistent and\nsigniﬁcant improvement on both metrics. It’s noteworthy that transformer-based net-\nwork has superior performance than conventional CNNs, indicating the effectiveness of\nutilizing global context to detect skin lesion. In addition, compared with TransUNet, our\nmethod leveraging the boundary-prior knowledge signiﬁcantly improves the segmenta-\ntion performance (1.8% on Dice and 2.1% on IoU), proving that the combination of\nboundary information and transformer architecture is indeed helpful to segment target\nlesion.\nFig. 3 visualizes ﬁve typical challenging cases of lesion segmentation results. It is\nobserved that our results are closest to the ground truth, when compared with our com-\npetitors. The ﬁrst three rows represent cases with various color, size and shape, and our\nBAT outperforms others with most stable segmentation performance, indicating the ro-\nbust advancement of global context. The last two rows highlight some small regions of\nambiguous boundary and it’s shown that our BAT is capable of tacking such problems,\ndue to the use of boundary-wise prior knowledge.\n3.4 Ablation Study\nWe further conduct ablation studies to demonstrate the effectiveness of three major\ncomponents in BAT: (1) the transformer-based self-attention mechanism (Trans.), (2)\nboundary-wise attention gate (BAG). As shown in the Table. 2, by the incorporation of\nself-attention mechanism, the IoU increases by a large margin on both datasets. This\nresult indicate that it’s essential to integrate global context to improve the skin lesion\ndetection. On the other hand, applying BAGs to guide transformer further improves the\nperformance signiﬁcantly, conﬁrming the effectiveness of boundary-wise prior knowl-\nedge to tackling challenging cases, such as lesions with ambiguous boundary.\n4 Conclusion\nWe present a novel and efﬁcient context-aware network, namely boundary-aware trans-\nformer (BAT) network, for accurate segmentation of skin lesion from dermoscopy im-\nages. Extensive experiments on two public datasets conﬁrm the effectiveness of our\nproposed BAT, to help yield much better segmentation results for skin lesions. Our full\nmodel outperforms state-of-the-art models by a large margin in segmentation accuracy\nand the intuitive visualization shows that our BAT has most satisfactory performance\non skin lesions with ambiguous boundary.\nBoundary-aware Transformers for Skin Lesion Segmentation 9\nReferences\n1. Ahn, E., Bi, L., Jung, Y .H., Kim, J., Li, C., Fulham, M., Feng, D.D.: Automated saliency-\nbased lesion segmentation in dermoscopic images. In: 2015 37th annual international con-\nference of the IEEE engineering in medicine and biology society (EMBC). pp. 3009–3012.\nIEEE (2015)\n2. Bi, L., Kim, J., Ahn, E., Kumar, A., Fulham, M., Feng, D.: Dermoscopic image segmentation\nvia multistage fully convolutional networks. IEEE Transactions on Biomedical Engineering\n64(9), 2065–2074 (2017). https://doi.org/10.1109/TBME.2017.2712771\n3. Bi, L., Kim, J., Ahn, E., Feng, D., Fulham, M.: Automated skin lesion segmentation via\nimage-wise supervised learning and multi-scale superpixel based cellular automata. In: 2016\nIEEE 13th International Symposium on Biomedical Imaging (ISBI). pp. 1059–1062. IEEE\n(2016)\n4. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\nobject detection with transformers. In: European Conference on Computer Vision. pp. 213–\n229. Springer (2020)\n5. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end\nobject detection with transformers. In: European Conference on Computer Vision. pp. 213–\n229. Springer (2020)\n6. Chen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., Lu, L., Yuille, A.L., Zhou, Y .: Tran-\nsunet: Transformers make strong encoders for medical image segmentation (2021)\n7. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution for se-\nmantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)\n8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De-\nhghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n9. Gehring, J., Auli, M., Grangier, D., Yarats, D., Dauphin, Y .N.: Convolutional sequence to se-\nquence learning. In: International Conference on Machine Learning. pp. 1243–1252. PMLR\n(2017)\n10. Gu, Z., Cheng, J., Fu, H., Zhou, K., Hao, H., Zhao, Y ., Zhang, T., Gao, S., Liu, J.: Ce-net:\nContext encoder network for 2d medical image segmentation. IEEE transactions on medical\nimaging 38(10), 2281–2292 (2019)\n11. Lee, H.J., Kim, J.U., Lee, S., Kim, H.G., Ro, Y .M.: Structure boundary preserving\nsegmentation for medical image with ambiguous boundary. In: 2020 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR). pp. 4816–4825 (2020).\nhttps://doi.org/10.1109/CVPR42600.2020.00487\n12. Li, Z., Liu, X., Creighton, F.X., Taylor, R.H., Unberath, M.: Revisiting stereo depth\nestimation from a sequence-to-sequence perspective with transformers. arXiv preprint\narXiv:2011.02910 (2020)\n13. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition. pp.\n3431–3440 (2015)\n14. Mathur, P., Sathishkumar, K., Chaturvedi, M., Das, P., Sudarshan, K.L., Santhappan, S., Nal-\nlasamy, V ., John, A., Narasimhan, S., Roselind, F.S., et al.: Cancer statistics, 2020: report\nfrom national cancer registry programme, india. JCO Global Oncology6, 1063–1075 (2020)\n15. Prangemeier, T., Reich, C., Koeppl, H.: Attention-based transformers for instance segmenta-\ntion of cells in microstructures. In: 2020 IEEE International Conference on Bioinformatics\nand Biomedicine (BIBM). pp. 700–707. IEEE (2020)\n16. Tu, Z., Bai, X.: Auto-context and its application to high-level vision tasks and 3d brain im-\nage segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 32(10),\n1744–1757 (2010). https://doi.org/10.1109/TPAMI.2009.186\n10 Wang and Wei et al.\n17. Valanarasu, J.M.J., Oza, P., Hacihaliloglu, I., Patel, V .M.: Medical transformer: Gated axial-\nattention for medical image segmentation. arXiv preprint arXiv:2102.10662 (2021)\n18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,\nPolosukhin, I.: Attention is all you need (2017)\n19. Yu, F., Koltun, V .: Multi-scale context aggregation by dilated convolutions. arxiv 2015. arXiv\npreprint arXiv:1511.07122 615 (2019)\n20. Yu, F., Koltun, V ., Funkhouser, T.: Dilated residual networks. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp. 472–480 (2017)\n21. Yu, L., Chen, H., Dou, Q., Qin, J., Heng, P.: Automated melanoma recognition in dermoscopy\nimages via very deep residual networks. IEEE Transactions on Medical Imaging36(4), 994–\n1004 (2017). https://doi.org/10.1109/TMI.2016.2642839\n22. Yuan, Y ., Chao, M., Lo, Y .C.: Automatic skin lesion segmentation using deep fully convolu-\ntional networks with jaccard distance. IEEE transactions on medical imaging 36(9), 1876–\n1886 (2017)\n23. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J., Xiang, T., Torr,\nP.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence perspective with\ntransformers. arXiv preprint arXiv:2012.15840 (2020)\n24. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net architecture\nfor medical image segmentation. In: Deep learning in medical image analysis and multi-\nmodal learning for clinical decision support, pp. 3–11. Springer (2018)",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210134189",
      "name": "Xiamen University Malaysia",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I14243506",
      "name": "Hong Kong Polytechnic University",
      "country": "HK"
    }
  ]
}