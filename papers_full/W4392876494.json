{
  "title": "Domain-specific LLM Development and Evaluation – A Case-study for Prostate Cancer",
  "url": "https://openalex.org/W4392876494",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2231377352",
      "name": "Amara Tariq",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2103670821",
      "name": "Man Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2922011578",
      "name": "Aisha Urooj",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2603417723",
      "name": "Avisha Das",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2108282211",
      "name": "Jiwoong Jeong",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2933101254",
      "name": "Shubham Trivedi",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2112862374",
      "name": "Bhavik Patel",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2122079672",
      "name": "Imon Banerjee",
      "affiliations": [
        "Mayo Clinic Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170773236",
    "https://openalex.org/W4251110191",
    "https://openalex.org/W4386026291",
    "https://openalex.org/W4382631838",
    "https://openalex.org/W3096151026",
    "https://openalex.org/W3182384093",
    "https://openalex.org/W4361859960",
    "https://openalex.org/W1963771093",
    "https://openalex.org/W4323565440",
    "https://openalex.org/W4385620111",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4386117070",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W4281557260"
  ],
  "abstract": "Abstract In this work, we present our strategy for developing domain-specific large language models which cover the vocabulary of the target domain and train on reliable sources of clinical information. Prostate cancer was chosen as a use-case for this study. We collected more than 1.8 million clinical notes and radiology and pathology reports for 15341 patients treated for prostate cancer in Mayo Clinic across three sites and outpatient clinics. In addition to domain-specific training data, we built domain-specific tokenizers and devised knowledge-guided training strategies for LLM development. During the self-supervised training, LLM was forced to predict domain-specific information by marking clinical terms using UMLS parser. We evaluated the model for downstream tasks of clinical information prediction and question answering using quantitative and user evaluation study to measure the accuracy, reliability and information completeness. We compared the domain-specific model against similarly sized general purpose model GPT-2 and a three-times larger domain specialized model. i.e., BioGPT. Our model outperformed GPT-2 on both tasks by a wide margin. Our model was also able to outperform BioGPT on clinical information prediction tasks and showed some advantages over BioGPT in question-answering tasks.",
  "full_text": "Domain-specific LLM Development and Evaluation – A Case-study for\nProstate Cancer\nAmara Tariq, Ph.D.1, Man Luo, PhD.1, Aisha Urooj, Ph.D.1, Avisha Das, Ph.D.1, Jiwoong\nJeong, M.S.1, Shubham Trivedi, B.S.1, Bhavik Patel, M.D., M.B.A.1, Imon Banerjee, Ph.D.1\n1Mayo Clinic Arizona\nAbstract\nIn this work, we present our strategy for developing domain-specific large language models which cover the\nvocabulary ofthetargetdomainandtrainonreliablesourcesofclinicalinformation.Prostatecancerwaschosenas\na use-case for this study. We collected more than 1.8 millionclinicalnotesandradiologyandpathologyreportsfor\n15341 patients treated for prostate cancer in Mayo Clinic across three sites and outpatient clinics. In addition to\ndomain-specific trainingdata,webuiltdomain-specifictokenizersanddevisedknowledge-guidedtrainingstrategies\nfor LLM development. During the self-supervised training, LLM was forced to predictdomain-specificinformation\nby marking clinical terms using UMLS parser. Weevaluatedthemodelfordownstreamtasksofclinicalinformation\nprediction and question answering using quantitative and userevaluationstudytomeasuretheaccuracy,reliability\nand information completeness. We compared the domain-specific model against similarly sized general purpose\nmodel GPT-2 and a three-times larger domain specialized model. i.e., BioGPT. OurmodeloutperformedGPT-2on\nboth tasks by a wide margin. Our model was also able to outperform BioGPT on clinical information prediction\ntasksandshowedsomeadvantagesoverBioGPTinquestion-answeringtasks.\nIntroduction\nProstate cancer is the most commonly diagnosed non-skin cancer among men in the United States with a 5-year\nsurvival rate of about 100% for local or regional cancer1,2. However, the diagnosis of prostate cancer is often\nfollowed by immediate decline in mental and physical health 3,4. Limited knowledge regarding the disease,\nembarrassment around physicalexaminationsuchasrectalexamanddiscussionofsexualsymptomswithhealthcare\nprofessionals, potentially exacerbated depending on the provider gender5,6, are known causes of poor outcome\namong older patients and/or those belonging ethnic minorities7–9. These uncertainties and anxietyrelatedtosexual\nsymptoms could lead patients to search for information through online search engines in which the patient may\nend-up with either incomplete, contradictory, misleading, and/or inaccurateinformation10.This,inturn,couldcause\nfurtherdelayintreatmentandpooroutcomes.\nDuring diagnosis, treatment planning, and post-treatment diseasemanagement,vastamountsofclinicalinformation\nis generated andrecordedbythecareteamaboutprostatecancerpatients.Thisisusually infree-textformincluding\nclinical notes and radiology and pathology reports. Current developments in the field of large language modeling\nprovide a unique opportunity to process this information for developing reliable agents for interactive knowledge\nsharing in a discreet way11–13. However, we hypothesizethatgeneralpurposeLLMs,likeGPT,areatadisadvantage\nwhen processing this information as they havenotbeentrainedforanyspecificdomain.Theymaygenerategeneric\nresponses to domain-specific questions. They also suffer from hallucination, which can be especially dangerous in\nthe sensitive domains like clinical medicine 13–16. Research effort has been put into domain-specific LLM\ndevelopment such as MedPALM17 (540B), MedAlpaca18 (13B), and BioGPT19 (1.5B). However, these models are\nalso broad andintendedtocovercompletedomainsofmedicineandbiology.Tocoversuchvastfields,thesemodels\nare growing in sizes reaching on the order of 100 billions of parameters. Medicine includes subspecialties, with\ndifferent domain specific knowledge such as oncology, preventive medicine, and emergency medicine where the\nlanguageandtreatmentsdiffer.\nLarge text corpora are generally used to train large models. GPT20 and LLaMA21 havebeentrainedontextcrawled\nfrom the world wide web without verifying the source reliability. Training of specialized LLMs like BioGPT and\nMedPALM have been focused on domain-specific knowledge corpora like millions of scientific abstracts from\nPubMed. However, patient records arearguablythemostcriticalsourceofinformationtocapturethedomainofany\ndisease. The records include several distinct documents including oncology notes, nursing notes, ED reports,\ndischarge summaries, and radiology and pathology reports. While GPT2 and LLaMA, and even medicine-focused\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nBioGPT, have been trained on extremely large amounts of information, their training data cannot include such\npatient-specific documents because of patient privacy requirements set by Health Insurance Portability and\nAccountability Act (HIPAA). Wearguethatthisputsthemataninherentdisadvantageintermsofunderstandingthe\nclinical decision making process and information dissemination to the patients themselves. While research papers\nand experttreatmentguidelinescanimpartknowledgeaboutlatesttreatmentoptionsandtheirgeneralprosandcons,\nclinical notes actually describe how physicians and patients may decide on a treatment option and their following\noutcomes. We argue that inclusion of clinical notes and reports is critical when building a language understanding\nandgenerationtoolforanyparticulardisease.\nWe present our work on development of a domain-specific LLM to overcome theshortcomingsofgeneralpurpose\nLLMs when applied to sensitive domains like medical decision making and information dissemination. We chose\nprostate cancer as our specific usecase.WechosearelativelysmallersizeforourLLM(i..e,124Mparameters)and\ncollected and parsed prostate cancer related patients’ data including clinical notes and radiology and pathology\nreports for its training. We also devised domain-specific self-supervised training techniques to ensure that model\nlearned the nuances of the chosen domain. We evaluatedtheproposedLLMagainstsimilarlysizedgeneralpurpose\nLLM (GPT2-small) as well as larger domain-specific LLM (BioGPT) on two tasks; i) clinical information mask\nprediction, and ii) question-answering. We also measured the performanceinauserstudywith3independentraters\nwhoevaluatedthegeneratedresponsesintermsofcompleteness,correctnessandrelevance.\nMethodology\nIn this section, we describe several steps involved in the development and training of a domain-specific LLM.We\nargue that thesestepsformaframeworkthatcouldbereplicatedtootherdomainslikebreastcancer.Figure1 shows\ntheoverallframeworkfordomain-specificLLMdevelopmentandapplication.\nFigure 1: Domain-specific LLM training and application framework; Highlighted (orange-colored) modules\nindicatedomain-specificmodificationstoLLMdesignframework.\nDatacuration:\nWith the approval of the internal review board (IRB) atMayoClinic,wecollectedclinicalnotesconcerning23,665\npatients treated for prostate cancer atallthreemajorsitesofMayoClinic(Rochetser,Arizona,Florida).Statisticsof\nthese patients are provided in Table 1. This data included not only all clinical notes and radiology and pathology\nreports. We developed rule-based NLP techniques to gather information about cancer characteristics of these\npatients such as lesion size and Gleason score. These patients were also part of an enterprise-wide cancer registry\nwhichrecordedtheirclinicaloutcomeinformation.\nTextParsing:\nClinical data collected for the patients cohort included ~1.8 million clinicalnotes(includingoncologyandallother\nnotes) and 23665 radiology and pathology notes. This data was used fortrainingofourdomain-specificLLMafter\nseveralstepsofparsingandcleaning.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nTable 1: Cohort characteristics for the prostatecancerpatients.Earliestinclusiondateis2017,thusthefollowupis\nlimited.\nCharacteristics Value\nPatients 15,341\nTotalnumberofclinicnotes 1,852,213\nRadiologyreports 23,665\nFollowup mean:2.8+/-1.8years\nmax:6.6years\nAge 66.7+/-8.6years\nRace Caucasian:14,213(93%)\nAfricanAmerican:548(4%)\nAsian:171(1%)\nGleasonScore 6orlower:17%;7orhigher: 65%\nPSA PSA<=10:57%;10<PSA<20:13%;PSA>=20:10%\nTumorsize mean:366.3mm,min:1.0mm,max:990mm\nAnonymization:\nClinical notes contain not only important clinical information about patient-provider interaction, they also contain\nidentifiable information about both patients and providers. While this data was not intended for public release, we\nwanted to ensure patient and provider privacy was not violated even by sharing theweightsofourdomainspecific\nlanguage model. Large language models display behaviors like memorization which leaves open the possibility of\nunmasking identifiable patient information used during the training phase. Therefore, we anonymized all clinical\nnotes prior to their use in training of the proposed LLM usingapretrainedBiLSMmodelwithCRFoutputlayer22.\nInformation like patient and provider names, dates, location were replaced with special tokens like “[NAME]”,\n“[DATE]”, and “[LOC]”. Only imaging finding sections from radiology and pathology information were used for\nLLM training.Thesesectionsaredevoidofanypatientinformation.Electronicsignaturesofprovidersweredropped\nusingregularexpressiontoensureproviderprivacy.\nFiltering:\nTextual dataset cleaning steps included filtering of very short (< 3 words) and very long sentences (>100 words).\nGiven the repetition practice,wealsousedfuzzymatchingtechniquestofilteroutsentenceswhicharetoosimilarto\nother sentences within the clinical notes for one patient. This is done to improve the quality of training data.\nRepetition of standard/templated sentences may skew the behavior of the trained LLM towards generating such\ntemplatedsentencesatinferencetime.\nClinicalInformationMarking:\nWe wanted to ensure that domain-specific LLM was focused on cancer treatment, side-effects, outcomes and\nsymptoms related information during thetrainingphase.Toachievethis,wefirstneededtoidentifydomain-specific\ninformation from free-text clinical notes. We used the Unified Medical Language System (UMLS) metathesaurus\nmade available by the National Institute of Health23. UMLS framework is designed to standardize medical\nvocabularies and ontologies. Text parsing tools designed on top of UMLS identify medicalconceptsfromfree-text\nand map them to unique identifiers (CUI). Concepts are categorizedinto127“semtypes”.Weselected36semtypes\nrelevant to treatment, symptoms, side effects, clinical status andanatomicaldetails.Filteredtextfromclinicalnotes\nwas divided into sentences and passed through UMLS parser one by one and relevant medical entities and their\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nlocation in the sentences were stored. We used a python parser for UMLS designed on a type of medical named\nentity extractor for parsing24. Our processing indicated that 84% of the sentences from clinical notes included\nimportant medical concepts. Medical concepts have varying distribution in the data with 33% concepts with one\noccurrence and 0.3% with greater than 10000 occurrences. A total of 193,936 concepts were identified. We argue\nthat extremely common terms (>10000) may act as stop words such that they have no distinctive capabilities to\nestablish differences between different text samples. For example, the concept “prostate cancer”, , “lesion”,\n“prostate specific antigen”, “elevated PSA” would have little distinctive capabilities within a corpus of allprostate\ncancerpatients.\nDomain-specificTokenizer:\nModern LLMs rely onword-piecetokenizersthatmeetthechallengeofout-of-vocabularytokenbychoppingupthe\nword into smaller foundational pieces such as “carcinoma” may be chopped into “car_” and “_cinoma”. However,\nwe argue that chopping up important domain specific terms into multiple tokens may hinder model’s focus on\nlearning about the whole clinical termsandtheircontext. Apassagefullofrelevantmedicalinformationcontaining\nN words may be divided in M tokens when M>>N. In extreme cases, M may exceed the length of the context\nwindow causing the model to ignore some information.Ontheotherhand,adomain-specifictokenizermaybeable\nto preserve boundaries of medical concepts and entities, thusshorteningthelengthoftherelevanttextandallowing\nfor better attention within the textual context. Therefore,wedecidedtotrainadomain-specificwordpiecetokenizer\nwith vocab size of 50 thousand which wassimilarinsizetopopulartokenizerssuchGPT-2tokenizer(vocabsizeof\n50257).Thetokenizerwastrainedonfilteredclinicaltextasdescribedintheprevioussection.\nDomain-specificLLM\nWe based the architecture of our LLM on popular12-layermaskedself-attentiondecoderbasedarchitecturesimilar\nto the one used by GPT-220. The model was composed of about 124M trainable parameters. LLM training on\nfree-text in self-supervised fashion is a critical component of LLM development. Simple language based\nself-supervision pre-training tasks likethepredictionofthenexttokenseemenoughforgeneralpurposemodelslike\nGPT to gain fundamental understanding of natural language. We built upon this line of research by introducing\ndomain-specificself-supervisioninadditiontogenericself-supervision.OurLLMwastrainedintwophases.\nPhaseI-Generalpurposelanguageunderstanding\nUnder this phase, our LLM was trained on free text data of prostate cancer patients including clinical notes and\nradiology and pathology reports for the taskofnexttokenprediction.Tokensequencesforfree-textweregenerated\nusingourdomain-specifictokenizer.WenamedthemodeltrainedunderthistaskonlyasPCa-LLM.\nPhaseII-Domainspecificlanguageunderstanding\nIn phase II of training, we aimed for the model to learn clinical language specific to prostate cancer. As described\nearlier, we marked clinical terms in free-text data of prostate cancer patients using UMLS parser. In this phase of\ntraining,weusedmaskedclinicaltokenpredictionasaself-supervisedtrainingtask.\nEach text snippet may contain more than one clinical term. Terms were chosen with 50% probability formasking.\nNote that one term may be split into multiple tokens. Inourtrainingscheme,alltokensbelongingtothattermwere\nmaskedandthemodelwastrainedtopredictmaskedtokens.\nFor example, the sentence “common side effects of hormone therapy may include shrinkage of testicles” included\nclinical terms of “hormone therapy” and “shrinkage of testicles”. If “hormone therapy” was selected randomly for\nmarking, all tokens belonging to it ({“hormone”, “therapy”}) would be masked. In this case, the model received\n“common side effects of <mask> <mask> may include shrinkage of testicles” and was tasked with predicting\n“hormone”and“therapy”.\nWe argue that this training scheme allowed the domain-specific LLM to focus on domain-specific language in\naddition to learning correlation between treatments, side effects, symptoms, and cancer characteristics. Such\ncorrelations can be essential for domain specific downstream tasks like patient education or treatment\nrecommendation. For such downstream tasks, the model must be able to predict clinical terms using the context\nprovided in the input text. This phaseoftrainingessentiallytaughtthemodeltodothesame.Wecallthemodelthat\nunderwentbothphasesoftrainingPCa-MLM.\nEvaluationTasks\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nAfter training, we evaluated the model on two downstream tasks; i) masked clinical informationprediction,andii)\nquestion-answering.\nMaskedclinicalinformationprediction\nThis task was similar to phase II of training. We randomly selected clinical terms and masked them in a set of\nheld-out sentences (text not used during the training phase). The model was tasked with retrieving masked terms\nfrom the vocabulary of its tokenizer. We evaluated themodelthroughanevaluationmetricforinformationretrieval\nsystems, i.e., recall@K where K was varied between 1 and 10, based on ranked retrieval results generated by the\nmodelprobability.\nClinicalquestion-answering\nOne of the most valuable downstream applications for any LLM can be a question-answering framework that can\nallow the users to interact with the LLM in an interactive and flexible manner. With this in mind, we chose\nquestion-answeringasthesecondevaluationtask.\nWe manually curated questions from treatment guidelines set by the American Cancer Society for prostate cancer.\nWe browsed through each section of the guidelines and defined question-answer pairs of the following broad\ncategories.\n● Treatmentpathrecommendedforcanceratdifferentstages/riskcategory\n● Treatment path recommended for cancer at different stages/risk category under certain conditions (older\npatient,patientsinpoorhealth,patientswithcertaincomorbidities)\n● Sideeffectsoftreatment\n● Longtermrisksoftreatment\n● Medicationusedintreatment\n● Comparativeprosandconsofdifferenttreatment\nAbout 300 question answerpairsweremanuallycurated.Wethenemployedopen-sourcegeneralpurposeLLM,i.e.,\nLLaMA (13B)21 within a generative framework to paraphrase each question 4 different ways. After filtering for\nnonsensical paraphrasing such as empty strings, we were left with 1,222 QA pairs. Randomly selected unique 35\nQApairswerekeptinaholdoutsetforevaluationpurposesafterdroppingparaphrasesofthesamequestions.\nResults\nIn this section, we describe the results of evaluative experiments.Weselectedtwomodelsascomparativebaselines\nfor our model, i.e., GPT2-small and BioGPT.SelectedversionofGPT2wassimilarlysizedasourmodelbutlacked\nthe advantages of domain-specific vocabulary, training data, and training tasks. BioGPT was about three times\nbigger and was designed to cover a vast domain of medicineasawhole,andthus,haddomain-specificvocabulary.\nComparison with this model was meant to establish benefits of fine-grained domain definition (cancer vs. generic\nmedicine).\nFigure2:Distributionofclinicalconceptsacrossnumberoftokensformedbythreedifferenttokenizers\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nDomainvocabularyCoverage\nWe applied general purpose GPT-2 tokenizer, medicine focused BioGPT tokenizer, and our tokenizer on clinical\nterms extracted by UMLS parser. GPT2 tokenizer tended to chop up medicalconceptslike“prostatectomy”(GPT2\ntokenizer:2tokens,Ourtokenizer:1token),“diabetesmellitus” (GPT2tokenizer:4tokens,Ourtokenizer:2token),\netc. BioGPT tokenizer showed betterclinicalvocabularycoverageforupto3tokensthanourtokenizer.However,it\nstill chopped prostate cancer related words like “genitourinary” (GPT2:5tokens,BioGPT:5tokens,Ours:1token)\nand “erectile dysfunction” (GPT2: 5 tokens, BioGPT: 3 tokens, Ours: 2 token). Figure 2 shows distribution of\nmedical concepts chopped into token sets of varying length by the three different tokenizers. Even though GPT2\ncoverage is minimal for the clinical terms with lowest number of tokens <=1, BioGPT andourmodelhavesimilar\ncoverageofthemedicalterms.\nMaskedclinicalinformationprediction\nFor masked clinical information retrieval, each model produced a probability estimate for every tokens in its\nvocabulary for prediction as replacement for the masked token. Hence, a ranked list of all tokens in vocabulary\n(sorted in descendingorderofprobabilityestimate)wasavailableforeachmaskedtoken.Eachmodelwasevaluated\nto see if the correct token was found in top K tokens for each masked token (recall@K). The performance was\naggregated for all masked tokens (Table 2). We evaluated two phases of our model, i.e.,Phase I:PCa-LLM and\nPhase II:PCa-MLM. Both versions significantly outperformed both comparativemodelswithsignificantmarginat\neachK,evenwhenthevocabularycoverageforBioGPTissimilartothedomainspecificmodel.\nQuestion-answering\nFor evaluation of the question-answering task, we relied on a user study where 3 independent users were asked to\nevaluatetheresponsesofeachmodelonthebasisofthreecriteria.\nCorrectness:if the generated response was correct according to theclinicalguidelines;theresponsedidnothaveto\nbecompleteorcoverallaspectofgroundtruthanswer;\nCompleteness:if thegeneratedresponsewascomplete,coveredallaspectsofthegroundtruthanswer;\nRelevance: if the generatedresponsewasrelevanttothequestion;responsemaybeincorrectorincompletebasedon\ntheclinicalguideline;\nWhile correctnessandcompletenessevaluatetheaccuracyoftheresponse,thethirdcriteriawasintroducedtoassess\nthe model's comprehension of the question. Models, especially general purpose models - GPT2, may generate\ngenericresponsesiftheyareunabletounderstandthegranularityofthequestion.\nAfter de-identification of the model names, all users were asked to provide 1/0 response for each question for all\ncriteria; 1 indicated that the criteria was met (complete, correct or relevant) while 0 indicated that the generated\nresponse did not meet the criteria (incomplete, incorrect, or irrelevant). Responses in each category by every user\nwere summed over 35 questions. By aggregate opinion of users (median scores), our model outperformed both\ncomparative models. Table 3 shows the results from our user study where the median value for all the criteria are\nrated higherforourmodel. User3considersBioGPTresponsesratedhigherforcorrectnessandcompletenesswhile\nstill rating our model higher for relevance based on the generated responses being more relevant for the prostate\ncancer domain. Higher numbers indicate better performance. Table 4 shows sample answers generated by allthree\nmodels.\nTable2:ClinicalinformationretrievalperformancefordifferentLLMsandproposeddomainspecificmodel.PhaseI\ntraining-PCa-LLMandphaseIItrainingPCa-MLM.\nModel Recall@10 Recall@5 Recall@1\nGPT-2 0.017 0.010 ~0.000\nBioGPT 0.017 0.010 0.003\nPCa-LLM 0.228 0.173 0.076\nPCa-MLM 0.305 0.239 0.111\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nTable 3: User evaluation study for question-answering performance of different LLMs. Represents the count and\n(%).\nModel Criterion user 1 user 2 user 3 Median\nGPT2 Correctness 16(46%) 7(20%) 4(11%) 7(20%)\nCompleteness 12(34%) 6(17%) 3(9%) 6(17%)\nRelevance 31(89%) 17(49%) 9(26%) 17(49%)\nBioGPT Correctness 23(66%) 21(60%) 23 (66%) 23(66%)\nCompleteness 14(40%) 20(57%) 14 (40%) 14(40%)\nRelevance 35(100%) 26(74%) 27(77%) 27(77%)\nPCa-MLM Correctness 27 (77%) 27 (77%) 18(51%) 27 (77%)\nCompleteness 14 (40%) 23 (66%) 11(31%) 14 (40%)\nRelevance 35 (100%) 32 (91%) 29 (83%) 32 (91%)\nTable4:SampleanswersgeneratedbydifferentLLMsforthesamequestionsrelatedtoprostatecancer.\nQuestion 1 What treatment is recommended if the PSA level is greater than 20 and prostate\ncancer with a Gleason score of 8 to 10 is found in lymph nodes?\nGroundtruthAnswer hormonetherapy,possiblyalongwithexternalradiation\nGPT-2response itmeansthatwhenpeoplearegivendrugsthataresimilartowhattheyaregivenin\nclinicaltrials,\nBioGPTresponse hormonetherapy,possiblyalongwithexternalradiationtherapy,mayberecommended.\nThisisatypeoffocaltherapy\nPCaMLMresponse hormonetherapy,possiblyalongwithexternalradiationtherapy\nQuestion 2 What is ablative treatment?\nGroundtruthAnswer heatorcoldorothermethodstodestroy(ablate)prostatetissue,ratherthanremovingit\nwithsurgeryortreatingitwithradiation.Methodsincludehighintensityfocused\nultrasound(HIFU)andcryotherapy.\nGPT-2response surgery(TURP)mayberecommendedtorelievesymptomslikebleedingandurinary\nobstruction\nBioGPTresponse Cryotherapy(alsocalledcryosurgeryorcryoablation)istheuseofverycoldtemperatures\ntofreeze\nPCaMLMresponse HIFUuseshighlyfocusedultrasoundbeamstoheatanddestroyprostatetissueandshrink\nprostateovertime\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nQuestion 3 What are the side effects of bisphosphonates\nGroundtruthAnswer flu-likesymptomsandboneorjointpain,kidneyproblems,sopatientswithpoorkidney\nfunctionmightnotbeabletobetreatedwiththesemedicines\nGPT-2response CorticosteroidsaredrugsthattargetPD-1,acheckpointproteinonimmunesystemcells\nBioGPTresponse denosumab(Xgeva),abisphosphonate-likezoled\nPCaMLMresponse pain,swellinginthelegsorfeet,nausea,anddiarrhea.Thesedrugscanalsocausekidney\ndamage.\nQuestion 4 What are common side effects of chemotherapy drugs?\nGroundtruthAnswer Hairloss,Mouthsores,Lossofappetite,Nauseaandvomiting,Diarrhea,Increased\nchanceofinfections(fromhavingtoofewwhitebloodcells),Easybruisingorbleeding\n(fromhavingtoofewbloodplatelets),Fatigue(fromhavingtoofewredbloodcells)\nGPT-2response doctormightrecommendexternalradiationtherapytotheprostatebed(theareawherethe\nprostatewasbeforeitwasremoved),\nBioGPTresponse allergicreaction,nervedamage,leukemia,andincreasedriskofbloodclotsinthelegsor\nlungs.\nPCaMLMresponse fatigue,nausea,vomiting,anddiarrhea.\nQuestion 5 What are common side effects of brachytherapy?\nGroundtruthAnswer bowelproblems,urinaryproblems,erectionproblems,\nGPT-2response nausea,diarrhea,andfeelingweakortired.\nBioGPTresponse (1)bowelproblems,(2)urinaryproblems,(3)erectionproblems\nPCaMLMresponse Paininthetreatmentarea,Bloodintheurine,Anincreasedriskofaurinarytractinfection\n(UTI)\nDiscussion\nEven though general purpose huge LLMs have demonstratedamazingcapabilitiestogeneralizetoawidevarietyof\ndomains as zero-shot frameworks, their performance may still suffer in terms of relevance or specificity to a\nsensitive domain and may “hallucinate” while being forced to generate critical information14,16,25. Trainingofthese\nmodels require humongous text corpora usually curated from publicly available text contextavailableontheworld\nwide web. Privacy concerns make it impossible to train such models in private data which may be critical for\nmedicine related fields. We present a framework fordesign and developmentofdomain-specificLLMwithprostate\ncancer chosen as a use-case. Our framework includes domain-specific customization steps for tokenizer design,\ndata curation, and self-supervised two-phases training. We argue that development of domain-specific LLMs\nprovides a systematic solution to the challenges faced in terms of deployment of huge general purpose LLMs in\nsensitive domains. While some research hasbeendonetospecializeLLMsforsensitivedomainslikemedicineand\nbiology, this line of research usually focused on curation of large domain-specific datasets17,19. BioGPT and\nMedPALM are two popular biomedical LLMs developed to support this line of research. These LLMs still cover\nvast fields of medicine and biology and their training has been focused on public datasets. These LLMs have\nabsorbed the knowledge of vast sources of medicine related information but have had little exposure to the real\nclinical data and thus patient-specific treatment decisions. In comparison, our framework focuses on training the\nmodel on patient EHR data collected byhealthcareinstitutionsincludingclinicalnotesandradiologyandpathology\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \nreports. Thus, themodelstrainedunderourframeworkwillbeexposedtouniquecharacteristicsofpatient-physician\nexchangefortheparticulartargeteddomainandabletolearnthenuanceofthetreatmentmanagement.\nWe trained a small sized LLM (124M parameters) for the particular use case of prostate cancer which not only\noutperformed similarly sized general purpose LLM (GPT-2 small) but also outperformed the three times larger\nspecialized LLM (BioGPT) for two downstream tasks. Specialized tokenizer allows the LLM to understand and\nfocus on domain-specific terms instead of chopping them up into small generic tokens (see Figure 2). GPT2\ntokenizer chopsupclinicaltermsinalargernumberoftokenswhileourspecializedtokenizerandBioGPTtokenizer\ncan comprehend most of the clinical terms as a whole.Asthesetokenizersweredesignedforthefieldsofmedicine\nand biology, it tends to describe medicine related terms in fewer tokens. It covers more clinical terms within 3\ntokens splitting than or tokenizer. However, clinical terms which are more specific to prostate cancer(i.e., found\nmore commonly in prostate cancer related clinical text) are better handledbyourtokenizerthanBioGPTtokenizer.\nThese terms include “erectile dysfunction”, “primary malignant neoplasm”, “prostatic hypertrophy”,\n“cystolitholapaxy”, “cystourethroscopy”, and others. Inaddition,severalprostatecancerrelateddrugnamesarepart\nofourspecializedvocabularyincluding“abiraterone” and“docetaxel”.\nFor the task of masked clinical term prediction, it is intuitive that PCa-MLM (phase I and II of training)\noutperformed PCa-LLM (phase I training only) as phase II training was performed using a very similar\nself-supervision task. However, even PCa-LLM outperformed GPT-2 andBioGPTbywidemarginswhichhasbeen\ntrained using generic self-supervision tasks of next token prediction. This speaks to the benefits of specialized\ndomain-specific tokenizers. BioGPT tokenizer displayed somewhat better coverage of generic clinical vocabulary\nthan our tokenizer (as shown in Figure 2) but the model still performed extremely poorly for prediction of\ndomain-specific(prostatecancerrelated)clinicalinformationretrieval.\nOne of the main applications of our specialized LLM can be patienteducationasaninteractivequestionanswering\nframework. Therefore, we tested the performance of the model on a question-answering task. Note that these\nquestions were curated from the website of theAmerican Cancer Society. Since this data ispubliclyavailable,itis\nlikely to be included in the training data of GPT2 and BioGPT’strainingcorpus.Ontheotherhand,ourmodelwas\nonly trained on clinical data generated within the hospital. It was never trained explicitly onthetextualguidelines.\nDespite this, our model outperformed GPT2 by a wide margin on all three evaluation criteria. Table4showssome\ninteresting examples of the response. It is clear that GPT2 often generated generic or irrelevant responses. Even\nwhen answering questions about side effects (Question 5), it described generic side effects like “feeling weak and\ntired” which was not very specific to the treatment in question. BioGPT does not suffer from this problem. Its\nanswers were mostly related to the question even when it failed to produce the correct answer. For example, it\ngenerated names of bisphosphonatedrugswhenthequestionwasaboutsideeffectsofbisphosphonates(question3).\nIn answer toquestion1,itgeneratedatreatmentoptionwhichisavailableforcancer(focaltherapy)butwasnotpart\nof the ground truth answer. PCaMLM and BioGPT responses differed from each other for question 2 and 4, but\nwerestillrelevantandsomewhatappropriateforthequestion.\nOur experiments clearly establish the benefits of developing and employing domain-specific LLMs for sensitive\ndomains like medicine and healthcare,especiallyforbuildinginteractiveknowledgedisseminationtools.Weplanto\nexpand upon this line of research bydevelopingpatientandphysicianfacingchatbotsontopofourdomainspecific\nLLMfortheusecaseofprostatecancer.Whileitwillbechallengingtocuratetrainingdatasets,i.e.,question-answer\npairs, manually, we plan to build upon automatic question-answer generation frameworks26. This line of research\nwillprovideasolutiontoreliabilityandspecificityissuesfacedbygeneralpurposechatbotlikeChatGPTinthefield\nofmedicine.\nReferences\n1. BrawleyOW.TrendsinprostatecancerintheUnitedStates. JournaloftheNationalCancerInstitute\nMonographs.2012;2012(45):152-156.\n2. HenleySJ,WardEM,ScottS,etal.Annualreporttothenationonthestatusofcancer,partI:Nationalcancer\nstatistics.Cancer.2020;126(10):2225-2249.\n3. PetrosNG,Alvarsson-HjortJ,HadlaczkyG,etal.PredictorsoftheUseofaMentalHealth–FocusedeHealth\nSysteminPatientsWithBreastandProstateCancer:BayesianStructuralEquationModelingAnalysisofa\nProspectiveStudy. JMIRcancer.2023;9:e49775.\n4. CrumpC,StattinP,BrooksJD,etal.Risksofalcoholanddrugusedisordersinprostatecancersurvivors:a\nnationalcohortstudy. JNCICancerSpectrum.2023;7(4):pkad046.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint \n5. FinkM,KleinK,SayersK,etal.Objectivedatarevealsgenderpreferencesforpatients’primarycarephysician.\nJournalofPrimaryCare&CommunityHealth.2020;11:2150132720967221.\n6. SteinkohlF,LugerAK,GruberL,etal.Acceptanceoffemaleurologistsamongpatientswithsuspectedprostate\ndisease.TranslationalAndrologyandUrology.2021;10(7):2938.\n7. RaviP,KarakiewiczPI,RoghmannF,etal.Mentalhealthoutcomesinelderlymenwithprostatecancer.In:Vol\n32.Elsevier;2014:1333-1340.\n8. ShawB,WalterFM,HamiltonW,MartinsT.Symptomappraisalandhelpseekinginmaleswithsymptomsof\npossibleprostatecancer:aqualitativestudywithanethnicallydiversesampleinLondon. BritishJournalof\nGeneralPractice.2023;73(732):e502-e510.\n9. WrayRJ,McClureS,VijaykumarS,etal.ChangingtheconversationaboutprostatecanceramongAfrican\nAmericans:resultsofformativeresearch. Ethnicity&health.2009;14(1):27-43.\n10.MuziiB,DiBelloF,CarraturoF,etal.MentalHealthofProstateCancerPatients:ContentReviewon\nYouTubeTM.InternationalJournalofEnvironmentalResearchandPublicHealth.2023;20(6):4721.\n11.HarrisE.Largelanguagemodelsanswermedicalquestionsaccurately,butcan’tmatchclinicians’knowledge.\nJAMA.Publishedonline2023.\n12.OmiyeJA,GuiH,RezaeiSJ,ZouJ,DaneshjouR.LargeLanguageModelsinMedicine:ThePotentialsand\nPitfalls:ANarrativeReview. AnnalsofInternalMedicine.2024;177(2):210-220.\n13.ThirunavukarasuAJ,TingDSJ,ElangovanK,GutierrezL,TanTF,TingDSW.Largelanguagemodelsin\nmedicine.Naturemedicine.2023;29(8):1930-1940.\n14.AlkaissiH,McFarlaneSI.ArtificialhallucinationsinChatGPT:implicationsinscientificwriting. Cureus.\n2023;15(2).\n15.ChenS,KannBH,FooteMB,etal.Useofartificialintelligencechatbotsforcancertreatmentinformation.\nJAMAoncology.2023;9(10):1459-1462.\n16.SallamM.ChatGPTutilityinhealthcareeducation,research,andpractice:systematicreviewonthepromising\nperspectivesandvalidconcerns.In:Vol11.MDPI;2023:887.\n17.SinghalK,AziziS,TuT,etal.Largelanguagemodelsencodeclinicalknowledge. Nature.\n2023;620(7972):172-180.\n18.HanT,AdamsLC,PapaioannouJM,etal.MedAlpaca--anopen-sourcecollectionofmedicalconversationalAI\nmodelsandtrainingdata. arXivpreprintarXiv:230408247.Publishedonline2023.\n19.LuoR,SunL,XiaY,etal.BioGPT:generativepre-trainedtransformerforbiomedicaltextgenerationand\nmining.Briefingsinbioinformatics.2022;23(6):bbac409.\n20.RadfordA,WuJ,ChildR,LuanD,AmodeiD,SutskeverI.Languagemodelsareunsupervisedmultitask\nlearners.OpenAIblog.2019;1(8):9.\n21.TouvronH,LavrilT,IzacardG,etal.Llama:Openandefficientfoundationlanguagemodels. arXivpreprint\narXiv:230213971.Publishedonline2023.\n22.TrienesJ,TrieschniggD,SeifertC,HiemstraD.Comparingrule-based,feature-basedanddeepneuralmethods\nforde-identificationofdutchmedicalrecords. arXivpreprintarXiv:200105714.Publishedonline2020.\n23.BodenreiderO.Theunifiedmedicallanguagesystem(UMLS):integratingbiomedicalterminology. Nucleic\nacidsresearch.2004;32(suppl_1):D267-D270.\n24.SoldainiL,GoharianN.Quickumls:afast,unsupervisedapproachformedicalconceptextraction.In:;2016:1-4.\n25.KojimaT,GuSS,ReidM,MatsuoY,IwasawaY.Largelanguagemodelsarezero-shotreasoners. Advancesin\nneuralinformationprocessingsystems.2022;35:22199-22213.\n26.LuoM,MitraA,GokhaleT,BaralC.Improvingbiomedicalinformationretrievalwithneuralretrievers.In:Vol\n36.;2022:11038-11046.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 19, 2024. ; https://doi.org/10.1101/2024.03.15.24304362doi: medRxiv preprint ",
  "topic": "Prostate cancer",
  "concepts": [
    {
      "name": "Prostate cancer",
      "score": 0.6653075814247131
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5937083959579468
    },
    {
      "name": "Prostate",
      "score": 0.5131295323371887
    },
    {
      "name": "Cancer",
      "score": 0.38919597864151
    },
    {
      "name": "Medicine",
      "score": 0.3742264211177826
    },
    {
      "name": "Internal medicine",
      "score": 0.2021178901195526
    },
    {
      "name": "Mathematics",
      "score": 0.15522560477256775
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210127938",
      "name": "Mayo Clinic Hospital",
      "country": "US"
    }
  ],
  "cited_by": 7
}