{
  "title": "Multi-Label Multimodal Emotion Recognition With Transformer-Based Fusion and Emotion-Level Representation Learning",
  "url": "https://openalex.org/W4321022102",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4222642045",
      "name": "Hoai-Duy Le",
      "affiliations": [
        "Chonnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2630003892",
      "name": "Guee Sang Lee",
      "affiliations": [
        "Chonnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2107287756",
      "name": "Soo Hyung Kim",
      "affiliations": [
        "Chonnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2103556748",
      "name": "Seungwon Kim",
      "affiliations": [
        "Chonnam National University"
      ]
    },
    {
      "id": "https://openalex.org/A2484437051",
      "name": "Hyung Jeong Yang",
      "affiliations": [
        "Chonnam National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2885719698",
    "https://openalex.org/W4286593296",
    "https://openalex.org/W2548844710",
    "https://openalex.org/W2991485489",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W6797613833",
    "https://openalex.org/W6793736971",
    "https://openalex.org/W4312384316",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W6783497617",
    "https://openalex.org/W3093051361",
    "https://openalex.org/W3206529771",
    "https://openalex.org/W6801230018",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W3007282427",
    "https://openalex.org/W6792098614",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3206996142",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3048939150",
    "https://openalex.org/W4312653797",
    "https://openalex.org/W2997136715",
    "https://openalex.org/W6798617289",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W2769905613",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W3184087575",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3099638501",
    "https://openalex.org/W3087434251"
  ],
  "abstract": "Emotion recognition has been an active research area for a long time. Recently, multimodal emotion recognition from video data has grown in importance with the explosion of video content due to the emergence of short video social media platforms. Effectively incorporating information from multiple modalities in video data to learn robust multimodal representation for improving recognition model performance is still the primary challenge for researchers. In this context, transformer architectures have been widely used and have significantly improved multimodal deep learning and representation learning. Inspired by this, we propose a transformer-based fusion and representation learning method to fuse and enrich multimodal features from raw videos for the task of multi-label video emotion recognition. Specifically, our method takes raw video frames, audio signals, and text subtitles as inputs and passes information from these multiple modalities through a unified transformer architecture for learning a joint multimodal representation. Moreover, we use the label-level representation approach to deal with the multi-label classification task and enhance the model performance. We conduct experiments on two benchmark datasets: Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Carnegie Mellon University Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) to evaluate our proposed method. The experimental results demonstrate that the proposed method outperforms other strong baselines and existing approaches for multi-label video emotion recognition.",
  "full_text": "Received 22 December 2022, accepted 3 February 2023, date of publication 13 February 2023, date of current version 16 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3244390\nMulti-Label Multimodal Emotion Recognition\nWith Transformer-Based Fusion and\nEmotion-Level Representation Learning\nHOAI-DUY LE, GUEE-SANG LEE\n, SOO-HYUNG KIM\n , SEUNGWON KIM,\nAND HYUNG-JEONG YANG\nDepartment of Artificial Intelligence Convergence, Chonnam National University, Gwangju 61186, South Korea\nCorresponding author: Hyung-Jeong Yang (hjyang@jnu.ac.kr)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korea Government (MSIT)\nunder Grant NRF-2020R1A4A1019191; in part by the BK21 Fostering Outstanding Universities for Research (FOUR) funded by the\nMinistry of Education (MOE), South Korea, and in part by NRF.\nABSTRACT Emotion recognition has been an active research area for a long time. Recently, multimodal\nemotion recognition from video data has grown in importance with the explosion of video content due\nto the emergence of short video social media platforms. Effectively incorporating information from mul-\ntiple modalities in video data to learn robust multimodal representation for improving recognition model\nperformance is still the primary challenge for researchers. In this context, transformer architectures have\nbeen widely used and have significantly improved multimodal deep learning and representation learning.\nInspired by this, we propose a transformer-based fusion and representation learning method to fuse and enrich\nmultimodal features from raw videos for the task of multi-label video emotion recognition. Specifically, our\nmethod takes raw video frames, audio signals, and text subtitles as inputs and passes information from these\nmultiple modalities through a unified transformer architecture for learning a joint multimodal representation.\nMoreover, we use the label-level representation approach to deal with the multi-label classification task and\nenhance the model performance. We conduct experiments on two benchmark datasets: Interactive Emotional\nDyadic Motion Capture (IEMOCAP) and Carnegie Mellon University Multimodal Opinion Sentiment and\nEmotion Intensity (CMU-MOSEI) to evaluate our proposed method. The experimental results demonstrate\nthat the proposed method outperforms other strong baselines and existing approaches for multi-label video\nemotion recognition.\nINDEX TERMS Multimodal fusion, multi-label video emotion recognition, transformers.\nI. INTRODUCTION\nOver the past few years, there has been an explosion in short\nvideo content from the global rise in short video platforms\nsuch as TikTok, YouTube Shorts, and Facebook Reels with\nthe increasing popularity of mobile devices. Online videos\nhave become the predominant data type that users use to share\ntheir activities and interact with each other in cyberspace.\nInevitably, the scientific and industrial communities have\ngiven much attention to video data analysis, especially in\nvideo sentiment analysis and emotion recognition [1], [2]\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Muhammad Asif\n.\nbecause of its applications in diverse fields. People represent\ntheir emotions through multiple modalities. Naturally, lan-\nguage, voice, and facial expression are the primary methods\nby which humans convey their emotions. Moreover, how\npeople combine these methods to express their emotions is\nvery complicated. Under certain circumstances, only relying\non information from a single modality to predict human\nemotions will easily lead to inaccurate predictions.\nLet us give a few examples of this phenomenon. Suppose\na photographer discovers that his luggage bag containing his\ncamera is missing after landing. He then went to the airport\nstaff to declare and expect to receive the luggage back. After\na while of arguing and being given a hard time by the airport\n14742 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 11, 2023\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nstaff, he was informed that someone had stolen his baggage\nand they could not find it. Realizing that he was deliberately\nwasted his time by airport staff to cover up the fact that\nthey did not ensure the safety of his things. He left with an\nangry ‘‘Thank you very much’’. His angry emotion will be\neasily recognized by his facial expressions and tone of voice.\nOn the contrary, if we solely rely on the sentence ‘‘Thank\nyou very much’’, we would obviously think he is grateful.\nAnother example is the case of a woman who has just been\npromoted and had to work in a foreign office for two years.\nShe is sad because she is about to be separated from her\nhusband and daughter. When asked by her husband about the\nduration of the trip, she sadly replied, ‘‘It is for two years’’.\nAnalogously, if we use only the text, we will think that the\nwoman’s emotion is neutral. By combining her voice and\nfacial expression, we determine that she is unhappy. From\nthese examples, we can be aware of synthesizing information\nfrom multiple modalities is crucial to comprehend human\nemotions fully.\nHowever, designing an effective fusion method for dif-\nferent modalities of video data to obtain a robust joint rep-\nresentation is still a challenging research problem. There\nhave been numerous efforts to design appropriate fusion\ntechniques for incorporating multimodal video information.\nEarly approaches merely concatenated high-level features\nfrom all modalities to make a prediction (early fusion) or sum\nall unimodal decisions with learnable weights (late fusion)\nto draw the final inference [3], [4]. These fusion methods\nachieved higher accuracy than unimodal methods, but the\nimprovement was limited because there was no interaction\nbetween modalities during training. With the recent advances\nin deep learning, especially attention mechanisms [5] and\ntransformers [6], later studies were predominantly based on\nthose techniques to explore more sophisticated multimodal\nfusion methods [7], [8], [9], [10]. These studies commonly\nprocess multimodal learning by pairwise or triplet combi-\nnation input rather than from all the multiple signals con-\ncurrently. Nonetheless, it is incompatible with how humans\ncontemporaneously perceive multiple information resources\nfrom the video.\nIn this paper, following the success of the transformers\nin multimodal deep learning, we present a fusion method\nbuilt upon the transformer’s architecture to effectively fuse\nmultimodal features and learn a joint multimodal represen-\ntation from the raw video data for multi-label video emo-\ntion recognition. Specifically, our proposed model takes raw\nvideo frames, audio waveforms, and text transcripts of the\nvideo as inputs. The model then extracts high-level features\nfrom raw data using appropriate deep neural architectures\nfor each modality and later leverages the multi-head atten-\ntion mechanism of the transformers to learn a robust joint\nmultimodal representation from the multimodal features. The\nmulti-head attention mechanism in the transformer scans\nthrough each element of the input sequence to learn a refined\nsequence of features that emphasized important elements\nand faded redundant. We utilize this property in learning\nthe correlation between different modalities from the mul-\ntimodal input sequence. Furthermore, different from exist-\ning transformer-based methods using only the output of the\n‘‘CLS’’ token (classification token) to make predictions,\nwe process the entire output sequence of the transformers\nfor classification. We use the trainable query embeddings\nand the cross-attention in the transformers decoder to learn\nthe emotion-level representations from the joint multimodal\nfeatures to enhance the multi-label emotion recognition\nperformance.\nWe evaluate our proposed method on two standard bench-\nmark multimodal emotion recognition datasets: IEMOCAP\n[11] and CMU-MOSEI [12]. The experiments demonstrate\na significant improvement over the strong baseline methods\nwith a gap of +0.2% accuracy and +3.8% F1-score on the\nIEMOCAP, +2.0% weight accuracy and +0.6% F1-score\non the CMU-MOSEI. Overall, the main contributions of our\nstudy are as follows:\n• We propose a simple but effective multimodal fusion\nmodule, which adopts the multi-head attention in the\ntransformer encoder to perform cross-attention and\nsimultaneously integrate informative information at the\ntoken level between multimodal features of video data.\n• We propose a combination of emotion-level embed-\nding over the fused multimodal features to learn the\nemotion-related features from multimodal representa-\ntion. Extensive experiments verify the advantages of this\ncombination in improving performance.\n• We conduct extensive experiments and provide thorough\nablation studies to demonstrate the effectiveness of our\nproposed approach to multimodal learning and how our\nmethod improves the model performance in video emo-\ntion recognition.\nThe rest of the paper is organized as follows. We provide\nan overview of prior related studies in Section II. We present\nthe details of our method in Section III. We then describe the\nextensive experiments and experimental results in Section IV.\nFinally, we conclude the paper in Section V.\nII. RELATED WORK\nIn this section, we present the related work in two parts: mul-\ntimodal emotion recognition and multimodal transformers.\nIn each subsection, we first briefly review the progressive\nexisting works and then discuss the ideas for solving remain-\ning constraints.\nA. MULTIMODAL EMOTION RECOGNITION\nEmotion recognition has been an active research area for\nmany decades. Learning robust representation from multiple\nmodalities has recently become an attractive research direc-\ntion for boosting emotion recognition performance. Mul-\ntimodal emotion recognition aims to integrate information\nfrom multiple signals such as sound, language, and image in\nvideos for recognizing human emotions.\nVOLUME 11, 2023 14743\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nMost of the previous studies take hand-crafted features\nextracted by traditional feature extraction algorithms as\ninput to train the deep neural network. Tsai et al. [7] pro-\nposed Multimodal Transformer (MulT) which uses pair-\nwise transformers to perform bidirectional cross-attention\nbetween visual-textual, visual-audio, and textual-audio\nfor the unaligned multimodal affective recognition task.\nDai et al. [13] explored transferring emotion embeddings\nfrom textual modality to learn visual and acoustic emotion\nembeddings and analyzed adaptation in few-shot learning.\nHazarika et al. [14] concentrated on enriching multimodal\nfeatures before the fusion process by introducing MISA\nwhich learns modality-specific and modality-invariant rep-\nresentations for multimodal sentiment analysis. In con-\ntrast, Han et al. [15] paid attention to constructing a\nfusion scheme for multimodal data. They considered tex-\ntual information as the main modality and then designed\na Transformer-based fusion network to integrate comple-\nmentary information from text-visual and text-audio pairs.\nBesides, Han et al. [16] also presented MultiModal InforMax\n(MMIM) which applies mutual information concepts in fus-\ning multimodal features for multimodal sentiment analysis.\nOther approaches received hand-crafted input features for\nmultimodal sentiment analysis and emotion recognition as\nwell [17], [18], [19].\nHowever, it is generally not sensible to train a deep neural\nnetwork from hand-crafted input features. In this context,\nDai et al. [20] recently indicated the limitations of training\ndeep neural networks from hand-crafted input features. They\nreorganized two benchmark datasets in the multimodal emo-\ntion recognition tasks to make training from raw data become\nfeasible. Inspired by their work, we explore a powerful fusion\nmodule learned from raw video, audio, and text modalities\nand further apply it to the task of video emotion recognition.\nB. MULTIMODAL TRANSFORMERS\nTransformers [6] were originally proposed for the sequence-\nto-sequence machine translation task and have been widely\napplied in many other tasks, such as image classifica-\ntion [21], object detection [22], and audio classification [23].\nRecently, the transformers have been extended to multimodal\ndeep learning and have been proven effective, especially\nin learning from textual, visual, and audio modalities of\nvideo data. Some early works primarily followed co-attention\nlearning strategies for pairwise modalities. Tan et al. [24]\nproposed Learning Cross-Modality Encoder Representations\nfrom Transformers (LXMERT) framework which is a fully\ntransformer-based network to learn the cross-modality rep-\nresentation of images and languages. They constructed a\nself-attention learning block for each modality followed by\na co-attention learning block to finalize the joint image-\ntext representations. Similarly, Cheng et al. [25] described a\nco-attention network but for audiovisual synchronization.\nLater studies tend to use joint learning with modality-\nspecific transformers trained by contrastive losses for\nmultimodal fusion. Akbari et al. [9] presented a convolution-\nfree transformer-based framework to learn multimodal rep-\nresentations from raw video frames, text transcripts, and\naudio waveforms of videos in a self-supervised setting. The\nframework contains separate transformers for each modality\nand has been trained by a combination of contrastive losses\nbetween visual-text and visual-audio pairs. Nagrani et al. [8]\nintroduced a transformers-based approach for fusing audio\nand visual information in the video by adding bottleneck units\nto bridge two sequences of visual and audio features before\ninputting into modality-specific multi-layer transformers.\nShvetsova et al. [10] proposed using shared transformer\nencoders to encode the uni-modal features including text,\nvisual, audio, and pairwise multimodal features comprising\ntext-visual, text-audio, and visual-audio pairs. They then\ntrained the network with combinatorial contrastive losses of\npairwise uni-modality and pairs from uni-modal and pair-\nwise modalities. More recently, Mercea et al. [26] modified\nthe standard transformer to perform temporal and enforce\ncross-attention between visual and audio modalities in the\nzero-shot learning setting for the video classification task.\nIn this work, we focus on textual, visual, and audio multi-\nmodal fusion for video data. Rather than dividing multiple\nmodalities into pairwise or triplet combinations, we syn-\nchronously integrate information from all modalities with a\nunified transformer-based architecture. We assume that the\nmodalities in a video have inseparable relationships and that\nhumans perceive multiple video modalities simultaneously\ninstead of separately in pairs.\nIII. PROPOSED METHOD\nGiven an input video segment V containing multiple modali-\nties including a text transcription, a sequence of video frames,\nan audio waveform signal, and a pre-defined set of K emo-\ntions, multi-label emotion recognition is to predict whether\neach emotion is present in the video. In this section, we pro-\nvide a detailed description of our proposed approach to solve\nthe above problem.\nFigure 1 illustrates the overall architecture of our proposed\nmethod for multi-label multimodal emotion recognition tak-\ning a video clip as an input and outputting the revealed emo-\ntions in the video. The model consists of three modules: the\nfeature extraction module, the multimodal fusion module, and\nthe emotion-level embedding module. First, we employ a fea-\nture extraction module to encode the text transcription, video\nframes, and audio signal inputs into hidden representations.\nThen, in the fusion module, we leverage the multi-head atten-\ntion in transformer encoders to fuse the features of multiple\ninput modalities. After that, we construct the emotion-level\nembedding module using transformer decoders to learn the\nemotion-level representations from the fused multimodal rep-\nresentation. Finally, we apply a fully feed-forward layer fol-\nlowed by a sigmoid activation layer to make predictions and\napply weighted binary cross-entropy loss to train the network.\nThe detail of the proposed method is described in the below\nsubsections.\n14744 VOLUME 11, 2023\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nFIGURE 1. Architecture of our proposed method.It consists of three main modules: (1) a backbone module containing\nthree feature extractors for textual, visual, and acoustic modalities, (2) a Transformer-based fusion module that attends\nand fuses multimodal information, and (3) an emotion-level embedding and classification head module that matches\nthe fused multimodal features to emotion-level representations and outputs the final emotion predictions.\nA. FEATURE EXTRACTION\nThis study focuses on designing multimodal fusion and\nenriching representation learning for video data to improve\nmodel performance. Thus, we use the identical feature extrac-\ntion following the baseline [20] rather than proposing a\nnew extractor. Dai et al. [20] proposed a multimodal model\nlearned from raw video data for multimodal emotion recog-\nnition. First, a feature extractor is employed to extract fea-\ntures from each modality. Then the late fusion approach is\napplied to fuse multimodal information and predict the final\nemotional classes. In detail, the feature extraction module\nleveraged in [20] and our work consists of three modality-\nspecific extractors. We use a pre-trained BERT-based model\nto extract a set of word embeddings from textual modality. For\nvisual (video frames) and acoustic (Mel spectrogram chunks)\nmodalities, we utilize two individual CNN networks (trained\nfrom scratch) as the backbones of each modality. We enable\nfusing multimodal features by including a projection network\ncontaining multiple fully-connected layers followed by a\nnon-linear activation function for each modality to map the\nmultimodal features to the same size. Furthermore, we use\ntransformer encoders to capture the temporal information of\nthe sequence of hidden features from video frames and audio\nspectrograms. Consequently, we obtain three sequences of\nhidden representations from the textual, visual, and acoustic\nmodalities denoted as T ∈ Rnt ×d , I ∈ Rni×d , and A ∈ Rna×d ,\nrespectively. In which, nt , ni, and na are the numbers of words\nin the transcription, the number of sampled video frames, and\nthe number of Mel spectrogram chunks, respectively; d is\nthe size of feature dimensionality. For the textual modality,\nT ∈ Rnt ×d consists of nt word embeddings extracted from the\npre-trained BERT-based model and projected to d size. For\nthe visual and acoustic modalities, I ∈ Rni×d and A ∈ Rna×d\ncontain ni and na feature vectors of size d captured from\nvideos frames and audio spectrogram chunks, respectively.\nB. TRANSFORMER-BASED MULTIMODAL FUSION\nMODULE\nUnlike previous transformer-based multimodal fusion meth-\nods, we propose synchronously fusing features from differ-\nent modalities rather than dividing combinations of possible\npairwise modalities. We first summarize the multi-head atten-\ntion in the transformer [6] and then describe our proposed\nextension to multimodal fusion for video data. Given an input\nsequence S ∈ Rn×d containing n vectors of size d, the\nmulti-head self-attention block in the transformer parallelly\nVOLUME 11, 2023 14745\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nFIGURE 2. Illustration of learning class-level embeddings (Figure 2. b) compared to using the output of the CLS token\n(Figure 2. a) for classification.\nprojects S to multiple sets of three components named query\nQi ∈ Rn×dk , key Ki ∈ Rn×dk , and value Vi ∈ Rn×dv in h\ndifferent subspaces (h is the number of heads, dk = d/h, and\nusually dk = dv). On each of these sets of projected queries,\nkeys, and values, a single attention function is performed as\nfollows:\nAttention(Qi, Ki, Vi) = softmax(QiKT\ni√dk\n)Vi.\nsoftmax(xi) = exp(xi)\n6jexp(xj) (1)\nThe dot product QKT is the form of the similarity measure\nand the Attention(Q, K, V ) is the sum weighted by atten-\ntion weight (softmax score). The final joint representation is\nobtained by averaging all attention head outputs with train-\nable weights:\nMultiHead(Q, K, V ) = Concat(head1, . . . ,headh)W o,\nheadi = Attention(Qi, Ki, Vi), i ∈ 1, . . . ,h,\n(2)\nwhere W o ∈ Rhdv×d represents the learnable parameters.\nA single dot product attention enables the model to scan\nthrough each element in the input sequence and learns which\nelements it should attend to. The multi-head attention enables\nthis process to be executed from different representation sub-\nspaces. In other words, the transformers provide a mecha-\nnism to selectively accumulate information from the entire\ninput sequence with regard to the output. Furthermore, the\nmulti-head attention is enduring with the order of vectors\nin the input sequence. Therefore, it is naturally suitable to\nfuse multimodal information by applying multi-head atten-\ntion over the input sequence which is the order-agnostic\ncombination of features from multiple modalities.\nAfter passing the feature extraction module, we obtain\nthree sequences of hidden representations T ∈ Rnt ×d , I ∈\nRni×d , and A ∈ Rna×d from multiple modalities including\ntext transcription, image frames, and acoustic signals, respec-\ntively. We then concatenate them into a unified sequence of\nmultimodal features added with a classification token ([CLS]\ntoken) at the start and use it as the input sequence for the\nfusion module. We adopt the vanilla transformer [6] encoder\nand stack at Le multiple blocks to construct the multimodal\nfusion module. The standard transformer encoder consists of\na multi-head self-attention layer (MSA), normalization layers\n(Norm), and a position-wise feed-forward network (FFN).\nThe fused multimodal representation Fi ∈ Rns×d (ns =\nnt + ni + na) at block i is calculated as follows:\nFi\ntemp = Norm(MSA(Fi−1) + Fi−1), (3)\nFi = Norm(FFN(Fi\ntemp) + Fi\ntemp), (4)\nin which i ∈ 1, . . . ,Le and F0 = concat(Ecls, T , I, A).\nC. EMOTION-LEVEL EMBEDDING MODULE\nIn contrast to previous transformer-based works which com-\nmonly use the output of the classification token (‘‘[CLS]’’\ntoken) to perform classification with linear layers, we make\nuse of the entire outputted sequence from transformer\nencoders of the fusion module to enrich features for the\nmulti-label emotion recognition task as illustrated in Figure 2.\nRather than learning a unique representation and then using\nit to make predictions for all emotions, we adopt the idea\nof learning multiple embeddings, in which each embedding\nis oriented toward each specific emotion. We leverage the\ncross-attention in the transformer decoder to pool multiple\nemotion-level embeddings for a single video inspired by [22],\n[27], and [28].\n14746 VOLUME 11, 2023\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nThe emotion-level embedding module takes the sequence\nof features from the output of the fusion module F ∈ Rns×d\nas input and generates the emotion-level representation E ∈\nRC×d (C equal to the number of emotional classes and d\nis the feature dimensional size) for the video. First, a set\nof emotion-specific embeddings E0 ∈ RC×d is randomly\ninitialized and used to project the query vector Q. It will\nbe learned during the training process. Simultaneously, the\nsequence of refined multimodal features outputted from the\nfusion module is used to project K and V vectors. The video\nemotion-level representation is then learned using a series of\nNd transformer decoders:\nE†\ni = Norm(Ei−1 + MHA(Ei−1, Ei−1, Ei−1)) (5)\nE††\ni = Norm(E†\ni + MHA(E†\ni , F, F)) (6)\nEi = Norm(E††\ni + FFN(E††\ni )) (7)\nwhere Norm, MHA, and FFN are the normalization layer,\nMulti-Head Attention layer, and feed-forward network,\nrespectively; i ∈ {1, . . . ,Nd }.\nD. OBJECTIVE FUNCTION\nGiven an input video labeled by a multi-hot vector Y =\n[y0, y1, . . . ,yC ], yi ∈ {0,1}, our proposed model outputs\nthe confidence scores of all classes Y = [p0, p1, . . . ,pC ],\npi ∈ [0, 1]. A confidence score is a probability given by\nthe model to represent how confident the model assigns the\ninput to a certain class. The network is trained as end-to-\nend learning with binary cross-entropy loss (BCE) to adapt to\nthe multi-label classification. Because of the class imbalance\nproblem in the datasets, we add weights to the loss of positive\nsamples. The loss for each training sample is formulated as\nfollows:\nL(y, p) =\n∑\n− 1\nC [wi.yi.log(pi) + (1 − yi).log(1 − pi)],\n(8)\nwi = ni\npi\n, (9)\nwhere ni and pi denote the number of negative and positive\nsamples of class i, respectively. For mini-batch learning, the\ntotal loss is the average of all sample losses in the batch.\nIV. EXPERIMENTS\nA. DATASETS AND METRICS\nWe conduct experiments on two benchmarked datasets\nincluding Interactive Emotional Dyadic Motion Capture\n(IEMOCAP) [11] and CMU Multimodal Opinion Sentiment\nand Emotion Intensity (CMU-MOSEI) [12]. We re-use the\nreorganized version of these datasets from Dai et al. work [20]\ninstead of the original version because the input of our method\nis raw video. We also follow the split for training, validation,\nand testing in [20]. Table 1 and table 2 show the statistics of\nboth IEMCAP and CMU-MOSEI datasets.\nTABLE 1. The detailed statistics in IEMOCAP and CMU-MOSEI datasets.\n‘‘t, v, a’’stands for textual, visual, and audio modalities, respectively.\nTABLE 2. The detailed emotional label distribution of IEMOCAP and\nCMU-MOSEI datasets.\n1) IEMOCAP [11]\ncontains 151 recorded dialogue videos. In each video, two\nspeakers have a dyadic conversation with multiple utterances.\nOriginally, the dataset is annotated by nine emotional labels.\nDue to the imbalance problem, [20] preserves only six cat-\negories among them including angry, happy, excited, sad,\nfrustrated, and neutral. The dialogue videos are sliced at the\nutterance level into 7380 sub-clips. Because of the shortage of\nidentifiers for data samples in the original training-validation-\ntesting split from [11] and [20] constructed a new split from\nsliced videos with a 70%-10%-20% ratio for training, valida-\ntion, and testing configuration, respectively.\n2) CMU-MOSEI [12]\nis the largest dataset for multimodal sentiment analysis and\nemotion recognition tasks. The dataset consists of 23,259\nutterance-video segments sampled from 3,837 YouTube\nvideos from 1,000 distinct speakers. It is labeled into six\nemotion categories: happy, sad, angry, fearful, disgusted, and\nsurprised. After cleaning misaligned and mismatched data\nsamples, 20,477 videos remain in the dataset. The new dataset\nsplit is done by following the CMU-MOSEI split for the\nsentiment analysis task.\n3) EVALUATION METRICS\nWe use the standard accuracy and F1-score as evaluation\nmetrics for the IEMOCAP dataset consistent with previous\nworks. For CMU-MOSEI, because of the imbalance of pos-\nitive and negative samples in each emotion class, weighted\naccuracy is used instead of standard accuracy; besides,\nF1-score is also calculated to evaluate the model perfor-\nmance. The F1-score and the weighted accuracy are defined\nas follows:\nF1 = 2 × Precision × Recall\nPrecision + Recall = 2TP\n2TP + FP + FN ,\n(10)\nVOLUME 11, 2023 14747\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nTABLE 3. Results on the IEMOCAP dataset.Comparison with strong baselines and existing methods.∗: excerpted from previous papers,†: reproduced\nfrom open-source code with hyper-parameter provided in the original paper.\nWAcc = TP × N/P + TN\n2N , (11)\nwhere TP (resp. TN) stands for true positive (resp. true\nnegative), FP (resp. FN) denotes false positive (resp. false\nnegative), and N (resp. P) is total negative (resp. positive)\nsamples.\nB. IMPLEMENTATION DETAILS\n1) MODEL SETTING\nWe implement the proposed model with Pytorch [29] frame-\nwork v.1.8.1. To ensure comparability, we follow [20] to\nbuild up the backbone for the feature extraction module.\nIn particular, we use a pre-trained ALBERT-base model [30]\nto extract word embeddings from text transcription. The max\nlength of the text is limited to 50. For all experiments, the\nvideo frames are sampled every 1s and then passed through\na pre-trained MTCNN [31] model to detect and crop face\nregions. The cropped faces are resized to 64 × 64 and used\nas input for the visual backbone. For the acoustic modality,\nthe audio signals are converted to the Mel spectrogram form\nand sliced into a sequence of chunks with a time window of\n400 ms. We construct the VGG19 [32] architecture trained\nfrom scratch as the backbone for video frames and audio\nMel spectrogram chunks. The projection networks have two\nfully-connected layers with ReLU activation. The number of\nencoders and decoders for the image encoder, audio encoder,\nfusion module, and emotion-level embedding network used\nfor each dataset are clarified in the ablation study. All used\ntransformer encoders and decoders have 4 heads (h = 4) with\na hidden size of 256 (d = 256) and a feed-forward network\ndimensional of 2048.\n2) MODEL TRAINING\nWe trained the network using the Adam [33] optimizer\nwith a cosine decay learning rate schedule [34]. The initial\npre-trained ALBERT model’s learning rate is 5 × 10−5,\nand the initial learning rate for the other layer’s weights is\n5 × 10−4. We set the batch size to 32 and train the model\non 1 RTX8000 GPU with 48GB RAM. We train 10 epochs\nfor the CMU-MOSEI and 25 epochs for the IEMOCAP\ndataset.\n3) BASELINES\nWe compare our method with the following baselines and\nexisting approaches. First, we compare with strong unimodal\nmethods trained from raw data including ALBERT for tex-\ntual, and VGG19 for visual and audio. Second, we compare\nwith multimodal methods trained from hand-crafted features\nincluding the Emotion Embeddings (EmoEmbs) model [13],\nand the Multimodal Transformer (MulT) model [7]. They\nare considered strong baselines. Finally, we compare our\nmethod with the FE2E [20] which merely adopts late fusion\nfor fusing multimodal features extracted from raw video data.\nWe consider the FE2E as our main competitor because it is the\nfirst work using end-to-end training manner from raw data for\nvideo emotion recognition.\nC. MAIN RESULTS\nTable 3 provides the quantitative results of our pro-\nposed method compared with other strong baselines on the\nIEMOCAP dataset. Our approach performs better than the\nbaselines and previous methods, with an F1 score of 60.9%\nand an accuracy of 85.9%. Table 4 summarizes the compara-\ntive results of our model and other competitive existing meth-\nods on the CMU-MOSEI dataset. Our network outperforms\nall other approaches in terms of the F1 score and achieves\na conspicuous accuracy improvement with a gap of +2%\ncompared to the FE2E. From both tables, we can further\nobserve that our proposed approach significantly enhances\nperformance on minority emotions (anger, excitement, and\nhappiness on the IEMOCAP; disgust, fear, and surprise on the\nCMU-MOSEI). These results demonstrate the effectiveness\nof our proposed network in fusing multimodal video data\nand learning powerful representations for multi-label emotion\nrecognition. Moreover, the results reinforce that multimodal\nlearning is superior to unimodal and that training from raw\ndata is better than hand-crafted features.\nD. ABLATION STUDY\nWe carried out ablation studies to evaluate the influence\nof the transformer-based fusion module and emotion-level\nembedding module on the model performance. To examine\nthe impacts of a module, we conduct the same experimental\n14748 VOLUME 11, 2023\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nTABLE 4. Results on the CMU-MOSEI dataset.Comparison with strong baselines and existing methods.∗: excerpted from previous papers,†: reproduced\nfrom open-source code with hyper-parameter provided in the original paper.\nTABLE 5. Component-wise ablation analysis on both IEMOCAP and\nCMU-MOSEI datasets.\nTABLE 6. Ablation analysis on the effects of the number of transformer\nencoders and decoders on both IEMOCAP and CMU-MOSEI datasets.\n‘‘ImgEnc’’ , ‘‘SpecEnd’’ , and ‘‘FusionEnc’’ stand for the number of\ntransformer encoders in image feature extraction, audio feature\nextraction, and fusion module, respectively. ‘‘EmoDec’’ implies the\nnumber of transformer decoders in the emotion-level embedding\nnetwork.\nsettings using the proposed method with and without\nthe component. The contributions of these modules are\ndepicted in Table 5. Particularly, in the first test case,\nwe exclude the emotion-level embedding module to explore\nthe effects of the fusion module. By using the fusion mod-\nule, we improve the Acc and F1 score by +0.3% and\n+1.39% on the IEMOCAP and the Wacc and F1 score by\n+0.86% and +0.9% on the CMU-MOSEI. For the sec-\nond test case, we attach the emotion-level embedding mod-\nule to the fusion module to train the model. As can be\nobserved in Table 5, the model performance is improved\nmore. On the IEMOCAP, adding emotion-level embed-\nding achieves 0.9% Acc and 2.31% F1 score improvement.\nOn the CMU-MOSEI, the Wacc and F1 score increases\nare 1.46% and 1.21%, respectively. In summary, the pro-\nposed network combined transformer-based fusion module\nand emotion-label embedding module achieves the highest\nperformance in terms of both accuracy and F1 score on both\nIEMOCAP and CMU-MOSEI datasets.\nWe further provide ablation studies in order to select the\noptimal number of transformer encoder layers for the fea-\nture extraction module and fusion module, and the optimal\nnumber of transformer decoder layers for the emotion-level\nembedding network. Table 6 shows the experimental results\nfor both the IEMOCAP and CMU-MOSEI datasets. Our\nmethod achieves the highest performance with 2 layers for\ntransformer encoders and decoders in all modules on the\nIEMOCAP and 4 layers for all modules on the CMU-MOSEI.\nThese configurations are quantitatively consistent with the\nsize of the datasets. The larger the dataset, the more layers\nare needed.\nV. CONCLUSION\nIn this paper, we presented an end-to-end Transformer-based\nfusion and emotion-level representation learning method for\nmulti-label multimodal emotion recognition. Our approach\ntakes a raw video as input rather than hand-crafted fea-\ntures used in most other previous works. We leverage the\nmulti-head attention in the transformers to concurrently\nfuse the multimodal features of video data at multiple\nlayers. Moreover, we proposed learning emotion-level rep-\nresentations from fused multimodal features to improve\nmodel performance in the multi-label recognition task.\nWe demonstrated the effectiveness of our proposed model on\ntwo standard benchmark datasets. The experimental results\nshow that our model outperforms other strong baselines\nand previous existing methods. The provided ablation study\nclearly illustrates the contribution of the fusion module and\nthe emotion-level embeddings module to the model perfor-\nmance improvement.\nDuring the experimental process, we observe that visual\nmodality (video frames) is the most computational expense.\nThis makes the training process more time-consuming and\naffects usability in real-world applications. In fact, in a\nsequence of video frames, there are a lot of redundant and\nalmost identical frames. In future work, we will investigate\nlearning to filter irrelevant and duplicated frames to reduce\nVOLUME 11, 2023 14749\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nthe computational cost so that the model can be practical in\nreal-world scenarios.\nREFERENCES\n[1] Z. Li, Y . Fan, B. Jiang, T. Lei, and W. Liu, ‘‘A survey on sentiment\nanalysis and opinion mining for social multimedia,’’ Multimedia Tools\nAppl., vol. 78, no. 6, pp. 6939–6967, 2019.\n[2] R. Kaur and S. Kautish, ‘‘Multimodal sentiment analysis: A survey and\ncomparison,’’ in Research Anthology on Implementing Sentiment Analysis\nAcross Multiple Disciplines, 2022, pp. 1846–1870.\n[3] S. Chen, X. Li, Q. Jin, S. Zhang, and Y . Qin, ‘‘Video emotion recognition\nin the wild based on fusion of multimodal features,’’ in Proc. 18th ACM\nInt. Conf. Multimodal Interact., Oct. 2016, pp. 494–500.\n[4] J. D. S. Ortega, P. Cardinal, and A. L. Koerich, ‘‘Emotion recognition using\nfusion of audio and video features,’’ in Proc. IEEE Int. Conf. Syst., Man\nCybern. (SMC), Oct. 2019, pp. 3847–3852.\n[5] M.-T. Luong, H. Pham, and C. D. Manning, ‘‘Effective approaches to\nattention-based neural machine translation,’’ 2015, arXiv:1508.04025.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. U. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Advances in\nNeural Information Processing Systems, vol. 30, I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\nEds. Red Hook, NY , USA: Curran Associates, 2017. [Online]. Available:\nhttps://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c\n1c4a845aa-Paper.pdf\n[7] Y .-H.-H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and\nR. Salakhutdinov, ‘‘Multimodal transformer for unaligned multimodal\nlanguage sequences,’’ in Proc. 57th Annu. Meeting Assoc. Comput. Lin-\nguistics, 2019, p. 6558.\n[8] A. Nagrani, S. Yang, A. Arnab, A. Jansen, C. Schmid, and C. Sun, ‘‘Atten-\ntion bottlenecks for multimodal fusion,’’ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 34, 2021, pp. 14200–14213.\n[9] H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y . Cui, and\nB. Gong, ‘‘V ATT: Transformers for multimodal self-supervised learning\nfrom raw video, audio and text,’’ in Proc. Adv. Neural Inf. Process. Syst.,\nvol. 34, 2021, pp. 24206–24221.\n[10] N. Shvetsova, B. Chen, A. Rouditchenko, S. Thomas, B. Kings-\nbury, R. Feris, D. Harwath, J. Glass, and H. Kuehne, ‘‘Everything at\nonce—Multi-modal fusion transformer for video retrieval,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2022,\npp. 20020–20029.\n[11] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, ‘‘IEMOCAP: Interactive emo-\ntional dyadic motion capture database,’’ Lang. Resour. Eval., vol. 42, no. 4,\npp. 335–359, 2008.\n[12] A. Bagher Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency,\n‘‘Multimodal language analysis in the wild: CMU-MOSEI dataset and\ninterpretable dynamic fusion graph,’’ in Proc. 56th Annu. Meeting Assoc.\nComput. Linguistics, vol. 1. Melbourne, QC, Australia: Association for\nComputational Linguistics, Jul. 2018, pp. 2236–2246. [Online]. Available:\nhttps://aclanthology.org/P18-1208\n[13] W. Dai, Z. Liu, T. Yu, and P. Fung, ‘‘Modality-transferable emotion\nembeddings for low-resource multimodal emotion recognition,’’ 2020,\narXiv:2009.09629.\n[14] D. Hazarika, R. Zimmermann, and S. Poria, ‘‘MISA: Modality-invariant\nand -Specific representations for multimodal sentiment analysis,’’ in Proc.\n28th ACM Int. Conf. Multimedia, Oct. 2020, pp. 1122–1131.\n[15] W. Han, H. Chen, A. Gelbukh, A. Zadeh, L.-P. Morency, and S. Poria, ‘‘Bi-\nbimodal modality fusion for correlation-controlled multimodal sentiment\nanalysis,’’ in Proc. Int. Conf. Multimodal Interact., Oct. 2021, pp. 6–15.\n[16] W. Han, H. Chen, and S. Poria, ‘‘Improving multimodal fusion with hierar-\nchical mutual information maximization for multimodal sentiment analy-\nsis,’’ inProc. Conf. Empirical Methods Natural Lang. Process.Punta Cana,\nDominican Republic: Association for Computational Linguistics, 2021,\npp. 9180–9192. [Online]. Available: https://aclanthology.org/2021.emnlp-\nmain.723\n[17] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency,\n‘‘Tensor fusion network for multimodal sentiment analysis,’’ 2017,\narXiv:1707.07250.\n[18] Z. Liu, Y . Shen, V . B. Lakshminarasimhan, P. P. Liang, A. Zadeh, and\nL.-P. Morency, ‘‘Efficient low-rank multimodal fusion with modality-\nspecific factors,’’ 2018, arXiv:1806.00064.\n[19] A. Shenoy and A. Sardana, ‘‘Multilogue-Net: A context aware RNN for\nmulti-modal emotion detection and sentiment analysis in conversation,’’\n2020, arXiv:2002.08267.\n[20] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, ‘‘Multimodal end-to-\nend sparse model for emotion recognition,’’ in Proc. Conf. North Amer.\nChapter Assoc. Comput. Linguistics, Human Lang. Technol.Toronto, ON,\nCanada: Association for Computational Linguistics, 2021, pp. 5305–5316.\n[Online]. Available: https://aclanthology.org/2021.naacl-main.417\n[21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[22] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nEur. Conf. Comput. Vis.Springer, 2020, pp. 213–229.\n[23] Y . Gong, C.-I. Lai, Y .-A. Chung, and J. Glass, ‘‘SSAST: Self-supervised\naudio spectrogram transformer,’’ in Proc. AAAI Conf. Artif. Intell., vol. 36,\nno. 10, 2022, pp. 10699–10709.\n[24] H. Tan and M. Bansal, ‘‘LXMERT: Learning cross-modality encoder\nrepresentations from transformers,’’ 2019, arXiv:1908.07490.\n[25] Y . Cheng, R. Wang, Z. Pan, R. Feng, and Y . Zhang, ‘‘Look, listen, and\nattend: Co-attention network for self-supervised audio-visual represen-\ntation learning,’’ in Proc. 28th ACM Int. Conf. Multimedia, Oct. 2020,\npp. 3884–3892.\n[26] O.-B. Mercea, T. Hummel, A. Koepke, and Z. Akata, ‘‘Tempo-\nral and cross-modal attention for audio-visual zero-shot learning,’’ in\nProc. Eur. Conf. Comput. Vis. Cham, Switzerland: Springer, 2022,\npp. 488–505.\n[27] R. You, Z. Guo, L. Cui, X. Long, Y . Bao, and S. Wen, ‘‘Cross-\nmodality attention with semantic graph embedding for multi-label\nclassification,’’ in Proc. AAAI Conf. Artif. Intell., vol. 34, 2020,\npp. 12709–12716.\n[28] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, ‘‘Query2Label: A sim-\nple transformer way to multi-label classification,’’ 2021, arXiv:2107.\n10834.\n[29] A. Paszke, S. Gross, F. Massa, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, and A. Köpf, ‘‘PyTorch: An\nimperative style, high-performance deep learning library,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 32, 2019, pp. 1–12.\n[30] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n‘‘ALBERT: A lite BERT for self-supervised learning of language\nrepresentations,’’ in Proc. 8th Int. Conf. Learn. Represent. (ICLR),\nAddis Ababa, Ethiopia, Apr. 2020, pp. 1–17. [Online]. Available:\nhttps://openreview.net/forum?id=H1eA7AEtvS\n[31] J. Xiang and G. Zhu, ‘‘Joint face detection and facial expression recogni-\ntion with MTCNN,’’ in Proc. 4th Int. Conf. Inf. Sci. Control Eng. (ICISCE),\n2017, pp. 424–427.\n[32] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ 2014, arXiv:1409.1556.\n[33] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[34] I. Loshchilov and F. Hutter, ‘‘SGDR: Stochastic gradient descent with\nwarm restarts,’’ 2016, arXiv:1608.03983.\nHOAI-DUY LE received the B.S. degree in\nelectronics and telecommunication engineering\nfrom the Ho Chi Minh University of Technol-\nogy, Vietnam, in 2018. He is currently pursu-\ning the master’s degree with the Department\nof Artificial Intelligence Convergence, Chonnam\nNational University, South Korea. His research\ninterests include multimodal deep learning, video\nunderstanding, affective computing, data mining,\nand social media analytics.\n14750 VOLUME 11, 2023\nH.-D. Le et al.: Multi-Label Multimodal Emotion Recognition\nGUEE-SANG LEE received the B.S. degree\nin electrical engineering and the M.S. degree\nin computer engineering from Seoul National\nUniversity, South Korea, in 1980 and 1982, respec-\ntively, and the Ph.D. degree in computer sci-\nence from Pennsylvania State University, in 1991.\nHe is currently a Professor with the Department\nof Artificial Intelligence Convergence, Chonnam\nNational University, South Korea. His research\ninterests include image processing, computer\nvision, and video technology.\nSOO-HYUNG KIM received the B.S. degree in\ncomputer engineering from Seoul National Uni-\nversity, in 1986, and the M.S. and Ph.D. degrees in\ncomputer science from the Korea Advanced Insti-\ntute of Science and Technology, in 1988 and 1993,\nrespectively. Since 1997, he has been a Professor\nwith the Department of Artificial Intelligence Con-\nvergence, Chonnam National University, South\nKorea. His research interests include pattern\nrecognition, document image processing, medical\nimage processing, and ubiquitous computing.\nSEUNGWON KIM received the bachelor’s\nand master’s degrees from the University of\nTasmania, in 2008 and 2010, respectively, and\nthe Ph.D. degree from the HIT Lab NZ,\nNew Zealand, in 2016, under the supervision of\nProf. M. Billinghurst. During his Ph.D. study,\nhe developed a remote collaboration system sup-\nporting stabilized sketch and pointer cues. He is\ncurrently a Professor with the Department of\nArtificial Intelligence Convergence, Chonnam\nNational University, South Korea. His research interests include remote\ncollaboration using augmented virtual communication cues and sharing\nexperiences between distance users.\nHYUNG-JEONG YANGreceived the B.S., M.S.,\nand Ph.D. degrees from Chonbuk National Univer-\nsity, South Korea. She is currently a Professor with\nthe Department of Artificial Intelligence Conver-\ngence, Chonnam National University, Gwangju,\nSouth Korea. Her main research interests include\nmultimedia data mining, medical data analysis,\nsocial network service data mining, and video data\nunderstanding.\nVOLUME 11, 2023 14751",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8420008420944214
    },
    {
      "name": "Feature learning",
      "score": 0.6461772322654724
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5969868898391724
    },
    {
      "name": "Multimodal learning",
      "score": 0.594045877456665
    },
    {
      "name": "Transformer",
      "score": 0.570540726184845
    },
    {
      "name": "Modalities",
      "score": 0.5292263031005859
    },
    {
      "name": "Multi-task learning",
      "score": 0.4783896207809448
    },
    {
      "name": "Deep learning",
      "score": 0.45810699462890625
    },
    {
      "name": "Machine learning",
      "score": 0.4348374307155609
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.42356887459754944
    },
    {
      "name": "Task (project management)",
      "score": 0.34946900606155396
    },
    {
      "name": "Speech recognition",
      "score": 0.34142744541168213
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111277659",
      "name": "Chonnam National University",
      "country": "KR"
    }
  ],
  "cited_by": 59
}