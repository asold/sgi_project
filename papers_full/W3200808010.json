{
  "title": "Dynamic Knowledge Distillation for Pre-trained Language Models",
  "url": "https://openalex.org/W3200808010",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2304534086",
      "name": "Shuhuai Ren",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107643647",
      "name": "Xu Sun",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174510164",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3129779966",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3034201598",
    "https://openalex.org/W2981828710",
    "https://openalex.org/W582134693",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2903996579",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1528361845",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2951013084",
    "https://openalex.org/W3115348206",
    "https://openalex.org/W3171870632",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3217415632",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2964059111",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W1580375566",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2994896922",
    "https://openalex.org/W3196295870",
    "https://openalex.org/W4302023899",
    "https://openalex.org/W3095273266",
    "https://openalex.org/W1484084878",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3103884771",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Knowledge distillation (KD) has been proved effective for compressing large-scale pre-trained language models. However, existing methods conduct KD statically, e.g., the student model aligns its output distribution to that of a selected teacher model on the pre-defined training dataset. In this paper, we explore whether a dynamic knowledge distillation that empowers the student to adjust the learning procedure according to its competency, regarding the student performance and learning efficiency. We explore the dynamical adjustments on three aspects: teacher model adoption, data selection, and KD objective adaptation. Experimental results show that (1) proper selection of teacher model can boost the performance of student model; (2) conducting KD with 10% informative instances achieves comparable performance while greatly accelerates the training; (3) the student performance can be boosted by adjusting the supervision contribution of different alignment objective. We find dynamic knowledge distillation is promising and provide discussions on potential future directions towards more efficient KD methods.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 379–389\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n379\nDynamic Knowledge Distillation for Pre-trained Language Models\nLei Li†, Yankai Lin§, Shuhuai Ren†, Peng Li§, Jie Zhou§, Xu Sun†\n†MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University\n§Pattern Recognition Center, WeChat AI, Tencent Inc., China\n{lilei, shuhuai_ren}@stu.pku.edu.cn xusun@pku.edu.cn\n{yankailin, patrickpli, withtomzhou}@tecent.com\nAbstract\nKnowledge distillation (KD) has been proved\neffective for compressing large-scale pre-\ntrained language models. However, existing\nmethods conduct KD statically, e.g., the stu-\ndent model aligns its output distribution to that\nof a selected teacher model on the pre-deﬁned\ntraining dataset. In this paper, we explore\nwhether a dynamic knowledge distillation that\nempowers the student to adjust the learning\nprocedure according to its competency, regard-\ning the student performance and learning efﬁ-\nciency. We explore the dynamical adjustments\non three aspects: teacher model adoption, data\nselection, and KD objective adaptation. Exper-\nimental results show that (1) proper selection\nof teacher model can boost the performance\nof student model; (2) conducting KD with\n10% informative instances achieves compara-\nble performance while greatly accelerates the\ntraining; (3) the student performance can be\nboosted by adjusting the supervision contribu-\ntion of different alignment objective. We ﬁnd\ndynamic knowledge distillation is promising\nand provide discussions on potential future di-\nrections towards more efﬁcient KD methods.1\n1 Introduction\nKnowledge distillation (KD) (Hinton et al., 2015)\naims to transfer the knowledge from a large teacher\nmodel to a small student model. It has been widely\nused (Sanh et al., 2019; Jiao et al., 2020; Sun et al.,\n2019) to compress large-scale pre-trained language\nmodels (PLMs) like BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019) in recent years.\nBy knowledge distillation, we can obtain a much\nsmaller model with comparable performance, while\ngreatly reduce the memory usage and accelerate\nthe model inference.\nAlthough simple and effective, existing meth-\nods usually conduct the KD learning procedure\n1Our code is available at https://github.com/\nlancopku/DynamicKD\nstatically, e.g., the student model aligns its out-\nput probability distribution to that of a selected\nteacher model on the entire pre-deﬁned corpus. In\nother words, the following three aspects of KD are\nspeciﬁed in advance and remain unchanged during\nthe learning procedure: (1) the teacher model to\nlearn from (learning target); (2) the data used to\nquery the teacher (learning material); (3) the ob-\njective functions and the corresponding weights\n(learning method). However, as the student com-\npetency evolves during the training stage, it is par-\nticularly unreasonable to pre-deﬁne these learning\nsettings and keep them unchanged. Conducting\nKD statically may lead to (1) unqualiﬁed learning\nfrom a too large teacher, (2) repetitive learning on\ninstances that the student has mastered, and (3)\nsub-optimal learning on alignments that are unnec-\nessary. This motivates us to explore an interesting\nproblem: whether a dynamic KD framework con-\nsidering the student competency evolution during\ntraining can bring beneﬁts, regarding the student\nperformance and learning efﬁciency?\nIn this paper, we propose a dynamic knowledge\ndistillation (Dynamic KD) framework, which at-\ntempts to empower the student to adjust the learn-\ning procedure according to its competency. Speciﬁ-\ncally, inspired by the success of active learning (Set-\ntles, 2009), we take the prediction uncertainty, e.g.,\nthe entropy of the predicted classiﬁcation probabil-\nity distribution, as a proxy of the student compe-\ntency. We strive to answer the following research\nquestions: (RQ1) Which teacher is proper to learn\nas the student evolves? (RQ2) Which data are ac-\ntually useful for student models in the whole KD\nstage? (RQ3) Does the optimal learning objec-\ntive change in the KD process? In particular, we\nﬁrst explore the impact of the teacher size to dy-\nnamic knowledge distillation. Second, we explore\nwhether dynamically choosing instances that the\nstudent is uncertain for KD can lead to a better per-\nformance and training efﬁciency trade-off. Third,\n380\nDynamicDataSelection\nStudent\nInputData__________\n__________\n__________…\n__________\n__________\n__________…\nDataSelection\nPredictionUncertainty\nDynamicTeacherAdoption\nStudent\nSmallTeacher\nBigTeacher\n__________\n__________\n…__________\nSelectedData\n__________Student Teacher\n… …\nClassDistribution\nDynamicSupervisionAdjustment\nHiddenState HiddenState\nClassDistribution\nFigure 1: The three aspects of dynamic knowledge distillation explored in this paper. Best viewed in color.\nwe explore whether the dynamic adjustment of the\nsupervision from alignment of prediction probabil-\nity distributions and the alignment of intermediate\nrepresentations in the whole KD stage can improve\nthe performance.\nOur experimental results show that: (1) A larger\nteacher model with more layers may raise a worse\nstudent. We show that selecting the proper teacher\nmodel according to the competency of the student\ncan improve the performance. (2) We can achieve\ncomparable performance using only 10% informa-\ntive instances selected according to the student pre-\ndiction uncertainty. These instances also evolve\nduring the training as the student becomes stronger.\n(3) We can boost the student performance by dy-\nnamically adjusting the supervision from different\nalignment objectives of the teacher model.\nOur observations demonstrate the limitations of\nthe current static KD framework. The proposed\nuncertainty-based dynamic KD framework only\nmakes the very ﬁrst attempt, and we are hoping this\npaper can motivate more future research towards\nmore efﬁcient and adaptive KD methods.\n2 Background: Knowledge Distillation\nGiven a student model S and a teacher model T,\nknowledge distillation aims to train the student\nmodel by aligning the outputs of the student model\nto that of the teacher. For example, Hinton et al.\n(2015) utilize the teacher model outputs as soft\ntargets for the student to learn. We denote S(x)\nand T(x) as the output logit vector of the stu-\ndent and the teacher for input x, respectively. The\nKD can be conducted by minimizing the Kullback-\nLeibler (KL) divergence distance between the stu-\ndent and teacher prediction:\nLKL = KL (σ(S(x) /τ) ||σ(T(x) /τ)) , (1)\nwhere σ(·) denotes the softmax function and τ is a\ntemperature hyper-parameter. The student parame-\nters are updated according to the KD loss and the\noriginal classiﬁcation loss, i.e., the cross-entropy\nover the ground-truth label y:\nLCE = −ylog σ(S(x)) , (2)\nL= (1 −λKL)LCE + λKLLKL, (3)\nwhere λKL is the hyper-parameter controlling the\nweight of knowledge distillation objective. Recent\nexplorations also ﬁnd that introducing KD objec-\ntives of alignments between the intermediate repre-\nsentations (Romero et al., 2015; Sun et al., 2019)\nand attention map (Jiao et al., 2020; Wang et al.,\n2020) is helpful. Note that conventional KD frame-\nwork is static, i.e., the teacher model is selected\nbefore KD and the training is conducted on all train-\ning instances indiscriminately according to the pre-\ndeﬁned objective and the corresponding weights of\ndifferent objectives. However, it is unreasonable to\nconduct the KD learning procedure statically as the\nstudent model evolves during the training. We are\ncurious whether adaptive adjusting the settings on\nteacher adoption, dataset selection and supervision\nadjustment can bring beneﬁts regarding student per-\nformance and learning efﬁciency, motivating us to\nexplore a dynamic KD framework.\n3 Dynamic Knowledge Distillation\nIn this section, we introduce the dynamic knowl-\nedge distillation. The core idea behind is to em-\npower the student to adjust the learning procedure\naccording to its current state, and we investigate\nthe three aspects illustrated in Figure 1.\n3.1 Dynamic Teacher Adoption\nThe teacher model plays a vital role in KD, as it pro-\nvides the student soft-targets for helping the student\nlearn the relation between different classes (Hinton\net al., 2015). However, there are few investigations\nregarding how to select a proper teacher for the\n381\nMethod RTE IMDB CoLA Avg.\nBERTBASE 67.8 89.1 54.2 70.4\nBERTLARGE 72.6 90.4 60.1 74.4\nNo KD 63.7 86.3 39.0 63.0\nKD w/ BERTBASE 64.9 86.9 39.4 63.7\nKD w/ BERTLARGE 64.5 86.5 38.2 63.1\nKD w/ Ensemble 64.9 86.7 39.9 63.8\nUncertainty-Hard 66.9∗ 86.3 42.7∗ 65.3\nUncertainty-Soft 66.4∗ 87.1∗ 41.0 64.8\nTable 1: We ﬁnd that bigger teacher with better perfor-\nmance raises a worse student model. Results are aver-\nage of 3 seeds on the validation set. ∗ denotes statisti-\ncally signiﬁcant improvement over the best performing\nbaseline with p< 0.05.\nstudent in KD for PLMs during the training dynam-\nically. In the KD of PLMs, it is usually all teacher\nmodels are with the same model architecture, i.e.,\nhuge Transformer (Vaswani et al., 2017) model.\nThus, the most informative factor of teacher mod-\nels is their model size, e.g., the layer number of the\nteacher model and the corresponding hidden size.\nThis motivates us to take a ﬁrst step to explore the\nimpact of model size.\n3.1.1 Bigger Teacher Not Always Raises\nBetter Student\nSpeciﬁcally, we are curious about whether learning\nfrom a bigger PLM with better performance can\nlead to a better distilled student model. We con-\nduct probing experiments to distill a 6-layer student\nBERT model from BERTBASE with 12 layers, and\nBERTLARGE with 24 layers, respectively. We con-\nducts the experiment on two datasets, RTE (Ben-\ntivogli et al., 2009) and CoLA (Warstadt et al.,\n2019), where two teacher models exhibit clear per-\nformance gap, and a sentiment classiﬁcation bench-\nmark IMDB (Maas et al., 2011). Detailed experi-\nmental setup can be found in Appendix A.\nAs shown in Table 1, we surprisingly ﬁnd that\nwhile the BERTLARGE teacher clearly outperforms\nthe small BERT BASE teacher model, the student\nmodel distilled by the BERTBASE teacher achieves\nbetter performance on all three datasets. This phe-\nnomenon is counter-intuitive as a larger teacher is\nsupposed to provide better supervision signal for\nthe student model. We think that there are two pos-\nsible factors regarding the size of teacher model\nthat leading to the deteriorated performance:\n(1) The predicted logits of the teacher model be-\ncome less soft as the teacher model becomes larger\nand more conﬁdent about its prediction (Guo et al.,\n10 8 6 4\nNumber of Student Layer\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60Accuracy Gain (%)\nIMDB\n10 8 6 4\nNumber of Student Layer\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0Matthews Correlation Gain (%)\nCoLA\nFigure 2: Performance gain of distilled student model\nwith various layer sizes. The teacher model is\nBERTBASE with 12 layers.\n2017; Desai and Durrett, 2020), which decreases\nthe effect of knowledge transfer via the soft targets.\nWe ﬁnd that a smaller τ also leads to a decreased\nperformance of the student model, indicating the\nthe less-softened teacher prediction will decrease\nthe student performance.2\n(2) The capacity gap between the teacher and\nstudent model increases as the teacher becomes\nlarger. The competency of the student model can\nnot match that of the large teacher model, which\nweakens the performance of KD.\nTo explore the combined inﬂuence of these fac-\ntors, we distill student models with different layers\nand plot the performance gain compared to directly\ntraining the student model without distillation in\nFigure 2. It can be found that by decreasing the stu-\ndent size, the better supervision from teacher model\nboosts the performance, while the two counterac-\ntive factors dominate as the gap becomes much\nlarger, decreasing the performance gain. We notice\nthat this phenomenon is also observed by Mirzadeh\net al. (2020) in computer vision tasks using convo-\nlutional networks, showing that it is a widespread\nissue and needs more in-depth investigations. Note\nthat BERTBASE and BERTLARGE also differs from\nthe number of hidden size, the experiments regard-\ning the hidden size, where the phenomenon also\nexists and corresponding results can be found in\nAppendix C.\n3.1.2 Uncertainty-based Teacher Adoption\nOur preliminary observations demonstrate that se-\nlecting a proper teacher model for KD is signiﬁcant\nfor the student performance. While the capacity\ngap is an inherent problem once the teacher and the\nstudent are set, we are curious about whether dy-\nnamically querying the proper teacher according to\nthe student competency during training can make\n2Refer to Appendix B for details.\n382\nthe full use of teacher models. Without loss of gen-\nerality, we conduct KD to train a student model\nfrom two teacher models with different numbers\nof Transformer layers. We assume that during the\ninitial training stage, the student can rely more on\nthe small teacher model, while turns to the large\nteacher for more accurate supervision when it be-\ncomes stronger. Speciﬁcally, we propose to utilize\nthe student prediction uncertainty as a proxy of the\ncompetency, inspired by the successful applications\nin active learning (Settles, 2009), and design two\nuncertain-based teacher adoption strategies:\nHard Selection: The instances in one batch are\nsorted according to the student prediction uncer-\ntainty, i.e., the entropy of predicted class distribu-\ntion. Then the instances are evenly divided into\ninstances that the student most uncertain about and\ninstances that model is most conﬁdent about. For\nthe uncertain part, the small teacher is queried for\nsupervision signals, while the large teacher pro-\nvides the soft-label for the instances that the student\nis conﬁdent about.\nSoft Selection : The corresponding KD loss\nweights from two teachers are adjusted softly at\ninstance-level. Formally, given two teacher model\nT1 (BERTBASE , in our case) and T2 (BERTLARGE ),\nwe can re-write the KD objective in the multiple\nteacher setting as:\nLKD = w1LT1\nKL + w2LT2\nKL (4)\nwhere LT\nKL denotes matching loss of the output\nlogits of the student model and the teacher model t.\nThe w1 and w2 controls the relative contribution of\nthe supervisions from the two teachers. We adap-\ntively down-weight the supervision from the large\nteacher when the student are uncertain about the\ntraining instances. The prediction uncertainty is\nadopted as a measurement of the student compe-\ntency for instance x:\nux = Entropy (σ(S(x))) (5)\nwhere σis a normalization function, e.g, softmax\nfunction for mapping the logit vector to probabil-\nity distribution. The w1 and w2 are adjusted as\nfollows:\nw1 = ux\nU , w 2 = 1 −ux\nU (6)\nwhere U is a normalization factor which re-scales\nthe weight to [0,1]. In this way, the student will\npay more attention to the small teacher when it is\nuncertain about the current instance, while relies\non the large teacher when it is conﬁdent about its\nprediction.\n3.1.3 Experiments\nSettings We conduct experiments to distill\na 6-layer student model from BERT BASE and\nBERTLARGE , on RTE, CoLA and IMDB, follow-\ning settings of probing analysis in Section 3.1.1.\nResults The results of the proposed selection\nstrategies are shown in Table 1. We observe that\nthe hard selection strategy achieves an overall 65.3\naccuracy which outperforms directly learning from\nthe ensemble of two teacher models. This demon-\nstrates that the proposed strategy is effective by\nselecting the proper teacher model to learn. The\nsoft selection strategy also outperforms the base-\nline while falls little behind with the hard version.\nWe attribute it to that the provided supervisions\nof two teachers are of different softness, thus may\nconfuse the student model.\n3.2 Dynamic Data Selection\nThe second research question we want to explore\nis which data will be more beneﬁcial for the perfor-\nmance of the student. As the distillation proceeds,\nthe student is becoming stronger, thus the repet-\nitive learning on the instances those the student\nhas mastered on can be eliminated. If there are\nsuch instances that are vital for the learning of the\nstudent model, can we only conduct KD on these\ninstances, for improving the learning efﬁciency?\nBesides, do the vital instances remain static or also\nevolve with the student model? These questions\nmotivate us to explore the effect of dynamically\nselecting instances.\n3.2.1 Uncertainty-based Data Selection\nWe propose to actively select informative instances\naccording to student prediction uncertainty, in-\nspired by the successful practice of active learn-\ning (Settles, 2009). Formally, given N instances\nin one batch, for each instance x, the correspond-\ning output class probability distribution over the\nclass label y of the student model is P(y |x) =\nσ(S(x)). We compute an uncertainty score ux\nfor x using the follow strategies with negligible\ncomputational overhead:\nEntropy (Settles, 2009), which measures the un-\ncertainty of the student prediction distribution:\nux = −\n∑\ny\nP(y|x) logP(y|x) . (7)\n383\nMethod #FLOPsSST-5 IMDB MRPC MNLI-m / mm\nBERTBASE (Teacher) - 52.0 89.1 86.8 84.0 / 84.4\nVanilla KD 45.1B 47.4 86.8 80.2 81.7 / 82.0\nRandom 22.6B 46.8 86.4 79.7 81.4 / 81.6\nUncertainty-Entropy28.2B 46.7 86.8 79.4 81.5 / 82.0\nUncertainty-Margin28.2B 46.6 86.8 79.4 81.4 / 81.9\nUncertainty-LC 28.2B 46.5 86.8 79.4 81.4 / 81.9\n∆ - - 0.6 0.0 - 0.5 - 0.2 / 0.0\nTable 2: Dynamic data selection results with r set to\n0.5. Results are averaged of 3 seeds on the validation\nset. ∆ denotes the minimal performance degradation of\ndifferent selection strategies compares to vanilla KD.\nDataset # Train # Aug Train # Dev # Test # Class\nSST-5 8.8k 176k 1.1k 2.2k 5\nIMDB 20k 400k 5k 25k 2\nMNLI 393k 786,0k 20k 20k 3\nMRPC 3.7k 74k 0.4k 1.7k 2\nRTE 2.5k 50k 0.3k 3k 2\nCoLA 8.5k 170k 1k 1k 2\nTable 3: Statistics of datasets. # Aug Train denotes\nthe number of the augmented training dataset following\nJiao et al. (2020).\nMargin, which is computed as the margin between\nthe ﬁrst and second most probable class y∗\n1 and y∗\n2:\nux = P(y∗\n1 |x) −P(y∗\n2 |x) . (8)\nLeast-Conﬁdence (LC), which indicates how un-\ncertain the model about the predicted class ˆy =\narg maxy P(y|x):\nux = 1 −P(ˆy|x) (9)\nWe rank the instances in a batch according to its\nprediction uncertainty, and only choose the topN×\nrinstances to query the teacher model, where r∈\n(0,1] is the selection ratio controlling the number\nto query. Note that in binary classiﬁcation tasks\nlike IMDB, the selected subsets using the above\nstrategies are identical. We also design a Random\nstrategy that selects N ×rinstances randomly, to\nserve as a baseline for evaluating the effectiveness\nof selection strategies.\n3.2.2 Experiments\nSettings We conduct the investigation experi-\nments on two sentiment classiﬁcation datasets\nIMDB (Maas et al., 2011) and SST-5 (Socher et al.,\n2013), and natural language inference tasks in-\ncluding MRPC (Dolan and Brockett, 2005) and\nMNLI (Williams et al., 2018). The statistics of\ndataset and the implementation details can be found\nin Table 3 and D, respectively. We report accuracy\nas the performance measurement for all the evalu-\nated tasks.\nBesides, we also provide the corresponding com-\nputational FLOPs for comparing the learning efﬁ-\nciency. In more detail, we divide the computational\ncost Cof KD tinto three parts: student forward Fs,\nteacher forward Ft and student backward Bs for\nupdating parameters. Note that Fs ≈Bs ≪Ft,\nas the teacher model is usually much larger than\nthe student model. By actively learning only from\nN×rinstances that the student are most uncertain\nabout, the cost is reduced to:\nC′= N×Fs + N×r×Bs + N×r×Ft. (10)\nFor example, the number of computational FLOPs\nof a 6-layer student BERT model is 11.3B\nwhile that of a 12-layer BERT teacher model is\n22.5B (Jiao et al., 2020). When ris set to 0.1, the\ntotal KD cost is reduced from 45.1B to 14.7B.\nResults with Original Dataset The results when\nrset to 0.5 are listed in Table 2. Overall, it can be\nfound that selecting the instances only lead negligi-\nble degradation of the student performance, com-\npared to that of Vanilla KD, showing the effec-\ntiveness of the uncertainty-based strategies. Inter-\nestingly, the random strategy perform closely to\nthe uncertainty-based strategies, which we attribute\nto that the underlying informative data space can\nalso be covered by random selected instances. Be-\nsides, we notice that performance drop is smaller\non the tasks with larger training data size. For\nexample, selecting informative instances with pre-\ndiction entropy leads to 0.2 accuracy drop on the\nMNLI dataset consisting 393k training instances,\nwhile causes 0.8 performance drop on MRPC with\n3.7k instances. A possible reason is that for the\ntiny dataset, the underlying data distribution is not\nwell covered by the training data, therefore further\ndown-sampling the training data results in a larger\nperformance gap. To verify this, we turn to the\nthe setting where the original training dataset is\nenriched with augmentation techniques.\nResults with Augmented Dataset Following\nTinyBERT (Jiao et al., 2020), we augment the train-\ning dataset 20 times with BERT mask language\nprediction, as it has been prove effective for dis-\ntilling a powerful student model. Our assumption\nis that with the data augmentation technique, the\ntraining set can sufﬁciently cover the possible data\n384\nMethod #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓)\nBERTBASE (Teacher) - 53.7 88.8 87.5 83.9 / 83.4 79.5 -\nTinyBERT† 24.9B - - 86.4 82.5 / 81.8 - -\nTinyBERT 24.9B 51.4 87.6 86.2 82.6 / 82.0 78.0 0.0\nRandom 2.49B 51.1 87.0 83.3 80.8 / 80.5 76.5 1.5\nUncertainty-Entropy 4.65B 51.5 87.7 86.5 81.8 / 81.0 77.7 0.3\nUncertainty-Margin 4.65B 51.6 87.7 86.5 81.6 / 81.1 77.7 0.3\nUncertainty-LC 4.65B 51.2 87.7 86.5 81.4 / 80.8 77.5 0.5\nTable 4: Test results when the selection ratior= 0.1 for dynamic data selection on various tasks. #FLOPs denotes\nthe average computational cost of KD for each instance. †denotes results from Jiao et al. (2020).\n5 10 20 40 80\nSelection Ratio (%)\n50.0\n50.5\n51.0\n51.5\n52.0\n52.5Accuracy (%)\nRandom\nMargin\nEntropy\nLC\nFigure 3: We plot the mean accuracy on the valida-\ntion set of 3 seeds ( ±one standard deviation) under\ndifferent selection ratios of various strategies. Orange\ndashed line denotes the performance of vanilla KD.\nspace, thus selecting the informative instances will\nnot lead to signiﬁcant performance drop. Besides,\nit is of great practical value to accelerate the KD\nprocedure via reducing the queries to teacher model\non the augmented dataset. For example, it costs\nabout $3,709 for querying all the instances of the\naugmented MNLI dataset as mentioned by Krishna\net al. (2019).3 By only querying a small portion\nof instances to the teacher model, we can greatly\nreduce economic cost and ease the possible environ-\nmental side-effects (Strubell et al., 2019; Schwartz\net al., 2019; Xu et al., 2021) that may hinder the\ndeployments of PLMs on downstream tasks.\nThe results with TinyBERT-4L as the backbone\nmodel and r = 0.1 are listed in Table 4. We can\nobserve that uncertainty-based selection strategy\ncan maintain the superior performance while sav-\ning the computational cost, e.g., the FLOPs is re-\nduced from 24.9B to 4.65B with negligible aver-\nage performance decrease. In tasks like SST-5\nand IMDB, selecting 10% most informative in-\nstances according to student prediction entropy can\n3The cost is estimated according to Google Cloud\nnatural language API: https://cloud.google.com/\nnatural-language/pricing.\neven outperform the original TinyBERT using the\nwhole dataset. Among these strategies, the least-\nconﬁdence strategy performs relatively poor, as it\nonly takes the maximum probability into consider-\nation while neglects the full output distribution.\nPerformance under Different Ratios We vary\nthe selection ratio rto check the results of different\nstrategies on the augmented SST-5 dataset. The\nresults are shown in Figure 3. Our observations\nare: (1) There exists a trade-off between the per-\nformance and the training costs, i.e., increasing the\nselection ratio generally improves the performance\nof student model, while results in bigger training\ncosts. (2) We can achieve the full performance\nusing about 20% training data. It indicates that\nthe training data support can be well covered with\nabout 20% data, thus learning from these instances\ncan sufﬁciently train the student model. It validates\nour motivation to select informative instances for\nreducing the repetitive learning caused by data re-\ndundancy. (3) Selection strategies based on the\nuncertainty of student prediction can make the bet-\nter use of limited query, performing better than\nthe random selection, especially when the query\nnumber is low.\n3.2.3 Analysis\nWe further conduct experiments on the augmented\nSST-5 dataset to gain insights about the property\nof selected instances and visualize the distribution\nof selected instances for intuitive understanding.\nProperty of Selected Instances We plot the\nteacher prediction entropy and the distance from\nthe selected instances to the corresponding category\ncenter. From Figure 4 (left), we observe for hard\ninstances with high uncertainty that selected by the\nstudent model, the teacher model also regards them\nas difﬁcult. It indicates that the instance difﬁculty\nis an inherent property of data and uncertainty-\n385\n0 1 2 3\nTraining Epoch\n0.235\n0.240\n0.245\n0.250\n0.255\n0.260Teacher Entropy\n0 1 2 3\nTraining Epoch\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0Distance to Category Center\nRandom\nMargin\nLC\nEntropy\nFigure 4: Training dynamics comparison of the se-\nlected instances. The entropy strategy can distinguish\nmore informative instances from the whole dataset.\n0\n1\n2\n3\n4\ns0\ns1\ns2\ns3\ns4\n(a) Random\n0\n1\n2\n3\n4\ns0\ns1\ns2\ns3\ns4 (b) Margin\nFigure 5: The t-SNE visualization of instance repre-\nsentations. Uncertainty-based strategies select the in-\nstances close to the class boundary, which is useful for\nthe learning of the student model. Best viewed in color.\nbased criterion can discover these hard instances\nfrom the whole dataset. Besides, the teacher’s en-\ntropy of selected instances increases as the training\nproceeds, showing that the selected instances also\nevolve during the training as the student is becom-\ning stronger. The right part in Figure 4 demon-\nstrates that uncertainty-based selection will pick up\nthe instances that are far away from the category\ncenter than the ones are randomly picked. These in-\nstances are more informative for the student model\nto learn the decision boundary of different classes.\nVisualization of Selected Instances We further\nvisualize the distribution of instances in the fea-\nture space, i.e., the representation before the classi-\nﬁer layer, using t-SNE (Maaten and Hinton, 2008)\nand highlight the selected instances with the cross\nmarker. We compare the best performing strategy\nmargin and random selection on SST-5. As shown\nin Figure 5, the instances randomly selected are\ndistributed uniformly in feature space. The mar-\ngin strategy instead picks the instances close to the\nclassiﬁcation boundary. The results demonstrate\nthat the uncertainty-based selection criterion can\nhelp the student model pay more attention to the in-\nstances that are vital for making correct predictions,\nthus achieving a comparable performance with a\nmuch lower computational cost.\nIn all, our analysis experiments show that the\nuncertainty-based selection is effective for picking\ninstances that are close to the classiﬁcation bound-\nary. Besides, the selected instances also evolve as\nthe student model becomes stronger. By learning\nfrom these instances, the computational cost of KD\nis greatly reduced with a negligible accuracy drop.\n3.3 Dynamic Supervision Adjustment\nWe ﬁnally explore the question of the optimal learn-\ning objective functions. Previous studies have\nshown that integrating the alignments on the in-\ntermediate representation (Romero et al., 2015;\nSanh et al., 2019; Sun et al., 2019) and attention\nmap (Jiao et al., 2020; Wang et al., 2020) between\nthe student and the teacher model can further boost\nthe performance. We are interested in whether the\ndynamic adjustment of the supervision from differ-\nent alignment objectives can bring extra beneﬁts.\nAs the ﬁrst exploration, we only consider the com-\nbination of the KL-divergence distance with the\nteacher prediction and the hidden representation\nalignments:\nLKD = λKL ∗LKL + λPT ∗LPT (11)\nwhere LPT is called PaTient loss, which measures\nthe alignment between the normalized internal rep-\nresentations of the teacher and student model (Sun\net al., 2019):\nLPT =\nM∑\ni=1\n\nhs\ni\n∥hs\ni ∥2\n−\nht\nIpt(j)ht\nIpt(j)\n\n2\n\n2\n2\n(12)\nwhere M is the number of student layer, Ipt(i)\ndenotes the corresponding alignment of the teacher\nlayer for the student i-th layer, hs\ni and ht\ni denote\nrepresentation of i-th layer of student and teacher\nmodel, respectively.\n3.3.1 Uncertainty-based Supervision\nAdjustment\nDifferent from previous studies which set the cor-\nresponding alignment objective weights via hyper-\nparameter search and keep them unchanged during\nthe training, we propose to adjust the weights ac-\ncording to the student prediction uncertainty for\neach instance. The motivation behind is that we\nassume it is unnecessary to force the student model\n386\nMethod SST-5 MRPC RTE Avg.\nBERTBASE (Teacher) 52.0 86.8 67.8 68.9\nVanilla KD 47.4 80.2 64.9 64.2\nBERT-PKD 46.6 80.8 65.1 64.2\nUncertainty 48.1 81.5 ∗ 66.4∗ 65.3\nTable 5: Results of dynamic adjusting the supervision\nweights, showing the uncertainty-based adjustment is\neffective. ∗ denote results are statistically signiﬁcant\nwith p< 0.05.\nto align all the outputs of the teacher model dur-\ning the whole training stage. As the training pro-\nceeds, the student is become stronger and it may\nlearn the informative features different from the\nteacher model. Therefore, there is no need to force\nthe student to act exactly with the teacher model,\ni.e., requiring the intermediate representations of\nthe student to be identical with the teacher. For-\nmally, we turn the weight of KD objective into\na function of the student prediction uncertainty\nu(x) = Entropy (σ(S(s))):\nλKL = λ∗\nKL(1 −ux\nU ), λ PT = λ∗\nPT\nux\nU (13)\nwhere λ∗\nKL and λ∗\nPT are pre-deﬁned weight for\neach objective obtained by parameter search and\nU is the normalization factor. In this way, the con-\ntribution of these two objectives are adjusted dy-\nnamically during the training for each instances.\nFor instances that the student is conﬁdent about,\nthe supervision from the internal representation\nalignment is down-weighted. Thus the student is\nfocusing mimicking the ﬁnal prediction probabil-\nity distribution with the teacher based on its own\nunderstanding of the instance. On the contrary, for\ninstances that the student is confusing, the supervi-\nsion from teacher model representations can help\nit learn the feature of the instance better.\n3.3.2 Experiments\nSettings The student model is set to 6-layer and\nBERTBASE is adopted as the teacher model. For\nintermediate layer representation alignment, we\nadopt the Skip strategy, i.e., Ipt = {2,4,6,8,10}\nas it performs best as described in BERT-PKD. We\nconduct experiments on the sentiment analysis task\nSST-5, and two natural language inference tasks\nMRPC and RTE. For λ∗\nKL and λ∗\nPT , we adopt the\nsearched parameters provided by Sun et al. (2019).\nResults The results of adaptive adjusting the su-\npervision weights are listed in Table 5. We ob-\n0 1 2 3\nTraining Epoch\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150KL Loss Weight\nRTE\n0 1 2 3\nTraining Epoch\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\nCoLA\nFigure 6: Evolution of the dynamically adjusted weight\nof KL-divergence loss weight.\nserve that the proposed uncertainty-based super-\nvision adjustment can outperform the static ver-\nsion BERT-PKD on all the tasks, showing that the\nproper adjustment of the KD objectives is effective\nfor improving the student performance. We also\nplot the batch average of the KL loss weight in\nFigure 6. As expected, the corresponding weight\nof the prediction probability alignment objective is\nincreasing as the student becomes more conﬁdent\nabout its predictions, thus paying more attention to\nmatching the output distribution with the teacher\nmodel. Interestingly, we ﬁnd that at the initial stage\nof training, the KL weight is decreasing. It indi-\ncates that the learning by aligning the intermediate\nrepresentations can help the student quickly gain\nthe understanding the task, thus improving the con-\nﬁdence of predictions.\n4 Discussions\nAfter the preliminary explorations on the three as-\npects of Dynamic KD, we observe that it is promis-\ning for improving the efﬁciency and the distilled\nstudent performance. Here we provide potential\ndirections for further investigations.\n(1) From uncertainty-based selection criterion\nto advanced methods. In this paper, we utilize\nstudent prediction uncertainty as a proxy for se-\nlecting teachers, training instances and supervision\nobjectives. More advanced methods based on more\naccurate uncertainty estimations (Gal and Ghahra-\nmani, 2016; Zhou et al., 2020), clues from training\ndynamics (Toneva et al., 2018), or even a learnable\nselector can be developed.\n(2) From isolation to integration. As a prelim-\ninary study, we only investigate the three dimen-\nsions independently. Future work can adjust these\ncomponents simultaneously and investigate the un-\nderlying correlation between these three dimen-\nsions for a better efﬁciency-performance trade-off.\n(3) More ﬁne-grained investigations regarding\ndifferent components in the Dynamic KD frame-\n387\nwork: (i) For teacher adoption, exploring whether\ndynamically training a student model from more\nteacher models or teacher models with different\narchitectures can bring extra beneﬁts; (ii) For\ndata selection, it will be interesting to investigate\nwhether the informative data is model-agnostic,\nand whether dynamically selecting data from dif-\nferent domains can improve the generalization per-\nformance; (iii) For supervision adjustment, investi-\ngations on the effect of combinations of different\nobjectives can be promising.\n5 Related Work\nOur work relates to recent explorations on applying\nKD for compressing the PLMs and active learning.\nKnowledge Distillation for PLMs Knowledge\ndistillation (Hinton et al., 2015) aims to transfer\nthe dark knowledge from a large teacher model to\na compact student model, which has been proved\neffective for obtaining compact variants of PLMs.\nThose methods can be divided into general distil-\nlation (Sanh et al., 2019; Turc et al., 2019; Wang\net al., 2020) and task-speciﬁc distillation (Sun et al.,\n2019; Jiao et al., 2020; Xu et al., 2020; Li et al.,\n2020a; Liang et al., 2021; Li et al., 2020b; Wu et al.,\n2021). The former conducts KD on the general text\ncorpus while the latter trains the student model on\nthe task-speciﬁc datasets. In this paper, we focus\non the latter one as it is more widely adopted in\npractice. Compared to existing static KD work, we\nare the ﬁrst to explore the idea of Dynamic KD,\nmaking it more ﬂexible, efﬁcient and effective.\nActive Learning (Settles, 2009), where a learn-\ning system is allowed to choose the data from\nwhich it learns from, for achieving better perfor-\nmance with fewer labeled data. Traditional se-\nlection strategies include uncertainty-based meth-\nods (Scheffer et al., 2001; Settles, 2009), which\nselect the instances that model is most uncertain\nabout, query-by-committee (Freund et al., 1997),\nwhich select instances with highest disagreements\nbetween a set of classiﬁers, and methods based on\ndecision theory (Roy and McCallum, 2001). In this\npaper, inspired by the success of active learning,\nwe introduce Dynamic KD that utilizes the differ-\nent strategies like prediction entropy as a proxy\nof student competency to adaptively adjust the dif-\nferent aspects of KD. Our explorations show that\nthe uncertainty-based strategies are effective for\nimproving the efﬁciency and performance of KD.\n6 Conclusion\nIn this paper, we introduce dynamic knowledge\ndistillation, and conduct exploratory experiments\nregarding teacher model adoption, data selection\nand the supervision adjustment. Our experimental\nresults demonstrate that the dynamical adjustments\non the three aspects according to the student un-\ncertainty is promising for improving the student\nperformance and learning efﬁciency. We provide\ndiscussions on the potential directions worth ex-\nploring in the future, and hope this work can moti-\nvate studies towards more environmental-friendly\nknowledge distillation methods.\nAcknowledgements\nWe thank all the anonymous reviewers for their\nconstructive comments and Xuancheng Ren for his\nvaluable suggestions in preparing the manuscript.\nThis work was supported by a Tencent Research\nGrant. Xu Sun is the corresponding author of this\npaper.\nReferences\nLuisa Bentivogli, Ido Kalman Dagan, Dang Hoa,\nDanilo Giampiccolo, and Bernardo Magnini. 2009.\nThe ﬁfth pascal recognizing textual entailment chal-\nlenge. In TAC Workshop.\nShrey Desai and Greg Durrett. 2020. Calibration of pre-\ntrained transformers. In EMNLP, pages 295–302,\nOnline.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP).\nYoav Freund, H Sebastian Seung, Eli Shamir, and Naf-\ntali Tishby. 1997. Selective sampling using the\nquery by committee algorithm. Machine learning,\n28(2):133–168.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as a\nbayesian approximation: Representing model uncer-\ntainty in deep learning. In ICML, pages 1050–1059.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In ICML, pages 1321–1330.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\n388\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of EMNLP, pages\n4163–4174.\nKalpesh Krishna, Gaurav Singh Tomar, Ankur P\nParikh, Nicolas Papernot, and Mohit Iyyer. 2019.\nThieves on sesame street! Model extraction of\nBERT-based APIs. In ICLR.\nJianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng\nXu, Min Yang, and Yaohong Jin. 2020a. BERT-\nEMD: Many-to-many layer mapping for BERT com-\npression with earth mover’s distance. In EMNLP,\npages 3009–3018.\nLei Li, Yankai Lin, Shuhuai Ren, Deli Chen, Xu-\nancheng Ren, Peng Li, Jie Zhou, and Xu Sun. 2020b.\nAccelerating pre-trained language models via cali-\nbrated cascade. arXiv preprint arXiv:2012.14682.\nKevin J. Liang, Weituo Hao, Dinghan Shen, Yufan\nZhou, Weizhu Chen, Changyou Chen, and Lawrence\nCarin. 2021. MixKD: Towards efﬁcient distillation\nof large-scale language models. In ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn ACL, pages 142–150.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. JMLR, 9:2579–2605.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang Li,\nNir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distil-\nlation via teacher assistant. In AAAI, pages 5191–\n5198.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2015. FitNets: Hints for thin deep nets. In\nICLR.\nNicholas Roy and Andrew McCallum. 2001. Toward\noptimal active learning through sampling estimation\nof error reduction. In ICML, pages 441–448.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\nIn NeurIPS Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing.\nTobias Scheffer, Christian Decomain, and Stefan Wro-\nbel. 2001. Active hidden markov models for infor-\nmation extraction. In International Symposium on\nIntelligent Data Analysis, pages 309–318. Springer.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\nOren Etzioni. 2019. Green AI. arXiv preprint\narXiv:1907.10597.\nBurr Settles. 2009. Active learning literature survey.\nComputer Sciences Technical Report 1648, Univer-\nsity of Wisconsin–Madison.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631–1642.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In ACL, pages 3645–3650.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In EMNLP-IJCNLP, pages 4323–4332.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. 2018. An empirical study of example\nforgetting during deep neural network learning. In\nICLR.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. arXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS, pages 5998–6008.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. MiniLM: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In NeurIPS.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTACL, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL-\nHLT, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In System Demonstrations, EMNLP, pages\n38–45.\n389\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang.\n2021. One teacher is enough? pre-trained language\nmodel distillation from multiple teachers. In Find-\nings of ACL-IJCNLP 2021, pages 4408–4413.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. BERT-of-Theseus: Com-\npressing BERT by progressive module replacing. In\nEMNLP, pages 7859–7869.\nJingjing Xu, Liang Zhao, Junyang Lin, Rundong Gao,\nXu Sun, and Hongxia Yang. 2021. KNAS: green\nneural architecture search. In ICML, pages 11613–\n11625.\nYikai Zhou, Baosong Yang, Derek F Wong, Yu Wan,\nand Lidia S Chao. 2020. Uncertainty-aware curricu-\nlum learning for neural machine translation. In ACL,\npages 6934–6944.\nA Teacher Size Exploration Settings\nWe conduct the knowledge distillation with\nBERTBASE and BERTLARGE as teacher models. The\nstudent model is set to a 6-layer student BERT. For\ntraining the teacher, the teacher model is ﬁne-tuned\nusing the script provided by Huggingface Trans-\nformers library. The ﬁne-tuning learning rate is\n2e-5 with a linear warm-up learning rate schedule\nfor the ﬁrst 10% training steps. Batch size is 32,\ntraining epoch is set to 3, and the max length of\ninput sentence is set to 128. The statistics of used\ndatasets are listed in Table 3.\nFor distilling the student model, the student\nmodel is initialized using the ﬁrst 6 layers weights\nof BERTBASE . We adopt the KL-divergence dis-\ntance as the KD objective. λKL is set to 0.5 and we\nempirically ﬁnd this setting works well. The same\ntraining hyper-parameters as ﬁne-tuning the teacher\nmodel are used for distillation. The performance is\nevaluated on the validation set and averaged on 3\nrandom seeds.\nB Impacts of Prediction Smoothness\nTo examine the inﬂuence of less-softened teacher\npredictions, we conduct distillation experiments\nwith various temperature τ using the hyper-\nparameters identical with previous experiments,\nto mimicking the sharpen impact introduced by\nthe larger teacher size. In more detail, we setup a\nstudent model with 6 layers as before and select\nthe BERTBASE as the teacher model. The results\nare illustrated in Figure 7. We observe on both\ndatasets, the decreased temperature τ will lead a\nperformance decrease. It indicates that the less-\nsoftened probability distribution indeed weakens\nthe performance of knowledge distillation.\n10 3\n10 2\n10 1\n100\nTemperature\n62.0\n62.5\n63.0\n63.5\n64.0\n64.5\n65.0Accuracy (%)\n(a) RTE\n10 3\n10 2\n10 1\n100\nTemperature\n85.8\n85.9\n86.0\n86.1\n86.2\n86.3Accuracy (%)\n (b) IMDB\nFigure 7: Distillation performance with varying tem-\nperature τ on different datasets.\nC Impacts of Teacher Hidden Size\nAs mentioned in the main paper, we observe that\nlarger teacher may not raise a student model with\nbetter performance. We further conduct experi-\nments regarding the hidden size of teacher model.\nSpeciﬁcally, we setup a student with 6 layers with\n256 hidden units. The small teacher and the large\nteacher are a BERT model of 12-layer with 256\nhidden units and a BERT of 12-layer with 768 hid-\nden units, respectively. Out experiments show that\non the CoLA dataset, the student model distilled\nwith the small teacher can achieve 11.9 matthews\ncorrelation score while that of model distilled by\nthe large teacher is 8.8. The result on the IMDB\nis consistent, i.e., 83.2 accuracy for student model\ndistilled by the large teacher and 83.4 accuracy for\nthe student distilled by the small teacher. These\nresults again verify the phenomenon that the larger\nteacher may not always raise a better student model.\nD Implementation Details\nOur implementation is based on PyTorch and Hug-\ngingface transformers library. Model is optimized\nwith AdamW optimizer with linear learning rate\nwarm-up. The sentence length is set to 64 for SST-\n5 and 128 for the rest datasets. Our teacher model\nis BERTBASE . The model is trained with learning\nrate 2e-5 and batch size 32 for 3 epochs. λKL is\nset to 0.5, with temperature τ set to 1.\nFor experiments using TinyBERT, we select\nTinyBERT4 v2 as our backbone model, and con-\nduct the general distillation for 10 epochs on the\naugmented dataset. We further train the model for3\nepochs on the augmented dataset and choose learn-\ning rates from {1e-5,2e-5,3e-5}and batch sizes\nfrom {16,32}based on the performance on the val-\nidation set. The performance are evaluated on the\ntest sets and τ is set to 1, following the practice of\nTinyBERT.",
  "topic": "Distillation",
  "concepts": [
    {
      "name": "Distillation",
      "score": 0.7780922651290894
    },
    {
      "name": "Computer science",
      "score": 0.7589766979217529
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6183353662490845
    },
    {
      "name": "Machine learning",
      "score": 0.5687524080276489
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5527254939079285
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5047985315322876
    },
    {
      "name": "Language model",
      "score": 0.4573303163051605
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4398162066936493
    },
    {
      "name": "Chemistry",
      "score": 0.08265328407287598
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}