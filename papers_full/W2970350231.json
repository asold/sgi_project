{
  "title": "Towards Zero-shot Language Modeling",
  "url": "https://openalex.org/W2970350231",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2655028980",
      "name": "Edoardo Maria Ponti",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1969142033",
      "name": "Ivan Vulić",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148165152",
      "name": "Ryan Cotterell",
      "affiliations": [
        "Technion – Israel Institute of Technology",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1972118651",
      "name": "Roi Reichart",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1927037681",
      "name": "Anna Korhonen",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2566957588",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W3216065487",
    "https://openalex.org/W4919037",
    "https://openalex.org/W2803552920",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2113069506",
    "https://openalex.org/W2111051539",
    "https://openalex.org/W2016156565",
    "https://openalex.org/W2799266483",
    "https://openalex.org/W2041532239",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2470055869",
    "https://openalex.org/W2585756977",
    "https://openalex.org/W1484082930",
    "https://openalex.org/W2000196122",
    "https://openalex.org/W4298419326",
    "https://openalex.org/W2963351454",
    "https://openalex.org/W4295371519",
    "https://openalex.org/W4248681815",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2888456631",
    "https://openalex.org/W2251324968",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W2038594453",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2619808423",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2789051838",
    "https://openalex.org/W2099111195",
    "https://openalex.org/W2964170290",
    "https://openalex.org/W2889947987",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W1964045210",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2045656233",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2171361956",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2963694384",
    "https://openalex.org/W2741986357",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963003887",
    "https://openalex.org/W2883158411",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2478708596",
    "https://openalex.org/W2105716221",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W3151534266",
    "https://openalex.org/W2062707514",
    "https://openalex.org/W2883582441",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Edoardo Maria Ponti, Ivan Vulić, Ryan Cotterell, Roi Reichart, Anna Korhonen. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing,pages 2900–2910\nHong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics\nACL Anthology [revision] 6 Aug 2021\nTowards Zero-shot Language Modeling\nEdoardo M. Ponti1, Ivan Vuli´c1, Ryan Cotterell2, Roi Reichart3, Anna Korhonen1\n1Language Technology Lab, TAL, University of Cambridge\n2Computer Laboratory, University of Cambridge\n2Faculty of Industrial Engineering and Management, Technion, IIT\n1,2{ep490,iv250,rdc42,alk23}@cam.ac.uk\n3roiri@ie.technion.ac.il\nAbstract\nCan we construct a neural model that is in-\nductively biased towards learning human lan-\nguages? Motivated by this question, we aim\nat constructing an informative prior over neu-\nral weights, in order to adapt quickly to held-\nout languages in the task of character-level lan-\nguage modeling. We infer this distribution\nfrom a sample of typologically diverse train-\ning languages via Laplace approximation. The\nuse of such a prior outperforms baseline mod-\nels with an uninformative prior (so-called ‘ﬁne-\ntuning’) in both zero-shot and few-shot set-\ntings. This shows that the prior is imbued\nwith universal phonological knowledge. More-\nover, we harness additional language-speciﬁc\nside information as distant supervision for\nheld-out languages. Speciﬁcally, we condition\nlanguage models on features from typologi-\ncal databases, by concatenating them to hid-\nden states or generating weights with hyper-\nnetworks. These features appear beneﬁcial in\nthe few-shot setting, but not in the zero-shot\nsetting. Since the paucity of digital texts af-\nfects the majority of the world’s languages, we\nhope that these ﬁndings will help broaden the\nscope of applications for language technology.\n1 Introduction\nWith the success of recurrent neural networks and\nother black-box models on core NLP tasks, such as\nlanguage modeling, researchers have turned their\nattention to the study of the inductive bias such neu-\nral models exhibit (Linzen et al., 2016; Marvin and\nLinzen, 2018; Ravfogel et al., 2018). A number of\nnatural questions have been asked. For example,\ndo recurrent neural language models learn syntax\n(Marvin and Linzen, 2018)? Do they map onto\ngrammaticality judgments (Warstadt et al., 2019)?\nHowever, as Ravfogel et al. (2019) note, “[m]ost of\nthe work so far has focused on English.” Moreover,\nthese studies have almost always focused on train-\ning scenarios where a large number of in-language\nsentences are available.\nIn this work, we aim to ﬁnd a prior distribu-\ntion over network parameters that generalize well\nto new human languages. The recent vein of re-\nsearch on the inductive biases of neural nets im-\nplicitly assumes a uniform (unnormalizable) prior\nover the space of neural network parameters (Rav-\nfogel et al., 2019, inter alia). In contrast, we take\na Bayesian-updating approach: First, we approxi-\nmate the posterior distribution over the network pa-\nrameters using the Laplace method (Azevedo-Filho\nand Shachter, 1994), given the data from a sample\nof seen training languages. Afterward, this distri-\nbution serves as a prior for maximum-a-posteriori\n(MAP) estimation of network parameters for the\nheld-out unseen languages.\nThe search for a universal prior for linguistic\nknowledge is motivated by the notion of Universal\nGrammar (UG), originally proposed by Chomsky\n(1959). The presence of innate biological proper-\nties of the brain that constrain possible human lan-\nguages was posited to explain why children learn\nlanguages so quickly despite the poverty of the\nstimulus (Chomsky, 1978; Legate and Yang, 2002).\nIn turn, UG has been connected with Greenberg\n(1963)’s typological universals by Grafﬁ (1980)\nand Gilligan (1989): this way, the patterns observed\nin cross-lingual variation could be explained by\nan innate set of parameters wired into a language-\nspeciﬁc conﬁguration during the early phases of\nlanguage acquisition.\nOur study explores the task of character-level\nlanguage modeling. Speciﬁcally, we choose an\nopen-vocabulary setup, where no token is treated\nas unknown, to allow for a fair comparison among\nthe performances of different models across differ-\nent languages (Gerz et al., 2018a,b; Cotterell et al.,\n2018; Mielke et al., 2019). We run experiments un-\nder several regimes of data scarcity for the held-out\n2900\nlanguages (zero-shot, few-shot, and joint multilin-\ngual learning) over a sample of 77 typologically\ndiverse languages.\nAs an orthogonal contribution, we also note that\nrealistically we are not completely in the dark about\nheld-out languages, as coarse-grained grammati-\ncal features are documented for most world’s lan-\nguages and available in typological databases such\nas URIEL (Littell et al., 2017). Hence, we also\nexplore a regime where we condition the universal\nprior on typological side information. In particu-\nlar, we consider concatenating typological features\nto hidden states (Östling and Tiedemann, 2017)\nand generating the network parameters with hyper-\nnetworks receiving typological features as inputs\n(Platanios et al., 2018).\nEmpirically, given the results of our study, we\noffer two ﬁndings. The ﬁrst is that neural recur-\nrent models with a universal prior signiﬁcantly out-\nperform baselines with uninformative priors both\nin zero-shot and few-shot training settings. Sec-\nondly, conditioning on typological features further\nreduces bits per character in the few-shot setting,\nbut we report negative results for the zero-shot set-\nting, possibly due to some inherent limitations of\ntypological databases (Ponti et al., 2019).\nThe study of low-resource language modeling\nalso has a practical impact. According to Simons\n(2017), 45.71% of the world’s languages do not\nhave written texts available. The situation is even\nmore dire for their digital footprint. As of March\n2015, just 40 out of the 188 languages documented\non the Internet accounted for 99.99% of the web\npages.1 And as of April 2019, Wikipedia is trans-\nlated only in 304 out of the 7097 existing languages.\nWhat is more, Kornai (2013) prognosticates that the\ndigital divide will act as a catalyst for the extinction\nof many of the world’s languages. The transfer of\nlanguage technology may help reverse this course\nand give space to unrepresented communities.\n2 LSTM Language Models\nIn this work, we address the task of character-level\nlanguage modeling. Whereas word lexicalization is\nmostly arbitrary across languages, phonemes allow\nfor transferring universal constraints on phonotac-\ntics2 and language-speciﬁc sequences that may be\nshared across languages, such as borrowings and\n1https://w3techs.com/technologies/\noverview/content_language/all\n2E.g. with few exceptions (Evans and Levinson, 2009, sec.\n2.2.2), the basic syllabic structure is vowel–consonant.\ncognates (Brown et al., 2008). Since languages\nare mostly recorded in text rather than phonemic\nsymbols (IPA), however, we focus on characters as\na loose approximation of phonemes.\nLet Σℓ be the set of characters for language\nℓ. Moreover, consider a collection of languages\nT⊔E partitioned into two disjoint sets of observed\n(training) languages T and held-out (evaluation)\nlanguages E. Then, let Σ = ∪ℓ∈(T⊔E)Σℓ be the\nunion of character sets in all languages. A univer-\nsal, character-level language model is a probability\ndistribution over Σ∗.3 Let x ∈Σ∗be a sequence\nof characters. We write:\np(x |w) =\nn∏\nt=1\np(xt |x<t,w) (1)\nwhere t is a time step, x0 is a distinguished begin-\nning-of-sentence symbol, w are the parameters,\nand every sequence x ends with a distinguished\nend-of-sentence symbol xn.\nWe implement character-level language models\nwith Long Short-Term Memory (LSTM ) networks\n(Hochreiter and Schmidhuber, 1997). These en-\ncode the entire history x<t as a ﬁxed-length vector\nht by manipulating a memory cell ct through a set\nof gates. Then we deﬁne\np(xt |x<t,w) = softmax(W ht + b). (2)\nLSTM s have an advantage over other recurrent ar-\nchitectures as memory gating mitigates the problem\nof vanishing gradients and captures long-distance\ndependencies (Pascanu et al., 2013).\n3 Neural Language Modeling with a\nUniversal Prior\nThe fundamental hypothesis of this work is that\nthere exists a prior p(w) over the weights of a\nneural language model that places high probability\non networks that describe human-like languages.\nSuch a prior would provide an inductive bias that\nfacilitates learning unseen languages. In practice,\nwe construct it as the posterior distribution over\nthe weights of a language model of seen languages.\nLet Dℓ be the examples in language ℓ, and let the\nexamples in all training languages beD= ∪ℓ∈TDℓ.\nTaking a Bayesian approach, the posterior over\n3Note that Σ is also augmented with punctuation and white\nspace, and distinguished beginning-of-sequence and end-of-\nsequence symbols, respectively.\n2901\nweights is given by Bayes’ rule:\np(w |D)\n  \nposterior\n∝\n∏\nℓ∈T\np(Dℓ |w)\n  \nlikelihood\np(w)\n\nprior\n(3)\nWe take the prior of eq. (3) to be a Gaussian with\nzero mean and covariance matrix σ2 I, i.e.\np(w) = 1√\n2πσ2 exp\n(\n− 1\n2σ2 ||w||2\n2\n)\n. (4)\nHowever, computation of the posterior p(w |D)\nis woefully intractable: recall that, in our setting,\neach p(x |w) is an LSTM language model, like the\none deﬁned in eq. (2). Hence, we opt for a simple\napproximation of the posterior, using the classic\nLaplace method (MacKay, 1992). This method has\nrecently been applied to other transfer learning or\ncontinuous learning scenarios in the neural network\nliterature (Kirkpatrick et al., 2017; Kochurov et al.,\n2018; Ritter et al., 2018).\nIn §3.1, we ﬁrst introduce the Laplace method,\nwhich approximates the posterior with a Gaussian\ncentered at the maximum-likelihood estimate.4 Its\ncovariance matrix is amenable to be computed with\nbackpropagation, as detailed in §3.2. Finally, we\ndescribe how to use this distribution as a prior to\nperform maximum-a-posteriori inference over new\ndata in §3.3.\n3.1 Laplace Method\nFirst, we (locally) maximize the logarithm of the\nRHS of eq. (3):\nL(w) =\n∑\nℓ∈T\nlog p(Dℓ |w) + logp(w) (5)\nWe note thatL(w) is equivalent to the log-posterior\nup to an additive constant, i.e.\nlog p(w |D) = L(w) −log p(D) (6)\nwhere the constant log p(D) is the log-normalizer.\nLet w⋆ be a local maximizer of L.5 We now ap-\nproximate the log-posterior with a second-order\nTaylor expansion around w⋆:\nlog p(w |D) ≈ (7)\nL(w⋆) + 1\n2(w−w⋆)⊤H (w −w⋆) −log p(D)\n4Note that, in general, the true posterior is multi-modal.\nThe Laplace method instead approximates it with a unimodal\ndistribution.\n5In practice, non-convex optimization is only guaranteed to\nreach a critical point, which could be a saddle point. However,\nthe derivation of Laplace’s method assumes that we do reach\na maximizer.\nwhere H is the Hessian matrix. Note that we\nhave omitted the ﬁrst-order term, since the gra-\ndient ∇L(w) = 0 at the local maximizer w⋆.\nThis quadratic approximation to the log-posterior\nis Gaussian, which can be seen by exponentiating\nthe RHS of eq. (7):\nexp\n[\n−1\n2 (w −w⋆)⊤(−H)(w −w⋆)\n]\n√\n(2π)d|−H|−1\n≜ N(w⋆,−H−1) (8)\nwhere exp(L(w⋆)) is simpliﬁed from both numer-\nator and denominator. Since w⋆ is a local maxi-\nmizer, H is a negative semi-deﬁnite matrix.6 The\nfull derivation is given in App. C.\nIn principle, computing the Hessian is possible\nby running backpropagation twice: This yields a\nmatrix with d2 entries. However, in practice, this is\nnot possible. First, running backpropagation twice\nis tedious. Second, we can not easily store a matrix\nwith d2 entries since dis the number of parameters\nin the language model, which is exceedingly large.\n3.2 Approximating the Hessian\nTo cut the computation down to one pass, we ex-\nploit a property from theoretical statistics: Namely,\nthat the Hessian of the log-likelihood bears a close\nresemblance to a quantity known as the Fisher in-\nformation matrix. This connection allows us to de-\nvelop a more efﬁcient algorithm that approximates\nthe Hessian with one pass of backpropagation.\nWe derive this approximation to the Hessian of\nL(w) here. First, we note that due to the linearity\nof ∇2, we have\nH = ∇2L(w)\n= ∇2\n(∑\nℓ∈T\nlog p(Dℓ |w) + logp(w)\n)\n=\n∑\nℓ∈T\n∇2 log p(Dℓ |w)\n  \nlikelihood\n+ ∇2 log p(w)\n  \nprior\n(9)\nNote that the integral over languages ℓ ∈T is a\ndiscrete summation, so we may exchange addends\nand derivatives such as is required for the proof.\nWe now discuss each term of eq. (9) individually.\nFirst, to approximate the likelihood term, we draw\non the relation between the Hessian and the Fisher\n6Note that, as a result, our representation of the Gaussian\nis non-standard; generally, the precision matrix is positive\nsemi-deﬁnite.\n2902\ninformation matrix. A basic fact from information\ntheory (Cover and Thomas, 2006) gives us that the\nFisher information matrix may be written in two\nequivalent ways:\n−E\n[\n∇2 log p(D| w)\n]\n(10)\n= E\n[\n∇log p(D| w) ∇log p(D| w)⊤\n]\n  \nexpected Fisher information matrix\nThis equality suggests a natural approximation of\nthe expected Fisher information matrix—the ob-\nserved Fisher information matrix\n− 1\n|D|\n∑\nx∈D\n∇2 log p(x |w) (11)\n≈ 1\n|D|\n∑\nx∈D\n∇log p(x |w) ∇log p(x |w)⊤\n  \nobserved Fisher information matrix\nwhich is tight in the limit as |D|→∞ due to the\nlaw of large numbers. Indeed, when we have a\nlarge number of training exemplars, the average\nof the outer products of the gradients will be a\ngood approximation to the Hessian. However, even\nthis approximation still has d2 entries, which is far\ntoo many to be practical. Thus, we further use a\ndiagonal approximation. We denote the diagonal\nof the observed Fisher information matrix as the\nvector f ∈Rd, which we deﬁne as\nf =\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|\n[\n∇log p(x |w)\n]2 (12)\nwhere the (·)2 is applied point-wise. Computation\nof the Hessian of the prior term in eq. (9) is more\nstraightforward and does not require approximation.\nIndeed, generally, this is the negative inverse of the\ncovariance matrix, which in our case means\n∇2 log p(w) = −1\nσ2 I (13)\nSumming the (approximate) Hessian of the log-\nlikelihood in eq. (12) and the Hessian of the prior\nin eq. (13) yields our approximation to the Hessian\nof the log-posterior\n˜H = −diag(f) − 1\nσ2 I (14)\nThe full derivation of the approximated Hessian is\navailable in App. D.\n3.3 MAP Inference\nFinally, we harness the posterior p(w | D) ≈\nN(w⋆,−˜H−1) as the prior over model parame-\nters for training a language model on new, held-out\nlanguages via MAP estimation. This is only an\napproximation to full Bayesian inference, because\nit does not characterize the entire distribution of\nthe posterior, just the mode (Gelman et al., 2013).\nIn the zero-shot setting, this boils down to using\nthe mean of the prior w⋆ as network parameters\nduring evaluation. In the few-shot setting, instead,\nwe assume that some data for the target language\nℓ ∈E is available. Therefore, we maximize the\nlog-likelihood given the target language data plus\na regularizer that incarnates the prior, scaled by a\nfactor of λ:\nL(w) =\n∑\nℓ∈E\nlog p(Dℓ |w) (15)\n+ λ\n2 (w −w⋆)⊤˜H (w −w⋆)\nWe denote the the prior N(w⋆,−˜H−1) that fea-\ntures in eq. (15) as UNIV , as it incorporates uni-\nversal linguistic knowledge. As a baseline for this\nobjective, we perform MAP inference with an unin-\nformative prior N(0,I), which we label NINF . In\nthe zero-shot setting, this means that the parame-\nters are sampled from the uninformative prior. In\nthe few-shot setting, we maximize\nL(w) =\n∑\nℓ∈E\nlog p(Dℓ |w) −λ\n2 ||w||2\n2 (16)\nNote that, owing to this formulation, the unin-\nformed NINF model does not have access to the\nposterior of the weights given the data from the\ntraining languages.\nMoreover, as an additional baseline, we consider\na common approach for transfer learning in neural\nnetworks (Ruder, 2017), namely ‘ﬁne-tuning.’ Af-\nter ﬁnding the maximum-likelihood value w⋆ on\nthe training data, this is simply used to initialize\nthe weights before further optimizing them on the\nheld-out data. We label this method FITU.\n4 Language Modeling Conditioned on\nTypological Features\nRealistically, the prior over network weights should\nalso be augmented with side information about the\ngeneral properties of the held-out language to be\nlearned, if such information is available. In fact,\n2903\nlinguists have documented such information even\nfor languages without plain digital texts available\nand stored it in the form of attribute–value features\nin publicly accessible databases (Croft, 2002; Dryer\nand Haspelmath, 2013).\nThe usage of such features to inform neural NLP\nmodels is still scarce, partly because the evidence\nin favor of their effectiveness is mixed (Ponti et al.,\n2018, 2019). In this work, we propose a way to\ndistantly supervise the model with this side infor-\nmation effectively. We extend our non-conditional\nlanguage models outlined in §3 (BARE ) to a series\nof variants conditioned on language-speciﬁc prop-\nerties, inspired by Östling and Tiedemann (2017)\nand Platanios et al. (2018). A fundamental differ-\nence from these previous works, however, is that\nthey learn such properties in an end-to-end fash-\nion from the data in a joint multilingual learning\nsetting. Obviously, this is not feasible for the zero-\nshot setting and unreliable for the few-shot setting.\nRather, we represent languages with their typologi-\ncal feature vector, which we assume to be readily\navailable both for both training and held-out lan-\nguages.\nLet tℓ ∈ [0,1]f be a vector of f typological\nfeatures for language ℓ∈T⊔E . We reinterpret the\nconditional language models within the Bayesian\nframework by estimating their posterior probability\np(w |D,F) ∝\n∏\nℓ∈T\np(Dℓ |w) p(w |tℓ) (17)\nWe now consider two possible methods to estimate\np(w | tℓ). For both of them, we ﬁrst encode\nthe features through a non-linear transformation\nf(tℓ) = ReLU(W tℓ+ b), where W ∈Rr×f and\nb ∈Rr, r ≪f. A ﬁrst variant, labeled OEST , is\nbased on Östling and Tiedemann (2017). Assum-\ning the standard LSTM architecture where ot is the\noutput gate and ct is the memory cell, we modify\nthe equation for the hidden state ht as follows:\nht =\n(\not ⊙tanh(ct)\n)\n⊕f(tℓ) (18)\nwhere ⊙stands for the Hadamard product and ⊕\nfor concatenation. In other words, we concatenate\nthe typological features to all the hidden states.\nMoreover, we experiment with a second variant\nwhere the parameters of the LSTM are generated\nby a hyper-network (i.e., a simple linear layer with\nweight W ∈R|w|×r) that transforms f(tℓ) into\nw. This approach, labeled PLAT, is inspired by\nPlatanios et al. (2018), with the difference that they\ngenerate parameters for an encoder-decoder archi-\ntecture for neural machine translation.\nOn the other hand, we do not consider the condi-\ntional model proposed by Sutskever et al. (2014),\nwhere f(tℓ) would be used to initialize the val-\nues for h0 and c0. During the evaluation, for all\ntime steps t, ht and ct are never reset on sentence\nboundaries, so this model would ﬁnd itself at a dis-\nadvantage because it would require either to erase\nthe sequential history cyclically or to lose memory\nof the typological features.\n5 Experimental Setup\nData The source for our textual data is the\nBible corpus7 (Christodouloupoulos and Steedman,\n2015).8 We exclude languages that are not written\nin the Latin script and duplicate languages, result-\ning in a sample of 77 languages. 9 Since not all\ntranslations cover the entire Bible, they vary in\nsize. The text from each language is split into train-\ning, development, and evaluation sets (80-10-10\npercent, respectively). Moreover, to perform MAP\ninference in the few-shot setting, we randomly sam-\nple 100 sentences from the train set of each held-out\nlanguage.\nWe obtain the typological feature vectors from\nURIEL (Littell et al., 2017).10 We include the fea-\ntures related to 3 levels of linguistic structure, for\na total of 245 features: i) syntax, e.g. whether the\nsubject tends to precede the object. These origi-\nnate from the World Atlas of Language Structures\n(Dryer and Haspelmath, 2013) and the Syntactic\nStructures of the World’s Languages (Collins and\nKayne, 2009); ii) phonology, e.g. whether a lan-\nguage has distinctive tones; iii) phonological in-\nventories, e.g. whether a language possesses the\nretroﬂex approximant /õ/. Both ii) and iii) were\noriginally collected in PHOIBLE (Moran et al.,\n2014). Missing values are inferred as a weighted\naverage of the 10 nearest neighbor languages in\nterms of family, geography, and typology.\n7http://christos-c.com/bible/\n8This corpus is arguably representative of the variety of the\nworld’s languages: it covers 28 families, several geographic\nareas (16 languages from Africa, 23 from Americas, 26 from\nAsia, 33 from Europe, 1 from Oceania), and endangered or\npoorly documented languages (39 with less than 1M speakers).\n9These are identiﬁed with their 3-letter ISO 639-3 codes\nthroughout the paper. For the corresponding language names,\nconsult www.iso.org/standard/39534.html.\n10www.cs.cmu.edu/~dmortens/uriel.html\n2904\nNINF UNIV NINF UNIV NINF UNIV\nBARE BARE OEST BARE BARE OEST BARE BARE OEST\nacu 8.491 3.244 3.472 fra 8.587 4.066 4.467 por 8.491 3.751 4.219\nafr 8.607 3.229 3.995 gbi 8.610 3.823 3.912 pot 8.600 5.336 5.359\nagr 8.603 3.779 3.946 gla 8.490 4.179 3.956 ppk 8.596 4.506 4.599\nake 8.602 5.753 6.281 glv 8.606 4.349 4.612 quc 8.605 4.063 4.118\nalb 8.490 4.571 5.017 hat 8.594 4.186 4.620 quw 8.488 3.560 4.027\namu 8.610 4.912 5.959 hrv 8.606 4.050 3.441 rom 8.603 3.669 4.056\nbsn 8.591 5.046 5.695 hun 8.493 4.836 5.030 ron 8.588 5.011 5.690\ncak 8.603 4.068 4.326 ind 8.604 3.796 4.311 shi 8.601 5.496 5.946\nceb 8.488 3.668 3.850 isl 8.596 5.039 5.629 slk 8.491 4.304 4.512\nces 8.600 4.369 4.461 ita 8.605 4.023 3.752 slv 8.604 3.661 4.106\ncha 8.594 4.366 4.353 jak 8.488 4.051 4.793 sna 8.596 4.146 4.283\nchq 8.598 6.940 7.623 jiv 8.601 3.866 4.039 som 8.614 4.159 4.470\ncjp 8.494 4.600 4.985 kab 8.596 4.659 5.400 spa 8.489 3.645 4.020\ncni 8.604 3.740 4.651 kbh 8.607 4.663 4.950 srp 8.604 3.414 3.437\ndan 8.593 3.471 4.599 kek 8.491 4.666 4.944 ssw 8.593 4.064 3.780\ndeu 8.599 4.102 4.214 lat 8.601 3.703 4.093 swe 8.605 4.210 3.892\ndik 8.490 4.447 4.533 lav 8.588 5.415 6.130 tgl 8.487 3.639 3.878\ndje 8.603 3.725 3.996 lit 8.602 4.794 4.853 tmh 8.602 4.830 4.711\ndjk 8.592 3.663 3.874 mam 8.488 4.292 5.076 tur 8.592 5.574 5.935\ndop 8.609 5.950 7.351 mri 8.606 3.440 4.074 usp 8.604 4.127 4.337\neng 8.488 3.816 4.028 nhg 8.588 4.323 4.450 vie 8.490 7.137 7.484\nepo 8.605 3.818 4.116 nld 8.601 3.851 4.326 wal 8.605 4.027 4.585\nest 8.606 6.807 8.261 nor 8.492 3.174 3.902 wol 8.607 4.290 4.420\neus 8.605 4.118 4.321 pck 8.603 4.053 4.233 xho 8.602 4.171 4.276\newe 8.490 5.049 5.497 plt 8.603 4.364 4.648 zul 8.488 3.218 4.109\nﬁn 8.604 4.308 4.338 pol 8.601 5.158 5.556 ALL 8.572 4.343 4.691\nTable 1: BPC scores (lower is better) for the Z ERO -SHOT learning setting, with the uninformed prior (N INF ) and\nthe universal prior (U NIV ): see §2 for the descriptions of the priors. Note that for N INF there is no difference\nbetween a B ARE model and a conditional model (O EST ). Colors deﬁne the partition in which each language\n(rows) has been held out.\nBARE OEST BARE OEST BARE OEST BARE OEST\nacu 1.413 1.308 eng 1.355 1.350 kek 1.131 1.133 slk 1.844 1.754\nafr 1.471 1.457 epo 1.471 1.450 lat 1.792 1.758 slv 1.848 1.793\nagr 1.701 1.581 est 0.333 0.150 lav 2.146 1.931 sna 1.489 1.457\nake 1.453 1.377 eus 1.763 1.635 lit 1.895 1.833 som 1.477 1.468\nalb 1.590 1.552 ewe 2.084 1.944 mam 1.654 1.548 spa 1.559 1.525\namu 1.402 1.340 ﬁn 1.716 1.680 mri 1.342 1.330 srp 1.832 1.756\nbsn 1.232 1.172 fra 1.465 1.432 nhg 1.302 1.238 ssw 1.890 1.697\ncak 1.281 1.221 gbi 1.398 1.331 nld 1.621 1.601 swe 1.619 1.595\nceb 1.193 1.185 gla 3.403 1.839 nor 1.623 1.590 tgl 1.221 1.210\nces 1.872 1.795 glv 1.932 1.644 pck 1.731 1.711 tmh 2.786 2.301\ncha 1.934 1.790 hat 1.480 1.454 plt 1.296 1.286 tur 1.801 1.773\nchq 1.265 1.220 hrv 2.059 1.974 pol 1.743 1.698 usp 1.290 1.214\ncjp 1.706 1.565 hun 1.887 1.847 por 1.586 1.552 vie 1.648 1.637\ncni 1.348 1.290 ind 1.356 1.336 pot 2.484 2.144 wal 1.561 1.457\ndan 1.727 1.693 isl 1.845 1.808 ppk 1.538 1.439 wol 2.053 1.890\ndeu 1.532 1.512 ita 1.615 1.583 quc 1.393 1.291 xho 1.680 1.634\ndik 1.979 1.835 jak 1.415 1.322 quw 1.498 1.418 zul 1.880 1.620\ndje 1.570 1.550 jiv 1.705 1.572 rom 1.706 1.587 ALL 1.652 1.550\ndjk 1.515 1.435 kab 1.955 1.791 ron 1.572 1.537\ndop 1.810 1.676 kbh 1.436 1.371 shi 2.057 1.903\nTable 2: BPC results (lower is better) for the JOINT learning setting, with the uninformed NINF prior. These results\nconstitute the expected ceiling performance for language transfer models.\n2905\nNINF FITU UNIV NINF FITU UNIV\nBARE OEST BARE OEST BARE OEST BARE OEST\nacu 4.203 2.117 2.551 2.136 kbh 4.644 2.362 2.434 2.288\nafr 4.423 3.620 3.042 2.773 kek 4.613 2.809 3.015 2.714\nagr 4.268 3.282 3.403 2.457 lat 4.239 4.342 3.416 3.202\nake 4.318 2.168 2.238 2.180 lav 4.765 2.867 3.842 2.917\nalb 4.544 3.186 3.302 3.084 lit 4.769 3.752 3.592 3.668\namu 4.486 2.820 3.948 2.080 mam 4.525 2.274 2.873 2.363\nbsn 4.546 1.861 2.678 1.850 mri 3.795 3.482 3.010 2.459\ncak 4.426 1.994 2.053 1.956 nhg 4.373 2.004 2.480 1.965\nceb 4.084 2.562 2.595 2.470 nld 4.469 3.008 2.908 2.903\nces 4.984 4.651 4.190 3.680 nor 4.453 3.152 2.954 3.054\ncha 4.329 2.546 2.899 2.525 pck 4.246 4.011 3.532 3.030\nchq 4.941 1.948 2.078 1.963 plt 4.201 2.532 2.742 2.490\ncjp 4.424 2.389 2.880 2.393 pol 4.853 3.852 3.620 3.788\ncni 4.185 2.797 3.018 1.982 por 4.446 3.231 3.198 3.098\ndan 4.719 3.211 3.127 3.180 pot 4.299 3.773 3.944 2.763\ndeu 4.589 3.103 3.007 2.953 ppk 4.439 2.220 2.736 2.236\ndik 4.380 2.640 3.020 2.667 quc 4.538 2.154 2.242 2.108\ndje 4.382 3.815 3.398 2.898 quw 4.223 2.196 2.547 2.158\ndjk 4.130 2.064 2.446 2.085 rom 4.378 3.121 3.257 2.455\ndop 4.508 2.506 2.562 2.448 ron 4.579 3.273 3.734 3.216\neng 4.436 2.808 2.913 2.719 shi 4.509 2.963 3.092 2.970\nepo 4.469 3.609 3.511 2.825 slk 4.873 3.722 3.812 3.631\nest 3.618 1.952 2.487 1.962 slv 4.633 4.630 3.527 3.501\neus 4.354 2.628 2.705 2.567 sna 4.455 2.910 3.114 2.870\newe 4.590 2.806 3.336 2.786 som 4.257 3.048 2.908 2.934\nﬁn 4.385 4.339 3.830 3.312 spa 4.507 3.223 3.149 3.090\nfra 4.551 3.086 3.276 2.981 srp 4.561 4.467 3.367 3.380\ngbi 4.250 2.138 2.170 2.054 ssw 4.370 2.611 2.924 2.570\ngla 4.159 2.377 2.835 2.395 swe 4.657 3.266 3.184 3.177\nglv 4.346 3.523 3.702 2.644 tgl 4.060 2.546 2.592 2.436\nhat 4.468 2.929 3.048 2.849 tmh 4.618 4.087 4.218 3.125\nhrv 4.615 3.845 3.608 3.588 tur 4.846 3.509 4.282 3.552\nhun 4.806 3.589 3.709 3.522 usp 4.529 2.114 2.189 2.073\nind 4.377 3.317 3.258 2.420 vie 5.185 3.018 3.751 3.015\nisl 4.744 3.174 3.703 3.101 wal 4.398 2.986 3.623 2.278\nita 4.370 3.384 3.196 3.178 wol 4.621 2.898 2.968 2.826\njak 4.532 2.113 2.650 2.126 xho 4.561 3.415 3.208 3.289\njiv 4.338 3.413 3.475 2.504 zul 4.564 2.625 2.866 2.622\nkab 4.649 2.783 3.574 2.800 ALL 4.467 3.007 3.120 2.731\nTable 3: BPC scores (lower is better) for the F EW-SHOT learning setting, with N INF , F ITU and U NIV priors.\nColors deﬁne the partition in which each language (rows) has been held out.\nLanguage Model We implement the LSTM\nfollowing the best practices and choosing the\nhyper-parameter settings indicated by Merity et al.\n(2018b,a). Speciﬁcally, we optimize the neural\nweights with Adam (Kingma and Ba, 2014) and a\nnon-monotonically decayed learning rate: its value\nis initialized as 10−4 and decreases by a factor of\n10 every 1/3rd of the total epochs. The maximum\nnumber of epochs amounts to 6 for training on DT,\nwith early stopping based on development set per-\nformance, and the maximum number of epochs is\n25 for few-shot learning on Dℓ∈E.\nFor each iteration, we sample a language pro-\nportionally to the amount of its data: p(ℓ) ∝|Dℓ|,\nin order not to exhaust examples from resource-\nlean languages in the early phase of training. Then,\nwe sample without replacement from Dℓ a mini-\nbatch of 128 sequences with a variable maximum\nsequence length.11 This length is sampled from\na distribution m ∼N (µ = 125,σ = 5).12 Each\nepoch ends when all the data sequences have been\nsampled.\n11This avoids creating insurmountable boundaries to back-\npropagation through time (Tallec and Ollivier, 2017).\n12The learning rate is therefore scaled by ⌊m⌉\nµ · |DT |\n|T|·|Dℓ|,\nwhere ⌊·⌉is an operator that rounds to the closest integer.\n2906\nWe apply several techniques of dropout for regu-\nlarization, including variational dropout (Gal and\nGhahramani, 2016), which applies an identical\nmask to all the time steps, with p = 0.1 for char-\nacter embeddings and intermediate hidden states\nand p = 0 .4 for the output hidden states. Drop-\nConnect (Wan et al., 2013) is applied to the model\nparameters U of the ﬁrst hidden layer with p= 0.2.\nFollowing Merity et al. (2018b), the underlying\nlanguage model architecture consists of 3 hidden\nlayers with 1,840 hidden units each. The dimen-\nsionality of the character embeddings is 400. We\ntie input and output embeddings following Merity\net al. (2018a). For conditional language models, the\ndimensionality of f(tℓ) is set to 115 for the OEST\nmethod based on concatenation (Östling and Tiede-\nmann, 2017), and 4 (due to memory limitations)\nin the PLAT method based on hyper-networks (Pla-\ntanios et al., 2018). For the regularizer in eq. (15),\nwe perform grid search over the hyper-parameter\nλ: we ﬁnally select a value of 105 for UNIV and\n10−5 for NINF .\nRegimes of Data Paucity We explore different\nregimes of data paucity for the held-out languages:\n•ZERO -SHOT transfer setting: we split the sample\nof 77 languages into 4 partitions. The languages in\neach subset are held out in turn, and we use their\ntest set for evaluation.13 For each subset, we further\nrandomly choose 5 languages whose development\nset is used for validation. The training set of the\nrest of the languages is used to estimate a prior over\nnetwork parameters via the Laplace approximation.\n•FEW-SHOT transfer setting: on top of the zero-\nshot setting, we use the prior to perform MAP in-\nference over a small sample (100 sentences) from\nthe training set of each held-out language.\n•JOINT multilingual setting: the data includes\nthe full training set for all 77 languages, including\nheld-out languages. This serves as a ceiling for the\nmodel performance in cross-lingual transfer.\n6 Results and Analysis\nThe results for our experiments are grouped in Ta-\nble 1 for the ZERO -SHOT regime, in Table 3 for the\nFEW-SHOT regime, and in Table 2 for the JOINT\nmultilingual regime, which constitutes a ceiling to\ncross-lingual transfer performances. The scores\nrepresent Bits Per Character (BPC; Graves, 2013):\n13Holding out each language individually would not in-\ncrease the sample of training languages signiﬁcantly, while\ninﬂating the number of experimental runs needed.\nthis metric is simply deﬁned as the negative log-\nlikelihood of test data divided by ln 2. We compare\nthe results along the following dimensions:\nInformativeness of Prior Our main result is that\nthe UNIV prior consistently outperforms the NINF\nprior across the board and by a large margin in both\nZERO -SHOT and FEW-SHOT settings. The scores of\nthe naïvest baseline, ZERO -SHOT NINF BARE , are\nconsiderably worse than both ZERO -SHOT UNIV\nmodels: this suggests that the transfer of informa-\ntion on character sequences is meaningful. The low-\nest BPC reductions are observed for languages like\nVietnamese (15.94% error reduction) or Highland\nChinantec (19.28%) where character inventories\ndiffer the most from other languages. Moreover,\nthe ZERO -SHOT UNIV models are on a par or better\nthan even the FEW-SHOT NINF models. In other\nwords, the most helpful supervision comes from a\nuniversal prior rather than from a small in-language\nsample of sentences. This demonstrates that the\nUNIV prior is truly imbued with universal linguis-\ntic knowledge that facilitates learning of previously\nunseen languages.\nThe averaged BPC score for the other baseline\nwithout a prior, FINE -TUNE , is 3.007 for FEW-\nSHOT OEST , to be compared with 2.731 BPC of\nUNIV . Note that ﬁne-tuning is an extremely com-\npetitive baseline, as it lies at the core of most state-\nof-the-art NLP models (Peters et al., 2019). Hence,\nthis result demonstrates the usefulness of Bayesian\ninference in transfer learning.\nConditioning on Typological InformationAn-\nother important result regards the fact that condi-\ntioning language models on typological features\nyields opposite effects in the ZERO -SHOT and FEW-\nSHOT settings. Comparing the columns of the\nBARE and OEST models in Table 1 reveals that\nthe non-conditional baseline BARE is superior for\n71 / 77 languages (the exceptions being Chamorro,\nCroatian, Italian, Swazi, Swedish, and Tuareg). On\nthe other hand, the same columns in Table 3 and Ta-\nble 2 reveal an opposite pattern: OEST outperforms\nthe BARE baseline in 70 / 77 languages. Finally,\nOEST surpasses the BARE baseline in the JOINT\nsetting for 76 / 77 languages (save Q’eqchi’).\nWe also also take into consideration an alter-\nnative conditioning method, namely PLAT. For\nclarity’s sake, we exclude this batch of results from\nTable 1 and Table 3, as this method proves to be\nconsistently worse than OEST . In fact, the average\n2907\nBPC of PLAT amounts to 5.479 in the ZERO -SHOT\nsetting and 3.251 in the FEW-SHOT setting. These\nscores have to be compared with 4.691 and 2.731\nfor OEST , respectively.\nThe possible explanation behind the mixed evi-\ndence on the success of typological features points\nto some intrinsic ﬂaws of typological databases.\nPonti et al. (2019) has shown how their feature\ngranularity may be too coarse to liaise with data-\ndriven probabilistic models, and inferring missing\nvalues due to the limited coverage of features re-\nsults in additional noise. As a result, language mod-\nels seem to be damaged by typological features in\nabsence of data, whereas they beneﬁt from their\nguidance when at least a small sample of sentences\nis available in the FEW-SHOT setting.\nData Paucity Different regimes of data paucity\ndisplay uneven levels of performance. The best\nmodels for each setting (ZERO -SHOT UNIV BARE ,\nFEW-SHOT UNIV OEST , and JOINT OEST ) reveal\nlarge gaps between their average scores. Hence, in-\nlanguage supervision remains the best option when\navailable: transferred language models always lag\nbehind their supervised equivalents.\n7 Related Work\nLSTMs have been probed for their inductive bias to-\nwards syntactic dependencies (Linzen et al., 2016)\nand grammaticality judgments (Marvin and Linzen,\n2018; Warstadt et al., 2019). Ravfogel et al. (2019)\nhave extended the scope of this analysis to typolog-\nically different languages through synthetic varia-\ntions of English. In this work, we aim to model\nthe inductive bias explicitly by constructing a prior\nover the space of neural network parameters.\nFew-shot word-level language modeling for truly\nunder-resourced languages such as Yongning Na\nhas been investigated by Adams et al. (2017)\nwith the aid of a bilingual lexicon. Vinyals et al.\n(2016) and Munkhdalai and Trischler (2018) pro-\nposed novel architectures (Matching Networks and\nLSTMs augmented with Hebbian Fast Weights, re-\nspectively) for rapid associative learning in English,\nand evaluated them in few-shot cloze tests. In this\nrespect, our work is novel in pushing the problem\nto its most complex formulation, zero-shot infer-\nence, and in taking into account the largest sample\nof languages for language modeling to date.\nIn addition to those considered in our work, there\nare also alternative methods to condition language\nmodels on features. Kalchbrenner and Blunsom\n(2013) used encoded features as additional biases\nin recurrent layers. Kiros et al. (2014) put forth a\nlog-bilinear model that allows for a ‘multiplicative\ninteraction’ between hidden representations and\ninput features (such as images). With a similar de-\nvice, but a different gating method, Tsvetkov et al.\n(2016) trained a phoneme-level joint multilingual\nmodel of words conditioned on typological features\nfrom Moran et al. (2014).\nThe use of the Laplace method for neural trans-\nfer learning has been proposed by Kirkpatrick et al.\n(2017), inspired by synaptic consolidation in neuro-\nscience, with the aim to avoid catastrophic forget-\nting. Kochurov et al. (2018) tackled the problem\nof continuous learning by approximating the pos-\nterior probabilities through stochastic variational\ninference. Ritter et al. (2018) substitute diagonal\nLaplace approximation with a Kronecker factored\nmethod, leading to better uncertainty estimates.\nFinally, the regularizer proposed by Duong et al.\n(2015) for cross-lingual dependency parsing can be\ninterpreted as a prior for MAP estimation where\nthe covariance is an identity matrix.\n8 Conclusions\nIn this work, we proposed a Bayesian approach to\ntransfer language models cross-lingually. We cre-\nated a universal prior over neural network weights\nthat is capable of generalizing well to new lan-\nguages suffering from data paucity. The prior was\nconstructed as the posterior of the weights given\nthe data from available training languages, inferred\nvia the Laplace method. Based on the results of\ncharacter-level language modeling on a sample of\n77 languages, we demonstrated the superiority of\nthis prior imbued with universal linguistic knowl-\nedge over uninformative priors and unnormalizable\npriors (i.e., the widespread ﬁne-tuning approach)\nin both zero-shot and few-shot settings. Moreover,\nwe showed that adding language-speciﬁc side in-\nformation drawn from typological databases to the\nuniversal prior further increases the levels of perfor-\nmance in the few-shot regime. While cross-lingual\ntransfer still lags behind supervised learning when\nsufﬁcient in-language data are available, our work\nis a step towards bridging this gap in the future.\nAcknowledgements\nThis work is supported by the ERC Consolidator\nGrant LEXICAL (no 648909). RR was partially\nfunded by ISF personal grants No. 1625/18.\n2908\nReferences\nOliver Adams, Adam Makarucha, Graham Neubig,\nSteven Bird, and Trevor Cohn. 2017. Cross-lingual\nword embeddings for low-resource language model-\ning. In Proceedings of EACL, pages 937–947.\nAdriano Azevedo-Filho and Ross D. Shachter. 1994.\nLaplace’s method approximations for probabilistic\ninference in belief networks with continuous vari-\nables. In Proceedings of UAI, pages 28–36.\nCecil H. Brown, Eric W. Holman, Søren Wichmann,\nand Viveka Velupillai. 2008. Automated classiﬁca-\ntion of the world’s languages: A description of the\nmethod and preliminary results. STUF-Language\nTypology and Universals Sprachtypologie und Uni-\nversalienforschung, 61(4):285–308.\nNoam Chomsky. 1959. A review of B.F. Skinner’s Ver-\nbal Behavior. Language, 35(1):26–58.\nNoam Chomsky. 1978. A naturalistic approach to lan-\nguage and cognition. Cognition and Brain Theory ,\n4(1):3–22.\nChristos Christodouloupoulos and Mark Steedman.\n2015. A massively parallel corpus: The Bible in\n100 languages. Language Resources and Evalua-\ntion, 49(2):375–395.\nChris Collins and Richard Kayne. 2009. Syntactic\nstructures of the world’s languages. http://\nsswl.railsplayground.net/.\nRyan Cotterell, Sabrina J. Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of NAACL-HLT,\npages 536–541.\nThomas M. Cover and Joy A. Thomas. 2006. Elements\nof Information Theory. Wiley-Interscience.\nWilliam Croft. 2002. Typology and Universals. Cam-\nbridge University Press.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online. Max Planck Institute for Evo-\nlutionary Anthropology.\nLong Duong, Trevor Cohn, Steven Bird, and Paul Cook.\n2015. Low resource dependency parsing: Cross-\nlingual parameter sharing in a neural network parser.\nIn Proceedings of ACL, pages 845–850.\nNicholas Evans and Stephen C. Levinson. 2009. The\nmyth of language universals: Language diversity\nand its importance for cognitive science. Behavioral\nand Brain Sciences, 32(5):429–448.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Proceedings of NeurIPS, pages\n1019–1027.\nAndrew Gelman, Hal S. Stern, John B. Carlin, David B.\nDunson, Aki Vehtari, and Donald B. Rubin. 2013.\nBayesian data analysis. Chapman and Hall/CRC.\nDaniela Gerz, Ivan Vuli´c, Edoardo Ponti, Jason Narad-\nowsky, Roi Reichart, and Anna Korhonen. 2018a.\nLanguage modeling for morphologically rich lan-\nguages: Character-aware modeling for word-level\nprediction. Transactions of the Association of Com-\nputational Linguistics, 6:451–465.\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018b. On the rela-\ntion between linguistic typology and (limitations of)\nmultilingual language modeling. In Proceedings of\nEMNLP, pages 316–327.\nGary Martin Gilligan. 1989. A cross-linguistic ap-\nproach to the pro-drop parameter. Ph.D. thesis, Uni-\nversity of Southern California.\nGiorgio Grafﬁ. 1980. Universali di Greenberg e gram-\nmatica generativa in la nozione di tipo e le sue arti-\ncolazioni nelle discipline del linguaggio. Lingua e\nStile Bologna, 15(3):371–387.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850.\nJoseph H. Greenberg. 1963. Some universals of gram-\nmar with particular reference to the order of mean-\ningful elements. Universals of Language, 2:73–113.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\nEMNLP, pages 1700–1709.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the National Academy of Sciences ,\n114(13):3521–3526.\nRyan Kiros, Ruslan Salakhutdinov, and Rich Zemel.\n2014. Multimodal neural language models. In Pro-\nceedings of ICML, pages 595–603.\nMax Kochurov, Timur Garipov, Dmitry Podoprikhin,\nDmitry Molchanov, Arsenii Ashukha, and Dmitry\nVetrov. 2018. Bayesian incremental learning for\ndeep neural networks. In Proceedings of ICLR\n(Workshop Papers).\nAndrás Kornai. 2013. Digital language death. PloS\nOne, 8(10):e77056.\nJulie Anne Legate and Charles D. Yang. 2002. Em-\npirical re-assessment of stimulus poverty arguments.\nThe Linguistic Review, 18(1-2):151–162.\n2909\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nPatrick Littell, David R. Mortensen, Ke Lin, Kather-\nine Kairis, Carlisle Turner, and Lori Levin. 2017.\nURIEL and lang2vec: Representing languages as\ntypological, geographical, and phylogenetic vectors.\nIn Proceedings of EACL, pages 8–14.\nDavid JC MacKay. 1992. A practical Bayesian frame-\nwork for backpropagation networks. Neural compu-\ntation, 4(3):448–472.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of EMNLP, pages 1192–1202.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian\nRoark, and Jason Eisner. 2019. What kind of lan-\nguage is hard to language-model? In Proceedings\nof ACL, pages 4975–4989.\nSteven Moran, Daniel McCloy, and Richard Wright, ed-\nitors. 2014. PHOIBLE Online. Max Planck Institute\nfor Evolutionary Anthropology, Leipzig.\nTsendsuren Munkhdalai and Adam Trischler. 2018.\nMetalearning with Hebbian fast weights. arXiv\npreprint arXiv:1807.05076.\nRobert Östling and Jörg Tiedemann. 2017. Continuous\nmultilinguality with language vectors. In Proceed-\nings of the EACL, pages 644–649.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\n2013. On the difﬁculty of training recurrent neu-\nral networks. In Proceedings of ICML, pages 1310–\n1318.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of\nRepL4NLP-2019, pages 7–14.\nEmmanouil Antonios Platanios, Mrinmaya Sachan,\nGraham Neubig, and Tom Mitchell. 2018. Contex-\ntual parameter generation for universal neural ma-\nchine translation. In Proceedings of EMNLP, pages\n425–435.\nEdoardo Maria Ponti, Helen O’horan, Yevgeni Berzak,\nIvan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina\nShutova, and Anna Korhonen. 2019. Modeling lan-\nguage variation and universals: A survey on typo-\nlogical linguistics for natural language processing.\nComputational Linguistics, 45(3):559–601.\nEdoardo Maria Ponti, Roi Reichart, Anna Korhonen,\nand Ivan Vuli´c. 2018. Isomorphic transfer of syntac-\ntic structures in cross-lingual NLP. In Proceedings\nof ACL, pages 1531–1542.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the inductive biases of RNNs with syn-\nthetic variations of natural languages. In Proceed-\nings of NAACL-HLT, pages 3532–3542.\nShauli Ravfogel, Yoav Goldberg, and Francis Tyers.\n2018. Can LSTM learn to capture agreement? The\ncase of Basque. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 98–107.\nHippolyt Ritter, Aleksandar Botev, and David Barber.\n2018. Online structured Laplace approximations for\novercoming catastrophic forgetting. In Proceedings\nof NIPS, pages 3738–3748.\nSebastian Ruder. 2017. An overview of multi-task\nlearning in deep neural networks. arXiv preprint\narXiv:1706.05098.\nGary F. Simons. 2017. Ethnologue: Languages of the\nworld, 22nd edition. Dallas, Texas: SIL Interna-\ntional.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of NIPS, pages 3104–3112.\nCorentin Tallec and Yann Ollivier. 2017. Unbias-\ning truncated backpropagation through time. arXiv\npreprint arXiv:1705.08209.\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,\nGuillaume Lample, Patrick Littell, David\nMortensen, Alan W. Black, Lori Levin, and\nChris Dyer. 2016. Polyglot neural language models:\nA case study in cross-lingual phonetic represen-\ntation learning. In Proceedings of NAACL-HLT ,\npages 1357–1366.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap,\nDaan Wierstra, et al. 2016. Matching networks for\none shot learning. In Proceedings of NIPS , pages\n3630–3638.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun,\nand Rob Fergus. 2013. Regularization of neural net-\nworks using DropConnect. In Proceedings of ICML,\npages 1058–1066.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\n2910\nA Character Distribution\nEven within the same setting, BPC scores vary\nenormously across languages in both the ZERO -\nSHOT and FEW-SHOT settings, which requires an\nexplanation. Similarly to Gerz et al. (2018a,b), we\nrun a correlation analysis between language mod-\neling performance and basic statistics of the data.\nIn particular, we ﬁrst create a vector of unigram\ncharacter counts for each language, shown in Fig. 1.\nThen we estimate the cosine distance between the\nvector of each language and the average of all the\nothers in our sample. This cosine distance is a mea-\nsure of the ‘exoticness’ of a language’s character\ndistribution.\nPearson’s correlation between such cosine dis-\ntance and the perplexity of UNIV BARE in each\nlanguage reveals a strong correlation coefﬁcient\nρ= 0.53 and a statistical signiﬁcance of p< 10−6\nin the ZERO -SHOT setting. On the other hand, such\ncorrelation is absent ( ρ = −0.13) and insigniﬁ-\ncant p >0.2 in the FEW-SHOT setting. In other\nwords, if a few examples of character sequences\nare provided for a target language, language mod-\neling performance ceases to depend on its unigram\ncharacter distribution.\nB Probing of Learned Posteriors\nFinally, it remains to establish which sort of knowl-\nedge is embedded in the universal prior. How to\nprobe a probability distribution over weights in\nthe non-conditional UNIV BARE language model?\nFirst, we study the signal-to-noise ratio of each pa-\nrameter wi, computed as |µi|\nσi\n, in each of the 4 splits.\nIntuitively, this metric quantiﬁes the ‘informative-\nness’ of each parameter, which is proportional to\nboth the absolute value of the mean and the inverse\nstandard deviation of the estimate. The probabil-\nity density function of the signal-to-noise ratio is\nshown in Fig. 2. From this plot, it emerges that\nthe estimated uncertainty is generally low (small\nσi denominators yield high values). Most crucially,\nthe signal-to-noise values concentrate on the left of\nthe spectrum. This means that most weights will\nnot incur any penalty for changing during few-shot\nlearning based on eq. (15); on the other hand, there\nis a bulk of highly informative parameters on the\nright of the spectrum that are very likely to remain\nﬁxed, thus preventing catastrophic forgetting. All\nsplits display such a pattern, although somewhat\nshifted.\nSecond, to study the effect of conditioning the\nuniversal prior on typological features, I gener-\nate random sequences of 25 characters from the\nlearned prior in each language. The ﬁrst character\nis chosen uniformly at random, and the subsequent\nones are sampled from the distribution given by\neq. (1) with a temperature of 1. The resulting texts\nare shown in Table 4. Although this would warrant\na more thorough and systematic analysis, from a\ncursory view it is evident of the sequences abide\nwith universal phonological patterns, e.g. favor-\ning vowels as syllabic nuclei and ordering conso-\nnants based on sonority hierarchy. Moreover, the\nlanguage-speciﬁc information clearly steers pre-\ndicted sequences towards the correct inventory of\ncharacters, as demonstrated by Vietnamese (VIE )\nand Lukpa (DOP ) in Table 4.\n2911\nLIT javen šuksyr sun siriai tes pije nuks SHI ereswrin an daγtartnaas ni mad yanó\nNOR s hech far binje alrn bre a ver e hior JAK ﬁ pelo ayok musam nejaz jih tewat ushi\nKEK sx er taj chan linam laj âtebke naque SWE ssiar ˇ rades perdeshen heklui tart si a\nJIV da tum suuam sιtas nekkin una tekaru ni DIK e wEn ke nuN ni piyitia de run ye e ke\nDJE a ciya toi milkak mo to yen nga suci EWE å mula pe ose le ake mente amesa ke kul\nSLK o je to temokoé lostave sa jesé gukli ALB I kur je ki thet je ji tin nuk t tho\nCES e je jek jem neuteN rekssýj jazá níb ws CNI u pen mireshisinoe airitcsa ateani yi\nPOR uˇ c somo ai jegparase saves e iper to POT neta ynimka nekin linaayi meu carií a\nSPA esquár y lues dusme allis nencec adi ZUL ởnakan kuná bencro krileke konusti k\nGLV ayr sh˙zi ayn ai sephson a gil or geee QUW ai chimira kachisinyra poi apre asyu\nPOL eteni na hidi cếho o˙z swchj jeci i cil AGR ji ica ama kujaa muri wajetar aumam hu\nQUC ûs xe cä wija ro pio kin cbi’ ij jejac DOP btElO ιtelaγa kO nϵιzûγιnEk@ pO\nWAL banjake la dos que benthi shivegina EUS cerer nagcermac istirinun qatserite\nXHO ukayla azigeecoa kosubentisiili jen maky HUN elyet a bukot aky azraá ot mu háláj y\nSOM ao kun adku i sir jija i befey yadui GLA o e kere hhó sho dhöìr te ilailui a tu a\nTGL ikugy peo asha atan kao amai kain ak a PCK u gihiha ki mi dhia mea la hen a puh ih\nCJP pae yei aje kin trheka pän awawa ri s AFR mal hoor in e sheei wer var buerkeas en\nACU animmhi mustatur tukaw aants aastasai a USP okan mi ykis ris rajajkujij taka ja\nFIN i koin suu meit ja ii soi tetot jasw IND t berka duhah menkad kemia ukus keri ya\nMRI oki ka benoka ai ki kimanka pikaka ko ROM hal kus seke nukertia dehe neshes hos n\nSLV ˇ ciˇ cvim koko si neˇ ce pau ku meta noj neTMH @rofm sibarn awigtir ϵli d usi leped\nHRV ca ka te zet jon jem nezin isak ve u ITA tri cordia io si si conse de namni nel\nEPO j li inij keris ec xom el e sepon kaj SRP e se a nil do zasom kuz je sefe nij hoˇ c\nAMU ´mibinya na ñero melee cano’ ndo’ cy’oc NLD e suet en de semeshord ak abaido zin\nKBH ¨xe aquangmomnaynangmuacha tojam LAT ifte quissi fetam remnas emens in timnex\nCEB abithon kayay isa atoug giraban sula MAM í la Nil a cheh tjea nut tej quxen kaj\nGBI fuma ome pani de imoako kema kaye ntul VIE hẩ kì đãi bi ầt ni γì sa hiổ v ¯u r\nENG g ban urse auth ahen ant msesher at nhe\nISL j noka nie leli maken ti aide ni itsim a EST inam acha dius dempegun geben parug j\nSNA xe yare ske tengker ci bendar nu derbe CHA ê duka ka kina kia nextis ne aka nisa\nRON ma awa nasil ko khe ni koy koj tikis t FRA dis assan in man usia issokoj mulel e me\nKAB je cana ka casa chomdis mear de ber h DJK okrana anginar matom iliantarinta a non\nNHG chun neyal den ma kashtaka asa as riste LAV ilu kagsa eriri isi paj ewri bus os\nDAN dnepse aa aye sas ningli inas giksaj abe BSN as juhma yainawa nusa wali apai basti\nPPK ios yena mona kemewascoj ni ne maa HAT a kuneati ua veskos oramaj meseqen ye k\nSSW nta yoti gesi kela nii ikasgaber ni tus TUR che a shachmo ềspi meng rinnaj e ish em\nWOL alen kokpan fed man benu pei ei kestam AKE n jes silem semmo caja arka wagtoa doo\nDEU ke giko si obi rer nin eber tun ke ele CHQ shas nej neysakun kina alistad mesabe\nCAK tej je awem titoj lunik c’u chis m ni PLT Vwi meyak me imai anet alavis edte kin\nTable 4: Randomly generated text on observed languages (top) and held-out languages (bottom) in the 4th split.\n2912\n/uni00000004/uni00000008/uni0000000c/uni0000000f/uni00000012/uni0000001f/uni00000022/uni0000003f/uni00000041/uni00000045/uni00000048/uni0000004b/uni0000004e/uni00000051/uni00000054/uni00000057/uni0000005a/uni0000005d/uni00000060/uni0000009e/uni00000069/uni0000006c/uni0000006f/uni00000072/uni00000074/uni000000e8/uni00000079/uni0000007c/uni0000007e/uni000000ea\n/uni00000044/uni00000046/uni00000058\n/uni00000044/uni00000049/uni00000055\n/uni00000044/uni0000004a/uni00000055\n/uni00000044/uni0000004e/uni00000048\n/uni00000044/uni0000004f/uni00000045\n/uni00000044/uni00000050/uni00000058\n/uni00000045/uni00000056/uni00000051\n/uni00000046/uni00000044/uni0000004e\n/uni00000046/uni00000048/uni00000045\n/uni00000046/uni00000048/uni00000056\n/uni00000046/uni0000004b/uni00000044\n/uni00000046/uni0000004b/uni00000054\n/uni00000046/uni0000004d/uni00000053\n/uni00000046/uni00000051/uni0000004c\n/uni00000047/uni00000044/uni00000051\n/uni00000047/uni00000048/uni00000058\n/uni00000047/uni0000004c/uni0000004e\n/uni00000047/uni0000004d/uni00000048\n/uni00000047/uni0000004d/uni0000004e\n/uni00000047/uni00000052/uni00000053\n/uni00000048/uni00000051/uni0000004a\n/uni00000048/uni00000053/uni00000052\n/uni00000048/uni00000056/uni00000057\n/uni00000048/uni00000058/uni00000056\n/uni00000048/uni0000005a/uni00000048\n/uni00000049/uni0000004c/uni00000051\n/uni00000049/uni00000055/uni00000044\n/uni0000004a/uni00000045/uni0000004c\n/uni0000004a/uni0000004f/uni00000044\n/uni0000004a/uni0000004f/uni00000059\n/uni0000004b/uni00000044/uni00000057\n/uni0000004b/uni00000055/uni00000059\n/uni0000004b/uni00000058/uni00000051\n/uni0000004c/uni00000051/uni00000047\n/uni0000004c/uni00000056/uni0000004f\n/uni0000004c/uni00000057/uni00000044\n/uni0000004d/uni00000044/uni0000004e\n/uni0000004d/uni0000004c/uni00000059\n/uni0000004e/uni00000044/uni00000045\n/uni0000004e/uni00000045/uni0000004b\n/uni0000004e/uni00000048/uni0000004e\n/uni0000004f/uni00000044/uni00000057\n/uni0000004f/uni00000044/uni00000059\n/uni0000004f/uni0000004c/uni00000057\n/uni00000050/uni00000044/uni00000050\n/uni00000050/uni00000055/uni0000004c\n/uni00000051/uni0000004b/uni0000004a\n/uni00000051/uni0000004f/uni00000047\n/uni00000051/uni00000052/uni00000055\n/uni00000053/uni00000046/uni0000004e\n/uni00000053/uni0000004f/uni00000057\n/uni00000053/uni00000052/uni0000004f\n/uni00000053/uni00000052/uni00000055\n/uni00000053/uni00000052/uni00000057\n/uni00000053/uni00000053/uni0000004e\n/uni00000054/uni00000058/uni00000046\n/uni00000054/uni00000058/uni0000005a\n/uni00000055/uni00000052/uni00000050\n/uni00000055/uni00000052/uni00000051\n/uni00000056/uni0000004b/uni0000004c\n/uni00000056/uni0000004f/uni0000004e\n/uni00000056/uni0000004f/uni00000059\n/uni00000056/uni00000051/uni00000044\n/uni00000056/uni00000052/uni00000050\n/uni00000056/uni00000053/uni00000044\n/uni00000056/uni00000055/uni00000053\n/uni00000056/uni00000056/uni0000005a\n/uni00000056/uni0000005a/uni00000048\n/uni00000057/uni0000004a/uni0000004f\n/uni00000057/uni00000050/uni0000004b\n/uni00000057/uni00000058/uni00000055\n/uni00000058/uni00000056/uni00000053\n/uni00000059/uni0000004c/uni00000048\n/uni0000005a/uni00000044/uni0000004f\n/uni0000005a/uni00000052/uni0000004f\n/uni0000005b/uni0000004b/uni00000052\n/uni0000005d/uni00000058/uni0000004f\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000016/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000017/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000019/uni00000013\nFigure 1: Unigram character distribution (x-axis) per language (y-axis). Note how some rows stand out as outliers.\n10−5 10−4 10−3\n|µ|\nσ\n10−5\n10−4\n10−3\n10−2\n10−1\n100\n101\n102\npdf\nFigure 2: Probability density function of the signal-to-noise ratio for each parameter of the learned posteriors in\nthe UNIV BARE language models on splits 1 (blue), 2 (red), 3 (green), 4 (gold). The plot is in log-log scale.\n2913\nC Derivation of the Laplace Approximation\np(w |D) = exp\n(\nL(w)\n)\n∫\nexp\n(\nL(w)\n)\ndw Bayes rule\n≈ exp\n[\nL(w⋆) + (w −w⋆)⊤∇L(w⋆) + 1\n2 (w −w⋆)⊤H (w −w⋆)\n]\n∫\nexp\n[\nL(w⋆) + (w −w⋆)⊤∇L(w⋆) + 1\n2 (w −w⋆)⊤H (w −w⋆)\n]\ndw Taylor expansion\n= exp\n[\nL(w⋆) + 1\n2 (w −w⋆)⊤H (w −w⋆)\n]\n∫\nexp\n[\nL(w⋆) + 1\n2 (w −w⋆)⊤H (w −w⋆)\n]\ndw ∇L(w)|w⋆ = 0\n= exp\n(\nL(w⋆)\n)\nexp\n[\n−1\n2 (w −w⋆)⊤(−H) (w −w⋆)\n]\nexp\n(\nL(w⋆)\n)∫\nexp\n[\n−1\n2 (w −w⋆)⊤(−H)(w −w⋆)\n]\ndw exponential of sum\n= exp\n[\n−1\n2 (w −w⋆)⊤(−H)(w −w⋆)\n]\n√\n(2π)d|−H|−1\nintegration and simpliﬁcation\n≜ N(w⋆,−H−1)\n(19)\nD Derivation of the Approximated Hessian\nWe assume w ∼N(0,σ2I). Given the relationship among the expected Fisher Information I(w), the\nobserved Fisher Information J(w), the observed Fisher Information based on |D|samples JD(w), and\nthe Hessian H:\n−I(w) = −EJ(w) ≈− 1\n|D|JD(w) = 1\n|D|H = 1\n|D|∇2L(w) (20)\nwe can derive our approximation of 1\n|D|H:\n2914\n1\n|D|∇2L(w)\n= 1\n|D|∇2\n(∑\nℓ∈T\nlog p(Dℓ |w) + logp(w)\n)\ndeﬁnition of L(w)\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|∇2 log p(x |w) + ∇2 log p(w) linearity of ∇2\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|∇\n(∇p(x |w)\np(x |w)\n)\n+ ∇2 log p(w) derivative of logarithm\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|\np(x |w)∇2p(x |w) −∇p(x |w)∇p(x |w)⊤\np(x |w)2\n+ ∇2 log p(w) quotient rule\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|\n[\n∇2p(x |w)\np(x |w) −\n(∇p(x |w)\np(x |w)\n)(∇p(x |w)\np(x |w)\n)⊤]\n+ ∇2 log p(w) rearrange and simplify\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n1\n|T|·|D ℓ|\n[∇2p(x |w)\np(x |w) −∇log p(x |w) ∇log p(x |w)⊤\n]\n+ ∇2 log p(w) derivative of logarithm\n≈\n∑\nℓ∈T\n1\n|T|\n\nEx∼p(·|w)\n∇2p(x |w)\np(x |w) − 1\n|Dℓ|\n∑\nx∈Dℓ\n∇log p(x |w) ∇log p(x |w)⊤\n\n\n+ ∇2 log p(w) sample average as expectation\n=\n∑\nℓ∈T\n1\n|T|\n\n\n∫ ∇2p(x |w)\np(x |w) p(x |w) dx − 1\n|Dℓ|\n∑\nx∈Dℓ\n∇log p(x |w) ∇log p(x |w)⊤\n\n\n+ ∇2 log p(w) expectation as integral\n=\n∑\nℓ∈T\n1\n|T|\n\n∇2\n∫\np(x |w) dx − 1\n|Dℓ|\n∑\nx∈Dℓ\n∇log p(x |w) ∇log p(x |w)⊤\n\n\n+ ∇2 log p(w) simplify\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n−1\n|T|·|D ℓ|∇log p(x |w) ∇log p(x |w)⊤+ ∇2 log p(w) derivative of constant\n≈\n∑\nℓ∈T\n∑\nx∈Dℓ\n−1\n|T|·|D ℓ|diag\n[\n∇log p(x |w)\n]2\n+ ∇2 log p(w) diagonal approximation\n=\n∑\nℓ∈T\n∑\nx∈Dℓ\n−1\n|T|·|D ℓ|diag\n[\n∇log p(x |w)\n]2\n− 1\nσ2 I second derivative of log-probability\n(21)\n2915",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6696338653564453
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5337000489234924
    },
    {
      "name": "Shot (pellet)",
      "score": 0.47611743211746216
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.42570197582244873
    },
    {
      "name": "Natural language processing",
      "score": 0.37044399976730347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3592519760131836
    },
    {
      "name": "Speech recognition",
      "score": 0.34892067313194275
    },
    {
      "name": "Linguistics",
      "score": 0.1385761797428131
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210107233",
      "name": "Language Science (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    }
  ]
}