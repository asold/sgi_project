{
  "title": "Don’t Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information",
  "url": "https://openalex.org/W4287888682",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5016443023",
      "name": "Tomasz Limisiewicz",
      "affiliations": [
        "Charles University"
      ]
    },
    {
      "id": "https://openalex.org/A5066030227",
      "name": "David Mareček",
      "affiliations": [
        "Charles University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035241006",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W3154654049",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2963524349",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W3035348475",
    "https://openalex.org/W2949969209",
    "https://openalex.org/W2963381846",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3175232426",
    "https://openalex.org/W2952328691",
    "https://openalex.org/W3120329789",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4226462293",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W3138819813"
  ],
  "abstract": "The representations in large language models contain multiple types of gender information. We focus on two types of such signals in English texts: factual gender information, which is a grammatical or semantic property, and gender bias, which is the correlation between a word and specific gender. We can disentangle the model’s embeddings and identify components encoding both types of information with probing. We aim to diminish the stereotypical bias in the representations while preserving the factual gender signal. Our filtering method shows that it is possible to decrease the bias of gender-neutral profession names without significant deterioration of language modeling capabilities. The findings can be applied to language generation to mitigate reliance on stereotypes while preserving gender agreement in coreferences.",
  "full_text": "Proceedings of the The 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 17 - 29\nJuly 15, 2022 ©2022 Association for Computational Linguistics\nDon’t Forget About Pronouns: Removing Gender Bias in Language Models\nWithout Losing Factual Gender Information\nTomasz Limisiewicz and David Mareˇcek\nInstitute of Formal and Applied Linguistics, Faculty of Mathematics and Physics\nCharles University, Prague, Czech Republic\n{limisiewicz, marecek}@ufal.mff.cuni.cz\nAbstract\nThe representations in large language models\ncontain multiple types of gender information.\nWe focus on two types of such signals in En-\nglish texts: factual gender information, which\nis a grammatical or semantic property, and gen-\nder bias, which is the correlation between a\nword and specific gender. We can disentangle\nthe model’s embeddings and identify compo-\nnents encoding both types of information with\nprobing. We aim to diminish the stereotypi-\ncal bias in the representations while preserving\nthe factual gender signal. Our filtering method\nshows that it is possible to decrease the bias of\ngender-neutral profession names without sig-\nnificant deterioration of language modeling ca-\npabilities. The findings can be applied to lan-\nguage generation to mitigate reliance on stereo-\ntypes while preserving gender agreement in\ncoreferences.1\n1 Introduction\nNeural networks are successfully applied in natural\nlanguage processing. While they achieve state-\nof-the-art results on various tasks, their decision\nprocess is not yet fully explained (Lipton, 2018).\nIt is often the case that neural networks base their\nprediction on spurious correlations learned from\nlarge uncurated datasets. An example of such a spu-\nrious tendency is gender bias. Even the state-of-the-\nart models tend to counterfactually associate some\nwords with a specific gender (Zhao et al., 2018a;\nStanovsky et al., 2019). The representations of pro-\nfession names tend to be closely connected with\nthe stereotypical gender of their holders. When the\nmodel encounters the word “nurse”, it will tend to\nuse female pronouns (“she”, “her”) when referring\nto this person in the generated text. This tendency\nis reversed for words such as “doctor”, “professor”,\nor “programmer”, which are male-biased.\n1Our code is available on GitHub: github.com/\ntomlimi/Gender-Bias-vs-Information\nFigure 1: A schema is presenting the distinction be-\ntween gender bias of nouns and factual (i.e., grammat-\nical) gender in pronouns. We want to transform the\nrepresentations to mitigate the former and preserve the\nlatter.\nIt means that the neural model is not reliable\nenough to be applied in high-stakes language pro-\ncessing tasks such as connecting job offers to ap-\nplicants’ CVs (De-Arteaga et al., 2019). If the\nunderlying model was biased, the high-paying\njobs, which are stereotypically associated with men,\ncould be inaccessible for female candidates. When\nwe decide to use language models for that purpose,\nthe key challenge is to ensure that their predictions\nare fair.\nThe recent works on the topics aimed to dimin-\nish the role of gender bias by feeding examples of\nunbiased text and training the network (de Vassi-\nmon Manela et al., 2021) or transforming the rep-\nresentations of the neural networks post-hoc (with-\nout additional training) (Bolukbasi et al., 2016).\nHowever, those works relied on the notion that to\nde-bias representation, most gender signal needs\nto be eliminated. It is not always the case, pro-\nnouns and a few other words (e.g.:“king” -“queen”;\n“boy” - “girl”) have factual information about gen-\nder. A few works identified gendered words and\n17\nexempted them from de-biasing (Zhao et al., 2018b;\nKaneko and Bollegala, 2019). In contrast to these\napproaches, we focus on contextual word embed-\ndings. In contextual representations, we want to\npreserve the factual gender information for gender-\nneutral words when it is indicated by context, e.g.,\npersonal pronoun. This sort of information needs to\nbe maintained in the representations. In language\nmodeling, the network needs to be consistent about\nthe gender of a person if it was revealed earlier\nin the text. The model’s ability to encode factual\ngender information is crucial for that purpose.\nWe propose a method for disentangling the fac-\ntual gender information and gender bias encoded\nin the representations. We hypothesise that seman-\ntic gender information (from pronouns) is encoded\nin the network distinctly from the stereotypical\nbias of gender-neutral words (Figure 1). We apply\nan orthogonal probe, which proved to be useful,\ne.g., in separating lexical and syntactic informa-\ntion encoded in the neural model (Limisiewicz and\nMareˇcek, 2021). Then we filter out the bias sub-\nspace from the embedding space and keep the sub-\nspace encoding factual gender information. We\nshow that this method performs well in both de-\nsired properties: decreasing the network’s reliance\non bias while retaining knowledge about factual\ngender.\n1.1 Terminology\nWe consider two types of gender information en-\ncoded in text:\n• Factual gender is the grammatical (pronouns\n“he”, “she”, “her”, etc.) or semantic (“boy”,\n“girl”, etc.) feature of specific word. It can also\nbe indicated by a coreference link. We will\ncall words with factual gender as gendered in\ncontrast to gender-neutral words.\n• Gender bias is the connection between a\nword and the specific gender with which it\nis usually associated, regardless of the factual\npremise.2 We will refer to words with gender\nbias as biased in contrast to non-biased.\nPlease note that those definitions do not preclude\nthe existence of biased and at the same time gender-\nneutral words. In that case, we consider bias stereo-\ntypical and aim to mitigate it in our method. On the\n2For instance, the words “nurse”, “housekeeper” are as-\nsociated with women, and words “doctor”, “mechanic” with\nmen. None of those words has a grammatical gender marking\nin English.\nother hand, we want to preserve bias in gendered\nwords.\n2 Methods\nWe aim to remove the influence of gender-biased\nwords while keeping the information about factual\ngender in the sentence given by pronouns. We\nfocus on interactions of gender bias and factual\ngender information in coreference cues of the fol-\nlowing form:\n[NOUN] examined the farmer for injuries because\n[PRONOUN] was caring.\nIn English, we can expect to obtain the factual\ngender from the pronoun. Revealing one of the\nwords in coreference link should impact the pre-\ndiction of the other. Therefore we can name two\ncausal associations:\nCI: bias noun →f. genderpronoun\nCII : f. gender pronoun →biasnoun\nIn our method, we will primarily focus on two\nways bias and factual gender interact. For gender-\nneutral nouns (in association CI), the effect on\npredicting masked pronouns would be primarily\ncorrelated with their gender bias. At the same time,\nthe second association is desirable, as it reveals\nfactual gender information and can improve the\nmasked token prediction of a gendered word. We\ndefine two conditional probability distributions cor-\nresponding to those causal associations:\nPI(ypronoun|X,b)\nPII (ynoun|X,f) (1)\nWhere yis a token predicted in the position of\npronoun and noun, respectively; X is the context\nfor masked language modeling. band f are bias\nand factual gender factors, respectively. We model\nthe bias factor by using a gender-neutral biased\nnoun. Below we present examples for introducing\nfemale and male bias: 3\nExample 1:\nbf The nurse examined the farmer for injuries because\n[PRONOUN] was caring.\nbm The doctor examined the farmer for injuries because\n[PRONOUN] was caring\n3We use [NOUN] and [PRONOUN] tokens for a better\nexplanation, in practice, they both are masked by the same\nmask token, e.g. [MASK] in BERT (Devlin et al., 2019).\n18\nSimilarly, the factual gender factor is modeled\nby introducing a pronoun with a specific gender in\nthe sentence:\nExample 2:\nff [NOUN] examined the farmer for injuries because she\nwas caring.\nfm [NOUN] examined the farmer for injuries because he\nwas caring.\nWe aim to diminish the role of bias in the predic-\ntion of pronouns of a specific gender. On the other\nhand, the gender indicated in pronouns can be use-\nful in the prediction of a gendered noun. Mathemat-\nically speaking, we want to drop the conditionality\non bias factor in PI from eq. (1), while keeping the\nconditionality on gender factor in PII .\nPI(ypronoun|X,b) →PI(ypronoun|X)\nPII (ynoun|X,f) ̸→PII (ynoun|X) (2)\nTo decrease the effect of gender signal from the\nwords other than pronoun and noun, we introduce\na baseline, where both pronoun and noun tokens\nare masked:\nExample 3:\n∅ [NOUN] examined the farmer for injuries because\n[PRONOUN] was caring.\n2.1 Evaluation of Bias\nManifestation of gender bias may vary significantly\nfrom model to model and can be attributed mainly\nto the choice of the pre-training corpora as well as\nthe training regime. We define gender preference\nin a sentence by the ratio between the probability\nof predicting male and female pronouns:\nGP(X) =PI([pronounm]|X)\nPI([pronounf ]|X) (3)\nTo estimate the gender bias of a profession name,\nwe compare the gender preference in a sentence\nwhere the profession word is masked (example 3\nfrom the previous paragraph) and not masked (ex-\nample 1). We define relative gender preference:\nRGPnoun = log(GP(Xnoun)) −log(GP(X∅)) (4)\nXnoun denotes contexts in which the noun is re-\nvealed (example 1), and X∅ corresponds to exam-\nple 3, where we mask both the noun and the pro-\nnoun. Our approach focuses on the bias introduced\nby a noun, especially profession name. We subtract\nlog(GP(X∅)) to single out the bias contribution\ncoming from the noun.4 We use logarithm, so the\nresults around zero would mean that revealing noun\ndoes not affect gender preference.5\n2.2 Disentangling Gender Signals with\nOrthogonal Probe\nTo mitigate the influence of bias on the predictions\neq. (2), we focus on the internal representations\nof the language model. We aim to inspect con-\ntextual representations of words and identify their\nparts that encode the causal associations CI and\nCII . For that purpose, we utilize orthogonal struc-\ntural probesproposed by Limisiewicz and Mareˇcek\n(2021).\nIn structural probing, the embedding vectors are\ntransformed in a way so that distances between\npairs of the projected embeddings approximate a\nlinguistic feature, e.g., distance in a dependency\ntree (Hewitt and Manning, 2019). In our case, we\nwant to approximate the gender information in-\ntroduced by a gendered pronoun f (factual) and\ngender-neutral noun b(bias). The f takes the val-\nues −1 for female pronouns and, 1 for male ones,\nand 0 for gender-neutral “they”. The bis the rela-\ntive gender preference (eq. (4)) for a specific noun\n(b≡RGPnoun).\nOur orthogonal probe consists of three trainable\ncomponents:\n• O: orthogonal transformation, mapping rep-\nresentation to new coordinate system.\n• SV: scaling vector, element-wise scaling of\nthe dimensions in a new coordinate systems.\nWe assume that dimensions that store probed\ninformation are associated with large scaling\ncoefficients.\n• i: intercept shifting the representation.\nOis a tunable orthogonal matrix of size demb ×\ndemb, SV and iare tunable vectors of length demb,\nwhere demb is the dimensionality of model’s em-\nbeddings. The probing losses are the following:\nLI =\n⏐⏐||SV I ⊙(O ·(hb,P −h∅,P )) −iI||d −b\n⏐⏐\nLII =\n⏐⏐||SV II ⊙(O ·(hf,N −h∅,N)) −iII ||d −f\n⏐⏐,\n(5)\n4Other parts of speech may also introduce gender bias, e.g.,\nthe verb “to work”. We note that our setting can be generalized\nto all words, but it is outside of the scope of this work.\n5The relative gender preferencewas inspired by total effect\nmeasure proposed by Vig et al. (2020).\n19\nwhere, hb,P is the vector representation of masked\npronoun in example 1; hf,N is the vector repre-\nsentation of masked noun in example 2; vectors\nh∅,P and h∅,N are the representations of masked\npronoun and noun respectively in baseline example\n3.\nTo account for negative values of target factors\n(band f) in eq. (5), we generalize distance metric\nto negative values in the following way:\n||− →v||d = ||max(− →0 ,− →v)||2 −||min(− →0 ,− →v)||2\n(6)\nWe jointly probe for both objectives (orthogo-\nnal transformation is shared). Limisiewicz and\nMareˇcek (2021) observed that the resulting scaling\nvector after optimization tends to be sparse, and\nthus they allow to find the subspace of the embed-\nding space that encodes particular information.\n2.3 Filtering Algorithm\nIn our algorithm we aim to filter out the latent\nvector’s dimensions that encode bias. Particularly,\nwe assume that, when ||hb,P −h∅,P ||→ 0 then\nPI(ypronoun|X,b) →PI(ypronoun|X)\nWe can diminish the information by masking\nthe dimensions with a corresponding scaling vector\ncoefficient larger than small ϵ.6 The bias filter is\ndefined as:\nF−b = − →1 [ϵ>abs (SVI)], (7)\nwhere abs(·) is element-wise absolute value and− →1 is element-wise indicator. We apply this vector\nto the representations of hidden layers:\nˆh= OT ·(F−b ⊙(O·h) +abs(SVI) ⊙iI) (8)\nTo preserve factual gender information, we pro-\npose an alternative version of the filter. The di-\nmension is kept when its importance (measured by\nthe absolute value of scaling vector coefficient) is\nhigher in probing for factual gender than in probing\nfor bias. We define factual gender preserving filter\nas:\nF−b,+f = F−b + − →1 [ϵ≤abs(SVI) <abs(SVII )]\n(9)\nThe filtering is performed as in eq. (8) We ana-\nlyze the number of overlapping dimensions in two\nscaling vectors in Section 3.2.\n6We take epsilon equal to 10−12. Our results weren’t\nparticularly vulnerable to this parameter, we show the analysis\nin appendix C.\n3 Experiments and Results\nWe examine the representation of two BERT mod-\nels (base-cased: 12 layers, 768 embedding size; and\nlarge-cased: 24 layers, 1024 embedding size, De-\nvlin et al. (2019)), andELECTRA (base-generator:\n12 layers, 256 embedding size Clark et al. (2020)).\nAll the models are Transformer encoders trained\non the masked language modeling objective.\n3.1 Evaluation of Gender Bias in Language\nModels\nBefore constructing a de-biasing algorithm, we\nevaluate the bias in the prediction of three language\nmodels.\nWe evaluate the gender bias in language mod-\nels on 104 gender-neutral professional words from\nthe WinoBias dataset (Zhao et al., 2018a). The\nauthors analyzed the data from the US Labor Force\nStatistics. They annotated 20 professions with the\nhighest share of women as stereotypically female\nand 20 professions with the highest share of men\nas stereotypically male.\nWe run the inference on the prompts in five for-\nmats presented in Table 1 and estimate with equa-\ntion eq. (4). To obtain the bias of the word in the\nmodel, we take mean RGPnoun computed on all\nprompts.\n3.1.1 Results\nWe compare our results with the list of stereotypical\nwords from the annotation of Zhao et al. (2018a).\nSimilarly, we pick up to 20 nouns with the highest\nand positive RGP as male-biased and up to 20\nnouns with the lowest and negativeRGP as female-\nbiased. These lists differ for models.\nTable 2 presents the most biased words accord-\ning to three models. Noticeably, there are differ-\nences between empirical and annotated bias. Espe-\ncially word “salesperson” considered male-biased\nbased on job market data was one of the most\nskewed toward the female gender in 2 out of 3\nmodels. The full results of the evaluation can be\nfound in appendix D.\n3.2 Probing for Gender Bias and Factual\nGender Information\nWe optimize the joint probe, where orthogonal\ntransformation is shared, while scaling vectors\nand intercepts are task specific. The probing ob-\njective is to approximate: CI ) the gender bias\nof gender-neutral nouns ( b ≡ RGPnoun); and\n20\nPrompt PRONOUN PRONOUN 2\n[PRONOUN] is [NOUN]. She He\n[PRONOUN] was [NOUN]. She He\n[PRONOUN]works as [NOUN]. She He\n[PRONOUN] job is [NOUN]. Her His\n[NOUN]said that [PRONOUN] loves [PRONOUN 2] job. he she her his\n[NOUN] said that [PRONOUN] hates [PRONOUN 2] job. she he her his\nTable 1: List of evaluation prompts used in the evaluation of relative gender preference. The tag [NOUN] masks a\nnoun accompanied by an appropriate determiner.\nMost Female Biased Most Male Biased\nNOUN N Models Avg. RGP Annotated NOUN N Models Avg. RGP Annotated\nhousekeeper 3/3 -2.009 female carpenter 3/3 0.870 male\nnurse 3/3 -1.840 female farmer 3/3 0.753 male\nreceptionist 3/3 -1.602 female guard 3/3 0.738 male\nhairdresser 3/3 -0.471 female sheriff 3/3 0.651 male\nlibrarian 2/3 -0.279 female firefighter 3/3 0.779 neutral\nvictim 2/3 -0.102 neutral driver 3/3 0.622 male\nchild 2/3 -0.060 neutral mechanic 2/3 0.719 male\nsalesperson 2/3 -0.056 male engineer 2/3 0.645 neutral\nTable 2: Evaluated empirical bias in analyzed Masked Language Models. Column number shows the count of\nmodels for which the word was considered biased. Annotated is the bias assigned in Zhao et al. (2018a) based on\nthe job market data.\nCII ) the factual gender information of pronouns\n(f ≡f. genderpronoun).\nWe use WinoMT dataset 7 (Stanovsky et al.,\n2019) which is a derivate of WinoBias dataset\n(Zhao et al., 2018a). Examples are more challeng-\ning to solve in this dataset than in our evaluation\nprompts (Table 1). Each sentence contains two po-\ntential antecedents. We use WinoMT for probing\nbecause we want to separate probe optimization\nand evaluation data. Moreover, we want to iden-\ntify the encoding of gender bias and factual gender\ninformation in more diverse contexts.\nWe split the dataset into train, development, and\ntest sets with non-overlapping nouns, mainly pro-\nfession names. They contain 62, 21, and 21 unique\nnouns, corresponding to 2474, 856, and 546 sen-\ntences. The splits are designed to balance male and\nfemale-biased words in each of them.\n3.2.1 Results\nThe probes on the models’ top layer give a good\napproximation of factual gender – Pearson corre-\n7The dataset was originally introduced to evaluate gender\nbias in machine translation.\nlation between predicted and gold values in the\nrange from 0.928 to 0.946. Pearson correlation for\nbias was high for BERT base (0.876), BERT large\n(0.946), and lower for ELECTRA ( 0.451).8\nWe have identified the dimensions encoding con-\nditionality CI and CII . In Figure 2, we present\nthe number of dimensions selected for each objec-\ntive and their overlap. We see that bias is encoded\nsparsely in 18 to 80 dimensions.\n3.3 Filtering Gender Bias\nThe primary purpose of probing is to construct\nbias filters based on the values of scaling: F−b\nand F−b,+f . Subsequently, we perform our de-\nbiasing transformation eq. (7) on the last layers of\nthe model. The probes on top of each layer are\noptimized separately.\nAfter filtering, we again compute RGP for all\nprofessions. We monitor the following metrics to\nmeasure the overall improvement of the de-biasing\nalgorithm on the set of 104 gender-neutral nouns\nSGN :\n8For ELECTRA , we observed higher correlation of the\nbias probe on penultimate layer 0.668.\n21\n(a) BERT base (out of 768 dims)\n (b) BERT large (out of 1024 dims)\n (c) ELECTRA (out of 256 dims)\nFigure 2: The number of selected dimensions for each of the tasks: CI , CII , and shared for both tasks.\nMSEGN = 1\n|SGN |\n∑\nw∈SGN\nRGP(w)2 (10)\nMean squared errorshow how far from zero\nRGP is. The advantage of this metric is that the\nbias of some words cannot be compensated by the\nopposite bias of others. The main objective of de-\nbiasing is to minimize mean squared error.\nMEANGN = 1\n|SGN |\n∑\nw∈SGN\nRGP(w)2 (11)\nMean shows whether the model is skewed to-\nward predicting specific gender. In cases when the\nmean is close to zero, but MSE is high, we can\ntell that there is no general preference of the model\ntoward one gender, but the individual words are\nbiased.\nVARGN = MSEGN −MEAN2\nGN (12)\nVariance is a similar measure to MSE. It is\nuseful to show the spread of RGP when the mean\nis non-zero.\nAdditionally, we introduce a set of 26 gen-\ndered nouns (SG) for which we expect to observe\nnon-zero RGP. We monitor MSE to diagnose\nwhether semantic gender information is preserved\nin de-biasing:\nMSEG = 1\n|SG|\n∑\nw∈SG\nRGP(w) (13)\n3.3.1 Results\nIn Table 3, we observe that in all cases, gender\nbias measured by MSEGN decreases after filter-\ning of bias subspace. The filtering on more than\nSetting FL MSE MSE MEAN V AR\ngendered gender-neutral\nBERT B - 6.177 0.504 0.352 0.124\n-bias 1 2.914 0.136 -0.056 0.133\n2 2.213 0.102 -0.121 0.088\n+f. gender 1 3.780 0.184 -0.067 0.180\n2 2.965 0.145 -0.144 0.124\nELECTRA - 1.360 0.367 0.163 0.340\n-bias 1 0.100 0.124 0.265 0.054\n2 0.048 0.073 0.200 0.033\n+f. gender 1 0.901 0.186 0.008 0.185\n2 0.488 0.101 -0.090 0.093\nBERT L - 1.363 0.099 0.235 0.044\n-bias 1 0.701 0.051 0.166 0.024\n2 0.267 0.015 0.069 0.011\n4 0.061 0.033 0.162 0.007\n+f. gender 1 1.156 0.057 0.145 0.036\n2 0.755 0.020 0.011 0.020\n4 0.292 0.010 0.037 0.009\nAIM: ↑ ↓ ≈ 0 ↓\nTable 3: Aggregation of relative gender preferencein\nprompts for gendered and gender-neutral nouns. FL\ndenotes the number of the model’s top layers for which\nfiltering was performed.\none layer usually further brings this metric down.\nIt is important to note that the original model dif-\nfers in the extent to which their predictions are\nbiased. The mean square error is the lowest for\nBERT large (0.099), noticeably it is lower than in\nother analyzed models after de-biasing (except for\nELECTRA after 2-layer filtering 0.073).\nThe predictions of all the models are skewed\ntoward predicting male pronoun when the noun is\nrevealed. Most of the pronouns used in the evalua-\ntion were professional names. Therefore, we think\nthat this result is the manifestation of the stereotype\nthat career-related words tend to be associated with\nmen.\nAfter filtering BERT base becomes slightly\nskewed toward female pronouns (MEANGN <0).\n22\nSetting FL Accuracy\nBERT L BERT B ELECTRA\nOriginal - 0.516 0.526 0.499\n-bias 1 0.515 0.479 0.429\n2 0.504 0.474 0.434\n4 0.479 - -\n+f. gender 1 0.515 0.479 0.434\n2 0.510 0.480 0.433\n4 0.489 - -\nTable 4: Top-1 accuracy for all tokens in EWT UD\n(Silveira et al., 2014). FT is the number of the model’s\ntop layers for which filtering was performed.\nFor the two remaining models, we observe that\nkeeping factual gender signal performs well in de-\ncreasing MEANGN .\nAnother advantage of keeping factual gender\nrepresentation is the preservation of the bias in\nsemantically gendered nouns, i.e., higher MSEG.\n3.4 How Does Bias Filtering Affect Masked\nLanguage Modeling?\nWe examine whether filtering affects the model’s\nperformance on the original task. For that pur-\npose, we evaluate top-1 prediction accuracy for\nthe masked tokens in the test set from English\nWeb Treebank UD (Silveira et al., 2014) with 2077\nsentences. We also evaluate the capability of the\nmodel to infer the personal pronoun based on the\ncontext. We use the GAP Coreference Dataset\n(Webster et al., 2018) with 8908 paragraphs. In\neach test case, we mask a pronoun referring to a\nperson usually mentioned by their name. In the\nsentences, gender can be easily inferred from the\nname. In some cases, the texts also contain other\n(un-masked) gender pronouns.\n3.4.1 Results: All Tokens\nThe results in Table 4 show that filtering out bias\ndimensions moderately decrease MLM accuracy:\nup to 0.037 for BERT large; 0.052 for BERT base;\n0.07 for ELECTRA . In most cases exempting fac-\ntual gender information from filtering decreases the\ndrop in results.\n3.4.2 Results: Personal Pronouns in GAP\nWe observe a more significant drop in results in the\nGAP dataset after de-biasing. The deterioration can\nbe alleviated by omitting factual gender dimensions\nin the filter. For BERT large and ELECTRA this\nsetting can even bring improvement over the orig-\ninal model. Our explanation of this phenomenon\nSetting FL Accuracy\nOverall Male Female\nBERT L - 0.799 0. 816 0.781\n-bias 1 0.690 0.757 0.624\n2 0.774 0.804 0.744\n4 0.747 0.770 0.724\n+f. gender 1 0.754 0.782 0.726\n2 0.785 0.801 0.769\n4 0.801 0.807 0.794\n-f. gender 1 0.725 0.775 0.675\n2 0.763 0.788 0.738\n4 0.545 0.633 0.458\nBERT B - 0.732 0.752 0.712\n-bias 1 0.632 0.733 0.531\n2 0.597 0.706 0.487\n+f. gender 1 0.659 0.734 0.584\n2 0.620 0.690 0.549\n-f. gender 1 0.634 0.662 0.606\n2 0.604 0.641 0.567\nELECTRA - 0.652 0.680 0.624\n-bias 1 0.506 0.731 0.280\n2 0.485 0.721 0.249\n+f. gender 1 0.700 0.757 0.642\n2 0.691 0.721 0.661\n-f. gender 1 0.395 0.660 0.129\n2 0.473 0.708 0.239\nTable 5: Top-1 accuracy for masked pronouns in GAP\ndataset (Webster et al., 2018). FT is the number of the\nmodel’s top layers for which filtering was performed.\nis that filtering can decrease the confounding in-\nformation from stereotypically biased words that\naffect the prediction of correct gender.\nIn this experiment, we also examine the filter,\nwhich removes all factual-gender dimensions. Ex-\npectedly such a transformation significantly de-\ncreases the accuracy. However, we still obtain rela-\ntively good results, i.e., accuracy higher than 0.5,\nwhich is a high benchmark for choosing gender by\nrandom. Thus, we conjecture that the gender signal\nis still left in the model despite filtering.\nSummary of the Results: We observe that the\noptimal de-biasing setting is factual gender preserv-\ning filtering (F−b,+f ). This approach diminishes\nstereotypical bias in nouns while preserving gen-\nder information for gendered nouns (section 3.3).\nMoreover, it performs better in masked language\n23\nmodeling tasks (section 3.4).\n4 Related Work\nIn recent years, much focus was put on evaluat-\ning and countering bias in language representations\nor word embeddings. Bolukbasi et al. (2016) ob-\nserved the distribution of Word2Vec embeddings\n(Mikolov et al., 2013) encode gender bias. They\ntried to diminish its role by projecting the embed-\ndings along the so-called gender direction, which\nseparates gendered words such as he and she. They\nmeasure the bias as cosine similarity between an\nembedding and the gender direction.\nGenderDirection ≈− →he−−→she (14)\nZhao et al. (2018b) propose a method to diminish\ndifferentiation of word representations in the gen-\nder dimension during training of the GloVe embed-\ndings (Pennington et al., 2014). Nevertheless, the\nfollowing analysis of Gonen and Goldberg (2019)\nargued that these approaches remove bias only par-\ntially and showed that bias is encoded in the multi-\ndimensional subspace of the embedding space. The\nissue can be resolved by projecting in multiple di-\nmensions to further nullify the role of gender in\nthe representations (Ravfogel et al., 2020). Drop-\nping all the gender-related information, e.g., the\ndistinction between feminine and masculine pro-\nnouns can be detrimental to gender-sensitive appli-\ncations. Kaneko and Bollegala (2019) proposed a\nde-biasing algorithm that preserves gendered infor-\nmation in gendered words.\nUnlike the approaches above, we work with con-\ntextual embeddings of language models. Vig et al.\n(2020) investigated bias in the representation of the\ncontextual model (GPT-2, Radford et al. (2019)).\nThey used causal mediation analysis to identify\ncomponents of the model responsible for encod-\ning bias. Nadeem et al. (2021) and Nangia et al.\n(2020) propose a method of evaluating bias (includ-\ning gender) with counterfactual test examples, to\nsome extent similar to our prompts.\nQian et al. (2019) and Liang et al. (2020) employ\nprompts similar to ours to evaluate the gender bias\nof professional words in language models. The\nlatter work also aims to identify and remove gender\nsubspace in the model. In contrast to our approach,\nthey do not guard factual gender signal.\nRecently, Stanczak and Augenstein (2021) sum-\nmarized the research on the evaluation and mitiga-\ntion of gender bias in the survey of 304 papers.\n5 Discussion\n5.1 Bias Statement\nWe define bias as the connection between a word\nand the specific gender it is usually associated with.\nThe association usually stems from the imbalanced\nnumber of corpora mentions of the word in male\nand female contexts. This work focuses on the\nstereotypical bias of nouns that do not have other-\nwise denotation of gender (semantic or grammat-\nical). We consider such a denotation as factual\ngender and want to guard it in the models’ repre-\nsentation.\nOur method is applied to language models, hence\nwe recognize potential application in language gen-\neration. We envision the case where the language\nmodel is applied to complete the text about a per-\nson, where we don’t have implicit information\nabout their gender. In this scenario, the model\nshould not be compelled by stereotypical bias to\nassign a specific gender to a person. On the other\nhand, when the implicit information about a per-\nson’s gender is provided in the context, the gener-\nated text should be consistent.\nLanguage generation is becoming ubiquitous in\neveryday NLP applications (e.g., chat-bots, auto-\ncompletion Dale (2020)). Therefore it is important\nto ensure that the language models do not propagate\nsex-based discrimination.\nThe proposed method can also be implemented\nin deep models for other tasks, e.g., machine trans-\nlation systems. In machine translation, bias is es-\npecially harmful when translating from English\nto languages that widely denote gender grammati-\ncally. In translation to such languages generation of\ngendered nouns tends to be made based on stereo-\ntypical gender roles instead of factual gender infor-\nmation provided in the source language (Stanovsky\net al., 2019).\n5.2 Limitations\nIt is important to note that we do not remove the\nwhole of the gender information in our filtering\nmethod. Therefore, a downstream classifier could\neasily retrieve the factual gender of a person men-\ntioned in a text, e.g., their CV .\nThis aspect makes our method not applicable\nto downstream tasks that use gender-biased data.\nFor instance, in the task of predicting a profession\nbased on a person’s biography (De-Arteaga et al.,\n2019), there are different proportions of men and\nwomen among holders of specific professions. A\n24\nclassifier trained on de-biased but not de-gendered\nembeddings would learn to rely on gender property\nin its predictions.\nAdmittedly, in our results, we see that the pro-\nposed method based on orthogonal probesdoes\nnot fully remove gender bias from the representa-\ntions section 3.3. Even though our method typically\nidentifies multiple dimensions encoding bias and\nfactual gender information, there is no guarantee\nthat all such dimensions will be filtered. Noticeably,\nthe de-biased BERT base still underperform off-\nthe-shelf BERT large in terms of MSEGN . The\nreason behind this particular method was its ability\nto disentangle the representation of two language\nsignals, in our case: gender bias and factual gender\ninformation.\nLastly, the probe can only recreate linear trans-\nformation, while in a non-linear system such\nas Transformer, the signal can be encoded non-\nlinearly. Therefore, even when we remove the\nwhole bias subspace, the information can be re-\ncovered in the next layer of the model (Ravfogel\net al., 2020). It is also the reason why we decided\nto focus on the top layers of models.\n6 Conclusions\nWe propose a new insight into gender informa-\ntion in contextual language representations. In de-\nbiasing, we focus on the trade-off between remov-\ning stereotypical bias while preserving the semantic\nand grammatical information about the gender of\na word from its context. Our evaluation of gender\nbias showed that three analyzed masked language\nmodels (BERT large, BERT based, and ELEC-\nTRA ) are biased and skewed toward predicting\nmale gender for profession names. To mitigate this\nissue, we disentangle stereotypical bias from fac-\ntual gender information. Our filtering method can\nremove the former to some extent and preserve the\nlatter. As a result, we decrease the bias in predic-\ntions of language models without significant dete-\nrioration of their performance in masked language\nmodeling task.\nAknowlegments\nWe thank anonymous reviewers and our colleagues:\nJoão Paulo de Souza Aires, Inbal Magar, and\nYarden Tal, who read the previous versions of this\nwork and provided helpful comments and sugges-\ntions for improvement. The work has been sup-\nported by grant 338521 of the Grant Agency of\nCharles University.\nReferences\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-\nrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Ian Goodfellow, Andrew Harp,\nGeoffrey Irving, Michael Isard, Yangqing Jia, Rafal\nJozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\nLevenberg, Dan Mané, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\nFernanda Viégas, Oriol Vinyals, Pete Warden, Mar-\ntin Wattenberg, Martin Wicke, Yuan Yu, and Xiao-\nqiang Zheng. 2015. TensorFlow: Large-scale ma-\nchine learning on heterogeneous systems. Software\navailable from tensorflow.org.\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man is\nto computer programmer as woman is to homemaker?\ndebiasing word embeddings. In Proceedings of the\n30th International Conference on Neural Information\nProcessing Systems, NIPS’16, page 4356–4364, Red\nHook, NY , USA. Curran Associates Inc.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nRobert Dale. 2020. Natural language generation: The\ncommercial state of the art in 2020. Natural Lan-\nguage Engineering, 26:481–487.\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\nlach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kenthapadi,\nand Adam Tauman Kalai. 2019. Bias in bios: A case\nstudy of semantic representation bias in a high-stakes\nsetting. In FAT* ’19: Conference on Fairness, Ac-\ncountability, and Transparency.\nDaniel de Vassimon Manela, David Errington, Thomas\nFisher, Boris van Breugel, and Pasquale Minervini.\n2021. Stereotype and skew: Quantifying gender bias\nin pre-trained and fine-tuned language models. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 2232–2242, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n25\nHila Gonen and Yoav Goldberg. 2019. Lipstick on a\npig: Debiasing methods cover up systematic gender\nbiases in word embeddings but do not remove them.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 609–614,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2019.\nGender-preserving debiasing for pre-trained word\nembeddings. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1641–1650, Florence, Italy. Associa-\ntion for Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nDeng Liang, Chen Zheng, Lei Guo, Xin Cui, Xiuzhang\nXiong, Hengqiao Rong, and Jinpeng Dong. 2020.\nBERT enhanced neural machine translation and se-\nquence tagging model for Chinese grammatical error\ndiagnosis. In Proceedings of the 6th Workshop on\nNatural Language Processing Techniques for Edu-\ncational Applications, pages 57–66, Suzhou, China.\nAssociation for Computational Linguistics.\nTomasz Limisiewicz and David Mareˇcek. 2021. Intro-\nducing orthogonal constraint in structural probes. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 428–442,\nOnline. Association for Computational Linguistics.\nZachary C. Lipton. 2018. The Mythos of Model Inter-\npretability. Queue, 16(3):31–57.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality.\nIn Advances in Neural Information Processing Sys-\ntems, volume 26. Curran Associates, Inc.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nYusu Qian, Urwa Muaz, Ben Zhang, and Jae Won Hyun.\n2019. Reducing gender bias in word-level language\nmodels with a gender-equalizing loss function. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics: Student Re-\nsearch Workshop, pages 223–228, Florence, Italy.\nAssociation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael\nTwiton, and Yoav Goldberg. 2020. Null it out: Guard-\ning protected attributes by iterative nullspace projec-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7237–7256, Online. Association for Computational\nLinguistics.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor, John\nBauer, and Christopher D. Manning. 2014. A gold\nstandard dependency corpus for English. In Pro-\nceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC-2014).\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\nmoyer. 2019. Evaluating gender bias in machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1679–1684, Florence, Italy. Association for\nComputational Linguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart Shieber. 2020. Investigating gender bias in lan-\nguage models using causal mediation analysis. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 12388–12401. Curran Associates,\nInc.\nKellie Webster, Marta Recasens, Vera Axelrod, and\nJason Baldridge. 2018. Mind the gap: A balanced\ncorpus of gendered ambiguou. In Transactions of the\nACL, page to appear.\n26\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018a. Gender bias\nin coreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018b. Learning gender-neutral word\nembeddings. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 4847–4853, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nA Technical Details\nWe use batches of size 10. Optimization is con-\nducted with Adam (Kingma and Ba, 2015) with\ninitial learning rate 0.02 and meta parameters:\nβ1 = 0.9, β2 = 0.999, and ϵ = 10−8. We use\nlearning rate decay and an early-stopping mecha-\nnism with a decay factor10. The training is stopped\nafter three consecutive epochs not resulting in the\nimprovement of the validation loss learning rate.\nWe clip each gradient’s norm at c = 1.0. The\northogonal penalty was set to λO = 0.1.\nWe implemented the network in TensorFlow 2\n(Abadi et al., 2015). The code will be available on\nGitHub.\nA.1 Computing Infrastructure\nWe optimized probes on a GPU core GeForce GTX\n1080 Ti. Training a probe on top of one layer of\nBERT large takes about 5 minutes.\nA.2 Number of Parameters in the Probe\nThe number of the parameters in the probe depends\non the model’s embedding size demb. The orthog-\nonal transformationmatrix consist of d2\nemb; both\nintercept and scalling vector have demb parame-\nters. Altogether, the size of the probe equals to\nd2\nemb + 4·demb.\nB Details about Datasets\nWinoMT is distributed under MIT license; EWT\nUD under Creative Commons 4.0 license; GAP\nunder Apache 2.0 license.\nC Results for Different Filtering\nThresholds\nIn table 6 we show how the choice of filtering\nthreshold ϵ affects the results of our method for\nEpsilon MSE MSE MEAN V AR\ngendered gender-neutral\n10−2 0.762 0.083 0.233 0.029\n10−4 0.756 0.081 0.230 0.028\n10−6 0.764 0.074 0.213 0.029\n10−8 0.738 0.078 0.225 0.027\n10−10 0.721 0.082 0.234 0.027\n10−12 0.701 0.051 0.166 0.024\n10−14 0.709 0.043 0.138 0.023\n10−16 0.770 0.023 0.013 0.022\nTable 6: Tuning of filtering threshold ϵ. Results for\nfiltering bias in the last layer of BERT large.\nNOUN Relative Gender Preference\nBERT base BERT large ELECTRA Avg.\nFemale Gendered\ncouncilwoman -4.262 -2.050 -0.832 -2.381\npolicewoman -4.428 -1.710 -0.928 -2.355\nprincess -3.486 -1.598 -1.734 -2.273\nactress -3.315 -1.094 -2.319 -2.242\nchairwoman -4.020 -1.818 -0.629 -2.156\nwaitress -2.806 -1.167 -2.475 -2.150\nbusimesswoman -3.202 -1.696 -1.096 -1.998\nqueen -2.752 -0.910 -2.246 -1.969\nspokeswoman -2.543 -2.126 -1.017 -1.895\nstewardess -3.484 -2.215 0.089 -1.870\nmaid -3.092 -0.822 -1.452 -1.788\nwitch -2.068 -0.706 -1.476 -1.416\nnun -2.472 -0.974 -0.613 -1.353\nMale Gendered\nwizard 0.972 0.314 0.237 0.508\nmanservant 0.974 0.493 0.115 0.527\nsteward 0.737 0.495 0.675 0.636\nspokesman 0.846 0.591 0.515 0.651\nwaiter 1.003 0.473 0.639 0.705\npriest 0.988 0.442 0.928 0.786\nactor 1.366 0.392 0.632 0.797\nprince 1.401 0.776 0.418 0.865\npoliceman 1.068 0.514 1.202 0.928\nking 1.399 0.658 0.772 0.943\nchairman 1.140 0.677 1.069 0.962\ncouncilman 1.609 1.040 0.419 1.023\nbusinessman 1.829 0.549 0.985 1.121\nTable 7: List of gendered nouns with evaluated bias in\nthree analyzed models (RGP).\nBERT large. We decided to pick the threshold\nequal to 10−12, as lowering it brought only minor\nimprovement in MSEGN .\nD Evaluation of Bias in Language Models\nWe present the list of 26 gendered words and their\nempirical bias in table 7. Following tables tables 8\nand 9 show the evaluation results for 104 gender-\nneutral words.\n27\nNOUN Relative Gender Preference Bias Class\nBERT base BERT large ELECTRA Avg. BERT base BERT large ELECTRA Annotated\nhousekeeper -2.813 -0.573 -2.642 -2.009 female female female female\nnurse -2.850 -0.568 -2.103 -1.840 female female female female\nreceptionist -1.728 -0.776 -2.302 -1.602 female female female female\nhairdresser -0.400 -0.228 -0.785 -0.471 female female female female\nlibrarian 0.019 -0.088 -0.768 -0.279 neutral female female female\nassistant -0.477 0.020 -0.117 -0.192 female neutral neutral female\nsecretary -0.564 0.024 -0.027 -0.189 female neutral neutral female\nvictim -0.075 0.091 -0.323 -0.102 female neutral female neutral\nteacher 0.129 0.175 -0.595 -0.097 neutral neutral female female\ntherapist 0.002 0.016 -0.233 -0.072 neutral neutral female neutral\nchild -0.100 0.073 -0.154 -0.060 female neutral female neutral\nsalesperson -0.680 -0.206 0.719 -0.056 female female male male\npractitioner 0.150 0.361 -0.621 -0.037 neutral neutral female neutral\nclient -0.157 0.250 -0.165 -0.024 female neutral female neutral\ndietitian 0.175 0.003 -0.143 0.012 neutral neutral female neutral\ncook -0.150 0.141 0.048 0.013 female neutral neutral male\neducator 0.278 0.144 -0.375 0.015 neutral neutral female neutral\ncashier 0.009 0.041 0.017 0.023 neutral neutral neutral female\ncustomer -0.401 0.328 0.142 0.023 female neutral neutral neutral\nattendant -0.157 0.226 0.010 0.027 female neutral neutral female\ndesigner 0.200 0.173 -0.232 0.047 neutral neutral female female\ncleaner 0.151 0.099 -0.089 0.053 neutral neutral neutral female\nteenager 0.343 0.088 -0.210 0.074 neutral neutral female neutral\npassenger 0.015 0.151 0.100 0.089 neutral neutral neutral neutral\nguest 0.162 0.258 -0.150 0.090 neutral neutral female neutral\nsomeone 0.026 0.275 0.082 0.128 neutral neutral neutral neutral\nstudent 0.307 0.281 -0.195 0.131 neutral neutral female neutral\nclerk 0.107 0.216 0.105 0.143 neutral neutral neutral female\nvisitor 0.471 0.273 -0.280 0.155 neutral neutral female neutral\ncounselor 0.304 0.165 0.009 0.159 neutral neutral neutral female\neditor 0.244 0.161 0.081 0.162 neutral neutral neutral female\nresident 0.528 0.300 -0.304 0.174 neutral neutral female neutral\npatient 0.009 0.305 0.217 0.177 neutral neutral neutral neutral\nhomeowner 0.422 0.158 -0.002 0.192 neutral neutral neutral neutral\nadvisee 0.175 0.252 0.168 0.199 neutral neutral neutral neutral\npsychologist 0.259 0.232 0.124 0.205 neutral neutral neutral neutral\nnutritionist 0.474 0.134 0.020 0.210 neutral neutral neutral neutral\ndispatcher 0.250 0.118 0.284 0.217 neutral neutral neutral neutral\ntailor 0.572 0.382 -0.250 0.235 neutral male female female\nemployee 0.124 0.228 0.371 0.241 neutral neutral neutral neutral\nowner 0.044 0.213 0.493 0.250 neutral neutral neutral neutral\nadvisor 0.339 0.271 0.148 0.253 neutral neutral neutral neutral\nwitness 0.287 0.319 0.187 0.264 neutral neutral neutral neutral\nwriter 0.497 0.237 0.060 0.265 neutral neutral neutral female\nundergraduate 0.575 0.148 0.075 0.266 neutral neutral neutral neutral\nveterinarian 0.616 0.007 0.209 0.278 neutral neutral neutral neutral\npedestrian 0.446 0.226 0.170 0.281 neutral neutral neutral neutral\ninvestigator 0.518 0.228 0.120 0.289 neutral neutral neutral neutral\nhygienist 0.665 0.274 -0.040 0.300 neutral neutral neutral neutral\nbuyer 0.529 0.190 0.183 0.300 neutral neutral neutral neutral\nsupervisor 0.257 0.228 0.426 0.304 neutral neutral neutral male\nworker 0.151 0.267 0.511 0.310 neutral neutral neutral neutral\nbystander 0.786 0.117 0.072 0.325 male neutral neutral neutral\nTable 8: List of gender-neutral nouns with their evaluated bias RGP. Female and male bias classes are assigned for\n20 lowest negative and 20 highest positiveRGP values. Annotated bias from Zhao et al. (2018a). Part 1 of 2.\n28\nNOUN Relative Gender Preference Bias Class\nBERT base BERT large ELECTRA Avg. BERT base BERT large ELECTRA Annotated\nchemist 0.579 0.311 0.107 0.332 neutral neutral neutral neutral\nadministrator 0.428 0.236 0.350 0.338 neutral neutral neutral neutral\nexaminer 0.445 0.281 0.296 0.341 neutral neutral neutral neutral\nbroker 0.376 0.358 0.295 0.343 neutral neutral neutral neutral\ninstructor 0.413 0.196 0.436 0.348 neutral neutral neutral neutral\ndeveloper 0.536 0.338 0.172 0.349 neutral neutral neutral male\ntechnician 0.312 0.362 0.400 0.358 neutral neutral neutral neutral\nbaker 0.622 0.287 0.178 0.362 neutral neutral neutral female\nplanner 0.611 0.341 0.147 0.366 neutral neutral neutral neutral\nbartender 0.628 0.282 0.293 0.401 neutral neutral neutral neutral\nparamedic 0.787 0.094 0.333 0.405 male neutral neutral neutral\nprotester 0.722 0.498 0.019 0.413 neutral male neutral neutral\nspecialist 0.501 0.363 0.392 0.419 neutral male neutral neutral\nelectrician 0.935 0.283 0.076 0.431 male neutral neutral neutral\nphysician 0.438 0.359 0.502 0.433 neutral neutral neutral male\npathologist 0.817 0.307 0.181 0.435 male neutral neutral neutral\nanalyst 0.645 0.315 0.361 0.440 neutral neutral neutral male\nappraiser 0.729 0.305 0.302 0.445 neutral neutral neutral neutral\nonlooker 0.978 0.093 0.274 0.448 male neutral neutral neutral\njanitor 0.702 0.493 0.174 0.456 neutral male neutral male\nmover 0.717 0.407 0.253 0.459 neutral male neutral male\nchef 0.682 0.348 0.352 0.460 neutral neutral neutral neutral\nlawyer 0.696 0.271 0.421 0.462 neutral neutral neutral male\nparalegal 0.829 0.247 0.313 0.463 male neutral neutral neutral\ndoctor 0.723 0.355 0.322 0.467 neutral neutral neutral neutral\nauditor 0.654 0.329 0.504 0.496 neutral neutral neutral female\nofficer 0.465 0.463 0.584 0.504 neutral male male neutral\nsurgeon 0.368 0.417 0.733 0.506 neutral male male neutral\nprogrammer 0.543 0.304 0.684 0.510 neutral neutral male neutral\nscientist 0.568 0.427 0.548 0.514 neutral male neutral neutral\npainter 0.721 0.298 0.555 0.525 neutral neutral male neutral\npharmacist 0.862 0.244 0.495 0.534 male neutral neutral neutral\nlaborer 0.996 0.557 0.058 0.537 male male neutral male\nmachinist 0.821 0.449 0.361 0.544 male male neutral neutral\narchitect 0.790 0.243 0.609 0.547 male neutral male neutral\ntaxpayer 0.785 0.525 0.339 0.550 male male neutral neutral\nchief 0.595 0.472 0.628 0.565 neutral male male male\ninspector 0.631 0.344 0.726 0.567 neutral neutral male neutral\nplumber 1.186 0.468 0.205 0.620 male male neutral neutral\nconstruction worker 0.770 0.326 0.769 0.622 male neutral male male\ndriver 0.847 0.415 0.603 0.622 male male male male\nmanager 0.456 0.346 1.084 0.628 neutral neutral male male\nengineer 0.562 0.385 0.987 0.645 neutral male male neutral\nsheriff 0.850 0.396 0.708 0.651 male male male male\nCEO 0.701 0.353 0.989 0.681 neutral neutral male male\nmechanic 0.752 0.307 1.098 0.719 male neutral male male\nguard 0.907 0.586 0.720 0.738 male male male male\naccountant 0.610 0.291 1.350 0.750 neutral neutral male female\nfarmer 1.044 0.477 0.736 0.753 male male male male\nfirefighter 1.294 0.438 0.604 0.779 male male male neutral\ncarpenter 0.934 0.415 1.263 0.870 male male male male\nTable 9: List of gender-neutral nouns with their evaluated bias RGP. Female and male bias classes are assigned for\n20 lowest negative and 20 highest positiveRGP values. Annotated bias from Zhao et al. (2018a). Part 2 of 2.\n29",
  "topic": "Gender bias",
  "concepts": [
    {
      "name": "Gender bias",
      "score": 0.8044240474700928
    },
    {
      "name": "Grammatical gender",
      "score": 0.7690033912658691
    },
    {
      "name": "Computer science",
      "score": 0.6420689821243286
    },
    {
      "name": "Focus (optics)",
      "score": 0.5860974788665771
    },
    {
      "name": "Word (group theory)",
      "score": 0.5796825289726257
    },
    {
      "name": "Natural language processing",
      "score": 0.5569632649421692
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5078368782997131
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4681684672832489
    },
    {
      "name": "Language model",
      "score": 0.42373886704444885
    },
    {
      "name": "Linguistics",
      "score": 0.4175987243652344
    },
    {
      "name": "Psychology",
      "score": 0.3773188292980194
    },
    {
      "name": "Cognitive psychology",
      "score": 0.32041746377944946
    },
    {
      "name": "Social psychology",
      "score": 0.18504005670547485
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Noun",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}