{
    "title": "Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds",
    "url": "https://openalex.org/W4285606658",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2097816882",
            "name": "Heng Wang",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2144428860",
            "name": "Chaoyi Zhang",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2112561679",
            "name": "Jianhui Yu",
            "affiliations": [
                "University of Sydney"
            ]
        },
        {
            "id": "https://openalex.org/A2098283194",
            "name": "Weidong Cai",
            "affiliations": [
                "University of Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950784811",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W4288329833",
        "https://openalex.org/W4238700468",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2552002300",
        "https://openalex.org/W2032846947",
        "https://openalex.org/W4287553517",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2990818246",
        "https://openalex.org/W3182910454",
        "https://openalex.org/W3203949114",
        "https://openalex.org/W2976658881",
        "https://openalex.org/W3174377922",
        "https://openalex.org/W3095974555",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3173271937",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3107521863",
        "https://openalex.org/W2988715931",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3034655362",
        "https://openalex.org/W3193171560",
        "https://openalex.org/W3015373178",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963758027",
        "https://openalex.org/W2594519801",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2964062501",
        "https://openalex.org/W2903617461",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W2979987972",
        "https://openalex.org/W2903435684"
    ],
    "abstract": "Dense captioning in 3D point clouds is an emerging vision-and-language task involving object-level 3D scene understanding. Apart from coarse semantic class prediction and bounding box regression as in traditional 3D object detection, 3D dense captioning aims at producing a further and finer instance-level label of natural language description on visual appearance and spatial relations for each scene object of interest. To detect and describe objects in a scene, following the spirit of neural machine translation, we propose a transformer-based encoder-decoder architecture, namely SpaCap3D, to transform objects into descriptions, where we especially investigate the relative spatiality of objects in 3D scenes and design a spatiality-guided encoder via a token-to-token spatial relation learning objective and an object-centric decoder for precise and spatiality-enhanced object caption generation. Evaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed SpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in CIDEr@0.5IoU, respectively. Our project page with source code and supplementary files is available at https://SpaCap3D.github.io/.",
    "full_text": "Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds\nHeng Wang, Chaoyi Zhang , Jianhui Yu and Weidong Cai\nSchool of Computer Science, University of Sydney, Australia\n{heng.wang, chaoyi.zhang, jianhui.yu, tom.cai}@sydney.edu.au\nAbstract\nDense captioning in 3D point clouds is an emerg-\ning vision-and-language task involving object-level\n3D scene understanding. Apart from coarse se-\nmantic class prediction and bounding box regres-\nsion as in traditional 3D object detection, 3D dense\ncaptioning aims at producing a further and finer\ninstance-level label of natural language description\non visual appearance and spatial relations for each\nscene object of interest. To detect and describe ob-\njects in a scene, following the spirit of neural ma-\nchine translation, we propose a transformer-based\nencoder-decoder architecture, namely SpaCap3D,\nto transform objects into descriptions, where we\nespecially investigate the relative spatiality of ob-\njects in 3D scenes and design a spatiality-guided\nencoder via a token-to-token spatial relation learn-\ning objective and an object-centric decoder for pre-\ncise and spatiality-enhanced object caption genera-\ntion. Evaluated on two benchmark datasets, Scan-\nRefer and ReferIt3D, our proposed SpaCap3D out-\nperforms the baseline method Scan2Cap by 4.94%\nand 9.61% in CIDEr@0.5IoU, respectively. Our\nproject page with source code and supplementary\nfiles is available at https://SpaCap3D.github.io/.\n1 Introduction\nWith continuous advance of deep learning in both computer\nvision and natural language processing, a variety of multi-\nmodal studies in these two areas have gained increasingly ac-\ntive attention [Uppal et al., 2022]. Dense captioning, as first\nintroduced in image domain [Johnson et al., 2016], is a rep-\nresentative task among them to describe every salient pixel-\nformed area with a sequence of words. Just as many other\nmultimodal tasks, the scope of conventional dense captioning\nresearch is mainly restricted to 2D space [Yang et al., 2017;\nYin et al., 2019; Li et al., 2019b; Kim et al., 2019 ]. In re-\ncent past, with the popularity of 3D point-based scene data\ncollection and application, 3D scene understanding and anal-\nysis have become feasible and prominent [Qi et al., 2018;\nWaldet al., 2020; Zhanget al., 2021; Zhaoet al., 2021]. Also,\ntwo newly introduced dense annotation datasets tailored for\n3D indoor scenes [Dai et al., 2017], ScanRefer [Chen et al.,\nthis\nis\na\nround\ntable\n.\nit\nis\nin\nthe\nmiddle\nof\ntwo\nchairs\n.\n<eos>\nSpatiality-guided Transformer\n(a)\n(b)\n(c)\n(d)\nFigure 1: Dense captioning for a target point-cloud object. Target\nand its surrounding objects are marked in green and red, respec-\ntively. (a) Point-based scene input. (b) Detected vision tokens.\n(c) Neighbor-to-target contribution visualization in our encoder. (d)\nTarget-to-word contribution visualization in our decoder. Detailed\nexplanations including the color scheme used for attention heads,\ncan be found in the supplementary.\n2020] and ReferIt3D [Achlioptas et al., 2020 ], create more\nopportunities in 3D multimodal research. Facilitated by these\ndatasets and pioneering point cloud processing techniques[Qi\net al., 2017; Qi et al., 2019 ], dense captioning has been re-\ncently lifted from 2D to 3D [Chen et al., 2021 ] to localize\nand describe each object in a 3D point cloud scene, which\nis beneficial for applications such as robotics manipulation,\naugmented reality, and autonomous driving.\nIn real world, human descriptions of an object or instruc-\ntions to navigate a robot always involve good understanding\nand capturing of relative spatiality in 3D space [Landau and\nJackendoff, 1993; Skubic et al., 2004]. In 3D dense caption-\ning datasets, spatial language (above, under, left, right, in\nfront of, behind, etc.) could be ubiquitous, taking up 98.7%\nand 90.5% in ScanRefer and ReferIt3D respectively accord-\ning to their dataset statistics. However, such critical 3D spa-\ntiality has not been well explored in previous work [Chen\net al., 2021 ]. Also, the sequential training strategy in their\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1393\nadopted RNN-based captioner could make it prohibitively\nlong to reach convergence. In contrast, attention mechanism\nin prevalent Transformer [Vaswani et al., 2017 ] is capable\nof not only long-range relationship learning but also efficient\nparallel training. However, relation learning in transformer-\nbased architectures depends only on the final task objective\nand lacks an explicit guidance on the relation, which could\nmake it hard to precisely learn how 3D spatially-related an\nobject is with respect to another one in our task.\nTo bridge the gap, in this work, we conduct careful rel-\native spatiality modeling to represent 3D spatial relations\nand propose a spatiality-guided Transformer for 3D dense\ncaptioning. Building upon a detection backbone which de-\ncomposes the input 3D scene into a set of object candidates\n(i.e., tokens), we propose SpaCap3D, a transformer-based\nencoder-decoder architecture, which consists of a spatiality-\nguided encoder where the relation learning among tokens is\nadditionally supervised with atoken-to-token spatial relation-\nship guidance whose labels are on-the-fly generated main-\naxis spatial relations based on our relative spatiality modeling\nand an object-centric decoder to transform each spatiality-\nenhanced vision token into a description, as shown in Fig-\nure 1. With faster training and efficient usage of data at hand,\nour proposed method exceeds Scan2Cap [Chen et al., 2021]\nby 4.94% and 9.61% in CIDEr@0.5IoU on ScanRefer [Chen\net al., 2020 ] and Nr3D from ReferIt3D [Achlioptas et al.,\n2020], respectively. To iterate, our contributions are three-\nfold:\n• We propose a token-to-token spatial relation learning\nobjective with relative spatiality modeling to guide the\nencoding of main-axis spatial relations for better repre-\nsentation of 3D scene objects.\n• An integrated and efficient transformer-based encoder-\ndecoder architecture, SpaCap3D, is proposed for 3D\ndense captioning, consisting of a spatiality-guided en-\ncoder and an object-centric decoder.\n• We achieve a new state-of-the-art performance on Scan-\nRefer and Nr3D from ReferIt3D over previous work.\n2 Related Work\n2.1 Dense Captioning: from 2D to 3D\nFollowing [Johnson et al., 2016], current state-of-the-art 2D\ndense captioning methods use a region proposal network to\ndetect salient regions and extract their CNN-based features as\nrepresentation where a RNN captioner is applied to generate\nphrases or sentences. [Yang et al., 2017 ] attached a late-\nfusion context feature extractor LSTM with a captioner one\nto emphasize contextual cues. [Yin et al., 2019] and [Li et\nal., 2019b] proposed to consider not only the global context\nbut also the neighboring and the target-guided object context,\nrespectively. In [Kim et al., 2019], a sub-pred-obj relation-\nship was learnt via a triple-stream network.\nAs 2D image is a projection of 3D world without depth\ndimension, spatial relations expressed in 2D dense caption-\ning are usually implicit and ambiguous. To directly tackle\n3D world, Scan2Cap [Chen et al., 2021] proposed 3D dense\ncaptioning on point cloud data. In Scan2Cap, relations\namong object proposals are learnt through a message pass-\ning network where only angular deviation relations whose\nlabels [Avetisyan et al., 2019 ] (i.e., transformation matri-\nces) are hard to collect and incomplete are taken into con-\nsideration, while captions are generated by RNN-based de-\ncoder following 2D dense captioning methods, which is time-\nconsuming in training. Compared to Scan2Cap, our work\nfocuses on more common spatial relations, and the relation\nlabels are easy to obtain for all objects during training as our\nlabel generation process only requires access to the bound-\ning box information (i.e., box center and size). In addition,\nfast parallel-training in transformer-based architectures guar-\nantees the efficiency of our method.\n2.2 Transformers in Image Captioning\nAlthough Transformers in dense captioning have not been\nexplored to the best of our knowledge, there are a string of\nworks in the related image captioning area. To learn bet-\nter region representations, encoders in existing work were\nincorporated with learnable prior knowledge [Cornia et al.,\n2020], geometric weight learnt from geometry features [Her-\ndade et al., 2019], region and attribute representations [Li et\nal., 2019a], inter- and intra-layer global representations [Ji et\nal., 2021 ], or proposal- and grid-level features [Luo et al.,\n2021]. Their decoders focus on how to learn the implicit\nrelationship among region proposals so that a general and\noverall image-level caption can be generated. Yet, dense cap-\ntioning in 3D world involves more diversities and degrees of\nfreedom in object arrangements and it emphasizes finer and\ndenser object-level descriptions, which captures more inter-\nactions between an object and its surrounding environment.\nTo tackle these challenges, we use location-aware positional\nencoding to encode global position and a spatiality-guided\nencoder with token-to-token spatial relation learning objec-\ntive to learn relative 3D spatial structures, while our object-\ncentric decoder transforms each spatiality-enhanced object\nvisual representation into a description.\n3 Method\nWe present our spatiality-guided Transformer as SpaCap3D\nfor 3D dense captioning in Figure 2. We first use a detec-\ntor to decompose input scene into object proposals which\nwe refer to as vision tokens, and then feed them into a\nspatiality-guided encoder for token-to-token relative 3D spa-\ntiality learning. Lastly, a shared object-centric decoder is con-\nditioned on each spatiality-enhanced object vision token to\ndescribe them individually.\n3.1 3D Object Detection\nFor an input point cloud of size N ×(3 +K) including a\n3-dim coordinate and extra K-dim features such as colors,\nnormals, and multi-view features for each point, we first ap-\nply an object detector to generate M object candidates which\nare input tokens to later components. To make fair compar-\nisons with existing work, we deploy the same seminal detec-\ntor V oteNet[Qi et al., 2019] with PointNet++ [Qi et al., 2017]\nas feature aggregation backbone to produce initial object fea-\ntures X∈ RM×C. We also keep the vote cluster center coor-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1394\nthis is a big black tv . it is above a thin table\nthis is a long kitchen counter . it is below two windows\nthis is a gray trash can . it is to the right of a table\nspatiality-enhanced \nvision tokens\nx\ny\nz\nM × C\nDecoder\nDecoder\nDecoder\nshared\nN × (3 +K)\nObject \nDetector\nFeature Extractor\nM × (3 +C)\ninput\nobject candidates\nC-d features\n3-d coordinates\n+\n f \nM × C\nEncoder\n3 × M × M\nx-axis y-axis z-axis\nMain-axis Spatial Relation Label Maps\nposition-encoded \nvision tokens\n-1\n0\n+1\n-1\n-1\n0\n0\n+1\n+1\n+1\n0\n+1\n-1\n-1\n0\n0\n-1\n+1\n-1\n0\n-1\n-1\n+1\n0\n0\n+1\n+1\nt1\nt2\ntM\nt′ 1\nt′ 2\nt′ M\n“tv”\n“counter”\n“trash can”\nt1\nt2\ntM\n“tv”t1\n“counter”t2\n“trash can”tM\ngenerated object-level captions\nFigure 2: The overview of our proposed method SpaCap3D for spatiality-guided 3D dense captioning. The encoder-decoder framework\nconsists of an object detector to generate object proposals (i.e., tokens), a learnable function f to project coordinates, a token-to-token\nspatial relation-guided encoder to incorporate relative 3D spatiality into tokens, and a shared object-centric decoder to generate per-object\ndescriptions.\n(+1,0,+1)\ny\nx\nz\n(-1,-1,0)\n(-1,-1,0)\n(-1,+1,0)\nx\ny\nz\ncouch pillow chair-1 coﬀee table chair-2\nFigure 3: An example of our 3D spatiality modeling of main-axis\nspatial relations. With respect to the couch, chair-2 is in the neg-\native half x-axis, positive half y-axis, and on the same floor along\nz-axis, hence its spatial relation to couch is represented as (-1, +1,\n0). As spatial relation is relative, the relation of couch to chair-2 is\nexpressed reversely as (+1, -1, 0).\ndinates P∈ RM×3 from its proposal module as global loca-\ntion information for later positional encoding.\n3.2 Token-to-Token Spatial Relationship Learning\nTo generate spatiality-enhanced captions, we carefully con-\nduct a relative spatiality modeling from which spatial rela-\ntions among tokens can be formulated and learnt through a\ntoken-to-token (T2T) spatial relationship learning objective.\nRelative Spatiality Modeling\nWe first introduce how we model the relative spatiality in 3D\nscenes. We construct a local 3D coordinate system (right-\nhanded) with an object itself as the origin center, and the re-\nlation of a surrounding object with respect to the center object\ncan be represented as a (λx, λy, λz)-triplet where each entry\nλ ∈{+1 , −1, 0}indicates which half axis the surrounding\ni\nj+\n-\n(a) Absolute.\ni\nj\nα\nβ\n+\n- (b) Covered.\nj\ni\n+\n-\nα\nβ (c) Covering.\nFigure 4: Three cases when an object oi is to the positive direction\nof another object oj along x-/y- axis. Top view. The arrow points to\nthe positive direction. α and β are the lower and upper area limits,\nrespectively.\nobject sits along different axes (+1 for positive, -1 for nega-\ntive, and 0 for same position), as illustrated in Figure 3. Note\nwe only consider coarse direction such as positive or negative\nand ignore the exact displacement along axes to decomplex-\nify the modeling. Specifically, according to the intersection\nof two objects, the definition of a positive relation can vary.\nBefore discussing it, we introduce the notations we use in the\nfollowing. □i indicates the bounding box of an objectoi. And\n□k\ni:∧/∨/|denotes the parameters of the bounding box along\nk-axis where k ∈{x, y, z}, i.e., ∧for minimum value, ∨for\nmaximum value, and |for side length. As relations along z-\naxis involve different heights while those along x-/y- axis are\ngrounded to the same floor level, we discuss the criteria of\nbeing positive for them separately.\nSame floor. Depending on how two objects overlap with\neach other, we categorize the criteria ofoi being positive w.r.t.\noj along x-/y- axis into three cases as illustrated in Figure 4.\n(a) Absolute positive: when the overlapping area does not\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1395\nexceed the side length of oj and both the bottom and up of oi\nare above those of oj: (□k\ni:∧ ≥□k\nj:∧) ∩(□k\ni:∨ > □k\nj:∨) (b)\nCovered positive: when the overlapping area equals the side\nlength of oi itself, i.e, oi is completely covered by oj, and it\nresides at the upper area of oj: (□k\ni:∧> □k\nj:∧+ α ×□k\nj:|) ∩\n(□k\nj:∧+β×□k\nj:|< □k\ni:∨≤□k\nj:∨) (c) Covering positive: this\nis the reverse situation of condition (b) when the overlapping\narea equals the side length of oj instead, and oj lags at oi’s\nlower area: (□k\ni:∧ < □k\nj:∧ < □k\ni:∧+ α ×□k\ni:|) ∩(□k\nj:∨ <\n□k\ni:∧+ β ×□k\ni:|). We additionally define two objects are\nat the same position when the bottom and up of one object\nare within a certain tolerance ϵ from those of the other one:\n(|□k\ni:∨−□k\nj:∨|≤ϵ) ∩(|□k\ni:∧−□k\nj:∧|≤ϵ). The lower and\nupper area limits α, β, and tolerance ϵ are empirically set as\n0.3, 0.7, and 0.1×□k\nj:|, respectively.\nVarious heights. We define oi is at positive direction to oj\nwhen oi is over the lower area ofoj: □z\ni:∧≥□z\nj:∧+α×□z\nj:|.\nPositional Encoding\nBefore spatial relation learning, as shown in Figure 2, we ap-\nply a learnable functionf(·) to each vote cluster centerp ∈P\nto incorporate the global location information into each token.\nSpecifically, f(·) is defined as:\nf(p) = (σ(BN (pW1)))W2, (1)\nwhere W1 ∈R3×C and W2 ∈RC×C are two linear trans-\nformations to project 3-dim geometric features into the same\nhigh dimensional space as general features X. We use ReLU\nas the activation function σ and a Batch Normalization (BN)\nlayer to adjust the feature distribution. The input tokens\nT = {t1, t2, ..., tM }∈ RM×C for our encoder are then cre-\nated by adding the newly projected C-dim global geometry\ninto X.\nSpatial Relation Learning\nWhen learning a token-to-token spatial relationship, we aim\nto capture the corresponding object-to-object relation. For\na target token ti and its neighboring token tj, we select\ntheir ground truth objects oi and oj as the ones with the\nnearest centers to their predicted centers. We can gener-\nate three main-axis spatial relation label maps, Rx, Ry, and\nRz, for the M tokens based on their ground truth objects’\nrelations, as per the criteria defined above. Label entries\b\nrx\ni,j, ry\ni,j, rz\ni,j\n\t\n∈{+1 , −1, 0}define how the object oi rep-\nresented by token ti is in positive/negative/same direction\nalong x-, y-, and z-axis to another objectoj represented by tj,\nrespectively. In a standard transformer, the encoder is com-\nposed of n repetitions of a multi-head self-attention (MSA)\nlayer and a feed-forward network (FFN). A normalization\nand residual link (AddNorm) is applied for each layer. The\nupdated token t′\ni after attention mechanism is defined as the\nsummation of ωi,jtj for j = 0, 1, ..., Mwhere ωi,j represents\nthe attention coefficient of ti to tj. In other words, the up-\ndated token is comprised of different contribution of its neigh-\nboring tokens. To encode such contribution with relative 3D\nspatiality information, we apply our relation prediction head\n(RPH) to each contribution ωi,jtj. As illustrated in Figure 5,\nSelf-attention\nFFN\nSelf-attention\nFFN\nattention\nM × M × 1\nvalue\nM × 1 × C\nM × M × C\ncontribution\nRelation Predict Head\nM × M × 3\ny-axis\nM × M × 3\nz-axis\nM × M × 3\nx-axis -1\n0\n+1\n-1\n-1\n0\n0\n+1\n+1\n+1\n0\n+1\n-1\n-1\n0\n0\n-1\n+1\n-1\n0\n-1\n-1\n+1\n0\n0\n+1\n+1\nStandard Transformer Encoder\n× (n − 1)\nRx\nRy\nRz\n̂Rx\n̂Ry\n̂Rz×\nFigure 5: Detailed encoder architecture. AddNorm is omitted for\nbrevity.\nMasked \nSelf-attention\nFFN\n× n\n<sos>\nthis\nis\ntable\nsalient area vision tokens\nCross-attention\nTarget-aware Masked \nSelf-attention\nFFN\n<sos>\nthis\nis\ntable\ntarget vision token\n1 0 0 0\n1 1 0 0\n1 1 1 0\n1 1 1 1\n1 1 1 1\n0\n0\n0\n0\n1\nTarget-aware Mask\nthis\nis\na\n<eos>\nStandard Transformer Decoder in \nImage Captioning\nObject-centric Decoder\n× n\nFigure 6: Detailed decoder architecture. Positional encoding and\nAddNorm for decoder are omitted for brevity.\nthe relation prediction happens at the last encoder block. We\nuse a standard three-layer MLP with two ReLU activated C-\ndim hidden layers and a linear output layer. The output of the\nRPH is a 9-dim vector where each three represents the pre-\ndicted relation along a main axis. The T2T spatial relation\nlearning is hence guided by our relation loss as:\nLrelation =\nX\nk∈{x,y,z}\nLCE ( ˆRk, Rk), (2)\nwhere LCE denotes three-class cross-entropy loss.\n3.3 Object-centric Decoder\nIn image captioning Transformers, the decoder consisting of\nn stacks of a masked MSA layer, a cross-attention layer with\nthe output of encoder, and a FFN, attends all salient area vi-\nsion tokens to conclude one sentence describing the whole\nimage. On the other hand, in dense captioning, the target is\neach object. Therefore, we propose an object-centric decoder\nwith target-aware masked self-attention layer to update each\nword token by attending both its previous words and the tar-\nget vision token as depicted in Figure 6. Compared to the\nstandard decoder, our design can fulfill the dense captioning\ntask but in a more concise and efficient manner. More specif-\nically, it would stack a target vision token mask (in pink) on\nthe basis of the existing word token mask (in blue) and feed\nthe target vision token as well as word tokens together into\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1396\nDatasets Methods Input C@0.5IoU B-4@0.5IoU\nM@0.5IoU R@0.5IoU mAP@0.5IoU\nScanRefer Scan2Cap xyz+normal+mv 39.08 23.32\n21.97 44.78 32.21\nOurs-base xyz 40.19 24.71\n22.01 45.49 32.32\nOurs xyz 42.53 25.02\n22.22 45.65 34.44\nOurs xyz+normal+rgb 42.76 25.38 22.84\n45.66 35.55\nOurs xyz+normal+mv 44.02 25.26 22.33\n45.36 36.64\nNr3D/ReferIt3D Scan2Cap xyz+normal+mv 24.10 15.01\n21.01 47.95 32.21\nOurs-base xyz 31.06 17.94\n22.03 49.63 30.65\nOurs xyz 31.43 18.98\n22.24 49.79 33.17\nOurs xyz+normal+rgb 33.24 19.46 22.61 50.41 33.23\nOurs xyz+normal+mv 33.71 19.92 22.61 50.50 38.11\nTable 1: Quantitative comparison with SOTA methods on ScanRefer and Nr3D/ReferIt3D. Ours-base is the baseline variant of standard\nTransformer adapted for 3D dense captioning, where we use standard encoder with sinusoidal positional encoding and late-guide decoder.\nThe input denotes various combinations of different information: xyz refers to points’ coordinates. normal means the normal vector of each\npoint. rgb uses color information and mv stands for pretrained 2D multi-view features.\nthe self-attention layer. Considering ours as an early-guide\nway to condition the decoder on the target object, we also im-\nplemented a late-guide variant and have it ablated in Table 2.\n3.4 Learning Objective\nWe define our final loss asL = δ∗Ldet +Ldes +ζ ∗Lrelation,\nwhere Lrelation is our proposed T2T spatial relation learning\nobjective defined in Equation 2. As for the object detection\nloss Ldet and the description loss Ldes, we follow Scan2Cap,\nand more details can be found in [Chen et al., 2021]. δ and ζ\nare set as 10 and 0.1, respectively, to maintain similar magni-\ntude of different losses.\n4 Experiments\n4.1 Datasets, Metrics, and Implementation Details\nDatasets. We evaluate our proposed method on ScanRe-\nfer [Chen et al., 2020 ] and Nr3D from ReferIt3D [Achliop-\ntas et al., 2020], both of which provide free-form human de-\nscriptions for objects in ScanNet [Dai et al., 2017 ]. Same\nas Scan2Cap [Chen et al., 2021 ], for ScanRefer/Nr3D, we\ntrain on 36,665/32,919 captions for 7,875/4,664 objects from\n562/511 scenes and evaluate on 9,508/8,584 descriptions for\n2,068/1,214 objects from 141/130 scenes.\nMetrics. We benchmark the performances on both detec-\ntion and captioning perspectives. For detection, we use\nthe mean average precision thresholded by 0.5 IoU score\n(mAP@0.5). For captioning, we employ m@0.5IoU where\nonly the prediction whose IoU is larger than 0.5 will be con-\nsidered [Chen et al., 2021 ]. The captioning metric m can\nbe the one especially designed for image captioning such as\nCIDEr (C) [Vedantamet al., 2015], or those focusing more on\nmachine translation or on text summarizing such as BLEU-4\n(B-4) [Papineni et al., 2002 ], METEOR (M) [Banerjee and\nLavie, 2005], and ROUGE (R) [Lin, 2004].\nImplementation Details. To make fair comparisons, we\nuse the same training and testing protocols as Scan2Cap.\nFollowing Scan2Cap, we set the input number of points N\nas 40,000 and the number of object proposal M as 256.\nThe output feature dimension C from object detector is 128.\nFor Transformer, we set the number of encoder and decoder\nblocks n as 6 and the number of heads in multi-head at-\ntentions as 8. The dimensionality of input and output of\neach layer is 128 except that for the inner-layer of feed-\nforward networks as 2048. We implement AddNorm as pre-\nLN where outputs are first normalized and then added with\ninputs. Keeping the length of descriptions within 30 tokens\nand marking words not appearing in GloVE [Pennington et\nal., 2014] as unknown, we learn the word embedding from\nscratch encoded with sinusoidal positional encoding. We\nimplement1 our proposed model in PyTorch [Paszke et al.,\n2019] and train end-to-end with ADAM [Kingma and Ba,\n2015] in a learning rate of 1 ×10−3. The detection back-\nbone is fine-tuned together with the end-to-end training from\nthe pretrained V oteNet [Qi et al., 2019 ] model provided by\nScan2Cap. To avoid overfitting, we apply the same weight\ndecay factor 1 ×10−5 and the same data augmentation as\nScan2Cap. We firstly randomly flip the input point cloud\nalong the YZ- and XZ-plane and then rotate along x-, y-, and\nz-axis by a random angle within [−5◦, 5◦]. We finally trans-\nlate the point cloud along all axes by a random distance within\n[−0.5, 0.5] meters. All experiments were trained on a single\nGeForce RTX 2080Ti GPU with a batch size of 8 samples\nfor 50 epochs, while the model is checked and saved when it\nreaches the best CIDEr@0.5IoU on val split every 2000 iter-\nations. Training with our proposed framework takes around\n33 and 29 hours for ScanRefer and Nr3D/ReferIt3D, respec-\ntively. During inference, we use non-maximum suppression\nto remove overlapping proposals and only keep those propos-\nals whose IoUs with ground truth bounding boxes are larger\nthan 0.5. With a batch size of 8, the inference time includ-\ning evaluation with the four captioning metrics, for ScanRe-\nfer and Nr3D/ReferIt3D, is 83 and 75 seconds, respectively.\nWe implement the attention visualization shown in Figure 1\nand the supplementary based on [Vig, 2019].\n4.2 Quantitative Comparison\nTable 1 presents the quantitative comparison with SOTA\nmethods, showing our proposed SpaCap3D with xyz-input\n1https://github.com/heng-hw/SpaCap3D\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1397\nOurs w/o T2T: this is a white trash \ncan. it is to the left of the toilet.\nOurs: this is a black trash can. it is \nunder a sink.\nGT: this white bright lamp is \nabove the desk that is to the \nright of the bed. \nOurs w/o T2T: this is a white \nlamp. it is to the left of the bed.\nOurs: this is a white lamp. it is \non a desk.\nGT: the window is rectangular and \nas a clear middle area . it is to the \nleft of the desk and shelves.\nOurs w/o T2T: this is a window. it \nis to the left of the bed.\nOurs: this is a window in the \nmiddle of the room. it is to the left \nof the desk.GT: this is a black computer monitor that is sitting \non a desk. the monitor is facing a black chair.\nOurs w/o T2T: this is a black monitor. it is to the \nleft of the desk.\nOurs: this is a black monitor. it is on a desk.\nGT: the trash can is under the sink. \nit is closest to the divider on the \nright.\nGT: there is a round brown table. \nit is between two chairs near a \nwall of the room.\nOurs w/o T2T: this is a round \ntable. it is to the left of the chair.\nOurs: this is a round table. it is in \nbetween two chairs.\nGT: this is a pillow on the left \nside of the bed. it is to the left \nof another pillow.\nOurs w/o T2T: this is a white \npillow. it is on the bed.\nOurs: this is a white pillow. it is \non the left side of the bed.\nGT: it is a white pillow. the pillow is \non the right of the bed on the right.\nOurs w/o T2T: this is a white \npillow. it is on the bed.\nOurs: this is a white pillow. it is on \nthe right side of the bed.\nGT: it is a black suitcase by the bed. it is sitting \non the ﬂoor.\nOurs w/o T2T: this is a black suitcase. it is on the \nﬂoor.\nOurs: this is a black suitcase. it is on the ﬂoor by \nthe bed.\nGT: it is a rolling, black, ofﬁce chair. this chair is \nat the back of the table and is the third chair \nfrom the end.\nOurs w/o T2T: this is a black chair. it is at a \ntable.\nOurs: this is a black chair. it is in the middle of \nthe table.\nGT: this is a black chair. it is at the end of the \ntable.\nOurs w/o T2T: this is a black chair. it is at a table.\nOurs: this is a black chair. it is at the corner of the \ntable.\n(a) (b) (c)\n(d) (e) (f)\nFigure 7: Visualization for our method with and without token-to-token (T2T) spatial relation guidance. Caption boxes share the same color\nwith detection bounding boxes for ground truth (green), ours w/ T2T (blue), and ours w/o T2T (pink). Imprecise parts of sentences produced\nby ours w/o T2T guidance are marked in red, and correctly expressed spatial relations predicted by T2T-guided method are underscored.\noutperforms not only the baseline Scan2Cap but also the stan-\ndard Transformer (ours-base) in all metrics. It is worth not-\ning that SpaCap3D manages to exceed Scan2Cap, even when\nSpaCap3D only takes simple coordinates as inputs whereas\nScan2Cap uses much richer pretrained multi-view features.\nTo be comparable with Scan2Cap’s input setting, we also pro-\nvide a variant of our proposed method with color and normal\nfeatures added and it achieves better results as expected. The\ninclusion of multi-view features can further boost the perfor-\nmance and we point out that it requires 6 more hours to train\ncompared to its simpler counterpart (i.e., xyz+normal+rgb).\nThe average forward pass time per training batch is around\n0.5s and 0.2s, and the average per-batch inference time is\naround 11.7s and 2.3s, respectively for Scan2Cap and our\nSpaCap3D (taking the same xyz-input), which demonstrates\nour efficiency.\n4.3 Qualitative Analysis\nTo visualize the importance of relative spatiality, we display\nsome detect-and-describe results of the proposed method with\nand without relative 3D spatiality learning in Figure 7. If\nthe T2T guidance is discarded, the generated descriptions\ncould lack unique spatial relations and tend to be general as\nshown in Figure 7 (a) and (b). While our spatiality-guided\nTransformer distinguishes two chairs at a table (“middle”and\n“corner”) and two pillows on the bed (“left” and “right”)\nby their spatiality, the method without such guidance could\ncollapse into generic expressions lacking specific spatial re-\nlations. Also in Figure 7 (b), our proposed method with T2T\nis capable of describing more relations for the suitcase com-\npared to the one without T2T guidance (“on the floor by the\nbed” vs. “on the floor”). Figure 7 (c), (d), and (e) show cases\nwhen T2T guidance boosts correct spatial relation prediction.\nWe also emphasize Figure 7 (f) where a table is in between\ntwo chairs. Instead of just describing the relation between the\ntable and one chair, the T2T-guided method considers the ex-\nistence of both chairs and generates more thoughtful expres-\nsion. More results are displayed in Supplementary Figure 3.\n4.4 Ablation Study\nComponent Analysis\nWe investigate components of our proposed architecture, the\nlate-guide and early-guide decoder, attention-based encoder\nwith vote center-based positional encoding, and token-to-\ntoken spatial relation learning (T2T), in Table 2. Model A\nand B adopt the decoder alone and the outcome that Model\nB achieves 3.56% and 1.17% improvement over Model A on\ncaptioning and detection respectively demonstrates the supe-\nriority of our proposed early-guide decoder. Based on Model\nB, Model C uses an attention-based encoder to learn the long-\nrange dependency among object proposals, which leads to a\ndetection performance increase by 1.31%. With the guidance\nof our T2T spatial relation learning objective, the encoder\nfunctions better as can be seen in the results from Model D\nwhich performs the best in both captioning and detection.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1398\nModel Decoder Encoder T2T C@0.5IoU\n(captioning)\nmAP@0.5IoU\n(detection)late-guide early-guide\nA ✓ 37.80 30.97\nB ✓ 41.36 32.14\nC ✓ ✓ 41.14 33.45\nD ✓ ✓ ✓ 42.53 34.44\nTable 2: Ablation study on different components of our proposed method. T2T denotes the token-to-token spatial relation learning objective.\nPositional encoding C@0.5IoU\nnon-learnable none 39.41\nsinusoidal 39.44\nlearnable random 42.29\nbox center 42.49\nbox center* 40.04\nvote center 42.53\nTable 3: Ablation study on choices of encoder’s positional encoding.\nbox center∗ indicates concatenation of box center and size.\nPositional Encoding Analysis\nTo verify the choice of learnable vote center-based posi-\ntional encoding for encoder, we elaborate on different ways\nin Table 3. The non-learnable sinusoidal method in standard\nTransformer has slightly better effect over the one without\nany positional encoding, showing the necessity of such en-\ncoding in our task. For learnable encoding, we compare with\nrandom one used in 2D object detection Transformer [Car-\nion et al., 2020 ] and box center-based one adopted in 3D\nobject detection Transformer [Liu et al., 2021 ]. More de-\ntails of these learnable positional encoding methods can be\nfound in the supplementary file. We find that random learn-\nable positional encoding can boost the performance compared\nto non-learnable ones and the incorporation of object position\ninformation can further advance the performance. Among all\nlearnable encoding ways, our vote center-based one achieves\nthe best results.\n5 Conclusion\nIn this work, we propose a new state-of-the-art framework\ndubbed as SpaCap3D for the newly emerging 3D dense cap-\ntioning task. We propose to formulate object relations with\nrelative 3D spatiality modeling, based on which we build a\ntransformer-based architecture where a spatiality-guided en-\ncoder learns how objects interact with their surrounding en-\nvironment in 3D spatiality via a token-to-token spatial rela-\ntion learning guidance, and a shared object-centric decoder\nis conditioned on each spatiality-enhanced token to individu-\nally generate precise and unambiguous object-level captions.\nExperiments on two benchmark datasets show that our inte-\ngrated framework outperforms the baseline method by a great\ndeal in both accuracy and efficiency.\nReferences\n[Achlioptas et al., 2020] Panos Achlioptas, Ahmed Abdelre-\nheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas.\nReferit3D: Neural listeners for fine-grained 3D object\nidentification in real-world scenes. In ECCV, pages 422–\n440. Springer, 2020.\n[Avetisyan et al., 2019] Armen Avetisyan, Manuel Dahnert,\nAngela Dai, Manolis Savva, Angel X Chang, and Matthias\nNießner. Scan2CAD: Learning CAD model alignment in\nRGB-D scans. In CVPR, pages 2614–2623, 2019.\n[Banerjee and Lavie, 2005] Satanjeev Banerjee and Alon\nLavie. METEOR: An automatic metric for MT evaluation\nwith improved correlation with human judgments. In ACL\nWorkshop: Intrinsic and Extrinsic Evaluation Measures\nfor Machine Translation and/or Summarization, pages 65–\n72, 2005.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. In ECCV, pages 213–229. Springer, 2020.\n[Chen et al., 2020] Dave Zhenyu Chen, Angel X Chang, and\nMatthias Nießner. ScanRefer: 3D object localization in\nRGB-D scans using natural language. In ECCV, pages\n202–221. Springer, 2020.\n[Chen et al., 2021] Zhenyu Chen, Ali Gholami, Matthias\nNießner, and Angel X Chang. Scan2Cap: Context-aware\ndense captioning in RGB-D scans. In CVPR, pages 3193–\n3203, 2021.\n[Cornia et al., 2020] Marcella Cornia, Matteo Stefanini,\nLorenzo Baraldi, and Rita Cucchiara. Meshed-memory\ntransformer for image captioning. In CVPR, pages 10578–\n10587, 2020.\n[Dai et al., 2017] Angela Dai, Angel X Chang, Manolis\nSavva, Maciej Halber, Thomas Funkhouser, and Matthias\nNießner. ScanNet: Richly-annotated 3D reconstructions\nof indoor scenes. In CVPR, pages 5828–5839, 2017.\n[Herdade et al., 2019] Simao Herdade, Armin Kappeler,\nKofi Boakye, and Joao Soares. Image captioning: trans-\nforming objects into words. In NeurIPS, pages 11137–\n11147, 2019.\n[Ji et al., 2021] Jiayi Ji, Yunpeng Luo, Xiaoshuai Sun, Fuhai\nChen, Gen Luo, Yongjian Wu, Yue Gao, and Rongrong\nJi. Improving image captioning by leveraging intra-and\ninter-layer global representation in transformer network.\nIn AAAI, volume 35, pages 1655–1663, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1399\n[Johnson et al., 2016] Justin Johnson, Andrej Karpathy, and\nLi Fei-Fei. DenseCap: Fully convolutional localization\nnetworks for dense captioning. In CVPR, pages 4565–\n4574, 2016.\n[Kim et al., 2019] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun\nOh, and In So Kweon. Dense relational captioning: Triple-\nstream networks for relationship-based captioning. In\nCVPR, pages 6271–6280, 2019.\n[Kingma and Ba, 2015] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In ICLR,\n2015.\n[Landau and Jackendoff, 1993] Barbara Landau and Ray\nJackendoff. “What” and “where” in spatial language\nand spatial cognition. Behavioral and Brain Sciences,\n16(2):255–265, 1993.\n[Li et al., 2019a] Guang Li, Linchao Zhu, Ping Liu, and\nYi Yang. Entangled transformer for image captioning. In\nICCV, pages 8928–8937, 2019.\n[Li et al., 2019b] Xiangyang Li, Shuqiang Jiang, and Jun-\ngong Han. Learning object context for dense captioning.\nIn AAAI, volume 33, pages 8650–8657, 2019.\n[Lin, 2004] Chin-Yew Lin. Rouge: A package for automatic\nevaluation of summaries. In ACL Workshop: Text Summa-\nrization Branches Out, pages 74–81, 2004.\n[Liu et al., 2021] Ze Liu, Zheng Zhang, Yue Cao, Han Hu,\nand Xin Tong. Group-free 3D object detection via trans-\nformers. In ICCV, pages 2949–2958, 2021.\n[Luo et al., 2021] Yunpeng Luo, Jiayi Ji, Xiaoshuai Sun, Li-\nujuan Cao, Yongjian Wu, Feiyue Huang, Chia-Wen Lin,\nand Rongrong Ji. Dual-level collaborative transformer for\nimage captioning. In AAAI, volume 35, pages 2286–2293,\n2021.\n[Papineni et al., 2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for au-\ntomatic evaluation of machine translation. In ACL, pages\n311–318, 2002.\n[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco\nMassa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. In NeurIPS, pages\n8026–8037, 2019.\n[Pennington et al., 2014] Jeffrey Pennington, Richard\nSocher, and Christopher D Manning. Glove: Global\nvectors for word representation. In EMNLP, pages\n1532–1543, 2014.\n[Qi et al., 2017] Charles R Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. InNeurIPS, pages 5099–5108,\n2017.\n[Qi et al., 2018] Charles R Qi, Wei Liu, Chenxia Wu, Hao\nSu, and Leonidas J Guibas. Frustum pointnets for 3D ob-\nject detection from RGB-D data. In CVPR, pages 918–\n927, 2018.\n[Qi et al., 2019] Charles R Qi, Or Litany, Kaiming He, and\nLeonidas J Guibas. Deep hough voting for 3D object de-\ntection in point clouds. In ICCV, pages 9277–9286, 2019.\n[Skubic et al., 2004] Marjorie Skubic, Dennis Perzanowski,\nSamuel Blisard, Alan Schultz, William Adams, Magda\nBugajska, and Derek Brock. Spatial language for human-\nrobot dialogs. IEEE Transactions on Systems, Man,\nand Cybernetics, Part C (Applications and Reviews),\n34(2):154–167, 2004.\n[Uppal et al., 2022] Shagun Uppal, Sarthak Bhagat, Deva-\nmanyu Hazarika, Navonil Majumder, Soujanya Poria,\nRoger Zimmermann, and Amir Zadeh. Multimodal re-\nsearch in vision and language: A review of current and\nemerging trends. Information Fusion, 77:149–171, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, pages 5998–6008, 2017.\n[Vedantam et al., 2015] Ramakrishna Vedantam,\nC Lawrence Zitnick, and Devi Parikh. Cider: Consensus-\nbased image description evaluation. In CVPR, pages\n4566–4575, 2015.\n[Vig, 2019] Jesse Vig. A multiscale visualization of attention\nin the transformer model. InACL: System Demonstrations,\npages 37–42, 2019.\n[Wald et al., 2020] Johanna Wald, Helisa Dhamo, Nassir\nNavab, and Federico Tombari. Learning 3D semantic\nscene graphs from 3D indoor reconstructions. In CVPR,\npages 3961–3970, 2020.\n[Yang et al., 2017] Linjie Yang, Kevin Tang, Jianchao Yang,\nand Li-Jia Li. Dense captioning with joint inference and\nvisual context. In CVPR, pages 2193–2202, 2017.\n[Yin et al., 2019] Guojun Yin, Lu Sheng, Bin Liu, Nenghai\nYu, Xiaogang Wang, and Jing Shao. Context and attribute\ngrounded dense captioning. In CVPR, pages 6241–6250,\n2019.\n[Zhang et al., 2021] Chaoyi Zhang, Jianhui Yu, Yang Song,\nand Weidong Cai. Exploiting edge-oriented reasoning for\n3D point-based scene graph analysis. In CVPR, pages\n9705–9715, 2021.\n[Zhao et al., 2021] Lichen Zhao, Daigang Cai, Lu Sheng,\nand Dong Xu. 3DVG-Transformer: Relation modeling for\nvisual grounding on point clouds. In ICCV, pages 2928–\n2937, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1400"
}