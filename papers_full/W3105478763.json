{
  "title": "Context Analysis for Pre-trained Masked Language Models",
  "url": "https://openalex.org/W3105478763",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2124327469",
      "name": "Yi-An Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2970520603",
      "name": "Garima Lalwani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965939310",
      "name": "Yi Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2785760873",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2952307023",
    "https://openalex.org/W2962777840",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2951309718",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W1712618182",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2508661145",
    "https://openalex.org/W2952732525",
    "https://openalex.org/W4298061300",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3035183289",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W2962862931"
  ],
  "abstract": "Pre-trained language models that learn contextualized word representations from a large un-annotated corpus have become a standard component for many state-of-the-art NLP systems. Despite their successful applications in various downstream NLP tasks, the extent of contextual impact on the word representation has not been explored. In this paper, we present a detailed analysis of contextual impact in Transformer- and BiLSTM-based masked language models. We follow two different approaches to evaluate the impact of context: a masking based approach that is architecture agnostic, and a gradient based approach that requires back-propagation through networks. The findings suggest significant differences on the contextual impact between the two model architectures. Through further breakdown of analysis by syntactic categories, we find the contextual impact in Transformer-based MLM aligns well with linguistic intuition. We further explore the Transformer attention pruning based on our findings in contextual analysis.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3789–3804\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3789\nContext Analysis for Pre-trained Masked Language Models\nYi-An Lai Garima Lalwani Yi Zhang\nAWS AI HLT\n{yianl,glalwani,yizhngn}@amazon.com\nAbstract\nPre-trained language models that learn contex-\ntualized word representations from a large un-\nannotated corpus have become a standard com-\nponent for many state-of-the-art NLP systems.\nDespite their successful applications in vari-\nous downstream NLP tasks, the extent of con-\ntextual impact on the word representation has\nnot been explored. In this paper, we present\na detailed analysis of contextual impact in\nTransformer- and BiLSTM-based masked lan-\nguage models. We follow two different ap-\nproaches to evaluate the impact of context: a\nmasking based approach that is architecture\nagnostic, and a gradient based approach that\nrequires back-propagation through networks.\nThe ﬁndings suggest signiﬁcant differences on\nthe contextual impact between the two model\narchitectures. Through further breakdown of\nanalysis by syntactic categories, we ﬁnd the\ncontextual impact in Transformer-based MLM\naligns well with linguistic intuition. We fur-\nther explore the Transformer attention pruning\nbased on our ﬁndings in contextual analysis.\n1 Introduction\nPre-trained masked language models (MLM) such\nas BERT (Devlin et al., 2019) and ALBERT (Lan\net al., 2019) have set state-of-the-art performance\non a broad range of NLP tasks. The success is often\nattributed to their ability to capture complex syntac-\ntic and semantic characteristics of word use across\ndiverse linguistic contexts (Peters et al., 2018). Yet,\nhow these pre-trained MLMs make use of the con-\ntext remains largely unanswered.\nRecent studies have started to inspect the linguis-\ntic knowledge learned by pre-trained LMs such as\nword sense (Liu et al., 2019a) , syntactic parse trees\n(Hewitt and Manning, 2019), and semantic rela-\ntions (Tenney et al., 2019). Others directly analyze\nmodel’s intermediate representations and attention\nweights to understand how they work (Kovaleva\net al., 2019; V oita et al., 2019).\nWhile previous works either assume access to\nmodel’s internal states or take advantage of model’s\nspecial structures such as self-attention maps, these\nanalysis are difﬁcult to generalize as the architec-\ntures evolve. In this paper, our work complements\nthese previous efforts and provides a richer under-\nstanding of how pre-trained MLMs leverage con-\ntext without assumptions on architectures. We aim\nto answer following questions: (i) How much con-\ntext is relevant to and used by pre-trained MLMs\nwhen composing representations? (ii) How far\ndo MLMs look when leveraging context? That\nis, what are their effective context window sizes?\nWe further deﬁne a target word’s essential context\nas the set of context words whose absence will\nmake the MLM indiscriminate of its prediction.\nWe analyze linguistic characteristics of these essen-\ntial context words to better understand how MLMs\nmanage context.\nWe investigate the contextual impacts in MLMs\nvia two approaches. First, we propose the context\nperturbation analysis methodology that gradually\nmasks out context words following a predetermined\nprocedure and measures the change in the target\nword probability. For example, we iteratively mask\nwords that have the least change to the target word\nprobability until the probability deviates too much\nfrom the start. At this point, the remaining words\nare relevant to and used by the MLM to represent\nthe target word, since further perturbation causes a\nnotable prediction change. Being model agnostic,\nour approach looks into the contextualization in the\nMLM task itself, and quantify them only on the\noutput layer. We refrain from inspecting internal\nrepresentations since new architectures might not\nhave a clear notion of ”layer” with inter-leaving\njump connections such as those in Guo et al. (2019)\nand Yao et al. (2020).\n3790\nThe second approach is adapted from Falenska\nand Kuhn (2019) and estimates the impact of an\ninput subword to the target word probability via the\nnorm of the gradients. We study pre-trained MLMs\nbased on two different architectures: Transformer\nand BiLSTM. The former is essentially BERT and\nthe latter resembles ELMo (Peters et al., 2018).\nAlthough the scope in this work is limited to the\ncomparison between two popular architectures, the\nsame novel methodology can be readily applied to\nmultilingual models as well as other Transformer-\nbased models pre-trained with MLM.\nFrom our analysis, when encoding words using\nsentence-level inputs, we ﬁnd that BERT is able\nto leverage 75% of context on average in terms\nof the sentence length, while BiLSTM has the ef-\nfective context size of around 30%. The gap is\ncompelling for long-range context more than 20\nwords away, wherein, BERT still has a65% chance\nto leverage the words in comparison to BiLSTM\nthat only has 10% or less to do so. In addition,\nwhen restricted to a local context window around\nthe target word, we ﬁnd that the effective context\nwindow size of BERT is around 78% of the sen-\ntence length, whereas BiLSTM has a much smaller\nwindow size of around 50%. With our extensive\nstudy on how different pre-trained MLMs operate\nwhen producing contextualized representations and\nwhat detailed linguistic behaviors can be observed,\nwe exploited these insights to devise a pilot appli-\ncation. We apply attention pruning that restricts\nthe attention window of BERT based on our ﬁnd-\nings. Results show that the performance remains\nthe same with its efﬁciency improved. Our main\ncontributions can be brieﬂy summarized as:\n•Standardize the pre-training setup (model size,\ncorpus, objective, etc.) for a fair comparison\nbetween different underlying architectures.\n•Novel design of a straight-forward and intu-\nitive perturbation-based analysis procedure to\nquantify impact of context words.\n•Gain insights about how different architec-\ntures behave differently when encoding con-\ntexts, in terms of number of relevant context\nwords, effective context window sizes, and\nmore ﬁne-grained break-down with respect to\nPOS and dependency structures.\n•Leverage insights from our analysis to con-\nduct a pilot application of attention pruning\non a sequence tagging task.\n2 Related Work\nPre-training language models (LM) to learn contex-\ntualized word representations from a large amount\nof unlabeled text has been shown to beneﬁt down-\nstream tasks (Howard and Ruder, 2018; Peters et al.,\n2018; Radford et al., 2019). Masked language mod-\neling (MLM) introduced in BERT (Devlin et al.,\n2019) has been widely used as the pre-training task\nin works including RoBERTa (Liu et al., 2019b),\nSpanBERT (Joshi et al., 2020), and ALBERT (Lan\net al., 2019). Many of them employ the Trans-\nformer architecture (Vaswani et al., 2017) that uses\nmulti-head self-attention to capture context.\nTo assess the linguistic knowledge learned by\npre-trained LMs, probing task methodology sug-\ngest training supervised models on top of the word\nrepresentations (Ettinger et al., 2016; Hupkes et al.,\n2018; Belinkov and Glass, 2019; Hewitt and Liang,\n2019). Investigated linguistic aspects span across\nmorphology (Shi et al., 2016; Belinkov et al., 2017;\nLiu et al., 2019a), syntax (Tenney et al., 2019; He-\nwitt and Manning, 2019), and semantics (Conneau\net al., 2018; Liu et al., 2019a).\nAnother line of research inspects internal states\nof pre-trained LMs such as attention weights (Ko-\nvaleva et al., 2019; Clark et al., 2019) or interme-\ndiate word representations (Coenen et al., 2019;\nEthayarajh, 2019) to facilitate our understanding\nof how pre-trained LMs work. In particular, V oita\net al. (2019) studies the evolution of representa-\ntions from the bottom to top layers and ﬁnds that,\nfor MLM, the token identity tends to be recreated\nat the top layer. A close work to us is Khandel-\nwal et al. (2018), they conduct context analysis on\nLSTM language models to learn how much context\nis used and how nearby and long-range context is\nrepresented differently.\nOur work complements prior efforts by analyz-\ning how models pre-trained by MLM make use\nof context and provides insights that different ar-\nchitectures can have different patterns to capture\ncontext. Distinct from previous works, we leverage\nno speciﬁc model architecture nor intermediate rep-\nresentations while performing the context analysis.\nAnother related topic is generic model inter-\npretations including LIME (Ribeiro et al., 2016),\nSHAP (Lundberg and Lee, 2017), and Ancona et al.\n(2017). Despite the procedural similarity, our work\nfocuses on analyzing how pre-trained MLMs be-\nhave when encoding contexts and our methodology\nis both model-agnostic and training-free.\n3791\nModel MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg\nBERT (Devlin et al., 2019) 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBiLSTM + ELMo 72.9/73.4 65.6 71.7 90.2 35.0 64.0 80.8 50.1 67.1\nBERT (ours) 84.6/84.0 71.0 91.5 93.6 55.7 86.2 88.6 67.4 80.3\nBiLSTM (ours) 70.9/70.2 63.0 73.7 90.6 30.5 67.6 81.2 54.6 66.9\nTable 1: GLUE benchmark test results. BiLSTM+ELMo numbers are cited from (Wang et al., 2018). The compa-\nrable performance to previous works validates our pre-training process.\n3 Masked Language Modeling\nGiven a sentence X = ( w1,w2,...,w L) where\neach word wi is tokenized into li subwords\n(si1,...,s ili ), a portion of tokens are randomly\nmasked with the[MASK] token. MLMs are trained\nto recover the original identity of masked tokens\nby minimizing the negative log likelihood (NLL).\nIn practice, BERT (Devlin et al., 2019) randomly\nreplaces 15% tokens by [MASK] for 80% of the\ncases, keep the original token for 10% of the time,\nand replace with a random token for the remaining\n10% of the cases.\nFor context analysis, we perform the masking\nand predictions at the word level. Given a target\nword wt, all its subwords are masked X\\t =\n(...s(t−1)lt−1 ,[MASK],..., [MASK],s(t+1)1...).\nFollowing Devlin et al. (2019), the conditional\nprobability of wt can be computed from outputs of\nMLMs with the independence assumption between\nsubwords:\nP(wt|X\\t) = P(st1 ...s tlt |X\\t)\n=\nlt∏\ni=1\nP(sti|X\\t).\n(1)\nTo investigate how MLMs use context, we pro-\npose procedures to perturb the input sentence from\nX\\t to ˜X\\t and monitor the change in the target\nword probability P(wt|X\\t).\n4 Approach\nOur goal is to analyze the behaviors of pre-trained\nMLMs when leveraging context to recover identity\nof the masked target word wt, e.g. to answer ques-\ntions such as how many context words are consid-\nered and how large the context window is. To this\nend, we apply two analysis approaches. The ﬁrst\none is based on the masking or perturbation of input\ncontext which is architecture agnostic. The second\ngradient-based approach requires back-propagation\nthrough networks.\nOur ﬁrst approach performs context perturba-\ntion analysis on pre-trained LMs at inference time\nand measures the change in masked target word\nprobabilities. To answer each question, we start\nfrom X\\t and design a procedure Ψ that itera-\ntively processes the sentence from last perturbation\n˜Xk+1\n\\t = Ψ( ˜Xk\n\\t). The patterns of P(wt|˜Xk\n\\t) of-\nfer insights to our question. An example of Ψ is to\nmask out a context word that causes the least or neg-\nligible change inP(wt|˜Xk\n\\t). It’s worth mentioning\nthat as pre-trained LMs are often used off-the-shelf\nas a general language encoder, we do not further\nﬁnetune the model on the analysis dataset but di-\nrectly analyze how they make use of context. In\npractice, we loop over a sentence word-by-word to\nset the word as the target ﬁrst and use rest of words\nas the context for our masking process. Since we\ndo the context analysis only with model inference,\nthe whole process is fast - around half day on a\n4-GPU machine to process 12k sentences.\nOur second approach estimates the impact of an\ninput subword sij to P(wt|X\\t) by using deriva-\ntives. Speciﬁcally, we adapt the IMPACT score\nproposed in Falenska and Kuhn (2019) to our ques-\ntions. The score IMPACT (sij,wt) can be computed\nwith the gradients of the negative log likelihood\n(NLL) with respect to the subword embedding:\nIMPACT(sij,wt) =\n∥∂(logP(wt|X\\t))\n∂sij ∥\n∑L\nm\n∑lm\nn ∥∂−logP(wt|X\\t)\n∂smn ∥\n. (2)\nThe l2-norm of the gradient is used as the impact\nmeasure and normalized over all the subwords in\na sentence. In practice, we report the impact of a\ncontext word wi by adding up the scores from its\nsubwords ∑li\nj IMPACT (sij,wt).\nWe investigate two different encoder architec-\ntures of pre-trained MLMs. The ﬁrst one is BERT\nthat employs 12 Transformer encoder layers, 768\ndimension, 3072 feed-forward hidden size, and110\nmillion parameters. The other uses a standard bi-\ndirectional LSTM (Hochreiter and Schmidhuber,\n3792\n60\n 50\n 40\n 30\n 20\n 10\n 0 10 20 30 40 50 60\nRelative Position\n0\n20\n40\n60\n80\n100Probability of being used (%)\nBERT on EWT\nBERT on GUM\nBiLSTM on EWT\nBiLSTM on GUM\n(a) Masking-based context impacts\n60\n 50\n 40\n 30\n 20\n 10\n 0 10 20 30 40 50 60\nRelative Position\n0\n2\n4\n6\n8\n10\n12Mean Impact (%)\nBERT on EWT\nBERT on GUM\nBiLSTM on EWT\nBiLSTM on GUM (b) Gradient-based context impacts\nFigure 1: Analysis of how much context is used by MLMs. (a) Context words at all relative positions have\nsigniﬁcantly higher probabilities to be considered by BERT, compared with BiLSTM. (b) Gradient-based IMPACT\nscore also shows that BERT considers more distant context than BiLSTM, impact scores are normalized to 100%.\nEWT GUM\nSentences 9,673 3,197\nWords 195,093 67,585\nMean Length 20.17 21.14\nMedian Length 17 19\nMax Length 159 98\nTable 2: Statistics of datasets used for analysis\n1997) that has 3 layers, 768 embedding dimension,\n1200 hidden size, and around 115 million param-\neters. The BiLSTM model parameters are chosen\nso that they resemble ELMo while being close to\nBERT in model size. To have a fair comparison, we\npre-train both encoders from scratch on the uncased\nWikipedia-book corpus (wikibook) with the same\npre-training setup as in Devlin et al. (2019). For\nBiLSTM, we add a linear layer and a LayerNorm\n(Ba et al., 2016) on top, to project outputs into 768\ndimension. We validate our pre-trained models by\nﬁne-tuning them on GLUE benchmark (Wang et al.,\n2018) in single-task manner and report test perfor-\nmance comparable to previous works in Table 1.\nOur pre-trained BiLSTM-based MLM also gets\ncomparable results to ELMo (Peters et al., 2018).\nWe perform MLM context analysis on two En-\nglish datasets from the Universal Dependencies\n(UD) project, English Web Treebank (EWT) (Sil-\nveira et al., 2014) and Georgetown University Mul-\ntilayer corpus (GUM) (Zeldes, 2017). Datasets\nfrom the UD project provide consistent and rich lin-\nguistic annotations across diverse genres, enabling\nus to gain insights towards the contexts in MLMs.\nWe use the training set of each dataset for analy-\nsis. EWT consists of 9,673 sentences from web\nblogs, emails, reviews, and social media with the\nmedian length being 17 and maximum length be-\ning 159 words. GUM comprises 3,197 sentences\nfrom Wikipedia, news articles, academic writing,\nﬁctions, and how-to guides with the median length\nbeing 19 and maximum length being98 words. The\nstatistics of datasets are summarized in Table 2.\n5 How much context is used?\nSelf-attention is designed to encode information\nfrom any position in a sequence, whereas BiL-\nSTMs model context through the combination of\nlong- and short-term memories in both left-to-right\nand right-to-left directions. For MLMs, the entire\nsequence is provided to produce contextualized rep-\nresentations, it is unclear how much context in the\nsequence is used by different MLMs.\nIn this section, we ﬁrst propose a perturbation\nprocedure Ψ that iteratively masks out a context\nword contributing to the least absolute change of\nthe target word probability P(wt|˜Xk\n\\t). That is, we\nincrementally eliminate words that do not penalize\nMLMs predictions one by one, until further mask-\ning cause P(wt|˜Xk\n\\t) to deviate too much from the\noriginal probability P(wt|X\\t). At this point, the\nremaining unmasked words are considered being\nused by the MLM since corrupting any of them\ncauses a notable change in target word prediction.\nIn practice, we identify deviations using the\nnegative log likelihood (NLL) that corresponds\nto the loss of MLMs. Assuming NLL has a vari-\nance of ϵat the start of masking, we stop the per-\nturbation procedure when the increase on NLL\nlog P(wt|X\\t) −log P(wt|˜Xk\n\\t) exceeds 2ϵ. We\nobserve that NLLs ﬂuctuate around [−0.1,0.1] at\n3793\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95<100100\nMasked context (% of length)\n0\n2\n4\n6\n8Increase in NLL\nBERT on EWT\nBERT on GUM\nBiLSTM on EWT\nBiLSTM on GUM\nFigure 2: Context usage analysis for MLMs via elimi-\nnation of irrelevant context. BERT uses about 75% of\ncontext while BiLSTM uses around 30%.\nthe start of masking, hence we terminate our proce-\ndure when the NLL increase reaches 0.2. We report\nthe effective context size in terms of percentage of\nlength to normalize the length impact. The analysis\nprocess is repeated using each word in a sentence\nas the target word for all sentences in the dataset.\nFor our second approach, we follow equation 2\nto calculate the normalized impact of each subword\nto the target word and aggregate them for each con-\ntext word to get IMPACT (wi,wt). We group the IM-\nPACT scores by relative position of a wordwi to the\ntarget word wt and plot the average. To compare\nwith our ﬁrst approach, we also use masking-based\nmethod to analyze that for a word with a speciﬁc\nrelative position, what would be its probability of\nbeing used by a MLM.\nBERT uses distant context more than BiLSTM.\nAfter our masking process, a subset of context\nwords are tagged as ”being used” by the pre-trained\nLM. In Figure 1a, we aggregated results in terms\nof relative positions (context-word-to-target-word)\nfor all targets and sentences. ”Probability of being\nused %” denotes when a context word appears at\na relative position to target, how likely is it to be\nrelevant to the pre-trained LM.\nFigure 1a shows that context words at all relative\npositions have substantially higher probabilities to\nbe considered by BERT than BiLSTM. And BiL-\nSTM focuses sharply on local context words, while\nBERT leverages words at almost all the positions.\nA notable observation is that both models consider\na lot more often, words within distance around\n[−10,10] and BERT has as high as 90% probabil-\nity to use the words just before and after the target\nword. Using gradient-based analysis, Figure 1b\nshows similar results that BERT considers more\ndistant context than BiLSTM and local words have\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95<100100\nMasked context (% of length)\n0\n2\n4\n6\n8\n10Increase in NLL\nNOUN - BERT\nADJ - BERT\nVERB - BERT\nDET - BERT\nADP - BERT\nNOUN - BiLSTM\nADJ - BiLSTM\nVERB - BiLSTM\nDET - BiLSTM\nADP - BiLSTM\n(a) Masking-based: Different syntactic categories\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95<100100\nMasked context (% of length)\n0\n2\n4\n6\n8\n10Increase in NLL\nShort - BERT\nMedium - BERT\nLong - BERT\nShort - BiLSTM\nMedium - BiLSTM\nLong - BiLSTM\n(b) Masking-based: Different length buckets\nFigure 3: Context usage analysis for MLMs, instances\nbucketed by syntactic categories of target words or in-\nput lengths. (a) More context is used to model con-\ntext words than function words. (b) BERT uses ﬁxed\namounts of context while BiLSTM’s context usage per-\ncentage varies by input length.\nmore impact to both models than distant words.\nThere are notable differences between two anal-\nysis approaches. Since the gradient-based IM-\nPACT score is normalized into a distribution across\nall positions, it does not show the magnitude of\nthe context impact on the two different models.\nOn the other hand, the masking-based analysis\nshows that BERT uses words at each position more\nthan BiLSTM based on absolute probability values.\nAnother important difference is that the gradient-\nbased approach is a glass-box method and requires\nback-propagation through networks, assuming the\nmodels to be differentiable. On the other hand,\nthe masking-based approach treats the model as a\nblack-box and has no differentiability assumption\non models. In the following sections, we will con-\ntinue analysis with the masking-based approach.\nBERT uses75% of words in a sentence as con-\ntext while BiLSTM considers 30%. Figure 2\nshows the increase in NLL when gradually mask-\ning out the least relevant words. BERT’s NLL\nincreases considerably when 25% of context are\n3794\nmasked, suggesting that BERT uses around75% of\ncontext. For BiLSTM, its NLL goes up remarkably\nafter 70% of context words are masked, meaning\nthat it considers around 30% of context. Albeit\nhaving the same capacity, we observe that BERT\nuses more than two times of context words into\naccount than BiLSTM. This could explain the su-\nperior ﬁne-tuning performance of BERT on tasks\ndemanding more context to solve. We observe that\npre-trained MLMs have consistent behaviors across\ntwo datasets that have different genres. For the fol-\nlowing analysis, we report results combining EWT\nand GUM datasets.\nContent words needs more context than func-\ntion words. We bucket instances based on the\npart-of-speech (POS) annotation of the target word.\nOur analysis covers content words including nouns,\nverbs and adjectives, and function words includ-\ning adpositions and determiners. Figure 3a shows\nthat both models use signiﬁcantly more context\nto represent content words than function words,\nwhich is aligned with linguistic intuitions (Boyd-\nGraber and Blei, 2009). The ﬁndings also show\nthat MLMs handle content and function words in\na similar manner as regular language models do,\nwhich are previously analyzed by Wang and Cho\n(2016); Khandelwal et al. (2018).\nBiLSTM context usage percentage varies by\ninput sentence length, whereas for BERT, it\ndoesn’t. We categorize sentences with length\nshorter than 25 as short, between 25 and 50 as\nmedium, and more than 50 as long. Figure 3b\nshows that BiLSTM uses 35% of context for short\nsentences, 20% for medium, and only10% for long\nsentences. On the other hand, BERT leverages\nﬁxed 75% of context words regardless of the sen-\ntence length.\n6 How far do MLMs look?\nIn the previous section, we looked at how much\ncontext is relevant to the two MLMs via an elimi-\nnation procedure. From Figure 1a and 1b, we also\nobserve that local context is more impactful than\nlong-range context for MLMs. In this section, we\ninvestigate this notion of locality of context even\nfurther and try to answer the question of how far\naway do MLMs actually look at in practice, i.e.,\nwhat is the effective context window size (cws) of\neach MLM.\nFor context perturbation analysis, we introduce\na locality constraint to the perturbation procedure\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100\nAccess to combined context window size on both sides (% of available context)\n0\n2\n4\n6\n8\n10Increase in NLL\nBERT on EWT\nBiLSTM on EWT\nBERT on GUM\nBiLSTM on GUM\nFigure 4: Change in NLL as the context window size\naround target word (left and right combined) changes\nwhile masking words. We aim to identify how local\nversus distant context impacts the target word prob-\nability differently. We start with masking all the\nwords around the target, i.e., the model only relies\non its priors learned during pre-training (cws ∼0%\n1). We iteratively increase the cws on both sides\nuntil all the surrounding context is available (cws\n∼100%). Details of the masking procedure can be\nfound in Appendix. We report the increase in NLL\ncompared to when the entire context is available\nlog P(wt|X\\t) −log P(wt|˜Xk\n\\t), with respect to\nthe increasing cws. This process is repeated using\neach word as the target word, for all the sentences\nin the dataset. We aggregate and visualize the re-\nsults similar to section 5 and use the same threshold\n(0.2) as before to mark the turning point.\nAs shown in Figure 4, increasing the cws around\ntarget word reduces the change of NLL until a point\nwhere the gap is closed. The plot clearly highlights\nthe differences in the behavior of two models -\nfor BERT, words within cws of 78% impact the\nmodel’s ability to make target word predictions,\nwhereas, for BiLSTM, only words within cws of\n50% affect the target word probability. This shows\nthat BERT, leveraging entire sequence by self-\nattention, looks at a much wider context window\nsize (effectivecws ∼78%) in comparison to the\nrecurrent architecture BiLSTM (effectivecws\n∼50%). Besides, BiLSTM shows a clear notion\nof contextual locality that it tends to consider very\nlocal context for target word prediction.\nFurthermore, we investigate the symmetricity of\ncws on either side by following the same procedure\nbut now separately on each side of the target word.\nWe iteratively increase cws either on left side or\nright side while keeping the rest of the words un-\nmasked. More details of the analysis procedure can\n1% here denotes the percent of available context w.r.t.\n(sentence-length - 1) context words, excluding target word.\n3795\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nNOUN - BERT\nNOUN - BiLSTM\n(a) Target word belonging to POS - NOUN\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nDET - BERT\nDET - BiLSTM\n(b) Target word belonging to POS - DET\nFigure 5: Symmetricity analysis of context window\nsize for two target word syntactic categories from short\nsentences l≤25 (a) For NOUN as target, BERT looks\nat words within the window [-16, 16], while BiLSTM\nhas the context window [-7, 7]. (b) When target word\nis DET, BERT looks at words within the window [-14,\n18], while BiLSTM has the context window [-1, 3].\nbe found in the Appendix. The analysis results are\nfurther bucketed by the POS categories of target\nwords as well as input sentence lengths, similar to\nSection 5, to gain more ﬁne-grained insights. In\nFigure 5, we show the symmetricity analysis of\ncws for short length sentences and target word with\nPOS tags - NOUN and DET. The remaining plots\nfor medium and long length sentences with target\nword from other POS tags are shown in Appendix\ndue to the lack of space.\nFrom Figure 5, both models show similar behav-\niors across different POS tags when leveraging sym-\nmetric/asymmetric context. The cws attended to on\neither side is rather similar when target words are\nNOUN, whereas for DET, we observe both mod-\nels paying more attention to right context words\nthan the left. This observation aligns well with\nlinguistic intuitions for English language. We can\nalso observe the striking difference between two\nmodels in effective cws, with BERT attending to a\nmuch larger cws than BiLSTM. The difference in\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95<100100\nMasked context (% of length)\n0\n2\n4\n6\n8\n10Increase in NLL\nBERT on EWT\nBERT on GUM\nBiLSTM on EWT\nBiLSTM on GUM\nFigure 6: Identifying essential context by masking\nmost important words. 35-40% of context is critical\nto BERT while BiLSTM sees about 20% as essential.\nthe left and right cws for DET appears to be more\npronounced for BiLSTM in comparison to BERT.\nWe hypothesize that this is due to BiLSTM’s over-\nall smaller cws (left + right) which makes it only\nattend to the most important words that happen to\nbe mostly in the right context.\n7 What kind of context is essential?\nThere is often a core set of context words that is\nessential to capture the meaning of target word.\nFor example, “Many people think cotton is the\nmost comfortable to wear in hot weather. ”\nAlthough most context is helpful to understand the\nmasked word fabric, cotton and wear are essential\nas it would be almost impossible to make a guess\nwithout them.\nIn this section, we deﬁne essential context as\nwords such that when they are absent, MLMs\nwould have no clue about the target word identity,\ni.e., the target word probability becomes close to\nmasking out the entire sequence P(wt|˜Xmask all).\nTo identify essential context, we design the pertur-\nbation Ψ to iteratively mask words bringing largest\ndrop in P(wt|˜Xk\n\\t) until we reach a point, where\nthe increase in NLL just exceeds the 100% mask\nsetting ( log P(wt|X\\t) −log P(wt|˜Xmask all)).\nThe words masked using above procedure are la-\nbelled as essential context words. We further ana-\nlyze linguistic characteristics of the identiﬁed es-\nsential context words.\nBERT sees35% of context as essential, whereas\nBiLSTM perceives around20%. Figure 6 shows\nthat on average, BERT recognizes around 35% of\ncontext as essential when making predictions, i.e.,\nwhen the increase in NLL is on par with mask-\ning all context. On the other hand, BiLSTM sees\nonly 20% of context as essential. This implies that\nBERT would be more robust than the BiLSTM-\n3796\nContext Distance All targets NOUN ADJ VERB DET ADP\nFull-context Linear 9.37 9.33 9.23 8.97 9.47 9.47\nBERT-essential Linear 6.25 6.42 5.89 5.87 5.65 6.11\nBiLSTM-essential Linear 5.49 6.43 6.03 6.32 4.20 3.77\nFull-context Tree 3.63 3.37 3.73 2.83 4.13 4.31\nBERT-essential Tree 2.91 2.66 2.88 2.20 3.18 3.46\nBiLSTM-essential Tree 2.74 2.66 2.90 2.28 2.74 2.73\nTable 3: Mean distances from essential context words to target words. Linear means linear positional distance and\nTree denotes the dependency tree walk distance. Results are bucketed by part-of-speech tags of target words.\nwhoisresponsiblefor[completing]allpaperworkforenteringa newmarket?PRONAUXADJSCONJVERB \nnsubjcop mark DETNOUNSCONJVERB DETADJNOUNPUNCTadvcl detobj markacl detamodobjpunctroot\ni lovehowitreallydependsonhowa reallyis , hePRONVERBADV ADJ\nconj\nDETNOUN PUNCT[good]horseyourhorse nothowtalentedis .PRONADVVERBADPADV PRONNOUNADVAUX ADVADVADJPRONAUXPUNCTnsubjadvmodadvmodnsubjccomp\nadvmodmarkadvcl\ndetobl:npmodnmod:possnsubjadvmodcoppunct\nadvmodadvmodnsubjcop\nroot punct\ntheserviceisgreatandduringweekendsit to butthe[wait]DET VERBAUX NOUNNOUN PUNCTtendsgetbusy, isworthwhile.PARTADP ADJPRONNOUN DET AUXADJPUNCTADJCCONJ VERB CCONJdetnsubjcop nsubjoblcasecc conj auxmarkxcomp det copnsubjccpunctconjpunctroot\nFigure 7: Essential context identiﬁed by BERT along with POS tags and dependency trees. Words in brackets are\ntargets. Words underlined are essential.\nbased encoder in the presence of noisy input, a\nﬁnding also supported by Yin et al. (2020); Jin et al.\n(2019), as it will be harder to confuse the model\ncompletely given larger size of essential context\nwords set in comparison to BiLSTM.\nEssential words are close to target words in\nboth linear position and on dependency tree.\nTable 3 calculates the mean distances from\nidentiﬁed essential words to the target words\non combined EWT and GUM datasets. Both\nthe models tend to identify words much closer\nto the target as essential, whether we consider\nlinear positional distance or node distance in\ndependency trees. We use annotated dependency\nrelations to extract the traversal paths from each\nessential word to the target word in dependency\ntree. We ﬁnd that the top 10 most frequent\ndependency paths often correspond with the\ncommon syntactic structures in natural language.\nFor example, when target words are NOUN,\nthe top 3 paths are DET(up:det)⇒NOUN,\nADP(up:case)⇒NOUN, ADJ(up:amod)⇒\nNOUN for both models. Further, we also look at\nthe dependency paths of essential words which\nare unique to each model. The comparison shows\nthat words of common dependency paths are\nsometimes identiﬁed as essential by BERT but\nnot by BiLSTM and vice versa. This suggests\nthat there is room to improve MLMs by making\nthem consistently more aware of input’s syntactic\nstructures, possibly by incorporating dependency\nrelations into pre-training. The full lists of top\ndependency paths are presented in the Appendix.\nFigure 7 shows examples of essential words from\nBERT with POS tags and dependency relations.\nWords in square brackets are target words and the\nunderlined words are essential words. We observe\nthat words close to the target in the sentence as well\nas in the dependency tree are often seen as essential.\nWe can also see that BERT often includes the root\nof the dependency tree as an essential word.\n8 Application: Attention Pruning for\nTransformer\nAs a pilot application, we leverage insights from\nanalysis in previous sections to perform attention\npruning for Transformer. Transformer has achieved\nimpressive results in NLP and has been used for\nlong sequences with more than 10 thousand tokens\n(Liu et al., 2018). Self-attention for a sequence of\n3797\nModel Dev F1 Test F1\nBERT - Full 94.9(0.2) 90.8(0.1)\nBERT - Dynamic Pruning 94.7(0.2) 90.6(0.2)\nBERT - Static Pruning 94.5(0.2) 90.3(0.1)\nTable 4: CoNLL-2003 Named Entity Recognition re-\nsults (5 seeds). The attention pruning based on our ﬁnd-\nings gives comparable results to the original BERT.\nlength Lis of O(L2) complexity in computation\nand memory. Many works attempt to improve the\nefﬁciency of self-attention by restricting the num-\nber of tokens that each input query can attend to\n(Child et al., 2019; Kitaev et al., 2020).\nOur analysis in Section 6 shows that BERT has\neffective cws of around 78%. We perform a dy-\nnamic attention pruning by making self-attention\nneglect the furthest 22% of tokens. Due to the\nO(L2) complexity, this could save around 39% of\ncomputation in self-attention. We apply this lo-\ncality constraint to self-attention when ﬁne-tuning\nBERT on a downstream task. Speciﬁcally, we\nuse the CoNLL-2003 Named Entity Recognition\n(NER) dataset (Sang and Meulder, 2003) with200k\nwords for training. We ﬁne-tune BERT for NER\nin the same way as in Devlin et al. (2019). We\nalso explore a static attention pruning that restricts\nthe attention span to be within [−5,+5]2. Results\nin Table 4 show that BERT with attention prun-\ning has comparable performance to the original\nBERT, implying successful application of our anal-\nysis ﬁndings. Note that we use an uncased vocab-\nulary, which could explain the gap compared to\nDevlin et al. (2019).\n9 Conclusion\nIn our context analysis, we have shown that BERT\nhas an effective context size of around75% of input\nlength, while BiLSTM has about 30%. The differ-\nence in context usage is striking for long-range con-\ntext beyond 20 words. Our extensive analysis of\ncontext window size demonstrate that BERT uses\nmuch larger context window size than BiLSTM.\nBesides, both models often identify words with\ncommon syntactic structures as essential context.\nThese ﬁndings not only help to better understand\ncontextual impact in masked language models, but\nalso encourage model improvements in efﬁciency\nand effectiveness in future works. On top of that,\ndiving deep into the connection between our con-\n2 With average training set sentence length of14, this span\nequates to cws of 78%.\ntext analysis and a model’s robustness to noisy texts\nis also an interesting topic to explore.\nAcknowledgments\nThe authors would like to acknowledge the entire\nAWS Lex Science team for thoughtful discussions,\nhonest feedback, and full support. We are also very\ngrateful to the reviewers for insightful comments\nand helpful suggestions.\nReferences\nMarco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and\nMarkus Gross. 2017. Towards better understanding\nof gradient-based attribution methods for deep neu-\nral networks. arXiv preprint arXiv:1711.06104.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nYonatan Belinkov, Llu´ıs M`arquez i Villodre, Hassan\nSajjad, Nadir Durrani, Fahim Dalvi, and James R.\nGlass. 2017. Evaluating layers of representation in\nneural machine translation on part-of-speech and se-\nmantic tagging tasks. In IJCNLP.\nJordan L Boyd-Graber and David M Blei. 2009. Syn-\ntactic topic models. In Advances in neural informa-\ntion processing systems, pages 185–192.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers. ArXiv, abs/1904.10509.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does bert\nlook at? an analysis of bert’s attention. ArXiv,\nabs/1906.04341.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda B. Vi´egas, and Martin Wat-\ntenberg. 2019. Visualizing and measuring the geom-\netry of bert. In NeurIPS.\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single vector: Probing sentence\nembeddings for linguistic properties. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of bert, elmo, and gpt-2 embeddings. ArXiv,\nabs/1909.00512.\n3798\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In RepE-\nval@ACL.\nAgnieszka Falenska and Jonas Kuhn. 2019. The (non-\n)utility of structural features in bilstm-based depen-\ndency parsers. In ACL.\nZhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei\nLu. 2019. Densely connected graph convolutional\nnetworks for graph-to-sequence learning. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:297–312.\nJohn Hewitt and Percy Liang. 2019. Designing\nand interpreting probes with control tasks. In\nEMNLP/IJCNLP.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In NAACL-HLT.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and’diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. Journal of Artiﬁcial\nIntelligence Research, 61:907–926.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2019. Is bert really robust? a strong base-\nline for natural language attack on text classiﬁcation\nand entailment. arXiv: Computation and Language.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In ACL.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. ArXiv,\nabs/2001.04451.\nOlga V . Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. In EMNLP/IJCNLP.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. ArXiv, abs/1903.08855.\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and Noam\nShazeer. 2018. Generating wikipedia by summariz-\ning long sequences. ArXiv, abs/1801.10198.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nScott M Lundberg and Su-In Lee. 2017. A uniﬁed\napproach to interpreting model predictions. In Ad-\nvances in neural information processing systems ,\npages 4765–4774.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. ArXiv, abs/1802.05365.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. ” why should i trust you?” explain-\ning the predictions of any classiﬁer. In Proceed-\nings of the 22nd ACM SIGKDD international con-\nference on knowledge discovery and data mining ,\npages 1135–1144.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition.\nArXiv, cs.CL/0306050.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural mt learn source syntax? In\nEMNLP.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Christopher D. Manning. 2014. A\ngold standard dependency corpus for English. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC-\n2014).\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence struc-\nture in contextualized word representations. ArXiv,\nabs/1905.06316.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n3799\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In EMNLP/IJCNLP.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nTian Wang and Kyunghyun Cho. 2016. Larger-context\nlanguage modelling with recurrent neural network.\nIn ACL.\nShaowei Yao, Tianming Wang, and Xiaojun Wan.\n2020. Heterogeneous graph transformer for graph-\nto-sequence learning. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7145–7154.\nFan Yin, Quanyu Long, Tao Meng, and Kai-Wei\nChang. 2020. On the robustness of language en-\ncoders against grammatical errors. arXiv preprint\narXiv:2005.05683.\nAmir Zeldes. 2017. The GUM corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\n3800\nA Appendix\nB Context Window Size Analysis\nB.1 Masking Strategies for Context Window\nSize Analysis\nAs mentioned in Section 6, for analyzing how far\nmasked LMs look at within the available context,\nwe follow a masking strategy with locality con-\nstraints applied. The masking strategy is as follows\n- we start from no context available, i.e., all the\ncontext words masked and iteratively increase the\navailable context window size (cws) on both sides\nsimultaneously, till the entire context is available.\nThis procedure is also depicted in Figure 8. For\nsymmetricity analysis ofcws, we follow similar\nprocess as above but considering each side of the\ntarget word separately. Hence, when considering\ncontext words to the left, we iteratively increase the\ncws on the left of target word, keeping the rest of\nthe context words on the right unmasked as shown\nin Figure 9.\nIteration 1[ MASK] [ MASK ] [ MASK ] potent [ MASK ] [ MASK ]\nTarget Word\n[ MASK ]\nSentence It is a very potent psychological weapon\nIteration 2[ MASK] [ MASK ] [ MASK ] potent psychological[ MASK ]\nTarget Word\nvery\nIteration 3[ MASK] [ MASK ] a potent psychologicalweapon\nTarget Word\nvery\nIteration 4[ MASK] is a potent psychologicalweapon\nTarget Word\nvery\nIteration 5 It is a potent psychologicalweapon\nTarget Word\nvery\nFigure 8: Masking strategy for context window size\nanalysis\nB.2 Additional Plots for Symmetricity\nAnalysis of Context Window Size\nIn Figure 10, we show various plots investigating\nhow context around the target word impact’s model\nperformance as we look at left and right context\nseparately. Figures 10a, 10d, 10g, 10j, 10m show\nleft and right cws for sentences belonging to short\nlength category ( l ≤25). The trends show that,\nwhere NOUN, ADJ, VERB leverage somewhat\nsymmetric context windows, DET and ADP show\nasymmetric behavior relying more heavily on right\ncontext words for both the models - BERT and\nBiLSTM. Similar observations can be made for\nsentences belonging to medium length bucket (l>\n25 and l≤ 50) with ADP being an exception\nwhere BiLSTM shows more symmetric context\ndifferent than BERT, as shown in Figures 10b, 10e,\n10h, 10k, 10n. However, for sentences belonging to\nIteration 1[ MASK] [ MASK ] [ MASK ] potent\nTarget Word\n[ MASK ]\nSentence It is a very potent psychological weapon\nIteration 2[ MASK] [ MASK ] [ MASK ] potent\nTarget Word\nvery\nIteration 3[ MASK] [ MASK ] a potent psychologicalweapon\nTarget Word\nvery\nIteration 4[ MASK] is a potent psychologicalweapon\nTarget Word\nvery\nIteration 5 It is a potent psychologicalweapon\nTarget Word\nvery\npsychologicalweapon\npsychologicalweapon\nFigure 9: Masking strategy for symmetricity analysis\nof cws on the left\nlong length bucket (l> 50), left and right context\nwindow sizes are leveraged quite differently.\nWe can also see that BiLSTM leverages almost\nsimilar number of context words as we moved on to\nbuckets of longer sentence lengths in comparison\nto BERT which can leverage more context when\nits available. This is aligned with our observation\nfrom Section 5.\nC Dependency Paths from Essential\nWords to Target Words\nGiven a target word, BERT or BiLSTM identiﬁes\na subset of context words as essential. Based on\nthe dependency relations provided in the datasets,\nwe extract the dependency paths starting from each\nessential word to the target words, i.e., the path to\ntraverse from an essential word to the given target\nword in the dependency tree. We summarize the top\n10 most frequent dependency paths recognized by\nBERT or BiLSTM given the target words being a\nspeciﬁc part-of-speech category. Table 5, 6, 7, 8, 9\nshow the results for NOUN, ADJ, VERB, DET, and\nADP, respectively. Theup and down denote the di-\nrection of traversal, followed by the corresponding\nrelations in the dependency tree. We can see that\nthe top dependency paths for BERT and BiLSTM\nare largely overlapped with each other. We also\nobserve that these most frequent dependency paths\nare often aligned with common syntactic patterns.\nFor example, the top 3 paths for NOUN are DET\n=(up:det)⇒ NOUN that could be “the” cat ,\nADP =(up:case)⇒ NOUN that could be “at”\nhome, and ADJ =(up:amod)⇒ NOUN which\ncould be “white” car. This implies that both mod-\nels could be aware of the common syntactic struc-\ntures in the natural language.\nTo further compare the behaviors of BERT and\nBiLSTM when identifying essential context, we\ncount the occurrence of dependency paths based on\nthe disjoint essential words. That is, given an input\nsentence, we only count the dependency paths of\n3801\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nNOUN - BERT\nNOUN - BiLSTM\n(a) BERT looking at context window size\n[-16, 16]; biLSTM looking at context\nwindow size [-7, 7]\n50\n 45\n 40\n 35\n 30\n 25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25 30 35 40 45\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nNOUN - BERT\nNOUN - BiLSTM\n(b) BERT looking at context window size\n[-29, 32]; biLSTM looking at context\nwindow size [-12, 12]\n160\n 140\n 120\n 100\n 80\n 60\n 40\n 20\n 00 20 40 60 80 100 120 140 160\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nNOUN - BERT\nNOUN - BiLSTM\n(c) BERT looking at context window size\n[-135, 72]; biLSTM looking at context\nwindow size [-19, 5]\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADJ - BERT\nADJ - BiLSTM\n(d) BERT looking at context window size\n[-15, 16]; biLSTM looking at context\nwindow size [-5, 5]\n50\n 45\n 40\n 35\n 30\n 25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25 30 35 40 45\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADJ - BERT\nADJ - BiLSTM\n(e) BERT looking at context window size\n[-28, 30]; biLSTM looking at context\nwindow size [-7, 6]\n160\n 140\n 120\n 100\n 80\n 60\n 40\n 20\n 00 20 40 60 80 100 120 140 160\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADJ - BERT\nADJ - BiLSTM\n(f) BERT looking at context window size\n[-54, 77]; biLSTM looking at context\nwindow size [-6, 4]\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nVERB - BERT\nVERB - BiLSTM\n(g) BERT looking at context window size\n[-14, 16]; biLSTM looking at context\nwindow size [-7, 6]\n50\n 45\n 40\n 35\n 30\n 25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25 30 35 40 45\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nVERB - BERT\nVERB - BiLSTM\n(h) BERT looking at context window size\n[-28, 30]; biLSTM looking at context\nwindow size [-9, 8]\n160\n 140\n 120\n 100\n 80\n 60\n 40\n 20\n 00 20 40 60 80 100 120 140 160\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nVERB - BERT\nVERB - BiLSTM\n(i) BERT looking at context window size\n[-102, 148]; biLSTM looking at context\nwindow size [-10, 9]\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nDET - BERT\nDET - BiLSTM\n(j) BERT looking at context window size\n[-14, 18]; biLSTM looking at context\nwindow size [-1, 3]\n50\n 45\n 40\n 35\n 30\n 25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25 30 35 40 45\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nDET - BERT\nDET - BiLSTM\n(k) BERT looking at context window size\n[-25, 31]; biLSTM looking at context\nwindow size [-1, 2]\n160\n 140\n 120\n 100\n 80\n 60\n 40\n 20\n 00 20 40 60 80 100 120 140 160\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nDET - BERT\nDET - BiLSTM\n(l) BERT looking at context window size\n[-50, 75]; biLSTM looking at context\nwindow size [-2, 2]\n25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADP - BERT\nADP - BiLSTM\n(m) BERT looking at context window\nsize [-13, 16]; biLSTM looking at context\nwindow size [-2, 3]\n50\n 45\n 40\n 35\n 30\n 25\n 20\n 15\n 10\n 5\n 00 5 10 15 20 25 30 35 40 45\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADP - BERT\nADP - BiLSTM\n(n) BERT looking at context window size\n[-25, 30]; biLSTM looking at context\nwindow size [-3, 3]\n160\n 140\n 120\n 100\n 80\n 60\n 40\n 20\n 00 20 40 60 80 100 120 140 160\nAccess to |x| context window (in #words) on corresponding side\n0\n2\n4\n6\n8\n10Increase in NLL\nADP - BERT\nADP - BiLSTM\n(o) BERT looking at context window size\n[-99, 113]; biLSTM looking at context\nwindow size [-3, 3]\nFigure 10: Symmetricity analysis of context window size for different syntactic categories of target word belonging\nto sentences from buckets of different lengths; along the rows, we consider sentences of different lengths for a given\nsyntactic category: (a) - (c)analysis for NOUN; (d) - (f)analysis for ADJ; (g) - (i)analysis for VERB; (j) - (l)\nanalysis for DET; (m) - (o)analysis for ADP; along the columns, we consider different syntactic categories for\ngiven bucket ranging from short (ﬁrst column), medium (second column) to long (third column)\n3802\nBERT BiLSTM\nDET=(up:det)⇒NOUN DET=(up:det)⇒NOUN\nADP=(up:case)⇒NOUN ADP=(up:case)⇒NOUN\nADJ=(up:amod)⇒NOUN ADJ=(up:amod)⇒NOUN\nVERB=(down:obj)⇒NOUN VERB=(down:obj)⇒NOUN\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUNVERB=(down:obl)⇒NOUN\nNOUN=(down:compound)⇒NOUN ADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN\nNOUN=(up:compound)⇒NOUN NOUN=(up:nmod)⇒NOUN\nNOUN=(up:nmod)⇒NOUN NOUN=(down:nmod)⇒NOUN\nNOUN=(down:nmod)⇒NOUN NOUN=(down:compound)⇒NOUN\nVERB=(down:obl)⇒NOUN NOUN=(up:compound)⇒NOUN\nTable 5: Top 10 most frequent dependency paths when the target words are NOUN.\nBERT BiLSTM\nNOUN=(down:amod)⇒ADJ NOUN=(down:amod)⇒ADJ\nDET=(up:det)⇒NOUN=(down:amod)⇒ADJ DET=(up:det)⇒NOUN=(down:amod)⇒ADJ\nADP=(up:case)⇒NOUN=(down:amod)⇒ADJ ADP=(up:case)⇒NOUN=(down:amod)⇒ADJ\nAUX=(up:cop)⇒ADJ AUX=(up:cop)⇒ADJ\nVERB=(down:obj)⇒NOUN=(down:amod)⇒ADJ ADV=(up:advmod)⇒ADJ\nADV=(up:advmod)⇒ADJ VERB=(down:obj)⇒NOUN=(down:amod)⇒ADJ\nADJ=(up:amod)⇒NOUN=(down:amod)⇒ADJ ADJ=(up:amod)⇒NOUN=(down:amod)⇒ADJ\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:amod)⇒ADJ PUNCT=(up:punct)⇒ADJ\nPUNCT=(up:punct)⇒NOUN=(down:amod)⇒ADJ VERB=(down:obl)⇒NOUN=(down:amod)⇒ADJ\nPUNCT=(up:punct)⇒ADJ PUNCT=(up:punct)⇒NOUN=(down:amod)⇒ADJ\nTable 6: Top 10 most frequent dependency paths when the target words are ADJ.\nBERT BiLSTM\nPRON=(up:nsubj)⇒VERB PRON=(up:nsubj)⇒VERB\nNOUN=(up:obj)⇒VERB NOUN=(up:obj)⇒VERB\nPUNCT=(up:punct)⇒VERB PUNCT=(up:punct)⇒VERB\nAUX=(up:aux)⇒VERB AUX=(up:aux)⇒VERB\nADV=(up:advmod)⇒VERB ADV=(up:advmod)⇒VERB\nADP=(up:case)⇒NOUN=(up:obl)⇒VERBADP=(up:case)⇒NOUN=(up:obl)⇒VERB\nNOUN=(up:obl)⇒VERB NOUN=(up:obl)⇒VERB\nPART=(up:mark)⇒VERB PART=(up:mark)⇒VERB\nDET=(up:det)⇒NOUN=(up:obj)⇒VERB DET=(up:det)⇒NOUN=(up:obj)⇒VERB\nSCONJ=(up:mark)⇒VERB SCONJ=(up:mark)⇒VERB\nTable 7: Top 10 most frequent dependency paths when the target words are VERB.\nBERT BiLSTM\nNOUN=(down:det)⇒DET NOUN=(down:det)⇒DET\nADP=(up:case)⇒NOUN=(down:det)⇒DET ADP=(up:case)⇒NOUN=(down:det)⇒DET\nADJ=(up:amod)⇒NOUN=(down:det)⇒DET ADJ=(up:amod)⇒NOUN=(down:det)⇒DET\nVERB=(down:obj)⇒NOUN=(down:det)⇒DET VERB=(down:obj)⇒NOUN=(down:det)⇒DET\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET ADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET\nVERB=(down:obl)⇒NOUN=(down:det)⇒DET VERB=(down:obl)⇒NOUN=(down:det)⇒DET\nNOUN=(up:compound)⇒NOUN=(down:det)⇒DET NOUN=(down:nmod)⇒NOUN=(down:det)⇒DET\nPROPN=(down:det)⇒DET NOUN=(up:compound)⇒NOUN=(down:det)⇒DET\nNOUN=(down:nmod)⇒NOUN=(down:det)⇒DET PROPN=(down:det)⇒DET\nNOUN=(up:nmod)⇒NOUN=(down:det)⇒DET NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET\nTable 8: Top 10 most frequent dependency paths when the target words are DET.\nBERT BiLSTM\nNOUN=(down:case)⇒ADP NOUN=(down:case)⇒ADP\nDET=(up:det)⇒NOUN=(down:case)⇒ADP DET=(up:det)⇒NOUN=(down:case)⇒ADP\nVERB=(down:obl)⇒NOUN=(down:case)⇒ADP VERB=(down:obl)⇒NOUN=(down:case)⇒ADP\nNOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP NOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP\nPROPN=(down:case)⇒ADP PROPN=(down:case)⇒ADP\nADJ=(up:amod)⇒NOUN=(down:case)⇒ADP ADJ=(up:amod)⇒NOUN=(down:case)⇒ADP\nDET=(up:det)⇒NOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP DET=(up:det)⇒NOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP\nPUNCT=(up:punct)⇒VERB=(down:obl)⇒NOUN=(down:case)⇒ADPPRON=(up:nmod:poss)⇒NOUN=(down:case)⇒ADP\nNOUN=(down:nmod)⇒PROPN=(down:case)⇒ADP NOUN=(down:nmod)⇒PROPN=(down:case)⇒ADP\nPRON=(up:nmod:poss)⇒NOUN=(down:case)⇒ADP PRON=(down:case)⇒ADP\nTable 9: Top 10 most frequent dependency paths when the target words are ADP.\n3803\nessential words which are unique to each model,\ne.g., words essential to BERT but not essential to\nBiLSTM. Our goal is to see for these essential\nwords unique to a model, whether some special\ndependency paths are captured by the model. Ta-\nble 10, 11, 12, 13, 14 show the results for NOUN,\nADJ, VERB, DET, and ADP, respectively. We\nobserve that around top 5 dependency paths for\nessential words unique to BERT or BiLSTM are\nmostly overlapping with each other as well as the\nresults in Table 5, 6, 7, 8, 9. This implies that\nsometimes words of common dependency paths\ncan be identiﬁed by BERT as essential while BiL-\nSTM fails to do so and sometimes it’s another way\naround. In other words, there is a room to make\nmodels to be more consistently aware of syntactic\nstructures of an input. The observation suggests\nthat explicitly incorporating dependency relations\ninto pre-training could potentially beneﬁt masked\nlanguage models.\n3804\nBERT BiLSTM\nDET=(up:det)⇒NOUN ADP=(up:case)⇒NOUN\nADP=(up:case)⇒NOUN DET=(up:det)⇒NOUN\nADJ=(up:amod)⇒NOUN VERB=(down:obl)⇒NOUN\nPUNCT=(up:punct)⇒NOUN VERB=(down:obj)⇒NOUN\nVERB=(down:obl)⇒NOUN PUNCT=(up:punct)⇒NOUN\nVERB=(down:obj)⇒NOUN NOUN=(up:nmod)⇒NOUN\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUNPUNCT=(up:punct)⇒VERB=(down:obj)⇒NOUN\nNOUN=(up:nmod)⇒NOUN NOUN=(down:nmod)⇒NOUN\nNOUN=(down:nmod)⇒NOUN PUNCT=(up:punct)⇒VERB=(down:obl)⇒NOUN\nNOUN=(up:compound)⇒NOUN PRON=(up:nsubj)⇒VERB=(down:obj)⇒NOUN\nTable 10: Top10 dependency paths from essential words unique to each model to the target words that are NOUN.\nBERT BiLSTM\nNOUN=(down:amod)⇒ADJ NOUN=(down:amod)⇒ADJ\nDET=(up:det)⇒NOUN=(down:amod)⇒ADJ PUNCT=(up:punct)⇒ADJ\nADP=(up:case)⇒NOUN=(down:amod)⇒ADJ ADP=(up:case)⇒NOUN=(down:amod)⇒ADJ\nVERB=(down:obj)⇒NOUN=(down:amod)⇒ADJ VERB=(down:obl)⇒NOUN=(down:amod)⇒ADJ\nPUNCT=(up:punct)⇒NOUN=(down:amod)⇒ADJ PUNCT=(up:punct)⇒NOUN=(down:amod)⇒ADJ\nAUX=(up:cop)⇒ADJ NOUN=(up:nmod)⇒NOUN=(down:amod)⇒ADJ\nADJ=(up:amod)⇒NOUN=(down:amod)⇒ADJ DET=(up:det)⇒NOUN=(down:amod)⇒ADJ\nPUNCT=(up:punct)⇒ADJ VERB=(down:obj)⇒NOUN=(down:amod)⇒ADJ\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:amod)⇒ADJPUNCT=(up:punct)⇒VERB=(down:obj)⇒NOUN=(down:amod)⇒ADJ\nVERB=(down:obl)⇒NOUN=(down:amod)⇒ADJ PRON=(up:nsubj)⇒ADJ\nTable 11: Top 10 dependency paths from essential words unique to each model to the target words that are ADJ.\nBERT BiLSTM\nPUNCT=(up:punct)⇒VERB PUNCT=(up:punct)⇒VERB\nADP=(up:case)⇒NOUN=(up:obl)⇒VERBNOUN=(up:obl)⇒VERB\nNOUN=(up:obj)⇒VERB NOUN=(up:obj)⇒VERB\nNOUN=(up:obl)⇒VERB PRON=(up:nsubj)⇒VERB\nDET=(up:det)⇒NOUN=(up:obj)⇒VERB DET=(up:det)⇒NOUN=(up:obl)⇒VERB\nPRON=(up:nsubj)⇒VERB VERB=(up:advcl)⇒VERB\nADV=(up:advmod)⇒VERB ADP=(up:case)⇒NOUN=(up:obl)⇒VERB\nNOUN=(up:nsubj)⇒VERB VERB=(down:advcl)⇒VERB\nCCONJ=(up:cc)⇒VERB VERB=(up:conj)⇒VERB\nSCONJ=(up:mark)⇒VERB VERB=(down:conj)⇒VERB\nTable 12: Top 10 dependency paths from essential words unique to each model to the target words that are VERB.\nBERT BiLSTM\nNOUN=(down:det)⇒DET NOUN=(down:det)⇒DET\nADP=(up:case)⇒NOUN=(down:det)⇒DET VERB=(down:obl)⇒NOUN=(down:det)⇒DET\nVERB=(down:obj)⇒NOUN=(down:det)⇒DET NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET\nNOUN=(up:nmod)⇒NOUN=(down:det)⇒DET NOUN=(down:nmod)⇒NOUN=(down:det)⇒DET\nVERB=(down:obl)⇒NOUN=(down:det)⇒DET PUNCT=(up:punct)⇒VERB=(down:obl)⇒NOUN=(down:det)⇒DET\nADJ=(up:amod)⇒NOUN=(down:det)⇒DET PUNCT=(up:punct)⇒NOUN=(down:det)⇒DET\nADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET ADP=(up:case)⇒NOUN=(down:det)⇒DET\nPRON=(up:nsubj)⇒VERB=(down:obj)⇒NOUN=(down:det)⇒DETADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:det)⇒DET\nNOUN=(up:compound)⇒NOUN=(down:det)⇒DET VERB=(down:obj)⇒NOUN=(down:det)⇒DET\nDET=(up:det)⇒NOUN=(down:nmod)⇒NOUN=(down:det)⇒DET PUNCT=(up:punct)⇒VERB=(down:obj)⇒NOUN=(down:det)⇒DET\nTable 13: Top 10 dependency paths from essential words unique to each model to the target words that are DET.\nBERT BiLSTM\nNOUN=(down:case)⇒ADP NOUN=(down:case)⇒ADPDET=(up:det)⇒NOUN=(down:case)⇒ADP VERB=(down:obl)⇒NOUN=(down:case)⇒ADPVERB=(down:obl)⇒NOUN=(down:case)⇒ADP PUNCT=(up:punct)⇒VERB=(down:obl)⇒NOUN=(down:case)⇒ADPADJ=(up:amod)⇒NOUN=(down:case)⇒ADP DET=(up:det)⇒NOUN=(down:case)⇒ADPPUNCT=(up:punct)⇒VERB=(down:obl)⇒NOUN=(down:case)⇒ADPADJ=(up:amod)⇒NOUN=(down:case)⇒ADPPROPN=(down:case)⇒ADP AUX=(up:aux)⇒VERB=(down:obl)⇒NOUN=(down:case)⇒ADPNOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP PROPN=(down:case)⇒ADPDET=(up:det)⇒NOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP PRON=(up:nsubj)⇒VERB=(down:obl)⇒NOUN=(down:case)⇒ADPADP=(up:case)⇒NOUN=(down:nmod)⇒NOUN=(down:case)⇒ADP NOUN=(up:nmod)⇒NOUN=(down:case)⇒ADPNOUN=(up:compound)⇒NOUN=(down:case)⇒ADP ADP=(up:case)⇒NOUN=(up:nmod)⇒NOUN=(down:case)⇒ADP\nTable 14: Top 10 dependency paths from essential words unique to each model to the target words that are ADP.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8014184236526489
    },
    {
      "name": "Transformer",
      "score": 0.7679198980331421
    },
    {
      "name": "Natural language processing",
      "score": 0.6552774310112
    },
    {
      "name": "Language model",
      "score": 0.6341028809547424
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6014829277992249
    },
    {
      "name": "Intuition",
      "score": 0.5683943033218384
    },
    {
      "name": "Cognitive science",
      "score": 0.11776694655418396
    },
    {
      "name": "Psychology",
      "score": 0.09880143404006958
    },
    {
      "name": "Engineering",
      "score": 0.07415452599525452
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}