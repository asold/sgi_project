{
  "title": "Reusability report: Learning the transcriptional grammar in single-cell RNA-sequencing data using transformers",
  "url": "https://openalex.org/W4388721931",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5035017747",
      "name": "Sameer Khan",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5054754981",
      "name": "Alberto Maillo",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016518691",
      "name": "Vincenzo Lagani",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5027667039",
      "name": "Robert Lehmann",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5055536778",
      "name": "Narsis A. Kiani",
      "affiliations": [
        "Karolinska Institutet"
      ]
    },
    {
      "id": "https://openalex.org/A5078466508",
      "name": "David Gómez-Cabrero",
      "affiliations": [
        "King Abdullah University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5035252549",
      "name": "Jesper Tegnér",
      "affiliations": [
        "Karolinska Institutet",
        "Karolinska University Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4212836561",
    "https://openalex.org/W2892221324",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4309693330",
    "https://openalex.org/W2952631022",
    "https://openalex.org/W2931036699",
    "https://openalex.org/W2951506174",
    "https://openalex.org/W2896518632",
    "https://openalex.org/W2800392236",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W6913024435"
  ],
  "abstract": "Abstract The rise of single-cell genomics is an attractive opportunity for data-hungry machine learning algorithms. The scBERT method, inspired by the success of BERT (‘bidirectional encoder representations from transformers’) in natural language processing, was recently introduced by Yang et al. as a data-driven tool to annotate cell types in single-cell genomics data. Analogous to contextual embedding in BERT, scBERT leverages pretraining and self-attention mechanisms to learn the ‘transcriptional grammar’ of cells. Here we investigate the reusability beyond the original datasets, assessing the generalizability of natural language techniques in single-cell genomics. The degree of imbalance in the cell-type distribution substantially influences the performance of scBERT. Anticipating an increased utilization of transformers, we highlight the necessity to consider data distribution carefully and introduce a subsampling technique to mitigate the influence of an imbalanced distribution. Our analysis serves as a stepping stone towards understanding and optimizing the use of transformers in single-cell genomics.",
  "full_text": "Nature Machine Intelligence | Volume 5 | December 2023 | 1437–1446\n 1437\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-023-00757-8\nArticle\nReusability report: Learning the \ntranscriptional grammar in single-cell \nRNA-sequencing data using transformers\nSumeer Ahmad Khan1,2,9, Alberto Maillo1,9, Vincenzo Lagani    1,2,3, \nRobert Lehmann    1, Narsis A. Kiani    4,5, David Gomez-Cabrero1,6 & \nJesper Tegner    1,5,7,8 \nThe rise of single-cell genomics is an attractive opportunity for data-hungry \nmachine learning algorithms. The scBERT method, inspired by the success \nof BERT (‘bidirectional encoder representations from transformers’) \nin natural language processing, was recently introduced by Yang \net al. as a data-driven tool to annotate cell types in single-cell genomics \ndata. Analogous to contextual embedding in BERT, scBERT leverages \npretraining and self-attention mechanisms to learn the ‘transcriptional \ngrammar’ of cells. Here we investigate the reusability beyond the original \ndatasets, assessing the generalizability of natural language techniques in \nsingle-cell genomics. The degree of imbalance in the cell-type distribution \nsubstantially influences the performance of scBERT. Anticipating an \nincreased utilization of transformers, we highlight the necessity to consider \ndata distribution carefully and introduce a subsampling technique to \nmitigate the influence of an imbalanced distribution. Our analysis serves \nas a stepping stone towards understanding and optimizing the use of \ntransformers in single-cell genomics.\nConvolutional neural networks (CNNs), generative adversarial net-\nworks (GANs), variational autoencoders (VAEs) and graph neural net-\nworks (GNNs) have been successful in addressing various data analyses \nin biomedicine, and genomics in particular. Single-cell genomics data, \ncomprising data from thousands of individual cells, have particularly \nbenefited from the use of neural networks 1,2. However, one critical \nopen problem in single-cell data analysis is labelling and discover -\ning individual cell types, which is a non-local problem as the cellular \ncontext is important. Yang and colleagues3 have recently proposed \nthe application of ‘transformers’ , a state-of-the-art natural language \nneural-network architecture, for cell-type annotation. By ‘drawing \nparallels between natural language processing and genomics’ and \n‘utilizing self-attention mechanisms’4, transformers can effectively \ncapture long-range dependencies within single-cell genomics data. The \nscBERT method proposed by Fan Yang et al. can annotate cell types, \ndetect novel cell types and is robust to batch effects.\nIn this Article we assess the reusability of scBERT, reported by \nYang and colleagues 3. Although we could largely reproduce their \nresults, our findings indicate that cell-type distribution plays a more \nimportant role than initially reported. T o validate this observation, we \nReceived: 24 January 2023\nAccepted: 12 October 2023\nPublished online: 16 November 2023\n Check for updates\n1Biological and Environmental Science and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi \nArabia. 2SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence, Thuwal, Saudi Arabia. 3Institute of Chemical Biology, Ilia State \nUniversity, Tbilisi, Georgia. 4Algorithmic Dynamic Lab, Department of Oncology and Pathology, Karolinska Institute, Stockholm, Sweden. 5Unit of \nComputational Medicine, Department of Medicine, Center for Molecular Medicine, Karolinska Institutet, Karolinska University Hospital, Stockholm, \nSweden. 6Translational Bioinformatics Unit, Navarrabiomed, Universidad Pública de Navarra (UPNA), IdiSNA, Pamplona, Spain. 7Computer, Electrical and \nMathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. 8Science for Life \nLaboratory, Solna, Sweden. 9These authors contributed equally: Sumeer Ahmad Khan, Alberto Maillo.  e-mail: jesper.tegner@kaust.edu.sa\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446 1438\nArticle https://doi.org/10.1038/s42256-023-00757-8\nas described in the following (Extended Data Table 1 and Extended \nData Fig. 1).\nReusability\nT o assess the applicability and generalizability of scBERT across new \ndatasets, we investigated its performance on a novel dataset—the Neu-\nrIPS dataset—which is a compilation of single-cell multi-omics data \ncollected from mobilized peripheral CD34+ haematopoietic stem and \nprogenitor cells (HSPCs) for cell-type annotation and the detection of \nnovel cell types tasks.\nChallenging the scBERT using an additional \ndataset, NeurIPS\nT o this end, we explored the application of scBERT on the NeurIPS data-\nset from the 2022 Kaggle competition, which is accessible via https://\nwww.kaggle.com/competitions/open-problems-multimodal/data, \nfor a cell-type annotation task for predicting seven different cell types \n(GitHub: scbert-reusability). The data comprise single-cell multi-omics \ndata from mobilized peripheral CD34+ cells (HSPCs) collected from \nfour healthy human donors. This dataset was generated using the 10X \nChromium Single Cell Multiome ATAC + Gene Expression technology \n(Multiome), which allows for the simultaneous measurement of gene \nexpression (RNA) and chromatin accessibility (ATAC) in single cells. \nAccordingly, we used gene expression (RNA) from this multi-omics data \nfor our experimentation. Specifically, we focused on the gene expres-\nsion (RNA) data from this multi-omics dataset, which encompassed \nseven distinct cell types, namely B-cell progenitor (BP, n_cells = 262), \nerythrocyte progenitor (EryP, n_cells = 3,402), haematopoietic stem \ncell (HSC, n_cells = 10,757), mast cell progenitor (MasP, n_cells = 2,175), \nmegakaryocyte progenitor (MkP, n_cells = 3,394), monocyte progenitor \n(MoP, n_cells = 258) and neutrophil progenitor (NeuP, n_cells = 3,663).\nAssessing interclass similarity among cell types in \nthe NeurIPS data\nUnderstanding the similarity among the cell types is key to discerning \nthe nuances of cell-type annotation and the detection of novel cell \ntypes. We visualized the different cell types in low two-dimensional \nuniform manifold approximation and projection (UMAP) plots (Fig. 1a). \nWe also conducted a correlation analysis between cell types to assess \nscBERT’s robustness in the NeurIPS dataset for cell-type annotation \nand detection of novel cell types tasks (Fig. 1b), providing both qualita-\ntive and quantitative perspectives on the similarity among cell types. \nThese findings indicated a substantial correlation between cell types. \nFurthermore, the UMAP plots show that individual clusters correspond-\ning to each cell type are not adequately distinct from one another, as \ndepicted in Fig. 1a. However, it is essential to acknowledge that the \npresence of high interclass similarity in the NeurIPS dataset does not \ndiminish the overall capabilities of scBERT for cell-type annotation and \nnovel cell-type detection, as shown in Fig. 2b,c. scBERT demonstrates \na robust performance when applied to datasets with diverse and less \nhomogeneous cell populations.\nscBERT shows good performance with NeurIPS \ndata for predicting cell types\nThe dataset of seven cell types was divided into two subsets, with 70% \nof the data allocated for training and 30% for testing. An additional \nsplit was performed on the training subset to further refine the model’s \nperformance, with 80% of the data utilized for model training and the \nremaining 20% for validation. We observed that scBERT performed \nbetter on this new dataset, with a validation mean accuracy value \nof 0.8510, compared with Seurat, which achieved a validation mean \naccuracy of 0.8013 (Fig. 3a). We used Seurat for comparison because \nSeurat showed the next best performance after scBERT for the cell-type \nannotation task. However, scBERT showed a slight decrease in mean \naccuracy value, 0.8397 (Table 1), although this was still better than \nevaluated scBERT on a new dataset. We found that scBERT performs \nwell in cell-type annotation tasks and performs similarly in detecting \nnew cell types, as shown on the datasets reported in the paper. How -\never, our results indicate that cell-type distribution influences the \nperformance of scBERT in annotation and novel cell-type detection \ntasks on the new dataset. In summary, we anticipate that the use of \ntransformers will be expanded beyond cell-type annotation to include \na variety of downstream tasks such as perturbation response predic-\ntion, multimodal integration and gene function analysis. On a broader \nnote, our analysis suggests that, when engaging in downstream analysis \nusing transformers, imbalanced data distributions remain an ongoing \nchallenge and should be carefully addressed in each case.\nDescription of the method and the original \nexperimental validation of scBERT\nThe scBERT model utilizes the advanced capabilities of the BERT \nmodel, which has demonstrated notable performance in natural \nlanguage-processing tasks. The authors of scBERT have adapted the \nBERT model for single-cell RNA sequencing (scRNA-seq) data by creat-\ning gene embeddings through gene2vec5, which encodes gene embed-\ndings within a predefined vector space to capture semantic similarities \nbetween genes. Additionally, the scBERT model incorporates expres-\nsion embeddings generated through term-frequency analysis to discre-\ntize continuous expression variables by binning and converting them \ninto 200-dimensional vectors. These embeddings are utilized as token \nembeddings within the scBERT model, allowing for consideration of \ntranscription levels of individual genes.\nThe scBERT model is first pretrained (self-supervised learning) \non large amounts of unlabelled scRNA-seq data, providing a general \nunderstanding of gene interactions. A fine-tuning process on unseen \nand user-specific scRNA-seq data for supervised cell-type annota -\ntion tasks follows this. In self-supervised learning, unlabelled data \nwere obtained from PanglaoDB6 and utilized in the fine-tuning stage \nwith task-specific data. During the self-supervised pretraining phase, \nmasked expression and gene embeddings are integrated as input and \nfed into the performer blocks. A reconstructor is employed to generate \noutputs, with the reconstruction loss being calculated based on the \noutput for masked genes. In the subsequent supervised fine-tuning \nstage, task-specific scRNA-seq data are input into the previously pre-\ntrained encoder.\nThe performance of scBERT was initially evaluated in comparison \nto other methods using seven scRNA-seq datasets comprising a com-\nprehensive representation of 17 major organ/tissue systems, 50 cellular \nsubtypes, over 500,000 cells and a variety of mainstream single-cell \nomics technologies (Drop-seq, 10X, SMART-seq and Sanger-Nuclei). \nOverall, the benchmark evaluation in ref. 3 comprehensively consid-\nered diversity in data size and complexity.\nReproducibility\nUsing the source code of scBERT from https://github.com/T encentAIL-\nabHealthcare/scBERT (git commit 8ac7c1e), we repeated the analyses \ndescribed in ref. 3 on the two datasets for which scBERT showed the best \nresults, that is, Zheng68k7 and MacParland8. The first is a peripheral \nblood mononuclear cells (PBMC) dataset, widely used for cell-type \nannotation performance assessment, and the MacParland dataset \nprofiles 8,444 cells from the human liver (belonging to 20 different \nhepatic cell populations). The Zheng68k data were available in a pre-\nprocessed format (https://github.com/T encentAILabHealthcare/\nscBERT), whereas the MacParland data were available as a raw count \nmatrix. We used standard preprocessing steps from scanpy9 that the \nauthors have reported in the https://github.com/T encentAILabHealth-\ncare/scBERT repository, that is, (filter, normalize and log1p) to make \nthe MacParland format suitable for model training. We replicated the \nmajority of the reported results. However, to fully assess the utility and \nefficacy of scBERT, we evaluated its performance on novel datasets, \nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446\n 1439\nArticle https://doi.org/10.1038/s42256-023-00757-8\nSeurat, which achieved a mean accuracy value of 0.8160 and F1 score \nof 0.6395 (Fig. 2b) when applied to the 30% test data. Compared to \nSeurat, scBERT demonstrated improved performance, as reflected in \nthe mean accuracy values. The P value obtained from a paired t-test \nwas 0.0004, indicating that the performance improvement of scBERT \nover Seurat is statistically significant. These results demonstrate the \npotential utility of using pretrained language models such as scBERT \nfor cell-type annotation tasks. Leveraging the pretrained knowledge \nembedded in these models can improve their performance compared to \nmodels trained from scratch. This is consistent with findings reported \nin the original scBERT paper, where the authors conducted an ablation \nstudy. The study demonstrated the value of pretraining in enhancing \nthe model’s downstream performance on cell-type annotation tasks.\nscBERT can detect only part of the novel cell types \nwithin NeurIPS data\nT o evaluate the ability of scBERT to detect novel cell types, we per-\nformed leave-one-out experiments in which scBERT was trained on all \nbut one cell type and then evaluated on its ability to identify the held-out \ncell type as a novel type. T o this end, we followed the same steps as \nreported in the original paper of applying a threshold of probability of \n<0.5 such that cells with a value less than 0.5 be treated as unassigned \nor novel cell types. We observed that scBERT was only able to detect \nthe neutrophil progenitor (NeuP) cell type as a novel cell type, and it \nstill did not perform well on detecting other cell types as novel cell \ntypes (Fig. 2c), with a mean accuracy score of 0.087. It is worth noting \nthat continued work is needed to develop scBERT representations and \ntraining procedures using scRNA-seq data to increase the efficacy of \ndetecting novel cell types.\nSubsampling improves scBERT’s performance in \ncell type annotation by balancing the cell-type \ndistribution\nAs shown in Figs. 2c and 3b,c, scBERT showed poor performance on \ncell types with fewer cells (that is, with an imbalance in the number \nof cells). We were thus curious to see how the distribution of cells \nby cell type affected cell-type annotation. The distribution of cells \nper cell type is shown in Fig. 2a. In the NeurIPS dataset, we observed \nthat the BP and MoP cells comprised 262 and 258 cells, respectively. \nWe thus subsampled other cell types at 300 cells per type, resulting \nUMAP1\nUMAP2\nBP\nEryP\nHSC\nMasP\nMkP\nMoP\nNeuP\nGene expression\na\nb BP EryP HSC MasP MkP MoP NeuP\nBP –\nEryP 0.91 –\nHSC 0.97 0.96 –\nMasP 0.92 0.99 0.97 –\nMkP 0.95 0.98 0.99 0.98 –\nMoP 0.95 0.97 0.98 0.97 0.98 –\nNeuP 0.91 0.96 0.95 0.98 0.96 0.96 –\nFig. 1 | NeurIPS dataset. a, UMAP plot of single-cell expression data, coloured by cell types (ground truth), for the NeurIPS dataset. b, Spearman correlation matrix of \nsingle-cell expression data between cell types in NeurIPS.\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446 1440\nArticle https://doi.org/10.1038/s42256-023-00757-8\n0\n3,000\n6,000\n9,000\nBP Er yP HSC MasP MkP MoP NeuP\nNumber of cells\nOr iginala\nb\nc\nd\n0\n3 000\n6 000\n9 000\nBP Er yP HSC MasP MkP MoP NeuP\nNumber of cells\nOv ersampling\n0\n100\n200\n300\n400\n500\nBP Er yP HSC MasP MkP MoP NeuP\nNumber of cells\nSubsampling\nNeuP\nn = 7 n = 7 n = 7\n0\n0.25\n0.50\n0.75\n1.00\nBP\nBP\nEryP\nEryP\nHSC\nHSC\nMasP\nMasP\nPredicted label\nTrue label\nMkP\nMkP\nMoP\nMoP\nNeuP\nNeuP\nBP EryP HSC MasP\nPredicted label\nMkP MoP NeuP BP EryP HSC MasP\nPredicted label\nMkP MoP NeuP\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nOr iginal Subsampling Ov ersampling\nAccur acy\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProbability threshold\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProbability threshold\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProbability threshold\nAccur acy\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccur acy\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccur acy\nOr iginal Subsampling\nKno wn cells\nNo v el cells\nOv ersampling\nOr iginal Subsampling Ov ersampling\nFig. 2 | Performance of scBERT on the NeurIPS dataset. a, Distribution of \ncells in the NeurIPS dataset: (1) original dataset (left); (2) subsampling (middle): \nreduced number of cells (300 cells); (3) oversampling (right): augmented \nnumber of cells (4,600 cells) of BP and MoP cell types. b, Heatmaps for the \nconfusion matrices of the prediction results on the NeurIPS dataset (test data, \n30%): (1) original dataset (left); (2) subsampling (middle): reduced number of \ncells (300 cells); (3) oversampling (right): augmented number of cells (4,600 \ncells) of BP and MoP cell types. c, Performance of scBERT on the discovery \nof novel n = 7 cell types on the NeurIPS dataset (original, subsampling and \noversampling). Process: we removed one cell type in the training process and \nadded it to the predicted dataset. This process was iterated on each cell type. \nThe box plots show the median (centre lines), interquartile range (hinges) and \n1.5 times the interquartile range (whiskers). d, Threshold analysis for prediction \naccuracy in the NeurIPS dataset: (1) original dataset (left), (2) subsampling \n(middle): reduced number of cells (300 cells), (3) oversampling (right): \naugmented number of cells (4,600 cells) of BP and MoP cell types.\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446\n 1441\nArticle https://doi.org/10.1038/s42256-023-00757-8\nin a relatively even cell distribution (Fig. 2a). We observed that a \nbalanced distribution of cells by cell type influences the F1 scores. \nFor the original data, the F1 score was 0.6395 (Table 1 ). In contrast, \nsubsampling enhanced the F1 score to 0.7041 (Fig. 2b and Table 1 ). \nSpecifically, cells that were previously poorly predicted due to their \nsmaller proportions, such as BP cells, saw an increase in F1 score from \n0.0964 to 0.6879 (Fig. 2b ), compared to other cell types. We also \nassessed the robustness of scBERT across various subsampling levels \n(150, 200, 250 and 350). We found that scBERT still exhibits better \nperformance in terms of F1 score (Fig. 4a), even when we change the \nsubsampling levels and maintain the distribution of the cells by cell \ntype close to each other.\nAcc. = 0.143\n0.25\n0.50\n0.75\n1.00\na\nb\nc\nOr iginal Subsampling Ov ersampling\nAccur acy scBER T\nscBER T_f ocal_loss\nSeur at\n0.75\n0.80\n0.85\n0.90\nOr iginal\nAccur acy\n0.2\n0.4\n0.6\n0.8\nOr iginal Subsampling Ov ersampling\nscBER T\nscBER T_f ocal_loss\nSeur at\nn = 7\nn = 5 n = 5 n = 5\nn = 5n = 5 n = 5\nn = 7 n = 7 n = 7 n = 7 n = 7 n = 7 n = 7 n = 70\n0.2\n0.4\n0.6\n0.8\n1.0\nOr\niginal\nSubsampling Ov\nersampling\nOr\niginal\nSubsampling Ov\nersampling\nOr\niginal\nSubsampling Ov\nersampling\nF1 score\nF1 score\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score\nscBER T scBER T_f ocal_loss\nTraining\nPrediction\nSeur at\nFig. 3 | Benchmarking across scBERT , scBERT (focal loss) and Seurat. a, Left: \nPerformance of scBERT, scBERT (focal loss) and Seurat measured according to \naccuracy using fivefold cross-validation (n = 5) on the NeurIPS dataset (training \ndata, 70%): (1) original dataset; (2) subsampling: reduced number of cells  \n(300 cells); (3) oversampling: augmented number of cells (4,600 cells) of BP \nand MoP cell types. The dotted line represents the random accuracy result \n(1/7 = 0.143). Right: zoomed plot of the accuracy using the original dataset.  \nb, Performance of scBERT, scBERT (focal loss) and Seurat measured by F1 score \nusing fivefold cross-validation (n = 5) on the NeurIPS dataset (training data, 70%): \n(1) original dataset; (2) subsampling: reduced number of cells (300 cells); (3) \noversampling: augmented number of cells (4,600 cells) of BP and MoP cell types. \nc, Performance of scBERT, scBERT (focal loss) and Seurat in each cell type (n = 7 \ncell types) measured by F1 score on the NeurIPS dataset (original, subsampling \nand oversampling) in the training and prediction process. In the box plots, the \nmedian and upper and lower quartiles are represented by the centre line and box \nbounds, respectively. Whiskers display the largest and smallest values within  \n1.5 times the interquartile range from the quartiles.\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446 1442\nArticle https://doi.org/10.1038/s42256-023-00757-8\nBalanced cell distribution across various \nsubsampling levels enhances scBERT’s novel \ncell-type detection performance using NeurIPS \ndata\nOne possible effect of cell distribution on detecting novel cell types is \nthat it can influence the likelihood of detecting them. We thus asked \nwhether the distribution of cells affects the ability to detect novel cell \ntypes using scBERT. We used the same set-up when detecting novel \ncell types as used in the original NeurIPS dataset, but we used subsam-\npled data, that is, 300 cells per cell type (BP = 262 cells and MoP = 258 \ncells). We discovered that an even distribution of cells improves the \ndetection of novel cell types. The mean accuracy score improved from \n0.087 to 0.3187 (Fig. 2c). This suggests that cell distribution is essen-\ntial for scBERT when detecting novel cell types. We also evaluated the \nrobustness of scBERT with different subsampling levels (that is, 150, \n200 250 and 350). We observed that scBERT still exhibits better per-\nformance with different subsampling levels when the distribution of \ncells is roughly equal across different cell types, as shown in Fig. 4b.\nImproved scBERT performance in cell-type \nannotation and detection of novel cell types \nacross varied oversampling levels using NeurIPS \ndata\nNext, we asked whether balancing the cell-type representation by \nincreasing the count of cell types with the fewest cells improves or \ndegrades the performance of scBERT for annotation and detecting \nnovel cell types. T o this end, we performed data oversampling utiliz-\ning the ‘synthetic minority oversampling technique’ (SMOTE)10. We \noversampled the cell types with a low number of cells and increased \nthe mean of the cells with the maximum number of cells (4,600 cells \nfor the BP and MoP cell types). We observed that oversampling slightly \nimproved the F1 score from 0.7041 to 0.7353 (Fig. 4a) compared to \nwhen we subsampled to a lower number of samples per cell type (300 \ncells). Moreover, we examined the robustness of scBERT with different \noversampling levels (that is, 1,000, 2,000, 3,000, 4.000 and 5,000 \nsamples) and observed that scBERT consistently performs well for \ncell-type annotation tasks, even when the sampling levels are changed, \nas shown in Fig. 4a and Extended Data Fig. 2.\nHowever, scBERT shows a slight improvement in novel cell-type \ndetection compared to when there is a reasonable amount of imbalance \nin the data (0.087 to 0.181), but not as well as when we subsampled to \n300 cells per cell type (0.3187) (Fig. 2c). The rationale for this is that, \nunlike when we subsampled 300 cells per cell type, which had compara-\ntive F1 scores across all cell types, the oversampling raises the F1 score \nof the enhanced cells and influences the F1 score across other cell types. \nWe further evaluated the efficacy of scBERT across increasing oversam-\npling levels (that is, 1,000, 2,000, 3,000, 4,000 and 5,000 samples). \nOur observations reveal that scBERT maintains its performance across \nthese different oversampling levels when compared to the performance \nseen with the original and various subsampled distributions across cell \ntypes, as demonstrated in Fig. 4a. These results suggest that scBERT \nexhibits effective performance across a range of sample sizes.\nWe also investigated the impact of random oversampling, a type \nof resampling used for assessing the robustness of scBERT in cell-type \nannotation and the detection of novel cell types, in addition to SMOTE \noversampling. For this purpose, cells from minority classes, BP and \nMoP, were randomly duplicated with replacements to create a more \nbalanced dataset. This procedure was iteratively executed to aug -\nment the BP and MoP cell counts to 1,000, 2,000, 3,000, 4,000 and \n5,000, respectively. We examined scBERT’s robustness using random \noversampling levels of 1,000, 2,000, 3,000, 4,000 and 5,000. Our \nobservations indicate that scBERT maintains strong performance \nwith an increase in sample size in both cell-type annotation and novel \ncell-type detection, as depicted in Fig. 4a and Extended Data Fig. 2. \nThese results suggest that scBERT demonstrates substantial robustness \nacross various oversampling levels, indicating its effective handling of \nincreased sample sizes.\nNo significant improvement in scBERT \nperformance with focal loss for cell-type \nannotation using NeurIPS data across varied cell \ndistributions\nWe also assessed the impact of using a focal loss function11 instead of \nthe cross-entropy loss, as utilized in the original publication during the \nfine-tuning phase. The focal loss function is designed to handle class \nimbalance issues in classification tasks. It introduces two additional \nparameters to the traditional cross-entropy loss: alpha, a class balancing \nweight, and gamma, a factor that adjusts the rate of down-weighting \nfor easy examples. T o this end, we trained the models using the focal \nloss function and observed a slightly inferior performance compared to \nmodels trained with cross-entropy loss. The mean validation accuracy \nwas 0.8488 for the focal loss, compared to 0.8510 for the cross-entropy \nloss in the original dataset, which exhibits an imbalance in the distribu-\ntion of cells per cell type. Furthermore, in scenarios of subsampling \nand oversampling, where there is an even distribution of cells per cell \ntype, scBERT with the default loss function (cross-entropy) still out-\nperformed the scBERT trained with the focal loss function (Fig. 3a,b \nand Extended Data Fig. 2): 0.7646 versus 0.7540. These results sug-\ngest that changing the loss function does not substantially improve \nscBERT’s performance in tackling the uneven distribution of cells per \ncell-type problem.\nImpact of probability threshold on scBERT’s \ndetection of novel cell types on NeurIPS data\nT o understand the impact of the chosen probability threshold on \nscBERT’s performance in detecting novel cell types, we conducted a \nsystematic analysis using various sampling strategies (subsampling of \n300 cells and oversampling of 4,600 cells) and probability threshold \nTable 1 | Performance analysis of scBERT on the NeurIPS dataset\nOriginal Subsampling Oversampling\nAccuracy F1 score Accuracy F1 score Accuracy F1 score\nSplit1 0.8465 0.6158 0.8057 0.7949 0.7642 0.7493\nSplit2 0.8489 0.6453 0.7456 0.7176 0.7506 0.7295\nSplit3 0.8504 0.6133 0.7491 0.7204 0.7458 0.7196\nSplit4 0.8593 0.6594 0.7774 0.7573 0.7611 0.7476\nSplit5 0.8501 0.6389 0.7456 0.741 0.7563 0.7209\nPrediction (data size 30%) 0.8397 0.6395 0.7145 0.7041 0.7572 0.7353\nA summary is shown of the performance of scBERT in the NeurIPS (original, subsampling (300 cells) and oversampling (4,600 cells)) dataset using fivefold cross-validation and the prediction \nresults for the test data (size 30%). The best model accuracies and F1 scores across all settings are marked in bold.\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446\n 1443\nArticle https://doi.org/10.1038/s42256-023-00757-8\nvalues ranging from 0.0 to 0.9. For novel cells, the accuracy metric was \ncomputed as the ratio of cells aptly classified as ‘unassigned’ to the \ntotal count of novel cells. In contrast, we used a multi-class accuracy \napproach for known cells after removing the ‘unassigned’ cells. This \ncalculated accuracy was further adjusted by subtracting the propor-\ntion of ‘unassigned’ cells discarded in the previous step, introducing a \npenalization for misclassification. Figure 2d depicts the mean accuracy \nfor different cell types, both known and novel, across varying probabil-\nity thresholds. As the threshold increases, we observe a corresponding \ndecrease in the detection accuracy for known cells, while the accuracy \nfor novel cells improves. This trade-off underscores the critical role \nthat threshold settings play in balancing the accurate classification of \nknown cells and the discovery of novel cell types. Notably, when using \nthe default threshold of 0.5, the subsampled dataset outperforms the \nothers, achieving the highest accuracy in novel cell detection (0.3189), \nclosely followed by the oversampled dataset (0.1804), and finally, the \noriginal dataset (0.0878). As illustrated in Fig. 2d, we recommend \nadjusting the threshold upwards to ~0.7 or 0.8 in the original dataset \nto substantially enhance the detection accuracy of novel cells.\nSubsampling with balanced cell distribution \nimproves scBERT’s performance on cell-type \nannotation on the Zheng68k data\nWe repeated the same steps for the Zheng68k dataset as used in our \nanalysis using NeurIPS data to validate the observation of the effect of \ncell distribution per cell type on cell-type annotation. We divided the \ncell types into subsamples of 2,000 cells each (Fig. 5a). We excluded \nCD4+ T Helper2 (n = 97) and CD34+ (n = 242) cells. The reason for exclud-\ning these two cell types is that the other cell types have over 2,000 cells, \nand subsampling them to a much smaller number will remove infor-\nmation about the cell types. Using the Zheng68k dataset, we trained \nthe model (fine-tuning the pretrained model) on this equally distrib-\nuted dataset per cell type and then used this fine-tuned model for the \ncell-type annotation task. We observed that the distribution impacts the \nannotation of cell type. We improved the F1 score to 0.6683 and boosted \nthe prediction scores of the cell types for which scBERT demonstrated \npoor performance when these cell types (CD4+/CD45RA+/CD25− naïve \nT, dendritic) have a lower proportion of cells than the other cell types.\nBalanced cell distribution through subsampling \nimproves scBERT’s performance in detecting \nnovel cell types on the Zheng68k dataset\nWe have demonstrated how scBERT’s efficacy in identifying novel cell \ntypes on NeurIPS data is impacted by the distribution of cells across \ncell types. T o further validate this observation, we applied scBERT to \nthe subsampled Zheng68k dataset, for which there is an equal distri-\nbution of cells per cell type. scBERT showed improved performance \nin the mean accuracy value in detecting novel cell types compared to \nwhen we have a relatively smaller number of cells in specific cell types \n(Fig. 5b). This demonstrates that the performance of scBERT relies on \n0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1 score\nOr iginal\nSubsampling\nSMO TE\nRandom ov er .\nOr iginal\nSubsampling\nSMO TE\nRandom ov er .\n0\nOr\niginal 150 200 250 300 350 1,000 2,000 3,000 4,000 4,600 5,000 1,000 2,000 3,000 4,000 5,000 Or\niginal 150 200 250 300 350 1,000 2,000 3,000 4,000 4,600 5,000 1,000 2,000 3,000 4,000 5,000\n0.2\n0.4\n0.6\n0.8\n1.0\na\nb Accur acy\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccur acy\nOr\niginal 150 200 250 300 350 1,000 2,000 3,000 4,000 4,600 5,000 1,000 2,000 3,000 4,000 5,000\nn = 7\nFig. 4 | Performance of scBERT on the NeurIPS dataset with varied \nsubsampling and oversampling. a, Accuracy and F1 score of the prediction \nperformance on the NeurIPS dataset: original, subsampling and oversampling \n(using SMOTE and random oversampling). b, Performance of scBERT discovery \nof novel cell types (n = 7) on the NeurIPS dataset: original, subsampling and \noversampling (using SMOTE and random oversampling). Process: we removed \none cell type in the training process and added it to the predicted dataset. This \nprocess was iterated on each cell type (using a probability threshold of 0.5). In \nthe box plots, lower and upper hinges represent the first and third quartiles, the \ncentre lines the median, and whiskers the range of 1.5 times the interquartile.\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446 1444\nArticle https://doi.org/10.1038/s42256-023-00757-8\nthe distribution of cells for detecting novel cell types and cell-type \nannotation.\nDiscussion\nIn this study we have demonstrated that reusing the code and data \nprovided by Yang and colleagues3 is sufficient to reproduce the main \npublished results in the cell-type annotation task. However, it was chal-\nlenging to identify the novel cell types in the MacParland dataset. Ide-\nally, scBERT should have detected all the cell types that were excluded \nin the training data and included during prediction as novel ones, but \nwe found that it did not perform well in this scenario.\nIn addition to reproducing the results of ref. 3, we found that the \ndistribution of cells per cell type affects the effectiveness of scBERT \nin cell-type annotation and detecting novel cell types. This is because, \nin both the cell-type annotation and the task of recognizing novel cell \ntypes, scBERT performs poorly on cell types with fewer cells than other \ncell types. In our analysis, we conducted two studies, subsampling \nthe number of cells per cell type around the cell type with fewer cells \nthan the other cell types, and, in the second set, augmenting the cell \ntypes with fewer cells to see how the distribution affects scBERT’s \nperformance on cell-type annotation and detecting novel cell types. \nThe empirical results show that the distribution affects scBERT’s per-\nformance when the cell types are evenly distributed; that is, it improves \nits performance across cell-type annotation and novel cell-type detec-\ntion. Furthermore, the performance of scBERT appears sensitive to \nthe degree of skewness of the distributions, an effect that becomes \nabundantly clear when detecting novel cell types.\nFor future directions in part based on our analysis, further work \nis needed to understand the reasons for this distributional sensitivity \nof transformers in single-cell genomics and to develop methods to \nmitigate it. Possible directions include examining the effects of class \nimbalance on the training of transformer models and developing objec-\ntive functions and training procedures tailored to uneven cell-type \ndistributions. Furthermore, including domain knowledge and appro-\npriate regularizations, which could guide the model towards learning \nbiologically plausible predictions, could potentially aid in detecting \nnovel cell types. Nevertheless, our reusability analysis demonstrates \nthe potential of scBERT in detecting novel cell types and provides \ninsights for future improvements.\nMore broadly, we envision that transformer models could be applied \nto various downstream single-cell analysis tasks, such as differential \nexpression analysis, multimodal data integration, gene function \nanalysis and drug-response prediction. With the comprehensive \nunderstanding of cellular transcriptional contexts that scBERT learns \nduring pretraining, it can be used for perturbation response prediction. \nIn this scenario, the model will be fine-tuned on perturbation data.  \n0\n5,000\n10,000\n15,000\n20,000\n25,000\na\nb\nCD14\n+  monocyte CD19\n+  B\nCD34\n+\nCD4\n+  T Helper2\nCD4\n+ /CD25TReg\nCD4\n+ /CD45RA\n+ /CD25\n–  naiv\neT\nCD4\n+ /CD45R\nO\n+  memor\ny\nCD56\n+  NK\nCD8\n+  cytoto\nxic T\nCD8\n+ /CD45RA\n+  naiv\ne cytoto\nxic\nDendr\nitic\nNumber of cells\n0\n1,000\n2,000\n3,000\nNumber of cells\nOr iginal\nCD14\n+  monocyte\nCD19\n+  B\nCD34\n+\nCD4\n+  T Helper2\nCD4\n+ /CD25TReg\nCD4\n+ /CD45RA\n+ /CD25\n–  naiv\neT\nCD4\n+ /CD45R\nO\n+  memor\ny\nCD56\n+  NK\nCD8\n+  cytoto\nxic T\nCD8\n+ /CD45RA\n+  naiv\ne cytoto\nxic\nDendr\nitic\nSubsampling\nn = 9 n = 9\n0\n0.1\n0.2\n0.3\n0.4\nOr iginal Subsampling\nAccur acy\nFig. 5 | Performance of scBERT on the subsampled Zheng68k dataset.  \na, Distribution of cells in the Zheng68k dataset: (1) original dataset (left) and \n(2) subsampling (right), where we reduce the number of cells (2,000 cells) and \nremove CD34+ and CD4+ T helper2 cells. b, Performance of scBERT discovery \nof n = 9 novel cell types on the Zheng68k dataset (original and subsampling). \nProcess: we removed one cell type in the training process and added this to the \npredicted dataset. This process was iterated on each cell type. Box plots show the \nmedian (centre lines), interquartile range (hinges) and 1.5 times the interquartile \nrange (whiskers).\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446\n 1445\nArticle https://doi.org/10.1038/s42256-023-00757-8\nThe data should consist of paired observations of each cell’s initial state \nand the state after perturbation, allowing scBERT to learn the relationship \nbetween these states. In the context of multimodal data integration, \nas scBERT is originally pretrained on scRNA-seq data, applying it to \nmultimodal data would require incorporating the additional data \nmodalities into the model’s input. One possible approach might be to \nconcatenate the normalized representations of each data type into a \nsingle vector for each cell. The model would then be fine-tuned on the \ncombined data, allowing it to learn correlations between the different data \ntypes and thereby enhance the richness of its cell-state representations. \nMoreover, for gene function analysis, scBERT’s architecture could be \nused with a change in the output layer to predict the functional category \nor categories associated with each gene. The model would need to be \nfine-tuned on gene annotation data, where each gene is associated with \none or more functional categories. However, our reusability analysis \nstresses the need to consider the specific characteristics of the task and \ndata distribution balance when applying a transformer architecture. \nAs with novel cell-type detection, it will be essential to understand and \naddress the effects of potential data imbalances. In general, it appears \nto be a delicate balance between the power of transformers to detect \nsubtle correlations in datasets versus their sensitivity to skewed data \ndistributions and class imbalances. How this plays out for different tasks \nin single-cell genomics and biomedicine remains to be investigated.\nMethods\nReproducibility experiments\nWe found a few errors in the syntax of the command line arguments \nas they were included before the scripts (fine-tuning the pretrained \nmodel, predicting using a fine-tuned model, and detecting novel cell \ntypes) as stated in the ‘Usage’ section of scBERT’s GitHub repository. \nHowever, after rectifying these minor issues, the code was easy to \nexecute to reproduce the results.\nCell-type annotation\nOur reproducibility experiments on cell-type annotation followed the \nsteps outlined by the authors 3 in the GitHub repository, where they \nused 100% of the data for training and the same dataset for testing. \nWe ran the experiment in exactly the same way as the original authors, \nincluding using the same random seeds specified in the code. We dis-\ncovered that on the Zheng68k dataset7, our F1 score of 0.677 (Extended \nData Table 1) and theirs of 0.691 deviated by 0.014; furthermore, we \nobtained a mean accuracy value of 0.7802 on the Zheng68k dataset, \nwhereas the original publication reports 0.7590 (0.021 difference). On \nthe MacParland dataset, we obtained an F1 score of 0.9602 as compared \nto the F1 score reported in the paper of 0.9588, deviating by 0.0014, \nwhile our respective mean accuracy value deviated by 0.020 (ours, \n0.9558, Fig. 1a; theirs, 0.9760; Extended Data Table 1). Our analysis \nrevealed a slight deviation in the mean accuracy score and F1 score, \nprobably due to excluding a specific immune cell type, CD4+ T Helper2, \nfrom the original study. On further investigation, it was observed that \nthis cell type—CD4+ T Helper2—has a lower abundance of cells (n = 97) \nthan the other cell types, resulting in a relatively lower accuracy than \nthe results reported in the original study (Extended Data Fig. 1b).\nWe also divided the datasets into two subsets, with 70% allocated \nfor training and 30% for testing. An additional split was performed on \nthe training subset to further refine the model’s performance, with \n80% of the data utilized for model training and the remaining 20% for \nvalidation. T o this end, we observed that the prediction accuracy of \nscBERT decreased in mean accuracy (0.7802 to 0.7551; Extended Data \nFig. 1a). However, we found that scBERT continued to perform well \nin the second phase of this set-up, where we used 70% of the data for \ntraining and the remaining 30% for testing, with a mean accuracy score \nof 0.7533 (Extended Data Fig. 1b) and F1 score of 0.6523.\nWe performed the same experiments on the MacParland dataset \nand divided it into 70% for training scBERT and 30% as test data for the \nprediction. We discovered little difference in mean accuracy values \nwhen training and testing scBERT with 70% MacParland data versus \n100% MacParland data (0.9532 and 0.9558, respectively; Extended \nData Fig. 1a). However, with 30% test data, the mean accuracy score was \n0.9372 and the F1 score 0.9400 (Extended Data Fig. 1b).\nDetecting novel cell types\nIn many scenarios, the reference dataset may not include all the dif -\nferent types of cells in the query dataset. This can be a problem for \nmarker-based methods, which rely on pre-selected markers for known \ncell types and may struggle to identify new, unseen cell types. On the \nother hand, correlation-based methods often assign novel cell types \nto the closest available class, which may not always be accurate. In \ncontrast, machine learning-based methods can supposedly automati-\ncally detect novel cell types by evaluating the predicted probabilities. \nThis allows for a more precise and flexible classification of cell types.\nT o this end, as reported in this paper, we tested the reproducibility \nof scBERT in detecting novel cell types on the MacParland dataset. The \ncell types excluded for reproducibility were plasma cells, alpha-beta \nT cells, gamma-delta T cells and mature B cells. We discovered that \nscBERT predicts only plasma cells as novel cell types. It cannot detect \nthe other cell types (alpha-beta T cells, gamma-delta T cells and mature \nB cells) as novel cell types (Extended Data Fig. 1c).\nData availability\nThe NeurIPS data used in our study are available in the figshare reposi-\ntory at https://figshare.com/projects/scbert-reusability/157203 . \nThe Zheng68K dataset was downloaded from GitHub, and the Mac -\nParland data from https://www.ncbi.nlm.nih.gov/geo/query/acc.\ncgi?acc=GSE115469. Source data are provided with this paper.\nCode availability\nThe original scBERT code is available at https://github.com/T encentAIL-\nabHealthcare/scBERT. Our GitHub content, with detailed instructions, \nis available at https://github.com/TranslationalBioinformaticsUnit/\nscbert-reusability. The code can also be accessed via Zenodo at https://\ndoi.org/10.5281/zenodo.8191571 ref. 12.\nReferences\n1. Ma, Q. & Xu, D. Deep learning shapes single-cell data analysis. \nNat. Rev. Mol. Cell Biol. 23, 303–304 (2022).\n2. Wainberg, M., Merico, D., Delong, A. & Frey, B. J. Deep learning in \nbiomedicine. Nat. Biotechnol. 36, 829–838 (2018).\n3. Yang, F. et al. scBERT as a large-scale pretrained deep language \nmodel for cell type annotation of single-cell RNA-seq data. Nat. \nMach. Intell. 4, 852–866 (2022).\n4. Cui, H., Wang, C., Maan, H., Duan, N. & Wang, B. scFormer: \na universal representation learning approach for single-cell \ndata using transformers. Preprint at bioRxiv https://doi.\norg/10.1101/2022.11.20.517285 (2022).\n5. Du, J. et al. Gene2vec: distributed representation of genes based \non co-expression. BMC Genomics 20, 82 (2019).\n6. Franzén, O., Gan, L.-M. & Björkegren, J. L. M. PanglaoDB: a web \nserver for exploration of mouse and human single-cell RNA \nsequencing data. Database 2019, baz046 (2019).\n7. Zheng, G. X. Y. et al. Massively parallel digital transcriptional \nprofiling of single cells. Nat. Commun. 8, 14049 (2017).\n8. MacParland, S. A. et al. Single cell RNA sequencing of human \nliver reveals distinct intrahepatic macrophage populations. Nat. \nCommun. 9, 4383 (2018).\n9. Wolf, F. A., Angerer, P. & Theis, F. J. SCANPY: large-scale single-cell \ngene expression data analysis. Genome Biol. 19, 15 (2018).\n10. Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. \nSMOTE: synthetic minority over-sampling technique. J. Artif. Intell. \nRes. 16, 321–357 (2002).\nNature Machine Intelligence | Volume 5 | December 2023 | 1437–1446 1446\nArticle https://doi.org/10.1038/s42256-023-00757-8\n11. Lin, T. Y., Goyal, P., Girshick, R., He, K. & Dollar, P. Focal loss for \ndense object detection. IEEE Trans. Pattern Anal. Mach. Intell. 42, \n318–327 (2020).\n12. Khan, S. A. et al. Translational bioinformatics unit/scBERT-reusability: \n2.0.0. Zenodo https://doi.org/10.5281/zenodo.8191571 (2023).\nAcknowledgements\nKing Abdullah University of Science and Technology supported this \nwork, which was also partially funded by the SDAIA-KAUST Center of \nExcellence in Data Science and Artificial Intelligence.\nAuthor contributions\nS.A.K. and A.M. conducted the reproducibility and reusability \nexperiments, curated the datasets, drafted the first version of the \nmanuscript and integrated all the edits. V.L. and R.L. contributed \nideas, and N.A.K., D.G.-C. and J.T. contributed ideas and supervised \nthe overall analysis, manuscript writing and final edits.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-023-00757-8.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00757-8.\nCorrespondence and requests for materials should be addressed  \nto Jesper Tegner.\nPeer review information Nature Machine Intelligence thanks Harald \nBinder, Benjamin Haibe-Kains and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2023\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00757-8\nExtended Data Fig. 1 | Reproducible performance of scBERT on Zheng68k \nand MacParland datasets. a) Performance of scBERT measured by accuracy on \nZheng68k and MacParland dataset (size 70% and 100%) using fivefold cross-\nvalidation (n = 5). b) Heatmap for the confusion matrices of the prediction results \non the Zheng68k and MacParland datasets (test data, 30%). Left side: Zheng68k \n(accuracy = 0.7533 and F1 score = 0.6523) Right side: MacParland (accuracy = \n0.9372 and F1 score = 0.9400) c) Performance of scBERT discovery novel cell \ntypes on the MacParland dataset by removing alpha-beta T cell, gamma-delta \nT cell (gamma-delta_T_Cells_1 and gamma-delta_T_Cells_2), mature B cell, and \nplasma cell population (denominated as novel) during the training process. \nAccuracy of detecting n = 15 known and n = 5 novel cell types. In box plots, the \nlower and upper hinges represent the first and third quartiles, the center lines the \nmedian, with whiskers in the range of 1.5-times the interquartile.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00757-8\nExtended Data Fig. 2 | Confusion matrices. Heatmap for the confusion \nmatrices of the prediction results. 1) Subsampling sizes: 150, 200, 250 and 350. \n2) SMOTE oversampling sizes: 1000, 2000, 3000, 4000 and 5000. 3) Random-\noversampling sizes: 1000, 2000, 3000, 4000 and 5000. 4) Focal-loss: original, \nsubsampling (size = 300) and oversampling using SMOTE (size = 4600). 5) Seurat: \noriginal, subsampling (size = 300) and oversampling using SMOTE (size = 4600).\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-023-00757-8\nExtended Data Table 1 | Results of performance of scBERT in Zheng68k and MacParland\nSummary table of performance of scBERT in Zheng68k and MacParland (size 70% and 100%) using fivefold cross-validation and the prediction results in test data (size 30%). The prediction \nanalysis was not performed when the whole dataset was used for training.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7178640961647034
    },
    {
      "name": "Reusability",
      "score": 0.6654765605926514
    },
    {
      "name": "Grammar",
      "score": 0.6254850029945374
    },
    {
      "name": "Genomics",
      "score": 0.5677451491355896
    },
    {
      "name": "Generalizability theory",
      "score": 0.5547100305557251
    },
    {
      "name": "Transformer",
      "score": 0.5480192303657532
    },
    {
      "name": "Encoder",
      "score": 0.4611928462982178
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3801012337207794
    },
    {
      "name": "Biology",
      "score": 0.2104511260986328
    },
    {
      "name": "Gene",
      "score": 0.17509520053863525
    },
    {
      "name": "Programming language",
      "score": 0.1453835368156433
    },
    {
      "name": "Genome",
      "score": 0.11411625146865845
    },
    {
      "name": "Engineering",
      "score": 0.1117827296257019
    },
    {
      "name": "Software",
      "score": 0.09062579274177551
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71920554",
      "name": "King Abdullah University of Science and Technology",
      "country": "SA"
    },
    {
      "id": "https://openalex.org/I28166907",
      "name": "Karolinska Institutet",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I1335113835",
      "name": "Karolinska University Hospital",
      "country": "SE"
    }
  ],
  "cited_by": 23
}