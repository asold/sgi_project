{
  "title": "TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation",
  "url": "https://openalex.org/W3182372246",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2349005916",
      "name": "Chen Bing-zhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351327442",
      "name": "Liu Yi-shu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003519119",
      "name": "Zhang Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2319573878",
      "name": "LU Guangming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743406535",
      "name": "Kong, Adams Wai-kin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2991139962",
    "https://openalex.org/W3092622437",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2980998394",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3015388942",
    "https://openalex.org/W3025800305",
    "https://openalex.org/W2888442043",
    "https://openalex.org/W3148874463",
    "https://openalex.org/W2999309192",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3114814504",
    "https://openalex.org/W2742087205",
    "https://openalex.org/W3173693036",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W2041352300",
    "https://openalex.org/W2913026733",
    "https://openalex.org/W3010971576",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2766069452",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2142514727",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W2971567587",
    "https://openalex.org/W3081752372",
    "https://openalex.org/W2807880213",
    "https://openalex.org/W2046969815",
    "https://openalex.org/W2905338897",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2964216718",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2932083555",
    "https://openalex.org/W3033857292",
    "https://openalex.org/W3092462072",
    "https://openalex.org/W2951005624",
    "https://openalex.org/W1904878066",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W2798122215"
  ],
  "abstract": "Accurate segmentation of organs or lesions from medical images is crucial for reliable diagnosis of diseases and organ morphometry. In recent years, convolutional encoder-decoder solutions have achieved substantial progress in the field of automatic medical image segmentation. Due to the inherent bias in the convolution operations, prior models mainly focus on local visual cues formed by the neighboring pixels, but fail to fully model the long-range contextual dependencies. In this paper, we propose a novel Transformer-based Attention Guided Network called TransAttUnet, in which the multi-level guided attention and multi-scale skip connection are designed to jointly enhance the performance of the semantical segmentation architecture. Inspired by Transformer, the self-aware attention (SAA) module with Transformer Self Attention (TSA) and Global Spatial Attention (GSA) is incorporated into TransAttUnet to effectively learn the non-local interactions among encoder features. Moreover, we also use additional multi-scale skip connections between decoder blocks to aggregate the upsampled features with different semantic scales. In this way, the representation ability of multi-scale context information is strengthened to generate discriminative features. Benefitting from these complementary components, the proposed TransAttUnet can effectively alleviate the loss of fine details caused by the stacking of convolution layers and the consecutive sampling operations, finally improving the segmentation quality of medical images. Extensive experiments on multiple medical image segmentation datasets from different imaging modalities demonstrate that the proposed method consistently outperforms the state-of-the-art baselines. Our code and pre-trained models are available at: https://github.com/YishuLiu/TransAttUnet.",
  "full_text": "IEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 1\nTransAttUnet: Multi-level Attention-guided U-Net\nwith Transformer for Medical Image Segmentation\nBingzhi Chen, Yishu Liu, Yingjian Li, Zheng Zhang, Senior Member, IEEE,\nGuangming Lu, Member, IEEE, and Adams Wai Kin Kong, Member, IEEE.\nAbstract—Accurate segmentation of organs or lesions from\nmedical images is crucial for reliable diagnosis of diseases and\norgan morphometry. In recent years, convolutional encoder-\ndecoder solutions have achieved substantial progress in the ﬁeld\nof automatic medical image segmentation. Due to the inherent\nbias in the convolution operations, prior models mainly focus\non local visual cues formed by the neighboring pixels, but\nfail to fully model the long-range contextual dependencies. In\nthis paper, we propose a novel Transformer-based Attention\nGuided Network called TransAttUnet1, in which the multi-level\nguided attention and multi-scale skip connection are designed to\njointly enhance the performance of the semantical segmentation\narchitecture. Inspired by Transformer, the self-aware attention\n(SAA) module with Transformer Self Attention (TSA) and Global\nSpatial Attention (GSA) is incorporated into TransAttUnet to ef-\nfectively learn the non-local interactions among encoder features.\nMoreover, we also use additional multi-scale skip connections\nbetween decoder blocks to aggregate the upsampled features with\ndifferent semantic scales. In this way, the representation ability\nof multi-scale context information is strengthened to generate\ndiscriminative features. Beneﬁtting from these complementary\ncomponents, the proposed TransAttUnet can effectively alleviate\nthe loss of ﬁne details caused by the stacking of convolution layers\nand the consecutive sampling operations, ﬁnally improving the\nsegmentation quality of medical images. Extensive experiments\non multiple medical image segmentation datasets from differ-\nent imaging modalities demonstrate that the proposed method\nconsistently outperforms the state-of-the-art baselines.\nIndex Terms—Medical Image Segmentation; Transformer;\nMulti-level Guided Attention; Multi-scale Skip Connection.\nI. I NTRODUCTION\nI\nN clinical diagnosis, the purpose of medical image seg-\nmentation is to delineate the objects of interest from the\ncomplex background on various biomedical images [1]–[3],\n©©2022 IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nB. Chen is with the Shenzhen Medical Biometrics Perception and\nAnalysis Engineering Laboratory, Harbin Institute of Technology, Shenzhen\n518055, China, and also with the School of Computer Science and En-\ngineering, Nanyang Technological University, Singapore 639798. (e-mail:\nchenbingzhi.smile@gmail.com)\nY . Liu, Y . Li, Z. Zhang, and G. Lu are with the Shenzhen Medical\nBiometrics Perception and Analysis Engineering Laboratory, Harbin Institute\nof Technology, Shenzhen 518055, China. (e-mail: liuyishu.smile@gmail.com,\nhit lyj@126.com, darrenzz219@gmail.com, luguangm@hit.edu.cn)\nA. Kong is with the School of Computer Science and Engi-\nneering, Nanyang Technological University, Singapore, 639798. (e-mail:\nAdamsKong@ntu.edu.sg)\n1The preprint of our work has been posted at: https://doi.org/10.48550/\narXiv.2107.05274. Our code and pre-trained models are available at: https:\n//github.com/YishuLiu/TransAttUnet\nSkin lesion                             Lung field Pneumonia\nImages\nGround Truth\nGland\nGland\nNuclei \nFig. 1. Examples of medical images with the corresponding semantic\nsegmentation annotations.\nsuch as X-ray, Computerized Tomography (CT), Magnetic\nResonance Imaging (MRI), and Ultrasound. It is useful for\nquantitative diagnosis and morphological analysis of speciﬁc\nlesions in human organs and tissues. As shown in Fig. 1, it\nrequires enormous effort and patience to handle the complex\ncontours and textures. However, traditional manual annotation\nheavily relies on clinical experiences. Measurements based\non clinicians’ manual annotations might be very accurate\nbut highly labor-intensive under standard clinical settings.\nTherefore, it is in great demand to develop accurate medical\nimage segmentation methods.\nIn recent years, deep encoder-decoder architectures have\nproven to be effective in recovering details of the segmented\nobjects and gradually become the industry’s facto benchmark\nfor medical image segmentation. The main purpose of convo-\nlutional operations used in deep encoder-decoder architectures\nis to extract local features of images by gathering local\ninformation from the neighboring pixels. Typically, the stacks\nof convolution layers and the consecutive sampling operations\nconstantly extend the receptive ﬁeld and aggregate the global\nﬁlter responses to determine the coarse object boundaries.\nBeneﬁtting from deconvolution operations [4], Fully Convo-\nlutional Networks (FCN) [5] can naturally operate on images\nwith various sizes and generate appropriate dimension outputs\naccording to the corresponding inputs. Inspired by FCN, U-\nNet [6] directly combines low-level features from the analysis\npath and deep features in the expansion path through encoder-\ndecoder skip connections, which can achieve the trade-off ca-\npability between local information and contextual information.\nDespite the interesting design and encouraging performance,\nthe above design limits the information ﬂow due to the\nconvolutional operations and becomes a bottleneck for per-\nformance enhancement. Speciﬁcally, these architectures have\nseveral drawbacks: (1) these architectures generally lack the\narXiv:2107.05274v2  [eess.IV]  9 Jul 2022\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 2\nability to model long-range feature dependencies due to the\ninherent inductive biases [7] in the convolution computing\nparadigm; (2) the low-level features might be prevented from\nbeing transmitted to the subsequent convolutional layers in the\nprocess of pooling and convolution [8], which compromise\nthe quality of local information and degrade the semantic\nsegmentation performance; (3) the existing skip connection\nmechanism is only performed on the same scale feature maps\nwithout exploring the relationship between feature maps from\ndifferent decoding stages, which cannot guarantee the consis-\ntency of the feature representations and semantic embeddings\n[9].\nTo solve these issues, researchers have put considerable\neffort in developing various variants of U-Net. The exist-\ning works follow three research lines: (1) attention-guided\napproaches; (2) context-based approaches; (3) Transformer-\nbased approaches. For example, prior attention-guided works,\ne.g. Attention U-Net [10] and Channel-UNet [8], attempted\nto leverage various attention mechanisms to optimize the\nspatial information of feature maps extracted from encoding,\ndecoding, and output stages. The main idea of these attention\nmechanisms is to generate a conﬁdence mask to recalibrate\nthe response of the original feature maps. Different from the\nabove approaches, the second one is to explore the contextual\ninformation by using multi-scale connections. It is indisputable\nthat both high-level abstract information and low-level pixel\ninformation are of great signiﬁcance in developing accurate\nsegmentation. Therefore, varying degrees of context-based\nshortcut connection components have been embedded into the\nU-shaped architectures, such as Unet++ [9], and MA-Unet\n[11], in order to capture broader and deeper contextual rep-\nresentations. Recently, Transformer [12] [13] is getting great\nattention from computer vision (CV) researchers. Especially,\nsome recent works, such as TransUNet [14] and MedT [15],\nhave tried to incorporate Transformer with CNN-based model\nto boost the performance of medical image segmentation. In\ngeneral, Transformer-based approaches have very high compu-\ntation complexities to process feature maps, and often required\nmodels pretrained on a large external dataset. As the core\nof Transformer, the multi-head mechanism [16] can capture\nthe long-range contextual information by running through\nthe scaled dot-product attention multiple times in parallel.\nTherefore, it is undoubtedly an ideal supplemental part that\ncan efﬁciently compensate for the design ﬂaws of U-Net.\nInspired by the aforementioned advanced works, in this\npaper we propose a novel multi-level attention-guided U-Net\nwith Transformer, dubbed TransAttUnet, that can effectively\nenhance the segmentation accuracy of traditional U-shaped\narchitecture by jointly utilizing multi-level guided attention\nand multi-scale skip connection. The architecture of the pro-\nposed TransAttUnet is shown in Fig. 2. Speciﬁcally, a robust\nself-aware attention (SAA) module is ﬁrst embedded into the\nproposed TransAttUnet as the bridge between the encoder and\ndecoder subnetworks. As the key component of TransAttUnet,\nthe purpose of the SAA module is to concurrently leverage\nthe powerful abilities of transformer self attention (TSA) and\nglobal spatial attention (GSA) to establish effective long-\nrange interactions and global spatial relationships between\nencoder semantic features. Motivated by the idea of residual\nand dense shortcut connections, a multi-scale skip connection\nscheme is added into decoder sub-networks with a series\nof transition operations, including upsampling, concatenation,\nand convolution. Thus, it can ﬂexibly aggregate the residual or\ndense contextual feature maps from decoder blocks of varying\nsemantic scales step by step, in order to generate more discrim-\ninative feature representations. With the contributions of these\ncomplementary components, the proposed method can achieve\naccurate semantic segmentation masks of medical images. We\nevaluate the effectiveness of the proposed TransAttUnet on\nmultiple medical image datasets from different imaging modal-\nities. Our experiments mainly involve ﬁve typical challenges in\nclinical diagnosis, including: (1) Skin lesion segmentation on\ndermatoscopic images [17]; (2) Lung segmentation on chest\nX-ray images; (3) COVID-19 pneumonia lesion segmentation\non chest CT images; (4) Nuclei segmentation on divergent\nimages [18], and (5) Gland segmentation on histology images\n[19]. Our main contributions are summarized as follows:\n• This paper proposes a Transformer-Attention based U-\nshaped framework (TransAttUnet), that integrates the\nadvantages of multi-level guided attention and multi-scale\nskip connections into the standard U-Net to improve\nsegmentation performance for various medical images.\n• With the co-cooperation of the transformer self attention\nand global spatial attention, our TransAttUnet can achieve\nthe strong ability to model contextual semantic informa-\ntion and global spatial relationships, which can guarantee\nthe consistency of feature representations and semantic\nembeddings.\n• Compared with the one-step cascade connection, the\nproposed residual or dense step-growth connections not\nonly can reduce the disturbance of noise, but also enable\nthe model to mitigate the loss of ﬁne details caused by\ndirectly upsampling with large scales.\n• Extensive experimental results on the ﬁve medical image\ndatasets demonstrate the superiorities and generalizability\nof the proposed TransAttUnet for automatic medical\nimage segmentation in comparison with state-of-the-art\nbaselines.\nThe rest of this paper is organized as follows. Section II\nreviews some related works of automatic medical image\nsegmentation, and Section III describes the proposed TransAt-\ntUnet. Next, the comprehensive experiments and visualization\nanalyses are reported in Section IV. Finally, Section V makes\na conclusion of this work.\nII. R ELATED WORK\nIn this section, we make a brief overview of two related\ngroups of works. The ﬁrst group mainly involves the state-\nof-the-art variants of U-Net for automatic medical image seg-\nmentation, while the second group introduces the applications\nof Transformer used in computer vision.\nA. Variants of U-Net\nMany efforts have been devoted to optimizing the structure\nof the U-Net in the ﬁeld of automatic medical image segmenta-\ntion. Typically, these variants can be roughly divided into two\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 3\n...\n c\nc\n...\nc\nRelevance\nEmbedding\nPosition\nEncoding\n Q\nK\nV\n+\nRelevance\nEmbedding\nW\nM\nN\nConv\nConv\nConv\nConv\nConv\nConv\nL\nBilinear Upsampling\n+\nL\nL\n Linear Projection\nMultiplication\nL\n Linear Projection\nMultiplication\n+\n+\nc\nc\nElement-wise Addition\nConcatenation\n+\nc\nElement-wise Addition\nConcatenation\nL\n Linear Projection\nMultiplication\n+\nc\nElement-wise Addition\nConcatenation\n  Max-pooling\n  Upsampling\nSkip-Connection \nReshape/Permute\n  Max-pooling\n  Upsampling\nSkip-Connection \nReshape/Permute\nInput Image\n Output Mask\nEncoder\n Decoder\nTSA\nGSA\nConv 3x3\nNorm.\nReLU\nConv 3x3\nNorm.\nReLU\nConv 3x3\nNorm.\nReLU\nMulti-scale skip connection\nFig. 2. Illustration of the proposed TransAttUnet for automatic medical image segmentation. (a) Both TSA and GSA mechanisms are embedding into the\nSAA module to model the long-range interactions and global spatial relationships. (b) The multi-scale skip connections between decoder blocks are designed\nto aggregate the downsampled features of varying semantic scales by progressive upsampling, concatenation, and convolution.\ncategories, i.e., attention guided approaches and multi-scale\ncontext approaches.\n1) Attention Guided Approaches: Many attention-guided\nmethods have been proposed to accurately segment the objects\nof interest in medical images of varying imaging modalities.\nTo enhance the feature learning ability of U-Net, Attention\nU-Net [10] utilizes the attention gate (AG) to suppress ir-\nrelevant feature responses and highlight salient features, in\norder to improve the model sensitivity and prediction accuracy.\nChannel-UNet [8] introduces the spatial channel convolution\nto determine the optimal mapping relationship among spa-\ntial information from different patches. By simultaneously\nrecalibrating the different types of features at the spatial and\nchannel levels, SCAU-Net [20] can guide the model to neglect\nirrelevant information and focus on more discriminant regions\nof the image. Similarly, 3D attention U-Net [21] applied the\nchannel and spatial attention into the decoder subnetwork\nof 3D U-Net [22] to segment brain tumors in MRI images.\nXLSor [23] makes use of the criss-cross attention module\nto aggregate long-range pixel-wise contextual information in\nboth horizontal and vertical directions for lung segmentation\nin chest X-ray images. Residual Attention U-Net [24] applies\nthe soft attention mechanism to improve the capability of the\nmodel to distinguish a variety of symptoms of the COVID-\n19 in chest CT images. However, the direct application of\nthe attention mechanism might result in the loss of available\nfeature representations, especially when the judgment for the\nregion of interest goes wrong, which is inadequate to meet the\nneeds of an ideal model.\n2) Multi-Scale Context Approaches : To make full use\nof the multi-scale context information, prior works have at-\ntempted to combine the low-level features from the shal-\nlow layers with the high-level feature of the deep layers\nfor retaining the detailed image information. For example,\nUNet++ [9] proposes a highly ﬂexible multi-scale feature\nfusion scheme by aggregating features of varying semantic\nscales with redesigned skip connections. U 2-Net [25] exploits\nResidual U-blocks to capture intra-stage contextual informa-\ntion and directly cascades the inter-block feature mapping of\nthe decoder subnetwork to mitigate the loss of ﬁne details.\nMA-Unet [11] establishes a multi-scale mechanism to directly\naggregating global contextual information of different scales\nfrom intermediate layers as ﬁnal feature representations, and\nit also utilizes additional attention mechanisms to improve the\nprediction accuracy for medical image segmentation. However,\none-step cascade connection may ignore some valuable details\nin the large-scale upsampling process, which still suffers from\nthe challenge of information loss.\nB. Transformer in CV\n1) Transformer for Various Vision Tasks: With the devel-\nopment of Transformer used in various NLP tasks, more and\nmore Transformer-based methods are developed for CV tasks.\nIn particular, ViT [26] presents a pure self-attention Vision\nTransformer for image recognition, which is the ﬁrst attempt of\nthe transformer-based method to surpass the traditional CNN-\nbased works. By introducing Transformer into CNNs, DETR\n[27] proposes a fully end-to-end object detector to eliminate\nthe hand-designed components in CNN-based object detectors.\nSubsequently, SETR [28] replaces the encoder in the standard\nFCN architecture with Transformer and achieves encouraging\nperformance on the natural image segmentation task.\n2) Transformer for Medical Image Segmentation : In-\nspired by ViT, TransUNet [14] adopts Transformer as encoder\nand applies it with U-Net to enhance the performance of\nmedical image segmentation tasks. TransFuse [29] combines\nTransformer and CNN in a parallel style to improve efﬁciency\nfor modeling global context information. Similarly, MCTrans\n[30] utilizes Transformer to incorporate rich context modeling\nand semantic relationship mining for accurate biomedical\nimage segmentation. Besides, MedT [15] proposes a Gated\nAxial-Attention model that utilizes Transformer based gated\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 4\nposition-sensitive axial attention mechanism for medical image\nsegmentation. Unlike these works, the proposed TransAttUnet\naims to investigate the feasibility of applying Transformer\nto overcome the inability of U-Net to model long-range\ncontextual interactions.\nIII. M ETHODOLOGY\nThis section mainly introduces the proposed TransAttUnet.\nFirstly, we give a brief overview of TransAttUnet. Then,\nwe present the principles and the structure of TransAttUnet,\nfollowed by a detailed description of each component. Finally,\nthe uniﬁed loss function used in our TransAttUnet is presented.\nA. Overview of TransAttUnet\nAn input medical image X ∈RC×H×W , where C is the\nnumber of channels and H×W represents the spatial resolu-\ntion of image instance. The purpose of the automatic medical\nimage segmentation task is to predict the corresponding pixel-\nwise semantic label maps with the size of H ×W. The\ngeneral learning framework is illustrated in Fig. 2. As with the\nprevious works, the proposed TransAttUnet is also built on the\nstandard encoder-decoder U-shaped architecture, as shown in\nFig. 2. To overcome the limitations mentioned in Section I,\nTransAttUnet aims to leverage multi-level complementary\nself-aware attention components, as well as multi-scale skip\nconnections [9] [11] to further improve the semantic segmenta-\ntion quality of medical images. Compared with the standard U-\nNet, the SAA module in TransAttUnet can beneﬁt greatly from\nboth TSA and GSA mechanisms to capture the long-range\ncontextual information, improving the representation ability\nof the encoder semantic features. Furthermore, the multi-\nscale skip connections used in TransAttUnet are designed to\nachieve the residual and dense shortcut connections between\nthe intermediate layers of different semantic scales, which can\naggregate the contextual information for multi-scale prediction\nfusion.\nB. Self-aware Attention Module\nFirstly, the proposed TransAttUnet augments the standard\nU-Net with a robust and effective self-aware attention module,\nthat is positioned at the bottom of U-shaped architecture as\nthe bridge between the encoder and decoder subnetworks.\nThis duple mainly contains two independent self-attention\nmechanisms, i.e., transformer self attention (TSA) and global\nspatial attention (GSA), which help capture the wider and\nricher contextual representations.\n1) Transformer Self Attention: The TSA component is\nbuilt on the multi-head self-attention function from Trans-\nformer, which allows the model to jointly attend to semantic\ninformation from global representation subspaces. To capture\nthe contextual information of absolute and relative position, the\nTSA component ﬁrst introduces the learnt positional encoding\nto the input of the encoder features, which can be shared across\nall attention layers for a given query/key-value sequence. The\nmulti-head attention mechanism can be calculated separately\nin multiple single attention heads before being combined\nthrough another embedding. The pipeline of transformer self\nattention component is depicted with the green part in Fig. 2.\nSpeciﬁcally, the encoder features F ∈ Rc×h×w is em-\nbedding into three inputs, including the matrix of queries\nQ ∈ Rc×(h×w), the matrix of keys K ∈ Rc×(h×w), and\nV ∈Rc×(h×w).\nQ = F ·Wq,K = F ·Wk,V = F ·Wv, (1)\nwhere Wq, Wk, and Wv are the embedding matrices of dif-\nferent linear projections. Then, a scaled dot-product operation\nwith softmax normalization between Q and the transposed\nversion of K is conducted to generate the matrix of contextual\nattention map A ∈Rc×c, in which represents the similarities\nof given elements from Q with respect to global elements of\nK. To obtain the aggregation of values weighted by attention\nweights, the contextual attention map A would be multiplied\nby V . The multi-head attention can be formulated as:\nTSA(Q,K,V ) =softmax(QKT\n√dk\n)V , (2)\nwhere √dk is the dimensionality of query/key-value sequence.\nFinally, we reshape the optimized feature maps to obtain the\nﬁnal output of TSA, i.e., Ftsa ∈Rc×h×w.\n2) Global Spatial Attention: Meanwhile, the SAA module\nalso utilizes the GSA component to selectively aggregate\nglobal context to the learned features and encode broader\ncontextual positional information into local features, which\ncan improve the intra-class compact and optimize the feature\nrepresentations. The architecture of global spatial attention is\nshown with the blue part in Fig. 2.\nFirstly, two different types of convolutional operations are\napplied to the encoder features Fen to generate the feature\nmaps Fc′\np ∈ Rc′×h×w and Fc\np ∈ Rc×h×w, c′ = c/8.\nSubsequently, Fc′\np is reshaped and transposed into feature\nmaps M ∈ R(h×w)×c′\nand N ∈ Rc′×(h×w), while Fc\np\nis transposed in to W ∈ Rc×(h×w), respectively. Then, a\nmatrix multiplication operation with softmax normalization is\nperformed on M and N, resulting in the position attention\nmaps B ∈R(h×w)×(h×w), which can be deﬁned as:\nBi,j = exp(Mi ·Nj)∑n\ni=1 exp(Mi ·Nj), (3)\nwhere Bi,j measures the impact of ith position on jth position,\nand n = h×w is the number of pixels. After that, W is\nmultiplied with B, and the resulting feature at each position\ncan be formulated as:\nGSA(M,N,W)p =\nh×w∑\nq=1\n(WqBp,q). (4)\nSimilarly, we reshape the resulting features to generate the\nﬁnal output of GSA, i.e., Fgsa ∈Rc×h×w.\n3) Attention Embedding Fusion: To make full use of the\nobtained contextual information and spatial relationships, a\nweighted combination scheme for the original and attention\nfeature embeddings is used at the end of the SSA module,\nwhich is deﬁned as:\nFSAA = λ1Ftsa + λ2Fgsa + Fen, (5)\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 5\nF4\nF1\nF3\nF2\nFCC\nC\n(a) Cascade connection\nF4\nF1\nF3\nF2\nFCC\nC\nCC\nC\nCC\nC (b) Residual connection\nF4\nF1\nF3\nF2\nFCC\nC\nCC\nC\nCC\nC (c) Dense connection\nFig. 3. The comparison of the one-off cascade connections and our residual or dense step-growth connections. (a) Cascade connection [25] [11] [14]: The\nfeature maps from all scales are directly concatenated to form a uniﬁed tensor; (b) Residual connection: For each decoder block, the input and output features\nof the current block are concatenated and applied as the input of the subsequent block. (c) Dense connection: For each decoder block, the upsampling features\nof previous decoder blocks are considered as inputs, and its own feature maps are used as inputs into all subsequent blocks.\nwhere λ1 and λ2 are the scale parameters that controls the\nimportance of the self attention maps and spatial attention\nmaps, respectively. Both of them are initialized as 0 and are\ngradually increased to assign more weight to the important\nfeatures. In this way, we can further optimize the feature\nrepresentations with semantic consistency.\nC. Multi-scale Skip Connection\nNotably, many advanced works [25] [11] [14] have demon-\nstrated the effectiveness of multi-scale feature fusion in encod-\ning global and local contexts. Speciﬁcally, the multi-scale skip\nconnection scheme aims to aggregate the features of varying\nsemantic scales with a series of transition operations, including\nupsampling, concatenation, and convolution. Inspired by previ-\nous works, three different types of connections, Cascade Con-\nnection, Residual Connection, and Dense Connection shown\nin Fig. 3 are investigated in this study.\n1) Cascade Connection: The feature maps of varying se-\nmantic scale from all the blocks are up-sampled to a common\nresolution through bilinear interpolation, and all of them would\nbe directly concatenated into a uniﬁed feature representation,\nwhich is formulated as:\nF = fn(υ1(F1) ⊕υ2(F2) ⊕... ⊕Fn), (6)\nwhere ⊕denotes concatenation operations,υn(·) and fn(·) are\nthe upsampling and mixed convolution operations in nth stage,\nrespectively.\n2) Residual Connection: For each decoder block, the input\nfeature maps are up-sampled to the resolution of outputs\nthrough bilinear interpolation, and then concatenated with the\noutput feature maps as the inputs of the subsequent block,\nwhich is formulated as:\nFn = fn((Fn) ⊕υn−1(Fn−1)). (7)\n3) Dense Connection: The upsampling features of previous\nencoder blocks are integrated as the inputs of the current\nblock, and the output feature maps are used as inputs into\nall subsequent blocks, which is formulated as:\nFn = fn(υ1(F1) ⊕υ2(F2) ⊕... ⊕υn−1(Fn−1)). (8)\nIn particular, the proposed TransAttUnet focuses on two\ndifferent multi-scale skip connection schemes, i.e., the residual\nconnection and dense connection, to guide the upsampling\nprocess in the decoder subnetwork. Compared to the exist-\ning works that only using the one-off cascade connection,\nthe residual or dense step-growth connections can gradually\naggregate multiple decoder features of varying semantic scales\nto generate the most discriminative feature representations. In\nthis way, the proposed TransAttUnet not only can mitigate\nthe loss of ﬁne details caused by over-upsampling, but also\nalleviate the problems of vanishing-gradient and overﬁtting.\nD. Loss Function\nIn the training phase, the proposed TransAttUnet is trained\nwith an objective function in an end-to-end manner. The\nobjective function is calculated by the Sorensen-Dice loss and\nBinary Cross-Entropy function with a pixel-wise soft-max over\nthe ﬁnal feature maps, which can be expressed as:\nLBCE =\nt∑\ni=1\n(yi log(pi) + (1−yi) log(1−pi)),\nLDice = 1−\n∑t\ni=1 yipi + ε∑t\ni=1 yi + pi + ε\n,\nL = α·LBCE + β·LDice,\n(9)\nwhere t is the total number of pixels in each image, yi\nrepresents the ground-truth value of the ith pixel, and pi is\nthe conﬁdence score of the ith pixel in prediction results. In\nour experiment, α= β = 0.5, and ε= 10−6.\nIV. E XPERIMENTS\nIn this section, we evaluate the performances of the\nproposed TransAttUnet framework on multiple benchmark\ndatasets by comparing it with the state-of-the-art baselines.\nNext, we make a detailed discussion for the ablation studies.\nFinally, visualization analysis of decoder stages is presented.\nA. Datasets\nTo verify the effectiveness and efﬁciency of our TransAt-\ntUnet, we ﬁrst conducted comparative experiments for the task\nof skin lesion segmentation on ISIC-2018 [31] dataset, and\nlung ﬁeld segmentation on the combination of the JSRT [32],\nMontgomery [33], and NIH [23] datasets. Moreover, the pro-\nposed TransAttUnet is also evaluated on the Clean-CC-CCII\ndataset [34], 2018 Data Science Bowl (Bowl) dataset [35],\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 6\nand the Gland Segmentation (GlaS) dataset [19]. Typically, the\nimages from the CC-CCII, Bowl, and GLAS datasets might\ncontain multiple segmenting objects with varying sizes and\ntextures, which greatly increases the segmentation difﬁculty\nand complexity.\n1) ISIC-2018: The ISIC-2018 is a large-scale dataset of\ndermoscopy images to develop the applications of automated\ndiagnosis of melanoma from dermoscopic images. It is pro-\nvided for ISIC-2018 challenge [31] and contains three tasks,\nincluding lesion segmentation, lesion attribute detection, and\ndisease classiﬁcation. In particular, this paper focuses on the\ntask of lesion segmentation from dermoscopic images by\nvarious types of dermoscopy. It includes 2596 images with\nthe corresponding annotations and these images are randomly\nsplit into 2076 images for training and 520 images for testing.\n2) JSRT, Montgomery & NIH: Three datasets of frontal\nchest X-ray images, JSRT, Montgomery, and NIH are involved\nin our experiments for automatic lung ﬁeld segmentation.\nThe JSRT dataset comprises 247 chest X-ray images, among\nwhich 154 images are abnormal with pulmonary nodule and\n93 images are normal. By contrast, the Montgomery dataset\ncontains 138 chest X-ray images, including 80 normal patients\nand 58 patients with manifested tuberculosis. Different from\nthe JSRT and Montgomery, the NIH dataset contains 178 chest\nX-ray images with various severity of lung diseases, which\ncan hugely complicate the task of lung ﬁeld segmentation.\nFollowing the settings of previous works [23], we combine\nthese datasets and randomly split them into 407 images for\ntraining and 178 images for testing.\n3) Clean-CC-CCII: As a publicly available chest CT\ndataset used in the ﬁeld of automated COVID-19 diagnosis,\nthe Clean-CC-CCII dataset contains thousands of annotated\nCT scans from 2,698 patients. In particular, it also provides\nresearchers with high-quality annotation of infection marks\nto develop a robust model of COVID-19 pneumonia lesion\nsegmentation. In our experiment, 260 CT slices with pixel-\nwise annotations of COVID-19 pneumonia lesion are selected\nand randomly split into two subsets, i.e., a training set of 200\nimages and a test set of 60 images.\n4) Bowl: The Bowl dataset is established for the develop-\nment of robust automatic nucleus segmentation algorithms. It\nprovides participants with a training set of 671 nuclei images\nalong with pixel-wise masks for the nuclei and a test set\nof 3020 images, which are extracted from 15 diverse image\nsets of biological experiments. Note that each image contains\ndozens of nuclei with different sizes. Due to the lack of\nthe annotation masks of the test set, we only evaluate the\nperformance of the proposed method based on the training\nset. Following the settings of the existing work [36], the\ntraining set is split into three subsets: 80% for training, 10%\nfor validation, and 10% for testing.\n5) GlaS: The GlaS dataset is published by the Colon\nHistology Images Challenge Contest of MICCAl’2015 that\naims to improve methods for quantifying the morphology\nof glands. It consists of 165 colon histology images derived\nfrom 16 H&E stained histological sections of stage T3 or T4\ncolorectal adenocarcinoma from different patients. In partic-\nular, each sample is processed in the laboratory on different\noccasions, resulting in high inter-subject variability in both\nstain distribution and tissue structure. In our experiments, the\nGlaS dataset is split into two subsets: 85 images for training\nand 80 images for testing, which is consistent with the previous\nworks [37] [15].\nB. Experimental Settings\n1) Baselines: In our experiments, three versions of\nTransAttUnet, i.e., TransAttUnet C, TransAttUnet D and\nTransAttUnet R are evaluated. In particular, TransAttUnet C\nrepresents that the decoder blocks of TransAttUnet are linked\nwith the one-off cascade connections [25] [11] [14]. By con-\ntrast, TransAttUnet D utilizes the dense operations to connect\nthe decoder blocks, while TransAttUnet R utilizes the residual\noperations to connect the decoder blocks. In addition to the\nvanilla U-Net [6], three broad approaches are involved in our\ncomparative experiments as baselines, i.e., the attention-guided\napproaches, multi-scale context approaches, and Transformer-\nbased approaches.\n• Attention-guided approaches: Many advanced attention\nguided models, i.e., Attention U-Net [10], Attention R2U-\nNet [38], Channel-UNet [8], XLSor [23], and FANet\n[39], and PraNet [40], are introduced to compare with\nthe proposed TransAttUnet.\n• Multi-scale context approaches : Meanwhile, several\nmulti-scale context models are used as the major con-\ntenders, including Unet++ [9], R2U-Net [38] ResUNet\n[41], ResUNet++ [42], BCDU-Net [43], KiU-Net [37],\nand DoubleU-Net [36].\n• Transformer-based approaches: Moreover, some state-\nof-the-art Transformer-based models, i.e., MedT [15],\nMCTrans [30], Swin-Unet [44] and SegFormer [45] are\nalso considered as important baselines.\n2) Implementation Details: To make a fair comparison\nwith the existing works, the input images from Clean-CC-\nCCII, JSRT, Montgomery, and NIH are resized to 512 ×512\nfor training and test, while the images of ISIC-2018 and Bowl\nare resized to 256×256. Besides, the input images provided by\nGlaS are resized to 128×128 in a uniﬁed manner. Note that, we\nemploys 8 parallel attention heads in TSA module. Moreover,\nwe adopt the stochastic gradient descent (SGD) [46] optimizer\nwith momentum 0.9 and weight decay 0.0001 to optimize the\ntraining process. Furthermore, the proposed TransAttUnet is\nimplemented by using the deep learning toolbox PyTorch [47],\nand all the experiments run on 1 Nvidia Titan XP GPU with 12\nGB memory. The proposed TransAttUnet framework is trained\nfor 100 epochs with a batch size of 4. Besides, the initial\nlearning rate is 0.0001, which decays by a factor of 10 for\nevery 40 epochs. In our experiments, the results are given as\nthe probability maps directly outputted by the models, which\ncan be binarized with a threshold of 0.5 to get the binary\nmasks for performance evaluation.\n3) Evaluation Metrics: In our experiments, we adopt the\nmean Dice coefﬁcient (DICE) [48] as the key evaluation metric\nto measure the extent of similarity between the predicted mask\nand ground truth. Besides, four additional criteria, i.e., mean\nIntersection over Union (IoU), accuracy (ACC), recall (REC),\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 7\nand precision (PRE) scores are calculated pixel-wisely and\nused to evaluate the quantitative segmentation performance.\nThese metrics are associated with four values, i.e., true-\npositive (TP), true-negative (TN), false-positive (FP), and\nfalse-negative (FN),\nDice = 2 × TP\n2 × TP + FP + FN ,\nIoU = TP\nTP + FP + FN ,\nAccuracy = TP + TN\nTP + TN + FP + FN ,\nRecall = TP\nTP + FN ,\nPrecision = TP\nTP + FP ,\n(10)\nC. Experimental Results\n1) Evaluation on Skin Lesion Segmentation: To evaluate\nthe effectiveness of the proposed TransAttUnet, we ﬁrst con-\nduct the experiments on the ISIC-2018 dataset for the task of\nskin lesion segmentation. The comparison results of evaluation\nmetrics are presented in TABLE I, and the corresponding\nquantitative results are illustrated in Fig. 4(a).\nFrom Table I, we have the following observations: 1) Com-\nparing with the vanilla U-Net (67.40%), we can observe that\nthe attention-guided models, such as Channel-UNet (84.82%)\nand PraNet (87.46%), are obviously superior to the vanilla U-\nNet (67.40%) with the guidance of various attentional mecha-\nnisms. These improvements can demonstrate that attention-\naware mechanism can play a vital role in medical image\nsegmentation. 2) By aggregating the context information from\nvarying semantic scales, it is easy to see that the multi-scale\ncontext approaches, i.e., BCDU-Net (85.10%) and DoubleU-\nNet (89.62%), have the powerful ability in dealing with the\ntask of skin lesion segmentation, which proves the effective-\nness of the multi-scale context fusion scheme. 3) By con-\ntrast, the recent Transformer-based framework, i.e, MCTrans\n(90.35%), is superior to the above works. It can demonstrate\nthe superiority of the Transformer in learning the long-range\ndependencies of different pixels. 4) Although MCTrans can\nachieve reliable performance, the proposed TransAttUnet still\noutperforms MCTrans (90.74% vs. 90.35%), which veriﬁes\nthe efﬁcacy of jointly exploring multi-level guided attention\nand multi-scale skip connection. Moreover, our TransAttUnet\nachieves the highest scores on almost all evaluation metrics\nand shows a more precise and ﬁne segmentation output of\nthe proposed network than the existing baselines, as shown in\nFig. 4(a). 5) Both the diagnostic sensitivity of the proposed\nTransAttUnet R and TransAttUnet D outperform TransAt-\ntUnet C, which demonstates that the residual or dense step-\ngrowth connections can better aggregate multiple decoder\nfeatures of varying semantic scales rather than the one-\noff cascade connections. Furthermore, the proposed TransAt-\ntUnet R surpasses the DICE score of TransAttUnet D by\n0.6%. That is probably because the dense connection used\nin TransAttUnet might lead to a fair amount of redundant\ninformation. Therefore, these comparative results demonstrate\nTABLE I\nCOMPARISONS WITH THE STATE -OF-THE -ART BASELINES ON THE\nISIC-2018 DATASET. ALL RESULTS WERE ANALYSED IN PERCENTAGE (%)\nTERMS . RESULTS OF THE MODEL WITH “*” ARE REIMPLEMENTED BY THE\nRELEASED SOURCE CODES . THE “-” DENOTES THE CORRESPONDING\nRESULT IS NOT PROVIDED . FOR EACH COLUMN , THE BEST AND SECOND\nBEST RESULTS ARE HIGHLIGHTED IN RED AND BLUE , RESPECTIVELY .\nMethod Year DICE IoU ACC REC PRE\nU-Net [6] 2015 67.40 54.90 - 70.80 -\nAttention U-Net [10] 2018 66.50 56.60 - 71.70 -\nR2U-Net [38] 2018 67.90 58.10 - 79.20 -\nAtt R2UNet [38] 2018 69.10 59.20 - 72.60 -\nResUNet* [42] 2019 79.15 70.15 92.28 82.43 84.77\nChannel-UNet* [8] 2019 84.82 75.92 94.10 94.01 81.04\nBCDU-Net [43] 2019 85.10 - - 7850 -\nFANet [39] 2021 87.31 80.23 - 86.50 92.35\nPraNet* [40] 2021 87.46 80.23 95.37 91.28 87.59\nDoubleU-Net [36] 2020 89.62 82.12 - 87.80 94.59\nSwin-Unet* [44] 2021 89.72 82.90 - 90.32 92.04\nSegFormer* [45] 2021 90.24 83.60 - 91.12 92.10\nMCTrans [30] 2021 90.35 - - - -\nTransAttUnet C - 89.25 81.46 95.06 89.90 91.59\nTransAttUnet D - 90.14 83.04 96.14 90.42 92.17\nTransAttUnet R - 90.74 83.80 96.38 90.93 92.42\nthe effectiveness of the proposed TransAttUnet for automated\nskin lesion segmentation.\n2) Evaluation on Lung Field Segmentation: Moreover,\nwe evaluate our TransAttUnet to solve the task of lung ﬁeld\nsegmentation from the chest X-ray images on the combination\nof JSRT, Montgomery, and NIH datasets. The comparison\nresults of evaluation metrics are presented in TABLE II and the\ncorresponding quantitative results are illustrated in Fig. 4(b).\nBased on the experimental results in Table II, we have some\nnew observations as follows: 1) Firstly, we can observe that the\ncorresponding scores of the evaluation metrics are quite close.\nThat is because that the testing chest X-ray images provided\nby the JSRT and Montgomery datasets are generally normal,\nand the noise interference caused by various lesion features\nis reduced in semantic segmentation. 2) Nevertheless, the\nproposed TransAttUnet achieves better evaluation results than\nprevious state-of-the-art models and yields the highest DICE\nscore of 98.88% for lung ﬁeld segmentation, which demon-\nstrates the efﬁcacy of the designed TransAttUnet. As shown\nin Fig. 4(b), the segmentation outputs of our TransAttUnet\nare more close to the ground truths in comparison with other\nbaselines. 3) In particular, the performance of TransAttUnet\nis clearly superior to the previous baselines, especially for U-\nNet (96.17%) with the improvement of 2.71% in terms of the\nDICE score. This improvement demonstrates that the proposed\nTransAttUnet beneﬁts greatly from encoder-decoder guided\nattention and multi-scale skip connections, which help learn\nthe global contextual information and discriminative features\nto distinguish the lung ﬁeld from the surrounding structures. 4)\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 8\nInput U-Net Attention U-Net PraNet DoubleU-Net TransAttUnet(Ours) Ground Truth\n(a) Quantitative results for skin lesion segmentation.\nInput U-Net Attention U-Net Unet++ PraNet TransAttUnet(Ours) Ground Truth\n(b) Quantitative results for lung ﬁeld segmentation.\nPraNet\nPraNet\n Attention U-Net Unet++\nAttention U-Net Unet++\nInput U-Net TransAttUnet(Ours) Ground Truth\n(c) Quantitative results for pneumonia lesion segmentation.\nFig. 4. Comparison of quantitative results between the state-of-the-art baselines and the proposed TransAttUnet on the (a) ISIC 2018 dataset, (b) the\ncombination of JSRT, Montgomery, and NIH datasets, (c) the Clean-CC-CCII dataset, respectively. To make better visualize the differences between lung\nsegmentation results and ground truths, we highlight the key region with the appropriate boxes.\nLikewise, the proposed TransAttUnet consistently outperforms\nthe recent works, i.e., FANet (98.28%) and PraNet (98.36%),\nwhich can verify the abilities of TransAttUnet in improving\nthe segmentation quality of the details.\n3) Evaluation on Pneumonia Lesion Segmentation: Due\nto the epidemic of COVID-19 in recent years, it has been\na hot topic in the ﬁeld of medical image analysis. Note\nthat it is difﬁcult to accurately and completely identify the\nsegmenting objects of pneumonia lesions from chest CT due\nto the complex textures and shapes. Thus, we also perform the\nexperiments on the Clean-CC-CCII dataset to further ensure\nthat the proposed TransAttUnet is appropriate for pneumonia\nlesion segmentation. The comparison results of evaluation\nmetrics are presented in TABLE III and the corresponding\nquantitative results are illustrated in Fig. 4(c).\nOn the basis of the above results, we have the follow-\ning observations: 1) It can be observed that the proposed\nTransAttUnet consistently outperforms the previous baselines\nand yields the highest DICE score of 86.57%, which again\ndemonstrates the superiorities of our TransAttUnet method. 2)\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 9\nTABLE II\nCOMPARISONS WITH THE STATE -OF-THE -ART BASELINES ON THE JSRT\nAND MONTGOMERY DATASET . ALL RESULTS WERE ANALYSED IN\nPERCENTAGE (%) TERMS . RESULTS OF THE MODEL WITH “*” ARE\nREIMPLEMENTED BY THE RELEASED SOURCE CODES . THE “-” DENOTES\nTHE CORRESPONDING RESULT IS NOT PROVIDED . FOR EACH COLUMN ,\nTHE BEST AND SECOND BEST RESULTS ARE HIGHLIGHTED IN RED AND\nBLUE , RESPECTIVELY .\nMethod Year DICE IoU ACC REC PRE\nU-Net [6] 2015 96.17 92.71 98.21 94.94 97.50\nXLSor [23] 2019 97.54 - - 97.40 97.73\nResUNet* [41] 2020 97.12 94.45 98.64 96.61 97.70\nAttention U-Net* [10] 2018 97.59 95.31 98.81 98.82 96.41\nSwin-Unet* [44] 2021 97.67 95.48 98.71 95.42 98.36\nUnet++* [9] 2018 97.84 95.80 98.93 99.28 96.47\nResUNet++* [42] 2019 97.92 95.95 98.96 98.68 98.48\nFANet* [39] 2021 98.28 96.64 99.12 98.04 98.54\nPraNet* [40] 2021 98.36 96.80 99.17 98.26 98.48\nTransAttUnet C - 98.14 96.37 99.08 98.56 98.15\nTransAttUnet D - 98.56 97.18 99.27 98.88 98.26\nTransAttUnet R - 98.88 97.82 99.41 98.74 99.04\nTABLE III\nCOMPARISONS WITH THE STATE -OF-THE -ART BASELINES ON THE\nCLEAN -CC-CCII DATASET. ALL RESULTS WERE ANALYSED IN\nPERCENTAGE (%) TERMS . RESULTS OF THE MODEL WITH “*” ARE\nREIMPLEMENTED BY THE RELEASED SOURCE CODES . THE “-” DENOTES\nTHE CORRESPONDING RESULT IS NOT PROVIDED . FOR EACH COLUMN ,\nTHE BEST AND SECOND BEST RESULTS ARE HIGHLIGHTED IN RED AND\nBLUE , RESPECTIVELY .\nMethod Year DICE IoU ACC REC PRE\nU-Net* [6] 2015 82.39 71.25 99.19 78.99 87.62\nPraNet* [39] 2021 82.40 71.26 99.14 82.53 83.74\nResUNet* [42] 2019 82.90 71.96 99.17 83.60 83.35\nAttention U-Net* [10] 2018 83.92 73.40 99.24 83.23 86.07\nSwin-Unet [44] 2021 84.47 76.59 94.95 84.60 84.35\nUnet++* [9] 2018 84.64 74.43 99.30 81.98 86.52\nPraNet* [40] 2021 84.82 76.26 93.91 85.60 85.08\nSegFormer [45] 2021 84.96 76.56 94.31 85.76 85.63\nResUNet++* [42] 2019 85.17 75.36 99.27 86.40 85.61\nTransAttUnet C - 85.51 75.56 99.23 83.54 85.52\nTransAttUnet D - 86.08 76.25 99.32 86.91 86.23\nTransAttUnet R - 86.57 77.16 99.38 85.95 88.47\nBoth TransAttUnet D and TransAttUnet R are able to obtain\nbetter segmentation performance than TransAttUnet C and the\nprevious baselines. By contrast, the proposed TransAttUnet R\nachieves a better performance than TransAttUnet D (86.57%\nvs. 86.08%), which further veriﬁes the superiorities of the\nresidual connection in comparison with the dense connection.\n3) However, some reliable models, such as PraNet, achieve\nthe relatively low Dice score in the COVID-19 pneumonia\nlesion segmentation task. We suspected that it might suffer\nTABLE IV\nCOMPARISONS WITH THE STATE -OF-THE -ART BASELINES ON THE 2018\nDATA SCIENCE BOWL DATASET . ALL RESULTS WERE ANALYSED IN\nPERCENTAGE (%) TERMS . RESULTS OF THE MODEL WITH “*” ARE\nREIMPLEMENTED BY THE RELEASED SOURCE CODES . THE “-” DENOTES\nTHE CORRESPONDING RESULT IS NOT PROVIDED . FOR EACH COLUMN ,\nTHE BEST AND SECOND BEST RESULTS ARE HIGHLIGHTED IN RED AND\nBLUE , RESPECTIVELY .\nMethod Year DICE IoU ACC REC PRE\nU-Net [6] 2015 75.73 91.03 - - -\nFANet* [39] 2021 81.03 71.08 95.59 80.62 82.31\nChannel-UNet* [8] 2019 87.55 79.75 96.27 90.70 87.86\nUnet++ [9] 2018 89.74 92.55 - - -\nResUNet* [41] 2020 89.91 82.44 97.05 90.00 90.84\nAttention U-Net [10] 2018 90.83 91.03 - - 91.61\nPraNet* [40] 2021 90.85 85.34 - - 90.94\nDoubleU-Net [36] 2020 91.33 84.07 - 64.07 94.96\nTransAttUnet C - 90.04 84.36 97.05 90.03 91.23\nTransAttUnet D - 91.34 84.62 97.37 91.86 91.53\nTransAttUnet R - 91.62 84.98 97.46 91.85 91.93\nTABLE V\nCOMPARISONS WITH THE STATE -OF-THE -ART BASELINES ON THE GLAS\nDATASET. ALL RESULTS WERE ANALYSED IN PERCENTAGE (%) TERMS .\nRESULTS OF THE MODEL WITH “*” ARE REIMPLEMENTED BY THE\nRELEASED SOURCE CODES . THE “-” DENOTES THE CORRESPONDING\nRESULT IS NOT PROVIDED . FOR EACH COLUMN , THE BEST AND SECOND\nBEST RESULTS ARE HIGHLIGHTED IN RED AND BLUE , RESPECTIVELY .\nMethod Year DICE IoU ACC REC PRE\nU-Net [6] 2015 75.73 91.03 - - -\nResUNet* [41] 2020 80.88 69.11 81.49 85.11 80.01\nMedT [15] 2021 81.82 69.61 - - -\nUnet++ [9] 2018 81.83 69.61 - - -\nAttention U-Net [10] 2018 81.59 70.06 - - -\nKiU-Net [37] 2020 83.25 72.78 - - -\nFANet [39] 2021 84.67 74.30 - - -\nSwin-Unet [44] 2021 86.70 77.32 - 89.00 86.12\nSegFormer [45] 2021 87.36 79.71 - 85.56 86.53\nTransAttUnet C - 87.35 79.55 95.29 87.82 86.46\nTransAttUnet D - 88.37 80.08 88.47 89.19 88.49\nTransAttUnet R - 89.11 81.13 89.02 90.08 88.95\nfrom the overﬁtting problem due to the lack of training data.\n4) Besides, the proposed TransAttUnet outperforms the recent\nTransformer-based work, i.e., Swin-Unet (84.47%) and Seg-\nFormer (84.96%), which demonstrates the powerful ability of\nTransAttUnet in medical image segmentation. 5) Futhermore,\nFig. 4(c) shows that the superiority of our TransAttUnet over\nthe other methods when dealing with lesions at different scales,\nwhich proves the availability of the proposed TransAttUnet for\nCovid-19 pneumonia lesion segmentation.\n4) Evaluation on Nuclei Segmentation: In this part, we\nevaluate the proposed TransAttUnet on the 2018 Data Science\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 10\nU-Net++\n Attention U-Net DoubleU-Net\nInput U-Net TransAttUnet(Ours) Ground Truth\n(a) Quantitative results for nuclei segmentation.\nU-Net++ Attention U-Net KiU-NetInput U-Net TransAttUnet(Ours) Ground Truth\n(b) Quantitative results for gland segmentation.\nFig. 5. Comparison of quantitative results between the state-of-the-art baselines and the proposed TransAttUnet on the (a) 2018 Data Science Bowl dataset,\n(b) the GLAS dataset, respectively. To make better visualize the differences between lung segmentation results and ground truths, we highlight the key region\nwith the appropriate boxes.\nBowl dataset for multiple nuclei segmentation. The compari-\nson results of evaluation metrics are presented in TABLE IV\nand the corresponding quantitative results are illustrated in\nFig. 5(a).\nBased on the above comparative results, we have the fol-\nlowing observations: 1) Compared with the vanilla U-Net,\nthe use of encoder-decoder guided attention and multi-scale\nskip connections can lead to the further improvement of\nsegmentation quality, improving with the DICE score from\n75.76% to 91.92%. 2) Consistently, the proposed TransAttUnet\nis superior to the existing competitors of the attention-guided\nand multi-scale context approaches, such as Attention U-Net\n(91.62% vs. 90.93%) and ResUNet (91.62% vs. 89.91%).\n3) It can be seen that our TransAttUnet yields the highest\nscore on almost all evaluation metrics. Although DoubleU-Net\noutperforms in terms of precision, our TransAttUnet produces\nthe higher scores on other metrics, especially for the IoU score\nof 84.98% with the improvement of 0.91%. 4) As illustrated in\nFig. 5(a), we can observe that our TransAttUnet can effectively\ncapture the boundaries of cell nuclei and generate better\nsegmentation prediction. These comparative results can prove\nthe efﬁcacy of the proposed method for identifying multiple\nsegmenting objects.\n5) Evaluation on Gland Segmentation: Furthermore, we\nalso conduct comparative experiments on the GLAS dataset\nto demonstrate the validity of the proposed TransAttUnet for\nquantifying the morphology of glands. The comparison results\nof evaluation metrics are presented in TABLE V and the\ncorresponding quantitative results are illustrated in Fig. 5(b).\nAccording to these experimental results, we have the fol-\nlowing observations: 1) Note that GLAS is a small dataset that\ncontains multiple complex objects of interest. By incorporating\nwith the encoder-decoder guided attention and multi-scale skip\nconnections, our TransAttUnet achieves a better performance\nthan the existing baselines and contributes a new state-of-\nthe-art technique for automatic gland segmentation. 2) In\nparticular, the proposed TransAttUnet outperforms the recent\nTransformer-based work, i.e., MedT (81.82%), Swin-Unet\n(86.70%) and SegFormer (87.36%), which again demonstrates\nthe powerful ability of TransAttUnet in medical image seg-\nmentation. 3) Moreover, we can clearly see that both TransAt-\ntUnet D and TransAttUnet R clearly outperform TransAt-\ntUnet C and the previous state-of-the-art method, i.e., KiU-\nNet. In particular, TransAttUnet D achieves the improvement\nof 5.86% and 8.85% in terms of DICE and IoU scores, respec-\ntively. According to this signiﬁcant improvement, it has been\nproven the reliability and superiority of our TransAttUnet. 4)\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 11\nStage-3 Stage-4 PredictionStage-2 Zoom Ground TruthStage-1\nU-NetTransAttUnet\nFig. 6. Visualizations of feature maps produced by vanilla U-Net and the proposed TransAttUnet in different decoder stages based on the Clean-CC-CCII\ndataset. Best viewed with zoom in.\nTABLE VI\nCOMPARISONS OF MEAN DICE SCORE FOR ABLATION STUDIES WITH\nDIFFERENT EXPERIMENTAL SETTINGS . FOR EACH COLUMN , THE BEST\nRESULTS ARE HIGHLIGHTED IN RED .\nMethods Skin Lung Pneu. Bowl Gland\nTAU w/o TSA + GSA 83.89 97.55 83.56 84.54 81.59\nTAU w/o TSA + MSC 84.49 97.75 84.99 85.82 83.84\nTAU w/o GSA + MSC 85.12 97.79 85.03 86.55 84.64\nTAU w/o TSA 87.13 98.16 85.59 88.42 85.96\nTAU w/o GSA 87.86 98.24 85.63 88.91 86.78\nTAU w/o MSC 88.55 98.49 85.97 90.37 87.83\nTransAttUnet 90.74 98.88 86.57 91.62 89.11\nBesides, Fig. 5(b) demonstrates that our TransAttUnet can\nbetter distinguish the gland itself from the surrounding tissue,\nleading to excellent gland segmentation performance.\nAll results in the above experiments quantitatively demon-\nstrate the effectiveness and generalizability of the proposed\nTransAttUnet for medical image segmentation in various chal-\nlenging scenarios.\nD. Ablation Studies\nTo evaluate the effectiveness of each component added in\nthe proposed TransAttUnet, we conduct comprehensive abla-\ntion experiments by removing the components successively.\nThe experimental results are presented in Table VI. In detail,\n“TSA” denotes the proposed transformer self attention block;\n“GSA” denotes the designed global spatial attention block;\n“MSC” denotes the multi-scale skip connections between the\ndecoders; and “TAU” is considered as the “full” model.\nWhen both TSA and GSA blocks are removed, “TAU w/o\nTSA + GSA” would suffer serious performance degradation.\nNonetheless, it consistently outperforms the vanilla U-Net,\nwhich demonstrates the effectiveness of the multi-scale skip\nconnections between the decoders. Moreover, the results of\n“TAU w/o TSA + MSC” and “TAU w/o GSA + MSC” can\nindicate that the proposed TSA and GSA blocks are equally\neffective in developing the segmentation quality. Subsequently,\nwe also perform additional ablation studies by removing\nmultiple components to further verify the effectiveness of\nour work. For example, after removing the component of\nMSC, the evaluation scores of “TAU w/o MSC” drop to\n88.55%, 98.49%, 85.97%, 90.37%, and 87.83%, respectively.\nLikewise, the performance of “TAU w/o TSA” and “TAU\nw/o GSA” would degrade when we delete the other two\ncomponents in turn. Compared with the full model, the above\nexperimental results primarily prove the superiorities of the\ndesigned components. Apparently, all the components can\ncomplement and reinforce each other, which further veriﬁes\nthe combined effects of the above semantic segmentation.\nE. Visualizations of Decoder Stages\nCompared with vanilla U-Net, the proposed TransAttUnet\nbeneﬁts greatly from the long-range feature dependencies\nand the global contextual information. To further verify their\nabilities of semantic discrimination, we visualize feature maps\nof each decoder stage for both U-Net and TransAttUnet, as\nillustrated in Fig. 6.\nBased on the comparative results, we can make the follow-\ning observations: 1) It can be seen that the encoders of U-\nNet fail to make full use of the contextual information. With\nthe guidance of the multi-level non-local attention mechanisms\nthat effectively capture contexts, the obtained encoder features\ncan provide more global semantic information to our decoders\nat lower stages, which can generate the most discriminative\nfeatures. 2) Meanwhile, our TransAttUnet can make full use\nof the multi-scale contextual information to generate accu-\nrate predictions at later stages. As shown with the zoom-\nin patches of the deepest stage, the segmentation results of\nTransAttUnet are more detailed and reliable with a clearly\ndiscernible boundary. Therefore, we can conclude that the\nsemantic information learned by our method is more effective\nto improve the performance of medical image segmentation.\nV. C ONCLUSION AND FUTURE WORK\nIn this paper, we propose a novel Transformer based\nattention-guided U-Net called TransAttUnet, which concur-\nrently incorporates multi-level guided attention and multi-scale\nskip connections into U-Net for improving the segmentation\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 12\nquality of biomedical images. Speciﬁcally, the multi-level\nguided attention block is able to make full use of global\ncontextual information by concurrently exploring long-range\ninteractions and global spatial relationships between encoder\nsemantic features. Meanwhile, the multi-scale skip connection\nscheme can ﬂexibly aggregate contextual feature maps from\ndecoders of varying semantic scales to generate the dis-\ncriminative feature representations. Compared with previous\nadvanced works, the proposed TransAttUnet beneﬁts greatly\nfrom long-range feature dependencies and the multiscale con-\ntextual information, which ensures the feature representations\nwith semantic consistency. In this way, we can effectively\nmitigate the intrinsic limitations that occur in traditional U-\nshape architecture. Extensive experimental results on different\nbenchmark datasets demonstrate that the proposed TransAt-\ntUnet can achieve consistent performance improvements by\nintegrating the above novelties.\nDespite the fact that our study has some vital contributions,\nthere are still several limitations. Admittedly, the proposed\nTransAttUnet heavily relies on the global self-attention mecha-\nnism and thus suffers large memory footprint and computation\ncost. Moreover, the potential of Transformer for medical image\nsegmentation remains incomplete and underutilized in the\nstudy of our work, especially in the face of various biomedical\nimages. Therefore, further improvements in these aspects will\nbe investigated in our future work.\nREFERENCES\n[1] K. Roy, D. Banik, D. Bhattacharjee, O. Krejcar, and C. Kollmann,\n“Lwmla-net: A lightweight multi-level attention-based network for\nsegmentation of covid-19 lungs abnormalities from ct images,” IEEE\nTransactions on Instrumentation and Measurement , vol. 71, pp. 1–13,\n2022.\n[2] S. Pramanik, S. Ghosh, D. Bhattacharjee, and M. Nasipuri, “Segmen-\ntation of breast-region in breast thermogram using arc-approximation\nand triangular-space search,” IEEE Transactions on Instrumentation and\nMeasurement, vol. 69, no. 7, pp. 4785–4795, 2019.\n[3] X. Yang, Q. Wei, C. Zhang, K. Zhou, L. Kong, and W. Jiang, “Colon\npolyp detection and segmentation based on improved mrcnn,” IEEE\nTransactions on Instrumentation and Measurement , vol. 70, pp. 1–10,\n2020.\n[4] J. Fu, J. Liu, Y . Li, Y . Bao, W. Yan, Z. Fang, and H. Lu, “Contextual\ndeconvolution network for semantic segmentation,” Pattern Recognition,\nvol. 101, p. 107152, 2020.\n[5] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2015, pp. 3431–3440.\n[6] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical Image Computing and Computer-Assisted Intervention , 2015,\npp. 234–241.\n[7] W. Luo, Y . Li, R. Urtasun, and R. Zemel, “Understanding the effective\nreceptive ﬁeld in deep convolutional neural networks,” in Proceedings\nof the 30th International Conference on Neural Information Processing\nSystems, 2016, pp. 4905–4913.\n[8] Y . Chen, K. Wang, X. Liao, Y . Qian, Q. Wang, Z. Yuan, and P.-A. Heng,\n“Channel-unet: a spatial channel-wise convolutional neural network for\nliver and tumors segmentation,” Frontiers in Genetics, vol. 10, p. 1110,\n2019.\n[9] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:\nA nested u-net architecture for medical image segmentation,” in Deep\nlearning in medical image analysis and multimodal learning for clinical\ndecision support. Springer, 2018, pp. 3–11.\n[10] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,\nK. Mori, S. McDonagh, N. Y . Hammerla, B. Kainz et al., “Attention u-\nnet: Learning where to look for the pancreas,” Medical Image Analysis,\nvol. 53, no. 2, 2019, doi.org/10.1016/j.media.2019.01.012.\n[11] Y . Cai and Y . Wang, “Ma-unet: An improved version of unet based on\nmulti-scale and attention mechanism for medical image segmentation,”\narXiv preprint arXiv:2012.10952 , 2020.\n[12] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu, “Spa-\ntial transformer networks,” arXiv preprint arXiv:1506.02025 , 2015.\n[13] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and\nD. Tran, “Image transformer,” in International Conference on Machine\nLearning. PMLR, 2018, pp. 4055–4064.\n[14] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306 , 2021.\n[15] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,”\narXiv preprint arXiv:2102.10662 , 2021.\n[16] C. Tao, S. Gao, M. Shang, W. Wu, D. Zhao, and R. Yan, “Get the point\nof my utterance! learning towards effective responses with multi-head\nattention mechanism.” in International Joint Conference on Artiﬁcial\nIntelligencec, 2018, pp. 4418–4424.\n[17] Z. Yu, X. Jiang, F. Zhou, J. Qin, D. Ni, S. Chen, B. Lei, and T. Wang,\n“Melanoma recognition in dermoscopy images via aggregated deep\nconvolutional features,” IEEE Transactions on Biomedical Engineering ,\nvol. 66, no. 4, pp. 1006–1016, 2018.\n[18] A. Rashno, D. D. Koozekanani, P. M. Drayna, B. Nazari, S. Sadri,\nH. Rabbani, and K. K. Parhi, “Fully automated segmentation of ﬂuid/cyst\nregions in optical coherence tomography images with diabetic macular\nedema using neutrosophic sets and graph algorithms,” IEEE Transac-\ntions on Biomedical Engineering , vol. 65, no. 5, pp. 989–1001, 2017.\n[19] P. Mal ´ık, ˇS. Kri ˇstof´ık, and K. Knapov ´a, “Instance segmentation model\ncreated from three semantic segmentations of mask, boundary and\ncentroid pixels veriﬁed on glas dataset,” in 2020 15th Conference on\nComputer Science and Information Systems (FedCSIS) . IEEE, 2020,\npp. 569–576.\n[20] P. Zhao, J. Zhang, W. Fang, and S. Deng, “Scau-net: Spatial-channel\nattention u-net for gland segmentation,” Frontiers in Bioengineering and\nBiotechnology, vol. 8, 2020.\n[21] M. Islam, V . Vibashan, V . J. M. Jose, N. Wijethilake, U. Utkarsh,\nand H. Ren, “Brain tumor segmentation and survival prediction using\n3d attention unet,” in International MICCAI Brainlesion Workshop .\nSpringer, 2019, pp. 262–272.\n[22] ¨O. C ¸ ic ¸ek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,\n“3d u-net: learning dense volumetric segmentation from sparse anno-\ntation,” in International Conference on Medical Image Computing and\nComputer-Assisted Intervention. Springer, 2016, pp. 424–432.\n[23] Y .-B. Tang, Y .-X. Tang, J. Xiao, and R. M. Summers, “Xlsor: A robust\nand accurate lung segmentor on chest x-rays using criss-cross attention\nand customized radiorealistic abnormalities generation,” in International\nConference on Medical Imaging with Deep Learning . PMLR, 2019,\npp. 457–467.\n[24] X. Chen, L. Yao, and Y . Zhang, “Residual attention u-net for automated\nmulti-class segmentation of covid-19 chest ct images,” arXiv preprint\narXiv:2004.05645, 2020.\n[25] X. Qin, Z. Zhang, C. Huang, M. Dehghan, O. R. Zaiane, and M. Jager-\nsand, “U2-net: Going deeper with nested u-structure for salient object\ndetection,” Pattern Recognition, vol. 106, p. 107404, 2020.\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\n[27] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\n[28] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr et al. , “Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,” arXiv preprint\narXiv:2012.15840, 2020.\n[29] Y . Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and\ncnns for medical image segmentation,”arXiv preprint arXiv:2102.08005,\n2021.\n[30] Y . Ji, R. Zhang, H. Wang, Z. Li, L. Wu, S. Zhang, and P. Luo, “Multi-\ncompound transformer for accurate biomedical image segmentation,”\narXiv preprint arXiv:2106.14385 , 2021.\n[31] N. Codella, V . Rotemberg, P. Tschandl, M. E. Celebi, S. Dusza,\nD. Gutman, B. Helba, A. Kalloo, K. Liopyris, M. Marchetti et al., “Skin\nlesion analysis toward melanoma detection 2018: A challenge hosted\nby the international skin imaging collaboration (isic),” arXiv preprint\narXiv:1902.03368, 2019.\nIEEE TRANSACTIONS ON INSTRUMENTATION & MEASUREMENT, VOL. X, NO. X, 2022 13\n[32] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.-\ni. Komatsu, M. Matsui, H. Fujita, Y . Kodera, and K. Doi, “Development\nof a digital image database for chest radiographs with and without a\nlung nodule: receiver operating characteristic analysis of radiologists’\ndetection of pulmonary nodules,” American Journal of Roentgenology ,\nvol. 174, no. 1, pp. 71–74, 2000.\n[33] S. Jaeger, S. Candemir, S. Antani, Y .-X. J. W ´ang, P.-X. Lu, and\nG. Thoma, “Two public chest x-ray datasets for computer-aided screen-\ning of pulmonary diseases,” Quantitative Imaging in Medicine and\nSurgery, vol. 4, no. 6, p. 475, 2014.\n[34] X. He, S. Wang, S. Shi, X. Chu, J. Tang, X. Liu, C. Yan, J. Zhang, and\nG. Ding, “Benchmarking deep learning models and automated model\ndesign for covid-19 detection with chest ct scans,” medRxiv, 2020.\n[35] J. C. Caicedo, A. Goodman, K. W. Karhohs, B. A. Cimini, J. Ackerman,\nM. Haghighi, C. Heng, T. Becker, M. Doan, C. McQuin et al., “Nucleus\nsegmentation across imaging experiments: the 2018 data science bowl,”\nNature Mcethods, vol. 16, no. 12, pp. 1247–1253, 2019.\n[36] D. Jha, M. A. Riegler, D. Johansen, P. Halvorsen, and H. D. Johansen,\n“Doubleu-net: A deep convolutional neural network for medical im-\nage segmentation,” in 2020 IEEE 33rd International Symposium on\nComputer-Based Medical Systems (CBMS) . IEEE, 2020, pp. 558–564.\n[37] J. M. J. Valanarasu, V . A. Sindagi, I. Hacihaliloglu, and V . M. Patel,\n“Kiu-net: Towards accurate segmentation of biomedical images using\nover-complete representations,” in International Conference on Medical\nImage Computing and Computer-Assisted Intervention. Springer, 2020,\npp. 363–373.\n[38] M. Z. Alom, C. Yakopcic, T. M. Taha, and V . K. Asari, “Nuclei\nsegmentation with recurrent residual convolutional neural networks\nbased u-net (r2u-net),” in NAECON 2018-IEEE National Aerospace and\nElectronics Conference. IEEE, 2018, pp. 228–233.\n[39] N. K. Tomar, D. Jha, M. A. Riegler, H. D. Johansen, D. Johansen,\nJ. Rittscher, P. Halvorsen, and S. Ali, “Fanet: A feedback attention\nnetwork for improved biomedical image segmentation,” arXiv preprint\narXiv:2103.17235, 2021.\n[40] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao,\n“Pranet: Parallel reverse attention network for polyp segmentation,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention. Springer, 2020, pp. 263–273.\n[41] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “Resunet-a: a\ndeep learning framework for semantic segmentation of remotely sensed\ndata,” ISPRS Journal of Photogrammetry and Remote Sensing , vol. 162,\npp. 94–114, 2020.\n[42] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. De Lange,\nP. Halvorsen, and H. D. Johansen, “Resunet++: An advanced archi-\ntecture for medical image segmentation,” in 2019 IEEE International\nSymposium on Multimedia (ISM) . IEEE, 2019, pp. 225–2255.\n[43] R. Azad, M. Asadi-Aghbolaghi, M. Fathy, and S. Escalera, “Bi-\ndirectional convlstm u-net with densley connected convolutions,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision Workshops, 2019, pp. 1–10.\n[44] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n“Swin-unet: Unet-like pure transformer for medical image segmenta-\ntion,” arXiv preprint arXiv:2105.05537 , 2021.\n[45] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efﬁcient design for semantic segmentation\nwith transformers,” Advances in Neural Information Processing Systems,\nvol. 34, 2021.\n[46] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with\nwarm restarts,” in Proceedings of International Conference on Learning\nRepresentations, 2017.\n[47] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K ¨opf,\nE. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-\nperformance deep learning library,” in Conference and Workshop on\nNeural Information Processing Systems , 2019.\n[48] N. Anuar and A. B. M. Sultan, “Validate conference paper using dice\ncoefﬁcient,” Computer and Information Science , vol. 3, no. 3, p. 139,\n2010.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8067550659179688
    },
    {
      "name": "Segmentation",
      "score": 0.7228729128837585
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6560843586921692
    },
    {
      "name": "Encoder",
      "score": 0.6318573951721191
    },
    {
      "name": "Discriminative model",
      "score": 0.5367552042007446
    },
    {
      "name": "Transformer",
      "score": 0.521319568157196
    },
    {
      "name": "Upsampling",
      "score": 0.468568354845047
    },
    {
      "name": "Image segmentation",
      "score": 0.46515995264053345
    },
    {
      "name": "Pixel",
      "score": 0.43745267391204834
    },
    {
      "name": "Computer vision",
      "score": 0.4026017487049103
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39588499069213867
    },
    {
      "name": "Image (mathematics)",
      "score": 0.20915326476097107
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}