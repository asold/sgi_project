{
  "title": "The scientific knowledge of three large language models in cardiology: multiple-choice questions examination-based performance",
  "url": "https://openalex.org/W4396643106",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4378937136",
      "name": "Ibraheem Altamimi",
      "affiliations": [
        "King Saud University"
      ]
    },
    {
      "id": "https://openalex.org/A4378937140",
      "name": "Abdullah Alhumimidi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2237149822",
      "name": "Salem Alshehri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5096199123",
      "name": "Abdullah Alrumayan",
      "affiliations": [
        "King Saud bin Abdulaziz University for Health Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4287271752",
      "name": "Thamir Al-khlaiwi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2250985958",
      "name": "Sultan A. Meo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208915013",
      "name": "Mohamad-Hani Temsah",
      "affiliations": [
        "King Saud University",
        "King Saud Medical City"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6849384959",
    "https://openalex.org/W6850577281",
    "https://openalex.org/W4307688794",
    "https://openalex.org/W4320186815",
    "https://openalex.org/W6854136013",
    "https://openalex.org/W4366088379",
    "https://openalex.org/W4389437456",
    "https://openalex.org/W4381982883",
    "https://openalex.org/W4366824288",
    "https://openalex.org/W4386379702",
    "https://openalex.org/W6857228156",
    "https://openalex.org/W2170435532",
    "https://openalex.org/W6853415230",
    "https://openalex.org/W4376640725",
    "https://openalex.org/W3217309971",
    "https://openalex.org/W6849026655",
    "https://openalex.org/W4384695580",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W6854349233",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4380356334",
    "https://openalex.org/W4322008312",
    "https://openalex.org/W4391069701",
    "https://openalex.org/W4385286786",
    "https://openalex.org/W4379879923",
    "https://openalex.org/W4378603221",
    "https://openalex.org/W4319779057",
    "https://openalex.org/W4384821243",
    "https://openalex.org/W4380483612",
    "https://openalex.org/W3212282753",
    "https://openalex.org/W4387393234"
  ],
  "abstract": "Background: The integration of artificial intelligence (AI) chatbots like Google’s Bard, OpenAI’s ChatGPT, and Microsoft’s Bing Chatbot into academic and professional domains, including cardiology, has been rapidly evolving. Their application in educational and research frameworks, however, raises questions about their efficacy, particularly in specialized fields like cardiology. This study aims to evaluate the knowledge depth and accuracy of these AI chatbots in cardiology using a multiple-choice question (MCQ) format. Methods: The study was conducted as an exploratory, cross-sectional study in November 2023 on a bank of 100 MCQs covering various cardiology topics that was created from authoritative textbooks and question banks. These MCQs were then used to assess the knowledge level of Google’s Bard, Microsoft Bing, and ChatGPT 4.0. Each question was entered manually into the chatbots, ensuring no memory retention bias. Results: The study found that ChatGPT 4.0 demonstrated the highest knowledge score in cardiology, with 87% accuracy, followed by Bing at 60% and Bard at 46%. The performance varied across different cardiology subtopics, with ChatGPT consistently outperforming the others. Notably, the study revealed significant differences in the proficiency of these chatbots in specific cardiology domains. Conclusion: This study highlights a spectrum of efficacy among AI chatbots in disseminating cardiology knowledge. ChatGPT 4.0 emerged as a potential auxiliary educational resource in cardiology, surpassing traditional learning methods in some aspects. However, the variability in performance among these AI systems underscores the need for cautious evaluation and continuous improvement, especially for chatbots like Bard, to ensure reliability and accuracy in medical knowledge dissemination.",
  "full_text": null,
  "topic": "Chatbot",
  "concepts": [
    {
      "name": "Chatbot",
      "score": 0.761616051197052
    },
    {
      "name": "Internal medicine",
      "score": 0.589535653591156
    },
    {
      "name": "Cardiology",
      "score": 0.5723806023597717
    },
    {
      "name": "Medicine",
      "score": 0.5719272494316101
    },
    {
      "name": "Multiple choice",
      "score": 0.46581271290779114
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41886427998542786
    },
    {
      "name": "Computer science",
      "score": 0.2615845203399658
    },
    {
      "name": "Significant difference",
      "score": 0.21594873070716858
    }
  ]
}