{
  "title": "\"Mask and Infill\" : Applying Masked Language Model to Sentiment Transfer",
  "url": "https://openalex.org/W2969477568",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098081155",
      "name": "Wu Xing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1977774760",
      "name": "Zhang Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200858959",
      "name": "Zang, Liangjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224362729",
      "name": "Han, Jizhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2306809872",
      "name": "Hu Song-Lin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964008635",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2963034998",
    "https://openalex.org/W2963366196",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2951697117",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2952886204",
    "https://openalex.org/W2805041018",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W2963667126"
  ],
  "abstract": "This paper focuses on the task of sentiment transfer on non-parallel text, which modifies sentiment attributes (e.g., positive or negative) of sentences while preserving their attribute-independent content. Due to the limited capability of RNNbased encoder-decoder structure to capture deep and long-range dependencies among words, previous works can hardly generate satisfactory sentences from scratch. When humans convert the sentiment attribute of a sentence, a simple but effective approach is to only replace the original sentimental tokens in the sentence with target sentimental expressions, instead of building a new sentence from scratch. Such a process is very similar to the task of Text Infilling or Cloze, which could be handled by a deep bidirectional Masked Language Model (e.g. BERT). So we propose a two step approach \"Mask and Infill\". In the mask step, we separate style from content by masking the positions of sentimental tokens. In the infill step, we retrofit MLM to Attribute Conditional MLM, to infill the masked positions by predicting words or phrases conditioned on the context1 and target sentiment. We evaluate our model on two review datasets with quantitative, qualitative, and human evaluations. Experimental results demonstrate that our models improve state-of-the-art performance.",
  "full_text": "Mask and Inﬁll: Applying Masked Language Model to Sentiment Transfer\nXing Wu1,2,3 , Tao Zhang1,2 , Liangjun Zang1 , Jizhong Han1 and Songlin Hu1∗\n1Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China\n2School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China\n3Baidu Inc., Beijing, China\nwuxing03@baidu.com,{zhangtao,zangliangjun,hanjizhong,husonglin}@iie.ac.cn\nAbstract\nThis paper focuses on the task of sentiment trans-\nfer on non-parallel text, which modiﬁes senti-\nment attributes (e.g., positive or negative) of sen-\ntences while preserving their attribute-independent\ncontent. Due to the limited capability of RNN-\nbased encoder-decoder structure to capture deep\nand long-range dependencies among words, pre-\nvious works can hardly generate satisfactory sen-\ntences from scratch. When humans convert the sen-\ntiment attribute of a sentence, a simple but effective\napproach is to only replace the original sentimental\ntokens in the sentence with target sentimental ex-\npressions, instead of building a new sentence from\nscratch. Such a process is very similar to the task of\nText Inﬁlling or Cloze, which could be handled by\na deep bidirectional Masked Language Model (e.g.\nBERT). So we propose a two step approach “Mask\nand Inﬁll”. In the mask step, we separate style from\ncontent by masking the positions of sentimental to-\nkens. In the inﬁll step, we retroﬁt MLM to Attribute\nConditional MLM, to inﬁll the masked positions by\npredicting words or phrases conditioned on the con-\ntext1and target sentiment. We evaluate our model\non two review datasets with quantitative, qualita-\ntive, and human evaluations. Experimental results\ndemonstrate that our models improve state-of-the-\nart performance.\n1 Introduction\nThe goal of sentiment transfer [Huang and Belongie, 2017;\nLogeswaran et al., 2018] is to change the sentiment attribute\nof text while keeping its semantic content unchanged, with\nbroad applications of review sentiment transformation, news\nrewriting, and so on. For example, we could convert a posi-\ntive sentence “I highly recommend this movie” to a negative\none “I regret watching this movie”. Lacking supervised par-\nallel data, i.e. pairs of sentences with the same content but\ndifferent attributes, makes it hard to change the sentiment of\n∗Corresponding Author\n1In this paper, content and context are equivalent, style, attribute\nand label are equivalent.\nFigure 1: Process of our approach. In the mask stage, we explicitly\nidentify and mask the sentiment tokens in a sentence. In the inﬁll\nstage, we ﬁll the masked positions with new expressions conditioned\non their context and the target sentiment.\ntext without loss of its semantic content. Recently, several\nmodels have been proposed to learn sentiment transfer from\nnon-parallel text, notably [Shen et al., 2017; Fu et al., 2018;\nXu et al., 2018; Li et al., 2018]. Some of them [Shen et al.,\n2017; Prabhumoye et al., 2018; Fu et al., 2018] try to learn\nthe disentangled representation of content and attribute of a\nsentence in a hidden space, while the others [Xu et al., 2018;\nLi et al. , 2018 ] explicitly separate style from content in\nfeature-based ways and encode them into hidden represen-\ntations respectively. Afterwards, they utilize a RNN de-\ncoder (e.g. LSTM [Hochreiter and Schmidhuber, 1997;\nSutskever et al., 2014; Cho et al., 2014]) to generate a new\nsentence conditioned on the hidden representations of origi-\nnal content and target sentiment attribute. On the one hand, a\nshallow RNN encoder structure has limited ability to produce\nhigh-quality hidden representation. On the other hand, usage\nof RNN decoder restricts prediction ability to a short range,\nwhich often fails to produce long realistic sentences.\nIn this work, we leverage an important observation. When\npeople convert a sentence with a particular sentiment into one\narXiv:1908.08039v1  [cs.CL]  21 Aug 2019\nwith a different sentiment, we do not have to create a new sen-\ntence from scratch. Instead, we simply replace the sentimen-\ntal tokens of the sentence with other expressions indicative\nof a different sentiment. For example, given a negative re-\nview “terrible scenery and poor service”, a straightforward\npositive review is “beautiful scenery and good service”. Such\na process is very similar to the task of text inﬁlling [Zhu et\nal., 2019] or Cloze [Taylor, 1953]. Given the template “\nscenery and service”, we human feel easy to ﬁll the right\nwords because we have enough prior linguistic knowledge to\npredict the missing words from their contexts.\nWe transform the problem of sentiment transfer to the task\nof text inﬁlling by a pre-trained Masked Language Model\n(MLM). We utilize a pre-trained deep bidirectional language\nmodel (corresponding to human linguistic knowledge) to in-\nﬁll the masked positions. Our approach comprises two stages:\nMask and Inﬁll (Figure 1). In the mask stage, we explicitly\nidentify and mask the sentiment tokens in a given sentence. In\nthe inﬁll stage, we ﬁll the masked positions with new expres-\nsions conditioned on their context and the target sentiment.\nThere exist two feature-based methods for identifying sen-\ntiment attribute markers (i.e. words or phrases that are in-\ndicative of a particular sentiment attribute). Attention-based\nmethod [Xu et al., 2018] trains a self-attention[Bahdanau et\nal., 2015 ] sentiment classiﬁer, where the learned attention\nweights can be used as signals to identify sentiment mark-\ners, and the tokens with weights higher than the average are\nregarded as sentiment attribute markers. The attention-based\nmethod prefers isolated single words to n-gram phrases,\nwhich restricts MLM to produce sentences of diverse expres-\nsions. Frequency-ratio method [Li et al., 2018] constructs an\nattribute marker dictionary of n-gram phrases for each sen-\ntiment attribute and looks up the dictionary to identify sen-\ntiment phrases in sentences. Frequency-ratio method is in-\nclined to mask longer phrases, and its performance heavily\nrelies on the quality of attribute marker dictionaries. Thus,\nwe explore a simple fused method to utilize the merits of both\nmethods. Speciﬁcally, we combine the attention-based clas-\nsiﬁer with the frequency-ratio method, ﬁltering out false at-\ntribute markers and discovering new single sentiment words.\nWhen the frequency-ratio method fails to identify any at-\ntribute marker or recognize too many ones (with insufﬁcient\ncontent left), we utilize the attention-based method directly.\nMLM predicts the masked tokens only conditioned on their\ncontext, considering no attribute information. To ﬁll the\nmasked positions in a sentence with expressions compatible\nto a particular sentiment, we retroﬁt MLM to Attribute Con-\nditional Masked Language Model (AC-MLM) by integrat-\ning attribute embeddings with the original input for MLM.\nFurthermore, we introduce a pre-trained sentiment classi-\nﬁer to constrain AC-MLM, which ensures the generated sen-\ntences compatible with the target sentiment. To deal with\nthe discrete nature of language generation, we utilize soft-\nsampling [Hu et al., 2017] to guide the optimization of AC-\nMLM, where soft-sampling is used to back-propagate gradi-\nents through the sampling process by using an approximation\nof the sampled word vector.\nWe evaluate our models on two review datasets Yelp and\nAmazon by quantitative, qualitative, and human evaluations.\nExperimental results show that our method achieves state-of-\nthe-art results on accuracy, BLEU[Papineni et al., 2002] and\nhuman evaluations.\nOur contributions are summarized as follows:\n•We propose a two-stage “Mask and Inﬁll” approach to\nsentiment transfer task, capable of identifying both sim-\nple and complex sentiment markers and producing high-\nquality sentences.\n•Experimental results show that our approach outper-\nforms most state-of-the-art models in terms of both\nBLEU and accuracy scores.\n•We retroﬁt MLM to AC-MLM for labeled sentence gen-\neration. To the best of our knowledge, it is the ﬁrst work\nto apply a pre-trained masked language model to labeled\nsentence generation task.\n2 Related Work\n2.1 Non-parallel Style Transfer\n[Shen et al. , 2017; Fu et al. , 2018; Xu et al. , 2018; Li et\nal., 2018; Yang et al., 2018] are most relevant to our work.\n[Shen et al., 2017] assumed a shared latent content distribu-\ntion across different text corpora, and leverages reﬁned align-\nment of latent representations. [Fu et al. , 2018 ] learned a\nrepresentation that only contains the content information by\nmulti-task learning and adversarial training.[Xu et al., 2018]\nproposed a cycled reinforcement learning method by collabo-\nration between a neutralization module and an emotionaliza-\ntion module. [Li et al., 2018] separated attribute and content\nby deleting attribute markers, and attempted to reconstruct the\nsentence using the content and the retrieved target attribute\nmarkers with an RNN. [Yang et al., 2018] used a target do-\nmain language model as the discriminator to guide the gener-\nator to produce sentences. Unlike these models, which adopt\nRNN as encoder and decoder, we utilize pre-trained MLM to\ncapture longer linguistic structure and better language repre-\nsentation.\n2.2 Fine-tuning on Pre-trained Language Model\nLanguage model pre-training has attracted widespread at-\ntention, and ﬁne-tuning on pre-trained language models has\nproven to be effective in improving many downstream natural\nlanguage processing tasks. [Dai and Le, 2015] improved Se-\nquence Learning with recurrent networks by pre-training on\nunlabeled data. [Radford et al., 2018] improved the perfor-\nmance largely on many sentence-level tasks from the GLUE\nbenchmarks [Wang et al., 2018]. [Howard and Ruder, 2018]\nproposed Universal Language Model Fine-tuning (ULMFiT),\nwhich was a general transfer learning method for ﬁne-tuning\na language model. [Radford et al., 2018] demonstrated that\nby generative pre-training language model on unlabeled text\nfrom diverse corpora, large gains could be achieved on a di-\nverse range of tasks. By introducing a deep bidirectional\nmasked language model, BERT[Devlin et al., 2018] obtained\nnew state-of-the-art results on a broad range of tasks. Un-\nlike previous works ﬁne-tuning pre-trained language models\nto perform discriminative tasks, we aim to apply a pre-trained\nmasked language model on generative task.\nFigure 2: The overall model architecture consists of two modules.\nThe Mask module is the fusion-method introduced in Section 3.2.\nThe Inﬁll module consists of two parts, reconstruction and discrim-\nination, corresponding to Equation 10.\n3 Approach\nIn this section, we introduce our proposed approach. An\noverview is presented in Section 3.1. The mask process is\nintroduced in Section 3.2. The details of the inﬁll process are\nshown in Section 3.3.\n3.1 Overview\nGiven a corpus of labeled sentences: D =\n{(S1,a1),..., (Sm,am)}, where Si is a sentence and ai\nis the corresponding attribute. We deﬁne Aas the set of all\npossible attributes, Da = {S : (S,a) ∈ D}as the set of\nsentences with attribute a.\nOur goal is to learn a model that takes as input (S,ˆa),\nwhere S is a sentence with original sentiment attribute and\nˆa is target sentiment attribute, outputs a sentence ˆS that re-\ntains the semantic content of Swhile exhibiting ˆa.\nThe proposed approach consists of two parts: a mask mod-\nule and an inﬁll module, as shown in Figure 2. The mask\nmodule combines the strength of two methods, utilizing a\ncandidate phrase vocabulary to ﬁnd the potential attribute\nmarkers in sentences, then performs mask operation. Then\nthe masked sentences are sent into the inﬁll module, which in-\nﬁlls masked positions via AC-MLM. During the second stage,\nfor better attribute compatibility of generated sentences, we\nintroduce a pre-trained classiﬁer as the discriminator. Due to\nthe discrete nature of language generation, we draw support\nfrom soft-sampling to guide the optimization of our model.\nThe implement details are shown in Algorithm 1.\n3.2 Identify and Mask Attribute Marker\nWe ﬁrst introduce the frequency-ratio method, then the\nattention-base method. At last, we propose a fusion-method\nby combing the strength of these two methods.\nFrequency-ratio Method\nWe deﬁne for any ain A, let count(u,Da) denotes the num-\nber of times an n-gram uappear in Da, we score the salience\nof uwith respect to aby its smoothed frequency-ratio :\nsc(u,a) = count(u,Da) +λ(∑\na′∈A,a′̸=acount(u,Da′ )\n)\n+ λ\n(1)\nwhere λ is the smoothing parameter. We declare u to be a\ncandidate attribute marker for aif sc(u,a) is above a speci-\nﬁed threshold γc, these candidate attribute markers constitute\na candidate attribute marker vocabulary Va.\nAttention-based Method\nEach word in a sentence contributes differently to the label\nor attribute of the whole sentence, we train an attention-based\nclassiﬁer to extract the attribute relative extent. Given a se-\nquence S of N tokens, < t1,t2,...,t N >, we adopt a bidi-\nrectional LSTM to encode the sentence and concatenate the\nforward hidden state and the backward hidden state for each\nword, and obtain:\nH = (h1,h2,··· ,hN) (2)\nwhere N is the length of given sentence. The attention mech-\nanism produces a vector of attention weightsa and a vector of\nweighted hidden states c. Finally a softmax layer is followed\nto transform c into probability distribution y:\na = softmax(w ·tanh(WHT)) (3)\nc = a ·H (4)\ny = softmax(W′ ·c) (5)\nwhere w, W, W\n′\nare projection parameters. After being\nwell-trained, the attention-based classiﬁer can be used to ex-\ntract attention weights, which capture the sentiment informa-\ntion of each word. For simplicity, following[Xu et al., 2018],\nwe set the averaged attention value in a sentence as the thresh-\nold, words with attention weights higher than the threshold\nare viewed as attribute markers.\nFusion-Method\nHowever, there are fake attribute markers in the candidate\nvocabulary2. So we use a pre-trained classiﬁer, i.e. the\nattention-based classiﬁer, to calculate the probability pof be-\ning real for each candidate attribute marker with Equation 5\nand update the salience score:\ns(u,a) =sc(u,a) ∗p (6)\nWe ﬁlter out fake attribute markers with s(u,a) below\nthreshold γ, obtaining a high-quality attribute marker vocab-\nulary Vfor attribute a.\nAfter the vocabulary constructed, we identify and mask\nas many attribute markers as possible by matching the sen-\ntences, because within a sentence style words are usually less\nthan content words. Lastly, if frequency-ratio method identi-\nﬁes no attribute marker or too many attribute markers (with\ninsufﬁcient content left, especially for short sentences), we\nre-process it with the attention-based method3 by identifying\nthe positions with high weights.\n3.3 Text Inﬁlling with MLM\nAttribute Conditional Masked Language Model\nUnlike traditional language models which predict tokens\nbased on previously generated tokens, Masked Language\nModel (MLM) predicts the masked tokens according to their\n2In the original positive vocabulary of Yelp dataset, “he did a”\nscores as high as 26.0, yet it is not a positive attribute marker.\n3Note that we use the attention-based classiﬁer twice: the ﬁrst\ntime, it is used as discriminator to improve the quality of candidate\nattribute vocabulary, the second time, it is used to extract attention\nweights for a sentence.\ncontext, rather than reconstructing the entire input. For the\ntask of sentiment transfer, inﬁlled words or phrases should\nbe consistent to a target attribute. Therefore, we propose At-\ntribute Conditional Masked Language Model (AC-MLM) to\ninﬁll masked positions conditioned on both their context and\na target label, by adding attribute embeddings into original\ninput embeddings. Let ˆa ∈ Abe a target attribute, S =\n⟨t1,t2,...,t N⟩a sentence of N tokens, M = {ti1 ,...,t im }the\nset of masked tokens, S = S\\M the set of content tokens.\nReconstruction\nLacking parallel sentiment sentence pairs, so we train the\nAC-MLM to reconstruct the original sentence S conditioned\non the content S and original attribute a. The AC-MLM is\noptimized to minimize the reconstruction error of original\nmasked words:\nLrec = −\n∑\na∈A,ti∈M\nlogp(ti|S,a) (7)\nWell-trained AC-MLM model can take the sentence con-\ntent S and a target attribute ˆa as input, and output p(·|ˆa,S)\nfor each masked position. After all masked positions are in-\nﬁlled, we get the transferred sentence:\nˆS = AC-MLM(S,ˆa) (8)\nSpeciﬁcally, we use the pre-trained BERT as our MLM,\nand substitute segmentation embeddings (which is useless\nwhen training single sentence) with attribute embeddings.\nPre-trained Classiﬁer Constraint\nTo further improve the transfer accuracy, we introduce a pre-\ntrained sentiment classiﬁer to the AC-MLM, i.e. AC-MLM-\nSS, “-SS” indicates with soft-sampling which will be intro-\nduced later.\nDiscrimination Given the content S and a target attribute\nˆa, when a sentence is successfully transferred, the label pre-\ndicted by classiﬁer should be consistent to the target attribute.\nSo the AC-MLM-SS is further optimized to minimize the dis-\ncrimination discrepancy:\nLacc = −logp(ˆa|ˆS) (9)\nwhere ˆS indicates transferred sentence generated by AC-\nMLM.\nBy combining Equation 7 and Equation 9 we obtain the\nobjective function of AC-MLM-SS :\nminθL= Lrec + ηLacc (10)\nwhere θ is AC-MLM-SS’s parameters and η is a balancing\nparameter.\nTo propagate gradients from the discriminator through the\ndiscrete samples, we adopt soft-sampling.\nSoft-sampling Soft-sampling back-propagates gradients\nby using an approximation of the sampled word vector. The\napproximation replaces the sampled token ti (represented as\na one-hot vector) at each step with the probability vector\nˆti ∼softmax(ot/τ) (11)\nwhich is differentiable w.r.t the AC-MLM’s parameters. The\nresulting soft generated sentence is fed into the discriminator\nto measure the ﬁtness to the target attribute.\nAlgorithm 1 Implementation of “Mask and Inﬁll” approach.\n1: Pre-train attention-based classiﬁer Cls (Eq.2-5)\n2: Construct attribute marker vocabulary V (Eq.1,6)\n3: for every sentence Sin D do\n4: Mask attribute markers within Sby looking up V, getting S\n5: if Sis too short or Sis the same as Sthen\n6: Re-mask with attention weights calculated by Cls (Eq.3)\n7: end if\n8: end for\n9: for each iteration i=1,2,...,M do\n10: Sample a masked sentence Swith attribute a\n11: Reconstruct Swith Sand a, calculating Lrec based (Eq.7)\n12: ˆa= the target attribute\n13: Construct ˆS(Eq.8)\n14: calculating Lacc (Eq.9)\n15: Update model parameters θ\n16: end for\n4 Experiment\nIn this section, we evaluate our method on two review datasets\nYelp and Amazon. Firstly, we introduce datasets, baseline\nmodels, training details, and evaluation metrics. Secondly,\nwe compare our approach to several state-of-the-art systems.\nFinally, we present our experimental results and analyze each\ncomponent in detail.\n4.1 Datasets\nWe experiment our methods on two review datasets from [Li\net al. , 2018 ]: Yelp and Amazon [He and McAuley, 2016 ],\neach of which is randomly split into training, validation and\ntesting sets. Examples in YELP are sentences from business\nreviews on Yelp, and examples in AMAZON are sentences\nfrom product reviews on Amazon. Each example is labeled\nas having either positive or negative sentiment. The statistics\nof datasets are shown in Table 1.\n4.2 Baselines\nWe compared our methods to existing relevant models:\nCrossAligned [Shen et al. , 2017 ], StyleEmbedding and\nMultiDecoder [Fu et al. , 2018 ], CycledReinforce [Xu et\nal., 2018 ]4, TemplateBased, RetrievalOnly, DeleteOnly and\nDeleteAndRetrieval [Li et al., 2018], LM+Classifer [Yang et\nal., 2018].\n4.3 Experiment Details\nMask step\nFor the frequency-ratio method, we consider n-gram up to\n4-gram and set the smoothing parameter λto 1. Other hyper-\nparameters are following [Li et al., 2018]. For the attention-\nbased method, we train the attention-based classiﬁer for 10\nepochs with accuracy 97% for Yelp and 82% for Amazon.\nFor the fusion-method, we consider a sentence with content\nless than 5 tokens as insufﬁcient content, and re-process it\nwith the attention-based method.\n4https://github.com/lancopku/Unpaired-Sentiment-Translation\nDataset Attributes Train Dev Test\nYELP Positive 270K 2000 500\nNegative 180K 2000 500\nAMAZON Positive 277K 985 500\nNegative 278K 1015 500\nTable 1: Dataset statistics.\nInﬁll step\nWe use pre-trained BERT base as MLM and substitute seg-\nment embedding layer with attribute embedding layer. The\ninput size is kept compatible with original BERT and rele-\nvant hyperparameters can be found in [Devlin et al., 2018].\nThe pre-trained discriminator is a CNN-based classiﬁer[Kim,\n2014] with convolutional ﬁlters of size 3, 4, 5 and use Word-\nPiece embeddings. The hyperparameters in Equation 10 and\n11 are selected by a grid-search method using the validation\nset. We ﬁne-tune BERT to AC-MLM for 10 epochs, and fur-\nther train 6 epochs to apply discriminator constraint.\n4.4 Evaluation\nAutomatic Evaluation\nWe compute automatic evaluation metrics by employing au-\ntomatic evaluation tools from [Li et al., 2018] and [Yang et\nal., 2018]. Accuracy score assesses whether outputs have the\ndesired attribute. BLEU score is computed between the out-\nputs and the human references. A high BLEU score primar-\n5https://github.com/lijuncen/Sentiment-and-Style-Transfer\n6[Yanget al., 2018] only evaluated their models on Yelp. For fair\ncomparison, we only evaluate our models on Yelp too.\nYELP AMAZON\nACC (%) BLEU ACC(%) BLEU\nCrossAligned 73.7 3.1 74.1 0.4\nStyleEmbedding 8.7 11.8 43.3 10.0\nMultiDecoder 47.6 7.1 68.3 5.0\nCycledReinforce 85.2 9.9 77.3 0.1\nTemplateBased 81.7 11.8 68.7 27.1\nRetrievalOnly 95.4 0.4 70.3 0.9\nDeleteOnly 85.7 7.5 45.6 24.6\nDeleteAndRetrieval 88.7 8.4 48.0 22.8\nw/frequency-ratio\nAC-MLM 55.0 12.7 28.7 31.0\nAC-MLM-SS 90.5 11.6 75.7 26.0\nw/attention-based\nAC-MLM 41.5 15.9 31.2 32.1\nAC-MLM-SS 97.3 14.1 75.9 28.5\nw/fusion-method\nAC-MLM 43.5 15.3 42.9 30.7\nAC-MLM-SS 97.3 14.4 84.5 28.5\nTable 2: Automatic evaluation performed by tools from [Li et al.,\n2018]5. “ACC” indicates accuracy, “BLEU” measures content sim-\nilarity between the outputs and the human references. “AC-MLM”,\nrepresents attribute conditional masked language model. “w/” rep-\nresents “with”.“-SS” represents with soft-sampling.\nACC (%) BLEU\nYang’s results\nLM 85.4 13.4\nLM+Classiﬁer 90.0 22.3\nw/frequency-ratio\nAC-MLM 58.0 18.7\nAC-MLM-SS 93.7 17.5\nw/attention-based\nAC-MLM 40.0 21.8\nAC-MLM-SS 98.5 20.5\nw/fusion-method\nAC-MLM 44.1 21.3\nAC-MLM-SS 97.3 20.7\nTable 3: Automatic evaluation on Yelp dataset performed by tools\nfrom [Yang et al., 2018]6.\nily indicates that the system can correctly preserve content\nby retaining the same words from the source sentence as the\nreference.\nTable 2 shows the performance obtained with [Li et al. ,\n2018]’s tools. Our base model AC-MLM achieves all the\nbest BLEU, but performs poorly in accuracy. After apply-\ning the discriminator constraint to apply constraint (i.e. AC-\nMLM-SS), the accuracy improves signiﬁcantly, though with\nthe slight decline of BLEU. Previous models are hard to per-\nform well on accuracy and BLEU simultaneously. Among\nour models, AC-MLM-SS using fusion-methods inmask step\nachieves the most satisfactory performance considering on\nboth accuracy and BLEU. Table 3 shows the performance\non [Yang et al. , 2018 ]’s tools. Compared to Yang’s best\nmodel LM+Classiﬁer, we perform better on accuracy, but\nslightly lower on BLEU.\nHuman Evaluation\nWe hired three annotators (not authors) to rate outputs for\nour models and [Li et al. , 2018 ]’s best model (DeleteAn-\ndRetrieval). We adopt three criteria range from 1 to 5 (1\nis very bad and 5 is very good): grammaticality, similarity\nto the target attribute, and preservation of the source con-\ntent. For each dataset, we randomly sample 200 examples\nfor each target attribute. Table 5 shows the human eval-\nuation results. Among them, our AC-MLM-SS with the\nfusion-method achieves best results. Pre-trained MLM in-\nﬁlls the masked positions considering the context from both\ndirections, being able to meet grammatical requirements bet-\nter. Beneﬁting from explicitly separating content and style,\nour sentences’ structures are kept and we perform better on\nthe preservation of the source content. Then by introducing\nthe pre-trained discriminator, MLM inﬁlls more accurate at-\ntribute words.\n5 Analysis\nTrade-off between Content and Attribute Our loss func-\ntion in Equation 10 consists of two parts: reconstruction loss\nand discriminative loss. Reconstruction loss guides MLM to\ninﬁll contextual-compatible words. Compared to weak label-\nconstraint introduced by attribute embeddings, discriminative\nFrom negative to positive (YELP)\nSource it ’s not much like an actual irish pub , which is depressing .\nHuman It’s like an actual irish pub .\nCrossAligned it ’s not good for a clean and inviting , i textcolorbluelove food .\nStyleEmbedding it ’s not much like an neat of vegetarian - but tiny crust .\nMultiDecoder it ’s not much like an vegetarian bagel ... much is food .\nCycledReinforce it ’s not much like an actual irish pub , excellent sweet sweet !\nTemplateBased it ’s not much like an actual irish pub , which is most authentic .\nRetrievalOnly i like their food , i like their prices and i like their service .\nDeleteOnly it ’s not like much an actual irish pub , which is very fun .\nDeleteAndRetrieval it ’s not much like an actual irish pub , which is my favorite .\nAC-MLM it ’s not much like an actual irish pub , which is quite nice .\nAC-MLM-SS it ’s pretty much like an actual irish pub , which is very fantastic.\nTable 4: Example outputs on YELP. “Human” line is a human annotated sentence. Original negative attribute markers are colored in red,\ntransferred positive ones are colored in blue.\nYELP AMAZON\nGra Con Att Gra Con Att\nDeleteAndRetrieval 3.4 3.5 3.6 3.5 3.2 3.3\nw/frequency-ratio\nAC-MLM-SS 3.9 3.2 4.2 3.8 3.6 3.7\nw/attention-based\nAC-MLM-SS 4.0 3.8 4.4 3.9 3.7 3.7\nw/fusion-method\nAC-MLM-SS 4.2 4.0 4.4 4.1 4.0 4.0\nTable 5: Human evaluation results on two datasets. We show av-\nerage human ratings for grammaticality (Gra), content preservation\n(Con), target attribute match (Att).\nFigure 3: The trend of BLEU with the increase of accuracy.\nloss from classiﬁer improves accuracy largely by encouraging\nmore attribute-compatible words, which decreases the diver-\nsity. The two losses are adversarial and cooperate to balance\nthe accuracy and BLEU of transferred sentences. We plot\nthe trade-off curve of the two indicators in Figure 3. We can\nmodify the hyperparameter η in Equation 10 to control the\ntrend.\nComparing Three Mask Methods In general, fusion-\nmethod performs best on automatic evaluation. On the\nhigh-quality Yelp dataset, fusion-base method gains slight\nimprovement and performs closely to the attention-based\nmethod. But on the Amazon dataset, not as good as Yelp,\nthe fusion-method gains large improvement by combining the\nmerits of the attention-based and frequency-ratio methods.\nBeneﬁts of Masked Language Model Compared to the\nmodels using LSTM decoder in Table 2, our models highly\nimprove the BLEU on both datasets. Compared to [Yang\net al., 2018] in table 3, which introduces the target domain\nlanguage model as the discriminator, we also achieve com-\nparable performance. But training a target domain language\nmodel from scratch is very time consuming and in need of\nlarge corpus. We directly ﬁne-tune on the pre-trained BERT.\nConnection to PG-Net Our models can be viewed as a col-\nlaboration between copying and generating, which is similar\nto the hybrid pointer-generator network [See et al., 2017] for\ntext summarization task. Copying aids accurate reproduction\nof content information, while retaining the ability to produce\nstylish words through the generator. Unlike PGNet which\ncopies words from the source text via pointing, our approach\ncopies words from the source via masking.\n6 Conclusions and Future Work\nIn this paper, we focus on non-parallel sentiment transfer task\nand propose a two-stage “mask and inﬁll” approach that en-\nables training in the absence of parallel training data. Experi-\nmental results on two review datasets show that our approach\noutperforms the state-of-the-art systems, both in transfer ac-\ncuracy and semantic preservation. For future work, we would\nlike to explore a ﬁne-grained version (more than two senti-\nments) of sentiment transfer, and explore how to apply the\nmasked language model to other tasks of natural language\ngeneration beyond style transfer.\nAcknowledgments\nThis research is supported in part by the National Key\nResearch and Development Program of China under\nGrant 2018YFC0806900 and 2017YFB1010000, the Bei-\njing Municipal Science and Technology Project under Grant\nZ181100002718004.\nReferences\n[Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track\nProceedings, 2015.\n[Cho et al., 2014] Kyunghyun Cho, Bart van Merrienboer,\nC ¸ aglar G¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. Learning phrase\nrepresentations using RNN encoder-decoder for statistical\nmachine translation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Process-\ning, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A\nmeeting of SIGDAT, a Special Interest Group of the ACL ,\npages 1724–1734, 2014.\n[Dai and Le, 2015] Andrew M Dai and Quoc V Le. Semi-\nsupervised sequence learning. pages 3079–3087, 2015.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Fu et al., 2018] Zhenxin Fu, Xiaoye Tan, Nanyun Peng,\nDongyan Zhao, and Rui Yan. Style transfer in text: Ex-\nploration and evaluation. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), New Orleans, Louisiana, USA, February 2-7, 2018 ,\npages 663–670, 2018.\n[He and McAuley, 2016] Ruining He and Julian McAuley.\nUps and downs: Modeling the visual evolution of fashion\ntrends with one-class collaborative ﬁltering. pages 507–\n517, 2016.\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and\nJ¨urgen Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\n[Howard and Ruder, 2018] Jeremy Howard and Sebastian\nRuder. Universal language model ﬁne-tuning for text clas-\nsiﬁcation. 1:328–339, 2018.\n[Hu et al., 2017] Zhiting Hu, Zichao Yang, Xiaodan Liang,\nRuslan Salakhutdinov, and Eric P Xing. Toward controlled\ngeneration of text. pages 1587–1596, 2017.\n[Huang and Belongie, 2017] Xun Huang and Serge Be-\nlongie. Arbitrary style transfer in real-time with adaptive\ninstance normalization. pages 1501–1510, 2017.\n[Kim, 2014] Yoon Kim. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Process-\ning, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A\nmeeting of SIGDAT, a Special Interest Group of the ACL ,\npages 1746–1751, 2014.\n[Li et al., 2018] Juncen Li, Robin Jia, He He, and Percy\nLiang. Delete, retrieve, generate: a simple approach\nto sentiment and style transfer. In Proceedings of the\n2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Orleans,\nLouisiana, USA, June 1-6, 2018, Volume 1 (Long Papers),\npages 1865–1874, 2018.\n[Logeswaran et al., 2018] Lajanugen Logeswaran, Honglak\nLee, and Samy Bengio. Content preserving text generation\nwith attribute controls. pages 5108–5118, 2018.\n[Papineni et al., 2002] Kishore Papineni, Salim Roukos,\nTodd Ward, and Wei-Jing Zhu. Bleu: a method for auto-\nmatic evaluation of machine translation. pages 311–318,\n2002.\n[Prabhumoye et al., 2018] Shrimai Prabhumoye, Yulia\nTsvetkov, Ruslan Salakhutdinov, and Alan W. Black.\nStyle transfer through back-translation. In Proceedings of\nthe 56th Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers , pages 866–876,\n2018.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan,\nTime Salimans, and Ilya Sutskever. Improving language\nunderstanding with unsupervised learning. Technical re-\nport, Technical report, OpenAI, 2018.\n[See et al., 2017] Abigail See, Peter J. Liu, and Christo-\npher D. Manning. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Lin-\nguistics, ACL 2017, Vancouver, Canada, July 30 - August\n4, Volume 1: Long Papers, pages 1073–1083, 2017.\n[Shen et al., 2017] Tianxiao Shen, Tao Lei, Regina Barzilay,\nand Tommi Jaakkola. Style transfer from non-parallel text\nby cross-alignment. pages 6830–6841, 2017.\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. Sequence to sequence learning with neural\nnetworks. pages 3104–3112, 2014.\n[Taylor, 1953] Wilson L Taylor. “cloze procedure”: A\nnew tool for measuring readability. Journalism Bulletin,\n30(4):415–433, 1953.\n[Wang et al., 2018] Alex Wang, Amanpreet Singh, Julian\nMichael, Felix Hill, Omer Levy, and Samuel R. Bow-\nman. GLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proceedings\nof the Workshop: Analyzing and Interpreting Neural Net-\nworks for NLP , BlackboxNLP@EMNLP 2018, Brussels,\nBelgium, November 1, 2018, pages 353–355, 2018.\n[Xu et al., 2018] Jingjing Xu, Sun Xu, Qi Zeng, Xiaodong\nZhang, Xuancheng Ren, Houfeng Wang, and Wenjie Li.\nUnpaired sentiment-to-sentiment translation: A cycled re-\ninforcement learning approach. pages 979–988, 2018.\n[Yang et al., 2018] Zichao Yang, Zhiting Hu, Chris Dyer,\nEric P Xing, and Taylor Berg-Kirkpatrick. Unsupervised\ntext style transfer using language models as discrimina-\ntors. pages 7298–7309, 2018.\n[Zhu et al., 2019] Wanrong Zhu, Zhiting Hu, and Eric Xing.\nText inﬁlling. CoRR, abs/1901.00158, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7949459552764893
    },
    {
      "name": "Sentence",
      "score": 0.7101846933364868
    },
    {
      "name": "Infill",
      "score": 0.7002924680709839
    },
    {
      "name": "Natural language processing",
      "score": 0.653224766254425
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6208149194717407
    },
    {
      "name": "Sentiment analysis",
      "score": 0.601982057094574
    },
    {
      "name": "Task (project management)",
      "score": 0.5526319146156311
    },
    {
      "name": "Scratch",
      "score": 0.539808452129364
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5397198796272278
    },
    {
      "name": "Encoder",
      "score": 0.5169755220413208
    },
    {
      "name": "CRFS",
      "score": 0.47258198261260986
    },
    {
      "name": "Process (computing)",
      "score": 0.4523150324821472
    },
    {
      "name": "Speech recognition",
      "score": 0.32021939754486084
    },
    {
      "name": "Conditional random field",
      "score": 0.2711736559867859
    },
    {
      "name": "Engineering",
      "score": 0.06744655966758728
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}