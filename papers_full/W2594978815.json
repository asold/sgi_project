{
  "title": "Data Noising as Smoothing in Neural Network Language Models",
  "url": "https://openalex.org/W2594978815",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2252931716",
      "name": "Xie, Ziang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222413866",
      "name": "Wang, Sida I.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225948551",
      "name": "Li Jiwei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "L\\'evy, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2373260554",
      "name": "Nie Ai-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223899260",
      "name": "Jurafsky, Dan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2587129682",
      "name": "Ng, Andrew Y.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2175402905",
    "https://openalex.org/W1800356822",
    "https://openalex.org/W2949176378",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2950304420",
    "https://openalex.org/W2250968750",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2271840356",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W3204406378",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2178103884",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2131494463",
    "https://openalex.org/W2300605907",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2311117160",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2210838531"
  ],
  "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.",
  "full_text": "Published as a conference paper at ICLR 2017\nDATA NOISING AS SMOOTHING IN NEURAL NETWORK\nLANGUAGE MODELS\nZiang Xie, Sida I. Wang, Jiwei Li, Daniel L´evy, Aiming Nie, Dan Jurafsky, Andrew Y. Ng\nComputer Science Department, Stanford University\n{zxie,sidaw,danilevy,anie,ang}@cs.stanford.edu,\n{jiweil,jurafsky}@stanford.edu\nABSTRACT\nData noising is an effective technique for regularizing neural network models.\nWhile noising is widely adopted in application domains such as vision and speech,\ncommonly used noising primitives have not been developed for discrete sequence-\nlevel settings such as language modeling. In this paper, we derive a connection\nbetween input noising in neural network language models and smoothing in n-\ngram models. Using this connection, we draw upon ideas from smoothing to\ndevelop effective noising schemes. We demonstrate performance gains when ap-\nplying the proposed schemes to language modeling and machine translation. Fi-\nnally, we provide empirical analysis validating the relationship between noising\nand smoothing.\n1 I NTRODUCTION\nLanguage models are a crucial component in many domains, such as autocompletion, machine trans-\nlation, and speech recognition. A key challenge when performing estimation in language modeling\nis the data sparsity problem: due to large vocabulary sizes and the exponential number of possi-\nble contexts, the majority of possible sequences are rarely or never observed, even for very short\nsubsequences.\nIn other application domains, data augmentation has been key to improving the performance of\nneural network models in the face of insufﬁcient data. In computer vision, for example, there exist\nwell-established primitives for synthesizing additional image data, such as by rescaling or applying\nafﬁne distortions to images (LeCun et al., 1998; Krizhevsky et al., 2012). Similarly, in speech\nrecognition adding a background audio track or applying small shifts along the time dimension has\nbeen shown to yield signiﬁcant gains, especially in noisy settings (Deng et al., 2000; Hannun et al.,\n2014). However, widely-adopted noising primitives have not yet been developed for neural network\nlanguage models.\nClassic n-gram models of language cope with rare and unseen sequences by using smoothing meth-\nods, such as interpolation or absolute discounting (Chen & Goodman, 1996). Neural network mod-\nels, however, have no notion of discrete counts, and instead use distributed representations to combat\nthe curse of dimensionality (Bengio et al., 2003). Despite the effectiveness of distributed represen-\ntations, overﬁtting due to data sparsity remains an issue. Existing regularization methods, however,\nare typically applied to weights or hidden units within the network (Srivastava et al., 2014; Le et al.,\n2015) instead of directly considering the input data.\nIn this work, we consider noising primitives as a form of data augmentation for recurrent neural\nnetwork-based language models. By examining the expected pseudocounts from applying the nois-\ning schemes, we draw connections between noising and linear interpolation smoothing. Using this\nconnection, we then derive noising schemes that are analogues of more advanced smoothing meth-\nods. We demonstrate the effectiveness of these schemes for regularization through experiments on\nlanguage modeling and machine translation. Finally, we validate our theoretical claims by examin-\ning the empirical effects of noising.\n1\narXiv:1703.02573v1  [cs.LG]  7 Mar 2017\nPublished as a conference paper at ICLR 2017\n2 R ELATED WORK\nOur work can be viewed as a form of data augmentation, for which to the best of our knowledge there\nexists no widely adopted schemes in language modeling with neural networks. Classical regulariza-\ntion methods such as L2-regularization are typically applied to the model parameters, while dropout\nis applied to activations which can be along the forward as well as the recurrent directions (Zaremba\net al., 2014; Semeniuta et al., 2016; Gal, 2015). Others have introduced methods for recurrent neural\nnetworks encouraging the hidden activations to remain stable in norm, or constraining the recurrent\nweight matrix to have eigenvalues close to one (Krueger & Memisevic, 2015; Arjovsky et al., 2015;\nLe et al., 2015). These methods, however, all consider weights and hidden units instead of the input\ndata, and are motivated by the vanishing and exploding gradient problem.\nFeature noising has been demonstrated to be effective for structured prediction tasks, and has been\ninterpreted as an explicit regularizer (Wang et al., 2013). Additionally, Wager et al. (2014) show\nthat noising can inject appropriate generative assumptions into discriminative models to reduce their\ngeneralization error, but do not consider sequence models (Wager et al., 2016).\nThe technique of randomly zero-masking input word embeddings for learning sentence represen-\ntations has been proposed by Iyyer et al. (2015), Kumar et al. (2015), and Dai & Le (2015), and\nadopted by others such as Bowman et al. (2015). However, to the best of our knowledge, no analysis\nhas been provided besides reasoning that zeroing embeddings may result in a model ensembling\neffect similar to that in standard dropout. This analysis is applicable to classiﬁcation tasks involving\nsum-of-embeddings or bag-of-words models, but does not capture sequence-level effects. Bengio\net al. (2015) also make an empirical observation that the method of randomly replacing words with\nﬁxed probability with a draw from the uniform distribution improved performance slightly for an\nimage captioning task; however, they do not examine why performance improved.\n3 M ETHOD\n3.1 P RELIMINARIES\nWe consider language models where given a sequence of indices X = (x1,x2,··· ,xT), over the\nvocabulary V, we model\np(X) =\nT∏\nt=1\np(xt|x<t)\nIn n-gram models, it is not feasible to model the full context x<t for large tdue to the exponential\nnumber of possible histories. Recurrent neural network (RNN) language models can (in theory)\nmodel longer dependencies, since they operate over distributed hidden states instead of modeling an\nexponential number of discrete counts (Bengio et al., 2003; Mikolov, 2012).\nAn L-layer recurrent neural network is modeled as h(l)\nt = fθ(h(l)\nt−1,h(l−1)\nt ), where l denotes the\nlayer index, h(0) contains the one-hot encoding of X, and in its simplest form fθ applies an afﬁne\ntransformation followed by a nonlinearity. In this work, we use RNNs with a more complex form of\nfθ, namely long short-term memory (LSTM) units (Hochreiter & Schmidhuber, 1997), which have\nbeen shown to ease training and allow RNNs to capture longer dependencies. The output distribution\nover the vocabularyV at time tis pθ(xt|x<t) = softmax(gθ(h(L)\nt )), where g: R|h|→R|V|applies\nan afﬁne transformation. The RNN is then trained by minimizing over its parametersθthe sequence\ncross-entropy loss ℓ(θ) =−∑\ntlog pθ(xt|x<t), thus maximizing the likelihood pθ(X).\nAs an extension, we also consider encoder-decoder or sequence-to-sequence (Cho et al., 2014;\nSutskever et al., 2014) models where given an input sequence X and output sequence Y of length\nTY, we model\np(Y|X) =\nTY∏\nt=1\np(yt|X,y<t).\nand minimize the loss ℓ(θ) =−∑\ntlog pθ(yt|X,y<t). This setting can also be seen as conditional\nlanguage modeling, and encompasses tasks such as machine translation, where X is a source lan-\n2\nPublished as a conference paper at ICLR 2017\nguage sequence and Y a target language sequence, as well as language modeling, where Y is the\ngiven sequence and X is the empty sequence.\n3.2 S MOOTHING AND NOISING\nRecall that for a given context length l, an n-gram model of order l+ 1is optimal under the log-\nlikelihood criterion. Hence in the case where an RNN with ﬁnite context achieves near the lowest\npossible cross-entropy loss, it behaves like an n-gram model.\nLike n-gram models, RNNs are trained using maximum likelihood, and can easily overﬁt (Zaremba\net al., 2014). While generic regularization methods suchL2-regularization and dropout are effective,\nthey do not take advantage of speciﬁc properties of sequence modeling. In order to understand\nsequence-speciﬁc regularization, it is helpful to examinen-gram language models, whose properties\nare well-understood.\nSmoothing for n-gram models When modeling p(xt|x<t), the maximum likelihood estimate\nc(x<t,xt)/c(x<t) based on empirical counts puts zero probability on unseen sequences, and thus\nsmoothing is crucial for obtaining good estimates. In particular, we consider interpolation, which\nperforms a weighted average between higher and lower order models. The idea is that when there\nare not enough observations of the full sequence, observations of subsequences can help us obtain\nbetter estimates.1 For example, in a bigram model,pinterp(xt|xt−1) =λp(xt|xt−1) + (1−λ)p(xt),\nwhere 0 ≤λ≤1.\nNoising for RNN models We would like to apply well-understood smoothing methods such as\ninterpolation to RNNs, which are also trained using maximum likelihood. Unfortunately, RNN\nmodels have no notion of counts, and we cannot directly apply one of the usual smoothing methods.\nIn this section, we consider two simple noising schemes which we proceed to show correspond to\nsmoothing methods. Since we can noise the data while training an RNN, we can then incorporate\nwell-understood generative assumptions that are known to be helpful in the domain. First consider\nthe following two noising schemes:\n• unigram noising For each xi in x<t, with probability γ replace xi with a sample from the\nunigram frequency distribution.\n• blank noising For each xi in x<t, with probability γreplace xi with a placeholder token “ ”.\nWhile blank noising can be seen as a way to avoid overﬁtting on speciﬁc contexts, we will see that\nboth schemes are related to smoothing, and that unigram noising provides a path to analogues of\nmore advanced smoothing methods.\n3.3 N OISING AS SMOOTHING\nWe now consider the maximum likelihood estimate of n-gram probabilities estimated using the\npseudocounts of the noised data. By examining these estimates, we draw a connection between\nlinear interpolation smoothing and noising.\nUnigram noising as interpolationTo start, we consider the simplest case of bigram probabilities.\nLet c(x) denote the count of a tokenxin the original data, and letcγ(x)\ndef\n= E˜x[c(˜x)] be the expected\ncount of xunder the unigram noising scheme. We then have\npγ(xt|xt−1) =cγ(xt−1,xt)\ncγ(xt−1)\n= [(1−γ)c(xt−1,xt) +γp(xt−1)c(xt)]/c(xt−1)\n= (1−γ)p(xt|xt−1) +γp(xt),\nwhere cγ(x) =c(x) since our proposal distributionq(x) is the unigram distribution, and the last line\nfollows since c(xt−1)/p(xt−1) =c(xt)/p(xt) is equal to the total number of tokens in the training\n1For a thorough review of smoothing methods, we defer to Chen & Goodman (1996).\n3\nPublished as a conference paper at ICLR 2017\nset. Thus we see that the noised data has pseudocounts corresponding to interpolation or a mixture\nof different order n-gram models with ﬁxed weighting.\nMore generally, let ˜x<t be noised tokens from ˜x. We consider the expected prediction under noise\npγ(xt|x<t) =E˜x<t [p(xt|˜x<t)]\n=\n∑\nJ\nπ(|J|)  \np(|J|swaps)\n∑\nxK\np(xt|xJ,xK)  \np(xt|noised context)\n∏\nz∈xK\np(z)\np(drawing z)\nwhere the mixture coefﬁcients are π(|J|) = (1 −γ)|J|γt−1−|J| with ∑\nJ π(|J|) = 1. J ⊆\n{1,2,...,t −1}denotes the set of indices whose corresponding tokens are left unchanged, and\nKthe set of indices that were replaced.\nBlank noising as interpolation Next we consider the blank noising scheme and show that it cor-\nresponds to interpolation as well. This also serves as an alternative explanation for the gains that\nother related work have found with the “word-dropout” idea (Kumar et al., 2015; Dai & Le, 2015;\nBowman et al., 2015). As before, we do not noise the token being predicted xt. Let ˜x<t denote the\nrandom variable where each of its tokens is replaced by “ ” with probability γ, and let xJ denote\nthe sequence with indices J unchanged, and the rest replaced by “ ”. To make a prediction, we use\nthe expected probability over different noisings of the context\npγ(xt|x<t) =E˜x<t [p(xt|˜x<t)] =\n∑\nJ\nπ(|J|)  \np(|J|swaps)\np(xt|xJ)  \np(xt|noised context)\n,\nwhere J ⊆{1,2,...,t −1}, which is also a mixture of the unnoised probabilities over subsequences\nof the current context. For example, in the case of trigrams, we have\npγ(x3|x1,x2) =π(2) p(x3|x1,x2) +π(1) p(x3|x1, ) +π(1) p(x3| ,x2) +π(0) p(x3| , )\nwhere the mixture coefﬁcient π(i) = (1−γ)iγ2−i.\n3.4 B ORROWING TECHNIQUES\nWith the connection between noising and smoothing in place, we now consider how we can improve\nthe two components of the noising scheme by considering:\n1. Adaptively computing noising probability γ to reﬂect our conﬁdence about a particular\ninput subsequence.\n2. Selecting a proposal distribution q(x) that is less naive than the unigram distribution by\nleveraging higher order n-gram statistics.\nNoising Probability Although it simpliﬁes analysis, there is no reason why we should choose\nﬁxed γ; we now consider deﬁning an adaptive γ(x1:t) which depends on the input sequence. Con-\nsider the following bigrams:\n“and the” “Humpty Dumpty”\nThe ﬁrst bigram is one of the most common in English corpora; its probability is hence well esti-\nmated and should not be interpolated with lower order distributions. In expectation, however, using\nﬁxed γ0 when noising results in the same lower order interpolation weight πγ0 for common as well\nas rare bigrams. Intuitively, we should deﬁne γ(x1:t) such that commonly seen bigrams are less\nlikely to be noised.\nThe second bigram, “Humpty Dumpty,” is relatively uncommon, as are its constituent unigrams.\nHowever, it forms what Brown et al. (1992) term a “sticky pair”: the unigram “Dumpty” almost\nalways follows the unigram “Humpty”, and similarly, “Humpty” almost always precedes “Dumpty”.\nFor pairs with high mutual information, we wish to avoid backing off from the bigram to the unigram\ndistribution.\n4\nPublished as a conference paper at ICLR 2017\nNoised γ(x1:2) q(x) Analogue\nx1 γ0 q(“ ”) = 1 interpolation\nx1 γ0 unigram interpolation\nx1 γ0N1+(x1,•)/c(x1) unigram absolute discounting\nx1,x2 γ0N1+(x1,•)/c(x1) q(x) ∝N1+(•,x) Kneser-Ney\nTable 1: Noising schemesExample noising schemes and their bigram smoothing analogues. Here\nwe consider the bigram probability p(x1,x2) = p(x2|x1)p(x1). Notation: γ(x1:t) denotes the\nnoising probability for a given input sequence x1:t, q(x) denotes the proposal distribution, and\nN1+(x,•) denotes the number of distinct bigrams in the training set where xis the ﬁrst unigram. In\nall but the last case we only noise the context x1 and not the target prediction x2.\nLet N1+(x1,•)\ndef\n= |{x2 : c(x1,x2) > 0}|be the number of distinct continutions following x1, or\nequivalently the number of bigram types beginning with x1 (Chen & Goodman, 1996). From the\nabove intuitions, we arrive at the absolute discountingnoising probability\nγAD(x1) =γ0\nN1+(x1,•)∑\nx2 c(x1,x2)\nwhere for 0 ≤γ0 ≤1 we have 0 ≤γAD ≤1, though in practice we can also clip larger nois-\ning probabilities to 1. Note that this encourages noising of unigrams that precede many possible\nother tokens while discouraging noising of common unigrams, since if we ignore the ﬁnal token,∑\nx2 c(x1,x2) =c(x1).\nProposal Distribution While choosing the unigram distribution as the proposal distribution q(x)\npreserves unigram frequencies, by borrowing from the smoothing literature we ﬁnd another distri-\nbution performs better. We again begin with two motivating examples:\n“San Francisco” “New York”\nBoth bigrams appear frequently in text corpora. As a direct consequence, the unigrams “Francisco”\nand “York” also appear frequently. However, since “Francisco” and “York” typically follow “San”\nand “New”, respectively, they should not have high probability in the proposal distribution as they\nmight if we use unigram frequencies (Chen & Goodman, 1996). Instead, it would be better to\nincrease the proposal probability of unigrams with diverse histories, or more precisely unigrams that\ncomplete a large number of bigram types. Thus instead of drawing from the unigram distribution,\nwe consider drawing from\nq(x) ∝N1+(•,x)\nNote that we now noise the prediction xt in addition to the context x1:t−1. Combining this new\nproposal distribution with the discounted γAD(x1) from the previous section, we obtain the noising\nanalogue of Kneser-Ney smoothing.\nTable 1 summarizes the discussed noising schemes.\n3.5 T RAINING AND TESTING\nDuring training, noising is performed per batch and is done online such that each epoch of training\nsees a different noised version of the training data. At test time, to match the training objective we\nshould sample multiple corrupted versions of the test data, then average the predictions (Srivastava\net al., 2014). In practice, however, we ﬁnd that simply using the maximum likelihood (uncorrupted)\ninput sequence works well; evaluation runtime remains unchanged.\n3.6 E XTENSIONS\nThe schemes described are for the language model setting. To extend them to the sequence-to-\nsequence or encoder-decoder setting, we noise both x<t as well as y<t. While in the decoder we\n5\nPublished as a conference paper at ICLR 2017\nNoising scheme Validation Test\nMedium models (512 hidden size)\nnone (dropout only) 84.3 80.4\nblank 82.7 78.8\nunigram 83.1 80.1\nbigram Kneser-Ney 79.9 76.9\nLarge models (1500 hidden size)\nnone (dropout only) 81.6 77.5\nblank 79.4 75.5\nunigram 79.4 76.1\nbigram Kneser-Ney 76.2 73.4\nZaremba et al. (2014) 82.2 78.4\nGal (2015) variational dropout (tied weights) 77.3 75.0\nGal (2015) (untied weights, Monte Carlo) — 73.4\nTable 2: Single-model perplexity on Penn Treebank with different noising schemes. We also com-\npare to the variational method of Gal (2015), who also train LSTM models with the same hidden\ndimension. Note that performing Monte Carlo dropout at test time is signiﬁcantly more expensive\nthan our approach, where test time is unchanged.\nNoising scheme Validation Test\nnone 94.3 123.6\nblank 85.0 110.7\nunigram 85.2 111.3\nbigram Kneser-Ney 84.5 110.6\nTable 3: Perplexity on Text8 with different noising schemes.\nhave y<t and yt as analogues to language model context and target prediction, it is unclear whether\nnoising x<t should be beneﬁcial. Empirically, however, we ﬁnd this to be the case (Table 4).\n4 E XPERIMENTS\n4.1 L ANGUAGE MODELING\nPenn Treebank We train networks for word-level language modeling on the Penn Treebank\ndataset, using the standard preprocessed splits with a 10K size vocabulary (Mikolov, 2012). The\nPTB dataset contains 929k training tokens, 73k validation tokens, and 82k test tokens. Following\nZaremba et al. (2014), we use minibatches of size 20 and unroll for 35 time steps when performing\nbackpropagation through time. All models have two hidden layers and use LSTM units. Weights\nare initialized uniformly in the range [−0.1,0.1]. We consider models with hidden sizes of 512 and\n1500.\nWe train using stochastic gradient descent with an initial learning rate of 1.0, clipping the gradient\nif its norm exceeds 5.0. When the validation cross entropy does not decrease after a training epoch,\nwe halve the learning rate. We anneal the learning rate 8 times before stopping training, and pick\nthe model with the lowest perplexity on the validation set.\nFor regularization, we apply feed-forward dropout (Pham et al., 2014) in combination with our\nnoising schemes. We report results in Table 2 for the best setting of the dropout rate (which we\nﬁnd to match the settings reported in Zaremba et al. (2014)) as well as the best setting of noising\n6\nPublished as a conference paper at ICLR 2017\n10 20 30 40 50 60\nEpochs\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200Perplexity\ntraining, unnoised\nvalidation, unnoised\ntraining, γ0 = 0.2\nvalidation, γ0 = 0.2\n(a) Penn Treebank corpus.\n0 10 20 30 40 50 60\nEpochs\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200Perplexity\ntraining, unnoised\nvalidation, unnoised\ntraining, γ0 = 0.6\nvalidation, γ0 = 0.6 (b) Text8 corpus.\nFigure 1: Example training and validation curves for an unnoised model and model regularized\nusing the bigram Kneser-Ney noising scheme.\nScheme Perplexity BLEU\ndropout, no noising 8.84 24.6\nblank noising 8.28 25.3 ( +0.7)\nunigram noising 8.15 25.5 ( +0.9)\nbigram Kneser-Ney 7.92 26.0 ( +1.4)\nsource only 8.74 24.8 ( +0.2)\ntarget only 8.14 25.6 ( +1.0)\nTable 4: Perplexities and BLEU scores for machine translation task. Results for bigram KN noising\non only the source sequence and only the target sequence are given as well.\nprobability γ0 on the validation set. 2 Figure 1 shows the training and validation perplexity curves\nfor a noised versus an unnoised run.\nOur large models match the state-of-the-art regularization method for single model performance\non this task. In particular, we ﬁnd that picking γAD(x1) and q(x) corresponding to Kneser-Ney\nsmoothing yields signiﬁcant gains in validation perplexity, both for the medium and large size mod-\nels. Recent work (Merity et al., 2016; Zilly et al., 2016) has also achieved impressive results on this\ntask by proposing different architectures which are orthogonal to our data augmentation schemes.\nText8 In order to determine whether noising remains effective with a larger dataset, we perform\nexperiments on the Text8 corpus3. The ﬁrst 90M characters are used for training, the next 5M for\nvalidation, and the ﬁnal 5M for testing, resulting in 15.3M training tokens, 848K validation tokens,\nand 855K test tokens. We preprocess the data by mapping all words which appear 10 or fewer times\nto the unknown token, resulting in a 42K size vocabulary. Other parameter settings are the same\nas described in the Penn Treebank experiments, besides that only models with hidden size 512 are\nconsidered, and noising is not combined with feed-forward dropout. Results are given in Table 3.\n4.2 M ACHINE TRANSLATION\nFor our machine translation experiments we consider the English-German machine translation track\nof IWSLT 20154. The IWSLT 2015 corpus consists of sentence-aligned subtitles of TED and TEDx\ntalks. The training set contains roughly 190K sentence pairs with 5.4M tokens. Following Luong &\nManning (2015), we use TED tst2012 as a validation set and report BLEU score results (Papineni\net al., 2002) on tst2014. We limit the vocabulary to the top 50K most frequent words for each\nlanguage.\n2Code will be made available at: http://deeplearning.stanford.edu/noising\n3http://mattmahoney.net/dc/text8.zip\n4http://workshop2015.iwslt.org/\n7\nPublished as a conference paper at ICLR 2017\n0 0.2 0.4 0.6 0.8 150\n100\n150\n200\nγ0 (unscaled)\nPerplexity\nγ = 0\nγ0\nγAD\nFigure 2: Perplexity with noising on Penn Tree-\nbank while varying the value of γ0. Using dis-\ncounting to scale γ0 (yielding γAD) maintains\ngains for a range of values of noising probabil-\nity, which is not true for the unscaled case.\nuniform unigram\n1\n2\n3\n4\np\nDKL(p∥ˆp)\nno noise\nγ = 0.1\nγ = 0.25\nFigure 3: Mean KL-divergence over validation\nset between softmax distributions of noised and\nunnoised models and lower order distributions.\nNoised model distributions are closer to the uni-\nform and unigram frequency distributions.\nWe train a two-layer LSTM encoder-decoder network (Sutskever et al., 2014; Cho et al., 2014) with\n512 hidden units in each layer. The decoder uses an attention mechanism (Bahdanau et al., 2014)\nwith the dot alignment function (Luong et al., 2015). The initial learning rate is 1.0 and we start\nhalving the learning rate when the relative difference in perplexity on the validation set between\ntwo consecutive epochs is less than 1%. We follow training protocols as described in Sutskever\net al. (2014): (a) LSTM parameters and word embeddings are initialized from a uniform distribution\nbetween [−0.1,0.1], (b) inputs are reversed, (c) batch size is set to 128, (d) gradient clipping is\nperformed when the norm exceeds a threshold of 5. We set hidden unit dropout rate to 0.2 across all\nsettings as suggested in Luong et al. (2015). We compare unigram, blank, and bigram Kneser-Ney\nnoising. Noising rate γis selected on the validation set.\nResults are shown in Table 4. We observe performance gains for both blank noising and unigram\nnoising, giving roughly +0.7 BLEU score on the test set. The proposed bigram Kneser-Ney noising\nscheme gives an additional performance boost of +0.5-0.7 on top of the blank noising and unigram\nnoising models, yielding a total gain of +1.4 BLEU.\n5 D ISCUSSION\n5.1 S CALING γ VIA DISCOUNTING\nWe now examine whether discounting has the desired effect of noising subsequences according to\ntheir uncertainty. If we consider the discounting\nγAD(x1) =γ0\nN1+(x1,•)\nc(x1)\nwe observe that the denominator c(x1) can dominate than the numerator N1+(x1,•). Common\ntokens are often noised infrequently when discounting is used to rescale the noising probability,\nwhile rare tokens are noised comparatively much more frequently, where in the extreme case when\na token appears exactly once, we have γAD = γ0. Due to word frequencies following a Zipﬁan\npower law distribution, however, common tokens constitute the majority of most texts, and thus\ndiscounting leads to signiﬁcantly less noising.\nWe compare the performance of models trained with a ﬁxed γ0 versus a γ0 rescaled using discount-\ning. As shown in Figure 2, bigram discounting leads to gains in perplexity for a much broader range\nof γ0. Thus the discounting ratio seems to effectively capture the “right” tokens to noise.\n8\nPublished as a conference paper at ICLR 2017\nNoising Bigrams Trigrams\nnone (dropout only) 2881 381\nblank noising 2760 372\nunigram noising 2612 365\nTable 5: Perplexity of last unigram for unseen bigrams and trigrams in Penn Treebank validation\nset. We compare noised and unnoised models with noising probabilities chosen such that models\nhave near-identical perplexity on full validation set.\n5.2 N OISED VERSUS UNNOISED MODELS\nSmoothed distributions In order to validate that data noising for RNN models has a similar effect\nto that of smoothing counts in n-gram models, we consider three models trained with unigram\nnoising as described in Section 4.1 on the Penn Treebank corpus with γ = 0(no noising), γ = 0.1,\nand γ = 0.25. Using the trained models, we measure the Kullback-Leibler divergenceDKL(p∥q) =∑\nipilog(pi/qi) over the validation set between the predicted softmax distributions, ˆp, and the\nuniform distribution as well as the unigram frequency distribution. We then take the mean KL\ndivergence over all tokens in the validation set.\nRecall that in interpolation smoothing, a weighted combination of higher and lower order n-gram\nmodels is used. As seen in Figure 3, the softmax distributions of noised models are signiﬁcantly\ncloser to the lower order frequency distributions than unnoised models, in particular in the case of\nthe unigram distribution, thus validating our analysis in Section 3.3.\nUnseen n-grams Smoothing is most beneﬁcial for increasing the probability of unobserved se-\nquences. To measure whether noising has a similar effect, we consider bigrams and trigrams in the\nvalidation set that do not appear in the training set. For these unseen bigrams (15062 occurrences)\nand trigrams (43051 occurrences), we measure the perplexity for noised and unnoised models with\nnear-identical perplexity on the full set. As expected, noising yields lower perplexity for these un-\nseen instances.\n6 C ONCLUSION\nIn this work, we show that data noising is effective for regularizing neural network-based sequence\nmodels. By deriving a correspondence between noising and smoothing, we are able to adapt ad-\nvanced smoothing methods for n-gram models to the neural network setting, thereby incorporat-\ning well-understood generative assumptions of language. Possible applications include exploring\nnoising for improving performance in low resource settings, or examining how these techniques\ngeneralize to sequence modeling in other domains.\nACKNOWLEDGMENTS\nWe thank Will Monroe for feedback on a draft of this paper, Anand Avati for help running exper-\niments, and Jimmy Wu for computing support. We also thank the developers of Theano (Theano\nDevelopment Team, 2016) and Tensorﬂow (Abadi et al., 2016). Some GPUs used in this work\nwere donated by NVIDIA Corporation. ZX, SW, and JL were supported by an NDSEG Fellowship,\nNSERC PGS-D Fellowship, and Facebook Fellowship, respectively. This project was funded in part\nby DARPA MUSE award FA8750-15-C-0242 AFRL/RIKF.\nREFERENCES\nMartın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale machine\nlearning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n9\nPublished as a conference paper at ICLR 2017\nMartin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.\narXiv preprint arXiv:1511.06464, 2015.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\nprediction with recurrent neural networks. In Neural Information Processing Systems (NIPS),\n2015.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. In Journal Of Machine Learning Research, 2003.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-\ngio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.\nPeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. Class-\nbased n-gram models of natural language. Computational linguistics, 1992.\nStanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language\nmodeling. In Association for Computational Linguistics (ACL), 1996.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in Neural Infor-\nmation Processing Systems, pp. 3061–3069, 2015.\nLi Deng, Alex Acero, Mike Plumpe, and Xuedong Huang. Large-vocabulary speech recognition\nunder adverse acoustic environments. In ICSLP, 2000.\nYarin Gal. A theoretically grounded application of dropout in recurrent neural networks.\narXiv:1512.05287, 2015.\nAwni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, et al. Deep speech: Scaling\nup end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 1997.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum´e III. Deep unordered compo-\nsition rivals syntactic methods for text classiﬁcation. InAssociation for Computatonal Linguistics\n(ACL), 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet Classiﬁcation with Deep Convolutional\nNeural Networks. In NIPS, 2012.\nDavid Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv preprint\narXiv:1511.08400, 2015.\nAnkit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter On-\ndruska, Ishaan Gulrajani, and Richard Socher. Ask me anything: Dynamic memory networks for\nnatural language processing. arXiv preprint arXiv:1506.07285, 2015.\nQuoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. A simple way to initialize recurrent networks\nof rectiﬁed linear units. arXiv preprint arXiv:1504.00941, 2015.\nY . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based Learning Applied to Document\nRecognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\nMinh-Thang Luong and Christopher D Manning. Stanford neural machine translation systems for\nspoken language domains. In Proceedings of the International Workshop on Spoken Language\nTranslation, 2015.\n10\nPublished as a conference paper at ICLR 2017\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. In Empirical Methods in Natural Language Processing\n(EMNLP), 2015.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nTom´aˇs Mikolov. Statistical language models based on neural networks. PhD thesis, PhD thesis,\nBrno University of Technology. 2012.[PDF], 2012.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics, pp. 311–318. Association for Computational Linguistics, 2002.\nVu Pham, Th ´eodore Bluche, Christopher Kermorvant, and J ´erˆome Louradour. Dropout improves\nrecurrent neural networks for handwriting recognition. In Frontiers in Handwriting Recognition\n(ICFHR), 2014 14th International Conference on, 2014.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss.\narXiv preprint arXiv:1603.05118, 2016.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine\nLearning Research, 2014.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pp. 3104–3112, 2014.\nTheano Development Team. Theano: A Python framework for fast computation of mathematical\nexpressions. arXiv e-prints, abs/1605.02688, May 2016. URL http://arxiv.org/abs/\n1605.02688.\nS. Wager, W. Fithian, S. I. Wang, and P. Liang. Altitude training: Strong bounds for single-layer\ndropout. In Advances in Neural Information Processing Systems (NIPS), 2014.\nStefan Wager, William Fithian, and Percy Liang. Data augmentation via levy processes. arXiv\npreprint arXiv:1603.06340, 2016.\nSida I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. Feature nois-\ning for log-linear structured prediction. In Empirical Methods in Natural Language Processing\n(EMNLP), 2013.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn ´ık, and J¨urgen Schmidhuber. Recurrent\nhighway networks. arXiv preprint arXiv:1607.03474, 2016.\n11\nPublished as a conference paper at ICLR 2017\nA S KETCH OF NOISING ALGORITHM\nWe provide pseudocode of the noising algorithm corresponding to bigram Kneser-Ney smoothing\nfor n-grams (In the case of sequence-to-sequence tasks, we estimate the count-based parameters\nseparately for source and target). To simplify, we assume a batch size of one. The noising algorithm\nis applied to each data batch during training. No noising is applied at test time.\nAlgorithm 1Bigram KN noising (Language modeling setting)\nRequire counts c(x), number of distinct continuations N1+(x,•), proposal distribution q(x) ∝\nN1+(•,x)\nInputs X, Y batch of unnoised data indices, scaling factor γ0\nprocedure NOISE BGKN( X,Y ) ⊿X = (x1,...,x t),Y = (x2,...,x t+1)\n˜X, ˜Y ←X,Y\nfor j = 1,...,t do\nγ ←γ0N1+(xj,•)/c(xj)\nif ∼Bernoulli(γ) then\n˜xj ∼Categorical(q) ⊿Updates ˜X\n˜yj ∼Categorical(q)\nend if\nend for\nreturn ˜X, ˜Y ⊿ Run training iteration with noised batch\nend procedure\n12",
  "topic": "Smoothing",
  "concepts": [
    {
      "name": "Smoothing",
      "score": 0.9262818694114685
    },
    {
      "name": "Computer science",
      "score": 0.7723682522773743
    },
    {
      "name": "Language model",
      "score": 0.6765681505203247
    },
    {
      "name": "Artificial neural network",
      "score": 0.6630123257637024
    },
    {
      "name": "Connection (principal bundle)",
      "score": 0.5792005062103271
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5460200905799866
    },
    {
      "name": "Machine translation",
      "score": 0.4907335042953491
    },
    {
      "name": "Machine learning",
      "score": 0.4886482357978821
    },
    {
      "name": "Sequence (biology)",
      "score": 0.42118513584136963
    },
    {
      "name": "Mathematics",
      "score": 0.09546127915382385
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}