{
    "title": "Neural Language Modeling with Visual Features",
    "url": "https://openalex.org/W2918609062",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4221454194",
            "name": "Anastasopoulos, Antonios",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2587552697",
            "name": "Kumar Shankar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3043320981",
            "name": "Liao, Hank",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2545177271",
        "https://openalex.org/W2963017553",
        "https://openalex.org/W1995820507",
        "https://openalex.org/W2293634267",
        "https://openalex.org/W2111078031",
        "https://openalex.org/W2797328513",
        "https://openalex.org/W2806386169",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W2625366777",
        "https://openalex.org/W2798597276",
        "https://openalex.org/W2784025607",
        "https://openalex.org/W1764126038",
        "https://openalex.org/W2948176064",
        "https://openalex.org/W2198114369",
        "https://openalex.org/W2149557440",
        "https://openalex.org/W2110933980",
        "https://openalex.org/W2119775030",
        "https://openalex.org/W3098232790",
        "https://openalex.org/W2164290393",
        "https://openalex.org/W2171361956",
        "https://openalex.org/W2524365899",
        "https://openalex.org/W1861492603"
    ],
    "abstract": "Multimodal language models attempt to incorporate non-linguistic features for the language modeling task. In this work, we extend a standard recurrent neural network (RNN) language model with features derived from videos. We train our models on data that is two orders-of-magnitude bigger than datasets used in prior work. We perform a thorough exploration of model architectures for combining visual and text features. Our experiments on two corpora (YouCookII and 20bn-something-something-v2) show that the best performing architecture consists of middle fusion of visual and text features, yielding over 25% relative improvement in perplexity. We report analysis that provides insights into why our multimodal language model improves upon a standard RNN language model.",
    "full_text": "Neural Language Modeling with Visual Features\nAntonios Anastasopoulos1,∗ Shankar Kumar2 and Hank Liao2\n1Language Technologies Institute, Carnegie Mellon University\n2Google Inc., NY\naanastas@cs.cmu.edu, {shankarkumar,hankliao}@google.com\nAbstract\nMultimodal language models attempt to in-\ncorporate non-linguistic features for the lan-\nguage modeling task. In this work, we extend\na standard recurrent neural network (RNN)\nlanguage model with features derived from\nvideos. We train our models on data that is\ntwo orders-of-magnitude bigger than datasets\nused in prior work. We perform a thorough ex-\nploration of model architectures for combining\nvisual and text features. Our experiments on\ntwo corpora (YouCookII and 20bn-something-\nsomething-v2) show that the best performing\narchitecture consists of middle fusion of visual\nand text features, yielding over 25% relative\nimprovement in perplexity. We report analy-\nsis that provides insights into why our multi-\nmodal language model improves upon a stan-\ndard RNN language model.\n1 Introduction\nLanguage models are vital components of a\nwide variety of systems for Natural Language\nProcessing (NLP) including Automatic Speech\nRecognition, Machine Translation, Optical Char-\nacter Recognition, Spelling Correction, etc. How-\never, most language models are trained and ap-\nplied in a manner that is oblivious to the environ-\nment in which human language operates (Ororbia\net al., 2018). These models are typically trained\nonly on sequences of words, ignoring the physical\ncontext in which the symbolic representations are\ngrounded, or ignoring the social context that could\ninform the semantics of an utterance.\nFor incorporating additional modalities, the\nNLP community has typically used datasets such\nas MS COCO (Lin et al., 2014) and Flickr\n(Rashtchian et al., 2010) for image-based tasks,\nwhile several datasets (Chen and Dolan, 2011;\n∗Work performed while the author was an intern at Google.\nYeung et al., 2014; Das et al., 2013; Rohrbach\net al., 2013; Hendricks et al., 2017) have been cu-\nrated for video-based tasks. Despite the lack of\nbig datasets, researchers have started investigat-\ning language grounding in images (Plummer et al.,\n2015; Rohrbach et al., 2016; Socher et al., 2014)\nand to lesser extent in videos (Regneri et al., 2013;\nLin et al., 2014). However, language ground-\ning has focused more on obtaining better word\nand sentence representations or other downstream\ntasks, and to lesser extent on language modeling.\nIn this paper, we examine the problem of in-\ncorporating temporal visual context into a recur-\nrent neural language model (RNNLM). Multi-\nmodal Neural Language Models were introduced\nin (Kiros et al., 2014), where log-linear LMs\n(Mnih and Hinton, 2007) were conditioned to han-\ndle both image and text modalities. Notably,\nthis work did not use the recurrent neural model\nparadigm which has now become thede facto way\nof implementing neural LMs.\nThe closest work to ours is that of Ororbia et al.\n(2018), who report perplexity gains of around 5–\n6% on three languages on the MS COCO dataset\n(with an English vocabulary of only 16K words).\nOur work is distinguishable from previous work\nwith respect to three dimensions:\n1. We train our model on video transcriptions\ncomprised of text and visual features. Thus,\nboth modalities of our model are temporal,\nin contrast to most previous work which uses\nstatic images. At the same time, our model\nrespects the temporal alignment between the\ntwo modalities, combining the text with its\nconcurrent visual context, mimicking a real\nnatural language understanding situation.\n2. We explore several architectures for combin-\ning the two modalities, and our best model\nreduces perplexity by more than 25% relative\nto a text-only baseline.\narXiv:1903.02930v1  [cs.CL]  7 Mar 2019\n<S>We will make mufﬁns\nlstm\n⊕\nlstm\nWe\nlstm\n⊕\nlstm\nwill\n...\n...\n...\nEarly Fusion\nlstm\n⊕\nlstm\nmake\n...\n...\n...\n...\n...\n...\nMiddle Fusion\nlstm\nlstm\n⊕\nmufﬁns\n...\n...\n...\n...\n...\n...\nLate Fusion Linear Combination\n⊗\n⊗\nadd\nWw\nWv\ninput to next layer\nWeighting\n⊙ σ mul\n⊕ input to next layer\nFigure 1: Visualization of our different Language Models. Given word and visual embeddings, the input can be\ncreated by three methods. Left panels: simple concatenation (examples with early, middle, and late fusion of the\nvisual embeddings). Top right panel: learning a linear combination of the two embeddings. Bottom right panel:\nlearn to weight the visual embedding based on the current word.Note: ⊕denotes concatenation, ⊗denotes matrix\nmultiplication, ⊙denotes dot product.\n3. The scale of our experiments is unprece-\ndented: we train our models on two orders\nof magnitude more data than any previous\nwork. This results in quite strong, hard-to-\nbeat baselines.\n2 Model\nA language model assigns to a sentence W =\nw1 ...w M the probability:\np(W) =\nM∏\nm=1\np(wm |w<m)\nwhere each word is assigned a probability given\nthe previous word history.\nFor a given video segment, we assume that there\nis a sequence of N video frames represented by\nfeatures V = v1 ...v N , and the corresponding\ntranscription W = w1 ...w M . In practice, we as-\nsume N = M since we can always assign a video\nframe to each word by replicating the video frames\nthe requisite number of times. Thus, our visually-\ngrounded language model models the probability\nof the next word given the history of previous\nwords as well as video frames:\np(W) =\nM∏\nm=1\np(wm |w<m,v<m)\n2.1 Combining the text and video modalities\nThere are several options for combining the text\nand video modalities. We opt for the simplest\nstrategy, which concatenates the representations.\nFor a word embedding wi and corresponding vi-\nsual representation vi, the input to our RNNLM\nwill be the concatenated vector ei = [wi ; vi]. For\nthe examples where we were unable to compute\nvisual features (see Section §3), we set vi to be a\nzero-vector.\nIn addition to concatenating the word and visual\nembedding, we explore two variants of our model\nthat allow for a ﬁner-grained integration of the two\nmodalities:\na. Learning a linear combination of the two\nmodalities In this case, the RNNLM is given as\ninput a vector ei that is a weighted sum of the two\nembeddings:\nei = Kwwi + Kvvi\nwhere Kw,Kv are learned matrices.\nb. Weighting the visual embedding condi-\ntioned on the wordHere, we apply the intuition\nthat some words could provide information as to\nwhether or not the visual context is helpful. In a\nsimplistic example, if the word history is the arti-\ncle “the,” then the visual context could provide rel-\nevant information needed for predicting the next\nword. For other word histories, though, the vi-\nsual context might not be needed or be even ir-\nrelevant for the next word prediction: if the previ-\nous word is “carpe”, the next word is very likely\nto be “diem”, regardless of visual context. We im-\nplement a simple weighting mechanism that learns\nModel Perplexity (Reduction)\nYouCook2 sth-sth\ntext-only 89.8 513.6\nLinear Comb. 84.8 (6%) 580.8 (–)\nWeighting 76.8 (14%) 538.8 (–)\nEarly Fusion 93.7 (–) 611.3 (–)\nMiddle Fusion 64.9 (28%) 411.4 (20%)\nLate Fusion 79.3 (12%) 485.5 (5%)\nTable 1: Middle Fusion of text and frame-level visual\nfeatures leads to signiﬁcant reductions in perplexity on\ntwo multimodal datasets.\na scalar weight for the visual embedding prior to\nconcatenation with the word embedding. The in-\nput to the RNNLM is now ei = [wi ; λvi], where:\nλ= σ(wi ·vi).\nThis approach does not add any new parameters to\nthe model, but since the word representations wi\nare learned, this mechanism has the potential to\nlearn word embeddings that are also appropriate\nfor weighting the visual context.\n2.2 Location of combination\nWe explore three locations for fusing visual fea-\ntures in an RNNLM (Figure 1). Our Early Fu-\nsion strategy merges the text and the visual fea-\ntures at the input to the LSTM cells. This embod-\nies the intuition that it is best to do feature com-\nbination at the earliest possible stage. The Middle\nFusion merges the visual features at the output of\nthe 1st LSTM layer while the Late Fusion strate-\ngies merges the two features after the ﬁnal LSTM\nlayer. The idea behind the Middle and Late fusion\nis that we would like to minimize changes to the\nregular RNNLM architecture at the early stages\nand still be able to beneﬁt from the visual features.\n3 Data and Experimental Setup\nOur training data consist of about 64M segments\nfrom YouTube videos comprising a total of 1.2B\ntokens (Soltau et al., 2017). We tokenize the\ntraining data using a vocabulary of 66K word-\npieces (Schuster and Nakajima, 2012). Thus, the\ninput to the model is a sequence of wordpieces.\nUsing wordpieces allows us to address out-of-\nvocabulary (OOV) word issues that would arise\nfrom having a ﬁxed word vocabulary. In practice,\na wordpiece RNNLM gives similar performance\nInputs to Perplexity\nMiddle Fusion YouCook2 sth-sth\ntext + video 64.9 411.4\ntext + zero vectors 99.0 537.7\nTable 2: Withholding visual context from our best\nmodel leads to worse performance (similar to an\nRNNLM trained only on text).\nas a word-level model (Mielke and Eisner, 2018).\nFor about 75% of the segments, we were able\nto obtain visual features at the frame level. The\nfeatures are 1500-dimensional vectors, extracted\nfrom the video frames at 1-second intervals, sim-\nilar to those used for large scale image classiﬁca-\ntion tasks (Varadarajan et al., 2015; Abu-El-Haija\net al., 2016). For a K-second video and N > K\nwordpieces, each feature is uniformly allocated to\nN/K wordpieces.\nOur RNNLM models consist of 2 LSTM layers,\neach containing 2048 units which are linearly pro-\njected to 512 units (Sak et al., 2014). The word-\npiece and video embeddings are of size 512 each.\nWe do not use dropout. During training, the batch\nsize per worker is set to 256, and we perform full\nlength unrolling to a max length of 70. The l2-\nnorms of the gradients are clipped to a max norm\nof 1.0 for the LSTM weights and to 10,000 for all\nother weights. We train with Synchronous SGD\nwith the Adafactor optimizer (Shazeer and Stern,\n2018) until convergence on a development set, cre-\nated by randomly selecting 1% of all utterances.\n4 Experiments\nFor evaluation we used two datasets, YouCook2\nand sth-sth, allowing us to evaluate our mod-\nels in cases where the visual context is relevant to\nthe modelled language. Note that no data from\nthese datasets are present in the YouTube videos\nused for training. The perplexity of our models is\nshown in Table 1.\nYouCookII dataset: The YouCookII dataset\n(YouCook2) (Das et al., 2013; Zhou et al., 2018)\nconsists of 2,000 instructional cooking videos,\neach annotated with steps localized in video. An\nexample annotation could be that of a video seg-\nment between 00:53–01:03 with the recipe step\n“cook bacon until crispy, then drain on paper\ntowel.” The dataset was manually created, so that\nfor each textual recipe segment the corresponding\nspray the pan with cooking spray Total Score\ntext-only 10.2 2.9 5.6 1.4 7.5 0.2 27.8\nMiddle Fusion 8.9 2.6 3.5 1.8 7.7 0.5 24.8\na) Our multimodal model has signiﬁcantly lower word-level perplexity on\nword-pieces that correspond to items shown in the video (“spray, pan”).\nplace cucumber salad and then the hot dog on the bun Total Score\ntext-only 7.7 14.5 4.8 1.9 2.9 3.3 6.6 3.1 4.5 0.9 5.0 55.1\nMiddle Fusion 6.6 11.9 5.3 2.0 4.0 3.5 6.1 4.6 4.0 1.3 7.7 57.0\nb) A rare example where the text-only model is overall better that the multimodal one. Still, though,\nentities (“cucumber”) that appear in the video receive better scores from the multimodal system.\nTable 3: Two sentences from YouCook2 with wordpiece-level negative log likelihood scores. Most gains ( high-\nlighted) of our Middle Fusion model come from word-pieces corresponding to entities that appear in the videos.\nvideo frame provides related context. Therefore,\nthis constrained scenario allows us to explicitly\ntest whether our language models indeed manage\nto take advantage of the visual context.\n20BN-Something-Something dataset v2:\n(Goyal et al., 2017): This dataset (henceforth\nsth-sth) consists of about 220K short videos\nof actions performed by humans with every day\nobjects, annotated with text descriptions of the\nactions. Each description follows a template e.g.\n“Taking something out of something.” This is\na very constrained scenario, where the objects\n(“something”) mentioned in the text deﬁnitely\nappear in the video. We evaluate on the predeﬁned\nvalidation set (25K videos) computing perplexity\n(PPL) on the textual action descriptions.\nOut of all the architectures we consider, only\ntwo lead to consistently better performance on\nboth datasets: Middle and Late Fusion. Late Fu-\nsion leads to modest improvements (12% and 5%\nrelative on the two datasets), but Middle Fusion\nis able to take better advantage of both modalities,\nleading to 28% and 20% relative reductions in per-\nplexity. In contrast, Early Fusion performs worse\nthan the baseline. We suspect that the crucial fac-\ntor for success with such architectures is allowing\nat least one lower layer of the RNNLM to be ded-\nicated to text-only modeling.\nThe variants that do not simply concatenate the\nword and video embeddings, but perform a ﬁne-\ngrainer integration, yield improvements on only\none dataset (YouCook2). The linear combination\napproach leads to 6% relative reduction, while the\nlearned weighting of the video embedding reduces\nperplexity by 14%.\nThe domain shift between training and the\nsth-sth dataset is reﬂected in quite high PPL\nscores. The videos are also much shorter (typ-\nically a few seconds) than the average YouTube\nvideo. We speculate that the length mismatch\nbetween training and test is responsible for the\nlower performance of the ﬁne-grained approaches\non sth-sth.\nDoes our model really use the visual features?\nIn order to conﬁrm that our model does utilize\nthe visual modality, we perform a simple exper-\niment of blinding it: we deprive the RNNLM of\nthe visual context, substituting the video embed-\ndings with zero vectors. The the results shown\nin Table 2. The performance is worse, but it is\nin fact comparable to a model trained only on the\ntext modality on YouCook2. This conﬁrms that\nour model indeed uses the visual context in a pro-\nductive way. Furthermore, it shows that our model\nis somewhat robust to the absence of visual con-\ntext; this is the result of training with 25% of our\ninstances lacking visual features.\nWhere do the improvements come from?We\nobtained wordpiece-level negative log likelihoods\nfor 50 randomly chosen sentences from the\nYouCook2 dataset. For the majority (88% of\nthe sentences), the Middle Fusion model had bet-\nter sentence-level scores than the text-only model.\nWe show two examples in Table 3. We ﬁnd that the\nlargest improvements are due to the added visual\ninformation: the highest reductions are found on\nword-pieces corresponding to entities that appear\nin the video.\n5 Conclusion\nWe present a simple strategy to augment a stan-\ndard recurrent neural network language model\nwith temporal visual features. Through an ex-\nploration of candidate architectures, we show that\nthe Middle Fusion of visual and textual features\nleads to a 20-28% reduction in perplexity relative\nto a text only baseline. These experiments were\nperformed using datasets of unprecedented scale,\nwith more than 1.2 billion tokens – two orders\nof magnitude more than any previously published\nwork. Our work is a ﬁrst step towards creating\nand deploying large-scale multimodal systems that\nproperly situate themselves into a given context,\nby taking full advantage of every available signal.\nReferences\nSami Abu-El-Haija, Nisarg Kothari, Joonseok Lee,\nApostol (Paul) Natsev, George Toderici, Bal-\nakrishnan Varadarajan, and Sudheendra Vijaya-\nnarasimhan. 2016. Youtube-8m: A large-scale video\nclassiﬁcation benchmark. In arXiv:1609.08675.\nDavid L Chen and William B Dolan. 2011. Collect-\ning highly parallel data for paraphrase evaluation.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies-Volume 1, pages 190–200.\nAssociation for Computational Linguistics.\nPradipto Das, Chenliang Xu, Richard F Doell, and Ja-\nson J Corso. 2013. A thousand frames in just a few\nwords: Lingual description of videos through latent\ntopics and sparse object stitching. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 2634–2641.\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent\nMichalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Pe-\nter Yianilos, Moritz Mueller-Freitag, et al. 2017.\nThe something something video database for learn-\ning and evaluating visual common sense. In The\nIEEE International Conference on Computer Vision\n(ICCV), volume 1, page 3.\nLisa Anne Hendricks, Oliver Wang, Eli Shechtman,\nJosef Sivic, Trevor Darrell, and Bryan Russell. 2017.\nLocalizing moments in video with natural language.\nIn Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), pages 5803–5812.\nRyan Kiros, Ruslan Salakhutdinov, and Rich Zemel.\n2014. Multimodal neural language models. In In-\nternational Conference on Machine Learning, pages\n595–603.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nSebastian J Mielke and Jason Eisner. 2018. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. arXiv preprint arXiv:1804.08205.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn Proceedings of the 24th international conference\non Machine learning, pages 641–648. ACM.\nAlexander G Ororbia, Ankur Mali, Matthew A Kelly,\nand David Reitter. 2018. Visually grounded, sit-\nuated learning in neural models. arXiv preprint\narXiv:1805.11546.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision , pages\n2641–2649.\nCyrus Rashtchian, Peter Young, Micah Hodosh, and\nJulia Hockenmaier. 2010. Collecting image annota-\ntions using amazon’s mechanical turk. In Proceed-\nings of the NAACL HLT 2010 Workshop on Creating\nSpeech and Language Data with Amazon’s Mechan-\nical Turk, pages 139–147. Association for Computa-\ntional Linguistics.\nMichaela Regneri, Marcus Rohrbach, Dominikus Wet-\nzel, Stefan Thater, Bernt Schiele, and Manfred\nPinkal. 2013. Grounding action descriptions in\nvideos. Transactions of the Association of Compu-\ntational Linguistics, 1:25–36.\nAnna Rohrbach, Marcus Rohrbach, Ronghang Hu,\nTrevor Darrell, and Bernt Schiele. 2016. Ground-\ning of textual phrases in images by reconstruction.\nIn European Conference on Computer Vision, pages\n817–834. Springer.\nMarcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater,\nManfred Pinkal, and Bernt Schiele. 2013. Translat-\ning video content to natural language descriptions.\nIn Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 433–440.\nHasim Sak, Andrew W. Senior, and Franc ¸oise Bea-\nufays. 2014. Long short-term memory recurrent\nneural network architectures for large scale acoustic\nmodeling. In Proc. INTERSPEECH.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In Proc. of ICASSP.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235.\nRichard Socher, Andrej Karpathy, Quoc V Le, Christo-\npher D Manning, and Andrew Y Ng. 2014.\nGrounded compositional semantics for ﬁnding and\ndescribing images with sentences. Transactions\nof the Association of Computational Linguistics ,\n2(1):207–218.\nHagen Soltau, Hank Liao, and Hasim Sak. 2017. Neu-\nral speech recognizer: Acoustic-to-word lstm model\nfor large vocabulary speech recognition. In Proc.\nInterspeech, pages 3707–3711.\nBalakrishnan Varadarajan, George Toderici, Sudheen-\ndra Vijayanarasimhan, and Apostol Natsev. 2015.\nEfﬁcient large scale video classiﬁcation. arXiv\npreprint arXiv:1505.06250.\nSerena Yeung, Alireza Fathi, and Li Fei-Fei. 2014.\nVideoset: Video summary evaluation through text.\narXiv preprint arXiv:1406.5824.\nLuowei Zhou, Chenliang Xu, and Jason J Corso. 2018.\nTowards automatic learning of procedures from web\ninstructional videos. In AAAI Conference on Artiﬁ-\ncial Intelligence."
}