{
  "title": "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer",
  "url": "https://openalex.org/W4382463911",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4382474470",
      "name": "Shibo Jie",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2245903860",
      "name": "Zhi-Hong Deng",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A4382474470",
      "name": "Shibo Jie",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    },
    {
      "id": "https://openalex.org/A2245903860",
      "name": "Zhi-Hong Deng",
      "affiliations": [
        "Peking University",
        "King University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W12634471",
    "https://openalex.org/W4282005462",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2167215970",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6810601475",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2013912476",
    "https://openalex.org/W6799423381",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W6764409202",
    "https://openalex.org/W3166140588",
    "https://openalex.org/W6682303744",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W6648487637",
    "https://openalex.org/W6644585003",
    "https://openalex.org/W3023528699",
    "https://openalex.org/W2616957565",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W4281401280",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W2733236492",
    "https://openalex.org/W2773577463",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4224024410",
    "https://openalex.org/W2980287048",
    "https://openalex.org/W2963704562",
    "https://openalex.org/W4287073339",
    "https://openalex.org/W2963048316",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W1993482030",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W2950248853",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W4312769570",
    "https://openalex.org/W4309984710",
    "https://openalex.org/W2952902402",
    "https://openalex.org/W4285595687",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2970418186",
    "https://openalex.org/W2150856297",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W2963838731"
  ],
  "abstract": "Recent work has explored the potential to adapt a pre-trained vision transformer (ViT) by updating only a few parameters so as to improve storage efficiency, called parameter-efficient transfer learning (PETL). Current PETL methods have shown that by tuning only 0.5% of the parameters, ViT can be adapted to downstream tasks with even better performance than full fine-tuning. In this paper, we aim to further promote the efficiency of PETL to meet the extreme storage constraint in real-world applications. To this end, we propose a tensorization-decomposition framework to store the weight increments, in which the weights of each ViT are tensorized into a single 3D tensor, and their increments are then decomposed into lightweight factors. In the fine-tuning process, only the factors need to be updated and stored, termed Factor-Tuning (FacT). On VTAB-1K benchmark, our method performs on par with NOAH, the state-of-the-art PETL method, while being 5x more parameter-efficient. We also present a tiny version that only uses 8K (0.01% of ViT's parameters) trainable parameters but outperforms full fine-tuning and many other PETL methods such as VPT and BitFit. In few-shot settings, FacT also beats all PETL baselines using the fewest parameters, demonstrating its strong capability in the low-data regime.",
  "full_text": "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer\nShibo Jie, Zhi-Hong Deng*\nSchool of Intelligence Science and Technology, Peking University\n{parsley, zhdeng}@pku.edu.cn\nAbstract\nRecent work has explored the potential to adapt a pre-trained\nvision transformer (ViT) by updating only a few parame-\nters so as to improve storage efficiency, called parameter-\nefficient transfer learning (PETL). Current PETL methods\nhave shown that by tuning only 0.5% of the parameters, ViT\ncan be adapted to downstream tasks with even better perfor-\nmance than full fine-tuning. In this paper, we aim to further\npromote the efficiency of PETL to meet the extreme storage\nconstraint in real-world applications. To this end, we propose\na tensorization-decomposition framework to store the weight\nincrements, in which the weights of each ViT are tensorized\ninto a single 3D tensor, and their increments are then de-\ncomposed into lightweight factors. In the fine-tuning process,\nonly the factors need to be updated and stored, termedFactor-\nTuning (FacT). On VTAB-1K benchmark, our method per-\nforms on par with NOAH, the state-of-the-art PETL method,\nwhile being 5× more parameter-efficient. We also present\na tiny version that only uses 8K (0.01% of ViT’s parame-\nters) trainable parameters but outperforms full fine-tuning and\nmany other PETL methods such as VPT and BitFit. In few-\nshot settings, FacT also beats all PETL baselines using the\nfewest parameters, demonstrating its strong capability in the\nlow-data regime.\nIntroduction\nThe de-facto paradigm for achieving state-of-the-art perfor-\nmance on visual tasks involves pre-training on large datasets\nlike ImageNet (Deng et al. 2009), and then fully fine-tuning\non downstream tasks. However, this paradigm is not storage-\nefficient because each downstream task necessitates the stor-\nage of an entire model, which becomes prohibitive in some\nscenarios (e.g., storing customized models for each user) as\nthe size of vision models grows exponentially.\nTo promote storage efficiency, recent work onparameter-\nefficient transfer learning (PETL) attempts to fine-tune only\na small part of the parameters to adapt the large pre-trained\nmodels to downstream tasks. These methods either insert ad-\nditional trainable structures (e.g., adapters (Houlsby et al.\n2019) or prompt tokens (Jia et al. 2022)) to a frozen vision\ntransformer (ViT) (Dosovitskiy et al. 2021) backbone or se-\nlectively fine-tune some of the ViT’s own parameters (e.g.,\n*Corresponding Author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n10 2\n 10 1\n 100 101 102\n# trainable param (M)\n64\n66\n68\n70\n72\n74\n76\n78Average Acc (%)\n19-T ask Average Accuracy on VTAB-1k\nFull\nBitFit\nVPT\nAdapter\nAdaptFormer\nLoRA\nNOAH\nFacT-TT (Ours)\nFacT-TK (Ours)\nFigure 1: Average accuracy vs. number of trainable parame-\nters (log axis) on VTAB-1K benchmark. Our FacT signifi-\ncantly reduces the number of trainable parameters.\nall bias parameters (Zaken, Goldberg, and Ravfogel 2022)).\nRecently, LoRA (Hu et al. 2022) also shows that optimizing\nthe low-rank decomposition matrices of weight increments1\nof the dense layers is a promising way to adapt large models.\nHowever, though LoRA’s matrix decomposition signif-\nicantly reduces the storage overhead of fine-tuned dense\nlayers, it is far from exploiting the low-rank properties of\nneural networks to the extreme. Inspired by recent work\non compression of the transformer-based pre-trained lan-\nguage models (PLMs) (Wang et al. 2022), we infer that dur-\ning fine-tuning, the weight increments of pre-trained ViT\nare also redundant in terms of intra-weight rank and inter-\nweight rank. The former is reflected in that the dense incre-\nments matrix can be low-rank as in LoRA, while the lat-\nter is implied by the fact that cross-layer weight-sharing\nhas already been adopted in some lightweight ViT struc-\ntures (Zhang et al. 2022), which is not taken into consid-\neration by LoRA. To fully develop the potential of PETL,\nwe propose a tensorization-decomposition framework. In-\nstead of decomposing the weight increment matrices indi-\nvidually like LoRA, we represent the whole ViT as a single\n1We refer to the weight’s change during fine-tuning as itsincre-\nment, which is a matrix or tensor of the same shape as the weight.\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1060\ntensor, in which all weight increment matrices are decom-\nposed together. This means that the different matrices could\nshare their factors to reduce inter-weight rank redundancy as\nwell. The factors are then fine-tuned with the ViT backbone\nfrozen, called Factor-Tuning(FacT). After fine-tuning, only\nthe classification head and these factors need to be stored,\nand thus the storage efficiency is significantly promoted.\nIn this paper, we explore the feasibility of applying var-\nious tensor decomposition formats to the tensorization-\ndecomposition framework, including Tensor-Train for-\nmat (Oseledets 2011) (denoted as FacT-TT) and Tucker\nformat (Lathauwer, Moor, and Vandewalle 2000) (denoted\nas FacT-TK). We also show that LoRA can be regarded as\na special case of FacT when using Matrix-Batch format. As\nthe experimental results on VTAB-1K benchmark shown in\nFig 1, both FacT-TT and FacT-TK significantly reduce\nthe number of trainable parameters while maintaining com-\npetitive performance. FacT-TK achieves performance on\npar with that of the current state-of-the-art (SOTA) method\nNOAH (Zhang, Zhou, and Liu 2022) with only 19% of\nNOAH’s parameters. In a more extreme case,FacT-TT can\nadapt the ViT with 85.8M parameters by only tuning the fac-\ntors with 8K parameters, while still outperforming full fine-\ntuning and some other PETL methods such as VPT (Jia et al.\n2022) and BitFit (Zaken, Goldberg, and Ravfogel 2022).\nIn few-shot learning, FacT-TT beats all other PETL base-\nlines, demonstrating its superiority in the low-data regime.\nThe contributions of our work are as follows:\n• Inspired by the assumption that weight increments of\npre-trained ViT during fine-tuning are rank-redundant,\nwe propose FacT, a tensorization-decomposition frame-\nwork to adapt ViT by tuning the factors of increments.\n• Under this framework, we propose FacT-TT and\nFacT-TK, decomposing the tensorized ViT with Tensor-\nTrain and Tucker format, respectively.\n• Experimental results show that our methods are much\nmore lightweight than other PETL methods yet maintain\ncompetitive performances on VTAB-1K benchmark and\nSOTA results on few-shot learning datasets, demonstrat-\ning the potential of PETL with an extremely small stor-\nage budget.\nRelated Work\nParameter-Efficient Transfer Learning\nPETL has been investigated in the field of both natural lan-\nguage processing (NLP) and computer vision, which aims\nto fine-tune a small number of trainable parameters to trans-\nfer large pre-trained models to downstream tasks. We here\nintroduce some PETL methods either proposed for ViT or\nported from PLMs.\nAdapter (Houlsby et al. 2019; Mahabadi, Henderson, and\nRuder 2021; Rebuffi, Bilen, and Vedaldi 2017) is typically\na bottleneck block composed of two fully connected layers,\nwhose weights are Wdown ∈ Rd×h and Wup ∈ Rh×d,\nwhere h << d. There are two common ways to insert\nadapters. The original way is sequential (Houlsby et al.\n2019; Pfeiffer et al. 2021), formulated as\nX′ ← X + ϕ(XW down)Wup\nwhere X ∈ RN×d is the output of Feed-Forward Network\n(FFN) blocks and ϕ is a nonlinear function. The other is par-\nallel (He et al. 2022; Chen et al. 2022), formulated as\nX′ ← X + FFN(X) + s · ϕ(XW down)Wup\nwhere s is a hyper-parameter, X is the input of FFN blocks.\nThe parallel adapter design is also referred to as Adapt-\nFormer by Chen et al. (2022). Concurrently, Jie and Deng\n(2022) suggest using convolutional adapters for ViT. Lian\net al. (2022b) propose shifting and scaling the intermediate\nfeatures of the models which can also be regarded as a sim-\nplified linear adapter.\nLoRA (Hu et al. 2022) decomposes the increments of\nquery transformation Wq and value transformation Wv\ninto low-rank Aq/v ∈ Rd×r and Bq/v ∈ Rr×d where\nr << d. The query and value are then computed as\nQ/V ← XW q/v + s · XAq/vBq/v\nin which s is a hyper-parameter.\nVPT (Jia et al. 2022) concatenates the input X with sev-\neral trainable promptsP ∈ Rl×d before feeding it into trans-\nformer layers. This extended sequence is formulated as\nX′ ← [X, P]\nIn VPT-Deep, these prompts are concatenated before every\nlayer and then discarded at the end of the layer. While in\nVPT-Shallow, the prompts are only inserted before the first\nlayer and will be maintained until the last layer.\nNOAH (Zhang, Zhou, and Liu 2022) focuses on combin-\ning existing PETL methods without manual design. It trains\na large supernet at first and then performs neural architecture\nsearch on hidden dimension h of Adapter, rank r of LoRA,\nand prompt length l of VPT.\nIn all the aforementioned methods, the parameters of pre-\ntrained ViT are frozen. There are also methods that only fine-\ntune a few of the pre-trained parameters without introducing\nnew parameters, such as BitFit (Zaken, Goldberg, and Rav-\nfogel 2022), which fine-tunes the bias parameters only.\nTensor Decomposition for Network Compression\nTensor decomposition is an important research area aiming\nto approximate a tensor with a set of low-rank factors that\nhas been studied for many years. Previous work on deep\nlearning has investigated the use of tensor decomposition\nto compress neural networks so as to reduce the size of\nmodels, including ConvNets (Denton et al. 2014; Lebedev\net al. 2015), RNNs (Winata et al. 2019; Ye et al. 2018; Yang,\nKrompass, and Tresp 2017), and Transformers (Noach and\nGoldberg 2020; Lan et al. 2020).\nNote that model compression and PETL have a similar\npurpose: to reduce the storage overhead. But the difference\nlies in that model compression aims at reducing the size\nof the whole model, while PETL only considers reducing\nthe trainable parameters on a pre-trained model since the\npre-trained weights are always required when fine-tuning\non subsequent new tasks. Therefore, in this paper, we take\ninspiration from model compression but compress the in-\ncrements of the weights instead of the pre-trained weights\nthemselves.\n1061\n/gid00007/gid00007/gid00015\n/gid00015/gid00042/gid00045/gid00040\n/gid00014/gid00009/gid00020/gid00002\nx /gid00013\n/gid00015/gid00042/gid00045/gid00040\nqW kW vW\noW\nupW\ndownW\nFigure 2: Illustration of tensorizing ViT. The ViT is ten-\nsorized into a single 12L × d × d tensor.\nMethod\nTensorizing Vision Transformer\nTensorizing a neural network means representing its param-\neters using a single tensor. Previous vision models such as\nResNet (He et al. 2016) usually use weights of different sizes\nin different layers, e.g., different kernel sizes and input/out-\nput channels. This property limits their capability of ten-\nsorization. However, due to the consistency of Transformer\nlayers in ViT, we can tensorize ViT in a much simpler way.\nBesides the patch embedding and classification head,\na ViT is composed of two types of blocks: Multi-Head\nSelf-Attention (MHSA) and Feed-Forward Network (FFN,\nalso referred to as Multi-Layer Perceptron). In MHSA,\nthe query, key, value, and output transformations are\nparametrized by Wq, Wk, Wv, Wo ∈ Rd×d, respectively.\nThese transformations are further divided into Nh heads:\n{W(i)\nq }Nh\ni=1, {W(i)\nk }Nh\ni=1, {W(i)\nv }Nh\ni=1, {W(i)\no }Nh\ni=1. Then, the\nMHSA is formulated as2\nMHSA(X) =\nNhX\ni=1\nsoftmax\n \nXW (i)\nq W(i)\nk\n⊺\nX⊺\n√\nd\n!\nXW (i)\nv W(i)\no\n⊺ (1)\nAn FFN block consists of two fully-connected (FC) lay-\ners. Ignoring the bias parameters for simplicity, the FFN is\nformulated as\nFFN(X) = GELU(XW up)Wdown (2)\nwhere Wup ∈ Rd×4d and Wdown ∈ R4d×d are the weights\nof the FC layers.\nThe FFN can also be regarded as a multi-head block. We\ndivide Wup and Wdown into four matrices of sized×d, i.e.,\n{W(i)\nup}4\ni=1 and {W(i)\ndown}4\ni=1, respectively. The FFN can be\nrewritten as\nFFN(X) =\n4X\ni=1\nGELU\n\u0010\nXW (i)\nup\n\u0011\nW(i)\ndown (3)\nIn each layer, there are four d × d matrices in the MHSA\nblock and eight d × d matrices in the FFN block. Supposing\n2We use superscript (i) to denote the i-th head.\nthe number of layers in a ViT is L, we can stack all weights\nof the Transformer layers together as a single 12L × d × d\ntensor3\nW =\n\b\n{Wj\nq,Wj\nk, Wj\nv, Wj\no} ∪ {Wj,(i)\nup }4\ni=1∪\n{Wj,(i)\ndown}4\ni=1\n\tL\nj=1 ∈ R12L×d×d (4)\nas shown in Fig 2.\nNote that the classification head, patch embedding, nor-\nmalization, and all bias parameters are not taken into ac-\ncount in this tensorized format since they are irregular and\nfew in number. For simplicity, we suppose the classifica-\ntion head is not tensorized, and others (patch embedding,\nnormalization, and bias parameters) are frozen during fine-\ntuning.\nFactor-Tuning: a Unified Perspective\nLet W0 denote a tensorized pre-trained ViT. During fine-\ntuning, the ViT is updated to Wft , and we use ∆W =\nWft − W0 to denote the increment of the ViT weight ten-\nsor. When fine-tuning, the gradient is calculated as4\ngW = ∂L(D; W)\n∂W (5)\nwhere D is the training dataset and W is initialized as W0.\nWe can rewrite this equation in another equivalent form\ngW = g∆W = ∂L(D; W0 + ∆W)\n∂∆W (6)\nwhere ∆W is initialized as a zero tensor.\nTraditional fine-tuning updates all parameters in a ViT,\nwhich means that we need to store at least a dense ∆W\n(for Eq (6)) or Wft (for Eq (5)) for each downstream task,\nresulting in a storage overhead of O(Ld2) per task.\nIn the two modules of ViT, all the weight matrices multi-\nply the inputs in a fully-connected way, implying the exis-\ntence of redundancy. In the field of NLP, many studies have\nalready found that the weight matrices in the transformer-\nbased PLMs are redundant in rank (Noach and Goldberg\n2020; Ma et al. 2019; Lan et al. 2020; Wang et al. 2022).\nThe rank redundancies include intra-weight redundancy,\nwhen each of the dense weight matrices can be decomposed\n(e.g., SVD) into and approximated by low-rank factors; and\ninter-weight redundancy, when the model could work well\nwith cross-layer shared weights (e.g., ALBERT (Lan et al.\n2020)). Since the blocks of ViT and PLMs are highly sim-\nilar, we infer that the weights of pre-trained ViT could be\nredundant in rank as well, suggesting that the rank of weight\nincrements ∆W could also be redundant.\nDue to the redundancies of ∆W, we can decompose\n∆W into some factors to promote storage efficiency. We\nconsider several well-known formats to decompose ∆W ∈\nR12L×d×d: Matrix-Batch format, Tensor-Train format (Os-\neledets 2011), and Tucker format (Lathauwer, Moor, and\nVandewalle 2000), as illustrated in Fig 3.\n3We use superscriptj to represent that the matrix is inj-th layer.\n4Tensor flattening notations are omitted for simplicity.\n1062\n/gid00007/gid00007/gid00015\n/gid00015/gid00042/gid00045/gid00040\n/gid00014/gid00009/gid00020/gid00002\nx /gid00013\n/gid00015/gid00042/gid00045/gid00040\nqW kW vW\noW\nupW\ndownW\n/gid00027/gid00068/gid00059/gid00059/gid00001/gid00027/gid00041/gid00033/gid00062/gid00039/gid00022/gid00027/gid00048/gid00050/gid00041/gid00007/gid00041/gid00041/gid00027/gid00048/gid00050/gid00041/gid00007/gid00041/gid00032\n/gid00039/gid00048/gid00070/gid00001/gid00067/gid00052/gid00061/gid00066/gid00062/gid00065Matrix-Batch format /gid00041/gid00052/gid00061/gid00066/gid00062/gid00065/gid00007/gid00041/gid00065/gid00048/gid00056/gid00061/gid00001/gid00053/gid00062/gid00065/gid00060/gid00048/gid00067 /gid00041/gid00068/gid00050/gid00058/gid00052/gid00065/gid00001/gid00053/gid00062/gid00065/gid00060/gid00048/gid00067\nW∆\nU\nV\nP\nCΣU\nV\nU\nV\n/gid00051\n/gid00051\n/gid00010/gid00011/gid00033/gid00051\n/gid00051\n/gid00065\n/gid00065\n/gid00051\n/gid00065\n/gid00051\n/gid00065\n/gid00065\n/gid00065\n/gid00010/gid00011/gid00033/gid00051\n/gid00065\n/gid00051\n/gid00065\n/gid00065\n/gid00065\n/gid00065/gid00065\n/gid00010/gid00011/gid00033\n/gid00010/gid00011/gid00033\n/gid00010/gid00011/gid00033\nFigure 3: Factor-Tuning with different tensor decomposition methods. Full fine-tuning optimizes the raw tensor. LoRA can be\nregarded as optimizing factors of Matrix-Batch format. FacT-TT and FacT-TK optimize Tensor-Train and Tucker factors,\nrespectively. Grey and white tensors/matrices indicate random and zero initialization, respectively.\n1 2 4 8 16 32\nRank r\n0\n100\n200\n300\n400\n500# trainable param (K)\nLoRA\nFacT-TT\nFacT-TK\nFigure 4: Number of trainable parameters vs. rank r.\nWith the same rank, FacT-TK uses fewer parameters than\nFacT-TT, and LoRA is significantly larger than FacT-TT\nand FacT-TK.\nInterpreting LoRA in our framework: Matrix-Batch\nMatrix-Batch format regards the first dimension of ∆W as\nthe batch dimension and decomposes all the matrices ofd×d\nin the batch individually. Formally,∆Wis decomposed into\nU ∈ R12L×d×r and V ∈ R12L×r×d, where\n∆Wi,:,: = s · Ui,:,:V i,:,: ∀i ∈ {1,2, ...,12L} (7)\nin which s is a hyper-parameter for scaling. If we only\nconsider Wq and Wv in tensorization, i.e., let W =\b\nWj\nq, Wj\nv\n\tL\nj=1 ∈ R2L×d×d instead, FacT with Matrix-\nBatch format will become exactly LoRA. The size of\nLoRA’s factors is4Ldr ∼ O(Ldr), where r << d.\nHowever, since the weight matrices are discomposed in-\ndividually in Matrix-Batch format, LoRA only reduces the\nintra-weight redundancy. We now introduce two other for-\nmats that take inter-weight redundancy into consideration.\nProposed Format I: Tensor-Train ∆W is decomposed\ninto U ∈ Rd×r1 , V ∈ Rd×r2 , and Σ ∈ R12L×r1×r2 , where\n∆W = s · Σ ×2 U⊺ ×3 V ⊺ (8)\nin which ×i is mode-i product, i.e.,\n∆Wi,j,k = s ·\nr1X\nt1=1\nr2X\nt2=1\nΣi,t1,t2 Uj,t1 V k,t2\n∀i ∈ {1,2, ...,12L}, ∀j, k∈ {1,2, ..., d}\n(9)\nFor simplicity, we setr = r1 = r2 << d. The size of factors\nis 2dr + 12Lr2 ∼ O(dr + Lr2).\nProposed Format II: Tucker ∆W is decomposed into\nU ∈ Rd×r2 , V ∈ Rd×r3 , P ∈ R12L×r1 , and C ∈\nRr1×r2×r3 , where\n∆W = s · C ×1 P⊺ ×2 U⊺ ×3 V ⊺ (10)\ni.e.,\n∆Wi,j,k = s ·\nr1X\nt1=1\nr2X\nt2=1\nr3X\nt3=1\nCt1,t2,t3 Pi,t1 Uj,t2 V k,t3\n∀i ∈ {1,2, ...,12L}, ∀j, k∈ {1,2, ..., d}\n(11)\nFor simplicity, we set r = r1 = r2 = r3 << d. The size of\nfactors is 2dr + 12Lr + r3 ∼ O(dr + Lr + r3).\nInspired by Hu et al. (2022), we adopt a decompose-then-\ntrain paradigm, i.e., decompose the ∆W before fine-tuning,\nand then update the factors end-to-end during fine-tuning.\nThis paradigm benefits fine-tuning process in several ways.\nFirst, since ∆W is initially a zero tensor, we can use a pre-\ndefined rule to initialize the factors directly instead of run-\nning the expensive decomposition algorithm. Second, de-\ncomposing a tensor means an uncontrollable loss of infor-\nmation. Because the factors are optimized via gradient de-\nscent, we can expect the most useful information to be re-\ntained when minimizing the training loss.\nIn each of the two formats, the factorV is zero-initialized\nand other factors are randomly initialized so that the ∆W\nis initially a zero tensor. After decomposition, we fine-tune\nthe factors end-to-end. Taking the Tensor-Train format as an\nexample, the gradient w.r.t.U is calculated as\ngU = ∂L(D; W0 + ∆W)\n∂U = s · gW\n∂(Σ ×2 U⊺ ×3 V ⊺)\n∂U (12)\nand the same for V and Σ. Note that the role of hyper-\nparameter s in Eq (12) is to adjust the learning rate of factors.\nAfter fine-tuning, we only need to store the lightweight\nfactors for each task. We use FacT-TT and FacT-TK to\ndenote FacT using Tensor-Train and Tucker formats, re-\nspectively. FacT-TT and FacT-TK reduce both intra- and\ninter-weight redundancies, so they can use fewer parame-\nters to store task-specific information and are more storage-\nefficient, as shown in Fig 4. The factors can be absorbed\ninto Wft before inference, so FacT adds no extra compu-\ntational cost or latency during the inference phase.\n1063\nNatural Specialized Structured\n# param (M)\nCifar100\nCaltech101\nDTD\nFlower102\nPets\nSVHN\nSun397\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr\n-Dist\nDMLab\nKITTI-Dist\ndSpr-Loc\ndSpr\n-Ori\nsNORB-Azim\nsNORB-Ele\nAverage\nTraditional F\nine-Tuning\nFull 85.8 68.9 87.7 64.3 97.2\n86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5\n57.5 46.7 25.7 29.1 68.9\nLinear 0 64.4 85.0 63.2 97.0\n86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4\n12.5 20.0 9.6 19.2 57.6\nPETL methods\nBitFit 0.103 72.8 87.0 59.2 97.5\n85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9\n66.6 40.0 15.7 25.1 65.2\nVPT-Shallow 0.063 77.7 86.9 62.6 97.5\n87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1\n68.7 36.1 20.2 34.1 67.8\nVPT-Deep 0.531 78.8 90.8 65.8 98.0 88.3\n78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8\n73.6 47.9 32.9 37.8 72.0\nAdapter 0.157 69.2 90.1 68.0 98.8\n89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3\n74.8 48.5 29.9 41.6 73.9\nAdaptFormer 0.157 70.8 91.2 70.5 99.1 90.9\n86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3\n76.3 45.7 31.7 41.1 74.7\nLoRA 0.295 67.1 91.4 69.4 98.8\n90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7\n47.1 31.0 44.0 74.5\nNOAH 0.361 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8\n48.3 32.8 44.2 75.5\nFacT-TT4 0.008 69.4 88.5 70.6 98.8 90.0 83.3 53.7 83.9 95.1 81.5 75.4 78.2 69.0 47.7 79.0 75.2 42.7 27.2 38.7 73.5\nFacT-TT≤16 0.037 71.3 89.6 70.7 98.9 91.0 87.8 54.6 85.2 95.5 83.4 75.7 82.0 69.0 49.8 80.0 79.2 48.4 34.2 41.4 75.3\nFacT-TK8 0.014 70.3 88.7 69.8 99.0 90.4 84.2 53.5 82.8 95.6 82.8 75.7 81.1 68.0 48.0 80.5 74.6 44.0 29.2 41.1 74.0\nFacT-TK≤32 0.069 70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6\nTable 1: Full results on the VTAB-1K benchmark. “# params” specifies the number of trainable parameters in backbones. Both\naverage accuracy and # params are averaged over group-wise average values. OurFacT-TK≤32 outperforms all previous PETL\nmethods while using significantly fewer parameters.\nVPT-Deep\nNOAH LoRA Adapter\nAdaptFormer\nFacT-TT FacT-TK\n78\n80\n82T est Acc (%)\n80.680.680.5\n79.0\n79.5\n80.2\n78.5\nNatural\nVPT-Deep\nNOAH LoRA Adapter\nAdaptFormer\nFacT-TTFacT-TK\n82\n84\n86\n85.3\n84.984.9\n84.1\n84.684.9\n82.4\nSpecialized\nVPT-Deep\nNOAH LoRA Adapter\nAdaptFormer\nFacT-TTFacT-TK\n55\n60\n65\n60.760.5\n58.858.5\n60.561.3\n55.0\nStructured\nFigure 5: Group-wise results on VTAB-1K. The width of the bars is proportional to the number of trainable parameters.\nExperiments\nTransfer Learning on VTAB-1K Benchmark\nFirst of all, we evaluate our method on the basic transfer\nlearning scenario – fine-tuning the pre-trained models on\nvarious downstream tasks.\nDatasets We use VTAB-1K benchmark (Zhai et al. 2019)\nto evaluate the performance of our methods in terms of\nPETL. VTAB-1K consists of 19 different visual classifica-\ntion datasets, which can be divided into three groups: Nat-\nural, Specialized, and Structured. Each dataset only con-\ntains 1,000 training samples. We report top-1 accuracy on\ntest sets in all experiments. These datasets cover a large\nrange of the possible domains where downstream tasks come\nfrom, and thus the effectiveness of PETL methods can be\nmeasured comprehensively.\nCompared Methods We compare our methods to vari-\nous competitive baselines, including BitFit (Zaken, Gold-\nberg, and Ravfogel 2022), VPT-Shallow (Jia et al. 2022),\nVPT-Deep (Jia et al. 2022), Adapter (Pfeiffer et al.\n2021; Mahabadi, Henderson, and Ruder 2021), Adapter-\nFormer (Chen et al. 2022), LoRA (Hu et al. 2022), and\ncurrent SOTA methodNOAH (Zhang, Zhou, and Liu 2022).\nFollowing Zhang, Zhou, and Liu (2022), the hidden dimen-\nsion h of Adapter and AdaptFormer and the rankr of LoRA\nare all set to 8. The prompt length l of VPT follows the\nrecipes in the original paper. We also report the results of\ntwo traditional transfer learning methods: Full fine-tuning,\nwhich updates all parameters on downstream tasks, andLin-\near probing, which learns a linear classification head on the\npre-trained backbone.\nFor our methods, we report four settings:FacT-TT4 with\nr = 4; FacT-TT≤16 with r searched from {1, 2, 4, 8, 16}\nfor each task; FacT-TK8 with r = 8; and FacT-TK≤32\nwith r searched from {2, 4, 8, 16, 32}. Following Zhang,\nZhou, and Liu (2022), we use AdamW optimizer with a\nlearning rate of 1e-3 and batch size of 64 to train for 100\nepochs. The hyper-parameter s is roughly swept from{0.01,\n0.1, 1, 10, 100}.\n1064\n12 4 8 16\n10\n20\n30\n40\n50\nFGVCAircraft\n12 4 8 16\n                                    # labeled training examples per class\n60\n65\n70\n75\n80\n85\n90\nOxfordPets\n12 4 8 16\n30\n40\n50\n60\n70\nFood101\n12 4 8 16\n10\n20\n30\n40\n50\n60\n70\nStanfordCars\n12 4 8 16\n40\n50\n60\n70\n80\n90\n100\nFlowers102\n12 4 8 16\n30\n40\n50\n60\n70\n80T est Acc (%)\nAverage\nNOAH (335K) LoRA (295K) Adapter (157K) AdaptFormer (157K) VPT-Deep (74K) FacT-TT (61K)\nFigure 6: Top-1 accuracy on fine-grained few-shot datasets. The average numbers of trainable parameters in backbones are\nshown in parentheses. Our FacT-TT outperforms other baselines using the fewest trainable parameters.\nPre-trained Backbone For all methods, we use a ViT-\nB/16 (Dosovitskiy et al. 2021) pre-trained on supervised\nImageNet-21K (Deng et al. 2009) as the backbone.\nResults Experimental results are shown in Table 1, from\nwhich we can see that:\n(1) FacT-TT and FacT-TK have competitive results\nwith respect to previous SOTA PETL methods while us-\ning much fewer trainable parameters. FacT-TT≤16 and\nFacT-TK≤32 introduce only 37K and 69K trainable pa-\nrameters, respectively. However, they outperform NOAH,\nthe previous SOTA methods with 361K trainable parame-\nters, on 11 out of 19 tasks. Moreover, FacT-TT≤16 and\nFacT-TK≤32 also achieve new SOTA results on 7 out of\n19 tasks. It’s worth noting that NOAH trains an additional\nlarge supernet for 500 epochs used for architecture search,\nand thus FacT is also superior to NOAH in terms of train-\ning efficiency.\n(2) For our two methods, neither FacT-TT nor\nFacT-TK shows an advantage over the other one. Note that\nTucker format can be regarded as further decomposing the\nΣ in Tensor-Train format into P and C. Although Tucker\nformat has a higher compression ratio, FacT-TK does not\nclearly outperform FacT-TT, implying that FacT-TT is\nsufficiently compact for adaptation and thus further com-\npression does not lead to obvious improvement.\n(3) Though sub-optimal to many baselines in terms of ab-\nsolute performance, FacT-TT4 and FacT-TK8 still pro-\nvide non-trivial improvement under extreme storage con-\nstraints. FacT-TT4 and FacT-TK8 use only 8K and 14K\nparameters (0.01% and 0.02% of the 85.8M ViT-B) to adapt\nthe ViT backbones. In contrast, the parameters of the classi-\nfication head will be even more than 8K as long as the task\ncontains more than 10 classes. In other words, FacT-TT4\nand FacT-TK8 use trainable parameters of the same mag-\nnitude as linear probing, while achieving performance better\nthan full fine-tuning and VPT.\nIn Fig 5, we also report the group-wise results on VTAB-\n1K. FacT achieves SOTA results on Natural and Special-\nized, but underperforms NOAH on Structured. We still em-\nphasize that our methods are much more lightweight, and\nthus more efficient than other baselines.\nFine-Grained Few-Shot Learning\nFew-shot learning is a common scenario when the data of\ndownstream tasks are hard to obtain, and there are only a\nfew training samples for each task that can be utilized.\nDatasets To evaluate the capability of our method in\nthe low-data regime, we conduct experiments on five fine-\ngrained datasets in few-shot settings. The five datasets\ninclude FGVC-Aircraft (Maji et al. 2013), Oxford-\nPets (Parkhi et al. 2012), Food-101 (Bossard, Guillau-\nmin, and Gool 2014), Stanford Cars (Krause et al. 2013),\nand Oxford-Flowers102 (Nilsback and Zisserman 2006),\nwhich contains fine-grained classes from five categories:\naircraft, pets, food, cars, and flowers. Following previous\nwork (Zhang, Zhou, and Liu 2022), we evaluate in {1, 2,\n4, 8, 16}-shot settings.\nCompared Methods We compare our method with five\nbaselines that perform the best on VTAB-1K: VPT-Deep,\nAdapter, AdaptFormer, LoRA, and NOAH. The hyper-\nparameter h, r, and l of the baselines are all set to 8. As\nfor our method, we report the results of FacT-TT16, i.e.,\nFacT-TT with a fixed r = 16. Other settings are the same\nas on VTAB-1K. All results are averaged over three runs\nwith different random seeds.\nResults As the results shown in Fig 6, we can find that:\n(1) Though using the fewest trainable parameters,\nFacT-TT still achieves SOTA results on average. Note that\nall these datasets can be categorized into the Natural group,\nso this observation is in line with what we have found on\nVTAB-1K that FacT-TT is better at Natural tasks.\n(2) FacT-TT performs the best across all settings on four\nout of five datasets except for Food-101, where FacT-TT\nslightly underperforms NOAH in 8-shot and 16-shot set-\ntings.\nThese observations confirm the capability and efficiency\nof our method in the low-data regime. Again, it verifies the\neffectiveness of reducing intra- and inter-rank redundancies\nof weight increments for PETL.\n1065\nMethod # param (M)\nAvg. Nat. Spe. Str.\nFull 86.7 75.0 79.2\n86.2 59.7\nLinear 0 62.6 73.5 80.8 33.5\nBitFit 0.201 65.6 74.2 80.1 42.4\nVPT-Shallow 0.003 66.7 79.9 82.5 37.8\nVPT-Deep 0.162 71.6 76.8 84.5 53.4\nFacT-TT16 0.135 77.4 83.1 86.9 62.1\nTable 2: Results on VTAB-1K with Swin-B as backbone.\nAvg./Nat./Spe./Str.: Average/Natural/Specialized/Structured\nresults. # params: # of trainable parameters in backbones.\nFacT for Hierarchical Transformers\nAfter the original ViT was proposed, it was found that ViT\nlacks visual inductive bias which limits its performance, es-\npecially on dense prediction tasks. Subsequently, a series\nof studies improved ViT by introducing hierarchical struc-\ntures (Liu et al. 2021; Wang et al. 2021; Wu et al. 2021),\namong which Swin Transformer (Liu et al. 2021) is a widely\nused and representative design. Therefore, we also extend\nFacT to Swin Transformer.\nA challenge when applying tensorization to such hierar-\nchical structures is that the hidden dimension d is different\nacross layers. However, these models usually partition the\nlayers into several stages, where the hidden dimension is\nconsistent within each stage. Therefore, we propose a par-\ntitioned tensorization strategy for these models, which indi-\nvidually tensorizes each stage into a single tensor.\nTaking Swin-B as an instance, its four stages consist\n{2, 2, 18, 2} layers with hidden dimensions of {128, 256,\n512, 1024}, respectively. Therefore, we can tensorize them\ninto four tensors of size {24×128×128, 24×256×256,\n216×512×512, 24×1024×1024} following the steps de-\nscribed in Section 3.1. These tensors are then decomposed\nindividually.\nWe report the results of FacT-TT16 on VTAB-1K in Ta-\nble 2, using Swin-B pre-trained on supervised ImageNet-\n21K as the backbone. We compare our method with base-\nlines that can also be employed on Swin: Full, Linear, Bit-\nFit, and VPT. We can see that our FacT-TT outperforms\nother PETL methods by a large margin. Although parti-\ntioned tensorization weakens the efficiency ofFacT to some\nextent since we have to store a set of factors for each stage,\nFacT-TT still uses fewer parameters than VPT-Deep and\nBitFit. These results demonstrate that FacT can also be\napplied to Swin Transformer while keeping its advantages.\nSince FacT is an architecture-agnostic framework, it can be\nextended to various models (e.g., ConvNets (Liu et al. 2022)\nand MLPs (Tolstikhin et al. 2021; Lian et al. 2022a)) and\ntasks (e.g., NLP and multimodal tasks), as long as the mod-\nels can be tensorized appropriately.\nAblation Analyses\nWe ablate the tensorization-decomposition framework on\nVTAB-1K benchmark to show the effect of tensorization\nstrategy and decomposition rank.\n0 50 100 150 200\n# trainable param (K)\n78.0\n78.5\n79.0\n79.5\n80.0\n80.5Test Acc (%)\nNatural\nMHSA\nFFN\nAll\n0 50 100 150 200\n# trainable param (K)\n83.0\n83.5\n84.0\n84.5\n85.0\nSpecialized\nMHSA\nFFN\nAll\n0 50 100 150 200\n# trainable param (K)\n56\n58\n60\nStructured\nMHSA\nFFN\nAll\nFigure 7: Results on VTAB-1K across different ranks\nand tensorization strategies. All: the original tensorization\nmethod that tensorizes both MHSA and FFN; MHSA/FFN:\ntensorize MHSA/FFN blocks only. We report results with\nr ∈ {4, 8, 16, 32}for each setting.\nThe default tensorization method tensorizes both MHSA\nand FFN blocks, resulting in a 12L × d × d tensor. We here\nconsider two ablated strategies: tensorizing only MHSA or\nFFN blocks, i.e.,\nW =\n\b\nWj\nq, Wj\nk, Wj\nv, Wj\no\n\tL\nj=1 ∈ R4L×d×d (13)\nor\nW =\n\b\n{Wj,(i)\nup }4\ni=1 ∪ {Wj,(i)\ndown}4\ni=1\n\tL\nj=1 ∈ R8L×d×d\n(14)\nrespectively. The blocks that are not tensorized keep frozen\nduring fine-tuning. As for the decomposition part, we use\nTensor-Train format with rankr ∈ {4, 8, 16, 32}. The results\nare shown in Fig 7.\nOn all three groups, tensorizing MHSA is better than ten-\nsorizing FFN overall, suggesting that MHSA blocks play a\nmore important role than FFN in downstream transfer tasks.\nTensorizing all blocks is better than the other two strategies\non Specialized and Structured. We also find that only ten-\nsorizing MHSA is slightly superior to tensorizing all on Nat-\nural when r is larger, indicating that it is feasible to further\ndevelop the potential ofFacT by searching the tensorization\nstrategy for different datasets. Besides, we find that as rankr\nincreases, the average performance is accordingly improved\nacross the three tensorization settings. However, since the\nsize of factor Σ increases with the square of r, FacT-TT is\nno longer efficient when r becomes too large. But this lim-\nitation will not have a significant negative impact because\nthe performance is almost saturated when r ≥ 16 and thus a\nlarge r is not necessary.\nConclusion\nIn this paper, we propose FacT, a tensorization-\ndecomposition framework for PETL on ViT. Under this\nframework, we present an approach to making ViT ten-\nsorized, and employ two tensor decomposition methods to\nfactorize its increment. By updating and storing the factors\nonly, our methods reduce both intra- and inter-weight re-\ndundancies of weight increments and thus are much more\nefficient. FacT achieves competitive results on VTAB-1K\nbenchmark with a significantly reduced number of parame-\nters and outperforms all PETL baselines on few-shot learn-\ning. Our work demonstrates that the storage efficiency of\nPETL has not been fully exploited yet, and FacT provides\na promising framework for future work.\n1066\nReferences\nBossard, L.; Guillaumin, M.; and Gool, L. V . 2014.\nFood-101–mining discriminative components with random\nforests. In Proceedings of ECCV.\nChen, S.; Ge, C.; Tong, Z.; Wang, J.; Song, Y .; Wang, J.; and\nLuo, P. 2022. AdaptFormer: Adapting Vision Transformers\nfor Scalable Visual Recognition. InProceedings of NeurIPS.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In Proceedings of CVPR.\nDenton, E. L.; Zaremba, W.; Bruna, J.; LeCun, Y .; and Fer-\ngus, R. 2014. Exploiting Linear Structure Within Convolu-\ntional Networks for Efficient Evaluation. In Proceedings of\nNIPS.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In Proceedings of ICLR.\nHe, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; and Neubig,\nG. 2022. Towards a Unified View of Parameter-Efficient\nTransfer Learning. In Proceedings of ICLR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In Proceedings of CVPR.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nde Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-Efficient Transfer Learning for NLP. In\nProceedings of ICML.\nHu, E. J.; yelong shen; Wallis, P.; Allen-Zhu, Z.; Li, Y .;\nWang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank\nAdaptation of Large Language Models. In Proceedings of\nICLR.\nJia, M.; Tang, L.; Chen, B.; Cardie, C.; Belongie, S. J.; Har-\niharan, B.; and Lim, S. 2022. Visual Prompt Tuning. In\nProceedings of ECCV.\nJie, S.; and Deng, Z. 2022. Convolutional Bypasses\nAre Better Vision Transformer Adapters. arXiv preprint,\narXiv:2207.07039.\nKrause, J.; Stark, M.; Deng, J.; and Fei-Fei, L. 2013. 3d ob-\nject representations for fine-grained categorization. In Pro-\nceedings of CVPR workshops.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.;\nand Soricut, R. 2020. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In Pro-\nceedings of ICLR.\nLathauwer, L. D.; Moor, B. D.; and Vandewalle, J. 2000. A\nMultilinear Singular Value Decomposition. SIAM J. Matrix\nAnal. Appl.\nLebedev, V .; Ganin, Y .; Rakhuba, M.; Oseledets, I. V .; and\nLempitsky, V . S. 2015. Speeding-up Convolutional Neu-\nral Networks Using Fine-tuned CP-Decomposition. In Pro-\nceedings of ICLR.\nLian, D.; Yu, Z.; Sun, X.; and Gao, S. 2022a. AS-MLP: An\nAxial Shifted MLP Architecture for Vision. In Proceedings\nof ICLR.\nLian, D.; Zhou, D.; Feng, J.; and Wang, X. 2022b. Scaling &\nShifting Your Features: A New Baseline for Efficient Model\nTuning. In Proceedings of NeurIPS.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. InProceedings of\nICCV.\nLiu, Z.; Mao, H.; Wu, C.; Feichtenhofer, C.; Darrell, T.; and\nXie, S. 2022. A ConvNet for the 2020s. In Proceedings of\nCVPR.\nMa, X.; Zhang, P.; Zhang, S.; Duan, N.; Hou, Y .; Zhou, M.;\nand Song, D. 2019. A Tensorized Transformer for Language\nModeling. In Proceedings of NeurIPS.\nMahabadi, R. K.; Henderson, J.; and Ruder, S. 2021. Com-\npacter: Efficient Low-Rank Hypercomplex Adapter Layers.\nIn Proceedings of NeurIPS.\nMaji, S.; Rahtu, E.; Kannala, J.; Blaschko, M.; and Vedaldi,\nA. 2013. Fine-grained visual classification of aircraft. arXiv\npreprint, arXiv:1306.5151.\nNilsback, M.-E.; and Zisserman, A. 2006. A visual vocabu-\nlary for flower classification. In Proceedings of CVPR.\nNoach, M. B.; and Goldberg, Y . 2020. Compressing Pre-\ntrained Language Models by Matrix Decomposition. InPro-\nceedings of AACL/IJCNLP.\nOseledets, I. V . 2011. Tensor-Train Decomposition. SIAM\nJ. Sci. Comput.\nParkhi, O. M.; Vedaldi, A.; Zisserman, A.; and Jawahar, C.\n2012. Cats and dogs. In Proceedings of CVPR.\nPfeiffer, J.; Kamath, A.; R¨uckl´e, A.; Cho, K.; and Gurevych,\nI. 2021. AdapterFusion: Non-Destructive Task Composition\nfor Transfer Learning. In Proceedings of EACL.\nRebuffi, S.-A.; Bilen, H.; and Vedaldi, A. 2017. Learning\nmultiple visual domains with residual adapters. In Proceed-\nings of NIPS.\nTolstikhin, I. O.; Houlsby, N.; Kolesnikov, A.; Beyer, L.;\nZhai, X.; Unterthiner, T.; Yung, J.; Steiner, A.; Keysers, D.;\nUszkoreit, J.; Lucic, M.; and Dosovitskiy, A. 2021. MLP-\nMixer: An all-MLP Architecture for Vision. In Proceedings\nof NeurIPS.\nWang, B.; Ren, Y .; Shang, L.; Jiang, X.; and Liu, Q. 2022.\nExploring extreme parameter compression for pre-trained\nlanguage models. In Proceedings of ICLR.\nWang, W.; Xie, E.; Li, X.; Fan, D.; Song, K.; Liang, D.; Lu,\nT.; Luo, P.; and Shao, L. 2021. Pyramid Vision Transformer:\nA Versatile Backbone for Dense Prediction without Convo-\nlutions. In Proceedings of ICCV.\nWinata, G. I.; Madotto, A.; Shin, J.; Barezi, E. J.; and Fung,\nP. 2019. On the Effectiveness of Low-Rank Matrix Fac-\ntorization for LSTM Model Compression. arXiv preprint,\narXiv:1908.09982.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. CvT: Introducing Convolutions to Vi-\nsion Transformers. In Proceedings of ICCV.\nYang, Y .; Krompass, D.; and Tresp, V . 2017. Tensor-Train\nRecurrent Neural Networks for Video Classification. InPro-\nceedings of ICML.\n1067\nYe, J.; Wang, L.; Li, G.; Chen, D.; Zhe, S.; Chu, X.; and\nXu, Z. 2018. Learning Compact Recurrent Neural Networks\nWith Block-Term Tensor Decomposition. InProceedings of\nCVPR.\nZaken, E. B.; Goldberg, Y .; and Ravfogel, S. 2022. Bit-\nFit: Simple Parameter-efficient Fine-tuning for Transformer-\nbased Masked Language-models. In Proceedings of ACL.\nZhai, X.; Puigcerver, J.; Kolesnikov, A.; Ruyssen, P.;\nRiquelme, C.; Lucic, M.; Djolonga, J.; Pinto, A. S.; Neu-\nmann, M.; Dosovitskiy, A.; Beyer, L.; Bachem, O.; Tschan-\nnen, M.; Michalski, M.; Bousquet, O.; Gelly, S.; and\nHoulsby, N. 2019. The Visual Task Adaptation Benchmark.\narXiv preprint, arXiv:1910.04867.\nZhang, J.; Peng, H.; Wu, K.; Liu, M.; Xiao, B.; Fu, J.; and\nYuan, L. 2022. MiniViT: Compressing Vision Transformers\nWith Weight Multiplexing. In Proceedings of CVPR.\nZhang, Y .; Zhou, K.; and Liu, Z. 2022. Neural Prompt\nSearch. arXiv preprint, arXiv:2206.04673.\n1068",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6969876885414124
    },
    {
      "name": "Transformer",
      "score": 0.6394973397254944
    },
    {
      "name": "Fine-tuning",
      "score": 0.6075057983398438
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5994510054588318
    },
    {
      "name": "Process (computing)",
      "score": 0.43479734659194946
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39857080578804016
    },
    {
      "name": "Machine learning",
      "score": 0.37167900800704956
    },
    {
      "name": "Engineering",
      "score": 0.12279528379440308
    },
    {
      "name": "Voltage",
      "score": 0.10020634531974792
    },
    {
      "name": "Physics",
      "score": 0.07391282916069031
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    }
  ],
  "cited_by": 70
}