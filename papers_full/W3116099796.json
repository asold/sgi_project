{
  "title": "BioMedBERT: A Pre-trained Biomedical Language Model for QA and IR",
  "url": "https://openalex.org/W3116099796",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2224443342",
      "name": "Souradip Chakraborty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2776124893",
      "name": "Ekaba Bisong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104394648",
      "name": "Shweta Bhatt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993619979",
      "name": "Thomas Wagner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3117594185",
      "name": "Riley Elliott",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1930299731",
      "name": "Francesco Mosconi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2479527281",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2052217781",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2963488798",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949894546",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2963339489",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4229641819",
    "https://openalex.org/W3013838212",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W1736726159",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2911489562"
  ],
  "abstract": "The SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with biomedical research. However, as the number of biomedical research papers continue to increase, the task of finding relevant articles to answer pressing questions has become significant. In this work, we propose a textual data mining tool that supports literature search to accelerate the work of researchers in the biomedical domain. We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks. We also leverage the new BREATHE dataset which is one of the largest available datasets of biomedical research literature, containing abstracts and full-text articles from ten different biomedical literature sources on which we pre-train our BioMedBERT model. Our work achieves state-of-the-art results on the QA fine-tuning task on BioASQ 5b, 6b and 7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings are used with Elasticsearch for the Information Retrieval task on the intelligently formulated BioASQ dataset. We believe our diverse dataset and our unique model architecture are what led us to achieve the state-of-the-art results for QA and IR tasks.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 669–679\nBarcelona, Spain (Online), December 8-13, 2020\n669\nBioMedBERT: A Pre-trained Biomedical Language Model for QA and IR\nSouradip Chakraborty∗\nWalmart Labs\nsouradip24@gmail.com\nShweta Bhatt∗\nJupiter\nshwetabhatt08@gmail.com\nFrancesco Mosconi\nZero to Deep Learning\nf@mosconi.me\nEkaba Bisong∗\nSiliconBlast\nekaba.bisong@siliconblast.com\nThomas O. Wagner III\nTPHS Research\nthomas.wagner@bertbiomed.org\nRiley D. Elliott\nTPHS Research\nriley.elliott@bertbiomed.org\nAbstract\nThe SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with\nbiomedical research. However, as the number of biomedical research papers continue to in-\ncrease, the task of ﬁnding relevant articles to answer pressing questions has become signiﬁcant.\nIn this work, we propose a textual data mining tool that supports literature search to accelerate\nthe work of researchers in the biomedical domain. We achieve this by building BioMedBERT,\na neural-based deep contextual understanding model for Question-Answering (QA) and Infor-\nmation Retrieval tasks. We also leverage the new BREATHE dataset which is one of the largest\navailable datasets of biomedical research literature, containing abstracts and full-text articles\nfrom ten different biomedical literature sources on which we pre-train our BioMedBERT model.\nOur work achieves state-of-the-art results on the QA ﬁne-tuning task on BioASQ 5b, 6b and\n7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings\nare used with Elasticsearch for the Information Retrieval task on the intelligently formulated\nBioASQ dataset. We believe our diverse dataset and our unique model architecture are what led\nus to achieve the state-of-the-art results for QA and IR tasks.\n1 Introduction\nThe COVID-19 pandemic reminded us of the need for a tool that biomedical researchers can use to sift\nthrough existing research to extract novel insights, and ultimately help them make novel drug discov-\neries. The rate of new publications in the biomedical ﬁeld is on the rise. PubMed reports that more\nthan 1 million biomedical research papers are published each year, amounting to nearly two papers per\nminute (Landhuis, 2016). For papers mentioning COVID-19 alone, as of June 2020 more than 8000 peer-\nreviewed publications have been published on PubMed. With the rate of scientiﬁc papers on COVID-19\ndoubling every fourteen days (Coren, 2020), it is imperative to have a language understanding tool that\ncan extract relevant information from credible literature, such as the research methodology, data, authors,\nresults, and citations (Hao, 2020).\nIn this paper, we address the problem from an information retrieval perspective, extracting the textual\nand contextual information from the corpus by taking a hierarchical approach. Traditional search ap-\nproaches such as Lucene-based Elasticsearch (Gormley and Tong, 2015) using BM-25 & Jaccard-based\nmatrices are efﬁcient in retrieving objective answers where the primary task is to extract speciﬁc parts of\n∗ Denotes ﬁrst author with equal contribution.\nGithub repo: https://github.com/BioMedBERT/biomedbert.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://\ncreativecommons.org/licenses/by/4.0/.\n670\nthe passage. However such methods struggle in the contextual retrieval of documents for which we need\nlatent space representation of the query and the corpus of passages.\nOur work leverages the BERT language model architecture to pre-train a large-scale biomedical lan-\nguage representation model, named BioMedBERT. The work is inspired by the research of (Lee et al.,\n2019) from Korea University & Clova AI research group. In said work, we use the new BREATHE\ndataset which combines full text articles and abstracts from ten data sources in the biomedical domain\nand use it to train our BioMedBERT model. We use theBERTLARGE model as a pre-training backbone\nto achieve new state-of-the-art results for question answering in the biomedical domain. In addition,\nwe obtained impressive results by combining BioMedBERT embeddings with Elasticsearch to obtain\nhighly relevant results for information retrieval. This is achieved by using a neural passage re-ranking\nmechanism, which learns the inherent structural dependencies in the query and the research articles. We\nvalidated our search algorithm by formulating BioASQ as a retrieval dataset.\n2 Related Work\nLatent space representation learning and vector space modeling have proven to be extremely successful\nin the natural language processing domain, where they have shown to efﬁciently encapsulate the hidden\nmeaning and context of sentences or passages. The journey of distributed word representation learning\nbegan with the efﬁcient Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fast-\nText (Joulin et al., 2016) which outperformed the traditional bag of words model by signiﬁcant margins\nin linguistic tasks, as these methods “largely” ignored the word context of the tokens. With the advent\nof sequential modeling and recurrent neural models, there was a signiﬁcant enhancement in information\ncontent of the latent representation learning, with LSTMs as the state of the art at the time (Greff et\nal., 2016). Non-recurrent sequence-to-sequence encoder-decoder models, also known as Transformers,\nwere introduced by (Vaswani et al., 2017). Transformers stacked attention layers into a multi-headed\nattention architecture. The attention mechanism makes it possible to learn long-running word dependen-\ncies between input and output sequences by computing a context vector of the encoder for each token\nin the sequence (Bahdanau et al., 2014). The BERT architecture, which is primarily the encoder part\nof the transformer, achieved notable model performances for several linguistic tasks. Pre-training and\nﬁne-tuning the BERT network on a domain speciﬁc corpus has also shown to outperform many language\nmodels (Devlin et al., 2018). Our work combines ideas from (Devlin et al., 2018) and (Lee et al., 2019)\nin using deep bidirectional transformers to learn contextual embeddings of word features from the large\nBREATHE corpora.\n3 BREATHE Dataset\nBiomedical Research Extensive Archive To Help Everyone, or BREATHE, is a new dataset collection of\nbiomedical research articles from leading medical archives. It is a combination of both full body texts\nand abstracts. The development and recent availability of this dataset signiﬁcantly inspired the work in\nthis paper. To the best of our knowledge, BREATHE is the largestdiverse collection of publicly available,\nmachine readable biomedical corpus for advanced language modeling (Goncharov et al., ).\nThe dataset collection process was done in line with “ethical” principles for scraping, along with public\nAPIs that were used when available. The primary advantage of the BREATHE dataset for our model is\nits source diversity. BREATHE contains full text articles and abstracts from nine sources including BMJ,\narXiv, medRxiv, bioRxiv, CORD-19, Springer Nature, NCBI, JAMA, and BioASQ (Goncharov et al., ).\nWe performed our experiments with BREATHE v1.0 dataset. BREATHE v1.0 contains more than 6M\narticles and about 4 billion words. BREATHE v2.0 is the most recent version (Goncharov et al., ) and\nthe results reported in this paper are from training BioMedBERT on BREATHE v1.0 dataset. Table 1\nsummarizes the BREATHE dataset.\n671\nCorpus BREATHE v1.0 BREATHE v2.0\nNum. of articles Num. of words Num. of articles Num. of words\nBMJ 19 41K 47 103K\narXiv 29 5.7K 71 14K\nmedRxiv 738 443K 1,831 1.1M\nNature Research 12,873 437M 32,376 1.1B\nbioRxiv 18,603 2.7M 44,272 6.4M\nCORD-19 27,753 9.2M 67,285 22.4M\nSpringer Nature 322,374 69M 808,265 173.2M\nNCBI 828,096 3.5B 2,080,168 9.3B\nBioASQ 4,920,278 2.5M 12,123,125 6.4M\nTotal 6,130,763 4.1B 15,090,155 10.6B\nTable 1: Corpora details for pre-training BioMedBERT.\n4 Methodology\nBioMedBERT was built on the foundation of the BERT architecture (Devlin et al., 2018). In our work, we\nleverage both the pre-training and ﬁne-tuning aspects of the BERT architecture which enhances accuracy\nwhile also ensuring the robustness of the model. BERT leverages a transformer architecture (Vaswani\net al., 2017) and uses the stacked encoder where each encoder consists of a multi-headed attention layer\nand a feed-forward network.\nBERT builds on the transformer architecture to train large unlabeled data over two self-supervised\ntasks, and they are the Masked Language Model (MLM) and Next Sentence Prediction (NSP). MLM\n(also referred to as the Cloze task) (Taylor, 1953) works by randomly masking 15% of the sequence\nand then attempting to predict the masked tokens, where as the NSP task helps the model to understand\nthe relationship between two sentences. It works by predicting if a sentence is the actual “following”\nsentence or if it is just a random sentence. For more details on the training procedure for BERT, the\nreader is referred to (Devlin et al., 2018). In training the BERT model, the total loss is computed as the\nsum of the masked-LM loss and next sentence prediction loss, both of which are cross-entropy, with the\nformer multi-class log-loss and the later binary log-loss.\nL =\n(\n−1\nN\nN∑\ni=1\ny.log(p) + (1−y)log(1 −p)\n)\n  \nNSP loss\n+\n(\n−\nN∑\ni=1\nq(y).log(q(y))\n)\n  \nMaskedLM loss\nBioMedBERT Model Architecture. Our model architecture retrained the BERTLARGE architecture\nwith 24 transformer blocks, with a hidden-size of 1024 and 16 attention heads.\n4.1 Pre-training BioMedBERT\nThe “de facto” BERT model was pre-trained on BooksCorpus (800M words) (Zhu et al., 2015) and\nEnglish Wikipedia (2.5B words) (Devlin et al., 2018). The BioBERT model from (Lee et al., 2019)\nwas pre-trained on different combinations of text-data from English Wikipedia, BookCorpus, PubMed\nAbstracts (4.5 billion words) and PubMed Central full-text abstracts (13.5 billion words). We trained the\nBioMedBERT model on BREATHE containing over 6 million articles from 9 different archives with 4\nbillion words. In pre-processing our dataset, we treated lists, tables and headers as a contiguous sequence\nof text and initialized our model using the pre-trained BERT weights. Initially we trained the model from\nscratch with a custom SentencePiece vocabulary for tokenization, but this did not yield good results\nwhen evaluated on downstream ﬁne-tuned tasks. We represented the input sequences as vectors using\nthe WordPiece embeddings, which contains 30,000 tokens (Wu et al., 2016). Using WordPiece allowed\nus to better leverage the pre-trained weights of BERT. WordPiece has a clever formulation to account for\n672\nFigure 1: BioMedBERT Information Retrieval Architecture\nwords not found in its vocabulary by breaking a word into subwords, thereby creating multiple tokens\nfor a given word. Also, WordPiece has a subword for every character in the alphabet.\n4.2 Fine-tuning BioMedBERT\nThe BioMedBERT model retains the architectural effectiveness of the BERT model, which itself lever-\nages the multi-head self attention mechanism of the transformer network which makes it amenable to\ntask speciﬁc ﬁne-tuning. It is important to note the [CLS] token assigned by the WordPiece tokenizer,\nwhich is a special classiﬁcation token to represent the aggregate sequence for downstream classiﬁca-\ntion tasks. In building on the framework provided by (Lee et al., 2019) and (Devlin et al., 2018), our\nmodel is ﬁne-tuned on three NLP tasks: Named Entity Recognition (NER), Relation Extraction (RE) and\nQuestion Answering (QA).\nNamed entity recognition is the most fundamental sub-task of information extraction, it involves clas-\nsifying and recognizing named entities such as drugs, diseases, etc. from unstructured biomedical text.\nThe single output layer predicts the token level BIO2 probabilities for each input sequence. We use pre-\ncision, recall, and F1 score as metrics for evaluation. The details of the datasets used for NER ﬁne-tuning\ntask are mentioned in Table 2.\nDataset Entity type Number of annotations\nNCBI Disease (Do˘gan et al., 2014) Disease 6881\nBC5CDR (Li et al., 2016) Disease 12694\nBC5CDR (Li et al., 2016) Drug/Chem. 15411\nBC4CHEMD (Krallinger et al., 2015) Drug/Chem. 79842\nBC2GM (Smith et al., 2008) Gene/Protein 20703\nJNLPBA (Kim et al., 2004) Gene/Protein 35460\nTable 2: Statistical overview of biomedical NER datasets.\nRelation extraction is another sub-task of information extraction which involves detection and classiﬁ-\ncation of relationships between named entity mentions in a biomedical corpus. In processing the results\nfor the relation extraction datasets, we incorporated the technique from BioBERT (Lee et al., 2019) in\nusing predeﬁned tags to anonymize target named entities. For evaluation of RE, we also used precision,\nrecall, and F1 score as metrics. Statistical details of the biomedical RE datasets are provided in Table 3.\nQuestion answering is the task of predicting the text span of the answer, given the question and pas-\nsage containing the answer. For general language question answering task, we ﬁne-tuned BioMedBERT\non SQuAD v1.0 and v2.0 datasets using the same BERT architecture used for SQuAD. For the biomed-\nical QA ﬁne-tuning task, we used BioASQ factoid datasets, as their data format is similar to that of the\n673\nDataset Entity type Number of relations\nGAD (Bravo et al., 2015) Gene-Disease 5330\nEU-ADR (Van Mulligen et al., 2012) Gene-Disease 355\nTable 3: Statistical overview of biomedical RE datasets.\nSQuAD datasets (Lee et al., 2019). For evaluating BioASQ, we used the evaluation code from BioBERT\nto exclude samples with unanswerable questions from the training set. And like (Lee et al., 2019)\nand (Wiese et al., 2017), we ﬁne-tuned BioASQ on weights that were initially ﬁne-tuned on SQuAD\nv1.1. However, unlike them, we also ﬁne-tuned BioASQ on SQuAD v2.0 weights which was beneﬁcial\nin outperforming state-of-the-art results for these tasks. This is another key contribution in this work.\nThe results are reported in Table 6 and the dataset details are provided in Table 4.\nDataset Number of questions\nBioASQ 4b-factoid (Tsatsaronis et al., 2015) 1307\nBioASQ 5b-factoid (Tsatsaronis et al., 2015) 1799\nBioASQ 6b-factoid (Tsatsaronis et al., 2015) 2251\nBioASQ 7b-factoid (Tsatsaronis et al., 2015) 2747\nTable 4: Statistical overview of biomedical QA datasets.\n5 Neural Information Retrieval & Biomedical Search\nIn this phase, the primary objective was to retrieve the most relevant research papers and articles in a\nranked order based on the query made by the researcher( qi). There are two major aspects that enhance\nthe complexity of this problem. First, the computational complexity associated with retrieval from a\nbiomedical corpus of millions of records(N). Second, the ability to accurately retrieve the relevant doc-\numents in a proper rank-ordered fashion, and to do so the process needs to account for a deep contextual\nunderstanding of the text.\nWe approach the problems in a 2-step hierarchical fashion where in the ﬁrst step, we primarily reduce\nthe search space complexity by using the Elasticsearch and BM-25 algorithm, a well-known metric in\nretrieval scenarios (Gormley and Tong, 2015). Elasticsearch itself operates in two stages. The ﬁrst stage\ncreates an extremely efﬁcient data structure in textual searching scenarios, an inverted index, of the entire\ncorpus of biomedical research papers. The second stage queries the entire corpus using the inverted index\nand scores the search results using the similarity (relevancy) ranking function BM25, and returns the top\nk search results based on that BM 25 score, where k is a tunable parameter (Turney and Pantel, 2010).\nThe output of this stage is an ordered state of retrieved biomedical papers sorted by BM 25 scores and\nlet (ar1, ar2, ar3...ark) be the top k retrieved documents.\nBM 25(t, d) = (k + 1)×(t, d)\n(k ×(1 −b + b( |d|\n|d|avg\n)))\n×id f(t, D)\nHere, f(t, d) represents the raw frequency of the term t in the document d, id f(t, D) is a function of\nnumber of documents, the term t has occurred in, given the universal set of documents D , |d|is the\nnumber of words in the document and k, bare parameters.\nThe results of the Elasticsearch based retrieval mechanism did not incorporate the contextual aspects\nof the query and the speciﬁc biomedical aspects of the corpus. As mentioned earlier, for effective IR\nin this domain, contextual consideration is highly relevant. When biomedical researchers search for\nrelevant studies, they search for speciﬁc combinations of topics and scenarios. For instance, they might\nsearch for “how does coronavirus impact the lungs and to what extent” rather than a generic search such\nas “what is coronavirus”. In order to best answer these types of speciﬁc queries, the system needs robust\ncontext for both the question and the possible answers.\n674\nTo solve this challenge, in the second step of the hierarchical search we use BioMedBERT to project\nthe query(qi), retrieve the top k papers(ar1, ar2, ar3...ark) in a d dimensional latent space and extract\nthe embeddings (Nogueira and Cho, 2019). Let the query embedding vector of qi be represented by\nQi ∈Rd and the matrix embedding representation of the top k retrieved papers be Ak×d, Ai ∈Rd. We\ncompute the cosine similarity as a vector-matrix product between the normalized query (Q′\ni = Qi\n||Qi||)\nand the normalised retrieved paper embedding matrix (A′\nk×d, whereA′\ni = Ai\n||Ai||). Let Z be the output\nafter computing the cosine similarity between the query and the papers, where Z = A′\nk×d ·Q′\ni and\nZ ∈Rk. Finally, we sorted the output cosine scores Z , while re-ranking and returning the most relevant\nbiomedical research papers conditioned on the query(qi).\nargmax\nj∈k\nZj\nThe primary intent of selecting cosine similarity as the preferred metric is that the vector cosine scores\nare normalized on each of the dimensions and hence are robust to scaling (Eibl and Gaedke, 2017).\n6 Results\n6.1 Datasets\nIn order to simplify the process of comparing our work with related works, we perform the ﬁne-tuning\nexperiments on the biomedical and general language datasets that are most widely used by other NLP\nresearchers. For NER ﬁne-tuning task, we used the eight pre-processed datasets provided by (Wang et\nal., 2019) mentioned in Table 7. The NER evaluations for all datasets mentioned in Table 7 are based on\nentity level exact matches. For the RE ﬁne-tuning task, we used the pre-processed GAD and EU-ADR\ndatasets from (Lee et al., 2020) which contain gene-disease relations. For general domain QA ﬁne-\ntuning task, we used SQuAD v1.1 and SQuAD v2.0 datasets (Rajpurkar et al., 2016) and for biomedical\ndomain QA task, we used pre-processed 4b, 5b and 6b BioASQ datasets provided by (Lee et al., 2020)\nand pre-processed BioASQ 7b dataset from (Yoon et al., 2019). We report the results in the form of\nmicro-average scores of all ﬁve test batches of BioASQ datasets for QA ﬁne-tuning and 10-fold cross\nvalidation scores for GAD and EU-ADR dataset for RE ﬁne-tuning task.\n6.2 Experimental Setup\nIn pre-training BioMedBERT, we use the setup provided by BERT (Devlin et al., 2018). For ﬁne-tuning\nand evaluation, we use the setup provided by (Lee et al., 2020). During the initial phase of experi-\nmentation, we explore training the BioMedBERT model from scratch on BREATHE using our custom\nvocabulary created with SentencePiece tokenizer (Kudo and Richardson, 2018). But after evaluation on\na downstream tasks of NER and QA, we found that we need to either add general domain data to our\ndataset or use the weights of BERT to initialize the model.\nHence, we trained our model using BERTLARGE weights as a transfer learning backbone on the\nBREATHE dataset. We use the same architecture and hyper-parameters as BERT and trained the model\nfor 68k epochs. The resulting improvements in results was very close to state-of-the-art scores for NER.\nThis motivated us to continue work on BioMedBERT and eventually train the model for 1M steps. The\npre-training of BioMedBERT model on Google Cloud v3-TPUs with 128 cores for 1M epochs took a\nlittle over 3 days. Fine-tuning tasks on the same TPU took less than an hour.\n6.3 Experimental Results\nWe provide the results for the selected downstream tasks in Tables 5, 6, 7 and 8. In each table, we\ncompare BioMedBERT’s performance against BERT and the extant state-of-the-art model for the corre-\nsponding task. Among the four ﬁne-tuning tasks selected for evaluation and comparison with previous\nworks, BioMedBERT achieves better results than the BERT model for nearly all of them on biomedical\ndatasets. BioMedBERT outperforms the state-of-the-art models on QA ﬁne-tuning task using SQuAD\nv2.0 dataset and also achieves close to state-of-the-art results in NER and RE ﬁne-tuning tasks for\n675\nbiomedical datasets,demonstrating its robustness in domain speciﬁc downstream tasks. The BioMed-\nBERT v1.0 model ﬁne-tuned on SQuAD v2.0 dataset and further ﬁne-tuned on BioASQ 5b, 6b and 7b\ndatasets (trained for 2 epochs) outperforms state-of-the-art MRR scores for all 3 datasets as seen in Ta-\nble 8 1. The best scores are highlighted in bold while the second best scores are underlined. By this,\nBioMedBERT may be viewed as the new state-of-the-art results for biomedical question-answering\ntasks.\nDataset Metrics BERT SOTA BioMedBERT\nGAD\nP 74.28 79.21 78.04\nR 85.11 89.25 82.01\nF 79.29 83.94 79.92\nEU-ADR\nP 75.45 81.05 76.34\nR 96.55 98.01 84.74\nF 84.62 86.51 78.43\nTable 5: Results for RE ﬁne-tuning on GAD and EU-ADR datasets.\nDataset Metrics (Dev) Human BERT BioMedBERT\nSQuAD v1.1 EM 82.3 86.2 86.12\nF1 91.2 92.2 92.46\nSQuAD v2.0 EM 86.2 78.7 80.85\nF1 89.5 81.9 83.96\nTable 6: Results for QA ﬁne-tuning on SQuAD v1.1 and v2.0 datasets.\nType Dataset Metrics BERT SOTA BioMedBERT\nDisease\nNCBI Disease\nP 84.12 89.04 86.02\nR 87.19 91.25 89.06\nF 85.63 89.71 87.51\nBC5CDR\nP 81.97 89.61 83.79\nR 82.48 87.84 85.06\nF 82.41 87.15 84.42\nDrug/chem\nBC5CDR\nP 90.94 94.26 92.04\nR 91.38 93.61 92.37\nF 91.16 93.47 92.21\nBC4CHEMD\nP 91.19 92.8 89.43\nR 88.92 91.92 83.58\nF 90.04 92.36 86.41\nGene/Protein\nBC2GM\nP 81.17 85.16 81.4\nR 82.42 85.12 83.26\nF 81.79 84.72 82.32\nJNLPBA\nP 69.57 74.43 70.93\nR 81.20 83.56 82.76\nF 74.94 83.56 76.39\nTable 7: Results for NER ﬁne-tuning on Biomedical NER datasets.\n1http://participants-area.bioasq.org/\n676\nDataset Metrics BERT SOTA BioMedBERT v1.0\nSQuAD v1.1 SQuAD v2.0\nepochs=2 epochs=5 epochs=2 epochs=5\nBioASQ 4b SAcc 27.33 34.76 29.99 32.92 32.59 32.30\nLAcc 44.72 50.88 48 50.31 48.44 50.31\nMRR 33.77 41.34 30.41 39.9 38.28 40.00\nBioASQ 5b SAcc 39.33 46.66 46.8 45.33 46.68 46.00\nLAcc 52.67 60.38 60.81 61.33 61.29 61.33\nMRR 44.27 52.12 51.65 51.85 52.14 51.75\nBioASQ 6b SAcc 33.54 42.86 41.84 39.13 41.92 42.86\nLAcc 51.55 61.18 61.06 60.87 62.12 60.25\nMRR 40.88 49.05 49.33 47.64 50.50 49.81\nBioASQ 7b SAcc - 40.12 41.98 46.3 40.72 41.36\nLAcc - 61.11 54.46 57.4 60.2 59.88\nMRR - 48.47 46.95 50.4 48.64 49.03\nTable 8: Results for QA ﬁne-tuning on BioASQ datasets.\n6.4 Information Retrieval Task Formulation & Experimental Results\nOne of the most challenging parts of our research was to create a validation framework for our end-to-end\nbiomedical retrieval methodology. To accomplish that, we had to ensure two major things. First, the vali-\ndation corpus should be biomedical in nature, as our embeddings are primarily trained on the biomedical\ncorpus. Second, the word length distribution of the validation corpus used for retrieval should be simi-\nlar to the word length distribution of the abstracts of the biomedical research papers in the BREATHE\ncorpus in order to have a meaningful validation. Both of these constraints were satisﬁed by formulating\nthe BioASQ dataset intelligently where we retrieve the ’context’ from the ’question’, rather than the\n’answers’ which are typically much smaller.\nAdditionally, we discovered the bias in the BioASQ dataset from a retrieval perspective, due to the\nhigh percentage of intersection between the words in the question and the context - which won’t be the\ncase in real life scenarios. Moreover, having a higher percentage intersection between questions and\nanswers will cause results to be biased towards only Elasticsearch based approach (Figure 2).\nTo debias the dataset, we remove the records that have a very high percentage of common thresholds\n(based on the Jaccard index) (Leskovec et al., 2020). The result in Table 9 shows that our methodology\nsigniﬁcantly outperforms other models on the re-structured BioASQ dataset, which is a major novelty in\nour research work.\nRetrieved from\nElasticsearch\nMRR@5 MRR@10\nES ES+\nBioMedBERT\nES+\nGlove\nES+\nBERT ES ES+\nBioMedBERT\nES+\nGlove\nES+\nBERT\nTop 20 0.172 0.253 0.100 0.125 0.20 0.285 0.124 0.157\nTop 30 0.173 0.237 0.090 0.115 0.20 0.273 0.116 0.138\nTop 40 0.173 0.231 0.100 0.113 0.21 0.272 0.111 0.132\nTop 50 0.172 0.225 0.082 0.097 0.20 0.269 0.098 0.117\nTable 9: MRR Evaluation on BioASQ dataset with ≤30% intersection.\n7 Discussion\nWe observe that the BioMedBERT model achieves state-of-the-art results in the QA tasks for both of\nthe datasets in the biomedical domain (see Table 8) as well as in the general language domain with\nSQuAD v1.1 and 2.0 (see Table 6). Additionally, the model showed robustness with an impressive\n677\nFigure 2: BioASQ Overlap and MRR Evaluation.\nperformance compared to BERT and the extant state-of-the-art in other language tasks such as Named\nEntity Recognition. While the model did not outperform the state-of-the-art, it was mostly on-par with\nBERT for most of the NER performance metrics. The same case for model-robustness can be made for\nour model’s performance in Relation Extraction tasks.\nCurrent challenges related to research into COVID-19 directed us to build a language mining tool\nto support question-answering and information retrieval for the biomedical domain. With respect to\nthe main purpose of this research, our results are the current state-of-the-art given the model\nperformances on BioASQ question-answering datasets. Our model outperformed numbers from\nBioBERT (Lee et al., 2019) on QA tasks irrespective of the fact that BioBERT was trained on a total\nof 18B words from the biomedical domain (13.5B from PMC full-text articles and 4.5B from PubMed\nabstracts), while our BioMedBERT model was trained on just over 4.1B words. The key reason for\nour better performance was the diversity in the datasets on which the BioMedBERT model was trained.\nDiversity helped in enhancing both the performance and robustness for the BioMedBERT model.\nAnother important and novel aspect of our work was how we framed the BioASQ dataset to validate\nour information retrieval methodology. Speciﬁcally, we debiased reﬂect reality and achieved, robust\nand relevant results. BioMedBERT embeddings coupled with Elasticsearch outperformed the retrieval\nperformance of the other models based on the Mean Reciprocal Rank values as shown in Table 9. Here we\nexperimented based on retrieving variable number of documents using Elasticsearch, and then re-ranked\nusing embeddings to possibly eliminate any further source of bias. Our methodology (BioMedBERT +\nES) outperforms the others by signiﬁcant margins for all the cases.\n8 Conclusion\nIn this paper, we present the BioMedBERT model pre-trained on the BREATHE v1.0 dataset, one of\nthe largest and most diverse datasets of biomedical research literature. BioMedBERT achieves state-\nof-the-art results when ﬁne-tuned on Question and Answering datasets, and also produces impres-\nsive performances on other language tasks such as Named Entity Recognition and Relation Extrac-\ntion. BioMedBERT embeddings coupled with Elasticsearch gives state-of-the-art performance on the\nre-framed BioASQ dataset. Moreover, the BioMedBERT model achieves state-of-the-art results for mul-\ntiple tasks even when only pre-trained on the BREATHE v1.0 dataset, which contains just over 6 million\narticles. Work is in progress to train an improved BioMedBERT model on the BREATHE v2 dataset\nwith over 16 million articles. We believe continued enhancements of the BioMedBERT model will help\nbiomedical researchers discover meaningful insights from literature faster, and make signiﬁcant improve-\nments in their ﬁeld.\nWe would like to thank Daniel Goncharov and 42 School Silicon Valley for their contributions to build-\ning the BREATHE dataset. We would also like to thank Dave Elliott with Google Cloud Platform and\nGoogle TensorFlow Research Cloud for providing infrastructure and guidance throughout the project.\n678\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to\nalign and translate. arXiv preprint arXiv:1409.0473.\n`Alex Bravo, Janet Pi ˜nero, N ´uria Queralt-Rosinach, Michael Rautschka, and Laura I Furlong. 2015. Extraction\nof relations between genes and diseases from text and large-scale data analysis: implications for translational\nresearch. BMC bioinformatics, 16(1):55.\nMichael J Coren. 2020. The number of scientiﬁc papers on the novel coronavirus is doubling every 14 days.\nhttps://qz.com/, Apr.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nRezarta Islamaj Do˘gan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for disease name\nrecognition and concept normalization. Journal of biomedical informatics, 47:1–10.\nMaximilian Eibl and Gaedke. 2017. Evaluating the impact of word embeddings on similarity scoring in practical\ninformation retrieval. In Gesellschaft fur Informatik.\nDaniel Goncharov, David Elliott, and Soonson Kwon. How the google ai community used cloud to help biomedical\nresearchers — google cloud blog.\nClinton Gormley and Zachary Tong. 2015. Elasticsearch: the Deﬁnitive Guide: A Distributed Real-time Search\nand Analytics Engine. ” O’Reilly Media, Inc.”.\nKlaus Greff, Rupesh K Srivastava, Jan Koutn ´ık, Bas R Steunebrink, and J ¨urgen Schmidhuber. 2016. Lstm: A\nsearch space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232.\nK Hao. 2020. Over 24,000 coronavirus research papers are now available in one place.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efﬁcient text\nclassiﬁcation. arXiv preprint arXiv:1607.01759.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. 2004. Introduction to the\nbio-entity recognition task at jnlpba. In Proceedings of the international joint workshop on natural language\nprocessing in biomedicine and its applications, pages 70–75. Citeseer.\nMartin Krallinger, Obdulia Rabal, Florian Leitner, Miguel Vazquez, David Salgado, Zhiyong Lu, Robert Leaman,\nYanan Lu, Donghong Ji, Daniel M Lowe, et al. 2015. The chemdner corpus of chemicals and drugs and its\nannotation principles. Journal of cheminformatics, 7(1):1–17.\nTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\nEsther Landhuis. 2016. Scientiﬁc literature: Information overload. Nature, 535(7612):457–458.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nJure Leskovec, Anand Rajaraman, and Jeffrey D. Ullman. 2020. Mining of massive datasets. Cambridge Univer-\nsity Press.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis,\nCarolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. Biocreative v cdr task corpus: a resource for\nchemical disease relation extraction. Database, 2016.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of\nwords and phrases and their compositionality. In Advances in neural information processing systems , pages\n3111–3119.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with bert. arXiv preprint arXiv:1901.04805.\n679\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word represen-\ntation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250.\nLarry Smith, Lorraine K Tanabe, Rie Johnson nee Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi\nLin, Roman Klinger, Christoph M Friedrich, Kuzman Ganchev, et al. 2008. Overview of biocreative ii gene\nmention recognition. Genome biology, 9(S2):S2.\nWilson L Taylor. 1953. “cloze procedure”: A new tool for measuring readability. Journalism quarterly ,\n30(4):415–433.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R\nAlvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An\noverview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC\nbioinformatics, 16(1):138.\nPeter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal\nof Artiﬁcial Intelligence Research, 37, 03.\nErik M Van Mulligen, Annie Fourrier-Reglat, David Gurwitz, Mariam Molokhia, Ainhoa Nieto, Gianluca Triﬁro,\nJan A Kors, and Laura I Furlong. 2012. The eu-adr corpus: annotated drugs, diseases, targets, and their\nrelationships. Journal of biomedical informatics, 45(5):879–884.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nXuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo Shang, Curtis Langlotz, and Jiawei\nHan. 2019. Cross-type biomedical named entity recognition with deep multi-task learning. Bioinformatics,\n35(10):1745–1752.\nGeorg Wiese, Dirk Weissenborn, and Mariana Neves. 2017. Neural domain adaptation for biomedical question\nanswering. arXiv preprint arXiv:1706.03610.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridg-\ning the gap between human and machine translation. arXiv preprint arXiv:1609.08144.\nWonjin Yoon, Jinhyuk Lee, Donghyeon Kim, Minbyul Jeong, and Jaewoo Kang. 2019. Pre-trained language\nmodel for biomedical question answering. In Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases, pages 727–740. Springer.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\n2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27.",
  "topic": "Leverage (statistics)",
  "concepts": [
    {
      "name": "Leverage (statistics)",
      "score": 0.8714749813079834
    },
    {
      "name": "Computer science",
      "score": 0.8193576335906982
    },
    {
      "name": "Task (project management)",
      "score": 0.6753097772598267
    },
    {
      "name": "Information retrieval",
      "score": 0.5463590621948242
    },
    {
      "name": "Language model",
      "score": 0.5424789190292358
    },
    {
      "name": "Question answering",
      "score": 0.5017344951629639
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4947606921195984
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46505507826805115
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.4225751459598541
    },
    {
      "name": "Biomedical text mining",
      "score": 0.41899359226226807
    },
    {
      "name": "Data science",
      "score": 0.3934444189071655
    },
    {
      "name": "Natural language processing",
      "score": 0.3822677731513977
    },
    {
      "name": "Text mining",
      "score": 0.258732408285141
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1330693074",
      "name": "Walmart (United States)",
      "country": "US"
    }
  ]
}