{
  "title": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students’ Formative Assessment Responses in Science",
  "url": "https://openalex.org/W4393119218",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Cohn, Clayton",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A3182563824",
      "name": "Hutchins Nicole",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2317259334",
      "name": "Le Tuan",
      "affiliations": [
        "DePauw University"
      ]
    },
    {
      "id": "https://openalex.org/A2166787625",
      "name": "Biswas, Gautam",
      "affiliations": [
        "Vanderbilt University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4381956362",
    "https://openalex.org/W1569239808",
    "https://openalex.org/W4309301551",
    "https://openalex.org/W4366808697",
    "https://openalex.org/W4288059447",
    "https://openalex.org/W6663855623",
    "https://openalex.org/W2037789405",
    "https://openalex.org/W3107048485",
    "https://openalex.org/W4381956458",
    "https://openalex.org/W6612715669",
    "https://openalex.org/W2171648197",
    "https://openalex.org/W4321442014",
    "https://openalex.org/W3209722613",
    "https://openalex.org/W4319996980",
    "https://openalex.org/W6689166158",
    "https://openalex.org/W2783152414",
    "https://openalex.org/W6642011314",
    "https://openalex.org/W2063747225",
    "https://openalex.org/W4384644567",
    "https://openalex.org/W4382567099",
    "https://openalex.org/W6782595193",
    "https://openalex.org/W2053170657",
    "https://openalex.org/W3211589388",
    "https://openalex.org/W4365460692",
    "https://openalex.org/W2902578445",
    "https://openalex.org/W3144501941",
    "https://openalex.org/W4317939587",
    "https://openalex.org/W4381956153"
  ],
  "abstract": "This paper explores the use of large language models (LLMs) to score and explain short-answer assessments in K-12 science. While existing methods can score more structured math and computer science assessments, they often do not provide explanations for the scores. Our study focuses on employing GPT-4 for automated assessment in middle school Earth Science, combining few-shot and active learning with chain-of-thought reasoning. Using a human-in-the-loop approach, we successfully score and provide meaningful explanations for formative assessment responses. A systematic analysis of our method's pros and cons sheds light on the potential for human-in-the-loop techniques to enhance automated grading for open-ended science assessments.",
  "full_text": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students’\nFormative Assessment Responses in Science\nClayton Cohn1, Nicole Hutchins1, Tuan Le2, Gautam Biswas1\n1Vanderbilt University\n2DePauw University\n{clayton.a.cohn, nicole.m.hutchins, gautam.biswas}@vanderbilt.edu, tuanle 2025@depauw.edu\nAbstract\nThis paper explores the use of large language models (LLMs)\nto score and explain short-answer assessments in K-12 sci-\nence. While existing methods can score more structured math\nand computer science assessments, they often do not provide\nexplanations for the scores. Our study focuses on employ-\ning GPT-4 for automated assessment in middle school Earth\nScience, combining few-shot and active learning with chain-\nof-thought reasoning. Using a human-in-the-loop approach,\nwe successfully score and provide meaningful explanations\nfor formative assessment responses. A systematic analysis of\nour method’s pros and cons sheds light on the potential for\nhuman-in-the-loop techniques to enhance automated grading\nfor open-ended science assessments.\nIntroduction\nImprovements in Science, Technology, Engineering, and\nMathematics (STEM) education have accelerated the shift\nfrom teaching and assessing facts to developing stu-\ndents’ conceptual understanding and problem-solving skills\n(NGSS 2013). To foster students’ developing scientific ideas\nand reasoning skills, it is crucial to have assessments that re-\nveal and support their progress (Harris et al. 2023). Forma-\ntive assessments play an important role in this endeavor, pro-\nviding timely feedback and guidance when students face dif-\nficulties, which helps them to develop self-evaluation skills\n(Bloom, Madaus, and Hastings 1971). However, the process\nof grading and generating personalized feedback from fre-\nquent formative assessments is time-consuming for teach-\ners and susceptible to errors (Rodrigues and Oliveira 2014;\nHaudek et al. 2011).\nLarge Language Models (LLMs) provide opportunities\nfor automating short answer scoring (Funayama et al. 2023)\nand providing feedback to help students overcome their dif-\nficulties (Morris et al. 2023). These approaches can also aid\nteachers in identifying students’ difficulties and generating\nactionable information to support student learning. To our\nknowledge, there is very little research that combines au-\ntomated formative assessment grading and feedback gener-\nation for science domains where understanding, reasoning,\nand explaining are key to gaining a deep understanding of\nscientific phenomena (Mao et al. 2018).\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nThis paper develops an approach for human-in-the-loop\nLLM prompt engineering using in-context learning and\nchain-of-thought reasoning with GPT-4 to support auto-\nmated analysis and feedback generation for formative as-\nsessments in a middle school Earth Science curriculum. We\npresent our approach, discuss our results, evaluate the limi-\ntations of our work, and then propose future research in this\narea of critical need in K-12 STEM instruction.\nBackground\nTo understand the difficulties students face when learning\nscience, teachers need to actively track students’ developing\nknowledge (Wiley et al. 2020). This is particularly impor-\ntant for open-ended, technology-enhanced learning environ-\nments that support students in their knowledge construction\nand problem-solving processes (Hutchins and Biswas 2023).\nIn these environments, knowledge and skill development\nhappen through system interactions that are difficult to mon-\nitor and interpret (Walkoe, Wilkerson, and Elby 2017). For-\nmative assessments, evaluation, and feedback mechanisms\naligned with target learning goals (Bloom, Madaus, and\nHastings 1971), can play a dual role: (1) help students rec-\nognize constructs that are important to learning, and (2) pro-\nvide teachers with a deeper understanding of student knowl-\nedge and reasoning to better support their developing STEM\nideas (Cizek and Lim 2023). However, grading formative as-\nsessments, particularly in K-12 STEM contexts, where stu-\ndents’ responses may not be well-structured and may vary\nconsiderably in vocabulary and stylistic expression, is time-\nconsuming and can result in erroneous scoring and incom-\nplete feedback (Liu et al. 2016). Moreover, grading these as-\nsessments at frequent intervals may become a burden rather\nthan an aid to teachers. Very little research has examined ef-\nfective mechanisms for generating automated grading and\nuseful formative feedback for K-12 students that are aligned\nwith classroom learning goals.\nAdvances in natural language processing (NLP) have pro-\nduced improved automated assessment scoring approaches\nto support teaching and learning (e.g., Adair et al. 2023;\nWilson et al. 2021). Proposed methodologies include data\naugmentation (Cochran, Cohn, and Hastings 2023), next\nsentence prediction (Wu et al. 2023), prototypical neural\nnetworks (Zeng et al. 2023), cross-prompt fine-tuning (Fu-\nnayama et al. 2023), human-in-the-loop scoring via sam-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23182\npling responses (Singla et al. 2022), and reinforcement\nlearning (Liu et al. 2022). While these methods have enjoyed\nvarying degrees of success, a majority of these applications\nhave targeted more structured mathematics and computer\nscience tasks (i.e., tasks that can be solved formulaically),\nbut their grading is different from scoring free-form short-\nanswer responses by middle school students in science do-\nmains. Data impoverishment concerns are common to edu-\ncational data sets and a key consideration in applying these\napproaches to science assessments (Cochran et al. 2023).\nThe data needed for training our models is small, imbal-\nanced, and non-canonical in terms of syntax and semantics,\nall of which may impact model performance (Cohn 2020).\nThis research tackles several critical issues, namely: (1)\ngrading open-ended, short-answer questions focused on sci-\nence conceptual knowledge and reasoning, (2) utilizing\nLLMs to generate explanations aligned with specified learn-\ning objectives for both students and teachers and (3) ad-\ndressing concerns related to data impoverishment. We hy-\npothesize that our approach supports automated scoring and\nexplanation that (1) aligns with learning objectives and stan-\ndards, (2) provides actionable insight to students, especially\nin addressing their difficulties, and (3) engages teachers in\nthe scoring and explanation generation process to resolve\ndiscrepancies and support the learning goals.\nMethods\nThis section presents our curricular context, study design,\ndataset, LLM, and the details of our approach. Additional\ninformation regarding the formative assessment questions,\nrubrics, prompts, and method application can be found in the\nGitHub repository1 along with test code and sample data.\nCurricular context\nThis paper evaluates formative assessments conducted in the\ncontext of Science Projects Integrating Computing and En-\ngineering (SPICE), an NGSS-aligned middle school earth\nsciences water runoff curriculum. Spanning three weeks, the\ncurriculum tasks students with redesigning their schoolyard\nto enhance functionalities, using surface materials that min-\nimize water runoff post-storm within specified cost and ac-\ncessibility constraints (Chiu et al. 2019). We focus on forma-\ntive assessments that are primarily linked to the conceptual\nunderstanding of water runoff and the conservation of matter\nprinciple (Hutchins et al. 2021).\nStudy Design and Dataset\nThis study utilized assessment data from two Vanderbilt\nUniversity-approved SPICE studies involving 270 students\nat a Southeastern U.S. public middle school. Data was re-\nmoved for non-consenting participants and some data was\nmissing because of absences and incomplete submissions.\nWe used evidence-centered design (ECD) (Mislevy and\nHaertel 2006) to align the assessments with the learning ob-\njectives of the SPICE curriculum.\n1https://github.com/oele-isis-vanderbilt/EAAI24\nFigure 1: The fictitious student’s conceptual model used by\nstudents to answers the assessment questions.\nFor this paper, we selected three questions that required\nstudents to analyze a pictorial model of water runoff (illus-\ntrated in Figure 1) and apply their conceptual knowledge\nand scientific reasoning to evaluate and explain the correct\nand incorrect components of the model. Each question was\nscored for at least one conceptual knowledge item, i.e., a\ncorrect application of a scientific fact. For example, in Q3,\nstudents had to identify that the arrow size representing total\nabsorption was incorrect. Q2 and Q3 also required scoring\nstudents’ scientific reasoning, i.e., the use of scientific prin-\nciples to explain an answer. For Q3, students could invoke\nthe conservation principle to explain that the absorption ar-\nrow could not be larger than the rainfall arrow. The rubric as-\nsigned 1 point (conceptual) for Q1. Q2 and Q3 were scored\nfor 4 points (2 items, 1 conceptual and 1 reasoning point for\neach item). For Q3, there were exactly 2 errors in the model.\nFor Q2, students could choose from more than two correct\nphenomena, which resulted in differences in the grading re-\nsults that we discuss later.\nModel\nEver since OpenAI released ChatGPT2 (a chatbot driven by\nthe foundation model GPT-3.5) in November 2022, LLMs\nhave received a tremendous amount of attention. Their abil-\nity to compose paper outlines, expository essays, and screen-\nplays, has made the use of ChatGPT ubiquitous across\nacademia, business, and news media. In March 2023, Ope-\nnAI released GPT-43 (OpenAI 2023), which is largely con-\nsidered the current state-of-the-art for LLMs (OpenAI 2023;\nZhao et al. 2023). For this reason, we chose to use GPT-4 as\nthe LLM to develop and evaluate our approach.\nApproach\nBrown et al. (2020) demonstrated that LLMs could “learn”\nfrom a few labeled instances in the prompt via in-context\nlearning (ICL). Unlike fine-tuning, which requires ex-\npensive parameter updates and may result in decreased\n2https://openai.com/blog/chatgpt\n3https://openai.com/research/gpt-4\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23183\nperformance for previously known tasks (Mosbach, An-\ndriushchenko, and Klakow 2020), ICL uses the labeled in-\nstances in the prompt to generate text during inference that\nbypasses traditional training. This means that by simply\nchanging the prompts, the same language model can be used\nacross domains, tasks, and datasets without the need to mod-\nify the network’s parameters. Wei et al. (2022) extended this\nwork by providing chain-of-thought (CoT) reasoning in the\nlabeled instances. In contrast to a traditional ICL instance\nthat only offers a question and its corresponding answer,\nCoT provides a reasoning chain with the answer. This helps\nthe model generate correct inferences, and this reasoning is\nincluded in the model’s response along with the answer.\nEliciting reasoning is particularly useful for formative as-\nsessment scoring in science, where the open-ended nature\nof the questions can make scoring alignment difficult even\nbetween humans. Rather than generating a score only, CoT\nprompting elicits an explanation for the LLM’s response, en-\nabling teachers to offer informed feedback to students. Al-\nternatively, teachers can refine the rubric to improve grading\nfor subsequent assessments. The model’s reasoning can also\nbe used to identify specific causes of misalignment between\nthe model and the teacher, which can then be leveraged to\nimprove model output.\nActive learning (Tan et al. 2023; Ren et al. 2021) takes\na human-in-the-loop approach to improving model training,\nwhere the human as an “oracle” is consulted to label addi-\ntional instances for inclusion in the next training iteration.\nBy integrating CoT reasoning and active learning, educators\nor researchers can scrutinize instances with incorrect pre-\ndictions to identify recurring patterns leading to the model’s\nerrors across multiple instances. These patterns can be rein-\ntroduced into the prompt using CoT reasoning to rectify dis-\ncrepancies between the model’s assessment and the human\nscorer. Moreover, combining CoT with active learning as-\nsists teachers and researchers in rectifying human errors in\nthe initial scoring. This is particularly relevant when the hu-\nmans confirm that the model’s scoring predictions are accu-\nrate.\nWe employ the inter-rater reliability (IRR) process to pin-\npoint scoring disagreements that may challenge the model,\naddressing them through CoT prompting. Active learning\nis then utilized to identify recurrent issues in the model’s\nalignment with the human scorers, and instances embody-\ning these patterns are incorporated into the prompt with rea-\nsoning chains to correct the alignment. Once active learning\nconcludes, the model is deployed for scoring new forma-\ntive assessment responses through inference, accompanied\nby CoT reasoning to generate student feedback, and when\nneeded, refining rubrics and formative assessment questions.\nFigure 2 provides a comprehensive overview of our ap-\nproach.\nResponse Scoring. Two of this paper’s authors indepen-\ndently scored a randomly chosen 20% of the student re-\nsponses for each of the three formative assessment ques-\ntions using the rubric. Next, while conducting IRR, instances\nwhere the humans both agreed and disagreed on students’\nscores were collected and included in the initial prompt.\nParticular attention was paid to the misalignments between\nthe graders that caused multiple instances to be scored dif-\nferently before consensus was reached. To achieve consen-\nsus, the two reviewers discussed each scoring disagreement\nuntil they reached a consensus on how that particular in-\nstance should be scored. The agreed-upon instances acted as\n“ground truth” exemplars for the model to initially align it-\nself with the human scorers. The instances where there were\ndisagreements were used to pinpoint specific reasons for\nmisalignment between the human scorers during IRR. We\nexpected that the model might encounter the same misalign-\nments during its scoring. This process was repeated for each\nof the three questions until Cohen’s k > 0.7 was achieved\nacross all subscores for each question, after which one of\nthis paper’s authors scored the full set of student responses.\nFor this work, all students’ responses were manually graded\nto ensure accuracy while evaluating our method. Disagree-\nments were resolved manually by the humans to form a con-\nsensus (described above). This consensus was used to align\nthe LLM responses via CoT reasoning. In future work, as\nwe collect more data, we will use the LLM to automatically\nscore students’ responses and evaluate samples of the LLM’s\ngenerations to ensure accuracy. Furthermore, we refrained\nfrom updating the rubric during Active Learning; however,\nwe intend to investigate this aspect in future research.\nBefore developing the initial prompt, we partitioned the\ndataset into training (80%) and testing (20%) instances for\nthe three sets of formative assessment responses. The train-\ning set played a dual role in prompt development. Initially,\nfew-shot examples were selected to construct the prompt,\nwhile the instances not utilized for few-shot learning served\nas a validation set for refining the prompt during active\nlearning. Due to token limitations and the time cost for in-\nstance labeling, only a limited number of labeled instances\nwere included in the prompt. As discussed in later sections,\nan excessive number of instances in the prompt can lead\nto overfitting. Furthermore, it is important that the valida-\ntion set during active learning is sufficiently large to ensure\naccurate identification of scoring trends. In this paper, the\nvalidation-to-training set ratio was ≈43:1.\nPrompt Development. For prompting, we opted for the\npersona pattern (White et al. 2023), where the model was in-\nstructed to adopt the persona of a middle school teacher eval-\nuating students’ formative assessment question responses.\nThe prompt also provided the model with the formative as-\nsessment question and rubric, and the model was instructed\nto use the rubric to score students’ responses. The rubric also\nprovided the model with the format to output its responses\nto improve readability and allow for programmatic parsing\nof the model’s generations.\nNext, we incorporated ground truth examples into the\nprompt, complemented by CoT reasoning clarifying the rea-\nsons for awarding or not awarding points for each subscore.\nFollowing this, a comparable CoT input was included for in-\nstances where human scorers diverged in their assessments.\nThis aimed at aligning the model with the IRR consen-\nsus, particularly when instances posed challenges similar\nto those faced by human reviewers in achieving consensus.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23184\nFigure 2: Our Chain-of-Thought Prompting + Active Learning approach. The green box encapsulates this process, where each\nof the blue diamonds is a step in that process. Yellow boxes represent the process’s application to the classroom.\nFor all labeled instances in the prompt, we used the follow-\ning CoT reasoning template: evidence in the student’s re-\nsponse + reference to the rubric + score. We used quotations\nfrom the student’s response as evidence, tying it back to the\nrubric, and providing a score and explanation to the model;\ne.g., “The student says X. The rubric states Y . Based on the\nrubric, the student earned a score of Z.” This approach mir-\nrored the original CoT publication (Wei et al. 2022), where\nalgebraic word problems were broken down step-by-step to\nhelp the model arrive at the correct solution.\nAdditional labeled instances were added to the prompt as\nneeded to balance the individual subscores. However, this\nwas constrained by the small and imbalanced nature of our\ndataset. While investigating the effect that data balance has\non the LLM’s performance is outside the scope of this work,\nin previous work (using a subset of the dataset used in this\npaper), we demonstrated that data balancing often improved\nlanguage model performance (Cochran et al. 2022). For Q2\nand Q3, balancing across 4 subscores was difficult, as adding\none more instance to augment one subscore inherently af-\nfected the balance across the other subscores. Sometimes,\nachieving a perfect balance was not possible in the training\nset, but we included at least one positive and one negative\ninstance across all subscores for each question’s prompt.\nActive Learning. Validation set instances were fed\nthrough the model with the initial prompt and few-shot ex-\namples, and a researcher performed error analysis to discern\npatterns in the incorrect LLM generations. Specifically, we\nnoted the reason for each incorrect scoring prediction and\nthe faulty reasoning chains that caused the model to misla-\nbel several instances. These reasoning chains were chosen\nas additional examples to add to the prompt, and CoT was\nused to correct the model’s reasoning errors. Candidate in-\nstances were prioritized for prompt inclusion based on the\ndegree to which their reasoning errors caused other inaccu-\nrate model predictions, which resulted in correcting several\nwrongly scored instances.\nThere were only a few incorrectly predicted scores in the\nvalidation set for Q1, so all of those instances were added\nto the prompt during Active Learning. For Q2 and Q3, the\nresearcher identified the n most useful instances, where n\nwas defined as the minimum number of instances in the val-\nidation set that simultaneously addressed all of the LLM’s\nreasoning errors and maintained data balance. This caused\nsome overfitting, so we will experiment with 1-shot active\nlearning to help mitigate this in future work. For all instances\nadded to the prompt during Active Learning, we used CoT\nto correct the model’s faulty reasoning chains. We also re-\nbalanced the few-shot instances across subscores during Ac-\ntive Learning to maintain data balance. In previous work, we\nshowed that balancing training data to create a uniform label\ndistribution can improve performance (Cochran et al. 2022).\nOther works have suggested balancing to achieve the true\ndistribution of the dataset’s labels (Min et al. 2022).\nIn general, active learning can be performed until one\nof several stopping conditions is triggered: (1) the model\nachieves convergence, i.e., it no longer produces any incor-\nrect validation scores; (2) the model predicts more validation\nscores incorrectly than in previous iterations, i.e., it overfits;\nand (3) there are not enough instances remaining in the val-\nidation set to achieve acceptable data balance in the prompt.\nTo test our method, we performed one iteration of active\nlearning for each of the three formative assessment ques-\ntions. For each subscore of a formative assessment question,\nwe first identified scoring error trends, i.e., are model scor-\ning errors mainly caused by false negatives (underscoring)\nor false positives (overscoring)? This alerted us to the “direc-\ntion” in which we needed to guide the model to better align\nwith the human scorers. We then examined the content of\nthe incorrect validation set generations to identify common\ncauses of incorrect scoring. We chose the most frequently\noccurring model reasoning error (i.e., the error that caused\nthe model to wrongly predict the greatest number of vali-\ndation set instances), and picked one of these instances to\ninsert back into the prompt.\nFor example, with theRunoff Arrow Directionsubscore in\nQ3, we found that the ratio of the model’s false positive to\nfalse negative predictions was 5:2. Additionally, we found\nthat the cause of more than half of the false positives was\ndue to the model awarding students a point for mentioning\nthat the arrows in the diagram needed to change direction.\nThis was incorrect because only the runoff arrow needed to\nchange direction. To correct the model’s reasoning error, we\nchose one of the incorrect validation instances that included\nthis reasoning error, inserted it into the prompt, and used\nCoT reasoning to help correct the model’s reasoning error\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23185\nQ1 Arrow Size n Acc F1 QWK\nZero-Shot 0 0.87 0.84 0.68\nFew-Shot 4 1.00 1.00 1.00\nFew-Shot, CoT 4 0.96 0.95 0.89\nCoT + AL 12 0.98 0.97 0.95\nTable 1: Performance comparisons for the Q1 Arrow Size\nsubscore. For all questions, the best-performing scoring im-\nplementation is in bold for each metric, for each subscore\n(and total score). n refers to the number of few-shot in-\nstances used in the prompt.\ngoing forward.\nResults\nWe evaluated our method by comparing our model perfor-\nmance to the held-out test set across 4 implementations:\nthree incremental baselines, and our Chain-of-Thought\nPrompting + Active Learning approach. We started with\na Zero-Shot baseline, where the rubric is included in the\nprompt, but no labeled examples were present. We then used\na Few-Shot baseline, where we provided the model with la-\nbeled instances in the prompt, but the labeled instances only\nconsisted of numerical scores (i.e., no CoT reasoning). Our\nthird and final baseline, Few-Shot, CoT, added CoT rea-\nsoning to the few-shot instances. Last, we employed our\nChain-of-Thought Prompting + Active Learning approach\nand compared it to the three baselines. Evaluating our ap-\nproach across these incremental baselines allowed us to ex-\namine the effects of adding specific parts of the pipeline\nand to understand the degree to which each component con-\ntributed to the model’s ability to score and explain formative\nassessment responses.\nTo compare implementations, we chose the Macro F1-\nScore and Cohen’s Quadratic Weighted Kappa (QWK) (Co-\nhen 1968) metrics. The F1-Score is prevalent in the literature\nfor evaluating overall model performance. Macro F1 was\nchosen, specifically, due to our dataset’s imbalance across\nsubscores. Often, scientific reasoning subscores are heavily\nweighted towards the negative class (i.e., a large majority\nof the students do not demonstrate scientific reasoning). Co-\nhen’s QWK was chosen because it is widely used in the au-\ntomated essay scoring (AES) literature (Singh et al. 2023;\nSingla et al. 2022). Unlike traditional Cohen’s k (Cohen\n1960), Cohen’s QWK accounts for the degree of disagree-\nment, making it well-suited for ordinal data. We included\naccuracy for reference, but we do not use it in our actual\nperformance comparisons.\nModel performance comparisons for each of the three for-\nmative assessment questions are shown in Tables 1, 2, and 3.\nQuestion 1: Q1 asked students what the different-sized\narrows in the diagram meant. A student received a point for\ncorrectly identifying that the diagram used the size of the\narrows to represent the quantity of water (concept: “Arrow\nSize”, see Table 1).\nQ1 took 2 rounds of IRR for the human scorers to reach\na consensus. The grading involved scoring for one possible\nQ2 Arrow Direction n Acc F1 QWK\nZero-Shot 0 0.91 0.89 0.78\nFew-Shot 5 0.87 0.79 0.60\nFew-Shot, CoT 5 0.98 0.98 0.95\nCoT + AL 10 0.98 0.98 0.95\nQ2 Arr. Dir., Reasoning n Acc F1 QWK\nZero-Shot 0 0.92 0.73 0.47\nFew-Shot 5 0.89 0.67 0.36\nFew-Shot, CoT 5 0.91 0.70 0.41\nCoT + AL 10 0.92 0.65 0.3\nQ2 Arrow Size n Acc F1 QWK\nZero-Shot 0 0.77 0.69 0.39\nFew-Shot 5 0.77 0.69 0.39\nFew-Shot, CoT 5 0.91 0.88 0.77\nCoT + AL 10 0.94 0.92 0.83\nQ2 Arr. Sz., Reasoning n Acc F1 QWK\nZero-Shot 0 0.96 0.82 0.65\nFew-Shot 5 0.98 0.90 0.79\nFew-Shot, CoT 5 0.94 0.77 0.55\nCoT + AL 10 0.96 0.82 0.65\nQ2 Total Score n Acc F1 QWK\nZero-Shot 0 0.60 0.59 0.65\nFew-Shot 5 0.53 0.52 0.55\nFew-Shot, CoT 5 0.75 0.80 0.80\nCoT + AL 10 0.85 0.79 0.87\nTable 2: Performance comparisons for Question 2.\npoint and no science reasoning subscores. GPT-4 aligned\nwith the human scorer to a “moderate” degree (QWK >=\n0.6) (McHugh 2012) even in a zero-shot setting. Once\nlabeled instances were added, the model achieved a perfect\nscore on the test set. When CoT reasoning was provided,\nperformance decreased for both Macro F1 and QWK, as the\nreasoning chains initially caused the model to deviate from\nthe human scorer. Once active learning was performed,\nhowever, much of that performance gap was closed due to\nthe additional few-shot instances and model reasoning error\ncorrections.\nQuestion 2: Q2 asked students to identify two things\nthat the diagram did well for 4 possible points: 2 for science\nconcepts, and 2 for science reasoning. For the science\nconcepts subscores, the student received a point for Arrow\nDirection if he or she correctly identified that the diagram\ndid a good job of showing that water originated from the\nsky in the form of rain, some water was absorbed, or some\nresulted in runoff. For Arrow Size, students received a\npoint if they discussed that the diagram did a good job of\nusing arrow size to represent the water amount. Each of the\nArrow Direction and Arrow Size subscores also included an\nadditional possible point if students demonstrated scientific\nreasoning in their responses (see Table 2).\nQ2 science concepts subscores (i.e., Arrow Direction and\nArrow Size) saw their best performance (or tied for best\nperformance) using the full Chain-of-Thought Prompting +\nActive Learning method. The science reasoning subscores’\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23186\nperformances decreased as additional components of the\nmethod were added, i.e., after CoT and active learning were\nintroduced. Overall, the total score was best when the com-\nplete method was used, as this resulted in the highest QWK\nvalue.\nQ2 was the most difficult for the human scorers to agree\non and it required three separate IRR rounds to achieve con-\nsensus. Some of the difficulty in scoring may be attributed\nto the open-ended nature of the question. There are multiple\nideas in the conceptual model that are correct, but students\nwere only asked to identify two things the diagram explained\nwell. Many students responded vaguely, and several students\nprovided both correct and incorrect statements in the same\nresponse. These types of ambiguous instances were difficult\nfor the human scorers to agree on even when they awarded\npoints based on the rubric. It seems the LLM encountered\nthe same types of issues.\nConsider a student whose Q2 response was, “[the arrow\nrepresents] the amount of absorption”. Arguably, the student\nunderstood that the model’s arrows corresponded to the\nquantity of water. However, the absorption arrow in the\ndiagram was incorrect (it was larger than the rainfall arrow,\nso it violated the law of conservation of matter). Because\nthe question asked for examples of things the diagram does\na good job of, and the absorption arrow was incorrect,\nboth reviewers felt responses like this one should not\nreceive a point for Arrow Size even though the student\nmay understand that arrow size corresponds to the amount\nof water. During our active learning validation run, the\nmodel incorrectly awarded several points to these types of\nresponses. When we attempted to use CoT to correct the\nmodel’s reasoning error, the model began to mislabel other\ninstances it had previously scored correctly, i.e., there was\noverfitting. Ultimately, the researchers agreed that both the\nQ2 question wording and the Q2 rubric need to be rewritten\nto provide clearer guidance to the students.\nQuestion 3: Q3 asked students to list two erroneous\nthings they would change in the conceptual model dia-\ngram. Like Q2, 4 total points were assigned to Q3: 2 for\nscience concepts and 2 for scientific reasoning. The science\nconcepts subscores were: (1) Runoff Direction, where the\nstudent received a point if he or she indicated that the\nrunoff arrow was pointing the wrong direction (uphill) and\n(2) Arrow Size, where a point was awarded if the student\nmentioned that the arrow sizes needed to change and adhere\nto the law of conservation of matter. Similar to Q2, students\ngot additional points if they demonstrated correct scientific\nreasoning in their responses (see Table 3).\nAll Q3 subscores (science concepts and scientific rea-\nsoning) improved performance across both metrics (except\nMacro F1 for total score) after we added the few-shot ex-\namples. When CoT was added, performance increased for\nboth Runoff Direction subscores but decreased substantially\nfor both Arrow Size subscores. We saw similar behavior\nin the Q1 Arrow Size subscore, where adding CoT rea-\nsoning caused the model to become misaligned with the\nhuman. Once the Active Learning component was added,\nhowever, all subscores except Runoff Direction achieved\nQ3 Runoff Direction n Acc F1 QWK\nZero-Shot 0 0.89 0.88 0.77\nFew-Shot 5 0.91 0.90 0.80\nFew-Shot, CoT 5 0.92 0.92 0.84\nCoT + AL 9 0.89 0.88 0.75\nQ3 Run. Dir., Reasoning n Acc F1 QWK\nZero-Shot 0 0.94 0.89 0.79\nFew-Shot 5 0.94 0.91 0.82\nFew-Shot, CoT 5 0.94 0.92 0.83\nCoT + AL 9 0.98 0.97 0.94\nQ3 Arrow Size n Acc F1 QWK\nZero-Shot 0 0.87 0.83 0.67\nFew-Shot 5 0.89 0.87 0.73\nFew-Shot, CoT 5 0.85 0.83 0.65\nCoT + AL 9 0.92 0.92 0.83\nQ3 Arr. Sz., Reasoning n Acc F1 QWK\nZero-Shot 0 0.98 0.90 0.79\nFew-Shot 5 1.00 1.00 1.00\nFew-Shot, CoT 5 0.94 0.82 0.64\nCoT + AL 9 1.00 1.00 1.00\nQ3 Total Score n Acc F1 QWK\nZero-Shot 0 0.74 0.80 0.85\nFew-Shot 5 0.75 0.73 0.87\nFew-Shot, CoT 5 0.75 0.71 0.79\nCoT + AL 9 0.81 0.80 0.90\nTable 3: Performance comparisons for Question 3.\ntheir best performance across both metrics.Runoff Direction\nachieved its best performance when CoT was added, but was\noverfit during active learning. Unlike Q2, where the best-\nperforming subscores were the science concepts, both sci-\nence reasoning subscores did better than their science con-\ncepts counterparts for both metrics.\nFor Q3, the human scorers achieved consensus quickly\nafter 1 round of IRR, and only one issue caused multiple\nscoring disagreements. The model’s reasoning errors for\nthe scientific reasoning subscores were easily addressed via\nCoT (relative to Q2). A major model reasoning error for\nQ3 was that it tended to cite the same piece of evidence\nto justify awarding points for different subscores (i.e.,\nit overscored). This was a disagreement with the human\nscorers, but we did not include it in the initial prompt or\nfew-shot CoT reasoning chains. Once this model reasoning\nerror was addressed during active learning, the issue was\nlargely mitigated, and performance improved across the\nboard.\nSummary: Across all questions, the model’s scoring\nmostly aligned with the human scorers. Of the 11 subscores\nand total scores, 9 of them saw “strong” agreement or better\n(QWK >= 0.8) at some point in the process (i.e., across the\n4 implementations: 3 baselines and our Chain-of-Thought\nPrompting + Active Learning approach). 4 subscores\nachieved “almost perfect” (QWK > 0.9) agreement. All\nsubscores except one (Q2 Arrow Direction Reasoning) saw\na Macro F1 of 0.90 or greater at some point in the process.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23187\nImportantly, we also demonstrated that both CoT reasoning\nand active learning run the risk of overfitting, particularly\nwhen applied to the less complex science concepts questions\n(e.g., Q1 Arrow Size and Q3 Runoff Direction) and the more\nambiguous scientific reasoning questions (e.g., Q2 Arrow\nDirection Reasoning and Q2 Arrow Size Reasoning). It\nshould also be noted that the level of agreement during IRR\nmay provide a ballpark expectation of model performance,\nas we found questions that were easier for the human scorers\nto agree on were also easier for the model to correctly align\nwith the human scorers. Similarly, in questions where\nthe human scorers had difficulty achieving consensus, the\nmodel had difficulty with scoring. More research needs to\nbe done to evaluate this quantitatively.\nComparing Model and Human Performance\nWe applied inductive coding (Charmaz 2006) to evaluate\nperformance and identify future directions to improve our\nhuman-in-the-loop approach. First, the lead author (not in-\nvolved in rubric creation and scoring) reviewed all instances\nin which the model and the human coder disagreed and iden-\ntified agreement with the model in 3 out of the 22 disagree-\nments (1 conceptual disagreement, 2 reasoning disagree-\nments). The research team reviewed the results to evaluate\nwhat may have caused scoring errors and to identify poten-\ntial future directions for improvement. During the review\nprocess, the team created memos of key findings (Hatch\n2002). The team compared the memos and came up with\nthree key themes for improvement in future work:\n1. Need for Additional Mechanisms to Target Model\nDeficiencies: Differences in scoring identified that the\nmodel showed a tendency to overfit in some cases. For\ninstance, if the CoT got too granular, the model demon-\nstrated issues that were related to keywords such as “be-\ncause” (e.g., the model identified it as a demonstration of\nreasoning), “arrow size” (e.g., the model assumed that\nuse of the terminology indicated a correct application\neven if correct attributions were not made to the scien-\ntific process), and vocabulary definitions (e.g., the model\ndid not realize “run off” and “runoff” were identical). In a\nsmall set of cases, the model cited a student’s faulty logic\nto justify awarding a point for a response and reused the\nsame piece of evidence to award points for both concept\nidentification and reasoning;\n2. Ability to Leverage the Model to Support Rubric Re-\nfinements: Comparing reviewer and model differences\nfor Q2 helped identify limitations in the original rubric\nfor such an open-ended question. Utilizing the results and\nthe explanations provided by the model, this human-in-\nthe-loop approach can benefit teachers and researchers\nin refining the rubrics and scoring mechanisms to better\nsupport instruction and student learning; and\n3. Resolve Unexplained Model Applications: In some\ncases, the model did not follow CoT reasoning and did\nnot provide evidence of its positive predictions even\nthough all positive prompt instances provided this evi-\ndence. This may be a potential limitation in the approach\nto providing feedback for positive performances.\nOverall, our approach was successful, but the instances\ndiscussed above provide opportunities for future work to\nimprove model output, rubric development, and sometimes\neven reworking questions to make them clearer.\nConclusion and Future Implications\nIn this paper, we employed a Chain-of-Thought Prompting +\nActive Learning approach for scoring and explaining forma-\ntive assessment question responses in a middle school Earth\nScience curriculum. Our results show that GPT-4, CoT rea-\nsoning, and active learning can be effectively leveraged to-\nward accurate grading of science formative assessments. In\nseveral cases, the model achieved “almost perfect” align-\nment with humans. The model generated relevant evidence\nlinked to the rubric to help explain its scoring, which could\nbenefit students and teachers. We also analyzed the model’s\nweaknesses and identified several areas for improving LLM-\nbased assessments.\nLimitations: With LLM approaches, ethical concerns arise\nwith regard to privacy, bias, and hallucinations (Zhuo et al.\n2023), and these concerns are amplified when they are de-\nployed in high-stakes environments (e.g., classrooms with\nchildren). In addition, while CoT has been shown to improve\nmodel performance over traditional ICL, the degree to which\nthe reasoning chains guide the model’s decision-making (if\nat all) is still an open question (Turpin et al. 2023). Our\nresults also show that CoT and active learning can lead to\noverfitting, in particular, with simpler, easier-to-define sub-\nproblems. In these cases, LLM approaches may be overkill,\nas Moore et al. (2023) recently demonstrated. Rule-based\nmethods outperformed GPT-4 in detecting common item-\nwriting flaws in student-generated multiple-choice ques-\ntions.\nLooking to the Future: Anecdotally, in an interview with\nmiddle school science teachers who implemented the cur-\nriculum, the teachers identified the potential benefits of these\nexplanations as tools to inform students on where to go\nnext in their learning, as opposed to assigning performance\nscores. We aim to extend this partnership with classroom\nteachers to mold the LLM’s output to best fit their needs,\nand investigate how we can best use our method to evaluate\nstudents’ learning performance and improve students’ learn-\ning. As we continue to refine our approach, we hope these\nenhancements will pave the way for more effective and effi-\ncient LLM applications in science education.\nAcknowledgments\nThis work is supported by the National Science Foundation\nunder awards DRL-2112635 and IIS-2017000. Any opin-\nions, findings, conclusions, and recommendations in this pa-\nper are those of the authors and do not necessarily reflect the\nviews of the National Science Foundation.\nReferences\nAdair, A.; Pedro, M. S.; Gobert, J.; and Segan, E. 2023.\nReal-Time AI-Driven Assessment and Scaffolding that Im-\nproves Students’ Mathematical Modeling during Science In-\nvestigations. In Wang, N.; Rebolledo-Mendez, G.; Mat-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23188\nsuda, N.; Santos, O. C.; and Dimitrova, V ., eds., Artificial\nIntelligence in Education, 202–216. Cham: Springer Nature\nSwitzerland. ISBN 978-3-031-36272-9.\nBloom, B.; Madaus, G.; and Hastings, J. 1971.Handbook on\nFormative and Summative Evaluation of Student Learning .\nNew York: McGraw-Hill.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. arXiv e-prints, arXiv:2005.14165.\nCharmaz, K. 2006. Constructing grounded theory: A prac-\ntical guide through qualitative analysis. Sage.\nChiu, J.; McElhaney, K.; Zhang, N.; Biswas, G.; Fried, R.;\nBasu, S.; Alozie, N.; and Hong, J. 2019. A Principled Ap-\nproach to NGSS-aligned Curriculum Development Integrat-\ning Science, Engineering, and Computation: A Pilot Study.\nIn NARST Annual International Conference. NARST.\nCizek, G. J.; and Lim, S. N. 2023. Formative assessment: an\noverview of history, theory and application. In Tierney, R. J.;\nRizvi, F.; and Ercikan, K., eds., International Encyclopedia\nof Education (Fourth Edition), 1–9. Oxford: Elsevier, fourth\nedition edition. ISBN 978-0-12-818629-9.\nCochran, K.; Cohn, C.; and Hastings, P. 2023. Improving\nNLP model performance on small educational data sets us-\ning self-augmentation. In Proceedings of the 15th Interna-\ntional Conference on Computer Supported Education (2023,\nto appear).\nCochran, K.; Cohn, C.; Hutchins, N.; Biswas, G.; and Hast-\nings, P. 2022. Improving automated evaluation of formative\nassessments with text data augmentation. In International\nConference on Artificial Intelligence in Education, 390–401.\nSpringer.\nCochran, K.; Cohn, C.; Rouet, J. F.; and Hastings, P.\n2023. Improving Automated Evaluation of Student Text Re-\nsponses Using GPT-3.5 for Text Data Augmentation. In In-\nternational Conference on Artificial Intelligence in Educa-\ntion, 217–228. Springer.\nCohen, J. 1960. A coefficient of agreement for nominal\nscales. Educational and psychological measurement, 20(1):\n37–46.\nCohen, J. 1968. Weighted kappa: nominal scale agreement\nprovision for scaled disagreement or partial credit. Psycho-\nlogical bulletin, 70(4): 213.\nCohn, C. 2020. BERT efficacy on scientific and medical\ndatasets: a systematic literature review. DePaul University.\nFunayama, H.; Asazuma, Y .; Matsubayashi, Y .; Mizumoto,\nT.; and Inui, K. 2023. Reducing the Cost: Cross-Prompt\nPre-finetuning for Short Answer Scoring. In International\nConference on Artificial Intelligence in Education, 78–89.\nSpringer.\nHarris, C.; Wiebe, E.; Grover, S.; and Pellegrino, J. 2023.\nClassroom-based STEM assessment: Contemporary issues\nand perspectives. Community for Advancing Discovery\nResearch in Education (CADRE). Education Development\nCenter, Inc.\nHatch, J. A. 2002. Doing qualitative research in education\nsettings. SUNY Press.\nHaudek, K. C.; Kaplan, J. J.; Knight, J.; Long, T.; Merrill,\nJ.; Munn, A.; Nehm, R.; Smith, M.; and Urban-Lurain, M.\n2011. Harnessing technology to improve formative assess-\nment of student conceptions in STEM: forging a national\nnetwork. CBE—Life Sciences Education, 10(2): 149–155.\nHutchins, N.; and Biswas, G. 2023. Using Teacher Dash-\nboards to Customize Lesson Plans for a Problem-Based,\nMiddle School STEM Curriculum. In LAK23: 13th In-\nternational Learning Analytics and Knowledge Conference,\nLAK2023, 324–332. New York, NY , USA: Association for\nComputing Machinery. ISBN 9781450398657.\nHutchins, N. M.; Basu, S.; McElhaney, K.; Chiu, J.; Fick,\nS.; Zhang, N.; and Biswas, G. 2021. Coherence across con-\nceptual and computational representations of students’ sci-\nentific models. In The International Society of the Learning\nSciences Annual Meeting 2021. International Society of the\nLearning Sciences (ISLS).\nLiu, E.; Stephan, M.; Nie, A.; Piech, C.; Brunskill, E.; and\nFinn, C. 2022. Giving Feedback on Interactive Student Pro-\ngrams with Meta-Exploration. Advances in Neural Informa-\ntion Processing Systems, 35: 36282–36294.\nLiu, O. L.; Rios, J. A.; Heilman, M.; Gerard, L.; and Linn,\nM. C. 2016. Validation of automated scoring of science as-\nsessments. Journal of Research in Science Teaching, 53(2):\n215–233.\nMao, L.; Liu, O. L.; Roohr, K.; Belur, V .; Mulholland, M.;\nLee, H.-S.; and Pallant, A. 2018. Validation of automated\nscoring for a formative assessment that employs scientific\nargumentation. Educational Assessment, 23(2): 121–138.\nMcHugh, M. L. 2012. Interrater reliability: the kappa statis-\ntic. Biochemia medica, 22(3): 276–282.\nMin, S.; Lyu, X.; Holtzman, A.; Artetxe, M.; Lewis, M.; Ha-\njishirzi, H.; and Zettlemoyer, L. 2022. Rethinking the role\nof demonstrations: What makes in-context learning work?\narXiv preprint arXiv:2202.12837.\nMislevy, R. J.; and Haertel, G. D. 2006. Implications of\nEvidence-Centered Design for Educational Testing. Educa-\ntional Measurement: Issues and Practice, 25(4): 6–20.\nMoore, S.; Nguyen, H. A.; Chen, T.; and Stamper, J. 2023.\nAssessing the Quality of Multiple-Choice Questions Using\nGPT-4 and Rule-Based Methods. In European Conference\non Technology Enhanced Learning, 229–245. Springer.\nMorris, W.; Crossley, S.; Holmes, L.; Ou, C.; McNamara,\nD.; and Dascalu, M. 2023. Using Large Language Models\nto Provide Formative Feedback in Intelligent Textbooks. In\nInternational Conference on Artificial Intelligence in Edu-\ncation, 484–489. Springer.\nMosbach, M.; Andriushchenko, M.; and Klakow, D.\n2020. On the stability of fine-tuning bert: Misconcep-\ntions, explanations, and strong baselines. arXiv preprint\narXiv:2006.04884.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23189\nNGSS. 2013. Next Generation Science Standards: For\nStates, By States. The National Academies Press.\nOpenAI. 2023. GPT-4 Technical Report. arXiv e-prints,\narXiv:2303.08774.\nRen, P.; Xiao, Y .; Chang, X.; Huang, P.-Y .; Li, Z.; Gupta,\nB. B.; Chen, X.; and Wang, X. 2021. A survey of deep active\nlearning. ACM computing surveys (CSUR), 54(9): 1–40.\nRodrigues, F.; and Oliveira, P. 2014. A system for formative\nassessment and monitoring of students’ progress. Comput-\ners & Education, 76: 30–41.\nSingh, S.; Pupneja, A.; Mital, S.; Shah, C.; Bawkar, M.;\nGupta, L. P.; Kumar, A.; Kumar, Y .; Gupta, R.; and Shah,\nR. R. 2023. H-AES: Towards Automated Essay Scoring for\nHindi. arXiv preprint arXiv:2302.14635.\nSingla, Y . K.; Krishna, S.; Shah, R. R.; and Chen, C. 2022.\nUsing sampling to estimate and improve performance of au-\ntomated scoring systems with guarantees. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 36\n(11), 12835–12843.\nTan, W.; Lin, J.; Lang, D.; Chen, G.; Ga ˇsevi´c, D.; Du, L.;\nand Buntine, W. 2023. Does informativeness matter? Active\nlearning for educational dialogue act classification. In Inter-\nnational Conference on Artificial Intelligence in Education,\n176–188. Springer.\nTurpin, M.; Michael, J.; Perez, E.; and Bowman, S. R. 2023.\nLanguage Models Don’t Always Say What They Think: Un-\nfaithful Explanations in Chain-of-Thought Prompting.arXiv\npreprint arXiv:2305.04388.\nWalkoe, J.; Wilkerson, M.; and Elby, A. 2017. Technology-\nMediated Teacher Noticing: A Goal for Classroom Practice,\nTool Design, and Professional Development. In Proceed-\nings of the 12th International Conference on Computer Sup-\nported Collaborative Learning (CSCL) 2017. International\nSociety of the Learning Sciences.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain-of-\nThought Prompting Elicits Reasoning in Large Language\nModels. arXiv e-prints, arXiv:2201.11903.\nWhite, J.; Fu, Q.; Hays, S.; Sandborn, M.; Olea, C.; Gilbert,\nH.; Elnashar, A.; Spencer-Smith, J.; and Schmidt, D. C.\n2023. A prompt pattern catalog to enhance prompt engi-\nneering with chatgpt. arXiv preprint arXiv:2302.11382.\nWiley, K. J.; Dimitriadis, Y .; Bradford, A.; and Linn, M. C.\n2020. From Theory to Action: Developing and Evaluating\nLearning Analytics for Learning Design. In Proceedings of\nthe Tenth International Conference on Learning Analytics &\nKnowledge, LAK ’20, 569–578. New York, NY , USA: As-\nsociation for Computing Machinery. ISBN 9781450377126.\nWilson, J.; Ahrendt, C.; Fudge, E. A.; Raiche, A.; Beard, G.;\nand MacArthur, C. 2021. Elementary teachers’ perceptions\nof automated feedback and automated scoring: Transform-\ning the teaching and learning of writing using automated\nwriting evaluation. Computers & Education, 168: 104208.\nWu, X.; He, X.; Liu, T.; Liu, N.; and Zhai, X. 2023. Match-\ning exemplar as next sentence prediction (mensp): Zero-shot\nprompt learning for automatic scoring in science education.\nIn International Conference on Artificial Intelligence in Ed-\nucation, 401–413. Springer.\nZeng, Z.; Li, L.; Guan, Q.; Gaˇsevi´c, D.; and Chen, G. 2023.\nGeneralizable Automatic Short Answer Scoring via Proto-\ntypical Neural Network. In International Conference on Ar-\ntificial Intelligence in Education, 438–449. Springer.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.;\nHou, Y .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al.\n2023. A survey of large language models. arXiv preprint\narXiv:2303.18223.\nZhuo, T. Y .; Huang, Y .; Chen, C.; and Xing, Z. 2023. Red\nteaming chatgpt via jailbreaking: Bias, robustness, reliabil-\nity and toxicity. arXiv preprint arXiv:2301.12867, 12–2.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n23190",
  "topic": "Formative assessment",
  "concepts": [
    {
      "name": "Formative assessment",
      "score": 0.9175960421562195
    },
    {
      "name": "Psychology",
      "score": 0.5584718585014343
    },
    {
      "name": "Mathematics education",
      "score": 0.4845349192619324
    },
    {
      "name": "Chain (unit)",
      "score": 0.4346494674682617
    },
    {
      "name": "Pedagogy",
      "score": 0.3517710864543915
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 25
}