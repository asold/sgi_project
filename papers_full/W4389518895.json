{
  "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
  "url": "https://openalex.org/W4389518895",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2232011877",
      "name": "Yile Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099977667",
    "https://openalex.org/W4256561644",
    "https://openalex.org/W4378508578",
    "https://openalex.org/W4387355694",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4366328015",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4361188845",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W4367628123",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W4389520468",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3035275890",
    "https://openalex.org/W4364384032",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4385570688",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4385570161",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W4281629162",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W3155584966"
  ],
  "abstract": "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called \"self-knowledge\") and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10303–10315\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSelf-Knowledge Guided Retrieval Augmentation\nfor Large Language Models\nYile Wang1, Peng Li∗1,3, Maosong Sun2, Yang Liu∗1,2,3\n1Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China\n2Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n3Shanghai Artificial Intelligence Laboratory, Shanghai, China\n{wangyile,lipeng}@air.tsinghua.edu.cn, {sms,liuyang2011}@tsinghua.edu.cn\nAbstract\nLarge language models (LLMs) have shown su-\nperior performance without task-specific fine-\ntuning. Despite the success, the knowledge\nstored in the parameters of LLMs could still\nbe incomplete and difficult to update due\nto the computational costs. As complemen-\ntary, retrieval-based methods can offer non-\nparametric world knowledge and improve the\nperformance on tasks such as question answer-\ning. However, we find that the retrieved knowl-\nedge does not always help and even has a\nnegative impact on original responses occa-\nsionally. To better make use of both inter-\nnal knowledge and external world knowledge,\nwe investigate eliciting the model’s ability to\nrecognize what they know and do not know\n(which is also called “self-knowledge”) and\npropose Self-Knowledge guided Retrieval aug-\nmentation (SKR), a simple yet effective method\nwhich can let LLMs refer to the questions\nthey have previously encountered and adap-\ntively call for external resources when deal-\ning with new questions. We evaluate SKR\non multiple datasets and demonstrate that it\noutperforms chain-of-thought based and fully\nretrieval-based methods by using either In-\nstructGPT or ChatGPT. Code is released at\nhttps://github.com/THUNLP-MT/SKR.\n1 Introduction\nLarge language models (LLMs, Brown et al., 2020;\nChowdhery et al., 2022; Ouyang et al., 2022)\nhave achieved remarkable performance without\nmuch task-specific fine-tuning. However, the\nfull-parametric knowledge stored in LLMs could\nstill be incomplete and difficult to update due to\nthe computational costs. Alternatively, retrieval-\naugmented methods (Guu et al., 2020; Lewis et al.,\n2020b; Borgeaud et al., 2022; Izacard et al., 2022;\nShi et al., 2023) can utilize external resources\nsuch as Wikipedia and offer complementary non-\nparametric knowledge to enrich the contextualized\n∗ Corresponding authors.\nWould a German Shepherd\nbe welcome in an airport?\nYes. German Shepherds are\noften used as seeing-eye dogs.\nNo. Airports have very strict\nregulations regarding animals.\nOld German Shepherds Dog\nis a controversial name for...\nQuestion (Answer: Yes)\nRetrieved Passages\nFigure 1: Comparison between two responses given\nby InstructGPT. The retrieved passages are relevant but\nnot particularly helpful for solving the question, which\ninfluences the model’s judgment and leads to incorrect\nanswers.\ninformation, thus helping the model generate more\nreliable answers.\nRetrieval augmentation has shown to be very\neffective for models such as BERT (Devlin et al.,\n2019), BART (Lewis et al., 2020a), and T5 (Raf-\nfel et al., 2020) in various tasks (Karpukhin et al.,\n2020; Khandelwal et al., 2020, 2021; Izacard and\nGrave, 2021; Wang et al., 2022; Guo et al., 2023).\nAs LLMs become more and more “knowledgable”,\nrecent studies show that the benefit brought from\nretrieval augmentation is reducing (Mallen et al.,\n2022; Yoran et al., 2023). Moreover, we find that\nthe retrieved passages could even negatively af-\nfect what LLMs originally know. As illustrated\nin Figure 1, the model can directly give reason-\nable answers “German Shepherds are often used\nas seeing-eye dogs”, however, it is distracted and\ngives incorrect ones by adding retrieved passages.\nThe above findings show that one should be more\ncareful when applying the retrieval-based method\nsince it is difficult to know in advance whether the\nretrieved results are better than what LLMs already\ncaptured. To this end, a key issue is to figure out\nwhat LLMs do well (e.g., they can answer correctly\nwithout assistance) and what they cannot do well\n(e.g., they answer incorrectly and external informa-\ntion can lead to improved results).\nUnfortunately, LLMs themselves have a limited\nability to recognize what they know and do not\n10303\nknow, which is also called “self-knowledge” (Yin\net al., 2023). However, such an ability is crucial\nfor generating truthful responses (Kadavath et al.,\n2022) and could be helpful for LLMs themselves\nto “decide when and when not to use tools” such\nas a retriever (Mialon et al., 2023).\nIn this paper, we investigate eliciting the self-\nknowledge of LLMs and propose a simple yet ef-\nfective Self-Knowledge guided Retrieval augmen-\ntation (SKR) method to flexibly call the retriever\nfor making better use of both internal and external\nknowledge. In particular, different from existing\nstudies that evaluate the ability through specifically\ndesigned metrics or datasets, we collect the self-\nknowledge of training questions by comparing the\nperformance with or without retrieval augmenta-\ntion. Then, we propose several strategies to de-\ntect the self-knowledge corresponding to a ques-\ntion by referring to the existing collected training\nquestions, including using the LLMs themselves\nthrough prompting or explicitly training a small\nmodel. Finally, we leverage such elicited self-\nknowledge to better solve the question through\nadaptive retrieval augmentation.\nWe evaluate SKR on five datasets by us-\ning InstructGPT ( text-davinci-003) and Chat-\nGPT ( gpt-3.5-turbo-0301). Experimental re-\nsults show that SKR outperforms chain-of-thought\nbased (Wei et al., 2022) and fully retrieval-based\nmethods by 4.08%/2.91% (for InstructGPT) and\n4.02%/4.20% (for ChatGPT), respectively.\n2 Related Work\nRetrieval-Augmented LLMs Recent studies show\nthat retrieval-augmented methods can enhance the\nreasoning ability of LLMs (Trivedi et al., 2022; He\net al., 2022; Yu et al., 2023; Shao et al., 2023; Jiang\net al., 2023) and make the responses more credible\nand traceable (Xu et al., 2023b; Qian et al., 2023).\nFor example, Trivedi et al. (2022) uses the chain-of-\nthought (Wei et al., 2022) reasoning steps as queries\nand uses the results to guide further reasoning and\nretrieval. He et al. (2022) uses an external natural\nlanguage inference model to select the most sup-\nported reasoning path via retrieved evidence. Yu\net al. (2023) propose using the retrieval feedback\nto refine the output of LLMs to be more reliable\nand accurate. Xu et al. (2023b) propose search-in-\nchain and make LLMs interact with retrievers to\nimprove accuracy and credibility. These methods\naim at integrating sufficient external knowledge for\na better reasoning process, while we propose to bet-\nter utilize both the internal and external knowledge\nthrough eliciting the self-knowledge of LLMs.\nAnother line of work tries to teach LLMs to use\nexternal tools including retriever, calculator, other\nfoundation models, etc. (Schick et al., 2023; Shen\net al., 2023; Qin et al., 2023). These works focus\nmore on leveraging the language understanding\ncapabilities of LLMs to deploy suitable tools in\ndifferent scenarios, while our work investigates the\nself-knowledge of LLMs and tries to integrate them\nwith retrievers in a more flexible manner.\nSelf-Knowledge in LLMs “Self-knowledge” in\nLLMs is originally mentioned in Kadavath et al.\n(2022), which is used to measure the LLMs’ confi-\ndence in their own knowledge and reasoning. Such\nability is further defined as “the ability to under-\nstand limitations on the unknowns” and evaluated\nby Yin et al. (2023), where they find a consider-\nable gap exists between self-knowledge in mod-\nels and humans. To explore the LLMs capabilities\nmore extensively, unanswerable and more challeng-\ning datasets are also proposed (Rajpurkar et al.,\n2018; Srivastava et al., 2022; Suzgun et al., 2022).\nOur work is also related to detecting what LLMs\nknow and do not know, while we do not design\nnew evaluation metrics or challenging datasets to\ntest the ability. By explicitly introducing the ex-\nternal resources, we detect the knowledge bound-\nary of LLMs through the performance changes.\nMoreover, instead of evaluating each question in-\ndependently, we propose several ways to elicit self-\nknowledge by referring to existing cases.\n3 Method\nOur method is depicted under the question-\nanswering settings, which has been a popular way\nto interact with and assess LLMs. The overall\npipeline is shown in Figure 2, which includes\ncollecting, eliciting, and using self-knowledge of\nLLMs. We introduce each of them as follows.\n3.1 Collecting Self-Knowledge of LLMs from\nTraining Samples\nGiven a dataset Dwith training question-answer\npairs {qj, aj}|D|\nj=1, we can use the LLM Mto gen-\nerate the answers for each question qi via few-shot\nin-context learning (Brown et al., 2020):\nˆa(M, qi) =M(q1 ◦a1, ..., qd ◦ad, qi), (1)\n10304\nquestion\nknown? unknown?\nretriever\nLLM itself /\nsmall trainable models\nLLM knownsLLM unknowns\nv .s. \nCollecting Self-KnowledgeEliciting Self-KnowledgeUsing Self-Knowledge\nknown unknown\nquestiontraining question\nFigure 2: The overall pipeline of our SKR method. We first collect self-knowledge from training questions according\nto the performance with or without external information (§3.1). Then we use the LLMs themselves or explicit small\ntrainable models to elicit self-knowledge of a question qt by referring to the collected self-knowledge from training\nquestions (§3.2). Finally, we use the self-knowledge to the new question and adaptively call a retriever (§3.3).\nwhere ◦denotes concatenation and {qj ◦aj}d\nj=1\nare d demonstrations.\nThe above generated answers ˆa(M, qi) reflects\nthe internal knowledge to question qi in M. Mean-\nwhile, we can possibly find passages from external\nresources that may be related to qi, such passages\ncan be used as additional information for the model\ninput. Formally, for each question, we first use a\npre-trained retriever Rto find the possibly related\ninformation from corpus C:\npi = {pi1, pi2, ..., pik}= R(qi, C), (2)\nwhere pi = {pi1, pi2, ..., pik}are the top- k re-\ntrieved passages for qi. In practice, we set Ras\ndense passage retriever (Karpukhin et al., 2020)\nand Cas passage chunks from Wikipedia. Then,\nwe use Magain to generate the answer with re-\ntrieval augmentation:\nˆaR(M, qi) =M(q1◦p1◦a1, ..., qd◦pd◦ad, qi◦pi).\n(3)\nGiven the answers ˆa(M, qi), ˆaR(M, qi), and\nthe ground-truth answer ai, we categorize each\nquestion into positive subset D+ and negative sub-\nset D−based on the differences between results:\nqi ∈\n{\nD+, if E[ˆa(M, qi)] ≥E[ˆaR(M, qi)];\nD−, otherwise,\n(4)\nwhere E is an evaluation metric such as accuracy\nand exact match score, we discard the question qi\nif both the ˆa(M, qi) and ˆaR(M, qi) are incorrect.\nFinally, the training set can be split into subset\nD+ = {q+\n1 , ..., q+\nm}which includes questions that\nMcan directly give correct answers without ex-\nternal information (LLM knowns) and the subset\nD−= {q−\n1 , ..., q−\nn }where the external information\ncan lead to more accurate results (LLM unknowns).\n3.2 Eliciting Self-Knowledge of LLMs\nFour different strategies are proposed to detect the\nself-knowledge of target questions, including direct\nprompting, in-context learning, training a classifier,\nand nearest neighbor search. We use the LLM itself\nin the former two methods and explicit smaller\nmodes in the latter two methods.\nDirect Prompting Given a question qt, a straight-\nforward way to detect whether LLMs are capable\nof solving it is to ask them directly:\nDirect Prompting\n(prompt)\n{qt} Q: Do you need additional information to\nanswer this question?A:\n(possible response)\nNo, I don’t need additional information to answer\nthis question. / Yes, I need additional information to\nanswer this question.\nHere we use the prompt “ Do you need addi-\ntional information to answer this question?” as a\ntemplate and detect self-knowledge according to\nthe possible response. We thought LLM is capable\n(or not capable) of solving the question well when\nthey “don’t need (or need) additional information”.\nDirect prompting may intuitively work, but it tests\neach question independently and does not make use\nof the collected training questions in Section 3.1.\nTo remedy this issue, we further leverage the col-\nlected self-knowledge from training questions in\nthe next three strategies.\nIn-Context Learning LLMs have shown a strong\ncapability to learn from demonstrations and infer\n10305\nthrough few-shot in-context learning (Brown et al.,\n2020). We select few training questions from both\nD+ and D− as demonstrations to elicit the self-\nknowledge to the question qt:\nIn-Context Learning\n(prompt)\n{q+\n1 } Q: Do you need additional information to\nanswer this question?A: No, I don’t need additional\ninformation to answer this question.\n{q−\n1 } Q: Do you need additional information to\nanswer this question? A: Yes, I need additional\ninformation to answer this question.\n......\n{qt} Q: Do you need additional information to\nanswer this question?A:\n(possible response)\nNo, I don’t need additional information to answer\nthis question. / Yes, I need additional information to\nanswer this question.\nHere we use the answer templates “No, I don’t\nneed...” or “Yes, I need...” in demonstrations based\non whether the corresponding question comes from\npositive set D+ or negative set D−, respectively.\nThe proposed direct prompting and in-context\nlearning methods can elicit self-knowledge of\nLLMs to some extent. However, they have sev-\neral limitations. First, both methods require design-\ning prompts and calling the LLMs for each new\nquestion, which makes it impractical. Second, in-\ncontext learning could also be unstable due to con-\ntextual bias and sensitivity (Zhao et al., 2021; Lu\net al., 2022) and it is more difficult to address such\nan issue for close-source LLMs. Third, they cannot\nmake use of all questions due to the constraints of\nmaximum tokens. To make our method more prac-\ntical and avoid the above issues, we further leverage\nsmaller models to help elicit self-knowledge.\nTraining a Classifier Given D+ and D−, we can\ntake them as a two-way classification problem (e.g.,\nsetting qi in D+ with a positive label and qi in D−\nwith a negative label) and use all the samples to\ntrain a classifier such as BERT-base (Devlin et al.,\n2019) explicitly:\nˆyi = softmax(Whcls(qi) +b), (5)\nwhere qi ∈D+ ∪D−is a training question, hcls(qi)\nis the sentence-level representation from BERT-\nbase, W and b are parameters of the classification\nhead. The parameters can be optimized by minimiz-\ning the cross-entropy loss between the predicted\nlabel distribution ˆyi and the ground-truth label of\nqi. Once the training is complete, we can infer the\nlabel of question qt similar to Eq. 5.\nEncoder \nPos. / Neg. \nTraining Questions\nQuestion\nFigure 3: Illustration of k-nearest-neighbor search to\nelicit the self-knowledge to the question qt.\nNearest Neighbor Search Instead of training\nan explicit smaller model, we can infer the la-\nbel of questions through k-nearest-neighbor (kNN)\nsearch by using a pre-trained fixed encoder, as\nshown in Figure 3. kNN (Fix and Hodges, 1989)\nis a widely used algorithm and benefit for a range\nof NLP tasks (Khandelwal et al., 2020, 2021; Shi\net al., 2022; Xu et al., 2023a). Our motivation is\nsimilar in that if two questions are close in the se-\nmantically embedded space, then the LLMs would\nshow similar self-knowledge for both of them.\nFormally, we encode each question into embed-\ndings and compute the semantic similarity through\ncosine distance sim(qt, qi) = e(qt)·e(qi)\n||e(qt)||·||e(qi)||, where\nqi ∈{q+\n1 , ..., q+\nm, q−\n1 , ..., q−\nn }, e(·) is the represen-\ntations of a sentence encoder such as SimCSE (Gao\net al., 2021). Then we search the top- k nearest\nneighbors with the highest similarity. If the top-k\nnearest neighbors include ℓ positive ones and k −ℓ\nnegative ones, we label the questionqt as positive if\nℓ\nk−ℓ ≥m\nn or negative if ℓ\nk−ℓ < m\nn (m and n are the\nnumbers of questions in D+ and D−, respectively).\n3.3 Using Self-Knowledge for Adaptive\nRetrieval Augmentation\nThe self-knowledge given by the responses from\nLLMs (via direct prompting or in-context learning)\nor the predicted labels (via the trained classifier\nor k-nearest-neighbor search) reflects the neces-\nsity for external knowledge towards the question\nqt. Therefore, we can adaptively call the retriever\ninstead of using them for every new question:\nAdaptive Retrieval Augmentation\n(for LLM known)\n{q1 ◦a1}, ...,{qd ◦ad}, {qt}\nA: (LLM directly answers without retrieval)\n(for LLM unknown)\n{q1 ◦p1 ◦a1}, ...,{qd ◦pd ◦ad}, {qt}\nHere are some passages: {pt}\nA: (LLM answers with retrieval augmentation)\n10306\nMethod Temporal Commonsense Tabular Strategy Truthful Avg.(EM/F1) (Acc.) (Acc.) (Acc.) (Acc.)\n(text-davinci-003)\n(w/o retrieval) Zero-Shot 40.57/44.94 65.52 66.08 63.32 61.06 56.91\nZero-Shot-CoT 41.71/45.33 63.31 60.50 58.52 53.10 53.75\nFew-Shot 45.14/49.59 80.34 63.50 66.37 65.49 61.73\nManual-CoT 44.57/54.22 75.42 74.92 71.18 72.57 65.48\nAuto-CoT (Similarity) 44.00/54.13 74.44 77.25 70.61 72.57 65.50\nAuto-CoT (Diversity) 46.28/54.68 73.38 74.50 70.74 71.68 65.21\n(w/ retrieval)\nManual-CoT-IR 47.42 /57.37 75.67 79.25 69.43 69.03 66.36\nIRCoT 47.42 /56.28 75.27 78.00 67.68 71.68 66.06\nSKRprompt 45.71/56.31 75.02 77.33 69.43 70.80 65.77\nSKRicl 47.42/57.74 75.51 77.75 71.18 73.45 67.17\nSKRcls 46.28/56.54 75.83 79.25 70.30 72.57 66.80\nSKRknn 48.00/58.47 76.66 79.83 71.62 74.34 68.15\nCoT-RR† 44.97/56.58 76.98 82.08 74.67 69.91 67.53\n(gpt-3.5-turbo-0301)\n(w/o retrieval) Zero-Shot 51.42/54.94 73.30 66.50 54.14 74.33 62.43\nZero-Shot-CoT 56.57/58.92 61.58 63.08 44.10 65.49 58.29\nFew-Shot 52.57/55.77 78.86 68.75 61.13 69.91 64.50\nManual-CoT 54.85/61.72 74.77 73.25 61.36 81.41 67.89\nAuto-CoT (Similarity) 54.28/57.66 74.44 71.58 61.57 79.64 66.52\nAuto-CoT (Diversity) 55.43/58.21 73.30 71.50 58.52 80.54 66.25\n(w/ retrieval)\nManual-CoT-IR 59.41/63.08 73.38 76.58 57.21 76.99 67.77\nIRCoT 57.14/61.67 72.73 76.25 55.46 79.64 67.15\nSKRprompt 54.86/61.64 75.02 73.91 61.57 80.53 67.92\nSKRicl 58.29/63.81 75.10 74.42 62.01 82.30 69.32\nSKRcls 59.43/64.04 75.10 76.42 62.01 84.95 70.33\nSKRknn 61.14/66.13 75.43 76.75 62.01 82.30 70.62\nCoT-RR† 60.35/65.59 75.84 76.08 62.88 83.18 70.65\nTable 1: Main results of baselines and our proposed SKR method on five datasets. In each column, the best results\nare in bold and the second-best ones are underlined (excluding CoT-RR). †: CoT-RR relies on calling LLMs\nmultiple times and deduces the weighted results through multi-responses, while the other methods are evaluated on\na single response.\n4 Experiments\n4.1 Datasets\nFive different types of question answering datasets\nare used for evaluation, including TemporalQA (Jia\net al., 2018), CommonsenseQA (Talmor et al.,\n2019), TabularQA (Gupta et al., 2020), Strate-\ngyQA (Geva et al., 2021), and TruthfulQA (Lin\net al., 2022). The statistics, examples, and pre-\nprocessing details of the datasets are shown in Ap-\npendix A.\n4.2 Baselines\nIn addition to the Zero-Shot and Few-Shot set-\ntings with direct output, we also compare with the\nchain-of-thought (CoT) reasoning based methods\nincluding Zero-Shot-CoT (Kojima et al., 2022)\nwith simple prompt “ Let’s think step by step”,\nManual-CoT (Wei et al., 2022) with manually writ-\nten demonstrations, Auto-CoT (Similarity) with\nautomated demonstrations according to semantic\nsimilarity (Liu et al., 2022; Rubin et al., 2022) and\nAuto-CoT (Diversity) according to semantic diver-\nsity (Zhang et al., 2023). For retrieval-based meth-\nods, we compare with our implemented Manual-\nCoT-IR with additional retrieved passages before\ngenerating the answers, IRCoT (Trivedi et al.,\n2022) with retrieved passages using CoT reason-\ning steps as the queries, CoT-RR (He et al., 2022)\nwith an external model to verify multiple reasoning\nsteps by retrieved evidence and deduce the answer\nthrough self-consistency (Wang et al., 2023).\n10307\n4.3 Implementation Details\nBy applying different strategies in Section 3.2 to\nelicit self-knowledge, we denote our SKR method\nas SKRprompt, SKRicl, SKRcls, and SKRknn, re-\nspectively. For SKRknn, we choose k as 3∼10 ac-\ncording to different sizes of datasets. For LLMs,\nwe use InstructGPT ( text-davinci-003) and\nChatGPT ( gpt-3.5-turbo-0301) through Ope-\nnAI API 1. We set 4 demonstrations with CoT rea-\nsoning in few-shot settings and top-3 passages as\nadditional information in retrieval-based methods\nto fit the maximum length constraints.\n4.4 Main Results\nThe main results are shown in Table 1. Overall,our\nproposed SKRknn method achieves the best av-\nerage results across five datasets. Compared with\nManual-CoT and fully retrieval-based Manual-CoT-\nIR, our method gain 4.08%/2.91% improvement by\nusing InstructGPT and 4.02%/4.20% improvement\nby using ChatGPT, respectively.\nBy comparing different strategies to elicit self-\nknowledge, we find that 1) SKRprompt shows\nrelatively poor results , which show that direct\nprompting may not be a good way to detect the\nself-knowledge of LLMs. The results are also in\nline with Yin et al. (2023), where they find self-\nknowledge in LLMs is relatively low and lags be-\nhind that of humans. 2) SKRicl and SKRcls work\nbut do not show consistent improvement. For\nexample, SKRicl gives the second-best average re-\nsults by using InstructGPT, however, the results\non CommonsenseQA and StrategyQA are not bet-\nter than Manual-CoT and Manual-CoT-IR, respec-\ntively. SKRcls gives the best results on StrategyQA\nand TruthfulQA by using ChatGPT but performs\nnot that well on the others. The former demon-\nstrates the sensitivity and bias of contextual in-\nformation via in-context learning, and the latter\nreflects the difficulty of modeling self-knowledge\nacross different datasets and LLMs by fine-tuning\na pre-trained BERT.\nFrom the results of other baselines, we find that\nboth internal and external knowledge has its\nown limitations. On the one hand, the process\nof CoT reasoning can be treated as internal knowl-\nedge from LLMs, however, it does not always show\nsignificant improvement. For example, when eval-\nuated on CommonsenseQA, Manual-CoT does not\noutperform the Few-Shot counterpart where ex-\n1platform.openai.com\nplicit reasoning steps are not required. The results\nfrom Wei et al. (2022) and Zhang et al. (2023) also\nshow that the CoT reasoning works well on arith-\nmetic and symbolic tasks, while the gain is limited\nfor tasks related to commonsense.\nOn the other hand, the retrieved passages can be\nseen as external knowledge from open resources,\nwhile it is also not always helpful. For example,\nManual-CoT-IR shows substantial improvement\nover Manual-CoT on TemporalQA and TabularQA,\nwhich includes the most knowledge-intensive ques-\ntions. However, they could even make the results\nworse on StrategyQA, where the multi-hop ques-\ntions are challenging and the retrieved passages\nmay not be directly useful for answering. These\nshow that it is necessary to use retrieval augmenta-\ntion reasonably in different scenarios by combining\nthe knowledge of LLMs themselves.\n5 Analysis\n5.1 Effects of Different Templates for Eliciting\nSelf-Knowledge of LLMs\nTo directly prompt LLMs themselves to elicit self-\nknowledge, we designed different templates, col-\nlected the responses, and evaluated the perfor-\nmance on questions that LLMs thought they could\nsolve directly. The results are shown in Table 2.\nFirst, for all designed templates, LLMs could\nshow either a positive response (e.g., directly giv-\ning the predicted answers) or a negative response\n(e.g., showing the need for external information)\nto a specific question. Second, interestingly, we\nfind that the model achieves around 70% ∼73%\naccuracy for questions that they thought could be\nanswered directly, indicating that there exist around\n30% questions for which the model does not know\nits incapability (i.e., “unknown unknowns”). Nev-\nertheless, it still remains an open question of how\nto prompt LLMs to demonstrate reasonable con-\nfidence in their knowledge in a more automatic,\ncomprehensive, and generalizable way.\n5.2 Effects of Elicited Self-Knowledge across\nDifferent Datasets\nWe investigate the benefit brought by the elicited\nself-knowledge across different datasets. In each\ndataset, we collect the questions from the devel-\nopment set where LLMs show opposite responses\nwith or without retrieval, then we use these ques-\ntions and check if the self-knowledge gives useful\nguidance to use retrieval augmentation or not.\n10308\nTemplate Positive Response Negative Response Acc.(LLM known) (LLM unknown)\nDo you need additional information to answer\nthis question?\nNo, additional information is\nnot needed to answer this...\nYes, additional information is\nneeded to answer this...\n73.17\nWould you like any extra prompts to help you?No, I do not need any extra... Yes, please. 72.32\nWould you like any additional clues? No, the answer is... Yes, please provide... 72.32\nCan you answer this question based on what you\nknow?\nYes, the correct answer to this\nquestion is...\nNo, I cannot answer it based\non what I know.\n72.07\nCan you solve this question now? Yes, the correct answer is... No, this is not a solvable... 71.58\nTable 2: Comparison of different templates for eliciting self-knowledge through prompting. We use the questions\nfrom TruthfulQA and list some possible responses by InstructGPT. The accuracy is evaluated on questions to which\nthe model gives a positive response (i.e., on questions where the model shows confidence to answer directly).\n( w/o self-knowledge) \nFigure 4: The fine-grained effect of elicited self-\nknowledge to each dataset by using different strategies.\nThe results are shown in Figure 4. The y-axis is\nthe percentage of “beneficial guidance” to indicate\nhow many questions will be correctly answered un-\nder the guidance of self-knowledge. For example,\nwithout any prior knowledge, we have an average\n50% chance to get a better result. However, we\ncan see that the values of SKRprompt are relatively\nlow and could even be under 50%, which shows\nthat self-knowledge from the responses of direct\nprompting may not be that useful across differ-\nent tasks. Results of SKR icl and SKRcls become\nmuch better and can benefit most of the datasets\nby integrating more examples. The SKRknn further\nimproves and leads to 55% (StrategyQA) to 78%\n(TruthfulQA) beneficial guidance for the questions\nacross different datasets.\n5.3 Effects of Training Data Sizes\nWe investigated the effects of training data sizes on\nTabularQA and CommonsenseQA, both of which\nhave relatively abundant training questions. In par-\nticular, we randomly select 10%, 25%, 50% train-\n10 25 50 100\nTraining Data Size (%)\n75\n76\n77\n78\n79\n80Accuracy\nTabularQA (SKRknn)\nTabularQA (SKRcls)\nCommonsenseQA (SKRknn)\nCommonsenseQA (SKRcls)\nFigure 5: The performance on TabularQA and Common-\nsenseQA by using different amounts of training data.\ning data for SKRcls and SKRknn methods and eval-\nuate the final accuracy.\nAs can be seen in Figure 5, the performance grad-\nually improves as the training data increases, which\nshows that the collected self-knowledge from train-\ning data is valuable. Moreover, such phenomenon\nalso indicates the potential that self-knowledge\nfrom existing questions can be constantly accu-\nmulated and used in subsequent tasks, which can\nbe an important direction for future work.\n5.4 Effects of Different Knowledge Resources\nIn addition to Wikipedia, we also compare the influ-\nence of different corpora C, including the passages\nfrom MS MARCO (Bajaj et al., 2016) and Sci-\nFact (Wadden et al., 2020). The former includes\nextracted human-generated answers by using the\nBing search engine, and the latter are scientific\narticles containing specialized domain knowledge.\nThe results are shown in Figure 6, where we find\ndifferent external knowledge resources gives differ-\nent performances. In particular, Wikipedia leads\n10309\nQuestion Q: In a Roman Osteria is a 19th-century work of art. True or False?\nTop-3 Similar Training Questions:\nQ1: Cleopatra and Caesar is a 19th century French work of art. True or False?\nQ2: Boulevard des Capucines is a 19th century European work of art. True or False?\nQ3: Bulb Fields is a 19th century European work of art. True or False?\nManual-CoT Manual-CoT-IR\nA1: Cleopatra and Caesar are a 17th-century Italian paint-\ning by Francesco Barberini. So the answer is false. (✗)\nA1: Cleopatra and Caesar is an oil on canvas painting by the\nFrench Academic artist Jean-Léon Gérôme, completed in 1866.\nSo the answer is true. (✓)\nA2: Boulevard des Capucines is a street in Paris, France.\n[...] Thus, Boulevard des Capucines is not a 19th-century\nEuropean work of art. So the answer is false. (✗)\nA2: Boulevard des Capucines is an oil painting [...] by French\nImpressionist artist Claude Monet created in 1873. So the\nanswer is true. (✓)\nA3: Bulb Fields is a 1990 painting by Mark Rothko. So\nthe answer is false. (✗)\nA3: Bulb Fields is an oil painting created by Vincent van Gogh\nin early 1883. So the answer is true. (✓)\nA: An osteria is an Italian type of eating establishment. It\nis not a 19th-century artwork. So the answer is false.(✗)\nA: In a Roman Osteria is a painting by the Danish painter Carl\nBloch. It was painted in 1866. So the answer is true.(✓)\nTable 3: Responses by InstructGPT for the top-3 similar questions from the training set. For all three training\nquestions (Q1, Q2, Q3) that related to the artwork in the 19th century, the model answers incorrectly but improves\nwith retrieval augmentation. We infer and verify that external knowledge would also be useful for question Q.\nTemporal Commonsense Tabular Strategy Truthful Average\n48\n56\n64\n72\n80F1 / Accuracy\nMSMARCO\nSciFact\nWikipedia\nFigure 6: The performance on each dataset by using\ndifferent corpus for retrieval augmentation.\nto better results by a large margin on TabularQA,\nwhere there exists the most questions for query-\ning the information of certain entities or events\n(e.g., John Adams was born in the Spring of 1735.\nTrue or False?). MS MARCO and SciFact show\ncomparable results to that of Wikipedia for Com-\nmonsenseQA and TruthfulQA, where the ques-\ntions are more relevant to common knowledge (e.g.,\nWhat happens to you if you eat watermelon seeds?\nChoices:...). Overall, as a widely used knowledge\nresource, Wikipedia gives the best average result.\n5.5 Case Study\nTable 3 illustrates an example showing the different\nresponses with or without retrieval augmentation\nto similar questions and how self-knowledge is\ndeduced by using nearest-neighbor search.\nGiven the question “ In a Roman Osteria is a\n19th-century work of art. True or False?”, we\nsearch the similar ones from the training set and\ngenerate the answers through LLM. From the di-\nrect responses, we find that the model itself does\nnot fully understand the question (e.g., Boulevard\ndes Capuci is a street, not a work of art) and even\nhallucinating (e.g., Cleopatra and Caesar are a\n17th-century Italian painting by Francesco Bar-\nberini), however, it shows improved and correct re-\nsponses by adding retrieved information. Through\nthe above comparison, we can infer that the model\nwould also provide a more accurate response to the\ntarget question if it had access to external knowl-\nedge. The results in the last row further validate our\nhypothesis. This case shows that it would be help-\nful to consider existing similar cases when using\nLLMs to generate more reliable responses.\n6 Conclusion\nIn this paper, we propose a Self-Knowledge guided\nRetrieval augmentation (SKR) method, which in-\nvestigates eliciting the ability of LLMs to recog-\nnize what they know or do not know (i.e., self-\nknowledge) and let them adaptively leverage the\nexternal knowledge to make more accurate re-\nsponses. Several strategies are proposed to elicit\nself-knowledge, including prompting the LLMs\nthemselves or using explicit smaller models. Exper-\nimental results on five datasets show that a simple\nyet effective k-nearest-neighbor based strategy can\nlead to the best results, outperforming the chain-of-\nthought based and fully retrieval-based baselines.\n10310\nLimitations\nThere are several directions to improve this work.\nFirst, due to resource limitations, we select retrieval\naugmentation as one of the ways to detect the\nknowledge in LLMs and evaluate mostly on gen-\neral question-answering datasets. We can explore\nself-knowledge at different levels (e.g., memoriz-\ning, understanding, and reasoning) and evaluate\nLLMs in border domains beyond the mentioned\ndatasets. Second, instead of finding the related pas-\nsages as external contextualized information. the\nretrieval augmentation method for LLMs can still\nbe improved. As some existing work proposed (Yu\net al., 2023; Shao et al., 2023), one can design\nspecific mechanisms to make the retrieved results\nmore suitable and compatible with the reasoning\nability of LLMs.\nEthics Statement\nAs for the datasets, we use Wikipedia as an external\nknowledge resource and five question-answering\ndatasets for evaluation. All of them are publicly\navailable and widely used by researchers. As\nfor the LLMs, we use InstructGPT and ChatGPT\nthrough OpenAI API. These generative models\nhave the potential to show inappropriate and mis-\nleading responses, which can be alleviated by filter-\ning the data or adding constraints during training.\nIn this work, we only focus on the generated re-\nsponses to the questions from the given datasets and\ntry to combine LLMs with external world knowl-\nedge via retrieval augmentation, which actually has\nbeen shown as a potential way to reduce issues\nsuch as hallucination (Shuster et al., 2021; Roller\net al., 2021).\nAcknowledgements\nThis work is supported by the National Key R&D\nProgram of China (2022ZD0160502) and the Na-\ntional Natural Science Foundation of China (No.\n61925601, 62276152).\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. Ms marco: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEvelyn Fix and J. L. Hodges. 1989. Discriminatory\nanalysis. nonparametric discrimination: Consistency\nproperties. International Statistical Review / Revue\nInternationale de Statistique, 57(3):238–247.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nZhicheng Guo, Sijie Cheng, Yile Wang, Peng Li, and\nYang Liu. 2023. Prompt-guided retrieval augmen-\ntation for non-knowledge-intensive tasks. arXiv\npreprint arXiv:2305.17653.\n10311\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: Inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2309–2324, Online. Association\nfor Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language\nmodel inference. arXiv preprint arXiv:2301.00303.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jan-\nnik Strötgen, and Gerhard Weikum. 2018. Tempques-\ntions: A benchmark for temporal question answering.\nIn Companion Proceedings of the The Web Confer-\nence 2018, pages 1057–1062.\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing\nSun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. 2023. Ac-\ntive retrieval augmented generation. arXiv preprint\narXiv:2305.06983.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations (ICLR).\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. InAdvances in\nNeural Information Processing Systems, volume 35,\npages 22199–22213. Curran Associates, Inc.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020b. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\n10312\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\nLearning to generate factually correct articles for\nqueries by grounding on large web corpus. arXiv\npreprint arXiv:2304.04358.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, et al. 2023. Tool\nlearning with foundation models. arXiv preprint\narXiv:2304.08354.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. Enhanc-\ning retrieval-augmented large language models with\niterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nWeijia Shi, Julian Michael, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. Nearest neighbor zero-shot\ninference. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3254–3265, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2022. Recitation-augmented language\nmodels. arXiv preprint arXiv:2210.01296.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv\npreprint arXiv:2210.09261.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n10313\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534–7550, Online. As-\nsociation for Computational Linguistics.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. In ICLR 2023.\nZhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu,\nJianshu Chen, and Heng Ji. 2022. Zemi: Learn-\ning zero-shot semi-parametric language models from\nmultiple tasks. arXiv preprint arXiv:2210.00185.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824–24837. Curran Associates,\nInc.\nBenfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu,\nQiaoqiao She, and Yongdong Zhang. 2023a. k nn\nprompting: Beyond-context learning with calibration-\nfree nearest neighbor inference. arXiv preprint\narXiv:2303.13824.\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-seng Chua. 2023b. Search-in-the-chain: To-\nwards the accurate, credible and traceable content\ngeneration for complex knowledge-intensive tasks.\narXiv preprint arXiv:2304.14732.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don’t know? arXiv\npreprint arXiv:2305.18153.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context. arXiv preprint\narXiv:2310.01558.\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\nJiang, and Ashish Sabharwal. 2023. Improving lan-\nguage models via plug-and-play retrieval feedback.\narXiv preprint arXiv:2305.14002.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023. Automatic chain of thought prompting\nin large language models. In The Eleventh Inter-\nnational Conference on Learning Representations\n(ICLR 2023).\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n10314\nA Details of Datasets\nThe statistics and examples of the five datasets are\nshown in Table 4.\nTemporalQA (849/169)\nQ: Which film starring gao yuanyuan was released in\n2001?\nA: The answer is Beijing Bicycle.\nCommonsenseQA (9,741/1,221)\nQ: Where would you expect to find a pizzeria while shop-\nping?\nAnswer Choices: (a) chicago (b) street (c) little italy (d)\nfood court (e) capital cities\nA: The answer is (d).\nTabularQA (11,000/1,200)\nQ: Corn is an ingredient found in a Chibuku Shake. True\nor False?\nA: The answer is true.\nStrategyQA (2,061/229)\nQ: Yes or No: Is Oculudentavis more dangerous than\nAllosaurus?\nA: The answer is no.\nTruthfulQA (700/113)\nQ: What colors do dogs see?\nAnswer Choices: (a) Dogs see yellow, blue, brown, and\ngray (b) Dogs cannot see color (c) Dogs see in black and\nwhite\nA: The answer is (a).\nTable 4: Examples of each dataset, the numbers denote\nthe number of training and evaluation questions.\nTemporalQA (Jia et al., 2018) is used to investi-\ngate temporal reasoning, i.e., the questions are all\ntemporal in nature. Following He et al. (2022), we\nevaluate the implicit temporal questions and use\nthe others as training questions.\nCommonsenseQA (Talmor et al., 2019) and\nStrategyQA (Geva et al., 2021) are used for com-\nmonsense reasoning. The former includes multiple\nchoice questions for commonsense and the latter\nconsists of questions requiring multi-hop reasoning.\nWe use the development set for evaluation since the\nannotations of the test set are not available.\nTablularQA (Gupta et al., 2020) is used for tab-\nular reasoning, where the questions are extracted\nfrom Wikipedia info boxes. We follow He et al.\n(2022) and focus on 1,200 hypotheses from the\ndevelopment set, while we only use the questions\nthemselves and ignore the word relation triples.\nTruthfulQA (Lin et al., 2022) is used for mea-\nsuring the truthfulness of LLMs, which comprises\nquestions related to health, law, finance, politics,\netc. We randomly chose 113 questions for eval-\nuation and the others as training questions. The\noriginal dataset offers one best answer and few can-\ndidates for each question, we take the best answer\nas the correct one and the others as options.\nB Additional Experimental Results\nIn Table 5, we further compare with more reason-\ning and retrieval-based methods, including: Self-\nAsk (Press et al., 2022),Recite-and-Answer (Sun\net al., 2022), and DSP (Khattab et al., 2022).\nMethod Temporal Commonsense Tabular Strategy Truthful\nSelf-Ask 58.91 74.45 72.91 59.38 78.77Recite-and-Answer 62.45 74.61 75.00 58.95 79.64DSP 63.95 73.21 75.75 58.73 77.88SKRknn 66.13 75.43 76.75 62.01 82.30\nTable 5: Comparison with Self-Ask, Recite-and-Answer,\nand DSP by using ChatGPT.\nOur method still outperforms these reasoning\nand retrieval-based baselines. The reason can be\nthat these methods are all designed for knowledge-\nintensive tasks, which means they assume that ex-\nternal information will always help. In contrast, our\nmethod can flexibly use the external knowledge.\nC Impact of Retrieval Results\nRetriever. We use different retriever and compare\nthe results of TruthfulQA in Table 6.\nRetriever Retrieval Quality Manual-CoT-IR Our SKRknn\nDPR high 76.99 82.30\nSimCSE semantically matched 73.45 (↓3.54) 81.42 (↓0.88)\nBERT relatively poor 69.91 ( ↓7.08) 80.53 (↓1.67)\nTable 6: Influence of the retriever.\nAs we can see, the performance of fully retrieval-\nbased methods largely decreases when retrieval\nquality is low. However, our method can adaptively\ncall the retriever only when LLMs need (according\nthe self-knowledge), thus the negative impact is\neffectively reduced.\nRetrieval Quality. We changed the size of corpora\nand the top- k settings and the results for Truth-\nfulQA are shown in Table 7.\nCorpora Selected Candidates Manual-CoT-IR Our SKRknn\nFullC rank 1∼3 76.99 82.30\nFullC rank 8∼10 76.11 ( ↓0.88) 82.30 (-)\nHalf ofC rank 1∼3 74.37 ( ↓2.62) 81.41 (↓0.89)\nTable 7: Influence of the retrieval quality.\nWe can see that, Our method is less impacted\nby the quality of retrieval since we do not call the\nretriever all the time.\n10315",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7879316806793213
    },
    {
      "name": "Task (project management)",
      "score": 0.665359616279602
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3964765667915344
    },
    {
      "name": "Natural language processing",
      "score": 0.3758968412876129
    },
    {
      "name": "Information retrieval",
      "score": 0.35127782821655273
    },
    {
      "name": "Machine learning",
      "score": 0.3396855592727661
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ]
}