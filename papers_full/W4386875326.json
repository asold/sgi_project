{
  "title": "Contrastive Decoding Improves Reasoning in Large Language Models",
  "url": "https://openalex.org/W4386875326",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4294050216",
      "name": "O'Brien, Sean",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2464276071",
      "name": "Lewis Mike",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W7090822393",
    "https://openalex.org/W7090700900",
    "https://openalex.org/W7089205933",
    "https://openalex.org/W7088572356",
    "https://openalex.org/W4414811222",
    "https://openalex.org/W4317897818"
  ],
  "abstract": "We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.",
  "full_text": "CONTRASTIVE DECODING IMPROVES REASONING IN\nLARGE LANGUAGE MODELS\nSean O’Brien1,2∗ Mike Lewis2\n1University of California, San Diego 2Meta AI\nseobrien@ucsd.edu, mikelewis@meta.com\nABSTRACT\nWe demonstrate that Contrastive Decoding – a simple, computationally light, and\ntraining-free text generation method proposed by Li et al 2022 – achieves large\nout-of-the-box improvements over greedy decoding on a variety of reasoning\ntasks. Originally shown to improve the perceived quality of long-form text gen-\neration, Contrastive Decoding searches for strings that maximize a weighted dif-\nference in likelihood between strong and weak models. We show that Contrastive\nDecoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on\nthe HellaSwag commonsense reasoning benchmark, and to outperform LLaMA\n2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark,\nin addition to improvements on a collection of other tasks. Analysis suggests\nthat Contrastive Decoding improves over existing methods by preventing some\nabstract reasoning errors, as well as by avoiding simpler modes such as copy-\ning sections of the input during chain-of-thought. Overall, Contrastive Decoding\noutperforms nucleus sampling for long-form generation and greedy decoding for\nreasoning tasks, making it a powerful general purpose method for generating text\nfrom language models.\nFigure 1: Contrastive decoding improves reason-\ning across model scales and reasoning tasks.\nFigure 2: Contrastive scoring significantly im-\nproves performance on HellaSwag, a standard\ncommonsense reasoning benchmark.\n1 I NTRODUCTION\nText is generated from large language models (LLMs) in different ways for different tasks. For open-\nended text generation tasks, truncated sampling is normally used, as the most likely strings under a\nmodel tend to be short and uninteresting (Holtzman et al., 2020). For reasoning problems, greedy\ndecoding is normally preferred, to avoid risking sampling errors. This bifurcation is undesirable; for\nexample it increases the likelihood of reasoning errors during open-ended generation.\n*Work done as an AI resident at Meta.\n1\narXiv:2309.09117v2  [cs.CL]  29 Sep 2023\nFigure 3: CD accentuates what the expert model has learned that the amateur model has not. Results\nare taken from greedy decoding with a 65B parameter expert, using α = 0.1, β = 0.5 for CD.\nWe explore the use of Contrastive Decoding (Li et al., 2022) for solving reasoning problems with\nLLMs. Contrastive Decoding (CD) searches for strings that maximize a weighted difference in\nlikelihood between a stronger expert and a weaker amateur model, and was shown to outperform\nexisting methods for open-ended text generation. It achieves this by avoiding undesirable modes of\nthe expert model’s distribution, such as short or generic strings, which tend to be the most likely\nunder any model, including the amateur.\nWe show that Contrastive Decoding outperforms greedy decoding on reasoning problems. On\nGSM8K, a widely used benchmark consisting of grade-school word math problems, contrastive de-\ncoding improves the performance of various LLaMA models by up to 8 absolute percentage points.\nThis result outperforms LLaMA 2, which has 5 billion more parameters and is trained on 40% more\ndata. On HellaSwag, using the CD objective to rank answers leads LLaMA to outperform all existing\nmodels except GPT-4. We find general improvement on arithmetic reasoning and multiple-choice\nranking tasks, including on models as large as LLaMA-65B, suggesting that Contrastive Decoding\ncould bring such widespread improvements to much larger models.\nWe also analyze the cause of the improvement from Constrastive Decoding. Empirically, we find\nthat Contrastive Decoding performs less surface-level copying from the prompt than greedy decod-\ning and misses fewer reasoning steps. This result suggests that, similarly to findings in Li et al.\n(2022), Contrastive Decoding works by reducing repetitive or other undesirable modes of the model\ndistribution. Our current method yields mixed results for commonsense reasoning tasks and slightly\ndegrades factual retrieval, both trends that encourage further refinement of the method.\nOverall, we show that Contrastive Decoding not only substantially improves LLM accuracies on\na range of benchmarks, but is also the first generation algorithm to achieve state-of-the-art results\nin both reasoning and text generation problems. These results allow a more unified method for\nimproving generation from language models across tasks.\n2 C ONTRASTIVE DECODING\n2.1 S IMPLIFIED FORMULATION\nThe original Contrastive Decoding formulation from Li et al. (2022) explicitly chooses two pa-\nrameters: α and the intermediate temperature of the amateur distribution τa, with the intermediate\ntemperature of the expert fixed atτe = 1. We slightly refactor the hyperparameter choice to be more\ninterpretable and simplify the algorithm by working directly in logit space.\n2\nLet s(i)\na and s(i)\ne be the unnormalized scores (logits) assigned to token i by the amateur and ex-\npert models, respectively. α is the same hyperparameter in the original paper: a proportion of the\nmaximum probability assigned by the expert model, with any tokens assigned a lower probability\nmasked out. β is a hyperparameter corresponding to the strength of the amateur penalty. We include\na leading (1 +β) coefficient to the expert logits to decouple the strength of the contrastive penalty\nfrom the expected scale of the output logits, cleanly delineating between the contrastive tradeoff and\nthe final sampling temperature. This matches the formulation of DExperts (Liu et al., 2021), with\nthe expert model serving both as the base prior and steering expert.\n1. Determine α-mask.\nVvalid = {j ∈ V, s(j)\ne ≥ log α + maxk∈V s(k)\ne }\n2. Subtract amateur logits.\ns(i)\nCD =\n(\n(1 +β)s(i)\ne − βs(i)\na i ∈ Vvalid\n−∞ i ̸∈ Vvalid\nA PyTorch implementation for this formulation, as well as the original, can be found in subsec-\ntion A.1 of the appendix. Our implementation takes three lines of readable code.\n2.2 P ROBABILISTIC INTERPRETATION\nOur implementation of α-masking has the same interpretation as in Li et al. (2022), given that the\nexpert temperature is fixed to τe = 1. We show the equivalence in Appendix A.2.\nFurther, we can consider the post-softmax probabilities produced by CD as a perturbation of the\nprobabilities predicted by the expert model. Not including α-masking, the probability assigned to\ntoken i by CD is a normalized adjustment of the probability assigned by the expert model:\np(i)\nCD ∝ p(i)\ne\n \np(i)\ne\np(i)\na\n!β\n(1)\nIt is therefore clear that as β → 0 the contrastive penalty disappears, and as β → ∞the distribution\ncollapses to the argmax of p(i)\ne /p(i)\na , which is the original formulation from Li et al. (2022).\n3 E XPERIMENTS\n3.1 E XPERIMENTAL SETUP\nModels. We use untuned models from the LLaMA 1 family (Touvron et al., 2023) at all scales.\nUnless otherwise stated, we use an untuned LLaMA-65B as the expert and an untuned, LLaMA-\narchitecture model with 1.5B parameters trained on the same data as the other LLaMA 1 models as\nan amateur. For one ablation study, we use models from the FLAN-T5 family (Chung et al., 2022).\nDecoding Parameters. We set β = 0.5 and α = 0.1 for all experiments unless otherwise stated.\nWe use greedy decoding, except for self-consistency experiments for which we sample at τ = 0.7\nfollowing Touvron et al. (2023).\nPrompting. For generation tasks, we use 8-shot chain-of-thought prompting, in line with Tou-\nvron et al. (2023). The examples are the same as in LLaMA for tasks contained in that paper, and\ntaken from Wei et al. (2023) for other mathematical tasks.\nDatasets. Following prior works, we evaluate on a number of datasets. The following tasks\nmeasure performance on algebraic word problems: AQuA (Ling et al., 2017), ASDiv (Miao et al.,\n3\n2021), GSM8K (Cobbe et al., 2021), and SV AMP(Patel et al., 2021). We also evaluate on MATH\n(Hendrycks et al., 2021b), a larger and more challenging benchmark.\nFor commonsense reasoning, we measure open-ended performance on CommonsenseQA (Talmor\net al., 2019) and StrategyQA (Geva et al., 2021). We also evaluate on a battery of multiple-\nchoice reasoning benchmarks: both the easy and challenge splits of the AI2 Reasoning Challenge\ndataset (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU\n(Hendrycks et al., 2021a), PIQA (Bisk et al., 2019), SIQA (Sap et al., 2019), and WinoGrande\n(Sakaguchi et al., 2019).\n3.2 H YPERPARAMETER SELECTION\nContrastive decoding has three major hyperparameters: the masking ratioα, the contrastive strength\nβ and the size of the amateur model. We find that results are fairly insensitive to α as long as β is\nreasonably small (below 1); unless otherwise stated we use α = 0.1 across experiments.\nNext we consider the size of the amateur model. In agreement with Li et al. (2022), we find that\nperformance benefits from smaller amateur models ( Figure 4); while a 1B-parameter amateur helps\nreasoning performance, a 7B-parameter amateur harms it. We also examine different types of am-\nateurs; ablation studies show that a partially-trained amateur performs better than a fully-trained\none, and that a poorly-prompted expert can be successfully used as an amateur as well (see subsec-\ntion 4.2).\nFinally, we examine the effect of β. The optimal value depends on the task, but for both generation\ntasks like GSM8K and multiple-choice ranking tasks like PIQA we find thatβ = 0.5 performs well.\nSetting β too high can place too much weight in the contrastive penalty and harm performance,\nespecially with a larger gap between amateur and expert models. β = 0 corresponds to standard\ngreedy decoding with no contrastive penalty. Results of β hyperparameter sweeps can be found in\nTable 1, Figure 4, Figure 5 and Appendix B.\nThe best result on GSM8K, with LLaMA-65B and β = 0.25, is 57.7 (Table 1), outperforming\nPaLM-540B (56.5), LLaMA-2 (56.8) and GPT-3.5 (57.1).* (Anil et al., 2023; OpenAI, 2023)\nFigure 4: Results on GSM8K with LLaMA-\n65B as the expert. While a 7B amateur harms\nperformance, a 1.5B amateur helps.\nExpert β = 0 β = 0.25 β = 0.5 β = 1\n7B 10.7 11.5 13.6 11.0\n13B 17.0 21.0 22.9 20.4\n30B 35.2 40.0 43.4 42.0\n65B 51.0 57.7 56.8 44.6\nTable 1: Results on GSM8K. β = 0.5 tends to give\ngood results across expert sizes.\n3.3 A RITHMETIC REASONING\nWe find that contrastive decoding tends to help on arithmetic reasoning tasks with chain-of-thought\nprompting; see Table 2 for all results. One exception to this is the MATH dataset, which proves to\nbe challenging for both standard and contrastive decoding. We conjecture that because contrastive\ndecoding amplifies skills that the expert has learned better than the amateur, it cannot help on tasks\nthat are well beyond the expert’s ability.\nWe also experiment with normalizing the α-masked CD scores via softmax, then temperature sam-\npling from the resulting distribution. This permits CD to generate multiple candidate reasoning\nchains to be used for self-consistency (taking the majority answer) (Wang et al., 2023b). We show\nacross both mathematical and commonsense reasoning, CD improves self-consistency performance.\n*OpenAI (2023) evaluates GPT-3.5 5-shot; all others are 8-shot.\n4\n(a)\n (b)\nFigure 5: Two examples of sweeping through β values on multiple-choice reasoning tasks across\nmodel scales. Dashed horizontal lines mark performance without contrastive decoding.\nTable 2: Results on math generation tasks. Contrastive decoding generally improves performance.\nModel CD AQuA ASDiv GSM8K MATH SV AMP Average\n7B ✗ 21.0∗ 40.2 10.7 3.0 27.3 20.4\n13B ✗ 18.1∗ 49.0 17.4 4.2 39.4 25.6\n30B ✗ 23.8 60.1 35.3 6.9 55.9 36.4\n65B ✗ 33.3 67.2 51.0 10.6 69.1 46.2\n65B maj@20 ✗ 38.2 73.6 68.0 – † 77.3 64.3\n7B ✓ 19.0∗ (-2.0) 39.7 (-0.5) 14.3 (+3.6) 2.9 (-0.1) 31.5 (+4.2) 21.5 (+1.1)\n13B ✓ 16.0∗ (-2.1) 52.0 (+3.0) 22.7 (+5.5) 3.8 (-0.4) 43.1 (+3.7) 27.5 (+1.9)\n30B ✓ 29.8 (+6.0) 62.5 (+2.4) 43.1 (+8.1) 8.1 (+1.2) 59.3 (+3.4) 40.6 (+4.2)\n65B ✓ 36.9 (+3.6) 71.9 (+4.7) 56.8 (+5.8) 10.3 (-0.3) 67.8 (-1.3) 48.7 (+2.5)\n65B maj@20 ✓ 39.4 (+1.2) 77.4 (+3.8) 74.0 (+6.0) –† 79.0 (+1.7) 67.5 (+3.2)\n3.4 C OMMONSENSE REASONING\nResults are more mixed for CommonsenseQA and StrategyQA. For both of these tasks, we 8-shot\nprompt our model and compute the exact match score against the ground-truth answers. We find that\ncontrastive decoding harms performance for smaller models, but that this harm equalizes somewhat\nfor the 65B model and evens out when using self-consistency. See Table 3 for full results.\nTable 3: CD harms commonsense reasoning with a smaller expert,\nbut performance evens out with a larger expert-amateur gap.\nModel CD CSQA StrategyQA Average\n7B ✗ 40.0 59.2 49.6\n13B ✗ 60.4 64.5 62.5\n30B ✗ 66.4 68.7 67.6\n65B ✗ 77.5 69.5 73.5\n65B maj@20 ✗ 77.0 79.3 78.2\n7B ✓ 37.3 (-2.7) 58.3 (-0.9) 47.8 (-1.8)\n13B ✓ 58.5 (-1.9) 65.5 (+1.0) 62.0 (-0.5)\n30B ✓ 62.8 (-3.6) 67.6 (-1.1) 65.2 (-2.4)\n65B ✓ 77.1 (-0.4) 71.5 (+2.0) 74.3 (+0.8)\n65B maj@20 ✓ 77.9 (+0.9) 79.3 (+0.0) 78.6 (+0.4)\n*In the AQuA task, the model selects one out of five given options. Thus the random baseline is 20%, and\nresults below that threshold are not meaningful.\n†Given the size of the dataset and length of generations, we do not evaluate maj @ 20 on MATH.\n5\n3.5 C ONTRASTIVE RANKING\nWe further evaluate a contrastive objective as a scoring function to rank answers to multiple-choice\nquestions. These tasks are zero-shot, multiple-choice cloze tasks; instead of open-ended generation\nthe model scores each potential completion, length-normalizing following Touvron et al. (2023).\nWe find comparable performance across most tasks, with more substantive gains on HellaSwag and\nARC-Challenge. Notably, on HellaSwag CD leads LLaMA-65B to score 88.0, which outperforms\nLLaMA-2 (85.3), GPT-3.5 (85.5) (OpenAI, 2023) and PALM 2-Large (86.8) (Anil et al., 2023).\nTable 4: Results on multiple-choice reasoning tasks. CD generally provides a modest boost.\nβ ARC-E ARC-C BoolQ HSwag PIQA SIQA WGrande MMLU Avg\n0.0 79.1 56.1 84.2 84.2 82.6 52.3 77.3 63.5 72.4\n0.5 79.0 59.5 84.3 87.4 83.1 53.3 77.8 63.4 74.9\n1.0 76.9 59.7 84.1 88.0 82.9 53.3 76.5 63.2 74.5\n4 A DDITIONAL STUDIES\n4.1 E FFECTS OF CONTRASTIVE DECODING\nCD is worse at arithmetic but better at logical reasoning. We conduct a manual error analysis\nof 100 randomly selected examples from the GSM8K set between continuations from greedy decod-\ning and CD ( β = 0.5, α= 0.1). We follow Wang et al. (2023a) and categorize wrong answers as\nprimarily being due to an arithmetic error, a missing step or a semantic misunderstanding. We add\none category of “degeneration,” chosen when the model lapses into excessive repetition. Our small-\nscale analysis finds that CD makes more arithmetic errors, but that this is offset by better semantic\nreasoning and fewer missing steps (see Table 5).\nTable 5: Proportion of errors in of a set of 100 GSM8K questions. CD makes more\narithmetic errors, but omits fewer steps and avoids semantic misunderstandings.\nCD Arithmetic Missing Step Semantic Degeneration Total Errors\n✗ 4% 22% 24% 4% 54%\n✓ 8% 20% 21% 3% 52%\nTo further explore the claim that the benefit of CD does not stem from arithmetic evaluation, we\ngenerate a toy dataset of 1,0000 multiplication and subtraction equations with operands up to four\ndigits and then 8-shot prompt models to complete the expression, measuring exact match accuracy.\nWe find that CD does not improve performance on this task, and in fact may degrade it slightly.\nResults are shown in Table 8.\nStandard CD\nCorrect % 44.6 51.1\nParseable % 95.2 95.6\nAverage # chars 215.2 217.2\nTable 6: High-level generation statistics\nfrom sampled generations on GSM8K.\nResponses are similar lengths, despite\nthe performance improvement from CD.\nFigure 6: CD reduces copying from the question\nin the generated Chain of Thought, as measured\nby n-gram overlap on GSM8K generations.\nCD reduces copying from the prompt. We analyze 26,000 sampled generations from CD-\nsampling on GSM8K against the corresponding set from temperature sampling; both of these sets\nof generations are used in our self-consistency study. We find that responses are roughly the same\nlength and follow the few-shot template roughly the same proportion of the time. This rules out the\n6\nhypothesis that contrastive decoding simply leads the model to follow the template better, prevents\ndegeneration or induces longer answers with more reasoning steps. Further, we run an automatic\nevaluation of greedy generations using ROSCOE (Golovneva et al., 2022) but do not find significant\ndifferences in any of these metrics. However, we measure the precision and recall of the tokens in\nthe prompt by the sampled generations and find that CD systematically reduces token-level copying\nfrom the prompt. This may be related to increased reasoning ability, as surface-level copying from\nthe prompt does not provide new information to the problem.\nCD can harm factual recall. Our primary claim is that contrastive decoding improves chain-\nof-thought reasoning. However, we also test CD on two pure factual-recall tests that do not utilize\nchain-of-thought: OpenBookQA (Mihaylov et al., 2018) and TriviaQA (Joshi et al., 2017). Open-\nBookQA (“OBQA”), is a multiple-choice completion task, while TriviaQA is a 5-shot generation\ntask. Reusing the same setup from reasoning leads to a slight degradation of performance, as seen\nin Table 7.\nTable 7: CD can harm perfor-\nmance on factual recall tasks.\nCD OBQA TriviaQA ∗\n✗ 60.0 72.2\n✓ 57.8 (-2.4) 69.9 (-2.1)\nTable 8: CD slightly harms perfor-\nmance on a synthetic task of evaluat-\ning arithmetic expressions.\nCD 7B 13B 30B 65B\n✗ 31.0 36.3 52.3 58.4\n✓ 30.9 35.6 52.2 57.6\nCD outperforms other reasoning enhancements in FLOP efficiency. We note that contrastive\ndecoding introduces relatively little overhead in comparison to other reasoning-enhancing methods.\nWe estimate that with a 1.5B amateur and 65.2B expert, contrastive decoding increases the total\nnumber of FLOPs by 3.25% (see section C of the appendix). This compares favorably to self-\nconsistency, which requires several extra full generation loops. We show in Figure 9 that CD is\nsignificantly more efficient than self-consistency.\n4.2 A BLATION STUDIES\nα-masking alone is not enough. When sampling and performing self-consistency, α-masking\nprevents the sampling of tokens the expert finds to be unlikely. It is natural to ask what portion of\nthe benefit comes purely from α-masking and not the contrastive objective itself.\nTo answer this, we setβ = 0but α = 0.1; that is, we mask out candidates based on the expert but do\nnot apply the contrastive objective. When sampling one path, we expectα-masking to improve over\ntemperature sampling alone as it eliminates unlikely results and thus provides a closer approximation\nto greedy sampling. This holds, but as we increase the number of paths we find no benefit from α-\nmasking alone. This suggests that the contrastive objective, and not α-masking, is the primary\nsource of improved self-consistency results. See Figure 7 for results of this ablation.\nCD requires chain-of-thought prompting to improve results. We next study whether con-\ntrastive decoding provides an advantage in the absence of chain-of-thought prompting. We remove\nthe chains of thought from the GSM8K fewshot prompt, and find that as expected performance drops\nfor both standard and contrastive decoding (Figure 8); further, without chains of thought contrastive\ndecoding provides no consistent improvement. As with the MATH dataset, solving problems with-\nout explicit reasoning steps may be too challenging of a task for the expert model, and thus leave\ntoo small a gap between the expert and amateur to contrastively exploit.\nCD can benefit non-LLaMA models. We conduct a short study to show that CD can benefit\nmodels outside of the LLaMA family. For this study, we choose the FLAN-T5 family as it is open-\nsource, has a wide range of model sizes that share a single tokenizer, and obtains good performance\non chain-of-thought reasoning tasks. We use FLAN-T5-XXL (11B) as the expert model and FLAN-\nT5-Small (80M) as amateur. We evaluate on GSM8K using the 8-shot random prompts from Fu\n*On manual examination, we find the set of correct answers provided by TriviaQA to be insufficient. Ran-\ndomly sampling 100 supposedly incorrect answers generated by CD and standard decoding, we find roughly\nhalf are in fact correct (46/100 with CD and 49/100 without). A rough linear extrapolation gives us estimates\nfor non-CD and CD scores of 85.8 and 83.7, respectively.\n7\nFigure 7: GSM8K scores via temperature sam-\npling and maj @ k with various values of k.\nα-masking alone does not yield significant im-\nprovement, while full CD does.\nFigure 8: Comparison of GSM8K scores with\nLLaMA-65B, both with and without chain-of-\nthought prompts. CD only helps when using\nCoT.\net al. (2023); note that GSM8K is within the set of tasks that FLAN-T5 is finetuned on. CD provides\na slight boost in performance, as seen in Table 9. We leave more extensive experiments on other\nfamilies of models to future work.\nFigure 9: FLOP increases, with increasing\ncompute from using more samples for self-\nconsistency. CD achieves similar or better per-\nformance with a smaller increase in FLOPs.\nCD β GSM8K\n✗ 0 16.4\n✓ 0.5 17.1\n✓ 1.0 17.4\nTable 9: FLAN-T5 per-\nformance on GSM8K. CD\nprovides a boost to perfor-\nmance.\nSmall-scale amateurs beat “negative prompting.” We experiment to determine if there is a\nmore effective weak amateur model to use for contrastive decoding. We define a set of “negative\nprompts” by sampling 7B model outputs on the fewshot prompts and collecting the incorrect re-\nsponses. We use these responses as fewshot prompts to mimic the failure modes of the family of\nmodels. These negative prompts should harm the performance of models they are prompted with,\nand specifically bias results towards the error distribution of the 65B model.\nWe find that contrasting with a negative prompt does not harm performance, but does not improve\nit as much as contrasting with a small amateur (see Table 10). In an ablation study, we find that\nnegative prompting does not harm performance that much; prompting a 65B model with incorrect\nfewshot examples on GSM8K gives a score of 41.3, which underperforms prompting with cor-\nrect examples (51.2) but significantly beats non-chain-of-thought prompting (13.5). This supports\nWang et al. (2023a), who find that even incorrect chain-of-thought rationales improve reasoning. A\nprompting strategy which better incapacitates the expert model might yield better results.\nMid-training checkpoints make for good amateurs. We experiment with checkpoints of a mid-\ntraining 7B-parameter LLaMA model taken 10% and 23% of the way through the full training run.\nEven while a fully-trained 7B amateur harms performance on GSM8K, we find that a partially-\ntrained amateur improves performance. We do not perform extensive hyperparameter sweeps here,\ninstead reusing α = 0.1, β = 0.5 as before. We do not pursue partially-trained amateurs for our main\nresults as results may vary based on the order of training data, but this result allows us to interpret\ncontrastive decoding as a first-order optimization step over the output of a model, highlighting the\nhigh-level behaviors that it learns later on in the course of training. See Table 11 for full results.\n8\nTable 10: On GSM8K, negative prompting out-\nperforms greedy decoding but weakens CD.\nExpert Greedy NP CD CD + NP\n7B 10.7 11.4 14.3 12.7\n13B 17.4 17.5 22.7 20.7\n30B 35.3 36.9 43.1 42.9\n65B 51.0 52.0 56.8 54.7\nTable 11: Early-training checkpoints can be\ngood amateurs, even when late-stage check-\npoints harm performance.\nAmateur Amateur Tokens GSM8K\n7B 130B 57.0\n7B 300B 56.8\n7B 1.3T 49.9\n5 R ELATED WORK\nSteering methods for reasoning. Other works more explicitly model the error distribution of\nreasoning steps and use this to steer decoding. For example GRACE (Khalifa et al., 2023) uses a\ncontrastive loss to train an external step-level discriminator, which it then uses to select between\ncandidate steps sampled from a base model. Using the interpretation of contrastive decoding as\nmutual distinguishability between amateur and expert, we see that our method is close to FUDGE\n(Yang & Klein, 2021) where the binary predictor is an estimate of the probability that the generated\ntoken has come from the expert rather than the amateur.\nPrompting Methods for Reasoning. There are many recent prompting methods to improve lan-\nguage model reasoning; see Qiao et al. (2023) for a survey. We perform our experiments with\nchain-of-thought prompting (Wei et al., 2023).\nSampling methods Several decoding methods exist to improve the quality of generations from\nlarge language models. For open-ended generation, truncated sampling schemes like top-k sampling\n(Fan et al., 2018), nucleus sampling (Holtzman et al., 2020) and typical sampling (Meister et al.,\n2023) have been shown to reduce repetition in comparison to greedy decoding and beam search\nwhile producing more coherent generations than standard temperature sampling. However, sampling\ncan still introduce errors into logical chains, and so greedy decoding is used to more effectively solve\nreasoning tasks. (Wei et al., 2023; Anil et al., 2023)\nContrastive Generation Methods. Our formulation’s objective can be interpreted as a special\ncase of DExperts (Liu et al., 2021), using the larger model as both an expert and base LM prior.\nYona et al. (2023) identify model biases with Contrastive Input Decoding, a contrastive-decoding-\nstyle technique similar to negative prompting that operates on perturbed text inputs.\nConcurrently to our work, Chuang et al. (2023) propose DoLA, which improves factuality and rea-\nsoning through contrastive decoding between the predictions of later layers and earlier layers in a\nlanguage model. We study a wider array of reasoning tasks and demonstrate that a 7B amateur is\ntoo large, finding greater gains in reasoning just by scaling down the amateur to 1.5B parameters.\nOur paper differentiates itself from Li et al. (2022), which initially proposed Contrastive Decoding,\nin several ways: by testing on standard reasoning benchmarks, by our exploration of β as a hyper-\nparameter, by ablations with various types of amateurs, and by a careful analysis of the combination\nof Contrastive Decoding with chain-of-thought prompting and self-consistency.\n6 L IMITATIONS\nOur investigation is also limited mainly to the LLaMA family of models. While the method contin-\nues to provide benefit to larger LLaMA models, further work is required to definitively establish the\neffect of contrastive decoding on larger, tuned models.\n7 C ONCLUSION\nOur study shows that contrastive decoding can improve chain-of-thought reasoning in large lan-\nguage models. While challenges like factual recall remain, this strengthens the case for contrastive\ndecoding as a simple, general-purpose method to elicit more desirable behavior from large language\nmodels.\n9\nREPRODUCIBILITY STATEMENT\nThe training process and model architecture for the 1.5B-parameter LLaMA model used as the ama-\nteur in several results is publicly available, but the weights are not, which limits public reproducibil-\nity of results relying on that model. The results on FLAN-T5, as well as the negative-prompting\nstudy and examination of 7B-LLaMA as an amateur, are all built on entirely open-source models\nand data.\nREFERENCES\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, and Zhifeng Chen et al. Palm 2 technical report,\n2023.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about\nphysical commonsense in natural language, 2019.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola:\nDecoding by contrasting layers improves factuality in large language models, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-\nlat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V . Le, and Jason Wei. Scaling instruction-finetuned language\nmodels, 2022.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge,\n2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: A\ncontinuous effort to measure large language models’ reasoning performance, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies, 2021.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning,\n2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021b.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration, 2020.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension, 2017.\n10\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels, 2020.\nMuhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, and Lu Wang.\nDiscriminator-guided multi-step reasoning with language models, 2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization,\n2022.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-\neration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 158–167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:\n10.18653/v1/P17-1015.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. Dexperts: Decoding-time controlled text generation with experts and anti-experts,\n2021.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cotterell. Locally typical sampling, 2023.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers, 2021.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering, 2018.\nOpenAI. Gpt-4 technical report, 2023.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\nword problems?, 2021.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei\nHuang, and Huajun Chen. Reasoning with language model prompting: A survey, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\nsarial winograd schema challenge at scale, 2019.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Common-\nsense reasoning about social interactions, 2019.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge, 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.\nTowards understanding chain-of-thought prompting: An empirical study of what matters, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n2023b.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc\nLe, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models,\n2023.\n11\nKevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators. In Pro-\nceedings of the 2021 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies. Association for Computational Linguistics,\n2021. doi: 10.18653/v1/2021.naacl-main.276.\nGal Yona, Or Honovich, Itay Laish, and Roee Aharoni. Surfacing biases in large language models\nusing contrastive input decoding, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence?, 2019.\n12\nA A PPENDIX\nA.1 C ODE IMPLEMENTATION\nWe include PyTorch implementations of contrastive decoding in Algorithm 1 and Algorithm 2\nAlgorithm 1: Original formulation\n# expert logits - unnormalized scores from the expert model\n# amateur logits - unnormalized scores from the amateur model\n# amateur temp - temperature to normalize amateur distribution\n# alpha - masking threshold\nexpert probs = softmax(expert logits, dim=-1)\namateur probs = softmax(amateur logits / amateur temp, dim=-1)\ncutoff = alpha*expert probs.max(dim=-1, keepdim=True).values\ndiffs = log(expert probs) - log(amateur probs)\ncd logits = diffs.masked fill(expert probs < cutoff, -float(’inf’))\nAlgorithm 2: Our formulation\n# expert logits - unnormalized scores from the expert model\n# amateur logits - unnormalized scores from the amateur model\n# alpha - masking threshold\n# beta - expert-amateur tradeoff parameter\ncutoff = log(alpha) + expert logits.max(dim=-1, keepdim=True).values\ndiffs = (1 + beta)*expert logits - beta*amateur logits\ncd logits = diffs.masked fill(expert logits < cutoff, -float(’inf’))\n13\nA.2 E QUIVALENCE - MASKING\nFor the sake of completeness, we show the equivalency between the two masking schemes. We\nrestrict our consideration to the generation of a single token. Let\n- V be the vocabulary size of each model\n- {ei}V\ni=1 be the logits produced by the expert model\n- {ai}V\ni=1 be the logits produced by the amateur model\n- Ne and Na be the normalization terms of the softmax function; for example, Ne = PV\nj=1 exp(ei)\nThe probability that the expert assigns to token i after the softmax is by definition pe(i) =exp(si)\nNe\nNow consider any token that is masked out. We have that pe(i) < α∗ pe(imax), where imax is the\nthe token that maximizes pe.\nBecause ex and log x are both strictly increasing functions, we obtain:\nsi − log Ne < log α + smax − log Ne\nsi < log α + smax\nThese two conditions are equivalent, and so we can mask tokens by thresholding their logits against\nlog α + smax.\nFurther, let us introduce an expert temperature parameterτ ∈ (0, ∞) that scales the logits arbitrarily.\nThen we obtain a new set of logits ci = si\nτ\nBy then substituting ατ = α1/τ , we obtain the same mask. Thus the mask is a function that depends\nonly on the quantity τ log α, or equivalently α exp(τ). As we later show, we can fix τ = 1 by\nintroducing a new hyperparameter β. So if we fix τ = 1, then our mask depends solely on α.\nLetting p(i)\ne correspond to the post-softmax probability that the expert assigns to token i, we have\nshown that the valid set produced by our method is the same as in the original:\nVvalid =\n\u001a\nj ∈ V, p(j)\ne ≥ 1\nα max\nk∈V\np(k)\ne\n\u001b\nA.3 E QUIVALENCE - LOGIT COMBINATION\nTo be concise, first define q(i) = pe(i)\npa(i) be the ratio of the probability assigned by the expert model\nover the probability from the amateur model, both on token i.\nFurther, let si denote the value of the logit that CD assigns to token i. Then\nsi = (1 +β) logpe(i) − β log pa(i)\nexp(si) =pe(i)q(i)β\npcd(i) ∝ pe(i)q(i)β\nExpanding this, we obtain:\npcd(i) ∝ exp ((1 +β)ei − βai)\nwhich is equivalent to simply linearly combining the expert and amateur logits.\nWhen we introduce temperatures, the equation becomes\n14\npcd(i) ∝ exp\n\u00121 +β\nτe\nei − β\nτa\nai\n\u0013\nmask(i) =f(τe log α)\nWhen sampling, the temperature with which we sample from the CD logits is introduced as τout\npcd(i) ∝ exp\n\u0012 1\nτout\n\u00121 +β\nτe\nei − β\nτa\nai\n\u0013\u0013\nThese four parameters τout, τe, τa and β combine into only two coefficients – one forei and one for\nei.\npcd(i) ∝ exp (κeei − κaai)\nWe now fix τa and τe to 1. We can obtain almost the same range of values with the τout, β formu-\nlation as with the κe, κa formulation. The only exception is the case for which κe = κa, which\nwe exclude after finding that weighing expert and amateur equally gives worse results than down-\nweighing the amateur. Despite this exception, we prefer theβ, τout formulation because it decouples\nthe scale of the final logits from the β parameter: in expectation β does not scale the logits up or\ndown, and so when sampling τ will affect generation diversity and β will affect the expert-amateur\ntradeoff.\n15\nB M ULTIPLE -CHOICE BETA SWEEP RESULTS\nHere we include the plots for all beta sweeps through the multiple-choice tasks.\n16\nC FLOP ESTIMATES\nWe follow Kaplan et al. (2020) and estimate the number of flops for one forward pass of a Trans-\nformer model with N non-embedding parameters as roughly 2N. For the purposes of this analysis,\nwe drop the negligibly small2nlayernctxdattn term. This allows us to consider the cost of generating\none token to be constant regardless of the context length.\nWith this approximation, we find that contrastive decoding adds 1.5\n65.2 ≈ 2.30% to the number of\nFLOPs per forward pass for a 65B expert model. To ensure a fair comparison, we also include the\nsmall increase in generation lengths induced by CD (see Table 6), bringing its total percent increase\nup to 3.25%.\n17",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.9100732207298279
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7317796349525452
    },
    {
      "name": "Computer science",
      "score": 0.7284369468688965
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5517241954803467
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.5436742305755615
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.539138913154602
    },
    {
      "name": "Copying",
      "score": 0.5283364057540894
    },
    {
      "name": "Natural language processing",
      "score": 0.4811226427555084
    },
    {
      "name": "List decoding",
      "score": 0.4120776951313019
    },
    {
      "name": "Speech recognition",
      "score": 0.32100480794906616
    },
    {
      "name": "Algorithm",
      "score": 0.24928945302963257
    },
    {
      "name": "Concatenated error correction code",
      "score": 0.09657290577888489
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Block code",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}