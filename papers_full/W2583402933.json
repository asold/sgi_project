{
    "title": "emLam -- a Hungarian Language Modeling baseline",
    "url": "https://openalex.org/W2583402933",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5043754820",
            "name": "Dávid Márk Nemeskey",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1591801644"
    ],
    "abstract": "This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced.",
    "full_text": "emLam – a Hungarian Language Modeling baseline\nDávid Márk Nemeskey\nInstitute for Computer Science and Control\nHungarian Academy of Sciences\nnemeskeyd@gmail.com\nAbstract. This paper aims to make up for the lack of documented baselines for\nHungarian language modeling. Various approaches are evaluated on three pub-\nlicly available Hungarian corpora. Perplexity values comparable to models of\nsimilar-sized English corpora are reported. A new, freely downloadable Hungar-\nian benchmark corpus is introduced.\n1 Introduction\nLanguage modeling (LM) is an integral part of several NLP applications, such as speech\nrecognition, optical character recognition and machine translation. It has been shown\nthat the quality of the LM has a significant effect on the performance of these systems [5,\n7]. Accordingly, evaluating language modeling techniques is a crucial part of research.\nFor English, a thorough benchmark of n-gram models was carried out by Goodman\n[10], while more recent papers report results for advanced models [20, 8]. Lately, the\nOne Billion Word Benchmark corpus (1B) [8] was published for the sole reason of\nmeasuring progress in statistical language modeling.\nThe last decade saw dramatic advances in the field of language modeling. Training\ncorpora grew from a few million words (e.g. the Brown corpus) to gigaword, such as 1B,\nwhile vocabulary size increased from a few 10k to several hundred thousands. Neural\nnetworks [3, 21, 19] overtook n-grams as the language model of choice. State-of-the-\nart LSTMp networks achieve up to 55% reductions in perplexity compared to 5-gram\nmodels [14].\nSurprisingly, these developments left few traces in the Hungarian NLP literature.\nAside from an interesting line of work on morphological modeling for speech recogni-\ntion [23, 18], no study is known to the author that addresses issues of Hungarian language\nmodeling. While quality works have been published in related fields, language model\nperformance is often not reported, or is not competitive: e.g. in their otherwise state-of-\nthe-art system, Tarján et al. [28] use a 3-gram model that achieves a perplexity of 4001\non the test set — a far cry from the numbers reported in [8] and here.\nIn this paper, we mean to fill this gap in two ways. First, we report baselines for var-\nious language modeling methods on three publicly available Hungarian corpora. Hun-\ngarian poses a challenge to word-based LM because of its agglutinative nature. The\nproliferation of word forms inflates the vocabulary and decreases the number of con-\ntexts a word form is seen during training, making the data sparsity problem much more\n1 Personal communication with the author.\n2 XIII. Magyar Számítógépes Nyelvészeti Konferencia\npronounced than it is for English. This makes it especially interesting to see how the\nperformance of the tested methods translate to Hungarian.\nSecond, we present a version of the Hungarian Webcorpus [11] that can be used as\na benchmark for LM models. Our motivation was to create the Hungarian equivalent of\nthe One Billion Word Benchmark corpus for English: a freely available data set that is\nlarge enough to enable the building of high-quality LMs, yet small enough not to pose a\nbarrier to entry to researchers. We hope that the availability of the corpus will facilitate\nresearch into newer and better LM techniques for Hungarian.\nThe software components required to reproduce this work, as well as the benchmark\ncorpus, comprise theemLam module2 of e-magyar.hu [30]. The scripts have been re-\nleased as free software under the MIT license, and can be downloaded from theemLam\nrepository3.\nThe rest of the paper is organized as follows. The benchmark corpora, as well as our\nsolution to the data sparsity problem is described in Section2. In Section3 we formally\ndefine the language modeling task and introduce the methods evaluated. Results are\npresented in Section4. Finally, Section5 contains our conclusions and ideas left for\nfuture work.\n2 The Hungarian Datasets\nWe selected three publicly available Hungarian corpora for benchmarking. The corpora\nare of various sizes and domains, which enabled us to evaluate both small- and large-\nvocabulary LM configurations. The corpus sizes roughly correspond to those of the\nEnglish corpora commonly used for LM benchmarks, making a comparison between\nthe two languages easier.\nThe Szeged Treebank [31] is the largest manually annotated corpus of Hungarian.\nThe treebank consists of CoNLL-style tsv files; we used a version in which the mor-\nphological features had been converted to KR codes to keep in line with the automatic\ntoolchain described below. At around 1.5 million tokens, it is similar in size to the Penn\nTreebank [16], allowing us a direct comparison of small-vocabulary LM techniques.\nThe filtered version of the Hungarian Webcorpus [11] is a semi-gigaword corpus\nat 589m tokens. It consists of webpages downloaded from the.hudomain that contain\nan “acceptable number of spelling mistakes”. The downloadable corpus is already to-\nkenized; we further processed it by performing lemmatization, morphological analysis\nand disambiguation with Hunmorph [29]: ocamorphfor the former two andhunlexfor\nthe latter.\nThe Hungarian Gigaword Corpus (MNSZ2) [25] is the largest public Hungarian\ncorpus. At around 1G tokens, it is comparable in size to the English 1B corpus. We\npreprocessed the raw text with the same tools as above.\nWe decided to use the ‘old’hun*tools because at the time of writing, thee-magyar\ntoolchain was not yet production ready, and the version of the Szeged corpus that uses\nthe new universal POS tags still contained conversion errors. Therefore, the results pub-\nlished here might be slightly different from what one can attain by running the scripts\n2 http://e-magyar.hu/hu/textmodules/emlam\n3 http://github.com/dlt-rilmta/emLam\nSzeged, 2017. január 26-27. 3\nin theemLamrepository, should the issues above be addressed. However, any such dif-\nferences will be, most likely, insignificant.\n2.1 Preprocessing\nAs mentioned before, the main challenge of modeling an agglutinative language is the\nnumber of distinct word forms. The solution that works well for English — putting all\nword forms into the vocabulary — is not reasonable: on one hand, the vocabulary size\nwould explode (see Table1); on the other, there is a good chance the training set does\nnot contain all possible word forms in the language.\nThe most common solution in the literature is to break up the words into smaller\nsegments [12, 2, 4]. The two main directions are statistical and morphological word\nsegmentation. While good results have been reported with the former, we opted for the\nlatter: not only is it linguistically more motivated, it also ensures that the tokens we end\nup with are meaningful, making the LM easier to debug.\nWe ran the aforementioned pipeline on all words in the corpus, and split all inflec-\ntional prefixes (as well as some derivational ones, such as<COMPAR>, <SUPERLAT>) into\nseparate tokens. Only inflections marked by the KR code are included; the default zero\nmorphemes (the nominative case marker and the present-tense third person singular for\nverbs) are not. A few examples:\njelmondatával ! jelmondat <POSS> <CAS<INS>>\nakartak ! akar <PAST> <PLUR>\nOne could say that by normalizing the text like this, we ”deglutenized” it; therefore, the\nresulting variants of the corpora shall be referred to as ”gluten-free” (GLF) from now\non.\nThe full preprocessing pipeline is as follows:\n1. Tokenization and normalization. The text was lowercased, converted toutf-8and\nand deglutenized\n2. (Webcorpus only) Duplicates sentences were removed, resulting in a 32.5% reduc-\ntion in corpus size.\n3. Tokens below a certain frequency count were converted into<unk> tokens. The\nword distribution proved different from English: with the same threshold as in the\n1B corpus (3), much more distinct tokens types remained. To be able to test LMs\nwith a vocabulary size comparable to 1B, we worked with different thresholds for\nthe two gigaword corpora: Webcorpus was cut at 5 words, MNSZ2 at 10. An ad-\nditional thresholding level was introduced at 30 (50) tokens to make RNN training\ntractable.\n4. Sentence order was randomized\n5. The data was divided into train, development and test sets; 90%–5%–5% respec-\ntively.\nTable 1 lists the main attributes of the datasets created from the three corpora. Where\nnot explicitly marked, the default count threshold (3) is used. The corresponding En-\nglish corpora are included for comparison. It is clear from comparing the raw and GLF\n4 XIII. Magyar Számítógépes Nyelvészeti Konferencia\ndatasets that deglutenization indeed decreases the size of the vocabulary and the number\nof OOVs by about 50%. Although not shown in the table, this reduction ratio remains\nconsistent among the various thresholding levels.\nAlso apparent is that, compared to the English corpora, the number of unique tokens\nis much bigger even in the default Hungarian GLF datasets. Preliminary inquiry into the\ndata revealed that three phenomena account for the majority of the token types between\nthe 3 and 30 (50) count marks: compound nouns, productive derivations and named en-\ntities (with mistyped words coming in at fourth place). Since neither the Szeged corpus,\nnor (consequently) the available morphological disambiguators take compounding and\nderivation into account, no immediate solution was available for tackling these issues.\nTherefore, we decided to circumvent the problem by introducing the higher frequency\nthresholds and concentrating on the problem of inflections in this study.\nDataset Sentences Tokens Vocabulary <unk>s Analysis\nSzeged 81 967 1 504 801 38 218 125 642 manualSzeged GLF 2 016 972 23 776 55 067\nWebcorpus\n26 235 007\n481 392 824 1 971 322 5 750 742\nautomaticWebcorpus GLF 683 643 265 960 588 3 519 326\nWebcorpus GLF-5 ” 625 283 4 647 706\nWebcorpus GLF-30 ” 185 338 9 393 015\nMNSZ2\n44 329 309\n624 830 138 2 988 629 11 614 583\nautomaticMNSZ2 GLF 852 232 675 1 714 844 5 729 509\nMNSZ2 GLF-10 ” 630 863 10 845 301\nMNSZ2 GLF-50 ” 197 542 19 547 859\nPTB 49 199 1 134 978 10 000 manual\n1B 30 607 716 829 250 940 793 471 automatic\nTable 1.Comparison of the three Hungarian corpora\nThe preprocessing scripts are available in the emLam repository.\n2.2 The Benchmark Corpus\nOf the three corpora above, the Hungarian Webcorpus is the only one that is freely\ndownloadable and available under a share-alike license (Open Content). Therefore, we\ndecided to make not only the scripts, but the preprocessed corpus as well, similarly\navailable for researchers.\nThe corpus can be downloaded as a list of tab-separated files. The three columns\nare the word, lemma and disambiguated morphological features. A unigram (word and\nlemma) frequency dictionary is also attached, to help create count-thresholded versions.\nThe corpus is available under the Creative Commons Share-alike (CC SA) license.\nSuch a corpus could facilitate language modeling research in two ways. First, any\nresult published using the corpus is easily reproducible. Second, the fact that it has been\nSzeged, 2017. január 26-27. 5\npreprocessed similarly to the English 1B corpus, makes comparisons such as those in\nthis paper possible and meaningful.\n3 Language Modeling\nThe task of (statistical) language modeling is to assign a probability to a word sequence\nS = w1; :::; wN . In this paper, we only consider sentences, but other choices (para-\ngraphs, documents, etc.) are also common. Furthermore, we only concern ourselves with\ngenerative models, where the probability of a word does not depend on subsequent to-\nkens. The probability ofS can then be decomposed using the chain rule, as\nP(S) =P(w1; :::; wN ) =\nN∑\ni=1\nP(wijw1; :::; wi\u00001): (1)\nThe condition (w1; :::; wi\u00001) is called thecontext of wi. One of the challenges of\nlanguage modeling is that the number of possible contexts is infinite, while the training\nset is not. Because of this, the full context is rarely used; LMs approximate it and deal\nwith the data sparsity problem in various ways.\nIn the following, we introduce some of the state-of-the-art methods in discrete and\ncontinuous language modeling.\n3.1 N-grams\nN-gram models work under the Markov assumption, i.e. the current word only depends\non n \u0000 1 preceding words:\nP(wijw1; :::; wi\u00001) \u0019 P(wijwi\u0000n+1; :::; wi\u00001): (2)\nAn n-gram model is a collection of such conditional probabilities.\nThe data sparsity problem is addressed by smoothing the probability estimation in\ntwo ways:backoff models recursively fall back to coarser (n\u00001; n\u00002; :::-gram) models\nwhen the context of a word was not seen during training, whileinterpolated models\nalways incorporate the lower orders into the probability estimation.\nA variety of smoothing models have been proposed over the years; we chose modi-\nfied Kneser-Ney (KN) [15, 9] as our baseline, since it reportedly outperforms all other\nn-gram models [10]. We used the implementation in the SRILM [27] library, and tested\ntwo configurations: a pruned backoff (the default)4 and, similar to [8], an unpruned in-\nterpolated model5. All datasets described in Table1 were evaluated; in addition, we also\ntested a GLF POS model, where lemmas were replaced with their respective POS tags.\n4 -kndiscount\n5 -kndiscount -gt1min 1 -gt2min 1 -gt3min 1 -gt4min 1 -gt5min 1 -interpolate1\n-interpolate2 -interpolate3 -interpolate4 -interpolate5\n6 XIII. Magyar Számítógépes Nyelvészeti Konferencia\n3.2 Class-based n-grams\nClass-based models exploit the fact that certain words are similar to others w.r.t. meaning\nor syntactic function. By clustering words into classesC according to these features, a\nclass-based n-gram model estimates the probability of the next word as\nP(wijw1; :::; wi\u00001; c1; :::; ci\u00001) \u0019 P(wijci)P(ci\u0000n+1; :::; ci\u00001): (3)\nThis is a Hidden Markov Model (HMM), where the classes are the hidden states and\nthe words are the observations. The techniques proposed for class assignment fall into\ntwo categories: statistical clustering [6, 17] and using pre-existing linguistic information\nsuch as POS tags [24]. In this paper, we chose the latter, as a full morphological analysis\nwas already available as a by-product of deglutenization.\nIt is generally agreed that class-based models perform poorly by themselves, but\nimprove word-based models when interpolated with them.\n3.3 RNN\nIn the last few years, Recurrent Neural Networks (RNN) have become the mainstream\nin language modeling research [19, 20, 32, 14]. In particular, LSTM [13] models rep-\nresent the state-of-the-art on the 1B dataset [14]. The power of RNNs come from two\nsources: first, words are projected into a continuous vector space, thereby alleviating\nthe sparsity issue; and second, their ability to encode the whole context into their state,\nthereby ”remembering” much further back than n-grams. The downside is that it can\ntake weeks to train an RNN, whereas an n-gram model can be computed in a few hours.\nWe ran two RNN baselines:\n1. the Medium regularized LSTM setup in [32]. We used the implementation6 in Ten-\nsorflow [1]\n2. LSTM-512-512, the smallest configuration described in [14], which uses LSTMs\nwith a projection layer [26]. The model was reimplemented in Tensorflow, and is\navailable from the emLam repository.\nDue to time and resource constraints, the first baseline was only run on the Szeged\ncorpus, and the second only on the smallest, GLF-30 (50) datasets.\n3.4 Language Model Evaluation\nThe standard metric of language model quality isperplexity (PPL), which measures how\nwell the model predicts the text data. Intuitively, it shows how many options the LM\nconsiders for each word; the lower the better. The perplexity of the sequencew1; :::; wN\nis computed as\nP P L= 2H = 2\n∑N\ni=1 \u0000 1\nN log2 P(wijw1;:::;wi\u00001); (4)\nwhere H is the cross-entropy.\n6 https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb\nSzeged, 2017. január 26-27. 7\nLanguage models typically perform worse when tested on a different corpus, due to\nthe differences in vocabulary, word distribution, style, etc. To see how significant this\neffect is, the models were not only evaluated on the test split of their training corpus,\nbut on the other two corpora as well.\n4 Evaluation\nThe results achieved by the n-gram models are reported in Table2–5. Table2 lists the\nperplexities achieved by KN 5-grams of various kinds; the odd one out is POS GLF,\nwhere the limited vocabulary enabled us to create up to 9-gram models. For MNSZ2,\nthe reported score is from the 7-gram model, which outperformed 8- and 9-grams.\nSimilar results reported by others on the PTB and 1B are included for comparison.\nA glance at the table shows that while word-based 5-grams performed much worse than\ntheir counterparts in English, the GLF-based models achieved similar scores.\nWhile the perplexities of GLF models on Webcorpus and MNSZ2 are comparatively\nclose, the perplexities of the word models are about 50% higher on Webcorpus. Finding\nthe cause of this discrepancy requires further research. Two possible candidates are data\nsparsity (at the same vocabulary size, Webcorpus is 25% smaller) and a difference in\nthe distribution of inflection configurations.\nCorpus Threshold Word GLF Full POS POS GLF\nSzeged 3 262.77 123.66 35.20 22.90\nWebcorpus\n1 N/A N/A 10.21 6.05\n5 328.22 67.90 N/A N/A\n30 259.79 63.44 N/A N/A\nMNSZ2\n1 N/A N/A 11.88 6.36\n10 233.52 61.92 N/A N/A\n50 174.65 55.53 N/A N/A\nPTB [22] N/A 141.2\n1B [8] 3 90\nTable 2.5-gram (9 for POS GLF) KN test results (PPL)\nTable 3 shows the best n-gram perplexities achieved by GLF models. It can be seen\nthat interpolated, unpruned models perform much better than backoff models.\nMeasuring class-based model performance led to surprising results. As mentioned\nearlier, the general consensus is that interpolating class- and word-based LMs benefit\nthe performance of the latter; however, our findings (Table4) did not confirm this. The\nclass-based model could only improve on the unigram model, and failed to do so for the\nhigher orders. The most likely explanation is that as the size of the vocabulary grows\nlarger, the emission entropy increases, which is mirrored by the perplexity. This would\nexplain why class-based n-grams seem to work on small corpora, such as the PTB, but\nnot on MNSZ2.\n8 XIII. Magyar Számítógépes Nyelvészeti Konferencia\nModel pruned backoff unpruned interpolated\nSzeged GLF 123.66 116.32\nWebcorpus GLF-5 67.90 58.62\nWebcorpus GLF-30 63.44 54.42\nMNSZ2 GLF-10 61.92 51.22\nMNSZ2 GLF-50 55.53 46.24\nPTB [22] 141.2 N/A\n1B [8] 90 67.6\nTable 3.The best KN 5-gram results\nAnother point of interest is the diminishing returns of PPL reductions as the n-gram\norders grow. While we have not experimented with 6-grams or higher orders, it seems\nprobable that performance of GLF models would peak at 6- or 7-grams on MNSZ2 (and\nWebcorpus). For word-based models, this saturation point arrives much earlier: while\nnot reported in the table, the perplexity difference between 4- and 5-gram models is only\n1-2 point. This implies that GLF models are less affected by data sparsity.\nModel GLF-10 POS !GLF-10 GLF-50 POS !GLF-50\n1-gram 2110 653.67 2175 568.53\n2-gram 127.17 327.74 115.38 285.72\n3-gram 84.81 294.20 73.31 256.60\n4-gram 66.06 274.70 59.41 239.64\n5-gram 61.92 261.79 55.53 228.40\nTable 4.Class (POS)-based model performance on the MNSZ2\nIt is a well-known fact that the performance of LMs degrade substantially when\nthey are not evaluated on the corpus they were trained on. This effect is clearly visible\nin Table5. It is also evident, however, that GLF datasets suffer from this problem to a\nmuch lesser extent: while the perplexity more than doubled for the word-based MNSZ2\nLMs, it only increased by 50–60% for GLF models. A similar effect can be observed\nbetween the full and GLF POS models.\nInterestingly, the Webcorpus word models exhibit the smallest perplexity increase\nof 10-15%. Contrasting this result with Table2 seems to suggest that there exists a\ntrade-off between predictive power and universality. However, it is worth noting that\nthe performance of these word models still lags well behind that of GLF models.\nFinally, Table 6 reports the perplexities achieved by the RNN models. Two con-\nclusions can be drawn from the numbers. First, in line with what has been reported for\nEnglish by many authors, RNNs clearly outperform even the best n-gram models. Sec-\nond, the numbers are similar to those reported in the original papers for English. This,\nSzeged, 2017. január 26-27. 9\nModel Evaluated on 1 tokens 5 (10) tokens 30 (50) tokens\nWebcorpus word MNSZ2 377.88 291.98\nMNSZ2 word Webcorpus 566.60 397.13\nWebcorpus GLF MNSZ2 109.71 94.59\nMNSZ2 GLF Webcorpus 92.51 84.91\nWebcorpus Full POS MNSZ2 16.14\nMNSZ2 Full POS Webcorpus 16.49\nWebcorpus POS GLF MNSZ2 8.35\nMNSZ2 POS GLF Webcorpus 7.73\nTable 5.Cross-validation results between Webcorpus and MNSZ2 with various thresh-\nolds.\ntogether with similar observations above for n-grams, proves that once the ”curse of ag-\nglutination” is dealt with, a GLF Hungarian is no more difficult to model than English.\nModel Dataset Perplexity\nMedium regularized Szeged GLF 35.20\nLSTM-512-512 Webcorpus GLF-30 40.46\nLSTM-512-512 MNSZ2 GLF-50 38.78\nMedium regularized [32] PTB 82.07\nLSTM-512-512 [14] 1B 54.1\nTable 6.LSTM model performance\n5 Conclusion\nThis work contributes to Hungarian language modeling in two ways. First, we reported\nstate-of-the-art LM baselines for three Hungarian corpora, from million to gigaword\nsize. We found that raw, word-level LMs performed worse than they do for English, but\nwhen the text was split into lemmas and inflectional affixes (the ”gluten-free” format),\nresults were comparable to those reported on similar-sized English corpora.\nSecond, we introduced a benchmark corpus for language modeling. To our knowl-\nedge, this is the first such dataset for Hungarian. This specially prepared version of\nthe Hungarian Webcorpus is freely available, allowing researchers to easily and repro-\nducibly experiment with new language modeling techniques. It is comparable in size to\nthe One Billion Word Benchmark corpus of English, making comparisons between the\ntwo languages easier.\n10 XIII. Magyar Számítógépes Nyelvészeti Konferencia\n5.1 Future Work\nWhile the methods reported here can be called state-of-the-art, many similarly effective\nmodeling approaches are missing. Evaluating them could provide additional insight into\nhow Hungarian ”works” or how Hungarian and English should be modeled differently.\nUnderstanding the unusual behaviour of word models on Webcorpus also calls for fur-\nther inquiry into language and corpus structure.\nThe performance of the models here was measured in isolation. Putting them into use\n(maybe with some adaptation) in NLP applications such as ASR or ML could answer\nthe question of whether the reduction in perplexity translates to similar reductions in\nWER or BLEU.\nThe most glaring problem touched upon, but not addressed, in this paper, is the\neffect of compounding and derivation on vocabulary size. A way to reduce the number of\nwords could be a more thorough deglutenization algorithm, which would split compound\nwords into their parts and strip productive derivational suffixes, while leaving frozen\nones such asház\u0001as\u0001ságuntouched. This could indeed be a case when a gluten free diet\ndoes make one slimmer.\nAcknowledgements\nThis work is part of thee-magyarframework and was supported by the Research Infras-\ntructure Development Grant, Category 2, 2015 of the Hungarian Academy of Sciences.\nReferences\n[1] Martın Abadi et al. “TensorFlow: Large-scale machine learning on heterogeneous\nsystems, 2015”. In:Software available from tensorflow. org1 (2015).\n[2] Mohamed Afify, Ruhi Sarikaya, Hong-Kwang Jeff Kuo, Laurent Besacier, and\nYuqing Gao. “On the use of morphological analysis for dialectal Arabic speech\nrecognition.” In:INTERSPEECH. 2006, pp. 277–280.\n[3] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. “A\nNeural Probabilistic Language Model”. In:Journal of Machine Learning Re-\nsearch 3 (2003), pp. 1137–1155. URL:http://www.jmlr.org/papers/\nv3/bengio03a.html.\n[4] Jan A Botha and Phil Blunsom. “Compositional Morphology for Word Repre-\nsentations and Language Modelling”. In:ICML. 2014, pp. 1899–1907.\n[5] Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean.\n“Large Language Models in Machine Translation”. In:Proceedings of the 2007\nJoint Conference on Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning (EMNLP-CoNLL). Prague, Czech\nRepublic: Association for Computational Linguistics, June 2007, pp. 858–867.\nURL: http://www.aclweb.org/anthology/D/D07/D07-1090.\n[6] P.F. Brown, V.J. Della Pietra, P.V. de Souza, J.C. Lai, and R.L. Mercer. “Class–\nbased n–gram models of natural language”. In:Computational Linguistics18.4\n(1992), pp. 467–480.\nSzeged, 2017. január 26-27. 11\n[7] Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, and Shankar Kumar.\nLarge Scale Language Modeling in Automatic Speech Recognition. Tech. rep.\nGoogle, 2012. URL:https://research.google.com/pubs/pub40491.\nhtml.\n[8] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants,\nPhillipp Koehn, and Tony Robinson. “One billion word benchmark for measuring\nprogress in statistical language modeling”. In:INTERSPEECH 2014, 15th Annual\nConference of the International Speech Communication Association, Singapore,\nSeptember 14-18, 2014. 2014, pp. 2635–2639.\n[9] Stanley F Chen and Joshua Goodman. An empirical study of smoothing tech-\nniques for language modeling. Tech. rep. TR-10-98. Cambridge, MA: Computer\nScience Group, Harvard University, Aug. 1998, p. 63.\n[10] Joshua T. Goodman. “A bit of progress in language modeling”. In:Computer\nSpeech & Language15.4 (2001), pp. 403–434.\n[11] Péter Halácsy, András Kornai, László Németh, András Rung, István Szakadát,\nand Viktor Trón. “Creating open language resources for Hungarian”. In:Pro-\nceedings of the Fourth International Conference on Language Resources and\nEvaluation (LREC 2004). ELRA, 2004, pp. 203–210.\n[12] Teemu Hirsimäki, Mathias Creutz, Vesa Siivola, and Mikko Kurimo. “Mor-\nphologically Motivated Language Models in Speech Recognition”. In:Proceed-\nings of AKRR’05, International and Interdisciplinary Conference on Adaptive\nKnowledge Representation and Reasoning. Espoo, Finland: Helsinki Univer-\nsity of Technology, Laboratory of Computer and Information Science, June\n2005, pp. 121–126. URL:http://www.cis.hut.fi/AKRR05/papers/\nakrr05tuulos.pdf.\n[13] Sepp Hochreiter and Jürgen Schmidhuber. “Long Short-Term Memory”. In:Neu-\nral Computation9.8 (Nov. 1997), pp. 1735–1780.\n[14] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui\nWu. “Exploring the limits of language modeling”. In: arXiv preprint\narXiv:1602.02410 (2016).\n[15] Reinhard Kneser and Hermann Ney. “Improved backing-off for m-gram language\nmodeling”. In:International Conference on Acoustics, Speech, and Signal Pro-\ncessing, 1995. ICASSP-95.Vol. 1. IEEE. 1995, pp. 181–184.\n[16] Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. “Building\na Large Annotated Corpus of English: The Penn Treebank”. In:Computational\nLinguistics 19 (1993), pp. 313–330.\n[17] Sven Martin, Jörg Liermann, and Hermann Ney. “Algorithms for bigram and\ntrigram word clustering”. In:Speech communication24.1 (1998), pp. 19–37.\n[18] Péter Mihajlik, Zoltán Tuske, Balázs Tarján, Bottyán Németh, and Tibor Fegyó.\n“Improved recognition of spontaneous Hungarian speech — Morphological and\nacoustic modeling techniques for a less resourced task”. In:IEEE Transactions\non Audio, Speech, and Language Processing18.6 (2010), pp. 1588–1600.\n[19] Tomas Mikolov. “Statistical Language Models Based On Neural Networks”. PhD\nthesis. Faculty of Information Technology, Brno University of Technology, 2012.\n12 XIII. Magyar Számítógépes Nyelvészeti Konferencia\n[20] Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan Cer-\nnockỳ. “Empirical Evaluation and Combination of Advanced Language Model-\ning Techniques.” In:INTERSPEECH. 2011, pp. 605–608.\n[21] Tomáš Mikolov, Anoop Deoras, Daniel Povey, Lukáš Burget, and Jan Černockỳ.\n“Strategies for training large scale neural network language models”. In:Auto-\nmatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on.\nIEEE. 2011, pp. 196–201.\n[22] Tomas Mikolov and Geoffrey Zweig. “Context dependent recurrent neural net-\nwork language model”. In:SLT. 2012, pp. 234–239.\n[23] Bottyán Németh, Péter Mihajlik, Domonkos Tikk, and Viktor Trón. “Statisztikai\nés szabály alapú morfológiai elemzők kombinációja beszédfelismerő alka-\nlmazáshoz”. In:Proceedings of MSZNY 2007. Szegedi Tudományegyetem, Nov.\n2007, pp. 95–105.\n[24] Thomas R Niesler, Edward WD Whittaker, and Philip C Woodland. “Compari-\nson of part-of-speech and automatically derived category-based language mod-\nels for speech recognition”. In:Acoustics, Speech and Signal Processing, 1998.\nProceedings of the 1998 IEEE International Conference on. Vol. 1. IEEE. 1998,\npp. 177–180.\n[25] Csaba Oravecz, Tamás Váradi, and Bálint Sass. “The Hungarian Gigaword Cor-\npus”. In:Proceedings of LREC 2014. 2014.\n[26] Hasim Sak, Andrew W Senior, and Françoise Beaufays. “Long short-term mem-\nory recurrent neural network architectures for large scale acoustic modeling.” In:\nINTERSPEECH. 2014, pp. 338–342.\n[27] Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. “SRILM at sixteen:\nUpdate and outlook”. In:Proceedings of IEEE Automatic Speech Recognition and\nUnderstanding Workshop. Vol. 5. 2011.\n[28] Balázs Tarján, Ádám Varga, Zoltán Tobler, György Szaszák, Tibor Fegyó, Csaba\nBordás, and Péter Mihajlik. “Magyar nyelvű, élő közéleti- és hírműsorok gépi\nfeliratozása”. In:Proc. MSZNY 2016. Szegedi Tudományegyetem, 2016, pp. 89–\n99.\n[29] Viktor Trón, Gyögy Gyepesi, Péter Halácsky, András Kornai, László Németh,\nand Dániel Varga. “Hunmorph: Open Source Word Analysis”. In:Proceedings\nof the ACL Workshop on Software. Ann Arbor, Michigan: Association for Com-\nputational Linguistics, 2005, pp. 77–85.\n[30] Tamás Váradi et al. “e-magyar: digitális nyelvfeldolgozó rendszer”. In:XIII.\nMagyar Számı́ tógépes Nyelvészeti Konferencia (MSZNY2017). Szeged, 2017,\n(this volume).\n[31] Veronika Vincze, Viktor Varga, Katalin Ilona Simkó, János Zsibrita, Ágos-\nton Nagy, Richárd Farkas, and János Csirik. “Szeged Corpus 2.5: Morpholog-\nical Modifications in a Manually POS-tagged Hungarian Corpus”. In:Proceed-\nings of the Ninth International Conference on Language Resources and Evalua-\ntion (LREC’14). Reykjavik, Iceland: European Language Resources Association\n(ELRA), May 2014. ISBN: 978-2-9517408-8-4.\n[32] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. “Recurrent neural network\nregularization”. In: (2014). arXiv:1409.2329 [cs.NE]."
}