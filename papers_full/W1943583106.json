{
  "title": "A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models",
  "url": "https://openalex.org/W1943583106",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101777591",
      "name": "Shiliang Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101454723",
      "name": "Hui Jiang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5086280874",
      "name": "Mingbin Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101505189",
      "name": "Junfeng Hou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5057227915",
      "name": "Li-Rong Dai",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2271177914",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W2170942820",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2072350286",
    "https://openalex.org/W2032676284",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2169585179",
    "https://openalex.org/W2107878631"
  ],
  "abstract": "In this paper, we propose the new fixed-size ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation. FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words. In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs). Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular RNN-LMs.",
  "full_text": "A Fixed-Size Encoding Method for Variable-Length Sequences\nwith its Application to Neural Network Language Models\nShiliang Zhang1, Hui Jiang2, Mingbin Xu2, Junfeng Hou1, Lirong Dai1\n1National Engineering Laboratory for Speech and Language Information Processing\nUniversity of Science and Technology of China, Hefei, Anhui, China\n2Department of Electrical Engineering and Computer Science\nYork University, 4700 Keele Street, Toronto, Ontario, M3J 1P3, Canada\n{zsl2008,hjf176}@mail.ustc.edu.cn, {hj,xmb}@cse.yorku.ca, lrdai@ustc.edu.cn\nAbstract\nIn this paper, we propose the new ﬁxed-\nsize ordinally-forgetting encoding (FOFE)\nmethod, which can almost uniquely en-\ncode any variable-length sequence of\nwords into a ﬁxed-size representation.\nFOFE can model the word order in a se-\nquence using a simple ordinally-forgetting\nmechanism according to the positions of\nwords. In this work, we have applied\nFOFE to feedforward neural network lan-\nguage models (FNN-LMs). Experimental\nresults have shown that without using any\nrecurrent feedbacks, FOFE based FNN-\nLMs can signiﬁcantly outperform not only\nthe standard ﬁxed-input FNN-LMs but\nalso the popular recurrent neural network\n(RNN) LMs.\n1 Introduction\nLanguage models play an important role in many\napplications like speech recognition, machine\ntranslation, information retrieval and nature lan-\nguage understanding. Traditionally, the back-off\nn-gram models [Katz, 1987, Kneser, 1995] are\nthe standard approach to language modeling. Re-\ncently, neural networks have been successfully\napplied to language modeling and have achieved\nthe state-of-the-art performance in many tasks.\nIn neural network language models (NNLM), the\nfeedforward neural networks (FNN) and recurrent\nneural networks (RNN) [Elman, 1990] are two\npopular architectures. The basic idea of NNLMs is\nto use a projection layer to project discrete words\ninto a continuous space and estimate word con-\nditional probabilities in this space, which may be\nsmoother to better generalize to unseen contexts.\nFNN language models (FNN-LM) [Bengio and\nDucharme, 2001,Bengio, 2003] usually use a lim-\nited history within a ﬁxed-size context window\nto predict the next word. RNN language mod-\nels (RNN-LM) [Mikolov, 2010, Mikolov, 2012]\nadopt a time-delayed recursive architecture for the\nhidden layers to memorize the long-term depen-\ndency in language. Therefore, it is widely re-\nported that RNN-LMs usually outperform FNN-\nLMs in language modeling. While RNNs are the-\noretically powerful, the learning of RNNs needs to\nuse the so-called back-propagation through time\n(BPTT) [Werbos, 1990] due to the internal recur-\nrent feedback cycles. The BPTT signiﬁcantly in-\ncreases the computational complexity of the learn-\ning algorithms and it may cause many problems\nin learning, such as gradient vanishing and ex-\nploding [Bengio, 1994]. More recently, some\nnew architectures have been proposed to solve\nthese problems. For example, the long short\nterm memory (LSTM) RNN [Hochreiter, 1997]\nis an enhanced architecture to implement the re-\ncurrent feedbacks using various learnable gates,\nand it has obtained promising results on hand-\nwriting recognition [Graves, 2009] and sequence\nmodeling [Graves, 2013]. Moreover, the so-\ncalled temporal-kernel recurrent neural networks\n(TKRNN) [Sutskever, 2010] have been proposed\nto handle the gradient vanishing problem. The\nmain idea of TKRNN is to add direct connections\nbetween units in all time steps and every unit is im-\nplemented as an efﬁcient leaky integrator, which\nmakes it easier to learn the long-term dependency.\nAlong this line, a temporal-kernel model has been\nsuccessfully used for language modeling in [Shi,\n2013].\nComparing with RNN-LMs, FNN-LMs can be\nlearned in a simpler and more efﬁcient way. How-\never, FNN-LMs can not model the long-term de-\npendency in language due to the ﬁxed-size input\nwindow. In this paper, we propose a novel encod-\ning method for discrete sequences, named ﬁxed-\nsize ordinally-forgetting encoding (FOFE), which\ncan almost uniquely encode any variable-length\narXiv:1505.01504v2  [cs.NE]  16 Jun 2015\nword sequence into a ﬁxed-size code. Relying\non a constant forgetting factor, FOFE can model\nthe word order in a sequence based on a sim-\nple ordinally-forgetting mechanism, which uses\nthe position of each word in the sequence. Both\nthe theoretical analysis and the experimental sim-\nulation have shown that FOFE can provide al-\nmost unique codes for variable-length word se-\nquences as long as the forgetting factor is prop-\nerly selected. In this work, we apply FOFE to\nneural network language models, where the ﬁxed-\nsize FOFE codes are fed to FNNs as input to\npredict next word, enabling FNN-LMs to model\nlong-term dependency in language. Experiments\non two benchmark tasks, Penn Treebank Corpus\n(PTB) and Large Text Compression Benchmark\n(LTCB), have shown that FOFE-based FNN-LMs\ncan not only signiﬁcantly outperform the stan-\ndard ﬁxed-input FNN-LMs but also achieve better\nperformance than the popular RNN-LMs with or\nwithout using LSTM. Moreover, our implementa-\ntion also shows that FOFE based FNN-LMs can\nbe learned very efﬁciently on GPUs without the\ncomplex BPTT procedure.\n2 Our Approach: FOFE\nAssume vocabulary size is K, NNLMs adopt the\n1-of-K encoding vectors as input. In this case,\neach word in vocabulary is represented as a one-\nhot vector e ∈RK. The 1-of-K representation is a\ncontext independent encoding method. When the\n1-of-K representation is used to model a word in a\nsequence, it can not model its history or context.\n2.1 Fixed-size Ordinally Forgetting Encoding\nWe propose a simple context-dependent encoding\nmethod for any sequence consisting of discrete\nsymbols, namely ﬁxed-size ordinally-forgetting\nencoding (FOFE). Given a sequence of words (or\nany discrete symbols), S = {w1,w2,··· ,wT },\neach word wt is ﬁrst represented by a 1-of-K rep-\nresentation et, from the ﬁrst word t= 1to the end\nof the sequence t = T, FOFE encodes each par-\ntial sequence (history) based on a simple recursive\nformula (with z0 = 0) as:\nzt = α·zt−1 + et (1 ≤t≤T) (1)\nwhere zt denotes the FOFE code for the partial\nsequence up to wt, and α(0 < α <1) is a con-\nstant forgetting factor to control the inﬂuence of\nthe history on the current position. Let’s take a\nFigure 1: The FOFE-based FNN language model.\nsimple example here, assume we have three sym-\nbols in vocabulary, e.g., A, B, C, whose 1-of-\nK codes are [1,0,0], [0,1,0] and [0,0,1] respec-\ntively. In this case, the FOFE code for the se-\nquence {ABC}is [α2,α, 1], and that of {ABCBC}\nis [α4,α + α3,1 +α2].\nObviously, FOFE can encode any variable-\nlength discrete sequence into a ﬁxed-size code.\nMoreover, it is a recursive context dependent en-\ncoding method that smartly models the order in-\nformation by various powers of the forgetting fac-\ntor. Furthermore, FOFE has an appealing property\nin modeling natural languages that the far-away\ncontext will be gradually forgotten due to α <1\nand the nearby contexts play much larger role in\nthe resultant FOFE codes.\n2.2 Uniqueness of FOFE codes\nGiven the vocabulary (of K symbols), for any se-\nquence S with a length of T, based on the FOFE\ncode zT computed as above, if we can always de-\ncode the original sequence Sunambiguously (per-\nfectly recovering S from zT ), we say FOFE is\nunique.\nTheorem 1 If the forgetting factor αsatisﬁes 0 <\nα≤0.5, FOFE is unique for any Kand T.\nThe proof is simple because if the FOFE code\nhas a value αt in its i-th element, we may de-\ntermine the word wi occurs in the position t of\nS without ambiguity since no matter how many\ntimes wi occurs in the far-away contexts ( < t),\nthey do not sum to αt (due to α ≤0.5). If wi ap-\npears in any closer context (> t), the i-th element\nFigure 2: Numbers of collisions in simulation.\nmust be larger than αt.\nFor 0.5 < α <1, we have the following theo-\nrem:\nTheorem 2 For 0.5 < α <1, given any ﬁnite\nvalues of K and T, FOFE is almost unique every-\nwhere for α∈(0.5,1.0), except only a ﬁnite set of\ncountable choices of α.\nThe complete proof [Oguz, 2015] is given in\nAppendix A. Based on Theorem 2, FOFE is\nunique almost everywhere between(0.5,1.0) only\nexcept a countable set of isolated choices of α. In\npractice, the chance to exactly choose these iso-\nlated values between (0.5,1.0) is extremely slim,\nrealistically almost impossible due to quantization\nerrors in the system. To verify this, we have run\nsimulation experiments for all possible sequences\nup to T = 20 symbols to count the number of\ncollisions. Each collision is deﬁned as the maxi-\nmum element-wise difference between two FOFE\ncodes (generated from two different sequences) is\nless than a small threshold ϵ. In Figure 2, we\nhave shown the number of collisions (out of the\ntotal 220 tested cases) for various α values when\nϵ = 0.01, 0.001 and 0.0001.1 The simulation\nexperiments have shown that the chance of col-\nlision is extremely small even when we allow a\nword to appear any times in the context. Ob-\nviously, in a natural language, a word normally\ndoes not appear repeatedly within a near context.\nMoreover, we have run the simulation to exam-\nine whether collisions actually occur in two real\ntext corpora, namely PTB (1M words) and LTCB\n(160M words), using ϵ = 0.01, we have not ob-\n1When we use a bigger value forα, the magnitudes of the\nresultant FOFE codes become much larger. As a result, the\nnumber of collisions (as measured by a ﬁxed absolute thresh-\nold ϵ) becomes smaller.\nserved a single collision for nine differentαvalues\nbetween [0.55,1.0] (incremental 0.05).\n2.3 Implement FOFE for FNN-LMs\nThe architecture of a FOFE based neural network\nlanguage model (FOFE-FNNLM) is as shown in\nFigure 1. It is similar to standard bigram FNN-\nLMs except that it uses a FOFE code to feed into\nneural network LM at each time instance. More-\nover, the FOFE can be easily scaled to other n-\ngram based neural network LMs. For example,\nFigure 3 is an illustration of ﬁxed-size ordinally\nforgetting encoding based tri-gram neural network\nlanguage model.\nFOFE is a simple recursive encoding method\nbut a direct sequential implementation may not be\nefﬁcient for the parallel computation platform like\nGPUs. Here, we will show that the FOFE compu-\ntation can be efﬁciently implemented as sentence-\nby-sentence matrix multiplications, which are par-\nticularly suitable for the mini-batch based stochas-\ntic gradient descent (SGD) method running on\nGPUs.\nGiven a sentence, S = {w1,w2,··· ,wT },\nwhere each word is represented by a 1-of-K code\nas et (1 ≤t ≤T). The FOFE codes for all par-\ntial sequences in S can be computed based on the\nfollowing matrix multiplication:\nS =\n\n\n1\nα 1\nα2 α 1\n... ... 1\nαT−1 ··· α 1\n\n\n\n\ne1\ne2\ne3\n...\neT\n\n\n= MV\nwhere V is a matrix arranging all 1-of-K codes\nof the words in the sentence row by row, and M\nis a T-th order lower triangular matrix. Each row\nvector of S represents a FOFE code of the partial\nsequence up to each position in the sentence.\nThis matrix formulation can be easily extended\nto a mini-batch consisting of several sentences.\nAssume that a mini-batch is composed of N se-\nquences, L= {S1 S2 ···SN }, we can compute\nthe FOFE codes for all sentences in the mini-batch\nas follows:\n¯S =\n\n\nM1\nM2\n...\nMN\n\n\n\n\nV1\nV2\n...\nVN\n\n\n= ¯M¯V\nFigure 3: Illustration of a 2nd-order FOFE based\nFNN-LM.\nWhen feeding the FOFE codes to FNN as\nshown in Figure 1, we can compute the activation\nsignals (assume f is the activation function) in the\nﬁrst hidden layer for all histories in Sas follows:\nH = f\n(\n(MV)UW+b\n)\n= f\n(\nM(VU)W+b\n)\nwhere U denotes the word embedding matrix that\nprojects the word indices onto a continuous low-\ndimensional continuous space. As above, VU\ncan be done efﬁciently by looking up the embed-\nding matrix. Therefore, for the computational ef-\nﬁciency purpose, we may apply FOFE to the word\nembedding vectors instead of the original high-\ndimensional one-hot vectors. In the backward\npass, we can calculate the gradients with the stan-\ndard back-propagation (BP) algorithm rather than\nBPTT. As a result, FOFE based FNN-LMs are the\nsame as the standard FNN-LMs in terms of com-\nputational complexity in training, which is much\nmore efﬁcient than RNN-LMs.\n3 Experiments\nWe have evaluated the FOFE method for NNLMs\non two benchmark tasks: i) the Penn Treebank\n(PTB) corpus of about 1M words, following the\nsame setup as [Mikolov, 2011]. The vocabu-\nlary size is limited to 10k. The preprocess-\ning method and the way to split data into train-\ning/validation/test sets are the same as [Mikolov,\n2011]. ii) The Large Text Compression Bench-\nmark (LTCB) [Mahoney, 2011]. In LTCB, we use\nthe enwik9 dataset, which is composed of the ﬁrst\nTable 1: The size of PTB and LTCB corpora in\nwords.\nCorpus train valid test\nPTB 930k 74k 82k\nLTC 153M 8.9M 8.9M\n109 bytes of enwiki-20060303-pages-articles.xml.\nWe split it into three parts: training (153M), val-\nidation (8.9M) and testing (8.9M) sets. We limit\nthe vocabulary size to 80k for LTCB and replace\nall out-of-vocabulary words by a <UNK>token.\nDetails of the two datasets can be found in Table\n1. 2\n3.1 Experimental results on PTB\nWe have ﬁrst evaluated the performance of the\ntraditional FNN-LMs, taking the previous several\nwords as input, denoted as n-gram FNN-LMs here.\nWe have trained neural networks with a linear pro-\njection layer (of 200 hidden nodes) and two hid-\nden layers (of 400 nodes per layer). All hidden\nunits in networks use the rectiﬁed linear activation\nfunction, i.e., f(x) = max(0,x). The nets are\ninitialized based on the normalized initialization\nin [Glorot, 2010], without using any pre-training.\nWe use SGD with a mini-batch size of 200 and an\ninitial learning rate of 0.4. The learning rate is kept\nﬁxed as long as the perplexity on the validation set\ndecreases by at least 1. After that, we continue six\nmore epochs of training, where the learning rate is\nhalved after each epoch. The performance (in per-\nplexity) of various n-gram FNN-LMs is shown in\nTable 2.\nFor the FOFE-FNNLMs, the net architecture\nand the parameter setting are the same as above.\nThe mini-batch size is also 200 and each mini-\nbatch is composed of several sentences up to 200\nwords (the last sentence may be truncated). All\nsentences in the corpus are randomly shufﬂed at\nthe beginning of each epoch. In this experiment,\nwe ﬁrst investigate how the forgetting factor α\nmay affect the performance of LMs. We have\ntrained two FOFE-FNNLMs: i) 1st-order (using\nzt as input to FNN for each time t; ii) 2nd-order\n(using both zt and zt−1 as input for each time t,\nwith a forgetting factor varying between[0.0,1.0].\nExperimental results in Figure 4 have shown that\na good choice of α lies between [0.5,0.8]. Us-\n2Matlab codes are available athttps://wiki.eecs.\nyorku.ca/lab/MLL/projects:fofe:start for\nreaders to reproduce all results reported in this paper.\nFigure 4: Perplexities of FOFE FNNLMs as a\nfunction of the forgetting factor.\ning a too large or too small forgetting factor will\nhurt the performance. A too small forgetting fac-\ntor may limit the memory of the encoding while a\ntoo large αmay confuse LM with a far-away his-\ntory. In the following experiments, we setα= 0.7\nfor the rest experiments in this paper.\nIn Table 2, we have summarized the perplexi-\nties on the PTB test set for various models. The\nproposed FOFE-FNNLMs can signiﬁcantly out-\nperform the baseline FNN-LMs using the same\narchitecture. For example, the perplexity of the\nbaseline bigram FNNLM is 176, while the FOFE-\nFNNLM can improve to 116. Moreover, the\nFOFE-FNNLMs can even overtake a well-trained\nRNNLM (400 hidden units) in [Mikolov, 2011]\nand an LSTM in [Graves, 2013]. It indicates\nFOFE-FNNLMs can effectively model the long-\nterm dependency in language without using any\nrecurrent feedback. At last, the 2nd-order FOFE-\nFNNLM can provide further improvement, yield-\ning the perplexity of 108 on PTB. It also outper-\nforms all higher-order FNN-LMs (4-gram, 5-gram\nand 6-gram), which are bigger in model size. To\nour knowledge, this is one of the best reported re-\nsults on PTB without model combination.\n3.2 Experimental results on LTCB\nWe have further examined the FOFE based FNN-\nLMs on a much larger text corpus, i.e. LTCB,\nwhich contains articles from Wikipedia. We have\ntrained several baseline systems: i) two n-gram\nLMs (3-gram and 5-gram) using the modiﬁed\nKneser-Ney smoothing without count cutoffs; ii)\nseveral traditional FNN-LMs with different model\nsizes and input context windows (bigram, trigram,\nTable 2: Perplexities on PTB for various LMs.\nModel Test PPL\nKN 5-gram [Mikolov, 2011] 141\nFNNLM [Mikolov, 2012] 140\nRNNLM [Mikolov, 2011] 123\nLSTM [Graves, 2013] 117\nbigram FNNLM 176\ntrigram FNNLM 131\n4-gram FNNLM 118\n5-gram FNNLM 114\n6-gram FNNLM 113\n1st-order FOFE-FNNLM 116\n2nd-order FOFE-FNNLM 108\nTable 3: Perplexities on LTCB for various lan-\nguage models. [M*N] denotes the sizes of the in-\nput context window and projection layer.\nModel Architecture Test PPL\nKN 3-gram - 156\nKN 5-gram - 132\n[1*200]-400-400-80k 241\n[2*200]-400-400-80k 155\nFNN-LM [2*200]-600-600-80k 150\n[3*200]-400-400-80k 131\n[4*200]-400-400-80k 125\nRNN-LM [1*600]-600-80k 112\n[1*200]-400-400-80k 120\nFOFE [1*200]-600-600-80k 115\nFNN-LM [2*200]-400-400-80k 112\n[2*200]-600-600-80k 107\n4-gram and 5-gram ones); iii) an RNN-LM with\none hidden layer of 600 nodes using the toolkit\nin [Mikolov, 2010], in which we have further used\na spliced sentence bunch in [Chen et al. 2014]\nto speed up the training on GPUs. Moreover, we\nhave examined four FOFE based FNN-LMs with\nvarious model sizes and input window sizes (two\n1st-order FOFE models and two 2nd-order ones).\nFor all NNLMs, we have used an output layer of\nthe full vocabulary (80k words). In these exper-\niments, we have used an initial learning rate of\n0.01, and a bigger mini-batch of 500 for FNN-\nLMMs and of 256 sentences for the RNN and\nFOFE models. Experimental results in Table 3\nhave shown that the FOFE-based FNN-LMs can\nsigniﬁcantly outperform the baseline FNN-LMs\n(including some larger higher-order models) and\nalso slightly overtake the popular RNN-based LM,\nyielding the best result (perplexity of 107) on the\ntest set.\n4 Conclusions\nIn this paper, we propose the ﬁxed-size ordinally-\nforgetting encoding (FOFE) method to almost\nuniquely encode any variable-length sequence into\na ﬁxed-size code. In this work, FOFE has been\nsuccessfully applied to neural network language\nmodeling. Next, FOFE may be combined with\nneural networks [Zhang and Jiang, 2015, Zhang\net. al., 2015] for other NLP tasks, such as sen-\ntence modeling/matching, paraphrase detection,\nmachine translation, question and answer and etc.\nAcknowledgments\nThis work was supported in part by the Science\nand Technology Development of Anhui Province,\nChina (Grants No. 2014z02006) and the Funda-\nmental Research Funds for the Central Universi-\nties from China, as well as an NSERC Discov-\nery grant from Canadian federal govenment. We\nappreciate Dr. Barlas Oguz at Microsoft for his\ninsightful comments and constructive suggestions\non Theorem 2.\nAppendix A. The Proof of Theorem 2\nTheorem 2: For 0.5 < α <1, given any ﬁnite\nvalues of K and T, FOFE is almost unique every-\nwhere for α∈(0.5,1.0), except only a ﬁnite set of\ncountable choices of α.\nProof: When we decode a given FOFE code of\nan unknown sequence S (assume the length of S\nis not more than T), for any single value 1.0 in\nthe i-th position of the FOFE code, there are only\ntwo possible cases that may lead to ambiguity in\ndecoding: (i) word wi appears in the current loca-\ntion of S; or (ii) word wi appears multiple times in\nthe history of Sand the total contribution of them\nhappens to be 1.0. For case (ii) to happen, the for-\ngetting factor αneeds to satisfy at least one of the\nfollowing polynomial equations:\nT∑\nt=1\nξt ·αt = 1.0 (2)\nwhere the above coefﬁcients, ξt, are equal to ei-\nther 1 or 0. If the word wi appears in the t-th lo-\ncation ahead in the history, we have ξt = 1. Oth-\nerwise, ξt = 0. We know, each equation in eq.(2)\nis a T-th (or lower) order polynomial equation. It\ncan have at most T real roots for α. Moreover,\nsince ξt = {0,1}, we can only have a ﬁnite set of\nequations in eq.(2). The total number is not more\nthan 2T . Therefore, in total, we can only have a\nﬁnite number of αvalues that may satisfy at least\none equation in eq.(2), i.e., at most T ·2T possi-\nble roots [Oguz, 2015]. Among them, only a frac-\ntion of these roots lies between (0.5,1.0). Except\nthese countable choices of αvalues, eq.(2) never\nholds for any other α values between (0.5,1.0).\nAs a result, case (ii) never happens in decoding\nexcept some isolated points of α. This proves that\nthe resultant FOFE code is almost unique between\n(0.5,1.0). ■\nReferences\n[Katz, 1987] Slava Katz. 1987. Estimation of probabil-\nities from sparse data for the language model com-\nponent of a speech recognizer. IEEE Transactions\non Acoustics, Speech and Signal Processing (ASSP),\nV olume 35, no 3, pages 400-401.\n[Kneser, 1995] Reinhard Kneser and Hermann Ney.\n1995. Improved backing-off for m-gram language\nmodeling. In Proc. of International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 181-184.\n[Werbos, 1990] Paul Werbos. 1990. Back-propagation\nthrough time: what it does and how to do it. Pro-\nceedings of the IEEE, volume 78, no 10, pages 1550-\n1560.\n[Bengio, 1994] Yoshua Bengio, Patrice Simard and\nPaolo Frasconi. 1994. Learning long-term depen-\ndencies with gradient descent is difﬁcult. IEEE\nTransactions on Neural Networks volume 5, no 2,\npages 157-166.\n[Bengio and Ducharme, 2001] Yoshua Bengio and Re-\njean Ducharme. 2001. A neural probabilistic lan-\nguage model. In Proc. of NIPS, volume 13.\n[Bengio, 2003] Yoshua Bengio, Rejean Ducharme, Pas-\ncal Vincent, and Christian Jauvin. 2003. A neural\nprobabilistic language model. Journal of Machine\nLearning Research , volume 3, no 2, pages 1137-\n1155.\n[Elman, 1990] Jeffery Elman. 1990. Finding structure\nin time. Cognitive science, volume 14, no 2, pages\n179-211.\n[Mikolov, 2010] Tomas Mikolov, Martin Karaﬁ ´at,\nLukas Burget, Jan Cernock `y and Sanjeev Khudan-\npur. 2010. Recurrent neural network based language\nmodel. In Proc. of Interspeech, pages 1045-1048.\n[Mikolov, 2011] Tomas Mikolov, Stefan Kombrink,\nLukas Burget, Jan Cernocky and Sanjeev Khudan-\npur. 2011. Extensions of recurrent neural network\nlanguage model. In Proc. of International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5528-5531.\n[Mikolov, 2012] Tomas Mikolov and Geoffrey Zweig.\n2012. Context dependent recurrent neural network\nlanguage model. In Proc. of SLT, pages 234-239.\n[Chen et al. 2014] X. Chen, Y . Wang, X. Liu, et al.\n2014. Efﬁcient GPU-based training of recurrent neu-\nral network language models using spliced sentence\nbunch. In Proc. of Interspeech.\n[Sutskever, 2010] Ilya Sutskever and Geoffrey Hinton.\n2010. Temporal-kernel recurrent neural networks.\nNeural Networks. pages 239-243.\n[Shi, 2013] Yong-Zhe Shi, Wei-Qiang Zhang, Meng\nCai and Jia Liu. 2013. Temporal kernel neural net-\nwork language model. InProc. of International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP). pages 8247-8251.\n[Hochreiter, 1997] Sepp Hochreiter and Jurgen\nSchmidhuber. 1997. Long short-term memory.\nNeural computation , volume 9, no 8, pages\n1735-1780.\n[Graves, 2009] Alex Graves and Jurgen Schmidhuber.\n2009. Ofﬂine handwriting recognition with multi-\ndimensional recurrent neural networks. In Proc. of\nNIPS. pages 545-552.\n[Graves, 2013] Alex Graves. 2013. Generating se-\nquences with recurrent neural networks. arXiv\npreprint arXiv:1308.0850.\n[Glorot, 2010] Glorot Xavier and Yoshua Bengio. 2010.\nUnderstanding the difﬁculty of training deep feed-\nforward neural networks. In Proc. of AISTATS.\n[Mahoney, 2011] Matt Mahoney. 2011.\nLarge Text Compression Benchmark. In\nhttp://mattmahoney.net/dc/textdata.html.\n[Oguz, 2015] Barlas Oguz. 2015. Personal Communi-\ncations.\n[Zhang and Jiang, 2015] Shiliang Zhang and Hui Jiang.\n2015. Hybrid Orthogonal Projection and Estimation\n(HOPE): A New Framework to Probe and Learn\nNeural Networks. arXiv:1502.00702.\n[Zhang et. al., 2015] Shiliang Zhang, Hui Jiang and\nLirong Dai. 2015. The New HOPE Way to Learn\nNeural Networks. Proc. of Deep Learning Workshop\nat ICML 2015.",
  "topic": "Forgetting",
  "concepts": [
    {
      "name": "Forgetting",
      "score": 0.8189897537231445
    },
    {
      "name": "Encoding (memory)",
      "score": 0.7313552498817444
    },
    {
      "name": "Computer science",
      "score": 0.6959123015403748
    },
    {
      "name": "Sequence (biology)",
      "score": 0.6338381767272949
    },
    {
      "name": "ENCODE",
      "score": 0.6336312294006348
    },
    {
      "name": "Feed forward",
      "score": 0.6243561506271362
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.5900635719299316
    },
    {
      "name": "Artificial neural network",
      "score": 0.5735137462615967
    },
    {
      "name": "Feedforward neural network",
      "score": 0.5189599394798279
    },
    {
      "name": "Word (group theory)",
      "score": 0.517997145652771
    },
    {
      "name": "Representation (politics)",
      "score": 0.5108211040496826
    },
    {
      "name": "Language model",
      "score": 0.5076104402542114
    },
    {
      "name": "Algorithm",
      "score": 0.43085113167762756
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4123589098453522
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3876855969429016
    },
    {
      "name": "Mathematics",
      "score": 0.21139901876449585
    },
    {
      "name": "Engineering",
      "score": 0.05738246440887451
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Control engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}