{
  "title": "Leveraging LLMs for the Quality Assurance of Software Requirements",
  "url": "https://openalex.org/W4401719686",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4367424689",
      "name": "Lubos, Sebastian",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4287178021",
      "name": "Felfernig, Alexander",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4225013186",
      "name": "Tran Thi Ngoc Trang",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": null,
      "name": "Garber, Damian",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": null,
      "name": "Mansi, Merfat El",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": null,
      "name": "Erdeniz, Seda Polat",
      "affiliations": [
        "Graz University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4367424688",
      "name": "Le, Viet-Man",
      "affiliations": [
        "Graz University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2107875488",
    "https://openalex.org/W6728021928",
    "https://openalex.org/W2758253540",
    "https://openalex.org/W2896989113",
    "https://openalex.org/W6804917177",
    "https://openalex.org/W991150271",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W4387142599",
    "https://openalex.org/W4387123446",
    "https://openalex.org/W4362514913",
    "https://openalex.org/W4382567209",
    "https://openalex.org/W4387428001",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4367185264",
    "https://openalex.org/W4313192501",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4385688511",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W2909904554",
    "https://openalex.org/W3006273876",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4399203757",
    "https://openalex.org/W3216590671",
    "https://openalex.org/W4402665833",
    "https://openalex.org/W2528618663",
    "https://openalex.org/W2162180335",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W2760476066"
  ],
  "abstract": "Successful software projects depend on the quality of software requirements.\\nCreating high-quality requirements is a crucial step toward successful software\\ndevelopment. Effective support in this area can significantly reduce\\ndevelopment costs and enhance the software quality. In this paper, we introduce\\nand assess the capabilities of a Large Language Model (LLM) to evaluate the\\nquality characteristics of software requirements according to the ISO 29148\\nstandard. We aim to further improve the support of stakeholders engaged in\\nrequirements engineering (RE). We show how an LLM can assess requirements,\\nexplain its decision-making process, and examine its capacity to propose\\nimproved versions of requirements. We conduct a study with software engineers\\nto validate our approach. Our findings emphasize the potential of LLMs for\\nimproving the quality of software requirements.\\n",
  "full_text": "Leveraging LLMs for the Quality Assurance of\nSoftware Requirements\n1st Sebastian Lubos\nGraz University of Technology, Austria\nslubos@ist.tugraz.at\n2nd Alexander Felfernig\nGraz University of Technology, Austria\nafelfern@ist.tugraz.at\n3rd Thi Ngoc Trang Tran\nGraz University of Technology, Austria\nttrang@ist.tugraz.at\n4th Damian Garber\nGraz University of Technology, Austria\ndgarber@ist.tugraz.at\n5th Merfat El Mansi\nGraz University of Technology, Austria\nmerfat.el-mansi@student.tugraz.at\n6th Seda Polat Erdeniz\nGraz University of Technology, Austria\nsedapolat@gmail.com\n7th Viet-Man Le\nGraz University of Technology, Austria\nvietman.le@ist.tugraz.at\nAbstract—Successful software projects depend on the quality\nof software requirements. Creating high-quality requirements is\na crucial step toward successful software development. Effective\nsupport in this area can significantly reduce development costs\nand enhance the software quality. In this paper, we introduce\nand assess the capabilities of a Large Language Model (LLM)\nto evaluate the quality characteristics of software requirements\naccording to the ISO 29148standard. We aim to further improve\nthe support of stakeholders engaged in requirements engineering\n(RE). We show how an LLM can assess requirements, explain\nits decision-making process, and examine its capacity to pro-\npose improved versions of requirements. We conduct a study\nwith software engineers to validate our approach. Our findings\nemphasize the potential of LLMs for improving the quality of\nsoftware requirements.\nIndex Terms—requirements engineering, software require-\nments, large language model, quality assurance, quality improve-\nment, empirical study\nI. I NTRODUCTION\nThe success of software projects depends on the quality of\nsoftware requirements [1]. Formulating high-quality require-\nments constitutes an essential step towards achieving favorable\noutcomes [2]. However, reaching such quality requires signif-\nicant collaborative efforts from stakeholders engaged in the\nproject. To assist this process, various tools and techniques\nhave demonstrated their efficiency in supporting multiple\nrelated tasks [3], including the classification [4], prioritization\n[5], [6], and quality assessment of requirements [7].\nA recent related development in this direction has emerged\nwith the introduction of powerful Large Language Models\n(LLMs) [8] such as GPT-4 [9] and Llama 2 [10]. These models\nshow remarkable capabilities in generating natural language,\nproducing a huge variety of accurately crafted responses\nto input prompts. Applying these models in requirements\nengineering has received increasing attention in past years.\nThe presented work has been developed within the research project\nSTREAMDIVER , which is funded by the Austrian Research Promotion Agency\n(FFG) under the project number 886205.\nAmong others, they have been explored for tasks such as\ninconsistency detection within sets of requirements [11], [12]\nand identification of incomplete requirements [13].\nWhile the existing literature explores interesting possibilities\nfor leveraging LLMs to assess and improve specific quality\naspects of requirements, addressing the broader issue of eval-\nuating the overall quality of software requirements, including\nmultiple quality characteristics, remains unexplored. The study\npresented in this paper investigates the usage of an LLM to\nassess whether individual requirement statements adhere to the\nset of quality characteristics defined in ISO 29148 [14]. We\noutline our approach to instruct the LLM in this assessment\ntask and investigate its capacity to transparently explain de-\ncisions, aiming to enhance trust in the system. Furthermore,\nwe examine the LLM’s potential to propose improved versions\nof requirements to correct identified quality flaws. To assess\nthe effectiveness of this approach, we present the findings of\nan empirical user study conducted with participants having\na background in software engineering, offering a proof-of-\nconcept evaluation of the approach’s benefits.\nWith this work, we provide a novel approach to using an\nLLM to transparently evaluate the quality of software require-\nments by explaining the decisions and presenting suggestions\nfor improvement. This includes three primary contributions.\nFirstly, we show how an LLM can be instructed to evaluate\nthe quality of software requirement statements, comparing its\naccuracy in flaw detection with that of study participants.\nSecondly, we analyze the LLM’s capacity to consistently\nexplain its assessments, thereby enhancing transparency and\nfostering trust in the evaluation. Thirdly, we investigate the\nLLM’s capability to offer constructive suggestions for im-\nproved requirements addressing identified flaws.\nThe remainder of the paper is organized as follows. Sec-\ntion II covers essential aspects required to follow the paper.\nThis includes requirement quality characteristics as defined in\nISO 29148 and fundamental knowledge related to applying\nLLMs in evaluating software requirements. Section III gives\narXiv:2408.10886v1  [cs.SE]  20 Aug 2024\nan overview of existing work on the application of LLM-\nenhanced techniques in the context of software requirements\nengineering. In Section IV, we present the research questions,\noutline the experimental setup of the user study, and provide a\ncomprehensive discussion of its results. Section V explores the\nimplications of the results, taking into account the limitations\nof this study and suggesting open issues for future work.\nFinally, Section VI summarizes and concludes the paper.\nII. B ACKGROUND\nA. Quality Characteristics of Software Requirements\nRequirements engineering (RE) is an important phase in\nthe software development process, and the creation of high-\nquality requirements is crucial for successful software devel-\nopment [2]. The ISO 29148 [14] describes the international\nstandard for systems and software engineering, including RE.\nThis standard defines nine characteristics (dimensions) for\nindividual software requirements, describing the capabilities,\ncharacteristics, constraints, and quality factors of a software\nsystem. Table I summarizes these quality characteristics.\nCharacteristic Description\nAppropriate The level of abstraction is adequate, excludes unnec-\nessary constraints, and avoids implementation details.\nComplete All information needed to understand the requirement\nis included in the description.\nConforming The representation of the requirement follows an ap-\nproved standard template.\nCorrect The need is accurately represented in the requirement.\nFeasible The requirement is realizable within the given system\nconstraints considering an acceptable risk.\nNecessary The requirement defines an essential aspect of the sys-\ntem and is irremovable without causing a deficiency.\nSingular The requirement defines only one aspect of the system.\nUnambiguous The requirement is clearly stated, understandable, and\nallows only one interpretation.\nVerifiable The requirement is formulated in a way that its ful-\nfillment can be proven or, in the best case, measured.\nTABLE I: Characteristics of high-quality software require-\nments as defined in ISO 29148 [14].\nTo determine if project requirements fulfill these quality\ncharacteristics, a review by project stakeholders is required.\nThese stakeholders need to understand the project’s scope\nand have experience in software projects. Their task is to\ndecide whether the software requirements meet the mentioned\nquality characteristics. This evaluation process can involve\nmultiple stakeholders reviewing the requirements and reaching\na consensus through a majority decision.\nB. Instructing LLMs to Evaluate Software Requirements\nA Large Language Model (LLM) is an advanced Natural\nLanguage Processing (NLP) model, demonstrating capabili-\nties in comprehending and generating human-like text [8].\nThese models find application across diverse domains, such\nas content generation [15], language translation [16], and code\ncompletion [17], enabling the creation of relevant information\nbased on contextual input. To execute various tasks, LLMs\nare provided with specific input queries known as prompts,\nguiding them to generate desired outputs or responses in\nnatural language [18], [19].\nWhen assessing individual software requirements, the LLM\nmust understand the project scope. To achieve this, the project\nscope description must be incorporated into the prompt. The\nLLM then has to be instructed to evaluate a given requirement,\nconsidering both, the project description and the definition of a\nspecified quality characteristic (refer to Table I), as additional\ncontextual information. The primary task of the LLM is then to\nclassify whether the requirement satisfies the specified quality\ndimension and to explain its decision. If a quality issue is iden-\ntified, the LLM is further instructed to propose an improved\nversion of the requirement that addresses the identified flaw.\nThe related LLM prompt used in our study is depicted in\nFigure 1, including the quality characteristic to be evaluated,\nits definition, and the project scope description. Additionally,\nthe model’s output space is limited to two options, indicating\nwhether the requirement meets the quality criteria or not.\nFig. 1: LLM prompt template used to evaluate a software\nrequirement for a specified quality characteristic and project.\nVariables are written in curly brackets “ {. . .}”.\nIII. R ELATED WORK\nThe integration of AI-enhanced tools for requirements en-\ngineering has received increased attention in recent years.\nThe application of machine learning (ML) and NLP forms a\ncrucial focus of research in this domain. Cheeliger et al. [20]\nconducted a comprehensive literature review on machine learn-\ning in requirements engineering, categorizing publications into\nfour tasks: preparation, collection, validation, and negotiation.\nThe review’s findings suggest that while the potential of ML\nin this field is promising, its seamless integration into existing\nengineering workflows remains challenging. Our work seeks to\ncontribute additional value, specifically in the validation task,\nby facilitating the quality assessment process.\nLeveraging LLMs in software requirement engineering is\nnot novel. Arora et al. [21] conducted a preliminary evaluation\nof using an LLM to elicitate software requirements for a real-\nworld application. The study results emphasized the feasibility\nof LLMs for this task. Fantechi et al. [11] explored the use\nof ChatGPT to identify conflicting requirements. The results\nof the study suggest that the model cannot replace human\njudgment but can complement manual analysis and speed\nup the process. In a similar direction, Bertram et al. [12]\nincorporated an LLM into a suggested toolchain to detect\ninconsistencies in large requirement specifications within the\nautomotive industry. The LLM translated the natural language\nrequirements into structured English, enabling formal process-\ning for consistency checking. Luitel et al. [13] investigated\nthe potential of LLMs in enhancing the completeness of\nrequirements expressed in natural language. The evaluation\nindicates that LLMs can assist in identifying incompleteness\nwithin requirements.\nOur objective is to enhance the understanding of LLMs\nin identifying quality issues within software requirements\nwith regard to various dimensions. In exploring the potential\nof LLMs, we investigate the applicability of the model to\nprovide explainable insights into its assessments, enhance the\ntransparency of decisions, and increase trust in the model.\nAdditionally, we investigate the model’s capability to offer\nconstructive suggestions for improving flawed requirements.\nIV. Q UALITY ASSURANCE OF SOFTWARE REQUIREMENTS\nWITH LLM S\nA. Methodology\nConsidering the rich capabilities of LLMs in comprehending\ncontexts [22] and reasoning over textual descriptions [23],\nwe expect their potential to support requirements engineering\ntasks. We expect these models to support the identification\nof potential weaknesses in requirements artifacts and, ideally,\nlocate quality issues precisely, offering explanations and sug-\ngestions for improvement. In doing so, these models might\nserve as valuable assistants, potentially reducing the need for\nextensive review cycles.\nTo evaluate our assumptions, we formulate the following\nhypotheses:\nH1: LLMs achieve comparable performance to software en-\ngineers in identifying quality issues in software require-\nments concerning various quality characteristics.\nH2: LLMs can reasonably explain identified quality issues.\nH3: LLMs can suggest improved requirement statements,\nenhancing their quality.\nBuilding upon these hypotheses, we define the following\nresearch questions:\nR1: How accurately can LLMs identify quality issues in soft-\nware requirements across different quality characteristics?\nR2: Can LLMs provide meaningful explanations for their\nquality assessments of software requirements?\nR3: Do LLMs have the ability to suggest improved versions\nof software requirements, addressing identified flaws?\nB. Study Participants\nOur empirical study involved individuals with educational\nand professional backgrounds in software engineering. While\nparticipants were not required to be experienced experts in\nRE, they needed practical experience working on software\nprojects. This background of participants warranted that par-\nticipants had fundamental knowledge in working with software\nrequirements. The study instructions included a definition and\nexplanation of the quality characteristics to ensure a clear\nunderstanding of those among all participants.\nFig. 2: LLM prompt template used to generate software\nrequirements for an example project to implement a Stopwatch\napp for Android smartphones . Variables are written in curly\nbrackets “{. . .}”.\nC. Dataset and Preprocessing\nFor our empirical study, we used two example projects\ndescribed in the following:\n1) Stopwatch Project: We initially considered a hypotheti-\ncal small project. As most software projects are typically large\nand complex, understanding their context and evaluating their\nrequirements requires high effort. For this reason, we decided\nto first consider a relatively simple project comprising ten\nrequirements, seven functional and three non-functional, to\nevaluate our proof-of-concept. The software to be described\nwith those requirements is a Stopwatch app designed for\nAndroid smartphones. We used an LLM 1 to generate this set\nof requirements by using the prompt shown in Figure 2, along\nwith the following project scope description: “ Develop an\nintuitive and user-friendly Stopwatch app for Android smart-\nphones that allows users to easily measure time intervals with\nprecision. The app should have a simple and clean interface,\nallowing users to start, pause, and reset the stopwatch easily.\nAdditionally, the app should display the total time elapsed and\nprovide options for split and lap times. The goal is to create an\nefficient and reliable tool that can be used in various contexts\nsuch as sports, cooking, or any other activity where accurate\ntime measurement is important .”\nThe LLM was not explicitly instructed to fulfill specific\nquality characteristics during the generation of requirements,\nwhich means that realistic and potentially flawed samples\nwere generated. To ensure their coherence and suitability as\nan explanatory example for our study, we carefully reviewed\nthe generated requirements 2. The generated requirements are\nshown in Table II\n2) DigitalHome Project: As a second example, we focused\non the software requirements of a real-world project included\nin the PURE dataset [24]. This dataset collects requirement\ndocuments from public projects. From those samples, we\nselected DigitalHome, a project detailing the requirements\nregarding a smart home prototype, as a representative case for\nour study. These requirements need to be evaluated by a group\n1Llama 2 [10] with 70 billion parameters as described in Section IV-D.\n2We deliberately refrained from correcting the requirements concerning the\nquality characteristics outlined in Table I, as we wanted to consider a realistic\nexample that could contain quality issues.\nId Requirement\nr1\nThe app shall allow users to start the stopwatch by tapping a\nprominent ’Start’ button.\nr2\nThe app shall allow users to pause the stopwatch by tapping a\nprominent ’Pause’ button.\nr3\nThe app shall allow users to reset the stopwatch to zero by\ntapping a prominent ’Reset’ button.\nr4 The app shall display the total time elapsed since the last reset.\nr5\nThe app shall provide an option for split times, allowing users\nto manually enter a split time and display it alongside the total\ntime elapsed.\nr6\nThe app shall provide an option for lap times, allowing users\nto manually enter a lap time and display it alongside the total\ntime elapsed.\nr7\nThe app shall allow users to view their previous splits and laps,\nincluding the time taken for each split and lap.\nr8\nThe app shall be designed with a simple and clean interface,\nensuring ease of use for users of all ages and skill levels.\nr9\nThe app shall be optimized for performance, ensuring that it\noperates efficiently and without significant lag or errors.\nr10\nThe app shall be compatible with Android smartphones running\nversion 10 or higher, ensuring that it can be installed and used\non a wide range of devices.\nTABLE II: Generated requirements for the development of a\nStopwatch app for Android smartphones .\nof study participants who understand the project scope, and\nwe assumed that a basic understanding of smart homes can be\npresumed. The project includes 63 requirements, comprising\n42 functional and 21 non-functional requirements.\nD. LLM-based Requirement Evaluation\nWe conducted an LLM-based evaluation of requirements\nutilizing the Llama 2 language model [10] with 70 billion\nparameters, fine-tuned to complete chat responses. We decided\nto perform our experiments with this LLM, as it achieved\ncompetitive benchmark performance3 at the time the study was\nconducted and was published as open-source4. For practicality,\nwe accessed the hosted model on Replicate5 with param-\neters temperature = 0.01, max new tokens = 2000. Given\nthat real-time responses were not a criterion for our evaluation,\nwe used the most accurate model despite its slower response\ngeneration speed ( ≈15-30 seconds ). Details on the LLM\ninstructions for this task are provided in Section II-B.\nE. Experimental Setup\nTo assess the validity of our hypotheses regarding the\ncapabilities of LLMs in evaluating the quality of software\nrequirements and to address the research questions outlined\nin Section IV-A, we implemented the following procedure for\nour experiments:\n1) Instruct study participants to evaluate the requirements\nof both example projects (see Section IV-C) based on\nthe ISO 29148 quality characteristics (see Section II-A)\nand their project scope description.\n2) Use the requirements of both example projects as input\nfor LLM requests, instructing it to assess their quality\n3https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard\n4https://github.com/meta-llama/llama\n5Model version: meta/llama-2-70b-chat:\n02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\n(see Section IV-D) and record the model’s responses.\nThe project scope descriptions and quality characteristic\ndefinitions are part of the LLM prompt.\n3) Instruct study participants to evaluate the LLM re-\nsponses, considering:\n• The agreement with the LLM’s decision ( R1)\n• The plausibility of the LLM’s explanation ( R2)\n• The quality of the improved requirement suggested\nby the LLM ( R3)\n4) Compare the assessment of quality flaws by the LLM\nand study participants, using the study participants’\nresponses as the baseline.\nEach requirement and its corresponding LLM response has\nbeen evaluated by at least four individuals with software\nengineering backgrounds. They used the project description\nand an explanation of quality characteristics as a guideline for\ntheir assessments. We used their evaluation to establish the\nground truth for our project examples. A majority vote was ap-\nplied to label the samples, determining if a requirement meets\nthe observed quality characteristic. If a majority consensus\nwas not reached or if the majority expressed uncertainty, we\nlabeled the requirement as not fulfilling the observed quality\ncharacteristic. Our assumption was that when a group of\nreviewers cannot reach a unanimous decision, the requirement\nis flawed, and additional review is needed.\nUsing the described process, we had two phases of study\nparticipants evaluating the quality characteristics of require-\nments. In the first evaluation phase, which we name indepen-\ndent assessment, the study participant classified requirements\nwithout knowledge about the decision of the LLM. In the\nsecond phase, the study participants knew the decision and\nexplanation of the LLM and decided whether they agreed.\nWe refer to the second phase as bound assessment . Study\nparticipants might have classified requirements differently in\nthe second phase, indicating that the LLM has convinced them.\nDuring this phase, study participants evaluated the explanation\nprovided by the LLM by categorizing it as either plausible,\nimplausible, or neutral. Similarly, the quality of suggested\nrequirement improvements by the LLM was evaluated.\nF . Results\n1) Agreement between Study Participants and LLM (R1):\nAs a first step, we evaluate the agreement between the LLM\nand study participants when assessing the quality charac-\nteristics of requirements in our investigated Stopwatch and\nDigitalHome projects. In line with our initial hypothesis, we\nanticipate a high level of agreement between the two. To\nmeasure this agreement, we employ the Cohen’s Kappa metric\n[25], a measure of inter-rater reliability that quantifies the\nextent to which two raters agree on the categorical labeling\nof data samples, accounting for the potential of agreement\nby chance. We considered an agreement between the study\nparticipants and LLM when their assessment of a requirement\nand quality characteristic was identical.\nIn Table III, the Cohen’s Kappa values are shown. For\nthe independent assessment, a weak agreement is observed\nProject Independent Assessment Bound Assessment\nStopwatch 0.4028 0.7545\nDigitalHome 0.0486 0.2223\nTABLE III: Cohen’s Kappa value indicating the inter-rater\nagreement between the study participants and LLM in classi-\nfying requirements as flawed. Independent assessment means\nthat both raters answered without knowledge about how the\nother rater answered, while bound assessment describes the sit-\nuation where the study participants classified the requirements\nbeing aware of the decision and explanation of the LLM.\nbetween the LLM and study participants’ evaluations for the\nStopwatch project (0.4 <= κ <0.6), according to the interpre-\ntation outlined by McHugh in [25]. In the bound assessment,\nwe could measure a substantial agreement (0.6 <= κ <0.8)\nbetween the LLM and study participants for the Stopwatch\nexample, which indicates that the study participants agreed\nwith the LLM assessment of requirements in many cases. For\nthe DigitalHome project, we could not establish an agree-\nment in the independent assessment, while a fair agreement\n(0.2 <= κ < 0.4) was identified in the bound assessment\nscenario. The metric values show that we cannot assert that\nthe LLM and study participants yield identical evaluations.\nWhile Cohen’s Kappa metric measures agreement between\nraters, it lacks an assessment of the validity of responses,\ni.e., whether two raters have correctly classified samples. To\nunderstand the values better, we provide an additional per-\nspective to the evaluations. The Tables IVa and IVb show the\nassessments of quality characteristics by study participants and\nthe LLM for the requirements of the Stopwatch project (refer\nto Table II). The first table indicates the majority decisions (see\nSection IV-E) of study participants if a requirement fulfills a\nquality characteristic or not. The second table shows the LLM\ndecisions for the same requirements. The sum values Σreq in\nthe tables reflect the number of fulfilled quality characteristics\nper requirement. The sum in column Σqc shows the number\nof requirements fulfilling a specific quality characteristic.\nThe evaluation summary reveals relevant findings. Firstly,\nthe total count of requirements fulfilling a quality characteristic\nis higher for study participants, suggesting that the LLM tends\nto evaluate more requirements negatively. The observation\ncan be confirmed for the DigitalHome project (see Table\nV), where the study participants decided on more than twice\nas many assessments ( 514) that a requirement fulfilled the\ncharacteristic, compared to the LLM ( 225). Secondly, sub-\nstantial differences exist in the positive evaluations of various\nquality dimensions, with the LLM notably assessing fewer\nrequirements as appropriate, correct, singular, unambiguous,\nand verifiable. Again, the same observation was made for the\nDigitalHome project. Table V shows the sum of requirements\nfulfilling a quality characteristic given the study participants’\nand LLM’s decisions.\nTo assess the validity of the LLM evaluation, we establish\nthe study participants’ assessment as the ground truth. This\ninformation is used to calculate the precision and recall of\nthe LLM’s capacity to identify quality flaws in requirements\n(a) Study participants’ assessment.\nr1 r2 r3 r4 r5 r6 r7 r8 r9 r10 Σqc\nAppropriate ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 7\nComplete ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ 2\nConforming ✓ ✗ ✓ ✗ ✓ ✗ ✓ ✓ ✗ ✓ 6\nCorrect ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ ✓ ✓ 9\nFeasible ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 10\nNecessary ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 10\nSingular ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ 7\nUnambiguous ✓ ✓ ✓ ✗ ✗ ✗ ✓ ✗ ✗ ✓ 5\nVerifiable ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✓ ✓ 6\nΣreq 7 6 8 6 6 4 7 5 5 8 62\n(b) LLM assessment.\nr1 r2 r3 r4 r5 r6 r7 r8 r9 r10 Σqc\nAppropriate ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✓ ✗ ✓ 3\nComplete ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ 2\nConforming ✓ ✗ ✗ ✓ ✗ ✗ ✓ ✓ ✗ ✓ 5\nCorrect ✗ ✗ ✗ ✓ ✓ ✗ ✓ ✓ ✓ ✓ 6\nFeasible ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 10\nNecessary ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 10\nSingular ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ 0\nUnambiguous ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ 2\nVerifiable ✗ ✗ ✗ ✓ ✗ ✗ ✗ ✗ ✗ ✓ 2\nΣreq 3 2 2 8 3 2 4 5 4 8 40\nTABLE IV: Requirements evaluation of the Stopwatch project.\n“✓” indicates a requirement that fulfills the quality characteris-\ntic. Σreq shows the number of fulfilled quality characteristics\nper requirement. Σqc shows the number of requirements\nfulfilling a quality characteristic.\nStudy Participants LLM\nappropriate 60 12\ncomplete 53 15\nconforming 61 42\ncorrect 63 27\nfeasible 61 56\nnecessary 59 48\nsingular 57 5\nunambiguous 49 18\nverifiable 51 2\nΣreqs 514 225\nTABLE V: The number of requirements in the DigitalHome\nproject fulfilling a quality characteristic based on the assess-\nment of the study participants and LLM evaluation. Σreqs is\nthe sum of fulfilled characteristics of requirements. The total\nnumber of evaluated requirements in this project is 63.\naccurately, using the equations 1 and 2.\nPrecision = #Flawed reqs. identified by LLM\n#Reqs. classified as flawed by LLM\n(1)\nRecall = #Flawed reqs. identified by LLM\n#All flawed reqs. (2)\nIn this context, the precision describes the proportion of\nidentified requirements with quality issues among all require-\nments the LLM has classified as flawed. The recall values\nrepresent how many requirements with quality issues in the\nground truth have been identified correctly by the LLM. The\nvalues for the independent and bound assessments are reported\nin Table VI.\nIndependent Assessment Bound Assessment\nProject Precision Recall Precision Recall\nStopwatch 0.50 0.8929 0.8837 1.0\nDigitalHome 0.1257 0.7414 0.3214 1.0\nTABLE VI: Precision and recall of LLM recognized require-\nment flaws. Independent assessment means that both raters\nanswered without knowledge about how the other rater an-\nswered, while bound assessment describes the situation where\nthe study participants classified the requirements being aware\nof the decision and explanation of the LLM.\nFor the independent assessment, the precision is relatively\nlow in both investigated projects, which implies a higher\nlikelihood of false positives. Quality issues seem to be in-\ncorrectly recognized by the LLM using the study participants’\nevaluation as ground truth. However, the precision value is\nhigher when reviewing the bound assessment. This can be\nexplained by the fact that the study participants’ decisions have\nchanged regarding the LLM assessment and explanation. We\nassume that the study participants received a new perspective\non the requirement and, therefore, reconsidered their decision.\nThis means manual verification is still required, even if the\nnumber of misclassified requirements is lower. For example,\nin the Stopwatch project, the LLM incorrectly identified 25 out\nof 90 items as flawed during the independent assessment. This\nnumber decreased to 5 in the bound assessment. Considering\nthe precision values for the DigitalHome project, a trend to\nhigher precision in the bound assessment can be observed.\nYet, the values are much lower, as the number of wrongly\nidentified flaws by the LLM is quite high. We assume that\nthis is related to the project scope, which is more complex\nthan the Stopwatch example.\nFor both projects and assessment phases, high recall values\nwere measured. Those affirm that a large number of quality\nflaws are indeed identified by the LLM. This outcome suggests\nthat the LLM can be effectively utilized to highlight quality\nissues in software requirements and guide stakeholders toward\nrequirements that may need improvement.\nBased on our analysis, we can answer the first research\nquestion, which focuses on the accuracy of LLMs in correctly\nidentifying quality issues in software requirements. For inde-\npendent assessment, the Cohen’s Kappa metric indicates weak\nor even non-existent agreement between study participants and\nthe LLM when determining whether a software requirement\nfulfills a quality characteristic (refer to Table III). However, the\nagreement increases for the bound assessment phase, which\nindicates that study participants changed their opinion toward\nthe LLM given its explanation. We assume that the evaluation\nof the LLM helped correct the participants’ assessment. This\nmight help identify potential quality issues with requirements,\nespecially when reviewers and stakeholders lack experience in\nRE. The LLM might partly compensate for this by suggesting\nadditional perspectives.\nThe recall values demonstrate that the LLM effectively\nidentified the majority of flawed software requirements. This\nsuggests it can accurately predict the presence of quality issues\nin software requirements (see Table VI). Nevertheless, manual\nStopwatch DigitalHome\nappropriate 100% 70%\ncomplete 100% 100%\nconforming 100% 100%\ncorrect 80% 100%\nfeasible 100% 100%\nnecessary 100% 100%\nsingular 100% 80%\nunambiguous 100% 100%\nverifiable 100% 70%\nTABLE VII: Percentages of LLM-generated quality-related\nexplanations considered plausible by study participants, cat-\negorized by project and quality characteristics.\nverification is still required, given the low precision values,\nas false positives may occur. For the first research question,\nwe conclude that LLMs can accurately identify quality flaws\nin many cases and show the potential to guide stakeholders\ntoward quality issues they might have overlooked otherwise.\n2) Meaningful Explanations of LLM-Assessment (R2):\nThe second research question investigates the capability of\nLLMs to provide reasonable explanations for their quality\nassessments of software requirements. To answer this question,\nwe engaged study participants to review the decision and\nexplanation offered by the LLM for each requirement and\nquality dimension. As shown in Table VII, the majority of\nstudy participants agreed that the LLM could plausibly explain\nits assessment of software requirement quality issues in both\ninvestigated example projects. Therefore, we can affirmatively\nanswer the second research question. We conclude that the\nLLM can provide reasonable explanations for its decisions.\nThe explanations were considered meaningful, regardless of\nwhether the LLM classified the requirement as flawless or\nhaving a quality issue. This result is promising for the suc-\ncessful integration of this approach in real-life settings, as\nthe LLM’s decision is traceable for reviewers and can be\naccurately validated. Nonetheless, we recommend replicating\nthe experiment with more complex requirements to observe\nif the LLM-generated explanations can also help generate\nreliable explanations for those.\n3) LLM-improved Requirements (R3): The third research\nquestion is whether an LLM can suggest improved versions\nof flawed software requirements. To answer this question, we\ninstructed study participants to evaluate the LLM-proposed\nimproved versions of original requirements in the Stopwatch\nand DigitalHome projects. The study participants determined\nwhether the suggested version improved the original require-\nment. As shown in Table VIII, the suggested version was\nconsidered an improvement in most cases. As the number of\nrequirements with potential issues with respect to feasibility\nand necessity was small (see Tables IVb and V), there were\nonly a few examples where an improvement could be eval-\nuated. Therefore, more examples should be reviewed in the\nfuture to validate this aspect further.\nOverall, the outcome emphasizes the LLM’s capacity to\nassist reviewers in correcting flaws in software requirements,\nthereby contributing to an overall improvement in the quality\nof requirements. Based on the collected data, we can affirma-\nStopwatch DigitalHome\nappropriate 100% 66%\ncomplete 100% 100%\nconforming 100% 100%\ncorrect 100% 100%\nfeasible - 50%\nnecessary - 50%\nsingular 90% 66%\nunambiguous 100% 100%\nverifiable 100% 90%\nTABLE VIII: Percentages of LLM-generated requirement im-\nprovements considered enhancements compared to the original\nrequirements by study participants. Values are categorized by\nproject and quality characteristics. For feasibility and necessity\nin the Stopwatch project, values are not available as this quality\nissue was not identified in the requirements and hence could\nnot be improved.\ntively answer the third research question, indicating that the\nLLM can suggest versions of requirements that address flaws\nwithin a given quality dimension. This shows that LLMs may\noffer immediate benefits in software requirements engineering.\nV. D ISCUSSION\nA. Study Results\nThe results of this empirical study provide valuable insights\ninto the potential and capabilities of leveraging an LLM to sup-\nport the development of high-quality software requirements.\nAs demonstrated by the results, the LLM successfully iden-\ntified a majority of the incorporated quality issues, showing\nits utility in the requirements review process. This ability\ncan significantly reduce review time by directing reviewers\nto requirements that may contain quality flaws. However, it is\nimportant to note that the intervention of the reviewers remains\nnecessary, as the LLM did not identify all quality issues in\nthe examined samples. The level of collaboration between\nhumans and LLMs is a topic of recent research. Faggioli et\nal. [26] outline various integration stages from full human\njudgment to complete automation. We assume that LLMs are\ncurrently most beneficial in assisting humans by suggesting\nand explaining potential flaws for the quality assurance of soft-\nware requirements. Those need to be subsequently verified and\ncorrected with the help of the LLM. The current capabilities\nof LLMs are not sufficient to fully automate this task.\nThe LLM consistently offered meaningful explanations,\nregardless of whether participants agreed with the LLM’s\nquality assessment. This suggests that the LLM is transparent\nin articulating its reasoning, potentially helping reviewers\nunderstand why a requirement was identified as having a\nquality issue. Additionally, these explanations may encour-\nage reviewers to revise their assessments by considering an\nalternative perspective. We believe that explaining decisions\nis crucial for establishing LLM-assisted review processes for\nsoftware requirements. Such explanations could facilitate the\nvalidation of assessments and increase trust in the system.\nMoreover, the recommendation of improved requirements\nappears beneficial and reliable based on the evaluation of our\nexamples. The study participants agreed that most of the LLM-\nsuggested requirements improved their quality, regardless of\nwhether the study participants agreed with the LLM’s quality\nassessment. This additional perspective might help reviewers\nby providing an alternative viewpoint and potentially refining\nthe formulation of requirements where quality issues are not\nparticularly severe. In this study, the LLM was explicitly\ninstructed to correct the identified flaw concerning the assessed\nquality characteristic, ignoring the possibility that a require-\nment might require improvement across multiple dimensions.\nAddressing the combination of different dimensions in a single\nprompt instruction to achieve an overall enhanced requirement\nremains an important consideration for future research.\nOverall, we believe that LLMs have the potential to enhance\nthe quality of software requirements, particularly by reducing\nthe manual review effort. It is crucial to interpret the findings\nof our study as a proof-of-concept, emphasizing the need to\ndevelop and evaluate ideas regarding this application further.\nB. Threats to Validity\nWhile the study presented in this paper provides valuable\ninsights into the potential of assuring and improving the\nquality of software requirements with LLMs, we acknowledge\ncertain limitations. Firstly, the evaluation was conducted using\na hypothetical example project and only one real-life scenario,\nimpacting the generalizability of the presented results. To\nobtain better evidence of the general applicability, we plan\nto examine the presented approach within upcoming real-life\nprojects, using the LLM as a supportive tool during the review\nphase of requirements. With this, we hope to learn more about\nits usefulness in practical settings.\nSecondly, the investigated projects were relatively simple,\nmeaning that the reported results might differ for larger\nprojects with more complex requirements. Additionally, we\ndid not address the token limit aspect in the prompts for\nmore extensive project descriptions. In such cases, a Retrieval\nAugmented Generation (RAG) [27], [28] approach could be\nemployed to overcome this challenge by loading relevant\nproject data into the prompt context as needed to reduce the\ncontext length.\nThirdly, our study did not explore and compare different\noptions to instruct LLMs. The focus was on one specific model\n(Llama 2) and a single prompt template, making it impossible\nto estimate the general quality of LLMs in evaluating software\nrequirements. The usage of different LLMs or prompts might\nlead to other results. Consequently, we consider future studies\naddressing this aspect by comparing various prompting strate-\ngies and models. Given the extensive research on LLMs and\nthe ongoing release of improved model versions, it is important\nto continuously evaluate new approaches for interaction and\napplication in experimental studies.\nC. Future Work\nIn addition to assessing the quality of individual require-\nments, we see the potential for applying our approach to\nevaluate the quality characteristics of requirement sets. Similar\nto the quality dimensions for individual statements, the ISO\n29148 [14] specifies characteristics for sets or requirements,\nencapsulating the quality aspects of a consistent solution\nthat meets stakeholder expectations while adhering to project\nconstraints. We believe that extending our approach to cover\nthese aspects could yield additional benefits.\nAdditionally, we consider the potential of using an LLM to\nidentify hidden dependencies between requirements as an open\nissue. Existing approaches for detecting these dependencies\n[29], [30] use natural language techniques to extract features\nas a basis for classification of dependencies between require-\nments. In our future research, we aim to develop a framework\nfor instructing an LLM to handle this task. We believe that the\nextensive general knowledge of LLMs, combined with their\nability to understand specific project contexts, could signifi-\ncantly enhance the accuracy of identifying these dependencies.\nFurthermore, we intend to analyze additional projects to pro-\nvide a more general understanding of the capabilities of LLMs\nin improving software requirements. For this, we consider the\napplication of Retrieval Augmented Generation [27], [28] for\ncomplex and huge projects as a promising approach for future\nresearch. In this context, exploring strategies to identify and\naccess the relevant information is essential to let the LLM\nunderstand the project scope.\nVI. C ONCLUSIONS\nIn this paper, we analyzed the possibilities of using an LLM\nto evaluate the quality of software requirements in accordance\nwith the quality characteristics defined in the ISO 29148\nstandard. We discuss an approach for instructing the LLM to\nassess requirements, explain its decision-making process, and\nsuggest enhanced versions when quality issues are detected.\nTo evaluate the LLM’s capabilities in fulfilling these tasks,\nwe conducted an empirical study involving participants with\nbackgrounds in software engineering. The findings indicate\nthat the LLM accurately identifies the majority of requirements\nwith quality flaws, demonstrating its potential to assist in the\nrequirement engineering process. Moreover, the study reveals\nthat the LLM provides reliable explanations for its decisions,\nwhich can improve trust in the system. The evaluation of LLM-\nsuggested improvements to requirements with quality issues\nindicates their effectiveness in resolving quality concerns.\nTo summarize, this paper highlights the utility of LLMs\nin ensuring the quality of software requirements, suggesting\npromising potentials for stakeholder support throughout the\nsoftware requirements engineering process, and motivating\nfuture related research.\nREFERENCES\n[1] M. I. Kamata and T. Tamai, “How does requirements quality relate to\nproject success or failure?” in 15th IEEE International Requirements\nEngineering Conference (RE 2007) , 2007, pp. 69–78.\n[2] A. Hussain, E. O. Mkpojiogu, and F. M. Kamal, “The role of re-\nquirements in the success or failure of software projects,” International\nReview of Management and Marketing , vol. 6, no. 7, p. 306–311, 2016.\n[3] A. Felfernig, G. Ninaus, H. Grabner, F. Reinfrank, L. Weninger,\nD. Pagano, and W. Maalej, An Overview of Recommender Systems\nin Requirements Engineering . Berlin, Heidelberg: Springer Berlin\nHeidelberg, 2013, pp. 315–332. [Online]. Available: https://doi.org/10.\n1007/978-3-642-34419-0 14\n[4] Z. Kurtanovi ´c and W. Maalej, “Automatically classifying functional\nand non-functional requirements using supervised machine learning,”\nin 2017 IEEE 25th International Requirements Engineering Conference\n(RE), 2017, pp. 490–495.\n[5] A. Felfernig, M. Stettinger, M. Atas, R. Samer, J. Nerlich, S. Scholz,\nJ. Tiihonen, and M. Raatikainen, “Towards utility-based prioritization\nof requirements in open source environments,” in 2018 IEEE 26th\nInternational Requirements Engineering Conference (RE) , 2018, pp.\n406–411.\n[6] S. Vijayakumar and N. P. S., “Use of natural language processing in\nsoftware requirements prioritization – a systematic literature review,”\nInternational Journal of Applied Engineering and Management Letters\n(IJAEML), vol. 5, no. 2, p. 152–174, Nov. 2021. [Online]. Available:\nhttps://www.supublication.com/index.php/ijaeml/article/view/399\n[7] E. Parra, C. Dimou, J. Llorens, V . Moreno, and A. Fraga,\n“A methodology for the classification of quality of requirements\nusing machine learning techniques,” Information and Software\nTechnology, vol. 67, pp. 180–195, 2015. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S0950584915001299\n[8] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural\nlanguage processing via large pre-trained language models: A survey,”\nACM Comput. Surv. , vol. 56, no. 2, sep 2023. [Online]. Available:\nhttps://doi.org/10.1145/3605943\n[9] OpenAI, “Gpt-4 technical report,” 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2307.09288\n[10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale\net al. , “Llama 2: Open foundation and fine-tuned chat models,”\narXiv preprint arXiv:2307.09288 , 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2307.09288\n[11] A. Fantechi, S. Gnesi, L. Passaro, and L. Semini, “Inconsistency\ndetection in natural language requirements using chatgpt: a preliminary\nevaluation,” in 2023 IEEE 31st International Requirements Engineering\nConference (RE), 2023, pp. 335–340.\n[12] V . Bertram, H. Kausch, E. Kusmenko, H. Nqiri, B. Rumpe, and\nC. Venhoff, “Leveraging natural language processing for a consistency\nchecking toolchain of automotive requirements,” in 2023 IEEE 31st\nInternational Requirements Engineering Conference (RE) , 2023, pp.\n212–222.\n[13] D. Luitel, S. Hassani, and M. Sabetzadeh, “Using language models\nfor enhancing the completeness of natural-language requirements,” in\nRequirements Engineering: Foundation for Software Quality , A. Ferrari\nand B. Penzenstadler, Eds. Cham: Springer Nature Switzerland, 2023,\npp. 87–104.\n[14] International Organization for Standardization, “ISO/IEC/IEEE Inter-\nnational Standard - Systems and software engineering – Life cycle\nprocesses – Requirements engineering,” ISO/IEC/IEEE 29148:2018(E),\npp. 1–104, 2018.\n[15] S. Moore, R. Tong, A. Singh, Z. Liu, X. Hu, Y . Lu, J. Liang, C. Cao,\nH. Khosravi, P. Denny, C. Brooks, and J. Stamper, “Empowering\neducation with llms - the next-gen interface and content generation,” in\nArtificial Intelligence in Education. Posters and Late Breaking Results,\nWorkshops and Tutorials, Industry and Innovation Tracks, Practitioners,\nDoctoral Consortium and Blue Sky , N. Wang, G. Rebolledo-Mendez,\nV . Dimitrova, N. Matsuda, and O. C. Santos, Eds. Cham: Springer\nNature Switzerland, 2023, pp. 32–37.\n[16] H. Huang, S. Wu, X. Liang, B. Wang, Y . Shi, P. Wu, M. Yang, and\nT. Zhao, “Towards making the most of llm for translation quality\nestimation,” in Natural Language Processing and Chinese Computing ,\nF. Liu, N. Duan, Q. Xu, and Y . Hong, Eds. Cham: Springer Nature\nSwitzerland, 2023, pp. 375–386.\n[17] X. Hou, Y . Zhao, Y . Liu, Z. Yang, K. Wang, L. Li, X. Luo,\nD. Lo, J. Grundy, and H. Wang, “Large language models for software\nengineering: A systematic literature review,” 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2308.10620\n[18] L. Reynolds and K. McDonell, “Prompt programming for large\nlanguage models: Beyond the few-shot paradigm,” in Extended\nAbstracts of the 2021 CHI Conference on Human Factors in\nComputing Systems , ser. CHI EA ’21. New York, NY , USA:\nAssociation for Computing Machinery, 2021. [Online]. Available:\nhttps://doi.org/10.1145/3411763.3451760\n[19] L. Beurer-Kellner, M. Fischer, and M. Vechev, “Prompting is\nprogramming: A query language for large language models,” Proc.\nACM Program. Lang., vol. 7, no. PLDI, jun 2023. [Online]. Available:\nhttps://doi.org/10.1145/3591300\n[20] C. Cheligeer, J. Huang, G. Wu, N. Bhuiyan, Y . Xu, and Y . Zeng,\n“Machine learning in requirements elicitation: a literature review,” AI\nEDAM, vol. 36, p. e32, 2022.\n[21] C. Arora, J. Grundy, and M. Abdelrazek, “Advancing requirements\nengineering through generative ai: Assessing the role of llms,” 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2310.13976\n[22] W. Zhou, S. Zhang, H. Poon, and M. Chen, “Context-faithful\nprompting for large language models,” 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2303.11315\n[23] J. Huang and K. C.-C. Chang, “Towards reasoning in large\nlanguage models: A survey,” 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2212.10403\n[24] A. Ferrari, G. O. Spagnolo, and S. Gnesi, “Pure: a dataset of\npublic requirements documents,” Sep. 2018. [Online]. Available:\nhttps://doi.org/10.5281/zenodo.1414117\n[25] M. McHugh, “Interrater reliability: The kappa statistic,” Biochemia\nmedica : ˇcasopis Hrvatskoga druˇstva medicinskih biokemiˇcara / HDMB,\nvol. 22, pp. 276–82, 10 2012.\n[26] G. Faggioli, L. Dietz, C. L. A. Clarke, G. Demartini, M. Hagen,\nC. Hauff, N. Kando, E. Kanoulas, M. Potthast, B. Stein, and\nH. Wachsmuth, “Perspectives on large language models for relevance\njudgment,” in Proceedings of the 2023 ACM SIGIR International\nConference on Theory of Information Retrieval , ser. ICTIR ’23. New\nYork, NY , USA: Association for Computing Machinery, 2023, p.\n39–50. [Online]. Available: https://doi.org/10.1145/3578337.3605136\n[27] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\naugmented generation for knowledge-intensive nlp tasks,” Advances in\nNeural Information Processing Systems , vol. 33, pp. 9459–9474, 2020.\n[28] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun,\nQ. Guo, M. Wang, and H. Wang, “Retrieval-augmented generation\nfor large language models: A survey,” 2024. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2312.10997\n[29] M. Atas, R. Samer, and A. Felfernig, “Automated identification of type-\nspecific dependencies between requirements,” in 2018 IEEE/WIC/ACM\nInternational Conference on Web Intelligence (WI) , 2018, pp. 688–695.\n[30] R. Samer, M. Stettinger, M. Atas, A. Felfernig, G. Ruhe, and G. Desh-\npande, “New approaches to the identification of dependencies between\nrequirements,” in 2019 IEEE 31st International Conference on Tools\nwith Artificial Intelligence (ICTAI) , 2019, pp. 1265–1270.",
  "topic": "Software quality analyst",
  "concepts": [
    {
      "name": "Software quality analyst",
      "score": 0.7659913897514343
    },
    {
      "name": "Software quality assurance",
      "score": 0.7335269451141357
    },
    {
      "name": "Software quality control",
      "score": 0.717922031879425
    },
    {
      "name": "Software requirements",
      "score": 0.6364073753356934
    },
    {
      "name": "Software quality",
      "score": 0.5779258012771606
    },
    {
      "name": "Computer science",
      "score": 0.5773143172264099
    },
    {
      "name": "Quality assurance",
      "score": 0.5399278998374939
    },
    {
      "name": "Software engineering",
      "score": 0.5260317921638489
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5149217844009399
    },
    {
      "name": "Software development",
      "score": 0.5049653649330139
    },
    {
      "name": "Software quality management",
      "score": 0.4653523564338684
    },
    {
      "name": "Requirements analysis",
      "score": 0.46186739206314087
    },
    {
      "name": "Software requirements specification",
      "score": 0.4532809853553772
    },
    {
      "name": "Software project management",
      "score": 0.4414498507976532
    },
    {
      "name": "Requirements engineering",
      "score": 0.42311161756515503
    },
    {
      "name": "Software peer review",
      "score": 0.41903048753738403
    },
    {
      "name": "Requirements management",
      "score": 0.4178040027618408
    },
    {
      "name": "Software",
      "score": 0.39009344577789307
    },
    {
      "name": "Software construction",
      "score": 0.3885715901851654
    },
    {
      "name": "Engineering",
      "score": 0.2599894404411316
    },
    {
      "name": "Operations management",
      "score": 0.07838302850723267
    },
    {
      "name": "External quality assessment",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4092182",
      "name": "Graz University of Technology",
      "country": "AT"
    }
  ],
  "cited_by": 19
}