{
  "title": "Large Language Models (LLMs) and Empathy – A Systematic Review",
  "url": "https://openalex.org/W4385623654",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A3046694060",
      "name": "Danna Brin",
      "affiliations": [
        "Tel Aviv University",
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2572125234",
      "name": "Yiftach Barash",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A4212339766",
      "name": "Alexander Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University",
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2809724045",
      "name": "Vera Sorin",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A3046694060",
      "name": "Danna Brin",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A2572125234",
      "name": "Yiftach Barash",
      "affiliations": [
        "Sheba Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2155772135",
      "name": "Eli Konen",
      "affiliations": [
        "Sheba Medical Center",
        "Tel Aviv University"
      ]
    },
    {
      "id": "https://openalex.org/A4212339766",
      "name": "Alexander Charney",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A2419298921",
      "name": "Girish Nadkarni",
      "affiliations": [
        "Icahn School of Medicine at Mount Sinai"
      ]
    },
    {
      "id": "https://openalex.org/A1945837108",
      "name": "Eyal Klang",
      "affiliations": [
        "Sheba Medical Center",
        "Icahn School of Medicine at Mount Sinai"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2890009290",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4378783467",
    "https://openalex.org/W4383501093",
    "https://openalex.org/W4377024131",
    "https://openalex.org/W4383909162",
    "https://openalex.org/W4375859913",
    "https://openalex.org/W4364377826",
    "https://openalex.org/W4380291947",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4384455669",
    "https://openalex.org/W3195074401",
    "https://openalex.org/W4378470708",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W4378908072",
    "https://openalex.org/W1969676322",
    "https://openalex.org/W2168222508",
    "https://openalex.org/W2169547194",
    "https://openalex.org/W4376122030",
    "https://openalex.org/W3003968294"
  ],
  "abstract": "Abstract Purpose Empathy, a cornerstone of human interaction, is a unique quality to humans that Large Language Models (LLMs) are believed to lack. Our study aims to review the literature on the capacity of LLMs in demonstrating empathy Methods We conducted a literature search on MEDLINE up to July 2023. Seven publications ultimately met the inclusion criteria. Results All studies included in this review were published in 2023. All studies but one focused on ChatGPT-3.5 by OpenAI. Only one study evaluated empathy based on objective metrics, and all others used subjective human assessment. The studies reported LLMs to exhibits elements of empathy, including emotions recognition and providing emotionally supportive responses in diverse contexts, most of which were related to healthcare. In some cases, LLMs were observed to outperform humans in empathy-related tasks. Conclusion LLMs demonstrated some aspects of empathy in variable scenarios, mainly related to healthcare. The empathy may be considered “cognitive” empathy. Social skills are a fundamental aspect of intelligence, thus further research is imperative to enhance these skills in AI.",
  "full_text": "Large Language Models (LLMs) and Empathy – A Systematic Review \nVera Sorin, MD1-3; Danna Brin, MD1,2; Yiftach Barash, MD1-3; Eli Konen, MD1,2; Alexander \nCharney, MD, PhD4-5; Girish Nadkarni, MD4-5; Eyal Klang, MD1-5 \n \n1Department of Diagnostic Imaging, Chaim Sheba Medical Center, Tel Hashomer, Israel \n2The Faculty of Medicine, Tel-Aviv University, Israel \n3DeepVision Lab, Chaim Sheba Medical Center, Tel Hashomer, Israel \n4Division of Data-Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA \n5The Charles Bronfman Institute of Personalized Medicine, Icahn School of Medicine at Mount Sinai, \nNew York, New York, USA. \n \nCorresponding Author: \nVera Sorin, MD \nDepartment of Diagnostic Imaging, Chaim Sheba Medical Center  \nAddress: Emek Haela St. 1, Ramat Gan, Israel, 52621.  \nTel: +972-3-5302530, Fax: +972-3-5357315, Email: verasrn@gmail.com \n \nAbstract \nPurpose: Empathy, a cornerstone of human interaction, is a unique quality to humans that \nLarge Language Models (LLMs) are believed to lack. Our study aims to review the literature \non the capacity of LLMs in demonstrating empathy \nMethods: We conducted a literature search on MEDLINE up to July 2023. Seven \npublications ultimately met the inclusion criteria.  \nResults: All studies included in this review were published in 2023. All studies but one \nfocused on ChatGPT-3.5 by OpenAI. Only one study evaluated empathy based on objective \nmetrics, and all others used subjective human assessment. The studies reported LLMs to \nexhibits elements of empathy, including emotions recognition and providing emotionally \nsupportive responses in diverse contexts, most of which were related to healthcare. In some \ncases, LLMs were observed to outperform humans in empathy-related tasks.  \nConclusion: LLMs demonstrated some aspects of empathy in variable scenarios, mainly \nrelated to healthcare. The empathy may be considered “cognitive” empathy. Social skills are \na fundamental aspect of intelligence, thus further research is imperative to enhance these \nskills in AI.  \n \n \n  \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nIntroduction \nEmpathy, a fundamental aspect of human interaction, can be characterized as the ability to \nexperience the emotions of another being within oneself. The origin of the word \"empathy\" \ndates back to the 1880s, when Theodore Lipps determined the word “einfuhlung” (“in-\nfeeling”) to describe the emotional appreciation of another’s feelings (1). Empathy involves \nrecognition of others' feelings, the causes of these feelings, and the ability to participate in an \nemotional experience of an individual without becoming part of it (1). In the context of \nhealthcare, empathy enables health care professionals and patients to communicate. It is \ndescribed as \"the ability to see the world through someone else’s eyes\", having the ability to \nimagine what someone else is thinking and feeling in a given situation (2).  \nLarge Language Models (LLMs) have demonstrated remarkable capabilities across various \ntasks\n, including text summarization, question-answering, and text generation (3). There are \nnumerous studies on potential applications in healthcare, as an educational tool and as a \nsupport tool in clinical work (4, 5). Some publications suggest that despite impressive natural \nlanguage processing abilities, LLMs lack empathy, a quality that is unique to humans (6-9).  \nFew studies in the literature, discuss and evaluate LLMs performance in tasks associated with \nemotional intelligence, theory of mind, and empathy. Thus, the aim of our study was to \nsystematically review the literature on the capacity of LLMs in demonstrating empathy. \nMethods \nWe searched the literature on LLMs and empathy using MEDLINE. Studies published up to \nJuly \n2023 were included. The search query was “((\"large language models\") OR (llms) OR \n(gpt) OR (chatgpt)) AND ((empathy) OR (\"emotional awareness\") OR (\"emotional \nintelligence\") OR (emotion))”. \nThe initial search yielded 34 studies. We also searched the \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nreferences lists of relevant studies, including some key studies from major medical journals, \nfor any additional studies that may have been missed during the initial search. This process \nresulted in the retrieval of additional three studies.  \nThe inclusion criteria for our study were English language full-length publications that \nevaluated empathy in LLMs outputs. Excluded were papers evaluating other topics related to \nemotional intelligence that were not specifically empathy. For example, papers focusing on \n\"theory of mind\" were excluded if they did not specifically address empathy. Two reviewers \n(VS, EK) independently performed the search and screened the titles and abstract of the \narticles resulting from the search. Differences in search results were resolved through \ndiscussion to reach a consensus. The reviewers then screened selected articles’ full-text for \nfinal inclusion. Ultimately, a total of seven publications were included in this review. Figure \n1 presents a flow diagram of the screening and inclusion process. \nResults \nAll seven studies included in this review were published in 2023. Despite the emergence of \nvarious LLMs, all studies but one focused on ChatGPT-3.5 by OpenAI. Six out of seven \nstudies evaluated ChatGPT’s empathy based on subjective human evaluation, as opposed to \nobjective metrics. All studies but one evaluated empathy in ChatGPT in medical context. The \nresults of the studies included are summarized in Table 1.\n \nEmpathy is essential in medicine, particularly when breaking bad news to patients. It allows \nphysicians to deliver difficult information in a manner that respects the patient's emotions and \nperspective. Webb (10) \nused ChatGPT to simulate a role-play of breaking bad news in the \nemergency department. The chatbot successfully set up a training scenario, role-played as a \npatient, and provided clear feedback through the application of the SPIKES (Setting up, \nPerception, Invitation, Knowledge, Emotions with Empathy, and Strategy or Summary) \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nframework for breaking bad news (10). In another study, Yeo et al. (11) tested ChatGPT's \nability to provide emotional support to patients diagnosed with hepatocellular carcinoma, and \ntheir caregivers. ChatGPT was able to acknowledge the likely emotional response of the \npatient to their diagnosis. Furthermore, the chatbot provided clear and actionable starting \npoints for a newly diagnosed patient, and offered motivational responses encouraging \nproactive steps. For caregivers, ChatGPT provided psychological and practical \nrecommendations (11).  \nAyers et al. (12) compared the quality and empathy of responses given by ChatGPT and \nphysicians to 195 randomly drawn patient questions from a social media forum. The study \nfound that patients preferred the chatbot’s responses over physician responses in 78.6% of \ncases. ChatGPT’s responses were rated significantly higher for both quality and empathy, \nwhile physician responses were 41% less empathetic than the chatbot responses. The authors \nnoted that ChatGPT tended to provide more lengthy responses, which could potentially be \nerroneously associated with greater empathy. They concluded that the chatbot may have \npotential in aiding drafting \nresponses to patient questions (12). \nAnother study also assessed empathy in chatbot’s responses to patient’s questions. Liu et al. \n(13) developed a model based on a pre-trained LLaMA-65B and fine-tuned to generate \nphysician-like responses that are professional and empathetic. They evaluated the model on \nten actual patient questions in primary care, and compared the responses to those generated \nby ChatGPT-3.5 and GPT-4, rating them based on empathy, responsiveness, accuracy and \nusefulness. When evaluating empathy, GPT-4 and ChatGPT\n-3.5 outperformed their model. \nInterestingly, all language models outperformed physician-generated responses significantly \n(13). \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nUnderstanding and addressing patients' emotions is fundamental in mental health.  Chen et al. \n(14) used ChatGPT-powered chatbots to simulate psychiatrists and patients in clinical \npsychiatric scenarios. The chatbots showed potential in simulating some aspects of empathy. \nHowever, they sometimes forgot initial instructions and repeated general empathy phrases too \noften. They also asked fewer in-depth questions about symptoms compared to physicians, \npotentially affecting their ability to fully understand the patient's condition. When simulating \npatients, the \nchatbots reported symptoms inaccurately (14). \nThe Levels of Emotional Awareness Scale (LEAS) is a psychological tool that assesses an \nindividual's capacity to identify and describe emotions in themselves and others, a \nfundamental aspect of empathy (15). Elyoseph et al. (16) compared the LEAS score of \nChatGPT to the general population norms. They found that ChatGPT demonstrated \nsignificantly higher emotional awareness performance. When repeating the test following one \nmonth interval, the chatbot’s performance further improved, almost reaching the maximum \npossible LEAS score.\n The authors propose that ChatGPT could be helpful for cognitive \ntraining of people with emotional awareness impairment, as well as for psychiatric \nassessment support \n(16).  \nZhao et al. (17) compared ChatGPT to supervised models in terms of emotional dialogue \nunderstanding and generation. The tasks they assessed included emotion recognition, emotion \ncause recognition, dialog act classification, empathetic response generation, and emotional \nsupport conversation. The authors found that while supervised models surpassed ChatGPT in \nemotion recognition, ChatGPT produced longer, more diverse, and context-specific \nresponses, especially when interacting with users in negative emotional states. Interestingly, \nZhao et al. (17) also observed a repetitive pattern in ChatGPT's empathy expressions, similar \nto the results described by Chen et al (14).  \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nDiscussion \nThis review shows that LLMs exhibit aspects of empathy, including recognition of emotions, \nand generation of emotionally supportive responses. Most studies evaluated empathy \nexhibited in LLMs in variable medical tasks. Some of the studies pointed out potential pitfalls \nin assessing empathy of LLMs, such as mistaking lengthy responses for increased empathy. \nOther challenges discussed involve the absence of human communication aspects in LLMs \nsuch as eye contact and tone of voice. Limitations unrelated directly to empathy included \noccasional deviations from correct responses and the models' short-term memory constraints. \nLLMs have shown impressive abilities in semantic understanding and logical reasoning (3). \nThis review supports the idea that LLMs may also demonstrate some abilities that resemble \nsocial intelligence. Theory of mind involves\n the understanding of others thoughts and \nemotions, and predicting or explaining their behaviors based on these inferences. This \nconcept is fundamental to social interactions, and it is a complex task, as it involves \nunderstanding not just the literal meaning of words in a conversation, but the underlying \nintentions, beliefs, and emotions (18). Several studies evaluated LLMs on theory of mind \ntasks, with varied performance, depending on the tasks and the models used (18-22).  \nThe definition of empathy varies among researchers and practitioners in social sciences (1). \nOne of the debates is whether it is a cognitive or affective concept, and most definitions of \nempathy include both (1). Cognitive empathy involves the ability to understand another’s \nfeelings, closely related to theory of mind (23). Affective empathy relates to experiencing \nemotions in response to an emotional stimulus (1). The ability of LLMs to demonstrate \nempathy in various fields as highlighted in this review, seems to align more with the \ncognitive aspect. It is nevertheless surprising \nthat in some cases the LLM outperformed \nhumans in empathy related tasks.  \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nResearch suggests that cognitive and affective empathy are distinct. For instance, people with \nautism often struggle with cognitive empathy but have normal levels of affective empathy, \nwhile psychopathic individuals typically show the reverse pattern (23). Neurological studies \ndemonstrated distinct brain regions associated with each type of empathy, which further \nsupports \nthis notion (24, 25). This differentiation raises questions about the potential \nevolution of artificial general intelligence (AGI). It is worth questioning if demonstrating \ncognitive empathy alone is sufficient, or whether affective empathy is imperative for \nachieving human-like emotional intelligence. If humans cannot distinguish between \nresponses generated by humans and LLMs, or if they prefer AI-generated responses as \ndemonstrated in the study by Ayers et al., perhaps emulating such empathy may be enough. \nNumerous studies support the remarkable performance of LLMs in clinical reasoning (4, 5), \nThese models can be applied to enhance the medical care patients receive, while decreasing \nthe workload of healthcare providers (26). Yet, empathy is a key factor in patient care. \nEmpathy in healthcare communication is linked to improved patient satisfaction, adherence to \ntreatment plans, and better outcomes (27). It allows for a more nuanced understanding of \npatients' emotional states and experiences, facilitating more compassionate and person-\ncentered care. As such, the ability of LLMs to integrate empathy can significantly enhance \nthe role of AI in healthcare, for both patients and healthcare providers. \nThis review has several limitations. First, as all but one study evaluated empathy based on \nsubjective assessment, we could not perform a meta-analysis. Second, we only assessed \nstudies directly discussing empathy, while there are many more that evaluate theory of mind \ntasks that are \nclosely related to “cognitive” empathy. Third, all studies assessed ChatGPT-3.5, \nand only one study evaluated a model based on LLaMA and GPT-4. This can potentially \nlimit the generalizability of findings to other LLMs. It is possible that alternative LLMs may \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \npresent different empathy characteristics. Moreover, LLMs are evolving fast, and possibly \nnewer LLMs will present higher cognitive like abilities. \nTo conclude, this review demonstrates that LLMs exhibit elements of cognitive empathy, \nbeing able to recognize emotions and provide emotionally supportive responses in various \ncontexts. Given that social skills are foundational to the concept of “intelligence”, further \nresearch is warranted to further develop that aspect in AI. Ultimately, as we continue to refine \nthese models, we approach closer to bridging the gap between artificial and human-like \ninteractions, opening opportunities for empathetic AI applications.   \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nReferences \n \n1. Cuff BMP, Brown SJ, Taylor L, Howat DJ. Empathy: A Review of the Concept. \nEmotion Review. 2014;8(2):144-53. \n2. Hajibabaee F, A. Farahani M, Ameri Z, Salehi T, Hosseini F. The relationship \nbetween empathy and emotional intelligence among Iranian nursing students. International \nJournal of Medical Education. 2018;9:239-43.\n \n3. Sallam M. ChatGPT Utility in Healthcare Education, Research, and Practice: \nSystematic Review on the Promising Perspectives and Valid Concerns. Healthcare. \n2023;11(6):887. \n4.\n Sorin V, Klang E, Sklair-Levy M, Cohen I, Zippel DB, Balint Lahat N, et al. Large \nlanguage model (ChatGPT) as a support tool for breast tumor board. npj Breast Cancer. \n2023;9(1). \n5.\n Barash Y, Klang E, Konen E, Sorin V. ChatGPT-4 Assistance in Optimizing \nEmergency Department Radiology Referrals and Imaging Selection. Journal of the American \nCollege of Radiology. 2023. \n6.\n Ilicki J. A Framework for Critically Assessing ChatGPT and Other Large Language \nArtificial Intelligence Model Applications in Health Care. Mayo Clinic Proceedings: Digital \nHealth. 2023;1(2):185-8. \n7.\n Nashwan AJ, Abujaber AA, Choudry H. Embracing the future of physician-patient \ncommunication: GPT-4 in gastroenterology. Gastroenterology & Endoscopy. 2023;1(3):132-\n5.\n \n8. Sun Y-X, Li Z-M, Huang J-Z, Yu N-z, Long X. GPT-4: The Future of Cosmetic \nProcedure Consultation? Aesthetic Surgery Journal. 2023;43(8):NP670-NP2. \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \n9. Carlbring P, Hadjistavropoulos H, Kleiboer A, Andersson G. A new era in Internet \ninterventions: The advent of Chat-GPT and AI-assisted therapist guidance. Internet \nInterventions. 2023;32:100621. \n10.\n Webb JJ. Proof of Concept: Using ChatGPT to Teach Emergency Physicians How to \nBreak Bad News. Cureus. 2023. \n11.\n Yeo YH, Samaan JS, Ng WH, Ting P-S, Trivedi H, Vipani A, et al. Assessing the \nperformance of ChatGPT in answering questions regarding cirrhosis and hepatocellular \ncarcinoma. Clinical and Molecular Hepatology. 2023;29(3):721-32. \n12. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing \nPhysician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a \nPublic Social Media Forum. JAMA Internal Medicine. 2023;183(6):589. \n13.\n Liu S, McCoy AB, Wright AP, Carew B, Genkins JZ, Huang SS, et al. Leveraging \nLarge Language Models for Generating Responses to Patient Messages. 2023. \n14.\n Chen S, Wu M, Zhu KQ, Lan K, Zhang Z, Cui L. LLM-empowered Chatbots for \nPsychiatrist and Patient Simulation: Application and Evaluation. arXiv preprint \narXiv:230513614. 2023. \n15.\n Lane RD, Smith R. Levels of Emotional Awareness: Theory and Measurement of a \nSocio-Emotional Skill. Journal of Intelligence. 2021;9(3):42. \n16. Elyoseph Z, Hadar-Shoval D, Asraf K, Lvovsky M. ChatGPT outperforms humans in \nemotional awareness evaluations. Frontiers in Psychology. 2023;14. \n17. Zhao W, Zhao Y, Lu X, Wang S, Tong Y, Qin B. Is ChatGPT Equipped with \nEmotional Dialogue Capabilities? arXiv preprint arXiv:230409582. 2023. \n18. Bubeck S, Chandrasekaran V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al. Sparks of \nartificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:230312712. \n2023. \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \n19. Sap M, LeBras R, Fried D, Choi Y. Neural theory-of-mind? on the limits of social \nintelligence in large lms. arXiv preprint arXiv:221013312. 2022. \n20. Kosinski M. Theory of mind may have spontaneously emerged in large language \nmodels. arXiv preprint arXiv:230202083. 2023. \n21.\n Marchetti A, Di Dio C, Cangelosi A, Manzi F, Massaro D. Developing ChatGPT’s \nTheory of Mind. Frontiers in Robotics and AI. 2023;10. \n22.\n Moghaddam SR, Honey CJ. Boosting Theory-of-Mind Performance in Large \nLanguage Models via Prompting. arXiv preprint arXiv:230411490. 2023. \n23.\n Blair RJR. Fine Cuts of Empathy and the Amygdala: Dissociable Deficits in \nPsychopathy and Autism. Quarterly Journal of Experimental Psychology. 2008;61(1):157-70. \n24.\n Shamay-Tsoory SG, Aharon-Peretz J, Perry D. Two systems for empathy: a double \ndissociation between emotional and cognitive empathy in inferior frontal gyrus versus \nventromedial prefrontal lesions. Brain. 2009;132(3):617-27.\n \n25. Zaki J, Weber J, Bolger N, Ochsner K. The neural bases of empathic accuracy. \nProceedings of the National Academy of Sciences. 2009;106(27):11382-7. \n26.\n Sorin V, Barash Y, Konen E, Klang E. Large language models for oncological \napplications. Journal of Cancer Research and Clinical Oncology. 2023;149(11):9505-8. \n27.\n Moudatsou M, Stavropoulou A, Philalithis A, Koukouli S. The Role of Empathy in \nHealth and Social Care Professionals. Healthcare. 2020;8(1):26. \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nFigure 1. Flow Diagram of the Inclusion Process. Flow diagram of the search and inclusion \nprocess based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses \n(PRISMA) guidelines \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nRecords identified through database searching (n = 34) \nScreening \nIncluded \n Eligibility \n Identification \nAdditional records identified through other sources \n(n = 3) \nRecords after duplicates removed (n = 37) \nRecords screened (n = 37) \n Records excluded as irrelevant to \nsubject (n = 18) \nFull-text articles assessed for \neligibility (n = 19) \nFull-text articles excluded.  \nThe studies did not report explicitly on \nempathy in LLMs. \n(n = 12) \n \nStudies included (n = 7) \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nTable 1. Studies Evaluating Aspects of Empathy Exhibited by Large Language Models \nStudy \n(ref) \nYear Journal Objective LLM Model Key Findings Limitations of LLMs \nWebb et \nal. (10) \n2023 Cureus Breaking bad news in emergency \nmedicine \nChatGPT-3.5 ChatGPT facilitated realistic scenario design, \nactive roleplay, and effective feedback through \nthe application of the SPIKES framework for \nbreaking bad news. \nSpecific limitations were not \ndiscussed. General \nlimitations discussed include \nlimited training dataset, \nperformance being \ninfluenced by prompts \ndesign, potential for \ninaccurate responses and \ninability to convey parts of \nhuman communications such \nas eye contact, pausing to \nlisten, and tone. \n \nAyers et \nal. (12) \n2023 JAMA \nIntern \nMed \nEmpathetic responses to patient \nquestions \nChatGPT-3.5 ChatGPT responses were preferred by \nevaluators over physicians in 78.6% evaluations \nand were rated of significantly higher quality \nand empathy. \nNot specifically discussed. \nChatGPT tended to provide \nmore lengthy responses, \nwhich could potentially be \nerroneously associated with \ngreater empathy. The study \ndid not assess the chatbot \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nresponses for accuracy or \nfabricated information. \nChen et al. \n(14) \n2023 arXiv Simulating psychiatrists and \npatients in clinical psychiatric \nscenarios, and evaluating the \nexpression of empathy in the \ninteractions \n \nChatbots based \non ChatGPT \nChatGPT-powered chatbots showed feasibility \nin simulating some aspects of empathy in \npsychiatric interactions  \nThe chatbots sometimes \nforgot initial instructions and \nshowed excessive repetition \nof general empathy phrases. \nAdditionally, they asked \nfewer in-depth questions \nabout the symptoms \ncompared to human doctors, \npotentially affecting their \nability to fully understand the \npatient's condition. The \npatient chatbots reported \nsymptoms inaccurately. \nZhao et al. \n(17) \n2023 arXiv Evaluate emotional dialogue \nunderstanding and generation \nand compare to other supervised \nmodels. \nChatGPT-3.5 Supervised models surpassed ChatGPT in \nemotion recognition. ChatGPT produced longer \nresponses, but responses were also more \nspecific to the context of the conversation \ncompared to other models. \n \nChatGPT generated longer \nresponses, and deviated from \nreference responses when \nevaluated based on word \noverlap metrics. It also \ndemonstrated limited \nunderstanding of some \nlabels. ChatGPT did not \nadhere to the same guidelines \nto determine emotions as \nwere used for the annotated \ndata for the supervised \nmodels.  \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint \nYeo et al. \n(11) \n2023 Clin Mol \nHepatol \nEmotional support for cirrhosis \nand HCC patients \nChatGPT-3.5 ChatGPT emulated empathetic responses and \noffered actionable recommendations for \npatients and caregivers. \nIn total, ChatGPT provided \ncomprehensive answers in \nless than 50% of questions. \nElyoseph \net al. (16) \n2023 Front. \nPsychol \nEmotional awareness \nperformance compared to the \ngeneral population norms \nChatGPT-3.5 ChatGPT demonstrated significantly higher \nemotional awareness performance than general \npopulation norms, with potential improvements \nover time \nNot specifically discussed.  \nLiu et al. \n(13) \n2023 medRxiv Fine-tuning an LLM to generate \nresponses to patient questions \nLLM based on \nLLaMA-65B; \nChatGPT-3.5, \nGPT-4 \nGPT-4 and ChatGPT-3.5 outperformed their \nmodel. Interestingly, all language models \noutperformed physician-generated responses \nsignificantly. \nOne of the reviewer \nindicated “excessive \nempathy” in the fine-tuned \nLLM model. Responses \ngenerated by GPT models \nwere considered too lengthy \nand required a relatively high \nreading level. \nRef= reference; LLM= large language model \nAll rights reserved. No reuse allowed without permission. \nperpetuity. \npreprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in \nThe copyright holder for thisthis version posted August 7, 2023. ; https://doi.org/10.1101/2023.08.07.23293769doi: medRxiv preprint ",
  "topic": "Empathy",
  "concepts": [
    {
      "name": "Empathy",
      "score": 0.9641051292419434
    },
    {
      "name": "Psychology",
      "score": 0.5659400224685669
    },
    {
      "name": "Cognition",
      "score": 0.43352001905441284
    },
    {
      "name": "Inclusion (mineral)",
      "score": 0.4138951301574707
    },
    {
      "name": "Clinical psychology",
      "score": 0.32888296246528625
    },
    {
      "name": "Social psychology",
      "score": 0.2983136773109436
    },
    {
      "name": "Psychiatry",
      "score": 0.18062660098075867
    }
  ]
}