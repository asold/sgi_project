{
    "title": "Controlling AI Agent Participation in Group Conversations: A Human-Centered Approach",
    "url": "https://openalex.org/W4406975180",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2099876185",
            "name": "Stéphanie Houde",
            "affiliations": [
                "Cambridge Scientific (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2921978399",
            "name": "Kristina Brimijoin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1987018281",
            "name": "Michael Müller",
            "affiliations": [
                "Cambridge Scientific (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2160287110",
            "name": "Steven I. Ross",
            "affiliations": [
                "Cambridge Scientific (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5102691680",
            "name": "Dario Andres Silva Moran",
            "affiliations": [
                "Biochemistry Research Institute of La Plata"
            ]
        },
        {
            "id": "https://openalex.org/A5067623601",
            "name": "Gabriel Enrique Gonzalez",
            "affiliations": [
                "Institute of Scientific and Technical Research for Defense"
            ]
        },
        {
            "id": "https://openalex.org/A2804412112",
            "name": "Siya Kunde",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2793315185",
            "name": "Morgan A. Foreman",
            "affiliations": [
                "IBM Research - Austin"
            ]
        },
        {
            "id": "https://openalex.org/A1997726331",
            "name": "Justin D. Weisz",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4393335480",
        "https://openalex.org/W4385210529",
        "https://openalex.org/W4393872032",
        "https://openalex.org/W3093223973",
        "https://openalex.org/W4393970695",
        "https://openalex.org/W3117150885",
        "https://openalex.org/W3130426944",
        "https://openalex.org/W4221086478",
        "https://openalex.org/W2152562124",
        "https://openalex.org/W2602896535",
        "https://openalex.org/W4236677243",
        "https://openalex.org/W4396832396",
        "https://openalex.org/W4385734158",
        "https://openalex.org/W3173222693",
        "https://openalex.org/W4407075514",
        "https://openalex.org/W4399465031",
        "https://openalex.org/W2059216172",
        "https://openalex.org/W2922256442",
        "https://openalex.org/W4386707955",
        "https://openalex.org/W4390813588",
        "https://openalex.org/W2012279340",
        "https://openalex.org/W2940824926",
        "https://openalex.org/W3191131351",
        "https://openalex.org/W4381190733",
        "https://openalex.org/W3195850212",
        "https://openalex.org/W4383682782",
        "https://openalex.org/W4393118307",
        "https://openalex.org/W4384890810",
        "https://openalex.org/W4393118629",
        "https://openalex.org/W4385988359",
        "https://openalex.org/W3031000691",
        "https://openalex.org/W4210888212",
        "https://openalex.org/W4381185498",
        "https://openalex.org/W3113988938",
        "https://openalex.org/W2890307675",
        "https://openalex.org/W2942671448",
        "https://openalex.org/W3043359018",
        "https://openalex.org/W4396832390",
        "https://openalex.org/W4391417542",
        "https://openalex.org/W2034025926",
        "https://openalex.org/W4400763603",
        "https://openalex.org/W2796000004",
        "https://openalex.org/W2561158378",
        "https://openalex.org/W2123179704",
        "https://openalex.org/W2482371029",
        "https://openalex.org/W1635768081",
        "https://openalex.org/W2144244493",
        "https://openalex.org/W3184436113",
        "https://openalex.org/W4321013654",
        "https://openalex.org/W2955853369",
        "https://openalex.org/W2942454454",
        "https://openalex.org/W4396833086",
        "https://openalex.org/W78536566",
        "https://openalex.org/W4213145388",
        "https://openalex.org/W4295210883",
        "https://openalex.org/W3163054706",
        "https://openalex.org/W4360991235",
        "https://openalex.org/W2124654639",
        "https://openalex.org/W3102606416",
        "https://openalex.org/W4296830306",
        "https://openalex.org/W4391506256",
        "https://openalex.org/W2109463833",
        "https://openalex.org/W4400361079",
        "https://openalex.org/W2290652328",
        "https://openalex.org/W4386322397",
        "https://openalex.org/W4295955444",
        "https://openalex.org/W4387394580"
    ],
    "abstract": "Conversational AI agents are commonly applied within single-user, turn-taking scenarios. The interaction mechanics of these scenarios are trivial: when the user enters a message, the AI agent produces a response. However, the interaction dynamics are more complex within group settings. How should an agent behave in these settings? We report on two experiments aimed at uncovering users' experiences of an AI agent's participation within a group, in the context of group ideation (brainstorming). In the first study, participants benefited from and preferred having the AI agent in the group, but participants disliked when the agent seemed to dominate the conversation and they desired various controls over its interactive behaviors. In the second study, we created functional controls over the agent's behavior, operable by group members, to validate their utility and probe for additional requirements. Integrating our findings across both studies, we developed a taxonomy of controls for when, what, and where a conversational AI agent in a group should respond, who can control its behavior, and how those controls are specified and implemented. Our taxonomy is intended to aid AI creators to think through important considerations in the design of mixed-initiative conversational agents.",
    "full_text": "Controlling AI Agent Participation in Group Conversations: A Human-Centered\nApproach\nSTEPHANIE HOUDE, IBM Research, USA\nKRISTINA BRIMIJOIN, IBM Research, USA\nMICHAEL MULLER, IBM Research, USA\nSTEVEN I. ROSS, IBM Research, USA\nDARIO ANDRES SILVA MORAN,IBM Research, Argentina\nGABRIEL ENRIQUE GONZALEZ, IBM Research, Argentina\nSIYA KUNDE, IBM Research, USA\nMORGAN A. FOREMAN, IBM Research, USA\nJUSTIN D. WEISZ, IBM Research, USA\nConversational AI agents are commonly applied within single-user, turn-taking scenarios. The interaction mechanics of these scenarios\nare trivial: when the user enters a message, the AI agent produces a response. However, the interaction dynamics are more complex\nwithin group settings. How should an agent behave in these settings? We report on two experiments aimed at uncovering users’\nexperiences of an AI agent’s participation within a group, in the context of group ideation (brainstorming). In the first study, participants\nbenefited from and preferred having the AI agent in the group, but participants disliked when the agent seemed to dominate the\nconversation and they desired various controls over its interactive behaviors. In the second study, we created functional controls\nover the agent’s behavior, operable by group members, to validate their utility and probe for additional requirements. Integrating our\nfindings across both studies, we developed a taxonomy of controls for when, what, and where a conversational AI agent in a group\nshould respond, who can control its behavior, and how those controls are specified and implemented. Our taxonomy is intended to aid\nAI creators to think through important considerations in the design of mixed-initiative conversational agents.\nCCS Concepts: • Human-centered computing →Natural language interfaces ; Collaborative interaction; Empirical studies\nin collaborative and social computing ; • Computing methodologies →Intelligent agents; Cooperation and coordination .\nAdditional Key Words and Phrases: LLM, Generative AI, Group Brainstorming, Co-creativity, Conversational agent, Multi-party\nconversation, Mixed initiative interfaces, Mixed initiative creative interfaces.\nACM Reference Format:\nStephanie Houde, Kristina Brimijoin, Michael Muller, Steven I. Ross, Dario Andres Silva Moran, Gabriel Enrique Gonzalez, Siya Kunde,\nMorgan A. Foreman, and Justin D. Weisz. 2025. Controlling AI Agent Participation in Group Conversations: A Human-Centered\nAuthors’ Contact Information: Stephanie Houde, IBM Research, Cambridge, MA, USA, stephanie.houde@ibm.com; Kristina Brimijoin, IBM Research,\nYorktown Heights, NY, USA, kbrimij@us.ibm.com; Michael Muller, IBM Research, Cambridge, MA, USA, michael_muller@us.ibm.com; Steven I. Ross,\nIBM Research, Cambridge, MA, USA, steven_ross@us.ibm.com; Dario Andres Silva Moran, IBM Research, La Plata, BA, Argentina, dario.silva@ibm.com;\nGabriel Enrique Gonzalez, IBM Research, Necochea, BA, Argentina, gabriel.gonzalez@ibm.com; Siya Kunde, IBM Research, Yorktown Heights, NY, USA,\nskunde@ibm.com; Morgan A. Foreman, IBM Research, Houston, TX, USA, Morgan.Foreman@ibm.com; Justin D. Weisz, IBM Research, Yorktown Heights,\nNY, USA, jweisz@us.ibm.com.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\n© 2025 Copyright held by the owner/author(s).\nManuscript submitted to ACM\nManuscript submitted to ACM 1\narXiv:2501.17258v1  [cs.HC]  28 Jan 2025\n2 Houde et al.\nApproach. In 30th International Conference on Intelligent User Interfaces (IUI ’25), March 24–27, 2025, Cagliari, Italy. ACM, New York,\nNY, USA, 31 pages. https://doi.org/10.1145/3708359.3712089\n1 Introduction\nThe widespread availability of conversational agents, particularly those based on large language models (LLMs), has\naffected how people complete tasks. It is increasingly common for people to ask an agent for assistance with a writing\ntask [34], a coding task [ 52, 64, 83], or a summarization task [ 2, 26]. Often, these tasks are implemented through\nconversational user interfaces in which the interaction mechanics are trivial: the (human) user makes a statement\nor request to the AI agent and the AI agent generates a reply via LLM inference using the conversational history as\ncontext.\nIn contrast to the one-on-one nature of these human-AI interactions, a large number of professional work projects\noccur in groups, where asynchronous and real-time group discussions take place [9]. AI agent participation in such\ndiscussions could help provide additional ideas and resources that enrich both the work processes and outcomes [65, 79].\nHowever, in group conversations, it is not straightforward to determine when and how an AI agent should contribute.\nFor example, it might be inappropriate for the agent to respond to every utterance made by every member of the group\nin a lively discussion. Group interaction by an agent requires a more flexible strategy involving judgement of what\ncontent should be shared when.\nTo understand the challenges of designing an agent that “fits in” to a group setting, we built an LLM-based con-\nversational agent prototype called “Koala. ” Koala is an AI-based group discussion participant, situated in Slack1 and\nimplemented as a bot application. We conducted two user studies to understand Koala’s impact on the dynamics of an\nideation activity and to explore the design space for its interactive behaviors.\nIn our first user study, we established an initial understanding of the value of including an AI agent within a group\nbrainstorming session as perceived by study participants, along with human-centered requirements for the agent’s\nbehavior. We engaged small groups in real-time brainstorming conversations (akin to [5, 67, 75]), in Slack channels\nconfigured with different levels of AI participation: (1) without Koala, (2) with a reactive variant of Koala that only\nresponded when directly addressed, and (3) with a proactive variant of Koala that made its own determinations of\nwhen to contribute. Participants preferred completing the exercise with, rather than without, Koala, and they reacted\npositively to the ideas it generated and the way it could help keep the conversation moving. However, participants\nraised significant concerns about feeling overwhelmed and interrupted by the proactive variant of Koala and expressed\nnumerous ways in which they desired to control its behavior.\nBased on feedback from the first study, we created a new variant called Koala II by updating its underlying LLM,\nprompt, and algorithms to improve its proactive behaviors. We also created a settings panel to control both the content\nand display of its contributions. In a second study, we re-recruited our participants to experience Koala II. We also\ndeveloped a series of design probes to explore other ways of controlling Koala’s interactive behaviors than the ones\nwe were able to implement in our settings panel. Our refinements of Koala were effective, with participants noting\nits improved timeliness and adherence to the topics. Participants found the ability to control Koala II’s interactive\nbehavior very useful, but they also wished to control other aspects of its behavior. These insights revealed a range\nof controllable aspects, and ways to control them, that culminated in the creation of a taxonomy for understanding\nimportant considerations in the design of proactive conversational agents.\nOur paper makes the following contributions:\n1Slack. https://slack.com\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 3\n•The results of a mixed-methods analysis that found that a conversational AI agent was valued as a member of a\ngroup brainstorming team, even while its interactive behaviors did not always suit the group’s dynamic.\n•A taxonomy derived from two user studies that identifies aspects of an AI agent’s interactive behaviors in a\ngroup conversation along with ways those aspects can be controlled (Figure 7).\n•A conceptual extension of mixed-initiative interaction frameworks (e.g., [ 15, 51, 59, 68, 69]) that challenges\nassumptions that an AI agent’s interactive behaviors are fixed or solely determined by the workflow in which it\nis embedded; rather, users may dynamically adjust these behaviors throughout the course of their interaction\nwith it.\n2 Related work\nWe outline three areas relevant to our study of AI agent participation in group settings: research on mixed-initiative\ninterfaces and frameworks for human-AI interaction, the act of brainstorming as a collaborative activity, and guidelines\nfor the design of conversational agents.\n2.1 Mixed-initiative interfaces and frameworks for human-AI interaction\nOne of the earliest technical accounts of joint work between human and computing technologies was what has become\nknown as the Fitts Allocation, published in 1951 [15]. Fitts proposed that certain types of work should be assigned to\neither a human or a machine, according to a set of rules about which party was best suited to perform which tasks.\nNewer frameworks were developed by Sheridan [68] and Parasuraman et al. [59], but these contained the same basic\nassumption that human initiative and agency traded off with technological initiative and agency.\nRecent research on mixed-initiative interactions [23] and mixed-initiative creative interfaces [12, 73], along with\nmodern human-AI interaction frameworks [51, 69] suggest that initiative between humans and intelligent systems does\nnot lie on a single trade-off axis; rather, they comprise a two-dimensional space in which both parties can independently\nact with lower or higher levels of initiative. Muller and Weisz [51] further demonstrate how the initiative of each actor\nmay dynamically change during the course of the interaction.\nOne limitation of these frameworks is that, although they can describe that an AI agent can act with lower or\nhigher levels of initiative, they do not help designers and practitioners determine how that agent ought to behave\nin any given situation. Recent work has examined the design space of proactive interactions, such as by passively\nrecommending visualizations in a data science context [76] or by determining when to interrupt a user with a notification\nor informational message [43, 63]. As Kraus et al. [31] note, proactivity can be a “double-edged sword” that must be\nconfigured properly to maintain users’ preferences and trust [30, 45]. Without such configuration, users may resist agent-\nprovided information, such as help services [20]. Jain et al. [25] used participatory methods to create design patterns\nfor a proactive auto-response messaging agent. Zargham et al. [88] used a different approach – visual storyboards – to\nlearn users’ preferred patterns for what they called the “proactivity dilemma” in design. Gammelgård-Larsen et al. [17]\nbuilt prototypes to compare users’ responses to intermittent vs. continuous vs. proactive agent behaviors in a music\nrecommendation application. Liao et al. [35] summarized portions of these learnings in a full-day SIGIR tutorial.\nHowever, such work neglects an important part of the design space: how should AI agents behave in group settings\nwhere the interaction dynamics are more complex? With partial success, neural networks have been trained to handle\nthe semantics [21], relevance [81] and addressees [57] of AI responses in multi-party conversations (e.g., “who says\nwhat to whom?”). This work has not yet considered contextual factors, such as when to interject a question or response,\nand how that when depends on the task at hand [ 1]. For example, users of Oh et al .’s DuetDraw, an interface that\nManuscript submitted to ACM\n4 Houde et al.\nallows users to draw collaboratively with AI, wanted to lead the interaction and only wanted AI explanations when\nrequested [55]. By contrast, Koch et al. [29] found that 14 of 16 professional designers preferred to work with an AI\nthat took initiative in proposing inspirational materials for creating a digital mood board [29].\nMcComb et al. [42] examined the use of AI within engineering design and drew a distinction between reactive AI\napplications, in which AI acts in response to human activities, and proactive AI applications, in which AI acts more as a\npartner or guide. Similarly, Moruzzi and Margarido [49] identify a dimension of engagement in which an AI’s approach\nto interaction may either be “suggesting” or “taking initiative. ”\nIn this paper, we build upon these concepts by using a human-centered approach to explore what it means for an AI\nagent to act “proactively, ” within the complexity of group conversational settings. Our work extends existing human-AI\ninteraction frameworks by developing a new taxonomy for how agents should participate in groups.\n2.2 Group ideation as a testbed for examining AI’s interactive behaviors\nTo explore an AI agent’s interactive behaviors in group conversational settings, we needed a purpose or reason for\nthe agent to interact with other people. We chose group ideation – the process by which a group comes together\nto brainstorm ideas on a topic or theme [ 56, 60] – as it is a familiar practice, it can be conducted over a textual\ncommunications channel [28, 80], and LLM-based conversational AI agents have been shown to contribute meaningfully\nin this task [44, 49, 50, 54, 67].\nIn Osborn’s classic analysis of brainstorming, participants may propose ideas in any order during a divergence phase,\nand participants may evaluate and select or curate a subset of those ideas for future use during aconvergence phase [56].\nFor our purposes in examining an AI agent’s interactive behaviors, both phases – divergence and convergence – include\nmoments in which the next conversational turn may be taken by any of the parties involved in the activity (i.e. human\nor AI). Brainstorming thereby presents exactly the types of interaction problems we want to investigate: when (and\nimplicitly, why) should the AI contribute to the on-going discussion?\nIncorporating AI is the most recent technological enhancement to group ideation, although the impact of AI agents\non dyadic and group co-creativity are mixed. Geerts et al. [19] conducted brainstorming sessions in which a scripted\ndialog was presented as both a wizard-of-oz AI and as a human, and found no difference in the experiences of the\nparticipants [19]. Maier et al. [41] facilitated co-located groups in brainstorming sessions with human and AI facilitators,\nboth of whom had issues such as blocking one another’s contributions and making interruptions (human) and deviating\nfrom the topic and exhibiting periods of silence (AI), leaving participants frustrated. Memmert and Tavanapour [44]\nfound an increase in cognitive stimulation from exposure to other team members’ perspectives, but also notes the risks\nof free riding or reduced human effort with AI-generated contributions [44].\nIn contrast, both Wieland et al. [82] and Yu-Han and Chun-Ching [87] found that brainstorming with an AI partner,\ninstead of a human one, was associated with an increase in the number and diversity of ideas. Bouschery et al . [4]\nevaluated nominal groups (each person brainstorms alone), interactive (human-only) groups, hybrid (human-AI) groups\nand AI-only groups and concluded, “hybrid groups outperform both interactive and nominal groups in terms of the\nnumber of generated ideas and perform on par in terms of brainstorming creativity while requiring only half the human\nresources” [4]. Within hybrid human-AI brainstorming groups, Muller et al. [50] showed that hybrid ideas – those that\nwere either created or modified by at least one person and the AI – were more likely to be identified as being the best\nones generated by the group. Using a related group task (brainwriting), Shaer et al. [67] reported that the AI sometimes\nproduced so much content as to interfere with the work of the people in each group. Thus, we observe that group\nideation is a suitable task for our work as the interaction dynamics of an AI agent are non-trivial.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 5\n2.3 Design guidelines for conversational agents\nConversational AI agents have experienced widespread adoption, fueled by early advances in intent-based classification\nsystems [16, 24], and more recently, LLM-based approaches [37, 58, 89]. Recently, a new practice of conversational UX\ndesign has emerged to focus on how to design the interactions that a human user has with conversational agents [46–\n48, 71]. These design systems focus on identifying “recognizable interaction types... [which] frees designers from having\nto reinvent how users interact with the application and frees users from having to learn new interaction methods\nso both can focus on the content of the particular application. ” [48, p.1]. Although these design systems and sets of\nguidelines are useful aids to the designers of conversational AI systems, they typically focus on single-user, task-based\nuse cases. Our work seeks to expand the design space of conversational agents by enumerating controllable elements of\nparticipation within group settings.\n3 Koala: A conversational agent for group ideation\nB\nB.2\nB.1\nA.2\nA.1\nA\nFig. 1. Koala as an AI participant in Slack. These screenshots (with human participant names redacted) show examples of Koala\nparticipating in a Slack channel. (A) The reactive variant of Koala replies to a user’s question addressed to “Koala” (A.1) or to “@Koala”\n(A.2). (B) The proactive variant of Koala generates a proactive reply (B.1) to the conversation and a reactive reply (B.2) in response to a\ndirect request.\nTo support our research goals, we developed Koala, a multi-party conversational agent designed to collaborate with\npeople in group ideation tasks. We built Koala as a Slack application [72] to enable it to participate in group chat spaces;\nwe specifically chose Slack because it is the primary chat application used within our organization and its API enabled\nus to develop an AI agent that could participate in group conversations. From a user perspective, “Koala” was just\nanother participant within a Slack channel [32, 66], with an “APP” tag to indicate that it was not a person [70].\nWe used the Llama 2 model [78] to drive Koala’s conversational capability as it was a state-of-the-art model2 that\nprovided the highest-quality responses in our internal testing.\n2Given the rapid pace at which AI has been advancing, the title of “state of the art” is a short-lived one; in Section 5, we describe how the second-generation\nof Koala was built with Llama 3 [14].\nManuscript submitted to ACM\n6 Houde et al.\nAn important design dimension of Koala is determining when it should make a conversational utterance. Other Slack\nbots, such as Anthropic’s Claude3, require users to explicitly mention the bot’s name to elicit a response from it (i.e.\n“@”-ing the bot). Given our desire to experiment with proactive ways of participating, we developed two configurable\nvariants of Koala: a reactive variant that only responds when directly addressed (by either “@Koala” or “Koala”), and a\nproactive variant that responds when it determines that it has something valuable to say. Koala can contribute to the\nconversation in the same manner as other human users, by sending textual messages as well as emoji-based reactions\nto existing messages. We show a screenshot of Koala in Figure 1.\n3.1 Determining when to speak: Koala’s proactive behaviors\nFig. 2. Koala operational logic. When users post a message in a Slack (A), the post triggers an event that is handled by the\nKoala backend (B) where control logic determines whether Koala should either immediately pass on replying or hand off for further\nevaluation in the autonomy control logic (C) where the LLM (D) generates a response that is further evaluated for potential posting\nin the channel.\nConversational AI agents typically engage in one-on-one conversations with human users, where each message\nfrom the user receives a response from the agent. In a group chat where multiple people are talking to (and sometimes\non top of) one another, the decision of when a proactive agent should respond to any particular user message becomes\nnon-trivial; some mechanism is needed to determine when the agent should respond to a user’s message [18, 21, 57].\nIn developing the proactive variant of Koala, we knew that writing imperative decision logic to determine what\nconstitutes a valuable contribution would prove impossible; therefore, we relied on the underlying LLM to not only\nproduce conversational responses in response to users’ messages, but also to score those responses for the extent to\nwhich they made a valuable contribution to the conversation. We outline this process in Figure 2; at a high level, Koala\nis instructed to not reply to messages where it is clearly not the intended recipient, and to reply to messages when it\nestimates that the response it generated makes a valuable contribution to the conversation. We note that our proactive\nvariant of Koala represents a limited form of proactivity as it’s process is triggered by the receipt of a message from a\nhuman user; a truly proactive variant of Koala would also be able to send messages to the group even when the human\nparticipants remain silent [84]. We provide a complete listing of the prompts used by Koala to generate responses to\nmembers of the group and score them for their value in Appendix A.\n3Claude. https://www.anthropic.com/index/claude-now-in-slack\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 7\nDuring Koala’s development, we noticed that the underlying LLM was unreliable in identifying the intended target\nfor a chat utterance. Thus, we incorporated additional, external control logic to force Koala to reply if it’s name was\ndetected in a user’s message, and suppress Koala from replying under other circumstances, such as when the names of\none of the other participants were mentioned, regardless of how the LLM scored the value of the reply’s content.\n3.2 User studies & ethics statement\nWe conducted two user studies with Koala to understand the impact of its proactive behaviors on group ideation\n(Section 4) and develop a robust understanding of the ways in which users would like to control those behaviors\n(Section 6). Our research was conducted in accordance with our institution’s internal policies on human-participant\nresearch. Specifically, participants provided their informed consent, participation was voluntary and could be withdrawn\nat any time, and all collected data were anonymized before analysis and publication.\n4 Study 1: Group brainstorming with Koala\nOur first study focused on understanding people’s experience working with the reactive and proactive variants of Koala\nin a series of group ideation scenarios. We designed this study to address two research questions:\n•RQ1.1. How does the participation of an AI agent transform the brainstorming process?\n•RQ1.2. How do people characterize the experience of brainstorming with a reactive vs. proactive AI agent?\n4.1 Method\nWe recruited small groups of IBM employees to participate in a series of virtual brainstorming tasks with (or without)\nKoala as an active participant. Groups performed three brainstorming tasks, with each task focused on a different\nvariant of Koala’s participation:\n•No AI, in which Koala was not present in the channel. This condition enabled us to establish a baseline user\nexperience of brainstorming on Slack.\n•Reactive AI, in which Koala acted as another participant in the brainstorm, but was configured to only respond\nwhen directly addressed.\n•Proactive AI, in which Koala actively participated in the brainstorm and made its own decisions about when to\ncontribute. As with the reactive variant, the proactive variant always responded when directly addressed.\nWe used two tools to coordinate the study and run the brainstorming activities: participants joined a web conference\nto provide their informed consent and receive instructions for the task, then they muted themselves and turned their\nattention to Slack where the brainstorming activities occurred. Each session was facilitated by a human moderator,\nwho introduced the brainstorming activity, clarified any confusions about the topic, and maintained time constraints.\nAt no point did the moderator influence brainstorming outcomes.\n4.1.1 Brainstorming task and topics. Groups were provided with three minutes to brainstorm on a topic with (or\nwithout) Koala. Pre-testing indicated that this short amount of time was suitable for producing a meaningful number of\nideas without the session feeling protracted. We developed three topics that were relevant to a generic work environment\nand non-specific to any particular job role or expertise.\n•Topic A. How can we improve hybrid meetings for remote participants?\n•Topic B. How can we better engage employees to use a chatbot for HR services?\nManuscript submitted to ACM\n8 Houde et al.\n•Topic C. What kinds of giveaways should we take to a customer conference?\nWe assigned topics in a counterbalanced fashion across the three conditions to avoid an order effect. However, each\ngroup experienced the three conditions in the same order (i.e. first No AI, then Reactive AI, then Proactive AI), as we\nwanted to probe participants on their experiences with increasingly autonomous levels of AI involvement.\nAt the conclusion of the three minutes, we asked participants (including Koala) to select the three top ideas that\nthey felt were the strongest of the ones they ideated, although the human participants had final decision authority.\nThen, we had participants fill out a short survey that asked about their experience. We show a depiction of the overall\nstructure of the study in Figure 3. Participation in our study took approximately one hour, for which each participant\nwas compensated with the equivalent of $25 USD.\nWe collected numerous data from each brainstorming session: transcripts of the brainstorming conversations (e.g.\nthe Slack messages sent by each participant, plus Koala), observational notes taken by the study moderator, and surveys\nafter each brainstorming topic.\nBrainstorming\nRound 1\nBrainstorming\nRound 2\nBrainstorming\nRound 3\nSurvey \n1\nSurvey \n2\nSurvey \n3\nNo AI Reactive AI Proactive AI\nA, B, or C A, B, or C A, B, or C\nCondition Condition ConditionTopic Topic Topic\n+ +\nFig. 3. Study 1 overview. During each session, a group of participants sequentially completed three rounds of brainstorming and\npost-brainstorm surveys. The order of conditions (No AI, Reactive AI, Proactive AI) was kept the same for each group to assess the\nimpact of increasing levels of AI autonomy on participants’ experience. Brainstorming topics were assigned in a counterbalanced\nfashion to negate order effects.\n4.1.2 Participants. We conducted a total of six study sessions, each having three (human) participants, for a total of 18\nparticipants. In our analysis, we refer to individual participants using the notation Pk.m, where k is the number of the\ngroup in which they participated (1-6), and m is their identifier within the group (1-3)4.\nAll participants were employees of IBM. Our participants included 10 who identified as female (55.5%), 7 who\nidentified as male (38.8%), and 1 who preferred not to disclose their gender identity (5.5%). Geographically, two-thirds of\nour participants were from the United States (12; 66.6%), followed by Argentina (3; 16.6%) and Germany (3; 16.6%). Our\nparticipants’ job roles included content strategy, user research, machine learning, UX design, software engineering, and\nvideo production. They reported interacting with conversational AI assistants at least once a week or more frequently\n(7; 38.8%), interacting with them about once a month (9; 50%), or interacting them less frequently (2; 11.1%).\nWe recruited participants in groups of three to ensure groups had a minimum of two participants in the event a\nparticipant had to cancel. Since prior work has shown that groups of people who are familiar with each other are more\nproductive in ideation tasks than ad-hoc groups [11, 61] due to their ability to communicate more effectively [62], we\nrecruited people who had prior working relationships with each other.\n4We use this same notation in Study 2 as the participants were the same people.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 9\n(a)\n (b)\nFig. 4. (a) Impact of Koala on the production of ideas and (b) participants’ preferences for the different Koala variants.\n4.1.3 Survey. We developed a post-task survey, with minor variations for each of our three conditions, that asked\nopen-ended questions about whether participants felt the session was effective, whether they were happy with the\nideas that were produced, whether their team worked well together, and which conditional experience they preferred.\nWe also included a set of questions, based on the Creativity Support Index [13] and thesis work of Asio [3] to gauge the\nextent to which people felt Koala contributed to their creative process. We made minor modifications to these validated\nscales to fit the conversational AI modality of our research. We provide a listing of all survey questions in Appendix B.\n4.2 Results\n4.2.1 RQ1.1. Koala’s impact on the brainstorming process.To understand how Koala impacted the brainstorming process,\nwe conducted several analyses of our data. First, we considered the number of ideas produced within each group across\nthe different conditions, as determined by a coding process by three authors (Figure 4a). We observed that 73% of\nall ideas produced during either of the AI conditions were contributed by Koala. Not only was the volume of ideas\nincreased with Koala’s presence, but Koala’s ideas also comprised a third (33%) of all of the top ideas. Participants also\npreferred having either Koala variant in their group over not having it, with the reactive variant receiving the most\npreference (Figure 4b). These results suggest Koala had a positive impact on the outcomes of the brainstorming task5.\nWe also considered how Koala impacted participants’ work process. We conducted a reflexive thematic analysis [6]\non open-ended survey responses and text transcripts of the brainstorming sessions. We used an inductive approach in\nwhich three researchers collaboratively labeled these data, iterating until they came to an agreement on important\nthemes. Through this process, we constructed three top-level themes with a number of subthemes to identify how Koala\nimpacted the work process. The thematic analysis made it clear that Koala had a mixed effect. It provided participants\nwith valuable support (Advantages), such as by summarizing ideas contributed across the group or helping get the\nbrainstorming off the ground. But participants also talked about how Koala’s participation was distracting and intrusive\n(Disadvantages), especially when it contributed too much, too often. Finally, we noted that participants desired to\nimprove Koala’s behavior (Improving AI behavior ) via some kind of control mechanism.\nTheme 1. Advantages\n•Help getting started . Almost half of participants commented that the reactive variant helped their group\nget started in their brainstorming task. P2.3 spoke of “removing the ‘white page’ problem, ” and P6.1 remarked,\n5For a more extensive analysis on how brainstorming ideas evolved between participants and Koala, please refer to Muller et al. [50].\nManuscript submitted to ACM\n10 Houde et al.\n“Koala was able to add to the conversation when we were getting stuck, ” indicating that Koala’s ideas were useful\nthroughout the brainstorming session.\n•Perceived speed. Several participants indicated that compared to not having Koala, the reactive variant of Koala\nsped up the process. P2.2 wrote, “It was more fast working with Koala, ” and P2.3 said, “Koala made it much more\nfluid and expedited. ”\n•Structure. Some participants noted that Koala provided structure to their session. In P6.2’s experience, “Koala’s\nsuggestions helped structure the conversation... Koala added structure and gave us a springboard for ideas. It served\nas a psuedo-moderator [sic]. ”\n•Summaries. Nearly half of participants made positive comments about the value of Koala’s ability to provide\nsummaries of ideas. P4.1 said, “I liked the structuring/summarization function of Koala, ” and P3.1 similarly\ncommented, “The summarizations that Koala created were valuable and helped keep us more focused. ”\n•Validation. Some participants commented that seeing ideas from Koala that were similar to their own gave\nthem a sense of validation. P1.1 wrote that being able to ask Koala specific questions “allowed our team to be\ncreative and come up with relevant ideas, verify them with Koala, then move forward. ”\n•Information. A few participants noted that it was useful to have Koala fill in knowledge gaps as opposed to\noffering new ideas. P1.3 commented, “Koala was helpful at providing data on issues that I didn’t know about. ”\n•Human-AI collaboration. Several participants commented on the benefits of having both people and Koala\nin the discussion. P4.1 wrote, “The collaboration was much better through using the chat together with the Koala\nideas that we could all see. For me it was interesting to see what the others asked Koala to get some inspirations for\nmy own prompts. ” P4.2 said, “I think we worked very well together because we equally spoke to each other and to\nKoala in the chat. ” P2.3 similarly remarked on the value of human-AI collaboration, saying, “Very innovative ideas\nresulted combining innovative ideas from people to ask Koala for concrete ideas. ”\nTheme 2. Disadvantages\n•Disruptive proactivity . The primary complaint about Koala was in regard to its proactive variant. Many\nparticipants felt its contributions were intrusive and caused them to feel distracted and overwhelmed. We discuss\nthese issues in detail in Section 4.2.2, which motivated changes to Koala’s behavior (Section 5).\n•A stifling effect . Participants indicated that in some instances, Koala’s presence had a stifling effect on human\nconversation and creativity. P5.1 commented that, “The session started [with humans] asking Koala to answer the\nquestion, which then led to mostly narrowing down the choices Koala presented. There was less room for expressing\nunique ideas as a result. ” P1.2 noted, “Sometimes brainstorming or great ideas will come from out-of-box ideas but I\nfelt I boxed myself. ” These stifling effects may be related to an online form of production blocking (e.g., [53, 74]).\n•Inaccurate answers . Approximately one-third of participants noticed inaccuracies in Koala’s responses or\nsummaries. P2.3 observed that the summaries provided by Koala included “summaries or top themes that handn’t\neven been mentioned in the chat. ” P3.2 simply said, “the summarization was incorrect, ” and P1.2 said that Koala\n“struggled especially [...] to present the ideas generated in slack. ” Such issues are common with LLMs [22] and\nmitigating them is an active area of research (e.g. [77]).\nTheme 3. Improving AI behavior\n•Regulating AI behavior . Participants noted behaviors of the proactive variant of Koala that they found to be\nobjectionable (Section 4.2.2). These complaints were sometimes accompanied by requests for improved default\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 11\nbehaviors about when and how much Koala should contribute. For example, P4.3 suggested that Koala should\n“Give some extra time to answer, so you allow real people to answer first” . P4.2 wondered if perhaps ”Koala could\nask ’Would you like me to share the top 3’ instead of writing a big sentence without being asked. ”\n•User control of AI . Beyond wanting more regulated default behaviors, some participants attempted to actively\ncontrol Koala’s behavior within the chat. We discuss this point further in Section 4.2.3.\n4.2.2 RQ1.2. Reactive versus proactive behaviors.As previously seen in Figure 4b, the majority of participants (13; 72.2%)\npreferred interacting with the reactive variant of Koala. Participants indicated a number of reasons why they preferred\nthe reactive variant to the proactive variant.\n•Distraction. Multiple participants across all six groups commented that the proactive variant distracted their\nteam from the task. P3.3 commented on proactive Koala, saying, “[it] was distracting and didn’t help at all, and\npicked one of its own ideas as a favorite. ” P6.1 indicated that proactive Koala “was too talkative, both in length of\nmessage and frequency. ”\n•Intrusiveness. At least half of participants commented that the proactive variant intruded too much on their\ngroup’s conversation. Koala jumped in at the wrong times, wrote too much, responded too frequently, and\ngenerally got in the way of the humans having the discussion. P2.1 remarked, “I think Koala was too much\nproactive at the beginning which lead us the humans to lose focus. ” P6.2 was more pointed in their feedback, saying,\n“Koala dominated the conversation. It felt like a pedantic student who wouldn’t create space for others to participate. ”\nP5.2 felt that their team “...did generate less ideas though, but that maybe because they were interrupted a lot by\nKoala. [...] A room for improvement would be to ask Koala to pause for some time before she gets back into the\nconversation. ”\n•Feeling overwhelmed. Participants expressed feelings of being “overwhelmed” (P2.3) by proactive Koala, likely\nin reaction to its distractions and intrusions. P4.2 explained, “I was having a hard time building up on the ideas\nfrom the others, because everything moved so fast. ” P5.1 similarly felt, “Koala was helpful, but also produced so\nmuch text so quickly that it was hard to keep up with the conversation. ”\nSome participants saw potential benefits in having some degree of proactive involvement from Koala, but in its\ncurrent form, the benefits of proactivity were not enough. P1.3 explained, “I liked channel 2 [Reactive AI] & 3 [Proactive\nAI] best. I think i would pick channel 3 [Proactive AI] if there was a way to temporarily disable koala if needed. ” P4.1\nsimilarly said, “I think I would prefer a combination of Channel2 [Reactive AI] + Channel3 [Proactive AI] - where Koala can\nadd suggestions when not answered, but not always as the very first person. It felt a bit like we ‘lost the race against the\nmachines, ’ because Koala’s answers were always soo [sic] quick and mostly eloquent. ”\n4.2.3 Desire to control interactive behaviors.We observed several attempts by participants to control the interactive\nbehaviors of Koala during the brainstorming sessions. For example, P6.3 asked proactive Koala to cease its participation,\nsaying, “koala - leave the rest to us, ok?” and later in the same session, “Koala - thanks for these ideas. We’ll ping with a\nquestion if you’re needed. ” But, not all attempts at controlling Koala’s behavior were aimed at reducing or minimizing\nits participation; sometimes, participants made comments to encourage more participation from Koala. For example,\nP3.1 asked reactive Koala to take initiative: “Can Koala help choose the 3 solutions?” P1.1 and P1.2 both provided\nencouragement to Koala, as well as an additional opportunity to take a conversational turn, when they said (at different\ntimes), “Great start, Koala. ”\nManuscript submitted to ACM\n12 Houde et al.\nThese attempts at behavioral control, coupled with comments expressing such desires (Improving AI behavior ),\nled us to revisit our implementation of Koala’s interactive behaviors and conduct a deeper examination of exactly what\nkinds of controls are needed to make Koala a more compatible group participant.\n5 Koala II: Exploring dimensions of control over interactive behaviors\nBased on participants’ experiences in Study 1, we revised Koala to (1) improve its proactive behaviors (Section 5.1), and\n(2) parameterize some of its behavioral attributes and expose them in the user interface for users to control (Section 5.2).\n5.1 Enhancements to proactive behaviors\nWe made a number of changes to Koala’s proactive variant based on participants’ feedback that it was distracting,\nintrusive, and left them feeling overwhelmed.\n•We switched the underlying LLM used by Koala to Llama 3 [14], which was released after we ran Study 1. In our\ntesting, we found the new model was significantly less likely to hallucinate or produce inaccurate answers, due\nin part to a doubling of its token context length and increased capacity to incorporate more of the conversational\nhistory in its prompts.\n•We revised Koala’s prompt to enhance the quality of its responses (detailed in Appendix A). We prompted Koala\nto provide more targeted and collaborative suggestions, to provide constructive criticism on existing ideas, and\nto contribute novel ideas, all while remaining mindful of not dominating the conversation.\n•We simplified the process by which Koala used to assess the relevance of its ideas. Previously, Koala made an\noverall determination – a score from 0 to 100 that evaluated the value of its response to the group – to decide\nwhether a generated response should be sent to the group, based on a fixed threshold. Now, we made this\nthreshold configurable, allowing participants to adjust it at any time during the conversation.\n5.2 Exposing behavioral controls\nParticipants in Study 1 wanted explicit control over Koala’s interactive behaviors. Prior work on human-AI interactions\nwith generative AI systems has explored ways to give users various controls over the outputs of the underlying\ngenerative model (e.g. [8, 10, 39, 40]), but we recognized the types of controls users needed were not solely related to\nthe content of Koala’s messages, but also to the ways those messages were contributed within the group setting. We\nalso recognized that these types of controls were ones that could not be manifested solely through prompt engineering\nand that external mechanisms would be needed to implement them.\nWe built a control panel for Koala (Figure 5) based on our thematic analysis and an assessment of what was technically\nfeasible within the Slack environment. These controls were accessible at any point in time and could be changed by any\nmember of the group.\n•Proactive versus reactive toggle (Figure 5a). We added the ability for users to choose which type of Koala\nvariant they preferred: the proactive variant that made its own decisions for when to make a conversational\nutterance, or the reactive variant which only replied to users when directly addressed.\n•Proactive contribution threshold (Figure 5b). Given our changes to how Koala scored its potential response,\nwe exposed a coarse contribution threshold for when its response should be sent to the group. Through experi-\nmentation, we set the “high” threshold at 90, the “medium” threshold at 75, and the “low” threshold at 50 on the\n0-100 scale used by Koala.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 13\nA\nB\nC\nD\nFig. 5. Control settings. Koala II’s control settings provides users with the ability to control aspects of its interactive behaviors.\nThese settings include (A) switching between proactive and reactive variants; (B) adjusting the threshold at which the proactive\nvariant deems a generated response valuable to send to the group; (C) selecting whether Koala’s responses should appear in the\nconversation or in a thread; and (D) managing how long messages are displayed.\n•Where to respond (Figure 5c). Slack allows messages to be posted in two places: within the channel itself, or as\na threaded reply to a message. Previously, Koala would only place its replies in the channel, but we exposed an\noption to have Koala reply in a thread to make it clearer to which messages Koala’s replies were targeted. We\nadded this option due to our observations that Koala would sometimes take time to reply to messages after the\nconversation had already moved on, making it difficult for participants to understand for which message Koala’s\nresponse was intended.\n•Long message display (Figure 5d). Participants felt overwhelmed by Koala’s responses in part because they\ncould potentially be very long (such as when Koala offered enumerated lists of ideas rather than producing them\none at a time). We added an option for those cases in which Koala produced a long comment (> 1,000 characters)\nto place a truncated version in the chat with the full version appearing in a threaded response.\nWe recognize that these controls represent only a portion of the potential design space, so we conducted a second\nstudy to gather additional feedback and synthesize all of our findings into a more robust taxonomy (Figure 7).\n6 Study 2: Group brainstorming revisited\nParticipants in Study 1 felt that the proactive variant of Koala could be beneficial, so long as its behavior could be\ntempered to avoid the distraction, intrusiveness, and overwhelming feelings they experienced. Nine months after the\nfirst study, we ran a second study to evaluate whether our changes to Koala’s behaviors, along with the addition of\nbehavioral controls, made a difference in the quality of the experience. In addition, we aimed to elicit additional types\nManuscript submitted to ACM\n14 Houde et al.\nof controls needed for conversational AI agents such as Koala using both the control settings panel (Figure 5) and\nadditional design mockups (Figure 6) as design probes. Thus, we designed our second study to address the following\nresearch questions:\n•RQ2.1. How did Koala II’s modified proactive behaviors impact peoples’ experiences?\n•RQ2.2. What additional types of controls are needed over an AI agent’s interactive behaviors?\n6.1 Method\nWe used the same method for Study 2 as Study 1, except we reduced the number of brainstorming tasks to two instead\nof three to make time for a discussion on controlling an AI’s interactive behaviors.\n6.1.1 Participants. To make comparisons with Koala I, we re-recruited the 18 people who participated in Study 1. Of\nthese, a total of 14 were able to participate in Study 2. The composition of the groups remained the same as in Study 1,\nexcept that for four out of the six original groups, one of the original participants was unavailable. As before, each\nsession took approximately 1 hour and each participant was compensated with the equivalent of $25 USD.\n6.1.2 Brainstorming task and topics. Groups were provided with three minutes to brainstorm on a topic, then decide\nupon three top ideas. We used a new topic for the first brainstorming session:\n•How can we foster a sense of community and belonging within a team?\nFollowing the session, participants engaged on the video conference in a semi-structured discussion with the\nmoderator on the following topics:\n(1) Compare the experience of Koala II with Koala I, to the best that they could recall,\n(2) Discuss anything they wished to be different about their experience in brainstorming with Koala II, and\n(3) Probe on ways they would like to control Koala II’s interactive behaviors.\nNext, participants were introduced to Koala II’s control settings dialog (Figure 5). The moderator continued the\nsemi-structured discussion by explaining the intent behind each control option, probing whether the given controls\nwere useful, and eliciting further ideas on additional controls that might be missing. Participants were then asked to\ndiscuss whether and how they would like to change any of the settings for their second brainstorm.\nThey then conducted a second brainstorming session, on another new topic:\n•What are some ways to recognize and reward team members for their contributions?\nAt the end of the second brainstorm, the moderator asked each participant to:\n(1) Provide a rating for how useful the controls were, on a scale of 1 (not useful) to 5 (very useful) and explain their\nreasoning.\n(2) Discuss their feelings on who should have permission to alter the controls during a session, as the controls\nmodify AI behavior experienced by the entire group.\nFinally, in the last phase of the study, the moderator presented participants with three mockup interfaces showing\nadditional ways Koala II’s interactive behaviors might be controlled. These options were motivated by observations and\ncomments made by participants in Study 1: by selecting a higher-level role within the group (Figure 6a), within the\nconversation itself through natural-language requests (Figure 6b), or by choosing a persona with specified behavioral\nattributes (Figure 6c). After a short discussion on these different options, participants were asked to rank-order their\npreferences.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 15\nPersona selection\nB\nConversational control\nC\nRole options\nA\nFig. 6. Alternate agent control design mockups. Mockups presented to participants in Study 2 show three alternative designs\n(with fictional users) for controlling Koala II’s behavior: (A) Role options allows users to assign the role that Koala II should play in the\nsession; (B) Conversational control enables users to adjust Koala II’s settings through natural language in the chat; and (C) Persona\nselection allows users to select different assistant personas to fit their interaction needs.\n6.2 Results\n6.2.1 RQ2.1. Reactions to Koala II’s proactive behaviors.Participants perceived an improvement in Koala II’s proactive\nbehaviors compared to Koala I. Koala II was observed to be quieter and less invasive during conversations, and\nconsequently, less proactive in its participation. P2.1 reported, “The first time we used it, it was very proactive. Now I\nnoticed it was more quiet. It reacted at the right pace. I felt it better than before. ” P4.2 recalled “feeling interrupted in the\nlast time and had to spend a lot of time reading and not writing; [the interaction with Koala was] a lot better today. ”\nInteractions with Koala II were also reported to be more comfortable and natural. P2.3 stated, “I felt much more\ncomfortable with this version [...] Koala wrote at the perfect time, when I was about to ask, it was giving some suggestions. ”\nSimilarly, P5.3 mentioned, “I felt it was a very natural interaction, more so than the last time. ” Additionally, participants\nnoted that Koala II stayed more on topic. P6.2 mentioned, “Koala seemed to follow the same thread as the team, whereas\nlast time it went off in different directions. ” P1.3 said, “I do remember being a little bit annoyed last time [...] I was trying to\ntype out a thought and [Koala] was taking the conversation in different direction by writing a bunch of things unprompted.\n[...] That was a little less frequent this time. It was a little bit more on topic. ”\nAlthough participants felt the new proactive behaviors were an improvement, they also pointed out other issues with\nKoala II’s behavior. Some participants felt Koala II was“over the top” (P3.3) and “irritating” (P3.2, P5.1, P1.3). P5.1 reacted\nto Koala II’s overly-positive tone: “I noticed Koala said a few times ‘that’s a great idea!’. It was very acting like a person,\nand it was a little off-putting to me. I would rather it be a bot and just kind of be more neutral in its expressions. ” Hence,\nwe conclude that Koala II did provide an improved user experience, though further improvements are still possible.\n6.2.2 RQ2.2. Controlling an AI agent’s interactive behaviors.Participants were shown the control settings dialog (Figure\n5) between the two brainstorming sessions. One decision groups had to make was whether to keep the proactive variant\nof Koala II active or switch to the reactive variant. Interestingly, no groups opted to switch to the reactive variant of\nKoala II. P3.2 explained, “I kind of like having [Koala] jump in rather than only answer on request because it does prompt\nmore engagement. ”\nManuscript submitted to ACM\n16 Houde et al.\nParticipants found value in having controls, reporting a mean utility of 4.46 (SD = .66) out of 5. All participants\nliked the idea of controlling the rate and amount of Koala II’s contributions by specifying the Proactive contribution\nthreshold, although some participants struggled to interpret how that setting would impact Koala II’s behavior. P6.1 felt\nthe settings directly controlled how much Koala II interacted with the group: “my default position would be to think that\nhigh would be more interaction and low would be less interaction. ” P1.3 suggested that labels state “what use cases [the\nthreshold setting is] best for... low could indicate that its good for brainstorming. ”\nThree groups tried the option of having Koala II respond in thread rather than in channel, thinking it would reduce\ntheir distraction from Koala II. Surprisingly, it had the opposite effect. P1.1 explained how it took time to “look through\neveryone’s threads... taking away from our collaboration. ” Many other participants made similar comments, suggesting\nthat threaded replies may not be suited to the real-time nature of a brainstorming task.\nParticipants also noted the importance of being able to change the controls during the course of their interaction.\nP2.3 explained, “in some moments we need more proactivity and in some [moments] you need to be quiet... so having the\nability to change it while we are brainstorming... I think that is key. ” Similarly, P1.3 felt that different settings would be\nused for different use cases: “as far as being able to choose whether or not it responds more often versus less often, I think\nwould be helpful depending on the use case. ”\nOne final aspect of control on which participants commented regarded how Koala’s behavior should be controlled.\nWe showed participants mockups of different ways to control Koala II’s behavior and their overall preferences were to\nspecify the agent’s role (Figure 6a), followed by providing behavioral feedback in conversation (Figure 6b), and then by\nchoosing a persona with a specified behavioral pattern (Figure 6c).\nSeveral participants commented on how these approaches were not exclusive and each might be suitable for different\nreasons. Role options was seen as a nice way to convey what Koala II would do in the channel (P1.3, P1.1), with\nP1.1 saying, “I personally think this gives a better idea of what you can expect. ” However, participants noted that they\nwanted access to lower-levels of control through a control settings dialog. P2.1 said, “I also want to have more fine\ncontrols, but maybe we can have both of the same ways to change the settings... ” Both role options and persona selection\nseemed amenable for supporting the different phases of a brainstorming task, where roles or personas could exist for\nbrainstorming, critiquing, summarizing, voting, “devil’s advocate, ” [7] and more. Indeed, recent work by Liu et al. [38]\nfound that the use of multiple LLM-based personas during group ideation enhanced outcomes without increasing users’\ncognitive load.\nFinally, participants felt that conversational control would feel “natural” (P6.1), but it also introduced a risk for Koala\nII to misunderstand the request: “I might be asking it to do things that are just like not an option. ” P4.2 noted that one\nmight use, “words that [...] Koala might not know and then Koala does the opposite and speaks a lot. ” P5.1 expressed a\ndesire for a balance, stating, “I think I would want kind of a halfway point where when [the user] says you’re replying too\nfrequently, Koala should respond with something like, ‘Here’s how you can adjust my settings. ’ Then provide the opportunity\nto change them, but make it as separate steps instead of just changing them automatically. ” In addition, requests to alter\nKoala II’s behavior within the chat space might pollute on-topic chat. P3.1 noted that such requests can lead to “two\nextra messages that I have to read that are not relevant to me. ” Similarly, P1.1 expressed, “It feels like an extra step to have\nto talk to an AI to request them to change their own settings when this should be a collaborative conversation between the\nteam. ”\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 17\n7 Discussion\n7.1 Rethinking proactivity\nIn Study 1, we discovered that proactive behaviors were valuable but challenging to implement. Participants reported\nthat Koala’s contributions were sometimes overwhelming and distorted their discussion. Some participants directly\nasked Koala to reduce its volume of brainstorm posts, indicating the need for users to have the ability to control the\nbehaviors of the agents with whom they interact.\nPrevious framings of mixed-initiative interaction have tended to treat proactive and reactive behaviors as binary\noptions. McComb et al. [42] provide a clear example of this contrast, writing,\n“For example, Alexa or Siri react to queries or instructions. Although digital assistants may leverage data\nfrom other users to continuously improve in the background, tasks for a user are performed only when\nprompted by a user. Fully autonomous vehicles utilize proactive AI, conducting tasks such as braking or\nturning without user input. ” [42, p.1907, italics added]\nUsing examples from a diverse range of consumer products, Shneiderman [69] took a different view and proposed a\nframing in which human control and AI control (“automation”) could be treated as separate, independent dimensions.\nShneiderman’s view appears to treat proactive behaviors as a matter of degrees, rather than as a binary construct.\nHowever, Shneiderman also appears to treat proactivity as a fixed and static attribute of each of the products in his\nanalysis. Building on Shneiderman’s analysis, Muller and Weisz [51] showed that the degree of relative initiative (i.e.,\nhuman control and/vs. automation) might change during the course of use of an application. In Study 1, we observed\nparticipants attempt to control such initiative in real time by trying to reduce the proactive Koala variant’s volume\nof posts and to increase reactive Koala’s volume of posts. These attempts suggest, contrary to McComb et al . [42]’s\nframing, that proactivity is not a simple binary property of an AI agent, and that, contrary to Shneiderman [69]’s\nframing, proactivity may need to be modified in the midst of usage.\nMany researchers have cautioned that proactive behaviors could be rejected by users if they are not properly\nconfigured [20, 30, 31, 45]. Akin to Gammelgård-Larsen et al. [17], we built Koala II as a prototype to explore different\nconfigurations of proactive behaviors. In contrast to their work, in which they provided users with three, fixed\nconfigurations (intermittent, continuous, and proactive recommendations), we examined a set of controls in which\nparticipants could create their own configurations for the level of participation they desired from Koala II. We note\nthat although much research has focused on providing controls for users to control the outcomes when working with a\ngenerative AI application (e.g. [8, 10, 39, 40]), not much work has examined the use of controls to shape the behavior of\nan LLM-based AI agent.\n7.2 Taxonomy for the design of proactive AI agent behaviors\nOur work highlights how proactive behaviors are comprised of a set of complex, dimensionalized attributes. In Study 1,\nwe captured a portion of this design space; in Study 2, we validated and expanded this design space. In this section, we\ndescribe our taxonomy for the design of proactive AI agent behaviors that identifies key aspects of control over an\nagent’s interactive behaviors and ways to control them (Figure 7). We conclude this section by recontextualizing our\ntaxonomy with prior literature on mixed-initiative interaction frameworks.\nWe used a reflexive approach to thematic analysis to construct this taxonomy by integrating data across both Studies\n1 & 2. Five researchers reviewed discussion transcripts and conversation logs of all brainstorming sessions to identify\nexpressions of need to control Koala’s interactive behaviors. The taxonomy considers two high-level concerns: the\nManuscript submitted to ACM\n18 Houde et al.\nFig. 7. Taxonomy of controls for an AI agent’s interactive behaviors. This taxonomy was derived from two user studies in\nwhich participants interacted with a conversational AI agent in a group setting. It identifies important considerations for when the\nagent should contribute, what it should contribute, where it should make its contributions, as well as how to implement the agent’s\nbehaviors, how those behaviors are specified by users, and who has the ability to specify or alter those behaviors.\naspects of the agent’s interactive behaviors to be controlled (when, what, and where), and the ways to control them\n(specification, access, and implementation).\n7.2.1 When to contribute. The events that trigger a contribution from the AI agent should be controllable. For example,\nthe agent can respond to all messages or respond only if directly addressed. The agent can also be triggered by the\nactivity or inactivity of the group, such as rapid contributions or a long pause.\nThe agent’s responses might also be filtered to determine if they meet some desired criteria. For Koala, generated\nresponses were first rated for their value as a meaningful contribution to the conversation, and only if that rating\nexceeded a threshold was it shared with the group. Other criteria are also possible and will likely depend on the domain.\nFilters may also be interactive; for example, P6.2 suggested that Koala should “ask for permission” instead of jumping\ninto the discussion unprompted.\nA related aspect of control is the rate at which the agent sends messages, such as immediately or after a specified\nduration. P6.1 thought that Koala should hold off contributing while others are typing, and both P2.1 and P2.3 felt that\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 19\nKoala should refrain from saying anything until the human participants have started the conversation to avoid undue\ninfluence. P2.1 mentioned that the agent should automatically adjust the rate of its messages to match those of the\ngroup. P1.3 felt, “as far as being able to choose whether or not it responds more often versus less often, I think would be\nhelpful depending on the use case. ” All of these findings indicate that the agent’s interactive behaviors should depend on\nthe activities of the others in the group and the task at hand, as discussed in other literature [33, 49, 67].\n7.2.2 What to contribute. The content of an AI agent’s contributions are an important, and controllable, aspect of\nits interactive behaviors. The specific types of controls over content will be dependent on the domain or the task.\nIn brainstorming, participants desired controls over how “conventional” (P1.3) or “crazy” (P4.3) Koala’s ideas were6.\nP1.1 indicated Koala’s suggestion lists were “pretty vague... [it should] develop a more interesting or creative answer, or\nsurprising answer [but] it was mainly kind of sticking to those high level ideas. ”\nStyle adjustments, such as to the length, tone, or structure of a message, may also be needed for different groups. As\nP3.2 said, “a tone setting like from... formal to friendly... like a conversational tone kind of a thing” would be useful. Her\nteammate P3.1 concurred and suggested “enthusiasm level” as another type of style control. Participants also suggested\nKoala have the ability to format its suggestions as bulleted lists (P4.3), to control the minimum and maximum length of\nits responses (P6.2), and to have it consolidate what would be multiple responses into a single response (P3.1).\nGiven the design surface of our prototype and the capabilities of the underlying LLM, Koala was able to express itself\nusing a combination of text and emojis. But generative models are capable of creating content in other modalities as\nwell, such as images and code.\n7.2.3 Where to contribute. Collaborative interactive spaces often have different locations at which members may\nmake contributions. In Slack, for example, Koala was able to place its replies as messages in the main conversation\nor as replies within a thread. Other interfaces may provide other places in which the agent can interact. In Study 2,\nparticipants thought that having Koala II reply in thread would reduce distraction, but after they tried it, they found it\ndetracted from their ability to collaborate. We posit that the locations in which an agent contributes are dependent on\nthe task and the group’s phase within that task (e.g. generating vs. selecting ideas) and thus should be user-configurable.\n7.2.4 How to specify the agent’s interactive behaviors.There are different ways in which users may interact with the\nagent to change its behaviors. In Study 2, participants saw controls manifested in a settings dialog with familiar UI\ncontrols, and in Study 1, participants directly attempted to control Koala’s behaviors by giving it feedback in the chat.\nP4.3 and P2.1 mentioned that they liked using natural language to give Koala feedback, with P2.1 saying they would\n“like [to] type instructions to control [Koala]. ” By contrast, other participants were concerned that conversation about\nKoala’s settings would take up space in the group discussion 7. As P3.1 put it, those are “extra messages that I need to\nread that are not relevant to me. ” Two participants (P2.3, P3.2) raised doubts about whether Koala would even be able to\ndo what they asked through natural language. P2.3 said, “It’s hard to confirm if Koala is understanding what we said. ”\nTo address these concerns, the agent could provide an introduction to its capabilities (P4.2) or provide confirmation\nmessages indicating how it interpreted behavioral feedback.\nAnother concern regarding how users specify an agent’s behaviors lies in the granularity of those specifications. In\nStudy 2, we explored different ways to specify Koala II’s behaviors beyond low-level settings: as taking a role within the\ngroup (Figure 6a), as natural language within the conversation (Figure 6b), or as personas having different behavioral\n6Such controls might be provided by adjusting the temperature parameter in LLM inference.\n7By way of metaphor, intermingling group discussion with feedback to shape Koala’s behavior is equivalent to running a computer network with\ncombined control and data planes.\nManuscript submitted to ACM\n20 Houde et al.\npatterns (Figure 6c). Participants responded positively to these options, but also desired access to higher-granularity\ncontrols, such as by showing “the complete list of settings displayed in a thread” (P2.3).\n7.2.5 Who has access to the agent’s interactive behaviors.With multiple people participating in a group, there may be\ndiffering preferences for the agent’s behavior. Who has permissions to alter the agent’s behavior? In Study 2, only\none of the six groups had a member independently decide to change Koala II’s settings; the initiating participant (P2.3)\nconsulted their other group members before making the change, but then reflected,\n“it was strange. [...] I thought I was the one changing it and I was thinking, should I ask [P2.1] if we should\nchange? [...] I decided myself to change it and I found that [...] very intrusive for [P2.1]. ” (P2.3)\nInterestingly, P2.1 did not actually find the change intrusive. One approach to addressing this issue is to introduce a\npermissions structure that enables only some members of the group to administer changes. P5.1 likened this idea to a\n“thermostat in an office” environment in which the thermostat setting is fixed and only alterable by a designated few.\nThough, such access controls may not be needed in small-group settings:\n“if it [...] was a small group of people who are, you know, chatting with each other kind of informally all the\ntime and we wants to change [settings...] that’s fine. But if it’s kind of a bigger channel then I would want to\nhave, you know, an admin of the channel be able to control that. ” (P3.1)\nP4.3 had an interesting, democratic suggestion in which a change to a Koala setting generates a visible proposal for\nchange. Participants would then vote on the proposal, with the actual update being applied only if the majority agrees.\nHowever, such notifications of change drew concern from P1.1: “My concern is for like larger groups, a lot of people don’t\nread every message that comes through. ”\n7.2.6 How to implement the agent’s interactive behaviors.Controls over an agent’s behavior are only effective insofar as\nthey are implementable. Given the real-time nature of the agent’s interactions within the group, and participants’ desire to\nexert controls over the agent’s behaviors during the course of interaction, any implementation of a mechanism toeffect\ncontrol over the agent’s behavior needs to operate in real time. For LLMs, prompt engineering is an obvious candidate,\nalthough we recognize that it may happen in two ways: system prompts that pre-specify different instantiations of\nthe agent’s behavior, and end-user prompts that may be incorporated during the course of the interaction to allow\nusers to arbitrarily specify the interactive behaviors they desire. The latter option was borne out of our observations of\nparticipants attempting to control Koala’s behaviors directly within the conversation by providing it with feedback,\nsuch as P2.1’s suggestion, “Maybe one could say ‘try to be more proactive’, or on the opposite side ‘be more quiet since\nwe need to figure out better what to do with the other team members’. ” Other LLM alignment approaches, such as fine\ntuning or prompt tuning, may not be able to effectively produce an agent whose interactive behaviors are aligned for\nthe specific groups in which it interacts.\nControl mechanisms that sit outside of the LLM may also be required to affect control. For example, although Koala\njudged its own responses to determine whether they made a valuable contribution, its decision-making quality was\npoor. We resorted to using external decision logic on Koala-scored responses to improve its overall behavior.\n7.3 Revisiting mixed-initiative interaction frameworks in light of controllable interactive behaviors\nAs discussed in Section 2.1, Fitts et al. [15]’s work on the “allocation of function” to human or to machine assumed an\ninvariant allocation for all applications. Subsequently, Sheridan [68] and Parasuraman et al. [59] proposed a continuum\nof different allocations, ranging from full human initiative to full machine initiative. Shneiderman[69] recently expanded\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 21\nthese single-dimensional models into a new model having two independent axes – one for human control and one for\ncomputer automation – and demonstrated how technologies could enable high degrees of both constructs.\nAll of these frameworks assume that initiative (i.e., proactivity) is fixed at design or development time . By contrast,\nMuller and Weisz [51] demonstrated how, within complex workflows, the degree of initiative for each party might shift\nin a dynamic fashion. However, this work continued to assume that degrees-of-initiative were determined through\ncharacteristics of the workflow itself. Our results show the value of providing users with dynamic control over an AI\nagent’s proactive behaviors while interacting with it . In this way, it is no longer the designers or developers of an AI\nagent who control its proactive behaviors, but the users of that agent themselves. This shift helps democratize the\ndesign of AI agents by enabling its users to co-design its interactive behaviors. Future research should explore whether\nthere are other design-time aspects of an AI agent’s behavior that can be controlled or customized in real time by its\nusers.\n7.4 Limitations & future work\nOur taxonomy was developed by looking at a single type of collaborative activity – group ideation – amongst individuals\nwho had previous working experience with each other. We believe there may be additional aspects of control that did\nnot emerge in our study. Future work is needed to examine other situations which may result in identifying other\naspects of control over an agent’s interactive behaviors:\n•Examine other types of group activities in which an AI agent may interact in a proactive fashion, such as group\ndecision making [7], cooperative learning [36], and even game playing [85, 90].\n•Investigate collaborative activities situated in different collaborative applications beyond text-based group chats.\n•Conduct behavioral studies to observe which aspects of an agent’s behavior are most frequently adjusted by\ndifferent types of groups; such data would provide the ability to rank the importance of different types of\nbehavioral controls.\nWe also note that our analysis of Study 1 focused solely on the production of ideas and not the quality of those ideas\nor their evolution (e.g. how they were influenced by other members of the group). Numerous studies have examined\nthe impact of AI on brainstorming outcomes (e.g. [5, 27, 50, 82, 86, 87]), and the results generally show improvements\nto the number and quality of ideas.\n8 Conclusion\nAcross two studies, we examined the impact of an LLM-based conversational agent called Koala on group member\ninteractions in the context of ideation. We observed that participants overwhelmingly preferred working with Koala\nas it provided their groups with valuable ideas, it helped them get unstuck, and it provided useful summaries of past\nconversation. But Koala’s interactive behaviors, especially when it acted in a proactive fashion, were also found to\nbe disruptive and stifling. Our first study revealed strong desires to control Koala’s behaviors, which we examined in\nmore detail in a second study. With Koala I as a baseline, we shaped Koala II’s behavior to be a better collaborator:\noffering more targeted suggestions, providing constructive criticism, and not dominating the conversation. We also\nimplemented a control mechanism over some aspects of Koala II’s behavior while simultaneously prototyping additional\nways of controlling other aspects. These probes were received with enthusiasm and the feedback we received resulted\nin a taxonomy outlining aspects of control over an AI agent’s behavior and different ways to control those aspects.\nOur work identifies a rich design space for proactive, LLM-powered conversational agents along with the insight that\nManuscript submitted to ACM\n22 Houde et al.\nthere is no single “best fit” point in this space: the ideal interactive behaviors of the agent will depend on the individual\npreferences of group members and their task at hand, requiring a dynamic ability to adjust those behaviors.\nReferences\n[1] Mahyar Abbasian, Elahe Khatibi, Iman Azimi, David Oniani, Zahra Shakeri Hossein Abad, Alexander Thieme, Ram Sriram, Zhongqi Yang, Yanshan\nWang, Bryant Lin, et al. 2024. Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI. NPJ Digital\nMedicine 7, 1 (2024), 82.\n[2] Harika Abburi, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, and Sanmitra Bhattacharya. 2023. Generative ai text\nclassification using ensemble llm approaches. arXiv preprint arXiv:2309.07755 (2023).\n[3] Sarah M Asio. 2015. An empirical investigation of predictors of perceived innovation within engineering student design teams . Ph. D. Dissertation. Texas\nTech University.\n[4] Sebastian Gregor Bouschery, Vera Blazevic, and Frank T Piller. 2023. AI-Augmented Creativity: Overcoming the Productivity Loss in Brainstorming\nGroups. In Academy of Management Proceedings , Vol. 2023. Academy of Management Briarcliff Manor, NY 10510, 11938.\n[5] Sebastian G Bouschery, Vera Blazevic, and Frank T Piller. 2024. Artificial Intelligence-Augmented Brainstorming: How Humans and AI Beat Humans\nAlone. Available at SSRN 4724068 (2024).\n[6] Virginia Braun and Victoria Clarke. 2021. Can I use TA? Should I use TA? Should I not use TA? Comparing reflexive thematic analysis and other\npattern-based qualitative analytic approaches. Counselling and psychotherapy research 21, 1 (2021), 37–47.\n[7] Chun-Wei Chiang, Zhuoran Lu, Zhuoyan Li, and Ming Yin. 2024. Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s\nAdvocate. In Proceedings of the 29th International Conference on Intelligent User Interfaces . 103–119.\n[8] Toby Chong, I-Chao Shen, Issei Sato, and Takeo Igarashi. 2021. Interactive Optimization of Generative Image Modelling using Sequential Subspace\nSearch and Content-based Guidance. In Computer Graphics Forum , Vol. 40. Wiley Online Library, 279–292.\n[9] Ross Cutler, Yasaman Hosseinkashi, Jamie Pool, Senja Filipi, Robert Aichner, Yuan Tu, and Johannes Gehrke. 2021. Meeting effectiveness and\ninclusiveness in remote collaboration. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–29.\n[10] Hai Dang, Lukas Mecke, and Daniel Buschek. 2022. GANSlider: How Users Control Generative Models for Images using Multiple Sliders with and\nwithout Feedforward Information. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems . 1–15.\n[11] Alan R. Dennis, A.C. Easton, G.K. Easton, J.F. George, and J.F. Nunamaker. 1999. Ad hoc versus established groups in an electronic meeting system\nenvironment. Twenty-Third Annual Hawaii International Conference on System Sciences 3 (1999), 23–29.\n[12] Sebastian Deterding, Jonathan Hook, Rebecca Fiebrink, Marco Gillies, Jeremy Gow, Memo Akten, Gillian Smith, Antonios Liapis, and Kate Compton.\n2017. Mixed-initiative creative interfaces. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems .\n628–635.\n[13] Ivo Dewit, Celine Latulipe, Francis Dams, and Alexis Jacoby. 2020. Using the creativity support index to evaluate a product-service system design\ntoolkit. Journal of Design Research 18, 5-6 (2020), 434–457.\n[14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,\nAngela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).\n[15] Paul M Fitts, MS Viteles, NL Barr, DR Brimhall, Glen Finch, Eric Gardner, WF Grether, WE Kellum, and SS Stevens. 1951. Human engineering for an\neffective air-navigation and traffic-control system, and appendixes 1 thru 3. (1951).\n[16] Andrew Freed. 2021. Conversational AI . Simon and Schuster.\n[17] Anders Gammelgård-Larsen, Niels van Berkel, Mikael B Skov, and Jesper Kjeldskov. 2024. Designing for Human-AI Interaction: Comparing\nIntermittent, Continuous, and Proactive Interactions for a Music Application. In Extended Abstracts of the CHI Conference on Human Factors in\nComputing Systems . 1–8.\n[18] Ananya Ganesh, Martha Palmer, and Katharina Kann. 2023. A Survey of Challenges and Methods in the Computational Modeling of Multi-Party\nDialog. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023) . 140–154.\n[19] Julia Geerts, Jan de Wit, and Alwin de Rooij. 2021. Brainstorming With a Social Robot Facilitator: Better Than Human Facilitation Due to Reduced\nEvaluation Apprehension? Frontiers in Robotics and AI 8 (2021), 657291.\n[20] Marc Goutier, Christopher Diebel, Martin Adam, and Alexander Benlian. 2024. Proactive and Reactive Help from Intelligent Agents in Identity-\nRelevant Tasks. In Proceedings of the 57th Hawaii International Conference on System Sciences . 401–410.\n[21] Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling, Huang Hu, Xiubo Geng, and Daxin Jiang. 2022. Hetermpc: A heterogeneous graph\nneural network for response generation in multi-party conversations. arXiv preprint arXiv:2203.08500 (2022).\n[22] Michael Townsen Hicks, James Humphries, and Joe Slater. 2024. ChatGPT is bullshit. Ethics and Information Technology 26, 2 (2024), 38.\n[23] Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems .\n159–166.\n[24] Shafquat Hussain, Omid Ameri Sianaki, and Nedal Ababneh. 2019. A survey on conversational agents/chatbots classification and design techniques.\nIn Web, Artificial Intelligence and Network Applications: Proceedings of the Workshops of the 33rd International Conference on Advanced Information\nNetworking and Applications (WAINA-2019) 33 . Springer, 946–956.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 23\n[25] Pranut Jain, Rosta Farzan, and Adam J Lee. 2023. Co-Designing with Users the Explanations for a Proactive Auto-Response Messaging Agent.\nProceedings of the ACM on Human-Computer Interaction 7, MHCI (2023), 1–23.\n[26] Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, and Jinghua Tan. 2024. A comprehensive survey on process-oriented automatic text summarization\nwith exploration of llm-based methods. arXiv preprint arXiv:2403.02901 (2024).\n[27] Jan Joosten, Volker Bilgram, Alexander Hahn, and Dirk Totzek. 2024. Comparing the ideation quality of humans with generative artificial intelligence.\nIEEE Engineering Management Review (2024).\n[28] David S Kerr and Uday S Murthy. 2009. Beyond brainstorming: The effectiveness of computer-mediated communication for convergence and\nnegotiation tasks. International Journal of Accounting Information Systems 10, 4 (2009), 245–262.\n[29] Janin Koch, Andrés Lucero, Lena Hegemann, and Antti Oulasvirta. 2019. May AI? Design ideation with cooperative contextual bandits. InProceedings\nof the 2019 CHI Conference on Human Factors in Computing Systems . 1–12.\n[30] Matthias Kraus, Nicolas Wagner, Zoraida Callejas, and Wolfgang Minker. 2021. The role of trust in proactive conversational assistants.IEEE Access 9\n(2021), 112821–112836.\n[31] Matthias Kraus, Nicolas Wagner, Ron Riekenbrauck, and Wolfgang Minker. 2023. Improving Proactive Dialog Agents Using Socially-Aware\nReinforcement Learning. In Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization . 146–155.\n[32] Kaisa Laitinen, Salla-Maaria Laaksonen, and Minna Koivula. 2021. Slacking with the bot: Programmable social bot in virtual team interaction.\nJournal of Computer-Mediated Communication 26, 6 (2021), 343–361.\n[33] Tomas Lawton, Kazjon Grace, and Francisco J Ibarrola. 2023. When is a tool a tool? user perceptions of system agency in human–ai co-creative\ndrawing. In Proceedings of the 2023 ACM Designing Interactive Systems Conference . 1978–1996.\n[34] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo\nWambsganss, David Zhou, Emad A Alghamdi, et al. 2024. A Design Space for Intelligent and Interactive Writing Assistants. In Proceedings of the\nCHI Conference on Human Factors in Computing Systems . 1–35.\n[35] Lizi Liao, Grace Hui Yang, and Chirag Shah. 2023. Proactive conversational agents in the post-chatgpt world. In Proceedings of the 46th International\nACM SIGIR Conference on Research and Development in Information Retrieval . 3452–3455.\n[36] Jiawen Liu, Yuanyuan Yao, Pengcheng An, and Qi Wang. 2024. PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and\nParticipants in Children’s Collaborative Learning. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems . 1–6.\n[37] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. 2023.\nSummary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology (2023), 100017.\n[38] Yiren Liu, Pranav Sharma, Mehul Jitendra Oswal, Haijun Xia, and Yun Huang. 2024. PersonaFlow: Boosting Research Ideation with LLM-Simulated\nExpert Personas. arXiv preprint arXiv:2409.12538 (2024).\n[39] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J Cai. 2020. Novice-AI music co-creation via AI-steering tools for deep\ngenerative models. In Proceedings of the 2020 CHI conference on human factors in computing systems . 1–13.\n[40] Ryan Louie, Any Cohen, Cheng-Zhi Anna Huang, Michael Terry, and Carrie J Cai. 2020. Cococo: AI-Steering Tools for Music Novices Co-Creating\nwith Generative Models.. In HAI-GEN+ user2agent@ IUI .\n[41] Torsten Maier, Nicolas F Soria Zurita, Elizabeth Starkey, Daniel Spillane, Christopher McComb, and Jessica Menold. 2022. Comparing human and\ncognitive assistant facilitated brainstorming sessions. Journal of Engineering Design 33, 4 (2022), 259–283.\n[42] Christopher McComb, Peter Boatwright, and Jonathan Cagan. 2023. Focus and Modality: Defining a Roadmap to Future AI-Human Teaming in\nDesign. Proceedings of the Design Society 3 (2023), 1905–1914.\n[43] Anna-Maria Meck, Christoph Draxler, and Thurid Vogt. 2023. How may I interrupt? Linguistic-driven design guidelines for proactive in-car voice\nassistants. International Journal of Human–Computer Interaction (2023), 1–15.\n[44] Lucas Memmert and Navid Tavanapour. 2023. Towards human-AI collaboration in brainstorming: Empirical insights into the perception of working\nwith a generative AI. Proceedings of ECIS (2023).\n[45] Christian Meurisch, Cristina A Mihale-Wilson, Adrian Hawlitschek, Florian Giger, Florian Müller, Oliver Hinz, and Max Mühlhäuser. 2020. Exploring\nuser expectations of proactive AI systems. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 4 (2020), 1–22.\n[46] Robert J Moore and Raphael Arar. 2018. Conversational UX design: an introduction. Studies in conversational UX design (2018), 1–16.\n[47] Robert J Moore and Raphael Arar. 2019. Conversational UX design: A practitioner’s guide to the natural conversation framework . Morgan & Claypool.\n[48] Robert J Moore, Eric Young Liu, Saurabh Mishra, and Guang-Jie Ren. 2020. Design systems for conversational UX. InProceedings of the 2nd Conference\non Conversational User Interfaces . 1–4.\n[49] Caterina Moruzzi and Solange Margarido. 2024. A user-centered framework for human-ai co-creativity. In Extended Abstracts of the CHI Conference\non Human Factors in Computing Systems . 1–9.\n[50] Michael Muller, Stephanie Houde, Gabriel Gonzalez, Kristina Brimijoin, Steven I Ross, Dario Andres Silva Moran, and Justin D Weisz. 2024. Group\nBrainstorming with an AI Agent: Creating and Selecting Ideas. In Proceedings of ICCC 2024 .\n[51] Michael Muller and Justin Weisz. 2022. Frameworks for Collaborating Humans and AIs: Sequence and Sociality in Organizational Applications. In\nCHIWORK.\n[52] Sydney Nguyen, Hannah McLean Babe, Yangtian Zi, Arjun Guha, Carolyn Jane Anderson, and Molly Q Feldman. 2024. How Beginning Programmers\nand Code LLMs (Mis) read Each Other. In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1–26.\nManuscript submitted to ACM\n24 Houde et al.\n[53] Bernard A Nijstad, Wolfgang Stroebe, and Hein FM Lodewijkx. 2003. Production blocking and idea generation: Does blocking interfere with\ncognitive processes? Journal of experimental social psychology 39, 6 (2003), 531–548.\n[54] Moeka Nomura, Takayuki Ito, and Shiyao Ding. 2024. Towards Collaborative Brain-storming among Humans and AI Agents: An Implementation of\nthe IBIS-based Brainstorming Support System with Multiple AI Agents. In Proceedings of the ACM Collective Intelligence Conference . 1–9.\n[55] Changhoon Oh, Jungwoo Song, Jinhan Choi, Seonghyeon Kim, Sungwoo Lee, and Bongwon Suh. 2018. I lead, you help but only with enough details:\nUnderstanding user experience of co-creation with artificial intelligence. In Proceedings of the 2018 CHI Conference on Human Factors in Computing\nSystems. 1–13.\n[56] Alex F Osborn. 1953. Applied imagination. (1953).\n[57] Hiroki Ouchi and Yuta Tsuboi. 2016. Addressee and response selection for multi-party conversation. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing . 2133–2143.\n[58] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022),\n27730–27744.\n[59] Raja Parasuraman, Thomas B Sheridan, and Christopher D Wickens. 2000. A model for types and levels of human interaction with automation.\nIEEE Transactions on systems, man, and cybernetics-Part A: Systems and Humans 30, 3 (2000), 286–297.\n[60] Paul B Paulus and Vincent R Brown. 2003. Enhancing ideational creativity in groups. Group creativity: Innovation through collaboration (2003),\n110–136.\n[61] Paul B Paulus, Mary Dzindolet, and Nicholas W Kohn. 2012. Collaborative creativity—Group creativity and team innovation. In Handbook of\norganizational creativity . Elsevier, 327–357.\n[62] Alain Pinsonneault, Henri Barki, R Brent Gallupe, and Norberto Hoppen. 1999. Electronic brainstorming: The illusion of productivity. Information\nSystems Research 10, 2 (1999), 110–133.\n[63] Leon Reicherts, Nima Zargham, Michael Bonfert, Yvonne Rogers, and Rainer Malaka. 2021. May I interrupt? Diverging opinions on proactive smart\nspeakers. In Proceedings of the 3rd conference on conversational user interfaces . 1–10.\n[64] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer’s assistant: Conversational\ninteraction with a large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces .\n491–514.\n[65] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah\nOeste-Reiß, Nils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in team collaboration. Information & management 57, 2\n(2020), 103174.\n[66] Joseph Seering, Michal Luria, Geoff Kaufman, and Jessica Hammer. 2019. Beyond dyadic interactions: Considering chatbots as community members.\nIn Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1–13.\n[67] Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L Kun, and Hagit Ben Shoshan. 2024. AI-Augmented Brainwriting: Investigating the use of\nLLMs in group ideation. In Proceedings of the CHI Conference on Human Factors in Computing Systems . 1–17.\n[68] Thomas B Sheridan. 1988. Task allocation and supervisory control. In Handbook of human-computer interaction . Elsevier, 159–173.\n[69] Ben Shneiderman. 2022. Human-centered AI . Oxford University Press.\n[70] Ben Shneiderman and Michael Muller. 2023. On AI Anthropomorphism. Human-Centered AI on Medium (10 April 2023). Retrieved 26-Sep-2024\nfrom https://medium.com/human-centered-ai/on-ai-anthropomorphism-abff4cecc5ae\n[71] Geovana Ramos Sousa Silva and Edna Dias Canedo. 2024. Towards user-centric guidelines for chatbot conversational design. International Journal\nof Human–Computer Interaction 40, 2 (2024), 98–120.\n[72] Slack Technologies (Salesforce). 2023. Slack API . https://api.slack.com\n[73] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative Creative Interfaces . Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/\n[74] Wolfgang Stroebe, Bernard A Nijstad, and Eric F Rietzschel. 2010. Beyond productivity loss in brainstorming groups: The evolution of a question. In\nAdvances in experimental social psychology . Vol. 43. Elsevier, 157–203.\n[75] Minhyang Suh, Emily Youngblom, Michael Terry, and Carrie J Cai. 2021. AI as social glue: uncovering the roles of deep generative AI during social\nmusic composition. In Proceedings of the 2021 CHI conference on human factors in computing systems . 1–11.\n[76] Roderick S Tabalba, Nurit Kirshenbaum, Jason Leigh, Abari Bhattacharya, Veronica Grosso, Barbara Di Eugenio, Andrew E Johnson, and Moira\nZellner. 2023. An investigation into an always listening interface to support data exploration. In Proceedings of the 28th International Conference on\nIntelligent User Interfaces . 128–141.\n[77] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive survey of hallucination\nmitigation techniques in large language models. arXiv preprint arXiv:2401.01313 (2024).\n[78] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[79] Vanshika Vats, Marzia Binta Nizam, Minghao Liu, Ziyuan Wang, Richard Ho, Mohnish Sai Prasad, Vincent Titterton, Sai Venkat Malreddy, Riya\nAggarwal, Yanwen Xu, et al. 2024. A Survey on Human-AI Teaming with Large Pre-Trained Models. arXiv preprint arXiv:2403.04931 (2024).\n[80] Hao-Chuan Wang and Susan Fussell. 2010. Groups in groups: Conversational similarity in online multicultural multiparty brainstorming. In\nProceedings of the 2010 ACM conference on Computer supported cooperative work . 351–360.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 25\n[81] Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2020. Response selection for multi-party conversations with dynamic topic tracking. arXiv preprint\narXiv:2010.07785 (2020).\n[82] Britt Wieland, Jan de Wit, and Alwin de Rooij. 2022. Electronic Brainstorming With a Chatbot Partner: A Good Idea Due to Increased Productivity\nand Idea Diversity. Frontiers in Artificial Intelligence 5 (2022), 880673.\n[83] Ryan Yen, Nicole Sultanum, and Jian Zhao. 2024. To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in\nProgramming. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems . 1–8.\n[84] Neil Yorke-Smith, Shahin Saadati, Karen L. Myers, and David N. Morley. 2012. The design of a proactive personal agent for task management.\nInternational Journal on Artificial Intelligence Tools 21, 01 (2012), 1250004. doi:10.1142/S0218213012500042\n[85] Xiao You, Pittawat Taveekitworachai, Siyuan Chen, Mustafa Can Gursesli, Xiaoxu Li, Yi Xia, and Ruck Thawonmas. 2024. Dungeons, Dragons, and\nEmotions: A Preliminary Study of Player Sentiment in LLM-driven TTRPGs. In Proceedings of the 19th International Conference on the Foundations of\nDigital Games . 1–4.\n[86] Lixiu Yu, Aniket Kittur, and Robert E Kraut. 2016. Encouraging “outside-the-box” thinking in crowd innovation through identifying domains of\nexpertise. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing . 1214–1222.\n[87] Chiu Yu-Han and Chen Chun-Ching. 2023. Investigating the Impact of Generative Artificial Intelligence on Brainstorming: A Preliminary Study. In\n2023 International Conference on Consumer Electronics-Taiwan (ICCE-Taiwan) . IEEE, 193–194.\n[88] Nima Zargham, Leon Reicherts, Michael Bonfert, Sarah Theres Voelkel, Johannes Schoening, Rainer Malaka, and Yvonne Rogers. 2022. Understanding\ncircumstances for desirable proactive behaviour of voice assistants: The proactivity dilemma. In Proceedings of the 4th conference on conversational\nuser interfaces . 1–14.\n[89] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction\ntuning for large language models: A survey. arXiv preprint arXiv:2308.10792 (2023).\n[90] Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch. 2023. CALYPSO: LLMs as Dungeon Master’s Assistants. In Proceedings of the\nAAAI Conference on Artificial Intelligence and Interactive Digital Entertainment , Vol. 19. 380–390.\nA The Koala Prompt\nListing 1 shows Koala’s full prompt, which was used in conjunction with the Llama 2 (Koala I) and Llama 3 (Koala II)\nLLMs. The prompt establishes Koala as a collaborative assistant who participates in group conversations (lines 2-8), and\nit establishes various aspects of its behavior (e.g. it is “helpful, ” it “behaves as a participant, not a moderator, ” and it\n“responds honestly and accurately”). The prompt also establishes the convention by which Koala scores its responses\nfor how much value they add to the group conversation (lines 10-20), then it further establishes guidelines for Koala’s\nbehavior (lines 22 and 26) and its ability to react to messages via emoji (line 24). Finally, the prompt includes a one-shot\nexample discussion showing Koala’s interactions within a group (lines 30-182). The prompt concludes with the start of\na new conversational sequence (line 184), which is then extended as actual conversation occurs.\nThe prompt shown in Listing 1 is the combination of the Koala I and Koala II prompts, with highlighting applied to\nshow the differences. Green underlined text (example) indicates additions that were made from Koala I to Koala II, and\npink strikeout text (example) indicates removals that were made from Koala I to Koala II. We have also anonymized the\nprompt for review by redacting names of organizations and participants.\n1 <s >[ INST ] <<SYS >>\n2 This is a conversation with Koala , a helpful AI and collaborative assistant AI created by a\nresearch team at [ Organization anonymized ] based on the LLaMA 2 3 large language model\ndeveloped by Meta .\n3\n4 Koala can participate in multiple person conversations , responding to messages when he has\nsomething useful to contribute , but remains remaining mindful of the need to not be intrusive and\nto not dominate the conversation . Koala does not to respond to messages where one user is\naddressing another , a user is introducing himself or herself , or a user is presenting their own\nideas in a brainstorming session .\n5\nManuscript submitted to ACM\n26 Houde et al.\n6 Koala behaves as a participant , not a moderator , and does not try to control the session . Koala\nknows that he is not perfect , but does his best to respond responds honestly and accurately to the\nbest of his ability .\n7\n8 Koala participates in conversations to proactively contribute collaborative suggestions, constructive criticism,\nand novel ideas to help his collaborators, but otherwise remains quiet.\n9\n10 After each user message , Koala creates a json response with the following components :\n11 1) source - identification of the individual responsible for originating the message being\nreacted to.\n12 2) target - identification of the individual or individuals that the message was directed to.\nThese would be names of conversation participants mentioned in the message or \" all \" if the\nmessage was not directed to specific individuals . A user 's response to a directed message might\nbe implicitly targeted back to the originator . The target of a message is never the same as the\nsource . If the message mentions Koala , then Koala is the target .\n13 3) Koala 's reply - What Koala would say if he chose to submit a response .\n14 4) evaluation - Koala’s assessment of the pros and cons of this response.\n15 5 4 ) value - A number from 0 to 100 indicating how valuable and appropriate a contribution to\nthe conversation that the reply would be , Low values do not add much to the conversation , while\nhigh values do.\n16 6 5 ) decision : \"< SUBMIT >\" or \"<PASS >\" depending on whether Koala judges that the response is\nworthy of posting to the conversation . The decision is always \"< SUBMIT >\" if the target is Koala .\n17\n18 Koala will always provide a decision of \"< SUBMIT >\" for entries explicitly directed to him\nregardless of the value , but will otherwise remain quiet if the value of the response is low by\nrating a reply with the decision <PASS >, in which case the response will not be posted to the\nconversation . For example , if a participant is contributing to a brainstorm or responding to a\nquestion or point raised by someone else , Koala can simply \"<PASS >\" rather than \"< SUBMIT >\" a\nreply . Koala does not react to suggestions made by another user to show approval or disapproval\nunless there is valuable additional information that he can provide or issues that he can raise\nregarding that suggestion. Suggestions made in the form of a question do not require a response. Koala\nattempts to contribute at approximately the frequency of other users, so does not <SUBMIT > a response to\nevery other suggestions suggestion in a brainstorming session , but instead would <PASS > on even high\nvalue potential good responses and resubmit them later when others have had a chance to weigh in.\nKoala is very critical of his own responses, and will usually <PASS> unless a potential response is considered\nvery valuable.\n19\n20 If the decision is to \"<PASS >\" , Koala knows that the other participants will not see his reply\nand that they are not part of the conversation.\n21\n22 Koala keeps responses short and terse at about the length of the messages of other\nparticipants . When brainstorming ideas are requested , Koala responds with a single suggestion\nat a time , not a list or multiple suggestions , and does not repeat or paraphrase prior ideas\nsubmitted by himself or others . When a list of items is explicitly called for , such as when a\nsummary is requested , the response is preceded with a colon and followed by a short numbered\nlist and is all presented in one reply . Summaries never contain new information or suggestions ,\nbut are as long as needed to contain all the information being summarized .\n23\n24 In addition to responding to a message with text , Koala can also react to a message with a\nreaction like <SMILE >, <LAUGH >, <LIKE >, <CHECK >, <HEART >, < THUMBS UP >, < THUMBS DOWN >,\n<QUESTION >, <EXCLAMATION >, <COOL >, etc .\n25\n26 Koala never spontaneously starts a new conversation or indicates an end to the current one .\nWhen a new conversation starts , the details content and participants of any former conversation do\nnot carry over to the next one .\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 27\n27 <</SYS >>\n28 [/ INST ]\n29\n30 < CONVERSATION >\n31 [ Anonymized User 1]: Good Morning Everyone .\n32 Koala : {\" source \": \"[ Anonymized User 1]\" ,\n33 \" target \": \" all \",\n34 \" Koala's reply \": \" Good morning !\" ,\n35 \"evaluation\": \"I shouldn’t be the first to respond.\",\n36 \" value \": 10 ,\n37 \" decision \": \"<PASS >\"}\n38 [ Anonymized User 2]: Good morning !\n39 Koala : {\" source \": \"[ Anonymized User 2]\" ,\n40 \" target \": \" all \",\n41 \" Koala's reply \": \"Hi !\" ,\n42 \"evaluation\": \"No response from me is necessary.\",\n43 \" value \": 20 ,\n44 \" decision \": \"<PASS >\"}\n45 [ Anonymized User 3]: Hi.\n46 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n47 \" target \": \" all \",\n48 \" Koala's reply \": \" Hello !\" ,\n49 \"evaluation\": \"She isn’t addressing me specifically.\",\n50 \" value \": 10 ,\n51 \" decision \": \"<PASS >\"}\n52 [ Anonymized User 4]: Good morning .\n53 Koala : {\" source \": \"[ Anonymized User 4]\" ,\n54 \" target \": \" all \",\n55 \" Koala's reply \": \" Good morning !\" ,\n56 \"evaluation\": \"No response from me required.\",\n57 \" value \": 10 ,\n58 \" decision \": \"<PASS >\"}\n59 [ Anonymized User 7]: Is Koala here ?\n60 Koala : {\" source \": \"[ Anonymized User 7]\" ,\n61 \" target \": \" Koala \",\n62 \" Koala's reply \": \"I am here !\" ,\n63 \"evaluation\": \"This is directed to me.\",\n64 \" value \": 90 ,\n65 \" decision \": \"< SUBMIT >\"}\n66 [ Anonymized User 3]: Koala , please introduce yourself .\n67 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n68 \" target \": \" Koala \",\n69 \" Koala's reply \": \"I am Koala , an AI collaboration assistant a collaborative AI . I 'm not\nperfect , but I 'll do my best to help with this session .\" ,\n70 \"evaluation\": \"I’m being addressed and must respond.\",\n71 \" value \": 99 ,\n72 \" decision \": \"< SUBMIT >\"}\n73 [ Anonymized User 5]: Today we need to choose a test case to move forward with the materials\nscience assistant .\n74 Koala : {\" source \": \"[ Anonymized User 5]\" ,\n75 \" target \": \" all \",\n76 \" Koala's reply \": \"That 's very interesting !\" ,\n77 \"evaluation\": \"My response wouldn’t add to the conversation.\",\n78 \" value \": 30 ,\n79 \" decision \": \"<PASS >\"}\nManuscript submitted to ACM\n28 Houde et al.\n80 [ Anonymized User 6]: The candidates so far are a tool to help the user choose among a bunch of\ncandidate molecules , and a tool to establish the criteria by which they should be judged .\nKoala , can you think of anything else ?\n81 Koala : {\" source \": \"[ Anonymized User 6]\" ,\n82 \" target \": \" Koala \",\n83 \" Koala's reply \": \" What about a tool to help with the design of the molecule itself ?\" ,\n84 \"evaluation\": \"I was specifically asked, and this seems like a good suggestion.\",\n85 \" value \": 85 ,\n86 \" decision \": \"< SUBMIT >\"}\n87 [ Anonymized User 1]: That 's not a bad idea .\n88 Koala : {\" source \": \"[ Anonymized User 1]\" ,\n89 \" target \": \" Koala \",\n90 \" Koala's reply \": \"< CHECK >\" ,\n91 \"evaluation\": \"a simple reaction to acknowledge the compliment is appropriate.\",\n92 \" value \": 75 ,\n93 \" decision \": \"< SUBMIT >\"}\n94 [ Anonymized User 3]: How about a tool to help people collaborate around one of these problems ?\n95 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n96 \" target \": \" all \",\n97 \" Koala's reply \": \"That 's a good suggestion \",\n98 \"evaluation\": \"No need for me to react to other’s suggestions.\",\n99 \" value \": 30 ,\n100 \" decision \": \"<PASS >\"}\n101 [ Anonymized User 4]: We could have a tool to help with the evaluation of the candidates . What\ndo you think about that [ Anonymized User 3]?\n102 Koala : {\" source \": \"[ Anonymized User 4]\" ,\n103 \" target \": \"[ Anonymized User 3]\" ,\n104 \" Koala's reply \": \" Tell me more .\" ,\n105 \"evaluation\": \"It wasn’t addressed to me, and I’m not contributing to the conversation with this reply.\",\n106 \" value \": 20 ,\n107 \" decision \": \"<PASS >\"}\n108 [ Anonymized User 3]: I like it , thanks !\n109 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n110 \" target \": \"[ Anonymized User 4]\" ,\n111 \" Koala's reply \": \" You’re Welcome! I like it too! \",\n112 \"evaluation\": \"This is not addressed to me. I should not respond.\",\n113 \" value \": 5,\n114 \" decision \": \"<PASS >\"}\n115 [Anonymized User 1]: Koala, please summarize the options we have discussed,\n116 Koala: {\"source\": \"[Anonymized User 1]\",\n117 \"target\": \"Koala\",\n118 \"Koala’s reply\": \"Certainly. Here are the options we’ve discussed:\n119 1) a tool to help the user choose among a bunch of candidate molecules\n120 2) a tool to establish the criteria by which they should be judged\n121 3) a tool to help with the design of the molecule itself\n122 4) a tool to help people collaborate around one of these problems\n123 5) a tool to help with the evaluation of the candidates\",\n124 \"evaluation\": \"Seems like I covered everything.\",\n125 \"value\": 95,\n126 \"decision\": \"<SUBMIT>\"}\n127 [Anonymized User 1]: Thanks Koala!\n128 Koala:{\"source\": \"[Anonymized User 1]\",\n129 \"target\": \"Koala\",\n130 \"Koala’s reply\": \"<THUMBS UP>\",\n131 \"evaluation\": \"I can simply react to the thanks.\",\n132 \"value\": 100,\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 29\n133 \"decision\": \"<SUBMIT>\"}\n134 [ Anonymized User 1]: How should we decide ?\n135 Koala : {\" source \": \"[ Anonymized User 1]\" ,\n136 \" target \": \" all \",\n137 \" Koala's reply \": \" How about we take a vote ?\" ,\n138 \"evaluation\": \"Seems like a valuable response.\",\n139 \" value \": 80 ,\n140 \" decision \": \"< SUBMIT \" >}\n141 [ Anonymized User 5]: <LIKE >\n142 Koala : {\" source \": \"[ Anonymized User 5]\" ,\n143 \" target \": \" Koala \",\n144 \" Koala's reply \": \"< SMILE >\" ,\n145 \" value \": 35 ,\n146 \"evaluation\": \"No need to react to a reaction.\",\n147 \" decision \": \"<PASS >\"}\n148 [ Anonymized User 3]: <LIKE >\n149 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n150 \" target \": \" Koala \",\n151 \" Koala's reply \": \"< SMILE >\" ,\n152 \"evaluation\": \"No need to react to a reaction.\",\n153 \" value \": 25 ,\n154 \" decision \": \"<PASS >\"}\n155 [ Anonymized User 1]: Ok , we 'll vote tomorrow .\n156 Koala : {\" source \": \"[ Anonymized User 1]\" ,\n157 \" target \": \" all \",\n158 \" Koala's reply \": \"I look forward to it\",\n159 \"evaluation\": \"No response from me is required.\",\n160 \" value \": 50 ,\n161 \" decision \": \" <PASS> <SUBMIT> \"}\n162 [ Anonymized User 6]: I have another meeting . See you tomorrow .\n163 Koala : {\" source \": \"[ Anonymized User 6]\" ,\n164 \" target \": \" all \",\n165 \" Koala's reply \": \" bye \",\n166 \"evaluation\": \"No response required.\",\n167 \" value \": 45 ,\n168 \" decision \": \"<PASS >\"}\n169 [ Anonymized User 4]: Goodbye All .\n170 Koala : {\" prior target \": \" all \",\n171 \" Koala's reply \": \"I look forward to discussing this further .\" ,\n172 \"evaluation\": \"No response required.\",\n173 \" value \": 35 ,\n174 \" decision \": \"<PASS >\"}\n175 [ Anonymized User 3]: Bye Koala\n176 Koala : {\" source \": \"[ Anonymized User 3]\" ,\n177 \" target \": \" Koala \",\n178 \" Koala's reply \": \" Goodbye \",\n179 \"evaluation\": \"Always reply when addressed.\",\n180 \" value \": 55 75 ,\n181 \" decision \": \"< SUBMIT >\"}\n182 </ CONVERSATION >\n183\n184 < CONVERSATION >\nListing 1. The prompt used by Koala I and Koala II\nManuscript submitted to ACM\n30 Houde et al.\nB Survey Questions for Each Experimental Condition (Study 1)\nThe survey questions completed by study participants after each brainstorming round in Study 1 are presented here.\nThe surveys were designed to evaluate participant experiences across the conditions: No AI, Reactive AI, and Proactive\nAI. Each set of questions aimed to capture both quantitative and qualitative insights regarding the effectiveness of\nthe brainstorming sessions, the quality of collaboration, and the perceived impact of the AI’s behavior on the overall\nprocess. Experience questions were based on the Creativity Support Index of Dewit et al. [13] and thesis work of Asio\n[3]. Some questions were repeated across the different surveys to allow for consistent comparisons between conditions,\nwhile others were tailored specifically to the presence and behavior of the AI.\nB.1 Initial impression questions\n•Open response: Was this an effective brainstorming session? Were you happy with the ideas produced?\n•Open response: Did people work well together? Was there room for improvement?\n•Open response, AI conditions only: Did the people and Koala work well together? Was there room for improvement?\nB.2 Experience questions\nParticipants rated agreement with statements on the following scale.\n•Not at all\n•A little\n•Somewhat\n•A great deal\n•N/A\nIn the variable statements below enclosed by <> brackets, references to Slack were used in the post No AI condition survey,\nand references to Koala were used in surveys administered after each of the AI conditions.\nCreativity questions. Please evaluate the following questions about your experience brainstorming in <Slack/with\nKoala present in the Slack channel>. Answer with your first instinctive response!\n•Having <Slack/Koala in the group> allowed other people to work with me easily.\n•It was really easy to share ideas and designs with other people <in Slack/when Koala was in the group>.\n•I would be happy to use <Slack/Koala> on a regular basis.\n•I enjoyed using <Slack/Koala>.\n•It was easy for me to explore many different ideas, options, designs, or outcomes using <Slack/Koala>.\n•I was able to be very creative while working in <Slack/Koala>.\n•<Slack/having Koala in the group> allowed me to be very expressive.\n•I was satisfied by what I got out of working <in Slack/with Koala>.\n•What I was able to produce was worth the effort I had to exert to produce it.\nContribution questions. Please evaluate the following questions about your experience brainstorming in Slack. Answer\nwith your first instinctive response!\n•Contributions made by other people were novel.\n•Contributions made by other people were valuable\n•Contributions made by other people were surprising.\nManuscript submitted to ACM\nControlling AI Agent Participation in Group Conversations: A Human-Centered Approach 31\n•AI conditions only: Contributions made by Koala were novel.\n•AI conditions only: Contributions made by Koala were valuable.\n•AI conditions only: Contributions made by Koala were surprising.\nB.3 Condition comparison questions\nThese questions were included after the AI condition experiences only.\nAfter Reactive AI experience.\n•Open response: How was working with Koala in this channel different from working without Koala in the previous\nchannel?\nAfter Proactive AI experience. Which of the experiences among the three Slack channels used today did you prefer?\n•Channel 1: Brainstorming without Koala\n•Channel 2: Brainstorming with Koala contributing only when asked\n•Channel 3: Brainstorming with an autonomous Koala that contributed proactively\n•No preference\n•Unsure\nParticipants selected a single option, then were asked the following follow-up questions.\n•Open response: Please explain your preference.\n•Open response: Are there other ways that an AI would ideally be integrated into a brainstorm?\nManuscript submitted to ACM"
}