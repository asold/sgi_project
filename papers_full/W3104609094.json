{
  "title": "Generating Radiology Reports via Memory-driven Transformer",
  "url": "https://openalex.org/W3104609094",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2122151123",
      "name": "Zhihong Chen",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A2103229335",
      "name": "Yan Song",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A4277606560",
      "name": "Tsung-Hui Chang",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A2113629113",
      "name": "Xiang Wan",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2116492146",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3104027471",
    "https://openalex.org/W2963870701",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4288284003",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2964195337",
    "https://openalex.org/W3035193825",
    "https://openalex.org/W4287992577",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2742049800",
    "https://openalex.org/W3101638898",
    "https://openalex.org/W2891585127",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2904551248",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W3098325931",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2803411968",
    "https://openalex.org/W4287899420",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2929309437",
    "https://openalex.org/W2903721568",
    "https://openalex.org/W2985703188",
    "https://openalex.org/W2990515515",
    "https://openalex.org/W3018792722",
    "https://openalex.org/W2808001006",
    "https://openalex.org/W3106165216",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2970401203"
  ],
  "abstract": "Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1439‚Äì1449,\nNovember 16‚Äì20, 2020.c‚Éù2020 Association for Computational Linguistics\n1439\nGenerating Radiology Reports via Memory-driven Transformer\nZhihong Chen‚ô†‚ô•, Yan Song‚ô†‚ô•‚Ä†, Tsung-Hui Chang‚ô†‚ô•, Xiang Wan‚ô•\n‚ô†The Chinese University of Hong Kong (Shenzhen)\n‚ô•Shenzhen Research Institute of Big Data\n‚ô†zhihongchen@link.cuhk.edu.cn\n‚ô†{songyan, changtsunghui}@cuhk.edu.cn\n‚ô•wanxiang@sribd.cn\nAbstract\nMedical imaging is frequently used in clinical\npractice and trials for diagnosis and treatment.\nWriting imaging reports is time-consuming\nand can be error-prone for inexperienced radi-\nologists. Therefore, automatically generating\nradiology reports is highly desired to lighten\nthe workload of radiologists and accordingly\npromote clinical automation, which is an es-\nsential task to apply artiÔ¨Åcial intelligence to\nthe medical domain. In this paper, we propose\nto generate radiology reports with memory-\ndriven Transformer, where a relational mem-\nory is designed to record key information of\nthe generation process and a memory-driven\nconditional layer normalization is applied to\nincorporating the memory into the decoder of\nTransformer. Experimental results on two pre-\nvailing radiology report datasets, IU X-Ray\nand MIMIC-CXR, show that our proposed ap-\nproach outperforms previous models with re-\nspect to both language generation metrics and\nclinical evaluations. Particularly, this is the\nÔ¨Årst work reporting the generation results on\nMIMIC-CXR to the best of our knowledge.\nFurther analyses also demonstrate that our ap-\nproach is able to generate long reports with\nnecessary medical terms as well as meaningful\nimage-text attention mappings.1\n1 Introduction\nRadiology report generation, which aims to au-\ntomatically generate a free-text description for a\nclinical radiograph (e.g., chest X-ray), has emerged\nas a prominent attractive research direction in both\nartiÔ¨Åcial intelligence and clinical medicine. It can\ngreatly expedite the automation of workÔ¨Çows and\nimprove the quality and standardization of health\ncare. Recently, there are many methods proposed\n‚Ä†Corresponding author.\n1Our code and the best performing models are released at\nhttps://github.com/cuhksz-nlp/R2Gen.\nFindings\nThe lungs are clear bilaterally. Specifically,\nno evidence of focal consolidation, or ple-\nural effusion. Minimal right basilar subse-\ngmental atelectasis noted. Cardio medias-\ntinal silhouette is unremarkable. Tortuosity\nof the thoracic aorta noted.\nImpression\nNo acute cardiopulmonary abnormality.\nFigure 1: An example chest X-ray image and its report\nincluding Ô¨Åndings and impression.\nin this area (Jing et al., 2018; Li et al., 2018; John-\nson et al., 2019; Liu et al., 2019; Jing et al., 2019).\nPractically, a signiÔ¨Åcant challenge of radiology\nreport generation is that radiology reports are long\nnarratives consisting of multiple sentences. As il-\nlustrated by Figure 1, a radiology report generally\nconsists of a section of Ô¨Åndings which describes\nmedical observations, including both normal and\nabnormal features, as well as an impression or con-\ncluding remark summarizing the most prominent\nobservations. Therefore, applying conventional im-\nage captioning approaches (Vinyals et al., 2015;\nAnderson et al., 2018) may be insufÔ¨Åcient for ra-\ndiology report generation, as such approaches are\ndesigned to brieÔ¨Çy describe visual scenes with short\nsentences. The ability to provide accurate clinical\ndescriptions for a radiograph is of the highest pri-\nority, which places a higher demand on the genera-\ntion process. Nevertheless, despite the difÔ¨Åculties\nposed by these evident length and accuracy require-\nments, radiology reports do have their own distinc-\ntive characteristics. An important feature to note\nis their highly patternized nature, as illustrated by\nthe sample report described above (Figure 1). On\nthe basis of this patternization, many approaches\nhave been proposed to address the challenges of\nradiology report generation. For example, Liu et al.\n(2019) found that a simple retrieval-based method\ncould achieve a comparative performance for this\ntask. Li et al. (2018) combined retrieval-based and\ngeneration-based methods with manually extracted\n1440\ntemplates. Although promising results may be ob-\ntained by the retrieval-based approaches, they are\nstill limited in the preparation of large databases,\nor the explicit construction of template lists to de-\ntermine the patterns embedded in various reports.\nIn this paper, we propose to generate radiology\nreports via memory-driven Transformer. In detail,\na relational memory (RM) is proposed to record\nthe information from previous generation processes\nand a novel memory-driven conditional layer nor-\nmalization (MCLN) is designed to incorporate the\nrelational memory into Transformer (Vaswani et al.,\n2017). As a result, similar patterns in different med-\nical reports can be implicitly modeled and memo-\nrized during the generation process, which thereby\ncan facilitate the decoding of Transformer and is\ncapable of generating long reports with informative\ncontent. Experimental results on two benchmark\ndatasets conÔ¨Årm the validity and effectiveness of\nour approach, where Transformer with RM and\nMCLN achieves the state-of-the-art performance\non all datasets. To summarize, the contributions of\nthis paper are four-fold:\n‚Ä¢We propose to generate radiology reports via a\nnovel memory-driven Transformer model.\n‚Ä¢We propose a relational memory to record the\nprevious generation process and the MCLN to\nincorporate relational memory into layers in the\ndecoder of Transformer.\n‚Ä¢Extensive experiments are performed and the re-\nsults show that our proposed models outperform\nthe baselines and existing models.\n‚Ä¢We conduct analyses to investigate the effect\nof our model with respect to different memory\nsizes and show that our model is able to generate\nlong reports with necessary medical terms and\nmeaningful image-text attention mappings.\n2 The Proposed Method\nGenerating radiology reports is essentially an\nimage-to-text generation task, for which there exist\nseveral solutions (Vinyals et al., 2015; Xu et al.,\n2015; Anderson et al., 2018; Cornia et al., 2019).\nWe follow the standard sequence-to-sequence\nparadigm for this task. In doing so, we treat the\ninput from a radiology image as the source se-\nquence X = {x1,x2,..., xS},xs ‚àà Rd, where\nxs are patch features extracted from visual ex-\ntractors and dthe size of the feature vector. The\ncorresponding report is the target sequence Y =\n{y1,y2,...,y T},yt ‚ààV, where ytare the generated\ntokens, T the length of generated tokens and V the\nvocabulary of all possible tokens. An overview of\nour proposed model is shown in Figure 2, where\nthe details are illustrated in following subsections.\n2.1 The Model Structure\nOur model can be partitioned into three major com-\nponents, i.e., the visual extractor, the encoder and\nthe decoder, where the proposed memory and the\nintegration of the memory into Transformer are\nmainly performed in the decoder. The overall de-\nscription of the three components and the training\nobjective of the task is detailed below.\nVisual Extractor Given a radiology image Img,\nits visual features X are extracted by pre-trained\nconvolutional neural networks (CNN), e.g., VGG\n(Simonyan and Zisserman, 2015) or ResNet (He\net al., 2016), and the encoded results are used as the\nsource sequence for all subsequent modules. The\nprocess is formulated as:\n{x1,x2,..., xS}= fv(Img) (1)\nwhere fv(¬∑) represents the visual extractor.\nEncoder In our model, we use the standard en-\ncoder from Transformer, where the outputs are the\nhidden states hi encoded from the input features\nxi extracted from the visual extractor:\n{h1,h2,..., hS}= fe(x1,x2,..., xS) (2)\nwhere fe(¬∑) refers to the encoder.\nDecoder The backbone decoder in our model is\nthe one from Transformer, where we introduce an\nextra memory module to it by improving the orig-\ninal layer normalization with MCLN for each de-\ncoding layer as shown in Figure 2. Therefore the\ndecoding process can be formalized as\nyt = fd(h1,..., hS,MCLN(RM(y1,...,y t‚àí1)))\n(3)\nwhere fd(¬∑) refers to the decoder and the details\nof the memory (RM) and MCLN are presented in\nfollowing subsections.\nObjective Given the aforementioned structure, the\nentire generation process can be formalized as a\nrecursive application of the chain rule\np(Y|Img) =\nT‚àè\nt=1\np(yt|y1,...,y t‚àí1,Img) (4)\nwhere Y = {y1,y2,...,y T}is the target text se-\nquence. The model is then trained to maximize\n1441\nPatch\nFeatures\n‚Ä¶‚Ä¶\nTransformer\nEncoder Layer\nMasked Multi-Head Attention\nOutput Embedding\nLinear\nSoftmax\nOutput\nProbabilities\nN√ó\nOutputs (shifted right)\nMLP\nMLP\nŒîŒ≤t\nŒîŒ≥tŒ≥\nŒ≤Mean Std\nMulti-Head Attention\nMemory-driven Conditional LN\nFeed Forward\nMemory-driven Conditional LN\nMulti-Head\nAttention\nMLP\nGate\nMemory-driven Conditional LN\nRelational\nMemory\n√óN\nDECODER\nVisual Extractor\nImage\nENCODER\nMt-1\nMt\nConcat\nExpand\nV K Q\nV K Q\nQ K V\nX\nmt\nyt-1\nZ\nImg\nr\nŒº ùùÇ\nŒ≥\"t\nŒ≤#t\nM$t\nFigure 2: The overall architecture of our proposed model, where the visual extractor, encoder and decoder are\nshown in gray dash boxes and the details of the visual extractor and encoder are omitted. The relational memory\nand memory conditional layer-normalization are illustrated in grey solid boxes with blue dash lines.\nP(Y|Img) through the negative conditional log-\nlikelihood of Y given the Img:\nŒ∏‚àó= arg max\nŒ∏\nT‚àë\nt=1\nlog p(yt|y1,...,y t‚àí1,Img; Œ∏)\n(5)\nwhere Œ∏is the parameters of the model.\n2.2 Relational Memory\nFor any relevant Img, they may share similar pat-\nterns in their reports and they can be used as good\nreferences for each other to help the generation pro-\ncess. As shown in Figure 1, patterns such as ‚ÄúThe\nlungs are clear bilaterally ‚Äù and ‚Äúno evidence of\nfocal consolidation, or pleural effusion‚Äù always ap-\npear in the reports of similar images and are shown\nsimultaneously. To exploit such characteristics, we\npropose to use an extra component, i.e., relational\nmemory, to enhance Transformer to learn from the\npatterns and facilitate computing the interactions\namong patterns and the generation process.\nIn doing so, the relational memory uses a matrix\nto transfer its states over generation steps, where\nthe states record important pattern information with\neach row (namely, memory slot) representing some\npattern information.2 During the generation, the\nmatrix is updated step-by-step with incorporating\nthe output from previous steps. Then, at time step\nt, the matrix from the previous step, Mt‚àí1, is func-\ntionalized as the query and its concatenations with\nthe previous output serve as the key and value to\nfeed the multi-head attention module. Given H\nheads used in Transformer, there are H sets of\nqueries, keys and values via three linear transfor-\nmations, respectively. For each head, we obtain\nthe query, key and value in the relational memory\nthrough Q = Mt‚àí1¬∑Wq, K = [Mt‚àí1; yt‚àí1]¬∑Wk\nand V = [Mt‚àí1; yt‚àí1] ¬∑Wv, respectively, where\nyt‚àí1 is the embedding of the last output (at step\nt‚àí1); [Mt‚àí1; yt‚àí1] is the row-wise concatena-\ntion of Mt‚àí1 and yt‚àí1. Wq, Wk and Wv are\n2Note that the rows (memory slots) and patterns do not\nfollow one-to-one mapping, where the entire matrix serves as\na whole unit to deliver the pattern information.\n1442\nForget Gate Input Gate yt-1\n\tœÉ \tœÉ\nM!t\nMt-1 Duplicate\nYt-1\nMt\nGt\nf Gt\ni\ntanh\ntanh\nFigure 3: The illustration of the gate mechanism.\nthe trainable weights of linear transformation of\nthe query, key and value, respectively. Multi-head\nattention is used to model Q, K and V so as to\ndepict relations of different patterns. As a result,\nZ = softmax(QK‚ä§/\n‚àö\ndk) ¬∑V (6)\nwhere dk is the dimension ofK, and Z the output of\nthe multi-head attention module. Consider that the\nrelational memory is performed in a recurrent man-\nner along with the decoding process, it potentially\nsuffers from gradient vanishing and exploding. We\ntherefore introduce residual connections and a gate\nmechanism. The former is formulated as\nÀúMt = fmlp(Z + Mt‚àí1) + Z + Mt‚àí1 (7)\nwhere fmlp(¬∑) refers to the multi-layer perceptron\n(MLP). The detailed structure of the gate mecha-\nnism in the relational memory is shown in Figure 3,\nwhere the forget and input gates are applied to bal-\nance the inputs from Mt‚àí1 and yt‚àí1, respectively.\nTo ensure that yt‚àí1 can be used for computation\nwith Mt‚àí1, it is extended to a matrix Yt‚àí1 by du-\nplicating it to multiple rows. Therefore, the forget\nand input gate are formalized as\nGf\nt = Yt‚àí1Wf + tanh(Mt‚àí1) ¬∑Uf (8)\nGi\nt = Yt‚àí1Wi + tanh(Mt‚àí1) ¬∑Ui (9)\nwhere Wf and Wi are trainable weights for Yt‚àí1\nin each gate; similarly,Uf and Ui are the trainable\nweights for Mt‚àí1 in each gate. The Ô¨Ånal output of\nthe gate mechanism is formalized as\nMt = œÉ(Gf\nt) ‚äôMt‚àí1 + œÉ(Gi\nt) ‚äôtanh( ÀúMt)\n(10)\nwhere ‚äôrefers to the Hadamard product and œÉthe\nsigmoid function and Mt is the output of the entire\nrelational memory module at step t.\n2.3 Memory-driven Conditional Layer\nNormalization\nAlthough memory shows its effectiveness in many\nNLP tasks (Sukhbaatar et al., 2015; Lample et al.,\nDATASET IU X-RAY MIMIC-CXR\nTRAIN VAL TEST TRAIN VAL TEST\nIMAGE # 5,226 748 1,496 368,960 2,991 5,159\nREPORT # 2,770 395 790 222,758 1,808 3,269\nPATIENT # 2,770 395 790 64,586 500 293\nAVG. L EN. 37.56 36.78 33.62 53.00 53.05 66.40\nTable 1: The statistics of the two benchmark datasets\nw.r.t. their training, validation and test sets, including\nthe numbers of images, reports and patients, and the\naverage word-based length (AVG. L EN.) of reports.\n2019), it is by default applied to encoding with\nrather isolated designs. However, given that text\ngeneration is a dynamic process and largely af-\nfected by the output at each decoding step, memory\nis expected to be closely integrated to the decoder.\nTherefore, we propose a novel MCLN and use\nit to incorporate the relational memory to enhance\nthe decoding of Transformer. Recall that in the con-\nventional Transformer, to improve generalization,\nŒ≥and Œ≤are two crucial parameters for scaling and\nshifting the learned representations,3 respectively.\nThus we propose to incorporate the relational mem-\nory via MCLN by feeding its output Mt to Œ≥and\nŒ≤. Consequently, this design takes the beneÔ¨Åt from\nthe memory while preventing it from inÔ¨Çuencing\ntoo many parameters of Transformer so that some\ncore information for generation is not affected.\nAs shown in Figure 2, in each Transformer de-\ncoding layer, we use three MCLNs, where the out-\nput of the Ô¨Årst MCLN is functionalized as the query\nto be fed into the following multi-head attention\nmodule together with the hidden states from the\nencoder as the key and value. To feed each MCLN,\nat step t, the output of the relational memory Mt is\nexpanded into a vectormt by simply concatenating\nall rows from Mt. Then, an MLP is used to predict\na change ‚àÜŒ≥t on Œ≥t from mt, and update it via\n‚àÜŒ≥t = fmlp(mt) (11)\nÀÜŒ≥t = Œ≥+ ‚àÜŒ≥t (12)\nSimilarly, ‚àÜŒ≤t and ÀÜŒ≤t are performed by\n‚àÜŒ≤t = fmlp(mt) (13)\nÀÜŒ≤t = Œ≤+ ‚àÜŒ≤t (14)\nAfterwards, the predicted ÀÜŒ≤t and ÀÜŒ≥t are applied to\nthe mean and variance results of the multi-head\n3In detail, Œ≥ is used to amplify the values in the learned\nrepresentation and Œ≤ provides a bias adjustment to them.\n1443\nDATA MODEL NLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L A VG. ‚àÜ P R F1\nIU\nX-R AY\nBASE 0.396 0.254 0.179 0.135 0.164 0.342 - - - -\n+RM 0.444 0.283 0.196 0.141 0.179 0.364 8.9% - - -\n+RM+MCLN 0.470 0.304 0.219 0.165 0.187 0.371 17.6% - - -\nMIMIC\n-CXR\nBASE 0.314 0.192 0.127 0.090 0.125 0.265 - 0.331 0.224 0.228\n+RM 0.330 0.200 0.133 0.095 0.128 0.265 3.7% 0.325 0.243 0.249\n+RM+MCLN 0.353 0.218 0.145 0.103 0.142 0.277 12.1% 0.333 0.273 0.276\nTable 2: The performance of all baselines and our full model on the test sets of IU X-R AY and MIMIC-CXR\ndatasets with respect to NLG and CE metrics. BL-n denotes BLEU score using up to n-grams; MTR and RG-L\ndenote METEOR and ROUGE-L, respectively. The average improvement over all NLG metrics compared to BASE\nis also presented in the ‚ÄúAVG. ‚àÜ‚Äù column. The performance of all models is averaged from Ô¨Åve runs.\nself-attention from the previous generated outputs:\nfmcln(r) = ÀÜŒ≥t ‚äôr ‚àí¬µ\nœÖ + ÀÜŒ≤t (15)\nwhere r refers to the output from the previous mod-\nule; ¬µand œÖare the mean and standard deviation of\nr, respectively. The result fmcln(r) from MCLN\nis then fed to the next module (for the 1st and 2nd\nMCLN) or used as the Ô¨Ånal output for generation\n(for the 3rd MCLN).\n3 Experiment Settings\n3.1 Datasets\nWe conduct our experiments on two datasets, which\nare described as follows:\n‚Ä¢IU X-RAY (Demner-Fushman et al., 2016)4: a\npublic radiography dataset collected by Indiana\nUniversity with 7,470 chest X-ray images and\n3,955 reports.\n‚Ä¢MIMIC-CXR (Johnson et al., 2019) 5: the\nlargest radiology dataset to date that consists of\n473,057 chest X-ray images and 206,563 reports\nfrom 63,478 patients.\nFor both datasets, we follow Li et al. (2018) to ex-\nclude the samples without reports. Then we apply\ntheir conventional splits. SpeciÔ¨Åcally, IU X-R AY is\npartitioned into train/validation/test set by 7:1:2 of\nthe entire dataset, and MIMIC-CXR ‚Äôs ofÔ¨Åcial split\nis adopted. The statistics of the datasets are shown\nin Table 1, with the numbers of images, reports,\npatients and the average length of reports.\n3.2 Baseline and Evaluation Metrics\nTo compare with our proposed model, the follow-\ning ones are used as the main baselines:\n4https://openi.nlm.nih.gov/\n5https://physionet.org/content/\nmimic-cxr/2.0.0/\n‚Ä¢BASE : this is the vanilla Transformer, with three\nlayers, 8 heads and 512 hidden units without\nother extensions and modiÔ¨Åcations.\n‚Ä¢BASE +RM: this is a simple alternative of our\nproposed model where the relational memory is\ndirectly concatenated to the output of the Trans-\nformer ahead of the softmax at each time step.\nThis baseline aims to demonstrate the effect of\nusing memory as an extra component instead of\nintegration within the Transformer.\nIn addition, we also compare our model with those\nin previous studies, including conventional image\ncaptioning models, e.g., ST (Vinyals et al., 2015),\nATT2IN (Rennie et al., 2017), ADAATT (Lu et al.,\n2017), TOPDOWN (Anderson et al., 2018), and\nthe ones proposed for the medical domain, e.g.,\nCOATT (Jing et al., 2018), HRGR (Li et al., 2018)\nand CMAS -RL (Jing et al., 2019).\nThe performance of the aforementioned models\nis evaluated by conventional natural language gen-\neration (NLG) metrics and clinical efÔ¨Åcacy (CE)\nmetrics6. The NLG metrics 7 include BLEU (Pa-\npineni et al., 2002), METEOR (Denkowski and\nLavie, 2011) and ROUGE-L (Lin, 2004). For clin-\nical efÔ¨Åcacy metrics, we use the CheXpert (Irvin\net al., 2019) 8 to label the generated reports and\ncompare the results with ground truths in 14 dif-\nferent categories related to thoracic diseases and\nsupport devices. Precision, recall and F1 are used\nto evaluate model performance for these metrics.\n6Note that CE metrics only apply to MIMIC-CXR be-\ncause the labeling schema of CheXpert is designed for\nMIMIC-CXR, which is different from that of IU X-R AY.\n7https://github.com/tylin/coco-caption\n8https://github.com/MIT-LCP/mimic-cxr/\ntree/master/txt/chexpert\n1444\nDATA MODEL NLG METRICS CE METRICS\nBL-1 BL-2 BL-3 BL-4 MTR RG-L P R F1\nIU\nX-R AY\nST‚ôÆ 0.216 0.124 0.087 0.066 - 0.306 - - -\nATT2IN‚ôÆ 0.224 0.129 0.089 0.068 - 0.308 - - -\nADAATT‚ôÆ 0.220 0.127 0.089 0.068 - 0.308 - - -\nCOATT‚ôÆ 0.455 0.288 0.205 0.154 - 0.369 - - -\nHRGR ‚ôÆ 0.438 0.298 0.208 0.151 - 0.322 - - -\nCMAS -RL‚ôÆ 0.464 0.301 0.210 0.154 - 0.362 - - -\nOURS 0.470 0.304 0.219 0.165 0.187 0.371 - - -\nMIMIC\n-CXR\nST‚ôØ 0.299 0.184 0.121 0.084 0.124 0.263 0.249 0.203 0.204\nATT2IN‚ôØ 0.325 0.203 0.136 0.096 0.134 0.276 0.322 0.239 0.249\nADAATT‚ôØ 0.299 0.185 0.124 0.088 0.118 0.266 0.268 0.186 0.181\nTOPDOWN ‚ôØ 0.317 0.195 0.130 0.092 0.128 0.267 0.320 0.231 0.238\nOURS 0.353 0.218 0.145 0.103 0.142 0.277 0.333 0.273 0.276\nTable 3: Comparisons of our full model with previous studies on the test sets of IU X-R AY and MIMIC-CXR\nwith respect to NLG and CE metrics. ‚ôÆ refers to that the result is directed cited from the original paper and ‚ôØ\nrepresents our replicated results by their codes.\n3.3 Implementation Details\nWe adopt the ResNet101 (He et al., 2016) pre-\ntrained on Imagenet (Deng et al., 2009) as the vi-\nsual extractor to extract patch features with the\ndimension of each feature set to 2,048. Note that\nfor IU X-R AY, we use two images of a patient as\ninput to ensure consistency with the experiment\nsettings of previous work. The Transformer in our\nproposed model and all baselines are randomly ini-\ntialized. For relational memory, its dimension and\nthe number of heads in multi-head attention are\nset to 512 and 8, respectively, and the number of\nmemory slots is set to 3 by default. For MCLN,\nwe use two MLPs to obtain ‚àÜŒ≥ and ‚àÜŒ≤ where\nthey do not share parameters. The model is trained\nunder cross entropy loss with ADAM optimizer\n(Kingma and Ba, 2015). We set the learning rate\nto 5e-5 and 1e-4 for the visual extractor and other\nparameters, respectively. We decay such rate by a\nfactor of 0.8 per epoch for each dataset and set the\nbeam size to 3 to balance the generation effective-\nness and efÔ¨Åciency. Note that the aforementioned\nhyper-parameters are obtained by evaluating the\nmodels on the validation sets of the two datasets.\n4 Results and Analyses\n4.1 Effect of Relational Memory\nTo illustrate the effectiveness of our proposed\nmethod, we experiment with the aforementioned\nbaselines on the two benchmark datasets. The\nresults are reported in Table 2, with BASE +RM+\nMCLN representing our full model (same below).\nThere are several observations. First, on NLG\nmetrics, both BASE +RM and BASE +RM+MCLN\noutperform the vanilla Transformer (BASE ) on both\ndatasets, which conÔ¨Årms the validity of incorporat-\ning memory into the decoding process in Trans-\nformer because that highly-patternized text in ra-\ndiology reports are reasonably modeled to some\nextent. Second, our full model achieves the best\nperformance over all baselines on different met-\nrics, and it particularly outperforms BASE +RM\nwith signiÔ¨Åcant improvement, which clearly in-\ndicates the usefulness of MCLN in incorporat-\ning memory rather than other ways of integra-\ntion. Third, on NLG metrics, when comparing\nbetween the datasets, the performance gains from\ntwo memory-driven models (i.e., BASE +RM and\nBASE +RM+MCLN ) over BASE on IU X-R AY are\nlarger than that of MIMIC-CXR . The reason be-\nhind might be that theIU X-R AY is relatively small\nand patterns among different reports in this dataset\nare more consistent so that our model helps more\nwith the proposed memory. Fourthly, on the CE\nmetrics on MIMIC-CXR , our full model shows\nthe same trend as that for NLG metrics, where\nit outperforms all its baselines in terms of preci-\nsion, recall and F1. This observation is impor-\ntant because higher NLG scores do not always re-\nsult in higher clinical scores (e.g., the precision of\nBASE +RM on CE is lower than that of BASE ), so\n1445\n|S| PARA . BL-1 BL-2 MTR RG-L\n1 76.6M 0.350 0.217 0.141 0.278\n2 81.4M 0.355 0.215 0.141 0.278\n3 86.1M 0.360 0.223 0.144 0.279\n4 90.8M 0.354 0.217 0.142 0.280\nTable 4: NLG scores of our full model on the MIMIC-\nCXR test set when different memory slots are used.\nPARA . denotes the number of parameters.\nthat the performance from CE further conÔ¨Årms the\neffectiveness of our method, whereas compared to\nBASE +RM, MCLN is able to leverage memory in\na rather Ô¨Åne-grained way and thus better produce\nreasonable descriptions for clinical abnormalities.\n4.2 Comparison with Previous Studies\nWe compare our full model (denoted as OURS )\nwith existing models on the same datasets, with\nall results reported in Table 3 on both NLG and\nCE metrics. There are several observations drawn\nfrom different aspects. First, Transformer conÔ¨Årms\nits superiority to sequence-to-sequence structures\nin this task, which is illustrated by the compar-\nison between our models (all baselines and our\nfull model) and ST. Our full model also outper-\nforms conventional image captioning models, e.g.,\nATT2IN, ADAATT and TOPDOWN , which are de-\nsigned to generate a short piece of text for an image.\nThis observation conÔ¨Årms that designing a speciÔ¨Åc\nmodel for long report generation is necessary for\nthis task. Second, memory shows its effectiveness\nin this task when compared with those complicated\nmodels, e.g., HRGR uses manually extracted tem-\nplates. Particularly, although on the two datasets,\nreinforcement learning (CMAS -RL) is proved to be\nthe best solution with a careful design of adaptive\nrewards, our model achieves the same goal with\na simpler method. Third, It is noticed that there\nare studies, e.g., HRGR , requires to utilize extra\ninformation for this task and our full model outper-\nforms them without such requirements. This ob-\nservation indicates that an appropriate end-to-end\ndesign (such as RM and MCLN) of using mem-\nory in Transformer can alleviate the need for extra\nresources to enhance this task.\n4.3 Analysis\nWe analyze several aspects of our model regarding\nits hyper-parameters and generation results.\n0 20 40 60 80 100\nLength\n0\n200\n400\n600\n800\n1000Report #\nGround-truth\nBASE\nBASE+RM\nBASE+RM+MCLN\nFigure 4: The length distributions of the generated re-\nports on the MIMIC-CXR test set from B ASE , BASE +\nRM and BASE +RM+MCLN , as well as the ground-truth.\nMemory Size To show the impacts of the mem-\nory size, we train RM with different numbers of\nmemory slots, i.e., |S|‚àà{ 1,2,3,4}and the results\non MIMIC-CXR are shown in Table 4. In general,\nsince memory size controls how much information\nis preserved in the past generation steps, it is con-\nÔ¨Årmed in the observation that enlarging memory\nsize by the number of slots results in better overall\nperformance, with |S|= 3 achieving the best re-\nsults. Still, we notice that the overall performance\ndrops when |S|= 4, which indicates that too large\nmemory may introduce redundant and invalid in-\nformation so as to negatively affect the generation\nprocess. Although enlarging memory size results in\nincreasing parameter numbers, it is demonstrated\nthat there are not too many parameters (compar-\ning to the total number of parameters) introduced\nwhenever adding one slot in the memory. This\nobservation suggests that the proposed model is\neffective and efÔ¨Åcient in learning with memory for\nthe radiology report generation task.\nReport Length In addition to NLG and CE met-\nrics, another important criterion to evaluate gen-\neration models is the length of generated reports\ncomparing to the ground-truth. In doing so, we cat-\negorize all reports generated on the MIMIC-CXR\ntest set into 10 groups (within [0, 100] with interval\nof 10) according to their round-down lengths and\ndraw curves for their numbers in each category for\nBASE , BASE +RM and BASE +RM+MCLN , as well\nas the ground-truth. The results are presented in\nFigure 4. Overall, more reports generated from\nBASE +RM and BASE +RM+MCLN are longer than\nthat from BASE and their length distributions are\ncloser to the ground-truth reports, which thus leads\nto better evaluation results on NLG metrics. The\nreason behind might be that the memory provides\n1446\nGround-truth\nIn comparison with study of there is\nagain enlargement of the cardiac s-\nilhouette with a pacer device in pla-\nce. No definite vascular congestion\nraising the possibility of underlying\ncardiomyopathy or pleural effusion.\nNo acute focal pneumonia. The rig-\nht picc line has been removed.\nThere is a left pectoral pacemaker\nwith leads terminating in the right\natrium and right ventricle. The hea-\nrt is enlarged. There is no pneumo-\nthorax or pleural effusion. The lungs\nare clear.\nIn comparison with the study of the-\nre is little change in the appearance\nof the pacer leads which extend to\nthe right atrium and apex of the right\nventricle. Continued enlargement of\nthe cardiac silhouette without vasc-\nular congestion or pleural effusion.\nNo evidence of pneumothorax.\nBASE BASE+RM+MCLN\nGround-truth\nThere are low lung volumes. Biba-\nsilar atelectasis have minimally im-\nproved. Mild vascular congestion\nhas minimally improved. There are\nno new lung abnormalities or pne-\numothorax. Bilateral pleural effusi-\nons are small. Right picc tip is at the\ncavoatrial junction.\nIn comparison with the study of th-\nere is little overall change. Again\nthere is some indistinctness of pul-\nmonary vessels consistent with el-\nevated pulmonary venous pressure.\nNo evidence of acute focal pneum-\nothorax.\nThe lung volumes are low. There is\na small left pleural effusion with as-\nsociated atelectasis. The right lung\nis clear. There is no pneumothorax.\nThe heart size is top normal. The hi-\nlar and mediastinal contours are no-\nrmal. A right subclavian catheter te-\nrminate in the mid svc.\nBASE BASE+RM+MCLN\nFigure 5: Illustrations of reports from ground-truth, B ASE and B ASE +RM+MCLN models for two X-ray chest\nimages. To better distinguish the content in the reports, different colors highlight different medical terms.\nmore detailed information for the generation pro-\ncess so that the decoder tends to produce more\ndiversiÔ¨Åed outputs than the original Transformer.\nParticularly, when comparing BASE +RM+MCLN\nand BASE +RM, the length distribution of the for-\nmer generated reports is closer to the ground-truth,\nwhich can be explained by that, instead of applying\nmemory to the Ô¨Ånal output, leveraging memory at\neach layer in Transformer is more helpful and thus\ncontrols the decoding process in a Ô¨Åne-grained way.\nThe above observations show that both memory\nand the way of using it are two important factors to\nenhance radiology report generation.\nCase Study To further investigate the effective-\nness of our model, we perform qualitative anal-\nysis on some cases with their ground-truth and\ngenerated reports from different models. Figure\n5 shows two examples of front and lateral chest X-\nray images from MIMIC-CXR and such reports,\nwhere different colors on the texts indicate differ-\nent medical terms. It is observed in these cases that\nBASE +RM+MCLN is able to generate descriptions\naligned with that written by radiologists with sim-\nilar content Ô¨Çow. For example, in both cases, pat-\nterns in the generated reports follow the structure\nthat starting from reporting abnormal Ô¨Åndings (e.g.,\n‚Äúcardiac silhouette‚Äù and ‚Äúlung volumes‚Äù), and then\nconcluding with potential diseases (e.g., ‚Äúpleural\neffusion‚Äù and ‚Äúatelectasis‚Äù). In addition, for the\nnecessary medical terms in the ground-truth re-\nports, BASE +RM+MCLN covers almost all of them\nin its generated reports while vanilla Transformer\ndid much worse, e.g., the key terms ‚Äúenlarged car-\ndiac silhouette‚Äù, ‚Äúatelectasis‚Äù and ‚Äúsmall pleural\neffusion‚Äù in the two examples are not generated.\nTo further investigate different models quali-\ntatively, we randomly select a chest X-ray on\nthe MIMIC-CXR test set and visualize the\nimage-text attention mappings from BASE and\nBASE +RM+MCLN . Figure 6 shows the interme-\ndiate image-text correspondences for several words\nfrom the multi-head attentions in the Ô¨Årst layer of\nthe decoders. It is observed that BASE +RM+MCLN\nis better at aligning the locations with the indicated\ndisease or parts. This observation suggests that our\nmodel not only enhances the power of radiology\nreport generation, but also improves the interaction\nbetween the images and the generated texts.\nError Analysis To analyze the errors from our\nmodel, especially in targeting the low CE scores,\nit is found that the class imbalance is severe on\nthe datasets and affects the model training and in-\nference, where majority voting is observed in the\ngeneration process. For example, on MIMIC-CXR,\nconsolidation only accounts for 3.9% in the train-\ning set so that the trained model only recognizes\nthat 2.9% results in this case compared with the\nground truth 6.3%. Thus how to address the data\nbias problem is a possible future work to improve\nthe accuracy of the generated radiology reports.\n5 Related Work\nThe most popular related task to ours is image cap-\ntioning (Vinyals et al., 2015; Xu et al., 2015; An-\nderson et al., 2018; Wang et al., 2019), which aims\nto describe images with sentences. Different from\nthem, radiology report generation requires much\nlonger generated outputs, and possesses other fea-\ntures such as patterns, so that this task has its own\ncharacteristics requiring particular solutions. For\nexample, Jing et al. (2018) proposed a co-attention\n1447\nBASE: As compared to the previous radiograph there is no relevant change. Moderate cardiomegaly with mild fluid overload but no\novert pulmonary edema. No pleural effusions. No pneumonia. Unchanged right internal jugular vein catheter.\nOriginal Image ‚Äúcardiomegaly‚Äù ‚Äúpulmonary‚Äù ‚Äúpleural‚Äù ‚Äúright‚Äù\nGround-truth: There are no old films available for comparison. The heart is moderately enlarged. There is a right ij cordis with tip in the\nupper svc. There is mild pulmonary vascular re-distribution but no definite infiltrates or effusion.\nBASE+RM+MCLN: A right internal jugular central venous catheter terminates in the mid svc. The heart is moderately enlarged. The\nmediastinal and hilar contours are within normal limits. There is no pneumothorax or large pleural effusion. The lungs appear clear.\nOriginal Image ‚Äúright‚Äù ‚Äúheart‚Äù ‚Äúpleural‚Äù ‚Äúlungs‚Äù\n1.0\n0.0\nFigure 6: Visualizations of image-text attention mappings between a speciÔ¨Åc chest X-ray and generated reports\nfrom BASE and BASE +RM+MCLN , respectively. Colors from blue to red represent the weights from low to high.\nmechanism and leveraged a hierarchical LSTM to\ngenerate reports. Li et al. (2018, 2019) proposed to\nuse a manually extracted template database to help\ngeneration with bunches of special techniques to\nutilize templates. Liu et al. (2019) proposed an ap-\nproach with reinforcement learning to maintain the\nclinical accuracy of generated reports. Compared\nto these studies, our model offers an alternative\nsolution to this task with an effective and efÔ¨Åcient\nenhancement of Transformer via memory.\nExtra knowledge (e.g., pre-trained embeddings\n(Song et al., 2017; Song and Shi, 2018; Zhang\net al., 2019) and pretrained models (Devlin et al.,\n2019; Diao et al., 2019)) can provide useful infor-\nmation and thus enhance model performance for\nmany NLP tasks (Tian et al., 2020a,b,c). SpeciÔ¨Å-\ncally, memory and memory-augmented neural net-\nworks (Zeng et al., 2018; Santoro et al., 2018; Diao\net al., 2020; Tian et al., 2020d) are another line\nof related research, which can be traced back to\nWeston et al. (2015), which proposed memory net-\nworks to leverage extra information for question\nanswering; then Sukhbaatar et al. (2015) improved\nit with an end-to-end design to ensure the model\nbeing trained with less supervision. Particularly for\nTransformer, there are also memory-based methods\nproposed. For example, Lample et al. (2019) pro-\nposed to solve the under-Ô¨Åtting problem of Trans-\nformer by introducing a product-key layer that is\nsimilar to a memory module. Banino et al. (2020)\nproposed MEMO, an adaptive memory to reason\nover long-distance texts. Compared to these stud-\nies, the approach proposed in this paper focuses on\nleveraging memory for decoding rather than encod-\ning, and presents a relational memory to learn from\nprevious generation processes as well as patterns\nfor long text generation. To the best of our knowl-\nedge, this is the Ô¨Årst study incorporating memory\nfor decoding with Transformer and applied for a\nparticular task, which may provide a reference for\nstudies in the line of this research.\n6 Conclusion\nIn this paper, we propose to generate radiology\nreports with memory-driven Transformer, where\na relational memory is used to record the infor-\nmation from previous generation processes and a\nnovel layer normalization mechanism is designed\nto incorporate the memory into Transformer. Ex-\nperimental results on two benchmark datasets illus-\ntrate the effectiveness of the memory by either con-\ncatenating it with the output or integrating it with\ndifferent layers of the decoder by MCLN, which ob-\ntains the state-of-the-art performance. Further anal-\nyses investigate how memory size affects model\nperformance and show that our model is able to\ngenerate long reports with necessary medical terms\nand meaningful image-text attention mappings.\n1448\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-Up and Top-Down Attention\nfor Image Captioning and Visual Question Answer-\ning. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 6077‚Äì\n6086.\nAndrea Banino, Adri `a Puigdom `enech Badia, Raphael\nK¬®oster, Martin J Chadwick, Vinicius Zambaldi,\nDemis Hassabis, Caswell Barry, Matthew Botvinick,\nDharshan Kumaran, and Charles Blundell. 2020.\nMEMO: A Deep Network for Flexible Combi-\nnation of Episodic Memories. arXiv preprint\narXiv:2001.10913.\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\nand Rita Cucchiara. 2019. M 2: Meshed-Memory\nTransformer for Image Captioning. arXiv preprint\narXiv:1912.08226.\nDina Demner-Fushman, Marc D Kohli, Marc B Rosen-\nman, Sonya E Shooshan, Laritza Rodriguez, Sameer\nAntani, George R Thoma, and Clement J McDon-\nald. 2016. Preparing a collection of radiology ex-\naminations for distribution and retrieval. Journal\nof the American Medical Informatics Association ,\n23(2):304‚Äì310.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. ImageNet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition , pages\n248‚Äì255.\nMichael Denkowski and Alon Lavie. 2011. Meteor\n1.3: Automatic Metric for Reliable Optimization and\nEvaluation of Machine Translation Systems. In Pro-\nceedings of the sixth workshop on statistical ma-\nchine translation, pages 85‚Äì91.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171‚Äì4186.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2019. ZEN: Pre-training Chinese\nText Encoder Enhanced by N-gram Representations.\narXiv preprint arXiv:1911.00720.\nShizhe Diao, Yan Song, and Tong Zhang. 2020.\nKeyphrase Generation with Cross-Document Atten-\ntion. arXiv preprint arXiv:2004.09800.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep Residual Learning for Image\nRecognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n770‚Äì778.\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,\nSilviana Ciurea-Ilcus, Chris Chute, Henrik Mark-\nlund, Behzad Haghgoo, Robyn Ball, Katie Shpan-\nskaya, et al. 2019. CheXpert: A Large Chest Ra-\ndiograph Dataset with Uncertainty Labels and Ex-\npert Comparison. In Proceedings of the AAAI Con-\nference on ArtiÔ¨Åcial Intelligence , volume 33, pages\n590‚Äì597.\nBaoyu Jing, Zeya Wang, and Eric Xing. 2019. Show,\nDescribe and Conclude: On Exploiting the Structure\nInformation of Chest X-ray Reports. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6570‚Äì6580.\nBaoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the\nAutomatic Generation of Medical Imaging Reports.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2577‚Äì2586.\nAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz,\nNathaniel R Greenbaum, Matthew P Lungren, Chih-\nying Deng, Roger G Mark, and Steven Horng.\n2019. MIMIC-CXR: A large publicly available\ndatabase of labeled chest radiographs. arXiv\npreprint arXiv:1901.07042.\nDiederik P Kingma and Jimmy Ba. 2015. Adam:\nA Method for Stochastic Optimization. CoRR,\nabs/1412.6980.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc‚ÄôAurelio Ranzato, Ludovic Denoyer, and\nHerv¬¥e J ¬¥egou. 2019. Large Memory Layers with\nProduct Keys. In Advances in Neural Information\nProcessing Systems, pages 8546‚Äì8557.\nChristy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P\nXing. 2019. Knowledge-Driven Encode, Retrieve,\nParaphrase for Medical Image Report Generation.\nIn Proceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, volume 33, pages 6666‚Äì6673.\nYuan Li, Xiaodan Liang, Zhiting Hu, and Eric P\nXing. 2018. Hybrid Retrieval-Generation Rein-\nforced Agent for Medical Image Report Generation.\nIn Advances in neural information processing sys-\ntems, pages 1530‚Äì1540.\nChin-Yew Lin. 2004. ROUGE: A Package for Auto-\nmatic Evaluation of Summaries. In Text Summariza-\ntion Branches Out, pages 74‚Äì81.\nGuanxiong Liu, Tzu-Ming Harry Hsu, Matthew Mc-\nDermott, Willie Boag, Wei-Hung Weng, Peter\nSzolovits, and Marzyeh Ghassemi. 2019. Clinically\nAccurate Chest X-Ray Report Generation. In Ma-\nchine Learning for Healthcare Conference , pages\n249‚Äì269.\nJiasen Lu, Caiming Xiong, Devi Parikh, and Richard\nSocher. 2017. Knowing When to Look: Adaptive\nAttention via A Visual Sentinel for Image Caption-\ning. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 375‚Äì\n383.\n1449\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a Method for Automatic\nEvaluation of Machine Translation. In Proceedings\nof the 40th annual meeting on association for com-\nputational linguistics, pages 311‚Äì318.\nSteven J Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nSequence Training for Image Captioning. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition, pages 7008‚Äì7024.\nAdam Santoro, Ryan Faulkner, David Raposo, Jack\nRae, Mike Chrzanowski, Theophane Weber, Daan\nWierstra, Oriol Vinyals, Razvan Pascanu, and Timo-\nthy Lillicrap. 2018. Relational recurrent neural net-\nworks. In Advances in neural information process-\ning systems, pages 7299‚Äì7310.\nKaren Simonyan and Andrew Zisserman. 2015. Very\nDeep Convolutional Networks for Large-Scale Im-\nage Recognition. CoRR, abs/1409.1556.\nYan Song, Chia-Jung Lee, and Fei Xia. 2017. Learn-\ning Word Representations with Regularization from\nPrior Knowledge. In Proceedings of the 21st Confer-\nence on Computational Natural Language Learning\n(CoNLL 2017), pages 143‚Äì152.\nYan Song and Shuming Shi. 2018. Complementary\nLearning of Word Embeddings. In Proceedings of\nthe 27th International Joint Conference on ArtiÔ¨Åcial\nIntelligence, pages 4368‚Äì4374.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-To-End Memory Networks. In Ad-\nvances in neural information processing systems ,\npages 2440‚Äì2448.\nYuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xi-\naojun Quan, Tong Zhang, and Yonggang Wang.\n2020a. Joint Chinese Word Segmentation and Part-\nof-speech Tagging via Two-way Attentions of Auto-\nanalyzed Knowledge. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8286‚Äì8296.\nYuanhe Tian, Yan Song, and Fei Xia. 2020b. Supertag-\nging Combinatory Categorial Grammar with Atten-\ntive Graph Convolutional Networks. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing.\nYuanhe Tian, Yan Song, Fei Xia, and Tong Zhang.\n2020c. Improving Constituency Parsing with Span\nAttention. In Findings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning.\nYuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and\nYonggang Wang. 2020d. Improving Chinese Word\nSegmentation with Wordhood Memory Networks.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8274‚Äì8285.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and\nDumitru Erhan. 2015. Show and Tell: A Neural Im-\nage Caption Generator. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 3156‚Äì3164.\nWeixuan Wang, Zhihong Chen, and Haifeng Hu. 2019.\nHierarchical Attention Network for Image Caption-\ning. In Proceedings of the AAAI Conference on Arti-\nÔ¨Åcial Intelligence, volume 33, pages 8957‚Äì8964.\nJason Weston, Sumit Chopra, and Antoine Bordes.\n2015. Memory Networks. CoRR, abs/1410.3916.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, Attend and Tell:\nNeural Image Caption Generation with Visual Atten-\ntion. In International conference on machine learn-\ning, pages 2048‚Äì2057.\nJichuan Zeng, Jing Li, Yan Song, Cuiyun Gao,\nMichael R Lyu, and Irwin King. 2018. Topic Mem-\nory Networks for Short Text ClassiÔ¨Åcation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3120‚Äì\n3131.\nHongming Zhang, Jiaxin Bai, Yan Song, Kun Xu,\nChanglong Yu, Yangqiu Song, Wilfred Ng, and\nDong Yu. 2019. Multiplex Word Embeddings for\nSelectional Preference Acquisition. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5250‚Äì5259.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7853344678878784
    },
    {
      "name": "Workload",
      "score": 0.600612998008728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5067134499549866
    },
    {
      "name": "Transformer",
      "score": 0.48144882917404175
    },
    {
      "name": "Medical imaging",
      "score": 0.42952844500541687
    },
    {
      "name": "Machine learning",
      "score": 0.37396109104156494
    },
    {
      "name": "Natural language processing",
      "score": 0.35030505061149597
    },
    {
      "name": "Medical physics",
      "score": 0.33094826340675354
    },
    {
      "name": "Computer engineering",
      "score": 0.3227171301841736
    },
    {
      "name": "Medicine",
      "score": 0.12211573123931885
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}