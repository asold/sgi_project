{
    "title": "Swin transformer-based GAN for multi-modal medical image translation",
    "url": "https://openalex.org/W4290612585",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4290614380",
            "name": "Shouang Yan",
            "affiliations": [
                "Yantai University"
            ]
        },
        {
            "id": "https://openalex.org/A2097600058",
            "name": "Chengyan Wang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2104163509",
            "name": "Weibo Chen",
            "affiliations": [
                "Philips (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2416686940",
            "name": "Jun Lyu",
            "affiliations": [
                "Yantai University"
            ]
        },
        {
            "id": "https://openalex.org/A4290614380",
            "name": "Shouang Yan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097600058",
            "name": "Chengyan Wang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2104163509",
            "name": "Weibo Chen",
            "affiliations": [
                "Philips (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2416686940",
            "name": "Jun Lyu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3166504651",
        "https://openalex.org/W2322516934",
        "https://openalex.org/W2068148346",
        "https://openalex.org/W3092212736",
        "https://openalex.org/W4281757305",
        "https://openalex.org/W4205953442",
        "https://openalex.org/W3212675060",
        "https://openalex.org/W2963768110",
        "https://openalex.org/W2903356598",
        "https://openalex.org/W3127329503",
        "https://openalex.org/W3007486523",
        "https://openalex.org/W3157076986",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2962932373",
        "https://openalex.org/W6761595417",
        "https://openalex.org/W6802349413",
        "https://openalex.org/W4214612132",
        "https://openalex.org/W4212875960",
        "https://openalex.org/W3206685025",
        "https://openalex.org/W4312694282",
        "https://openalex.org/W4312254739",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W6792112992",
        "https://openalex.org/W4312560592",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W2963727650",
        "https://openalex.org/W3011043518",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3008653537",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W4226157248",
        "https://openalex.org/W2128480089",
        "https://openalex.org/W3205963081",
        "https://openalex.org/W4226205381",
        "https://openalex.org/W4287901440",
        "https://openalex.org/W4225973304",
        "https://openalex.org/W3158858001",
        "https://openalex.org/W2974967908",
        "https://openalex.org/W2979387929",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W4386702657"
    ],
    "abstract": "Medical image-to-image translation is considered a new direction with many potential applications in the medical field. The medical image-to-image translation is dominated by two models, including supervised Pix2Pix and unsupervised cyclic-consistency generative adversarial network (GAN). However, existing methods still have two shortcomings: 1) the Pix2Pix requires paired and pixel-aligned images, which are difficult to acquire. Nevertheless, the optimum output of the cycle-consistency model may not be unique. 2) They are still deficient in capturing the global features and modeling long-distance interactions, which are critical for regions with complex anatomical structures. We propose a Swin Transformer-based GAN for Multi-Modal Medical Image Translation, named MMTrans. Specifically, MMTrans consists of a generator, a registration network, and a discriminator. The Swin Transformer-based generator enables to generate images with the same content as source modality images and similar style information of target modality images. The encoder part of the registration network, based on Swin Transformer, is utilized to predict deformable vector fields. The convolution-based discriminator determines whether the target modality images are similar to the generator or from the real images. Extensive experiments conducted using the public dataset and clinical datasets showed that our network outperformed other advanced medical image translation methods in both aligned and unpaired datasets and has great potential to be applied in clinical applications.",
    "full_text": "Swin transformer-based GAN\nfor multi-modal medical\nimage translation\nShouang Yan1, ChengyanWang2, WeiboChen3 and JunLyu1*\n1School of Computer and Control Engineering, Yantai University, Yantai, China,2Human Phenome\nInstitute, Fudan University, Shanghai, China,3Philips Healthcare, Shanghai, China\nMedical image-to-image translation is considered a new direction with many\npotential applications in the medical ﬁeld. The medical image-to-image\ntranslation is dominated by two models, including supervised Pix2Pix and\nunsupervised cyclic-consistency generative adversarial network (GAN).\nHowever, existing methods still have two shortcomings: 1) the Pix2Pix requires\npaired and pixel-aligned images, which are difﬁcult to acquire. Nevertheless, the\noptimum output of the cycle-consistency model may not be unique. 2) They are\nstill deﬁcient in capturing the global features and modeling long-distance\ninteractions, which are critical for regions with complex anatomical structures.\nWe propose a Swin Transformer-based GAN for Multi-Modal Medical Image\nTranslation, named MMTrans. Speciﬁcally, MMTrans consists of a generator, a\nregistration network, and a discrimi nator. The Swin Transformer-based\ngenerator enables to generate images with the same content as source\nmodality images and similar style information of target modality images. The\nencoder part of the registration network, based on Swin Transformer, is utilized\nto predict deformable vector ﬁelds. The convolution-based discriminator\ndetermines whether the target modality images are similar to the generator or\nfrom the real images. Extensive experiments conducted using the public dataset\nand clinical datasets showed that our network outperformed other advanced\nmedical image translation methods in both aligned and unpaired datasets and\nhas great potential to be applied in clinical applications.\nKEYWORDS\nmagnetic resonance imaging, Swin Transformer, generative adversarial network,\nmulti-modal, medical image translation frontiers\n1 Introduction\nMagnetic resonance imaging (MRI) has become one of the most widely used and\npowerful tools for clinical diagnosis and treatment nowadays. Since it is a non-invasive\nimaging method, MRI can yield multiple tissue contrasts by applying various pulse\nsequences and parameters without exposing the subject to radiation, thus generating\nFrontiers inOncology frontiersin.org01\nOPEN ACCESS\nEDITED BY\nJakub Nalepa,\nSilesian University of Technology,\nPoland\nREVIEWED BY\nAnjany Sekuboyina,\nTechnical University of Munich,\nGermany\nHyejoo Kang,\nLoyola University Chicago,\nUnited States\n*CORRESPONDENCE\nJun Lyu\nLjdream0710@pku.edu.cn\nSPECIALTY SECTION\nThis article was submitted to\nCancer Imaging and\nImage-directed Interventions,\na section of the journal\nFrontiers in Oncology\nRECEIVED 12 May 2022\nACCEPTED 11 July 2022\nPUBLISHED 08 August 2022\nCITATION\nYan S,Wang C,Chen W andLyu J\n(2022) Swin transformer-based\nGAN for multi-modal medical\nimage translation.\nFront. Oncol.12:942511.\ndoi: 10.3389/fonc.2022.942511\nCOPYRIGHT\n© 2022 Yan, Wang, Chen and Lyu. This\nis an open-access article distributed\nunder the terms of theCreative\nCommons Attribution License (CC BY).\nThe use, distribution or reproduction\nin other forums is permitted, provided\nthe original author(s) and the\ncopyright owner(s) are credited and\nthat the original publication in this\njournal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is\npermitted which does not comply with\nthese terms.\nTYPE Original Research\nPUBLISHED 08 August 2022\nDOI 10.3389/fonc.2022.942511\nmulti-modal MR images of the same anatomical structure (1, 2).\nSome common modalities are T1-weighted (T1), T2-weighted\n(T2), T1 with contrast enhancement (T1c), and T2 ﬂuid-\nattenuated inversion recovery (FLAIR) (3). Each modality has\nits own speci ﬁc pathological feature s. The complementary\ninformation about tissue morph ology allows physicians to\ndiagnose with greater accuracy and con ﬁdence. However,\nmany factors, such as limited scanning time and the expensive\ncost, hinder multi-modal MR imaging. Therefore, there has been\ngrowing interest in retrospec tively synthesizing missing or\ncorrupted modalities from other successfully acquired ones.\nBypassing the cost of additional scanning, this kind of medical\nimage-to-image translation method not only facilitates the\nreliability of clinical diagnosis but also promotes follow-up\nimage analysis tasks such as registration ( 4, 5)a n d\nsegmentation (6, 7).\nRecently, various deep learning methods have been exploited\nto solve the problem of medical image-to-image translation in an\nend-to-end manner. Previous studies (8) have demonstrated that\ngenerative adversarial network (GAN) has signiﬁcant potential\nin solving image-to-image translation problems. GAN is a\nframework that simultaneously trains a generator G and a\ndiscriminator D by an adversarial process. During the training\nprocess, the generator is used to translate the distribution of\nsource modality MRIs to the distribution of target modality\nMRIs. The discriminator is used to identify whether target\nmodality MRIs are likely from the generator or the real data.\nThese GAN-based approaches can be broadly divided into two\ncategories. One refers to the supervised Pix2Pix (8–12) GAN\napproach, which utilizes paired images from the source and\ntarget modalities. However, it relies on paired and pixel-aligned\nimages, which may not always be possible due to respiratory\nmovements or anatomical changes between the times when\nmulti-modality images are scanned. For instance, Isola et al.\nproposed (13) a conditional adversarial network for image-to-\nimage translation tasks. A three-dimensional (3D) auto-context-\nbased locality adaptive multi-modality GAN model (LA-GANs)\n(9) is developed to synthesize the high-quality FDG PET image\nfrom the low-dose one with the help of MRIs. Zhan et al. (10)\nutilized a conditional GAN for multimodal MRI synthesis by\nmodeling the non-linear mapping between input and output.\nThe other category involves unsupervised cycle-consistency\nGAN ( 14–16), which can be used for misaligned images\nthrough a cycle-consistency loss. However, it is known that the\ncycle-consistency framework may have multiple solutions (17,\n18), indicating that the results may not be accurate and sensitive\nto perturbation. To solve the mentioned problems, Kong et al.\n(19) proposed RegGAN, which incorporates a registration\nnetwork and regards the misaligned target images as noisy labels.\nHowever, the convolution kernel usually has a limited\nreceptive ﬁeld and thus cannot capture long-range\ndependencies, which are essential for MR image-to-image\ntranslation. Nowadays, vision transformer (20\n) is capable of\nmodeling global interactions between contexts and has\npromising performance in MRI restoration ( 21 , 22 ),\nsegmentation (23, 24), and registration (25, 26). Nevertheless,\nvision transformers for image restoration need to divide the\ninput image into small patches of ﬁxed size, which may\nintroduce border artifacts around each small patch in the\nrestored images. To solve this problem, Swin Transformer (27)\nhas been proposed to solve many vision problems (28, 29) since\nit integrates the advantages of both the convolutional neural\nnetwork (CNN) and the self-attention mechanism (30) with\nshifted windows.\nIn this paper, we propose a Swin Transformer-based GAN\nfor Multi-Modal Medical Image Translation, called MMTrans.\nMore speciﬁcally, our framework consists of three modules: a\nGenerator, a Registration Network, and a Discriminator. The\nGenerator is based on the framework of SwinIR (30), which is\nutilized to generate images with the same content as source\nmodality images and the similar style information of target\nmodality images. The registration network is a Swin\nTransformer model, which is trained to predict the deformable\nvector ﬁeld (DVF). For paired images, we assume that there\nexists a tiny mismatch between the source domain images and\nthe target domain images. Therefore, the mismatch can be\ncorrected by the registration network. For unpaired images, as\nshown in Figure 1, the G( x ) generates images with the same\nmorphology as T1 and the same style as T2, whileR( G( x ),y )\nrepresents the image with the same style and the same\nmorphology of T2. The discriminator, a CNN model,\ndetermines whether the target modality images are similar to\nthe generator or from the real images. Extensive experiments on\npaired and unpaired public and clinical data demonstrated that\nthe proposed MMTrans outperforms state-of-the-art\napproaches and has great potential to be applied in\nclinical practice.\nThis paper’s sections were arranged as follows: in Section 2,\nwe elaborate on the proposed MMTrans framework, including\nSwin Transformer Generator, Swin Transformer Registration,\nSwin Transformer Layer, and the loss function. In Section 3, we\ngive the details of the experiment. Then we present and discuss\nthe experimental results in Section 4 andﬁnally summarize the\nconclusions in Section 5.\n2 Methods\nThe task of this study is to synthesize translated modalities\nfrom given modalities in MR images. In order to obtain better\nperformance, we propose a Swin Transformer-based GAN for\nmulti-modal MRI translation.Figure 1 shows the ﬂowchart of\nthe whole framework. In this section, we will introduce in detail\nthe Swin Transformer Generator, Swin Transformer\nRegistration, Swin Transformer Layer, and loss functions.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org02\n2.1 Swin Transformer layer\nThe most signiﬁcant improvement and development of the\nSwin Transformer to the transformer are replacing the previous\nstandard multiple self-attention (MSA) module with a shift\nwindow-based module, with no substantial changes to the\nother layers. Each Swin Transformer block consisted of layer\nnorm, multi-headed self-focused modules, residual connections,\nand a two-level MLP with GELU non-linearity. Similar to\nprevious reports (31, 32), self-concern was calculated as follows:\nAttention Q, K, VðÞ = SoftMax Q K T\nﬃﬃﬃ\nd\np + B\n/C18/C19\nV (1)\nWe represented the query, key, and value asQ, K, and V∈\nRm 2×d\nrespectively; m2 represents the number of patches in the\nwindow, whileB depicts the dimension of the query or key. The\nvalues in B were selected from ^B ∈ R(2m−1)/C2 (2m+1),t h e\nbias matrix.\n2.2 Swin Transformer generator\nCurrently, the models that can only be applied to speciﬁc\nscenes or from minimal modeling capabilities generally perform\nwell in image translation. Most direct training CNNsﬁrst encode\nthe image as a high-level feature representation and then decode\nit to full spatial resolution. Thus, these models are challenging to\napply in medical imaging. Image-to-image translation is\nultimately about inputting a high-dimensional input tensor\nand then corresponding this tensor to an output tensor with a\ndifferent appearance but identical in basic structure. In the\nimage-to-image conversion, shallow and deep characteristics\nare extracted from the input and the actual images to achieve\nhigh-quality image translation. To achieve this goal, a Swin\nTransformer-based generati ve network for target image\ngeneration was constructed. By using the transformer to\nintroduce a self-attention mechanism into the encoder design,\ndeep hierarchical representations were extracted with rich\nremote dependencies from both the target and reference\nimages, which performed the translation task more efﬁciently\nand accurately. As shown in Figure 1 ,S w i nT r a n s f o r m e r\nGenerator (STG) consisted of multiple residual Swin\nTransformer blocks (RSTBs) , each using various Swin\nTransformer Layers (STLs) for local attention and cross-\nwindow interaction learning, and the RSTB used residual\nlearning to ensure the stability of feature extraction and 3 × 3\nconvolutional layers betw een RSTBs and STLs for feature\nenhancement. The feature extraction process of RSTBs was\nexpressed as follows:\nT\nRSTB = Conv FSTL/C0/C1\n+ TIN (2)\nwhere FSTL denotes the model generated from STLs; Conv means\n3 × 3 Conv2D, andTIN represents the input feature of RSTBs. As\nshown in Figure 1, each STL consisted of multi-headed self-\nattentive blocks and multi-layer perception. In this study, the\nnumber of RSTBs and STLs in STG is set at four and six,\nrespectively. As shown inFigure 1, the STG consisted of multiple\nRSTBs, each using various STLs for local attention and cross-\nFIGURE 1\nThe overall architecture of the proposed MMTrans, including the Swin Transformer Generator (STG) for target image translation and the Swin\nTransformer Registration (STR) for mismatch correction.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org03\nwindow interaction learning, and each RSTB used residual\nlearning to ensure the stability of feature extraction and 3 × 3\nconvolutional layers betwe en RSTB and STL for feature\nenhancement. The generation section was deﬁned as follows:\nGIMAGE = FREC TRSTB/C0/C1\n(3)\nwhere FREC represents the function of the recovery module\nthrough the long-skit connection. We used the STG module to\nfeed the low-frequency information wholly and directly into the\nrecovery module to extract high-frequency data from the depth\nfeatures. In the recovery, the subpixel convolutional layer was\nadopted by us.\n2.3 Swin Transformer registration\nCompared with the traditional image translation tasks, image\ntranslation in the medicalﬁeld is more difﬁcult because of the large\namount of detailed medical information contained in the structure\nof the medical images. This information is inevitably lost during\ntraining. The approach in th e current work required the\nconstruction of a network speciﬁcally for the speciﬁcm e d i c a l\nimage translation task to solve this problem. A primary\nregistration network was added to the image translation work in\nthis study of RegGAN (19). Therefore, it was feasible to use the\nregistration networks to train generators in the medical image\ntranslation process. We referred to a previous study (26)u s i n ga\nU-shaped structure as the structure of the registration network, both\nthrough the encoder-decoder paradigm, to achieve a smooth and\ngradual transition from the image to the registration. Unlike the\nprevious study (33) and its variants, the encoder part of our Swin\nTransformer Registration (STR) architecture better learned the\ndisplay’s global and remote semantic information interaction. In\nour STR network, a Swin Transformer Layer was added to the\nencoder part to perform the feature extraction process, which\nimproved the performance of our network by obtaining better\nglobal information. We also used alternative up-sampling, general\nconvolution, and jump junction,which allowed the image features\nextracted in the encoder part of the network to be passed directly to\nthe decoder section. We adopted a standard convolutional layer\nwith an available kernel size of 3 × 3 and a stride size of 2 × 2 for this\nwork. We added a LeakyRelu layer with the parameters equal to 0.2\nbehind the standard convolutional layer. As shown in the STR\nsection ofFigure 1, each rectangle represents a two-dimensional\nimage to better train the registration architecture SWR for target\nimage generation. The numbers of rectangles represented how\nmany ﬁlter convolutions were used in the process. The down-\nsampling operation was represented with bright red arrows, and the\nup-sampling procedure was represented with green arrows; the gray\nconnecting line represented the jump connection between the\nencoder and the decoder. Finally, the full-resolution image was\nfurther reﬁned after two layers of standard convolution. Our results\nshowed that this registration network could perform well in the task\nof image translation.\n2.4. Loss functions\nFirst, the underlying network framework involves GANs (34),\nwhere the generatorG and the discriminatorD are continuously\ntrained to play against each other during the training process and\nare eventually introduced to the desired ideal state. In this process,\nwe trained the generator to produce the medical target imageG( x )\nideally from the inputx image. Quite differently, this was the\ndiscriminator in our network, which was continuously trained to\nseparate from the ground truth imagey or the perfect target medical\nimage G( x ) developed by the generator. The adversarial loss\nf u n c t i o nw a sa sf o l l o w s :\nmin\nG\nmax\nD\nLAdv G, DðÞ\n= ϵy log D yðÞðÞ½/C138 + ϵx log 1/C0D G xðÞðÞðÞ½/C138 (4)\nAfter experiencing the target medical imageG( x ) produced\nafter generating the advers arial network, we added the\nregistration network R as a label noise model to correct the\ngenerated target image G( x ) to achieve better translation.\nThe correction loss is shown in Equation 5:\nmin\nG, R\nLCorr G, RðÞ = ϵx,y ∥ y − G xðÞ ∘ R G xðÞ , yðÞ ∥1½/C138 (5)\nIn Equation 5, we used R( G( x ),y )t or e p r e s e n tt h e\ndeformation ﬁeld operation and we used ° to describe the\nresampling operation. Our network’s registration network was\nconstructed based on the U-Net ( 35). In Equation 6, the\nsmoothness of the deformationﬁeld was evaluated by the loss\nfunction, and the gradient of the deformation ﬁeld was\nminimized.\nmin\nR\nLSmooth RðÞ = ϵx,y ∥ ∇R G xðÞ , yðÞ ∥2/C2/C3\n(6)\nFinally, the total loss function of our network is shown in\nEquation 7, which has three components.\nmin\nG, R\nmax\nD\nLTotal G, R, DðÞ = kLAdv + lLCorr + mLsmooth (7)\n3 Experiment\nIn the following paragraph, we introduced the experimental\nsetup, including the used data and practical methods, evaluation\nindicators, and some implementation details.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org04\n3.1 Dataset\nWe employ three different datasets to evaluate our method,\nas shown inTable 1:\n Open access BraTs2018 ( 36)d a t a s e t .T h ed a t a s e t\ncontains multi-contrast images, such as T1 and T2.\nBraTs2018 was selected because the original images\nwere paired and well aligned.\n Public fastMRI (37) dataset with paired multi-contrast\nknee DICOM images. We only used the coronal PD and\nPD-FS images.\n The clinical brain MRI dataset was acquired with a 3T\nPhilips Ingenia MRI system (Philips Healthcare, Best,\nthe Netherlands) scanner, including T1-weighted (T1W)\nand FLAIR imaging. The dataset consists of 17 healthy\nsubjects and ﬁve patients. All subjects gave their\ninformed consent for inclusion before they participated\nin the study with approval from the local institutional\nreview board (in accordance with the Declaration of\nHelsinki). The institutional review board has approved\nthe MRI scanning.\nWhen training on paired images, all the MRIs were well\naligned and normalized into the range of [0, 1]. However, when\ntraining on unpaired images, we randomly sample one image\nfrom T1 and the other one from T2.\n3.2 Implementation details\nOur proposal was implemented in PyTorch with an NVIDIA\nTesla V100 GPU (4 × 16 GB). The optimizer used was Adam at a\nlearning rate of 1e−4 to test all the developed methods. Each\ntraining process protected 80 epochs, and the weights of the\ndifferent loss functions werek =1 ,l= 20, andm= 10. The error\nmaps are calculated by calculating the absolute difference between\nthe generated images with the ground truth images. The error\nmaps are calculated by calculating the absolute difference between\nthe generated images with the ground truth images.\n3.3 Comparison methods and\nevaluation metrics\nTwo board-certiﬁed radiologists (with 7 and 10 years of\nexperience) independently reviewed the images synthesized by\nall the comparison methods. The synthesized images were\nanonymized, and the order of the image translation methods\nwas randomized. Three types of image quality measures (overall\nimage quality, image contrast, and structure outline) were scored\nwith 5-point Likert scale, 5-point Likert scale, and 3-point scale.\nThe 5-point Likert scale for overall image quality and image\ncontrast was as follows: 1, unacceptable; 2, poor; 3, acceptable; 4,\ngood; and 5, excellent. The 3-point scale for structure outline was\nas follows: 1, indistinct outline; 2, perceptible outline; and 3,\nTABLE 1 Three datasets and number of images for training validation and test.\nDatasets BraTs2018 fastMRI Clinical brain MRI\nOriginal image T1 PD T1\nTarget image T2 PD-FS T2\nTrain/valid/test 1,000/300/300 300/80/80 500/150/150\nFIGURE 2\nHyperparameter selection results in the objective function.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org05\nsharp outline. One-tailed Wilcoxon signed-rank tests based on\nthe ratings of two radiologists were used to test the difference\nbetween synthesized images of different methods and the ground\ntruth images. The signiﬁcance level was set as 0.01.\n4 Experimental results\n4.1 Hyperparameter selection\nWe obtain the hyperparametersk, l, and m by the greedy\nmethod, as shown inFigure 2. Note that the hyperparameter\ntuning is performed in the BraTs2018 dataset. The coefﬁcient of\nthe adversarial loss wasﬁrst adjusted. Whenk increased from\n0.01 to 1, PSNR and SSIM show a growth tendency. However, it\ncan be seen that both PSNR and SSIM decrease slightly whenk\nboosts from 1.0 to 100. Thus,k is set to 1. Whenk is ﬁxed, m is\nincreased from 0.01 to 100. As can be seen, the PSNR and SSIM\nvalues keep growing untilm reaches 10. Whenm >10, both PSNR\nand SSIM values show a declining trend. Therefore, we setm to\n10. In practice, we found that it is adequate to setl from 1 to 20\nsuch that the magnitude of different loss terms is balanced\ninto similar scales. As shown in Figure 2,w es e tl to 20.\nMoreover, it has been demons trated that adjusting the\nhyperparameter determination order will not affect the ﬁnal\nhyperparameter setting.\n4.2 Qualitative results\nFour tasks are used to evaluate and test the proposed image\ntranslation model. First, on the BraTs2018 paired dataset, the T2\nmodality of the T1 translation was used. On the public fastMRI\ndataset, converting PD modality to PD-FS modality is performed\nas a second task. The third task was to convert T1 mode to T2\nmode on a clinical brain MRI paired dataset. Finally, on the\nBraTs2018 unpaired dataset, the T2 modality of the T1\ntranslation was used. The translation performance of\nMMTrans is ﬁrst evaluated on BraTs2018 paired images;\nFigure 3 shows the comparison of the translation method\nproposed in the paired dataset BraTs2018 with other state-of-\nthe-art. Clearly, our proposed method produces better\ntranslation results, which are valuable in clinical applications.\nFor the second task (implementing PD image to PD-FS image\ntranslation using public fastMRI datasets),Figure 4 shows that\nour model generates target images with higher quality and better\nFIGURE 3\nQualitative results of T1 modality translation to T2 modality using the BraTs2018 paired dataset with different translation methods, displaying\ntranslation images and corresponding error map.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org06\nclarity as compared to other models. Admittedly, in the\nqualitative comparison results shown in Figure 5,t h eb e s t\nperformance of our method is also achieved in the third task\n(conversion from T1 mode to T2 mode images on top of the\nclinical brain MRI dataset). Lastly, the performance was\nevaluated using the BraTs2018 unpaired dataset; the results in\nFIGURE 5\nQualitative results of different translation methods for translating T2 from T1 using the paired clinical brain MRI dataset, showing translation\nimages and corresponding error maps.\nFIGURE 4\nQualitative results of different translation methods from PD to PD-FS using public fastMRI dataset, displaying translation images and\ncorresponding error map.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org07\nFigure 6 show that our proposed MMTrans achieves the best\ntranslation performance. InFigure 7, we show how MMTrans\ncorrects unpaired data. It can be seen that MMTrans will try its\nbest to eliminate the inﬂuence of unpaired through registration.\n4.3 Quantitative results\nThe values of quantitative metrics for the two raters are shown\nin Table 2 and Figure 8. Both raters agreed that our translated\nFIGURE 6\nQualitative results of different translation methods for synthesizing T2 from T1 on unpaired BraTs2018 dataset.\nFIGURE 7\nDisplay of MMTrans output on unpaired data. T1 and T2 are unpaired images.Translated represents the translation result of T1 to T2.Registered\nrepresents the registration result of the translated images.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org08\nimages signiﬁcantly improved overall quality (p < 0.01), image\ncontrast (p < 0.01), and deep brain structure contours (p <0 . 0 1 ) .\nMeanwhile, our synthetic T2 images and real T2 images were not\nsigniﬁcantly different for all measures of image quality (allp > 0.01)..\nThe translation performance of MMTrans isﬁrst evaluated\non BraTs2018 paired images. Table 3 shows the results of\nquantitative evaluations on the four tasks. The ﬁrst is\nevaluated on BraTs2018 paired images;Table 3 shows that our\nmodel dominates the PSNR, NAME, and SSIM metrics,\nindicating that our model achieves better target image\ntranslation. Based on Table 3, we can ﬁnd that our model\nmethod outperforms other image translation methods on the\nfastMRI public dataset, especially in image contrast restoration.\nAdmittedly, quantitative results on the third task (translation\nfrom T1 modality to T2 modality images on a clinical brain MRI\npaired dataset) suggest that our approach is the best solution.\nTABLE 2 Compare the mean scores of translated images given by P2P images, CycleGAN images, RegGAN images, and MMTrans images.\nRatings (mean ± standard deviation) p-Value\nMMTrans\nP2P CycleGAN RegGAN MMTrans GT\nQuality 4.25 ± 0.53 3.40 ± 1.13 4.75 ± 0.17 4.88 ± 0.07 0.029\nContrast 4.20 ± 0.56 3.75 ± 0.88 4.65 ± 0.24 4.95 ± 0.07 0.015\nOutline 2.50 ± 0.35 1.95 ± 0.74 2.75 ± 0.17 2.95 ± 0.03 0.023\nFIGURE 8\nTwo board-certiﬁed radiologists (7 and 10 years of experience) independently reviewed the results of images synthesized by all contrast\nmethods.\nTABLE 3 Quantitative metrics results (mean and standard deviation) on different datasets in terms of PSNR, MAE, and SSIM.\nDataset BraTs2018 (paired) fastMRI (paired)\nMetrics PSNR MAE (10 -2) SSIM (10 -2) PSNR MAE (10 -2) SSIM (10 -2)\nP2P 23.80 ± 3.81* 8.27 ± 1.90* 81.67 ± 3.80* 35.36 ± 2.37* 4.31 ± 0.90* 76.17 ± 6.40*\nCycleGAN 22.59 ± 3.26* 8.85 ± 1.90* 80.64 ± 3.40* 34.20 ± 2.64* 4.38 ± 1.30* 74.27 ± 8.60*\nRegGAN 24.08 ± 3.38* 8.18 ± 1.90* 82.83 ± 3.60* 37.28 ± 2.05* 4.28 ± 1.10* 80.18 ± 6.40*\nMMTrans 24.83 ± 3.36 8.06 ± 1.80 83.95 ± 3.70 39.37 ± 2.38 4.10 ± 1.20 81.17 ± 7.00\nDataset Clinical brain MRI (paired) BraTs2018 (unpaired)\nP2P 34.67 ± 4.08* 1.62 ± 1.10* 84.86 ± 8.00* 12.57 ± 2.8* 19.36 ± 3.50* 67.90 ± 1.90*\nCycleGAN 34.23 ± 2.74* 1.67 ± 0.10* 83.92 ± 7.30* 13.34 ± 2.11* 18.11 ± 3.40* 70.25 ± 2.50*\nRegGAN 36.13 ± 3.69* 1.34 ± 0.80* 86.37 ± 8.00* 14.02 ± 2.11* 17.82 ± 2.80* 71.17 ± 2.40*\nMMTrans 36.70 ± 3.13 1.28 ± 0.60 87.49 ± 8.00 14.86 ± 2.34 16.26 ± 2.80 73.09 ± 2.90\nThe best quantitative metrics results are marked in bold.\n*Signiﬁcant difference between different comparison methods and our proposed MMTrans.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org09\nFinally, the BraTs2018 unaligned dataset is used to evaluate the\nperformance of using T1 image transfer to T2 modality; the\nquantitative evaluation results of this task are shown inTable 3.\nComparing the three evaluation metrics inTable 3, MMTrans\nperforms better.\n4.4 Ablation study\nTo analyze the impact of STL modules in our proposed\narchitecture, we perform an ablation study for four different\nscenarios: 1) baseline GAN: both G and R consist of the\nconvolutional layer. 2) SwinG: we disable the registration module\nand only add the STL module to the G network. 3) SwinG+R: the\nSTL module is added to generator G, while the registration R is\nwithout the STL module. 4) MMTrans (ours): both G and R adopt\nSTL modules. The qualitative and quantitative results are shown in\nFigures 9, 10, respectively. First, adding the registration network\n(+R) obviously improves the performance of the method. Second,\nthe residual Swin Transformer group enables better modeling of\nlong-range dependency of MRIs since MRIs often have repeating\nvisual patterns and similar structures. As shown inFigure 11,\nadding the registration R network makes the translation more\naccurate, and the STL modules are added to generator G\nand registration R, which can learn more features of MRIs.\nTherefore, the proposed MMTrans can be regarded as a better\ntranslation scheme.\n5 Discussion\nIn contrast to previous image-to-image translation methods,\nthe Pix2Pix must be trained with enough paired data to generate\na clear image of the pathological target. However, unavoidable\nphysical factors during the MR image acquisition, including\nrespiratory motion or anatomical variations between the\nacquired pairs of photos, make it very dif ﬁcult to achieve\nprecisely matched MR data from the same individual. Even\nwith the excellent performance of the Pix2Pix mode, it must\nrequire a large number of pixels to align medical images, which\nis a very time-consuming task for MRI. Speci ﬁct oc y c l i c\nconsistency, it is impossible to meet the medical image\nrequirements at the accuracy level. Applying image translation\nto medical imaging requires a change in style between two\nimages and, more importantly, the ability to achieve higher-\nresolution conversions between speciﬁc pairs of medical images.\nThe result should be unique, and the translated image must\nmaximize the anatomical features of the original image. Our\nmodel performed best with paired and unpaired data combining\nimage translation with the Swin Transformer.\nThis work proposed a framework that can translate the medical\nimage patterns accurately. In medical image translation,\nconvolutional operations have a ﬁxed localization, making it\ndifﬁcult for CNN-based approaches to learn the display’s global\nand remote semantic information interaction. In other words,\nbecause the convolutional kernel can be considered as a small\nFIGURE 9\nQualitative results in ablation studies under different scenarios on the BraTs2018 dataset.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org10\npatch in which the acquired features are of local information, the\nglobal data are lost when remote dependency modeling training is\nperformed at its location, which also leads to the inability to obtain\nthe anatomical details contained inside the image during the\ntranslation process. With the help of the Swin Transformer, the\ngenerator part of our network was built based on the work, where\nthe input medical image is segmented into non-overlapping image\npatches; each patch can be referred to as a token, and these patches\nare then fed into an encoder created based on the transformer to\nlearn the deep feature representati o ni nt h ei m a g e .T h ec o n t e x t u a l\nfeatures known by the transformer are then obtained using a\ndecoder with patch extensions and fused with multiscale elements\nfrom the encodervia a jump connection to recover the spatial\nresolution of the feature images to further complete the translation\nof the target images. In our network, we also considered the global\ninformation of the picture to improve the performance of medical\nimage translation. For images generated by the generator, we added\ndeformable registration to our architecture to better train the\ngenerator in our network so that our model could yield better\nresults in the image translation.\nThis study also has limitations, and further modiﬁcations to\nMMTrans are required for the practical implementation of medical\nFIGURE 11\nThe learned attention map. Visualization of attention maps in SwinG, SwinG+R, and MMTrans. Blue and red values represent low and high\nresponse values, respectively.\nFIGURE 10\nAblation study results with different scenarios on the BraTs2018 dataset. *Signiﬁcant difference between different comparison methods and our\nproposed MMTrans (p < 0.01).\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org11\nimaging. Although computing with two-dimensional (2D) slices is\nsigniﬁcantly more ef ﬁcient than the 3D counterparts, the\ninformation retained in the 3D data is indispensable for most\nmedical imaging tasks. Therefore, future studies should further\nadapt MMTrans to 3D medical volumes.\nConclusion\nWe present a novel Swin Transformer-based GAN for\nMulti-Modal Medical Image Translation, named MMTrans.\nFirst, the Swin Transformer-based generator with long-range\ndependency modeling ability is utilized for target image\ngeneration. Furthermore, a U- shaped registration network\nwith Swin Transformer-based encoder is incorporated for\nbetter predicting deformable vectorﬁelds. Experimental results\nshow that our MMTrans is superior to the existing MRI image-\nto-image translation methods and has great potential to be used\nin clinical practice.\nData availability statement\nPublicly available datasets were analyzed in this study. This\ndata can be found here: http://www.med.upenn.edu/sbia/\nbrats2018/data.html.\nEthics statement\nAll subjects gave their informed consent for inclusion before\nthey participated in the study with approval from the local\ninstitutional review board. The institutional review board (at\nShanghai Ruijin Hospital) has approved the MRI scanning.\nWritten informed consent was obtained from the individual(s)\nfor the publication of any potentially identiﬁable images or data\nincluded in this article human studies are presented in\nthis manuscript.\nAuthor contributions\nConceptualization: CW.Methodology: SY,JL,and CW.Software:\nSY. Validation: JL. Formal analysis:SY. Investigation: SY. Resources:\nC W .D a t ac u r a t i o n :S Y .W r i t i n g— original draft preparation: SY.\nWriting— r e v i e wa n de d i t i n g :S Y ,J L ,a n dC W .V i s u a l i z a t i o n :S Y .\nSupervision: CW. Project administration: JL. Funding acquisition: JL\nand CW. Grammar proofreading: WC. All authors contributed to the\narticle and approved the submitted version.\nFunding\nThis research was supported by the National Natural Science\nFoundation of China (Grant Numbers: 61902338, 62001120),\nand the Shanghai Sailing Program (Grant/Award\nNumber: 20YF1402400).\nConﬂict of interest\nAuthor WC was employed by company Philips Healthcare.\nThe remaining authors declare that the research was\nconducted in the absence of any commercial or ﬁnancial\nrelationships that could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nReferences\n1. Zhan B, Li D, Wu X, Zhou J, Wang Y. Multi-modal mri image synthesisvia\ngan with multi-scale gate mergence.IEEE J Biomed Health Inf(2021) 26:17–26. doi:\n10.1109/JBHI.2021.3088866\n2. Moraal B, Roosendaal SD, Pouwels PJ, Vrenken H, Van Schijndel RA, Meier\nDS, et al. Multi-contrast, isotropic, single-slab 3d mr imaging in multiple sclerosis.\nNeuroradiol J(2009) 22:33–42. doi:10.1177/19714009090220S108\n3. Jackson EF, Ginsberg LE, Schomer DF, Leeds NE. A review of mri pulse\nsequences and techniques in neuroimaging.Surg Neurol (1997) 47:185–99. doi:\n10.1016/S0090-3019(96)00375-8\n4. Xu Z, Luo J, Yan J, Pulya R, Li X, Wells W, et al. Adversarial uni-and multi-modal\nstream networks for multimodal image registration.Int Conf Med Imag Comput Comp-\nAssist Intervent(2020) 12263:222–32. doi:10.1007/978-3-030-59716-0_22\n5. Lian C, Li X, Kong L, Wang J, Zhang W, Huang X, et al. Cocyclereg:\nCollaborative cycle-consistency me thod for multi-modal medical image\nregistration.Neurocomputing(2022) 500:799–808. doi:10.1016/j.neucom.2022.05.113\n6. Wang W, Yu X, Fang B, Zhao DY, Chen Y, Wei W, et al. Cross-modality lge-\ncmr segmentation using image-to-image translation based data augmentation.\nIEEE/ACM Trans Comput Biol Bioinf(2022). doi:10.1109/TCBB.2022.3140306\n7. Platscher M, Zopes J, Federau C. Image translation for medical image\ngeneration: Ischemic stroke lesion segmentation. Biomed Signal Process Contr\n(2022) 72:103283. doi:10.1016/j.bspc.2021.103283\n8. Dar SU, Yurt M, Karacan L, Erdem A, Erdem E, Cukur T. Image synthesis in\nmulti-contrast mri with conditional generative adversarial networks.IEEE Trans\nMed Imaging(2019) 38:2375–88. doi:10.1109/TMI.2019.2901750\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org12\n9. Wang Y, Zhou L, Yu B, Wang L, Zu C, Lalush DS, et al. 3d auto-context-\nbased locality adaptive multi-modality gans for pet synthesis.IEEE Trans Med\nImaging (2018) 38:1328–39. doi:10.1109/TMI.2018.2884053\n10. Zhan B, Li D, Wang Y, Ma Z, Wu X, Zhou J, et al. Lr-cgan: Latent representation\nbased conditional generative adversarial network for multi-modality mri synthesis.\nBiomed Signal Process Contr(2021) 66:102457. doi:10.1016/j.bspc.2021.102457\n11. Zhou T, Fu H, Chen G, Shen J, Shao L. Hi-Net: hybrid-fusion network for\nmulti-modal mr image synthesis.IEEE Trans Med Imaging(2020) 39:2772–81. doi:\n10.1109/TMI.2020.2975344\n12. Fei Y, Zhan B, Hong M, Wu X, Zhou J, Wang Y. Deep learning-based multi-\nmodal computing with feature disentanglement for mri image synthesis.Med Phys\n(2021) 48:3778–89. doi:10.1002/mp.14929\n13. Isola P, Zhu JY, Zhou T, Efros AA. Image-to-image translation with\nconditional adversarial networks. Proc IEEE Conf Comput Vision Pattern Recog\n(2017) 2017:1125–34. doi:10.1109/CVPR.2017.632\n14. Zhu JY, Park T, Isola P, Efros AA. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks.Proc IEEE Int Conf Comput Vision\n(2017), 2223–32. doi:10.1109/ICCV.2017.244\n15. Lee D, Kim J, Moon WJ, Ye JC. Collagan: Collaborative gan for missing image\ndata imputation. In:Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. Piscataway: IEEE Computer Society (2019). pp. 2487–96.\n16. Li H, Paetzold JC, Sekuboyina A, Koﬂer F, Zhang J, Kirschke JS, et al.\nDiamondgan: uni ﬁed multi-modal generative adversarial networks for mri\nsequences synthesis, in: International Conference on Medical Image Computing\nand Computer-Assisted Intervention. Lecture Notes in Computer Science. Cham,\nSwitzerland: Springer (2019) pp. 795–803.\n17. Sim B, Oh G, Lim S, Ye JC.Optimal transport, cyclegan, and penalized ls for\nunsupervised learning in inverse problems. Ethiopia: ICLR Press (2019).\n18. Moriakov N, Adler J, Teuwen J. Kernel of cyclegan as a principle\nhomogeneous space. New York: MIT Press (2020).\n19. Kong L, Lian C, Huang D, Li Z, Hu Y, Zhou Q, et al. Breaking the dilemma of\nmedical image-to-image translation.Adv Neural Inf Process Syst(2021) 34:1964–78.\n20. Arnab A, Dehghani M, Heigold G, Sun C, Luč ić M, Schmid C. Vivit: A video\nvision transformer, in:Proceedings of the IEEE/CVF International Conference on\nComputer Vision. International Conference on Computer Vision. Piscataway: IEEE\n(2021) pp. 6836–46.\n21. Guo P, Mei Y, Zhou J, Jiang S, Patel VM.Reconformer: Accelerated mri\nreconstruction using recurrent transformer(2022).\n22. Fabian Z, Soltanolkotabi M. Humus-net: Hybrid unrolled multi-scale\nnetwork architecture for accelerated mri reconstruction(2022).\n23. Hatamizadeh A, Tang Y, Nath V, Yang D, Myronenko A, Landman B, et al.\nUnetr: Transformers for 3d medical image segmentation. In:Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision. . Piscataway:\nIEEE Computer Society (2022). pp. 574–\n84.\n24. Yan X, Tang H, Sun S, Ma H, Kong D, Xie X. After-unet: Axial fusion\ntransformer unet for medical image segmentation. In:Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision .P i s c a t a w a y :I E E E\nComputer Society (2022). pp. 3971–81.\n25. Mok TC, Chung A. Afﬁne medical image registration with coarse-to-ﬁne\nvision transformer. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. Piscataway: IEEE Computer Society (2022). pp.\n20835–44.\n26. Wang Y, Qian W, Zhang X.A transformer-based network for deformable\nmedical image registration. New York: MIT Press (2022).\n27. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. (2021). Swin transformer:\nHierarchical vision transformer using shifted windows, in:Proceedings of the IEEE/\nCVF International Conference on Computer Vision, . pp. 10012–22.\n28. Zhang P, Dai X, Yang J, Xiao B, Yuan L, Zhang L, et al. Multi-scale vision\nlongformer: A new vision transformer for high-resolution image encoding. In:\nProceedings of the IEEE/CVF International Conference on Computer Vision .\nPiscataway: IEEE Computer Society (2021). pp. 2998–3008.\n29. Liu Z, Ning J, Cao Y, Wei Y, Zhang Z, Lin S, et al. Video swin transformer.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. Piscataway: IEEE Computer Society (2022). pp. 3202–11.\n3 0 . L i a n gJ ,C a oJ ,S u nG ,Z h a n gK ,V a nG o o lL ,T i m o f t eR .S w i n i r :I m a g er e s t o r a t i o n\nusing swin transformer. In:Proceedings of the IEEE/CVF International Conference on\nComputer Vision. Piscataway: IEEE Computer Society (2021). pp. 1833–44.\n31. Zhang H, Dana K, Shi J, Zhang Z, Wang X, Tyagi A, et al. Context encoding\nfor semantic segmentation. In:Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition. Piscataway: IEEE Computer Society (2018). pp.\n7151–60.\n32. Lugmayr A, Danelljan M, Timofte R, Fritsche M, Gu S, Purohit K, et al. Aim\n2019 challenge on real-world image super-resolution: Methods and results. In:\nIEEE/CVF international conference on computer vision workshop (ICCVW) .\nPiscataway: IEEE (2019). p. 3575–83.\n33. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional networks for\nbiomedical image segmentation. Int Conf Med Imag Comput Comput-Assist\nIntervent (2015) 9351:234–41. doi:10.1007/978-3-319-24574-4_28\n34. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S,\net al. Generative adversarial nets.Adv Neural Inf Process Syst(2014) 27.\n35. Emami H, Aliabadi MM, Dong M, Chinnam RB. Spa-gan: Spatial attention\ngan for image-to-image translation.IEEE Trans Multimed(2020) 23:391–401. doi:\n10.1109/TMM.2020.2975961\n36. Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J, et al.\nThe multimodal brain tumor image segmentation benchmark (brats).IEEE Trans\nMed Imaging(2014) 34:1993–2024. doi:10.1109/TMI.2014.2377694\n37. Zbontar J, Knoll F, Sriram A, Murrell T, Huang Z, Muckley MJ, et al. An\nopen dataset and benchmarks for accelerated mri.Fastmri (2018) 65.\nYan et al. 10.3389/fonc.2022.942511\nFrontiers inOncology frontiersin.org13"
}