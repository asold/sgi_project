{
  "title": "Shortformer: Better Language Modeling using Shorter Inputs",
  "url": "https://openalex.org/W3118895645",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2765070949",
      "name": "Ofir Press",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1978197916",
      "name": "Mike Lewis",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2995575179"
  ],
  "abstract": "Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5493–5505\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5493\nShortformer: Better Language Modeling Using Shorter Inputs\nOﬁr Press1,2 Noah A. Smith1,3 Mike Lewis2\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Facebook AI Research\n3Allen Institute for AI\nofirp@cs.washington.edu\nAbstract\nIncreasing the input length has been a driver\nof progress in language modeling with trans-\nformers. We identify conditions where shorter\ninputs are not harmful, and achieve perplex-\nity and efﬁciency improvements through two\nnew methods that decrease input length. First,\nwe show that initially training a model on\nshort subsequences before moving on to longer\nones both reduces overall training time and,\nsurprisingly, substantially improves perplex-\nity. Second, we show how to improve the ef-\nﬁciency of recurrence methods in transform-\ners, which let models condition on previously\nprocessed tokens when generating sequences\nthat exceed the maximal length the transformer\ncan handle at once. Existing methods re-\nquire computationally expensive relative posi-\ntion embeddings; we introduce a simple alter-\nnative of adding absolute position embeddings\nto queries and keys instead of to word embed-\ndings, which efﬁciently produces superior re-\nsults. We show that these recurrent models\nalso beneﬁt from short input lengths. Com-\nbining these techniques speeds up training by\na factor of 1.65, reduces memory usage, and\nsubstantially improves perplexity on WikiText-\n103, without adding any parameters.1\n1 Introduction\nScaling up transformer (Vaswani et al., 2017) lan-\nguage models (Radford et al., 2019; Lewis et al.,\n2019; Raffel et al., 2019; Brown et al., 2020) has\nbeen an important driver of progress in NLP. Lan-\nguage models require data to be segmented into\nsubsequences for both training and inference: mem-\nory constraints limit a language model to handling\nat most a few thousand tokens at once, while many\ntraining and evaluation datasets are much longer.\n1Our code is available at https://github.com/\nofirpress/shortformer\nRecent work focuses on increasing the length of in-\nput subsequences, which determines the maximum\nnumber of tokens a model can attend to (Baevski\nand Auli, 2018; Sukhbaatar et al., 2019; Kitaev\net al., 2020; Roy et al., 2020).\nWe challenge the assumption that longer input\nsubsequences are always better by showing that\nexisting transformers do not always effectively use\nthem. We then introduce new methods based on\nshorter input subsequences that improve runtime,\nmemory efﬁciency, and perplexity.\nWe ﬁrst investigate how input subsequence\nlength affects transformer language models ( §3).\nNa¨ıve evaluation—where we split a large evalua-\ntion set into multiple nonoverlapping subsequences,\neach evaluated independently—initially supports\nthe commonly-held belief that models that train\nand do inference on longer subsequences achieve\nbetter perplexity (Table 1, col. 3).\nHowever, when we evaluate each model with\na sliding window (Baevski and Auli, 2018), out-\nputting one token at a time using the maximal\namount of context, we ﬁnd—surprisingly—that\nmodels using subsequences exceeding 1,024 to-\nkens do not further improve performance (Table 1,\ncol. 5).\nWe conclude that the performance gains (using\nna¨ıve evaluation) of models that use longer sub-\nsequences occur not only because of their better\nmodeling ability, but partly because they divide the\nevaluation set into longer subsequences. This di-\nvision helps because of an issue we call the early\ntoken curse: by default, early tokens in a subse-\nquence will have short histories to attend to. Using\nlonger subsequences means fewer tokens will suf-\nfer from the early token curse. For example, when\nusing inputs of length 1,024, about 94% of tokens\nget to attend to more than 64 preceding tokens. If\nwe use inputs of length 128, only 50% of tokens\nget to attend to 64 or more preceding tokens.\n5494\nBased on this analysis, we explore how to im-\nprove models by using shorter inputs. We introduce\ntwo techniques.\nStaged Training (§4) First, we show that ini-\ntially training on shorter subsequences (before mov-\ning to longer ones) leads not only to much faster\nand more memory-efﬁcient training, but it surpris-\ningly also greatly improves perplexity, suggesting\nthat longer inputs are harmful early in training.\nPosition-Infused Attention (§5) Second, we\nconsider a natural way to avoid the early token\ncurse during training and inference: attending to\ncached representations from the previously evalu-\nated subsequence (Dai et al., 2019). This approach\ninterferes with conventional absolute position em-\nbeddings in a way that forced Dai et al. to use rela-\ntive position embeddings, which are computation-\nally expensive. We introduce a fast, simple alter-\nnative: instead of adding absolute position embed-\ndings to word embeddings—thereby entangling a\nword’s content and positional information—we add\nthem to the keys and queries in the self-attention\nmechanism (but not to the values). This does not\nincrease parameter count or runtime. Token repre-\nsentations can then be cached and reused in subse-\nquent computations. We show that when using this\nmethod, shorter subsequence models outperform\nlonger ones.\nFinally, we show additive gains from combin-\ning staged training and position-infused attention\n(Shortformer, §6), resulting in a model that trains\nmuch quicker and achieves better perplexity on\nWikiText-103. We also show that these results\ntransfer to language modeling on the Toronto Book\nCorpus (§A.5, appendix).\n2 Background and Experimental Setup\nTransformer language models map a list of tokens\nxn−L:n−1 to a probability distribution over the next\ntoken xn. We refer to the list of tokens as the cur-\nrent input subsequence(whose length is L). Causal\nmasking lets us make L predictions at once, with\nthe prediction for token i + 1conditioned on the\nith token and all previous inputs xn−L:i−1, but not\non future inputs. We deﬁne the number of tokens\nthe model can attend to at each timestep as its ef-\nfective context window. Note that L is not to be\nconfused with the (typically much greater) length\nof a training or evaluation dataset.\nDuring inference, language models can be used\nfor two distinct tasks: generation and evaluation. In\norder to deﬁne these tasks, we ﬁrst deﬁne nonover-\nlapping and sliding window inference.\nNonoverlapping Inference To evaluate a string\nlonger than L, we can evaluate each subsequence\nof L tokens independently. This fast approach is\ncommonly used during training; if used, tokens in\none subsequence cannot condition on those in the\nprevious subsequence, giving rise to the early token\ncurse discussed in §1. See Figure 1(a).\nSliding Window Inference An alternative to the\nabove is to use a sliding window during inference.\nHere, we choose a stride S between 1 and L −1\nand advance the window by S tokens after each\nforward pass.2 This means that L −S tokens from\nthe previous block are re-encoded, and only S new\ntokens are outputted. The advantage is that all\noutputs in each subsequence after the ﬁrst have\nat least L −S previous tokens to condition on.\nHowever, since tokens must be re-encoded multiple\ntimes, this approach is much slower. When S = 1,\nwe output one token every inference pass, each\nusing the maximal context window, but this is the\nslowest approach. See Figure 1(b).\nMinimal and Maximal Effective Context Win-\ndow Sizes In the nonoverlapping approach, the\nmin. and max. effective context window sizes are\n1 and L, respectively. In the sliding window ap-\nproach, the max. context window size is still L, but\nthe min. context window size is now L −S + 1.\nEvaluation vs. Generation In evaluation, a\nmodel assigns a perplexity score to a given se-\nquence. Evaluation is done using either nonover-\nlapping inference or with a sliding window of any\nstride; since we already have the target sequence\nwe can simultaneously make predictions for multi-\nple timesteps using causal masking. In generation,\na model generates a new sequence, as in demonstra-\ntions of GPT-3 (Brown et al., 2020). Generation is\ndone only with a sliding window with strideS = 1,\nwhich we refer to as token-by-token generation.\nDuring generation, we append to the input a single\nnew token, get a prediction from the model about\nthe next token (e.g., using beam search or picking\nthe token with the highest probability); the process\nis then repeated.3\n2Nonoverlapping inference can be viewed as sliding win-\ndow inference with stride L.\n3In this paper we do not consider open-ended generation;\nwe generate the dev. set, and for next-token prediction we\n5495\n(a)\na1b2c3 d1e2f3\n(b)\na1b2c3 b1c2d3 c1d2e3 d1e2f3\n(c)\na1b2c3 d4e5f6\nFigure 1: Language model modes for generating or evaluating 6 tokens ( a, b, . . . , f) when subsequence length\nL = 3. The numbers denote the position embeddings (P.E.). (a) Nonoverlapping (§2). (b) Sliding window, stride\nS = 1. Here, after the ﬁrst inference pass we ignore all outputs other than the last (§2). (c) Caching (§5.2) where\neach subsequence attends to representations of the previous one. (In the next iteration, tokens d, e and f become\nthe cache, with P.E. 1, 2 and 3, the three new tokens get P.E. 4, 5, and 6.)\nExperimental Setup Our baseline is the Baevski\nand Auli (2018) model, henceforth B&A, trained\nand evaluated on WikiText-103 (Merity et al.,\n2016). We use this baseline because of its promi-\nnent role in recent language modeling develop-\nments (Khandelwal et al., 2020; Press et al., 2020).\nThe training set contains 103.2 million tokens from\nEnglish Wikipedia. The B&A model has 16 trans-\nformer layers of dimension 1,024, with 8 heads\nin each self-attention sublayer, and feedforward\nsublayers with an inner dimension of 4,096. This\nmodel ties the word embedding and softmax matri-\nces (Press and Wolf, 2017; Inan et al., 2017) and\nuses sinusoidal position embeddings. It has a sub-\nsequence length of 3,072 tokens and achieves a\nperplexity of 18.65 ±0.24 (std. dev.) on the devel-\nopment set. In our experiments, other than varying\nthe subsequence length, we modify no other hyper-\nparameters, including the random seed and number\nof training epochs (205).\n3 How Does Context Window Size Affect\nTransformers?\nSegmenting a corpus into subsequences results in\ndifferent effective context windows for different\ntimesteps depending on where they fall in a seg-\nment. Subsequence length L is an upper bound\non the effective context window at each timestep.\nWhen making the ﬁrst prediction, the model attends\nonly to the ﬁrst input token. When making the sec-\nond prediction, the model attends to the ﬁrst two\ninputs, and so on, up to the Lth timestep where the\nmodel can attend to all input tokens when making\nthe Lth prediction.\n3.1 Context Window Size Matters\nTable 1 explores the effect of subsequence length\nin the B&A model on training runtime and on dev.\nset perplexity and runtime. 4 We ﬁx the number\nuse the ground truth token. This has the same complexity as\nsampling the token with the highest probability.\n4For consistency, throughout the paper we run inference\nwith a batch size of one. This causes models shorter than\nTrain Inference\nNonoverlapping Sliding Window\nSubseq.\nLength\n(Token-by-token)\nSpeed ↑ PPL ↓ Speed ↑ PPL ↓ Speed ↑\n32 28.3k 35.37 2.4k 24.98 74\n64 28.5k 28.03 4.8k 21.47 69\n128 28.9k 23.81 9.2k 19.76 70\n256 28.1k 21.45 14.8k 18.86 63\n512 26.1k 20.10 18.1k 18.41 37\n1024 22.9k 19.11 18.3k 17.97 18\n1536 18.4k 19.05 17.1k 18.14 11\n3072 13.9k 18.65 14.7k 17.92 5\nTable 1: Subsequence length’s effects on performance\nof the B&A model on the WikiText-103 dev. set. The\nbaseline is the last row. Token-by-token inf. was com-\nputed with a sliding window stride S = 1 to output\none token at a time; see §2. We measure speed in\ntok./sec. per GPU and use a batch size of 1 for inf.\nof tokens in each batch to 9,216 but vary the sub-\nsequence length L and batch size (so the product\nof the batch size and subsequence length remains\nat 9,216). We report results for both nonoverlap-\nping inference and sliding window inference with\nstride S = 1, which generates only one new token\nper forward pass; it thus has the maximal effec-\ntive context window for each generated token. We\nﬁnd that performance increases as S decreases un-\ntil it reaches a peak and then stops improving (not\nshown in Table 1).5\nWe derive the following conclusions:\nTraining on long sequences is expensive.\nModels trained on subsequences of length 256 are\ntwice as fast as models trained on subsequences of\n3,072 tokens, but gains for even shorter lengths are\nnegligible (Tab. 1, col. 2).\nLong subsequence lengths can improve re-\nsults. When using the na ¨ıve approach, nonover-\nL = 512to run slowly (in N.o. eval.), although during batched\nN.o. eval. they are slightly faster than the L = 512model.\n5For example, theL = 3,072 model’s performance peaked\nat S = 512 (used in Baevski and Auli (2018)) and then\nstopped improving. Thus, the result shown in Table 1 for\nthat model with S = 1 can also be achieved with S = 512\neven though that runs 500 times faster, at 2.5k tok./sec.\n5496\nlapping evaluation, we see a monotonic decrease in\ndev. perplexity when increasing L (Tab. 1, col. 3).\nIncreasing the minimum effective context\nwindow size is more important than increasing\nthe maximum one. Using a sliding window for\ntoken-by-token evaluation substantially improves\nresults for all models (Tab. 1, col. 5). Here, we\nsee negligible improvement between the models\ntrained with subsequence lengths of 1,024 and\n3,072 tokens (0.05 perplexity). This approach im-\nproves results by increasing the minimum amount\nof context available at each timestep which indi-\ncates that long contexts may not be beneﬁcial to\ntransformer models, but very short contexts are\nharmful. However, sliding window inference can\nbe expensive since each token is encoded many\ntimes. For example, token-by-token inference for\nthe L = 3,072 model is almost 300 times slower\nthan nonoverlapping inference.\n4 Training Subsequence Length\n§3 results show that models trained on shorter sub-\nsequences can be effective at test time, and are\nmuch faster to train. We further explore this below.\n4.1 Staged Training\nWe propose a two-stage training routine that ini-\ntially uses short input subsequences followed by\nlong subsequences.6 This method was previously\napplied to speed up the training of BERT (Devlin\net al., 2019), but we show that it also improves\nperplexity.\nWe use sinusoidal position embeddings; learned\nposition embeddings, which we do not consider,\ncreate a dependency between the parameterization\nand subsequence length. In our experiments, we\nneither modify nor reset the state of the optimiza-\ntion algorithm between the two stages.\n4.2 Experiments\nOur experimental setup is described in §2. We\ndo not change any hyperparameters other than re-\nducing subsequence length while correspondingly\nincreasing batch size to keep the number of tokens\nper batch constant. As in the baseline, all models\nare trained for 205 epochs.\nAll models are trained in two stages; the second\nstage always uses a subsequence length of 3,072,\n6Curriculum learning (Bengio et al., 2009) trains on easier\ninputs before progressing to harder ones. Our approach does\nnot change the order in which the training examples are given\nto the model, but instead modiﬁes their lengths.\nInitial Stage Subseqence Length\n32 64 128 256 512 1024 1536\n25 17.94 17.57 17.58 18.19 18.06 18.20 18.77\n50 17.81 17.59 17.52 18.08 18.01 18.14 18.62\n75 17.93 17.61 17.55 18.01 18.05 18.03 18.57\nInitial Stage Epochs\n100 18.14 17.67 17.62 18.00 18.10 18.00 18.51\n125 18.61 17.88 17.70 18.00 18.13 17.98 18.49\n150 19.45 18.37 17.98 18.01 18.15 18.00 18.49\n175 21.16 19.51 18.57 18.23 18.20 18.08 18.57\n200 35.38 28.03 23.80 21.45 19.63 18.56 18.84\nTable 2: Each model’s perplexity at the end of training\n(dev. set, nonoverlapping eval.). All models have a sub-\nsequence length of 3,072 tokens at the end of training.\nThe B&A baseline achieves 18.65 ±0.24 perplexity.\nsince that lead to the best performance (discussed\nat end of this subsection).\nAppendix Table 6 shows the time each training\nroutine takes to match the baseline model’s per-\nformance on the validation set of WikiText-103.7\nMany conﬁgurations match this performance in\nless than half the time it takes to train the baseline\nitself; some reach baseline performance in only\n37% of the time needed to train the baseline.\nAlthough all models take less time to train than\nthe baseline, Table 2 shows that many outper-\nform it. For example, the best model—trained\nwith subsequence length L = 128until epoch 50—\noutperforms the baseline by 1.1 perplexity despite\ncompleting training in 87% of the time the baseline\ntakes to do so. The model that trains with L = 128\nuntil epoch 100 achieves similarly strong results\n(17.62 perplexity) and ﬁnishes training in 74% of\nthe time it takes the baseline.8\nThese results are very robust to the choice of\ninitial stage subsequence length and number of\nepochs. Table 2 shows that all models with an\ninitial stage of L = 1,024 tokens or less that switch\nto the second stage at epoch 125 or before beat the\nbaseline by a large margin at the end of training.\nAdditionally, Appendix Table 6 shows that those\nmodels match the baseline’s perplexity in at most\n71% of the time it takes to train the baseline.\nWhen we use nonoverlapping evaluation, the\nB&A baseline obtains 18.65 perplexity on the\ndevelopment set; our best model obtains 17.52.\nWhen we use sliding window evaluation (following\nBaevski & Auli, we use stride S = 512), our best\n7Table 7 in the appendix shows theepoch at which every\nmodel matched the baseline’s performance.\n8Table 8 in the appendix shows the total time it took to\ntrain each model.\n5497\n(a)\nsat \nK V Q \nself-attention \ncat the (b)\nV\nQ\nself-attention\nthe1 cat2 sat3\nK\nFigure 2: Inputs to the self-attention sublayer, conventionally (left) and with position-infused attention (right), for\nL = 3, at timestep 3. The numbers denote the position embeddings.\nmodel obtains 16.89 perplexity, a large improve-\nment on the 17.92B&A result in that setting. On the\ntest set, using the same sliding window evaluation,\nour model obtains 17.56 perplexity, a substantial\ngain over the baseline’s 18.70 test-set perplexity.\nAppendix Table 10 shows that our best model uses\nalmost ﬁve times less memory during the ﬁrst stage\nthan the baseline.\nWe also found that setting L to less than 3,072\ntokens in the second stage degraded performance.\n(Appendix Table 9 shows staged training results\nwith an initial stage length of 128 for 50 epochs\n(as in the best model) and varying lengths for the\nsecond stage. We found this to also be true for other\ninitial stage lengths and epochs.) Unlike results in\nTable 1, where we show that models with L larger\nthan 1,024 do not substantially improve token-by-\ntoken generation perplexity, models trained using\nstaged training improve when given longer inputs\n(Appendix Table 9). Further, we explored using\nmore than two stages (up to six), but this did not\noutperform our two-stage curriculum.\nFinally, Appendix A.5 shows that staged train-\ning substantially improves on the Toronto Book\nCorpus (Zhu et al., 2015).\n5 Repositioning Position Embeddings\nSliding window inference substantially improves\nperformance by increasing the minimum effective\ncontext window size. But it is very slow. We could\nsolve this by letting the model attend to representa-\ntions of the previous subsequence during inference\non the current one.\nIn this case, the same token representations\nwould be used in different positions since a token\ngenerated near the end of one subsequence would\nbe cached and reused near the start of the next one.\nHowever, transformer model representations entan-\ngle positional and content information, so a cached\ntoken representation would encode an incorrect po-\nsition when reused in a new position.\nTransformerXL (Dai et al., 2019) uses relative\nposition embeddings to solve this problem. How-\never, that approach is slower and uses more param-\neters and memory than the baseline transformer.9\nWe solve this using no extra parameters, mem-\nory, or runtime. We also show that our method\ncan use much shorter input subsequences and still\nachieve superior performance.\nTransformer Language Models The baseline\ntransformer LM, given a token list T of length L\nand a tensor P containing the ﬁrst L position em-\nbeddings, produces L next-token predictions using\nthe following procedure:\n1. Embed each token in T, producing tensor X.\n2. Add the position embedding of each index to\nthe token at that index: X = X + P.\n3. Feed X through each transformer layer.\nThe self-attention sublayer in each\ntransformer layer is invoked as follows:\nself-attention(key=X, query=X, value=X)\n4. Transform the outputs of the last transformer\nlayer using the softmax layer, giving L next-\ntoken probability distributions.\n5.1 Position-Infused Attention (PIA)\nWe propose to let the model reuse previous out-\nputs by making each output contain no explicit\npositional information. To do this, we modify the\n9The self-attention coefﬁcients between q queries and k\nkeys in TransformerXL are the sum of two dot products of size\nq ·k; the unmodiﬁed attention sublayer and our PIA method\nboth compute only one dot product of sizeq·k. We also bench-\nmarked the TransformerXL model using its publicly released\ncode and found that their relative position embeddings slow\ninference by 22% and require 26% more parameters than their\nimplementation of the unmodiﬁed self-attention sublayer.\n5498\nmodel so that it does not add position embeddings\nat the beginning of the computation (step 2), but\nrather adds them to the query and key vectors at\neach layer (but not to the value vectors). The out-\nputs at each layer are the transformed, weighted\nsums of the value vectors, and, since the value vec-\ntors in our model do not contain explicit positional\ninformation, the outputs also do not.\nFormally, steps 1 and 4 do not change, step 2\nis omitted, and step 3 is modiﬁed to invoke the\nself-attention sublayer as follows:\nself-attention(key=X+P, query=X+P,\nvalue=X)\nFigure 2 (b) depicts this method.\nAlthough PIA sublayer outputs contain no ex-\nplicit positioning information, the attention mech-\nanism can still compute position-dependent out-\nputs because positional information is added to\nthe query and key vectors. Our method is imple-\nmentable in just a few lines of code.\n5.2 PIA Enables Caching\nIn the unmodiﬁed transformer, to generate a string\nwhose length exceeds L, it would have to be split\ninto separate subsequences, and the model would\nbe unable to attend to the previous subsequence\nwhen generating the current one.\nUsing PIA, we can store and attend to represen-\ntations of the previous subsequence since they no\nlonger contain any explicit positioning information.\nTherefore, all our PIA models use a cache, where\nrepresentations from the previous forward pass are\nstored and attended to in the next forward pass.\nCaching makes generation faster. The com-\nplexity of the attention mechanism isO(q·k) where\nq is the number of queries (outputs) and k is the\nnumber of key-value pairs (inputs). To generate\na sequence whose length exceeds L using token-\nby-token generation in the unmodiﬁed transformer\n(with subsequence length L), attention takes O(L2)\ntime (since there are L queries and L keys). Using\nPIA and caching, we can reuse L −1 of the pre-\nvious outputs at every layer. Thus, our attention\nsublayer takes O(L) time (because now there is a\nsingle query and L keys).\nOur approach is useful in scenarios where we\nneed to evaluate or generate sequences that are\nlonger than the model’s subsequence length. There-\nfore, it would not be applicable to sequence-to-\nsequence tasks such as sentence-level translation,\nwhere sequence lengths are short.\nMost language models, including B&A, train on\ntheir data as nonoverlapping subsequences. This\nmeans that training subsequences can be shufﬂed at\neach epoch and consumed in random order. How-\never, when using PIA, we would like the cache to\ncontain the previous subsequence. We therefore\ndo not shufﬂe the data, making the cached subse-\nquence the previously occurring one.\nFigure 1(c) depicts training with a cache that con-\ntains representations of the previous subsequence.\n5.3 Experiments\nWe use the experimental setup described in §2.\nThe B&A baseline achieves 18.65 on the devel-\nopment set. We train two additional baselines, the\nﬁrst uses PIA without caching and the second uses\ncaching but no PIA. If just PIA is used (without\ncaching), performance degrades to 19.35 perplex-\nity, but the model’s speed and memory usage do not\nchange. Using caching without PIA severely hurts\nperformance, obtaining 41.59 perplexity. Disabling\ndata shufﬂing in the PIA-only model achieves simi-\nlar performance to that model when it does use data\nshufﬂing, at 19.44 perplexity. Not shufﬂing the data\nis necessary for recurrent-style training that caches\npreviously computed subsequence representations.\nOur next experiments use the recurrent-style\ntraining of Dai et al. (2019), where we receive L\nnew tokens at every training iteration and attend to\nL′cached representations (of the subsequence of\ntokens that came immediately prior to theL new to-\nkens). As before, we output L predictions at every\ntraining iteration. This means that the maximal and\nminimal effective context window sizes are L′+ L\nand L′+ 1, respectively.\nIn all our models with PIA and caching, we set\nL′= L because a manual exploration of different\nmodels where L′̸= L did not yield better results.\nTable 3 compares the results of our models\nthat use PIA and caching to the baseline on the\nWikiText-103 dev. set. Evaluation and generation\nspeeds are shown in the nonoverlapping (N.o.) and\nsliding window (S.W., with stride S = 1) speed\ncolumns, respectively.10 Unlike in the baseline,\ntoken-by-token evaluation in our model achieves\nthe same perplexity as nonoverlapping evaluation\n10Note that Baevski and Auli (2018) show that the baseline\nmodel can also achieve 17.92 during S.W. evaluation, when\nS = 512, with a speed of 2.5k tokens per second.\n5499\nTrain Inference\nSubseq.\nLength\nSpeed ↑\nSpeed ↑ PPL ↓ N.o. S.W.\n32 22.0k 20.53 2.0k 49\n64 23.8k 19.07 4.1k 51\n128 24.4k 18.37 7.9k 50\n256 23.5k 17.92 12.8k 48\n512 21.5k 17.85 14.5k 46\n768 17.6k 18.16 13.8k 43\n1024 16.6k 18.19 13.9k 39\n1536 12.9k 19.11 7.9k 34\nBaseline 13.9k 18.65 14.7k -\n(3072) 17.92 - 5\nTable 3: Dev. perplexity and speed for PIA models\ntrained with different subsequence lengths ( L). PIA\nmodels attend to L new and L cached tokens at each in-\nference pass. N.o. is nonoverlapping eval.; S.W. is slid-\ning window eval., where we always use S = 1 (token-\nby-token) here. The baseline is evaluated with both\nevaluation methods. We measure speed in tok./sec. per\nGPU and use a batch size of 1 for inference.\nsince in both cases, the predictions for each in-\nput subsequence are conditioned not only on the\ncurrent input, but also on the previous input, mak-\ning the context window the same in both inference\nmodes (in both cases, at every timestep, the context\nwindow is all tokens up to that timestep).\nTable 3 shows that as we increase subsequence\nlength, perplexity improves, peaking at 512 before\nstarting to degrade. Our best model obtains 17.85\nperplexity, which is multiple standard deviations\nbetter than the baseline (18.65, N.o.). Table 5 in\n§6 shows a similar gain on the test set. The best\nmodel runs 1% slower than the baseline during N.o.\neval. (since caching reduces the speed gain from\nsmaller attention matrices in this mode). Table 10\n(appendix) shows that it uses less than half of the\nmemory the baseline does during training. Our best\nmodel trains 55% faster than the baseline.\nOur best model, with subsequence length 512,\nhas attention matrices of size 512 ·1,024 (since we\nhave 512 queries—one per every new token—and\n1,024 keys and 1,024 values—one per every new\ntoken and every cached token). In the baseline, all\nattention matrices are of size 3,072 ·3,072.\nCaching previously computed representations\nlets us do token-by-token generation efﬁciently\nwhen generating more than L tokens. Our model\nis nine times faster than the baseline at token-by-\ntoken generation even as it achieves better perplex-\nity and uses much less memory (Tab. 3, col. 5).\nFirst Stage\nSubseq. Length\nTrain Inference\nSpeed ↑ PPL ↓\n32 21.6k 17.66\n64 22.6k 17.56\n128 22.9k 17.47\n256 22.5k 17.50\nPIA + Cache w/o 21.5k 17.85Staged Training\nTable 4: Dev. perplexity for models that use PIA,\ncaching, and staged training (with ﬁnal subseq. length\nof 512). We measure speed in tok./sec. per GPU. Evalu-\nation speed is the same for all models, at 14.5k tok./sec.\nPIA and caching also greatly improve perplex-\nity on the Toronto Book Corpus; see A.5 in the\nappendix.\n6 Shortformer Results\nTo assess whether the gains from staged training,\nPIA and caching are additive, we take our best\ncaching PIA model, with subsequence length 512,\nand apply staged training to it, training it with a\nsubsequence length of between 32 to 256 for the\nﬁrst half of training. 11 Table 4 shows the results.\nAs in §4.2, where staged training was applied to\nthe unmodiﬁed baseline, the results are very robust\nto the choice of initial stage subsequence length,\nwith all the different choices improving perplexity\nover the model that does not use staged training.\nThe best model (with initial subsequence length\n128), which we call Shortformer, achieves 17.47\ndev. set perplexity and trains 65% faster than the\nbaseline. Since its attention matrices are of dimen-\nsion 512 ·1,024 (the baseline’s are3,072 ·3,072),\nour model uses less memory (§A.4, appendix). It\nhas the same number of parameters as the baseline.\nFigure 3 (appendix) compares our best models\nusing each method we presented (and their com-\nbination) to the baseline. It shows that combin-\ning caching, PIA and staged training (Shortformer)\nyields the quickest training and best perplexity\nwhen using nonoverlapping evaluation. Evaluation\nspeed is similar for all of these models.\nFinally, Table 5 compares our best models on the\ntest set of WikiText-103 to the state of the art.12\nShortformer is almost twice as fast to train as\nthe baseline and achieves superior results. Like the\n11We picked 50% of epochs as the length of the ﬁrst stage\nsince that produced near-optimal results at a fast speed in §4.\n12We benchmarked speed, on V100 GPUs, for all models\nthat had publicly available code.\n5500\nTrain Inference (Test)\nModel Param. ↓Speed ↑Mode Speed ↑PPL ↓\nBaseline 247M 13.9k N.o. 14.7k 19.40\nS.W. 2.5k 18.70\nTransformerXL∗ 257M 6.0k N.o. 3.2k 18.30\nSandwich T. 247M 13.9k S.W. 2.5k 17.96\nCompressive T. 329M - N.o. - 17.1\nRouting T. - - N.o - 15.8\nkNN-LM∗∗ 247M 13.9k S.W. 145 15.79\nPIA + Caching 247M 21.5k N.o. 14.5k 18.55\nStaged Training 247M 17.6k S.W. 2.5k 17.56\nShortformer 247M 22.9k N.o. 14.5k 18.15\nTable 5: Comparison of our best models to other strong\nLMs (see text for citations and explanations) evaluating\nthe WikiText-103 test set, where S = 512. We mea-\nsure speed in tok./sec. per GPU, and use a batch size of\n1 for inference. ∗TransformerXL runs on an older ver-\nsion of PyTorch, which might affect speed.∗∗kNN-LM\nrequires a 400GB datastore.\nbest model from §5.3, it is nine times faster than\nthe baseline for token-by-token generation.\nSince it uses a cache, sliding window evaluation\ndoes not increase Shortformer’s performance. By\ntraining the baseline with staged training (and no\nPIA or caching), we obtain a model (our best model\nfrom §4.2) that, with sliding window eval., obtains\neven better results, but that model is much slower\nthan Shortformer (Table 5, second-to-last row).\nShortformer outperforms the baseline’s perplex-\nity and performs within a standard deviation of\nthe Sandwich Transformer (Press et al., 2020)\nand TransformerXL. It does not outperform the\nCompressive Transformer (Rae et al., 2020), Rout-\ning Transformer (Roy et al., 2020) and kNN-\nLM (Khandelwal et al., 2020), which make or-\nthogonal improvements that can be applied to any\nlanguage model, at the price of slower decoding.\nCombining them with our approach may yield fur-\nther gains. These results are similar to those we\nobtain on the Toronto Book Corpus ( §A.5 in the\nappendix).\n7 Related Work\nStaged Training Devlin et al. (2019) used a\nstaged training routine for BERT by performing\nthe ﬁrst 90% of training on short subsequences (of\nlength 128) before moving on to longer ones (of\nlength 512). They use this method to speed train-\ning, but we show that also it improves perplexity\nand analyze different conﬁgurations of this method.\nMany recent papers have explored improving\ntransformer efﬁciency by reducing the quadratic\ncost of self-attention, motivated by scaling to\nlonger sequences (Kitaev et al., 2020; Roy et al.,\n2020; Tay et al., 2020). We instead demonstrate\nimproved results with shorter sequences, which\nnaturally also improve efﬁciency.\nOne way to reduce transformer memory usage is\nto sparsify the attention matrix by letting the model\nattend only to a subset of nearby tokens at each\ntimestep (Child et al., 2019; Beltagy et al., 2020;\nRoy et al., 2020). Training on shorter subsequence\nlengths is much more efﬁcient: we use multiple, but\nmuch smaller, attention matrices. Since attention\nuses memory and computation in a way that scales\nquadratically with input size, splitting the inputs\ninto multiple subsequences each processed indepen-\ndently lets us use less memory and run faster. Like\nour method, Beltagy et al. (2020) attend at each\ntimestep to a growing number of neighbors as train-\ning progresses, but they use ﬁve stages, which we\nfound not to be superior to our two-staged method.\nThe adaptive attention span model of Sukhbaatar\net al. (2019) learns the maximum effective context\nwindow sizes for each head at each layer indepen-\ndently. Like in our method, context window sizes\nare smaller at the start of training and lengthen\nas training progresses. We show that a simple\napproach of manually choosing two subsequence\nlengths is highly effective. In addition, keeping sub-\nsequence lengths equal across all heads and layers\nlets us save memory and runtime.\nPosition-Infused Attention TransformerXL\n(Dai et al., 2019) caches and attends to previous\nrepresentations using an attention sublayer that\nuses relative positioning (Shaw et al., 2018). It\nruns much slower than the unmodiﬁed attention\nsublayer, requires extra parameters, and requires\ninternally modifying the self-attention sublayer,\nwhile our PIA method (§5) does not.\nIn parallel with our work, Ke et al. (2020) com-\npute attention coefﬁcients by summing two atten-\ntion matrices, one based on position-position in-\nteractions and the other on content-content inter-\nactions. As in PIA, they do not add position em-\nbeddings at the bottom of the model. They present\nresults only for BERT, which uses much smaller\nsubsequences than our models.\n8 Conclusion\nOur results challenge the conventional wisdom that\nlonger subsequences are always better. By ﬁrst\n5501\ntraining on shorter subsequences and then progress-\ning to longer ones via staged training, we improve\nperplexity and reduce training time. We addition-\nally propose position-infused attention, which en-\nables caching and efﬁciently attending to previous\noutputs; we show that models using this method do\nnot require large input subsequences. We ﬁnally\nshow that these two methods can be combined to\nproduce a speedier and more accurate model.\nAcknowledgments\nWe thank Tim Dettmers, Jungo Kasai, Gabriel Il-\nharco, Hao Peng, Sewon Min, Mandar Joshi, Omer\nLevy, Luke Zettlemoyer, Julian Michael, Edward\nMisback, Soﬁa Serrano, Nikolaos Pappas, Jesse\nDodge, Myle Ott, and Sam Shleifer for their valu-\nable feedback and fruitful discussions.\nReferences\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\nCoRR, abs/1809.10853.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nYoshua Bengio, J ´erˆome Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Con-\nference on Machine Learning, ICML ’09, page\n41–48, New York, NY , USA. Association for Com-\nputing Machinery.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In ICLR.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking\npositional encoding in language pre-training.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels.\nOﬁr Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2996–3005, Online. Association for Computa-\ntional Linguistics.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 157–163, Valencia,\nSpain. Association for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learn-\ning Representations.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\n5502\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient transformers: A survey.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27.\n5503\nA Appendix\nA.1 Additional Staged Training Results\nTable 6 shows the time each staged training model\nneeds to match baseline performance, as a fraction\nof the time it takes to train the baseline. The fastest\nthree conﬁgurations each match the baseline’s per-\nformance in just 37% of the time it takes to train\nthe baseline. This result is very robust to hyperpa-\nrameter changes, as all models trained with initial\nsubsequence length of between 64 and 512, that\nswitch to the second stage at epoch 50 to 150, man-\nage to match the baseline’s performance in at most\n59% of the time it takes to train it.\nInitial Stage Subsequence Length\n0 32 64 128 256 512 1024 1536\n25 0.60 0.54 0.53 0.65 0.64 0.71\n50 0.53 0.48 0.47 0.54 0.59 0.63 0.81\n75 0.51 0.43 0.42 0.48 0.53 0.56 0.79\nInitial Stage Epochs\n100 0.52 0.40 0.38 0.41 0.47 0.50 0.73\n125 0.61 0.41 0.37 0.37 0.42 0.46 0.69\n150 0.48 0.39 0.37 0.40 0.44 0.66\n175 0.48 0.43 0.45 0.51 0.70\n200 0.59\nTable 6: Time needed to match baseline performance\n(dev. set, nonoverlapping eval.) as a fraction of time\nneeded to train the baseline (smaller is better). Mod-\nels never matching the baseline have empty cells. All\nmodels have a subsequence length of 3,072 tokens at\nthe end of training.\nInitial Stage Subsequence Length\n32 64 128 256 512 1024 1536\n25 136 123 122 146 144 155\n50 135 124 122 136 144 149 179\n75 143 128 125 136 144 145 181\nInitial Stage Epochs\n100 158 135 130 136 145 142 175\n125 190 149 141 140 146 144 174\n150 176 160 154 153 151 174\n175 191 178 177 176 189\n200 202\nTable 7: Epoch at which each model matches the base-\nline. Some models never match the baseline, and so\nthose cells are empty.\nTables 7 and 8 show the epoch at which each\nmodel matched the baseline’s performance and the\ntotal time it took to train each of our staged training\nmodels.\nInitial Stage Subsequence Length\n32 64 128 256 512 1024 1536\n25 0.94 0.94 0.94 0.94 0.94 0.95 0.97\n50 0.87 0.87 0.87 0.87 0.88 0.90 0.94\n75 0.81 0.81 0.81 0.81 0.82 0.85 0.90\nInitial Stage Epochs\n100 0.75 0.75 0.74 0.75 0.77 0.80 0.87\n125 0.68 0.68 0.68 0.69 0.71 0.75 0.84\n150 0.62 0.62 0.61 0.62 0.65 0.70 0.81\n175 0.56 0.56 0.55 0.56 0.59 0.66 0.78\n200 0.49 0.49 0.48 0.50 0.53 0.61 0.75\nTable 8: Total time needed to train each model as a\nfraction of the time needed for baseline training.\nA.2 Staged Training with Shorter Final Stage\nL\nIn section 4, all models presented used Staged\nTraining with a ﬁnal input subsequence length L\nof 3,072 tokens. In Table 9, we show the results\nof training with a ﬁrst stage with L = 128for 50\nepochs, and using varying subsequence lengths for\nthe second stage. The best result is obtained when\nthe second stage uses L = 3,072. In addition, in\nall of our other experiments (not presented here)\nwith different L and epoch number values for the\nﬁrst stage, we observed that using L = 3,072 for\nthe second stage always achieved the best perplexi-\nties. Models trained with staged training and eval-\nuated with a sliding window sometimes perform\nslightly worse when S is decreased, but this differ-\nence is much smaller than the standard deviation.\nThe L = 1536 and L = 3072 models peaked at\nS = 512, and then as S was decreased perplexity\nstarted slightly degrading.13\nA.3 Training Speed vs. Performance\nFigure 3 compares the validation performance and\ntraining speed of the baseline to our models.\nA.4 Memory Usage\nTo understand how much memory our models and\nthe baseline use during training, we ﬁnd the largest\nbatch size that we can load into memory for both\nour models and the baseline. Models that can simul-\ntaneously make more predictions are more memory\nefﬁcient.\n13We conjecture that this is because of a train-test mismatch\nthat occurs since the average effective context length during\ntraining is 3,072\n2 = 1,536 and so the model focuses on learning\nhow to make predictions for the tokens in the center of the\ninput, and does not perform as well when making predictions\nfor tokens at the end of the input (which is what we use when\nusing sliding window evaluation).\n5504\nFinal\nSubseq.\nLength\nInference PPL ↓\nNonoverlapping Sliding Window\nS = 512 S = 1\n256 21.26 - 18.72\n512 19.69 19.69 18.04\n1024 18.64 17.60 17.58\n1536 18.10 17.28 17.30\n3072 17.52 16.89 17.01\nTable 9: Inference perplexity for staged training mod-\nels trained with an initial stage subsequence length of\n128 for 50 epochs and varying second stage subse-\nquence length L (for the second stage’s 155 epochs).\nS is stride. To see how these models perform without\nstaged training, refer to Table 1.\nTraining\nMax Max\nModel Batch Size ↑ Predictions ↑\nBaseline 2 6,144\nStaged Training\nStage 1 230 29,440\nStage 2 2 6,144\nPIA + Caching 26 13,312\nShortformer\nStage 1 160 20,480\nStage 2 26 13,312\nTable 10: Memory usage of the baseline and our mod-\nels during WikiText-103 training. For each model we\nshow the maximal batch size that it could ﬁt on one\nGPU at once during training. The max predictions col-\numn denotes the number of tokens predicted at each\nfeedforward pass, which we calculate by multiplying\nbatch size by number of predictions per subsequence\n(which is equivalent to L). We benchmarked all mod-\nels on a V100 GPU, with 32GB of memory. Note that\nthe second stage in the staged training model matches\nthe performance of the baseline model, because those\narchitectures are identical. The same is true for the sec-\nond stage of the Shortformer and the PIA + Caching\nmodel.\n13k 15k 17k 19k 21k 23k\nTokens per Second ( )\n17.0\n17.5\n18.0\n18.5\n19.0Perplexity ( )\nShortformer\n                PIA + Cache \n Baseline\nStaged Training\nFigure 3: Dev. perplexity vs. training speed for the\nbaseline and our best staged training model, our best\nPIA and caching model, and our best combined model\n(Shortformer). All models are evaluated using nonover-\nlapping evaluation.\nTable 10 shows the memory usage for the base-\nline model and our models. Since our Shortformer\nmodel has much smaller attention matrices, during\ntraining it can make more than double the next-\ntoken predictions than the baseline can in each\nfeedforward pass. During inference, we use a batch\nsize of 1 throughout the paper, following (Dai et al.,\n2019), and in our experiments, the PIA + Caching\nmodel, the ﬁnal staged training model and the base-\nline all use a similar amount of memory during\nnonoverlapping evaluation. During token-by-token\ninference, the maximum number of predictions for\nthe baseline model is 7, whereas our model can ﬁt\na batch size of 39 (so 39 predictions are made dur-\ning token-by-token inference), making our model\nmore than 5 times more memory efﬁcient than the\nbaseline. Using a batch size of one is a realistic\nbenchmarking scenario: in large models such as\nGPT-3, a batch size of one is used during inference.\nA.5 Toronto Book Corpus\nTo verify that our results transfer to other datasets,\nwe ran our models on the Toronto Book Cor-\npus (TBC) (Zhu et al., 2015), a 700M token\ncollection of books that has previously been\nused in the training corpus of BERT (along\nwith English Wikipedia). We use the same\ntrain/development/test split as (Khandelwal et al.,\n2020) and (Press et al., 2020), as well as their tok-\nenization, which uses BERT’s vocabulary of 29K\nBPE subwords. As in (Khandelwal et al., 2020)\nand (Press et al., 2020), since the vocabulary is\nmuch smaller than WikiText-103’s, we use a tied\nword embedding and softmax matrix (Press and\nWolf, 2017; Inan et al., 2017), instead of using\n5505\nthe adaptive word embeddings (Baevski and Auli,\n2018) as in the WikiText-103 models.\nTo fairly compare our models to the ones\nfrom (Khandelwal et al., 2020) and (Press et al.,\n2020), our initial set of experiments on the TBC\nuse a maximum subsequence length of 1,024 (for\nstaged training), train for 59 epochs, and for all\nother hyperparameters we use the same values as\nthe ones we used for WikiText-103 (see Experi-\nment Setup in Section 2). In this setting, the base-\nline achieves a perplexity of 15.38±0.39 (standard\ndeviation) on the development set.\nWe do not tune the hyperparameters of our meth-\nods on the TBC, we simply use the same values as\nthe best ones that we found on the WikiText-103\ndataset. For staged training, our best model trained\nfor 50\n205 % of the epochs with L = 128 and spent\nthe rest of training with the same subsequence size\nas the baseline. For the TBC, we again trained\nthe staged training model model with L = 128\nfor the ﬁrst 50\n205 % of training, and then move on\nto L = 1 ,024, to match the Sandwich Trans-\nformer (Press et al., 2020) and kNN-LM (Khan-\ndelwal et al., 2020) which used 1,024 as the subse-\nquence length.\nFor the PIA + Caching model, we set L = 512,\nas we did for our best PIA + Caching on the\nWikiText-103 dataset.\nFor the Toronto Book Corpus Shortformer, we\ntrained for the ﬁrst half of training with L = 128\nbefore moving on to training with L = 512, as in\nour WikiText-103 models (Section 6).\nTrain Inference\nPPL ↓\nModel Speed ↑Mode Speed ↑ Dev. Test\nBaseline 24.0k N.o. 19.2k 15.38 12.73\nS.W. 9.6k 14.75 11.89\nkNN-LM∗ 24.0k S.W. - 14.20 10.89\nSandwich T. 24.0k S.W. 9.6k - 10.83\nPIA + Caching 20.5k N.o. 15.0k 13.86 11.20\nStaged Training 25.5k N.o. 19.2k 13.81 11.18\nS.W. 9.6k 13.13 10.72\nShortformer 21.3k N.o. 15.5k 13.40 10.88\nTable 11: Comparison of our best models to other\nstrong LMs trained on the Toronto Book Corpus (TBC).\nFollowing Khandelwal et al. (2020) and Press et al.\n(2020), for the baseline and our staged training model,\nwe set L = 1,024 and S = 512when using sliding win-\ndow (S.W.) evaluation in the TBC dataset. All models\nhave 261M parameters. ∗kNN-LM requires a 400GB\ndatastore.\nTable 11 shows that staged training and the Short-\nformer improve over the baseline by a wide margin\nand match the results of the Sandwich Transformer\nand the kNN-LM. As noted in Section 6, those con-\ntributions are orthogonal to ours, and combining\nthem might yield further gains. Since in Table 11\nthe ﬁnal stage of the staged training model (and the\nbaseline) both have L = 1,024, Shortformer lacks\na speed advantage in this scenario.\nTable 12 shows results for our staged training\nmodel trained with a ﬁnal stage subsequence length\nof 3,072 tokens, as in our WikiText-103 experi-\nments in Section 4. This model trains faster than\nthe L = 3,072 baseline and also achieves much\nbetter perplexity scores (the baseline in this setting\nachieves a perplexity of 14.52 ±0.15 (standard\ndeviation) on the development set). In addition,\nnote that the Shortformer model from Table 11\nachieves better perplexity than even the baseline\nwith L = 3,072, although Shortformer is much\nfaster to train and uses much smaller attention ma-\ntrices during inference (of size 512 ·1024; the base-\nline has attention matrices of size 3,072 ·3,072, as\nin Section 6).\nTrain Inference\nPPL ↓\nModel Speed ↑Mode Speed ↑ Dev. Test\nBaseline 14.2 N.o. 15.1 14.52 11.69\n(L = 3,072) S.W. 2.5k 14.14 11.43\nStaged Training 18.1 N.o. 15.1 13.19 10.76\n(L = 3,072) S.W. 2.5k 12.80 10.48\nTable 12: Comparison of the staged training model to\nthe baseline, when the subsequence length L is set to\n3,072. In this table, S = 512.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9959947466850281
    },
    {
      "name": "Language model",
      "score": 0.7829774618148804
    },
    {
      "name": "Transformer",
      "score": 0.7101402878761292
    },
    {
      "name": "Computer science",
      "score": 0.6911991834640503
    },
    {
      "name": "Position (finance)",
      "score": 0.46109142899513245
    },
    {
      "name": "Algorithm",
      "score": 0.4337453544139862
    },
    {
      "name": "Word (group theory)",
      "score": 0.4299895763397217
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31722402572631836
    },
    {
      "name": "Mathematics",
      "score": 0.18953749537467957
    },
    {
      "name": "Engineering",
      "score": 0.06079459190368652
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    }
  ],
  "cited_by": 13
}