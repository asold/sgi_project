{
    "title": "Whispered Tuning: Data Privacy Preservation in Fine-Tuning LLMs through Differential Privacy",
    "url": "https://openalex.org/W4391107696",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5046170366",
            "name": "Tanmay Singh",
            "affiliations": [
                "Bennett University"
            ]
        },
        {
            "id": "https://openalex.org/A5093762937",
            "name": "Harshvardhan Aditya",
            "affiliations": [
                "Bennett University"
            ]
        },
        {
            "id": "https://openalex.org/A5111982286",
            "name": "Vijay K. Madisetti",
            "affiliations": [
                "Georgia Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5073091147",
            "name": "Arshdeep Bahga",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6600424091",
        "https://openalex.org/W4385573200",
        "https://openalex.org/W4287889073",
        "https://openalex.org/W4319780902",
        "https://openalex.org/W6600175266",
        "https://openalex.org/W4365794479",
        "https://openalex.org/W6602127427",
        "https://openalex.org/W4235997592",
        "https://openalex.org/W6702248584"
    ],
    "abstract": "The proliferation of Large Language Models (LLMs) across various sectors underscored the urgency of addressing potential privacy breaches. Vulnerabilities, such as prompt injection attacks and other adversarial tactics, could make these models inadvertently disclose their training data. Such disclosures could compromise personal identifiable information, posing significant privacy risks. In this paper, we proposed a novel multi-faceted approach called Whispered Tuning to address privacy leaks in large language models (LLMs). We integrated a PII redaction model, differential privacy techniques, and an output filter into the LLM fine-tuning process to enhance confidentiality. Additionally, we introduced novel ideas like the Epsilon Dial for adjustable privacy budgeting for differentiated Training Phases per data handler role. Through empirical validation, including attacks on non-private models, we demonstrated the robustness of our proposed solution SecureNLP in safeguarding privacy without compromising utility. This pioneering methodology significantly fortified LLMs against privacy infringements, enabling responsible adoption across sectors.",
    "full_text": null
}