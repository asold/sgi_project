{
  "title": "RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training",
  "url": "https://openalex.org/W4285605548",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2180864782",
      "name": "Lüya Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1991796106",
      "name": "Feng Liang",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2096702515",
      "name": "Yangguang Li",
      "affiliations": [
        "Group Sense (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2103331717",
      "name": "Honggang Zhang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2001108395",
      "name": "Jing Shao",
      "affiliations": [
        "Group Sense (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6750275533",
    "https://openalex.org/W6786614245",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2618011341",
    "https://openalex.org/W6759363029",
    "https://openalex.org/W2734663976",
    "https://openalex.org/W6847742374",
    "https://openalex.org/W6868564194",
    "https://openalex.org/W3120857301",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W2962758679",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3160566314",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3213836217",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3169320628",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Recently, self-supervised vision transformers have attracted unprecedented attention for their impressive representation learning ability. However, the dominant method, contrastive learning, mainly relies on an instance discrimination pretext task, which learns a global understanding of the image. This paper incorporates local feature learning into self-supervised vision transformers via Reconstructive Pre-training (RePre). Our RePre extends contrastive frameworks by adding a branch for reconstructing raw image pixels in parallel with the existing contrastive objective. RePre equips with a lightweight convolution-based decoder that fuses the multi-hierarchy features from the transformer encoder. The multi-hierarchy features provide rich supervisions from low to high semantic information, crucial for our RePre. Our RePre brings decent improvements on various contrastive frameworks with different vision transformer architectures. Transfer performance in downstream tasks outperforms supervised pre-training and state-of-the-art (SOTA) self-supervised counterparts.",
  "full_text": "RePre: Improving Self-Supervised Vision Transformer with\nReconstructive Pre-training\nLuya Wang1 , Feng Liang2 , Yangguang Li3 , Honggang Zhang1 , Wanli Ouyang4 , Jing Shao3\n1Beijing University of Posts and Telecommunications\n2University of Texas at Austin\n3SenseTime Group Limited\n4The University of Sydney\n{wangluya,zhhg}@bupt.edu.cn, jeffliang@utexas.edu, {liyangguang,shaojing}@sensetime.com,\nwanli.ouyang@sydney.edu.au\nAbstract\nRecently, self-supervised vision transformers have\nattracted unprecedented attention for their impres-\nsive representation learning ability. However, the\ndominant method, contrastive learning, mainly re-\nlies on an instance discrimination pretext task,\nwhich learns a global understanding of the im-\nage. This paper incorporates local feature learn-\ning into self-supervised vision transformers via Re-\nconstructive Pre-training (RePre). Our RePre ex-\ntends contrastive frameworks by adding a branch\nfor reconstructing raw image pixels in parallel\nwith the existing contrastive objective. RePre is\nequipped with a lightweight convolution-based de-\ncoder that fuses the multi-hierarchy features from\nthe transformer encoder. The multi-hierarchy fea-\ntures provide rich supervisions from low to high\nsemantic information, which are crucial for our\nRePre. Our RePre brings decent improvements\non various contrastive frameworks with different\nvision transformer architectures. Transfer perfor-\nmance in downstream tasks outperforms super-\nvised pre-training and state-of-the-art (SOTA) self-\nsupervised counterparts.\n1 Introduction\nSelf-supervised pre-training, a method to learn general repre-\nsentations without expensive annotated data, has greatly fa-\ncilitated Natural Language Processing (NLP) [Radford et al.,\n2019; Devlin et al., 2018] and similar trends also in Computer\nVision (CV) [Chen et al., 2020b; Grill et al., 2020]. One of\nthe main ingredients for the success of self-supervised pre-\ntraining in NLP is using the scalable Transformer[Vaswaniet\nal., 2017], a self-attention-based architecture. In CV , Vision\nTransformer (ViT) [Dosovitskiy et al., 2020] has emerged\nas an alternative to Convolutional Neural Networks (CNN)\nsince its creation. Despite its remarkable performance, pre-\ntraining the vanilla ViT requires enormous labeled data (e.g.,\nJFT-300M [Sun et al., 2017] in [Dosovitskiy et al., 2020])\nand extensive computing resources. To avoid expensive la-\nTransformer\nEncoder\nContrastive\nLoss\nReconstruction\nDecoder\nReconstruction\nLoss\nProjector Predictor\nEMA\nProjectorTransformer\nEncoder\nsimilarity\nMulti-hierarchy\nfeatures\nFigure 1: Our RePre extends contrastive frameworks (bottom grey\npart) by adding a branch for reconstructing raw image pixels (top\npart). Contrastive framework (MoCo v3 [Chen et al., 2021] in this\nfigure) models image similarity and dissimilarity between two views\nin an embedding space. Our reconstruction decoder recovers raw\nimage pixelsusing multi-hierarchy features from the transformer en-\ncoder.\nbeled data, this paper studies pre-training self-supervised vi-\nsion transformers.\nThere is a major difference between the self-supervised\npre-training paradigm in NLP and CV: language transformers\nare pre-trained with masked/autoregressive language model-\ning [Devlin et al., 2018; Radford et al., 2019], while for vi-\nsion transformers, the dominant method is contrastive learn-\ning which is based on instance discrimination pretext task\n[Chen et al., 2021; Caronet al., 2021; Xieet al., 2021a]. Con-\ncretely, contrastive learning maximizes the similarity of rep-\nresentations obtained from different views of the same image,\nleading to a global visual understanding (see the bottom part\nof Fig. 1). However, the sole global feature is insufficient for\ndownstream tasks beyond image classification, such as object\ndetection and segmentation. Motivated by the intuition that a\ngood visual representation should contain global features as\nwell as fine-grained local features, we try to answer: could\nwe achieve the best of both worlds?\nTo achieve holistic visual representations, this paper in-\ncorporates fine-grained local feature learning in contrastive\nself-supervised vision transformers. Inspired by the widely\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1437\nused reconstructive pre-training in CV [Bao et al., 2021 ],\nNLP [Devlin et al., 2018], and speech [Hsu et al., 2021],\nwe choose a simple yet effective pretext task:Reconstruction\nPre-training from raw pixels. Intuitively, pixel reconstruct-\ning could let the network capture low semantics to learn fine-\ngrained local features [Ahn and Kwak, 2018]. Our RePre ex-\ntends contrastive frameworks by adding a branch for recon-\nstructing raw image pixels in parallel with the existing con-\ntrastive objective (see Fig. 1). We split an image into patches\nand all these RGB patches are reconstructed through a de-\ncoder. Worth mentioning, our neat RePre does not require\nmasking strategy [Hsu et al., 2021; Devlin et al., 2018] nor\nthe tokenizer in BEIT [Bao et al., 2021].\nOur initial trial is feeding the output of the last transformer\nencoder layer into the reconstruction decoder. However, it\nturns out this simple combination only brings marginal im-\nprovements. We argue that this ineffectiveness lies in the dis-\ncrepancy between the last layer’s high semantic features and\nthe low semantic pixel objective. Deep neural networks learn\nhierarchical semantic features via stacking layers [Dosovit-\nskiy et al., 2020; Liuet al., 2021]. As the processing hierar-\nchy goes up, the early layer captures simple low-level visual\ninformation (shallow features), and the late layer can effec-\ntively focus on complex high-level visual semantics (deep\nfeatures). Driven by this analysis, we propose to use the\nmulti-hierarchy features in the transformer encoder. We col-\nlect the low to high semantic features within the transformer\nencoder and use them as a whole to guide the reconstruction.\nThe reconstruction decoder is another essential part of our\nRePre. Inspired by U-Net shape [Ronneberger et al., 2015],\nour decoder gradually integrates the deep to shallow features\nfrom multiple hierarchies and regresses to predict the orig-\ninal RGB pixels directly with a simple L1 loss(see Fig. 2).\nTo combine multi-hierarchy features, the reconstruction de-\ncoder is consisted of several fusion layers. Interestingly, we\nfind that the fusion layer could be very lightweight, e.g., one\nor two convolution layers. Since our goal is to introduce ad-\nditional local features while keeping the high-level seman-\ntic features intact, the heavy reconstruction decoder would\nfocus too much on low semantic information, thus harm-\ning representation learning. Another favorable property of a\nlightweight decoder is its little training overload. Our RePre\nonly brings a negligible average of 4% workload in various\ncontrastive frameworks. The reconstruction decoder is only\nused during pre-training and dropped in the downstream fine-\ntuning phase.\nOur RePre is generic and can be plugged into arbitrary con-\ntrastive learning frameworks for various visual translator ar-\nchitectures. Extensive experiments demonstrate the effective-\nness and portability of this method. We validate our RePre\nin the latest contrastive learning frameworks (e.g., DINO,\nMOCO V3, MoBY , BYOL and SimCLR). Following stan-\ndard linear evaluation on ImageNet-1K, with RePre, these\nmethods improve top-1 accuracy by 0.5∼1.1%. Prominently,\nit also brings significant performance to the base methods on\ndense prediction tasks on the COCO and cityscape datasets,\neven outperforming supervised methods.\nOverall, our contributions are threefold:\n1. We incorporate fine-grained local feature learning\nin contrastive self-supervised vision transformers via\nadding a reconstruction branch. We adopt a simple yet\neffective objective: Reconstructive Pre-training (RePre)\nfrom raw RGB pixels.\n2. RePre utilizes multi-hierarchy fusion to provide rich su-\npervisions from intermediate features. We also find\na fast lightweight convolutional reconstruction decoder\ncould bring favorable results.\n3. Our RePre is general and easy to be plugged. Decent im-\nprovements are observed on various contrastive frame-\nworks with vision transformer and its variants. On dense\nprediction transfer tasks, RePre also brings significant\nimprovements even outperforming supervised methods.\n2 Related Work\n2.1 Self-supervised Vision Transformer\nSelf-supervised contrastive learning has been popular in com-\nputer vision. Before the emergence ViT, prior work mainly\nfocus on ResNet, e.g, MoCo [Chen et al., 2020c ], Sim-\nCLR [Chen et al., 2020b], BYOL [Grill et al., 2020], Sim-\nSiam [Chen and He, 2021 ]. More recently, researchers have\nincorporated contrastive learning with ViT. MoCo v3 [Chen\net al., 2021] proposes an empirical study by training ViT with\nthe MoCo framework. DINO [Caron et al., 2021] shows two\nnew properties of self-supervised ViT compared with super-\nvised ViT. MoBY [Xie et al., 2021a] extends the contrastive\nframework with a ViT variant, Swin Transformer [Liu et al.,\n2021]. All these methods share the same spirit: modeling im-\nage similarity and dissimilarity (or only similarity) between\ntwo or more views, leading to a global image understanding.\nThey lack attention to local and low semantic features, which\nare crucial for downstream tasks beyond image classification,\nsuch as object detection and segmentation. Our RePre is com-\nplementary to these contrastive methods via enhancing fine-\ngrained local feature learning.\n2.2 Reconstructive Pre-training\nReconstructive (or generative) objectives are highly success-\nful for pre-training in NLP, e.g., masked/autoregressive lan-\nguage modeling in BERT[Devlin et al., 2018] and GPT [Rad-\nford et al., 2019]. These methods hold out a portion of the\ninput tokens and train models to predict the missing con-\ntent. In the field of CV , pioneering iGPT[Chen et al., 2020a]\nlearns a giant self-supervised transformer by directly predict-\ning pixel values, producing competitive results with super-\nvised counterparts. More recently, BEiT [Bao et al., 2021]\nquantizes image patches as discrete tokens using an off-the-\nshelf discrete V AE(dV AE) tokenizer[Ramesh et al., 2021],\nthen proposes to predict the masked tokens. Following BEiT,\niBoT [Zhou et al., 2021] introduces an online tokenizer. Con-\ncurrent MAE [He et al., 2021 ] and SimMIM [Xie et al.,\n2021b] propose to reconstruct raw pixels via mask image\nmodeling. Differently, our RePre incorporates reconstruc-\ntive pixel objectives along with contrastive learning frame-\nworks. It pre-trains general vision transformers for various\ndownstream tasks. Moreover, our neat RePre reconstructs all\nthe image pixels, so it does not require a masking strategy or\ntokenizer.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1438\nTransformerBlock… ReconstructionDecoderTransformerEncoderclass token: for contrastiveimage token: for reconstructionmulti-hierarchy features\nTransformerBlock\nTransformerBlock…\nTransformerBlock\n…\nFor contrastive learning\nReshape\ninput\ninput\nreconstructedresult\nℒ!loss\nReshape\nconcatationfusionlayers\nPatchEmbedding\nfeatureflow\n1×1Conv\nFigure 2: Details of our reconstruction branch. We get low-to-high semantic (multi-hierarchy) features from the transformer encoder by\nsampling shallow-to-deep transformer blocks. Our decoder gradually integrates deep-to-shallow features and regresses to predict the original\nRGB pixels with a simpleL1 loss. The sequential image tokens are reshaped to 2D shape for convolution operators in reconstruction decoder.\nThe fusion block in decoder is simple: a concatenation followed by fusion layers. Worth mentioning, for transformer variants with scale\ndownsampling, such as the Swin Transformer, we need to upsample the high-level features before concatenation (details see Sec. 3.3).\n3 Method\nIn this section, we first discuss the contrastive learning frame-\nworks. Then we introduce two key components in our RePre:\nmulti-hierarchy features and a lightweight convolutional de-\ncoder (Fig. 2). Finally, we introduce the overall loss function\nof RePre.\n3.1 Revisiting Contrastive Learning Frameworks\nThe primary focus of contrastive learning is to learn image\nembeddings that are invariant to different augmented views\nof the same image while being discriminative among differ-\nent images. This is typically achieved by maximizing the\nsimilarity of representations obtained from different distorted\nversions of a sample using a variant of Siamese networks.\nAs shown in the bottom part of Fig. 1: a Siamese network\nis composed of two branches: an online branch and a target\nbranch, where the target branch keeps an Exponential Mov-\ning Average (EMA) of the online branch [Chen et al., 2021;\nXie et al., 2021a; Caron et al., 2021] or shares weights with\nthe online branch [Chen et al., 2020b; Chen and He, 2021 ]\n(not shown in Fig. 1). In particular, each branch encodes an\naugmented view to a single feature vector in the embedding\nspace, resulting in a level of a global feature.\nIn order to better prove the scalability and effectiveness of\nour RePre in arbitrary contrastive learning frameworks, we\nroughly split current contrastive frameworks into two types:\nmethods with negative samples, e.g., MoCo v3, SimCLR and\nmethods without negative samples, e.g., BYOL, SimSiam.\nMethods with negative samples contrast positive samples\nwith negative samples to prevent trivial solutions, i.e., all\nthe outputs collapsing into constant. Specifically, augmented\nviews created from the same samples are considered positive\npairs, and images from different samples are considered neg-\native pairs. The target branch outputs the representation of a\npositive sample and a set of negative samples, and the loss ex-\nplicitly pulls the pair of positive samples together while push-\ning apart the pair of negative samples. The loss function can\nbe thought of as a K + 1way softmax:\nLcontrast w neg = −log exp(q ·k+/τ)PK\ni=0 exp(q ·ki/τ)\n(1)\nWhere k+ is the target feature on the same image as q, called\nthe positive sample of q. ki is a target feature of negative\nsample; τ is a temperature term; K is the queue or batch size.\nMethods without negative samples only rely on positive\nsamples. They introduce asymmetric architecture to prevent\ncollapse. In particular, it appends a multi-layer perception\nas predictor to the encoder of the online branch, and it stops\nthe gradient through the target branch. In this case, the loss\nexplicitly pulls together the positive sample pairs, and the ob-\njective function is the negative cosine similarity between the\ntwo augmented views. Given the output of the online predic-\ntor p1 and the output of the target branch z2, the objective\nfunction is:\nLcontrast w/o neg = −< p1\n∥p1∥2\n, z2\n∥z2∥2\n> (2)\nWhere < ·, ·> denotes the inner product operator.\n3.2 Reconstruction with Multi-hierarchy Features\nFollowing the practice of ViT, we split theH ×W ×3 shape\nimage with patch size P. After patch embedding and lin-\near projection, we get z0 ∈R(N+1)×C, the sequential fea-\nture of an image, where N = H\nP ∗W\nP . The additional\n1 denotes the class token, C is the number of channels.\nThe sequential feature would iterate all L transformer blocks\nwithin the encoder. We denote the output tokens of each\nblock as {z1, z2, ...,zL}. In contrastive learning, z0\nL serves\nas the global image representation. For simplicity, we denote\nz excluding z0 as y, which stands for the representations of\npatches. For Swin Transformer, because there is no class\ntoken, we get y0 ∈ RN×C after patch embedding. Swin\nTransformer also has patch merging layers, which reduce the\nnumber of token by 1\n2 and increase the feature dimension by\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1439\n2×. The output embeddings of the last stage are averaged\nby a global average pooling layer and then sent to a linear\nclassifier for classification, unlike class token.\nOur initial trial is feeding the output of the last transformer\nblock yL into the reconstruction decoder. However, it turns\nout this simple combination only brings marginal improve-\nments (see Sec. 4.3). We argue that this ineffectiveness lies\nin the discrepancy between the last layer’s high semantic fea-\ntures and the low semantic pixel objective. Inspired by U-\nNet shape, we collect low-to-high semantic features from\nshallow-to-deep blocks and reconstruct raw pixels gradually.\nGiven a vanilla ViT with L transformer blocks, we sample\nK (K < L) hierarchical features with even intervals, i.e.,\nour multi-hierarchy feature Y = {y⌊ L\nK ⌋, y⌊ L\nK ⌋∗2, ...,yL},\nwhere ⌊·⌋is the floor function. K = 4 is a standard prac-\ntice in this paper. For a L = 12 ViT-S, we sample Y =\n{y3, y6, y9, y12}as multi-hierarchy feature. For Swin Trans-\nformer, which has downsampling operators, we can also get\nthe multi-hierarchy feature. We directly sample the last fea-\nture of each resolution stage.\n3.3 Lightweight Reconstruction Decoder\nWith the multi-hierarchy feature, our decoder gradually inte-\ngrates the deep-to-shallow features and regresses to predict\nthe original RGB pixels directly with a simple L1 loss(see\nFig. 2). Surprisingly, we find that a lightweight convolutional\ndecoder works pretty well (see Sec. 4.3), e.g., one or two fu-\nsion layers in each decoder block. A fusion layer consisits a\n3 ×3 convolution layer and a ReLU layer. In order to coop-\nerate with the convolutional operator, the sequential feature\ny ∈RN×C is reshaped to 2D feature x ∈R\nH\nP ×W\nP ×C. Like\nin U-Net, the shallow feature is merged into deep feature by\nconcatenation, resulting in a feature with shape H\nP ×W\nP ×2C.\nIn order to fuse the multi-hierarchy feature, our reconstruc-\ntion decoder consists of fusion layers in each K −1 block\n(details in Fig. 2). To predict all pixel values at a full reso-\nlution of input images, we apply a 1 ×1 convolution layer\nto map each feature vector in the final output of the decoder\nback to the original resolution. We let this vector take charge\nof the prediction of corresponding raw pixels. Then, we apply\na simple L1 loss between the original image and the decoder\noutput. In summary, the reconstruction objective is:\nLreconstruct = |img −decoder(Y)| (3)\nWhere |·| is theL1 loss, img is the augmented view before\nnormalization, Y is the multi-hierarchy feature, decoder(·)\nreturns the reconstructed image.\nOur decoder is also compatible with hierarchical vision\ntransformers as Swin Transformer. Because of downsam-\npling, we cannot directly concatenate the deep low-resolution\nfeature with the shallow high-resolution feature. Thus, we ap-\nply a bilinear interpolation upsampling operation on the deep\nfeature to make the alignment.\n3.4 Overall Loss of RePre\nOur RePre is optimized with contrastive loss and reconstruc-\ntive loss, which simultaneously learns the global features\nand fine-grained local features. The contrastive loss func-\ntion is consistent with the contrastive learning method we\nAlgorithm 1Pseudo code of RePre in a PyTorch-like style\n1: # reconst dec: convolutional reconstruction decoder\n2: # online enc, target enc: transformer-based encoder\n3: # online net = online enc + projector + predictor, predic-\ntor is None for symmetric methods\n4: # target net = target enc + projector\n5: for x in loader: do\n6: v1, v2 = aug(x), aug(x) # augmentation\n7: # # # Reconstructive pre-training # # #\nmulti\nhierarchy feats = online encoder(v1)\n8: reconst v = reconst dec(multi hierarchy feats) # re-\nconst v with shape H ∗W ∗3\n9: reconstruction loss = L1 loss(reconst v, v1) # Details\nin Sec. 3.3\n10: # # # Contrastive pre-training # # #\nq1, q2 = online\nnet(v1), online net(v2) # [N,C] each\n11: k1, k2 = target net(v1), target net(v2) # [N,C] each\n12: contrast loss = ctr loss(q1, k2) + ctr loss(q2, k1) #\nDetails in Sec. 3.1\n13: # # # Combining objectives # # #\nloss = λ1contrast\nloss + λ2reconstruction loss\n14: loss.backward()\n15: update(online net, reconst dec)\n# target net is optimized by EMA or gradient\n16: end for\nuse (details in Sec. 3.1). The reconstruction loss function\ncomputes the mean absolute error between the reconstructed\nand original images in pixel space (details in Sec. 3.3). We\nuse a weighted sum of these two loss functions as our over-\nall loss. To avoid expensive calculation by optimizing the\nweights through the grid search method, we incorporate the\nuncertainty weighting approach proposed by [Kendall et al.,\n2018]. In particular, each task is weighted by a function of\nits homoscedastic aleatoric uncertainty rather than by a fixed\nweight. The overall loss function is as follows:\nL= λ1Lcontrast + λ2Lreconstruct (4)\nWhere λ1,λ2 are learnable parameters.\n4 Experiments\nOur RePre is general and can be plugged into arbitrary con-\ntrastive learning frameworks with various vision transformer\narchitectures. We first study the linear evaluation of the im-\nage recognition task. Then we transfer our pre-trained models\ninto downstream object detection and semantic segmentation\ntasks. Finally, we do detailed ablation studies about the key\ncomponents of our RePre.\n4.1 Linear Evaluation\nLinear evaluation on ImageNet-1K dataset is a standard eval-\nuation protocol to assess the quality of learned representa-\ntions. After pre-training, we add a linear layer on the top of\nthe network. We only train this linear layer while fixing the\npre-trained network.\nFig. 3 and Table 1 list the apparent performance improve-\nments that our RePre brings to different advanced compara-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1440\n68\n70\n72\n74\n76\n78\nDINO MoBY\nMOCO V3MOCO V2\nBYOLSimCLR\nmethods under ViT-S\n68\n70\n72\n74\n76\n78\nDINO\nMOCO V3\nBYOLSimCLR\nmethods under ViT-B\n68\n70\n72\n74\n76\n78\nMoBY\nMOCO V3\nImageNet Top-1 Accuracy(%)\nbase methods base methods with RePre\nmethods underSwin-T\nFigure 3: Performance improvements brought by RePre when using\ndifferent contrastive learning frameworks and network architectures.\nBase Arch. Epoch Acc% Acc w/ RePre\nSimCLR 300 69.0 69.7(↑0.7)\nBYOL 300 71.4 72.2(↑0.8)\nMOCO v2 300 71.6 72.1(↑0.5)\nMOCO v2 800 72.7 73.4(↑0.7)\nMOCO v3 ViT-S 300 72.5 73.2(↑0.7)\nMOCO v3 600 73.4 73.9(↑0.5)\nMoBY 300 72.8 73.9(↑1.1)\nDINO 300 75.9 76.7(↑0.8)\nDINO 800 77.0 77.9(↑0.9)\nSimCLR 300 73.9 74.4(↑0.5)\nBYOL 300 73.9 74.8(↑0.9)\nMOCO V3 ViT-B 300 76.5 77.2(↑0.7)\nMOCO V3 600 76.7 77.5(↑0.8)\nDINO 800 78.2 79.2(↑1.1)\nMOCO v3 300 75.4 ∗ 76.4(↑1.0)\nMoBY Swin-T 100 70.9 71.8(↑0.9)\nMoBY 300 75.0 76.1(↑1.1)\nTable 1: More linear evaluation results on ImageNet benchmark.\nOur RePre brings consistent gains under different methods, archi-\ntectures and training epochs. ∗ is our reimplementation.\ntive learning methods based on different backbone architec-\ntures. Our pre-training and fine-tuning recipes are basically\nthe same as the contrastive learning methods. Since our re-\nconstruction decoder is lightweight, our RePre only brings a\nnegligible average of 4% workload. All our experiments are\nconducted on NVIDIA V100 GPUs.\n4.2 Transfer to Downstream Tasks\nWe further evaluate the transferring performance of the\nlearned representations on downstream tasks of COCO ob-\nject detection/instance segmentation and Cityscapes semantic\nsegmentation.\nObject Detection and Instance Segmentation\nWe perform object detection/instance segmentation experi-\nments on COCO with Mask R-CNN [He et al., 2017] frame-\nwork. Following standard practice, we use AdamW optimizer\nand 1×schedule. The shorter edges of the input images are\nresized to 800 while the longer side is at most 1333. To com-\npare with advanced research results, we use Swin-T as the\nMethod w/ RePre Epoch mAP bbox mAPmask\nIN Sup. − 100 41.6 38.4\n− 300 43.7 39.8\n× 100 41.5 38.3\nMoBY ✓ 100 42.1(↑0.6) 39.2(↑0.8)\n× 300 43.6 39.6\n✓ 300 44.8(↑1.2) 40.3(↑0.7)\nDINO × 100 42.2 38.7\n✓ 100 43.1(↑0.9) 40.0(↑1.3)\nTable 2: Results of object detection and instance segmentation fine-\ntuned on MS COCO. We use Mask R-CNN framework with Swin-T\nas the backbone. Our RePre models outperform supervised Ima-\ngeNet pre-training and self-supervised DINO with a decent margin.\nMethod Arch. mIoU mAcc(%)\nIN Supervised 71.33 80.30\nDINO ViT-S 72.96 81.32\nDINO w/ RePre 73.40 81.95\nTable 3: Results of semantic segmentation fine-tuned on Cityscapes.\nWe use SETR framework with ViT-S as the backbone. All the ViT-S\nmodelsare all pre-trained for 300 epochs.\nbackbone. As shown in Table 2, the performance of MoBY\nwith RePre is improved by 1.2% and 0.7% under the same\npre-training settings. Similarly, our RePre brings effective\nperformance gains of 0.9% and 1.3% for DINO.\nSemantic Segmentation\nWe adopt the SETR [Zheng et al., 2021] as the semantic seg-\nmentation strategy on Cityscapes and follow the training con-\nfig as original SETR. For a fair comparison, we use pretrained\nmodels based on ViT-S by 300-epoch. As shown in Table 3,\nDINO with RePre achieves the highest mIoU 73.40% and\nmAcc 81.95%. It outperforms both supervised and DINO\npretrained results. It proves that reconstruction pretraining\nextracts finer local-level information and is suitable to trans-\nfer for semantic segmentation tasks.\n4.3 Ablation Study\nTwo key components of our RePre are multi-hierarchy fea-\ntures and reconstruction decoder. In this part, we perform\ndetailed ablation studies on these two components. Without\nspecification, we use MOCO v3 as the contrastive learning\nframework and the pre-training epoch is 300.\nAblation on Multi-hierarchy Features\nTab. 4 shows the impact of multi-hierarchy features on per-\nformance with the our default convolutional decoder. As we\ncan see, using the ‘Single’ feature (last layer’s output) only\nbrings marginal improvements. We argue that this ineffec-\ntiveness lies in the discrepancy between the last layer’s high\nsemantic features and the low semantic pixel objective. Using\nmulti-hierarchy features (dubbed ‘Multi’), RePre improves\nMOCO v3 top-1 accuracy performance by 0.7% under DeiT-\nS and 1.0% under Swin-T. RePre also improves MoBY base-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1441\nMethod Arch. Single Multi Top-1 Acc(%)\nMOCO v3 – – 72.5\nMOCO v3 ✓ 72.8\nMOCO v3 ViT-S ✓ 73.2\nMoBY – – 72.8\nMoBY ✓ 73.1\nMoBY ✓ 73.9\nMOCO v3 – – 75.4\nMOCO v3 ✓ 75.7\nMOCO v3 Swin-T ✓ 76.4\nMoBY – – 75.0\nMoBY ✓ 75.4\nMoBY ✓ 76.1\nTable 4: Ablation study on positive impact of multi-hierarchy fea-\ntures. ‘Single’ and ’Multi’ denotes using the last layer output fea-\ntures or using the fused multi-hierarchy features respectively.\nOperator Layer Num. Arch. Top-1 Acc(%)\nw/o decoder – 72.5\nConv 1 73.0\nConv 2 73.2\nConv 4 ViT-S 73.2\nTransformer 1 71.8\nTransformer 2 72.0\nTransformer 4 71.4\nw/o decoder – 75.4\nConv 1 76.1\nConv 2 76.4\nConv 4 Swin-T 76.2\nTransformer 1 74.6\nTransformer 2 75.2\nTransformer 4 74.5\nTable 5: Ablation study on the fusion layer in reconstruction de-\ncoder. Operator and layer number denotes the type and the number\nof fusion layers.\nline top-1 accuracy performance by 1.1% under DeiT-S or\nSwin-T. We conjecture that multi-hierarchy features contains\ndifferent level of semantic information which is crucial for\nthe reconstruction.\nWe also show the comparison via showing the attention\nmap and t-SNE in Fig. 4. As we can see from the left\npart, when using multi-hierarchy features, models can iden-\ntify the edge area of the object more accurately and high-\nlight the core focus. The phoneme can also explain the ex-\ncellent performance of our RePre when transferring to detec-\ntion and segmentation tasks. The right part in Fig. 4 shows t-\nSNE [Van der Maaten and Hinton, 2008] visualization results.\nObviously, the learned representations with multi-hierarchy\nfeatures can be better divided into different categories.\nAblation on Fusion Layer in Reconstruction Decoder\nWe analyze that convolution can enhance the fine-grained lo-\ncal spatial correlation without damaging context semantics\ninformation. We validate it using the same basic Transformer\nreconstruction with \nmulti-hierarchy features\nof the transformer encoder\nreconstruction with\nthe last output feature \nof the transformer encoder\nreconstruction with \nmulti-hierarchy features\nof the transformer encoder\nreconstruction with\nthe last output feature \nof the transformer encoder\nreconstruction with\nSingle features\nreconstruction with\nMulti-hierarchy featuresSingle MultiInput\nFigure 4: Left part: The visualization of attention map. The first\ncolumn is original images. The second and the third column show\nclass token’s attentions when using last layer’s or multi-hierarchy\nfeatures.When using multi-hierarchy features, models can identify\nthe edge area of the object more accurately and highlight the core\nfocus. Right part: The visualization of t-SNE on ImageNet. We\nrandomly select 20 classes in the validation set. Each point repre-\nsents embedding from online transformer encoder.\nlayer as the backbone to replace the3 ×3 convolution (Conv)\nin the decoder fusion layer. Table 5 shows the positive ef-\nfect brought by convolution. ‘Layer Num’ represents the\nrepetition times of fusion layer. With a lightweight convolu-\ntional decoder, RePre improves baseline top-1 accuracy per-\nformance by 0.7% with ViT-S and 1.0% with Swin-T, which\ncould be a strong proof of our hypothesis. The results also\nvalidate our analysis that the heavy convoltion or transformer\nreconstruction decoder would focus too much on low seman-\ntic information, thus harming representation learning.\n5 Conclusion\nThis work proposes a simple yet effective objective: Re-\nconstructive Pre-training (RePre) from raw RGB pixels to\ntrain self-supervised vision transformers. Our RePre ex-\ntends contrastive frameworks by adding a branch for recon-\nstructing raw image pixels in parallel with the existing con-\ntrastive objective. RePre incorporates local feature learning\nwith a lightweight convolution-based decoder that fuses the\nmulti-hierarchy features from the transformer encoder. Our\nRePre is generic and can improve arbitrary contrastive learn-\ning frameworks with negligible additional cost. With the\nfact that self-supervised learning in CV was dominated by\ncontrastive objectives in the past several years, we hope our\nRePre could bring more insights into reconstructive (genera-\ntive) objectives in this area.\nAcknowledgments\nThis work was funded by National Natural Science Founda-\ntion of China under Grant No.62076034. We would like to\nthank Lan Yang, Kaiyue Pang and Yi-Zhe Song for the help\nof our work.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1442\nReferences\n[Ahn and Kwak, 2018] Jiwoon Ahn and Suha Kwak. Learn-\ning pixel-level semantic affinity with image-level super-\nvision for weakly supervised semantic segmentation. In\nProc. of CVPR, pages 4981–4990, 2018.\n[Bao et al., 2021] Hangbo Bao, Li Dong, and Furu Wei.\nBeit: Bert pre-training of image transformers. arXiv\npreprint arXiv:2106.08254, 2021.\n[Caron et al., 2021] Mathilde Caron, Hugo Touvron, Ishan\nMisra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vi-\nsion transformers. arXiv preprint:2104.14294, 2021.\n[Chen and He, 2021] Xinlei Chen and Kaiming He. Explor-\ning simple siamese representation learning. In Proc. of\nCVPR, pages 15750–15758, 2021.\n[Chen et al., 2020a] Mark Chen, Alec Radford, Rewon\nChild, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In Proc.\nof ICML, pages 1691–1703. PMLR, 2020.\n[Chen et al., 2020b] Ting Chen, Simon Kornblith, Moham-\nmad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. InICML,\npages 1597–1607. PMLR, 2020.\n[Chen et al., 2020c] Xinlei Chen, Haoqi Fan, Ross Girshick,\nand Kaiming He. Improved baselines with momentum\ncontrastive learning. arXiv preprint:2003.04297, 2020.\n[Chen et al., 2021] Xinlei Chen, Saining Xie, and Kaiming\nHe. An empirical study of training self-supervised vision\ntransformers. arXiv preprint arXiv:2104.02057, 2021.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Grill et al., 2020] Jean-Bastien Grill, Florian Strub, Flo-\nrent Altch ´e, Corentin Tallec, Pierre H Richemond, Elena\nBuchatskaya, Carl Doersch, Pires, et al. Bootstrap your\nown latent: A new approach to self-supervised learning.\narXiv preprint arXiv:2006.07733, 2020.\n[He et al., 2017] Kaiming He, Georgia Gkioxari, Piotr\nDoll´ar, and Ross Girshick. Mask r-cnn. In Proc. of ICCV,\npages 2961–2969, 2017.\n[He et al., 2021] Kaiming He, Xinlei Chen, Saining Xie,\nYanghao Li, Piotr Doll ´ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint\narXiv:2111.06377, 2021.\n[Hsu et al., 2021] Wei-Ning Hsu, Benjamin Bolte, Yao-\nHung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhut-\ndinov, and Abdelrahman Mohamed. Hubert: Self-\nsupervised speech representation learning by masked pre-\ndiction of hidden units. arXiv preprint:2106.07447, 2021.\n[Kendall et al., 2018] Alex Kendall, Yarin Gal, and Roberto\nCipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proc. of\nCVPR, pages 7482–7491, 2018.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon\nChild, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[Ramesh et al., 2021] Aditya Ramesh, Mikhail Pavlov,\nGabriel Goh, Scott Gray, Chelsea V oss, Alec Radford,\nMark Chen, and Ilya Sutskever. Zero-shot text-to-image\ngeneration. arXiv preprint arXiv:2102.12092, 2021.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In MICCAI, pages\n234–241. Springer, 2015.\n[Sun et al., 2017] Chen Sun, Abhinav Shrivastava, Saurabh\nSingh, and Abhinav Gupta. Revisiting unreasonable effec-\ntiveness of data in deep learning era. In Proc. of ICCV,\npages 843–852, 2017.\n[Van der Maaten and Hinton, 2008] Laurens Van der Maaten\nand Geoffrey Hinton. Visualizing data using t-sne. JMLR,\n9(11), 2008.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998–6008, 2017.\n[Xie et al., 2021a] Zhenda Xie, Yutong Lin, Zhuliang Yao,\nZheng Zhang, Qi Dai, Yue Cao, and Han Hu. Self-\nsupervised learning with swin transformers.arXiv preprint\narXiv:2105.04553, 2021.\n[Xie et al., 2021b] Zhenda Xie, Zheng Zhang, Yue Cao, Yu-\ntong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu.\nSimmim: A simple framework for masked image model-\ning. arXiv preprint arXiv:2111.09886, 2021.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,\nJianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethink-\ning semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proc. of CVPR, pages\n6881–6890, 2021.\n[Zhou et al., 2021] Jinghao Zhou, Chen Wei, Huiyu Wang,\nWei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot:\nImage bert pre-training with online tokenizer. arXiv\npreprint arXiv:2111.07832, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1443",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7312209606170654
    },
    {
      "name": "Computer science",
      "score": 0.7224019765853882
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6267347931861877
    },
    {
      "name": "Feature learning",
      "score": 0.5133696794509888
    },
    {
      "name": "Machine learning",
      "score": 0.3743130564689636
    },
    {
      "name": "Engineering",
      "score": 0.14554131031036377
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210128910",
      "name": "Group Sense (China)",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    }
  ]
}