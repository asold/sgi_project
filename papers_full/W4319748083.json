{
  "title": "Detection of oral squamous cell carcinoma in clinical photographs using a vision transformer",
  "url": "https://openalex.org/W4319748083",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2025525175",
      "name": "Tabea Flügge",
      "affiliations": [
        "Einstein Center Digital Future",
        "Humboldt-Universität zu Berlin",
        "Freie Universität Berlin",
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2679023597",
      "name": "Robert Gaudin",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Charité - Universitätsmedizin Berlin",
        "Freie Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2996746740",
      "name": "Antonis Sabatakakis",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Freie Universität Berlin",
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2095179269",
      "name": "Daniel Tröltzsch",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin",
        "Freie Universität Berlin",
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2075385092",
      "name": "Max Heiland",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin",
        "Humboldt-Universität zu Berlin",
        "Freie Universität Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4309189065",
      "name": "Niels van Nistelrooij",
      "affiliations": [
        "Radboud University Medical Center",
        "Radboud University Nijmegen"
      ]
    },
    {
      "id": "https://openalex.org/A2951566615",
      "name": "Shankeeth Vinayahalingam",
      "affiliations": [
        "Radboud University Medical Center",
        "Radboud University Nijmegen"
      ]
    },
    {
      "id": "https://openalex.org/A2025525175",
      "name": "Tabea Flügge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2679023597",
      "name": "Robert Gaudin",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2996746740",
      "name": "Antonis Sabatakakis",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A2095179269",
      "name": "Daniel Tröltzsch",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075385092",
      "name": "Max Heiland",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4309189065",
      "name": "Niels van Nistelrooij",
      "affiliations": [
        "Radboud University Medical Center",
        "Radboud University Nijmegen"
      ]
    },
    {
      "id": "https://openalex.org/A2951566615",
      "name": "Shankeeth Vinayahalingam",
      "affiliations": [
        "Radboud University Medical Center",
        "Radboud University Nijmegen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3128646645",
    "https://openalex.org/W1982019987",
    "https://openalex.org/W4324115179",
    "https://openalex.org/W2122935174",
    "https://openalex.org/W3171108477",
    "https://openalex.org/W2536483989",
    "https://openalex.org/W2786825840",
    "https://openalex.org/W2152630948",
    "https://openalex.org/W2062343956",
    "https://openalex.org/W2113700988",
    "https://openalex.org/W2592929672",
    "https://openalex.org/W6734494589",
    "https://openalex.org/W3201570765",
    "https://openalex.org/W2896469379",
    "https://openalex.org/W3088255189",
    "https://openalex.org/W3172244198",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W1973767902",
    "https://openalex.org/W2145054292",
    "https://openalex.org/W2070151882",
    "https://openalex.org/W3212369424",
    "https://openalex.org/W3044073403",
    "https://openalex.org/W3107263686",
    "https://openalex.org/W3210593880",
    "https://openalex.org/W2150473887"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports\nDetection of oral squamous cell \ncarcinoma in clinical photographs \nusing a vision transformer\nTabea Flügge 1,3*, Robert Gaudin 1, Antonis Sabatakakis 1, Daniel Tröltzsch 1, Max Heiland 1, \nNiels van Nistelrooij 2 & Shankeeth Vinayahalingam 2\nOral squamous cell carcinoma (OSCC) is amongst the most common malignancies, with an estimated \nincidence of 377,000 and 177,000 deaths worldwide. The interval between the onset of symptoms \nand the start of adequate treatment is directly related to tumor stage and 5-year-survival rates of \npatients. Early detection is therefore crucial for efficient cancer therapy. This study aims to detect \nOSCC on clinical photographs (CP) automatically. 1406 CP(s) were manually annotated and labeled \nas a reference. A deep-learning approach based on Swin-Transformer was trained and validated on \n1265 CP(s). Subsequently, the trained algorithm was applied to a test set consisting of 141 CP(s). \nThe classification accuracy and the area-under-the-curve (AUC) were calculated. The proposed \nmethod achieved a classification accuracy of 0.986 and an AUC of 0.99 for classifying OSCC on clinical \nphotographs. Deep learning-based assistance of clinicians may raise the rate of early detection of oral \ncancer and hence the survival rate and quality of life of patients.\nOral squamous cell carcinoma (OSCC) is among the most common malignancies worldwide, with a reported \nincidence of 377,713 and 117,757 deaths in  20201. The five-year survival rate is over 80% in the early stages, \ndecreasing to < 30% for advanced disease. More than 60% of the OSCCs are diagnosed at an advanced stage \nwith high morbidity and  mortality2–4. The incidence and mortality rates underline the importance of oral cancer \nscreening programs to improve early detection and therapeutic  success5–7.\nAlthough the golden standard is pathologically proven, early detection can be achieved visually as OSCCs \nstart superficially from squamous cell metaplasia. Nonetheless, the diagnostic accuracy of primary health care \nprofessionals is limited, with a sensitivity of 57.8% and a specificity of between 31 and 53%8,9. The lack of adequate \ntraining, substantial heterogeneity, and the lack of experience impede an effective diagnosis by primary health \ncare  professionals6,10. An automated assistance system may improve the diagnostic accuracy, allowing a more \nreliable and accurate assessment of the oral cavity, especially in the hands of less experienced professionals.\nWith advancements in artificial intelligence, deep learning algorithms have been adopted in computer-aided \ndetection and diagnosis (CAD). Mainly convolutional neural networks (CNN) have emerged as the state-of-\nthe-art approach to medical image analysis. CNN’s utilize convolutional kernels with small perceptual fields \nto extract features via weight sharing and local  connectivity11. Recently, transformers have been introduced as \nan alternative approach to CNNs. Transformers are based on an attention mechanism that efficiently estimates \neach pixel-pair  interplay12.\nIn oral and maxillofacial surgery, few studies have explored the capability of CNNs to automatically clas-\nsify OSCC on clinical photographs. These studies addressed the classification and detection of oral potentially \nmalignant  diseases13 and oral cancer  lesions5,14,15 using YOLOv5, ResNet-152, DensNet-161, Inception-v4 and \nEfficientNet-b4, respectively.\nHowever, none of the studies has explored the accuracy of vision transformers for classifying OSCC. This \nstudy aims to develop an automated oral cancer screening system using vision transformers as a fundamental \nbasis for a timely and accurate referral system.\nOPEN\n1Department of Oral and Maxillofacial Surgery, Charité – Universitätsmedizin Berlin, Corporate Member \nof Freie Universität Berlin and Humboldt Universität zu Berlin, Augustenburger Platz 1, 13353 Berlin, \nGermany. 2Department of Oral and Maxillofacial Surgery, Radboud University Nijmegen Medical Centre, \nP .O. Box 9101, 6500 HB Nijmegen, The Netherlands. 3Einstein Center for Digital Future, Wilhelmstraße 67, \n10117 Berlin, Germany. *email: tabea.fluegge@charite.de\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\nMaterial and methods\nData. In the present study, 1406 clinical photographs (CPs) were randomly collected from the Department \nof Oral and Maxillofacial Surgery, Charité - Universitätsmedizin Berlin, Germany (mean age of 60.8 years, age \nrange of 15–90 years). The photographs were acquired with single lens reflex (SLR) cameras with varying light \nexposures. The image resolution was a minimum of 72 dpi. CPs with masked lesions or foreign bodies (e.g., \nmirrors or tongue depressors) were excluded from further analyses as described in a previous  study16. All image \ndata were anonymized and de-identified before analysis. Informed consent for the analysis of data was obtained \nfrom all patients or their guardians in case of age below 18 years. This study has been conducted in accordance \nwith the code of ethics of the World Medical Association (Declaration of Helsinki). The approval of this study \nwas granted by the Institutional Review Board, the Ethics Committee of Charité – Universitätsmedizin Berlin \n(EA2/089/22).\nData classification. Different clinicians verified all CPs based on electronic medical records (EMR). CPs \nwith OSCCs needed to be biopsy-proven. All CPs were subsequently reviewed and revised by three clinicians \n(RG, DT, TF). The three reviewers have at least five years of clinical experience. Each clinician was instructed and \ncalibrated in the verification task using a standardized protocol before the selection and reviewing process. The \nfinal dataset consisted of 703 CPs of OSCC and 703 CPs of normal oral mucosae (Table 1).\nThe normal tissue dataset comprised photographs of the oral cavity without premalignant oral mucosal \nlesions or oral cancer. A further selection of the dataset to exclude possible anatomical variations or inflamma-\ntory conditions of the gingiva or mucosa was not performed.\nThe OSCC training dataset contained images of various tumor stages, including Tis (1%), T1 (28.9%), T2 \n(27.2%), T3 (16.3%), T4 (21.6%) and unknown tumor stages (5%). The locations were tongue (36.9%), floor \nof mouth (29.6%), maxilla (3.2%), mandible (15.8%), buccal mucosa (13.6%), palate (0.7%) and oropharynx \n(0.3%). The test and validation data sets contained a comparable distribution of tumor stages and locations with \na maximum deviation of 10% from the training dataset.\nTable 1.  Baseline characteristics of the malignant pathologies. Multiple locations of extended lesions possible \n(*).\nEntity Number of images Percentage\nOSCC 638 90.7\nVerrucous SCC 23 3.3\nSarcomatoid SCC 4 0.6\nCarcinoma in situ 13 1.9\nOSCC (clinical) 25 3.5\nGender\n Male 436 62\n Female 267 38\nLocation*\n Tongue 258 36.91\n Floor of mouth 208 29.64\n Maxilla 22 3.17\n Mandible 111 15.75\n Buccal mucosa 95 13.61\n Palate 7 0.65\n Oropharynx 2 0.28\nStaging\n Tis 7 1.06\n T1 203 28.9\n T2 191 27.2\n T3 115 16.3\n T4 152 21.6\n T unknown 35 5.0\nGrading\n G1 102 14.5\n G2 386 55\n G3 143 20.3\n G4 3 0.5\n G unknown 69 9.7\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\nThe model. The Swin-Transformer was used in this  study17. This transformer is characterized by its shift of \nthe window partition between consecutive self-attention layers. The shifted windows connect with preceding \nlayers’ window, increasing the modelling power efficiently. The employed model is shown in Fig. 1.\nModel training. The annotated were randomly divided into 3 sets of CPs, 1124 for training, 141 for valida-\ntion and 141 for testing. The validation set was used to evaluate the model performance during training, while \nthe hold-out test set was used to evaluate the model performance after training.\nThe Swin-Transformer was pre-trained on the ImageNet dataset and optimized using a stochastic gradient \ndescent with a learning rate of 5 ×  10–3, a momentum of 0.9 and a weight decay of 1 ×  10–4. No gradient clipping \nwas applied. The model was implemented in PyTorch 1.11.0 and trained on a 12 GB NVIDIA TITAN V GPU. \nModel training was previously described in a study on caries detection  radiographs16.\nStatistical analysis. The transformer predictions on the test set were compared to the histopathological \nground truth. Classification metrics are reported as follows for the test set: accuracy = TP+TN\nTP+TN +FP+FN , positive \npredictive value = TP\nTP+FP , F1-score = 2TP\n2TP+FP+FN , sensitivity = TP\nTP+FN , specificity = TN\nTN +FP , negative predic-\ntive value = TN\nTN +FN . TP , TN, FP , and FN denote true positives, true negatives, false positives, and false negatives, \nrespectively. Furthermore, the area-under-the-curve-receiver-operating-characteristics-curve (AUC) and con-\nfusion matrix are presented. Gradient-weighted Class Activation Mapping (Grad-CAM), a class-discriminative \nlocalization technique was applied, to generate visual explanations highlighting the important regions on CPs for \nclassifying OSCC. Statistical analysis was performed as in a previous  study16.\nResults\nTable 2 summarizes the classification performance of the Swin-Transformer on the test set, including the accu-\nracy, positive predictive value, sensitivity, specificity and negative predictive value. The classification accuracy \nwas 98,6%. The model achieved an AUC of 0.99 (Fig. 2). The confusion matrix is presented in Fig. 3.\nThe class activation heatmaps (CAM) of OSCC and normal oral mucosae are illustrated in Figs.  4 and 5. \nThese heatmaps visualize the discriminative regions used by the Swin-Transformer for the classification. Opti -\ncal inspection indicates a more centered and focused region of interest for OSCC. For normal mucosa, either a \nblank heatmap without any focus or a widely distributed focus was noticed.\nDiscussion\nOral squamous cell carcinoma is a common malignancy with overall high mortality and  morbidity2–4. The lack of \nexperience and training of primary health care professionals leads to diagnostic delays and consequently to more \nextensive surgical procedures with more extended hospitalization and lower survival  rates18–20. An automated \nassistance system for the clinician may increase the diagnostic accuracy while reducing the observer dependency. \nFigure 1.  Swin-transformer network.\nTable 2.  The Accuracy, positive predictive value (PPV), F1-score, sensitivity, specificity and negative \npredictive value (NPV) for the detection of OSCC on CP .\nAccuracy PPV F1-score Sensitivity Specificity NPV\n0.9858 0.9857 0.9857 0.9857 0.9859 0.9859\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\nThe vision transformer network introduced in this study is an accurate tool for the classification of OSCC based \non clinical photographs that are most often acquired for documentation purposes.\nIn the past, diagnostic methods such as vital staining, autofluorescence and chemiluminescence, narrow \nband imaging, and optical spectroscopy have been introduced and documented with varying sensitivity and \n specificity21. Vital staining had a sensitivity of 92.3% but most studies did not report the specificity. For autofluo-\nrescence, heterogeneous values of 50–100% for sensitivity and 12.5 and 75.5% for specificity were reported. Nar-\nrow band imaging showed high sensitivity between 84.62 and 93.93%, and specificity between 75.7 and 94.56%.\nDifferent studies have recently applied CNNs to classify oral cancer from an oral photograph. Warin et al. \nreported a F1-score of 0.9875 and an AUC of 0.99 with  DenseNet12113. Fu et al. achieved a similar F1-score \n(0.935–0.995) with a two-step  approach15. In the first step, a Single Shot MultiBox Detector was applied to detect \nthe region of interest. Subsequently, DenseNet assessed the pre-selected region of interest in the presence of \nOSCC. Welikala et al. achieved a significantly lower F1-score of 0.8707 with ResNet-101 22. Shamim et al. com-\npared multiple CNNs (e.g. AlexNet, GoogleNet, Inceptionv3, ResNet50, SqueezeNet and VGG19) to classify \ntongue lesions and achieved F1-scores ranging from 0.9048 to 0.975623.\nHowever, a direct comparison of these previous studies should be regarded with caution. The performance \nof the CNNs is highly dependent on the dataset, the hyperparameters and the architecture  itself24. The number \nof training and test sets varied greatly in the previous studies, and the data representativeness was unclear. Fur-\nthermore, clinical photographs were not standardized, and a high discrepancy was expected in perspective. For \nthese reasons, the replication and validation of the previous results remain impracticable.\nFigure 2.  Area-under-the-curve-receiver-operating-characteristics-curve. The ROC is created by plotting the \ntrue positive against the false positive rate at different thresholds.\nFigure 3.  Confusion matrix illustrating the binary classification results.\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\nIn the current study, the Swin-Transformer achieved a F1-score of 0.98 and an AUC of 0.99. The model had \none false positive prediction and one false negative prediction, independent of location, staging or grading. \nTwo key concepts are essential for high performance: hierarchical feature maps and shifted window attention. \nFirstly, hierarchical feature maps allow the intermediate tensors to be merged from layer to layer, reducing the \nspatial dimension (i.e. downsampling) of the feature maps effectively. In comparison to CNNs, patch merging \nis applied for downsampling instead of convolution operations. Secondly, the Swin-Transformer replaced the \nstandard multi-head self-attention with a window and shifted window self-attention. The standard multi-head \nself-attention performs a global self-attention, resulting in a quadratic complexity. For this reason, the win-\ndow self-attention computes attention only locally within specified windows. The shifted window self-attention \nFigure 4.  Class activation map for OSCC. The left column shows the CP of OSCC. The middle column \nrepresents the class activation map. The right column illustrates the overlay of CP and activation map.\nFigure 5.  Class activation map for normal mucosa. The left column shows the CP of normal mucosa. The \nmiddle column represents the class activation map. The right column illustrates the overlay of CP and activation \nmap.\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\naddresses global information loss using cross-window connections between different layers. These two modules \nreduce the quadratic complexity to linear  complexity17.\nAlthough a high performance was achieved using a transformer, there are limitations. The reported study is \nlimited by its monocentric design resulting in a database consisting of the local population. The photographic \nimages were acquired with high-quality cameras and did not regard clinical settings in which images may be \nacquired with cameras or mobile devices with lower image quality. The Swin-Transformer are strictly confined to \nthe employed train- and test set and may perform worse in real-world scenarios. Prospective studies are required \nto evaluate the diagnostic accuracy of the Swin-Transformer in a clinical setting.\nIn conclusion, the Swin-Transformer forms a promising foundation for further developing automatic screen-\ning of OSCC on clinical photographs. Deep learning-based assistance of clinicians may raise the rate of early \ndetection of oral cancer and hence the survival rate and quality of life of patients.\nData availability\nThe datasets analyzed in the current study are not publicly available due to data protection but are available from \nthe corresponding author on reasonable request.\nReceived: 25 June 2022; Accepted: 31 January 2023\nReferences\n 1. Sung, H. et al. Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 \ncountries. CA Cancer J. Clin. 71(3), 209–249 (2021).\n 2. Ragin, C. C. R., Modugno, F . & Gollin, S. M. The epidemiology and risk factors of head and neck cancer: A focus on human papil-\nlomavirus. J. Dent. Res. 86(2), 104–114 (2007).\n 3. Messadi, D. V ., Wilder-Smith, P . & Wolinsky, L. Improving oral cancer survival: the role of dental providers. J. Calif. Dent. Assoc. \n37(11), 789 (2009).\n 4. Chinn, S. B. & Myers, J. N. Oral cavity carcinoma: Current management, controversies, and future directions. J. Clin. Oncol. 33(29), \n3269 (2015).\n 5. Tanriver, G., Soluk Tekkesin, M. & Ergen, O. Automated detection and classification of oral lesions using deep learning to detect \noral potentially malignant disorders. Cancers 13(11), 2766 (2021).\n 6. Varela-Centelles, P . et al. Key points and time intervals for early diagnosis in symptomatic oral cancer: A systematic review. Int. J. \nOral Maxillofac. Surg. 46(1), 1–10 (2017).\n 7. Coca-Pelaz, A. et al. Head and neck cancer: A review of the impact of treatment delay on outcome. Adv. Ther. 35(2), 153–160 \n(2018).\n 8. Epstein, J. B., Güneri, P ., Boyacioglu, H. & Abt, E. The limitations of the clinical oral examination in detecting dysplastic oral lesions \nand oral squamous cell carcinoma. J. Am. Dent. Assoc. 143(12), 1332–1342 (2012).\n 9. Seoane, J., Warnakulasuriya, S., Varela-Centelles, P ., Esparza, G. & Dios, P . D. Oral cancer: Experiences and diagnostic abilities \nelicited by dentists in North-western Spain. Oral Dis. 12(5), 487–492 (2006).\n 10. Van der Waal, I. Are we able to reduce the mortality and morbidity of oral cancer; some considerations. Med. Oral Patol. Oral Cir. \nBucal. 18(1), e33 (2013).\n 11. Litjens, G. et al. A survey on deep learning in medical image analysis. Med Image Anal. 42, 60–88 (2017).\n 12. Matsoukas, C., Haslum, J. F ., Söderberg, M., & Smith, K. Is it time to replace cnns with transformers for medical images? Preprint \nat https:// arxiv. org/ pdf/ 2108. 09038. pdf (2021).\n 13. Warin, K., Limprasert, W ., Suebnukarn, S., Jinaporntham, S. & Jantana, P . Performance of deep convolutional neural network for \nclassification and detection of oral potentially malignant disorders in photographic images. Int. J. Oral Maxillofac. Surg.  51(5), \n699–704 (2022).\n 14. Song, B. et al. Automatic classification of dual-modalilty, smartphone-based oral dysplasia and malignancy images using deep \nlearning. Biomed. Opt Express 9(11), 5318–5329 (2018).\n 15. Fu, Q. et al. A deep learning algorithm for detection of oral cavity squamous cell carcinoma from photographic images: A retro -\nspective study. EClinicalMedicine 27, 100558 (2020).\n 16. Vinayahalingam, S. et al. Classification of caries in third molars on panoramic radiographs using deep learning. Sci. Rep. 11(1), \n12609 (2021).\n 17. Liu, Z. et al.. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International \nConference on Computer Vision, 10012–10022 (2021).\n 18. Scully, C. & Kirby, J. Statement on mouth cancer diagnosis and prevention. Br. Dent. J. 216(1), 37–38 (2014).\n 19. Conley, B. A. Treatment of advanced head and neck cancer: what Lessons have we learned?. J. Clin. Oncol. 24(7), 1023–1025 (2006).\n 20. Neal, R. D. et al. Is increased time to diagnosis and treatment in symptomatic cancer associated with poorer outcomes? Systematic \nreview. Br. J. Cancer 112(1), 92–107 (2015).\n 21. Mazur, M. et al. In vivo imaging-based techniques for early diagnosis of oral potentially malignant disorders—Systematic review \nand meta-analysis. Int. J. Environ. Res. 18(22), 11775 (2021).\n 22. Welikala, R. A. et al. Automated detection and classification of oral lesions using deep learning for early detection of oral cancer. \nIEEE Access 8, 132677–132693 (2020).\n 23. Shamim, M. Z., Syed, S., Shiblee, M., Usman, M. & Ali, S. Automated detection of oral pre-cancerous tongue lesions using deep \nlearning for early diagnosis of oral cavity cancer. Comput. J. 65(1), 91–104 (2022).\n 24. Vinayahalingam, S. et al. Automated chart filing on panoramic radiographs using deep learning. J. Dent. 115, 103864 (2021).\nAuthor contributions\nT.F . Contributed to conception and design, acquisition, analysis, and interpretation of data; drafted and critically \nrevised the manuscript.R.G.: Contributed to analysis and interpretation and critically revised the manuscript.A.S.: \nContributed to data acquisition, analysis, and interpretation, and critically revised the manuscript.D.T.: Contrib-\nuted to analysis and interpretation and critically revised the manuscript.M.H.: Contributed to conception and \ndesign, drafted and critically revised the manuscript.N.v.N.: Contributed to statistical evaluation, interpretation \nof data, and critically revised the manuscript.S.V .: Contributed to conception, design, data analysis, statistical \nevaluation, and data interpretation and critically revised the manuscript.\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:2296  | https://doi.org/10.1038/s41598-023-29204-9\nwww.nature.com/scientificreports/\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to T.F .\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Basal cell",
  "concepts": [
    {
      "name": "Basal cell",
      "score": 0.7277602553367615
    },
    {
      "name": "Medicine",
      "score": 0.6259180903434753
    },
    {
      "name": "Transformer",
      "score": 0.507485568523407
    },
    {
      "name": "Artificial intelligence",
      "score": 0.464493066072464
    },
    {
      "name": "Oncology",
      "score": 0.3775525689125061
    },
    {
      "name": "Internal medicine",
      "score": 0.3684076964855194
    },
    {
      "name": "Computer science",
      "score": 0.3416607975959778
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I7877124",
      "name": "Charité - Universitätsmedizin Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2802934949",
      "name": "Radboud University Medical Center",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I145872427",
      "name": "Radboud University Nijmegen",
      "country": "NL"
    }
  ],
  "cited_by": 57
}