{
  "title": "MWFormer: Multi-Weather Image Restoration Using Degradation-Aware Transformers",
  "url": "https://openalex.org/W4404687984",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2359992736",
      "name": "Zhu Ruo-xi",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A4222102732",
      "name": "Tu, Zhengzhong",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2120780592",
      "name": "Liu Jiaming",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A4222069071",
      "name": "Bovik, Alan C.",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2368037202",
      "name": "Fan Yi-bo",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4317496742",
    "https://openalex.org/W3215632849",
    "https://openalex.org/W2967584026",
    "https://openalex.org/W3173269149",
    "https://openalex.org/W2884068670",
    "https://openalex.org/W3194523157",
    "https://openalex.org/W2748263833",
    "https://openalex.org/W3118482223",
    "https://openalex.org/W3133769291",
    "https://openalex.org/W2963928582",
    "https://openalex.org/W3168684807",
    "https://openalex.org/W3035713416",
    "https://openalex.org/W4312373367",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W2764207251",
    "https://openalex.org/W4226453196",
    "https://openalex.org/W2963312584",
    "https://openalex.org/W2982795046",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2566376500",
    "https://openalex.org/W3121661546",
    "https://openalex.org/W3086979014",
    "https://openalex.org/W4283023197",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W3176096490",
    "https://openalex.org/W4312678820",
    "https://openalex.org/W3034242291",
    "https://openalex.org/W4312723380",
    "https://openalex.org/W2963800716",
    "https://openalex.org/W3186182384",
    "https://openalex.org/W3103549414",
    "https://openalex.org/W4362653529",
    "https://openalex.org/W4312880823",
    "https://openalex.org/W4386072291",
    "https://openalex.org/W4313192952",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W6640174519",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W4390872297",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W2760103357",
    "https://openalex.org/W2963866045",
    "https://openalex.org/W2950689937",
    "https://openalex.org/W4390873793",
    "https://openalex.org/W4386071513",
    "https://openalex.org/W3201952986",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2102166818",
    "https://openalex.org/W3216056779",
    "https://openalex.org/W4402727345",
    "https://openalex.org/W4399995659",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963920537",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3134429284",
    "https://openalex.org/W3104533206",
    "https://openalex.org/W2187089797"
  ],
  "abstract": "Restoring images captured under adverse weather conditions is a fundamental task for many computer vision applications. However, most existing weather restoration approaches are only capable of handling a specific type of degradation, which is often insufficient in real-world scenarios, such as rainy-snowy or rainy-hazy weather. Towards being able to address these situations, we propose a multi-weather Transformer, or MWFormer for short, which is a holistic vision Transformer that aims to solve multiple weather-induced degradations using a single, unified architecture. MWFormer uses hyper-networks and feature-wise linear modulation blocks to restore images degraded by various weather types using the same set of learned parameters. We first employ contrastive learning to train an auxiliary network that extracts content-independent, distortion-aware feature embeddings that efficiently represent predicted weather types, of which more than one may occur. Guided by these weather-informed predictions, the image restoration Transformer adaptively modulates its parameters to conduct both local and global feature processing, in response to multiple possible weather. Moreover, MWFormer allows for a novel way of tuning, during application, to either a single type of weather restoration or to hybrid weather restoration without any retraining, offering greater controllability than existing methods. Our experimental results on multi-weather restoration benchmarks show that MWFormer achieves significant performance improvements compared to existing state-of-the-art methods, without requiring much computational cost. Moreover, we demonstrate that our methodology of using hyper-networks can be integrated into various network architectures to further boost their performance. The code is available at: https://github.com/taco-group/MWFormer.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nMWFormer: Multi-Weather Image Restoration\nUsing Degradation-Aware Transformers\nRuoxi Zhu, Zhengzhong Tu †, Jiaming Liu, Alan C. Bovik, Life Fellow, IEEE and Yibo Fan ⋆†, Member, IEEE\nAbstract—Restoring images captured under adverse weather\nconditions is a fundamental task for many computer vision appli-\ncations. However, most existing weather restoration approaches\nare only capable of handling a specific type of degradation,\nwhich is often insufficient in real-world scenarios, such as rainy-\nsnowy or rainy-hazy weather. Towards being able to address\nthese situations, we propose a multi-weather Transformer, or\nMWFormer for short, which is a holistic vision Transformer\nthat aims to solve multiple weather-induced degradations using\na single, unified architecture. MWFormer uses hyper-networks\nand feature-wise linear modulation blocks to restore images\ndegraded by various weather types using the same set of\nlearned parameters. We first employ contrastive learning to\ntrain an auxiliary network that extracts content-independent,\ndistortion-aware feature embeddings that efficiently represent\npredicted weather types, of which more than one may oc-\ncur. Guided by these weather-informed predictions, the image\nrestoration Transformer adaptively modulates its parameters to\nconduct both local and global feature processing, in response\nto multiple possible weather. Moreover, MWFormer allows for\na novel way of tuning, during application, to either a single\ntype of weather restoration or to hybrid weather restoration\nwithout any retraining, offering greater controllability than\nexisting methods. Our experimental results on multi-weather\nrestoration benchmarks show that MWFormer achieves signif-\nicant performance improvements compared to existing state-of-\nthe-art methods, without requiring much computational cost.\nMoreover, we demonstrate that our methodology of using hyper-\nnetworks can be integrated into various network architectures\nto further boost their performance. The code is available at:\nhttps://github.com/taco-group/MWFormer\nIndex Terms—image restoration, adverse weather, multi-task\nlearning, low-level vision, transformer\nI. I NTRODUCTION\nI\nMAGES captured in the real world are often of defective\nquality due to adverse capture or environmental condi-\ntions. For example, CMOS-based cameras typical in mobile\ndevices often struggle to produce high-quality pictures in low\nlight. The photos produced under such conditions can be\nRuoxi Zhu, Jiaming Liu, and Yibo Fan are with Fudan University, Shanghai\n200433, China. (email: {rxzhu22@m., liujm22@m., fanyibo@}fudan.edu.cn).\nThey were supported in part by the National Key R&D Program of China\n(2023YFB4502802), in part by the National Natural Science Foundation\nof China (62031009), in part by Fudan-ZTE joint lab, in part by Alibaba\nInnovative Research (AIR) Program, in part by Alibaba Research Fellow\n(ARF) Program.\nZhengzhong Tu is now with the Department of Computer Science and\nEngineering, Texas A&M University, College Station, TX 77840, USA (email:\ntzz@tamu.edu). This work was done prior to his employment by Texas A&M\nUniversity, and he was not supported by any grant.\nA. C. Bovik is with the Laboratory for Image and Video Engineering\n(LIVE) at The University of Texas at Austin, Austin TX 78712, USA\n(email:bovik@ece.utexas.edu).\n⋆Yibo Fan is the corresponding author. †: equal advising.\nTask Specific Networks MWFormer (ours)\nRaindrop Rain Snow\nUnknown Weather\nAll-in-One\n(CVPR2020)\nTransWeather\n(CVPR2022)\nMWFormer\nOutput Clean Images\nMACs / G\nWeatherDiffusion\n(TPAMI2023)\nPSNR / dB\nFig. 1. Top row: Comparison of the MWFormer architecture with those\nof existing task-specific networks. Bottom: Restoration performance and\ncomputational cost of three versions of MWFormer having different numbers\nof channels against three competitive multi-weather restoration models. MW-\nFormer achieves generally better performance with 100× less computation\nthan the SOTA model WeatherDiffusion [1].\nnoisy, blurry, and under-exposed. Other common occurrences\nof degradation are caused by possibly multiple coincident\nweather conditions, such as rain, fog, and snow, that affect\nhuman-perceived image quality. When the images are fed to\nautomated vision systems, these distortions can severely ham-\nper the performances of computer vision algorithms, which\nare often trained on datasets of pictures taken under normal\nweather conditions. Failing to account for and ameliorate the\neffects of these and other natural phenomena can often lead\nto catastrophic outcomes in vision-dependent applications like\nautonomous driving, robotics, security, and surveillance, etc.\nDeveloping image processing algorithms that are able to\nanalyze and subsequently restore weather-degraded pictures\nis an active research topic [2]–[4]. In recent years, deep\nlearning-based restoration methods have been widely utilized\nto conduct weather-related image restoration tasks, such as\nderaining [3], [5], snow-removal [6]–[8], and dehazing [4], [9],\n[10]. Although these methods delivered promising results, each\narXiv:2411.17226v1  [cs.CV]  26 Nov 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nis designed to handle only a single type of adverse weather\ncondition. Whereas in many real-world scenarios, the weather\nconditions are generally unknown to the restoration algorithm.\nMoreover, there are often multiple commingled conditions,\nwhich result in multiply-distorted pictures that the above-\nmentioned methods are unable to adequately improve.\nRecently, several unified solutions have been proposed\nto restore images impaired by multiple coincident weather-\ninduced degradations [1], [11]–[13]. For example, the authors\nof [1], [2], [12] train single networks on combined datasets,\neach representative of a single weather condition, with the\nexpectation that the models would learn to adaptively process\neach weather degradation. However, these methods often de-\nliver unsatisfying and unbalanced generalization performances\nacross different weather types, and are unable to handle\nartifacts from co-occurring weather conditions. An important\nreason for this is that multiple coinciding distortions mutually\ninteract, creating new and highly diverse distortions.\nTowards making further progress on this important problem,\nwe propose an efficient, degradation-aware Multi-Weather\nTransFormer which we call MWFormer, that uses the ar-\nchitecture shown in Fig. 2. MWFormer is designed to provide\na strong restoration backbone for conducting image restoration\ntasks in the presence of unknown adverse weather conditions.\nMWFormer is able to account for different weather-induced\ndegradation types using a small auxiliary hyper-network that\nextracts degradation-informed features from an input image.\nThese features guide the generation of the parameters of\nthe image restoration backbone, allowing it to adaptively\nprocess the picture conditioned on the predicted weather\ndegradation. We also show that the new hypernet-based multi-\nweather feature extractor enables a novel way of test-time\ntuning to either handle a fixed weather condition with less\ncomputation, or to handle combined, hybrid weather-induced\ndegradations, without any retraining. This offers greater flex-\nibility and controllability than existing multi-task methods.\nNotably, the proposed model is the first one capable of\nhandling hybrid-weather degradations that were unseen during\ntraining. Some extended applications of the hyper-network\nhave also been developed, such as identifying the adverse\nweather type, and guiding the pre-trained weather-specific\nimage restoration models, which shows its versatility. Exper-\nimental results on benchmark datasets show that MWFormer\nis able to significantly outperform previous state-of-the-art\n(SOTA) models, both quantitatively and qualitatively, on a\nmulti-weather restoration benchmark. Our methodology can\nalso be integrated into various other network architectures to\nboost their performance in multi-weather restoration. To sum\nup, our contributions are summarized as follows:\n• We introduce a novel Transformer-based architecture\ncalled MWFormer for multi-weather restoration, which\ncan restore pictures distorted by multiple adverse weather\ndegradations using a single, unified model.\n• A hyper-network is employed to extract content-\nindependent weather-aware features that are used to\ndynamically modify the parameters of the restoration\nbackbone, allowing for degradation-dependent restoration\nand other related applications.\n• The feature vector produced by the hyper-network is\nleveraged to guide the restoration backbone’s behavior\nacross all dimensions and scales (i.e., locally spatial,\nglobally spatial, and channel-wise modulations).\n• Two variants of MWFormer are created—one for lower\ncomputational cost, and the other for addressing hybrid\nadverse weather degradations unseen during training.\n• Comprehensive experiments and ablation studies demon-\nstrate the efficacy of the proposed blocks and the supe-\nriority of MWFormer in terms of visual and quantitative\nmetrics. We also develop and analyze multi-weather\nrestoration models in the context of downstream tasks.\nII. R ELATED WORK\nImage Restoration. Image restoration is a long-standing\ncomputer vision problem that aims to reconstruct a high-\nquality image from a degraded input. Recently, there has been\na trend of employing end-to-end training of large neural net-\nworks on large-scale paired image datasets for a broad range\nof tasks, such as denoising [14], [15], deblurring [16]–[18],\nsuper-resolution [19], [20], low-light enhancement [21]–[23],\ndehazing [9], [10], deraining [12], [24], etc. The impressive\nadvancements on these problems have been mainly driven by\nthe development of novel network architectures. For example,\nencoder-decoder architectures have been widely adopted for a\nwide variety of restoration tasks [16]–[18], [22], [23], largely\nbecause of the efficacy of multi-scale feature learning. Simi-\nlarly, the spatial and channel self-attention mechanisms have\nbeen used to learn spatially focused and sparser features [3],\n[25]. More recently, multi-stage progressive networks [26]–\n[28] have been deployed on more challenging tasks like\ndeblurring and deraining, achieving impressive performances.\nImage Deraining. Rain can significantly degrade the quality\nof captured pictures. Extensive research efforts have aimed\nto mitigate the adverse effects of rain on images. Restoring\n“rainy” images involves two sub-tasks: eliminating rain streaks\nand removing raindrops. For instance, Li et al. [5] leveraged\na combination of dilated convolutional neural networks and\nrecurrent neural networks to effectively expunge rain streaks\nfrom pictures. Yasarla et al. [29] utilized a Gaussian Process-\nbased semi-supervised learning framework, demonstrating im-\npressive generalization capabilities on real-world images. Ba\net al. [30] proposed a novel deraining network trained on a\nnew and comprehensive dataset of real-world rainy images.\nBeyond merely addressing rain streaks, there’s an increasing\nemphasis on tackling the challenges posed by raindrops. Qian\net al. [31] introduced a dataset specifically designed to capture\nraindrop-related artifacts. They also trained an attentive GAN\nto effectively remove raindrops. Quan et al. [32] developed\na cascaded network designed to simultaneously remove both\nraindrops and rain streaks. More recently, Xiao et al. [24]\ndeveloped a Transformer architecture to conduct joint raindrop\nand rain streak removal, obtaining promising visual results.\nImage Desnowing. Snow is a complex atmospheric phe-\nnomenon that plagues the performance of computer vision\nmodels, such as the object detectors used in autonomous ve-\nhicles. DesnowNet [33] pioneered the use of deep learning to\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nEncoder\nMLP MLP\nAttention\nDepthwise \nConv\nMLP\nHyperMLP HyperMLP\n (a). Feature Extraction Network (b). Weather Type Aware Transformer Blocks\n...\nC\nScale2\nMLP\nEncoder Block\n...\nMLP\nMLP\nB\nR R\nB\nxN\nxN\nxM\n(c). Transformer Decoder\n...\nC\nB\nR\nElement-wise Addition\nElement-wise Multiplication\nConcatenate\nBroadcast\nReshape\nInput or Output\nGenerating Parameters\nDecoder\n \nIntra-PT \nBlock\nWeather Type Queries\nScale1\nTransformer \nBlock\nIntra-PT \nBlock\nGram \nMatrix\nMLP\nK,V\nQ Attention\nDepthwise \nConv\nMLP\nMLP\nPatch \nEmbedding\nPatch \nEmbedding\nMini-Patch \nEmbedding\nGram \nMatrix\nTransformer \nBlock\nPatch \nEmbedding\nEncoder Block\nPatch \nEmbedding\nTransformer \nBlock\nIntra-PT \nBlock\nUpsampling \nConv Block\nTransformer Decoder Conv Tails\nDecoder \nTransformer \nBlock\nDecoder \nTransformer \nBlock...\nxM\nDecoder Transformer Block\nUpsampling \nConv Block\nPatch\nEmbedding\nFig. 2. The architecture of MWFormer. The main image processing network consists of a Transformer encoder, a Transformer decoder, and convolution tails.\n(a) A feature extraction network learns to generate some of the parameters of the Transformer blocks and intra-patch Transformer blocks in the main network,\nthereby partially controlling the production of intermediate feature maps. (b) The Transformer block in the encoder of the main network, which is guided by\nthe feature vector. (c) Transformer decoder of the main network, whose queries are learnable parameters.\nconduct single-image desnowing, and the authors also built the\nfirst “snowy” picture dataset, called Snow-100K. Building on\nthis foundation, Chen et al. [34] addressed the veiling effect—\na phenomenon whereby snowflakes obscure and diminish\npicture clarity, by proposing a size- and transparency-aware\nsnow removal algorithm. Recently, Lin et al. [35] designed\na lightweight Laplace Mask Query Transformer for snow\nremoval, achieving SOTA performance.\nMulti-Weather Restoration. The existence of many dif-\nferent weather types in the real world poses a significant\nchallenge to single weather restoration models, leading to\ngrowing interest in developing image restoration models that\ncan effectively restore images affected by various complex\nweather conditions within a single, unified framework. Chen et\nal. [36] leveraged a two-stage knowledge learning mechanism\nto handle three different types of weather with a unified\nnetwork. Li et al. [12] designed an architecture called All-\nin-One, equipped with multiple encoders to capture differ-\nent degradations and a single decoder. While this approach\nis promising, its significant computational overhead poses\nchallenges for real-world applicability. Valanarasu et al. [2]\nunveiled a more efficient Transformer-based architecture called\nTransWeather by incorporating intra-patch Transformer blocks\n(intra-PT blocks) and using learnable weather-type queries.\nThe intra-PT blocks share the same architecture as the vanilla\nTransformer blocks, but take smaller patch embeddings as\ninput, which are sub-patches yielded from the original patch\nembeddings. These smaller sub-patches facilitate the network\nto extract finer details which are beneficial for mitigating\nsmaller degradations. Ozdenizci et al. [1] employed denoising\ndiffusion models to conduct multi-weather image restoration,\nsetting new benchmarks in performance. Yet, this approach\nsuffers from extremely slow inference time, making it un-\nsuitable for real-time deployments. Also, the model’s design\noverlooks specific treatments regarding the characteristics of\nvarious weather types. Zhu et al. [37] proposed a more ex-\nplainable method to extract the weather-general and weather-\ndependent features for multi-weather restoration. Besides, in\naddition to image restoration models, some researchers [38]\nhave also proposed image segmentation models that can handle\ndifferent real weather types.\nTransformers for Image Restoration. Building on founda-\ntional works [39], [40], Transformer architectures have become\npopular for various computer vision tasks including image\nrestoration, often significantly surpassing the previous CNN-\nbased solutions. The Image Processing Transformer (IPT) [41]\nwas the first to employ a pure Transformer architecture for\nimage processing tasks, which was pre-trained on a large\nnumber of corrupted image pairs using contrastive learning.\nThe pre-trained IPT could efficiently adapt to many image\nprocessing tasks after fine-tuning, outperforming state-of-the-\nart methods. The SwinIR [42] architecture, based upon the\nSwin Transformer [40], effectively handled low-level vision\ntasks by leveraging local-attention models. The Restormer [43]\narchitecture deployed a novel Transformer variant able to\ncapture long-range pixel interactions while remaining efficient\nusing a transposed attention mechanism. Furthermore, the\nUformer [44] presented a U-shaped Transformer architecture\nwith locally enhanced windows that has been shown to per-\nform remarkably well across diverse image restoration tasks.\nIII. P ROPOSED METHOD\nHere, we explain the technical details of the proposed\nMWFormer multi-weather restoration model. Our primary\nobjective is to learn a single, unified model capable of handling\nmultiple different weather degradations with the same set of\nlearned parameters. This is similar to the challenge of real-\nworld image denoising, where an algorithm is expected to deal\nwith various noise sources, types, and levels. Non-blind de-\nnoising generally outperforms blind denoising, since additional\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nnoise information helps a denoising network to better learn to\nadapt its parameters. Thus, adding an extra noise estimation\nmodule can enhance the performance of the blind denoising\nnetwork and increase its flexibility. Drawing inspiration from\nthis, we propose to deem different weather types as analogous\nto varying noise sources or types. Features descriptive of\nweather type can be extracted beforehand, then fed to the main\nrestoration network, which gains degradation adaptivity condi-\ntioned on the input weather types. Consequently, our proposed\nalgorithm may be bifurcated into two phases: weather-feature\nextraction (by the hyper-network) followed by a weather-type-\ninformed image restoration process.\nA. Overall Architecture\nAn overall schematic diagram of MWFormer is illustrated\nin Fig. 2, showing the two major components: (i) a restoration\nbackbone containing encoder and decoder blocks, which are\nresponsible for recovering a high-quality image from the\ndegraded input; (ii) a feature extraction network that yields\nweather-aware feature vectors. We adopt a Transformer-based\narchitecture as the restoration backbone. Besides the vanilla\nTransformer blocks, our encoder network contains extra intra-\nPT blocks introduced in Sec. II. The decoder of the backbone\nis similar to the design in [2], including learnable weather-type\nqueries that cross-attend to the key and value features from\nthe encoders. However, this architecture is still incapable of\nlearning to disentangle commingled weather features, arising\nfrom coexisting weather conditions, even if it is trained on\nmultiple weather datasets. Therefore, we have designed an ar-\nray of improvements that explicitly supply network flexibility\nin the multi-weather setting. The innovative designs we make\nare further explained in the following sections.\nB. Feature Extraction Network\nWeather variations can be viewed as distinct image “styles”,\nwhich are inherently decoupled from the image content. To il-\nlustrate this idea, consider two snapshots of an identical scene,\neach captured under different weather conditions and man-\nifesting distinct weather-related impairments. Each impaired\n(or “weather-styled”) picture should be treated differently by\nthe restoration network, but the two outputs should both faith-\nfully recover the image content. On the other hand, pictures\ncontaining different contents, but suffering from the same\nweather degradation, should lead to comparable responses\nfrom the network. This is analogous to image style transfer,\nwhich emphasizes decoupling image style and content. The\nGram matrix [45], which represents correlations within feature\nmaps, is commonly used to define image styles. Yet, the\noriginal form of the Gram matrix fails in the context of multi-\nweather restoration, as it represents artistic styles rather than\nweather-relevant features. To address this, we append trainable\nprojection layers—multi-layer perceptrons (MLPs)—on top of\nthe vanilla Gram matrix, to learn weather-specific “style”.\nThe architecture of our feature extraction network is shown\nin Fig. 2(a). We utilize the first two scales of the Transformer\nencoders, where a Gram matrix is computed at each scale.\nSince Gram matrices are symmetric, only the upper triangular\nparts of the two matrices are vectorized to save computation.\nThese vectors are further fed to the two projection layers\n(MLPs), thereby generating two 64-dimensional embeddings.\nFinally, the two embeddings are concatenated and projected\nonto a single feature vector v, which encodes the weather-\ndegradation information from the input image.\nThe feature extraction network is intended to cluster images\naffected by similar weather degradations, hence we utilize con-\ntrastive learning [46] to train it, wherein the loss is formulated\nas:\nLcon =\nX\n(a,b)∈P\n{I(a, b)[m−d(va, vb)]++[1−I(a, b)]d(va, vb)},\n(1)\nwhere P denotes every possible image pair in a batch, d(·)\ndenotes cosine similarity, m is a positive margin, and I(a, b) is\nan indicator that equals 1 when the two images (a, b) contain\nthe same weather impairments and 0 if they are captured under\ndifferent weather conditions. The definition of [·]+ operation\ncan be expressed as:\n[x]+ =\n(\n0, x ≤ 0,\nx, x > 0. (2)\nWhen calculating the contrastive loss, each possible image\npair is sampled from the batch. If the two images belong to\ntwo different datasets, the term d(va, vb) enforces that their\nfeature vectors are pushed away from each other. If the two\nimages belong to the same dataset, the term [m −d(va, vb)]+\npulls their feature vectors closer in the embedding space.\nConsequently, the learned feature extraction network is able to\ncluster the images affected by the same weather degradation.\nC. Image Restoration Network\nThe image restoration network contains two sets of learned\nparameters: fixed parameters that encode the general restora-\ntion priors relevant to all the tasks, and weather type-adaptive\nparameters that are generated by the feature extraction net-\nwork, as shown in Fig. 2(b). More specifically, the output\nimage Y is computed as:\nv = Ffeat (I; τ), (3)\nY = Fres(I; θfix , θadap(v)), (4)\nwhere Ffeat is the auxiliary feature extraction network\n(Sec. III-B) with parameters τ, and Fres is the image\nrestoration backbone. The parameters θfix and θadap(v) are\nthe weather-independent and weather-adaptive weights in the\nencoder stages, respectively. Since different weather types\nrequire varying scales of treatments—for example, derain-\ning mostly requires local contexts, while desnowing de-\nmands global understanding to differentiate snowflake and\nsnowpack—we inject weather type adaptivity in multiple pil-\nlars: spatial-wise, both locally and globally in the parameter\nspace, as well as channel-wise feature modulation, to enable\nbetter feature learning. The adaptivity is applied to both the\nTransformer block and the intra-PT blocks in the encoder\nstage. In the Transformer decoder blocks [2], the learnable\nweather-type queries attend to the input features, followed\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nby standard MLP and depth-wise convolution layers, yielding\nrestored output images Y .\nSpatially local adaptivity. Since vanilla Transformer ar-\nchitectures lack inductive biases expressive of local pixel\ninteractions, we add a depthwise convolution layer between\nthe two MLPs in each feed-forward network (FFN) in the\nTransformer blocks. Unlike previous models, however, we\nleverage the predicted weather type features v computed by\nthe hyper-network Ffeat to generate the parameters of the\ndepthwise convolution layers, so that pictures degraded by\ndifferent weather types will be processed by different filters\nadaptively. The feature vector v is fed into a 2-layer projection\nMLP (named HyperMLP in Fig. 2 since it is intended to\ngenerate the parameters of other modules), then reshaped\nto the 2D depthwise convolution kernels w ∈ RC×1×3×3\n(omitting the batch dimension) that are used to convolve the\ninput Xsl:\nWDWC = Reshape(Proj(v)), (5)\nFFN(Xsl, v) = MLP(σ(WDWC ∗ Xsl)), (6)\nwhere WDWC denotes the weights of the depthwise convo-\nlution generated by reshaping the projection of the v vector,\nXsl denotes the input of the spatially local operation (i.e.,\ndepthwise convolution), ∗ denotes depthwise convolution, and\nσ denotes nonlinear activation.\nSpatially global adaptivity. Compared with CNN archi-\ntectures, Transformers excel in capturing long-range spatial\nrelationships using self-attention layers that scan over all the\ntokens. To model adaptive global interactions, we use another\nhyper-network to compute the critical projecting parameters\nused in the self-attention operations. Formally, denoting an\ninput patch embedding of the spatially global operation (i.e.,\nself-attention block) as Xsg ∈ RN×Cin , three linear projection\nmatrices Wq, Wk and Wv are applied to obtain the query,\nkey, and value features Q = XsgWq, K = XsgWk and\nV = XsgWv. The matrix product of Q and KT is then cal-\nculated, yielding a global attention map to weighted-sum V .\nDifferent weather types may require different attention maps\nwhen conducting restoration, thus we employ the weather type\nfeature v again, to generate Wq, Wk and Wv, using a similar\nprojection as our design in Eq. (5). The result is then reshaped\nto match the dimensions of W ∈ Rdin×dout . Mathematically,\nWi = Reshape(Proj(v)), i = q, k, v, (7)\nQ = XsgWq, K = XsgWk, V = XsgWv, (8)\nMSA(Xsg) = softmax(QKT\n√\nd\n)V . (9)\nChannel-wise feature modulation. Except for weather-\ntype awareness in the parameter space, we also introduce\na dimension of degradation dependency in the intermediate\nfeature space. We apply a simple affine transformation on the\nlearned intermediate representations, which has been shown to\nbe effective in previous work [47]–[49]. The feature vector v\nis input to the projection MLPs before each patch embedding\nlayer to generate the weights γ ∈ RC and biases β ∈ RC.\nFor each channel, the weight and bias are then broadcast to\nall pixels of the corresponding feature map, which modulates\nthe input features X along the channel dimension:\nX′ = γX + β. (10)\nThese modulating blocks may be regarded as a form of\nchannel-attention mechanism that re-calibrates the importance\nof different channels, conditioned on the weather.\nD. Simplified Architecture for Fixed Weather Degradation\nBesides the aforementioned MWFormer architecture, we\nalso developed a lightweight test-time variant for less com-\nputational cost. Our design for learning representations of\nweather type using an auxiliary hyper-network, which we use\nto guide the restoration backbone, also enables a computation-\nefficient inference scheme when the weather type is already\nknown. Assuming that the learned weather-representation fea-\nture vectors for a given weather type lie near each other in the\nembedding space, then we can replace the feature extraction\nnetwork with a fixed feature vector that represents the weather\ntype, which is an approximation of the full-size model. More\nspecifically, we pre-calculate and store the average feature\nvector of images affected by each weather type during training,\nthen directly use these features while testing. This simplified\narchitecture is shown in Fig. 3(b) and formulated as\nY = Fres(I; θfix , θadap(¯vi)), (11)\nwhere i denotes a specific degradation type (e.g., rainstreak,\nraindrop, snow), and ¯vi is the average weather feature vector\ncomputed on images affected by the ith degradation type\nduring training.\nE. Multi-stage Architecture for Hybrid Weather Degradations\nWe have developed another test-time variant for hybrid\nadverse weather removal. Due to a current lack of hybrid\nweather datasets, previous restoration models, whether trained\nto handle single or multiple types of weather, are unable to suc-\ncessfully restore pictures captured under multiple simultaneous\nadverse weather conditions, such as rain + snow. However,\nMWFormer can be easily modified without re-training to\nhandle previously unseen multiple weather-degraded pictures,\nhence, it is more generalizable than prior models.\nFor example, consider a rain + snow hybrid weather con-\ndition. If a model is only trained on multiple single-weather\nrestoration datasets, then it may be capable of restoring images\ndegraded by any of the weather factors (in this case, rain or\nsnow), but not a combined hybrid weather condition (in this\ncase, rain + snow). Hence, we develop a two-stage network\narchitecture as a test-time variant of MWFormer to handle such\nhybrid weather conditions. In the first stage of inference, the\naverage feature vector for rainy images is used as the guidance\nof the image restoration backbone to produce an intermediate\nresult that is rain-free but still contains snowflakes. Then, this\nintermediate output containing only a single adverse weather\nis processed using MWFormer again in the second stage to\nremove snowflakes, yielding a final clean image. The overall\nprocess can be denoted as:\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nPretrained Model\nFeature Vector for the Weather\nStage1 Stage2\nSingle Weather Input Hybrid Weather Input\nFeature Vector for \nSingle Weather\nShared Weights\nPretrained Model Pretrained Model\n(b) Single Weather Restoration (c) Hybrid Weather Restoration(a) Unknown Weather Restoration\nUnknown Weather\nOutput Clean Images\nFig. 3. Comparison of the default architecture with two test-time variants applied in special cases. To conduct a single weather-type restoration, the feature\nextraction network is replaced by a fixed feature vector. To conduct hybrid weather restoration, the image processing network is cascaded to remove degradations\nsequentially, stage by stage.\nZ = Fres(I; θfix , θadap(¯vi)), (12)\nvz = Ffeat (Z; τ), (13)\nY = Fres(Z; θfix , θadap(vz)). (14)\nIf the image is subjected to more types of adverse weather,\nthen further stages may be cascaded, wherein each stage\nrestores a specific type of degradation. Note that the networks\nin the different stages share the same set of weights, thereby\noffering flexible test-time augmentation capability without\nrequiring any re-training.\nF . Extended Applications\nThe hyper-network that creates weather-informed feature\nvectors is a key aspect of our approach. Beyond generating\nparameters and modulating feature maps, these vectors have\ndiverse applications due to the hyper-network’s strong percep-\ntion of weather features. We present two extended applications\nto demonstrate the versatility of the proposed hyper-network.\n1) Weather-type identification: Our hyper-network, trained\nwith a contrastive learning strategy on a multi-weather restora-\ntion dataset, contains rich prior information on various weather\nfeatures. Leveraging this, we developed a weather-type iden-\ntification method using the hyper-network without re-training.\nTaking three adverse weather types as an example. Let ¯vrd,\n¯vr and ¯vs be the average feature vectors of three weather\ntypes (raindrop, rainstreak, and snow) respectively, which is\ncomputed during training. To identify the weather type of an\nimage I impaired by unknown adverse weather, the image’s\nfeature vector v is first computed using the feature extraction\nhyper-network Ffeat , then the cosine similarities between v\nand the average feature vectors of each weather type are\ncomputed:\nv = Ffeat (I). (15)\ndi = v · ¯vi\n∥v∥∥¯vi∥, i ∈ {rd, r, s}. (16)\nFinally, the scores related to each weather type are computed\nusing Softmax function:\nsi = edi\nedrd + edr + eds\n, i ∈ {rd, r, s}. (17)\nThe weather score si approximately indicates the probability\nthat the image is degraded by the adverse weather type i. If\nthis image is known to be affected by only one of the given\nadverse weather types, then it can be inferred that the highest-\nscoring type of weather i∗ exists in the image:\ni∗ = arg max\ni\nsi, i ∈ {rd, r, s}. (18)\n2) Guiding pre-trained weather-specific models: Most ex-\nisting adverse weather restoration models are trained for\nspecific weather types, making them effective for known con-\nditions but unable to handle unknown or even hybrid weather\nscenarios. This limits their real-world applicability. To make\nfull use of these weather-specific experts, we’ve developed a\nstrategy that utilizes the proposed hyper-network to guide ex-\nisting pre-trained weather-specific models for restoring images\naffected by unknown weather conditions.\nSuppose we have expert models for many different types\nof weather. When faced with an image affected by unknown\nweather conditions, our goal is to select the most suitable\nexpert model, so that the image quality can be improved\nas much as possible. Without loss of generality, assume that\nwe have three expert models for raindrop removal, rainstreak\nremoval and desnowing respectively. We first compute the\nweather scores of three weather types using Eq. (15) ∼ (17).\nThen, the highest scoring weather type is considered to be the\nmost typical and most impactful to image quality in this image.\nTherefore, the expert model corresponding to this weather type\nis selected to process the image. It should be noted that for\nimages affected by hybrid weather, although the degradation\nmay not be completely eliminated, our strategy is able to do as\nmuch as possible to improve the image quality with only one\npre-trained weather-specific model, whereas other strategies\ncannot achieve higher image quality with the same or even\nmore computational effort.\nIV. E XPERIMENTS\nIn this section, we first detail our experimental settings.\nThen, we compare the performance of MWFormer against ex-\nisting SOTA models both qualitatively and quantitatively. Fur-\nthermore, we also conducted comprehensive ablation studies\nto study the efficacy of different MWFormer model designs.\nFinally, we present some discussions on the effectiveness of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nTABLE I\nQUANTITATIVE COMPARISONS OF MWF ORMER AGAINST STATE -OF-THE -ART MULTI -WEATHER RESTORATION MODELS ON THREE TEST DATASETS .\nALONG EACH COLUMN , THE BEST SCORE IS BOLDFACED , WHILE THE OTHER TOP THREE ARE UNDERLINED .\nModel RainDrop [31] Outdoor-Rain [50] Snow100K [51] Average MACs (G) ↓PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑\nAirNet [13] 24.57 0.8583 18.48 0.6719 24.41 0.8045 22.49 0.7782 301.27\nChen et al. [36] 31.83 0.9289 25.45 0.8737 28.86 0.8886 28.71 0.8971 24.56\nAll-in-One [12] 31.12 0.9268 24.71 0.8980 28.33 0.8820 28.05 0.9023 12.26\nTransWeather [2] 30.17 0.9157 28.83 0.9000 29.31 0.8879 29.44 0.9012 6.13\nWeatherDiffusion [1] 30.71 0.9312 29.64 0.9312 30.09 0.9041 30.15 0.9222 475.16 × 50\nZhu et al. [37] 31.31 0.93 25.31 0.90 29.71 0.89 28.78 0.91 1.36\nMWFormer-S (ours) 31.09 0.9224 29.07 0.9010 30.05 0.8986 30.07 0.9073 3.57\nMWFormer-M (ours) 31.56 0.9246 29.70 0.9064 30.45 0.9029 30.57 0.9113 6.45\nMWFormer-L (ours) 31.73 0.9254 30.24 0.9111 30.70 0.9060 30.89 0.9142 10.41\nMWFormer-real∗ (ours) 31.91 0.9268 30.27 0.9121 30.92 0.9084 31.03 0.9158 10.41\n∗MWFormer-real is trained on a larger dataset mentioned in sub-section IV-A.\nInput AirNet [13] Chen et al. [36] Zhu et al. [37] TransWeather [2] WeatherDiffusion [1] MWFormer-L Ground Truth\nFig. 4. Qualitative comparisons on the RainDrop [31] test set. MWFormer effectively removed raindrop artifacts under various scenarios, yielding output\nimages with either fewer shadows or less blur than the other compared models.\nthe feature vectors in MWFormer and the generalization ability\nin Sec. V.\nA. Training Details\nFor a fair comparison, we first followed the settings in\n[1], [2], [12] to train MWFormer on the standard bench-\nmark for multi-weather restoration, which is a combination\nof three datasets: RainDrop [31], Outdoor-Rain [50], and\nSnow100K [51]. Similarly, we used the RainDrop test dataset\n[31], the Test1 dataset from Outdoor-Rain [50], and the\nSnow100K-L testset [51] for testing raindrop removal, drain-\ning with dehazing, and desnowing, respectively.\nWe first pre-trained the feature extraction network in MW-\nFormer over 10k iterations using Eq. (1) as the loss function,\nwith batch size 8 and learning rate 2e−4. Then, we trained\nthe image restoration network over 200k iterations using a\nweighted combination of the smooth L1 loss and perceptual\nloss [52]. In our implementation, the difference between the\nfeature maps extracted by a pretrained VGG16 (from the 3rd,\n8th, and 15th layers) of the predicted image and that of the\nground truth image was summed up as the perceptual loss.\nThe total loss function is given as:\nLall = L1 + λLperc, (19)\nwhere λ was fixed at 0.04. To avoid overfitting to a spe-\ncific dataset, we sampled approximately the same number of\ntraining examples from each dataset, respectively. Finally, the\nfeature extraction network and the image restoration network\nwere jointly fine-tuned over another 190k iterations using a\nreduced learning rate.\nWe instantiated three versions of MWFormer (Small,\nMedium, and Large), referred to as MWFormer-S, -M, and\n-L, of our proposed model by changing the number of base\nchannels. In MWFormer-L, the number of channels for each\nencoder scale was 64, 128, 320, and 512, respectively, whereas\nthe number of channels was reduced by the factors of 0.75 and\n0.5 to create MWFormer-M and MWFormer-S, respectively.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nInput AirNet [13] Chen et al. [36] Zhu et al. [37] TransWeather [2] WeatherDiffusion [1] MWFormer-L Ground Truth\nFig. 5. Visual comparisons on the Test1 [50] (rain+fog) set. MWFormer performed the best on both detail restoration and luminance retention. AirNet failed\nto remove most of the degradations, TransWeather recovered fewer details, and the WeatherDiffusion introduced color distortions.\nBesides, it should be noted that some images in this widely\nadopted benchmark have different distributions from real-\nworld scenes, which is likely to limit the model’s real-world\nperformance. For example, this dataset did not represent the\nveiling effect in multi-weather restoration [53]. To further\nimprove MWFormer’s applicability to real-world images, we\nre-trained MWFormer on a larger dataset, which is denoted\nMWFormer-real. Specifically, besides the previous benchmark\ndataset, we included another two datasets in the training set:\nthe training set of WeatherStream [54] that contains real-world\nframes containing rain-fog degradations, and the training set\nof the CSD dataset [55] that includes images impaired by\nsnowflakes and the veiling effects. We also re-trained Tran-\nsWeather [2] on this larger dataset for a fair comparison.\nB. Quantitative Comparisons\nWe used five state-of-the-art multi-weather restoration mod-\nels as comparisons: All-in-One [12], Chen et al. [36], Tran-\nsWeather [2], WeatherDiffusion [1] and Zhu et al. [37].\nAnother all-in-one image restoration model named AirNet\n[13] was also re-trained on the benchmark dataset for com-\nparison. Table I reports the performances using PSNR and\nSSIM [56] as performance metrics. The computational costs of\neach model, evaluated by the number of multiply-accumulate\noperations (MACs), are also listed. As may be seen from the\ntable, MWFormer-real performed best on all three datasets\namong all the compared methods in terms of PSNR, which\nis usually regarded as the most reliable measure of fidelity.\nMWFormer-L also performed better than any model trained\nusing the benchmark dataset regarding average PSNR. Though\nChen et al. [36] achieved better results on the Raindrop\ntestset, their model performed poorly under the other two\nweather conditions, and the imbalance performance is not\npreferable in practice. In terms of the more perceptual-oriented\nmetric SSIM, the diffusion-based WeatherDiffusion model, on\naverage, achieved the best scores, but MWFormer yielded\ncomparable results, performing among the top three.\nAlthough WeatherDiffusion [1] performed well in terms of\nSSIM on some datasets, it requires 2000 × more computation\nthan our largest model MWFormer-L, and requires 5000 ×\nmore computation than our smallest model MWFormer-S, if\nthe iterative sampling diffusion process is considered. Overall,\nour MWFormer appears to deliver the best trade-off between\nimage quality and computational cost.\nBesides, although WeatherDiffusion delivered the best\nSSIM results on the RainDrop and Outdoor-Rain sets, the\ndiffusion model is occasionally prone to hallucinative arti-\nfacts. One of its failure cases is shown in the third row of\nFig. 9, which exhibits unacceptable artifacts and stains that\nsignificantly alter the image contents. Since these restoration\nmodels are often employed as preprocessing modules for many\ndownstream recognition tasks, such as object detection and se-\nmantic segmentation for autonomous vehicles, hallucinations\nof image content obtained from diffusion-based models could\nlead to hazardous outcomes in real-world scenarios.\nMoreover, the comparison results of TransWeather-real and\nMWFormer-real are illustrated in Table. II, indicating that\nMWFormer still surpasses the existing leading models, such\nas TransWeather, if they are both trained on a larger dataset.\nAlso, by including more images closer to the real scene, the\nquantity metrics on all of the test sets are boosted.\nC. Qualitative Comparisons\nWe also obtained the visual results on each benchmark\ndataset as shown in Figs. 4 to 6. On the RainDrop test dataset,\nas shown in Fig. 4, AirNet failed to remove many of the\nraindrops. Both TransWeather and WeatherDiffusion produced\nartifacts such as shadows and hallucinations (see the first\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nInput AirNet [13] Chen et al. [36] Zhu et al. [37] TransWeather [2] WeatherDiffusion [1] MWFormer-L Ground Truth\nFig. 6. Visual comparisons on the Snow100K-L [51] testset. MWFormer efficaciously removed snowflakes, delivering cleaner pictures than the other models.\nTABLE II\nCOMPARISONS OF PERFORMANCES OF MWF ORMER -REAL AND TRANS WEATHER -REAL .\nModel RainDrop [31] Outdoor-Rain [50] Snow100K [51] WeatherStream [54] CSD [55]\nTransWeather-real [2] 30.99 / 0.9207 28.98 / 0.9002 30.00 / 0.8996 24.29 / 0.7468 32.99 / 0.9580\nMWFormer-real (ours) 31.91 / 0.9268 30.27 / 0.9121 30.92 / 0.9084 24.44 / 0.7488 34.60 / 0.9690\nInput TransWeather-real MWFormer-real Ground Truth\nFig. 7. Visual comparisons of TransWeather-real [2] and MWFormer-real on\ntwo additional testsets (WeatherStream testset [54] and CSD testset [55]) that\nare more consistent with real-world scenes.\ntwo rows). MWFormer, however, delivered visually pleasing\nresults without shadows or blur. On the Test1 (rain+fog)\ndataset shown in Fig. 5, MWFormer was able to restore both\nthe luminance and detail information accurately, while the\nresults from Chen et al. and TransWeather suffered from a\nloss of detail (note the texture in the last two rows), and the\nresults produced by Zhu et al. and WeatherDiffusion included\nshadows (see the first row). Additionally, WeatherDiffusion\nsometimes led to color distortion (see the second row). On\nthe Snow100K-L dataset shown in Fig. 6, MWFormer yielded\ncleaner images, while AirNet, Zhu et al. and WeatherDiffusion\ntended to interpret some snowflakes as other image details and\nincorrectly preserved them, thereby reducing the image quality.\nWe also compared MWFormer-real and TransWeather-real\non the two more realistic testsets: WeatherStream [54] and\nCSD [55] testsets. The visual results are shown in Fig. 7.\nOn the WeatherStream dataset, MWFormer-real removes the\nrainstreaks more thoroughly than TransWeather-real, lead-\ning to more visually pleasing results. On the CSD dataset,\nTransWeather-real sometimes wrongly retains the snowflakes\nand tends to blur the small but bright objects excessively.\nD. Performance on Hybrid Weather Degradations\nMore challenging but frequent scenarios are hybrid weather\nconditions. Hence, we also studied the performances of the\ncompared models on hybrid-weather-degraded images. Using\nthe weather synthesizing algorithms in [50], we simulated\nimages with hybrid degradations of rain + snow using images\nfrom Snow100K. The results of restoring these degraded\nimages are shown in Fig. 8. It can be seen that previous mod-\nels failed to restore these images since obvious snowflakes,\nrain streaks, or fog remain in their outputs. This may be\nbecause hybrid-weather-degraded images were not part of their\ntraining data; models trained on single weather types cannot\nbe expected to generalize to restore more complex weather\ndegradations. However, MWFormer, which is imbued with the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nInput TransWeather [2] WeatherDiffusion [1] MWFormer (stage1) MWFormer (2 stages) Ground Truth\nFig. 8. Visual comparisons on hybrid-weather degradations. While most of the compared models failed to handle the complex degradations, the 2-stage\nMWFormer model, which sequentially removes rain streaks and snowflakes in each stage, was able to deliver more visually appealing outcomes.\nInput AirNet [13] Chen et al. [36] Zhu et al. [37] TransWeather [2] WeatherDiffusion [1] MWFormer (ours)\nFig. 9. Qualitative results on real images (including hybrid-weather-degraded images) from [51] and [57]. MWFormer was able to remove the snowflakes\nwhile preserving the original image structure. However, AirNet [13] and WeatherDiffusion [1] generated undesirable artifacts. Furthermore, MWFormer can\ncapably remove the hybrid-weather degradations that were unseen during training, as shown in the last three rows.\nflexibility to conduct test-time augmentation (Fig. 3), is able\nto remove rain and snowflakes in two successive stages, yield-\ning clean, degradation-free images. We also demonstrate the\nefficacy of multi-stage application by visualizing the effects\nof the stage-by-stage degradation removal process in Fig. 8.\nTo study the alternative approaches of our proposed multi-\nstage MWFormer architecture for rain + snow restoration\nproblem, we compared four different strategies: First, we\napplied the simplest single-stage architecture that is intended\nfor single-weather restoration to the rain + snow problem.\nSecond, we applied the single-stage model to each image\ntwice in succession. Third, using a two-stage MWFormer, we\nconducted desnowing first, using the average feature vector as\nguidance, followed by deraining. Last, we reversed the order\nby deraining first and then desnowing, as shown in Fig. 3(c).\nThe performances of these models were tested on our synthetic\ndataset, which consists of diverse scenes, different rain levels,\nand different angles of rain streaks. Quantitative comparisons\nin Table III indicate that MWFormer performed best when\nderaining in the first stage, followed by desnowing in the\nsecond stage. This may be because snowflake appearance is\nsignificantly affected by rain accumulation; thus, the average\nfeature vector for snow-degraded images may not match these\nimages well. The intermediate results after deraining resemble\nthe snow-degraded images in the training set, which are\neasier for the network to process. These results powerfully\ndemonstrate the efficacy of the MWFormer model in dealing\nwith multi-weather scenarios.\nE. Generalization to Real Weather Degradations\nWe also compared MWFormer against other models on\nthe real weather-degraded images from the Snow100K-real\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nInput AirNet [13] TransWeather [2] WeatherDiffusion [1] MWFormer (ours)\nFig. 10. Task-driven comparisons on YOLO-V5 object detection. MWFormer helped deliver better detection performance than other compared methods. Note\nthat AirNet [13] and WeatherDiffusion [1] were implicated in causing false positives in the detection results, likely due to inadequate restoration performance.\nTABLE III\nCOMPARISONS OF PERFORMANCES OF THREE DIFFERENT MWF ORMER\nMODELS ON HYBRID RAIN + SNOW IMAGES .\nStrategy PSNR SSIM\nSingle Stage 21.24 0.7237\nTwo Stages with Default Settings 22.76 0.7665\nTwo Stages, Desnow First 21.98 0.7568\nTwo Stages, Derain First 24.80 0.7669\nTABLE IV\nNIQE SCORES OF MWF ORMER AND PREVIOUS SOTA METHODS ON\nREAL -WORLD DATASETS [51], [57].\nTransWeather [2] WeatherDiffusion [1] MWFormer\nNIQE ↓ 3.2550 3.0162 2.9469\nset [33] that contains pictures taken under real snowy condi-\ntions, and from RainDS-real dataset [57] that includes real-\nworld images with raindrops and rainstreaks. We used the\nMWFormer-L to process the images in the Snow100K-real\ndataset, and use its variant discussed in Sec. III-E to restore the\nhybrid-weather-degraded images in the RainDS-real dataset.\nNote that no ground truth is available for these images, so we\nmust rely on visual comparisons. As may be seen in Fig. 9,\nMWFormer was able to remove most of the snowflakes,\nyielding visually clean reconstructions as compared to other\nmethods. As for images impaired by both raindrops and\nrainstreaks, MWFormer also performed the best, owing to its\nflexibility for hybrid-weather degradation removal. Moreover,\nit should be observed that WeatherDiffusion was exceedingly\nsensitive to domain shift—its performance varied significantly\non different images, and it randomly generated unacceptable\nartifacts (the third row of Fig. 9). MWFormer, on the other\nhand, produced more visually consistent results in terms of\nreal-weather generalization, which may be attributed to the\nsmaller number of learnable parameters and the design of the\nweather-type feature learning. The quantitative comparisons\nusing NIQE [58] are reported in Table IV, indicating that\nMWFormer outperforms the previous state-of-the-art models\non this most widely adopted no-reference metric.\nF . Task-Driven Comparisons\nImage restoration results may be consumed either by hu-\nmans or by machines. It is likely that weather degradation\nremoval is more frequently used in machine vision systems,\ne.g., as a precursor to object detection for autonomous driving.\nWe studied this aspect by conducting a study of task-driven im-\nage restoration performance in the context of object detection.\nSpecifically, we evaluated the object detection performance\nof a pre-trained YOLO-V5 [59] object detector on images\nrestored by the compared models. As shown in Fig. 10, on real\nimages containing snowflakes, the pictures processed by MW-\nFormer were able to better boost the detection performance of\nthe YOLO-V5 as compared to applying the object detector\nto the original snow-degraded pictures. This suggests the\npotential of using MWFormer as a pre-processing component\nbefore object detectors in applications such as Autopilot [60].\nThe other image restoration methods, however, led to fewer\ndetected objects and even misclassified some objects. It is\nworth noting that on images affected by raindrops, MWFormer\nonly delivered slightly better detection performance than on\nthe original image, while the other approaches had little\neffect or even deteriorated the detection performance. This\nobservation is consistent with the empirical results in [3].\nLastly, we observed that AirNet and WeatherDiffusion tended\nto cause false positive cases (“skateboard” in the second row\nand “bird” in the bottom row) on some images, which could\nlead to unexpected and undesirable outcomes in real-world\napplications.\nRecognition-aware training. We also studied ways to show\nthat MWFormer can be specifically trained to benefit down-\nstream detection models. To do this, we created a recognition-\naware version of MWFormer by fine-tuning the base model\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nTABLE V\nABLATION STUDIES OF COMPONENTS OF THE MWF ORMER -L ARCHITECTURE .\nLocal Global Channel Fine-Tune RainDrop [31] Outdoor-Rain [50] Snow100K [51] Average\nPSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑\n30.72 0.9173 29.23 0.9007 30.07 0.8992 30.01 0.9057\n✓ 31.11 0.9222 29.70 0.9028 30.14 0.8998 30.32 0.9083\n✓ ✓ 31.19 0.9236 29.80 0.9055 30.18 0.9010 30.39 0.9100\n✓ ✓ ✓ 31.36 0.9235 29.89 0.9073 30.50 0.9041 30.58 0.9116\n✓ ✓ ✓ ✓ 31.73 0.9254 30.24 0.9111 30.70 0.9060 30.89 0.9142\nInput Visual-Oriented Detection-Oriented\nFig. 11. Comparisons of the effects of visual-oriented and recognition-aware\ntraining. It may be observed that the latter strategy yielded less visually\nappealing outcomes, but led to better detection accuracy.\nof MWFormer for a few more steps, replacing the perceptual\nloss with a recognition loss (including both classification loss\nand regression loss), calculated using a MobileNetV3-SSDLite\nobject detection network R with frozen weights. To calculate\nthe recognition loss, we assigned the detection results of\nthe clean image R(Iclean) as ground truth, and calculated\nits distance to the detection results on each restored image\nR(Fres(I; θfix , θadap(v))). The total loss was:\nLall = L1 + λ(Lcls + Lreg), (20)\nwhere Lcls + Lreg is the recognition loss consisting of two\nterms: Lcls is the classification loss implemented as the cross\nentropy between the predicted logits and the ground truth\nlabels, and Lreg is the regression loss implemented as smooth\nL1 loss between the predicted bounding box and the ground\ntruth.\nSeveral visualizations of the restored images with detection\nresults overlaid are shown in Fig. 11. The detected object\nbounding boxes are overlaid, along with their associated de-\ntection confidence score. Some interesting observations can be\ndrawn from this experiment. First, including the task-oriented\ntraining objective improved the performance of the down-\nstream detection tasks, consistent with the findings in [61].\nFurther, optimizing for human quality perception and machine\ntasks led to different visual effects in the output images, indi-\ncating that deep neural network-based detectors learn different\nrepresentations compared to human visual systems. Exploring\nmore task-oriented image restoration techniques is beyond the\nscope of this paper, and hence, we leave it to future work.\nResults on RainDrop Testset Results on Test1 (Rain+Fog) Testset Results on Snow100K-L Testset \nFig. 12. Boxplots of the weather scores of different datasets.\nG. Ablation Studies\nTo further understand and validate the efficacy of MW-\nFormer, we conducted several comprehensive ablation studies.\nWe used the MWFormer-L as the base model and ablated the\nvarious components trained using the same set of hyperparam-\neters. We first trained a baseline MWFormer-L model (without\nthe feature learning network) and gradually added 1) spatially\nlocal adaptivity, 2) spatially global adaptivity, 3) channel-\nwise feature modulation, and 4) joint fine-tuning, as explained\nin Section III-C. As may be seen in Table. V, each axis of\nweight adaptivity contributed to a notable performance gain on\nall the datasets, with Local adaptivity delivering the greatest\ngain on Raindrop and Outdoor-Rain datasets, while Channel\nadaptivity supplied the most benefit on the Snow100K. The\nfinal stage of joint fine-tuning can further boost the overall\nperformance by aligning the separately trained feature extrac-\ntion network with the image restoration backbone.\nWe also visualize the results of the ablation study in Fig.\n13. The baseline model (without the three proposed modules)\ncannot thoroughly remove the artifacts, as pointed out by the\nup arrow in the first row and the down arrow in the second\nrow. Some image details are also treated as artifacts and thus\nblurred, as indicated by the right arrow. The image quality can\nbe largely improved after adding the local adaptivity modules\nto the model, owing to the adaptive local operations. Then,\nby adding the global adaptivity module, the model gains a\nbetter global understanding of how to differentiate snowflakes\nor raindrops and their background. In the first row, the model\ntreats the content pointed by the right arrow as a light bulb\nrather than a snowflake. In the second row, the artifacts with\nthe same color as the grass are suppressed. Last, by adding the\nchannel-wise modulation module, the image details are further\nenhanced, as pointed out by the right arrow in the first row.\nH. Results of Extended Applications\nThe strategy for computing weather scores was tested on\nRainDrop testset [31], Test1 dataset [50] (rain+fog), and\nSnow100K-L testset [51]. Boxplots in Fig. 12 illustrate the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nInput baseline +L +L+G +L+G+C Ground Truth\nFig. 13. Visualization of the ablation study. “L”, “G” and “C” denote local adaptivity, global adaptivity and channel-wise feature modulation respectively.\nInput Simple Averaging Guided by Weather Scores\nFig. 14. Comparisons between the simple averaging strategy and our weather-\nscore-guided strategy on real-world hybrid-weather-degraded images [57].\ndistribution of weather scores for each dataset, showing that\neach dataset scored significantly higher for its corresponding\nweather type than for others. Of all 17,069 test images, only 2\nwere misclassified. Overall, our proposed weather score aligns\nwith the type of weather existing in the picture.\nWe also tested the strategy for guiding pre-trained expert\nmodels on real-world images with hybrid degradations [57].\nThree SOTA pre-trained models were selected as the weather-\nspecific experts: AST [62] for raindrop removal, ConvIR-Rain\n[63] for deraining, and ConvIR-Snow [63] for desnowing. Due\nto the absence of high-quality ground truth, we present the\nvisual results in Fig. 14. To simulate the possible scenarios\nin practical use, we also implemented a comparison strategy:\nprocessing input images with each expert model separately and\nthen averaging the outputs. This approach reflects how sys-\ntems without our hyper-network cannot determine the weather\ncharacteristics of the input and cannot select a suitable expert,\nleading to a simple fusion of results. As shown in Fig. 14,\nwhile the simple averaging strategy required more compu-\ntation, their results were far from satisfactory. In contrast,\nusing the proposed feature extraction hyper-network, we can\ncompute the weather scores of the input image and accordingly\nTransWeather Network MWFormer (ours)\nUnknown Weather\nOutput Clean Images\nUnknown Weather\nOutput Clean Images\nHybrid Weather Input\nMWFormer for Hybrid Weather Inputs (ours)\nFixed-Weight Models\nHyper-Networks\nDynamic-Weight Models\nFig. 15. Comparison between the baseline’s architecture and our architectures.\nselect the most appropriate expert model to eliminate the most\nvisually distracting degradations in the image.\nV. D ISCUSSIONS\nA. Detailed Comparisons with Our Baseline\nDifferent architectures: Fig. 15 compares the architecture\nof MWFormer and our baseline model TransWeather [2].\nAs for the overall framework, TransWeather only contains\nan image restoration backbone, whereas MWFormer addition-\nally uses a feature extraction network to guide the operations\nof the image restoration backbone adaptively. With a well-\ndesigned structure and training strategy, the feature extraction\nnetwork extracts information related to weather features from\nthe Gram matrices.\nAs for the architecture of the image restoration backbone,\nTransWeather employs a common image restoration network\narchitecture with all parameters fixed, which lacks a special\ndesign for the task of multi-weather restoration. On the con-\ntrary, our model is specifically designed for multi-weather\nrestoration by dividing the parameters into two groups, i.e., the\nfixed parameters encoding the general restoration knowledge,\nand the weather-adaptive parameters dynamically generated\nusing the feature vector. In addition to operations in the\nparameter space, the feature vector also modulates the image\nrestoration network in the feature space.\nAdditionally, two test-time variants have been developed:\none for reducing the computational cost, and the other for\nhandling hybrid adverse weather types unseen during training.\nThe proposed MWFormer is the first model capable of restor-\ning images degraded by the unseen hybrid adverse weather.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nTABLE VI\nCOMPARISONS OF PERFORMANCES OF VARIOUS NETWORK ARCHITECTURES WITH OR WITHOUT OUR PROPOSED ADAPTIVE METHODOLOGY . THE\nPERFORMANCES OF THESE NETWORK ARCHITECTURES CAN BE SIGNIFICANTLY IMPROVED IF COMBINED WITH OUR METHODOLOGY .\nModel RainDrop [31] Outdoor-Rain [50] Snow100K [51] Average\nPSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑\nRestormer 29.68 0.9042 28.29 0.8973 28.77 0.8768 28.91 0.8928\nAda-Restormer 29.80 0.9045 29.09 0.9035 29.21 0.8844 29.37 0.8975\nUformer 29.23 0.9266 25.41 0.8785 28.30 0.8732 27.65 0.8928\nAda-Uformer 29.88 0.9292 25.46 0.8841 28.82 0.8817 28.05 0.8983\nUNet 29.19 0.9031 26.40 0.8857 28.60 0.8745 28.06 0.8878\nAda-UNet 29.70 0.9070 27.71 0.8975 29.11 0.8819 28.84 0.8955\nDifferent applications: The application of TransWeather is\nrelatively limited, since it can only handle a few fixed weather\ntypes that have been already seen during the training phase.\nOur proposed MWFormer, with its flexibility, can restore the\nimages impaired by hybrid weather unseen during training.\nThis superiority over TransWeather indicates that MWFormer\nis more applicable to real-world scenarios, where different\nweather types may be commingled. Moreover, the proposed\nfeature extraction hyper-network not only can be combined\nwith MWFormer’s image restoration backbone, but also has a\nwider range of application scenarios, such as identifying the\nweather type, and guiding pre-trained weather-specific expert\nmodels, as introduced in Sec. III-F. Besides, we have also\nexplored ways of training the image restoration model to\nbenefit downstream detection tasks (Sec. IV-F), which is not\naddressed in TransWeather [2].\nB. Generalization Ability\nTo demonstrate the generalization ability of our methodol-\nogy, we integrated our approach into three different network\narchitectures and evaluated the results: two Transformer-based\narchitectures (Restormer [43] and Uformer [44]) and a CNN-\nbased architecture (UNet [64]).\nFor each of the architectures mentioned above, we trained\ntwo versions of the model: one using the original network\nstructure and the other combined with our proposed adaptive\nmethod (denoted as “Ada-xxx”), both models with the same\nhyperparameters and number of channels. For Ada-Restormer\nand Ada-Uformer, we used the feature vector generated by the\nhyper-network to guide the restoration backbone across three\ndimensions and scales: locally spatial-wise, globally spatial-\nwise, and channel-wise. This allows part of the restoration\nbackbone’s parameters to be adaptively generated and its\nintermediate feature maps to be modulated based on the\nfeature vector. Due to limited GPU memory, we reduced the\nencoder channels to 16 and 8 for the first scale of Ada-\nRestormer and Ada-Uformer, respectively, with a batch size of\n16 for both. For Ada-UNet architecture, considering that CNN\ncannot capture long-range dependencies, we only applied the\nadaptivity locally spatial-wise and channel-wise. In addition,\nwe removed the batch normalization layers in Ada-UNet and\nthe original UNet architecture, which are commonly regarded\nas unsuitable for image restoration tasks. The other settings\nare the same as those reported in Sec. IV-A.\nFig. 16. A t-SNE visualization of the distributions of the extracted feature\nvectors from different weather datasets. The feature extraction network learns\neffective embedding that is able to cluster images according to their weather\ndegradation types.\nThe quantitative results are reported in Table VI, indicating\nthat our method can significantly improve the performance of\nvarious network architectures on multiple datasets. These\npromising results show that our proposed approach can be used\nas a general approach to boost the performance of different\nnetwork architectures on multi-weather restoration tasks.\nC. Analysis of the Learned Weather Representation\nTo better illustrate how the learned feature vector improves\nthe performance of the image restoration network, we utilized\nt-SNE [65] to visualize the distributions of the weather-type\nfeatures learned by the feature extraction network Ffeat . As\nshown in Fig. 16, the computed feature embeddings quite\neffectively decoupled the weather degradations accross con-\ntents, since images degraded by the same weather type become\nclosely clustered with little overlap. This suggests that the\nfeature extraction network was able to learn to separate the\ncontent and degradation representations using contrastive loss.\nWe also examined the impact of feature vectors on image\nrestoration using the simple version (Fig. 3(b)) of MWFormer.\nUsing the raindrop removal as an example, we first tested the\nmodel on the Raindrop test set using the default setting for\nfixed weather degradation, meaning that the feature vector was\nthe average of all the feature vectors of the raindrop images\nfrom the Raindrop training set. We then replaced the default\nfeature vector with the feature vector computed on an arbitrary\nimage from the Raindrop testset and the Snow100K testset,\nrespectively. Numerical results in Table VII indicate that\nMWFormer performed the best when using the correct weather\ntype embedding, demonstrating that average feature vectors\neffectively represent their corresponding weather types. The\nperformance slightly declined when using an arbitrary feature\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nTABLE VII\nRESULTS OF USING DIFFERENT FEATURE VECTORS IN THE SIMPLIFIED\nVERSION OF OUR MODEL .\nAverage of Raindrop Arbitrary Raindrop Arbitrary Snow\nPSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑ SSIM ↑\n29.38 0.9073 26.93 0.8961 21.76 0.8139\nvector drawn from an image affected by the same weather\ntype, and significantly dropped when using a feature vector\nof a different weather type. Generally, these results show\nthat the vectors generated by our feature extraction network\neffectively encode weather-dependent information for guiding\nweather restoration tasks. Finally, owing to the design of\nfeature guidance of our MWFormer, the users have the capa-\nbility to arbitrarily control the action of the image restoration\nnetwork by providing a feature vector according to their prior\nknowledge. This kind of flexibility during inference time is a\nkey advantage that is unavailable in prior works.\nVI. C ONCLUDING REMARKS\nWe have introduced an efficient, all-in-one weather-aware\nTransformer, called MWFormer, for restoring images degraded\nby multiple adverse weather conditions. MWFormer consists\nof an encoder-decoder-based restoration backbone, augmented\nby an auxiliary feature extraction hyper-network that learns\nweather-type representations. The extracted feature vectors\ncan be used to adaptively guide the main image restora-\ntion backbone by weight-adaptivity along the local, global,\nand channel axes. They can also be used for weather-type\nidentification or guiding pre-trained expert models. Because\nof the availability of the auxiliary network, MWFormer can\nbe extended to deal with fixed single-weather cases with\nless computation or hybrid-weather cases that were unseen\nduring training. We conducted a spectrum of quantitative and\nqualitative studies on the multi-weather restoration benchmark\ndataset as well as on real-world datasets, and the results\nshow that MWFormer outperforms prior known multi-weather\nrestoration models without requiring much computational ef-\nfort. Our methodology can also be integrated into a variety of\nnetwork architectures to boost their performance.\nREFERENCES\n[1] O. ¨Ozdenizci and R. Legenstein, “Restoring vision in adverse weather\nconditions with patch-based denoising diffusion models,” IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence , vol. 45, no. 8,\npp. 10 346–10 357, 2023. 1, 2, 3, 7, 8, 9, 10, 11\n[2] J. M. Jose Valanarasu, R. Yasarla, and V . M. Patel, “Transweather:\nTransformer-based restoration of images degraded by adverse weather\nconditions,” in 2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) , 2022, pp. 2343–2353. 1, 2, 3, 4, 7, 8, 9,\n10, 11, 13, 14\n[3] S. Li, I. B. Araujo, W. Ren, Z. Wang, E. K. Tokuda, R. H. Junior,\nR. Cesar-Junior, J. Zhang, X. Guo, and X. Cao, “Single image deraining:\nA comprehensive benchmark analysis,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n3838–3847. 1, 2, 11\n[4] H. Wu, Y . Qu, S. Lin, J. Zhou, R. Qiao, Z. Zhang, Y . Xie, and\nL. Ma, “Contrastive learning for compact single image dehazing,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 10 551–10 560. 1\n[5] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeeze-\nand-excitation context aggregation net for single image deraining,” in\nComputer Vision – ECCV 2018, V . Ferrari, M. Hebert, C. Sminchisescu,\nand Y . Weiss, Eds. Cham: Springer International Publishing, 2018, pp.\n262–277. 1, 2\n[6] K. Zhang, R. Li, Y . Yu, W. Luo, and C. Li, “Deep dense multi-scale\nnetwork for snow removal using semantic and depth priors,” IEEE\nTransactions on Image Processing , vol. 30, pp. 7419–7431, 2021. 1\n[7] Y .-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, “Desnownet:\nContext-aware deep network for snow removal,” IEEE Transactions on\nImage Processing, vol. 27, no. 6, pp. 3064–3073, 2018. 1\n[8] M. Li, X. Cao, Q. Zhao, L. Zhang, and D. Meng, “Online rain/snow\nremoval from surveillance videos,” IEEE Transactions on Image Pro-\ncessing, vol. 30, pp. 2029–2044, 2021. 1\n[9] S. Zhao, L. Zhang, Y . Shen, and Y . Zhou, “Refinednet: A weakly\nsupervised refinement framework for single image dehazing,” IEEE\nTransactions on Image Processing , vol. 30, pp. 3391–3404, 2021. 1,\n2\n[10] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang,\n“Benchmarking single-image dehazing and beyond,” IEEE Transactions\non Image Processing , vol. 28, no. 1, pp. 492–505, 2018. 1, 2\n[11] L. Wang, Y . Wang, X. Dong, Q. Xu, J. Yang, W. An, and Y . Guo,\n“Unsupervised degradation representation learning for blind super-\nresolution,” in 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2021, pp. 10 576–10 585. 2\n[12] R. Li, R. T. Tan, and L.-F. Cheong, “All in one bad weather removal\nusing architectural search,” in 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2020, pp. 3172–3182. 2, 3, 7,\n8\n[13] B. Li, X. Liu, P. Hu, Z. Wu, J. Lv, and X. Peng, “All-in-one image\nrestoration for unknown corruption,” in 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , 2022, pp. 17 431–\n17 441. 2, 7, 8, 9, 10, 11\n[14] K. Zhang, W. Zuo, Y . Chen, D. Meng, and L. Zhang, “Beyond a gaussian\ndenoiser: Residual learning of deep cnn for image denoising,” IEEE\ntransactions on image processing , vol. 26, no. 7, pp. 3142–3155, 2017.\n2\n[15] K. Zhang, W. Zuo, and L. Zhang, “Ffdnet: Toward a fast and flexible\nsolution for cnn-based image denoising,” IEEE Transactions on Image\nProcessing, vol. 27, no. 9, pp. 4608–4622, 2018. 2\n[16] K. Zhang, W. Ren, W. Luo, W.-S. Lai, B. Stenger, M.-H. Yang, and\nH. Li, “Deep image deblurring: A survey,” International Journal of\nComputer Vision, vol. 130, no. 9, pp. 2103–2130, 2022. 2\n[17] O. Kupyn, V . Budzan, M. Mykhailych, D. Mishkin, and J. Matas,\n“Deblurgan: Blind motion deblurring using conditional adversarial net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 8183–8192. 2\n[18] O. Kupyn, T. Martyniuk, J. Wu, and Z. Wang, “Deblurgan-v2: De-\nblurring (orders-of-magnitude) faster and better,” in Proceedings of the\nIEEE/CVF international conference on computer vision, 2019, pp. 8878–\n8887. 2\n[19] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham, A. Acosta,\nA. Aitken, A. Tejani, J. Totz, Z. Wang et al. , “Photo-realistic single\nimage super-resolution using a generative adversarial network,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 4681–4690. 2\n[20] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee, “Enhanced deep\nresidual networks for single image super-resolution,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition\nworkshops, 2017, pp. 136–144. 2\n[21] X. Guo, Y . Li, and H. Ling, “Lime: Low-light image enhancement via\nillumination map estimation,” IEEE Transactions on image processing ,\nvol. 26, no. 2, pp. 982–993, 2016. 2\n[22] Y . Jiang, X. Gong, D. Liu, Y . Cheng, C. Fang, X. Shen, J. Yang, P. Zhou,\nand Z. Wang, “Enlightengan: Deep light enhancement without paired\nsupervision,” IEEE transactions on image processing, vol. 30, pp. 2340–\n2349, 2021. 2\n[23] Z. Meng, R. Xu, and C. M. Ho, “Gia-net: Global information aware\nnetwork for low-light imaging,” in European Conference on Computer\nVision. Springer, 2020, pp. 327–342. 2\n[24] J. Xiao, X. Fu, A. Liu, F. Wu, and Z.-J. Zha, “Image de-raining\ntransformer,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, pp. 1–18, 2022. 2\n[25] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha, “Recurrent squeeze-\nand-excitation context aggregation net for single image deraining,” in\nProceedings of the European conference on computer vision (ECCV) ,\n2018, pp. 254–269. 2\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16\n[26] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and\nL. Shao, “Multi-stage progressive image restoration,” in 2021 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2021,\npp. 14 816–14 826. 2\n[27] L. Chen, X. Lu, J. Zhang, X. Chu, and C. Chen, “Hinet: Half instance\nnormalization network for image restoration,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 182–192. 2\n[28] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y . Li,\n“Maxim: Multi-axis mlp for image processing,” in 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2022,\npp. 5759–5770. 2\n[29] R. Yasarla, V . A. Sindagi, and V . M. Patel, “Syn2real transfer learning\nfor image deraining using gaussian processes,” in 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2020,\npp. 2723–2733. 2\n[30] Y . Ba, H. Zhang, E. Yang, A. Suzuki, A. Pfahnl, C. C. Chandrappa,\nC. de Melo, S. You, S. Soatto, A. Wong, and A. Kadambi, “Not just\nstreaks: Towards ground truth for single image deraining,” in European\nConference on Computer Vision , 2022, p. 723–740. 2\n[31] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative\nadversarial network for raindrop removal from a single image,” in 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 2482–2491. 2, 7, 9, 12, 14\n[32] R. Quan, X. Yu, Y . Liang, and Y . Yang, “Removing raindrops and\nrain streaks in one go,” in IEEE Conf. Computer Vision and Pattern\nRecognition (CVPR), 2021, pp. 9143–9152. 2\n[33] Y .-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, “Desnownet:\nContext-aware deep network for snow removal,” IEEE Transactions on\nImage Processing, vol. 27, no. 6, pp. 3064–3073, 2018. 2, 11\n[34] W.-T. Chen, H.-Y . Fang, J.-J. Ding, C.-C. Tsai, and S.-Y . Kuo, “Jstasr:\nJoint size and transparency-aware snow removal algorithm based on\nmodified partial convolution and veiling effect removal,” in Computer\nVision–ECCV 2020: 16th European Conference, Glasgow, UK, August\n23–28, 2020, Proceedings, Part XXI 16 . Springer, 2020, pp. 754–770.\n3\n[35] J. Lin, N. Jiang, Z. Zhang, W. Chen, and T. Zhao, “Lmqformer:\nA laplace-prior-guided mask query transformer for lightweight snow\nremoval,” IEEE Transactions on Circuits and Systems for Video Tech-\nnology, pp. 6225–6235, 2023. 3\n[36] W.-T. Chen, Z.-K. Huang, C.-C. Tsai, H.-H. Yang, J.-J. Ding, and S.-Y .\nKuo, “Learning multiple adverse weather removal via two-stage knowl-\nedge learning and multi-contrastive regularization: Toward a unified\nmodel,” in 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022, pp. 17 632–17 641. 3, 7, 8, 9, 10\n[37] Y . Zhu, T. Wang, X. Fu, X. Yang, X. Guo, J. Dai, Y . Qiao, and\nX. Hu, “Learning weather-general and weather-specific features for\nimage restoration under multiple adverse weather conditions,” in 2023\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023, pp. 21 747–21 758. 3, 7, 8, 9, 10\n[38] S. Lee, T. Son, and S. Kwak, “Fifo: Learning fog-invariant features for\nfoggy scene segmentation,” in 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , 2022, pp. 18 889–18 899. 3\n[39] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” arXiv e-prints , p.\narXiv:2010.11929, Oct. 2020. 3\n[40] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), 2021, pp. 9992–10 002. 3\n[41] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu,\nand W. Gao, “Pre-trained image processing transformer,” in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2021, pp. 12 299–12 310. 3\n[42] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\n“Swinir: Image restoration using swin transformer,” in 2021 IEEE/CVF\nInternational Conference on Computer Vision Workshops (ICCVW) ,\n2021, pp. 1833–1844. 3\n[43] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M. Yang,\n“Restormer: Efficient transformer for high-resolution image restoration,”\nin 2022 IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022, pp. 5718–5729. 3, 14\n[44] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li, “Uformer: A\ngeneral u-shaped transformer for image restoration,” in 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2022,\npp. 17 662–17 672. 3, 14\n[45] L. A. Gatys, A. S. Ecker, and M. Bethge, “A neural algorithm of artistic\nstyle,” arXiv preprint arXiv:1508.06576 , 2015. 4\n[46] T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, “A simple\nframework for contrastive learning of visual representations,” ArXiv, vol.\nabs/2002.05709, 2020. 4\n[47] W. Peebles and S. Xie, “Scalable diffusion models with transformers,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 4195–4205. 5\n[48] T. Karras, S. Laine, and T. Aila, “A style-based generator architecture\nfor generative adversarial networks,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2019, pp. 4401–\n4410. 5\n[49] E. Perez, F. Strub, H. de Vries, V . Dumoulin, and A. C.\nCourville, “Film: Visual reasoning with a general conditioning\nlayer,” CoRR, vol. abs/1709.07871, 2017. [Online]. Available: http:\n//arxiv.org/abs/1709.07871 5\n[50] R. Li, L.-F. Cheong, and R. T. Tan, “Heavy rain image restoration:\nIntegrating physics model and conditional adversarial learning,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2019, pp. 1633–1642. 7, 8, 9, 12, 14\n[51] Y .-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang, “Desnownet:\nContext-aware deep network for snow removal,” IEEE Transactions on\nImage Processing, vol. 27, no. 6, pp. 3064–3073, 2018. 7, 9, 10, 11,\n12, 14\n[52] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time\nstyle transfer and super-resolution,” in Computer Vision – ECCV 2016 ,\nB. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Cham: Springer\nInternational Publishing, 2016, pp. 694–711. 7\n[53] P. W. Patil, S. Gupta, S. Rana, S. Venkatesh, and S. Murala, “Multi-\nweather image restoration via domain translation,” in 2023 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2023, pp. 21 639–\n21 648. 8\n[54] H. Zhang, Y . Ba, E. Yang, V . Mehra, B. Gella, A. Suzuki, A. Pfahnl,\nC. C. Chandrappa, A. Wong, and A. Kadambi, “Weatherstream: Light\ntransport automation of single image deweathering,” in 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023,\npp. 13 499–13 509. 8, 9\n[55] W.-T. Chen, H.-Y . Fang, C.-L. Hsieh, C.-C. Tsai, I.-H. Chen, J.-J. Ding,\nand S.-Y . Kuo, “All snow removed: Single image desnowing algorithm\nusing hierarchical dual-tree complex wavelet representation and con-\ntradict channel loss,” in 2021 IEEE/CVF International Conference on\nComputer Vision (ICCV) , 2021, pp. 4176–4185. 8, 9\n[56] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\nquality assessment: from error visibility to structural similarity,” IEEE\ntransactions on image processing , vol. 13, no. 4, pp. 600–612, 2004. 8\n[57] R. Quan, X. Yu, Y . Liang, and Y . Yang, “Removing raindrops and rain\nstreaks in one go,” in 2021 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , 2021, pp. 9143–9152. 10, 11, 13\n[58] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely\nblind” image quality analyzer,” IEEE Signal Processing Letters, vol. 20,\nno. 3, pp. 209–212, 2013. 11\n[59] G. Jocher, “YOLOv5 by Ultralytics,” May 2020. [Online]. Available:\nhttps://github.com/ultralytics/yolov5 11\n[60] Tesla, “Autopilot.” [Online]. Available: https://www.tesla.com/autopilot\n11\n[61] Z. Liu, H. Wang, T. Zhou, Z. Shen, B. Kang, E. Shelhamer, and\nT. Darrell, “Exploring simple and transferable recognition-aware im-\nage processing,” IEEE Transactions on Pattern Analysis and Machine\nIntelligence, vol. 45, no. 3, pp. 3032–3046, 2023. 12\n[62] S. Zhou, D. Chen, J. Pan, J. Shi, and J. Yang, “Adapt or perish:\nAdaptive sparse transformer with attentive feature refinement for image\nrestoration,” in 2024 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2024, pp. 2952–2963. 13\n[63] Y . Cui, W. Ren, X. Cao, and A. Knoll, “Revitalizing convolutional\nnetwork for image restoration,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence , pp. 1–16, 2024. 13\n[64] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional\nnetworks for biomedical image segmentation,” 2015. [Online].\nAvailable: https://arxiv.org/abs/1505.04597 14\n[65] L. Van der Maaten and G. Hinton, “Visualizing data using t-sne.” Journal\nof machine learning research , vol. 9, no. 11, pp. 2579–2605, 2008. 14",
  "topic": "Image restoration",
  "concepts": [
    {
      "name": "Image restoration",
      "score": 0.6908422708511353
    },
    {
      "name": "Computer science",
      "score": 0.562050461769104
    },
    {
      "name": "Image processing",
      "score": 0.49289464950561523
    },
    {
      "name": "Degradation (telecommunications)",
      "score": 0.48397088050842285
    },
    {
      "name": "Computer vision",
      "score": 0.4837470054626465
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4833326041698456
    },
    {
      "name": "Transformer",
      "score": 0.4575038254261017
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3421841859817505
    },
    {
      "name": "Engineering",
      "score": 0.14987677335739136
    },
    {
      "name": "Telecommunications",
      "score": 0.10498332977294922
    },
    {
      "name": "Electrical engineering",
      "score": 0.09522435069084167
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}