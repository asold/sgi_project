{
  "title": "Reducing Non-Normative Text Generation from Language Models",
  "url": "https://openalex.org/W3094997972",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A797816598",
      "name": "Peng, Xiangyu",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2647653751",
      "name": "Li Siyan",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4226798229",
      "name": "Frazier, Spencer",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2749488306",
      "name": "Riedl Mark",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2410983263",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2964263543",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3019416653",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2067624665",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2794632992",
    "https://openalex.org/W2566902129",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W2950065518",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2972020530",
    "https://openalex.org/W38739846",
    "https://openalex.org/W2240086230",
    "https://openalex.org/W2997764164",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2082445962",
    "https://openalex.org/W2992807692",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2972244901",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2965962253",
    "https://openalex.org/W2993398598",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4300635341",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2102381086",
    "https://openalex.org/W2122194975",
    "https://openalex.org/W1556726625"
  ],
  "abstract": "Large-scale, transformer-based language models such as GPT-2 are pretrained\\non diverse corpora scraped from the internet. Consequently, they are prone to\\ngenerating non-normative text (i.e. in violation of social norms). We introduce\\na technique for fine-tuning GPT-2, using a policy gradient reinforcement\\nlearning technique and a normative text classifier to produce reward and\\npunishment values. We evaluate our technique on five data sets using automated\\nand human participant experiments. The normative text classifier is 81-90%\\naccurate when compared to gold-standard human judgments of normative and\\nnon-normative generated text. Our normative fine-tuning technique is able to\\nreduce non-normative text by 27-61%, depending on the data set.\\n",
  "full_text": "Proceedings of The 13th International Conference on Natural Language Generation, pages 374–383,\nDublin, Ireland, 15-18 December, 2020.c⃝2020 Association for Computational Linguistics\n374\nReducing Non-Normative Text Generation from Language Models\nXiangyu Peng∗, Siyan Li∗, Spencer Frazier, and Mark Riedl\nGeorgia Institute of Technology\nAtlanta, GA 30332\n{xpeng62, sli613, sfrazier7, riedl}@gatech.edu\nAbstract\nLarge-scale, transformer-based language mod-\nels such as GPT-2 are pretrained on diverse cor-\npora scraped from the internet. Consequently,\nthey are prone to generating non-normative\ntext (i.e. in violation of social norms). We\nintroduce a technique for ﬁne-tuning GPT-2,\nusing a policy gradient reinforcement learn-\ning technique and a normative text classiﬁer to\nproduce reward and punishment values. We\nevaluate our technique on ﬁve data sets using\nautomated and human participant experiments.\nThe normative text classiﬁer is 81-90% accu-\nrate when compared to gold-standard human\njudgements of normative and non-normative\ngenerated text. Our normative ﬁne-tuning tech-\nnique is able to reduce non-normative text by\n27-61%, depending on the data set.\n1 Introduction\nHuman societies implicitly establish codes of ac-\nceptable behavior in social contexts. Normativ-\nity is behavior that conforms to expected societal\nnorms and contracts, whereas non-normative be-\nhavior aligns to values that deviate from these ex-\npected norms. Sumner (1967) deﬁnes norms as:\n“...informal rules that are not written, but, when\nviolated, result in severe punishments and social\nsanction upon the individuals, such as social and\nreligious exclusions.” Non-normativity does not\nconnote behavior devoid of value or immoral, but\nbehavior that fails to conform to social standards\nshared by other individuals in the relevant group,\norganization or society. Norms can also be thought\nof as actions taken by an entity which conform to\nan identity (Katzenstein, 1996), thus allowing oth-\ners to categorize behavior as in-group or out-group.\nDifferent societies and groups collectively have\ndifferent ideals about what actions constitute nor-\nmative behavior; group members use these ideals\n∗Equal contributions\nto heuristically guide their actions to avoid social\nostracization. For example, many societies have\nnorms against violence, or certain behaviors being\nconducted in public. Conﬂicts between individuals\ncan arise when enacting non-normative behaviors\nor uttering non-normative speech.\nThis paper examines generative language mod-\nels and the frequency at which they generate de-\nscriptions of non-normative behavior. Large-scale,\ntransformer-based neural language models such as\nELMo (Peters et al., 2018), BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), GPT-3, Grover\n(Zellers et al., 2019), CTRL (Keskar et al., 2019),\nT5 (Raffel et al., 2019), and XLNet (Yang et al.,\n2019) are trained on very large corpora such as text\nscraped from the internet, books, or both.\nThese language models generate text that is sta-\ntistically representative of the corpora they were\ntrained on. As such, text scraped from the inter-\nnet co-mingles text produced by many groups with\ndiffering norms, as well as text produced by peo-\nple intentionally using non-normative speech, like\n“trolling” language. Models trained on these data\ncan then produce undesirable, harmful output. Sto-\nries from the internet and books also contain norma-\ntive and non-normative situations (e.g., antagonists,\nas well as protagonists conducting conventionally\nnon-normative behaviors). Consequently, it is pos-\nsible, and often likely, for language models to gen-\nerate non-normative descriptions of behavior (mur-\nder, crime, suicide, racist actions, rude behavior,\netc.), exhibit biases against certain demographics\ngroups (Sheng et al., 2019; Solaiman et al., 2019),\nstereotypical biases (Nadeem et al., 2020) or racist\ntext when prompted with trigger phrases (Wallace\net al., 2019).\nValue alignment (Russell et al., 2015) is the con-\ncept that an agent is unable to perform actions that\ncause harm to humans. Harmful behavior is not\nlimited to physical actions by robots, the focus\n375\nof some AI value alignment research. We recog-\nnize that natural language communication can also\ncause harm. For example, Amazon Alexa, a virtual\nassistant AI, was reported to suggest a user commit\nsuicide.1 Frazier et al. (2019) developed a classiﬁer\nfor normative behavior which exhibits strong zero-\nshot and few-shot transfer across a variety of text\ncorpora. The authors speculate that their model—\nwhich they call a value-aligned prior—can bias\nmodel output perceived as more normative. In this\npaper we ask a different question: whether a value-\naligned prior can be used to reduce the generation\nof descriptions of non-normative behavior by neu-\nral language models.\nThe common approach to ﬁne-tuning language\nmodels is to provide additional corpora of exem-\nplars. If a corpus of exemplars is normative, the\nlanguage model can be trained to emulate this over\ntime. Generally, in the absence of very large nor-\nmative corpora, we need an alternative approach to\nﬁne-tuning language models. We use a reinforce-\nment learning approach to ﬁne-tuning language\nmodels, using the normative behavior classiﬁer\nof Frazier et al. (2019) as a non-differentiable re-\nward function. Our method back-propagates re-\nward relative to the degree of non-normativity of\ntext generated by the language model.\nWe evaluate our reinforcement learning ﬁne-\ntuning technique with three sets of experiments.\nFirst, we replicate the experiments by Frazier et al.\n(2019) on text generated by a language model in-\nstead of originally held-out corpus text. Second,\nwe show with automated and human participant ex-\nperiments that ﬁne-tuning on reward generated by\na normative classiﬁer model can reduce the genera-\ntion of non-normative text by 27 −61%. Third, we\nablate our technique and show with automated and\nhuman participant experiments that the ﬁne-tuning\ntechnique works with classiﬁers other than the nor-\nmative classiﬁer—speciﬁcally models trained to\nclassify negative-sentiment and toxic language.\n2 Background and Related Work\n2.1 Value Alignment and Normative Priors\nHumans have expectations that — just like other hu-\nmans — agents will avoid harmful actions, conform\nto personal values and to social norms (Bicchieri,\n2005), even when not explicitly communicated.\nThis is referred to as the value alignment problem\n1https://www.newsweek.com/amazon-echo%\n2Dtells-uk-woman-stab-herself-1479074/\n(Soares and Fallenstein, 2014; Russell et al., 2015;\nArnold et al., 2017; Abel et al., 2016). Harmful\nagent behavior can theoretically be mitigated by\ncasting values as preferences over action sequences.\nFor example Christiano et al. (2017) collected hu-\nman preferences to shape rewards for game-playing\nagents in reinforcement learning.\nInstead of preference learning, Frazier et al.\n(2019) used the BERT (Devlin et al., 2018) lan-\nguage model’s token embeddings to train a binary\nclassiﬁer. This model is used to differentiate be-\ntween normative and non-normative natural lan-\nguage sentences containing events, utterances and\ndescriptions of behavior. They obtained training\ndata from Goofus & Gallant (G&G), a children’s\neducational comic strip featuring two characters\nof the same names. Goofus always deviates from\nthe “proper” way to behave, while Gallant always\nperforms the behavior of an exemplary child in\nwestern society at the time the comics were created.\nAs a result, G&G is a naturally labeled source of\nnormative and non-normative text, for the speciﬁc\nsociety it represents.\nFrazier et al. (2019) demonstrated this method\ncould accurately classify descriptions of behav-\nior as normative or non-normative. Furthermore,\nthis classiﬁer retained high performance in zero-\nshot and few-shot transfer tasks. For example,\nthey show that their classiﬁer, trained on G&G\ncomics, can classify normative event descriptions\nin contemporary collections of popular plot points\nand science ﬁction plot summaries, instances of\nmedium- and far-transfer, respectively. The au-\nthors speculate that their classiﬁer model can bias\nagent behavior toward normative courses of action\nin other contexts. However, this was not directly\nshown. We ask whether a normative classiﬁer can\nbe used to ﬁne-tune the “behavior” of a large-scale\ntransformer-based language model.\n2.2 Language Model Training & Fine-Tuning\nLarge-scale transformer-based neural language\nmodels such as BERT and GPT-2 are trained on\nlarge corpora of text scraped from the web and\nbooks. They can be ﬁne-tuned to a speciﬁc domain\nof interest, commonly accomplished by providing\na corpus of exemplars from that domain. Over time,\nthe weights of the pre-trained model will shift and\nincreasingly generate passages which better emu-\nlate the corpus of exemplars. If the ﬁne-tuning cor-\npus of exemplars is normative, the language model\n376\nwill, in theory, learn to prefer normative language\nover time. Goofus & Gallant is one such normative\ncorpus, and if a language model is ﬁne-tuned on it\nthen it may prefer to generate normative language.\nGPT-2 (Radford et al., 2019), in particular, is\na large-scale transformer-based language model\ntrained on a large corpus of text scraped from web\npages and social media. Applying the concept\nof value alignment as preference learning, Ziegler\net al. (2019) use a reinforcement learning method\non the 774M-parameter version of GPT-2 to favor\nhuman-preferred text. Crowd workers were asked\nto select generated text completions from a set of\ngiven prompts that had positive sentiment. These\npreference values were used to ﬁne-tune GPT-2.\nThis is one possible technique for reinforcement-\nbased ﬁne-tuning; sentiment is, however, not nec-\nessarily a good measure of adherence to norms.\nWe replace the linear reward model for ﬁne-tuning\nGPT-2 (Ziegler et al., 2019) with a pre-trained nor-\nmative text classiﬁer.\nThe Plug and Play Language Models (PPLM)\n(Dathathri et al., 2019) also apply attribute classi-\nﬁers to ﬁne-tune language models; the technique is\ndemonstrated via generating text with a target sen-\ntiment and also decreasing the frequency of toxic\nlanguage. There are two limitations: (a) PPLM\ntrains a model to operate on a ﬁxed set of preﬁx\ninput, and (b) the classiﬁcation must be done on\na word-by-word basis and thus cannot easily be\napplied to problems where the normative valence\nof individual words relies on a single or multiple\nsentence context (e.g. quoting and admonishing\ntoxic speech). Our ﬁne-tuning technique, in con-\ntrast, works on arbitrary preﬁxes and assesses the\nthe normativity of entire sentences.\n2.3 Datasets\nWe make use of ﬁve datasets, chosen to represent a\ndiverse set of domains. The normative text classi-\nﬁer by Frazier et al. (2019) was tested on a corpus\nof science ﬁction plot summaries (Ammanabrolu\net al., 2019) as well as a new Plotto dataset, based\non a book by the same name that catalogues plot\npoints for scaffolding ﬁctional story-writing. Story\ncorpora are particularly good for testing problems\npertaining to textual descriptions of normative and\nnon-normative behavior. Stories contain antago-\nnists that frequently violate societal norms and pro-\ntagonists who are more likely to exemplify con-\ntemporary social norms. We recognize many sto-\nries require protagonists to perform non-normative\nbehaviors like violence against others to achieve\nnormative ends, further indicating the importance\nof accounting for a broader frame of context when\ndetermining normativity.\nThe science-ﬁction plot summary corpus (Am-\nmanabrolu et al., 2019) is a collection of 2,276 sto-\nries scraped from crowd-sourced plot summaries\non fan sites. These stories have an average length\nof 89.23 sentences. Sentences in this corpus tend to\ngive high-level overviews of the actions that charac-\nters are performing (e.g. “Lyta accuses Sinclair of\nattempting to murder the ambassador”). The sci-ﬁ\ncorpus also presents a transfer challenge because it\ninvolves a lot of novel entities—aliens, spaceships,\nlaser weapons, etc.—that do not exist in Goofus &\nGallant. It is notable that a normative text classiﬁer\ntrained on G&G would do well on zero-shot trans-\nfer to the sci-ﬁ corpus. This makes it an attractive\ndataset for our experiments for the same reasons.\nThe Plotto dataset consists of 900 sentences ex-\ntracted from a book, which catalogues plot points\nused in popular ﬁction. Frazier et al. (2019) pruned\nsome exceptionally anachronistic and misogynis-\ntic sentences from the corpora. These sentences\napproximate the level of abstraction in the sci-ﬁ\ncorpus but have more contemporary narratives.\nThe ROCstories (Mostafazadeh et al., 2016) cor-\npus contains 52,666 ﬁve-sentence stories, often\nabout everyday life situations (e.g. going for a jog,\ntaking a test in school, etc.). Unlike the previous\ntwo corpora, it covers a different space of more\ncommon, mundane events which usually do not\nhave strong normativity connotations.\nSentiment is often used as a surrogate for norma-\ntivity under the belief that non-normative behavior\nwould be associated with negative sentiment. The\nrelationship between normativity and sentiment is\nnot that simple, as we will show in Section 4.3. We\ninclude sentiment experiments using large review\ndatasets from IMDb, Yelp, and Amazon (Kotzias\net al., 2015) because (1) previous value alignment\nresearch has incorporated sentiment analysis, and\n(2) we want to test our techniques on classiﬁers\nother than the normative text classiﬁer.\nNon-normativity is a superset of toxic language\nin the sense that toxic language is non-normative,\nbut not all non-normative descriptions are toxic.\nWe also conduct experiments using toxic language\nclassiﬁers - ﬁne-tuned on sentiment corpora like\nthe dataset from the Toxic Comment Classiﬁcation\n377\nChallenge2 as an alternative to the normative text\nclassiﬁer ﬁne-tuned on G&G.\n3 Normative Fine-Tuning\nThe GPT-2 model is trained by minimizing its\ncross-entropy loss given by (Radford et al., 2019):\nlossw(X,y) =−log\n( exp(X[y])∑\ni∈V exp(X[i])\n)\n= −log (σ(X)y) (1)\nwhere X is a vector containing output logits and y\nis the index of the word from the ground truth in\nX. V is the model’s vocabulary,σis the softmax\nfunction, and σ(X)y is the ground truth probability\nof the word.\nTo punish GPT-2 for producing non-normative\ntext, we use a normative text classiﬁer to eval-\nuate the model’s performance and produce a re-\nward value, which is applied to the loss and back-\npropagated through GPT-2. Given that the norma-\ntivity of a sentence can only be determined by read-\ning the entire sentence, the classiﬁer must there-\nfore produce a single numeric value per sentence.\nSpeciﬁcally, we augment the cross-entropy loss\ncomputation with predictions from the pre-trained\nclassiﬁer. We deﬁne the sentence loss as:\nlosss(s) = 1\nn\n∑\nj∈s\nlossw(Xj,yj) +u(s) (2)\nwhere sis the continuation sentence generated by\nthe neural language model, n = |s|− 1 is the\nnumber of the words in the continuation sentence,\nXj is the jth logit vector, and yj is the ground truth\nindex for the jth word. u(s) is a function of the\noutput of the classiﬁer converted into a punishment\nvalue; a value of zero indicates no punishment, and\nhigher positive values indicating increasingly non-\nnormative sentences. The lossw counter-balances\nthe reward and prevents the generated texts from\ndescending into incoherent fragments.\nThe punishment function u(s) generates a value\nproportional to the average word loss so that it does\nnot become overwhelmed by lossw:\nu(s) =ρβ(1 −C(s))( 1\nn\n∑\nj∈s\nlossw(Xj,yj)) (3)\nwhere sis a continuation sentence, C(s) is the bi-\nnary {0,1}label given by the normative classiﬁer,\n2https://www.kaggle.com/c/jigsaw%\n2Dtoxic-comment-classification-challenge/\nInput\nSentencesInput\nSentencesInput\nSentences\nNormative\nText Classifiersentence\nCompute \nloss\nGPT-2\n...\n[0,1]\nFigure 1: Pipeline for ﬁne-tuning GPT-2 with the clas-\nsiﬁer. Loss is backpropagated through the output logits\nto GPT-2.\nρis a hyper-parameter to control the strength of\nthe penalty, and β = (1−i×0.05) decreases the\npenalty as the number of ﬁne-tuning iterations i\nincreases. That is, if the generated sentence is clas-\nsiﬁed as normative, a loss close to zero will be ap-\nplied to each logit generated by the language model.\nIf the generated sentence is non-normative, a higher\ntotal sentence loss will be applied to each logit. β\ndecreases the step size during back-propagation to\navoid over-shooting the local minima. lossw acts\nas a cycle loss component, punishing the sentences\nwith undesirable characteristics.\nThe ﬁne-tuning process is as follows: given a set\nof input sentences from a corpus, GPT-2 is used to\ngenerate successor sentences. We generate 60 to-\nkens and truncate at the ﬁrst punctuation mark (e.g.\nperiods, question marks). These continuation sen-\ntences are fed through a classiﬁer, which outputs\nthe binary label we treat as a rewardC(s) ∈{0,1}.\nSentences labeled as 0 are those with undesirable\ncharacteristic. As per Equation (3), the reward\nis used to calculate the punishment score by sub-\ntracting from 1.0 and scaling by the average word\nloss of the sentence. This value is then used to\ncompute a sentence loss as in Equation (2). The\nsentence loss is averaged to obtain the token-level\nloss, which is then added to each logit from the con-\ntinuation sentence and the loss is back-propagated\ninto GPT-2. The process is illustrated in Figure 1.\nTo prevent the model from deviating too much\nfrom the language in the original dataset, we feed\nthe ﬁne-tuned model with the same set of input sen-\ntences at every loop and use the output sentences\nto even further ﬁne-tune the model. As the model\nis trained, the output sentences will differ, and the\n378\nAccuracy Accuracy # of\nDataset (continuations) (test corpora) sent.\nPlotto 81.25 89.67 100\nSci-ﬁ 82.11 87.51 300\nROCstories 90.57 94.56 100\nToxic 86.84 94.27 400\nSentiment 88.14 93.90 200\nTable 1: Results of Mechanical Turk study. Accuracy\non generated continuations equals to the percentage of\nMechanical Turk worker labels equivalent to labels pro-\nduced by the normative classiﬁer when classifying gen-\nerated sentences, since Mechanical Turk worker labels\nare considered as ground truth label of generated con-\ntinuations. Accuracy on original corpora is measured\nby the classiﬁer on the held-out test sets of corpora sen-\ntences.\nreward value C(s) may change after every iteration\nas the model shifts its distribution.\n4 Experiments\nWe conduct three sets of experiments to (1) verify\nthe normative text classiﬁer on generated continu-\nations, (2) evaluate our reward-based ﬁne-tuning\nwith the normative text classiﬁer, (3) evaluate our\nreward-based ﬁne-tuning on other classiﬁers.\n4.1 Experiment 1: Replication of the\nNormative Classiﬁer\nThe normative text classiﬁer by Frazier et al. (2019)\nwas evaluated on original sentences from a num-\nber of corpora, including the science ﬁction story\ncorpus (sci-ﬁ) we use in subsequent evaluation ex-\nperiments. Generated text potentially constitutes a\nshift in the text distribution. Hence, the accuracy\nof classiﬁers on generated continuations must be\nvalidated.\nThe 117M parameter GPT-2 is ﬁne-tuned with\ntraining sets from Plotto, ROCstories, sci-ﬁ, Toxic\nand Sentiment datasets, separately, in order to shift\nthe output probability distribution of GPT-2 and to\nmake it generate text similar to the corpus we used\nfor training. The sci-ﬁ and Plotto corpora were\nused for ﬁne-tuning two different versions of the\nnormative text classiﬁer, starting with the classiﬁer\nby Frazier et al. (2019). Thus the original classi-\nﬁer originally trained on G&G was updated to the\nrespective domains; a few-shot transfer paradigm.\nWe ﬁne-tuned the classiﬁers for 2-5 iterations.\nFor the ROCstories, Toxic and Sentiment\ndatasets, we directly train a BERT-based classi-\nﬁer on the given labels instead of ﬁne-tuning the\nclassiﬁer that was ﬁrst trained on G&G. We found\nthe G&G-trained classiﬁer did not transfer well to\nROCstories and thus collected our own normative\nand non-normative labels. Toxic and Sentiment\nexperiments do not look at normativity so we did\nnot use the normative classiﬁer.\nA human participant study was then conducted\nto validate the ﬁne-tuned classiﬁer’s accuracy. Sen-\ntences from each corpus test set were randomly\nchosen and used as prompts for GPT-2 to generate\ncontinuation sentences. 70 crowd workers on Me-\nchanical Turk labeled those generated sentences as\nnormative or non-normative (or positive or negative\nsentiment, or toxic or non-toxic). Each sentence\nreceived at least three labels. We treat the majority\nlabel from humans participants as the ground-truth.\nTable 1 shows the accuracy of classiﬁers on gen-\nerated continuations and on sentences directly from\nthe test sets. Accuracy decreases on generated con-\ntinuations, but are on par with accuracy on sen-\ntences taken directly from the test corpora, and on\npar with the results from Frazier et al. (2019). This\nindicates that any distributional shift during gen-\neration is likely inconsequential and the classifer\nachieves good zero-shot transfer to more datasets.\n4.2 Experiment 2: Decreasing\nNon-Normative Generation\nIn this set of experiments, we seek to determine if,\nand by how much, the amount of non-normative be-\nhavior descriptions generated by GPT-2 decreases\nwhen ﬁne-tuned with the normative text classi-\nﬁer. We emphasis the decrease of non-normative\nlanguage because both normative and neutral lan-\nguages are acceptable.\nConsistent with Experiment 1, we ﬁrst ﬁne-tune\nthe 117M parameter version of GPT-2 with sen-\ntences sampled from three datasets: ROCstories,\nsci-ﬁ, and Plotto. This gives us three versions\nof GPT-2: GPT-ROCstories, GPT-sciﬁ and GPT-\nplotto, respectively. The 117M GPT-2 model is\nﬁne-tuned for two, three and ﬁve iterations sep-\narately on ROCstories, Plotto and sci-ﬁ to avoid\noverﬁtting. We then ﬁne-tune each of these mod-\nels a second time using the reinforcement learning,\nreward-based technique in Section 3. We refer to\nthese models as GPT-ROCstories-norm, GPT-sci-ﬁ-\nnorm, and GPT-Plotto-norm, respectively. Due to\nthe small size of the datasets, GPT-2 easily overﬁts\nduring training. Therefore, we only ﬁne-tune one\nof its 12 attention heads to avoid overﬁtting.\nWe evaluate the performance of our ﬁne-tuned\n379\n% non-norm. Test\nModel Auto Human Perpl. size\nGPT-ROCstories 64.15 58.49 81.097 50\nGPT-ROCstories-norm 26.42 22.64 82.958 50\nGPT-Plotto 81.25 72.91 34.271 50\nGPT-Plotto-norm 59.18 53.06 32.322 50\nGPT-sciﬁ 35.11 26.58 23.885 300\nGPT-sciﬁ-norm 15.79 18.27 24.522 300\nTable 2: The proportion of non-normative behavior and\nevents (% Non-norm) generated by different ﬁne-tuned\nmodels on different datasets. Ratios are measured us-\ning the normative text classiﬁer (automated) and Me-\nchanical Turk studies (human labeling).\nGPT-X-norm models ( X=plotto, sciﬁ, ROCsto-\nries) by analyzing the change in the proportion of\ngenerated text that is non-normative. We measure\nthe ratio of non-normative to normative text in two\nways. First, we use the normative text classiﬁer on\ncontinuations generated by baselines and ﬁne-tuned\nmodels. This is an automated evaluation; Exper-\niment 1 suggests the normative text classiﬁer has\nhigh accuracy on the continuations. However, the\ngold standard is the human participant labels. For\nour second evaluation metric, we hired 70 Mechan-\nical Turk workers to label generated continuation\nsentences as normative (including neutral) or non-\nnormative. At least 3 crowd workers labeled each\nsentence and the majority vote is considered as the\nground truth label.\nTable 2 shows the proportions of non-normative\nsentence continuations for both the baseline and\nthe ﬁne-tuned models. The results are summarized\nbelow. We note percentage decreases, which are\nthe relative percentage decrease compared to the\noriginal statistics.3\n•GPT-ROCstories generates non-normative\ncontinuations 64% of the time according to\nthe normative text classiﬁer, which reduces\nto 26% after further ﬁne-tuning, a 59% de-\ncrease. Humans label GPT-ROCstories con-\ntinuations as non-normative 58% of the time,\nwhich drops to 22%, a 61% decrease.\n•GPT-Plotto generates non-normative contin-\nuations 81% of the time according to the nor-\nmative text classiﬁer, which reduces to 59%\nafter further ﬁne-tuning, a 27% decrease. Hu-\nmans label GPT-Plotto continuations as non-\nnormative 72% of the time, which drops to\n3Percentage decrease is calculated by (p − ˆp)/p, where p\nand ˆp are the proportion of non-normative behavior (% Non-\nnorm) generated by GPT-X and GPT-X-norm, respectively.\nLabel Sentence\nNon-norm. Mollari now refuses to pay the two parents’\nexpenses and lives.\nNon-norm. Garibaldi slaps the door behind them and\nlocks it behind them.\nNon-norm. He considers himself morally superior to\nhis family because he is wealthy.\nNorm. Nathaniel repays his debt through an hon-\nest act of honest enterprise.\nNorm. He then makes a generous and appropriate\nsacriﬁce.\nNorm. He returns home to support his country.\nTable 3: Examples of generated normative and non-\nnormative sentences from GPT-Plotto and GPT-sciﬁ.\n53%, also a 27% decrease.\n•GPT-sciﬁ generates non-normative continua-\ntions 35% of the time according to the nor-\nmative text classiﬁer, which reduces to 15%\nafter further ﬁne-tuning, a 55% decrease. Hu-\nmans label GPT-sciﬁ continuations as non-\nnormative 26% of the time, which drops to\n18%, a 31% decrease.\nWe observe that the classiﬁer results are generally\nin line with the human evaluation results. The mod-\nels ﬁne-tuned on the Plotto dataset generally gener-\nate more non-normative continuation sentences and\nare more difﬁcult to induce normativity. The mod-\nels ﬁne-tuned on the sci-ﬁ dataset have the lowest\nfrequency of non-normative generations, but can\nstill be induced to produce lower frequencies with\nthis method.\nThe perplexity remains steady after ﬁne-tuning\nwith the normative text classiﬁer, indicating that\nthe GPT-X-norm models are not overﬁtting nor\nlosing their ﬂuency. Table 3 shows some examples\nof sentences generated by the ﬁne-tuned GPT-2\nmodels for the sci-ﬁ and Plotto domains.\n4.3 Experiment 3: Other Classiﬁers\nIn the previous sections we evaluate how the nor-\nmative text classiﬁer and our ﬁne-tuning technique\nwork together to decrease the generation of non-\nnormative text. In this section, we ablate our sys-\ntem and evaluate our ﬁne-tuning method indepen-\ndently of the normative text classiﬁer. We seek\nto determine whether the ﬁne-tuning technique is\ngeneral enough to work with other classiﬁers.\nWe replicate the experimental methodolgy in\nSection 4.2 but with the Toxic Comment Classiﬁca-\ntion dataset and the Sentiment dataset. Sentiment\nis often used as a surrogate for normativity because\n380\n% Non-norm Test\nModel Auto Human perpl. size\nGPT-toxic-ext 59.03 57.01 54.819 200\nGPT-toxic-ext-norm 27.75 30.70 62.302 200\nGPT-senti-ext 71.13 76.29 83.717 100\nGPT-senti-ext-norm 45.36 42.27 83.443 100\nGPT-toxic-bal 37.89 33.04 50.535 200\nGPT-toxic-bal-norm 24.79 28.76 60.100 200\nGPT-senti-bal 44.33 36.08 91.927 100\nGPT-senti-bal-norm 35.05 29.90 90.261 100\nTable 4: The proportion of non-normative behavior and\nevents (% non-norm.) generated by different ﬁne-tuned\nmodels on different datasets.\nnon-normative behavior might be inferred to be\nperceived with negative sentiment, and because\nlabeled sentiment data is more readily available.\nToxic language is a subclass of non-normative be-\nhavior.\nFirst, we train two classiﬁers using the same\ntechnique as Frazier et al. (2019). Speciﬁcally, we\nﬁne-tune BERT on the datasets with ground-truth\nsentiment and toxicity labels. Both classiﬁers are\nﬁne-tuned 5 times on training set of corpora and\ntested on test sets (see Table 1).\nWe follow the methodology in Section 4.2 to\nproduce new GPT-2 baseline models. To make the\nimprovement from applying our technique more\nvisible, we ﬁne-tuned the 117M GPT-2 with only\ntoxic or negative sentences when producing the\nbaseline models, and obtained GPT-senti-ext and\nGPT-toxic-ext, which frequently produce negative\nor toxic generated continuations. We also ﬁne-\ntuned the 117M GPT-2 with balanced datasets (half\nnegative texts and half toxic texts, respectively),\nand refer to these two models as GPT-senti-bal\nand GPT-toxic-bal. We then further ﬁne-tune these\nmodels using their respective classiﬁers per the\ntechnique in Section 4.2.\nTable 4 shows the percentage of textual continu-\nations that are either toxic or contain negative sen-\ntiment. Fine-tuning GPT-senti-ext with the senti-\nment classiﬁer can reduce negative sentiment from\n76% to 42%, a 45% reduction. Fine-tuning GPT-\nsenti-bal with the sentiment classiﬁer can reduce\nnegative sentiment from 36% to 29%, a 17% re-\nduction. Fine-tuning GPT-toxic-ext with the toxic\nclassiﬁer can reduce toxic language from 57% to\n30%, a 46% reduction. Fine-tuning GPT-toxic-bal\nwith the toxic classiﬁer can reduce toxic language\nfrom 33% to 28%, a 13% reduction. This shows\nthat the ﬁne-tuning technique working on sentence\nloss is agnostic to which classiﬁer is used.\nWe did not compare our results directly to\nthe Plug and Play Langauge Models (PPLM)\nwork (Dathathri et al., 2019), which also uses a\ntoxic word classiﬁer to ﬁne-tune a language model.\nPPLM ﬁne-tunes on a word-by-word basis instead\nof at the sentence unit, making it difﬁcult to account\nfor the context needed for determining normativity.\nFor toxic language reduction, their model is trained\nto operate on a pre-given set of prompts such as\n“black” or “asian”. For these prompts, given dur-\ning training, they can reduce GPT-2’s toxic lan-\nguage frequency from ∼10% to ∼6%. This is\nnon-signiﬁcant (p <0.23) though it is challeng-\ning to reduce a number that is already close to zero.\nWhen we prompt ourGPT-toxic-bal and GPT-toxic-\nbal-norm with “asian” and “black”, we see reduc-\ntions from 52% to 32% and from 70% to 50%,\nrespectively. Our classiﬁer has only ever seen the\nword “black” once and has never seen the word\n“asian”. We see higher occurrences of toxicity be-\ncause GPT-2 is ﬁne-tuned on the equal numbers\nof toxic and non-toxic sentences from the Toxic\ndataset (whereas PPLM compares against a non-\nﬁne-tuned version of GPT-2) and because GPT-2\nhas a pre-existing unjust bias toward these words.\nTo address the relationship between sentiment\nand normativity, we sample 300 sentences from the\nsci-ﬁ corpus and 300 sentences from the genera-\ntion results of the trained GPT-2 model and classify\nthem using the normative text classiﬁer and Sen-\ntiWordNet (Esuli and Sebastiani, 2006). Figure 2\nshows the percentage of sentences were classiﬁed\nas (a) both normative and positive/neutral sentiment\n(orange), (b) both non-normative and negative sen-\ntiment (blue), (c) normative but negative sentiment\n(green), and (d) non-normative but positive/neutral\nsentiment (brown). Only about half the sentences\ntested (53.08%) had sentiment and normativity la-\nbels that matched, whereas 46.92% of sentences\nhave conﬂicting labels.\n5 Discussion\nWe demonstrate how value-aligned priors such\nas normative text classiﬁers can act as a reward\nprovider to nudge the GPT-2 language model to-\nwards producing more normative and neutral de-\nscriptions of behaviors and events. Applying\nthis reward-based ﬁne-tuning technique reduces\nthe likelihood of generating sentences containing\nnon-normative behavior by approximately ∼27-\n61%, depending on the dataset. Some datasets\n381\nFigure 2: Differences between normative classiﬁcation\nand sentiment classiﬁcation.\nare more non-normative and more resistant to re-\nduction of non-normativity. Datasets with low non-\nnormativity to begin with are, naturally, more resis-\ntant.\nBeyond the numerical results, this shows ev-\nidence that policy-gradient based reinforcement\nlearning approaches to ﬁne-tuning can be an ef-\nfective means for reducing the generation of non-\nnormative descriptions. By using a normative text\nclassiﬁer—a value aligned prior —one can ﬁne-\ntune a language model to the desired domain and\nthen ﬁne-tune using the prior again to reduce non-\nnormative generation that stems either from GPT-2\nor from the domain corpus. We provide this ap-\nproach as an alternative—or in complement to—\ndebiasing techniques that attempt to correct for\nprejudicial bias in datasets prior to training. Our\napproach is roughly equivalent to teaching a lan-\nguage model to censor itself.\nThe policy-gradient based reinforcement learn-\ning technique using a value-aligned prior can be\neven more valuable with GPT-3, where ﬁne-tuning\nto a domain is less necessary. GPT-3 has been\ndemonstrated to be capable of non-normative, toxic,\nand prejudicially biased generation.\nBy using the normative text classiﬁer trained\non Goofus & Gallant comics, our results are lim-\nited to Western—and in particular American—\nmainstream ideals of normative behavior. We ac-\nknowledge that culture is not monolithic, even\nwithin the United States of America, and this repre-\nsents only one of many possible sources of norma-\ntivity. We cannot conclusively say that our results\nwill hold if we had normative text classiﬁers trained\nfrom different source materials. Because general\nsources of normative behavior are currently hard to\ncome by, we attempt to show generalization of our\ntechnique with experiments using sentiment and\ntoxic language.\nOne limitation of our work is that the normative\ntext classiﬁer can only classify individual sentences\nwithout context. Given the context-dependent na-\nture of normativity, the normative text classiﬁer\nmay overlook non-normative sentences that may\nappear normative out-of-context. This may lead\nto GPT-2 still producing this sentence in its non-\nnormative context. Another limitation is that ﬁne-\ntuning GPT-2 on Plotto, ROCstories and sci-ﬁ\ndatasets leads to it generating both neutral and nor-\nmative sentences. If a model that generates solely\nnormative sentences is desired, one can substitute\nthe normative classiﬁer with a ternary classiﬁer\nwith labels for normative, non-normative, and neu-\ntral sentences, and adjust the reward signals accord-\ningly. Furthermore, ﬁne-tuning classiﬁers requires\ndatasets with labeled exemplars, hence, in order\nto replicate our work to generate texts with some\nother desirable characteristics, datasets with labels\nwould be prerequisites.\nThe motivation of our work is to show how those\nwho are concerned with generating undesirable text\nlanguage models can obtain some control over the\ngeneration process. We look at normativity, but\nalso show how other criteria can be applied. How-\never, as is true for most algorithms, those with\nmalicious intent can ﬁnd ways to corrupt the inten-\ntions of the work. For example, equation (3) can be\ntrivially modiﬁed to punish normative text instead\nof non-normative text.\n6 Conclusions\nWe have shown that large-scale transformer-based\nneural language models can be made to gener-\nate text containing fewer descriptions of non-\nnormative behavior by applying data-efﬁcient,\npolicy-gradient reinforcement learning. As most\nlarge-scale language models are trained on datasets\nfrom the internet and from books, the potential\nfor intentional or unintentional non-normative lan-\nguage persists. We see this as a ﬁrst step toward\ndecreasing the potential for unintended, unaccept-\nable, anachronistic or harmful language.\nWhile our primary result is to show that we can\ndecrease the generation of non-normative behav-\nior descriptions, our normative classiﬁer of choice\nis rooted in Western/American norms and values.\nNormative classiﬁers are rare and datasets contain-\ning normative or preference learning examples are\ndifﬁcult to obtain. However, our results show that\neven small datasets of normative examples can be\nconverted into few-shot classiﬁers and applied to\n382\nnew domains. By replicating our results with sen-\ntiment and toxic classiﬁer, we show that our tech-\nnique is not speciﬁc to any one classiﬁer.\nReferences\nDavid Abel, James MacGlashan, and Michael L\nLittman. 2016. Reinforcement learning as a frame-\nwork for ethical decision making. In Workshops at\nthe Thirtieth AAAI Conference on Artiﬁcial Intelli-\ngence.\nPrithviraj Ammanabrolu, Ethan Tien, Wesley Che-\nung, Zhaochen Luo, William Ma, Lara J Martin,\nand Mark O Riedl. 2019. Story realization: Ex-\npanding plot events into sentences. arXiv preprint\narXiv:1909.03480.\nThomas Arnold, Daniel Kasenberg, and Matthias\nScheutz. 2017. Value alignment or misalignment–\nwhat will keep systems accountable? In Workshops\nat the Thirty-First AAAI Conference on Artiﬁcial In-\ntelligence.\nCristina Bicchieri. 2005. The grammar of society: The\nnature and dynamics of social norms . Cambridge\nUniversity Press.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nAdvances in Neural Information Processing Systems,\npages 4299–4307.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: a simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nAndrea Esuli and Fabrizio Sebastiani. 2006. Senti-\nwordnet: A publicly available lexical resource for\nopinion mining. In LREC, volume 6, pages 417–422.\nCiteseer.\nSpencer Frazier, Md Sultan Al Nahian, Mark Riedl,\nand Brent Harrison. 2019. Learning norms from sto-\nries: A prior for value aligned agents. arXiv preprint\narXiv:1912.03553.\nMary Fainsod Katzenstein. 1996. The culture of na-\ntional security: Norms and identity in world politics.\nColumbia University Press.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nDimitrios Kotzias, Misha Denil, Nando De Freitas, and\nPadhraic Smyth. 2015. From group to individual la-\nbels using deep features. In Proceedings of the 21th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pages 597–606.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849.\nMoin Nadeem, Anna Bethke, and Siva Reddy.\n2020. Stereoset: Measuring stereotypical bias\nin pretrained language models. arXiv preprint\narXiv:2004.09456.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nStuart Russell, Daniel Dewey, and Max Tegmark. 2015.\nResearch priorities for robust and beneﬁcial artiﬁcial\nintelligence. Ai Magazine, 36(4):105–114.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326.\nNate Soares and Benja Fallenstein. 2014. Aligning su-\nperintelligence with human interests: A technical re-\nsearch agenda. Machine Intelligence Research Insti-\ntute (MIRI) technical report, 8.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Radford,\nand Jasmine Wang. 2019. Release strategies and the\nsocial impacts of language models. arXiv preprint\narXiv:1908.09203.\nLeonard Wayne Sumner. 1967. Normative ethics and\nmetaethics. Ethics, 77(2):95–106.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. arXiv preprint\narXiv:1908.07125.\n383\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. CoRR, abs/1905.12616.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.",
  "topic": "Normative",
  "concepts": [
    {
      "name": "Normative",
      "score": 0.9308194518089294
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6892721056938171
    },
    {
      "name": "Computer science",
      "score": 0.6849828362464905
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6088016629219055
    },
    {
      "name": "Normative social influence",
      "score": 0.466849148273468
    },
    {
      "name": "Transformer",
      "score": 0.45806530117988586
    },
    {
      "name": "Natural language processing",
      "score": 0.44339776039123535
    },
    {
      "name": "Reinforcement learning",
      "score": 0.42344987392425537
    },
    {
      "name": "Epistemology",
      "score": 0.09369564056396484
    },
    {
      "name": "Engineering",
      "score": 0.07685190439224243
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 18
}