{
  "title": "Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph Transformer.",
  "url": "https://openalex.org/W3153241113",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2505188570",
      "name": "Shuaicheng Zhang",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2166497444",
      "name": "Lifu Huang",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2002475849",
      "name": "Ning, Qiang",
      "affiliations": [
        "Amazon (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3111992880",
    "https://openalex.org/W2048157132",
    "https://openalex.org/W2129615653",
    "https://openalex.org/W3035229828",
    "https://openalex.org/W2930865789",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964263366",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2606877529",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1523160384",
    "https://openalex.org/W2741502284",
    "https://openalex.org/W3099246072",
    "https://openalex.org/W2760579680",
    "https://openalex.org/W2970170773",
    "https://openalex.org/W2127194753",
    "https://openalex.org/W3207696368",
    "https://openalex.org/W1654173042",
    "https://openalex.org/W2603270557",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W2251325107",
    "https://openalex.org/W2739896562",
    "https://openalex.org/W2018720926",
    "https://openalex.org/W2123167824",
    "https://openalex.org/W30314283",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2251949648",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W3106484161",
    "https://openalex.org/W2741237963",
    "https://openalex.org/W1990886313",
    "https://openalex.org/W2157003655",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2140244223",
    "https://openalex.org/W2970942496",
    "https://openalex.org/W2468432491",
    "https://openalex.org/W3106178264",
    "https://openalex.org/W2571747590",
    "https://openalex.org/W3114178062",
    "https://openalex.org/W2798602728",
    "https://openalex.org/W2905224888",
    "https://openalex.org/W3003446182",
    "https://openalex.org/W2751696754"
  ],
  "abstract": "Extracting temporal relations (e.g., before, after, concurrent) among events is crucial to natural language understanding. Previous studies mainly rely on neural networks to learn effective features or manual-crafted linguistic features for temporal relation extraction, which usually fail when the context between two events is complex or wide. Inspired by the examination of available temporal relation annotations and human-like cognitive procedures, we propose a new Temporal Graph Transformer network to (1) explicitly find the connection between two events from a syntactic graph constructed from one or two continuous sentences, and (2) automatically locate the most indicative temporal cues from the path of the two event mentions as well as their surrounding concepts in the syntactic graph with a new temporal-oriented attention mechanism. Experiments on MATRES and TB-Dense datasets show that our approach significantly outperforms previous state-of-the-art methods on both end-to-end temporal relation extraction and temporal relation classification.",
  "full_text": "Extracting Temporal Event Relation with Syntax-guided Graph\nTransformer\nShuaicheng Zhang♣, Qiang Ning♠, Lifu Huang♣\n♣Virginia Tech, ♠Amazon\n♣{zshuai8,lifuh}@vt.edu, ♠qning@amazon.com\nAbstract\nExtracting temporal relations (e.g., before, af-\nter, and simultaneous) among events is crucial\nto natural language understanding. One of the\nkey challenges of this problem is that when the\nevents of interest are far away in text, the con-\ntext in-between often becomes complicated,\nmaking it challenging to resolve the temporal\nrelationship between them. This paper thus\nproposes a new Syntax-guided Graph Trans-\nformer network (SGT) to mitigate this issue,\nby (1) explicitly exploiting the connection be-\ntween two events based on their dependency\nparsing trees, and (2) automatically locating\ntemporal cues between two events via a novel\nsyntax-guided attention mechanism. Experi-\nments on two benchmark datasets, M ATRES\nand TB-D ENSE , show that our approach sig-\nniﬁcantly outperforms previous state-of-the-\nart methods on both end-to-end temporal rela-\ntion extraction and temporal relation classiﬁ-\ncation; This improvement also proves to be ro-\nbust on the contrast set of M ATRES . The code\nis publicly available at https://github.com/\nVT-NLP/Syntax-Guided-Graph-Transformer.\n1 Introduction\nTemporal relationship, e.g., Before, After, and Si-\nmultaneous, is important for understanding the\nprocess of complex events and reasoning over\nthem. Extracting temporal relationship automat-\nically from text is thus an important component in\nmany downstream applications, such as summariza-\ntion (Jiang et al., 2011; Ng et al., 2014), dialog un-\nderstanding and generation (Ritter et al., 2010; Sun\net al., 2021), reading comprehension (Harabagiu\nand Bejan, 2005; Sun et al., 2018; Ning et al., 2020;\nHuang et al., 2019) and future event prediction (Li\net al., 2021; Lin et al., 2022). While event mentions\ncan often be detected reasonably well (Lin et al.,\n2020; Huang and Ji, 2020; Wang et al., 2021, 2022),\nextracting event-event relationships, especially tem-\nporal relationship, still remains challenging (Chen\net al., 2021).\nTemporal Relation (e1     e2): Before \nS2:  Mr. Erdogan' s office (e 1 :  said ) he had (e 2 :  accepted ) the apology\n, \" In the name of the Turkish people \".  \nS3:  \"The desk thing really (e 1 :  stuck ) with me \", Ms. Ayotte (e 2 :  said ).  \nTemporal Relation (e1     e2): Before \nS1:  Now, Lockheed Martin which (e 1 :  bought ) an early version of\nsuch a computer from the Canadian company D-Wave systems two\nyears ago is confident enough in the technology to upgrade it to\ncommercial scale, becoming the first company to (e 2 :  use ) quantum\ncomputing as part of its business.\nbought r elcl Matin is becomingadvcl attr company use nsubj r elcl \nTemporal Relation (e1     e2): AFTER \nFigure 1: Examples of temporal relation annotations.\nEvent mentions are boldfaced, the temporal relations\nbetween these events are listed below each sentence,\nand the temporal cues deciding those temporal relations\nare highlighted in red.\nRecent studies (Han et al., 2019b; Ning et al.,\n2017; Vashishtha et al., 2019; Wang et al., 2020a)\nhave shown improved performance in temporal re-\nlation extraction by leveraging the contextual repre-\nsentations learned from pre-trained language mod-\nels (Devlin et al., 2018; Liu et al., 2019). However,\none remaining challenge of this task is that it re-\nquires accurate characterization of the connection\nbetween two event mentions and the cues indicat-\ning their temporal relationship, especially when the\ncontext is wide and complicated. For instance, by\nmanually examining 200 examples of human anno-\ntated temporal relations from the MATRES (Ning\net al., 2018) dataset, we ﬁnd that about 52% of the\ntemporal cues1 come from the connection between\ntwo event mentions (e.g., S1 in Fig. 1), 39% from\ntheir surrounding contexts (S2 in Fig. 1) and the\nremaining 9% from others, e.g., event co-reference\nor subordinate clause structures (S3 in Fig. 1).\n1Temporal cues refer to the words of which the seman-\ntic meaning or related syntactic relations can determine the\ntemporal relation of two event mentions.\narXiv:2104.09570v2  [cs.CL]  24 Oct 2022\nGraph Attention Over Ingoing and\nOutgoing Edges\nIngoingHe won the Gusher\nNo Yes No No\n...\nN\nN N\nGraph\nAttention\nSource\nNode\nNeighbor\nNode\nSource\nNode\nTarget\nNodePath Neighbor Node\nSyntax-guided Attention  \nover Src-To-Tar Path\nOutgoing\nAggregated Temporal\nInformation\nFusion of Event Node\nRepresentations\nTemporal Query\nEvent Detection\nhs hst hst ht\nhst\nwon finishing\nRepresentation \n of won\nRepresentation\nof finishing\nTemporal Relation Extraction Module\nht\nhswon\nInput  \nSentence\nBERT \nLayer\nthe\nGusher\nMarathon,\nfinishing\nin\n3:07:35\nHe\nEvent  \nExtraction \nModule\nSyntax-guided Graph Transformer\nEEM Linear\nFusion Layer\nFeed Forward Layer\nAdd &\nLayerNorm\nAdd &\nLayerNorm\nSyntax-guided\nAttention\nGraph  \nMulti-Head  \nAttention \nEncoder Stack\nInput Graph Structure\nHe wontheGusherMarathon,finishingin 3:07:35\nnsubj\nadvcl\ncompound\ndobj\ndet\nprep\npobj\nResidual\nConnection\nFigure 2: Architecture overview. The tokens highlighted with red and blue colors in the Input Sentence show the\nsource and target events to be detected. The bold edges in the Input Graph Structure indicate the triples from\nthe dependency path between the source and target event mentions as well as their surrounding context, and are\nconsidered by the syntax-guided attention.\nSyntactic features, such as dependency parsing\ntrees, have proved to be effective in characterizing\nthe connection of two event mentions in pre-neural\nmethods (Chambers, 2013; Chambers et al., 2014;\nMirza and Tonelli, 2016). However, how to make\nuse of these features has been under-explored since\nthe adoption of neural methods in this ﬁeld. This\npaper closes this gap with a novel Syntax-guided\nGraph Transformer (SGT) network – in addition to\nthe attention heads in a typical Graph Transformer,\nwe bring in a new attention mechanism that specif-\nically looks at the path from a source node to a\ntarget node over dependency parsing trees. SGT\nthus not only learns event representations as in\na typical Graph Transformer, but also provides a\nway to represent syntactic dependency informa-\ntion between a pair of events (for temporal relation\nextraction, this means attending to the aforemen-\ntioned temporal cues). We conduct experiments\non two benchmark datasets, MATRES (Ning et al.,\n2018) and TB-D ENSE (Cassidy et al., 2014) on\nboth end-to-end temporal relation extraction and\nclassiﬁcation, which demonstrate the effectiveness\nof SGT over previous state-of-the-art methods. Ex-\nperiments on the contrast set (Gardner et al., 2020)\nof MATRES further proves the robustness of our\napproach.\n2 Approach\nFigure 2 shows the overview of our approach.\nGiven an input sentence ˜s= [w1,w2,...,w n] with\nntokens, we aim to detect a set of event mentions\n{e1,e2,...}where each event mention ei may con-\ntain one or multiple tokens by leveraging the con-\ntextual representations learned from a pre-trained\nBERT (Devlin et al., 2018) encoder. Then, fol-\nlowing previous studies (Ning et al., 2019, 2017;\nHan et al., 2019b; Wang et al., 2020a), we consider\neach pair of event mentions that are detected from\none or two continuous sentences, and predict their\ntemporal relationship.\nTo effectively capture the temporal cues between\ntwo event mentions, we build a dependency graph\nfrom one or two input sentences and design a new\nSyntax-guided Graph Transformer network to au-\ntomatically learn a new contextual representation\nfor each event mention by considering the triples\nthat they are locally involved as well as the triples\nalong the dependency path of the two event men-\ntions within the dependency graph. Finally, the two\nevent mention representations are concatenated to\npredict their temporal relationship.\n2.1 Sequence Encoder\nGiven an input sentence ˜s = [w1,w2,...,w n], we\napply the same tokenizer as BERT (Devlin et al.,\n2018) to get all the subtokens. Then, we feed the\nsequence of subtokens as input to a pre-trained\nBERT model to get a contextual representation for\neach token wi. If a token wi is split into multiple\nsubtokens, we use the contextual representation of\nthe ﬁrst subtoken to represent wi. To enrich the\ncontextualized representations, for each token, we\ncreate a one-hot Part-of-Speech (POS) tag vector\nand concatenate it with BERT contextual embed-\ndings. In this way, we obtain a ﬁnal representation\nci2 for each wi. These representations will be later\nused for event mention detection and also as the\ninitial representations to our syntax-guided graph\ntransformer network.\n2.2 Event Detection\nTo detect event mentions from the sentence, we\ntake the contextual representation of each word\nas input to a binary linear classiﬁer to determine\nwhether it is an event mention or not, which is op-\ntimized by minimizing the following binary cross-\nentropy loss:\n˜yi = softmax(Weveci + beve)\nLeve = −\n∑\n˜s∈S\n|˜s|∑\ni=1\n∑\nπ∈{0,1}\nαπyi,πlog(˜yi,π)\nwhere Leve denotes the cross-entropy loss for event\ndetection. Sis the set of sentences in the training\ndataset. απ is a weight coefﬁcient for each class (0\nor 1) to mitigate the data imbalance problem and\nα0 + α1 = 1. yi,π is a binary indicator to show\nwhether π is the same as the groundtruth binary\nlabel (yi,π = 1) or not (yi,π = 0). ˜yi,π denotes the\nprobability of the i-th token in sbeing predicted\nwith a binary class label π. Weve and beve are\nlearnable parameters.\n2.3 Syntax-guided Graph Transformer\nFrom the example sentences in Fig. 1, the temporal\ncues for characterizing the temporal relationship be-\ntween two event mentions mainly come from their\nsurrounding contexts as well as their connections\nfrom their syntactic dependency path. However,\na sequence encoder usually fails to capture such\ninformation, especially when the context between\n2We use bold lower case symbols to denote vectors.\ntwo event mentions is complicated, thus we further\ndesign a new Syntax-guided Graph Transformer\n(SGT) network.\nGiven a source event es and a target event et\ndetected from one or two continuous sentences, we\napply a public dependency parser 3 to parse each\nsentence into a tree-graph and connect the graphs\nof two continuous sentences with an arbitrarycross-\nsentence edge (Peng et al., 2017; Cheng and Miyao,\n2017) pointing from the root node of the preceding\nsentence to the root node of the following one, and\nobtain a graph G= (V,E).For each node vi, we\nuse Nin\ni = {(vk,rki,vi) ∈E|vk,vi ∈V}and\nNout\ni = {(vi,rij,vj) ∈E|vi,vj ∈V}to denote\nall the neighbor triples of vi with in-going and out-\ngoing edges respectively, r ∈Υ where Υ is the\nlabel set for syntactic dependency relation, and use\nPij = {(vi,rig,vg),..., (vh,rhj,vj)}as the triple\nset along the path from vi to vj.\nNode Representation Initialization For each\nnode vi in graph G, we map it to a particular token\nwi′ from the original sentence and obtain a con-\ntextual representation ci′ from the BERT encoder.\nThen, we learn an initial node representation for\neach node vi as:\nh0\ni = Weci′ + be\nwhere We and be are learnable parameters.\nGraph Multi-head Self-attention Following\ntransformer model (Vaswani et al., 2017; Wang\net al., 2020b), we adapt the multi-head self-\nattention to learn a contextual representation for\neach node in the graph G. Each node vi in graph\nG is associated with a set of neighbor triples\nNin\ni ∪Nout\ni and a node representation hl−1\ni where\nlis the index of a layer in our transformer archi-\ntecture. To perform self-attention, we ﬁrst apply\na linear transformation to obtain a query vector\nbased on each node vi, and employ another two lin-\near transformations to get the key and value vectors\nbased on the node’s neighbor triples:\nQl\ni = Wm\nq hl−1\ni\nKl\nij = Wm\nk Rl−1\nij\nUl\nij = Wm\nu Rl−1\nij\nRl−1\nij = Wm\nr (hl−1\ni\nn\nrij\nn\nhl−1\nj ) + bm\nr\nwhere m is the index of a particular head. Ql\ni\ndenotes a query vector corresponding to node vi,\n3https://spacy.io/api/dependencyparser\nKl\nij and Ul\nij is a key and value vector respec-\ntively, and both of them are learned from a triple\n(vi,rij,vj) ∈Nin\ni ∪Nout\ni , which is represented as\nRij. mis the index of a particular head. f denotes\nthe concatenation operation. rij denotes the repre-\nsentation of a particular relation rij between vi and\nvj, which is randomly initialized and optimized\nby the model. Wm\nq , Wm\nk , Wm\nu , Wm\nr and bm\nr are\nlearnable parameters.\nFor each node vi, we then perform self-attention\nover all the neighbor triples that it is involved, and\ncompute a new context representation with multiple\nattention heads:\ngl\ni = (\nMn\nm\nHeadm\ni )Wo\nHeadm\ni = softmax(Ql\ni(Kl)⊤\n√dk\n)Ul\nwhere gl\niis the aggregated representation computed\nover all neighbor triples of node vi with M atten-\ntion heads at l-th layer. gl\ni will be later used to\nlearn the updated representation of node vi. √dk\nis the scaling factor denoting the dimension size of\neach key vector. Wo is a learnable parameter.\nSyntax-guided Attention To automatically ﬁnd\nthe indicative temporal cues for two event men-\ntions from their connection as well as surrounding\ncontexts, we design a new syntax-guided attention\nmechanism. For two event nodes vs and vt, we ﬁrst\nextract the set of nodes from the dependency path\nbetween vs and vt (including vs and vt), which is\ndenoted as Θst. We then get all the triples from the\ndependency path between vs and vt as well as the\ntriples that any node from Θst is involved, which\nare denoted as Φst = ∪vi∈Θst {Nin\ni ∪Nout\ni }∪Pst.\nTo compute the syntax-guided attention over all the\ntriples from Φst, we apply three linear transforma-\ntions to get the query, key and value vectors where\nthe query vector is obtained from the representation\nof two event mentions, and key and value vectors\nare computed from the triples in Φst:\n˜Q\nl\nst = ˜W\nm\nq ·(hl−1\ns\nn\nhl−1\nt )x\n˜K\nl\nij = ˜W\nm\nk ˜R\nl−1\nij\n˜U\nl\nij = ˜W\nm\nu ˜R\nl−1\nij\n˜R\nl−1\nij = ˜W\nm\nr (hl−1\ni\nn\nrij\nn\nhl−1\nj ) + ˜br\nwhere m is the index of a particular head,\n˜Q\nl\nst, ˜K\nl\nij, ˜U\nl\nij denote the query, key and value vec-\ntors respectively. ˜R\nl−1\nij is the representation of a\ntriple (vi,rij,vj) ∈Φst. ˜W\nm\nq , ˜W\nm\nk , ˜W\nm\nv and\n˜W\nm\nr are learnable parameters.\nGiven the query vector, we then compute the at-\ntention distribution over all triples fromΦst and get\nan aggregated representation to denote the meaning-\nful temporal features captured from the connection\nbetween two event mentions and their surrounding\ncontexts.\n˜gl\nst = (\nMn\nm\n˜Head\nm\nst) · ˜Wp\n˜Head\nm\nst = softmax(\n˜Q\nl\nst( ˜K\nl\n)⊤\n√dk\n) ·˜U\nl\nwhere ˜gl\nst is the aggregated temporal related in-\nformation from all the triples in Φst based on the\nsyntax-guided attention at l-th layer. Wp is a learn-\nable parameter.\nNode Representation Fusion Each event node\nin graph Gwill receive two representations learned\nfrom the multi-head self-attention and syntax-\nguided attention, thus we further fuse the two rep-\nresentations for both the source node vs and the\ntarget node vt:\nˆh\nl\ns = ˜Wf(gl\ns\nn\n˜gl\nst) , ˆh\nl\nt = ˜Wf(˜gl\nst\nn\ngl\nt)\nwhere gl\ns and gl\nt denote the context representations\nlearned from the multi-head self-attention for vs\nand vt. ˜gl\nst denotes the representation learned from\nthe triples from Φst using syntax-guided attention.\nˆh\nl\ns and ˆh\nl\nt are the fused representations of vs and\nvt, respectively. ˜Wf is a learnable parameter.\nFor each non-event node vi, which only receives\na context representation gl\ni learned from the multi-\nhead self-attention, we apply a linear projection\nand get a new node representation:\nˆh\nl\ni = Wtgl\ni\nOur Syntax-guided Graph Transformer encoder\nis composed of a stack of multiple layers, while\neach layer consists of the two attention mechanisms\nand the fusion sub-layer. We use residual connec-\ntion followed by LayerNorm for each layer to get\nthe ﬁnal representations of all the nodes:\nHl = LayerNorm( ˆH\nl\n+ Hl−1)\n2.4 Temporal Relation Prediction\nTo predict the temporal relation between two event\nmentions es and et, we concatenate the ﬁnal hidden\nstates of vs and vt obtained from the Syntax-guided\nGraph Transformer network, and apply a Feedfor-\nward Neural Network (FNN) to predict their rela-\ntionship\n˜yst = softmax(Wz(hL\ns\nn\nhL\nt ) + bt)\nwhere ˜ystdenotes the probabilities over all possible\ntemporal relations between event mentions es and\net.\nThe training objective is to minimize the follow-\ning cross-entropy loss function:\nLrel = −\n∑\nst∈∆\n∑\nx∈X\nβxyst,xlog(˜yst,x))\nwhere ∆ denotes the total set of event pairs for tem-\nporal relation prediction and X denotes the whole\nset of relation labels. yst,xis a binary indicator (0 or\n1) to show whether xis the same as the groundtruth\nlabel (yst,x = 1) or not ( yst,x = 0). We also as-\nsign a weight βx to each class to mitigate the label\nimbalance issue.\n3 Experiment\n3.1 Experimental Setup\nWe perform experiments on two public benchmark\ndatasets for temporal relation extraction: (1) TB-\nDENSE (Cassidy et al., 2014), which is a densely\nannotated dataset with 6 types of relations: Be-\nfore, After, Simultaneous, Includes, Is_included\nand Vague. (2) MATRES (Ning et al., 2018), which\nannotates verb event mentions along with 4 types\nof temporal relations: Before, After, Simultaneous\nand Vague. Additionally, we use POS tag infor-\nmation from MATRES provided by (Ning et al.,\n2019). For TB-D ENSE , we use spacy annotation\nfor predicting POS tag information which is based\non Universal POS tag set 4. For both benchmark\ndatasets, we use the same train/dev/test splits as pre-\nvious studies (Ning et al., 2019, 2017; Han et al.,\n2019a,b). Note that, for evaluation, similar as pre-\nvious work, we disregard the Vague relation from\nboth datasets (in the evaluation phase, we simply\nremove all ground truth Vague relation pairs). In\naddition, we will only consider event pairs from ad-\njacent sentences due to the fact that it will require\n4https://spacy.io/api/data-formats\nan exponential number of annotations if we also\nconsider event pairs from non-adjacent sentences,\nwhich is beyond the scope of this study. Table 1\nshows statistics of the two datasets and Table 2\nshows the label distribution.\nCorpora Train Dev Test\nTB-D ENSE # Documents 22 5 9\n# Relation Pairs 4,032 629 1,427\nMATRES # Documents 255 20 25\n# Relation Pairs 13K 2.6K 837\nTable 1: Data statistics for TB-D ENSE and MATRES\nLabels TB-D ENSE MATRES\nBefore 384 26.9% 417 49.8%\nAfter 274 19.2% 266 31.8%\nIncludes 56 3.9% - -\nIs_Included 53 3.7% - -\nSimultaneous 22 1.5% 31 3.7%\nVague 638 44.7% 133 15.9%\nTable 2: Label distribution for TB-D ENSE and M A-\nTRES . For each dataset, the ﬁrst column shows the num-\nber of instances of each relation type while the second\ncolumn shows the percentage.\nImplementation Details For fair comparisons\nwith previous baseline approaches, we use the pre-\ntrained bert-large-cased model5 for ﬁne-tuning and\noptimize our model with BertAdam. We optimize\nthe parameters with grid search: training epoch 10,\nlearning rate ∈{3e-6,1e-5}, training batch size\n∈{16,32}, encoder layer size ∈{4,12}, number\nof heads ∈{1,8}. During training, we ﬁrst opti-\nmize the event extraction module for 5 epochs to\nwarm up, and then jointly optimize both event ex-\ntraction and temporal relation extraction modules\nusing gold event pairs for another 5 epochs.\n3.2 Results\nWe evaluate SGT against two public benchmark\ndatasets under two settings: (1) joint event and tem-\nporal relation extraction (Table 3); (2) temporal\nrelation classiﬁcation, where the gold event men-\ntions are known beforehand (Table 4). Note in the\n“joint” setting, we adopt the same strategy proposed\nin (Han et al., 2019b): we ﬁrst train the event ex-\ntraction module, and then jointly optimize both\nevent extraction and temporal relation extraction\n5https://huggingface.co/transformers/pretrained_models.\nhtml\nDataset Model Pre-trained Model Event Detection Relation Extraction\nTB-D ENSE HNP19 (Han et al., 2019b) BERT Base 90.9 49.4\nOur Approach BERT Base 91.0 51.8\nMATRES\nCogCompTime2.0 (Ning et al., 2019) BERT Base 85.2 52.8\nHNP19 (Han et al., 2019b) BERT Base 87.8 59.6\nOur Approach BERT Base 90.5 62.3\nTable 3: Comparison of various approaches on joint event and relation extraction with F-score (%). Note that\nHPN19 ﬁxes BERT embeddings but relies on BiLSTM to capture the contextual features.\nDataset Model Pre-trained Model Relation Classiﬁcation (F-score %)\nTB-D ENSE\nLSTM (Cheng and Miyao, 2017) BERT Base 62.2\nHNP19 (Han et al., 2019b) BERT Base 64.5\nOur Approach BERT Base 66.7\nPSL (Zhou et al., 2020) RoBERTa Large 65.2\nDEER (Han et al., 2021) RoBERTa Large 66.8\nOur Approach BERT Large 67.1\nMATRES\nCogCompTime2.0 (Ning et al., 2019) BERT Base 71.4\nLSTM (Cheng and Miyao, 2017) BERT Base 73.4\nHNP19 (Han et al., 2019b) BERT Base 75.5\nOur Approach BERT Base 79.3\nHMHD20 (Wang et al., 2020a) RoBERTa Large 78.8\nDEER (Han et al., 2021) RoBERTa Large 79.3\nOur Approach BERT Large 80.3\nTable 4: Comparison of various approaches on temporal relation classiﬁcation with gold event mentions as input.\n(using gold event pairs as input to ensure training\nquality) modules. Overall, we observe that our ap-\nproach signiﬁcantly outperforms baseline systems\nin both settings, with up to 7.9% absolute F-score\ngain on MATRES and 2.4% on TB-D ENSE .\nFrom Table 3, we see that our approach achieves\nbetter performance on event detection than base-\nline methods though they are based on the same\nBERT encoder. This is possibly because, during\njoint training, our approach leverages the depen-\ndency parsing trees, which improves the contextual\nrepresentations of the BERT encoder. In Table 4,\nunlike other models which are based on larger con-\ntextualized embeddings such as RoBERTa, our ap-\nproach with BERT base achieves comparable per-\nformance, and further surpasses the state-of-the-art\nbaseline methods using BERT-large embeddings,\nwhich demonstrate the effectiveness of our Syntax-\nguided Graph Transformer network.\nSome studies (Ning et al., 2019; Han et al.,\n2019b; Wang et al., 2020a; Zhou et al., 2020) focus\non resolving the inconsistency in terms of the sym-\nmetry and transitivity of the temporal relations. For\nexample, if event A and event B are predicted as\nBefore, event B and event C are predicted asBefore,\nthen if event A and event C are predicted as Vague\nor After, it will be considered as inconsistent. How-\nModel Original\nTest\nContrast Consistency\nCogCompTime2.0\n(Ning et al., 2019)\n73.2 63.3 40.6\nOur Approach 77.0 64.8 49.8\nTable 5: Evaluation on the contrast set of M ATRES .\nOriginal Test indicates the accuracy on 100 examples\nsampled from the original M ATRES test set follow-\ning (Gardner et al., 2020). Contrast shows the accuracy\nscore on 401 examples perturbed from the original 100\nexamples. Consistency is deﬁned as the percentage of\nthe original 100 examples for which the model’s pre-\ndictions of the perturbed examples are all correct in the\ncontrast set.\never, our approach shows consistent predictions\nwith few inconsistent cases when Simultaneous re-\nlation is involved. This analysis also demonstrates\nthat our approach can correctly capture the tempo-\nral cues between two event mentions.\nWe also examine the correctness and robustness\nof our approach on a contrast set ofMATRES (Gard-\nner et al., 2020), which is created with small man-\nual perturbation based on the original test set of\nMATRES in a meaningful way, such as rephrasing\nthe sentence or simply changing a word of the sen-\ntence to alter the relation type. The contrast set\nS1: Before (e1: retiring) in 1984 , Mr. Lowe (e2: worked) as an inspector of schools with the\ndepartment of education and sciences , and he leaves three sons from a previous marriage .\nS2: Mr. Erdogan has long (e1: sought) an apology for the raid in May 2010 on the Mavi \nMarmara , which was part of a Flotilla that (e2: sought) to break Israel's blockade of gaza.  \nExamplePrediction\nBERT: Before\nBERT-GT: After\nBERT-SGT: After\nBERT: Before\nBERT-GT: Before\nBERT-SGT: After\nFigure 3: Comparison of the predictions from BERT, BERT-GT and our approach.\nprovides a local view of a model’s decision bound-\nary, thus it can be used to more accurately evalu-\nate a model’s true linguistic capabilities. Table 5\nshows that our approach signiﬁcantly outperforms\nthe baseline model on both the original test set and\nthe corresponding contrast set. The contrast consis-\ntency in Table 5 also indicates how well a model’s\ndecision boundary aligns with the actual decision\nboundary of the test instances, based on which we\ncan see that by explicitly capturing temporal cues,\nour approach is more accurate and robust than the\nbaseline method.\nAblation Study We further conduct ablation\nstudies to compare the performance of our ap-\nproach with two ablated versions of our method:\n(1) BERT with Graph Transformer (BERT-GT), for\nwhich we remove the syntaxic-guided attention and\nonly rely on the standard multi-head self-attention\nto obtain graph-based contextual representations\nof two event mentions and then predict their rela-\ntion; (2) BERT, where we further remove the Graph\nTransformer, and only use the pre-trained BERT\nlanguage model to encode the sentence and predict\nthe temporal relationship of two event mentions\nbased on their contextual representations.\nAblation F-score (%) Gain (%)\nBERT-SGT 79.3 0\nBERT-GT 77.5 -2.0\nBERT 75.5 -3.8\nTable 6: Ablation study on M ATRES . We use BERT\nbase as the comparison basis.\nTable 6 also shows that by adding Graph Trans-\nformer, BERT-GT achieves 2.0% absolute F-score\nimprovement over the BERT baseline model,\ndemonstrating the beneﬁt of dependency parsing\ntrees to temporal relation prediction. By further\nadding the new syntax-guided attention into Graph\nTransformer, the absolute improvement on F-score\n(1.8%) shows the effectiveness of our new Syntax-\nguided Graph Transformer and the importance of\ncapturing temporal cues from the connection of two\nevent mentions as well as their surround contexts.\nFigure 3 shows two examples as qualitative\nanalysis. In S1, BERT mistakenly predicts the\ntemporal relation as Before probably because it’s\nconfused by the context word Before. However, by\nincorporating the dependency graph, especially the\ntriples {worked, prep, Before}, { Before, pcomp,\nretiring} and the path between the two event men-\ntions, worked→prep→Before→pcomp→retiring,\nboth BERT-GT and our approach correctly\ndetermine the relation as After. In S2, both BERT\nand BERT-GT mistakenly predict the temporal\nrelation as Before as the context between the two\nevent mentions is very wide and complicated,\nand these two event mentions are not close\nwithin the dependency graph. However, by\nexplicitly considering and understanding the\nconnection between the two event mentions,\nsoughte1 →on→Marmara→was→part→Flotilla\n→soughte2 , our approach correctly predicts the\ntemporal relation between the two event mentions.\n3.3 SGT on Temporal Cues\nTo analyze the source of temporal cues for rela-\ntion prediction, we randomly sample 100 correct\nevent relation predictions given gold event men-\ntions from MATRES and select the triple that has\nthe highest temporal attention weight from the last\nlayer of the Syntax-guided Graph Transformer net-\nwork as a temporal cue candidate. We manually\nevaluate the validity of each temporal cue candi-\ndate, and further analyze if the cue is from the de-\npendency path between two event mentions, their\nsurrounding context, or both. Our analysis shows\nthat about 64% of the temporal cues are valid, 37%\nof them come from the dependency path, 17% are\nfrom local context, and the remaining 10% are\nfrom both. This veriﬁes our initial observation\nthat most of the temporal cues are from the depen-\ndency path between two event mentions as well as\ntheir surrounding context. It also demonstrates the\neffectiveness of our new syntax-guided attention\nmechanism.\n3.4 Impact of Wide Context\nWe further illustrate the impact of context width\nto both baseline model and our approach. For fair\ncomparison, we use three context width category,\n[context length <10, 10 <context length <20,\ncontext length > 20 ]. As we can see in Fig. 4,\nthe ﬁrst category has 267 pairs, the second cate-\ngory has 343 pairs and the third category has 817\npairs. From our results, we observe that the BERT\nbaseline cannot predict the temporal relation of\ntwo event mentions with wide context but rather\nworking well when the event mentions are close\nto each other. Our model overall performs slightly\nworse in the second category but in general is very\ngood at predicting the temporal relationship for\nthe event mentions with short and context width.\nThis also proves the beneﬁt of syntactic parsing\ntrees to the prediction of temporal relationship. For\nthe second category where the context length is\nwithin [10, 20], the performance of our approach\nslightly drops due to two reasons: (1) the training\nsamples within this range are not as sufﬁcient as\nthe other two categories; (2) for most event pairs\nfrom this category, their dependency path is very\nlong and there is no explicit temporal indicative\nfeatures within their context or dependency path,\nmaking it more difﬁcult for the model to predict\ntheir temporal relationship.\nFigure 4: Context width analysis on TB-D ENSE . The\nX axis shows the number of tokens between two events\nmentions. The left Y axis shows the data distribution\nof each width category indicating with blue bars. The\nright Y axis denotes the micro F-score for each width\ncategory.\n3.5 Remaining Errors\nWe randomly sample 100 classiﬁcation errors from\nthe output of our approach and categorize them into\nfour categories. As Figure 5 shows, the ﬁrst cate-\ngory is due to the complex or ambiguous context\n(54% of the total errors). The second category is\ndue to the complicated subordinate clause structure,\nespecially the clauses that are related to quote or\nreported speech, e.g., S2 in Figure 5. The third\nerror category is that our approach cannot correctly\ndifferentiate the actual events from the hypothetical\nand intentional events, while in most cases, the tem-\nporal relation among hypothetical and intentional\nevents is annotated as Vague. The last category is\ndue to the lack of sufﬁcient annotation. We ob-\nserve that none of the Simultaneous relation can\nbe correctly predicted for MATRES dataset as the\npercentage of Simultaneous (3.7%) is much lower\nthan other relation types. In TB-D ENSE dataset,\nlabels are even more imbalanced as the percentage\nof Vague relation is over 50% while the percentage\nof Includes, Is_Included and Simultaneous are all\nless than 4%.\n4 Related Work\nEarly studies on temporal relation extraction\nmainly model it as a pairwise classiﬁcation prob-\nlem (Mani et al., 2006; Verhagen et al., 2007; Verha-\ngen and Pustejovsky, 2008; Verhagen et al., 2010;\nBethard et al., 2016; MacAvaney et al., 2017) and\nrely on hand-crafted features and rules (Verhagen\nand Pustejovsky, 2008; Bethard et al., 2007) to ex-\ntract temporal event relations. Recently, deep neu-\nral networks (Dligach et al., 2017; Tourille et al.,\n2017) and large-scale pre-trained language mod-\nels (Han et al., 2019a, 2021; Wang et al., 2020a;\nZhou et al., 2020) are further employed and show\nstate-of-the-art performance.\nSimilar to our approach, several studies (Ling\nand Weld, 2010; Nikfarjam et al., 2013; Mirza\nand Tonelli, 2016; Meng et al., 2017; Cheng and\nMiyao, 2017; Huang et al., 2017) also explored\nsyntactic path between two events for temporal re-\nlation extraction. Different from previous work,\nour approach considers three important sources of\ntemporal cues: local context, denoting the neigh-\nbors of each event node within the dependency\ngraph; connection of two event mentions, which is\nbased on the dependency path between two event\nmentions; and rich semantics of concepts and de-\npendency relations, for example, the dependency\nS2: \"We were pleased that England and New Zealand knew about it, and we (e 1 : thought) that's where it would\nstop.\" He also (e 2 : talked) about his \" second job \" as the group's cameraman. (V ague )\nExampleError Category (Percent)\nSubordinate Clause (22%)\nComplex Context (54%)S1: \"This is not a Lehman , \" he (e 1 : said) to the disastrous chain reaction (e 2 : touched) off by the collapse\nof Lehman brothers in 2008 .  (After )\nHypothetical Events and\nIntentional Events (18%)\nS3: The day before Raymond Roth was  (e 1 : pulled) over, his wife, Ivana, showed authorities emails she\nhad discovered that  (e 2 : appeared) to detail a plan between him and his son to fake his death. (V ague ) \n \nS4: Microsoft (e 1 : said) it has identified three companies for the china program to (e 2 : run) through June .\n(Simultaneous )Imbalanced Labels (6%)\nFigure 5: Types of remaining errors\nrelation nmod between two event mentions usually\nindicates a Before relationship. All these indica-\ntive features are automatically selected and aggre-\ngated with the multi-head self-attention and our\nnew syntax-guided attention mechanism.\nOur work is also related to the variants of Graph\nNeural Networks (GNN) (Kipf and Welling, 2016;\nVeliˇckovi´c et al., 2018; Zhou et al., 2018), espe-\ncially Graph Transformer (Yun et al., 2019; Chen\net al., 2019; Hu et al., 2020; Wang et al., 2020b).\nDifferent from previous GNNs which aim to cap-\nture the context from neighbors of each node within\nthe graph, in our task, we aim to select and capture\nthe most meaningful temporal cues for two event\nmentions from their connections within the graph\nas well as their surrounding contexts.\n5 Conclusion\nTemporal relationship between events is important\nfor understanding stories described in natural lan-\nguage text, and a main challenge is how to dis-\ncover and make use of the connection between\ntwo event mentions, especially when the event pair\nis far apart in text. This paper proposes a novel\nSyntax-guided Graph Transformer (SGT) that rep-\nresents the connection between an event pair via\nadditional attention heads over dependency parsing\ntrees. Experiments on benchmarking datasets, MA-\nTRES , TB-D ENSE , and a contrast set of MATRES ,\nshow that our approach signiﬁcantly outperforms\nprevious state-of-the-art methods in a variety of set-\ntings, including event detection, temporal relation\nclassiﬁcation (where events are given), and tempo-\nral relation extraction (where events are predicted).\nIn the future, we will investigate the potential of\nthis approach to other relation extraction tasks.\nAcknowledgements\nWe thank the anonymous reviewers and area chair\nfor their valuable time and constructive comments.\nWe also thank the support from the Amazon Re-\nsearch Awards.\nReferences\nS. Bethard, J. H. Martin, and S. Klingenstein. 2007.\nTimelines from text: Identiﬁcation of syntactic tem-\nporal relations. In International Conference on Se-\nmantic Computing (ICSC 2007), pages 11–18.\nSteven Bethard, Guergana Savova, Wei-Te Chen, Leon\nDerczynski, James Pustejovsky, and Marc Verhagen.\n2016. Semeval-2016 task 12: Clinical tempeval. In\nProceedings of the 10th International Workshop on\nSemantic Evaluation (SemEval-2016) , pages 1052–\n1062.\nTaylor Cassidy, Bill McDowell, Nathanel Chambers,\nand Steven Bethard. 2014. An annotation frame-\nwork for dense event ordering. Technical report,\nCARNEGIE-MELLON UNIV PITTSBURGH PA.\nNathanael Chambers. 2013. NavyTime: Event and\ntime ordering from raw text. In Second Joint Con-\nference on Lexical and Computational Semantics\n(*SEM), Volume 2: Proceedings of the Seventh In-\nternational Workshop on Semantic Evaluation (Se-\nmEval 2013), pages 73–77, Atlanta, Georgia, USA.\nAssociation for Computational Linguistics.\nNathanael Chambers, Taylor Cassidy, Bill McDowell,\nand Steven Bethard. 2014. Dense event ordering\nwith a multi-pass architecture. Transactions of the\nAssociation for Computational Linguistics , 2:273–\n284.\nBenson Chen, Regina Barzilay, and Tommi Jaakkola.\n2019. Path-augmented graph transformer network.\narXiv preprint arXiv:1905.12712.\nMuhao Chen, Hongming Zhang, Qiang Ning, Manling\nLi, Heng Ji, Kathleen McKeown, and Dan Roth.\n2021. Event-centric natural language processing.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing: Tutorial Abstracts , pages 6–14,\nOnline. Association for Computational Linguistics.\nFei Cheng and Yusuke Miyao. 2017. Classifying tem-\nporal relations by bidirectional lstm over depen-\ndency paths. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 1–6.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDmitriy Dligach, Timothy Miller, Chen Lin, Steven\nBethard, and Guergana Savova. 2017. Neural tem-\nporal relation extraction. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 746–751.\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan\nBerant, Ben Bogin, Sihao Chen, Pradeep Dasigi,\nDheeru Dua, Yanai Elazar, Ananth Gottumukkala,\nNitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,\nDaniel Khashabi, Kevin Lin, Jiangming Liu, Nel-\nson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer\nSingh, Noah A. Smith, Sanjay Subramanian, Reut\nTsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.\n2020. Evaluating models’ local decision boundaries\nvia contrast sets. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1307–1323, Online. Association for Computational\nLinguistics.\nRujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan,\nRalph Weischedel, and Nanyun Peng. 2019a. Deep\nstructured neural network for event temporal rela-\ntion extraction. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 666–106, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nRujun Han, Qiang Ning, and Nanyun Peng. 2019b.\nJoint event and temporal relation extraction with\nshared representations and structured prediction. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 434–\n444, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRujun Han, Xiang Ren, and Nanyun Peng. 2021.\nECONET: Effective continual pretraining of lan-\nguage models for event temporal reasoning. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5367–\n5380, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nSanda Harabagiu and Cosmin Adrian Bejan. 2005.\nQuestion answering based on temporal inference. In\nProceedings of the AAAI-2005 workshop on infer-\nence for textual question answering, pages 27–34.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020. Heterogeneous graph transformer. In\nProceedings of The Web Conference 2020 , pages\n2704–2710.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. arXiv preprint arXiv:1909.00277.\nLifu Huang and Heng Ji. 2020. Semi-supervised\nnew event type induction and event detection. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 718–724.\nLifu Huang, Avirup Sil, Heng Ji, and Radu Florian.\n2017. Improving slot ﬁlling performance with at-\ntentive neural networks on dependency structures.\narXiv preprint arXiv:1707.01075.\nYexi Jiang, Chang-Shing Perng, and Tao Li. 2011. Nat-\nural event summarization. In Proceedings of the\n20th ACM international conference on Information\nand knowledge management, pages 765–774.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nManling Li, Sha Li, Zhenhailong Wang, Lifu Huang,\nKyunghyun Cho, Heng Ji, Jiawei Han, and Clare\nV oss. 2021. The future is not one-dimensional:\nComplex event schema induction by graph model-\ning for event prediction. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5203–5215.\nLi Lin, Yixin Cao, Lifu Huang, Shuang Li, Xuming Hu,\nLijie Wen, and Jianmin Wang. 2022. Inferring com-\nmonsense explanations as prompts for future event\ngeneration. arXiv preprint arXiv:2201.07099.\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020.\nA joint neural model for information extraction with\nglobal features. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7999–8009.\nXiao Ling and Daniel Weld. 2010. Temporal informa-\ntion extraction. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 24.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSean MacAvaney, Arman Cohan, and Nazli Goharian.\n2017. Guir at semeval-2017 task 12: a framework\nfor cross-domain clinical temporal information ex-\ntraction. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017) ,\npages 1024–1029.\nInderjeet Mani, Marc Verhagen, Ben Wellner, Chung-\nmin Lee, and James Pustejovsky. 2006. Machine\nlearning of temporal relations. In Proceedings of\nthe 21st International Conference on Computational\nLinguistics and 44th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 753–760.\nYuanliang Meng, Anna Rumshisky, and Alexey Ro-\nmanov. 2017. Temporal information extraction for\nquestion answering using syntactic dependencies in\nan LSTM-based architecture. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 887–896, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nParamita Mirza and Sara Tonelli. 2016. Catena: Causal\nand temporal relation extraction from natural lan-\nguage texts. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 64–75.\nJun Ping Ng, Yan Chen, Min-Yen Kan, and Zhoujun\nLi. 2014. Exploiting timelines to enhance multi-\ndocument summarization. In Proceedings of the\n52nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n923–933.\nAzadeh Nikfarjam, Ehsan Emadzadeh, and Graciela\nGonzalez. 2013. Towards generating a patient’s\ntimeline: extracting temporal relationships from\nclinical notes. Journal of biomedical informatics ,\n46:S40–S47.\nQiang Ning, Zhili Feng, and Dan Roth. 2017. A struc-\ntured learning approach to temporal relation extrac-\ntion. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1027–1037, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nQiang Ning, Sanjay Subramanian, and Dan Roth. 2019.\nAn improved neural baseline for temporal relation\nextraction. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) ,\npages 6203–6209, Hong Kong, China. Association\nfor Computational Linguistics.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. TORQUE: A reading\ncomprehension dataset of temporal ordering ques-\ntions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 1158–1172, Online. Associa-\ntion for Computational Linguistics.\nQiang Ning, Hao Wu, and Dan Roth. 2018. A multi-\naxis annotation scheme for event temporal relations.\nIn ACL.\nNanyun Peng, Hoifung Poon, Chris Quirk, Kristina\nToutanova, and Wen-tau Yih. 2017. Cross-sentence\nn-ary relation extraction with graph lstms. Transac-\ntions of the Association for Computational Linguis-\ntics, 5:101–115.\nAlan Ritter, Colin Cherry, and William B Dolan. 2010.\nUnsupervised modeling of twitter conversations. In\nHuman Language Technologies: The 2010 Annual\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics, pages 172–\n180.\nYajing Sun, Yong Shan, Chengguang Tang, Yue Hu,\nYinpei Dai, Jing Yu, Jian Sun, Fei Huang, and Luo\nSi. 2021. Unsupervised learning of deterministic\ndialogue structure with edge-enhanced graph auto-\nencoder. In Thirty-Fifth AAAI Conference on Arti-\nﬁcial Intelligence, AAAI 2021, Thirty-Third Confer-\nence on Innovative Applications of Artiﬁcial Intelli-\ngence, IAAI 2021, The Eleventh Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2021, Virtual Event, February 2-9, 2021 , pages\n13869–13877. AAAI Press.\nYawei Sun, Gong Cheng, and Yuzhong Qu. 2018.\nReading comprehension with graph-based temporal-\ncasual reasoning. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 806–817.\nJulien Tourille, Olivier Ferret, Aurelie Neveol, and\nXavier Tannier. 2017. Neural architecture for tem-\nporal relation extraction: A bi-lstm approach for de-\ntecting narrative containers. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n224–230.\nSiddharth Vashishtha, Benjamin Van Durme, and\nAaron Steven White. 2019. Fine-grained temporal\nrelation extraction. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2906–2919, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph Attention Networks. International\nConference on Learning Representations. Accepted\nas poster.\nMarc Verhagen, Robert Gaizauskas, Frank Schilder,\nMark Hepple, Graham Katz, and James Pustejovsky.\n2007. Semeval-2007 task 15: Tempeval tempo-\nral relation identiﬁcation. In Proceedings of the\nfourth international workshop on semantic evalua-\ntions (SemEval-2007), pages 75–80.\nMarc Verhagen and James Pustejovsky. 2008. Tempo-\nral processing with the tarsqi toolkit. In COLING\n2008: Companion Volume: Demonstrations , pages\n189–192.\nMarc Verhagen, Roser Sauri, Tommaso Caselli, and\nJames Pustejovsky. 2010. Semeval-2010 task 13:\nTempeval-2. In Proceedings of the 5th international\nworkshop on semantic evaluation, pages 57–62.\nHaoyu Wang, Muhao Chen, Hongming Zhang, and\nDan Roth. 2020a. Joint constrained learning for\nevent-event relation extraction. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 696–706,\nOnline. Association for Computational Linguistics.\nSijia Wang, Mo Yu, Shiyu Chang, Lichao Sun, and Lifu\nHuang. 2021. Query and extract: Reﬁning event\nextraction as type-oriented binary decoding. arXiv\npreprint arXiv:2110.07476.\nSijia Wang, Mo Yu, and Lifu Huang. 2022. The art of\nprompting: Event detection based on type speciﬁc\nprompts. arXiv preprint arXiv:2204.07241.\nTianming Wang, Xiaojun Wan, and Hanqi Jin. 2020b.\nAmr-to-text generation with graph transformer.\nTransactions of the Association for Computational\nLinguistics, 8:19–33.\nSeongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo\nKang, and Hyunwoo J Kim. 2019. Graph trans-\nformer networks. Advances in neural information\nprocessing systems, 32.\nJie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang,\nZhiyuan Liu, Lifeng Wang, Changcheng Li, and\nMaosong Sun. 2018. Graph neural networks: A re-\nview of methods and applications. arXiv preprint\narXiv:1812.08434.\nYichao Zhou, Yu Yan, Rujun Han, J Harry Cauﬁeld,\nKai-Wei Chang, Yizhou Sun, Peipei Ping, and Wei\nWang. 2020. Clinical temporal relation extrac-\ntion with probabilistic soft logic regularization and\nglobal inference. arXiv preprint arXiv:2012.08790.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7636762261390686
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6124561429023743
    },
    {
      "name": "Relationship extraction",
      "score": 0.5720411539077759
    },
    {
      "name": "Relation (database)",
      "score": 0.5369685888290405
    },
    {
      "name": "Transformer",
      "score": 0.5344079732894897
    },
    {
      "name": "Graph",
      "score": 0.5292054414749146
    },
    {
      "name": "Natural language processing",
      "score": 0.48015445470809937
    },
    {
      "name": "Information extraction",
      "score": 0.20052754878997803
    },
    {
      "name": "Data mining",
      "score": 0.17734944820404053
    },
    {
      "name": "Theoretical computer science",
      "score": 0.17463764548301697
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ],
  "cited_by": 10
}