{
    "title": "A Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical Concept Embeddings and Pretrained Language Models",
    "url": "https://openalex.org/W4385567033",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2140547979",
            "name": "Claudio Aracena",
            "affiliations": [
                "University of Chile"
            ]
        },
        {
            "id": "https://openalex.org/A3036123536",
            "name": "Fabián Villena",
            "affiliations": [
                "University of Chile"
            ]
        },
        {
            "id": "https://openalex.org/A2666157620",
            "name": "Matías Rojas",
            "affiliations": [
                "University of Chile"
            ]
        },
        {
            "id": "https://openalex.org/A2239150692",
            "name": "Jocelyn Dunstan",
            "affiliations": [
                "University of Chile"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2572147858",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W4394637965",
        "https://openalex.org/W3096590546",
        "https://openalex.org/W3080321165",
        "https://openalex.org/W2946119234",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3209071114",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W3105240624",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W4287888709",
        "https://openalex.org/W4285116174",
        "https://openalex.org/W3130583616",
        "https://openalex.org/W4226027641",
        "https://openalex.org/W2787481916",
        "https://openalex.org/W3105248300",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2577221992",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4212881197",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W4287818038",
        "https://openalex.org/W3198365756",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W2964303116"
    ],
    "abstract": "Using language models created from large data sources has improved the performance of several deep learning-based architectures, obtaining state-of-the-art results in several NLP extrinsic tasks. However, little research is related to creating intrinsic tests that allow us to compare the quality of different language models when obtaining contextualized embeddings. This gap increases even more when working on specific domains in languages other than English. This paper proposes a novel graph-based intrinsic test that allows us to measure the quality of different language models in clinical and biomedical domains in Spanish. Our results show that our intrinsic test performs better for clinical and biomedical language models than a general one. Also, it correlates with better outcomes for a NER task using a probing model over contextualized embeddings. We hope our work will help the clinical NLP research community to evaluate and compare new language models in other languages and find the most suitable models for solving downstream tasks.",
    "full_text": "Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI), pages 197 - 206\nDecember 7, 2022 ©2022 Association for Computational Linguistics\nA Knowledge-Graph-Based Intrinsic Test for Benchmarking Medical\nConcept Embeddings and Pretrained Language Models\nClaudio Aracena1, Fabián Villena1,2, Matías Rojas1,2, and Jocelyn Dunstan1,2\n1Faculty of Physical and Mathematical Sciences, University of Chile\n2Center for Mathematical Modeling, University of Chile\n{claudio.aracena,fabian.villena,jdunstan}@uchile.cl\nmatias.rojas.g@ug.uchile.cl\nAbstract\nUsing language models created from large data\nsources has improved the performance of sev-\neral deep learning-based architectures, obtain-\ning state-of-the-art results in several NLP ex-\ntrinsic tasks. However, little research is re-\nlated to creating intrinsic tests that allow us to\ncompare the quality of different language mod-\nels when obtaining contextualized embeddings.\nThis gap increases even more when working on\nspecific domains in languages other than En-\nglish. This paper proposes a novel graph-based\nintrinsic test that allows us to measure the qual-\nity of different language models in clinical and\nbiomedical domains in Spanish. Our results\nshow that our intrinsic test performs better for\nclinical and biomedical language models than a\ngeneral one. Also, it correlates with better out-\ncomes for a NER task using a probing model\nover contextualized embeddings. We hope our\nwork will help the clinical NLP research com-\nmunity to evaluate and compare new language\nmodels in other languages and find the most\nsuitable models for solving downstream tasks.\n1 Introduction\nIn healthcare, text plays a role of enormous impor-\ntance. One of the media that a medical practitioner\ncan persist is the text in clinical records (Dalianis,\n2018). Text is one of the richest forms of infor-\nmation inside the electronic health record, so it is\nfundamental to develop tools to extract information\nfrom these text sources. To create these tools in\nthis field, we must pay special attention to ensuring\nquality and reproducibility.\nAnalyzing unstructured texts written by humans\nis challenging since it is complex to formally under-\nstand and describe the rules governing human lan-\nguage, as it is ambiguous and constantly evolving.\nNatural Language Processing (NLP) is an interdis-\nciplinary field of artificial intelligence that seeks\nto develop algorithms capable of understanding,\ninterpreting, and manipulating these unstructured\ntexts (Jurafsky and Martin, 2000).\nIn the medical context, using NLP helps to ad-\ndress tasks such as extracting medical entities, dis-\nease coding, text classification, and relation ex-\ntraction, among others. However, one of the steps\nbefore solving any of these tasks is to create robust\nnumerical representations of the text so that the\ncomputer can handle this data.\nWord embeddings are dense, semantically mean-\ningful vector representations of a word. These\nmodels have proven to be a fundamental build-\ning block of neural network-based architectures\n(Lample et al., 2016). Although these models have\nobtained excellent results for several NLP tasks,\ntheir main drawback is that they provide a single-\nword representation in a given document. This is\nnot optimal since a word meaning may depend on\nthe sentence in which it appears. This type of word\nembedding is known as static word embeddings.\nContextual representation models handle this\nissue by creating word representations based on\nsentence-level context. These representations are\ncommonly retrieved from pretrained language mod-\nels (PLM). Classic examples of these models are\nELMO, BERT, RoBERTa, Flair, ALBERT, among\nothers. However, contextualized word embeddings\nmay not represent words as well as static ones, as\nresults obtained in Reimers and Gurevych (2019)\nsuggest.\nAlthough contextualized word embeddings have\nthese drawbacks, we can use these numeric repre-\nsentations of words to understand PLM represen-\ntations. Specifically, we are interested in study-\ning how domain-specific and general-domain PLM\nrepresent clinical and biomedical concepts. In this\nstudy, we aim to create a simple and efficient test\nfor measuring concept embeddings’ quality and\ncomparing clinical and biomedical PLM perfor-\nmance using a relevant knowledge base and graph,\nthe Unified Medical Language System (UMLS).\nA knowledge graph is an extensive network of\n197\nentities relevant to a specific domain. The network\ndescribes each entity’s semantic types, properties,\nand relationships. Knowledge graphs represent\nreal-world entities and their relations in a graph,\ndefine possible classes, and allow to relate arbitrary\nentities with each other (Ehrlinger and Wöß, 2016).\nThe UMLS is a knowledge graph that combines\nmany clinical and biomedical vocabularies and\nstandards to enable interoperability between com-\nputer systems (Bodenreider, 2004). The UMLS\nconsists of multiple knowledge sources. One is the\nmetathesaurus, a large, multi-purpose, and multi-\nlingual vocabulary database that contains informa-\ntion about biomedical and clinical-related concepts,\ntheir various names, and their relationships. An-\nother source is the semantic network, a consistent\ncategorization of all concepts represented in the\nmetathesaurus, providing a set of valuable relation-\nships between these concepts. In this work, we\nused both knowledge sources.\nTwo testing frameworks have been developed\nto measure the quality of language representations.\nFirst, an extrinsic test framework that uses the lan-\nguage representations to construct a more complex\narchitecture to solve a specific downstream task.\nSecond, an intrinsic test framework that measures\nthe capacity of the language representation to re-\nsolve semantic questions regarding the language\ndomain it represents (Zhai et al., 2016; Wang et al.,\n2019; Bakarov, 2018).\nTo construct intrinsic tests, we must compose\nquestions based on a source of truth. This source\ncan be expert knowledge, where we ask human\nexperts to write each one of these questions manu-\nally, or we can use a knowledge base to compose\nthese questions automatically. We used the UMLS\nknowledge graph to automatically derive a concept\nsimilarity intrinsic test using the length of the short-\nest path in the graph to compute a true similarity\nmeasure between concepts.\nThis intrinsic test will be used as a metric to\ncheck how good language representations are, but\nalso as a comparison measure of whether clini-\ncal and biomedical PLM are better compared to\ngeneral ones in downstream tasks such as Named\nEntity Recognition (NER).\n2 Related work\nPLM such as BERT (Devlin et al., 2019), ELMo\n(Peters et al., 2018), and GPT-2 (Radford et al.,\n2019) are able to produce contextualized word em-\nbeddings. It has been shown that contextualized\nword embeddings can achieve near state-of-the-art\nperformance in tasks such as POS tagging or NER\nusing probing models (Liu et al., 2019). Addi-\ntionally, contextualized word embeddings from top\nlayers of PLM produce more context-specific and\nanisotropic representations (Ethayarajh, 2019).\nRegarding the clinical and biomedical domain\nin English, there are several models to obtain con-\ntextualized embeddings, such as BioELMo (Jin\net al., 2019), Clinical BERT (Alsentzer et al., 2019),\nSciBERT (Beltagy et al., 2019), BioBERT (Lee\net al., 2020), among others. However, there re-\nmains a significant lack of language models in\nSpanish. The only models available are SciELO\nFlair (Akhtyamova et al., 2020), Clinical Flair\n(Rojas et al., 2022b), and clinical and biomedical\nversions of RoBERTa (Carrino et al., 2022). Al-\nthough these studies have shown that incorporating\ndomain-specific contextualized embeddings signifi-\ncantly improves the models’ performance in several\nextrinsic tasks, comparing their performances with\nintrinsic tests is still necessary.\nSince PLM creates word-level contextual repre-\nsentations, it is necessary to define a method for\ncombining these vectors to create sentence-level\nembeddings. For this purpose, a popular technique\nis the mean pooling of contextual word embed-\ndings (Reimers and Gurevych, 2019). However,\nthis method may lead to poor results if the PLM\nis not explicitly trained for similarity. Another\nstudy has proposed transforming the distribution\nof sentence-level embeddings to generate isotropic\nand smooth representations (Li et al., 2020). Cre-\nating these sentence-level representations is fun-\ndamental for testing the intrinsic tests proposed in\nthis research.\nCommon approaches to evaluate biomedical\nPLM performance are benchmarks such as BLUE\n(Peng et al., 2019) and BLURB (Gu et al., 2021),\nwhich are built for the English language. There\nis no relevant benchmark in Spanish, and every\nauthor selects some annotated datasets to evaluate\nPLM performance on specific downstream tasks.\nAlthough the amount of annotated datasets in Span-\nish is growing, there is a lack of intrinsic tasks that\ncan help to understand if a PLM is improving, and\nthis research tries to fill that gap.\n198\n3 Methods\nOur proposed method creates a semantic similar-\nity intrinsic test with medical concept pairs and\ntheir semantic distances. We extracted these con-\ncept pairs from the UMLS1 term graph and com-\nputed their distances as the length of the short-\nest directed path of parent relationships between\nthe concepts. We measured the correlation of the\nknowledge-graph-derived distance to the cosine\nsimilarity of the terms string descriptions on an em-\nbedding space projected using different language\nrepresentations. Finally, we compare these correla-\ntions with the performance on downstream tasks of\neach language representation.\n3.1 Concept pair selection and its graph\ndistances\nIn this vocabulary database, a concept is simply the\nmeaning of a medical entity. Each concept in the\nmetathesaurus has a unique and permanent concept\nidentifier (CUI).\nA UMLS concept can have multiple names be-\ncause the same meaning can be described with nu-\nmerous strings, for example, in different languages\nor source vocabularies. Each concept named de-\nscription is called an atom and is identified by an\natom identifier (AUI). To select a single concept\ndescription, we filtered out the atoms marked as\nnon-preferred in the metathresaurus. With this fil-\nter and by only selecting atoms in Spanish, we\nassigned a single string describing each medical\nconcept. In the UMLS Semantic Network, con-\ncepts are related using multiple relation types. The\nonly relation type we used to connect the concepts\nwas the parent relationship (PAR). We tried other\nrelationship types but continued with PAR relation-\nships because they are the most frequent. Child\nrelationships (CHD) have the same frequency as\nPAR relationships, given they are the inverse re-\nlation type of PAR. Thus we can choose any of\nthem.\nAfter the previous step, we imported concepts\nand their PAR relations into a graph database 2.\nNext, we queried the graph to select several ran-\ndom concepts and recursively extracted direct or\nrelated concepts at multiple distances. This means\nthere is a path of one or more PAR relations of\ndistance between pairs of concepts, as shown in\nFigure 1. Given that sometimes it is possible to\n1version 2022AA\n2Neo4j (https://neo4j.com/)\nfind multiple paths between two concepts, we only\nused the shortest path between them. This process\nallowed us to extract the path length between two\nconcepts. We select 20,000 concepts for this study\nto conduct the intrinsic tests rapidly. However, we\ncan choose more concepts if necessary.\nC0040426\nTooth structure\nC0011334\nDental caries\nC0266853\nEnamel caries\nC0266858\nIncipient enamel c.\nC4708523\nInitial state c.\nC0266854\nAcute enamel c.\nC0266846\nDentin caries\nFigure 1: PAR-related concepts from C0040426 (Tooth\nstructure). We highlight multiple paths,\n• • • •A dash-dot line represents the path between\nC0266858 (Incipient enamel caries) and\nC4708523 (Initial state caries) with a distance of 1\nPAR edge.\n• A dash-dash line represents the path between\nC0040426 (Tooth structure) and C0266846\n(Dentin caries) with a distance of 2 PAR edges.\n• A dot-dot line represents the path between\nC0040426 (Tooth structure) and C0266858 (In-\ncipient enamel caries) with a distance of 3 PAR\nedges.\n3.2 Generation of UMLS concepts’\nembeddings\nAfter selecting the pairs of concepts and their de-\nscriptions, we generate concepts’ embeddings us-\ning PLM. As UMLS concepts may contain more\nthan one token, extracting embeddings that can\nrepresent the whole concept and not just one\n199\nword is essential. To do this, we used mean\npooling of embeddings obtained for concept to-\nkens from a PLM. For models hosted in the Hug-\ngingface Model Repository3, we used the Python\nlibrary sentence-transformers4 (Reimers and\nGurevych, 2019), and for models hosted in the\nFlair repository, we used the Python libraryflair5\n(Akbik et al., 2019).\nAll of our experiments were conducted for Span-\nish language datasets. We generated concept em-\nbeddings for several PLM of interest with different\nbase architectures and domains. For the base archi-\ntectures, we selected BERT, RoBERTa, and Flair.\nAs for the domain, we chose, whenever possible,\ngeneral, biomedical, and clinical models. As we\ndid not find a publicly available BERT linguistic\nmodel for the clinical domain trained on Spanish\ntext, we tuned a general domain model in Span-\nish (Cañete et al., 2020) with clinical text obtained\nfrom the Chilean Waiting List Corpus (Báez et al.,\n2020, 2022).\n3.3 Implementation of intrinsic test\nWe build our intrinsic test as follows. First, we\ncalculate the cosine similarity between concept\nembedding pairs. Then, we obtained the Spear-\nman correlation between cosine similarity and path\nlength, which we called ρ. This simple process\nallowed us to get our first metric. We expect that\na greater path length between two concepts will\nresult in a lower cosine similarity, given that they\nare farther semantically. Therefore, the Spearman\ncorrelation (ρ) between these two distances over\nall concepts pairs will be negative. If we compare\nembeddings generated by different PLM, we could\nexpect that more domain-specific PLM will gen-\nerate embeddings with more semantic differences\nbetween concepts within the domain, resulting in\na more negative ρ. Thus, a more negative ρ indi-\ncates a PLM that can separate better semantically\nconcepts within a domain.\nAs a part of our analysis, we calculated the av-\nerage cosine similarity per path length. This step\nled us to obtain a complementary metric, the differ-\nence of mean cosine similarity for the shortest path\nlength and the longest path length, that we called\nδ. The rationality behind this metric is similar to\nwhat we found in the previous one. However, in\n3https://huggingface.co/models\n4https://github.com/UKPLab/\nsentence-transformers\n5https://github.com/flairNLP/flair\nthis case, a more positive δ indicates a PLM that\ncan better separate concepts semantically within a\ndomain.\n3.4 Comparison with extrinsic test\nOur intrinsic metrics were compared to extrinsic\nmetrics using the F1 score in relevant biomedical\nand clinical NER datasets. The idea of incorporat-\ning extrinsic tests is to check if having better values\nof our intrinsic metrics will translate into better\nperformance in downstream tasks for the selected\nPLM.\nTo build a reproducible extrinsic comparison\nfor all PLM base architectures, we create a prob-\ning task for NER. In other words, we extracted\ncontextualized embeddings from a PLM without\nfine-tuning for any downstream task, and those em-\nbeddings were input into a linear layer trained for\nNER.\nThe clinical and biomedical datasets in Spanish\nused for the NER probing task were:\n• CANTEMIST6 (Miranda-Escalada et al.,\n2020): annotated corpus with tumor morphol-\nogy mentions in 1,301 oncological clinical\ncase reports.\n• PharmaCoNER7 (Gonzalez-Agirre et al.,\n2020): annotated corpus with entities such\nas chemical compounds and drugs in 1,000\nclinical case studies.\n• CT-EBM-SP8 (Campillos-Llanos et al., 2021):\nannotated corpus with UMLS entities in 1,200\ntexts about clinical trials studies and clinical\ntrials announcements.\n• NUBes9 (Lopez et al., 2020): annotated cor-\npus with negation and uncertainty entities\nin anonymised health records (29,682 sen-\ntences).\n4 Results\nWe queried 20,000 pairs of random atoms to select\nUMLS concepts from the graph database. Figure 2\nshows the histogram of those pairs by path length.\nWe can see that pair frequency increases as path\nlength increase until seven parent relationships of\n6https://zenodo.org/record/3978041\n7https://zenodo.org/record/4270158\n8http://www.lllf.uam.es/ESP/nlpmedterm_en\n9https://github.com/Vicomtech/\nNUBes-negation-uncertainty-biomedical-corpus\n200\ndistance. After that point, the frequency of pairs\ndecreases until it reaches 14 relations of distance.\nWe removed all path lengths containing less than\n300 pairs of concepts to calculate the metrics ρ and\nδ.\n1 2 3 4 5 6 7 8 9 10 11 12\nPath length\n0\n500\n1000\n1500\n2000\n2500\n3000Frequency of concept pairs\nFigure 2: Histogram of UMLS concept pairs by path\nlength\nThen, we plot a boxplot of cosine similarity by\npath length for every PLM. Figure 3 shows such\na boxplot for a general-domain BERT trained in\nSpanish text (Cañete et al., 2020)10. This plot al-\nlows us to understand how cosine similarity dis-\ntributes along path length.\nIt is clear from the plot that average cosine simi-\nlarity decreases as path length increases. However,\nthe decline is near null or even negative from path\nlength four onwards. Moreover, the average cosine\nsimilarity is not going near zero. We hypothesize\nthis pattern is because all concepts are related to\nclinical and biomedical domains and also due to\nthe anisotropic behavior of sentence embeddings\nobtained from PLM. As discussed in Ethayarajh\n(2019), contextualized embeddings obtained from\nPLM tend to distribute not evenly in the embed-\nding space but in a small portion of it. Therefore,\nthey still have a relatively high similarity when\ncomparing dissimilar concepts.\nTo compare several PLM, we plot only average\ncosine similarity by path length for every language\nmodel, as shown in Figure 4. As we can see, av-\nerage cosine similarity by path length varies for\ndifferent base architectures and domains of PLM.\nHowever, they all repeat the same decline pattern\nas path length increases.\nSimilarly to Figure 3, Figure 4 does not show any\naverage cosine similarity going near zero. How-\never, the similarity level where each PLM stabilizes\n10Other models’ plots are included in the appendix\n1 2 3 4 5 6 7 8 9 10 11 12\nPath length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine similarity\nFigure 3: Boxplot of cosine similarity by path length\nfor a general-domain BERT trained on Spanish text.\nis different. Not surprisingly, language models\ntrained on a similar corpus or being a fine-tuned\nversion from another have comparable similarity\nlevels. RoBERTa-es-clinical was trained with the\nsame corpora as RoBERTa-es-biomedical plus a\nclinical corpus (Carrino et al., 2022), and BERT-\nes-clinical is a fine-tuned model from BERT-es-\ngeneral over a clinical corpus.\n2 4 6 8 10 12\nPath length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Cosine Similarity\nRoBERT a-es-clinical\nRoBERT a-es-biomedical\nRoBERT a-es-general\nFlair-es-clinical\nFlair-es-biomedical\nFlair-es-general\nBERT-es-clinical\nBERT-es-general\nFigure 4: Average cosine similarity by path length for\nmultiple language models\nTo measure the degree of the decline, we calcu-\nlated the metrics ρ and δ for all the selected PLM,\nas shown in Table 1. We notice that ρ and δ are\ngreater in absolute value for biomedical and clini-\ncal models than general ones within the same base\narchitecture. This means that given a PLM base\narchitecture, the degree of decline of the average co-\nsine similarity is greater for domain-specific mod-\nels than for general domain models. This finding\nsuggests that domain-specific PLM and their con-\ncept embeddings better represent UMLS concepts;\n201\nReference Architecture Domain ρ δ\nOurs BERT Clinical -0.38 0.25\n(Cañete et al., 2020) BERT General -0.30 0.18\n(Akhtyamova et al., 2020) Flair Biomedical -0.24 0.27\n(Rojas et al., 2022b) Flair Clinical -0.23 0.27\n(Akbik et al., 2018) Flair General -0.20 0.11\n(Carrino et al., 2021) RoBERTa Clinical -0.31 0.09\n(Carrino et al., 2021) RoBERTa Biomedical -0.28 0.13\n(Gutiérrez-Fandiño et al., 2022) RoBERTa General -0.23 0.03\nTable 1: Correlations and differences for each language representation. The table is sorted ascending by ρ and then\nby base architecture. Every ρ is statistically significant.\nArchitecture Domain CANTEMIST PharmaCoNER CT-EBM-SP NUBes\nBERT Clinical 0.739 (0.018) 0.577 (0.013) 0.742 (0.012) 0.791 (0.009)\nBERT General 0.757 (0.004) 0.582 (0.007) 0.714 (0.006) 0.797 (0.013)\nFlair Biomedical 0.784 (0.006) 0.615 (0.013) 0.725 (0.008) 0.792 (0.003)\nFlair Clinical 0.771 (0.009) 0.580 (0.021) 0.694 (0.000) 0.802 (0.003)\nFlair General 0.714 (0.013) 0.558 (0.002) 0.633 (0.002) 0.780 (0.005)\nRoBERTa Clinical 0.794 (0.009) 0.633 (0.010) 0.792 (0.012) 0.820 (0.004)\nRoBERTa Biomedical 0.784 (0.006) 0.626 (0.009) 0.794 (0.014) 0.821 (0.005)\nRoBERTa General 0.767 (0.014) 0.584 (0.006) 0.734 (0.005) 0.804 (0.003)\nTable 2: F1 scores and standard deviations for NER probing task over four datasets in Spanish. The table is sorted\naccording the same criteria as Table 1\nhence the similarity pattern displayed. However, it\nis important to note that we do not find this behav-\nior when comparing different base architectures.\nWe can see F1 scores for every NER probing\ntask by PLM in Table 2. As expected, we can see a\ntendency to obtain better F1 scores for clinical or\nbiomedical PLM than general ones. However, in\nthe case of BERT architecture, results are mixed.\nWe believe this behavior could be due to the cre-\nation of the clinical BERT model. Instead of being\ntrained from scratch with clinical and biomedical\ndata, it is a fine-tuned version of a general BERT.\nOn the other hand, clinical and biomedical Flair\nand RoBERTa models were trained from scratch\nwith domain-specific data.\nInterestingly, when ρ metric is greater for a clini-\ncal model compared to a biomedical one, F1 scores\nfor NER probing tasks are also greater, as we can\nsee in the case of RoBERTa architecture for CAN-\nTEMIST and PharmaCoNER datasets. In the case\nof CT-EBM-SP and NUBes, there are no such dif-\nferences, but F1 scores for clinical and biomedical\nare almost the same. On the contrary, when ρ met-\nric is greater for a biomedical model compared to a\nclinical one, then F1 scores present a similar behav-\nior, as we can see in the case of Flair architecture\nfor CANTEMIST, PharmaCoNER, and CT-EBM-\nSP datasets. And as same as the previous situation,\nF1 scores for another dataset (NUBes) are almost\nthe same. We do not observe this pattern for δ\nmetric.\nThis finding suggests that ρ metric could be ap-\nplied as a useful intrinsic test for comparing PLM\nwithin the same base architecture. However, it is\nimportant to note when comparing ρ metric for dif-\nferent base architectures, we do not find a clear\nrelation with F1 scores. Consequently, we present\nthe ρ metric as an intrinsic test to measure improve-\nments for PLM within the same base architecture.\n5 Conclusion and future work\nUsing domain-specific PLMs for downstream tasks\nhas allowed reaching the state-of-the-art in sev-\neral benchmarks. However, since these models are\ntrained in large corpora, fine-tuning them or train-\ning from scratch is time-consuming. Therefore,\nbefore using these models to solve downstream\ntasks, it is crucial to create intrinsic tests that vali-\ndate whether a domain-specific PLM yields better\nresults than its base version.\nIn this study, we build an intrinsic test for clini-\ncal and biomedical PLM using contextualized em-\n202\nbeddings and the UMLS knowledge graph. We\nsuggest that our intrinsic test can help compare\ndomain-specific PLM performance within its base\narchitecture, which could be used to evaluate im-\nprovements when building PLM. Our experimental\nresults show that this intrinsic test can capture im-\nprovements in clinical and biomedical PLM over\ngeneral ones. Also, it correlates with better results\nin a NER probing task over four datasets in Span-\nish.\nIn future work, we can implement this study for\nother languages. Additionally, we can compare\nour intrinsic test with other probing tasks such as\nPOS-tagging or coreference or even other clinical\ndownstream tasks such as patient mortality or un-\nplanned readmission. On the other hand, since our\nexperimental datasets contain nested entities, but\nfor simplicity, they were ignored, we would like to\nexplore the use of contextualized embeddings in\nmodels that can address them, such as those pro-\nposed in Rojas et al. (2022a). Finally, we can com-\npare several experimental settings, such as multiple\nnumbers of concept pairs.\nLimitations\nWe can group the limitations of our study in\nthe ones related to the graph knowledge, the se-\nlected PLM, comparison with other embedding\ntechniques, and language. First, regarding graph\nknowledge, we could have chosen several random\nsubsets of concept pairs of different lengths and\ntypes of relations to check if our findings are still\npresent. Second, we selected three base architec-\ntures, and all of them were of encoder type. Third,\nwe could have compared our results with static em-\nbeddings. And finally, we could have selected more\nlanguages for comparison.\nEthics Statement\nWe state that our work complies with the ACL Code\nof Ethics. We believe that our work could help the\nresearch community with a new tool for their work\nin clinical and biomedical PLM. Our study was\nbased on publicly available and anonymized data\nto avoid the privacy issues that clinical data may\nraise.\nAcknowledgements\nThis work was funded by ANID Chile: Basal Funds\nfor Center of Excellence FB210005 (CMM) and\nFB210017 (CENIA); Millennium Science Initiative\nProgram ICN17_002 (IMFD) and ICN2021_004\n(iHealth), Fondecyt grant 11201250, and Na-\ntional Doctoral Scholarships 21211659 (C.A.) and\n21220200 (F.V .). Regarding hardware, the research\nwas partially supported by the supercomputing\ninfrastructure of the NLHPC (ECM-02) and the\nPatagón supercomputer of Universidad Austral de\nChile (FONDEQUIP EQM180042).\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-\nart NLP. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54–59, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018.\nContextual string embeddings for sequence label-\ning. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1638–\n1649, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nLiliya Akhtyamova, Paloma Martínez, Karin Verspoor,\nand John Cardiff. 2020. Testing Contextualized Word\nEmbeddings to Improve NER in Spanish Clinical\nCase Narratives. IEEE Access, 8:164717–164726.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop ,\npages 72–78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nPablo Báez, Felipe Bravo-Marquez, Jocelyn Dunstan,\nMatías Rojas, and Fabián Villena. 2022. Auto-\nmatic extraction of nested entities in clinical referrals\nin spanish. ACM Transactions on Computing for\nHealthcare (HEALTH), 3(3):1–22.\nPablo Báez, Fabián Villena, Matías Rojas, Manuel\nDurán, and Jocelyn Dunstan. 2020. The Chilean\nwaiting list corpus: a new resource for clinical named\nentity recognition in Spanish. In Proceedings of the\n3rd Clinical Natural Language Processing Workshop,\npages 291–300, Online. Association for Computa-\ntional Linguistics.\nAmir Bakarov. 2018. A survey of word em-\nbeddings evaluation methods. arXiv preprint\narXiv:1801.09536.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n203\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nOlivier Bodenreider. 2004. The Unified Medical Lan-\nguage System (UMLS): integrating biomedical ter-\nminology. Nucleic Acids Research , 32(Database\nissue):D267–D270.\nLeonardo Campillos-Llanos, Ana Valverde-Mateos,\nAdrián Capllonch-Carrión, and Antonio Moreno-\nSandoval. 2021. A clinical trials corpus anno-\ntated with UMLS entities to enhance the access to\nevidence-based medicine. BMC Medical Informatics\nand Decision Making.\nCasimiro Pio Carrino, Jordi Armengol-Estapé, Asier\nGutiérrez-Fandiño, Joan Llop-Palao, Marc Pàmies,\nAitor Gonzalez-Agirre, and Marta Villegas. 2021.\nBiomedical and clinical language models for spanish:\nOn the benefits of domain-specific pretraining in a\nmid-resource scenario. CoRR, abs/2109.03570.\nCasimiro Pio Carrino, Joan Llop, Marc Pàmies, Asier\nGutiérrez-Fandiño, Jordi Armengol-Estapé, Joaquín\nSilveira-Ocampo, Alfonso Valencia, Aitor Gonzalez-\nAgirre, and Marta Villegas. 2022. Pretrained biomed-\nical language models for clinical NLP in Spanish.\nIn Proceedings of the 21st Workshop on Biomedi-\ncal Language Processing, pages 193–199, Dublin,\nIreland. Association for Computational Linguistics.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC at ICLR 2020.\nHercules Dalianis. 2018. Characteristics of Patient\nRecords and Clinical Corpora. In Clinical Text Min-\ning, pages 21–34. Springer International Publishing,\nCham.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLisa Ehrlinger and Wolfram Wöß. 2016. Towards a\ndefinition of knowledge graphs. In SEMANTiCS.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nAitor Gonzalez-Agirre, Antonio Miranda-Escalada, Ob-\ndulia Rabal, and Martin Krallinger. 2020. Pharma-\nCoNER corpus: gold standard annotations of Phar-\nmacological Substances, Compounds and proteins in\nSpanish clinical case reports. Zenodo. Funded by\nthe Plan de Impulso de las Tecnologías del Lenguaje\n(Plan TL).\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Trans. Comput. Healthcare,\n3(1).\nAsier Gutiérrez-Fandiño, Jordi Armengol-Estapé, Marc\nPàmies, Joan Llop-Palao, Joaquin Silveira-Ocampo,\nCasimiro Pio Carrino, Carme Armentano-Oller, Car-\nlos Rodriguez-Penagos, Aitor Gonzalez-Agirre, and\nMarta Villegas. 2022. MarIA: Spanish language mod-\nels. Procesamiento del Lenguaje Natural, 68(0):39–\n60.\nQiao Jin, Bhuwan Dhingra, William Cohen, and\nXinghua Lu. 2019. Probing biomedical embeddings\nfrom language models. In Proceedings of the 3rd\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP, pages 82–89, Minneapolis, USA. As-\nsociation for Computational Linguistics.\nDaniel Jurafsky and James H. Martin. 2000. Speech\nand Language Processing: An Introduction to Natu-\nral Language Processing, Computational Linguistics,\nand Speech Recognition, 1st edition. Prentice Hall\nPTR, USA.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270, San Diego, California. Association\nfor Computational Linguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36:1234 – 1240.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\n204\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nSalvador Lima Lopez, Naiara Perez, Montse Cuadros,\nand German Rigau. 2020. NUBes: A Corpus of\nNegation and Uncertainty in Spanish Clinical Texts.\nIn Proceedings of The 12th Language Resources and\nEvaluation Conference (LREC2020) , pages 5772–\n5781, Marseille, France. European Language Re-\nsources Association.\nAntonio Miranda-Escalada, Eulalia Farré, and Martin\nKrallinger. 2020. Named entity recognition, concept\nnormalization and clinical coding: Overview of the\ncantemist track for cancer text mining in Spanish, cor-\npus, guidelines, methods and results. In Proceedings\nof the Iberian Languages Evaluation Forum (Iber-\nLEF 2020), CEUR Workshop Proceedings.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: An evaluation of BERT and ELMo on ten bench-\nmarking datasets. In Proceedings of the 18th BioNLP\nWorkshop and Shared Task, pages 58–65, Florence,\nItaly. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMatias Rojas, Felipe Bravo-Marquez, and Jocelyn Dun-\nstan. 2022a. Simple yet powerful: An overlooked\narchitecture for nested named entity recognition. In\nProceedings of the 29th International Conference\non Computational Linguistics , pages 2108–2117,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nMatías Rojas, Jocelyn Dunstan, and Fabián Villena.\n2022b. Clinical flair: A pre-trained language model\nfor Spanish clinical natural language processing. In\nProceedings of the 4th Clinical Natural Language\nProcessing Workshop, pages 87–92, Seattle, W A. As-\nsociation for Computational Linguistics.\nBin Wang, Angela Wang, Fenxiao Chen, Yuncheng\nWang, and C.-C. Jay Kuo. 2019. Evaluating word em-\nbedding models: methods and experimental results.\nAPSIPA Transactions on Signal and Information Pro-\ncessing, 8:e19. Publisher: Cambridge University\nPress.\nMichael Zhai, Johnny Tan, and Jinho Choi. 2016. Intrin-\nsic and Extrinsic Evaluations of Word Embeddings.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 30(1).\nA Appendix\n205\nFigure 5: Boxplots of cosine similarity by path length for selected PLM trained in Spanish text\n206"
}