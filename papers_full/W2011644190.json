{
  "title": "Latent semantic information in maximum entropy language models for conversational speech recognition",
  "url": "https://openalex.org/W2011644190",
  "year": 2003,
  "authors": [
    {
      "id": "https://openalex.org/A5084723540",
      "name": "Yonggang Deng",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5014580424",
      "name": "Sanjeev Khudanpur",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W166966826",
    "https://openalex.org/W1529481723",
    "https://openalex.org/W2121847905",
    "https://openalex.org/W1586176709",
    "https://openalex.org/W2116236039",
    "https://openalex.org/W1665921526",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W1621933882",
    "https://openalex.org/W2096375461",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W2112874453"
  ],
  "abstract": "Latent semantic analysis (LSA), first exploited in indexing documents for information retrieval, has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal. In this paper we present an investigation into the use of LSA in language modeling for conversational speech recognition. We find that previously proposed methods of combining an LSA-based unigram model with an N-gram model yield much smaller reductions in perplexity on speech transcriptions than has been reported on written text. We next present a family of exponential models in which LSA similarity is a feature of a word-history pair. The maximum entropy model in this family yields a greater reduction in perplexity, and statistically significant improvements in recognition accuracy over a trigram model on the Switchboard corpus. We conclude with a comparison of this LSA-featured model with a previously proposed topic-dependent maximum entropy model.",
  "full_text": null,
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9781961441040039
    },
    {
      "name": "Trigram",
      "score": 0.8356147408485413
    },
    {
      "name": "Computer science",
      "score": 0.7885646820068359
    },
    {
      "name": "Language model",
      "score": 0.7397254705429077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6805799007415771
    },
    {
      "name": "Latent semantic analysis",
      "score": 0.6743313670158386
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.6626815795898438
    },
    {
      "name": "Natural language processing",
      "score": 0.6398593187332153
    },
    {
      "name": "Speech recognition",
      "score": 0.5218790173530579
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ],
  "cited_by": 9
}