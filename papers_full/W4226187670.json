{
  "title": "Reversible Linguistic Steganography With Bayesian Masked Language Modeling",
  "url": "https://openalex.org/W4226187670",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2147541054",
      "name": "Ching Chun Chang",
      "affiliations": [
        "University of Warwick"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4232836212",
    "https://openalex.org/W2159390040",
    "https://openalex.org/W2096050104",
    "https://openalex.org/W2124890704",
    "https://openalex.org/W2169552486",
    "https://openalex.org/W2116467012",
    "https://openalex.org/W1511938396",
    "https://openalex.org/W2138691165",
    "https://openalex.org/W6637162671",
    "https://openalex.org/W6640425456",
    "https://openalex.org/W2543927648",
    "https://openalex.org/W4247934040",
    "https://openalex.org/W2118085659",
    "https://openalex.org/W2137288380",
    "https://openalex.org/W1974641035",
    "https://openalex.org/W2151092698",
    "https://openalex.org/W2106950225",
    "https://openalex.org/W2142181334",
    "https://openalex.org/W3094904360",
    "https://openalex.org/W2125330192",
    "https://openalex.org/W2126975757",
    "https://openalex.org/W2147654286",
    "https://openalex.org/W6683232084",
    "https://openalex.org/W2949332170",
    "https://openalex.org/W1832326700",
    "https://openalex.org/W2162174114",
    "https://openalex.org/W2112507198",
    "https://openalex.org/W2025994941",
    "https://openalex.org/W1975586457",
    "https://openalex.org/W6691643910",
    "https://openalex.org/W1750309227",
    "https://openalex.org/W2157823837",
    "https://openalex.org/W1594001530",
    "https://openalex.org/W6691336473",
    "https://openalex.org/W2006134202",
    "https://openalex.org/W1516983852",
    "https://openalex.org/W2893556909",
    "https://openalex.org/W3085519426",
    "https://openalex.org/W3109254234",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2157195790",
    "https://openalex.org/W3106315753"
  ],
  "abstract": "Text authentication serves a vital role in the defense of digital identity and content against various types of cybercrime. The use of a digital signature is a common cryptographic technique for text authentication. Linguistic steganography can be applied to further conceal a digital signature within the corresponding text to facilitate data management. However, steganographic distortion lurking in the text, albeit almost imperceptible, has the potential to cause automatic computing machinery to make biased decisions. This has led to an interest in the pursuit of reversibility, the ability to reverse a steganographic process and remove distortion. In this article, we propose a reversible steganographic system for natural language text. We use a pre-trained transformer neural network for masked language modeling and embed messages in a reversible manner via predictive word substitution. Furthermore, we derive an adaptive steganographic route by taking account of predictive uncertainty, which is quantified based on a theoretical framework of Bayesian deep learning. Experimental results show that the proposed steganographic system can attain a proper balance between capacity, imperceptibility, and reversibility with close semantic and sentimental similarities between cover and stego texts.",
  "full_text": "714 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 10, NO. 2, APRIL 2023\nReversible Linguistic Steganography With\nBayesian Masked Language Modeling\nChing-Chun Chang\nAbstract— Text authentication serves a vital role in the defense\nof digital identity and content against various types of cyber-\ncrime. The use of a digital signature is a common cryptographic\ntechnique for text authentication. Linguistic steganography can\nbe applied to further conceal a digital signature within the\ncorresponding text to facilitate data management. However,\nsteganographic distortion lurking in the text, albeit almost\nimperceptible, has the potential to cause automatic computing\nmachinery to make biased decisions. This has led to an interest in\nthe pursuit of reversibility, the ability to reverse a steganographic\nprocess and remove distortion. In this article, we propose\na reversible steganographic system for natural language text.\nWe use a pre-trained transformer neural network for masked\nlanguage modeling and embed messages in a reversible manner\nvia predictive word substitution. Furthermore, we derive an\nadaptive steganographic route by taking account of predictive\nuncertainty, which is quantiﬁed based on a theoretical framework\nof Bayesian deep learning. Experimental results show that the\nproposed steganographic system can attain a proper balance\nbetween capacity, imperceptibility, and reversibility with close\nsemantic and sentimental similarities between cover and stego\ntexts.\nIndex Terms— Bayesian uncertainty analysis, masked language\nmodeling, reversible linguistic steganography.\nI. I NTRODUCTION\nA\nUTHENTICATION is an essential part of cyber security.\nIt is the process of validating the identity of users and the\nintegrity of digital content. As cyber space continues to expand\nin scope and scale, authentication plays an important role in\nmaintaining trust against various types of deception, including,\nbut not limited to, impersonating identities, spreading spam,\ndisseminating fake news, sending malicious links, and tamper-\ning with digital media. A digital signature is a mathematical\nproof of authenticity using modern cryptographic techniques\nsuch as encryption and hashing [1]. The incorporation of\na timestamp and other tamper-evident designs can further\nstrengthen security. Such auxiliary data, however, carry the\nrisk of accidental loss and mis management during storage,\ntransmission, or format transformation.\nSteganography is the practice of concealing information\n(e.g. a secret message, a copyright mark, or a serial number)\nwithin a carrier object [2]–[4]. Steganographic techniques have\nManuscript received 9 December 2021; revised 26 January 2022 and\n4 March 2022; accepted 18 March 2022. Date of publication 8 April 2022;\ndate of current version 3 April 2023.\nThe author is with the Department of Computer Science, University of\nWarwick, Coventry, CV4 7AL, U.K. (e-m ail: c.c.chang@warwickgrad.net).\nDigital Object Identiﬁer 10.1109/TCSS.2022.3162233\nbeen applied to covert communications [5], ownership identi-\nﬁcation [6], broadcast monitoring [7], and traitor tracing [8].\nIt can also serve as an authentication solution by embedding\nauxiliary metadata in a digital ﬁle, thereby mitigating the\nrisk of losing data. While steganographic distortion is often\nimperceptible to human sensory systems, it can bias deci-\nsions of automatic computing machinery. Previous studies of\nadversarial attacks have reported that machine learning models\ncan be vulnerable to imperceptible perturbations [9]–[11].\nSuch distortion might also be inadmissible in some sensitive\ncircumstances such as forensic science, legal proceedings,\nmedical diagnosis, and military reconnaissance.\nReversibility is the key to removing steganographic distor-\ntion. Reversible computing describes the notion that a compu-\ntational process can to some extent be time-reversible. Most\nreversible steganographic methods are designed for digital\nimagery [12]–[19], whereas methods for textual data remain\nrelatively undeveloped. A possible explanation is that many\nwell-developed tools are ava ilable for exploiting the redun-\ndancies in visual signals on which steganographic methods\nrely. In contrast, manipulating natural language texts can\nbe much more challenging, considering that even the tiniest\nchange in a character can be discernible to a careful reader.\nWith the worldwide popularization of social media and the\ntechnological advances in natural language processing (NLP),\ntextual data have become an important source of information.\nHence, reversible steganography for textual data has emerged\nas a promising research ﬁeld.\nTypographical steganography is a methodology that embeds\nmessages by manipulating the typeface, spacing, font size,\nor other typographical characteristics of texts [20]–[24].\nIt treats text documents as a special type of imagery and\nis usually applied to printed copies. However, portability\nand robustness are restricted because this class of method\nis unable to withstand retyping and font changes. Linguistic\nsteganography deals instead with natural language per se and\nis thereby able to resist such text processing. It exploits\nlinguistic knowledge and con ceals messages by modifying\nlinguistic properties, ideally without altering the sentence\nsemantics or degrading sentence ﬂuency. Linguistic steganog-\nraphy can be broadly categorized into the following classes:\nlexical class, syntactic class, and generative class. A typical\nlexical method is synonym substitution, which uses different\nsynonyms of a word to represent different message digits\n[25]–[27]. In principle, words belonging to the same synonym\nset have similar meanings and thus can be substituted for\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nCHANG: REVERSIBLE LINGUISTIC STEGANOGRAPHY WITH BAYESIAN MASKED LANGUAGE MODELING 715\neach other without causing notable semantic change. Syn-\ntactic methods are based on the fact that a sentence can\nbe transformed into semantically equivalent syntactic struc-\ntures such as active-to-passive voice conversion, subject–verb\ninversion, and topicalization [28]–[31]. Generative methods\nconsider further that a senten ce can be translated into a wide\nspectrum of forms or completely rewritten subject to minimal\nchange to the semantics. A feasible approach is to use a\nset of machine translators to generate multiple translations\nof a given sentence [32]–[35]. Controllable natural language\ngeneration has also emerged as a promising direction for\ngenerative-level linguistic steganography [36]–[40]. Although\nthese linguistic methods ﬁnd their applications in areas such\nas secret communications and ownership identiﬁcation, none\nof them meets the reversibility requirement for distortion-free\ntext authentication.\nIn this article, we study reversible linguistic steganography.\nWe apply a masked language model to generate a list of\npredictive words (Section II) and embed messages via predic-\ntive word substitution (Section III). The underlyi ng principle\nis that most words can be retrieved within a ﬁnite number\nof predictive words, and this type of redundancy can be\nleveraged for reversibility. The performance of the stegano-\ngraphic system is associated with the accuracy of the predictive\nmodel. We further observe that the carrier words can be\nselected more efﬁciently by exploiting predictive uncertainty\n(Section IV). To determine an efﬁcient route for message\nembedding, we study uncertainty quantiﬁcation based on a\ntheoretical framework of Bayesi an deep learning (Section V).\nThe remainder of this article is organized as follows.\nSection II provides a brief overview of masked language mod-\neling. Section III introduces a practical method of reversible\nlinguistic steganography. Section IV offers a further discus-\nsion on steganographic routing. Section V presents Bayesian\nuncertainty quantiﬁcation. Section VI shows the experimental\nresults for the proposed systems. Concluding remarks are\noffered in Section VII.\nII. M\nASKED LANGUAGE MODELING\nMasked language modeling is a ﬁll-in-the-blank task (a.k.a.\na cloze test), where a model attempts to predict a masked\nword from the surrounding context words. Tokenization is a\npreliminary step whereby a text sequence is split into tokens\n(e.g. words, punctuation marks, and numbers). A masked\nlanguage model takes an input sequence that contains one or\nmore mask tokens and estimates the probability distribution\nof each mask token over the entire vocabulary.\nGiven a sequence of tokens, a fundamental issue across\nnearly all NLP tasks is how to represent tokens as numer-\nical input in a computational model. The arguably simplest\nrepresentation is the one-hot vector. It is a binary vector of\n∥V ∥ elements, with all elements s et to 0 except the element\nat the index of the corresponding word in a dictionary, where\n∥V ∥ is the size of the vocabulary. This type of denotational\nrepresentation is sparse and treats each word as a completely\nindependent entity; hence, it cannot capture latent word seman-\ntics. In linguistics, the distributional hypothesis suggests that\nwords tend to have similar meanings if occurring in similar\ncontexts [41]. This gives rise to the notion of distributional\nrepresentation. For a given corpus, a co-occurrence matrix\nof size ∥V ∥×∥ V ∥ is computed by counting the number\nof times each word appears within a context window around\nthe word of interest. Words with similar co-occurrence pat-\nterns are expected to have similar meanings. However, the\nmatrix is of high sparsity due to the curse of dimensionality.\nWhile singular value decomposition (SVD) can be applied for\ndimensionality reduction, it has a disadvantage in terms of\nscalability. This algorithm is computationally expensive for\nlarge matrices and has to be performed again from scratch\nif the co-occurrence matrix is updated by incorporating new\nterms or corpora. A modern approach toward representation\nlearning is to train a neural network model on a certain task\n(e.g. language modeling) with each word vector initialized\nrandomly, and then update word vectors iteratively using\na stochastic optimization algorithm (e.g. gradient descent).\nA common limitation of early word vectorization models\n(e.g. word2vec [42] and GloVe [43]) is their inapplicability\nto polysemy and homonymy. Multiple meanings of a word\nare conﬂated into a single indistinguishable representation\nregardless of the context within which the word appears.\nContextual word representation is an advanced concept in\nwhich each word vector can be adapted dynamically to its\ncontext [44]. The BERT model (standing for bidirectional\nencoder representations from transformers) is a state-of-the-\nart neural network developed by Google for learning con-\ntextual word representation [45]. The model is based on\nthe transformer architecture which comprises multiple layers\nof self-attention modules and processes an input sequence\nbidirectionally [46]. To learn contextual word representation,\nthe model is trained to perform masked language modeling.\nThis task forces the model to learn more about contextual\ninformation. The knowledge learned from masked language\nmodeling can then be transferred to tackle other downstream\nNLP tasks (e.g. machine translation, sentiment analysis, topic\ncategorization). This is done by ﬁne-tuning: choosing relevant\nlayers in the pre-trained model, adding task-speciﬁc layers\non top of the model, and training the model on new data.\nThere are a variety of ways to use a pre-trained BERT model,\nand transfer learning is still a matter of ongoing research.\nNevertheless, our steganographic system relies only on a basic\nfunction of the BERT model, namely, masked language mod-\neling. The architectural details of the BERT masked language\nmodel are illustrated in Fig 1.\nIII. R\nEVERSIBLE LINGUISTIC STEGANOGRAPHY\nReversible linguistic steganography considers the following\nscenario. A sender wishes to communicate a message to a\nreceiver. The message is concealed in a carrier sequence of\ntext, causing recoverable distortions to the carrier sequence.\nWe refer to the original sequence as cover and its distorted\ncounterpart as stego. The message varies with speciﬁc appli-\ncations and is generally assumed to be a random binary\nsequence. The objective is to assure the accuracy of message\nextraction and text recovery, while keeping steganographic\ndistortion as low as possible.\n716 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 10, NO. 2, APRIL 2023\nFig. 1. Architecture of the BERT masked language model.\nThe proposed method is based on masked language model-\ning, as depicted in Figure 2. To begin with, we mask a cover\nword in a text sequence and form a masked sub-sequence by\ntaking the mask token and a ﬁxed number of context words on\nthe left-hand and right-hand sides. The masked sub-sequence\nis then fed into a pre-trained language model to obtain the\nprobability distribution of the masked cover word. We sort the\nprobabilities in descending order and derive a list of predictive\nwords. The core idea is to substitute the cover word with a\npredictive word (or, more precisely, map the index of the cover\nword to another index) to represent a message digit.\nLet us denote by x the index of the cover word in the list,\nwhere x ∈ N\n0 is a non-negative integer. We set a bound\nfor the indices such that x must be less than the bound.\nIf not, we skip the current word and proceed to the next\ncover word. We further set a threshold θ to separate the carrier\nand non-carrier indices. The ca rrier indices constitute a ﬁnite\nset of size ∥θ∥. To achieve reversibility, encoding must be\na bijective function (i.e. a one-to-one correspondence). The\nnumber of unique combinations between all possible carrier\nindices and message digits is given by the Cartesian product\nof the two sets. Since all possible message digits form a binary\nFig. 2. Linguistic steganography base d on the pre-trained masked language\nmodel.\nFig. 3. Carrier, noncarrier, and ambigu ity intervals w.r.t. different settings\nof threshold.\nset, message embedding doubles the size of the carrier set\nand causes an overlap between carrier and non-carrier indices.\nTo avoid ambiguity in message extraction, the non-carrier\nindices have to be shifted outward. Index shifting can induce\nout-of-bound indices. The set of out-of-bound indices is also\nof size ∥θ∥. These indices are kept unshifted, and one ﬂag\nbit is required to distinguish between a shifted and unshifted\nnon-carrier index in the ambiguity interval [(bound − θ),\n(bound − 1)]. The ﬂag bits are regarded as an overhead of\nreversibility. At ﬁrst glance, it seems that index shifting only\nacts to transfer the ambiguity interval from one end to another\nand serves little if any purpose. Yet, the overhead size is\nreduced substantially because the indices which are close to\nthe bound rarely occur. Given a functional predictive model,\nwe may reasonably assume that the frequency of an index\nvalue follows an exponential distribution (rather than a uniform\ndistribution); that is, a smaller index occurs more frequently\nand vice versa. An illustration of carrier interval, non-carrier\ninterval, and ambiguity interval with different θ settings is\nshown in Fig. 3. It can be seen that the maximum value of θ\nis equal to one-third of the bound. When θ is greater than this\nCHANG: REVERSIBLE LINGUISTIC STEGANOGRAPHY WITH BAYESIAN MASKED LANGUAGE MODELING 717\nAlgorithm 1 Encoding\nInput: x, v, m, countm\nOutput: x′, v, countm\n⊿ encoding\nv ← ∅\nif x < bound then\nif x <θ then ⊿ carrier zone\nx′ ← 2x + m[countm]\ncountm ← countm + 1\nelse ⊿ non-carrier zone\nx′ ← x + θ\nif (bound − θ) ≤ x′ < bound then ⊿ ambiguity zone\nv ← 0\nelse if x′ ≥ bound then ⊿ out of bound\nx′ ← x\nv ← 1\nelse ⊿ out of bound\nx′ ← x\nv. append(v) ⊿ update ﬂag list\nvalue, the carrier interval overlaps with the ambiguity interval\nand there is no point in embedding one message bit at the cost\nof recording one ﬂag bit.\nA practical coding method is presented as follows. Based\non the assumption about index frequency, we allocate a\nsmaller amount of distortion to a smaller carrier index when\nembedding a digit. Let m be a binary message digit to be\nembedded. If x is within θ, we encode x and m into a stego\nindex; otherwise, we shift x by θ\nx\n′ =\n{\n2x + m if x <θ\nx + θ otherwise. (1)\nIf the stego index is out of bound, we reset it to its original\nvalue and record the cases by a ﬂag bit\nv =\n{\n0i f (bound − θ) ≤ x′ < bound\n1i f x′ ≥ bound. (2)\nDecoding is operated in a ﬁrst-in last-out manner (i.e.\nin reverse order of encoding). In the decoding phase, a message\nbit is extracted by\nm =\n{\nmod\n(\nx′,2\n)\nif x′ < 2θ\n∅ otherwise (3)\na n da ni n d e xi sr e c o v e r e db y\nx =\n{\nﬂoor(x′/2) if x′ < 2θ\nx′ − θ otherwise. (4)\nIf x′ is in the ambiguity interval, we read a ﬂag bit to determine\nits original value. Pseudo-codes for the encoding and decoding\nprocedures are provided in Algorithms 1 and 2.\nIV . STEGANOGRAPHIC ROUTING\nSteganographic routing is the process of selecting a path\nfor embedding a payload. Differe nt paths can lead to different\nAlgorithm 2 Decoding\nInput: x′, mrev, v, countv\nOutput: x, mrev, countv\n⊿ decoding\nm ← ∅\nif x′ < bound then\nif x′ < 2θ then ⊿ carrier zone\nx ← ﬂoor(x′/2)\nm ← mod(x′,2)\nelse ⊿ non-carrier zone\nx ← x′ − θ\nif (bound − θ) ≤ x′ < bound then ⊿ ambiguity zone\nif v[countv]= 1 then\nx ← x′\ncountv ← countv − 1\nelse ⊿ out of bound\nx ← x′\nmrev. append(m) ⊿ update reversed message list\ntrade-off curves between capac ity and distortion. Routing is\nparticularly important in the case of a limited payload size\nbecause an optimal path can minimize distortion subject to\na given payload constraint. There are basically two types of\nrouting: static routing and dynamic routing. Static routing uses\na default or manually conﬁgured path pre-shared between the\nencoder and the decoder. It is easy to implement, but cannot\nminimize distortion. Dynamic routing, on the other hand,\nconstructs an adaptive path that reﬂects the degree of distortion\ncaused by modifying each word. Recall that our coding design\nintroduces a smaller degree of distortion to a smaller index\nand greater distortion to a larger index—that is to say, the\ndegree of distortion is inversely proportional to predictive\naccuracy. Therefore, optimal routing is to select a path in\ndescending order of predictive accuracy. While an optimal\npath is computable at the encoder, it cannot be reproduced\nat the decoder. The reason for this is that the word sequence\nused to derive the path is inconsistent with the word sequence\nreceived. A path is represented by a long sequence of digits,\nand storing such auxiliary information would be impractical.\nThe problem of designing a dynamic path that is computable\nfor both the encoder and the decoder is an intriguing one.\nThe most straightforward way to deal with textual data of\na sequential nature is sequential routing. However, stegano-\ngraphic distortion imposed upon the preceding words would\npropagate, thereby impairing predictive performance on suc-\nceeding words. In other words, the contextual clues from the\npast are distorted and only the clues from the future remain\nintact. Introducing randomness is conducive to mitigating error\npropagation. Words are randomly selected for carrying the\npayload, thereby reducing the chance of encountering distorted\ncontext words. Furthermore, a random seed for initializing a\npseudorandom number generator can serve as a secret key\nto enhance security. Nevertheless, the random variation on\nsequential routing is still a form of static routing and is not\nnecessarily optimal. As previously mentioned, the optimal path\nis associated with predictive accuracy. The encoder and the\n718 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 10, NO. 2, APRIL 2023\nFig. 4. Sequential and parallel r outings with and without randomness.\ndecoder cannot derive the same optimal path because predic-\ntive accuracy is self-depende nt—the accuracy of predicting a\ntarget word is related to the word itself. The target word must\nbe kept unchanged to derive the same quantity; however, it has\nto be changed to carry information. These two objectives are\nmutually incompatible.\nPredictive uncertainty is a concept closely related to pre-\ndictive accuracy and depends purely on the context words.\nWe can derive an alternative path in ascending order of uncer-\ntainty, assuming uncertainty is quantiﬁable. The synchronicity\nbetween the encoder and the decoder has to be ensured\nso that they can compute the same degree of uncertainty\nfor each target word. For that reason, the context words\nhave to be kept unchanged. This can be implemented by\nsampling target words at ﬁxed intervals of context words.\nFor instance, the context/target words are arranged in the\nfollowing manner: a target word, a segment of context words,\na target word, a segment of context words, and so forth.\nIf the intended payload is beyond the capacity offered by\nthe selected target words, we can select another set of target\nwords by simply shifting the intervals, resulting in multi-level\nmessage embedding. The maximum number of levels is equal\nto the length of the context segment plus one. For each\nlevel, we can construct an adaptive path in ascending order\nof uncertainty. We refer to this method as parallel routing in\nthe sense that a route is dynamically computed in each parallel\nlevel. Randomness can be introduced by randomly selecting\na context/target pattern for each level. A variety of routing\nmethods are illustrated in Fig. 4. Furthermore, we can set\nup an empirical threshold τ for ﬁltering out words of high\nuncertainty to improve performance. In other words, when\nthe prediction of a word is perceived to be highly uncertain,\nwe keep the word completely intact to minimize distortion.\nV. B\nAYESIAN UNCERTAINTY QUANTIFICATION\nMost deep learning models are deterministic functions\nwhich offer only predictions without uncertainty information.\nBayesian statistics offers a probabilistic interpretation of deep\nlearning models from which the underlying uncertainty can be\ncaptured [47]. For a given masked sequence s and a training\nset D, the predictive distribution of the masked word y is given\nby\np(y|s,D) =\n∫\np(y|s,/Phi1)\n \n \nlikelihood\np(/Phi1|D)d/Phi1\n \n \nposterior\n(5)\nwhere /Phi1denotes the model parameters. This can be inter-\npreted as the average predictio n over all plausible parameter\nsettings according to the parameter posterior. For deep learning\nmodels, the derivation of the parameter posterior is analyti-\ncally intractable; hence, we reso rt to variational inference to\napproximate the posterior by a variational distribution q(/Phi1),\nwhich belongs to a family of distributions of simpler form.\nBy substituting the parameter posterior with the variational\ndistribution and approximating the integral with Monte Carlo\nintegration, we derive that\np(y|s,D)\nVI\n≈\n∫\np(y|s,/Phi1)q(/Phi1)d/Phi1\nMC\n≈ 1\nT\nT∑\nt=1\np\n(\ny|s, ˆ/Phi1t\n)\n(6)\nwhere ˆ/Phi1t ∼ q(/Phi1). Sampling model parameters from a\nvariational distribution can be interpreted as dropout [48],\nwhich is a stochastic process of multiplying the output of\neach neurone by a random variable drawn from a Bernoulli\ndistribution. Each dropout conﬁguration corresponds to a\nplausible realization of a deep learning model with a portion\nof neurones deactiv ated. Applying T different dropout masks\nto the model is equivalent to performing stochastic forward\npasses for T repetitions, resulting in an ensemble of sparse\nneural network models. This process can be viewed as a proxy\nfor a probabilistic deep learning model and is referred to as\nMonte Carlo dropout [49].\nThe predictive distribution is derived by averaging the\nlikelihoods from T stochastic forward passes for each word\np(y = w\ni |s,D) ≈ 1\nT\nT∑\nt=1\np\n(\ny = wi |s, ˆ/Phi1t\n)\n(7)\nwhere wi denotes the ith word in the dictionary. For masked\nlanguage modeling, the likelihood is given by the normalized\nexponential function or softmax function\np\n(\ny = wi |s, ˆ/Phi1t\n)\n= softmax\n(\nfi\n(\ns, ˆ/Phi1t\n))\n= exp\n(\nfi\n(\ns, ˆ/Phi1t\n))\n∑\nj exp\n(\nf j\n(\ns, ˆ/Phi1t\n)) (8)\nwhere fi denotes the ith logit (i.e. raw prediction) from the\nmodel. In information theory, the uncertainty underlying a pre-\ndictive distribution can be measured by Shannon entropy [50]\nH =−\n∥ V ∥∑\ni=1\np(y = wi |s,D) ln p(y = wi |s,D). (9)\nCHANG: REVERSIBLE LINGUISTIC STEGANOGRAPHY WITH BAYESIAN MASKED LANGUAGE MODELING 719\nFig. 5. Bayesian uncertainty quan tiﬁcation via Monte Carlo dropout.\nEntropy is an encapsulation of information. It captures the\naverage amount of information in predictive distribution. It is\nmaximized when the predictive distribution is a uniform distri-\nbution; in other words, the model demonstrates the maximum\nuncertainty when each word is equally likely. It is minimized\nwhen only one word has a probability of 1 and all other words\nhave a probability of 0. An overview of Bayesian uncertainty\nquantiﬁcation is illustrated in Fig. 5.\nVI. E\nXPERIMENTS\nWe evaluate the proposed stego system with different set-\ntings of bound and threshold and compare different routings\nof interest. The trade-off between capacity, imperceptibility,\nand reversibility is examined with additional analysis on the\nsemantic and sentimental similarities between cover and stego\ntexts. Furthermore, a discussion on possible improvements is\nprovided.\nA. Experimental Setup\nOur cover text consists of 8 selected paragraphs from a work\nof classic English literature, Alice’s Adventure in Wonderland.\nThe text contains 711 words plus punctuation. Each letter is\nmade lower case. The BERT model has a vocabulary size of\n30 522 tokens. The number of context words on each side of\nthe target word (i.e. the length of the context segment) is set\nto 32 so that each input masked sub-sequence has 65 words.\nThe number of Monte Carlo dropout samples (i.e. stochastic\nforward passes) is set to 1000. The predictive accuracy of the\npre-trained BERT model can be represented by probability\ndistribution function (PDF) and cumulative distribution func-\ntion (CDF) of the word index, as shown in Fig. 6. It suggests\nFig. 6. Distribution of word index.\nthat more than 60% of words can be accurately predicted and\nabout 90% of words are among the top 25 predictions in the\ncase of no steganographic distortion.\nB. System Evaluation\nWe evaluate our steganographic system with different set-\ntings of bound and θ and analyze the trade-off between\ncapacity, imperceptibility, and reversibility. Capacity is mea-\nsured by the number of payload bits (absolute value) and\nthe payload bits per word (relative value). Imperceptibility is\nmeasured by the cosine similarity between the cover sequence\nand the stego sequence in the vector representation space.\nReversibility is not an all-or-nothing proposition. We quantify\nreversibility by the number of ﬂag bits used to disambiguate\ncolliding word indices (the lower the better). Figs. 7 and 8\nshow, respectively, the capacity–imperceptibility curves and\nthe capacity–reversibility curves from different routing meth-\nods. There is a general trend of decreasing imperceptibility and\nreversibility with increasing capacity. For the same threshold\n(θ = 1), a smaller bound preserves a better similarity because\nthere are fewer non-carrier words to be shifted. On the other\nhand, a smaller bound incurs more ﬂag bits because ambiguous\nindices, which are close to the bound, appear more frequently.\nWhen the threshold is raised to the maximum ( θ = bound/3),\nthe reachable capacity is increased. In addition, increased\nimperceptibility is obtained at the expense of lower reversibil-\nity. A comparison between parallel routing and sequential\nrouting conﬁrms the advantage of dynamic strategy over static\nstrategy. The parallel routing takes account of predictive uncer-\ntainty and constructs an adaptive path for message embedding.\nThe results also suggest that the random method has a positive\neffect on both the routing methods. Furthermore, when a few\ncarrier words are ﬁltered out by their uncertainty magnitudes\n(with a threshold τ), the reachable capac ity is reduced and\nyet greater imperceptibility and reversibility are achieved.\nA more advanced uncertainty anal yzer is expected to further\nimprove performance. We would also like to point out that the\ncurrent uncertainty analyzer requires computationally expen-\nsive Monte Carlo dropout. To facilitate real-time applications,\na more efﬁcient way to estimate uncertainty has yet to be\ndeveloped.\n720 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 10, NO. 2, APRIL 2023\nFig. 7. Capacity–imperceptibility curves of differen t routings w.r.t. various settings of bound and threshold.\nFig. 8. Capacity–reversibility curves of differen t routings w.r.t. various settings of bound and threshold.\nC. Semantics Analysis\nFigs. 9 and 10 display a part of the cover text and the\ncorresponding stego text, along with the word clouds for\nvisualizing the frequency distributions of cover words and\nstego words. The stego text is generated by the random parallel\nrouting with θ = 1, bound = 270 and τ = 1. We set the\ncapacity to 0.3 bits per word so that more than 200 payload\nbits are embedded. This number is arguably sufﬁcient for\nCHANG: REVERSIBLE LINGUISTIC STEGANOGRAPHY WITH BAYESIAN MASKED LANGUAGE MODELING 721\nFig. 9. Word cloud and sample paragraphs of cover text with highlighted\ncarrier words (in blue) and non-carrier words (in green).\nFig. 10. Word cloud and sample paragraph of stego text with highlighted\ncarrier words (in blue) and non-carrier words (in green).\n722 IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS, VOL. 10, NO. 2, APRIL 2023\nFig. 11. Sentiment analysis of cover and stego texts.\nmany authentication applications. Perfect reversibility is also\nguaranteed without any overhead information. We can observe\nthat the cover text and stego text are similar in terms of the\nsemantics and the frequency distribution of words. However,\ncloser inspection of the stego text shows that there are some\nunnatural word usages and grammatical mistakes. A possible\nreﬁnement may be made by ﬁltering out grammar words\nand named entities and manipulating content words only.\nFurthermore, a carefully designed word checker could be used\nto regulate the manipulations.\nD. Sentiment Analysis\nWe carry out a sentiment analysis on a stego text generated\nusing the aforementioned conﬁgurations. Fig. 11 reveals the\npositive/negative sentiment scores for each cover paragraph\nand each stego paragraph. The scores are obtained from a\ntransformer-based sentiment analyzer. It is observed that the\ncover text and the stego text have very similar sentiment pat-\nterns, suggesting that steganographic distortion only produces\nminimal ﬂuctuations in sentence sentiment. For particular\nsentiment-oriented applications, one may reﬁne the system\nby retaining some salient contributory words which have a\ndominant inﬂuence upon text sentiment.\nVII. C\nONCLUSION\nIn this work, we introduce a linguistic stego system with\nreversibility based on predictive word substitution. We use\na pre-trained masked language model to generate a list of\npredictive words and embed a message digit by replacing the\ntarget word with one of the predictive words. The underlying\nassumption of the reversible coding is that word indices\nfollow approximately an exponential distribution. We further\napply a theoretical framework of Bayesian deep learning to\nquantify the uncertainty in the masked language model and\nuse it to determine an adaptive route for message embedding.\nOur stego system achieves perfect reversibility without extra\nauxiliary information under limited capacity conditions. It also\nmaintains close vector space, and semantic an d sentimental\nsimilarities between cover and stego texts. Imperceptibility\nanalysis suggests that steganographic distortion is to some\nextent indiscernible in a computing sense. In reality, however,\neven an extra punctuation mark or unusual collocation may be\nnoted by a careful reader. Therefore, further improvement in\nimperceptibility is required. We also envisage further progress\nin uncertainty analysis such that the computational efﬁciency\nmeets real-time requirements. Furthermore, while the pro-\nposed stego system is based primarily on lexical substitution,\nsyntactic and generative methods also deserve investigation.\nWe hope this article can shed light on future research devoted\nto reversible linguistic steganography.\nR\nEFERENCES\n[1] R. L. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital\nsignatures and public-key cryptosystems,”Commun. ACM, vol. 21, no. 2,\npp. 120–126, 1978.\n[2] W. Bender, D. Gruhl, N. Morimot o, and A. Lu, “Techniques for data\nhiding,” IBM Syst. J., vol. 35, nos. 3–4, pp. 313–336, 1996.\n[3] R. J. Anderson and F. A. P. Petitcolas, “On the limits of steganography,”\nIEEE J. Sel. Areas Commun., vol. 16, no. 4, pp. 474–481, 1998.\n[4] F. A. P. Petitcolas, R. J. Anderson, and M. G. Kuhn, “Information\nhiding—A survey,” Proc. IEEE, vol. 87, no. 7, pp. 1062–1078, 1999.\n[5] J. Fridrich, M. Goljan, P. Lisonek, and D. Soukal, “Writing on wet\npaper,” IEEE Trans. Signal Process., vol. 53, no. 10, pp. 3923–3935,\n2005.\n[6] I. J. Cox, J. Kilian, F. T. Leighton, and T. Shamoon, “Secure spread\nspectrum watermarking for multimedia,” IEEE Trans. Image Process.,\nvol. 6, no. 12, pp. 1673–1687, 1997.\n[7] G. Depovere et al., “The VIV A project: Digital watermarking for\nbroadcast monitoring,” in Proc. Int. Conf. Image Process. (ICIP), Kobe,\nJapan, 1999, pp. 202–205.\n[8] S. He and M. Wu, “Joint coding and embedding techniques for multi-\nmedia ﬁngerprinting,” IEEE Trans. Inf. Forensics Secur., vol. 1, no. 2,\npp. 231–247, 2006.\n[9] C. Szegedy et al., “Intriguing properties of neural networks,” in Proc.\nInt. Conf. Learn. Represent. (ICLR), Banff, AB, Canada, 2014, pp. 1–10.\n[10] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing\nadversarial examples,” in Proc. Int. Conf. Learn. Represent. (ICLR),\nSan Diego, CA, USA, 2015, pp. 1–11.\n[11] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-\nsal adversarial perturbations,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Honolulu, HI, USA, 2017, pp. 86–94.\n[12] J. Fridrich, M. Goljan, and R. Du, “Invertible authentication,” Proc.\nSPIE, vol. 4314, pp. 197–208, 2001.\n[13] C. De Vleeschouwer, J.-F. Delaigle , and B. Macq, “Circular interpre-\ntation of bijective transformations in lossless watermarking for media\nasset management,” IEEE Trans. Multimedia, vol. 5, no. 1, pp. 97–105,\n2003.\n[14] M. U. Celik, G. Sharma, A. M. Tekalp, and E. Saber, “Lossless\ngeneralized-LSB data embedding,” IEEE Trans. Image Process., vol. 14,\nno. 2, pp. 253–266, 2005.\n[15] D. M. Thodi and J. J. Rodriguez, “E xpansion embedding techniques for\nreversible watermarking,” IEEE Trans. Image Process., vol. 16, no. 3,\npp. 721–730, 2007.\n[16] S. Lee, C. D. Yoo, and T. Kalker, “Reversible image watermarking based\non integer-to-integer wavelet transform,” IEEE Trans. Inf. Forensics\nSecur., vol. 2, no. 3, pp. 321–330, 2007.\n[17] V . Sachnev, H. J. Kim, J. Nam, S. Suresh, and Y . Q. Shi, “Reversible\nwatermarking algorithm using sorting and prediction,” IEEE Trans.\nCircuits Syst. Video Technol., vol. 19, no. 7, pp. 989–999, 2009.\n[18] C. Qin, C.-C. Chang, Y .-H. Hua ng, and L.-T. Liao, “An inpainting-\nassisted reversible steganographic scheme using a histogram shifting\nmechanism,” IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 7,\npp. 1109–1118, 2013.\n[19] C.-C. Chang, “Adversarial learning for invertible steganography,” IEEE\nAccess, vol. 8, pp. 198425–198435, 2020.\n[20] S. H. Low, N. F. Maxemchuk, J. T. Brassil, and L. O’Gorman, “Docu-\nment marking and identiﬁcation using both line and word shifting,” in\nProc. IEEE Conf. Compt. Commun. (INFOCOM), vol. 2, Boston, MA,\nUSA, 1995, pp. 853–860.\n[21] S. H. Low, N. F. Maxemchuk, and A. M. Lapone, “Document identi-\nﬁcation for copyright protection using centroid detection,” IEEE Trans.\nCommun., vol. 46, no. 3, pp. 372–383, 1998.\nCHANG: REVERSIBLE LINGUISTIC STEGANOGRAPHY WITH BAYESIAN MASKED LANGUAGE MODELING 723\n[22] J. T. Brassil, S. H. Low, and N. F. Maxemchuk, “Copyright protection\nfor the electronic distribution of text documents,” Proc. IEEE, vol. 87,\nno. 7, pp. 1181–1196, 1999.\n[23] L. Y . Por, T. F. Ang, and B. Delina, “WhiteSteg: A new scheme in\ninformation hiding using text steganography,” WSEAS Trans. Comp.,\nvol. 7, no. 6, pp. 735–745, 2008.\n[24] C. Xiao, C. Zhang, and C. Zheng, “F ontCode: Embedding information in\ntext documents using glyph perturbation,” ACM Trans. Graph., vol. 37,\nno. 2, pp. 1–16, 2018.\n[25] K. Winstein, “Lexical steganography through adaptive modulation of the\nword choice hash,” unpublished.\n[26] I. A. Bolshakov, “A method of li nguistic steganography based on\ncollocationally-veriﬁed synonymy,” in Proc. Int. Workshop Inf. Hiding\n(IH), Toronto, ON, Canada, 2005, pp. 180–191.\n[27] C.-Y . Chang and S. Clark, “Prac tical linguistic steganography using\ncontextual synonym substitution and a novel vertex coding method,”\nComput. Linguistics, vol. 40, no. 2, pp. 403–448, 2014.\n[28] M. Topkara, U. Topkara, and M. J. Atallah, “Words are not enough:\nSentence level natural language watermarking,” in Proc. ACM Int.\nWorkshop Contents Protection Secur. (MCPS), Santa Barbara, CA, USA,\n2006, pp. 37–46.\n[29] B. Murphy and C. V ogel, “The syntax of concealment: Reliable methods\nfor plain text information hiding,” Proc. SPIE, vol. 6505, pp. 351–362,\n2007.\n[30] H. M. Meral, B. Sankur, A. S. Özs oy, T. Göngör, and E. Sevinç, “Nat-\nural language watermarking vi a morphosyntactic alterations,” Comput.\nSpeech Lang., vol. 23, no. 1, pp. 107–125, 2009.\n[31] C.-Y . Chang and S. Clark, “The secret’s in the word order: Text-to-text\ngeneration for linguistic steganography,” in Proc. Int. Conf. Comput.\nLinguistic (COLING), Mumbai, India, 2012, pp. 511–528.\n[32] C. Grothoff, K. Grothoff, L. Alkhut ova, R. Stutsman, and M. Atallah,\n“Translation-based steganography,” in Proc. Int. Workshop Inf. Hiding\n(IH), Barcelona, Spain, 2005, pp. 219–233.\n[33] R. Stutsman, C. Grothoff, M. Ata llah, and K. Grothoff, “Lost in just the\ntranslation,” in Proc. ACM Symp. Appl. Comput. (SAC), Dijon, France,\n2006, pp. 338–345.\n[34] P. Meng, L. Hang, Z. Chen, Y . Hu, and W. Yang, “STBS: A sta-\ntistical algorithm for steganalysis of translation-based steganography,”\nin Proc. Int. Workshop Inf. Hiding (IH), Calgary, AB, Canada, 2010,\npp. 208–220.\n[35] C.-Y . Chang and S. Clark, “Adject ive deletion for linguistic steganog-\nraphy and secret sharing,” in Proc. Int. Conf. Comput. Linguistic\n(COLING), Mumbai, India, 2012, pp. 493–510.\n[36] P. Wayner, “Strong theoretical stegnography,”Cryptologia, vol. 19, no. 3,\npp. 285–299, 1995.\n[37] M. Chapman and G. I. Davida, “Hi ding the hidden: A software system\nfor concealing ciphertext as innocuous text,” in Proc. Int. Conf. Inf.\nCommun. Secur. (ICICS), Beijing, China, 1997, pp. 335–345.\n[38] Z. Yang, X. Guo, Z. Chen, Y . Huang, and Y . Zhang, “RNN-Stega:\nLinguistic steganography based on recurrent neural networks,” IEEE\nTrans. Inf. Forensics Secur., vol. 14, no. 5, pp. 1280–1295, 2019.\n[39] Z. Yang, S. Zhang, Y . Hu, Z. Hu, and Y . Huang, “V AE-Stega: Linguistic\nsteganography based on var iational auto-encoder,” IEEE Trans. Inf.\nForensics Secur., vol. 16, pp. 880–895, 2021.\n[40] S. Zhang, Z. Yang, J. Yang, and Y . Huang, “Linguistic steganography:\nFrom symbolic space to semantic space,” IEEE Signal Process. Lett.,\nvol. 28, pp. 11–15, 2021.\n[41] Z. S. Harris, “Distributional structure,” Word, vol. 10, nos. 2–3,\npp. 146–162, 1954.\n[42] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation\nof word representations in vector space,” in Proc. Int. Conf. Learn.\nRepresent. (ICLR), Scottsdale, AZ, USA, 2013, pp. 1–12.\n[43] J. Pennington, R. Socher, and C. D. Manning, “GloVe: Global vectors for\nword representation,” in Proc. Conf. Empirical Methods Natural Lang.\nLinguistics (EMNLP), 2014, pp. 1532–1543.\n[44] M. E. Peters et al., “Deep contextualized word representations,” in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics (NAACL),N e w\nOrleans, LA, USA, 2018, pp. 2227–2237.\n[45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transform ers for language understanding,” in\nProc. Conf. North Amer. Chapter Assoc. Comput. Linguistics (NAACL),\nMinneapolis, MN, USA, 2019, pp. 4171–4186.\n[46] A. Vaswani et al., “Attention is all you need,” in Proc. Annu. Conf.\nNeural Inf. Process. Syst. (NeurIPS), vol. 30, Long Beach, CA, USA,\n2017, pp. 1–11.\n[47] Y . Gal, “Uncertainty in deep lear ning,” Ph.D. dissert ation, Dept. Eng.,\nUniv. Cambridge, Cambridge, U.K., 2016.\n[48] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural networks\nfrom overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 56, pp. 1929–1958,\n2014.\n[49] Y . Gal and Z. Ghahramani, “Dropout as a Bayesian approximation:\nRepresenting model uncertainty in deep learning,” in Proc. Int. Conf.\nMach. Learn. (ICML), New York, NY , USA, 2016, pp. 1050–1059.\n[50] C. E. Shannon, “A mathematical theory of communication,” Bell Syst.\nTech. J., vol. 27, no. 3, pp. 379–423, 1948.\nChing-Chun Chang received the Ph.D. degree\nin computer science from the University of War-\nwick, Coventry, U.K., in 2019. He participated\nin a short-term scientiﬁc mission supported by\nEuropean Cooperation in Science and Technology\nActions with the Faculty of Computer Science,\nOtto von Guericke University Magdeburg, Germany,\nin 2016. He was granted the Marie-Curie Fellowship\nand participated in a research and innovation staff\nexchange scheme supported by Marie Skłodowska-\nCurie Actions with the Department of Electrical and\nComputer Engineering, New Jersey Institute of Technology, USA, in 2017.\nHe was a Visiting Scholar with the School of Computer and Mathematics,\nCharles Sturt University, Australia, in 2018, and the School of Information\nTechnology, Deakin University, Australia, in 2019. He was a Research\nFellow with the Department of Electroni c Engineering, Tsinghua University,\nChina, in 2020. He has been a Post-Doctoral Fellow with the National\nInstitute of Informatics, Japan, since 2021. His research interests include\nsteganography, watermarking, forensic s, biometrics, cybersecurity, applied\ncryptography, image processing, computer vision, natural language processing,\ncomputational linguistics, machine l earning, and artiﬁcial intelligence.",
  "topic": "Steganography",
  "concepts": [
    {
      "name": "Steganography",
      "score": 0.91338050365448
    },
    {
      "name": "Computer science",
      "score": 0.7615504264831543
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5958859920501709
    },
    {
      "name": "Steganography tools",
      "score": 0.5131693482398987
    },
    {
      "name": "Authentication (law)",
      "score": 0.5120880603790283
    },
    {
      "name": "Natural language processing",
      "score": 0.4926682114601135
    },
    {
      "name": "Distortion (music)",
      "score": 0.4773528575897217
    },
    {
      "name": "Cryptography",
      "score": 0.4441896677017212
    },
    {
      "name": "Digital signature",
      "score": 0.4437007009983063
    },
    {
      "name": "Natural language",
      "score": 0.4245308041572571
    },
    {
      "name": "Information hiding",
      "score": 0.41247886419296265
    },
    {
      "name": "Embedding",
      "score": 0.3590945601463318
    },
    {
      "name": "Computer security",
      "score": 0.25584810972213745
    },
    {
      "name": "Amplifier",
      "score": 0.0
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.0
    },
    {
      "name": "Hash function",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39555362",
      "name": "University of Warwick",
      "country": "GB"
    }
  ],
  "cited_by": 16
}