{
  "title": "Breaking the Silence: the Threats of Using LLMs in Software Engineering",
  "url": "https://openalex.org/W4389768548",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5040439036",
      "name": "June Sallou",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5043941055",
      "name": "Thomas Durieux",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067127346",
      "name": "Annibale Panichella",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4298384852",
    "https://openalex.org/W4362598291",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384345708",
    "https://openalex.org/W2945440850",
    "https://openalex.org/W4384026634",
    "https://openalex.org/W4389520710",
    "https://openalex.org/W4383175397",
    "https://openalex.org/W4307479317",
    "https://openalex.org/W1599808047",
    "https://openalex.org/W4385270018",
    "https://openalex.org/W4384345745",
    "https://openalex.org/W4384024738",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W4288080275",
    "https://openalex.org/W3199249334",
    "https://openalex.org/W4379743664",
    "https://openalex.org/W4220722393",
    "https://openalex.org/W4310881984",
    "https://openalex.org/W4386302809",
    "https://openalex.org/W4402042086",
    "https://openalex.org/W4385187421",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W1526710119",
    "https://openalex.org/W4384304865",
    "https://openalex.org/W3130178918",
    "https://openalex.org/W4287668913",
    "https://openalex.org/W4377163996",
    "https://openalex.org/W4312632714",
    "https://openalex.org/W4386982649",
    "https://openalex.org/W1971650562",
    "https://openalex.org/W2156723666",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W3109966548",
    "https://openalex.org/W4367694420",
    "https://openalex.org/W4367059011",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4381687070",
    "https://openalex.org/W3177813494"
  ],
  "abstract": "Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.",
  "full_text": "Breaking the Silence: the Threats of Using LLMs in Software\nEngineering\nJune Sallou\nTU Delft\nThe Netherlands\nJ.Sallou@tudelft.nl\nThomas Durieux\nTU Delft\nThe Netherlands\nthomas@durieux.me\nAnnibale Panichella\nTU Delft\nThe Netherlands\nA.Panichella@tudelft.nl\nAbstract\nLarge Language Models (LLMs) have gained considerable traction\nwithin the Software Engineering (SE) community, impacting vari-\nous SE tasks from code completion to test generation, from program\nrepair to code summarization. Despite their promise, researchers\nmust still be careful as numerous intricate factors can influence the\noutcomes of experiments involving LLMs. This paper initiates an\nopen discussion on potential threats to the validity of LLM-based\nresearch including issues such as closed-source models, possible\ndata leakage between LLM training data and research evaluation,\nand the reproducibility of LLM-based findings. In response, this\npaper proposes a set of guidelines tailored for SE researchers and\nLanguage Model (LM) providers to mitigate these concerns. The\nimplications of the guidelines are illustrated using existing good\npractices followed by LLM providers and a practical example for\nSE researchers in the context of test case generation.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚Üí Empirical software val-\nidation; ‚Ä¢ Computing methodologies ‚Üí Machine learning ; ‚Ä¢\nGeneral and reference ‚ÜíEvaluation.\nACM Reference Format:\nJune Sallou, Thomas Durieux, and Annibale Panichella. 2024. Breaking the\nSilence: the Threats of Using LLMs in Software Engineering. In New Ideas\nand Emerging Results (ICSE-NIER‚Äô24), April 14‚Äì20, 2024, Lisbon, Portugal.\nACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3639476.3639764\n1 Introduction\nIn recent years, the utilization of Large Language Models (LLMs) has\ngained substantial traction within the Software Engineering (SE)\ncommunity. Equipped with language understanding and generation\ncapabilities, these models have impacted various SE research and\npractice aspects. From code generation to bug detection and natural\nlanguage interactions with codebases, LLMs have played a pivotal\nrole in recent SE advancements [20, 26, 29, 34, 41].\nDespite their promise, researchers must tread cautiously when\nmaking claims about the effectiveness of their approaches. The\noutcomes of experiments involving LLMs can be influenced by\nnumerous intricate factors that can be challenging to discern or\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICSE-NIER‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\n¬© 2024 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0500-7/24/04.\nhttps://doi.org/10.1145/3639476.3639764\nFigure 1: ChatGPT‚Äôs detailed answers about a specific bug in\nDefects4J.\ncontrol. This intricacy underscores the need to thoroughly examine\nthe validity of research findings when LLMs are involved.\nFor instance, consider the evaluation of LLMs using well-known\nprojects such as those included in Defects4J [24]. Although ChatGPT-\n3.5, as a general-purpose chatbot, is not fine-tuned for specific\nSE tasks, it possesses precise knowledge of the bugs within these\nprojects (see Figure 1). Thus, when OpenAI models are employed\nfor tasks like patch generation [42], fault localization [40], or test\ngeneration [35] for Defects4J, they have superior performance com-\npared to an unknown code [34]. This foreknowledge arises from the\npre-trained process, as ChatGPT has been trained on a large variety\nof datasets, including scientific papers (though the specifics are not\nfully disclosed). This raises severe concerns about the threats to\nboth construct (training and evaluating on the same dataset ) and\nexternal (do the results hold for unknown projects/code? ) validity.\nThis paper aims to initiate a community-wide discussion and\nraise awareness of these issues to facilitate collective progress.\nSpecifically, we focus on three key threats to validity: 1 Using\nclosed-source LLMs and their implications w.r.t. data privacy and\nthe models‚Äô evolution unpredictability. 2 The blurry separation\nbetween training, validation, and test sets and the corresponding\npotential explicit/implicit data leakage. 3 Reproducibility of the\npublished research outcomes over time, due to the non-stochastic\nnature of LLMs answers, the non-transparent releases ofnew model\nversions, and the lack of complete traceability.\nWhile recent papers acknowledge some of these concerns (e.g., [23,\n34, 44]), we highlight the necessity for further empirical method-\nologies‚Äîsuch as code obfuscation, multiple independent prompts\nor queries, and metadata provision‚Äîto alleviate and address these\nconcerns. Therefore, we present an initial set of guidelines aimed\nat mitigating these threats, specifically targeting SE researchers\nand Language Model (LM) providers. While we provide a list of\nactionable suggestions, we emphasize the importance of the wide\narXiv:2312.08055v2  [cs.SE]  8 Jan 2024\nICSE-NIER‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Sallou, et al.\nSE community to follow our initial steps and build a more compre-\nhensive list of threats to validity and research methods to address\nthem, ultimately advancing the field while ensuring the reliability\nand validity of LLM-based research contributions.\nTo illustrate the practical application of our guidelines, we demon-\nstrate their implementation in the context of test case generation ‚Äîa\nwell-established SE task [21, 30]‚Äî using ChatGPT 3.5 on two buggy\ncode snippets from Defects4J [ 24]. We provide a full replication\npackage [13] including (1) the implementation of our guidelines,\nand (2) the results of our analysis. Finally, we highlight existing\ngood practices from LLMs providers that align with our guidelines.\n2 Threats to Validity for LLMs\nThis section opens the discussion related to LLM-based research\nwithin the SE research community.\n2.1 Closed-Source Models\nA significant portion of the LLMs community heavily relies on\nclosed-source models, as evidenced by the prevalence of ChatGPT-\nrelated papers at this year‚Äôs ICSE conference. This dependence\nstems from factors such as the models‚Äô effectiveness, availability,\nand cost-effectiveness. To put this into perspective, deploying an\nopen-source LLM model comparable to ChatGPT, like the Falcon\n180B model, would incur a monthly cost of $29,495 on AWS.1\nHowever, the utilization of closed-source models introduces sig-\nnificant threats to the validity of the research approach.\nModel Evolution Unpredictability. One primary concern\narises from the lack of control over the evolution of these closed-\nsource models. New models may be released into production [33],\nand notable changes in the output of OpenAI models have been\nobserved [14]. Such changes can occur during or after the research\napproach has been presented, potentially making the presented\nresults obsolete. Moreover, this concern is also particularly pro-\nnounced for incremental works that use LLM. Indeed, distinguish-\ning whether the improvements claimed in the new contribution are\nthe result of changes to the LLMs‚Äô models or due to the novelty of\nthe contribution becomes a complex task.\nPrivacy Implications. Another significant aspect of concern\nis privacy. Closed-source models often lack transparency, making\nit difficult to assess the privacy implications associated with their\nusage [5] as well as potential copyright infringements.\n2.2 Implicit Data Leakage\nLLMs are trained on vast textual datasets such as Wikipedia, GitHub,\nand StackOverflow [15, 36] and derive their understanding of se-\nmantics and contextual word relationships from this diverse data.\nThese models contain millions (e.g., BERT) to billions (e.g., ChatGPT-\n4 and LLaMA) of parameters, which undergo a series of iterative\noptimizations during pre-training that minimize the loss function.\nData leakage due to pre-training . Pre-training in unsuper-\nvised or semi-supervised learning does not tailor models for specific\nsoftware engineering tasks they will be later evaluated on after\nparameter re-tuning. Nonetheless, questions can be raised whether\nLLMs potentially memorize existing code samples used for evalu-\nation, instead of generating new, unseen code [22]. For instance,\nprior studies [ 11, 23, 25, 32] highlighted vulnerabilities in code\n1Visited on 7-Dec-2023, 8x A100 80GB on https://aws.amazon.com/ec2/instance-types/\ngenerated by Codex [15], originating from its training set. Siddiq\net al. [ 34] investigated the performances of three LLMs (Codex,\nChatGPT3.5 [28], and StarCoder [27]) in generating unit tests for\nJava programs. They reported remarkable performance discrep-\nancies between the HumanEval [12] (>69% branch coverage) and\nthe SF110 [21] (2% branch coverage) datasets. We remark that the\nformer is available on GitHub [12] while the latter is not (available\non SourceForge instead).\nAdditionally, previous studies in metamorphic testing [8, 17, 43,\n45] have demonstrated how semantic-preserving changes to code\nsnippets can effectively deceive LLMs. These approaches create\nnew data points that differ from the original ones by at least one\nmetamorphic change, increasing the likelihood that LLMs will not\nrecognize the code snippets seen during pre-training.\nData leakage due to fine-tuning . LMs applied to specific SE\ntasks require parameter tuning via supervised learning, adjusting\nparameters using labeled datasets specific to the task. Despite ef-\nforts to separate training, validation, and test sets, ensuring clear\ndistinctions is not guaranteed. In practice, different projects might\nhave common dependencies and use the same pre-defined APIs.\nFor instance, within the Java ecosystem, numerous libraries/tools\noften rely on common dependencies like Log4j, Apache Commons,\nSpring, GSon, and others. Consequently, a scenario of data leakage\ncan arise if the LLM is trained on ‚Äúproject A‚Äù that employs a specific\nAPI, and the resulting model is subsequently used to fix the usage\nof the same API in another project within the test set.\n2.3 Reproducibility\nSeveral concerns can be raised w.r.t the reproducibility of the LLMs\noutputs. The ability to obtain identical results following the same\nprocedure by external parties is proven to be challenging.\nOutput Variability. LLMs exhibit variability in their outputs,\neven when using identical input. Running the same prompt several\ntimes may not result in identical output, rendering the usage of\nLLMs non-deterministic. Examples of such phenomena have been\ndescribed in the literature, including other application domains,\nsuch as the medical domain [ 19]. We also experimented with an\nexample in software engineering involving a code generation task.\nIn this use case, we demonstrate that running a prompt to generate\nPython code multiple times results in different responses from the\nGPT-3.5 model. To avoid data contamination, we get inspiration\nfrom the methodology outlined by Chen et al. [14]. We use a coding\nchallenge from the LeetCode [2] platform as the prompt, employing\nthe same prompt twice in two separate sessions2 on the same day\n(September 12th, 2023). During these sessions (whose chat links\nare provided as footnotes), we observe distinct codes generated\nby GPT-3.5, with varying function operations reasoning, variable\nnames, and initialization values or expressions.\nTime-Based Output Drift. Furthermore, there is no assurance\nthat the results will remain consistent over time. As discussed in\nSection 2.1, many LLMs are closed-source, and there are no estab-\nlished practices akin to regression testing to account for output\nvariability. Running the same prompt at a later time (e.g., days or\nmonths) may lead to a drift in the outcome due to potential retrain-\ning between sessions, reinforcement learning between sessions, or\n2first session: https://chat.openai.com/share/a0c7ef5c-74ce-466b-a1d5-5f44e03a626d,\nsecond session:https://chat.openai.com/share/6566acff-12eb-470a-a043-3e2294cf6406\nBreaking the Silence: the Threats of Using LLMs in Software Engineering ICSE-NIER‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\nadjustments based on user feedback. Chen et al. [14] explored this\ntime-based output drift in terms of accuracy for two versions of\nGPT models over a three-month interval. They show that, over a\nrange of tasks and application domains, the overall accuracy of out-\nputs decreases, accompanied by a remarkable mismatch in answers.\nIn particular, for code generation, the mismatch is evaluated at 20%\nfor GPT-3.5 and 50% for GPT-4 between March 2023 and June 2023.\nMoreover, the number of executable outputs drops from 52% to 10%\nfor GPT-3.5, and from 22% to 2% for GPT-4.0 for the same period.\nTraceability. Another critical concern associated with the wide-\nspread adoption of LLMs is the lack of traceability. Currently, con-\nnecting the output of LLMs to specific prompts, along with ‚Äôcon-\nfiguration‚Äô details such as the version of the used LLM, the date of\nthe query, and other specifications, can be a challenging task.\n3 Discussion and Guidelines\nIn this section, we present initial guidelines and methodologies\naddressing the mentioned threats. While we provide a list of action-\nable suggestions as opening steps, we encourage the SE community\nto work toward establishing standards and expectations at the same\nlevel as those commonly applied with traditional AI techniques. We\norganize the guidelines according to the actors they involve: the\nLLM providers, and the SE researchers using LLMs.\n3.1 Guidelines for LLM Providers\n3.1.1 Closed-Source Models We foresee two main strategies to\naddress this category of threats to validity:\nEnhance model transparency. LLM providers should priori-\ntize transparency by furnishing comprehensive information about\ntheir models. This should encompass details on the model‚Äôs creation\nprocess and the methodology used for data selection during train-\ning. Furthermore, providers should share statistics and data-point\ninformation to shed light on the model‚Äôs training dataset. Ideally, an\nAPI service could be established, enabling users to verify if a partic-\nular data source was included in the model‚Äôs training or validation\ndatasets. Such a service would not only enhance transparency but\ncould also serve privacy and copyright verification purposes.\nUse versioning information. Providers should provide their\nmodel version, and they should adopt a versioning nomenclature\nthat distinguishes major revisions from minor updates. This enables\nusers to discern the significance of model version changes.\n3.1.2 Data Leakage In light of concerns regarding closed-source\nmodels, LLM providers should provide services that allow researchers\nto verify which projects and sources were considered during pre-\ntraining. A positive example is CodeBERT, whose provides do\nnot disclose pre-training code but enable verification of included\nprojects for pre-training through the train split data.3\n3.1.3 Reproducibility We propose two methods to address this:Use\na fixed random seed. LLM providers should ensure the inclusion\nof a settable random seed during the inference of LLMs, render\nthe inference deterministic for each specific case. This practice\nwould help address the variability of output concerning traceability\nand reproducibility. In the case of closed-source LLMs, a dedicated\nAPI could be made accessible, allowing the user to set the seed\n3https://huggingface.co/datasets/code_search_net\nwithout requiring access to the entire model. Toward this direction,\nOpenAPI has recently released a beta feature 4 that allows users to\nset fixed seeds during prompting, although deterministic answer is\nnot fully guaranteed due to different back-end settings.\nUse an archiving system. In addition to the Versioning Infor-\nmation, we advocate for the usage of a general archiving system, to\nensure that external parties can reproduce the observations made\nby LLMs. It should be noted that some efforts are already being\nmade in that direction. We can cite the HuggingFace platform [1],\nwhich provides pre-trained models for download with information\nabout the model training, file versioning, and datasets. Zenodo [3]\nis another example of storing and making versioned models and\ndatasets accessible and reusable, which is commonly used in the\nresearch community. However, the use of such platforms is not\nyet a regular and consistent practice. Moreover, a dedicated LLM\nplatform is still missing, as LLM sizes are generally large, posing\nchallenges for downloading or uploading\n3.2 Guidelines for SE Researchers\nThis section outlines guidelines for SE researchers. Along with pre-\nsenting guidelines, we exhibit their practical applicability through\na showcase example, using ChatGPT3.5 to generate JUnit test cases\nfor two buggy programs in the Defect4j dataset [24], Chart-11 and\nMath-5. The prompts with the collected answers, data analysis, and\nmetadata are available in our replication package [13].\n3.2.1 Reproducibility To address the threats to the reproducibility\nof LLMs-based approaches, we proposed the following guidelines:\nAssess output variability. Due to output variability, running\nthe LLMs‚Äô inference only once is insufficient to ensure reproducibil-\nity. Therefore, we argue in favor of conducting multiple replication\nruns and using variability metrics during the evaluation. For our\nshowcase example, we queried ChatGPT3.5 ten times over different\ndays using the same prompts (see our replication package) and\ntargeting only the buggy methods (i.e., no the entire classes). We\nthen analyzed the resulting branch coverage and test execution\nresults. For Math-5, we report an average branch coverage of 70%\nfor the tested Java method with a large variability (interquartile\nrange or IQR) of 27.5%. We also observe variability in the number\nof generated tests (between 5 and 7) and number of failing tests\n(between 1 and 2). ForChart-11, the generated tests achieve 71% of\nbranch coverage for the tested Java method, with 20% IQR. ChatGPT\nalso generates between 1 and 5 failing tests for this method.\nProvide execution metadata. Associated with the LLMs infer-\nence results, we argue that relevant additional data should be made\naccessible and considered during the evaluation of LLMs. Such in-\nformation includes, but is not limited to: (i) Model information: To\nreproduce the LLMs‚Äô results and evaluation, information concern-\ning the model is necessary (e.g., version, seed, etc.). Furthermore,\nthe model itself should be accessible to enable its use, at least in\na black box manner. (ii) Prompts: The exact inputs (queries) used\nfor the inference and evaluation of the LLMs. (iii) Date of LLMs\nquery: The date is relevant data to share to address the time-based\noutput drift (that can happen because of retraining of models, or re-\ninforcement learning from past human feedback and interactions).\n(iv) Output variability metrics and associated assessment package:\n4https://platform.openai.com/docs/guides/text-generation/reproducible-outputs\nICSE-NIER‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal Sallou, et al.\nInformation concerning the assessment of output variability would\nenable the user to understand the risk regarding consistency in\nusing the LLM in question. Providing the package containing the\nprompts and results used during this evaluation would help to\nensure reproducibility. (v) Scope of reproducibility: To ensure the\ntrusted and controlled usage of LLMs, information about the scope\nin which the model has been trained or assessed should be disclosed,\nincluding the application domains and studied use cases.\nWe provide an example of metadata (written using the JSON\nformat) for the showcase of this paper in our replication material.\n3.2.2 Data Leakage A few recommendations can be made to tackle\nthe crucial concerns about the potential data leakage:\nAssess LLMs on metamorphic data . Metamorphic testing is\nactive research for the model robustness [8, 17, 45]. Metamorphic\ntesting generates new data samples (code) by applying metamorphic\ntransformations to the validation or test sets. These new snippets\nmaintain semantic and behavioral equivalence with the original\ncode, yet exhibit structural differences (e.g., distinct Abstract Syntax\nTrees). Prior studies have shown that CodeBERT and code2vec are\nnot robust, i.e., they produce different (worse) results when obfus-\ncating the variable names [17], introducing unused variables [45],\nreplacing tokens with plausible alternatives [16], and wrapping-up\nexpressions in identity-lambda functions [8, 9].\nTherefore, we advise researchers to complement the analysis\nof the LLMs performance with new data samples generated with\nmetamorphic testing. The selection of metamorphic transforma-\ntions should align with the specific task at hand. For example, code\nobfuscation (for method/class names) should not hinder the ability\nof LLMs to generate unit tests or patches successfully. Instead, iden-\ntifier names are crucial for NLP-related tasks (e.g., method name\nprediction), and other metamorphic transformations should be ap-\nplied (e.g., wrapping up expressions in identity-lambda functions).\nTo show the practicability of this guideline, we have applied\ncode metamorphic transformations to the Java methods of Math-5\nand Chart-11. In particular, we (1) removed the javadoc and (2)\nrenamed the method under test and its input parameters. For re-\nnaming, we did not use randomly generated string but opted for\nsynonyms and English words (e.g., changing the method name\nreciprocal() in complementary() for Math-5) to maintain the\nnaturalness of the transformed code [43]. While we do not observe\na significant difference in terms of branch coverage achieved for\nChart-11 (ùëù-value=0.79 according to the Wilcoxon test) between\nthe original and transformed code, we report a small negative ef-\nfect size ( ÀÜùê¥12=0.63) w.r.t. the number of failing tests (larger for the\ntransformed code). The difference we obtained forMath-5 on the ob-\nfuscated code is much more significant. While ChatGPT constantly\ngenerated tests for the original program with a branch coverage\nof 71%, it struggled to generate any meaningful tests on the trans-\nformed code. In particular, ChatGPT always created non-compiling\ntests with clear examples of hallucination [34, 46], i.e., invoking\nmethods/constructors that were never included in the prompts.\nUse different sources . As shown by Siddiq et al. [ 34], LLMs\nachieve much worse results on projects from SourceForge com-\npared to GitHub. Hence, we recommend researchers gather soft-\nware projects and data from multiple sources.\nCode clone detection . Given the current low transparency\nof closed-source LLMs, tracing the projects used for pre-training\nis challenging, if not impossible. However, researchers can use\nwell-established code clone detection techniques [4] to check if the\ngenerated code (e.g., test cases) is similar to code seen in online\nrepositories (e.g., manually-written test cases).\nCheck for common dependencies . To prevent implicit data\nleakage between training, evaluation, and test sets (for task-specific\nevaluation), researchers should (1) cross-compare the external de-\npendencies between projects belonging to different sets, (2) use\ncode cloning techniques to assess whether similar code (e.g., API\nuses) appear between different projects from the different sets.\n3.2.3 Closed-Source Models Perform comparative analysis. Re-\nsearchers are encouraged to run experiments using both open-\nsource and closed-source LLMs. This comparative approach can\nprovide additional insights into the strengths and limitations of\neach. Notably, the open-source LLM community has witnessed an\nexpansion in availability, with models like llama2 [37] and Falcon\n180B [7] emerging as viable options. llama2 models, in particular,\noffer the advantage of running on consumer-grade devices.\nFramework Facilitation. To streamline the evaluation process\nacross multiple models, researchers can leverage frameworks such\nas ONNX5. ONNX simplifies the transition between various models,\nenhancing the efficiency and consistency of experimentation.\n4 Conclusion and Future Work\nIn this article, we have initiated a discussion about the usage of\nLLMs in SE contributions, along with the challenges and threats to\nvalidity they bring. We have identified three primary challenges:\nthe reliance on closed-source LLMs, potential data leakages, and\nconcerns about reproducibility. To mitigate these, we propose an\ninitial set of guidelines. We aim to encourage an ongoing dialogue\nwithin the community to navigate these challenges effectively.\nIt is essential to continue reflecting and staying critical about\nthe usage of LLMs in SE. We must collectively define guidelines\nand expectations within our community. We believe the conversa-\ntion should be spread by organizing panels with different experts\nand stakeholders. We should also monitor the evolution of good\npractices in the literature and maintain community guidelines.\nWe emphasize the importance of disseminating evaluation expec-\ntations from the SE to the ML community, fostering mutual under-\nstanding of evolving practices. It‚Äôs noteworthy that metrics drawn\nfrom Natural Language Processing (NLP) studies (e.g., counting the\nnumber of compiling unit tests generated by LLMs as done in [6, 38])\ndo not adequately reflect the well-established performance metrics\nfor SE tasks that do not have an NLP focus (e.g., see the existing\nstandards for assessing unit test generation tools [10, 18, 31]).\nWe strongly believe that breaking the silence as a community\nwill enhance the validity and reliability of our LLM-based contribu-\ntions and the SE community in general. The goal is to ultimately\nadvance the field while ensuring high research quality and high\nmethodological standards (considering different aspects, including\ndata privacy [5], carbon footprint [39], etc.).\n5https://onnx.ai/\nBreaking the Silence: the Threats of Using LLMs in Software Engineering ICSE-NIER‚Äô24, April 14‚Äì20, 2024, Lisbon, Portugal\nReferences\n[1] 2023. Hugging Face ‚Äì The AI community building the future. https://huggingface.\nco [Online; accessed 11. Sept. 2023].\n[2] 2023. LeetCode - The World‚Äôs Leading Online Programming Learning Platform.\nhttps://leetcode.com [Online; accessed 12. Sept. 2023].\n[3] 2023. Zenodo. https://zenodo.org [Online; accessed 11. Sept. 2023].\n[4] Qurat Ul Ain, Wasi Haider Butt, Muhammad Waseem Anwar, Farooque Azam,\nand Bilal Maqbool. 2019. A systematic review on code clone detection. IEEE\naccess 7 (2019), 86121‚Äì86144.\n[5] Ali Al-Kaswan and Maliheh Izadi. 2023. The (ab)use of Open Source Code to\nTrain Large Language Models. In 2023 IEEE/ACM 2nd International Workshop on\nNatural Language-Based Software Engineering (NLBSE) .\n[6] Saranya Alagarsamy, Chakkrit Tantithamthavorn, and Aldeida Aleti. 2023.\nA3Test: Assertion-Augmented Automated Test Case Generation. arXiv preprint\narXiv:2302.10352 (2023).\n[7] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\nRuxandra Cojocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, Julien\nLaunay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme\nPenedo. 2023. The Falcon Series of Language Models: Towards Open Frontier\nModels. (2023).\n[8] Leonhard Applis, Annibale Panichella, and Ruben Marang. 2023. Searching for\nQuality: Genetic Algorithms and Metamorphic Testing for Software Engineering\nML. In Proc. of the Genetic and Evolutionary Computation Conference . 1490‚Äì1498.\n[9] Leonhard Applis, Annibale Panichella, and Arie van Deursen. 2021. Assessing\nRobustness of ML-Based Program Analysis Tools using Metamorphic Program\nTransformations. In 2021 36th IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE) . 1377‚Äì1381.\n[10] Andrea Arcuri and Lionel Briand. 2014. A hitchhiker‚Äôs guide to statistical tests\nfor assessing randomized algorithms in software engineering. Software Testing,\nVerification and Reliability 24, 3 (2014), 219‚Äì250.\n[11] Owura Asare, Meiyappan Nagappan, and N Asokan. 2022. Is github‚Äôs copi-\nlot as bad as humans at introducing vulnerabilities in code? arXiv preprint\narXiv:2204.04741 (2022).\n[12] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen\nTian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang,\net al. 2022. Multi-lingual evaluation of code generation models. arXiv preprint\narXiv:2210.14868 (2022).\n[13] Authors. 2023. https://github.com/LLM4SE/obfuscated-ChatGPT-experiments\n[14] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is ChatGPT‚Äôs behavior\nchanging over time? arXiv preprint arXiv:2307.09009 (July 2023). arXiv:2307.09009\n[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[16] J√ºrgen Cito, Isil Dillig, Vijayaraghavan Murali, and Satish Chandra. 2022. Coun-\nterfactual explanations for models of code. InProceedings of the 44th International\nConference on Software Engineering: Software Engineering in Practice . 125‚Äì134.\n[17] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020. Embedding\nJava Classes with code2vec: Improvements from Variable Obfuscation. In2020\nIEEE/ACM 17th International Conference on Mining Software Repositories (MSR) .\nIEEE, New York, NY, USA, 243‚Äì253. https://doi.org/10.1145/3379597.3387445\n[18] Xavier Devroey, Alessio Gambi, Juan Pablo Galeotti, Ren√© Just, Fitsum Kifetew,\nAnnibale Panichella, and Sebastiano Panichella. 2023. JUGE: An infrastructure\nfor benchmarking Java unit test generators. Software Testing, Verification and\nReliability 33, 3 (2023), e1838.\n[19] Richard H. Epstein and Franklin Dexter. 2023. Variability in Large Language\nModels‚Äô Responses to Medical Licensing and Certification Examinations. Com-\nment on ‚ÄúHow Does ChatGPT Perform on the United States Medical Licensing\nExamination? The Implications of Large Language Models for Medical Education\nand Knowledge Assessment‚Äù. JMIR Medical Education 9, 1 (July 2023), e48305.\nhttps://doi.org/10.2196/48305\n[20] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei\nTan. 2023. Automated Repair of Programs from Large Language Models. In 2023\nIEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE,\n1469‚Äì1481. https://doi.org/10.1109/ICSE48619.2023.00128\n[21] Gordon Fraser and Andrea Arcuri. 2011. EvoSuite: Automatic Test Suite Gen-\neration for Object-Oriented Software. In Proceedings of the 19th ACM SIGSOFT\nESEC/FSE (Szeged, Hungary) (ESEC/FSE ‚Äô11) . ACM, New York, NY, USA, 416‚Äì419.\nhttps://doi.org/10.1145/2025113.2025179\n[22] Huseyin Atahan Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor\nR√ºhle, James Withers, and Robert Sim. 2021. Training Data Leakage Analysis in\nLanguage Models. (February 2021). https://www.microsoft.com/en-us/research/\npublication/training-data-leakage-analysis-in-language-models/\n[23] Kevin Jesse, Toufique Ahmed, Premkumar T Devanbu, and Emily Morgan. 2023.\nLarge Language Models and Simple, Stupid Bugs. arXiv preprint arXiv:2303.11455\n(2023).\n[24] Ren√© Just, Darioush Jalali, and Michael D. Ernst. 2014. Defects4J: A Database\nof Existing Faults to Enable Controlled Testing Studies for Java Programs. In\nProceedings of the 2014 International Symposium on Software Testing and Analysis\n(San Jose, CA, USA) (ISSTA 2014). ACM, NY, USA, 437‚Äì440.\n[25] Anjan Karmakar, Julian Aron Prenner, Marco D‚ÄôAmbros, and Romain Robbes.\n2022. Codex Hacks HackerRank: Memorization Issues and a Framework for Code\nSynthesis Evaluation. arXiv (Dec. 2022). https://doi.org/10.48550/arXiv.2212.\n02684 arXiv:2212.02684\n[26] Caroline Lemieux, Jeevana Priya Inala, Shuvendu K. Lahiri, and Siddhartha Sen.\n2023. CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-\nTrained Large Language Models. In Proc. of the 45th International Conference\non Software Engineering (ICSE ‚Äô23) . IEEE Press, Melbourne, Victoria, Australia,\n919‚Äì931. https://doi.org/10.1109/ICSE48619.2023.00085\n[27] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou,\nSilvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474\n(2022).\n[28] OpenAI. 2023. OpenAI. https://openai.com/ Accessed on September 14th, 2023.\n[29] Ipek Ozkaya. 2023. Application of Large Language Models to Software Engi-\nneering Tasks: Opportunities, Risks, and Implications. IEEE Software 40, 3 (April\n2023), 4‚Äì8. https://doi.org/10.1109/MS.2023.3248401\n[30] Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. 2015. Refor-\nmulating branch coverage as a many-objective optimization problem. In 2015\nIEEE 8th international conference on software testing, verification and validation\n(ICST). IEEE, 1‚Äì10.\n[31] Sebastiano Panichella, Alessio Gambi, Fiorella Zampetti, and Vincenzo Riccio.\n2021. Sbst tool competition 2021. In 2021 IEEE/ACM 14th International Workshop\non Search-Based Software Testing (SBST) . IEEE, 20‚Äì27.\n[32] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt. 2023. Examining Zero-\nShot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium\non Security and Privacy (SP) . IEEE, Los Alamitos, CA, USA, 2339‚Äì2356. https:\n//doi.ieeecomputersociety.org/10.1109/SP46215.2023.10179420\n[33] Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. 2023. On\nthe Challenges of Using Black-Box APIs for Toxicity Evaluation in Research.\narXiv:2304.12397 [cs.CL]\n[34] Mohammed Latif Siddiq, Joanna Santos, Ridwanul Hasan Tanvir, Noshin Ulfat,\nFahmid Al Rifat, and Vinicius Carvalho Lopes. 2023. Exploring the Effectiveness of\nLarge Language Models in Generating Unit Tests.arXiv preprint arXiv:2305.00418\n(2023).\n[35] Yutian Tang, Zhijie Liu, Zhichao Zhou, and Xiapu Luo. 2023. ChatGPT vs\nSBST: A Comparative Assessment of Unit Test Suite Generation. arXiv preprint\narXiv:2307.00588 (2023).\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971 (2023).\n[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[38] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel\nSundaresan. 2020. Unit test case generation with transformers and focal context.\narXiv preprint arXiv:2009.05617 (2020).\n[39] Roberto Verdecchia, June Sallou, and Lu√≠s Cruz. 2023. A systematic review of\nGreen AI. WIREs Data Mining and Knowledge Discovery 13, 4 (2023), e1507.\nhttps://doi.org/10.1002/widm.1507\n[40] Yonghao Wu, Zheng Li, Jie M Zhang, Mike Papadakis, Mark Harman, and\nYong Liu. 2023. Large Language Models in Fault Localisation. arXiv preprint\narXiv:2308.15276 (2023).\n[41] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated\nprogram repair in the era of large pre-trained language models. In Proceedings of\nthe 45th International Conference on Software Engineering (ICSE 2023). Association\nfor Computing Machinery .\n[42] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Go-\ning: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. arXiv preprint\narXiv:2304.00385 (2023).\n[43] Zhou Yang, Jieke Shi, Junda He, and David Lo. 2022. Natural attack for pre-trained\nmodels of code. In Proceedings of the 44th International Conference on Software\nEngineering. 1482‚Äì1493.\n[44] Wentao Ye, Mingfeng Ou, Tianyi Li, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie\nFu, Gang Chen, Junbo Zhao, et al. 2023. Assessing Hidden Risks of LLMs: An\nEmpirical Study on Robustness, Consistency, and Credibility. arXiv preprint\narXiv:2305.10235 (2023).\n[45] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial examples for models of\ncode. Proc. of the ACM on Programming Languages 4, OOPSLA (2020), 1‚Äì30.\n[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting\nHuang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren‚Äôs Song in the AI\nOcean: A Survey on Hallucination in Large Language Models. arXiv preprint\narXiv:2309.01219 (2023).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5531041026115417
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5007500648498535
    },
    {
      "name": "Test (biology)",
      "score": 0.4510180950164795
    },
    {
      "name": "Best practice",
      "score": 0.4339454174041748
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.41707369685173035
    },
    {
      "name": "Political science",
      "score": 0.183834046125412
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98358874",
      "name": "Delft University of Technology",
      "country": "NL"
    }
  ]
}