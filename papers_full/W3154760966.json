{
  "title": "Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events",
  "url": "https://openalex.org/W3154760966",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3035065131",
      "name": "Hossein Rajaby Faghihi",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A212051758",
      "name": "Parisa KordJamshidi",
      "affiliations": [
        "Michigan State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2919420119",
    "https://openalex.org/W1793121960",
    "https://openalex.org/W2890399523",
    "https://openalex.org/W2889317091",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2963436881",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963984224",
    "https://openalex.org/W2897513992",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W2963870701",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3127399811",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2986801687",
    "https://openalex.org/W2971253865",
    "https://openalex.org/W2964285770",
    "https://openalex.org/W2252016937",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963372003",
    "https://openalex.org/W3154690330",
    "https://openalex.org/W3045971363"
  ],
  "abstract": "Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model~(TSLM model) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a $3.1\\%$ increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4560–4570\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4560\nTime-Stamped Language Model: Teaching Language Models to\nUnderstand the Flow of Events\nHossein Rajaby Faghihi\nMichigan State University\nrajabyfa@msu.edu\nParisa Kordjamshidi\nMichigan State University\nkordjams@msu.edu\nAbstract\nTracking entities throughout a procedure de-\nscribed in a text is challenging due to the dy-\nnamic nature of the world described in the pro-\ncess. Firstly, we propose to formulate this task\nas a question answering problem. This en-\nables us to use pre-trained transformer-based\nlanguage models on other QA benchmarks\nby adapting those to the procedural text un-\nderstanding. Secondly, since the transformer-\nbased language models cannot encode the ﬂow\nof events by themselves, we propose a Time-\nStamped Language Model (TSLM model) to\nencode event information in LMs architec-\nture by introducing the timestamp encoding.\nOur model evaluated on the Propara dataset\nshows improvements on the published state-\nof-the-art results with a 3.1% increase in F1\nscore. Moreover, our model yields better re-\nsults on the location prediction task on the\nNPN-Cooking dataset. This result indicates\nthat our approach is effective for procedural\ntext understanding in general.\n1 Introduction\nA procedural text such as a recipe or an instruction\nusually describes the interaction between multiple\nentities and their attribute changes at each step of\na process. For example, the photosynthesis pro-\ncedure can contain steps such as 1. Roots absorb\nwater from soil ; 2. The water ﬂows to the leaf ;\n3. Light from the sun and CO2 enter the leaf ; 4.\nThe water, light, and CO2 combine into a mixture;\n5. Mixture forms sugar . Procedural text under-\nstanding is a machine reading comprehension task\ndeﬁned on procedural texts. Answering questions\nsuch as \"what is the location of the mixture at step\n4\", in the above example, requires tracking enti-\nties’ interactions to predict their attributes at each\nstep (Dalvi et al., 2018; Bosselut et al., 2018). This\nis quite challenging due to the dynamic nature of\nthe entities’ attributes in the context.\nTransformer-based language models have shown\npromising results on multi-hop or single-hop\nquestion answering benchmarks such as Hot-\npotQA (Yang et al., 2018), SQuAD (Rajpurkar\net al., 2016), and Drop (Dua et al., 2019). However,\nit is hard to expect LMs to understand the ﬂow of\nevents and pay attention to the time in the proce-\ndure (e.g., step 4) without extra modeling efforts.\nIn recent research, different approaches are taken\nto address procedural reasoning based on lan-\nguage models using QA formulations. Following\nthe intuition that attributes of entities can be re-\ntrieved based on the current and previous steps,\nDynaPro (Amini et al., 2020) modiﬁes the input to\nonly contain those sentences in the input at each\ntime. This will provide a different input to the\nmodel based on each question to help it detect\nchanges after adding each step. KG-MRC (Das\net al., 2018) also generates a dynamic knowledge\ngraph at each step to answer the questions. How-\never, this intuition is contradicted in some scenar-\nios such as detecting inputs of the process. For\ninstance, the answer to the question \"Where is light\nas step 0?\" is \"Sun\", even if it is not mentioned in\nthe ﬁrst sentence of the process. Inputs are entities\nthat are not created in the process.\nThe architecture of the QA transformer-based\nLMs is very similar to the traditional attention\nmechanism. Other methods such as ProLo-\ncal (Dalvi et al., 2018) and ProGlobal (Dalvi et al.,\n2018) have structured this task by ﬁnding the at-\ntention of each entity to the text at each step. To\nbe sensitive to the changes at each step, ProLo-\ncal manually changes the model’s input by remov-\ning all steps except the one related to the question.\nProGlobal computes attention to the whole context\nwhile adding a distance value. Distance value is\ncomputed for each token based on its distance to\nthe direct mention of the entity at each step.\nThe current language models convey rich linguis-\ntic knowledge and can serve as a strong basis for\n4561\nsolving various NLP tasks (Liu et al., 2019; Devlin\net al., 2019; Yang et al., 2019). That is why most\nof the state-of-the-art models on procedural reason-\ning are also built based on current language mod-\nels (Amini et al., 2020; Gupta and Durrett, 2019).\nFollowing the same idea, we investigate the chal-\nlenges that current models are facing for dealing\nwith procedural text and propose a new approach\nfor feeding the procedural information into LMs\nin a way that the LM-based QA models are aware\nof the taken steps and can answer the questions\nrelated to each speciﬁc step in the procedure.\nWe propose the Time-Stamped Language\nmodel (TSLM model), which uses timestamp em-\nbedding to encode past, current, and future time of\nevents as a part of the input to the model. TSLM\nutilizes timestamp embedding to answer differently\nto the same question and context based on differ-\nent steps of the process. As we do not change the\nportion of the input manually, our approach en-\nables us to beneﬁt from the pre-trained LMs on\nother QA benchmarks by using their parameters\nto initialize our model and adapt their architecture\nby introducing a new embedding type. Here, we\nuse RoBERTa (Liu et al., 2019) as our baseline\nlanguage model.\nWe evaluate our model on two bench-\nmarks, Propara (Dalvi et al., 2018) and NPN-\nCooking (Bosselut et al., 2018). Propara contains\nprocedural paragraphs describing a series of events\nwith detailed annotations of the entities along with\ntheir status and location. NPN-Cooking contains\ncooking recipes annotated with their ingredients\nand their changes after each step in criteria such as\nlocation, cleanliness, and temperature.\nTSLM differs from previous research as its pri-\nmary focus is on using pre-trained QA models and\nintegrating the ﬂow of events in the global repre-\nsentation of the text rather than manually chang-\ning the part of the input fed to the model at each\nstep. TSLM outperforms the state-of-the-art mod-\nels in nearly all metrics of two different evalua-\ntions deﬁned on the Propara dataset. Results show\na 3.1% F1 score improvement and a 10.4% im-\nprovement in recall. TSLM also achieves the state-\nof-the-art result on the location accuracy on the\nNPN-Cooking location change prediction task by a\nmargin of 1.55%. In summary, our contribution is\nas follows:\n• We propose Time-Stamped Language\nModel (TSLM model) to encode the meaning\nof past, present, and future steps in processing\na procedural text in language models.\n• Our proposal enables procedural text under-\nstanding models to beneﬁt from pre-trained\nLM-based QA models on general-domain QA\nbenchmarks.\n• TSLM outperforms the state-of-the-art mod-\nels on the Propara benchmark on both\ndocument-level and sentence-level evalua-\ntions. TSLM improves the performance state-\nof-the-art models on the location prediction\ntask of the NPN-Cooking (Bosselut et al.,\n2018) benchmark.\n• Improving over two different procedural text\nunderstanding benchmarks suggests that our\napproach is effective, in general, for solving\nthe problems that require the integration of\nthe ﬂow of events in a process.\n2 Problem Deﬁnition\nAn example of a procedural text is shown in Table\n1. The example is taken from the Propara (Dalvi\net al., 2018) dataset and shows the photosynthesis\nprocedure. At each row, the ﬁrst column is list\nof the sentences, each of which forms one step\nof the procedure. The second column contains\nthe number of the step in the process and the rest\nare the entities interacting in the process and their\nlocation at each step. The location of entities at step\n0 is their initial location, which is not affected by\nthis process. If an entity has a known or unknown\nlocation (speciﬁed by “?”) at step 0, we call it an\ninput.\nThe procedural text understanding task is de-\nﬁned as follows. Given a procedure p contain-\ning a list of n sentences P = {s1, ...sn}, an en-\ntity e and a time step ti, we ﬁnd L, the location\nof that entity and specify the status S of that en-\ntity. Status S is one value in the predeﬁned set of\n{non-existence, unknown-location, known-location}. loca-\ntion L is a span of text in the procedure that is spec-\niﬁed with its beginning and end token. We formu-\nlate the task as ﬁnding function F that maps each\ntriplet of entity, procedure and time step to a pair\nof entity location and status: (S, L) =F(e, P, ti)\n3 Proposed Procedural Reasoning Model\n3.1 QA Setting\nTo predict the status and the location of entities at\neach step, we model F with a question answering\nsetting. For each entity e, we form the input Qe as\n4562\nParticipants\nParagraph State number Water Light CO2 Mixture Sugar\n(Before the process starts) State 0 Soil Sun ? - -\nRoots absorb water from soil State 1 Root Sun ? - -\nThe water ﬂows to the leaf State 2 Leaf Sun ? - -\nLight from the sun and CO2 enter the leaf State 3 Leaf Leaf Leaf - -\nThe water, light, and CO2 combine into a mixture State 4 - - - Leaf -\nMixture forms sugar State 5 - - - - Leaf\nTable 1: An example of procedural text and its annotations from the Propara dataset (Dalvi et al., 2018). \"-\" means\nentity does not exist. \"?\" means the location of entity is unknown.\nfollows:\nQe =[CLS] Where is e? [SEP]\ns1 [SEP] s2 [SEP] ..., sn [SEP] (1)\nAlthough Qe is not a step-dependent representa-\ntion and does not incorporate any different informa-\ntion for each step, our mapping function needs to\ngenerate different answers for the question \"Where\nis entity e?\" based on each step of the procedure.\nFor instance, consider the example in Table 1 and\nthe question \"where is water?\", our model should\ngenerate different answers at four different steps.\nThe answer will be “root”, “leaf”, “leaf”, “non-\nexistence” for steps 1 to 4, respectively.\nTo model this, we create pairs of (Qe, ti) for\neach i ∈ {0, 1, ..., n}. For each pair, Qe is\ntimestamped according to ti using Timestamp(.)\nfunction described in Sec. 3.2 and mapped to\nan updated step-dependent representation, Qti\ne =\nTimestamp(Qe, ti).\nThe updated input representation is fed to a lan-\nguage model (here ROBERTA) to obtain the step-\ndependent entity representation, Rti\ne , as shown in\nEquation 2. We discuss the special case of i = 0in\nmore details in Sec. 3.2.\nRti\ne = RoBERTa (Qti\ne ) (2)\nWe use the step-dependent entity representation,\nRti\ne , and forward it to another mapping function\ng(.) to obtain the location and status of the entity\ne in the output. In particular the output includes\nthe following three vectors, a vector representing\nthe predictions of entity status S, another vector\nfor each token’s probability of being the start of\nthe location span L, and a third vector carrying the\nprobability of each word being the last token of\nthe location span. The outputs of the model are\ncomputed according to the Equation 3.\n(status, Start_prob, End_prob) =g(Rti\ne ) (3)\nwhere Re is the tokens’ representations output of\nRoBERTa (Liu et al., 2019), andg(.) is a function\nwe apply on the token representations to get the\nﬁnal predictions. We will discuss each part of the\nmodel separately in the following sections.\n3.2 Timestamp Embedding\nThe timestamp embedding adds the step informa-\ntion to the inputQe to be considered in the attention\nmechanism. The step attention is designed to dis-\ntinguish between current (what is happening now),\npast (what has happened before), and future (what\nhas not yet happened) information.\nWe use the mapping function Timestamp(.)\nfrom the pair (Qe, ti) to add a number along with\neach token in Qe and retrieve the step-dependent\ninput Qti\ne as shown in Figure 1. The Mapping func-\ntion Timestamp(.) integrates past, current, and\nfuture representations to all of the tokens related to\neach part. Timestamp(.) function assigns number\n1 for past, 2 for current, and 3 for future tokens in\nthe paragraph by considering one step of the pro-\ncess as the current event. These values are used\nto compute an embedding vector for each token,\nwhich will be added to its initial representation as\nshown in Figure 2. The special number 0 is as-\nsigned to the question tokens, which are not part of\nthe process timeline. For predicting State 0 (The\ninputs of the process), we set all the paragraph\ninformation as the current step.\n3.3 Status classiﬁcation\nTo predict the entities’ status, we apply a linear\nclassiﬁcation module on top of the [CLS] token\nrepresentation in Re as shown in Equation 4.\nAttribute = Softmax (WT Re[C]) (4)\nwhere Re[C] is the representation of the [CLS]\ntoken which is the ﬁrst token in Re.\n4563\nWhere is Water? Roots absorb water from soil. The water ﬂows to the leaf. The water, light and CO2 combine into a mixture.\n \nStep 0 Ignore Current\nStep 1 Ignore Current Future\nStep 2 Ignore Current FuturePast\nStep 3 Ignore CurrentPast\nQuestion Paragraph \n2 2 2 2 ... 2 2 \n2 2 2 2 2 3 3 ... 3 3 \n1 1 1 1 1 2 2 ... 3 ... 3 \n1 1 1 1 ... 2 2 2 ... \n0 0 0 \n0 0 0 \n0 0 0 \n0 0 0 \nFigure 1: An example of timestamp embedding in a procedural text. The question is always ignored with value\n\"0\". At each step i, the tokens from that step are paired with “current” value, tokens from steps 0 to i are paired\nwith “past” value, and the tokens from stepi to last step are paired with the “future” value.\n3.4 Span prediction\nWe predict a location span for each entity for\neach step of the process as shown in Equation\n5, we follow the popular approach of selecting\nstart/end tokens to detect a span of the text as\nthe ﬁnal answer. We compute the probability of\neach token being the start or the end of the an-\nswer span. If the index with the highest probabil-\nity to be the start token is tokenstart and for the\nend token is tokenend, the answer location will be\nLocation = P[tokenstart : tokenend].\nStart_prob = Softmax(WT\nstartRti\ne )\nEnd_prob = Softmax(WT\nendRti\ne )\ntokenstart = arg max\ni\n(Start_prob)\ntokenend = arg max\ni\n(End_prob)\n(5)\n3.5 Training\nWe use the cross-entropy loss function to train the\nmodel. At each prediction for entity e at times-\ntamp ti, we compute one loss value lossattribute\nregarding the status prediction and one loss value\nlosslocation for the span selection. The vari-\nable losslocation is the summation of the losses\nof the start token and the end token prediction,\nlosslocation = losslocationstart + losslocationend.\nThe ﬁnal loss of entity e at time ti is computed\nas in Equation 6.\nLosse\ni = losse\n(i,attribute) + losse\n(i,location) (6)\nTransformer Model\nQuestion Paragraph\nTimeStamp\nEmbeddingWord EmbeddingPosition\nEmbeddingType Embedding\nStep\nNumber\n+ \nCLS T1 T2 T3 T4 ... Tn\nStart/End Span predictionAttribute Prediction\nCurrent step of the question\n-/?/Location\nCLS Where IS W ater SEP Root Absorbs W ater ... SEP \nFigure 2: An overview of the proposed model. The\n“Timestamp Embedding” module is introduced in this\nwork and the rest of the modules are taken from basic\nlanguage model architecture.\n3.6 Inference\nAt inference time, we apply two different post-\nprocessing rules on the outputs of the model. First,\nwe impose that the ﬁnal selected location answer\nshould be a noun phrase in the original procedure.\nConsidering that a location span is a noun phrase,\nwe limit the model to do asoftmax over tokens of\nnoun phrases in the paragraph to select the start and\nend tokens. Second, we apply consistency rules to\nmake sure that our predicted status of entities are\nconsistent. We deﬁne the two following rules:\n• An entity can not be created if it has been\nalready destroyed : if Sti\ne is \"non-existence\"\nand Sti+1\ne is unknown or known location, then\nfor every step j, if Stj\ne is unknown or known\nlocation and Stj+1\ne is \"non-existence\", then i\n< j.\n4564\n• An entity cannot be created/destroyed twice\nin a process: if Stj\ne and Sti\ne are both \"-\", Stj+1\ne\nand Sti+1 are both either known or unknown\nlocation, then i = j.\nSti\ne is the status of entity e at step ti of the process.\nWe do not apply an optimization/search algo-\nrithm to ﬁnd the best assignment over the predic-\ntions according to the deﬁned constraints. The\nconstraints are only applied based on the order of\nthe steps to ensure that the later predictions are\nconsistent with the ones made before.\n4 Experiments\n4.1 Datasets\nPropara (Dalvi et al., 2018): This dataset was\ncreated as a benchmark for procedural text under-\nstanding to track entities at each step of a process.\nPropara contains 488 paragraphs and 3,300 sen-\ntences with annotations that are provided by crowd-\nworkers. The annotations ( 81,000) are the location\nof entities at each step of the process. The location\ncan be either the name of the location, unknown\nlocation, or speciﬁed as non-existence.\nNPN-Cooking (Bosselut et al., 2018): This is\na benchmark containing textual cooking instruc-\ntions. Annotators have speciﬁed ingredients of\nthese recipes and explained the recipe using differ-\nent changes happening on each ingredient at each\nstep of the instructions. These changes are reported\nin categories such as location, temperature, clean-\nliness, and shape. We evaluate our model on the\nlocation prediction task of this benchmark, which\nis the hardest task due to having more than 260\ncandidate answers. We do not use the candidates\nto ﬁnd the locations in our setting; Instead, we ﬁnd\na span of the text as the ﬁnal location answer. This\nis a relatively harder setting but more ﬂexible and\ngeneralizable than the classiﬁcation setting.\n4.2 Implementation Details\nWe use SGD optimizer implemented by Py-\ntorch (Paszke et al., 2017) to update the model\nparameters. The learning rate for the Propara im-\nplementation is set to 3 −e4 and is updated by\na scheduler with a 0.5 coefﬁcient every 50 steps.\nWe use 1 −e6 as the learning rate and a scheduler\nwith 0.5 coefﬁcient to update the parameters ev-\nery ten steps on the NPN-Cooking implementation.\nThe implementation code is publicly available at\nStep Entity Action Before After\n1 Water Move Root Leaf\n2 Water Destroy Leaf -\n1 Sugar Create - Leaf\n2 Sugar None Leaf Leaf\nTable 2: A sample table to evaluate the Propara\ndocument-level task.\nGitHub1.\nWe use RoBERTa (Liu et al., 2019) ques-\ntion answering architecture provided by Hugging-\nFace (Wolf et al., 2019). RoBERTa is pretrained\nwith SQuAD (Rajpurkar et al., 2016) and used as\nour base language model to compute the token rep-\nresentations. Our model executes batches contain-\ning an entity at every step and makes updates based\non the average loss of entities per procedure. The\nnetwork parameters are updated after executing one\nwhole example. The implementation code will be\npublicly available on GitHub after acceptance.\n4.3 Evaluation\nSentence-level evaluation is introduced in (Dalvi\net al., 2018) for Propara dataset. This evaluation\nfocuses on the following three categories.\n• Cat1 Is e created (destroyed/moved) during\nthe process?\n• Cat2 When is e created (destroyed/moved)\nduring the process?\n• Cat3 Where is e created (destroyed/moved\nfrom or to) during the process?\nDocument-level evaluation is a more comprehen-\nsive evaluation process and introduced later in (Tan-\ndon et al., 2018) for Propara benchmark. Currently,\nthis is the default evaluation in the Propara leader-\nboard containing four criteria:\n• What are the Inputs? Which entities existed\nbefore the process began and do not exist after\nthe process ends.\n• What are the Outputs? Which entities got\ncreated during the process?\n• What are the Conversions? Which entities\ngot converted to other entities?\n• What are the Moves? Which entities moved\nfrom one location to another?\nThe document-level evaluation requires models to\nreformat their predictions in a tabular format as\nshown in Table 2. At each row of this table, for\neach entity at a speciﬁc step, we can see the action\n1https://github.com/HLR/TSLM\n4565\nSentence-level Document-level\nModel Cat1 Cat2 Cat3 MacroAvg MicroAvg P R F1\nProLocal (Dalvi et al., 2018) 62.7 30.5 10.4 34.5 34.0 77.4 22.9 35.3\nProGlobal (Dalvi et al., 2018) 63.0 36.4 35.9 45.1 45.4 46.7 52.4 49.4\nEntNet (Henaff et al., 2017) 51.6 18.8 7.8 26.1 26.0 50.2 33.5 40.2\nQRN (Seo et al., 2017) 52.4 15.5 10.9 26.3 26.5 55.5 31.3 40.0\nKG-MRC (Das et al., 2018) 62.9 40.0 38.2 47.0 46.6 64.5 50.7 56.8\nNCET (Gupta and Durrett, 2019) 73.7 47.1 41.0 53.9 54.0 67.1 58.5 62.5\nXPAD (Dalvi et al., 2019) - - - - - 70.5 45.3 55.2\nProStruct (Tandon et al., 2018) - - - - - 74.3 43.0 54.5\nDYNAPRO (Amini et al., 2020) 72.4 49.3 44.5 55.4 55.5 75.2 58.0 65.5\nTSLM (Our Model) 78.81 56.8 40.9 58.83 58.37 68.4 68.9 68.6\nTable 3: Results from sentence-level and document-level evaluation on Propara. Cat i evaluations are deﬁned in\nSection 4.3.\napplied on that entity, the location of that entity\nbefore that step, and the location of the entity after\nthat step. Action takes values from a predeﬁned\nset including, “None”, “Create”, “Move”, and “De-\nstroy”. The exact action can be speciﬁed based on\nthe before and after locations.\nWe have to process our (Status S, Location L)\npredictions at each step to generate a similar tabular\nformat as in Table 2. We deﬁne ri\ne as a row in this\ntable which stores the predictions related to entitye\nat step ti. To ﬁll this row, we ﬁrst process the status\npredictions. If the status prediction S is either “-\n” or “?”, we ﬁll those values directly in the after\nlocation column. The before location column value\nof ri\ne is always equal to the after location column\nvalue of ri−1\ne . If the status is predicted to be a\n“Known Location”, we ﬁll the predicted location\nspan L into the after location column of ri\ne.\nThe action column is ﬁlled based on the data\nprovided in before and after locations columns. If\nthe before location is/isn’t \"-\" and after location\nis not/is \"-\", then the action is \"Create\"/\"Destroy\".\nIf the before and after locations are equal, then\nthe action is \"None\" and if the before and after\nlocations are both spans and are different from each\nother, the action is \"Move\".\nNPN-Cooking location change: We evaluate our\nmodel on the NPN-Cooking benchmark by com-\nputing the accuracy of the predicted locations at\nsteps where the locations of ingredients change.\nWe use the portion of the data that has been anno-\ntated by the location changes to train and evaluate\nour model. In this evaluation, we do not use the sta-\ntus prediction part of our proposed TSLM model.\nSince training our model on the whole training set\ntakes a very long time (around 20 hours per iter-\nation), we use a reduced number of samples for\ntraining. This is a practice that is also used in other\nprior work (Das et al., 2018).\n4.4 Results\nThe performance of our model on Propara\ndataset (Dalvi et al., 2018) is quantiﬁed in Table 3.\nResults show that our model improves the SOTA\nby a 3.1% margin in the F1 score and improves the\nRecall metric with 10.4% on the document-level\nevaluation. On the sentence-level evaluation, we\noutperform SOTA models with a 5.11% in Cat1,\nand 7.49% in Cat2 and by a 3.4% margin in the\nmacro-average. We report Table 3 without consid-\nering the consistency rules and evaluate the effect\nof those in the ablation study in Sec. 4.5.\nIn Table 5, we report a more detailed quanti-\nﬁed analysis of TSLM model’s performance based\non each different criteria deﬁned in the document-\nlevel evaluation. Table 5 shows that our model\nperforms best on detecting the procedure’s outputs\nand performs worst on detecting the moves. De-\ntecting moves is essentially hard for TSLM as it is\npredicting outputs based on the whole paragraph\nat once. Outperforming SOTA results on the input\nand output detection suggests that TSLM model\ncan understand the interactions between entities\nand detect the entities which exist before the pro-\ncess begins. The detection of input entities is one\nof the weak aspects of the previous research that\nwe improve here.\nA recent unpublished research (Zhang et al.,\n2021) reports better results than our model. How-\never, their primary focus is on common-sense rea-\n4566\nModel Accuracy Training Samples Prediction task\nNPN-cooking (Bosselut et al., 2018) 51.3 ∼ 83, 000 (all data) Classiﬁcation\nKG-MRC (Das et al., 2018) 51.6 ∼ 10, 000 Span Prediction\nDynaPro (Amini et al., 2020) 62.9 ∼ 83, 000 (all data) Classiﬁcation\nTSLM (Our Model) 63.73 ∼ 10, 000 Span Prediction\n64.45 ∼ 15, 000 Span Prediction\nTable 4: Results on the NPN-Cooking benchmark. Both class prediction and span prediction tasks are the same but\nuse two different settings, one selects among candidates, and the other chooses a span from the recipe. However,\neach model has used a different setting and a different portion of the training data. The information of the data\nsplits was not available that makes a fair comparison hard.\nCriteria Precision Recall F1\nInputs 89.8 71.3 79.5\nOutputs 85.6 91.4 88.4\nConversions 57.7 56.7 57.2\nMoves 40.5 56 47\nTable 5: Detailed analysis of TSLM performance on\nthe Propara test set on four criteria deﬁned in the\ndocument-level evaluation.\nsoning and their goal is orthogonal to our main\nfocus in proposing TSLM model. Such approaches\ncan be later integrated with TSLM to beneﬁt from\ncommon-sense knowledge on solving the Propara\ndataset.\nThe reason that TSLM performs better at recall\nand worse atprecision is that our model looks at the\nglobal context, which increases the recall and low-\ners the precision when local information is strongly\nimportant. The same phenomenon (better recall) is\nobserved in ProGlobal, which also considers global\ninformation as we do, compared to ProLocal.\nTable 4 shows our results on the NPN-Cooking\nbenchmark for the location prediction task. Re-\nsults are computed by only considering the steps\nthat contain a location change and are reported\nby computing the accuracy of predicting those\nchanges. Our results show that TSLM outperforms\nthe SOTA models with a 1.55% margin on accu-\nracy even after training on 15,000 training samples.\nTo be comparable with the KG-MRC (Das et al.,\n2018) experiment on NPN-Cooking which is only\ntrained on 10k samples, we report the performance\nof our model trained on the same number of sam-\nples, where TSLM gets a 12.1% improvement over\nthe performance of KG-MRC (Das et al., 2018).\n4.5 Ablation Study\nTo evaluate the importance of each module one at\na time, we report the performance of the TSLM\nby removing the noun-phrase ﬁltering at infer-\nence, the consistency rules, timestamp embedding,\nSQuAD (Rajpurkar et al., 2016) pre-training, and\nby replacing RoBERTa (Liu et al., 2019) with\nBERT (Devlin et al., 2019). These variations are\nevaluated on the development set of the Propara\ndataset and reported in Table 6. As stated before\nand shown in Table 6, it is impossible to remove\nthe timestamp embedding as that is the only part of\nthe model enabling changes in the answer at each\nstep. Hence, by removing that, the model cannot\nconverge and yields a 25% decrease on the F1\nscore. The simple consistency and span ﬁltering\nrules are relatively easy to be learned by the model\nbased on the available data, therefore adding those\ndoes not affect the ﬁnal performance of the model.\nTSLMBERT experiment is designed to ensure\na fair comparison with previous research (Amini\net al., 2020) which has used BERT as their base lan-\nguage model. The comparison of TSLMBERT to\n-SQuAD Pre-training and - Timestamp Embedding\nin Table 6 indicates that using RoBERTa instead\nof BERT is not as much important as our main\nproposal (using Time-stamp encoding) in TSLM\nmodel. Also, TSLMBERT achieves 66.7% F1 score\non the Propara test set, which is 1.2% better than\nthe current SOTA performance.\nBy removing the SQuAD pre-training phase, the\nmodel performance drops with a 10.6% in the F1\nscore. This indicates that despite the difference\nbetween the procedural text understanding and the\ngeneral MRC tasks, it is quite beneﬁcial to design\nmethods that can transfer knowledge from other\nQA data sources to help with procedural reasoning.\nThis is crucial as annotating procedural texts is\nrelatively more expensive and time-consuming.\n4567\nModel P R F1\nTSLMRoBERTa 72.9 74.1 73.5\n- constraints 73.8 73.3 73.5\n- noun-phrase ﬁltering 73.5 73.3 73.4\n- SQuAD Pre-training 78.8 52.2 62.8\n- Timestamp Embedding 94.6 32.6 48.5\nTSLMBERT 69.2 73.5 71.3\nTable 6: Ablation study results on the development set\nof the Propara document-level task. “- constraints”, “-\nSpan ﬁltering”, and “- Timestamp Encoding” shows\nour model performance while removing those modules.\n-SQuAD Pre-training is when we do not pre-train our\nbase language model on SQuAD. TSLM BERT is when\nwe use BERT as the base language model.\n5 Discussion\nWe provide more samples to support our hypoth-\nesis in solving the procedural reasoning task and\nanswer some of the main questions about the ideas\npresented in TSLM model.\nWhy is the whole context important? The main\nintuition behind TSLM is that the whole context,\nnot just previous information, matters in reason-\ning over a process. Here, we provide some sam-\nples from Propara to show why this intuition is\ncorrect. Consider this partial paragraph, \"Step i:\nWith enough time the pressure builds up greatly.\nStep i + 1: The resulting volcano may explode.\".\nLooking at the annotated status and location, the\n\"volcano\" is being created at Stepi without even be-\ning mentioned in that step. This is only detectable\nif we look at the next step saying \"The resulting\nV olcano...\".\nAs another example, consider this partial para-\ngraph: \"Step i: Dead plants form layers called peat.\n... Step i + 3: Pressure squeezes water out of the\npeat.\". The annotation indicates that the location of\n\"water\" is being changed to \"peat\" at step i, which\nis only possible to detect if the model is aware of\nthe following steps indicating that the water comes\nout of the peat.\nPositional Embedding VS Time-stamp encod-\ning: As mentioned before the whole context (fu-\nture and past events) is essential for procedural\nreasoning at a speciﬁc step. However, the reason-\ning should focus on one step at a time, given the\nwhole context. While positional encoding encodes\nthe order of information at the token-level for rea-\nsoning over the entire text, we need another level\nof encoding to specify the steps’ positions (bound-\naries) and, more importantly, to indicate the step\nthat the model should focus on when answering a\nquestion.\nAdvantages/Disadvantages of TSLM model :\nTSLM integrates higher-level information into the\ntoken representations. This higher-level infor-\nmation can come from event-sequence (time of\nevents), sentence-level, or any other higher source\nthan the token-level information. The ﬁrst advan-\ntage of TSLM is that it enables designing a model\nwhich is aware of the whole context, while previous\nmethods had to customize the input at each step to\nonly contain the information of earlier steps. Fur-\nthermore, using TSLM enables us to use pretrained\nQA models on other datasets without requiring us\nto retrain them with the added time-stamped en-\ncoding. One main disadvantage of TSLM model,\nwhich is natural due to the larger context setting in\nthis model, is not being sensitive to local changes,\nwhich is consistent with the observation in the com-\nparison between ProGlobal and ProLocal models.\n6 Related Works\nScoNe (Long et al., 2016), NPN-Cooking (Bosselut\net al., 2018), bAbI (Weston et al., 2015), Process-\nBank (Berant et al., 2014), and Propara (Dalvi et al.,\n2018) are benchmarks proposed to evaluate models\non procedural text understanding. Processbank (Be-\nrant et al., 2014) contains procedural paragraphs\nmainly concentrated on extracting arguments and\nrelations for the events rather than tracking the\nstates of entities. ScoNe (Long et al., 2016) aims to\nhandle co-reference in a procedural text expressed\nabout a simulated environment. bAbI (Weston\net al., 2015) is a simpler machine-generated tex-\ntual dataset containing multiple procedural tasks\nsuch as motion tracking, which has encouraged the\ncommunity to develop neural network models sup-\nporting explicit modeling of memories (Sukhbaatar\net al., 2015; Santoro et al., 2018) and gated re-\ncurrent models (Cho et al., 2014; Henaff et al.,\n2017). NPN-Cooking (Bosselut et al., 2018) con-\ntains recipes annotated with the state changes of\ningredients on criteria such as location, tempera-\nture, and composition. Propara (Dalvi et al., 2018)\nprovides procedural paragraphs and detailed anno-\ntations of entity locations and the status of their\nexistence at each step of a process.\nInspired by Propara and NPN-Cooking bench-\nmarks, recent research has focused on tracking en-\ntities in a procedural text. Query Reduction Net-\n4568\nworks (QRN) (Seo et al., 2017) performs gated\npropagation of a hidden state vector at each step.\nNeural Process Network (NPN) (Bosselut et al.,\n2018) computes the state changes at each step by\nlooking at the predicted actions and involved en-\ntities. Prolocal (Dalvi et al., 2018) predicts loca-\ntions and status changes locally based on each sen-\ntence and then globally propagates the predictions\nusing a persistence rule. Proglobal (Dalvi et al.,\n2018) predicts the status changes and locations\nover the whole paragraph using distance values\nat each step and predicts current status based on\ncurrent representation and the predictions of the\nprevious step. ProStruct (Tandon et al., 2018) aims\nto integrate manually extracted rules or knowledge-\nbase information on VerbNet (Schuler, 2005) as\nconstraints to inject common-sense into the model.\nKG-MRC (Das et al., 2018) uses a dynamic knowl-\nedge graph of entities over time and predicts lo-\ncations with spans of the text by utilizing read-\ning comprehension models. Ncet (Gupta and Dur-\nrett, 2019) updates entities representation based\non each sentence and connects sentences together\nwith an LSTM. To ensure the consistency of pre-\ndictions, Ncet uses a neural CRF over the changing\nentity representations. XPAD (Dalvi et al., 2019)\nis also proposed to make dependency graphs on\nthe Propara dataset to explain the dependencies of\nevents over time. Most recently, DynaPro (Amini\net al., 2020) feeds an incremental input to pre-\ntrained LMs’ question answering architecture to\npredict entity status and transitions jointly.\nTSLM differs from recent research, as we pro-\npose a simple, straightforward, and effective tech-\nnique to make our model beneﬁt from pre-trained\nLMs on general MRC tasks and yet enhance their\nability to operate on procedural text understand-\ning. We explicitly inject past, current, and future\ntimestamps into the language models input and im-\nplicitly train the model to understand the events’\nﬂow rather than manually feeding different portions\nof the context at each step. Procedural reasoning\nhas also been pursued within the multi-modality\ndomain (Yagcioglu et al., 2018; Rajaby Faghihi\net al., 2020; Amac et al., 2019) which has additional\nchallenges of aligning the representation spaces of\ndifferent modalities.\n7 Conclusion\nWe proposed the Time-Stamped Language\nModel (TSLM model), a novel approach based\non a simple and effective idea, which enables\npre-trained QA models to process procedural\ntexts and produce different outputs based on each\nstep to track entities and their changes. TSLM\nutilizes a timestamp function that causes the\nattention modules in the transformer-based LM\narchitecture to incorporate past, current, and\nfuture information by computing a timestamp\nembedding for each input token. Our experiments\nshow a 3.1% improvement on the F1 score and\na 10.4% improvement over the Recall metric on\nPropara Dataset. Our model further outperforms\nthe state-of-the-art models with a 1.55% margin in\nthe NPN-Cooking dataset accuracy for the location\nprediction task.\nAs a future direction, it is worth investigating\nhow common-sense knowledge can be integrated\nwith the TSLM setting by augmenting the process\ncontext using external sources of related domain\nknowledge. We also intend to investigate the effec-\ntiveness of our approach on similar tasks on other\ndomains and benchmarks. As another future direc-\ntion, it can be effective to apply an inference algo-\nrithm to impose the global consistency constraints\nover joint predictions in procedural reasoning in-\nstead of using naive post-processing rules.\nAcknowledgements\nThis project is partially funded by National Science\nFoundation (NSF) CAREER Award #2028626\nand the Ofﬁce of Naval Research (ONR) grant\n#N00014-20-1-2005.\nReferences\nMustafa Sercan Amac, Semih Yagcioglu, Aykut Erdem,\nand Erkut Erdem. 2019. Procedural reasoning net-\nworks for understanding multimodal procedures. In\nProceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL) , pages\n441–451.\nAida Amini, Antoine Bosselut, Bhavana Dalvi Mishra,\nYejin Choi, and Hannaneh Hajishirzi. 2020. Pro-\ncedural reading comprehension with attribute-aware\ncontext ﬂow. In Proceedings of the Conference on\nAutomated Knowledge Base Construction (AKBC).\nJonathan Berant, Vivek Srikumar, Pei-Chun Chen,\nAbby Vander Linden, Brittany Harding, Brad Huang,\nPeter Clark, and Christopher D Manning. 2014.\nModeling biological processes for reading compre-\nhension. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1499–1510.\n4569\nAntoine Bosselut, Omer Levy, Ari Holtzman, Corin En-\nnis, Dieter Fox, and Yejin Choi. 2018. Simulating\naction dynamics with neural process networks. In\nProceedings of the 6th International Conference for\nLearning Representations (ICLR).\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nBhavana Dalvi, Lifu Huang, Niket Tandon, Wen-tau\nYih, and Peter Clark. 2018. Tracking state changes\nin procedural text: a challenge dataset and models\nfor process paragraph comprehension. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1595–1604, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nBhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-\ntau Yih, and Peter Clark. 2019. Everything hap-\npens for a reason: Discovering the purpose of ac-\ntions in procedural text. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4496–4505, Hong Kong,\nChina. Association for Computational Linguistics.\nRajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan,\nAdam Trischler, and Andrew McCallum. 2018.\nBuilding dynamic knowledge graphs from text using\nmachine reading comprehension. In International\nConference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAditya Gupta and Greg Durrett. 2019. Tracking dis-\ncrete and continuous entity state for process under-\nstanding. In Proceedings of the Third Workshop\non Structured Prediction for NLP, pages 7–12, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2017. Tracking the world\nstate with recurrent entity networks. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nReginald Long, Panupong Pasupat, and Percy Liang.\n2016. Simpler context-dependent logical forms via\nmodel projections. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1456–\n1465, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nHossein Rajaby Faghihi, Roshanak Mirzaee, Sudar-\nshan Paliwal, and Parisa Kordjamshidi. 2020. La-\ntent alignment of procedural concepts in multimodal\nrecipes. In Proceedings of the First Workshop on\nAdvances in Language and Vision Research , pages\n26–31.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAdam Santoro, Ryan Faulkner, David Raposo, Jack\nRae, Mike Chrzanowski, Theophane Weber, Daan\nWierstra, Oriol Vinyals, Razvan Pascanu, and Timo-\nthy Lillicrap. 2018. Relational recurrent neural net-\nworks. In Advances in neural information process-\ning systems, pages 7299–7310.\nKarin Kipper Schuler. 2005. Verbnet: A broad-\ncoverage, comprehensive verb lexicon.\nMin Joon Seo, Sewon Min, Ali Farhadi, and Han-\nnaneh Hajishirzi. 2017. Query-reduction networks\nfor question answering. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference\nTrack Proceedings. OpenReview.net.\nSainbayar Sukhbaatar, arthur szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks.\nIn Advances in Neural Information Processing Sys-\ntems, volume 28, pages 2440–2448. Curran Asso-\nciates, Inc.\n4570\nNiket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih,\nAntoine Bosselut, and Peter Clark. 2018. Reasoning\nabout actions and state changes by injecting com-\nmonsense knowledge. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 57–66, Brussels, Belgium.\nAssociation for Computational Linguistics.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M. Rush, Bart van Merriënboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards ai-complete\nquestion answering: A set of prerequisite toy tasks.\nCite arxiv:1502.05698.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2019.\nHuggingface’s transformers: State-of-the-art natural\nlanguage processing. ArXiv, abs/1910.03771.\nSemih Yagcioglu, Aykut Erdem, Erkut Erdem, and Na-\nzli Ikizler-Cinbis. 2018. Recipeqa: A challenge\ndataset for multimodal comprehension of cooking\nrecipes. In EMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2369–2380, Brussels, Belgium. Association\nfor Computational Linguistics.\nZhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu, and\nDaxin Jiang. 2021. Knowledge-aware procedural\ntext understanding with multi-stage training. In Pro-\nceedings of the Web Conference 2021.",
  "topic": "ENCODE",
  "concepts": [
    {
      "name": "ENCODE",
      "score": 0.8354605436325073
    },
    {
      "name": "Computer science",
      "score": 0.8272269368171692
    },
    {
      "name": "Transformer",
      "score": 0.7438607215881348
    },
    {
      "name": "Timestamp",
      "score": 0.7393977642059326
    },
    {
      "name": "Language model",
      "score": 0.703435480594635
    },
    {
      "name": "Natural language processing",
      "score": 0.5162599086761475
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5000503063201904
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4896836578845978
    },
    {
      "name": "Question answering",
      "score": 0.4380197525024414
    },
    {
      "name": "Task (project management)",
      "score": 0.43533551692962646
    },
    {
      "name": "Real-time computing",
      "score": 0.09160244464874268
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}