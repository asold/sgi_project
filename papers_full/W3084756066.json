{
    "title": "Improving Indonesian Text Classification Using Multilingual Language Model",
    "url": "https://openalex.org/W3084756066",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3084457292",
            "name": "Ilham Firdausi Putra",
            "affiliations": [
                "Bandung Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A27977001",
            "name": "Ayu Purwarianti",
            "affiliations": [
                "Bandung Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3084457292",
            "name": "Ilham Firdausi Putra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A27977001",
            "name": "Ayu Purwarianti",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963047628",
        "https://openalex.org/W2950797315",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2973089652",
        "https://openalex.org/W3015566443",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2766665609",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W3101601200",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "Compared to English, the amount of labeled data for Indonesian text classification tasks is very small. Recently developed multilingual language models have shown its ability to create multilingual representations effectively. This paper investigates the effect of combining English and Indonesian data on building Indonesian text classification (e.g., sentiment analysis and hate speech) using multilingual language models. Using the feature-based approach, we observe its performance on various data sizes and total added English data. The experiment showed that the addition of English data, especially if the amount of Indonesian data is small, improves performance. Using the fine-tuning approach, we further showed its effectiveness in utilizing the English language to build Indonesian text classification models.",
    "full_text": " \nImproving Indonesian Text Classification Using \nMultilingual Language Model \n \nIlham Firdausi Putra \nSchool of Electrical Engineering and Informatics \nInstitut Teknologi Bandung \nIndonesia \nilhamfputra31@gmail.com \nAyu Purwarianti1,2 \n1U-CoE AI-VLB \n2School of Electrical Engineering and Informatics \nInstitut Teknologi Bandung \nIndonesia \nayu@stei.itb.ac.id \nAbstract— Compared to English, the amount of labeled data \nfor Indonesian text classification tasks is very small. Recently \ndeveloped multilingual language models have shown its ability \nto create multilingual representations  effectively. This paper  \ninvestigates the effect of combining English and Indonesian data \non building Indonesian text classification (e.g. , sentiment \nanalysis and hate  speech) using multilingual language models. \nUsing the feature-based approach, we observe its performance \non various data sizes and  total added English data. The \nexperiment showed that the addition of English data, especially \nif the amount of Indonesian data is small, improves \nperformance. Using the fine -tuning approach, we further \nshowed its effectiveness in utilizing the English language to build \nIndonesian text classification models. \nKeywords—multilingual language model, text classification, \nsentiment analysis, hate speech classification, Indonesian text \nI. INTRODUCTION \nWith the development of Indonesian people's access to the \ninternet, more and more text data is available digitally. This \ndata is full of information and very useful if processed. For \nexample, for business owners, citizen comments on the \ninternet can be analyzed for sentiment to determine their \nreaction to something . Then for those who have a website, \ndetecting violations in online conversations such as hate \nspeech or abuse automatically can be very helpful.  \nBuilding such good automated text classifiers needs a lot \nof labeled data, which the Indonesian language lacks \ncompared to other languages such as English. For example, \nthe tasks of sentiment analysis and hate speech detection have \nEnglish dataset like Yelp Review 1 (598K) and Jigsaw Toxic \nComment 2  (1902K) respectively. In contrast with the \nIndonesian language dataset from recent publications such as \n[2] (12K) & [3] (11K) for sentiment analysis, and [4] (12K) \nfor hate speech detection.  \nRecent deep, contextualized language models trained on \ncorpora of multiple languages provide powerful, general -\npurpose linguistic representation across language [1, 5]. These \nlanguage models were trained on a corpus of multiple \nlanguages without any multilingual objective. Surprisingly, an \nempirical investigation has shown that its hidden \nrepresentation does share a common subspace that represents \nuseful linguistic information in a language -agnostic way [6]. \nBased on these findings, we seek to improve  the Indonesian \n \n1 https://www.yelp.com/dataset \nlanguage's lack of data with English data by utilizin g \nmultilingual language models. \n In this paper, we  investigate the effect of combining \nEnglish and Indonesian data on building Indonesian sentiment \nanalysis and hate  speech detection using multilingual \nlanguage models. Our findings show that the addition o f the \nEnglish dataset using the feature-based approach, especially if \nIndonesian language data is small, improves Indonesian text \nclassification performance. However, there are cases where \nthe addition of excessive English data decreases classification \nperformance. Using a fine -tuning approach, we further \nimprove the result of previous research on sentiment analysis \n[2, 3] and hate  speech detection [4] on the Indonesian \nlanguage. \nII. RELATED WORKS \nPrevious works have conducted Indonesian sentiment \nanalysis usin g various text representation and model \napproaches. Farhan and Khodra [2] conducted experiments on \nvarious text representation s with support vector machine, \nrandom forest, and multilayer perceptron as  a model. \nEvaluation of their  dataset yields 0.8521 F1 -score using a \nneural network with TF -IDF as its feature. Crisdayanti and \nPurwarianti [3] conducted experiments on various text \nrepresentations and neural network topology models. \nEvaluation of their dataset yields 0.9369 F1 -score using Bi -\nLSTM with word embedding enhanced with paragraph vector. \nIbrohim and Budi [4] conducted experiments on multi -label \nhate speech and abusive language in Indonesian twitter. \nEvaluation of their dataset yields 77.36% av erage accuracy \nusing random forest with unigram as its feature. Previous \nworks on text classification using multilingual language \nmodels have focused on its zero -shot capability across 15 \nXNLI languages [5]. Using its largest multilingual model, [5] \nshowed that their model  is very competitive and even \noutperforming the monolingual language model  in various \nlanguages. There is no research in Indonesian text \nclassification using multilingual language models, especially \nin sentiment analysis and hate speech classification. \nIII. MULTILINGUAL LANGUAGE MODEL FOR INDONESIAN \nTEXT CLASSIFICATION \nThe latest Indonesian text classification model has been \ndeveloped using representation from word embeddings and \nsequential models such as recurrent neural networks (RNN), \n2 https://www.kaggle.com/c/jigsaw-unintended-bias-in-\ntoxicity-classification/data \nlong short-term memory (LSTM) [13] or, gated recurrent unit \n(GRU) [14] neural networks. While the combination of word \nembedding with RNN, LSTM, or GRU as the model has been \nvery successful, it still possesses some limitations [1, 8]. First, \nthe nature of sequential processing restricts the model since it \ncan only attend to the previous token. Second, pre -trained \nword representation failed to capture deeper meaning on a \ndocument of text. Both problems were solved using  the \nrecently developed language model, such as BERT [1]. \nBERT use Transformer architecture and pre -trained on \nmillions of texts with masked language model (MLM) \nobjective. MLM randomly masked a token on its input and \nasked the model to predict the masked token. This novel \nobjective, paired with the Transformer and its self -attention \nmechanism, allows the model to learn from the context of the \nwhole document. This method  has proven massively \nsuccessful, topping various benchmark and significantly \nimprove the performance from previousl y state -of-the-art \nsequential based language model such as ELMo [15].  \nOn the other hand, labeled Indonesian text data is scarce \nin comparison to English text data. Unfortunately, cross -\nlingual representation of text has enabled models to do transfer \nlearning across languages. It is now possible to train the model \non one language, and fine -tune it to a downstream task in \nanother language. To utilize the English language data on the \nIndonesian language task, we need a cross -lingual \nrepresentation of English and Indonesian language on shared \nvector space.  \nThere are two main methods o f producing cross-lingual \nrepresentation on shared vector space: alignment and joint \noptimization [9]. Earlier works on producing cross -lingual \nrepresentation have r elied on the former. Mikolov, Le, & \nSutskever [10] trained an embedding of two different \nlanguages independently and aligned the two using small \nbilingual data. While it had great success,  [9] argues that there \nare critical downsides such as it rely on ha ving good parallel \ndata and a key assumption that the embedding spaces of each \nlanguage are isomorphic, which [11, 12] proves does not hold \nfor many language pairs \nThe r ecent development on joint training methods has \nshown great results in obtaining cross -lingual representation \nwithout any parallel data. The method simply trains \nTransformer based language models on corpora of multiple \nlanguages, without any multilingual objective. The resulting \nmodels have shown good generalization ability across \nlanguage a nd language -agnostic representation inside its \nhidden state [6]. Using this cross-lingual and strong pre -\ntrained representation, we add English text data to train our \nIndonesian text classification model. \nOne such multilingual language model that we used in the \nexperiments is Multilingual BERT [1] and XLM -RoBERTa \n[5] (henceforth, mBERT and XLM-R). Denoting the number \nof Transformer blocks as L, the hidden size as H, and the \nnumber of self-attention heads as A, Multilingual BERT was \nusing the BERT-base architecture (L=12, H=768, A=12, Total \nParameters=110M) and was trained on a concatenation of \nWikipedia corpora from 104 languages. Because the size of \nWikipedia for a given language may vary greatly, a certain \nlanguage like English may be over-represented. To counteract \nthe imbalance, exponentially smoothed weighting of the data \nwas performed to over -sample low recourse languages and \nunder-sample high-resource languages. \nThe XLM-R model was designed to improve  mBERT in \nvarious ways. It follows  the improvement in the BERT pre-\ntraining approach [7]. It was trained with dynamic masking, \nlonger sequences, bigger batches, and without the Next \nSentence Prediction objective. The XLM-R Large also has \nsubstantially more parameters and was trained on  a l arger \nbalanced dataset from the CommonCrawl corpus that contains \n100 languages.  The XLM -R variant that we use in the \nexperiment is XLM-R Large (L = 24, H = 1024, A = 16, 550M \nparams). \nIV. EXPERIMENTS \nIn this section, we describe the building blocks of our \nexperiment. Fig. 1. show the overview of the experiment . \nOverall, it  consists of three training data scenarios, two \ntraining approaches, and five datasets.  \n \nFig. 1. Experiment overview \nA. Training Data Scenarios \nWe investigate the model performance in three different \nscenarios. Each differs by the combination of the language \nused in its training data: monolingual, zero -shot, and \nmultilingual. In the monolingual scenario, we use the \nIndonesian language text to trai n and validate the model. In \nthe zero-shot scenario, we use the English language text to \ntrain the model while being validated on Indonesian text. \nLastly, we use a combination of Indonesian and English text \nto train the model while being validated on Indonesian text in \nthe multilingual scenario. Using these scenarios, we observe \nthe improvement of the added English text. \nB. Training Approaches \nThere are two approaches on applying large pre -trained \nlanguage representation to downstream tasks: feature-based \nand fine-tuning [1]. On the feature-based approach, we extract \nfixed features from the pre-trained model. In this experiment, \nwe use the last hidden state, which is 768 for mBERT and \n1024 for XLM-R Large, as the feature. This extracted feature \nis then fed into a single dense layer, the only layer we trained \non the feature-based approach, connected with dropout before \nfinally end ing on a sigmoid function. In contrast, the fine-\ntuning approach trains all the language model parameters,  \n110M for mBERT and 550M for XLM-R Large, including the \nlast dense layer, on the training data binary cross-entropy loss. \nUsing the feature -based scenario, we run many \nexperiments as the expensive and multilingual representation \nhave been precomputed on all the data. In all training data \nscenarios, we vary the total data used. More specifically, we \n\ntrain the model using [500, 1000, 2500, 5000, 7500, Max] text \ndata. Specific to multilingual training data scenario, we vary \nthe amount of added English data by [0.25, 0.5, 0.75, 1, 1.5, 2, \n3, 4, 5, 6, 7, 8, 9, 10] times the amount of Indonesian text data. \nWe refer to a multilingual experiment with added English data \nN times the amount of Indonesian text data as multilingual(N). \nIn contrast to the feature-based scenarios, fine-tuning the \nfull language model is expensive and resource -intensive. \nHowever, as shown in [1], fully fine-tuning the full language \nmodel will result in a better text classifier. We fine-tuned the \nbest performing model on  the feature-based scenarios. The \nexperiment was reduced to only using the maximum total data \nand an added English data multiplier up to 3. \nC. Datasets \nTABLE I.  SENTIMENT DATASET DETAILS \nData Train Test Source Label \nFarhan & \nKhodra [2] \nPositive 6281 1125 \nNegative 6108 1304 \nCrisdayanti & \nPurwarianti [3] \nPositive 7151 208 \nNegative 3830 204 \nYelp Review \nPositive 299000 N/A \nNegative 299000 N/A \n  \nFor the Indonesian language sentiment analysis dataset, \nwe used data originated from previous related work by [2, 3]. \nFarhan and Khodra [2] used reviews crawled from \nTripAdvisor. Crisdayanti and Purwarianti [2] used text from \nTwitter, Zomato, TripAdvisor, Facebook , Instagram, and \nQraved. For the English language dataset, we used data from \nYelp that they have open -sourced. The details on the train, \ntest, and label distribution for each data can be seen in Table \nI. \n For the Indonesian  language hate speech dataset, we used \ndata originated from previous related work by [4]. Ibrohim \nand Budi [4] crawled tweets from Twitter and annotated the \ntext with the help of 30 diverse annotators. The annotation not \nonly includes whether or not the text falls into hate  speech, \nabusive, or normal, but also its target, category, and level. For \nthe English language, we use data from Jigsaw. The data \ncomes from various conversations over the internet, annotated \nby dozens and up to thousands of annotators per text. The data \nalso has rich and detailed labels up to its level and category.  \nTABLE II.  HATE SPEECH DATASET DETAILS \nData Train Test Source Label \nIbrohim & Budi \n[4] \nHate \nspeech 6578 731 \nNormal 5274 1586 \nJigsaw Toxic \nComment \nHate \nspeech 152111 N/A \nNormal 1750083 N/A \n \n However, the fine-grained labels between the English and \nthe Indonesian hate speech data did not exactly match. More \nimportantly, the two dataset s differ in their main label. \nIbrohim and Budi [4] annotate the text into normal, abusive, \nor hate  speech. Meanwhile, Jigsaw annotate s the text into \n \nFig. 2. Feature-based experiment result with XLM-R on [2] (left), [3] (middle), and [4] (right) \n \n \nFig. 3. Feature-based experiment result with mBERT on [2] (left), [3] (middle), and [4] (right) \n \n\nnormal or toxic. We simplify the label of [4] into a simple \nbinary label indicating the text is normal or hate  \nspeech/abusive. The hate speech dataset details can be seen in \nTable II. \nD. Training Reproducibility and Hyperparameters \nFrom the dataset detailed in Table I & II, the training data \nfurther split into training and validation set with a 90:10 ratio. \nThe split was done in a stratified fashion, conserving the \ndistribution of labels between the training & validation s et. \nThe result is a dataset separated into training, validation, and \ntest sets.  \nEach experiment will train the model using the training set \nand validate it to the validation set on each epoch. After each \nepoch, we will evaluate whether we will continue, reduce the \nlearning rate, or stop the training process based on validation \nset performance and the hyperparameter set on each condition. \nIn the end, we use the model from the best performing epoch \nbased on its validation performance to predict the test set. \nOn the feature-based experiment, we set the final layer \ndropout probability to 0.2, the learning rate reducer patience \nto 5, and the early stopping patience to 12. On full fine -tune \nexperiment, we set the final layer dropout probability to 0.2, \nthe learning rate reducer patience to 0, and the early stopping \npatience to 4. Every validation and prediction use 0.5 as its \nlabel threshold. \nTo ensure reproducibility, we set every random seed \npossible on each experiment. On the feature -based \nexperiment, we average the result of 6 different runs by \nvarying the seed from 1 -6. Running the same experiment on  \nthe feature-based approach will result in the same final score. \nOn the full fine-tune experiment, we only run one experiment. \nWhile the resul t should not differ substantially,  the exact \nreproducibility cannot be guaranteed as the training was done \non a TPU3. \nV. RESULTS \nWe build the system as mentioned in the previous section. \nThe next subsection talk in detail about the result of each \ntraining approach. \nA. Feature-based experiment \nThe result of the feature-based experiment with XLM-R \nmodel on all datasets can be seen in Fig. 1. Through this result, \nwe can see that adding English data can help the performance \nof the model. On [2] & [3] dataset, adding  English language \ndata consistently improves the performance. However, on [4], \nthere is a point where the added English data results in worse \nperformance. We hypothesize this is due to the large \ndifference in what constitutes hate speech (or toxic by Jigsaw \ndataset) between the datasets used.  \nThe result of the feature-based experiment with mBERT \nmodel on all datasets can be seen in Fig. 2. The same \nphenomenon is observed on mBERT based experiment, \nalthough the performance is substantially lower. This is \nexpected as XLM -R is designed to improve mBERT on \nvarious design choices.  \nDefining the gain as the difference between monolingual \nand its highest multilingual performance, Table III shows the \n \n3 https://suneeta-mall.github.io/2019/12/22/Reproducible-\nml-tensorflow.html \ngains averaged on all datasets across total data and model. The \nhighest gain can be seen on the lowest amount of total data \nused, 500, wit h an F1-score gain of 0.176 using XLM -R \nmodel and 0.129 using mBERT  model. The results suggest \nthat the lower the amount of data used; the more gains yield \nby adding English data to the training set. \nTABLE III.  AVERAGE F1-SCORE GAINS \nTotal Data \nAverage Gain \nXLM-R mBERT \n500 0.176221 0.129394 \n1000 0.165718 0.109215 \n2500 0.118456 0.051226 \n5000 0.095780 0.029564 \n7500 0.086930 0.028043 \nMAX 0.077875 0.020184 \n \nB. Full fine-tune experiment \n The result of fully fine -tuning all parameters, in addition \nto utilizing English data, proved to be effective in building a  \nbetter Indonesian text classification model. On [2] dataset, the \nhighest performance achieved on the zero-shot scenario where \nit yielded 0.893 F1 -score, improving the previous works of \n0.834. On [3] dataset, the highest performance achieved on \nmultilingual(1.5) scenario where it yielded perfect F1 -score, \nimproving the previous works of 0,9369. On [4] dataset, the \nhighest performance achieved on multilingual(3) scenario \nwhere it yielded 0.898 F1 -score and 89.9% accuracy. To \nprovide a fair comparison with the previous work by Ibrohim \n& Budi [4], we also ran the experiment using the original label \nand monolingual scenario. The experiment yielded 89.52% \naverage accuracy, improving the previous works of 77.36%. \nVI. CONCLUSION \nIn this work, we investigate the use of Indonesian and \nEnglish text data with multilingual language models to  \nimprove Indonesian language's lack of data. Through \nexperiments on sentiment analysis and hate speech detection \ntask, we show that the addition of English text data with the \nutilization of multilingual language models can improve \nmodel performance on Indonesian tasks. The less the \nIndonesian language text data used, the greater improvement \nyielded f rom adding English text data. Finally, the full \nutilization of a pre -trained language model combined with \nadded data by multilingual representation has successfully \nimproved the result of Indonesian sentiment analysis and hate \nspeech detection. \nACKNOWLEDGEMENT \nThis research is partly funded by ITB P3MI research for \nthe Informatics research group in the School of Electrical \nEngineering and Informatics. \n \n \nREFERENCES \n \n[1] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “BERT: Pre -\ntraining of Deep Bidirectional Transformers for Language \nUnderstanding,” in Proceedings of the 2019 Conference of the North \nAmerican Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long and Short Papers), \nMinneapolis, Minnes ota, Jun. 2019, pp. 4171 –4186, doi: \n10.18653/v1/N19-1423. \n[2] A. N. Farhan and M. L. Khodra, “Sentiment-specific word embedding \nfor Indonesian sentiment analysis,” in 2017 International Conference \non Advanced Informatics, Concepts, Theory, and Applications \n(ICAICTA), Aug. 2017, pp. 1 –5, doi: \n10.1109/ICAICTA.2017.8090964.  \n[3] A. Purwarianti and I. A. P. A. Crisdayanti, “Improving Bi -LSTM \nPerformance for Indonesian Sentiment Analysis Using Paragraph \nVector,” in 2019 International Conference of Advanced Informatics: \nConcepts, Theory and Applications (ICAICTA), Sep. 2019, pp. 1 –5, \ndoi: 10.1109/ICAICTA.2019.8904199. \n[4] M. O. Ibrohim and I. Budi, “Multi -label Hate Speech and Abusive \nLanguage Detection in Indonesian Twitter,” in Proceedings of the \nThird Workshop on Abusive L anguage Online, Florence, Italy, Aug. \n2019, pp. 46–57, doi: 10.18653/v1/W19-3506.  \n[5] A. Conneau et al., “Unsupervised Cross -lingual Representation \nLearning at Scale,” Jul. 2020, pp. 8440 –8451, doi: \n10.18653/v1/2020.acl-main.747.  \n[6] T. Pires, E. Schlinger, and D. Garrette, “How multilingual is \nMultilingual BERT?,” arXiv:1906.01502 [cs], Jun. 2019. \n[7] Y. Liu et al., “RoBERTa: A Robustly Optimized BERT Pretraining \nApproach,” arXiv:1907.11692 [cs], Jul. 2019.  \n[8] A. Vaswani et al., “Attention is all you need,” in Proceed ings of the \n31st International Conference on Neural Information Processing \nSystems, Long Beach, California, USA, Dec. 2017, pp. 6000–6010.  \n[9] Z. Wang, J. Xie, R. Xu, Y. Yang, G. Neubig, and J. Carbonell, “Cross-\nlingual Alignment vs Joint Training: A Comparat ive Study and A \nSimple Unified Framework,” arXiv:1910.04708 [cs], Feb. 2020. \n[10] T. Mikolov, Q. V. Le, and I. Sutskever, “Exploiting Similarities among \nLanguages for Machine Translation,” arXiv:1309.4168 [cs], Sep. 2013.  \n[11] A. Søgaard, S. Ruder, and I. Vulić, “O n the Limitations of \nUnsupervised Bilingual Dictionary Induction,” in Proceedings of the \n56th Annual Meeting of the Association for Computational Linguistics \n(Volume 1: Long Papers), Melbourne, Australia, Jul. 2018, pp. 778 –\n788, doi: 10.18653/v1/P18-1072. \n[12] B. Patra, J. R. A. Moniz, S. Garg, M. R. Gormley, and G. Neubig, \n“Bilingual Lexicon Induction with Semi-supervision in Non-Isometric \nEmbedding Spaces,” in Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics, Florence, It aly, Jul. 2019, \npp. 184–193, doi: 10.18653/v1/P19-1018.  \n[13] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory.” MIT \nPress, Nov. 01, 1997.  \n[14] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical Evaluation \nof Gated Recurrent Neural Networks on Sequence Modeling,” \narXiv:1412.3555 [cs], Dec. 2014. \n[15] M. Peters et al., “Deep Contextualized Word Representations,” in \nProceedings of the 2018 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long Papers), New Orleans, Louisiana, Jun. \n2018, pp. 2227–2237, doi: 10.18653/v1/N18-1202. \n \n "
}