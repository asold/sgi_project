{
    "title": "CPFTransformer: transformer fusion context pyramid medical image segmentation network",
    "url": "https://openalex.org/W4389439369",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100419510",
            "name": "Jiao Li",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5108883920",
            "name": "Jinyu Ye",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5023878117",
            "name": "Ruixin Zhang",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100898962",
            "name": "Yue Wu",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5093439415",
            "name": "Gebremedhin Samuel Berhane",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5025916246",
            "name": "Hongxia Deng",
            "affiliations": [
                "Taiyuan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100569759",
            "name": "Hong Shi",
            "affiliations": [
                "Shenzhen Polytechnic"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2412782625",
        "https://openalex.org/W2928133111",
        "https://openalex.org/W2966434031",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3197957534",
        "https://openalex.org/W4311088269",
        "https://openalex.org/W4291110778",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Introduction The application of U-shaped convolutional neural network (CNN) methods in medical image segmentation tasks has yielded impressive results. However, this structure’s single-level context information extraction capability can lead to problems such as boundary blurring, so it needs to be improved. Additionally, the convolution operation’s inherent locality restricts its ability to capture global and long-distance semantic information interactions effectively. Conversely, the transformer model excels at capturing global information. Methods Given these considerations, this paper presents a transformer fusion context pyramid medical image segmentation network (CPFTransformer). The CPFTransformer utilizes the Swin Transformer to integrate edge perception for segmentation edges. To effectively fuse global and multi-scale context information, we introduce an Edge-Aware module based on a context pyramid, which specifically emphasizes local features like edges and corners. Our approach employs a layered Swin Transformer with a shifted window mechanism as an encoder to extract contextual features. A decoder based on a symmetric Swin Transformer is employed for upsampling operations, thereby restoring the resolution of feature maps. The encoder and decoder are connected by an Edge-Aware module for the extraction of local features such as edges and corners. Results Experimental evaluations on the Synapse multi-organ segmentation task and the ACDC dataset demonstrate the effectiveness of our method, yielding a segmentation accuracy of 79.87% (DSC) and 20.83% (HD) in the Synapse multi-organ segmentation task. Discussion The method proposed in this paper, which combines the context pyramid mechanism and Transformer, enables fast and accurate automatic segmentation of medical images, thereby significantly enhancing the precision and reliability of medical diagnosis. Furthermore, the approach presented in this study can potentially be extended to image segmentation of other organs in the future.",
    "full_text": "Frontiers in Neuroscience 01 frontiersin.org\nCPFTransformer: transformer \nfusion context pyramid medical \nimage segmentation network\nJiao Li 1, Jinyu Ye 1, Ruixin Zhang 1, Yue Wu 1, \nGebremedhin Samuel Berhane 1, Hongxia Deng 1*  and Hong Shi 2*\n1 College of Computer Science and Technology, Taiyuan University of Technology, Taiyuan, China, \n2 School of Artificial Intelligence, Shenzhen Polytechnic, Shenzhen, China\nIntroduction: The application of U-shaped convolutional neural network (CNN) \nmethods in medical image segmentation tasks has yielded impressive results. \nHowever, this structure’s single-level context information extraction capability \ncan lead to problems such as boundary blurring, so it needs to be  improved. \nAdditionally, the convolution operation’s inherent locality restricts its ability to \ncapture global and long-distance semantic information interactions effectively. \nConversely, the transformer model excels at capturing global information.\nMethods: Given these considerations, this paper presents a transformer fusion \ncontext pyramid medical image segmentation network (CPFTransformer). The \nCPFTransformer utilizes the Swin Transformer to integrate edge perception \nfor segmentation edges. To effectively fuse global and multi-scale context \ninformation, we introduce an Edge-Aware module based on a context pyramid, \nwhich specifically emphasizes local features like edges and corners. Our \napproach employs a layered Swin Transformer with a shifted window mechanism \nas an encoder to extract contextual features. A decoder based on a symmetric \nSwin Transformer is employed for upsampling operations, thereby restoring the \nresolution of feature maps. The encoder and decoder are connected by an Edge-\nAware module for the extraction of local features such as edges and corners.\nResults: Experimental evaluations on the Synapse multi-organ segmentation task \nand the ACDC dataset demonstrate the effectiveness of our method, yielding a \nsegmentation accuracy of 79.87% (DSC) and 20.83% (HD) in the Synapse multi-\norgan segmentation task.\nDiscussion: The method proposed in this paper, which combines the context \npyramid mechanism and Transformer, enables fast and accurate automatic \nsegmentation of medical images, thereby significantly enhancing the precision \nand reliability of medical diagnosis. Furthermore, the approach presented in this \nstudy can potentially be extended to image segmentation of other organs in the \nfuture.\nKEYWORDS\nmedical image segmentation, Swin Transformer, Edge-Aware module, context pyramid \nfusion network, multiscale feature\nOPEN ACCESS\nEDITED BY\nTolga Cukur,  \nBilkent University, Türkiye\nREVIEWED BY\nHasan Atakan Bedel,  \nBilkent University, Türkiye  \nJingang Shi,  \nXi'an Jiaotong University, China\n*CORRESPONDENCE\nHongxia Deng  \n denghongxia@tyut.edu.cn  \nHong Shi  \n shh51@sina.com\nRECEIVED 04 September 2023\nACCEPTED 22 November 2023\nPUBLISHED 07 December 2023\nCITATION\nLi J, Ye J, Zhang R, Wu Y, Berhane GS, \nDeng H and Shi H (2023) CPFTransformer: \ntransformer fusion context pyramid medical \nimage segmentation network.\nFront. Neurosci. 17:1288366.\ndoi: 10.3389/fnins.2023.1288366\nCOPYRIGHT\n© 2023 Li, Ye, Zhang, Wu, Berhane, Deng and \nShi. This is an open-access article distributed \nunder the terms of the Creative Commons \nAttribution License (CC BY). The use, \ndistribution or reproduction in other forums is \npermitted, provided the original author(s) and \nthe copyright owner(s) are credited and that \nthe original publication in this journal is cited, \nin accordance with accepted academic \npractice. No use, distribution or reproduction is \npermitted which does not comply with these \nterms.\nTYPE Original Research\nPUBLISHED 07 December 2023\nDOI 10.3389/fnins.2023.1288366\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 02 frontiersin.org\n1 Introduction\nAutomatic and accurate segmentation of medical images is of \ngreat significance to disease diagnosis. Image segmentation is an \nimportant part of medical image analysis, such as the segmentation of \ncomputed tomography (CT) images of chest organs and chambers in \ncardiac MRI images. Aided diagnosis and image-guided clinical \nsurgery (Chen J. et al., 2021 ; Hatamizadeh et al., 2021 ), as well as \naccurate automatic segmentation, can be used to derive quantitative \nassessments of pathology or for subsequent diagnosis, treatment \nplanning, and monitoring of disease progression.\nResearch methods about it emerge in an endless stream, and most \nof the state-of-the-art medical image segmentation frameworks are \nbased on U-Net (Ronneberger et al., 2015) or its variants, using skip \nconnections combined with encoder-decoder architecture; many \nalgorithms follow this technical route. Usually, this structure preserves \nthe single-granularity information of the encoder layer through skip \nconnections. However, in doing so, it ignores the rich multi-scale \nspatial information, thus losing a lot of edge information, significantly \naffecting its performance in segmentation tasks. Furthermore, CNN’s \ninherent inductive bias makes it at a disadvantage to obtain global \nlong-range semantic information interactions.\nTherefore, it is difficult for CNN-based methods to learn global \nand long-range semantic information interactions. Recently, \nTransformer has also been applied to image processing (Carion et al., \n2020), inspired by the great success of Transformer in natural language \nprocessing (Vaswani et al., 2017). Although CNN has achieved many \nexcellent results in the field of image segmentation due to its \ncharacteristics of fast speed, low complexity, and high accuracy, it is \nnot as good as Transformer in long-distance modeling. Therefore, the \ncritical advantage of Transformer’s global attention combined with the \nexcellent properties of CNN can build better segmentation networks. \nCurrently, preliminary studies have attempted to apply Transformers \nto the field of medical image segmentation ( Chen J. et  al., 2021 ; \nHatamizadeh et al., 2021; Cao et al., 2022). Now that both CNN and \nTransformer architectures demonstrate their unique advantages, it \nmakes sense to combine the advantages of both architectures for a \ncomprehensive analysis.\nSpecifically, the main contributions of this paper can \nbe summarized as follows:\n 1 This paper presents a novel medical image segmentation \nnetwork called the Transformer fusion context pyramid \nmedical image segmentation network (CPFTransformer). To \nextract local features such as edges and corners, this paper \ndesigns an Edge-Aware (EA) block. This block uses convolution \nkernels of different sizes to extract features of different scales in \nmultiple branches. High-level features express more semantic \ninformation, shallower features carry more detailed \ninformation. The features of the shallow backbone are mainly \nused to generate edge features, so it is best to extract local \nfeatures such as edges and corners as much as possible to \nimprove the segmentation effect.\n 2 To address the limitations of fixed-scale convolution operations \nin target segmentation, this paper uses Swin Transformer to \nconstruct a boundary-specific medical image segmentation \nnetwork, maximizing the advantage of the Transformer’s focus \non global information to build a symmetric encoder-decoder \ntype architecture. In the encoder, self-attention is achieved \nlocally to globally; in the decoder, global features are upsampled \nto the input resolution for the corresponding pixel-level \nsegmentation prediction.\n 3 To enhance the precision and definition of segmented edges, \nthis paper introduces a boundary loss and supplementary \ninformation to augment the region loss. The proposed \napproach incorporates a joint loss that combines the Dice loss, \ncross-entropy loss, and boundary loss, thereby improving the \naccuracy and clarity in segmentation tasks.\n 4 Experiments in the Synapse multi-organ CT image \nsegmentation task and the ACDC MRI image segmentation \ntask show that the Transformer fusion context pyramid medical \nimage segmentation network achieves higher values in the Dice \nCoefficient and Hausdorff Distance evaluation metrics.\n2 Related Work\nIn this section, topics related to medical image segmentation are \ndiscussed and reviewed in terms of encoder-decoder architecture, \nvisual Transformer-based model, and contextual pyramidal \nfeature fusion.\n2.1 Encoder-decoder structure\nMost state-of-the-art medical image segmentation frameworks \nare based on U-Net or its variants, which uses a skip-connected \nencoder-decoder architecture to extract semantic features through \nsuccessive convolution and pooling operations. Many algorithms \nfollow this technical route. Despite the simple network structure, it \ncan be  well applied to different medical segmentation tasks. \n3DU-Net ( Çiçek et  al., 2016 ) replaces 2D with 3D convolution \noperations; Res-UNet (Xiao et al., 2018 ) replaces each sub-module \nof U-Net with residual connections respectively; and U-Net++ \n(Zhou et  al., 2018 ) greatly reduces the number of parameters \ncombines depth supervision. UNet3+ (Huang et al., 2020) proposed \nfull-scale skip connections to use multi-scale information fully; \nR2U-Net ( Alom et  al., 2018 ) achieved better performance in \ndifferent medical image segmentation tasks with the same \ncomputational load as U-Net; Ibtehaz and Rahman (2020) proposed \nMultiRes blocks to extract semantic information from multiple \nscales, and they also uses regular paths to alleviate the semantic gap \nbetween two symmetric encoder and decoder layers. Yuan et al. \n(2023) proposed CTC-Net, which designs two encoders by Swin \nTransformers and Residual CNNs to produce complementary \nfeatures in Transformer and CNN domains, then uses a Cross-\ndomain Fusion Block to blend them.\nThe common problem of these encoder-decoder-based \narchitectures is that semantic features are usually extracted layer-by-\nlayer during encoding, while the size and details of feature maps are \nrecovered layer-by-layer during decoding. These approaches enable \nend-to-end pixel segmentation but lack rich contextual information, \nthus losing a lot of edge information. In medical image segmentation, \nit is often necessary to consider richer contextual information around \nthe boundary region to be segmented.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 03 frontiersin.org\n2.2 Visual transformer and its variants\nPreviously, inspired by the great success of Transformers in \nnatural language processing ( Vaswani et  al., 2017 ), researchers \ntried to introduce Transformer into the field of vision ( Dosovitskiy \net  al., 2020 ). Vision Transformer (ViT), the first purely \nTransformer-based architecture, is proposed to perform image \nrecognition task using 2D image patches with positional \nembeddings as input and performed on large datasets. When \ntraining, ViT performs on par with CNN-based methods. In 2021, \nTransformers-based methods such as ( Zheng et al., 2021 ) were first \napplied to semantic segmentation. By modeling the global context \nof each layer of the Transformer, this encoder can be combined \nwith a simple decoder to provide a powerful segmentation model. \nFurthermore, the Data Efficient Image Transformer (DeiT; \nTouvron et al., 2020 ) also shows that Transformer can be trained \non medium-sized datasets. To ease the difficulty of training ViT, \nDeiT describes several training strategies to make ViT train well \non ImageNet. Using the Swin Transformer as the visual backbone, \nLiu et al. (2021)  achieved state-of-the-art performance in image \nclassification, object detection, and semantic segmentation. Cao \net al. proposed Swin-Unet ( Cao et al., 2022 ) to leverage the power \nof the Swin Transformer for medical image segmentation. ( Han \net al., 2021 ; Liu et al., 2021 ; Valanarasu et al., 2021 ; Wang et al., \n2021; Xie et al., 2021 ; Zhang et al., 2021 , 2022; Guo et al., 2022 ) \nleverage features from Transformers and CNNs to improve \nsegmentation models. Various combinations of Transformers and \nCNN are currently applied to multimodal brain tumor \nsegmentation ( Wang et  al., 2021a ) and 3D medical image \nsegmentation ( Peiris et al., 2021 ).\nCommon attention mechanisms usually focus on relationships in \nspace and channels, capturing only local dependencies and failing to \nexploit the multi-scale contextual information around the target \nregion fully. Therefore, multi-scale self-attention is crucial for \ncapturing richer feature dependencies. There have been many attempts \nto apply Transformers to the field of medical image segmentation. \nHowever, little attention is paid to segmentation boundary \ninformation, and one of the key elements of segmentation is the \nsegmentation edge.\n2.3 The method of contextual feature \npyramid\nMulti-scale feature fusion has been extensively studied and proven \neffective for dense prediction tasks (Cai et al., 2016; Zhao et al., 2016; \nChen Y . et al., 2017). The feature pyramid has the characteristics of \ndifferent resolutions at different scales, and objects of different sizes \ncan have appropriate feature representations at the corresponding \nscales. By fusing multi-scale information, objects of different sizes at \ndifferent scales can be  predicted, significantly improving the \nmodel’s performance.\nThere are roughly two ways to construct existing feature pyramids. \nThe first is to generate layers of different resolutions through multiple \ndownsampling, which is widely used. The more common applications \nare FPN (Lin et al., 2016) and YOLO_v3 (Redmon and Farhadi, 2018). \nFPN adopts the idea of divide and conquer, which means detecting \nlarge objects in the higher layers of the pyramid and small objects in \nthe lower layers of the pyramid. The second comprises multiple \nbranched convolutions with different void fractions and is currently \nused in ASPP , RFP , etc. Chen L. et al. (2018) proposed Atrous Spatial \nPyramid Pooling (ASPP) to robustly segment objects by capturing \nimage context at multiple scales. A simple approach is to resample the \ninput image into a multi-resolution input pyramid, feed it to multiple \nor shared networks, and then aggregate the output features (Tompson \net al., 2014; Chen L. et al., 2017; Chen C. et al., 2021). Feature pyramids \nfuse multi-scale features via pyramid pooling (Schlemper et al., 2019) \nor ASPP spatial pyramid pooling.\nConvolutional neural network gathers information from \nneighboring pixels and loses spatial information due to pooling \noperations. Therefore, it is difficult for CNN to learn global and long-\nrange semantic information interactions. Some studies have attempted \nto address this problem by using atrous convolutional layers, self-\nattention mechanisms (Zhao et al., 2016; Wang et al., 2017), and image \npyramids. The multi-scale fusion based on the semantic map proposed \nin this paper is a global fusion of space and semantics, and different \nscale features at any location can help each other. Therefore, the \nnetwork structure of the feature pyramid can handle the multi-scale \nvariation problem in object detection with a slight increase in the \namount of computation. It will be beneficial to apply it to the field of \nimage segmentation.\n3 Proposed method\nThe general architecture of CPFTransformer proposed in this \npaper is shown in Figure 1. CPFTransformer consists of an encoder, a \ndecoder, and two Edge-Aware modules. The reason there are three EA \nmodules in Figure 1 is that EA modules can be theoretically added to \nthese three positions. However, in section 4.5.2, it is proved that \nadding only the first two modules has the best effect. Therefore, in \nFigure 1, we mark the third module with a dotted line. The basic unit \nof the encoder and decoder is the Swin Transformer block ( Liu \net al., 2021).\nFirst, the encoder segments the input medical image into 4 × 4 \nnon-overlapping patch blocks and then projects the feature \ndimensions to arbitrary dimensions (denoted as C) through a linear \nembedding layer. The transformed token (patch token) is passed \nthrough a four-layer Swin Transformer block and a patch merging \nlayer to generate hierarchical features. Specifically, the patch merging \nlayer is responsible for downsampling and adding dimensionality, \nand the Swin Transformer block is responsible for feature learning. \nInspired by U-Net, a symmetric decoder based on Swin Transformer \nis designed, which consists of Swin Transformer blocks and patch \nexpanding layers. In contrast to the patch merging layer, the patch \nexpanding layer is specifically designed to perform upsampling, it \nreshapes the feature maps of adjacent dimensions into an upsampled \nfeature map with two times resolution. A final patch expanding layer \nis then used to perform 4× upsampling, recovering the image’s \nresolution and mapping the features to the input resolution size \n(W × H). Then a linear projection layer is used to output pixel-level \nsegmentation predictions of these upsampled features.\nIn the middle of the encoder-decoder structure, instead of \nconnecting with a simple jump connection, this paper introduces a \nkind of contextual pyramid module for connection, fusing the \nextracted contextual features with multi-scale features through the \nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 04 frontiersin.org\nEdge-Aware module to compensate for the loss of spatial information \nat multiple scales, better extracting local features such as edges and \ncorners (Sun et al., 2022), and providing the decoder with different \nlevels of global contextual information by reconstructing the \njump connection.\n3.1 Swin Transformer block\nThe Swin Transformer block differs from the traditional \nMultiheaded Self-Attention (MSA) block in that it is constructed \nbased on a shift window and comprises two consecutive \nTransformers. Each Swin Transformer block consists of a \nLayerNorm (LN) layer, a multiheaded self-attentive module, a \nresidual connection, and a two-layer MLP with GELU nonlinearity. \nThe two consecutive Transformer blocks employ a window-based \nmultiheaded self-attentive (W-MSA) module and a shift-window-\nbased multiheaded self-attentive (SW-MSA) module. Based on such \na window division mechanism, the Swin Transformer block can \nbe represented as:\n \nzW MS AL Nz zl ll = ()( )+−−- 11\n (1)\n \nzM LP LN zzl l l=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n +\n \n(2)\n \nzS WM SA LN zzl ll + = ()( )+1 -\n (3)\n \nzM LP LN zzl ll+ ++=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n +1 11\n \n(4)\nwhere z land MLP represent the output blocks of the (S)W-MSA \nmodule and the MLP , respectively.\nSelf-attention is calculated as follows:\n \nAttentionQ KV SoftMax QK\nd\nBV\nT\n,,() =+\n\n\n\n\n\n  \n(5)\nwhere QK VR Md,, ∈ ×\n2\n denotes the query, key, and value matrices. \nM 2 and d denote the number of patches in a window and the \ndimension of a query or key, respectively. Moreover, the values in B \nare taken from the bias matrix BR MM ∈ −() ×+()21 21 .\n3.2 Encoder\nA medical image is partitioned into non-overlapping patches of \nsize 4 × 4. First, C-dimensional tokens with a resolution of HW\n44×  are \nfed into two consecutive Swin Transformer blocks for representation \nlearning, where the feature dimension and resolution are kept \nFIGURE 1\nThe structure of CPFTransformer, which consists of an encoder, two EA modules and a decoder. The encoder and decoder are constructed based on \nthe Swin Transformer block. The EA module is composed of a multi-scale contextual pyramid module.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 05 frontiersin.org\nconstant. By this partitioning method, the feature dimension of each \npatch is 44 34 8×× = . In addition, a linear embedding layer is used to \nproject the feature dimension in an arbitrary dimension (denoted by \nC). The transformed patches are passed through several Swin \nTransformer blocks and the patch merging layer to generate \nhierarchical feature representations. At the same time, the patch \nmerging layer reduces the number of tokens (2× downsampling) and \nincreases the feature dimension to two times the original one. This \nprocess will be repeated four times in the encoder. The patch merging \nlayer is responsible for downsampling and dimensionality increase, \nand the Swin Transformer block is used for feature \nrepresentation learning.\n3.2.1 Patch merging layer\nThe input patches are divided into four parts and connected by a \npatch merging layer. By doing this, the feature resolution is \ndownsampled by a factor of 2. Also, since the concatenation operation \nresults in a 4-fold increase in feature dimensionality, a linear layer is \napplied to the concatenated features to unify the feature dimensionality \nto two times the original dimensionality. The patch merging layer \nachieves the downsampling and feature dimension increase without \nconvolution or interpolation operations.\n3.3 Decoder\nCorresponding to the encoder, a symmetric decoder is \nconstructed, which is also built based on the Swin Transformer block. \nTherefore, corresponding to the patch merging layer used in the \nencoder, we build a patch expanding layer in the decoder for extracting \ndepth features for upsampling. The patch expanding layer reshapes the \nfeature map of the adjacent dimension into a higher-resolution feature \nmap (2× upsampling). Accordingly, it reduces the feature dimension \nto half of the original dimension. Finally, a 4× upsampling is \nperformed using the network structure’s last patch expanding layer to \nrecover the image’s resolution features mapped to the input resolution, \nwhich is then passed through a linear projection layer for these \nupsampled features to output pixel-level segmentation predictions.\n3.3.1 Patch expanding layer\nUsing the first patch expanding layer as an example, a linear layer \nis applied to the input features WH C32 32 8××\n\n\n  to increase the feature \ndimension to twice the original dimension WH C32 32 16××\n\n\n  before \nupsampling. Then, we use a rearrangement operation to extend the \nresolution of the input features to twice the input resolution and \nreduce the feature size to one-fourth of the input size \nWH C WH C32 32 16 16 16 4×× →× ×\n\n\n. The patch expanding layer is the inverse \noperation of the patch merging layer. For the patch merging layer in \nencoder, in this paper, the patch expanding layer is specially designed \nin decoder for upsampling and feature dimension increase.\n3.4 Edge-aware module\nTo address the limitations of fixed-scale convolutional operations \nin target segmentation, this paper designs an EA module to improve \nthe robustness of the network structure through information at \ndifferent scales. In addition, skip connections, commonly used in \nU-shaped networks, introduce irrelevant confusion and semantic gaps \nwhen the receiving fields do not match ( Wang et al., 2022). In this \npaper, the EA module of the global context pyramid structure is \nproposed to solve these problems, as shown in Figure 2. In the EA \nmodule, the jump connections are reconstructed. And during the \ndecoding process, the shallow features display detailed boundary \ninformation and also bring some background noise. Therefore, the EA \nmodule is used to extract edge features and further guide the decoder, \nwhile suppressing shallow noise and refining the contours of \nthe object.\nIn this block, we use convolutional kernel operations of different \nsizes to extract features of different scales in multiple branches, \nenabling the network to learn more contextual information by fusing \nspatial information of different granularity. Based on the U-shaped \nstructure, this paper first designs multiple EA modules between the \nencoder and decoder, aiming to provide the decoder with different \nlevels of global contextual multi-scale information by reconfiguring \nthe hopping connections. It is well known that higher-level features \nexpress more semantic information, while shallow-level features carry \nmore details. Therefore, the features of shallow depth trunks are used \nto generate edge features. The proposed EA module extracts as many \nshallow features as possible for generating edge features.\nIn the EA module, the feature maps of this stage are combined \nwith the feature maps of all higher stages to reconstruct the skip \nconnections. Take the Stage 2 EA module for example, as shown in \nFigure  2, the features of each stage are first mapped to the same \nchannel space as Stage 2 in a 3 × 3 convolution. In detail, features F3 \nand F4 are upsampled to the same size as F2 and connected. In order \nto extract global contextual information from different levels of feature \nmaps, three separable convolutions ( Chollet, 2016 ) (Dsconv@1, \nDsconv@2, and Dsconv@4) with different dilation rates (1, 2, and 4) \nare used in parallel, where separable convolutions are used to reduce \nthe model parameters. It is worth noting that the number rate of \nparallel paths and expansions varies with the number of fusion stages. \nFinally, the final feature map is obtained by convolution. The EA \nmodule can be summarized for each stage (regular convolution is \nignored to simplify the formulation):\n GC DC Fk ik\ni sconv ik\nik\ni k ik=⊗ ()( )( )=\n=−\n=\n=−55 22@  (6)\nwhere Gk is the insertion of stage k, Fk is the feature map of stage \nk in the encoder, ⊗ −2ik  is the upsampling operation rate for 2ik− , c \ndenotes the concatenation operation and D sconv ik@2 −  is the \nseparable dilation convolution expansion rate for 2ik− .\nTo reduce the computational cost, the network in this paper uses \nonly two EA modules. The global semantic high-level information \nflow can be  gradually directed to different stages by introducing \nencoders and decoders between multiple EA modules.\n3.5 Loss function\nOne of the main challenges in medical image segmentation is the \nimbalance of classification distribution. Traditional methods generally \nemploy Dice loss or cross-entropy to perform the segmentation task. \nLoss functions widely used in convolutional neural network (CNN) \nsegmentation, such as Dice loss or cross-entropy loss, which typically \nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 06 frontiersin.org\nevaluate pixel-level accuracy, are based on the integration (summation) \nof the segmented region without global constraints on the \nsegmentation shape. Models trained in this way often fail to produce \ncomplete segmentations with distribution shifts. For highly \nunbalanced segmentations, the values of these region losses vary \nsignificantly across segmentation classes, often by several orders of \nmagnitude, which may affect training performance and stability. \nMoreover, the loss functions, such as BCE, IoU, and Dice, which are \nwidely used in current segmentation tasks, do not penalize the \nincorrect segmentation of the boundaries.\nTherefore, to further optimize the model in this paper, a boundary \nloss (Kervadec et al., 2021) is introduced in this paper, which appears \nas a distance metric on contour (or shape) space instead of regions. \nThis alleviates the difficulty of region loss under highly unbalanced \npartitioning problems because it uses the integral over the region \nboundary (interface) rather than the unbalanced integral over the \nregion. In addition, the boundary loss provides information that \ncomplements the region loss.\nThe expression of the boundary loss used in this paper is \nas follows:\n \nBG qd qθφ() = ()∫\nΩ  \n(7)\nwhere s :Ω→ {}01,  is binary indicator function of region \nSs q: () = 1 if qS∈  belongs to the target and 0 otherwise. φG : Ω→  \ndenotes the level set representation of boundary ∂ () =− ()Gq DqGG: φ  \nif qG∈  and φGG qD q() = ()  otherwise.\nIn the experiments, the joint loss Ltotal consisting of Dice loss, \ncross-entropy loss, and boundary loss will be  used to perform all \nsegmentation tasks in this paper.\n4 Experiments\n4.1 Datasets\n4.1.1 Synapse multi-organ segmentation dataset\nThe dataset includes 3,779 axial abdominal clinical CT images of \n30 cases. 18 samples were divided into the training set and 12 samples \ninto the test set. The volume of each CT image consists of 85 ~ 198 \nslices of 512 × 512 pixels, with a voxel spatial resolution of \n([0.54 ~ 0.54] × [0.98 ~ 0.98] × [2.5 ~ 5.0]) mm3. In this paper, the \naverage Dice similarity coefficient (DSC) and average Hausdorff  \ndistance (HD) on eight abdominal organs (aorta, gallbladder, spleen, \nleft kidney, right kidney, liver, pancreas, and stomach) were used as \nevaluation indexes.\n4.1.2 ACDC dataset\nACDC is a public cardiac MRI dataset that includes a sample of \n100 cases. A series of short-axis slices covering the heart from the base \nof the left ventricle to the apex with a thickness of 5–8 mm. The spatial \nresolution in the short-axis plane ranges from 0.83 to 1.75 mm pixel2 / , \ncorresponding to labels including left ventricle (LV), right ventricle \n(RV), and myocardium (MYO). The dataset was divided into 70 \ntraining samples (1,930 axial slices), 10 validation samples, and 20 \ntest samples.\n4.2 Experimental setup\n4.2.1 Environment\nCPFTransformer is implemented based on Python 3.6 and \nPytorch 1.7.0. For all training cases, data enhancements such as flips \nand rotations are used to increase the diversity of the data. The input \nFIGURE 2\nThe Edge-Aware module. Taking the reconstructed skip connection at stage 2 as an example, by integrating the global context, the global information \nflow is transmitted from a higher stage (stage 3 and 4) to the decoder.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 07 frontiersin.org\nimage size is set to 224 × 224, and the patch size is set to 4. The model \nin this paper is trained on an Nvidia V100 GPU with 32 GB of memory.\nDuring the training period, the batch size is 24 and an SGD \noptimizer with a momentum of 0.9 and a weight decay of 1e−4 is used \nto optimize the back propagation model in this paper.\n4.2.2 Evaluation metrics\nTwo types of metrics are used in this paper to evaluate all models.\nDice similarity coefficient (DSC) for evaluating the degree of \noverlap between prediction and ground truth segmentation map:\n \nDSC PG\nPG= ∩\n+\n2\n \n(8)\nwhere P is the predicted segmentation map and G is the \nground truth.\nHausdorff distance (HD), which measures the maximum \nsymmetric distance between two segmentation maps:\n \ndP Gd pg dp gH\npP gG gG pP\n,, ,,() = () ()\n\n\n\n\n\n∈ ∈ ∈ ∈\nma xs up infs up inf\n \n(9)\nwhere d .()  is the Euclidean distance, sup and inf  are the upper \nand lower extremes, respectively. In this paper, 95% HD is used to \neliminate the effect of a minimal subset of outliers.\n4.3 Experiment results on synapse dataset\nOn the Synapse multi-organ CT dataset, a comparison of the \nCPFTransformer proposed in this paper with previous state-of-the-art \nmethods is shown in Table 1. The experimental results show that the \nsegmentation accuracy of the Swin Transformer fused multi-scale \ncontextual pyramid-based method in this paper achieves 79.87% \n(DSC↑) and 20.83% (HD ↓). Compared with Unet and the recent \nTransUnet, Swinunet, and MTunet (Wang et al., 2021) methods, the \nalgorithm in this paper has a slight improvement in the DSC \nevaluation metric and HD evaluation metric, which indicates that the \nmethod in this paper can achieve better segmentation edge prediction.\nThe segmentation results of different methods on the Synapse \nmulti-organ CT dataset are shown in Figure 3. The figure shows that \nthe CNN-based methods tend to have over-segmentation problems, \nwhich may be due to the local nature of convolutional operations. This \npaper demonstrates a medical image segmentation network based on \nSwin Transformer’s contextual pyramid fusion multi-scale feature \ncombination. The network captures rich multi-scale contextual \ninformation using pyramid structure fusion. It computes the \nconvolution of local correlation between adjacent pixels, which \nperforms well in extracting local features such as edges and corners to \nobtain better segmentation results.\nTable 1 provides a quantitative evaluation of the experimental \nresults. U-net is the original method for generating adversarial \nnetworks; Transunet uses the encoding structure of Transformer’s \ngenerative model and the decoding structure of CNN; Swinunet is the \ncodec structure using the pure Transformer method; CPFTransformer \nis the method proposed in this paper. The table shows that using the \ncontextual pyramid structure combined with the Swin Transformer \nmethod, DSC is improved to 79.87%, and HD is improved to 20.83%. \nThe experiments demonstrate that introducing the contextual pyramid \nmechanism in the Swin Transformer network can effectively and \nprecisely target the edges to accomplish the task of abdominal multi-\norgan segmentation.\nTo verify the performance of the model, it was compared with an \nexpert segmentation approach, i.e., labeling. Then the pre-processed \nslicing results were reduced to the nii format dataset by 3D \nreconstruction to compare the segmentation results under 3D data. \nFigure 4 shows the segmentation results of the model under different \nlayers of the Synapse dataset. The first column is the original image \nafter slicing the Synapse dataset, the second column is the labeling \nresult, the third column is the segmentation result of the network \nmodel proposed in this paper, and the fourth column is the \nsegmentation result of the 3D reconstruction. As can be seen from the \nfigure, by comparing with the results of expert segmentation in the \nsecond column, the experimental segmentation results of this paper \nin the third column are segmented precisely in detail and similar to \nthe labeled segmentation results. Moreover, the edges are rounded \nafter the 3D reconstruction of the sliced data.\n4.4 Experiment results On ACDC dataset\nLike the Synapse dataset, the proposed CPFTransformer is in the \nACDC dataset to perform medical image segmentation. The \nTABLE 1 Quantitative comparison of synapse dataset using the most advanced algorithm.\nMethod DSC (%) HD (mm) Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach\nU-net 76.85 39.70 89.07 69.72 77.77 68.60 93.43 53.98 86.67 75.58\nR50U-Net 74.68 36.87 84.18 62.84 79.19 71.29 93.35 48.23 84.41 73.92\nAtt-UNet 77.77 36.02 89.55 68.88 77.98 71.11 93.57 58.04 87.30 75.75\nTransunet 77.48 31.69 87.23 63.13 81.87 77.02 94.08 55.86 85.08 75.62\nMTunet 78.59 26.59 87.92 64.99 81.47 77.29 93.06 59.46 87.75 76.81\nSwinunet 79.13 21.55 85.47 66.53 83.28 79.61 94.29 56.58 90.66 76.60\nCPFTrans-\nformer\n79.87 20.83 87.71 68.78 83.19 79.15 94.37 58.47 90.35 76.93\nColumns 4–11 represent the Deice similarity coefficients obtained on each organ. The best values are bolded.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 08 frontiersin.org\nFIGURE 4\nSynapse segmentation results after 3D reconstruction.\nFIGURE 3\nVisual comparison of segmentation results of different methods in Synapse dataset. The first column is the original image, and the second column is \nthe ground truth. The area highlighted by the red box shows that CPFTransformer performs better segmentation than other most advanced methods \nand results, and the segmentation edge is closer to the basic facts.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 09 frontiersin.org\nexperimental results are summarized in Table 2. Using MRI pattern \nimage data as input, CPFTransformer can still achieve excellent \nperformance with an accuracy of 90.00%. This indicates that our \nmethod has good generalization ability and robustness. As shown in \nFigure 5, the first column is the original image after slicing the ACDC \ndataset; the second column is the labeling result, the third column is \nthe segmentation result of the network model proposed in this paper, \nand the fourth column is the segmentation result of 3D reconstruction. \nAs can be seen from the figure, by comparing the results with those of \nthe labeled results in the second column, the experimental \nsegmentation results of this paper in the third column are segmented \naccurately in detail, basically similar to the labeled segmentation \nresults. The edge rounding can be seen after the 3D reconstruction of \nthe sliced data.\n4.5 Ablation experiment\nTo investigate the effect of different factors on the model \nperformance, an ablation study was conducted on the Synapse dataset \nin this paper. Same as training, 18 samples are used for training and \n12 samples are used for testing.\n4.5.1 The effect of position of the EA module\nThe EA module in this paper was added to the 1/4, 1/8, and 1/16, \n1/32 resolution scales to explore the effect of position on segmentation \nperformance. The segmentation performance of the model can \nbe  seen in Figure  6. Positions 1 and 2 generally perform more \naccurately than position three. Therefore, to make the model more \nrobust, the case of position three will not be used in this paper.\nTABLE 2 Quantitative comparison of ACDC dataset using the most advanced algorithm.\nMethod DSC (%) RV Myo LV\nR50AttnUNet (Chen C. et al., 2021) 86.90 83.27 84.33 93.53\nR50 ViT (Wang et al., 2021b) 86.19 82.51 83.01 93.05\nTransUnet (Chen J. et al., 2021) 89.71 86.67 87.27 95.18\nSwinUnet (Zheng et al., 2021) 88.07 85.77 84.42 94.03\nCPFTransformer 91.36 88.76 90.01 96.06\nColumns 3–5 represent the Deice similarity coefficients obtained on each organ. The best values are bolded.\nFIGURE 5\nACDC segmentation results and segmentation results after 3D reconstruction.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 10 frontiersin.org\nFIGURE 6\nAblation experiment of synapse dataset. EA1, EA2, and EA3, respectively, represent the effect between different positions where the edge sensing \nmodule is located as stage 1, stage 2, and stage 3.\n4.5.2 The effect of number of the EA module\nThe discussion of the number of EA modules in this paper \nincorporates location restrictions so that there can be a maximum \nof three blocks and a minimum of zero blocks. The effect of the \nnumber on the segmentation performance is explored. The \nsegmentation performance of the model can be seen quantitatively \nin Table  3. With three EA modules causing redundancy and \nmediocre results, the effect of two blocks is generally better than the \none block. Therefore, two EA modules will be used in this paper to \nmake the model more robust.\nIn this paper, through ablation experiments, we conclude about \nthe effect of the number of EA modules. Adding three EA modules \ncauses too much redundancy in the extracted information, resulting \nin poor results, and segmenting the edges of low-dimensional \ninformation features is more beneficial. When adding EA1 and EA2 \nmodules in the low-dimensional features, the effect is better, and the \nhigh-resolution low-level features and low-resolution high-level \nfeatures are fused to help delineate the detailed boundary.\n4.5.3 Effect of loss function\nThe loss functions used in this paper include Dice loss and cross-\nentropy loss commonly used in segmentation and boundary loss is \nintroduced. After performing ablation experiments on the three \nlosses, as shown in Table  4, it is found that in order to make the \nTABLE 3 Ablation experiment of synapse dataset.\nNumber of modules DSC (%) HD (mm)\nThree (EA1, 2, 3) 76.34 25.37\nTwo (EA1, 2) 79.87 20.53\nTwo (EA1, 3) 78.82 23.24\nTwo (EA2, 3) 77.65 23.07\nOne (EA1) 76.34 25.37\nOne (EA2) 77.37 23.53\nOne (EA3) 75.82 23.24\nZero 79.13 21.55\nThe effect of application and network between different EA modules and different locations. The best values are bolded.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 11 frontiersin.org\nsegmentation effect better, this paper will adopt a joint training \nmethod for the three losses.\n4.5.4 Discussion\nAlthough CPFTransformer can achieve good results in medical \nimage segmentation, it also has a significant disadvantage. Compared \nwith the traditional convolutional medical image segmentation \nmodels, the Transformer model combined with the edge-aware \nmodule of the context pyramid, although it integrates more global \nmulti-scale context information, it also leads to a slower overall \nconvergence speed and training. Time becomes longer.\nThere are two main directions for the future of this research. First, \nwe will explore compressing CPFTransformer to eliminate redundant \nparameters and reduce computational overhead while maintaining its \neffectiveness. Finally, CPFTransformer is designed based on 2D \nimages, but 3D medical images have essential application value. In the \nfuture, CPFTransformer will be further improved to make it suitable \nfor 3D medical image segmentation tasks.\n5 Conclusion\nThis paper proposes an edge-aware medical image segmentation \nnetwork (CPFTransformer) using Swin Transformer. We design an \nEdge-Aware module based on a context pyramid to fuse global \nmulti-scale context information, mainly for local features such as \ncorners. It focuses on addressing the weakness of global multi-scale \ncontextual information capture and integration in U-shaped \nnetworks, and a novel Edge-Aware module is inserted into the \nU-shaped framework with a context-pyramid-based boundary-\naware module to develop and fuse rich global multi-scale context \ninformation. Fusing high-resolution low-level features and \nlow-resolution high-level features helps delineate detailed \nsegmentation edges.\nThis paper conducts comprehensive experiments on different \ntypes of medical image segmentation tasks to verify the effectiveness \nand generality of the proposed CPFTransformer, including the \nabdominal multi-organ segmentation task and ACDC dataset. \nExperiments on segmentation tasks show that the edge-aware context \npyramid network based on Swin Transformer performs better. Our \nproposed CPFTransformer achieves excellent and consistent \nperformance on two different segmentation tasks, which indicates that \nthe proposed CPFTransformer is more practical and scalable than the \nothers. Our method can achieve better performance with further \nprocessing and can be extended to other medical image segmentation \ntasks, which is our recent work.\nData availability statement\nPublicly available datasets were analyzed in this study. The data \nare publicly available at https://www.synapse.org (Synapse) and \nhttps://acdc.creatis.insa-lyon.fr (ACDC).\nAuthor contributions\nJL: Conceptualization, Methodology, Project administration, \nSoftware, Writing – original draft, Writing – review & editing. JY: \nProject administration, Validation, Writing – original draft. RZ: \nSupervision, Writing – review & editing. YW: Supervision, Writing \n– review & editing. GB: Supervision, Writing – review & editing. HD: \nFormal analysis, Funding acquisition, Resources, Supervision, Writing \n– review & editing. HS: Project administration, Validation, Writing \n– review & editing.\nFunding\nThe author(s) declare financial support was received for the \nresearch, authorship, and/or publication of this article. This research \nwas funded by the Central Guided Local Science and Technology \nDevelopment Fund of Shanxi Province, project name: research on key \ntechnologies for improving low-quality image quality, grant number \nYDZJSX2022A016 and the general program of the National Natural \nScience Foundation of China, project name: research on the method \nof cross species comparison between human and macaque based on \nhigh-precision characteristics of brain images, grant number 61976150.\nConflict of interest\nThe authors declare that the research was conducted in the \nabsence of any commercial or financial relationships that could \nbe construed as a potential conflict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors \nand do not necessarily represent those of their affiliated organizations, \nor those of the publisher, the editors and the reviewers. Any product \nthat may be evaluated in this article, or claim that may be made by its \nmanufacturer, is not guaranteed or endorsed by the publisher.\nTABLE 4 Ablation experiment of synapse dataset.\nLoss DSC (%) HD (mm)\nLce 76.47 29.25\nLdice 75.23 31.39\nLce + Ldice 78.96 21.32\nLce + Lb 77.98 26.93\nLdice + Lb 76.70 25.37\nLce + Ldice + Lb 79.87 20.53\nDifferent effects of loss function application and network. The best values are bolded.\nLi et al. 10.3389/fnins.2023.1288366\nFrontiers in Neuroscience 12 frontiersin.org\nReferences\nAlom, M.Z., Hasan, M., Y akopcic, C., Taha, T.M., and Asari, V .K. (2018). Recurrent \nresidual convolutional neural network based on U-net (R2U-net) for medical image \nsegmentation. arXiv [Preprint]. doi: 10.48550/arXiv.1802.06955\nCai, Z., Fan, Q., Feris, R.S., and Vasconcelos, N. (2016). “ A unified multi-scale deep \nconvolutional neural network for fast object detection” in European conference on \ncomputer vision, 354–370.\nCao, H., Wang, Y ., Chen, J., Jiang, D., Zhang, X., Tian, Q., et al. (2022). “Swin-unet: \nUnet-like pure transformer for medical image segmentation” in Computer Vision—\nECCV 2022 Workshops, 205–218.\nCarion, N., Massa, F ., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020). \n“End-to-end object detection with transformers” in Computer vision—ECCV. 12346.\nChen, C., Fan, Q., and Panda, R. (2021). “CrossViT: cross-attention multi-scale vision \ntransformer for image classification” in 2021 IEEE/CVF International Conference on \nComputer Vision (ICCV), 347–356.\nChen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., et al. (2021). TransUNet: \nTransformers make strong encoders for medical image segmentation. arXiv [Preprint]. \ndoi: 10.48550/arXiv.2102.04306\nChen, L., Papandreou, G., Kokkinos, I., Murphy, K. P ., and Yuille, A. L. (2018). DeepLab: \nsemantic image segmentation with deep convolutional nets, Atrous convolution, and fully \nconnected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. 40, 834–848. doi: 10.1109/\nTPAMI.2017.2699184\nChen, L., Papandreou, G., Schroff, F ., and Adam, H. (2017). Rethinking Atrous convolution \nfor semantic image segmentation. arXiv [Preprint]. doi: 10.48550/arXiv.1706.05587\nChen, Y ., Wang, Z., Peng, Y ., Zhang, Z., Yu, G., and Sun, J. (2017). “Cascaded pyramid \nnetwork for multi-person pose estimation” in 2018 IEEE/CVF Conference on Computer \nVision and Pattern Recognition, 7103–7112.\nChollet, F . (2016). “Xception: deep learning with Depthwise separable convolutions” \nin 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , \n1800–1807.\nÇiçek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., and Ronneberger, O. (2016). “3D \nU-net: learning dense volumetric segmentation from sparse annotation” in International \nConference on Medical Image Computing and Computer-Assisted Intervention, 9901, \n424–432\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, et al. \n(2020). An image is worth 16x16 words: Transformers for image recognition at scale. \narXiv [Preprint]. doi: 10.48550/arXiv.2010.11929\nGuo, S., Sheng, S., Lai, Z., and Chen, S. (2022). “Trans-U: transformer enhanced U-net \nfor medical image segmentation” in International Conference on Computer Vision, Image \nand Deep Learning & International Conference on Computer Engineering and Applications \n(CVIDL & ICCEA), 628–631.\nHan, K., Xiao, A., Wu, E., Guo, J., Xu, C., and Wang, Y . (2021). Transformer in \ntransformer. arXiv [Preprint]. doi: 10.48550/arXiv.2103.00112\nHatamizadeh, A., Y ang, D., Roth, H.R., and Xu, D. (2021). “UNETR: transformers for \n3D medical image segmentation” in 2022 IEEE/CVF Winter Conference on Applications \nof Computer Vision (WACV), 1748–1758.\nHuang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y ., et al. (2020). “UNet 3+: a full-\nscale connected UNet for medical image segmentation” in ICASSP  2020–2020 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), 1055–1059.\nIbtehaz, N., and Rahman, M. S. (2020). MultiResUNet: rethinking the U-net \narchitecture for multimodal biomedical image segmentation. Neural Netw 121, 74–87. \ndoi: 10.1016/j.neunet.2019.08.025\nKervadec, H., Bouchtiba, J., Desrosiers, C., Granger, E., Dolz, J., and Ayed, I. B. (2021). \nBoundary loss for highly unbalanced segmentation. Med. Image Anal. 67:101851. doi: \n10.1016/j.media.2020.101851\nLin, T., Dollár, P ., Girshick, R.B., He, K., Hariharan, B., and Belongie, S.J. (2016). \n“Feature pyramid networks for object detection” in 2017 IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 936–944.\nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., et al (2021). “Swin transformer: \nhierarchical vision transformer using shifted windows” in 2021 IEEE/CVF International \nConference on Computer Vision (ICCV), 9992–10002.\nPeiris, H., Hayat, M., Chen, Z., Egan, G.F ., and Harandi, M. (2021). “ A robust \nvolumetric transformer for accurate 3D tumor segmentation” in International Conference \non Medical Image Computing and Computer-Assisted Intervention.\nRedmon, J., and Farhadi, A. (2018). YOLOv3: An Incremental Improvement. arXiv \n[preprint]. doi: 10.48550/arXiv.1804.02767\nRonneberger, O., Fischer, P ., and Brox, T. (2015). U-net: convolutional networks for \nbiomedical image segmentation. Med. Image Comput. Comput. Assist. Interv.  9351, \n234–241. doi: 10.1007/978-3-319-24574-4_28\nSchlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., et al. (2019). \nAttention gated networks: learning to leverage salient regions in medical images. Med. \nImage Anal. 53, 197–207. doi: 10.1016/j.media.2019.01.012\nSun, Y ., Wang, S., Chen, C., and Xiang, T. (2022). Boundary-guided camouflaged \nobject detection. arXiv [preprint]. doi: 10.48550/arXiv.2207.00794\nTompson, J., Goroshin, R., Jain, A., LeCun, Y ., and Bregler, C. (2014). “Efficient object \nlocalization using convolutional networks” in 2015 IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 648–656.\nTouvron, H., Cord, M., Douze, M., Massa, F ., Sablayrolles, A., and J'egou, H. (2020). \n“Training data-efficient image transformers & distillation through attention” in \nInternational Conference on Machine Learning.\nValanarasu, J., Oza, P ., Hacihaliloglu, I., and Patel, V .M. (2021). “Medical transformer: \ngated axial-attention for medical image segmentation” in International Conference on \nMedical Image Computing and Computer-Assisted Intervention.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., et al. (2017). \nAttention is all you need. Adv. Neural Inf. Proces. Syst. 30, 6000–6100.\nWang, H., Cao, P ., Wang, J., and Zaiane, O. R. (2022). Uctransnet: rethinking the skip \nconnections in u-net from a channel-wise perspective with transformer. Proc. AAAI \nConf. Artif. Intellig. 36, 2441–2449. doi: 10.1609/aaai.v36i3.20144\nWang, W ., Chen, C., Ding, M., Li, J., Yu, H., and Zha, S. (2021a). “TransBTS: \nmultimodal brain tumor segmentation using transformer” in International Conference \non Medical Image Computing and Computer-Assisted Intervention.\nWang, X., Girshick, R.B., Gupta, A.K., and He, K. (2017). “Non-local neural \nnetworks” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. \n7794–7803.\nWang, W ., Xie, E., Li, X., Fan, D., Song, K., and Liang, D. (2021b). “Pyramid vision \ntransformer: a versatile backbone for dense prediction without convolutions” in IEEE/\nCVF International Conference on Computer Vision (ICCV), 548–558.\nWang, H., Xie, S., Lin, L., Iwamoto, Y ., Han, X., Chen, Y ., et al. (2021). “Mixed \ntransformer U-net for medical image segmentation” in ICASSP  2022–2022 IEEE \nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP) , \n2390–2394\nXiao, X., Lian, S., Luo, Z., and Li, S. (2018). “Weighted res-UNet for high-quality \nretina vessel segmentation” in 2018 9th International Conference on Information \nTechnology in Medicine and Education (ITME), 327–331.\nXie, Y ., Zhang, J., Shen, C., and Xia, Y . (2021). CoTr: Efficiently bridging CNN and \ntransformer for 3D medical image segmentation.  arXiv [Preprint]. doi: \n10.1007/978-3-030-87199-4_16\nYuan, F ., Zhang, Z., and Fang, Z. (2023). An effective CNN and transformer \ncomplementary network for medical image segmentation. Pattern Recogn. 136:109228. \ndoi: 10.1016/j.patcog.2022.109228\nZhang, Y ., Liu, H., and Hu, Q. (2021). TransFuse: Fusing transformers and CNNs for \nmedical image segmentation. arXiv [Preprint]. doi: 10.1007/978-3-030-87193-2_2\nZhang, J., Liu, Y ., Wu, Q., Wang, Y ., Liu, Y ., Xu, X., et al. (2022). SWTRU: star-shaped \nwindow transformer reinforced U-net for medical image segmentation. Comput. Biol. \nMed. 150:105954. doi: 10.1016/j.compbiomed.2022.105954\nZhao, H., Shi, J., Qi, X., Wang, X., and Jia, J. (2016). “Pyramid scene parsing network” \nin 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , \n6230–6239.\nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., et al. (2021). “Rethinking \nsemantic segmentation from a sequence-to-sequence perspective with transformers” \nin 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , \n6877–6886.\nZhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., and Liang, J. (2018). “UNet++: a nested \nU-net architecture for medical image segmentation” in Deep Learning in Medical Image \nAnalysis and Multimodal Learning for Clinical Decision Support: 4th International \nWorkshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018 , held in \nconjunction with MICCAI 2018, Granada, Spain, S."
}