{
  "title": "Multi-encoder Transformer Network for Automatic Post-Editing",
  "url": "https://openalex.org/W2902385320",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2123687210",
      "name": "Jaehun Shin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096652942",
      "name": "Jong Hyeok Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2758488395",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2758074402",
    "https://openalex.org/W2251994258",
    "https://openalex.org/W3006530332",
    "https://openalex.org/W2964034111",
    "https://openalex.org/W2758520323",
    "https://openalex.org/W2756923230",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2902170865",
    "https://openalex.org/W2963344439",
    "https://openalex.org/W2963816901",
    "https://openalex.org/W2963261349",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W2251816503"
  ],
  "abstract": "This paper describes the POSTECH’s submission to the WMT 2018 shared task on Automatic Post-Editing (APE). We propose a new neural end-to-end post-editing model based on the transformer network. We modified the encoder-decoder attention to reflect the relation between the machine translation output, the source and the post-edited translation in APE problem. Experiments on WMT17 English-German APE data set show an improvement in both TER and BLEU score over the best result of WMT17 APE shared task. Our primary submission achieves -4.52 TER and +6.81 BLEU score on PBSMT task and -0.13 TER and +0.40 BLEU score for NMT task compare to the baseline.",
  "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 2: Shared Task Papers, pages 840–845\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64097\n \n \nAbstract \nThis paper describes the POSTECH’s sub-\nmission to the WMT 2018 shared task on \nAutomatic Post -Editing (APE). We pro-\npose a new neural end-to-end post-editing \nmodel based on the t ransformer network. \nWe modified the encoder-decoder attention \nto reflect the relation between the machine \ntranslation output, the source and the post-\nedited translation in APE problem. Experi-\nments on WMT 17 English-German APE \ndata set show an improvement in both TER \nand BLEU score over the best result of \nWMT17 APE shared task.  Our primary \nsubmission achieves -4.52 TER and +6.81 \nBLEU score on PBSMT task and -0.13 \nTER and +0.40 BLEU score for NMT task \ncompare to the baseline. \n1 Introduction \nAlthough machine translation technology has im-\nproved, machine translation output inevitably in-\nvolves errors and the type of errors in the output \nvaries depending on the machine translation sys-\ntem. Correcting those systematic errors inside the \nsystem may cause other problems such as increase \nof the decoding  complexity (Chatterjee et al., \n2015). For this reason , A utomatic Post -Editing \n(APE) is suggested as an alternative to enhance the \nperformance of the machine translation. \nAPE aims at the automatic correction of system-\natic errors in the machine translation output with-\nout any modification of the original machine trans-\nlation system (Bojar et al, 2015; Bojar et al, 2016; \nBojar et al, 2017). Basically, APE problem can be \ndefined as a translation  problem from machine \ntranslation output (mt) to post-edited sentence (pe), \nbut source sentence (src) is used as an additional \nsource for the problem. As a result, APE problem \nbecomes a multi-source translation problem be-\ntween two sources (mt, src) and a target (pe). \nDue to the additional source, APE has two trans-\nlation directions, the  mt→pe direction and the  \nsrc→pe direction. Previous researches have sug-\ngested various methods to combine the two direc-\ntions with neural network architecture, such as log-\nlinear combination of two  translation model s \n(Junczys-Dowmunt and Grundkiewicz, 2016), fac-\ntored translation model (Hokamp, 2017) and multi-\nencoder architecture (Libovický et al., 2016; Chat-\nterjee et al., 2017; Junczys-Dowmunt and Grund-\nkiewicz, 2017; Variš and Bojar, 2017). \nAmong the methods, we focus on the multi-en-\ncoder approach because it is more appropriate to \nmodel the multi-source translation problem. Also, \nconsidering the importance of proper attention \nmechanism, as shown in the research of Junczys-\nDowmunt and Grundkiewicz ( 2017), we use the \ntransformer network (Vaswani et al., 2017) com-\nposed of a novel attention mechanism. \nWith this consideration, our submission to the \nWMT 2018 shared task on Automatic Post-Editing \nis a neural multi-encoder model based on the trans-\nformer network. We extend the  transformer net-\nwork implementation in Tensor2Tensor (Vaswani \net al., 2018) library to implement our model. We \nparticipated in both PBSMT task and NMT task \nwith this multi-encoder model. \nIn this paper, we introduce the multi-encoder \ntransformer network for APE. The remainder of \nthe paper is organized as follows: Section 2 con-\ntains the related work. Section 3 describes our \nmethod. Section 4 gives the experimental results, \nand Section 5 is the conclusion. \n2 Related Work \n2.1 Multi-Encoder Architecture \nFor a multi-source translation problem, the proper \nmodeling of the relation between the multiple \nsources and the target  is important. Combining \ntwo separate single-source translation models for \nMulti-encoder Transformer Network for Automatic Post-Editing \n \n \n \nJaehun Shin and Jong-hyeok Lee \nDepartment of Computer Science and Engineering, \nPohang University of Science and Technology \n{jaehun.shin, jhlee}@postech.ac.kr \n \n \n840\n \n \neach source -target relation  (Junczys-Dowmunt \nand Grundkiewicz, 2016) or constructing single \ninput by combining the all sources (Hokamp, 2017) \nmay be a solution , but these are not the exactly \nmodeling the multi-source translation problem. \nZoph and Knight (2016) proposed the basic \nmodel of  the multi-source translation problem. \nTheir multi-encoder architecture  uses trilingual \ndata and contains separate encoders for each input \nto model the conditional probability of the ta rget \nover the two sources.  Libovický et al. ( 2016) \nshowed the application of this multi-encoder archi-\ntecture to model APE problem. They used the same \narchitecture in both APE task and multi -modal \ntranslation task, because the two tasks can be de-\nfined as multi-source translation problem. \nAlthough their model did not show a good result \nin the competition, the idea of multi-encoder archi-\ntecture succeeded in the following WMT evalua-\ntion (Chatterjee et al., 2017 ; Junczys-Dowmunt \nand Grundkiewicz, 2017; Variš and Bojar., 2017) \nand achieved good results. \n2.2 Transformer Network \nTransformer network is a  novel neural machine \ntranslation architecture proposed by Vaswani et al. \n(2017), which avoids recurrence and convolution \nand focuses on the attention mechanism. The net-\nwork utilizes  an encoder-decoder architecture  \nbased on the stacked layers and each layer uses a \nnew novel attention mechanism called multi-head \nattention. \nMulti-head attention is a variation of scaled dot-\nproduct attention. It employs a number of attention \nheads for information from different representation \nsubspaces at different positions. With this charac-\nteristic, multi-head attention can model  the de-\npendency between tokens regardless of their dis-\ntance up to the number of heads. \nTransformer network uses the multi-head atten-\ntion in three different ways: self -attention in en-\ncoder, masked self-attention in decoder, and en-\ncoder-decoder attention. The self-attention and the \nmasked self-attention model the internal depend-\nency of the input and the output respectively, and \nthe encoder-decoder attention models the depend-\nency between the input and the output. \nWith this attention mechanism, transformer net-\nwork achieved the state -of-the-art result on the \nWMT 2014 English -to-German and English -to-\nFrench translation tasks, and were faster to train \nthan other prior models (Vaswani et al., 2017).  \n3 Multi-Encoder Transformer Network \nIn a normal multi-source translation problem, all of \nthe sources and the target are assumed to be a dif-\nferent representation of a common abstracted \nmeaning. However, in APE problem, we cannot \nadopt this assumption because the machine trans-\nlation output is considered to have systematic er-\nrors. These errors make a gap between the machine \ntranslation o utput and the post -edited sentence . \nTherefore, for APE problem, we should aim to re-\nduce the gap, not to find the common abstracted \nmeaning. In this intuition, the t hree directions \nshould be considered to model the APE problem, \nsentence c orrection (mt→pe), ideal translation \n(src→pe), and original translation (src→mt). \nEven though Bérard et al. (2017) used a chained \narchitecture for the context information of original \ntranslation, most of previous approaches focused \non combining sentence correction and ideal trans-\nlation. However, in terms of reducing the gap, APE \nproblem is close to modeling the relation between \noriginal translation and ideal translation, rather \nthan the relation between the machine translation \noutput and the post-edited sentence. \nOur multi-encoder transformer network is based \non this idea. Figure 1 illustrates the overall archi-\ntecture of our multi-encoder transformer network \nFigure 1: The overall architecture of multi-encoder \ntransformer network for automatic post-editing task. \n841\n \n \nfor APE problem. We extend transformer network \nto have two encoders, one for the machine transla-\ntion output and the other for the source sentence. \nEach encoder has its own self-attention layer and \nfeed-forward layer to process each input separately. \nAlso, we add two multi-head attention layers to de-\ncoder, one for  original translation  dependency \n(src→mt) and another for ideal translation depend-\nency ( src→pe). After these attention  layers, the \nwords common to both the machine translation \noutput and the post-edited sentence have similar \ndependency on the source sentence, so those com-\nmon words obtain similar source contexts. Then \nwe apply multi-head attention between the output \nof those attention layers, expecting that the source \ncontext helps the decoder to recognize those com-\nmon words which should be remained in post-ed-\nited sentence. \nIn short, we added the second encoder for the \nsource sentence t o the transformer network and \nmodified the encoder-decoder attention structure \nto reflect the relation between the original transla-\ntion and the ideal translation. \n4 Experimental Results \n4.1 Data \nWe used WMT’18 official data set (Chatterjee et \nal., 2018) for PBSMT task and NMT task individ-\nually. The official PBSMT data set consists of \ntraining data, development data and two test data \n(2016, 2017), and the official NMT dataset consists \nof training data and development data. \nWe adopted the artificial training data (Junczys-\nDowmunt and Grundkiewicz, 2016 ) as an addi-\ntional training data for both tasks. Table 1 summa-\nrizes the statistic of the data sets. In addition, the \nartificial-small data set is the subset of the artifi-\ncial-large data set. \n4.2 Training Parameters \nWe used the base model parameters of transformer \nnetwork: 6 stacks, 8 heads, 512 hidden dimension, \n2,048 feed-forward dimension, 64 key dimension, \n64 value dimension, dropout probabilities 0.1 and \nAdam optimization with β1=0.9, β2=0.997 and \nε=10-9. \nWe built a shared word piece vocabulary with \nsize of 216 from the combined set of PBSMT train-\ning data set and artificial-large data set for PBSMT \nmodel. For NMT model, we used the combined set \nof official data and artificial-small data to build the \nvocabulary, with consideration of the difference \nbetween two tasks. \nFor training, we used a mini batch size of 2,048 \nwith max sequence length of 256 and initial learn-\ning rate of 0.2. We set warmup steps to 16k and \ntrained the model during 160k steps. Model check-\npoints were saved every 1,000 mini batch es. We \nselect this model as our base model. \n4.3 Tuning \nAfter 160k steps of training, we tuned the base \nmodel in two step. For the first tuning step, we re-\nduced the training data to the sum of  the official \ntraining data set and artificial-small data set. We \ntrained the base model on the reduced training data \nduring 30k steps more and selected the model with \nthe lowest validation loss (1st-tuned). \nFor the second tuning step, we used the official \ntraining data to fine-tune the 1st-tuned model. We \nused the same tuning method with 1k training step. \nThe model with lowest validation was selected as \nthe final model (2nd-tuned). \n4.4 Evaluation \nWe evaluated the models using the WMT data set, \ncomputing the  TER ( Snover et al ., 2006 ) and \nBLEU (Papineni et al., 2002 ) scores on the de-\ncoded output. The decoding parameter is the same \nas the default decoding parameter of the Ten-\nsor2tensor. We used the scores of original machine \ntranslation output as the baseline to compare our \nresults. Table 2 shows the results of the evaluation \non PBSMT data set and NMT data set. \nThe result on PBSMT data set is comparable to \nthe last year ’s top result without  any additional \npost-processing. In contrast, the result on NMT \ndata set shows almost no improvement. We guess \nthat the different characteristics of PBSMT artifi-\ncial data set from the NMT training data set causes \nthe result. \nTask Data set Sentences TER \nPBSMT \ntraining set 23,000 25.35 \ndevelopment set 1,000 24.81 \ntest set 2016 2,000 24.76 \ntest set 2017 2,000 24.48 \nartificial-small 526,368 25.55 \nartificial-large 4,391,180 35.37 \nNMT training set 13,442 14.89 \ndevelopment set 1,000 15.08 \nTable 1: Statistics for WMT APE data sets. \n \n842\n \n \n4.5 Submitted System \nWe used checkpoint averaging to make an ensem-\nble model for submission candidates. For the better \nresult, we used various checkpoint saving frequen-\ncies in the second tuning step and trained the model \nfive times for each frequency . Then, we applied \ncheckpoint averaging on the models with follow-\ning conditions: top-5 models (top5), top-5 models \nin a fixed checkpoint frequency (fix5), five top-1 \nmodels for various checkpoint frequencies (var5). \nWe used TER score on the development data set to \nselect the models. In addition, we chose the top-1 \nmodel to the submission candidate. Table 3 sum-\nmarizes the result of the four submission candi-\ndates on both PBSMT and NMT data set. For the \nsubmission, we chose three models with low TER \nscore and high BLEU score.  \nTable 4 shows the official result of the submitted \nmodel on WMT18 test data set. Our primary sub-\nmission for PBSMT achieves -4.52 TER and +6.81 \nBLEU scores and our primary submission on NMT \ntask -0.13 TER and +0.40 BLEU scores compare \nto the baseline. \n5 Conclusion \nIn this paper, we propose  a multi-encoder trans-\nformer network for APE task . We modified the \nstructure of encoder-decoder attention to reflect the \nrelation between machine translation output, \nsource sentence and post-edited sentence in APE. \nOur multi-encoder model showed a comparable re-\nsult to the top result of last year’s competition on \nPBSMT task, although almost no improvement on \nNMT task. \nTask Systems TER↓ BLEU↑ \nPBSMT \nWMT18-Baseline 24.24  62.99  \nPRIMARY (top5) 19.72  69.80  \nCONTRASTIVE1 (fix5) 19.63  69.87  \nCONTRASTIVE2 (var5) 19.74  69.70  \nNMT \nWMT18-Baseline 16.84  74.73  \nPRIMARY (fix5) 16.71  75.13  \nCONTRASTIVE1 (top1) 16.70  75.14  \nCONTRASTIVE2 (var5) 16.71  75.20  \nTable 4: The official results of the submitted models to WMT18 APE task.. \n \nmodel \nPBSMT   NMT \ndev test 2016 test 2017  dev \nTER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑ \nMutli-T2T_top5-avg 18.87  71.72  19.15  70.88  18.82  70.86    14.97  77.22  \nMutli-T2T_fix5-avg 18.88  71.68  19.22  70.80  18.90  70.78   14.96  77.25  \nMutli-T2T_var5-avg 18.85  71.83  19.19  70.75  18.85  70.68   14.97  77.25  \nMutli-T2T_top1 18.91  71.66  19.23  70.78  18.91  70.74    14.94  77.26  \nTable 3: The results of submitted models on WMT APE data set. \nmodel \nPBSMT   NMT \ndev test 2016 test 2017  dev \nTER↓ BLEU↑ TER↓ BLEU↑ TER↓ BLEU↑   TER↓ BLEU↑ \nMT Baseline 24.81  62.92  24.76  62.11  24.48  62.49   15.08  76.76  \nMulti-T2T_base  22.80  66.36  22.70  65.84  22.98  65.46   16.73  74.43  \nMulti-T2T_1st-tuned 21.11  68.78  21.20  67.95  21.64  67.33   15.76  76.02  \nMulti-T2T_2nd-tuned 19.05  71.79  19.14  70.98  19.26  70.50    15.27  76.88  \nChatterjee et al. (2017)* 19.22  71.89  19.32  70.88  19.60  70.07    ─ ─ \nTable 2: The result of multi-encoder transformer network on WMT APE data set. \n843\n \n \nAcknowledgments \nThis work was supported by Institute for Infor-\nmation & communications Technology Promotion \n(IITP) grant funded by the Korea government  \n(MSIT) (R7119-16-1001, Core technology devel-\nopment of the real-time simultaneous speech trans-\nlation based on knowledge enhancement). \nReferences  \nAlexandre B érard, Laurent Besacier, and Olivier \nPietquin. 2017. LIG -CRIStAL Submission for the \nWMT 2017 Automatic Post -Editing Task. In Pro-\nceedings of the Second Conference on Machine \nTranslation, pages 623-629. \nOndřej Bojar, Rajen Chatterjee, Christian Federmann,  \nBarry Haddow, Matthias Huck, Chris Hokamp,  \nPhilipp Koehn, Varvara Logacheva, Christof Monz, \nMatteo Negri, Matt Post, Carolina Scarton, Lucia  \nSpecia, and Marco Turchi. 2015. Findings of the  \n2015 workshop on statistical machine translation. In \nProceedings of the Tenth Workshop on Statistical \nMachine Translation . Association for Computa -\ntional Linguistics, Lisbon, Portugal, pages 1-46. \nOndřej Bojar, Rajen Chatterjee, Christian Federmann, \nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Loga-\ncheva, Christof Monz, Matteo Negri, Aurelie \nNeveol, Mariana Neves, Martin Popel, Matt Post, \nRaphael Rubino, Carolina Scarton, Lucia Specia, \nMarco Turchi, Karin Verspoor, and Marcos Zampi-\neri. 2016. Findings of the 2016 conference on ma-\nchine translation. In Proceedings of the First Con-\nference on Machine Translation . Association for \nComputational Linguistics, Berlin, Germany, pages \n131–198. \nOndřej Bojar, Rajen Chatterjee, Christian Federmann, \nYvette Graham , Barry Haddow, Shujian Huang, \nMatthias Huck, Philipp Koehn, Qun Liu, Vavara \nLogacheva Christof Monz, Matteo Negri, Matt Post, \nRaphael Rubino, Lucia Specia, Marco Turchi. 2017. \nFindings of the 2017 conf erence on machine trans-\nlation (wmt17). In Proceedings of the Second Con-\nference on Machine Translation, pages 169-214. \nRajen Chatterjee, Marion Weller, Matteo Negri, and  \nMarco Turchi. 2015 . Exploring the Planet of  the \nAPEs: a  Comparative Study of State -of-the-art \nMethods for MT Automatic Post -Editing. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language \nProcessing (Volume 2: Sh ort Papers) , pages 156 -\n161. \nRajen Chatterjee, Amin Farajian, Matteo Negri, Marco \nTurchi, Ankit Srivastava, and Santanu Pal. 2017 . \nMulti-source Neural Automatic Post -Editing: \nFBK’s participation in the WMT 2017 APE shared \ntask. In Proceedings of the Second Conference on \nMachine Translation (Volume 2: Shared Task Pa-\npers), pages 630-638. \nRajen Chatterjee, Matteo Negri, R aphael Rubino and \nMarco Turchi. 2018. Findings of the WMT 2018 \nShared Task on  Automatic Post-Editing. In Pro-\nceedings of the Third Conference on Machine \nTranslation, Volume 2: Shared Task Papers . Asso-\nciation for Computational Linguistics, Brussels, \nBelgium.  \nChris Hokamp. 2017 . Ensembling Factored Neural \nMachine Translation Models for Automatic Post -\nEditing and Quality Estimation. In Proceedings of \nthe Second Conference on Machine  Translation, \npages 647-654. \nMarcin Junczys-Dowmunt and Roman Grundkiewicz.  \n2016. Log-linear combinations of monolingual and  \nbilingual neural machine translation models for au-\ntomatic post-editing. In Proceedings of the First \nConference on Machine Translation. Association \nfor Computational Linguistics, Berlin, Germany, \npages 751–758. \nMarcin Junczys-Dowmunt and Roman Grundkiewicz. \n2017. The AMU -UEdin Submission to the WMT \n2017 Shared Task on Automatic Post -Editing. In \nProceedings of the Second Conference on Machine \nTranslation, pages 639-646. \nJindřich Libovick ý, Jind řich Helcl, Marek Tlust ý, \nOndřej Boj ar, and Pavel Pecina. 2016. CUNI  sys-\ntem for wmt16 automatic post -editing and multi-\nmodal translation tasks. In Proceedings of the First \nConference on Mac hine Translation . Association \nfor Computational Linguistics, Berlin, Germany, \npages 646-654. \nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic \nevaluation of machine translation. In Proceedings of \nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311-318 \nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A Study of \nTranslation Edit Rate with Targeted Human Anno-\ntation. In Proceedings of Association for Machine \nTranslation in the Americas. V ol. 200, No. 6 \nDušan Variš and Ondřej Bojar. 2017. CUNI System for \nWMT17 Automatic Post-Editing Task. In Proceed-\nings of the Second Conference on Machine Transla-\ntion, pages 661-666. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N. Gomez,  Łukasz \nKaiser and Illia Polosukhin. 2017. Attention is all It \nshows that our multi-encoder model has a sufficient \n844\n \n \npotential to solve APE problem. you need. In Ad-\nvances in Neural Information Processing Systems, \npages 5998-6008. \nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet,  Aidan N. Gomez,  Stephan Gouws,  \nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki \nParmar, Ryan Sepassi,  Noam Shazeer, and  Jakob \nUszkoreit. 2018. Tensor2tensor for neural machine \ntranslation. arXiv preprint arXiv:1803.07416 . \nhttps://arxiv.org/abs/1803.07416 \nBarret Zoph and Kevin Knight. 2016. Multi -source \nneural translation. CoRR abs/1601.00710.  \nhttp://arxiv.org/abs/1601.00710. \n845",
  "topic": "BLEU",
  "concepts": [
    {
      "name": "BLEU",
      "score": 0.8546329736709595
    },
    {
      "name": "Computer science",
      "score": 0.8157516717910767
    },
    {
      "name": "Transformer",
      "score": 0.7831124663352966
    },
    {
      "name": "Encoder",
      "score": 0.7385424375534058
    },
    {
      "name": "Machine translation",
      "score": 0.7237017154693604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5738430619239807
    },
    {
      "name": "Task (project management)",
      "score": 0.5550633668899536
    },
    {
      "name": "Natural language processing",
      "score": 0.5511861443519592
    },
    {
      "name": "Speech recognition",
      "score": 0.4966035485267639
    },
    {
      "name": "Artificial neural network",
      "score": 0.44446659088134766
    },
    {
      "name": "Decoding methods",
      "score": 0.41240882873535156
    },
    {
      "name": "Algorithm",
      "score": 0.10834726691246033
    },
    {
      "name": "Voltage",
      "score": 0.06769353151321411
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}