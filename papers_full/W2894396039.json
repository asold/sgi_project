{
  "title": "Adaptive Pruning of Neural Language Models for Mobile Devices",
  "url": "https://openalex.org/W2894396039",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4289619967",
      "name": "Tang, Raphael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2672090663",
      "name": "Lin, Jimmy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2952165242",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2114766824",
    "https://openalex.org/W2964250984",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2786386501",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2419597278",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2545300838",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963748792"
  ],
  "abstract": "Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a \"knob\" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.",
  "full_text": "ADAPTIVE PRUNING OF NEURAL LANGUAGE MODELS\nFOR MOBILE DEVICES\nRaphael Tang & Jimmy Lin\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\nWaterloo, Ontario, Canada\n{r33tang,jimmylin}@uwaterloo.ca\nABSTRACT\nNeural language models (NLMs) exist in an accuracy–efﬁciency tradeoff space\nwhere better perplexity typically comes at the cost of greater computation com-\nplexity. In a software keyboard application on mobile devices, this translates into\nhigher power consumption and shorter battery life. This paper represents the ﬁrst\nattempt, to our knowledge, in exploring accuracy–efﬁciency tradeoffs for NLMs.\nBuilding on quasi-recurrent neural networks (QRNNs), we apply pruning tech-\nniques to provide a “knob” to select different operating points. In addition, we\npropose a simple technique to recover some perplexity using a negligible amount\nof memory. Our empirical evaluations consider both perplexity as well as energy\nconsumption on a Raspberry Pi, where we demonstrate which methods provide\nthe best perplexity–power consumption operating point. At one operating point,\none of the techniques is able to provide energy savings of 40% over the state of\nthe art with only a 17% relative increase in perplexity.\n1 I NTRODUCTION\nAn emerging application of neural language models (NLMs) is smart software keyboards on such\nmobile devices as smartphones and tablets that provide next-word prediction, allowing users to input\nentire words with a single tap. For example, the apps SwiftKey1 and Swype2 both advertise the use\nof neural networks for predictions. According to Google Play Store, SwiftKey has more than 100\nmillion downloads, demonstrating its popularity.\nBased on standard metrics such as perplexity, neural techniques represent an advance in the state\nof the art in language modeling (Merity et al., 2018b). Better models, however, come at a cost in\ncomputational complexity, which translates to higher power consumption. In the context of mo-\nbile devices, energy efﬁciency is, of course, an important optimization objective. A casual web\nsearch, for example, reveals numerous complaints from users of the above apps about battery drain,\nindicating that this is not a hypothetical concern.\nIn reality, neural language models exist in a accuracy–efﬁciency tradeoff space. Although this fact\nhas been recognized for applications such as image recognition (Canziani et al., 2016) and keyword\nspotting (Tang et al., 2018), to our knowledge no one in the NLP community has explored these\ntradeoffs. All previous papers on NLMs simply report single-point perplexity ﬁgures. In contrast,\nthe high-level goal of our work is to understand the tradeoffs between neural modeling accuracy and\nreal-world efﬁciency constraints: in addition to perplexity, NLMs should be evaluated in terms of\nFLOPs,3 milliJoule per query (mJ/q), and inference latency. We conduct exactly such experiments,\nusing the Raspberry Pi (which shares the same architecture as most mobile devices today) as a more\nconvenient hardware platform.\nIdeally, NLMs should provide a “knob” that allows developers to tune accuracy–efﬁciency tradeoffs.\nIn this paper, we explore pruning approaches that take a pre-trained quasi-recurrent neural network\n1http://www.swiftkey.com/\n2http://www.swype.com/\n3Convention from literature deﬁnes number of FLOPs as the total number of additions and multiplications.\n1\narXiv:1809.10282v1  [cs.CL]  27 Sep 2018\nThe\nquick\nbrown\nfox\njumps\nover\nthe\nlazy\ntime\nFigure 1: An illustration of the ﬁrst QRNN layer for language modeling. In this visualization, a\nQRNN layer with a window size of two convolves and pools using embeddings from the input. Note\nthe absence of recurrent weights.\n(QRNN; Bradbury et al., 2017), representing the state of the art in NLM today, and provides exactly\nsuch a knob. Furthermore, our techniques allow these tradeoffs to be tuned at inference time, which\nallows a mobile device to adaptively control its behavior, e.g., favor efﬁciency at the cost of accuracy\nwhen the battery is low.\nThus, this paper makes the following contributions: First, to our knowledge, we are the ﬁrst to\ncomprehensively explore accuracy–efﬁciency tradeoffs for NLMs with experimental evaluation of\nenergy consumption on a Raspberry Pi. Second, we evaluate a number of inference-time pruning\ntechniques that takes any pre-trained QRNN and provides a tunable accuracy–efﬁciency “knob”.\n2 B ACKGROUND AND RELATED WORK\n2.1 Q UASI -RECURRENT NEURAL NETWORKS\nQuasi-recurrent neural networks (QRNNs; Bradbury et al., 2017) achieve highly competitive per-\nplexity on word-level language modeling datasets, including state-of-the-art perplexity on WikiText-\n103 (Merity et al., 2018b). Although applying such techniques as dynamic evaluation (Krause et al.,\n2017), Hebbian softmax (Rae et al., 2018), and mixture of softmaxes (Yang et al., 2017) can pro-\nduce lower perplexity, our focus is on the recurrent architecture. Thus, we explore the task of pruning\nQRNNs without using any other extensions.\nEach word is encoded as a one-hot vector and then fed into a linear layer, which produces lower-\ndimensional word embeddings for the QRNN layers. A single QRNN layer consists of two distinct\ncomponents—convolution and recurrent pooling—that alternate to imitate an LSTM (Hochreiter &\nSchmidhuber, 1997). Given a stacked sequence of inputs X = x1 ⊕···⊕ xn ∈Rk×n (e.g., word\nembeddings in language modeling), the one-dimensional convolution layer is deﬁned as\nZ = tanh(Wz ·X)\nF = σ(Wf ·X)\nO = σ(Wo ·X)\nwhere Wz, Wf, Woare the weights associated with the input, forget, and output gates, respectively,\n·represents a masked convolution along time, and σdenotes the sigmoid function. For W{z,f,o}∈\nRm×(k×r), mis the number of output channels,kis the number of input channels, andrthe window\nsize across time. Without loss of generality, we henceforth represent W{z,f,o}as two-dimensional\nmatrices ∈Rm×s, where s= k×r. The outputs are fed into a recurrent pooling layer:\nct = ft ⊙ct−1 + (1 −ft) ⊙zt ht = ot ⊙ct\nwhere ⊙denotes element-wise product. Altogether, these two layers deﬁne a single QRNN\nlayer (Bradbury et al., 2017; see Figure 1). Multiple layers can be stacked for greater expressiveness,\nwhere the output h1:n of the previous layer is the input X to the current layer.\nWe tie the weights between the input and output layers, as used by Merity et al. (2018a) and proposed\nby Inan et al. (2017). In addition to improving perplexity, weight tying reduces the number of\nparameters and hence the memory footprint, which is beneﬁcial to our task.\n2\n2.2 P RUNING\nWeight pruning is an effective strategy for reducing the computational footprint of a model. An inﬂu-\nential pioneering work, LeCun et al. (1990) proposes to discard weights using a error-approximation\napproach based on Hessian diagonals. More recent work suggests pruning weights with small mag-\nnitudes (Han et al., 2016), with quantization and Huffman coding as additional steps. However,\nthese approaches introduce irregular sparsity to the weights, and they assume that re-training the\nweights is feasible.\nIn this work, we take a different approach and focus on techniques that eliminate entire ﬁlters.\nThis is because modern implementations of feedforward evaluation (e.g., im2col and particularly\nNEON instruction on ARM processors) take advantage of dense matrix multiplications. Pruning\nindividual weights without changing the dimensions of the weight matrices has minimal effect on\npower consumption—this is conﬁrmed by our initial exploratory studies on the Raspberry Pi. Hence,\nwe only examine pruning techniques that discard entire ﬁlters of the convolutional layers:\nRandom pruning. A simple baseline (Mittal et al., 2018) is random ﬁlter pruning, where n% of\nthe ﬁlters are randomly pruned, layer-by-layer. Interestingly, Mittal et al. (2018) ﬁnd that random\npruning is competitive with more advanced methods.\nFilter norm.Li et al. (2017) propose ranking ﬁlters by theirL1-norms, and then dropping offn% of\nthe smallest ﬁlters on a layer-by-layer basis. Mittal et al. (2018) have previously found thatL1-norm\nﬁlter pruning (Li et al., 2017) outperforms a multitude of competing approaches.\nMean activation norm.Among other approaches, Molchanov et al. (2016) suggest pruning ﬁlters\nwhose mean activations are small. This approach is especially effective on ReLU, which both creates\nsparse activations and forces them to be non-negative.\nL0 regularization. Louizos et al. (2018) applyL0 regularization to neural networks in order to learn\nsparse, efﬁcient structures. Formally, deﬁne an objective\nR(θ) = L(θ) + λ∥θ∥0 θ∗= arg min\nθ\nR(θ)\nwhere Lis the original loss function and θ the weights. The dependence on the hypothesis and\ntraining examples has been omitted for brevity. The optimal solution entails a non-differentiable\nobjective and iteration over all 2|θ|possibilities to choose the best θ∗; hence, Louizos et al. (2018)\npropose the following relaxation of the objective:\nˆR(θ,φ) = Ez∼p(z|φ) [L(θ⊙z)] + λ\n|θ|∑\ni=1\n(1 −Q(zi ≤0; φi))\nθ∗,φ∗= arg min\nθ,φ\nˆR(θ,φ)\nwhere z ∼p(z|φ) is a binary discrete random mask parameterized by φ, and Qis the CDF. Intu-\nitively, for some choice ofφ, the number of active parameters (on average) is penalized. Inspired by\nthe Concrete distribution (Maddison et al., 2016), Louizos et al. (2018) propose the hard concrete\ndistribution for z, further relaxing the discrete random mask into a continuous one:\ns = σ((log u −log(1 −u) + logα)/β)\nz = min(1,max(0,(ζ−γ)s + γ))\nwhere u ∈R|θ|is a continuous random vector such that ui ∼Uniform [0,1], φ = log αare the\nmask parameters, and γ = −0.1,ζ = 1.1,β = 2/3 are scaling hyperparameters. Note that β can\nalso be included as part of the mask parameters φ; we follow Louizos et al. (2018) and ﬁx β = 2/3.\nLouizos et al. (2018) then apply the reparameterization trick (Kingma & Welling, 2014; Rezende\net al., 2014) and make a Monte Carlo approximation to the objective:\nˆR(θ,φ) = 1\nN\nN∑\ni=1\n(\nL(θ⊙z(i))\n)\n+ λ\n|θ|∑\ni=1\n(1 −Q(zi ≤0; φi))\n3\nA closed form expression is derived for the penalty, (1 −Q(zi ≤0; φi)) = σ(log αi −βlog −γ\nζ ).\nAt test time, the following estimator is used:\nz = min(1,max(0,σ(log α)(ζ−γ) + γ)\n3 I NFERENCE -TIME PRUNING\nIn this section, we explain how the various techniques in Section 2.2 can be adapted to QRNNs.\nFor the following methods, we assume that a pre-trained model is provided. We denote the weights\nat QRNN layer l as W(l). In all methods, we tie the indices across Wz,Wf,Wo. For example,\nif ﬁlter i is selected for pruning at layer l, then W(l)\n{z,f,o} := W(l)\n{z,f,o}[−i,:], where −i denotes\nexclusion of index i. This allows the removal of the column [:,−i] in the next layer as well.\nRandom pruning. We apply random pruning to Wz, Wf, and Wo. That is, we randomly prune\nﬁlters associated with the same indices across the three weights.\nFilter norm. We apply ﬁlter norm pruning (Li et al., 2017), with the ﬁlter norms of Wz acting as\nthe criteria. We ﬁnd Wz most helpful, since small ﬁlter norms should result in small hidden outputs,\nwhich is not necessarily the case for Wf and Wo.\nMean activation norm. The hidden output H = h1 ⊕···⊕ hn is a natural candidate for col-\nlecting mean activation statistics. Intuitively, if ∥H:,i∥1 is small on average, then the ith ﬁlters for\nWz,Wf,Wo are less important. Statistics are collected using a single pass of the entire training\nset. For inference-time pruning, we store the collected statistics.\nL0 regularization. Since we are given a pre-trained model and are prohibited from altering the\nweights, we learn the mask parameters only: φ∗= arg minφ ˆR(θ,φ). We also enforce the sparsity\non entire rows of Wz, which corresponds to “group sparsity” in Louizos et al. (2018). Speciﬁcally,\nwe formulate the regularization on a feature map level instead, with Z as the target:\nZ(l) :=\n(\ndiag(z(l))W(l)\nz\n)\n·X = Z(l) ⊙z(l)\nZ is chosen for the property that the ith feature map for h is zero if Zi is zero for c0 = 0.\nThis approach entails training and storing extra mask parameters for each operating point. However,\nwe ﬁnd this to be a non-issue for our task, since there are few operating points—three or four at\nmost, out of which we use two for L0 regularization—so the extra storage is negligible.\n3.1 W ITH SINGLE -RANK UPDATE\nAt speciﬁc operating points (e.g., 40% and 80% FLOPs), pre-trained weight updates can be stored\nand applied at inference-time to recover some perplexity. Suppose W ∈Rm×n is a weight matrix\nin a neural network, and W∗ ∈Rm×n is some known set of weights that results in a lower loss.\nClearly, ∆W := W∗−W can be stored and added at inference-time to obtain a better neural net-\nwork. However, it is obvious that this scheme is wasteful, since W∗could have directly substituted\nW in the ﬁrst place.\nSacriﬁcing a negligible amount of storage to recover some perplexity, we propose learning a single-\nrank weight matrix update\n∆W := uv⊺,u ∈Rm,v ∈Rn\nto each weight in the convolution layers. Speciﬁcally, the process is as follows, beginning with a\npre-trained model:\n1. Prune a pre-determined set of ﬁlters for some operating point (e.g., 40% FLOPs).\n2. Initialize the weight updates ∆Wl = u(l)v(l)⊺\n,u(l)\ni ,v(l)\ni ∼ p(ϵ) for each convolution\nlayer l, in our case Normal(0,0.1).\n3. Fixing the existing weights Wl for each convolution layer, train a single-rank update such\nthat W∗\nl := Wl + ∆Wl, where W∗\nl is used as the new weight.\n4. Store ∆Wl for use at inference time on the same operating point.\n4\n4 E XPERIMENTAL SETUP\nWe evaluate the aforementioned pruning techniques for word-level language modeling on Penn\nTreebank (PTB) (Marcus et al., 1993; as preprocessed by Mikolov et al., 2010) and WikiText-103\n(WT103) (Merity et al., 2017). We denote the models for PTB and WT103 as ptb-qrnn and\nwt103-qrnn, respectively.\n4.1 D ATASETS AND TASKS\nFor each model, we report word-level perplexity and recall-at-three (R@3), deﬁned as the percentage\nof top three token–logit outputs that contain the true next token. For example, if {“cat”, “dog”,\n“baby”}are the top three predicted tokens for, “I adopted a ,” with “dog” being the ground\ntruth, then the prediction is correct, regardless of the rank of “dog”.\nPenn Treebank.Built from Wall Street Journal articles, Penn Treebank (PTB) is a small yet popular\nword-level dataset for language modeling. In the standard pre-processed version (Mikolov et al.,\n2010), the dataset contains roughly 887K, 70K, and 78K training, validation, and testing tokens,\nrespectively. The number of unique tokens is capped at 10,000, yielding a relatively large 4.8%\nout-of-vocabulary (OOV) rate.\nWikiText-103. Merity et al. (2017) introduce WikiText-2 and WikiText-103, datasets based on\nfreely available Wikipedia articles. We use only WikiText-103, since WikiText-2 was designed to be\nsimilar to Penn Treebank. With 103 million training tokens, WikiText-103 is 103 times as large as\nPTB. WikiText-103 contains around 217K tokens for validation, and 245K for testing. The number\nof unique tokens is 267K, resulting in a 0.4% OOV rate, signiﬁcantly lower than that of PTB.\n4.2 H YPERPARAMETERS AND TRAINING\nIn all of the models, we chose the hyperparameters as suggested in Merity et al.’s codebase. 4\nFor ptb-qrnn, we used a four-layer QRNN with 1550 hidden units for each layer and a 400-\ndimensional embedding. For wt103-qrnn, we used a four-layer QRNN with 2500 hidden units\nand 400-dimensional embeddings, along with a tied adaptive softmax (Merity et al., 2018b). In both\nmodels, the ﬁrst layer uses a window size of two, while the rest use a windows size of one.\nFollowing Merity et al. (2018a), we also adopted the regularization techniques randomized back-\npropagation through time, embedding dropout, temporal activation regularization (TAR), activation\nregularization (AR), and variational dropout. We followed the same training process as well, with\nnon-monotonically triggered ASGD (NT-ASGD) as the optimizer. We use the same hyperparame-\nters as Merity et al. (2018a) and Merity et al. (2018b) for each model–dataset pair.\nDuring the training of wt103-qrnn, we follow Merity et al. (2018b), using a tied adaptive soft-\nmax (Grave et al., 2017; Merity et al., 2018b) layer. At inference time, we use a regular softmax\ninstead, since we require R@3.\nPruning. We selected a number of distinct operating points that represent discrete points in the\naccuracy–efﬁciency tradeoff space. Based on previous work (Tang et al., 2018), ﬂoating-point op-\nerations (FLOPs) is a good proxy of both energy usage and latency, and so we use FLOPs as a way\nof selecting our operating points. In L0 regularization, the λdecay strength was selected so that the\nresulting model corresponds to roughly the FLOPs targets: To achieve 80% and 60% FLOPs for the\nmodel on PTB, we used λ = 5.5 ×10−4,8.5 ×10−4, respectively. To achieve about 70% FLOPs\non WT103, we chose λ= 6 ×10−4.\nWe trained the hard concrete mask parameters for roughly 5000 steps using Adam with a learning\nrate of 5 ×10−3. Since the weight decay penalty is incompatible with the objective, we removed it\nwhile training the mask.\nFor mean activation pruning, which requires some training examples to collect statistics, we used\nthe entire training set forptb-qrnn. Since WikiText-103 is large, we used roughly 10% of the ﬁrst\ntraining examples for collecting statistics on wt103-qrnn.\n4https://github.com/salesforce/awd-lstm-lm\n5\nSingle-rank update (SRU).For the PTB model, the single-rank update was trained for 10 epochs\nusing NT-ASGD (Merity et al., 2018a) with a non-monotonic interval of three. For WikiText-103,\nthe update was trained for 2000 steps using Adam with a learning rate of 5 ×10−3. All other\nhyperparameters were the same as those used during the training stage.\n4.3 I NFRASTRUCTURE DETAILS\nWe trained all of our models on a commodity machine with a Titan V GPU, i7-4790k CPU, and 16\nGB of RAM. We used PyTorch 0.4.0 (commit 1807bac) for developing and running our models.\nWe deployed our models on a Raspberry Pi (RPi) 3 Model B (ARM Cortex-A53) running Raspbian\nStretch (4.9.41-v7+). Speciﬁcally, we copied the trained models over to the RPi, and ran them at the\nsame operating points accordingly.\nWe plugged the RPi into a Watts Up Pro meter, a wattmeter that reports power usage at the rate of\n1 Hz via a USB cable, which is connected back to the RPi. Evaluating on the test set, we collected\npower draw statistics on 350 next-word predictions, which were averaged to produce a millijoule\nper query (mJ/q) estimate. We obtained latency estimates in a similar manner by averaging the\nmilliseconds per query (ms/q). Finally, we subtracted off the idle power usage of the RPi to obtain a\nbetter estimate of the actual power for each query.\nAlthough our ﬁnal application is NLMs running on mobile devices such as smartphones and tablets,\nthere are many challenges to directly evaluating on such hardware. The Raspberry Pi is a convenient\nstand-in since it uses exactly the same ARM processor architecture as nearly all mobile devices\ntoday. Evaluation on the RPi is widely adopted for research on efﬁcient NNs today (Amato et al.,\n2017; Tang et al., 2018).\n5 R ESULTS AND DISCUSSION\nIn our results for PTB and WT-103, we compare to state-of-the-art results in the past. In general, we\nﬁnd that QRNNs are strong competitors to LSTM approaches, and achieve state-of-the-art perplexity\non WikiText-103 (Merity et al., 2018b).\n# Method\nModel Quality Footprint w/SRU\nVal. Test R@3 % FLOPs ms/q mJ/q Test R@3\n1 Skip LSTM 60.9 58.3 – – – – – –\n2 AWD-LSTM 60.0 57.3 – – 223 295 – –\n3 Orig. 59.0 56.8 44.7% 100% 224 296 – –\n4 L0 reg. 63.0 60.7 43.6% 80% 185 227 59.3 44.1%\n5 L0 reg. 69.2 66.8 42.1% 60% 142 183 64.0 42.7%\n6 Random 68.2 66.0 42.9% 80% 182 238 61.1 43.8%\n7 Filter norm 76.1 72.7 42.4% 80% 182 238 66.1 43.1%\n8 Mean activation 68.3 66.1 42.6% 80% 182 238 61.0 43.5%\nTable 1: Select pruning results on Penn Treebank using a 4-layer QRNN, along with past results\ndrawn from the original papers. Skip LSTM refers to the four-layer skip LSTM from Melis et al.\n(2018), and AWD-LSTM is from Merity et al. (2018a). The four-layer QRNN (Merity et al., 2018b)\nis the same model that we use, but we achieve better perplexity following the same methodology.\nThe best results of each category are bolded. “w/SRU” denotes the results after applying an SRU.\nFor PTB, we note that a 20-point increase in perplexity may only correspond to a few points decrease\nin R@3, showing that perplexity changes on a much different scale than accuracy does (see Table 1,\nrows 3 and 7). Furthermore, lower perplexity does not necessarily imply higher accuracy (see rows\n5 and 7), conﬁrming that perplexity alone cannot completely determine the recall. In Table 1, we\nchose 75 as the cutoff-point for perplexity—further results are illustrated in Figure 2. For WT-103,\nwe observe trends similar to those of PTB; A large drop in perplexity corresponds to a much smaller\ndecrease in R@3 (see Table 2, rows 3 and 4).\n6\n# Method\nModel Quality Footprint w/SRU\nVal. Test R@3 % FLOPs sec/q J/q Test R@3\n1 Rae-LSTM 36.0 36.4 – – – – – –\n2 4-layer QRNN 32.0 33.0 – – 1.24 1.48 – –\n3 Orig. 31.9 32.8 51.5% 100% 1.24 1.48 – –\n4 L0 reg. 65.8 65.4 43.1% 69% 0.912 1.06 56.9 44.7%\n5 Mean activation 89.8 92.9 38.9% 70% 0.942 1.10 55.7 46.0%\n6 Filter norm 85.9 88.2 41.7% 70% 0.942 1.10 59.2 45.4%\n7 Random 80.9 81.4 42.9% 70% 0.942 1.10 54.2 46.1%\nTable 2: Select pruning results on WikiText-103 using a 4-layer QRNN, along with past results,\ndrawn directly from the original papers. Note that Rae et al. (2018) primarily explore Hebbian\nsoftmax; Rae-LSTM refers to their LSTM model without any extensions. Bolded are the best results\nfor each category.\n5.1 A CCURACY –EFFICIENCY TRADEOFFS\nWe illustrate the accuracy–efﬁciency tradeoff space of the PTB and WT103 models in Figure 2. For\neach model, we tabulate the results at ﬁxed intervals according to the approximated percentage of\nFLOPs, relative to that of the unpruned model. We omit results that exceed 100 in test perplexity,\nsince they are insufﬁcient for language modeling in practice.\n40 50 60 70 80 90 100\n% FLOPs in full model\n60\n65\n70\n75\n80\n85\n90\n95\n100Word-level Perplexity\nPruning on QRNN for PTB\nL_0\nFN\nMA\nRand.\n60 70 80 90 100\n% FLOPs in full model\n40\n50\n60\n70\n80\n90\n100Word-level Perplexity\nPruning on QRNN for WikiText-103\nL_0\nFN\nMA\nRand.\nFigure 2: Full experimental results on Penn Treebank and WikiText-103. We illustrate the\nperplexity–efﬁciency tradeoff space on the test set obtained before applying the single-rank update.\nSurprisingly, random ﬁlter pruning is a strong baseline, which supports the ﬁndings from Mittal\net al. (2018). Random pruning not only outperforms ﬁlter norm and mean activation pruning, but\nalso regains perplexity more easily with a single-rank update. From Table 1 (rows 6–8) and Table 2\n(rows 5–7), we see that random pruning displays equivalent or superior performance to ﬁlter norm\nand mean activation pruning. Interestingly, random pruning achieves the lowest perplexity with a\nsingle-rank update (Table 2, rows 4–7), out of all the baseline approaches on WT103.\nOn the other hand, ﬁlter norm pruning is relatively weak, doing worse than random pruning in all\ncases—with or without a single-rank update—suggesting that ﬁlter norm pruning has no practical\nbeneﬁt over random pruning. L0 regularization (Louizos et al., 2018) works best, as shown in rows\n4–5 in Table 1 and row 4 in Table 2.\nIn general, testing on Penn Treebank and WikiText-103—two very different datasets—gives us con-\nsistent results, thus demonstrating the robustness of L0 regularization (Louizos et al., 2018), com-\npared to the other pruning approaches.\n7\nFigure 3: Illustration depicting pruning on a truncated subset of the ﬁrst layer’s weights from the\nPTB model, where each row corresponds to a different technique, and each column a different\noperating point. From left to right, the operating points are 100%, 80%, 70%, 60%, and 50% FLOPs.\nFor each of the subﬁgures, we concatenate from top to bottom the ﬁrst 25 ﬁlters of W{z,f,o}, and\nfrom left to right the ﬁrst 75 elements in each ﬁlter, yielding square visualizations. All the pruning\ntechniques appear to be dropping weights differently—we note that, for L0 regularization (row 4),\nthe dropped weights remain largely constant throughout.\n5.2 P OWER USAGE AND LATENCY\nOn the Raspberry Pi, the PTB models are relatively fast, while the WT103 models are high latency,\ntaking over one second (Table 2, rows 2–3 and 8) for the full models. For type-ahead prediction\non a mobile device, the WT103 models are unsuitable as-is; further steps (e.g., more pruning then\nre-training, vocabulary reduction, quantization) would be required to deploy the models for prac-\ntical use. Supporting the ﬁndings from Tang et al. (2018), the number of FLOPs scales linearly\nwith latency and power: Full experimental results from Figure 2 yield Pearson’s r2 = 0 .98 for\nboth latency– and power–FLOPs measurements, suggesting a strong linear relationship between the\nnumber of FLOPs and both latency and power.\nIn terms of extra parameters, a single-rank update costs less than 74 KB for ptb-qrnn, and less\nthan 120 KB for wt103-qrnn. Mean activation statistics requires 20 KB for ptb-qrnn, and 30\nKB for wt103-qrnn. Mask parameters for L0 regularization cost about 20 KB on each power\nlevel for ptb-qrnn, and 30 KB for wt103-qrnn. Filter norm pruning and random pruning do\nnot require any extra storage.\n6 C ONCLUSION\nMotivated by the mass adoption of smart software keyboards on mobile devices, we explore the\ntask of inference-time pruning on QRNNs, state-of-the-art neural language models. Starting with\nexisting training-time pruning methods, we extend their usability to QRNNs at run-time, obtaining\nmultiple operating points in the accuracy–efﬁciency tradeoff space. To recover some perplexity\nusing a negligible amount of memory, we propose to train and store single-rank weight updates at\ndesired operating points.\n8\nACKNOWLEDGMENTS\nWe are grateful for Meng Dong’s work on power measurements and debugging for the RPi experi-\nments, and we thank the reviewers for their time and feedback.\nREFERENCES\nGiuseppe Amato, Fabio Carrara, Fabrizio Falchi, Claudio Gennaro, Carlo Meghini, and Claudio\nVairo. Deep learning for decentralized parking lot occupancy detection. Expert Systems with\nApplications, 72:327–334, 2017.\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural net-\nworks. In International Conference on Learning Representations, 2017.\nAlfredo Canziani, Adam Paszke, and Eugenio Culurciello. An analysis of deep neural network\nmodels for practical applications. arXiv preprint arXiv:1605.07678, 2016.\n´Edouard Grave, Armand Joulin, Moustapha Ciss ´e, David Grangier, and Herv ´e J ´egou. Efﬁcient\nsoftmax approximation for GPUs. In Doina Precup and Yee Whye Teh (eds.), Proceedings of\nthe 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine\nLearning Research, pp. 1302–1310, International Convention Centre, Sydney, Australia, 06–11\nAug 2017. PMLR. URL http://proceedings.mlr.press/v70/grave17a.html.\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding. In International Conference on Learning\nRepresentations, 2016.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. InInternational Conference on Learning Representations,\n2017.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. InInternational Conference\non Learning Representations, 2014.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural\nsequence models. arXiv preprint arXiv:1709.07432, 2017.\nYann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural\ninformation processing systems, pp. 598–605, 1990.\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for\nefﬁcient convnets. In International Conference on Learning Representations, 2017.\nChristos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through\nl 0 regularization. In International Conference on Learning Representations, 2018.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\nG´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. In International Conference on Learning Representations, 2018.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In International Conference on Learning Representations, 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM\nlanguage models. In International Conference on Learning Representations, 2018a. URL\nhttps://openreview.net/forum?id=SyyGPP0TZ.\n9\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. An analysis of neural language modeling\nat multiple scales. arXiv preprint arXiv:1803.08240, 2018b.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. InEleventh Annual Conference of the International Speech\nCommunication Association, 2010.\nDeepak Mittal, Shweta Bhardwaj, Mitesh M Khapra, and Balaraman Ravindran. Recovering\nfrom random pruning: On the plasticity of deep convolutional neural networks. arXiv preprint\narXiv:1801.10447, 2018.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional\nneural networks for resource efﬁcient transfer learning. CoRR, abs/1611.06440, 2016.\nJack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. Fast parametric learning with\nactivation memorization. arXiv preprint arXiv:1803.10049, 2018.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. In Eric P. Xing and Tony Jebara (eds.), Pro-\nceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings\nof Machine Learning Research, pp. 1278–1286, Bejing, China, 22–24 Jun 2014. PMLR. URL\nhttp://proceedings.mlr.press/v32/rezende14.html.\nRaphael Tang, Weijie Wang, Zhucheng Tu, and Jimmy Lin. An experimental analysis of the power\nconsumption of convolutional neural networks for keyword spotting. In Proceedings of the 2018\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2018), pp.\n5479–5483, 2018.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax\nbottleneck: a high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\n10",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9920996427536011
    },
    {
      "name": "Computer science",
      "score": 0.7907955050468445
    },
    {
      "name": "Pruning",
      "score": 0.7739951610565186
    },
    {
      "name": "Language model",
      "score": 0.6685307621955872
    },
    {
      "name": "Mobile device",
      "score": 0.5845053195953369
    },
    {
      "name": "Artificial neural network",
      "score": 0.542369544506073
    },
    {
      "name": "Computation",
      "score": 0.5272142887115479
    },
    {
      "name": "Energy consumption",
      "score": 0.4357711672782898
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4304617941379547
    },
    {
      "name": "Software",
      "score": 0.42175763845443726
    },
    {
      "name": "Point (geometry)",
      "score": 0.4212188720703125
    },
    {
      "name": "Machine learning",
      "score": 0.37630143761634827
    },
    {
      "name": "Algorithm",
      "score": 0.12350675463676453
    },
    {
      "name": "Programming language",
      "score": 0.10517245531082153
    },
    {
      "name": "Engineering",
      "score": 0.07489877939224243
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 4
}