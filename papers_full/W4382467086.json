{
  "title": "Revisiting Classifier: Transferring Vision-Language Models for Video Recognition",
  "url": "https://openalex.org/W4382467086",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099340509",
      "name": "Wenhao Wu",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2558615216",
      "name": "Zhun Sun",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6793119350",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W3189981867",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6794053914",
    "https://openalex.org/W4304083971",
    "https://openalex.org/W3015538334",
    "https://openalex.org/W6756911974",
    "https://openalex.org/W2904378456",
    "https://openalex.org/W6771446814",
    "https://openalex.org/W2942642798",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W2987283559",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W3172726419",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4226283196",
    "https://openalex.org/W6785761836",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2126579184",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W2108950639",
    "https://openalex.org/W4312254032",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2990451341",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W4285606530",
    "https://openalex.org/W2786010684",
    "https://openalex.org/W4290055985",
    "https://openalex.org/W2761659801",
    "https://openalex.org/W2734663976",
    "https://openalex.org/W2934628279",
    "https://openalex.org/W2772114784",
    "https://openalex.org/W3113713694",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W3153751099",
    "https://openalex.org/W3111932027",
    "https://openalex.org/W2964436909",
    "https://openalex.org/W4313483457",
    "https://openalex.org/W4286750450",
    "https://openalex.org/W4286750519",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W6810371925",
    "https://openalex.org/W4223492139",
    "https://openalex.org/W6797602710",
    "https://openalex.org/W6891781542",
    "https://openalex.org/W4287203089",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W4307106676",
    "https://openalex.org/W3034658206",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2963689837",
    "https://openalex.org/W3204261852",
    "https://openalex.org/W2887051120",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W24089286",
    "https://openalex.org/W4312302951",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3207340843",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W3200114289",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2980037812",
    "https://openalex.org/W4225755514",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W4312266966",
    "https://openalex.org/W4313136445",
    "https://openalex.org/W3174568846",
    "https://openalex.org/W4312614039",
    "https://openalex.org/W4386072365",
    "https://openalex.org/W3175528717",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287115696",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W4386065852",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3174873881",
    "https://openalex.org/W3010010212",
    "https://openalex.org/W3176125528",
    "https://openalex.org/W4214746887",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4312480274",
    "https://openalex.org/W2996901793",
    "https://openalex.org/W4312658081",
    "https://openalex.org/W4237044863"
  ],
  "abstract": "Transferring knowledge from task-agnostic pre-trained deep models for downstream tasks is an important topic in computer vision research. Along with the growth of computational capacity, we now have open-source vision-language pre-trained models in large scales of the model architecture and amount of data. In this study, we focus on transferring knowledge for video classification tasks. Conventional methods randomly initialize the linear classifier head for vision classification, but they leave the usage of the text encoder for downstream visual recognition tasks undiscovered. In this paper, we revise the role of the linear classifier and replace the classifier with the different knowledge from pre-trained model. We utilize the well-pretrained language model to generate good semantic target for efficient transferring learning. The empirical study shows that our method improves both the performance and the training speed of video classification, with a negligible change in the model. Our simple yet effective tuning paradigm achieves state-of-the-art performance and efficient training on various video recognition scenarios, i.e., zero-shot, few-shot, general recognition. In particular, our paradigm achieves the state-of-the-art accuracy of 87.8% on Kinetics-400, and also surpasses previous methods by 20~50% absolute top-1 accuracy under zero-shot, few-shot settings on five video datasets. Code and models are available at https://github.com/whwu95/Text4Vis.",
  "full_text": "Revisiting Classifier: Transferring Vision-Language Models for Video Recognition\nWenhao Wu1, Zhun Sun2, Wanli Ouyang3*\n1The University of Sydney, NSW, Australia\n2Baidu Inc., Beijing, China\n3Shanghai Artificial Intelligence Laboratory, Shanghai, China\nwhwu.ucas@gmail.com, sunzhun@baidu.com, wanli.ouyang@sydney.edu.au\nAbstract\nTransferring knowledge from task-agnostic pre-trained deep\nmodels for downstream tasks is an important topic in com-\nputer vision research. Along with the growth of computa-\ntional capacity, we now have open-source vision-language\npre-trained models in large scales of the model architec-\nture and amount of data. In this study, we focus on trans-\nferring knowledge for video classification tasks. Conven-\ntional methods randomly initialize the linear classifier head\nfor vision classification, but they leave the usage of the text\nencoder for downstream visual recognition tasks undiscov-\nered. In this paper, we revise the role of the linear classifier\nand replace the classifier with different knowledge from the\npre-trained model. We utilize the well-pre-trained language\nmodel to generate a good semantic target for efficient trans-\nferring learning. The empirical study shows that our method\nimproves both the performance and the training speed of\nvideo classification, with a negligible change in the model.\nOur simple yet effective tuning paradigm achieves state-of-\nthe-art performance and efficient training on various video\nrecognition scenarios, i.e., zero-shot, few-shot, and general\nrecognition. In particular, our paradigm achieves the state-of-\nthe-art accuracy of 87.8% on Kinetics-400, and also surpasses\nprevious methods by 20∼50% absolute top-1 accuracy under\nzero-shot, few-shot settings on five video datasets. Code and\nmodels are available at https://github.com/whwu95/Text4Vis.\n1 Introduction\nPre-training a task-agnostic model using large-scale gen-\neral datasets and then transferring its learning feature rep-\nresentations to downstream tasks is a paradigm in many\ncomputer vision applications. While in the last decade,\nthe convolutional-based models that are optimized on the\nImageNet (Deng et al. 2009) dataset with a supervised\nstyle dominated this field. Owing to the dramatically in-\ncreasing computational capacity, now we can train models\nthat have several magnitude more model parameters and\nFLOPs on various image and even video datasets in ei-\nther supervised (Sun et al. 2017) or self-supervised (He\net al. 2020; Huang et al. 2021; Fang et al. 2022) style. Re-\ncently, contrastive-based vision-language pre-training (Rad-\nford et al. 2021) manifest their superior capabilities in im-\n*Corresponding author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Inter-class correlation maps of “embeddings of\nclass labels” for 20 categories on Kinetics-400. Left: The\nextracted textual vectors of class labels.Right: The “embed-\ndings” from learned classifier. The color thresholds are ad-\njusted for a better view. Please zoom in for the best view.\nproving downstream tasks performance such as classifica-\ntion (Radford et al. 2021), captioning (Mokady, Hertz, and\nBermano 2021), image generation (Ramesh et al. 2021), to\nname a few. These models are powerful for two reasons:\ni) the employed large-scale weakly-related datasets provide\nrich semantics and diverse representations of concepts; ii)\nthe representation vectors of images and texts are roughly\naligned in the semantic embedding space. However, the most\ncommon approach to using these models is fine-tuning the\nvisual encoder on specific tasks. Although the rich semantics\nand diverse representations of concepts benefit the down-\nstream tasks, the usage of the textual encoder is still left\noverlooked.\nIn this study, we aim to improve the transferability of such\nvision-language pre-training models for downstream classi-\nfication tasks, with the help of their textual encoders. Our\nmotivation comes from the semantic similarity among the\nground-truth labels. To demonstrate this, we employ the Ki-\nnetics video recognition dataset (Kay et al. 2017) for the\nanalysis. We extract the embedded textual vectors of class\nlabels using the textual encoder of CLIP. We then calculate\nthe correlation between the embedded textual vectors. The\nplot is shown on the left of Figure 1. Not surprisingly, the\nextracted textual vectors of class labels exhibit certain inter-\nclass correlations since part of them include the same verbs\nin their labels, e.g., playing <something>. Meanwhile, the\nlabels with different verbs show a negligible inter-class cor-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2847\nrelation, e.g., drinking and driving.\nNext, we examine the final projection head of a vanilla\nvideo recognition framework. We conduct the visual-only\nfine-tuning progress with the visual encoder that is also re-\nleased by CLIP (Radford et al. 2021). The detailed configu-\nrations are provided in Section 4.3. The projection head is a\nmatrix of d × c to compute the pre-softmax values (or log-\nits) from the d-dimensional feature vectors for the c classes.\nNon-rigorously, we can consider thed-dimensional row vec-\ntors as the embeddings of the class labels, allowing us to ex-\nplore the inter-class correlation between these learned “em-\nbeddings”, as shown on the right side of Figure 1. Interest-\ningly, these learned “embeddings” also reveal certain corre-\nlations after the training, despite being initialized randomly\nand optimized without knowing any textual information 1.\nTherefore, we suppose that the semantic information con-\ntained in the samples does correlate with inter-classes. Fol-\nlowing this motivation, we replace the projection matrix\nwith several variants: i) The projection matrix whose row\nvectors are randomly sampled (trivial correlation); ii) The\nprojection matrix whose row vectors are orthogonal to each\nother (non-correlated); iii) The projection matrix that is ini-\ntialized using the visual statistic knowledge to provide max-\nimized the correlation between labels (see Section 2.2); iv)\nThe projection matrix with fixed embedded textual vectors\nprovides the “proper” correlation. In the empirical stud-\nies, we find that textual knowledge significantly improves\nthe transferability of pre-trained models, regarding both the\nclassification accuracy and the convergence speed. Our main\ncontributions are summarized as follows:\n• We build a new recognition paradigm to improve the\ntransferability using visual knowledge and textual knowl-\nedge from the well-pre-trained vision-language model.\n• We conduct extensive experiments on popular video\ndatasets (i.e., Kinetics-400 & 600, UCF-101, HMDB-51\nand ActivityNet) to demonstrate the transferability of our\nsolution in many types of transfer learning,i.e., zero-shot\n/ few-shot / general video recognition. Our approach de-\nmocratizes the training on video datasets and achieves\nstate-of-the-art performance on various video recogni-\ntion settings, e.g., 87.8% top-1 accuracy on Kinetics-400,\nand outperforms previous methods by 20∼50% absolute\ntop-1 accuracy under zero-shot, few-shot settings.\n2 Methodology\nDenotations. In the paper, we use bold letters to denote\nVector, and capital italic letters to denote Tensor or\nMatrix, e.g., we employ z ∈ Rd to denote the feature\nvector extracted from a pre-trained model of dimension d,\nwe employ W ∈ Rd×c to denote the projection matrix for\nthe c−class linear classifier. Without ambiguity, we also use\ncapital italic letters to denote the modality in subscripts, es-\npecially we employ V and T to denote the Visual modality\nand Textual modality, respectively. We further employ low-\nercase italic letters to denote functions or neural networks.\nFor instance, we employ gV (·, ΘV ) and gT (·, ΘT ) to denote\n1That is, optimized with cross-entropy loss with one-hot labels\nthe visual and textual encoder, respectively. Besides, we em-\nploy calligraphic letters, e.g., D, to denote sets of elements.\n2.1 Revisiting of Previous Tuning Paradigms\nStandard Vision Transferring Paradigm. As shown in\nFigure 2(a), we start with the most ordinary scenario, where\na visual encoder model gV is optimized using a large-\nscale dataset D that contains visual samples with or with-\nout ground-truth labels. On our labeled downstream dataset\n˜D = {(x1, y1), (x2, y2), . . .}, our empirical learning target\ncan be written as\ng∗\nV , W∗ = argmin\nΘV ,W\nEx,y∼ ˜D\n\u0002\nH(y|σ(W · gV (x)))\n\u0003\n, (1)\nwhere H(ˆp|p) stands for the CrossEntropy between the\npredicted distribution p and the ground-truth distribution ˆp,\nσ denotes the softmax operation, W ∈ Rc×d denotes the\nlinear projection matrix for classification. The formulation\nin Eq. 1 is a standard visual feature transferring paradigm,\nwhere the visual encoder gV and the projection matrix (clas-\nsifier) W are learned simultaneously.\nVision-Language Learning Paradigm. As shown in Fig-\nure 2(b), we then review the contrastive learning paradigm of\nthe vision-language models. This paradigm has been widely\nused for vision-language pre-training i.e., CLIP (Radford\net al. 2021), and also been extend to video-text fine-\ntuning, i.e., ActionCLIP (Wang, Xing, and Liu 2021),\nCLIP4Clip (Luo et al. 2022). Given a weakly related vision-\nlanguage pair (e.g., image-text, video-text) dataset D =\n{(xV,1, xT,1), (xV,2, xT,2)...}. With slight abuse of the no-\ntations, we employ thexV , xT to denote a mini-batch of size\nb, then we minimize the following target,\ng∗\nV , g∗\nT = argmin\nΘV ,ΘT\nExV ,xT ∼ ˜D\n\u0002\nH(Q|σ(gV (xV )T·gT (xT )))\n\u0003\n,\n(2)\nwhere Q is the set that contains b one-hot labels of size\nc, with their 1, 2, . . . , b-th element being 1 (b < c, de-\nnoting the positive vision-language pairs. Here we clar-\nify that, the definition in Eq. 2 is not the rigorous form\nof the Noise-Contrastive Estimation (NCE) loss proposed\nin (Van den Oord, Li, and Vinyals 2018). Instead, we em-\nploy the cross entropy version implementation in (Radford\net al. 2021; Chen, Xie, and He 2021). This implementation\ndepicts a connection between the standard feature transfer-\nring paradigm and ours. In which the gT (xT ) can be con-\nsidered as the projection matrix that map the visual feature\ngV (xV ) to the given label set Q.\n2.2 Our Proposed Paradigm\nAs discussed in Section 1, we replace the learnable ran-\ndomly initialized linear projection matrix W with pre-\ndefined matrix ˜W. Similarly, the training target can be writ-\nten as\ng∗\nV = argmin\nΘV\nEx,y∼ ˜D\n\u0002\nH(y|σ( ˜W · gV (x)))\n\u0003\n. (3)\nNote that ˜W is not in the optimization targets, since we\nfreeze it from updating during the fine-tuning of the down-\nstream tasks. We do this for two reasons: Firstly, it could\n2848\n\u000bF\f\u00035HYLVLWLQJ\u0003WKH\u0003FODVVLILHU\u0003IRU\u0003HIILFLHQW\u0003WXQLQJ\u0003\n7H[WXDO\n(QFRGHU&/6\u0014\u000f\u0003&/6\u0015\u000f\u0003\u0011\u0011\u0011\u000f\u0003&/6F\nJ7\n\u000bD\f\u00036WDQGDUG\u0003YLVLRQ\u0010RQO\\\u0003WXQLQJ\u0003SDUDGLJP\n9LVXDO\n(QFRGHU\n&ODVVLILHU\u0003:\nF\u0003×\u0003G\n(PE\u0011 /RJLWV9LGHRV\nE\u0003×\u0003G\nE\u0003×\u0003FJ9\n&URVV\u0010(QWURS\\\n/RVV 9LVXDO\n(QFRGHU\n&ODVVLILHU\u0003:\nF\u0003×\u0003G\n(PE\u0011 ORJLWV9LGHRV\nE\u0003×\u0003G\nE\u0003×\u0003FJ9\n&URVV\u0010(QWURS\\\n/RVV\nF\u0003×\u0003G\n)URP\u0003D\u0003IUR]HQ\u0003FODVVLILHU\u0003SHUVSHFWLYH\u001d\n4\u001d\u0003+RZ\u0003WR\u0003REWDLQ\u0003LQWHU\u0010FODVV\u0003FRUUHODWLRQ\"\u0003\n$\u0015\u001d\u00037UDQVIHUULQJ\u0003WH[WXDO\u0003VHPDQWLF\u0003NQRZOHGJH\u0011\n9LVXDO\n(QFRGHU9LGHRV\nE\u0003×\u0003G\nJ9\n/LQHDU\u0003'LVFULPLQDQW\n$QDO\\VLV\n$\u0014\u001d\u00037UDQVIHUULQJ\u0003YLVXDO\u0003VWDWLVWLF\u0003NQRZOHGJH\u0011\n\u000bE\f\u00039LVLRQ\u0010ODQJXDJH\u0003WXQLQJ\u0003SDUDGLJP\n7H[WXDO\n(QFRGHU\nD\u0003YLGHR\u0003RI\u0003D\nSHUVRQ^&/6`\n9LGHRV\n(PE\u0011\n/RJLWV9LVXDO\n(QFRGHU\nE\u0003×\u0003G\nE\u0003×\u0003G\nE\u0003[\u0003E\nJ7\nJ9\n&RQWUDVWLYH\n/RVV\n([LVWLQJ\u0003WUDQVIHUULQJ\u0003SDUDGLJP\u0003IRU\u0003YLGHR\u0003UHFRJQLWLRQ\n&RHIILFLHQW\n&ODVVLILHU\u0003:/DEHOV\n&ODVVLILHU\u0003:\n(PE\u0011 \u001d\u00039LVXDO\u0003HPEHGGLQJ (PE\u0011 \u001d\u00037H[WXDO\u0003HPEHGGLQJ\n&/6\u0003\u001d\u0003&ODVV\u0003QDPH \u001d\u0003)UR]HQ\u0003SDUDPHWHUV \u001d\u0003'RW\u0003SURGXFW\n(PE\u0011\n(PE\u0011\n(PE\u0011\nFigure 2: Illustration of transferring vision-language pre-trained models for video recognition. (a) The widely-used standard\nvision-only tuning paradigm with cross-entropy loss. (b) The vision-language tuning paradigm with contrastive loss. (c) Revis-\niting the role of the classifier to transfer knowledge from vision-language pre-trained models (e.g., CLIP).\npreserve the textual knowledge from being disturbed by the\nrandomness brought by the mini-batch. For instance, when\nsome classes are missing, their embedded feature vector\nmight be broken by the other classes; Secondly, we want to\nprovide a fair comparison between different initialization of\n˜W. Now we consider how to initialize ˜W. To examine how\nthe correlation between the semantic information contained\nin the samples helps, we investigate the following four types\nof initialization, which represent different degrees of inter-\nclass correlation.\nRandomized Matrix. For the most simple randomized\nmatrix case, we set each row of the ˜W with a random Gaus-\nsian vector of zero mean and standard deviation, that is\n˜W ∼ N(0, Id), (4)\nwhere Id denotes the identity matrix of dimension d × d.\nArithmetically, a trivial “correlation” would appear between\nthe row of the ˜W, since the sampling size is significantly\nsmall to be biased. Evidently, the trivial “correlation” can-\nnot indicate the real correspondence between the classes due\nto its stochasticity. Therefore we expect the model to have\ninferior performance since it needs to avoid these incorrect\ncorrelations when learning the visual feature representation.\nRandomized Orthogonal Matrix. We follow the ap-\nproach of the randomized matrix. We then remove the cor-\nrelation by ensuring the row vectors are orthogonal. This is\nachieved by QR decomposition. Concretely, sinced > c, we\nfirst generate a random matrix of size d × d and select the\nfirst c rows as our projection matrix. Formally, we have,\n˜Wj ∼ QR(U)j, j= 1, 2, . . . , c,\nUi ∼ N(0, Id), i= 1, 2, . . . , d, (5)\nwhere U is the intermediate randomized matrix, QR(U) is\nthe row orthogonal matrix obtained through the QR decom-\nposition. Similar to the randomized matrix, we also expect\nthis initialization to have inferior performance. Given the\nfact that the one-hot label vectors are also orthogonal to each\nother, it will not be helpful to project the visual feature vec-\ntors with an orthogonal matrix, which increases the difficulty\nof learning meaningful visual features.\nLinear Discriminant Projection. We consider another\nway of initializing the projection matrix. We employ the\nmulti-class Fisher’s linear discriminant analysis (LDA) to\nlearn a linear classifier, then employ the weight matrix of\nthe classifier as our initialization of the projection matrix.\nSpecifically, we use the pre-trained visual encoder to extract\nvisual embeddings of samples in the train split, then perform\nLDA on the pre-extracted visual embeddings of the training\nset to generate the LDA coefficient. Finally, we use the LDA\ncoefficient to initialize ˜W and freeze it for fine-tuning the vi-\nsual encoder on the dataset. We compute the LDA projection\nfollowing previous work (Li, Zhu, and Ogihara 2006). Intu-\nitively, the LDA simultaneously maximizes the inter-class\ncovariance and minimizes intra-class covariance. We, there-\nfore, term this as the maximal correlation initialization us-\ning the visual statistic knowledge. As an essential classifier,\nthis type of initialization delivers reasonable performance,\nbut it is largely dependent on the data employed to com-\npute the projection matrix. When the data is limited, the es-\ntimated correlation will be biased. On the other hand, in our\nproposed paradigm, the pre-trained textual encoder provides\nunbiased correlations for fine-tuning.\nTextual Embedding Vectors. We finally describe the\nparadigm to transfer textual semantic knowledge from a\npre-trained textual encoder. Briefly, the projection weight\n˜W is composed of the embedded textual feature vectors\nof the labels. Given a set of tokenized class labels L =\n{l1, l2, . . . ,lc}, we have\n˜Wi ∼ gT (li), i= 1, 2, . . . , c, (6)\nwhere ˜Wi the i-th row vector in matrix ˜W. And ˜Wi is ini-\n2849\ntialized using the textual encoder output of the textual label\nof the i-th class. In the experimental analysis, we investigate\ntwo types of textual feature encoders: i) The encoder that\nis trained with a visual encoder in the contrastive style, i.e.,\nCLIP; ii) The encoder that is trained solely using only tex-\ntual samples on tasks such as masked language modeling,\ni.e., DistilBERT (Sanh et al. 2019).\n3 Related Works\nVisual Recognition. Convolutional networks have long\nbeen the standard for backbone architectures in image recog-\nnition (Krizhevsky, Sutskever, and Hinton 2012; He et al.\n2016; Simonyan and Zisserman 2014; Ioffe and Szegedy\n2015) and video recognition (Carreira and Zisserman 2017;\nQiu, Yao, and Mei 2017; Xie et al. 2018; Tran et al. 2018).\nInspired by the Transformer (Vaswani et al. 2017) scaling\nsuccesses in Natural Language Processing, Vision Trans-\nformer (ViT) (Dosovitskiy et al. 2020) applies a standard\nTransformer directly to images, which delivers impressive\nperformance on image recognition. Since then, ViT (Doso-\nvitskiy et al. 2020) has led a new trend in image recogni-\ntion backbone architectures, shifting from CNNs to Trans-\nformers. To improve performance, follow-up studies, e.g.,\nDeiT (Han et al. 2021), Swin (Liu et al. 2021), have been\ndeveloped. Also, many works has begun to adopt trans-\nformers in video recognition, such as TimeSFormer (Berta-\nsius, Wang, and Torresani 2021), ViViT (Arnab et al. 2021),\nVideoSwin (Liu et al. 2022), and MViT (Fan et al. 2021).\nImage-Language Pre-training. Recently, CLIP (Radford\net al. 2021) provides good practice in learning the coordi-\nnated vision-language pretraining models using the image-\ntext InfoNCE contrastive loss (Van den Oord, Li, and\nVinyals 2018). Based on CLIP, several variants (Jia et al.\n2021; Li et al. 2022; Yuan et al. 2021; Yu et al. 2022) have\nbeen proposed by combining more types of learning tasks\nsuch as image-text matching and masked image/language\nmodeling. These contrastively learned models have two de-\nserved properties for downstream tasks: the abundant visual\nfeature representations and the aligned textual feature repre-\nsentations. Yet another study (Yang et al. 2022) merged the\ndownstream classification task into the pretraining progress,\nwhich demonstrates a decent improvement of accuracy over\nthe standard cross-entropy loss.\nTransferring CLIP Models for Video-Text Learning.\nRecently, many video-text retrieval methods (Wang, Zhu,\nand Yang 2021; Zhao et al. 2022; Luo et al. 2022; Wu et al.\n2023a) have benefited from vision-language pre-training as\nwell. Moreover, several recent works (Wang, Xing, and Liu\n2021; Ju et al. 2022; Wu et al. 2023b) extend the CLIP (Rad-\nford et al. 2021) to train a downstream video-text matching\nmodel with contrastive loss, then perform video recognition\nusing the similarity between learned video and text embed-\ndings during inference. Instead of these contrastive-based\nmethods, we investigate the correlations of the linear clas-\nsifier for efficient feature transferring in the standard visual\nrecognition paradigm. Then we directly transfers visual and\ntextual knowledge for video recognition. In comparison to\ncontrastive-based methods, we demonstrate the superiority\nof our method in efficient training in Table 8. We hope that\nthe simple and effective paradigm can serve as a new base-\nline for future work.\n4 Experiments: Video Recognition\n4.1 Setups\nTo evaluate our method for video recognition, we conduct\nexperiments on five popular datasets,i.e., Kinetics-400 (Kay\net al. 2017), Kinetics-600 (Carreira et al. 2018), UCF-\n101 (Soomro, Zamir, and Shah 2012), HMDB-51 (Kuehne\net al. 2011) and ActivityNet-v1.3 (Caba Heilbron et al.\n2015). See Supplementary for statistics of these datasets.\nTraining & Inference. The video recognition task takes a\nvideo as input, and then fed it into a learned encoder to esti-\nmate the action category of the video. Given a video, we first\nuniformly sample T (e.g., 8, 16, 32) frames over the entire\nvideo. Then we utilize ResNet (He et al. 2016) or ViT (Doso-\nvitskiy et al. 2020) as the video encoders. The classifier in\nour paradigm is intialized from the textual embedding of the\nclass names and then frozen (fixed), leaving only the param-\neters in the video encoder to be learned. To trade off ac-\ncuracy and speed, we consider two inference strategies: (1)\nSingle View: We use only 1 clip per video and the center\ncrop for efficient evaluation, (e.g., as in Section 4.3). (2)\nMultiple Views: This is a widely used setting in previous\nworks (Feichtenhofer et al. 2019; Carreira and Zisserman\n2017) to sample multiple clips per video with several spatial\ncrops in order to get higher accuracy. For comparison with\nSOTAs, we use four clips with three crops (“4×3 Views”) in\nTable 1. See Supplementary for training hyperparameters.\n4.2 Main Results\nComparison to State-of-the-Arts. In Table 1, on the chal-\nlenging Kinetics-400 dataset, we compare to state-of-the-\narts that are pre-trained on large-scale datasets such as\nImageNet-21K (Deng et al. 2009), IG-65M (Ghadiyaram,\nTran, and Mahajan 2019), JFT-300M (Sun et al. 2017), FLD-\n900M (Yuan et al. 2021) and JFT-3B (Zhai et al. 2022). Up to\nnow, none of the three largest datasets (i.e., JFT-300M, FLD-\n900M, JFT-3B) is open-sourced and also does not provide\npre-trained models. Thus, we use the CLIP (Radford et al.\n2021) checkpoints, which are publicly available 2 and have\nbeen trained on 400 million web image-text pairs (namely\nWIT-400M). We can observe that our model outperforms all\nJFT-pretrained methods in terms of Top-1 and Top-5 accu-\nracy. We achieve an accuracy of 87.8% , which improves\neven further by 1.3% over Florence (Yuan et al. 2021), al-\nthough their model and data scale are both 2× larger than\nours. Besides, our model is even better than CoVeR (Zhang\net al. 2021), and their data scale is 7.5× larger.\nTo verify the generalization ability of our method, we fur-\nther evaluate the performance of our method on the well-\nknown untrimmed video benchmark, ActivityNet-v1.3. We\nfinetuned the Kinetics-400 pre-trained models with 16\nframes on the Activitynet-v1.3 dataset and report the top-1\naccuracy and mean average precision (mAP) following the\n2https://github.com/openai/CLIP/blob/main/clip/clip.py\n2850\nMethod Input\nPre-train Top-1 Top-5 FLOPs×Views Param\nNL I3D-101\n(Wang et al. 2018) 128×2242 IN-1K 77.7 93.3 359×10× 3 61.8\nMVFNetEn (Wu et al. 2021a) 24×2242 IN-1K 79.1 93.8 188×10× 3 -\nSlowFast NL101 (Feichtenhofer et al. 2019) 16×2242 Scratch 79.8 93.9 234×10× 3 59.9\nX3D-XXL (Feichtenhofer 2020) 16×4402 Scratch 80.4 94.6 144×10× 3 20.3\nMethods with\nlarge-scale pre-training\nTimeSformer-L (Bertasius, Wang, and Torresani 2021) 96×224 2 IN-21K 80.7 94.7 2380×1× 3 121.4\nViViT-L/16×2 (Arnab et al. 2021) 32×3202 IN-21K 81.3 94.7 3992×4× 3 310.8\nVideoSwin-L (Liu et al. 2022) 32×3842 IN-21K 84.9 96.7 2107×10× 5 200.0\nip-CSN-152 (Tran et al. 2019) 32×2242 IG-65M 82.5 95.3 109×10× 3 32.8\nViViT-L/16×2 (Arnab et al. 2021) 32×3202 JFT-300M 83.5 95.5 3992×4× 3 310.8\nTokLearner-L/10 (Ryoo et al. 2021) 32×2242 JFT-300M 85.4 96.3 4076×4× 3 450\nMTV-H (Yan et al. 2022) 32×2242 JFT-300M 85.8 96.6 3706×4× 3 -\nCoVeR (Zhang et al. 2021) 16×4482 JFT-300M 86.3 - -×1× 3 -\nFlorence (Yuan et al. 2021) 32×3842 FLD-900M 86.5 97.3 -×4× 3 647\nCoVeR (Zhang et al. 2021) 16×4482 JFT-3B 87.2 - -×1× 3 -\nVideoPrompt (Ju et al. 2022) 16×2242 WIT-400M 76.9 93.5 - -\nActionCLIP (Wang, Xing, and Liu 2021) 32×2242 WIT-400M 83.8 96.2 563×10× 3 141.7\nOurs V\niT-L/14 32×2242 WIT-400M 87.1 97.4 1662×4× 3 230.7\nOurs ViT-L/14 32×3362 WIT-400M 87.8 97.6 3829×1×3 230.7\nTable 1: Comparisons with SOTAs on Kinetics-400. “Views” indicates # temporal clip × # spatial crop. The magnitudes are\nGiga (109) and Mega (106) for FLOPs and Param. “IN” denotes ImageNet.\nMethod T\nop-1 mAP\nListenToLook\n(Gao et al. 2020) - 89.9\nMARL (Wu et al. 2019) 85.7 90.1\nDSANet (Wu et al. 2021b) - 90.5\nTSQNet (Xia et al. 2022a) 88.7 93.7\nNSNet (Xia et al. 2022b) 90.2 94.3\nOurs V\niT-L 92.9 96.5\nOurs ViT-L (336↑) 93.3 96.9\nTable 2: Comparisons with SOTAs on ActivityNet.\nofficial evaluation metrics. As shown in Table 2, our method\noutperforms recent SOTAs with a clear margin. To the best\nof our knowledge, our method achieves the best performance\n(96.9%) on ActivityNet. We also evaluate our method on the\nUCF-101 and HMDB-51 datasets to demonstrate its capac-\nity to generalize to smaller data. We achieve the mean class\naccuracy of 98.2% on UCF and 81.3% on HMDB, respec-\ntively. Please see supplementary for more comparisons on\nUCF-101 and HMDB-51.\nFew-Shot Video Recognition. Video recognition using\nonly a few samples is known as few-shot video recognition.\nWe study a more challenging K-shot C-way situation in-\nstead of the conventional 5-shot 5-way configuration. We\nscale the task up to categorize all categories in the dataset\nwith just K samples per category for training. The lower\nand upper bound of this situation are denoted by the term\n“Zero-shot” and “All-shot” respectively. Table 3 reports the\nTop-1 accuracy for the four datasets. In this extreme sce-\nnario of few data, we use CLIP-pretrained ViT-L/14 with 8\nframes and TAP for few-shot video recognition. In these ex-\nMethod shot\nHMDB UCF ANet K400\nVideoSwin\n(Liu et al. 2022) 2 20.9 53.3 - -\nVideoPrompt (Ju et al. 2022) 5 56.6 79.5 - 58.5\nX-Florence (Ni et al. 2022) 2 51.6 84.0 - -\nOurs V\niT-L\n0 53.8 71.9 75.6 61.0\n1\n72.7 96.4 89.0 75.8\n2 73.5 96.6 90.3 78.2\nAll 80.1\n96.9 91.1 84.7\nTable 3: Comparisons with SOTAs on few-shot recognition.\ntremely data-poor situations (e.g., even with just one shot),\nwe can see that our method offers amazing transferability\nto diverse domain data. Our approach, in contrast, demon-\nstrates robustness by outperforming SOTAs by a large mar-\ngin. For instance, when comparing accuracy on HMDB-51\nwith 2-shot, our method outperforms Swin, X-Florence by\n+52.6% and +21.9% respectively. See Supplementary for\ntraining details.\nZero-Shot Video Recognition. Furthermore, we conduct\nexperiments in the open-set setting. We use our Kinetics-\n400 pre-trained models (i.e., ViT-L with 8 frames) to per-\nform the zero-shot evaluation on four other video datasets.\nOn UCF-101, HMDB-51 and ActivityNet, there are two ma-\njor evaluation protocols following (Brattoli et al. 2020): half\nclasses evaluation and full classes evaluation. Please see\nSupplementary for the details of two evaluation protocols\nand the Kinetics-600 evaluation. We present comprehensive\ncomparisons on four datasets in Table 4, our method shows\na strong cross-dataset generalization ability. Our method\nshows a large improvement upon previous zero-shot recog-\n2851\nMethod UCF∗ / UCF HMDB ∗ / HMDB ANet ∗/ ANet Kinetics-600\nGA (Mishra et al. 2018) 17.3±1.1 / - 19.3±2.1 / - - -\nTS-GCN (Gao, Zhang, and Xu 2019) 34.2±3.1 / - 23.2±3.0 / - - -\nE2E (Brattoli et al. 2020) 44.1 / 35.3 29.8 / 24.8 26.6 / 20.0 -\nDASZL (Kim et al. 2021) 48.9±5.8 / - - / - - -\nER (Chen and Huang 2021) 51.8±2.9 / - 35.3±4.6 / - - 42.1±1.4\nResT (Lin et al. 2022) 58.7±3.3 / 46.7 41.1±3.7 / 34.4 32.5 / 26.3 -\nOurs 85.8±3.3 / 79.6 58.1±5.7 / 49.8 84.6±1.4 / 77.4 68.9±1.0\nTable 4: Comparisons with SOTAs on zero-shot video recognition. We directly evaluate our method without any additional\ntraining on cross-dataset video recognition. ANet is in short for ActivityNet. ∗ means half classes evaluation.\nnition methods (+27.1% on UCF-101, +17.0% on HMDB-\n51, +52.1% on ActivityNet, +26.8% on Kinetics-600).\n4.3 Ablations on Kinetics\nIn this section, we conduct extensive ablation experiments\non the Kinetics-400 dataset. Unless specified otherwise, we\nuse ViT-B/16 with 8 frames as the video backbone and a\nsingle view for testing. The default settings are marked in\ngray . See Supplementary for more ablations.\nDifferent Initializations to the Offline Classifier. We set\ndifferent initializations described in Section 2.2 to the offline\nclassifier W ∈ Rd×c and then train our visual encoder on\nKinetics-400. Table 5 lists their comparisons. We show that\nfeeding the offline classifier a random d-by-c matrix with a\nnormal distribution reduces performance significantly. Then\nwe assign the orthogonal matrix to the classifier, and see that\nremoving the inter-class correlation of the classifier will re-\nsult in inferior performance. Furthermore, we term the linear\ndiscriminate projection as the maximal correlation initializa-\ntion. To do so, we first sample 60 videos from each class in\nthe training set and utilize the pre-trained visual encoder to\nextract visual embeddings from these 24,000 videos. Finally,\nwe learn the linear classifier by performing linear discrimi-\nnant analysis on these visual embeddings and their labels.\nWe can see the LDA projection achieves a strong baseline.\nFinally, we study the textual embeddings from different\ntextual encoders. We choose DistilBERT (Sanh et al. 2019)\nand CLIP (Radford et al. 2021) as the textual encoder to\npre-extract the text embeddings of c categories. We observe\nthat DistilBERT performs the same performance as CLIP’s\ntextual encoder. This may be because both DistillBERT and\nCLIP are pre-trained with large-scale data, so they both\nhave strong language modeling capabilities and can gener-\nate good semantic targets. Although the good semantic tar-\ngets generated by DistillBERT are not aligned with the vi-\nsual features of CLIP, it is easy to fit them with trainable vi-\nsual encoders. We also observe that the loss of DistillBERT\nwill be higher than CLIP in the early stage, but it will quickly\ndecrease to the same level.More visualizations of these clas-\nsifiers are in Supplementary.\nComparison with Vision-Only Tuning Paradigm. As a\ncomparison with our method, we train the unimodality video\nmodel, which consists of the same visual encoder and a\nOffline classifier from Top 1\nRandom normal matrix 59.3\nRandom orthogonal matrix 59.4\nLinear discriminant projection 80.8\nDistilBERT 81.4\nTextual encoder of CLIP\n81.5\nTable 5: Exploration of different frozen classifiers.\nZero-shot 2-shot Full-shot\nVision-Only 0.2 21.6 75.3\nVision-Text 54.2 65.3 80.1\nTable 6: Comparisons with vision-only framework.\nlearnable classifier with random initialization. To produce\nvideo embedding, we just apply temporal average pool-\ning (TAP) to frame embeddings. As shown in Table 6, our\nVision-Text method leads to obvious improvement with the\nsame training recipe, especially in the data-poor situation.\nTemporal Modeling. Here we explore more temporal\nmodelings for ViT and ResNet: (1) TAP: Temporal aver-\nage pooling is the most straightforward temporal modeling.\n(2) T1D: The channel-wise temporal 1D convolutions, is a\ncommon strategy (Wu et al. 2021a; Wang et al. 2021; Liu\net al. 2020), to perform efficient temporal interaction in the\nlatter stages (i.e., res 4−5) of ResNet. (3) T-Trans: The em-\nbeddings of frames are fed to a multi-layer (e.g., 6-layer)\ntemporal transformer encoder. (4) TokenT1D: We use T1D\nto model temporal relations for [class] token features that\nare aggregated from local features via attention in the vi-\nsion transformer. We perform the TokenT1D in multiple po-\nsitions of a vision transformer. Results are shown in Table 7.\nOn both backbones, TAP provides simple baselines and T-\nTrans exhibits the best top-1 accuracy. Both of them main-\ntain the original frame-level representations and then per-\nform temporal modeling. An interesting thing we observed\nis that T1D does not seem to work in this scenario. The rea-\nson lies in that T1D may have the potential to break the\nlearned strong representations provided by CLIP. TokenT1D\nis another internal-backbone temporal modeling, and it does\n2852\nBackbone Modeling Top-1 Top-5\nResNet-50\nTAP 71.2 90.4\nT1D 67.2 88.5\nT-Trans 74.3 91.7\nVIT-B/16\nTAP 80.1 95.0\nTokenT1D 80.4 95.0\nT-Trans 81.5 95.5\nTable 7: Temporal modeling for video encoders.\nnot yield a performance drop, and even slightly improves the\nTAP baseline. We believe this is because TokenT1D is only\nimposed on the global [class] token instead of patch tokens,\nresulting in minimal modifications on pre-trained features.\nOurs v.s. Contrastive-Based Paradigm. We make a com-\nparison with the Contrastive-based tuning method i.e., Ac-\ntionClip (Wang, Xing, and Liu 2021) mentioned in Sec-\ntion 3. This paradigm treats the recognition task as a video-\ntext matching problem with contrastive loss, thus requiring a\nbatch gathering to collect embeddings of all batches across\nall GPUs and calculate cosine similarity for a given batch\nacross all other batches. In Table 8, we compare it with\nthe Contrastive-based paradigm and observe that it does not\nwork well without batch gathering. This is due to contrastive\nlearning favors a large batch size (e.g., CLIP used 256\nGPUs with a batch size of 128 per GPU to maintain a large\n32768×32768 similarity matrix). Besides, involving batch\ngather will multiply the training time. Also, in this case, the\npre-trained textual encoder still needs to be updated, which\nrequires larger GPU memory. However, our paradigm em-\nploys pre-extracted text embeddings as our classifier, so the\nonly learned part is the visual encoder. Results show that our\nmethod achieves the best accuracy-cost trade-off. Specifi-\ncally, our method achieves the performance of 81.5% with\nViT-B/16, which takes only 10 hours to run the training us-\ning 8 GPUs (2× faster than the matching counterpart). See\nSupplementary for details about the batch gathering.\nParadigm Batch\nGather\nTextual\nEncoder Top-1 V100-days\nContrastive-\nBased\n✓ online 81.2 6.7 (10 ∗)\n✓ offline 80.7 6.6\n✗ online 77.8 3.5\n✗ offline 76.1 3.3\nOurs ✗ offline 81.5 3.3\nTable 8: Oursvs. Contrastive-based paradigm with ViT-B/16\non Kinetics-400. The number of V100 days is the number of\nV100 GPU used for training multiplied by the training time\nin days. ∗ indicates the official result (Wang, Xing, and Liu\n2021) via “Data-parallel training” on 3090 GPUs. For effi-\ncient training and fair comparison, we implement all experi-\nments with “Distributed Data-parallel training” in this Table.\nViews Top-1 GFLOPs\nSingle→Multiple 81.5→82.9 90.3→90.3×12\nTable 9: Two classic evaluation protocols.\nMethod Top-1 FLOPs Params Throughput\nViViT-L/16-320 81.3 3992G 310.8M 4.2 vid/s ∗\nOurs ViT-B/32 78.5 23.7G 71.6M 322.5 vid/s\nOurs ViT-B/16 81.5 90.3G 69.9M 126.5 vid/s\nOurs ViT-L/14 85.4 415.4G 230.4M 35.5 vid/s\nTable 10: Analysis on throughput. “vid/s” represents the av-\nerage number of videos per second. The larger “vid/s” rep-\nresents higher efficiency.∗ is the official result with TPU-v3.\nMore Instantiations. Table 10 presents the results of\nour method using different visual encoders, indicating that\ndeeper backbones can achieve better performance. Table 9\npresents the results of our method under two evaluation pro-\ntocols mentioned in Section 4.1, where the multi-view eval-\nuation protocol results in additional improvements.\nAnalysis on Efficiency. In Table 10, we present the com-\nputational cost and efficiency of our models. We follow the\ncommon inference settings by using a single NVIDIA A100\nGPU to measure the throughput. We use a batch size of 16 to\nmeasure the throughput. Our models achieve the29× faster\nthroughput and 44× fewer FLOPs compared with the pre-\nvious transformer-based method ViViT (Arnab et al. 2021)\nunder the same accuracy.\n5 Limitation and Conclusion\nLimitation: The performance of the proposed paradigm is\nrestricted to how the category labels are represented. For in-\nstance, in tasks such as human re-identification, the labels\nare often set as numerical values such as 0, 1, 2, etc. In this\ncase, we cannot transfer any semantic information from the\ntextual encoders, while transferring visual statistic knowl-\nedge (i.e., LDA classifier) could be helpful.\nConclusion: We present a new paradigm for improving\nthe transferability of visual recognition that is based on\nthe knowledge from the textual encoder of the well-trained\nvision-language model. The empirical study shows that our\nmethod improves both the performance and the convergence\nspeed of visual classification. The proposed approach has\nsuperior performance on both general and zero-shot/few-\nshot recognition and achieves state-of-the-art performance\non video recognition tasks, and democratizes transferring on\nchallenging video datasets, i.e., Kinetics-400.\nAcknowledgments\nThis paper is supported by the Australian Research Council\nGrant DP200103223, Australian Medical Research Future\nFund MRFAI000085, CRC-P Smart Material Recovery Fa-\ncility (SMRF) – Curby Soft Plastics, and CRC-P ARIA -\nBionic Visual-Spatial Prosthesis for the Blind.\n2853\nReferences\nArnab, A.; Dehghani, M.; Heigold, G.; Sun, C.; Lu ˇci´c, M.;\nand Schmid, C. 2021. Vivit: A video vision transformer. In\nICCV, 6836–6846.\nBertasius, G.; Wang, H.; and Torresani, L. 2021. Is Space-\nTime Attention All You Need for Video Understanding? In\nICML, 813–824. PMLR.\nBrattoli, B.; Tighe, J.; Zhdanov, F.; Perona, P.; and Chalupka,\nK. 2020. Rethinking zero-shot video classification: End-to-\nend training for realistic applications. In CVPR, 4613–4623.\nCaba Heilbron, F.; Escorcia, V .; Ghanem, B.; and Car-\nlos Niebles, J. 2015. Activitynet: A large-scale video bench-\nmark for human activity understanding. In CVPR, 961–970.\nCarreira, J.; Noland, E.; Banki-Horvath, A.; Hillier, C.; and\nZisserman, A. 2018. A short note about kinetics-600. arXiv\npreprint arXiv:1808.01340.\nCarreira, J.; and Zisserman, A. 2017. Quo vadis, action\nrecognition? a new model and the kinetics dataset. InCVPR.\nChen, S.; and Huang, D. 2021. Elaborative rehearsal for\nzero-shot action recognition. In ICCV, 13638–13647.\nChen, X.; Xie, S.; and He, K. 2021. An empirical study of\ntraining self-supervised vision transformers. In ICCV.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 248–255.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFan, H.; Xiong, B.; Mangalam, K.; Li, Y .; Yan, Z.; Malik, J.;\nand Feichtenhofer, C. 2021. Multiscale vision transformers.\nIn ICCV, 6824–6835.\nFang, B.; Wu, W.; Liu, C.; Zhou, Y .; He, D.; and Wang, W.\n2022. Mamico: Macro-to-micro semantic correspondence\nfor self-supervised video representation learning. In ACM\nMM, 1348–1357.\nFeichtenhofer, C. 2020. X3D: Expanding Architectures for\nEfficient Video Recognition. In CVPR, 203–213.\nFeichtenhofer, C.; Fan, H.; Malik, J.; and He, K. 2019. Slow-\nfast networks for video recognition. In ICCV, 6202–6211.\nGao, J.; Zhang, T.; and Xu, C. 2019. I know the relation-\nships: Zero-shot action recognition via two-stream graph\nconvolutional networks and knowledge graphs. In AAAI,\nvolume 33, 8303–8311.\nGao, R.; Oh, T.-H.; Grauman, K.; and Torresani, L. 2020.\nListen to look: Action recognition by previewing audio. In\nCVPR, 10457–10467.\nGhadiyaram, D.; Tran, D.; and Mahajan, D. 2019. Large-\nscale weakly-supervised pre-training for video action recog-\nnition. In CVPR, 12046–12055.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y .\n2021. Transformer in transformer. NeurIPS, 15908–15919.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In CVPR, 9729–9738.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR, 770–778.\nHuang, D.; Wu, W.; Hu, W.; Liu, X.; He, D.; Wu, Z.; Wu, X.;\nTan, M.; and Ding, E. 2021. Ascnet: Self-supervised video\nrepresentation learning with appearance-speed consistency.\nIn ICCV, 8096–8105.\nIoffe, S.; and Szegedy, C. 2015. Batch normalization: Accel-\nerating deep network training by reducing internal covariate\nshift. In ICML, 448–456. PMLR.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;\nLe, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling\nup visual and vision-language representation learning with\nnoisy text supervision. In ICML, 4904–4916. PMLR.\nJu, C.; Han, T.; Zheng, K.; Zhang, Y .; and Xie, W. 2022.\nPrompting visual-language models for efficient video under-\nstanding. In ECCV, 105–124. Springer.\nKay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.;\nVijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev,\nP.; et al. 2017. The kinetics human action video dataset.\narXiv preprint arXiv:1705.06950.\nKim, T. S.; Jones, J.; Peven, M.; Xiao, Z.; Bai, J.; Zhang,\nY .; Qiu, W.; Yuille, A.; and Hager, G. D. 2021. Daszl: Dy-\nnamic action signatures for zero-shot learning. InAAAI, vol-\nume 35, 1817–1826.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. NeurIPS.\nKuehne, H.; Jhuang, H.; Garrote, E.; Poggio, T.; and Serre,\nT. 2011. HMDB: a large video database for human motion\nrecognition. In ICCV, 2556–2563.\nLi, J.; Li, D.; Xiong, C.; and Hoi, S. 2022. Blip: Boot-\nstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In ICML.\nLi, T.; Zhu, S.; and Ogihara, M. 2006. Using discriminant\nanalysis for multi-class classification: an experimental in-\nvestigation. Knowledge and information systems, 10(4).\nLin, C.-C.; Lin, K.; Wang, L.; Liu, Z.; and Li, L. 2022.\nCross-modal Representation Learning for Zero-shot Action\nRecognition. In CVPR, 19978–19988.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012–10022.\nLiu, Z.; Luo, D.; Wang, Y .; Wang, L.; Tai, Y .; Wang, C.; Li,\nJ.; Huang, F.; and Lu, T. 2020. TEINet: Towards an Efficient\nArchitecture for Video Recognition. InAAAI, 11669–11676.\nLiu, Z.; Ning, J.; Cao, Y .; Wei, Y .; Zhang, Z.; Lin, S.; and Hu,\nH. 2022. Video swin transformer. In CVPR, 3202–3211.\nLuo, H.; Ji, L.; Zhong, M.; Chen, Y .; Lei, W.; Duan, N.; and\nLi, T. 2022. CLIP4Clip: An empirical study of CLIP for end\nto end video clip retrieval and captioning. Neurocomputing,\n508: 293–304.\n2854\nMishra, A.; Verma, V . K.; Reddy, M. S. K.; Arulkumar, S.;\nRai, P.; and Mittal, A. 2018. A generative approach to zero-\nshot and few-shot action recognition. In WACV, 372–380.\nMokady, R.; Hertz, A.; and Bermano, A. H. 2021. Clip-\ncap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734.\nNi, B.; Peng, H.; Chen, M.; Zhang, S.; Meng, G.; Fu, J.;\nXiang, S.; and Ling, H. 2022. Expanding language-image\npretrained models for general video recognition. In ECCV.\nQiu, Z.; Yao, T.; and Mei, T. 2017. Learning spatio-temporal\nrepresentation with pseudo-3d residual networks. In ICCV.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In ICML, 8748–8763. PMLR.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. In ICML, 8821–8831. PMLR.\nRyoo, M. S.; Piergiovanni, A.; Arnab, A.; Dehghani, M.;\nand Angelova, A. 2021. TokenLearner: What Can 8\nLearned Tokens Do for Images and Videos? arXiv preprint\narXiv:2106.11297.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Dis-\ntilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSoomro, K.; Zamir, A. R.; and Shah, M. 2012. UCF101:\nA dataset of 101 human actions classes from videos in the\nwild. arXiv preprint arXiv:1212.0402.\nSun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Re-\nvisiting unreasonable effectiveness of data in deep learning\nera. In ICCV, 843–852.\nTran, D.; Wang, H.; Torresani, L.; and Feiszli, M. 2019.\nVideo classification with channel-separated convolutional\nnetworks. In ICCV, 5552–5561.\nTran, D.; Wang, H.; Torresani, L.; Ray, J.; LeCun, Y .; and\nPaluri, M. 2018. A Closer Look at Spatiotemporal Convolu-\ntions for Action Recognition. In CVPR, 6450–6459.\nVan den Oord, A.; Li, Y .; and Vinyals, O. 2018. Represen-\ntation learning with contrastive predictive coding. arXiv e-\nprints, arXiv–1807.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWang, L.; Tong, Z.; Ji, B.; and Wu, G. 2021. TDN: Tem-\nporal difference networks for efficient action recognition. In\nCVPR, 1895–1904.\nWang, M.; Xing, J.; and Liu, Y . 2021. Actionclip: A\nnew paradigm for video action recognition. arXiv preprint\narXiv:2109.08472.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nLocal Neural Networks. In CVPR, 7794–7803.\nWang, X.; Zhu, L.; and Yang, Y . 2021. T2vlad: global-local\nsequence alignment for text-video retrieval. In CVPR.\nWu, W.; He, D.; Lin, T.; Li, F.; Gan, C.; and Ding, E. 2021a.\nMVFNet: Multi-View Fusion Network for Efficient Video\nRecognition. In AAAI, volume 35, 2943–2951.\nWu, W.; He, D.; Tan, X.; Chen, S.; and Wen, S. 2019. Multi-\nAgent Reinforcement Learning Based Frame Sampling for\nEffective Untrimmed Video Recognition. In ICCV, 6222–\n6231.\nWu, W.; Luo, H.; Fang, B.; Wang, J.; and Ouyang, W.\n2023a. Cap4Video: What Can Auxiliary Captions Do for\nText-Video Retrieval? In CVPR.\nWu, W.; Wang, X.; Luo, H.; Wang, J.; Yang, Y .; and\nOuyang, W. 2023b. Bidirectional Cross-Modal Knowledge\nExploration for Video Recognition with Pre-trained Vision-\nLanguage Models. In CVPR.\nWu, W.; Zhao, Y .; Xu, Y .; Tan, X.; He, D.; Zou, Z.; Ye, J.; Li,\nY .; Yao, M.; Dong, Z.; et al. 2021b. DSANet: Dynamic Seg-\nment Aggregation Network for Video-Level Representation\nLearning. ACM MM, 1903–1911.\nXia, B.; Wang, Z.; Wu, W.; Wang, H.; and Han, J. 2022a.\nTemporal Saliency Query Network for Efficient Video\nRecognition. In ECCV, 741–759.\nXia, B.; Wu, W.; Wang, H.; Su, R.; He, D.; Yang, H.; Fan, X.;\nand Ouyang, W. 2022b. NSNet: Non-saliency Suppression\nSampler for Efficient Video Recognition. In ECCV, 705–\n723.\nXie, S.; Sun, C.; Huang, J.; Tu, Z.; and Murphy, K. 2018.\nRethinking spatiotemporal feature learning: Speed-accuracy\ntrade-offs in video classification. In ECCV, 305–321.\nYan, S.; Xiong, X.; Arnab, A.; Lu, Z.; Zhang, M.; Sun, C.;\nand Schmid, C. 2022. Multiview transformers for video\nrecognition. In CVPR, 3333–3343.\nYang, J.; Li, C.; Zhang, P.; Xiao, B.; Liu, C.; Yuan, L.; and\nGao, J. 2022. Unified contrastive learning in image-text-\nlabel space. In CVPR, 19163–19173.\nYu, J.; Wang, Z.; Vasudevan, V .; Yeung, L.; Seyedhos-\nseini, M.; and Wu, Y . 2022. CoCa: Contrastive Caption-\ners are Image-Text Foundation Models. arXiv preprint\narXiv:2205.01917.\nYuan, L.; Chen, D.; Chen, Y .-L.; Codella, N.; Dai, X.; Gao,\nJ.; Hu, H.; Huang, X.; Li, B.; Li, C.; et al. 2021. Florence: A\nNew Foundation Model for Computer Vision.arXiv preprint\narXiv:2111.11432.\nZhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L. 2022.\nScaling vision transformers. In CVPR, 12104–12113.\nZhang, B.; Yu, J.; Fifty, C.; Han, W.; Dai, A. M.; Pang,\nR.; and Sha, F. 2021. Co-training Transformer with Videos\nand Images Improves Action Recognition. arXiv preprint\narXiv:2112.07175.\nZhao, S.; Zhu, L.; Wang, X.; and Yang, Y . 2022. Center-\nCLIP: Token Clustering for Efficient Text-Video Retrieval.\nSIRIR.\n2855",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8607499003410339
    },
    {
      "name": "Classifier (UML)",
      "score": 0.7157238721847534
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6533805131912231
    },
    {
      "name": "Encoder",
      "score": 0.6106879115104675
    },
    {
      "name": "Machine learning",
      "score": 0.4928581714630127
    },
    {
      "name": "Language model",
      "score": 0.4300769865512848
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40088701248168945
    },
    {
      "name": "Speech recognition",
      "score": 0.36678773164749146
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    }
  ],
  "cited_by": 83
}