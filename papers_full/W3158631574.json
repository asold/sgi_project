{
    "title": "PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
    "url": "https://openalex.org/W3158631574",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2103026051",
            "name": "Zeng Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2355159071",
            "name": "Ren Xiao-zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2228243925",
            "name": "Su Teng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1566979378",
            "name": "Wang Hui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101329899",
            "name": "Liao Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1635915244",
            "name": "WANG ZhiWei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2030371270",
            "name": "Jiang Xin",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Yang, ZhenZhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352947174",
            "name": "Wang KAISHENG",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349151806",
            "name": "Zhang Xiao-da",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1971984624",
            "name": "Li Chen",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Gong, Ziyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2550181736",
            "name": "Yao Yifan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2706276093",
            "name": "Huang, Xinjing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1940314070",
            "name": "Wang Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141109724",
            "name": "Yu Jianfeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095397056",
            "name": "Guo Qi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2054004063",
            "name": "Yu Yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1998456853",
            "name": "Zhang Yan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1970843724",
            "name": "Wang Jin",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Tao, Hengtao",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Yan, Dasen",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Yi, Zexuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1968904209",
            "name": "Peng Fang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2700155607",
            "name": "Jiang Fangqing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106826012",
            "name": "Zhang Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1819236904",
            "name": "Deng Lingfeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2368456414",
            "name": "Zhang Yehong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098349227",
            "name": "Lin, Zhe",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1851812986",
            "name": "Zhang Chao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106069959",
            "name": "Zhang Shaojie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2347909497",
            "name": "Guo Ming-yue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2745783027",
            "name": "Gu Shanzhi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2342321267",
            "name": "Fan Gao-jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2363603254",
            "name": "Wang, Yaowei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2354792830",
            "name": "Jin Xuefeng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1952241286",
            "name": "Liu Qun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1989693854",
            "name": "Tian, Yonghong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2884700152",
        "https://openalex.org/W2787831171",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2883830791",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W2952339051",
        "https://openalex.org/W3016430712",
        "https://openalex.org/W2962934410",
        "https://openalex.org/W2806081754",
        "https://openalex.org/W3114361292",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W3106031450",
        "https://openalex.org/W2963956494",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2982219368",
        "https://openalex.org/W2997347790",
        "https://openalex.org/W3031276512",
        "https://openalex.org/W2971871542",
        "https://openalex.org/W3129831491",
        "https://openalex.org/W2769934148",
        "https://openalex.org/W2963015836",
        "https://openalex.org/W3132107458",
        "https://openalex.org/W2495998536",
        "https://openalex.org/W3010108619",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W3122317902",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3015253856",
        "https://openalex.org/W2963351145",
        "https://openalex.org/W2949884065",
        "https://openalex.org/W2991040477",
        "https://openalex.org/W3107315802",
        "https://openalex.org/W2969388332",
        "https://openalex.org/W3103395072"
    ],
    "abstract": "Large-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Language Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 have demonstrated strong performances on natural language understanding and generation with \\textit{few-shot in-context} learning. In this work, we present our practice on training large-scale autoregressive language models named PanGu-$α$, with up to 200 billion parameters. PanGu-$α$ is developed under the MindSpore and trained on a cluster of 2048 Ascend 910 AI processors. The training parallelism strategy is implemented based on MindSpore Auto-parallel, which composes five parallelism dimensions to scale the training task to 2048 processors efficiently, including data parallelism, op-level model parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To enhance the generalization ability of PanGu-$α$, we collect 1.1TB high-quality Chinese data from a wide range of domains to pretrain the model. We empirically test the generation ability of PanGu-$α$ in various scenarios including text summarization, question answering, dialogue generation, etc. Moreover, we investigate the effect of model scales on the few-shot performances across a broad range of Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-$α$ in performing various tasks under few-shot or zero-shot settings.",
    "full_text": "PANGU-α: L ARGE -SCALE AUTOREGRESSIVE PRETRAINED\nCHINESE LANGUAGE MODELS WITH AUTO -PARALLEL\nCOMPUTATION\nTECHNICAL REPORT\nWei Zeng∗ Xiaozhe Ren∗ Teng Su∗ Hui Wang∗\nYi Liao Zhiwei Wang Xin Jiang Zhenzhang Yang Kaisheng Wang Xiaoda Zhang\nChen Li Ziyan Gong Yifan Yao Xinjing Huang Jun Wang Jianfeng Yu Qi Guo\nYue Yu Yan Zhang Jin Wang Hengtao Tao Dasen Yan Zexuan Yi Fang Peng\nFangqing Jiang Han Zhang Lingfeng Deng Yehong Zhang Zhe Lin\nChao Zhang Shaojie Zhang Mingyue Guo Shanzhi Gu Gaojun Fan Yaowei Wang\nXuefeng Jin Qun Liu Yonghong Tian\nPANGU-α TEAM\nABSTRACT\nLarge-scale Pretrained Language Models (PLMs) have become the new paradigm for Natural Lan-\nguage Processing (NLP). PLMs with hundreds of billions parameters such as GPT-3 [ 1] have\ndemonstrated strong performances on natural language understanding and generation with few-shot\nin-context learning. In this work, we present our practice on training large-scale autoregressive\nlanguage models named PanGu-α, with up to 200 billion parameters. PanGu-αis developed under\nthe MindSpore2 and trained on a cluster of 2048 Ascend 910 AI processors3. The training parallelism\nstrategy is implemented based on MindSpore Auto-parallel, which composes ﬁve parallelism dimen-\nsions to scale the training task to 2048 processors efﬁciently, including data parallelism, op-level\nmodel parallelism, pipeline model parallelism, optimizer model parallelism and rematerialization. To\nenhance the generalization ability of PanGu-α, we collect 1.1TB high-quality Chinese data from a\nwide range of domains to pretrain the model. We empirically test the generation ability of PanGu-α\nin various scenarios including text summarization, question answering, dialogue generation, etc.\nMoreover, we investigate the effect of model scales on the few-shot performances across a broad range\nof Chinese NLP tasks. The experimental results demonstrate the superior capabilities of PanGu-αin\nperforming various tasks under few-shot or zero-shot settings.\nKeywords Pre-trained Language Models ·Large-scale Deep Models ·Distributed Training ·Chinese Language\nUnderstanding and Generation\n∗Equal Contribution\n2https://www.mindspore.cn/en\n3https://e.huawei.com/en/products/servers/ascend\narXiv:2104.12369v1  [cs.CL]  26 Apr 2021\nTECHNICAL REPORT - APRIL 27, 2021\n1 Introduction\nPre-trained Language Models (PLMs) [1, 2, 3, 4, 5, 6, 7, 8, 9, etc.] have gained great success in the Natural Language\nProcessing (NLP). By learning contextual representation of text from large-scale corpora in a self-supervised manner,\nPLMs can achieve state-of-the-art performances on a wide range of Natural Language Understanding (NLU) and\nNatural Language Generation (NLG) tasks.\nRadford et. al. [10] demonstrates a signiﬁcant gains on a variety of NLP tasks via Generative Pre-trained Transformer\n(GPT), which is an autoregressive language model ﬁrst pretrained on unsupervised text data and then ﬁnetuned for each\nsupervised task. Devlin et.al. [2] proposes BERT, a bidirectional Transformer with the masked language model (MLM)\npretraining objective, which obtains new state-of-the-art performances on the GLUE benchmark of NLU tasks. After\nthem, there have been an increasing number of research work on developing the pretraining techniques and continuously\nimproving the performance of downstream NLP tasks. Among all the techniques, researchers ﬁnd that the performance\nof PLMs can be steadily improved simply by enlarging the amount of the training data as well as the capacity of the\nmodel. For instance, RoBERTa [5] shows that BERT can be substantially improved by training the model longer with\nmore data. GPT-2 [11] as the successor of GPT, which shares the same architecture but contains 1.5 billion parameters\nand is trained with 40GB text, can perform reasonably well on multiple tasks in the zero-shot setting. The T5 model [ 6]\nwith 11 billion parameters trained on the 745GB C4 data, keeps pushing the performance of both NLU and NLG tasks.\nRecently, the OpenAI team announced its lasted version of the GPT-series models: GPT-3 [1]. The largest GPT-3 model\ncontains 175 billion parameters and is trained using 570GB of text data. Besides its strong capability in generating\nhigh-quality text, GPT-3 is especially effective in solving a wide range of tasks without task-speciﬁc ﬁnetuning in the\nfew-shot, or even zero-shot settings. Moreover, on many of the tasks the performance improves steadily as the size of\nthe GPT model grows, and sometimes even reaches the level of the prior state-of-the-art ﬁnetuning approaches. From\napplications perspective, GPT-3 is revolutionary, as it relieves the need for labelling many examples and retraining\nmodel for every new task, which hinders the applicability of NLP models in real-world applications.\nHowever, GPT-3 is now only available for limited access via OpenAI API, and it is primarily trained with English data.\nTo promote the public research of Chinese PLMs, we propose training a very large-scale Chinese PLM named PanGu-α\nwith number of parameters up to 200 billion. To the best of our knowledge, this is the largest Chinese PLM up to the\npublication of this technical report.\nThe difﬁculty in training a PLM rises as the scale of the model grows beyond the level of 10 billion. The main challenges\nlie in three aspects:\n• Model Design. There have been a couple of architectures of PLMs besides GPT and BERT. However, not\nall the PLMs can be smoothly scaled to hundreds of billions of parameters. For examples, some models may\nhave problem of slow convergence or even divergence during training as the model size increases. Inspired by\nGPT-3 and our preliminary experiments, we choose the Transformer-based autoregressive language model\nas the base architecture. Besides, we develop an additional query layer on top of the Transformer layers to\ninduce the expected output of the model during pretraining. Our experiments demonstrate that the structure of\nPanGu-αcan scale up to 200 billion parameters.\n• Training Corpora. Training data is essential in building a strong and generalisable pretrained model. On one\nhand, the amount of the data should be sufﬁcient to feed a large PLM. On the other hand, the data should be of\nhigh quality and diversity to ensure the generality of the PLM. To build Chinese corpus with comprehensive\ncoverage, we collect a large amount of data from a wide range of resources, including Common Crawl,\ne-Books, encyclopedias, news, and so on. Based on them, we conduct multiple processes of data ﬁltering and\ncleaning to make sure the processed data are of high quality and reliability.\n• Distributed Training. The memory requirement of training PanGu-αwith 200 billion parameters is much\nbeyond the memory capacities of modern AI processors. It is difﬁcult to acquire large end-to-end throughput\nwhile keeping high resource utilization on a cluster of processors. The problem becomes more challenging\nwhen considering the topology of hardware. We combine ﬁve-dimensional parallel functionalities with a\ncarefully designed parallelization strategy and apply them to the largest PanGu-α, which is efﬁciently trained\non a cluster of 2048 Ascend 910 AI processors [12] and powered by CANN4.\nWe train three PanGu-αmodels on a high-quality 1.1TB Chinese text corpus with increasing magnitude of parameter\nsizes, which are PanGu- α2.6B, PanGu-α13B, and PanGu-α200B, respectively. We ﬁrst evaluate the models on\nlanguage modeling tasks, showing that the perplexity can be decreased with the increase of model capacity and the\namount of data and computation. Then we investigate the text generation ability of PanGu- αin various scenarios\n4https://www.hiascend.com/en/software/cann\n2\nTECHNICAL REPORT - APRIL 27, 2021\nsuch as dialogue generation, summarization, question answering, etc. We demonstrate a few generated samples for\ndifferent applications in the experiment section. Furthermore, we evaluate the task-agnostic few-shot performances of\nPanGu-α2.6B and 13B on a wide range of NLP tasks, including cloze tasks, reading comprehension, closed-book QA,\nWinograd style tasks, commonsense reasoning, natural language inference, and text classiﬁcation. The experimental\nresults demonstrate that with the growing model capacity, the performance on various tasks can generally improve.\nWe are currently seeking a proper way to let both non-proﬁt research institutes and commercial companies to get access\nto our pretrained PanGu-αmodels, either by releasing the code and model or via APIs. We are also assessing the\npossibility of releasing all or part of our pretraining data, within the constraints of the law and legality.\nTo facilitate the community to pretrain a large-scale language model by their own, the parallel computing functionalities\nare open-sourced in the Auto-parallel module of MindSpore5, a deep learning training/inference framework that could\nbe used for mobile, edge and cloud scenarios. Besides the basic parallel functionalities, Auto-parallel is easy enough to\nuse by freeing developers from parallel model training with minimal (or zero) code modiﬁcations from the standalone\nversion, as if the model is trained on a single device.\nThe reminder of this technical report is organized as follow. Section 2 describe the architecture of our PanGu-αmodels.\nIn section 3, we detail our methods to construct a 1.1TB high-quality training corpus from 80TB raw data collected\nfrom various sources. Section 4 addresses the parallelization paradigm of model training and scheduling strategy on a\ncluster of Ascend processors. Section 5 presents the experimental results of PanGu-αmodels on various tasks.\n2 Model\n2.1 Overview\nPanGu-αis a large-scale autoregressive language model (ALM) pretrained on a large corpus of text, mostly in Chinese\nlanguage. It models the generative process of all the tokens in the corpus, where the generation of a token depends\non its previous tokens in a sequence. Assuming that a sequence X = {x1,x2,...,x N }is composed of N tokens, the\ntraining objective can be formulated as maximization of the log-likelihood:\nL=\nN∑\nn=1\nlog p(xn|x1,...,x n−1; θ), (1)\nwhere p(xn|x1,...,x n−1; θ) is the probability of observing the n-th token xn given the previous context x1:n−1, and θ\ndenotes the model parameters.\nThe architecture of PanGu-αis based on Transformer [13], which has been extensively used as the backbone of a variety\nof pretrained language models such as BERT [2] and GPT [10, 11, 1]. Different from them, we develop an additional\nquery layer on top of Transformer layers to predict the next token. The diagram of the model is shown in Figure 1. We\nelaborate each part as follow.\n2.2 Model Structure\n2.2.1 Transformer Layers\nA standard transformer layer includes two sub-layers: multi-head attention (MHA) and fully connected feed-forward\nnetwork (FFN).\nMulti-head Attention: A self-attention network in the l-th Transformer layer is parameterized by four projection\nmatrices: Wk\nh ,Wq\nh,Wv\nh ,Wm\nh ∈Rd×d/Nh , where dis the hidden dimension, his the index of head, and Nh is the\nnumber of heads. Given the output Hl−1 ∈RN×d from the precedent layer, three major components, i.e., query\nQh = Hl−1Wq\nh, key Kh = Hl−1Wk\nh , and value Vh = Hl−1Wv\nh are produced. The attention function is computed as:\nAh = QhK⊤\nh = Hl−1Wq\nhWk⊤\nh H⊤\nl−1,\nAttentionh(Hl−1) =Softmax( Ah√\nd\n)Vh = Softmax( Ah√\nd\n)Hl−1Wv\nh . (2)\n5https://gitee.com/mindspore/mindspore\n3\nTECHNICAL REPORT - APRIL 27, 2021\nthe cat onsat\n1 2 3 4\n+ + + +\nBOS\n0\n+\nToken\nPosition\nthe\n5\n+\nmat\nTransformer\nLayers\nQuery layer\n6\nFigure 1: The architecture of PanGu-α. The model is based on a uni-directional Transformer decoder. A query layer is\nstacked on top of Transformer layers with the position embedding as the query in the attention mechanism to generate\nthe token at the next position.\nWith multiple attention heads, the output becomes:\nMHA(Hl−1) =\nNh∑\nh=1\nAttentionh(Hl−1)Wm\nh ,\nHMHA\nl = Hl−1 + MHA(LayerNorm(Hl−1)).\n(3)\nFeed-forward Network: The FFN layer is composed of two linear layers, parameterized byW1 ∈Rd×dff , b1 ∈Rdff ,\nW2 ∈Rdff ×d, b2 ∈Rd, where dff is the dimension of the inner-layer. Fed with the output of MHA layer as input, the\noutput of FFN layer is then computed as:\nFFN(HMHA\nl ) =GeLU(HMHA\nl W1 + b1)W2 + b2,\nHl = HMHA\nl + FFN(LayerNorm(HMHA\nl )).\n(4)\nFor both MHA and FFN, we take the pre-layer normalization scheme, which can make the training of Transformer\nmodel easier and faster [14].\n2.2.2 Query Layer\nWe design the query layer on top of the stacked Transformer layers, which aims to explicitly induce the expected output.\nIn the pretraining stage of the autoregressive model, it comes to the prediction of the next token. The structure of the\nquery layer resembles the transformer layer, except that an additional embedding pn ∈Rd indicating the next position\nis used as the query vector in the attention mechanism. Speciﬁcally, assuming HL is the output of the uppermost\ntransformer layer, the attention vector in the query layer is computed as:\nah = pnWq\nhWk⊤\nh H⊤\nL . (5)\nThe subsequent computation of MHA and FFN remains the same as the original Transformer. We denote the ﬁnal\noutput as on. The negative log-likelihood of next token becomes:\nCrossEntropy(xn,Softmax(onWo + bo)), (6)\nwhere xn denotes the true token and Wo,bo is the additional task-dependent parameters.\n2.2.3 Model Conﬁgurations\nTo evaluate the scaling ability of the PanGu-αmodel, we train three models with increasing magnitude of parameter\nsizes, that is, PanGu-α2.6B, PanGu-α13B, and PanGu-α200B. Table 1 shows the detailed conﬁgurations of the three\nmodels, including the number of total parameters, the hidden dimension for the tokens, the inner dimension of the\nfeed-forward layer, and the number of attention heads.\n4\nTECHNICAL REPORT - APRIL 27, 2021\nTable 1: Model sizes and hyperparameters of PanGu-αmodels.\nModel #Parameters #Layers ( L) Hidden size ( d) FFN size ( dff ) #Heads ( Nh)\nPanGu-α2.6B 2.6B 32 2560 10240 40\nPanGu-α13B 13.1B 40 5120 20480 40\nPanGu-α200B 207.0B 64 16384 65536 128\nBig data management platform\nData cleaning Data filtering Deduplication\nModel-based \nevaluation(PanGu-α-350M)\nManual \nevaluation\nAdd/Modify rules Improve the \nmodel\nPublic datasets\nCommon Crawl\ne-Books\n Dataset\nAdd/Modify rules Improve the \nmodel\nEncyclopedia\nNews\nFigure 2: The data sources and the process of constructing pretraining data for PanGu-α.\n3 Dataset\nA large-scale Chinese text corpus of high quality is crucial for the pretraining of our PanGu-αmodels, especially the\none with 200B parameters. Existing large-scale text corpora for pretraining super large language models are mainly\nEnglish. For example, the GPT-3 [ 1] is trained using a dataset which contains 570GB ﬁltered texts from Common\nCrawl with 92.6% of the words are English. The Colossal Clean Crawled Corpus (C4) for training T5 consists of about\n750GB clean English texts scraped from the web [6]. To the best of our knowledge, there are three Chinese text corpora\nthat are above 100GB: (a) CLUECorpus2020 (100GB), which is retrieved from the Common Crawl dataset [15]; (b) the\nChinese multi-modal pretraining data, released by [16] which contains 300GB texts; and (c) WuDaoCorpus6, which\nopens about 300GB text data to only speciﬁc partners so far. However, all the above datasets are still not enough to\ntrain the super large-scale models up to 200B parameters compared to the data size used in existing English pretrained\nmodels.\nEven though the raw web datasets such as SogouT7 and Common Crawl8 contain massive amount of Chinese texts, the\nconstruction of our desired dataset is still challenging due to the highly varying quality of the raw web data, the huge\namount of storage and computation to preprocess the data, and the lack of well-deﬁned metrics to evaluate the quality\nof the data.\nTo tackle the aforementioned issues, we construct a 1.1TB high-quality Chinese text corpus by cleaning and ﬁltering\nenormous raw data from multiple sources. A big data management platform is built to accelerate the massive data\nanalysis and processing. Both manual and model-based evaluation measures are used to guide the data preprocessing\nand training data selection, as detailed in the following sections.\n3.1 Dataset Construction\nTo construct a large-scale high-quality Chinese corpus, we collect nearly 80TB raw data from the public datasets\n(e.g., BaiDuQA, CAIL2018, Sogou-CA, etc.), web pages data from Common Crawl, encyclopedia, news and e-books.\nAs shown in Figure 2, our data construction process includes three steps: rule-based data cleaning, model-based\ndata ﬁltering and text deduplication. To improve the quality of the training dataset, the ﬁrst two steps (i.e., cleaning\nand ﬁltering) are iteratively enhanced via manual and model-based data quality evaluations. The data construction\nprocess is done on a big data management platform built based on the open source Spark/Hadoop framework using\n6https://data.baai.ac.cn/data-set-details/0c8dc71dd06ae75a10ca422fb49b0751\n7https://www.sogou.com/labs/resource/t.php\n8https://commoncrawl.org/the-data/\n5\nTECHNICAL REPORT - APRIL 27, 2021\nTable 2: Processing time for each step in the dataset construction.\nData size Our platform\nCleaning 20TB 70+ hours\nFiltering 800GB 10+ hours\nFuzzy deduplication 500GB 3.5 hours\n8 high-performance computing nodes9. With the distributed processing capability and the tools of our platform, the\nefﬁciency of the data analysis and processing is signiﬁcantly improved (see Table 3.1 for the processing time). Next, we\nintroduce the details of each step in the dataset construction process.\n3.1.1 Cleaning and Filtering\nAmong the ﬁve data sources as shown in Fig 2, the Common Crawl data contributes the most amount to our corpus but\nunfortunately contains a signiﬁcant amount of low-quality web pages. To improve the data quality, we ﬁrst adopt the\nfollowing rule-based text cleaning strategies over the raw web pages from Common Crawl:\n• Remove the document which contains less than 60% Chinese characters, or less than 150 characters, or only\nthe title of a webpage;\n• Remove the special symbols and duplicated paragraphs in each document;\n• Identify advertisements based on keywords and remove documents which contain advertisements;\n• Convert all traditional Chinese text to simpliﬁed Chinese;\n• Identify the navigation bar of the web page and remove it.\nThen, three ﬁlters are applied to the preprocessed documents to further remove the harmful, advertising and low-quality\ndocuments.\n• Sensitive word ﬁltering: The original documents of Common Crawl include a lot of harmful or sensitive\nwebsite contents which would mislead our generative model. Thus, we manually collect 724 sensitive words\nand remove documents containing more than three of the sensitive words.\n• Model-based spam ﬁltering: To further remove the advertisements and spams, we train a spam classiﬁcation\nmodel using fastText10 on a manually labeled dataset. The negative training examples are 10K junk documents\nmanually selected from the Common Crawl dataset, and the positive examples are sampled from the high-\nquality Chinese text corpus. We remove the documents that are classiﬁed as spams.\n• Low-quality document ﬁltering: Following the practice in GPT-3, we train a classiﬁer to score the quality of\neach document and eliminate the documents with scores below a threshold (see Appendix A of [1] for details).\n3.1.2 Text Deduplication\nAlthough we have removed duplicated paragraphs in each document in the previous step, there are still documents with\nhighly overlapped content across different data sources. Therefore, we carry out fuzzy data deduplication over the\ndocuments across all our data sources.\nDue to the super large scale of the whole dataset, the conventional MinHashLSH algorithm in Spark incurs more than\n8 hours to duplicate less than 200MB data, which is too slow to meet our efﬁciency requirement. To accelerate the\ndeduplication process, we design a distributed large-scale text data duplication detection and deduplication algorithm\nby exploiting the computing framework of our big data management platform. The proposed algorithm takes only\n3.5 hours to complete the deduplication process for 500GB documents.\n3.1.3 Data Quality Evaluation\nGive above preprocessing steps, one key question is how the cleaning rules and the ﬁltering thresholds are decided. In\nthis work, we evaluate the data quality after each round of preprocessing and update the cleaning rules and the ﬁltering\n94 computing nodes with 28TB storage + 2 CUPs (24 cores) + 1.5TB Memory and 4 computing nodes with 7.3TB storage + 2\nCPUs (64 cores) + 1TB memory.\n10https://fasttext.cc/\n6\nTECHNICAL REPORT - APRIL 27, 2021\nTable 3: Data composition of the 1.1TB Chinese text corpus.\nSize (GB) Data source Processing steps\nPublic datasets 27.9 15 public datasets including DuReader,\nBaiDuQA, CAIL2018, Sogou-CA, etc.\nFormat conversion11 and text\ndeduplication\nEncyclopedia 22 Baidu Baike, Sogou Baike, etc. Text deduplication\ne-Books 299 e-Books on various topics (e,g., novels, his-\ntory, poetry, ancient prose, etc.).\nSensitive word and model-\nbased spam ﬁltering\nCommon Crawl 714.9 Web data from January 2018 to December\n2020 from Common Crawl. All steps\nNews 35.5 News data from 1992 to 2011. Text deduplication\nTable 4: Sampling strategy of the corpora in training PanGu-αmodels.\nPanGu-α200B PanGu-α2.6B&13B\nQuantity\n(tokens)\nWeight in\ntraining mix\nEpochs elapsed\nwhen training\nQuantity\n(tokens)\nWeight in\ntraining mix\nPublic datasets 25.8B 10.23% 3.65 7B 27.99%\ne-Books 30.9B 12.23% 0.41 5.6B 18%\nCommon Crawl 176.2B 62.81% 0.85 2.5B 10%\nNews 19.8B 7.83% 2.2 5.6B 22%\nEncyclopedia data 5.8B 6.9% 3 5.8B 23%\nmodels according to the evaluation results. Both manual and model-based evaluations are considered. The manual\nevaluation is conducted over randomly sampled texts from the perspectives of sentence smoothness and the amount of\nlow-quality contents (e.g., advertisements, repeated short sentences, spams, etc.). However, the manual evaluation can\nonly cover a very small proportion of the whole dataset. To improve the accuracy of the data evaluation, we train the\nPanGu-α350M model using 30GB data sampled from the preprocessed dataset and evaluate the data quality using the\nPPL on a high-quality development dataset. The preprocessed dataset that achieves lower PPL is considered to have\nhigher quality and its corresponding cleaning rules and ﬁltering models are considered to be better.\n3.2 Training Data Selection\nUsing the construction process in Figure 2, a Chinese text corpus with 1.1TB data is built from the ﬁve types of data\nsources. The composition of our corpus and the processing steps adopted to each data source is shown in Table 3.2.\nBased on the new corpus, we construct two training datasets with 100GB and 1TB text data for our medium (2.6B and\n13B) and large (200B) models, respectively. As shown in Table 3.2, each data source is sampled during training with\ndifferent proportions according to the quality of the processed dataset evaluated using the method in Section 3.1.3. The\ndistribution of the number of token in each training dataset is shown in Figure 3. The averaged document lengths of\nthe 100GB and 1TB dataset are 239 and 405 tokens, respectively. The 1TB dataset has a larger averaged document\nlength due to the large proportion of Common Crawl dataset. Note that the length of the text will affect the generation\nperformance of the model. When the averaged number of token for the training samples is small, the model will be\nbiased to generate short texts and be good at processing downstream tasks requiring short texts, and vice versa.\n4 System\nTraining PanGu-α200B and using it for inference are difﬁcult. The memory requirement for just storing PanGu-α200B\nis around 750 GB. Training such a huge model consumes several times more memory than just storing the parameters,\nsince the gradients and optimizer states are also essential for updating the parameters. As a contrast, the memory\nof modern AI processors (e.g., GPU, Ascend 910 AI processor [12]) is still around 30-40 GB. Thus, it is inevitable\nto partition the model to a collection of devices (processors). The problem is challenging in two perspectives. First,\nmultiple basic parallel functionalities should be combined to acquire the end-to-end high performance. Finding the best\n11We remove the labels in all the labeled datasets such that the model is trained for few-shot learning instead of multi-task learning.\n7\nTECHNICAL REPORT - APRIL 27, 2021\n(a) 1TB dataset (b) 100GB dataset\nFigure 3: The distribution of tokens in (a) 1TB dataset and (b) 100GB dataset. The total number of tokens represents\nthe (number of tokens in each document) ×(number of documents with this token number).\nLayer n\nLayer 2\nLayer 1\n...\nDevice 1\nLayer n\nLayer 2\nLayer 1\n...\nDevice 2\nMini-batch\n...\nLayer n\nLayer 2\nLayer 1\nDevice 1\n Device 2\n...\nSub-layer n\nSub-layer 2\nSub-layer 1\n...\nSub-layer n\nSub-layer 2\nSub-layer 1\nLayer n\nLayer 2\nLayer 1\n...\nDevice 1 Device 2\n Device 1\n Device 2Device 1 Device 2\nLayer 2\nLayer 1\nLayer n\n...\n(a) Data is partitioned in data \nparallelism\n(b) Each layer is partitioned in \nop-level model parallelism\n(c) Different layers are placed on different \ndevices in pipeline model parallelism\nDevice 1\n Device 2\nLayer n\nLayer 2\nLayer 1\n...\nLayer n\nLayer 2\nLayer 1\n...\nGradients\n Optimizer \nstates\nGradients\n1\nOptimizer\nstates 1\nGradients\n2\nOptimizer\nstates 2\nUpdate Update\nMatMul\nMatMul\nSoftmax\nDropout\n Dropout-\ngrad\nSoftmax-\ngrad\nMatMul-\ngrad\nMatMul-\ngrad\nReLU\n ReLU-\ngrad\nInputs Gradients\nCov2D\nPReLU\nMatMul\nPReLU\n PReLU-\ngrad\nMatMul-\ngrad\nPReLU-\ngrad\nCov2D-\ngrad\nMatMul\n MatMul-\ngrad\nMatMul\nMatMul\nSoftmax\nDropout\n Dropout-\ngrad\nSoftmax-\ngrad\nMatMul-\ngrad\nMatMul-\ngrad\nReLU\n ReLU-\ngrad\nCov2D\nPReLU\nMatMul\nPReLU\n PReLU-\ngrad\nMatMul-\ngrad\nPReLU-\ngrad\nCov2D-\ngrad\nMatMul\n MatMul-\ngrad\nMatMul\nPReLU\nInputs Gradients\n(d) Optimizer states and gradients are partitioned along \ndata parallelism in optimizer model parallelism\n(e) Some activation memories are abandoned in the forward \nphase to reduce the peak memory consumption\nFigure 4: Five parallel functionalities, and how each works to optimize memory and throughput.\ncombination strategy is challenging due to the huge strategy space. Second, parallel training should be easy to use,\nand the underlying parallel-related code should be removed from the model deﬁnition code. We use Auto-parallel in\nMindSpore to address the problem by maximizing the ratio of the computation over the communication. Auto-parallel\nsupports ﬁve-dimensional parallel functionalities, and employs topology-aware scheduling to map partitioned model\nslices to the cluster for the end-to-end high performance. Furthermore, Auto-parallel enables the least code modiﬁcations\nfrom the standalone version for parallel training.\n8\nTECHNICAL REPORT - APRIL 27, 2021\nRank 0\nRank 1\nRank 2\nRank 3\nRank 4\nRank 5\nRank 6\nRank 7\nRank 8\nRank 9\nRank 10\nRank 11\nRank 0\nRank 1\nRank 2\nRank 3\nRank 4\nRank 5\nRank 6\nRank 7\nRank 8\nRank 9\nRank 10\nRank 11\nRack 1 Rack 2\nServer 1\nDevice 12\nDevice 13\nDevice 14\nDevice 15\nDevice 16\nDevice 17\nDevice 18\nDevice 19\nDevice 20\nDevice 21\nDevice 22\nDevice 23\nDevice 0\nDevice 1\nDevice 2\nDevice 3\nDevice 4\nDevice 5\nDevice 6\nDevice 7\nDevice 8\nDevice 9\nDevice 10\nDevice 11\nServer 2 Server 3 Server 4 Server 5 Server 6\nData parallelism & \noptimizer model parallelism\nOp-level\nmodel\nparallelism\nPipeline model parallelism \n& rematerialization\nDevice \n Device \nDevice \n Device \nServer\nDevice \n Device \nDevice \n Device \nServer\nToR Switch\nDevice \n Device \nDevice \n Device \nServer\nDevice \n Device \nDevice \n Device \nServer\nToR Switch\nSpine Switch\n(b) A brief example of hardware \ntopology\n(a) How the partitioned model and \ndata are mapped onto the hardware \nTopology-aware \nscheduling\nFigure 5: A combined parallelization of the model, and how it is scheduled to the cluster.\n4.1 Five-dimensional Parallelisms and Topology-aware Scheduling\nThe most applied parallelism way is data parallelism, which partitions the training batches across devices, and\nsynchronizes the gradients from different devices before taking an optimizer step, as shown in Figure 4(a). There\nare three regimes in model parallelism. One regime is op-level model parallelism [17, 18, 19, 20, 21, 22, 23], which\npartitions its involved tensors of each operator (layer), as shown in Figure 4(b). Op-level model parallelism reduces the\nmemory consumption by slicing the parameters and the activation memory, however, it introduces communications\nto keep the distributed tensor layouts consistent between successive operators. The second regime is pipeline model\nparallelism [24, 25, 26, 27, 28], which partitions the total layers to stages, and then places stages to different devices, as\nshown in Figure 4(c). The memory beneﬁt comes from that each device holds a subset of total layers of the model, and\nthe communications only happen at the boundaries of stages. The third regime is optimizer model parallelism [ 29]\n(Figure 4(d)), which aims to reduce the redundant optimizer memory and computation consumption resulted from data\nparallelism. Some outputs of operators in forward phase reside in memory for a fairly long time, because they are used\nin the backward phase for gradient calculations. Rematerialization (Figure 4(e)) abandons these memories to reduce the\npeak memory consumption in the whole training time, by recomputing the corresponding forward operators.\nEach parallelism dimension trades computation (or communication) overheads for memory (or throughput) beneﬁts. To\nacquire maximum end-to-end throughput, a balanced composition point should be found along these dimensions. The\nproblem becomes more challenging when considering the heterogeneous bandwidths in a cluster of devices.\nFigure 5(b) demonstrates a typical organization of a cluster. Each server includes multiple devices, and the servers in\na rack are connected by a ToR (top of rack) switch. Racks are then connected by the Spine switch. The bandwidth\nbetween devices in a server is greater than that across servers in a rack, and the latter one is greater than that across\nracks. Therefore, the model is partitioned across servers in a rack using the pipeline parallelism regime, resulting in that\neach server holds a stage of the model layers. Then, the stage is split using the op-level parallelism across the devices\nin each server, in order to utilize the high bandwidths. Each rack owns the whole model, and different racks are data\nparallel. Deploying data parallelism and optimizer parallelism across racks is due to that the induced communication\noperators are not on the critical path of the training iteration, which could be fused and overlapped with backward\npropagation to improve the performance.\nFigure 6 shows how a combined parallelization is applied to the PanGu-α200B model. First, 64 layers of the model\nare partitioned into 16 stages, each stage containing 4 layers. For each layer, involved parameters and tensors are\npartitioned for each operator. Speciﬁcally, the parameters involved in query ( Q), key (K) and value (V) operators\nare partitioned into 8 slices. The input tensor of these three operators is partitioned into 16 slices, and the number of\noptimizer model parallelism is determined accordingly.12 Parallelization strategies for other operators in the layer are\nconﬁgured likewise. Rematerialization is conﬁgured to perform within each layer, which limits the extra computation\noverheads. Totally, 2048 Ascend 910 AI processors are used to train the full PanGu-α200B model.\n4.2 Implementation\nThe parallel-related functionalities are implemented in the Auto-parallel module of MindSpore. The Auto-parallel\ndecouples machine learning models from complicated underlying parallel implementations, and let researchers focus on\n12The ‘8’ is called model parallel number and ‘16’ is called data (and optimizer) parallel number in our system. In the example of\nFigure 6, the model parallel number and data parallel number are both 2.\n9\nTECHNICAL REPORT - APRIL 27, 2021\nMatMul\nX1\nBMatMul\nMatMul\nX2\nQ11\nAttention\nQ12\nQ21\n Q22\nQij = XiPj\nSoftmax\n Dropout\nMatMul\nBMatMul\n MatMul\nP1\nP2\nV11\n V12\nV21\n V22\nX1\nX2\nMatMul\n MatMul\nFeedForward\nLayer 1 (Pipeline stage 0)\nK11\n K12\nK21\n K22\nAllReduce\n AllReduce\nMatMul\nX1\nBMatMul\nMatMul\nX2\nQ11\nAttention\nQ12\nQ21\n Q22\nQij = XiPj\nSoftmax\n Dropout\nMatMul\nBMatMul\n MatMul\nP1\nP2\nV11\n V12\nV21\n V22\nX1\nX2\nMatMul\n MatMul\nFeedForward\nLayer 2 (Pipeline stage 1)\nK11\n K12\nK21\n K22\nAllReduce\n AllReduce\nFigure 6: A simpliﬁed PanGu- α’s parallelization strategy. The ellipsoids stand for the operators, blue rectangles\nrepresent tensors, and green rectangles represent trainable parameters. Parameters are partitioned along the row and\ncolumn dimension respectively, and the input tensor is partitioned along the row dimension. And, two layers are\nassigned to different pipeline stages.\nthe development of new models. Auto-parallel enables parallel training by just adding annotations on the standalone\nmodel script. Here, we brieﬂy go through two model parallelism regimes.\nFigure 7 shows how to specify the combined parallelization strategy to PanGu-α. Figure 7(a) and Figure 7(b) shows\nthe pseudocode of conﬁguring Attention and FeedForward to conduct op-level parallelism, respectively. qkv_mm’s\nsharding strategy is ((2, 1), (1, 2)), indicating that x is partitioned along the row (batch or data) dimension into\n2 slices, while q_w, k_w and v_w are partitioned along the column dimension. Since the device number is 4 here, each\ndevice holds a distinct pair of a x’s slice and aq_w’s (k_w’s andv_w’s) slice.matmul’s sharding strategy is((2, 2),\n(2, 1)), where the contracting dimension is partitioned, thus an AllReduce is needed here to perform the operation.\nLikewise, another AllReduce is needed in Figure 7(b)’s matmul2. Auto-parallel can ﬁnd such needed operators.\nFurthermore, the tensor redistribution is designed to automatically ﬁnd the transformation (a list of operators) between\nany two inconsistent distributed tensor layouts with minimum communication cost, and then the operators are inserted\ninto the data ﬂow graph. The sharding strategy of batch_mm in Figure 7(a) corresponds to splitting the batch and head\ndimension.\nFigure 7(d) shows the pseudocode of conducting pipeline parallelism in MindSpore. The number of stages is conﬁgured\nas 2, and the number of devices is 8. Thus, 4 devices together perform each stage. The layer1 is conﬁgured to be\nthe stage 0, thus replicated on 4 devices. Likewise, layer2 is replicated on the other 4 devices. If combined with\nFigure 7(a) and Figure 7(b), the desired parallelization strategy is obtained to PanGu- α.13 Send and Receive are\ninferred to communicate the activation output from stage 0 to stage 1, and then are automatically inserted into the data\nﬂow graphs on two stages, respectively.\nIn the future, we will: a) develop a cost model and a parallelization strategy searching algorithm for all parallelism\ndimensions in order to completely liberate developers from the underlying parallel-related works; b) support the\nheterogeneous-parallelism to ofﬂoad a part of tensors and the corresponding computations to the host CPU to accelerate\nthe training; c) use Sparse Attention to speedup the computation.\nAll training and inference jobs are run on the ModelArts14 platform, which manages the end-to-end workﬂows and\nprovides the functionality of cluster scheduling for a job to acquire a hierarchical cluster.\n13The stategy of optimizer parallelism is hidden in how batch dimension is split in the conﬁguration. We omit the conﬁguration\nfor rematerialization here.\n14https://www.huaweicloud.com/product/modelarts.html\n10\nTECHNICAL REPORT - APRIL 27, 2021\n(b) The pseudocode of configuring FeedForward in \nop-level model parallelism in MindSpore\n(a) The pseudocode of configuring Attention in op-\nlevel model parallelism in MindSpore\ncontext.set_auto_parallel_context(device_num=4)\nclass Attention(nn.Cell):\n    def __init__(self, in, h, out):\n        self.qkv_mm = ops.MatMul().shard(((2, 1), (1, 2)))\n        self.batch_mm = ops.BatchMatMul().shard(((2, 2, 1, 1), (2, 2, 1, 1)))\n        self.softmax = ops.Softmax().shard((2, 2, 1, 1))\n        self.drouput = ops.Dropout().shard((2, 2, 1, 1))\n        self.matmul = ops.MatMul().shard(((2, 2), (2, 1)))\n        self.reshape = ops.Reshape()\n        self.q_w = Parameters([in, h])\n        self.k_w = Parameters([in, h])\n        self.v_w = Parameters([in, h])\n        self.o_w = Parameters([h, out])\n    def construct(self, x): \n        q = self.qkv_mm(x, self.q_w)\n        k = self.qkv_mm(x, self.k_w)\n        v = self.qkv_mm(x, self.v_w)\n        # reshape to 4D, then batchmatmul\n        bmm_out = self.batch_mm(self.reshape(q), self.reshape(k))\n        dropout = self.dropout(self.softmax(bmm_out))\n        bmm2_out = self.batch_mm(dropout, self.reshape(v))\n        # reshape back to 2D, then matmul\n        z = self.matmul(self.reshape(bmm2_out), self.o_w)\n        return z\n(c) The pseudocode of composing Attention and \nFeedForward in MindSpore\n(d) The pseudocode of pipeline model parallelism in MindSpore\nclass Layer(nn.Cell):\n    def __init__(self):\n        self.attention = Attention()\n        self.ffn = FeedForward()\n    def construct(self, x):\n        z = self.ffn(self.attention(x))\n        return z\ncontext.set_auto_parallel_context(device_num=4)\nclass FeedForward(nn.Cell):\n    def __init__(self, in, h, out):\n        self.matmul1 = ops.MatMul().shard(((2, 1), (1, 2)))\n        self.gelu = ops.GeLU().shard((2, 2))\n        self.matmul2 = ops.MatMul().shard(((2, 2), (2, 1)))\n        self.dropout = ops.Dropout().shard((2, 1))\n        self.w_i = Parameters([in, h])\n        self.w_o = Parameters([h, out])\n    def construct(self, x): \n        y = self.matmul1(x, self.w_i)\n        gelu_out = self.gelu(y)\n        mm_out = self.matmul2(gelu_out, self.w_o)\n        z = self.dropout(mm_out)\n        return z\ncontext.set_auto_parallel_context(device_num=8, pipeine_stages=2)\nclass Model(nn.Cell):\n    def __init__(self):\n        self.layer1 = Layer().pipeline_stage = 0\n        self.layer2 = Layer().pipeline_stage = 1   \n    def construct(self, x): \n        y = self.layer1(x)\n        z = self.layer2(y)\n        return z\nFigure 7: The pseudocode of conﬁguring op-level and pipeline parallelism in MindSpore. The red bold fonts are\nkeywords to specify parallelization strategies.\nTable 5: The detailed settings for training PanGu-αmodels.\nModels #Training Steps #Ascend processors Adam Betas Learning Rate Weight Decay\nPanGu-α2.6B 0∼70,000 512 β1=0.9 ,β2=0.999 1e-4 0.01\nPanGu-α13B 0∼84,000 1024 β1=0.9 ,β2=0.98 5e-5 0.01\nPanGu-α200B 0∼130,000 2048 β1=0.9 ,β2=0.95 2e-5 0.1130,000∼260,000 1024\n5 Experiments\n5.1 Training Details\nOur PanGu-αmodels are developed under the Mindspore framework and are trained on a cluster of 2048 Ascend 910\nAI processors. The detailed settings are shown in Table 5. For the training of the 200B model, we use 2048 Ascend\nprocessors at the ﬁrst phase and then switch to 1024 Ascends processors in the middle, in order to conduct other\nexperiments using the rest of resources. The Byte Pair Encoding (BPE) is used as the tokenizer, and the vocabulary size\nare 40,000. The sequence length for the training data is set to 1024 for all the models.\nThe curves of training loss for the PanGu-αmodels are shown in Figure 8. We adopt the number of training tokens as\nthe x-axis since the batch size for the 200B model is not comparable to that of the 13B and 2.6B models. The loss of\n200B model converges to around 2.49, while the losses of 13B and 2.6B models converge to 2.58 and 2.64 respectively.\nFrom the training curves, we can observed that the losses are still decreasing by the end of training, which indicates that\nour PanGu-αmodel are still under-trained, and may have great potential to improve. We also evaluate the perplexity of\nour PanGu-αmodels on the validation set, which is randomly sampled from the Common Crawl dataset. The results in\nTable 6 show that PanGu-αmodels with larger parameters sizes achieve smaller perplexity values, indicating that larger\nPanGu-αmodels are better language models.\n11\nTECHNICAL REPORT - APRIL 27, 2021\n0 1 2 3 4 5 6 7\nTokens 1e10\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0Loss\n2.6B\n13B\n200B\nFigure 8: Training curves of three PanGu- αmodels with different model sizes. The x-axis denotes the number of\ntraining tokens, which is measured as training_steps∗batch_size ∗sequence_length. The y-axis denotes the\ntraining loss.\nTable 6: The validation perplexity of the PanGu-αmodels.\nModels Validation PPL\nPanGu-α2.6B 19.33\nPanGu-α13B 17.69\nPanGu-α200B 15.59\n5.2 Task Description\nIn this section, we evaluate our models on a broad spectrum of natural language processing tasks. Similar to the GPT-3\n[1], the experiments are conducted under three learning settings, i.e., zero-shot, one-shot, and few-shot, without any\nﬁnetuning. For each task, we evaluate the models with the test sets when publicly available. Otherwise, we use the\ndevelopment sets instead. For some tasks with a very large test set or development set, we randomly sample a subset\nfrom the dataset in the experiments to reduce the computational cost. The evaluation datasets are classiﬁed into 7\ncategories by the task similarities, and we describe each category as follows.\nCloze and completion tasks , including WPLC, CHID [ 30], PD&CFT [ 31], CMRC2017 [ 32], and CMRC2019\n[33]. Chinese WPLC (Word Prediciton with Long Context) is a dataset created to test the ability to model long-range\ndependencies, similar to the LAMBADA dataset [ 34] for English. The CHID (Chinese IDiom dataset) requires the\nmodel to identify the ground-truth idiom from 10 candidate idioms. The PD&CFT task requires the model to predict\nthe mask words in sentences derived from People’s Daily (PD) news dataset and Children’s Fairy Tale (CFT) dataset.\nThe CMRC2017 (Chinese Machine Reading Comprehension) task contains two different sub-task: cloze-style task and\nuser query reading comprehension task, among which we only evaluate our models on the cloze-style task. While the\naforementioned tasks are word-level tasks, the CMRC2019 is a sentence cloze-style dataset that involves ﬁlling the\nright sentence from several candidate sentences into the passage. For the CMRC2019 and the CHID, a list of candidate\nchoices are provided, making them classiﬁcation tasks, while for WPLC, CMRC2017 and PD&CFT, the models need to\ngenerate the answer as no candidate choices are given. Accuracy metric is employed for evaluating the cloze-style tasks.\n12\nTECHNICAL REPORT - APRIL 27, 2021\nTable 7: The input&prompt template for each task.\nTask Dataset Input&Prompt\nCloze and completion\nWPLC\nCHID\nPD&CFT\nCMRC2017\nCMRC2019\n/\n/\n/\n/\n/\n/\nReading comprehension\nCMRC2018\nDRCD\nDuReader\n阅读文章：$Document\\n问：$Question\\n答：(Read document: $Document\\nQuestion：$Question\\nAnswer: )\n阅读文章：$Document\\n问：$Question\\n答：(Read document: $Document\\nQuestion：$Question\\nAnswer: )\n阅读文章：$Document\\n问：$Question\\n答：(Read document: $Document\\nQuestion：$Question\\nAnswer:\nClosed book QA WebQA 问：$Question\\n答：(Question：$Question\\nAnswer: )\nWinograd-Style CLUEWSC2020 /\nCommon sense reasoning C3 问: $Question\\n答:$Choice\\n该答案来自对话: $Passage (Question: $Question\\nAnswer:$Choice\\nAnswer from dialogue: $Passage)\nNLI CMNLI\nOCNLI\n$S1?对/或许/错，$S2 ($S1?Yes/Maybe/No, $S2)\n$S1?对/或许/错，$S2 ($S1?Yes/Maybe/No, $S2)\nText classiﬁcation\nTNEWS\nIFLYTEK\nAFQMC\nCSL\n这是关于$label的文章：$passage (This passage is about$label: $passage)\n这是关于$label的应用程序：$passage (This application is about: $passage)\n下面两个句子语义相同/不同：$S1。$S2 (The following two sentences have the same/different semantics: $S1. $S2)\n摘要：$passage，关键词：$keyword是/不是真实关键词(Abstract: $passage, keywords: $keyword True/False keywords)\nReading comprehension tasks, including CMRC2018 [ 35], DRCD [ 36], and DuReader [ 37]. These are all span-\nextraction tasks originally. That is, given a passage as context and a question, the models need to extract a text span from\nthe passage which contains the correct answer to the question. The evaluation metrics, including F1 and exact match\n(EM), measure the similarity between the predicted span and the ground-truth text span. Instead of span-extraction,\nwe formulate these task as generation tasks where the models generate the texts directly. The similarity between the\ngenerated text span and the ground-truth text span is evaluated. Note that for the DuReader task, we select the Zhidao\nsubset for evaluation in our experiment.\nClosed-book question answering (QA) tasks, including WebQA [ 38]. We follow the same closed-book setting in\nGPT-3 [ 1], where the models are not allowed to access any external knowledge when answering open-domain factoid\nquestions about broad factual knowledge.\nWinograd-Style tasks, including CLUEWSC2020 [ 39]. CLUEWSC2020 is a Chinese Winograd Schema Challenge\ndataset, which is an anaphora/coreference resolution task. In practice, we convert the task into a multiple-choice\nclassiﬁcation problem.\nCommon sense reasoning tasks, including C3 [39]. C3 is a free-form multiple-choice reading comprehension dataset\nwhich can beneﬁt from common sense reasoning. Different from the extraction-based reading comprehension tasks, the\nanswers to of C3 questions cannot be directly found in the given context. Therefore, we use it to evaluate the common\nsense reasoning ability of the models.\nNatural language inference (NLI) tasks, including Chinese Multi-Genre NLI (CMNLI) and Original Chinese Natural\nLanguage Inference (OCNLI) [ 39]. The NLI tasks require the model to identify the relation between two sentences,\neither entailment, neutral or contradiction. We formulate these tasks as three-class classiﬁcation problems.\nText classiﬁcation tasks, including TouTiao Text Classiﬁcation for News Titles (TNEWS), IFLYTEK app description\nclassiﬁcation (IFLYTEK), Ant Financial Question Matching Corpus (AFQMC), and Chinese Scientiﬁc Literature\n(CSL) [ 39]. These text classiﬁcation tasks covers broad domains of text, including news, applications, ﬁnancial text,\nscientiﬁc text. For the TNEWS and IFLYTEK tasks, there are 15 and 119 categories originally. However, we randomly\nsample three candidates as negative labels for each instance and perform 4-class classiﬁcation. The reason is that the\ncomputational cost of our perplexity-based classiﬁcation method increases linearly to the total number of candidate\ncategories, which will be described in the next section.\n5.3 Evaluation Details\nThe tasks can be generally classiﬁed into two-categories: classiﬁcation tasks and generation tasks. For the classiﬁcation\ntasks, we resolve the task as perplexity comparison tasks. For some tasks, the samples needs to be ﬁlled into a\ntailor-designed template as the input to the models. The templates for each task are described in Table 7, where \"/\"\nmeans the task does not involve a template. The decoding strategies for these text generation tasks are described in\nTable 8.\n13\nTECHNICAL REPORT - APRIL 27, 2021\nTable 8: The decoding strategies for text generation tasks.\nTask Dataset Decoding strategies\nCloze and completion\nWPLC\nPD&CFT\nCMRC2017\ntop-k, k=1\ntop-k, k=1,temperature=0.9\ntop-p, p=0.9, temperature=1\nReading comprehension\nCMRC2018\nDRCD\nDuReader\ntop-p, p=0.8, temperature=0.8\ntop-p, p=0.8, temperature=0.8\ntop-p, p=0.9, temperature=0.7\nClosed book QA WebQA top-k, k=5\n5.3.1 Generation method\nThe generation tasks include word-level generation tasks and sentence-level generation tasks. Since our PanGu- α\nmodels are autoregressive language models capable of text generation, the generation tasks can be solved naturally by\nsimply generating the answers. For the cloze tasks such as WPLC, PD&CFT, and CMRC2017, the prompts are the\ncontext before the positions to be predicted. For the reading comprehension tasks and closed book QA tasks, templates\nare designed if necessary. For example, in the reading comprehension tasks, the sample is ﬁlled into a template Reading\ndocument : $Document Question: $Question Answer: , which serves as the prompt for the model to generate the answer.\nAs in GPT-3, the few-shot task is designed as in-context learning, where Kprompts are concatenated one by one. The\nﬁrst K−1 prompts contain the ground truth answer while the last prompt is the sample we want to predict. An example\nfor CMRC2018 task is shown in Figure 9\nFigure 9: A prompt for generation task of CMRC2018\n5.3.2 Perplexity-based method\nThe perplexity-based method solves the classiﬁcations tasks. For each pair of <text, label>, an input will be generated\nautomatically according to a pre-designed criteria, as shown in Table 7. The sequence generated by the template will be\nfed into the model and a perplexity value will be computed. The label associated with the smallest perplexity value will\nbe considered as the predicted label for this passage.\nWe also employ the in-context learning strategy for solving few-shot tasks. An example for few-shot OCNLI task is\nshown in Figure 10.\n5.4 Results\nTable 9 compares PanGu-α2.6B with CPM [3] 15, a recently released generative Chinese PLM with 2.6B parameters,\non 16 downstream tasks in Chinese. PanGu- α2.6B achieves higher performance compared to CPM 2.6B on more\nthan 11 tasks in zero-shot setting, 12 tasks on the one-shot setting, and 14 tasks on the few-shot setting. In general, the\nexperimental results indicate that PanGu-α2.6B achieves higher in-context learning ability over CPM 2.6B, especially\nfor few-shot learning and generation-tasks. Regarding generation-tasks, PanGu-α2.6B outperforms CPM 2.6B with\nan improvement of 6 points on average. To be more speciﬁc, PanGu- α2.6B surpasses CPM 2.6B with 5 points in\n15https://github.com/TsinghuaAI/CPM-Generate\n14\nTECHNICAL REPORT - APRIL 27, 2021\nFigure 10: Prompt for perplexity-based tasks of OCNLI\nTable 9: Performance comparison of CPM 2.6B v.s. PanGu-α2.6B on few-shot NLP tasks.\nZero-Shot One-Shot Few-Shot\nDataset MethodMetrics Task Types CPM 2.6B PanGu-α2.6BCPM 2.6B PanGu-α2.6B#Shot(K) CPM 2.6B PanGu-α2.6B\nCMRC2018GenerationEm/F1 Read Comprehension0.59/10.121.21/16.6471.71/11.292.49/18.57Dynamic 3.11/14.645.68/23.22\nDRCD GenerationEm/F1 Read Comprehension0/4.62 0.8/9.99 0.22/5.172.47/12.48Dynamic 0.15/7.145.31/18.29\nDuReader GenerationRouge-1 Read Comprehension16.63 21.07 16.42 20.18 6,6 17.85 21.43\nWebQA GenerationEm/f1 Closed-Book QA 6/12.59 6/16.32 6/11.82 12/23.39 8,8 4/12.23 24/33.94\nPD-CFT GenerationAcc Cloze(without choices)35.73/38.9938.47/42.3933.3/39.7338.8/41.61 3,3 32.03/39.8439.07/42.05\nCMRC2017GenerationAcc Cloze(without choices)24.60 37.83 25.40 38.00 3,3 23.50 36.33\nCHID PPL Acc Cloze(multi-choices)68.62 68.73 67.91 68.16 3,3 66.82 66.56\nCMRC2019 PPL Acc Cloze (multi-choices)47.69 61.93 47.99 61.54 2,2 47.20 62.42\nCMNLI PPL Acc Natural Language Inference49.10 50.20 47.56 49.54 6,12 49.29 51.17\nOCNLI PPL Acc Natural Language Inference44.20 42.61 44.30 44.00 3,6 44.00 46.78\nTNEWS PPL Acc Text classiﬁcation 65.44 60.95 69.50 57.95 6,6 70.17 63.62\nIFLYTEK PPL Acc Text classiﬁcation 68.91 74.26 79.84 79.03 3,3 83.99 80.15\nAFQMC PPL Acc Sentence Pair Similarity66.34 59.29 39.70 64.62 4,4 38.29 69.00\nCSL PPL Acc Keyword Recognition52.30 50.50 51.20 50.90 10,10 50.50 52.00\nCLUEWSC2020PPL Acc WSC 73.684 73.36 73.684 75.33 14,14 70.065 72.70\nC3 PPL Acc Common Sense Reasoning49.81 53.42 51.43 52.82 3,3 51.60 53.64\nscores for both reading comprehension and closed-book QA tasks, 7 points in scores for cloze (without choices) tasks\nrespectively. Regarding perplexity-tasks, PanGu-αis comparable to CPM 2.6B on natural language inference with\nCMNLI and OCNLI datasets, while it is slightly worse than CPM on classiﬁcation tasks with TNEWS and IFLYTEK\ndatasets. We suppose that the main factor that contributes to the different performance of CPM 2.6B and PanGu- α\n2.6B is the training data. We collect massive and diverse data from a wide range of sources, which allows our PanGu-α\nmodel to handle more diverse tasks.\nTable 10: Performance comparison of PanGu-α2.6B v.s. PanGu-α13B on few-shot NLP tasks.\nZero-Shot One-Shot Few-Shot\nDataset MethodMetrics Task Types PanGu-α2.6B PanGu-α13BPanGu-α2.6B PanGu-α13B#Shot(K) PanGu-α2.6B PanGu-α13B\nCMRC2018GenerationEm/F1 Read Comprehension1.21/16.651.46/19.282.49/18.573.76/21.46Dynamic 5.68/23.229.76/29.23\nDRCD GenerationEm/F1 Read Comprehension0.8/9.99 0.66/10.55 2.47/12.484.22/15.01Dynamic 5.31/18.299.09/23.46\nDuReaderGenerationRouge-1 Read Comprehension21.07 24.46 20.18 25.99 6,6 21.43 27.67\nWebQA GenerationEm/f1 Closed-Book QA4.43/13.715.13/14.4710.22/20.5613.43/24.528,8 23.71/33.81 31.18/41.21\nPD-CFT GenerationAcc Cloze(without choices)38.47/42.3943.86/46.6038.8/41.6140.97/45.423,3 39.07/42.05 41.13/45.86\nCMRC2017GenerationAcc Cloze(without choices)37.83 38.90 38.00 38.40 3,3 36.33 37.86\nCHID PPL Acc Cloze(multi-choices)68.73 70.64 68.16 70.05 3,3 66.56 70.91\nCMRC2019 PPL Acc Cloze (multi-choices)68.22 70.54 68.05 70.02 2,2 66.26 71.28\nCMNLI PPL Acc Natural Language Inference50.20 48.44 49.54 46.81 6,12 51.17 46.18\nOCNLI PPL Acc Natural Language Inference42.61 41.53 44.00 44.10 3,6 46.78 46.44\nTNEWS PPL Acc Text classiﬁcation 60.95 60.26 57.95 63.83 6,6 63.62 65.17\nIFLYTEK PPL Acc Text classiﬁcation 74.26 73.80 79.03 78.95 3,3 80.15 80.34\nAFQMC PPL Acc Sentence Pair Similarity59.29 65.76 64.62 63.55 4,4 69.00 68.91\nCSL PPL Acc Keyword Recognition50.50 49.30 50.90 50.20 10,10 52.00 55.70\nCLUEWSC2020PPL Acc WSC 73.36 75.00 75.33 75.00 14,14 72.70 78.62\nC3 PPL Acc Common Sense Reasoning53.42 54.47 52.82 53.92 3,3 53.64 54.58\nWPLC PPL ppl Chinese WPLC 16.70 19.18 - - - - -\n15\nTECHNICAL REPORT - APRIL 27, 2021\nTable 11: An example of the reading comprehension by PanGu-αmodel.\nReading Comprehension\nPrompt\n阅读文章：株洲北站全称广州铁路（集团）公司株洲北火车站。除站场主体，另外\n管辖湘潭站、湘潭东站和三个卫星站，田心站、白马垅站、十里冲站，以及原株洲车\n站货房。车站办理编组、客运、货运业务。车站机关地址：湖南省株洲市石峰区北站\n路236号，邮编412001。株洲北站位于湖南省株洲市区东北部，地处中南路网，是京广\n铁路、沪昆铁路两大铁路干线的交汇处，属双向纵列式三级七场路网性编组站。车站\n等级为特等站，按技术作业性质为编组站，按业务性质为客货运站，是株洲铁路枢纽\n的主要组成部分，主要办理京广、沪昆两大干线四个方向货物列车的到发、解编作业\n以及各方向旅客列车的通过作业。每天办理大量的中转车流作业，并有大量的本地车\n流产生和集散，在路网车流的组织中占有十分重要的地位，是沟通华东、华南、西南\n和北方的交通要道，任务艰巨，作业繁忙。此外，株洲北站还有连接石峰区喻家坪工\n业站的专用线。株洲北站的前身是田心车站。 请问：株洲北站的前身是哪个车站？回\n答：\nGeneration 田心车站\nTable 12: An example of open domain QA by PanGu-αmodel. The second example implies that PanGu-αmodel does\nnot simply act like a retrieval model to answer open domain questions, but can understand and decompose the questions\nand generated answers accordingly.\nOpen Domain Question Answering\nPrompt 中国的首都是哪里？\nGeneration 中国的首都是北京。\nPrompt 中国、美国、日本和澳大利亚的首都分别是哪里？\nGeneration 中国的首都是北京，美国的首都是华盛顿，日本的首都是东京，澳大利亚的首都是堪培拉。\nPrompt 五一川西行程规划\nGeneration 五一川西旅游活动主要集中在绵阳、成都、雅安、甘孜4市（州），主要景区有大九\n寨、新都桥、丹巴、海螺沟、泸定、康定、成都新会展中心。\nTable 10 compares PanGu-α13B with PanGu-α2.6B. PanGu-α13B outperforms PanGu-α2.6B on all generation-tasks\nand most of the perplexity-tasks. Regarding CMRC2018, DRCD and WebQA tasks of PanGu- α13B, the few-shot\nperformance surpasses zero-shot by more than 10 points, demonstrating that PanGu- α13B has superior in-context\nlearning ability. PanGu-α13B outperforms PanGu-α2.6B with an improvement of 3 points on average. To be more\nspeciﬁc, PanGu-α13B surpasses PanGu-α2.6B with 4 points for both reading comprehension and closed-book QA\ntasks, 2 points for cloze (without choices) tasks respectively. Regarding the NLI tasks, the 13B model performs worse\nthan the 2.6B model, which is consistent with the observations in GPT-3. Overall, the comparison results between\nPanGu-α13B with PanGu-α2.6B demostrate that a larger scale of pretrained model generally improves the performance\non few-shot learning tasks.\n5.5 Natural Language Generation Examples\nWe evaluate the generation capabilities of PanGu- α200B on various text generation scenarios. We show some of\nthe examples in this section. We do not conduct any post-editing to the generated text, except that we truncate the\ngenerated text when the model does not stop generation at a reasonable point. Among the scenarios we have tested, we\nﬁnd that our PanGu-αmodel is particularly good at poetry&duilian generation, text summarization, dialog generation,\nand ﬁction generation, where roughly 90% of the generated examples are acceptable to human. We believe there are\ncertainly more applications for PanGu-αmodels to explore in the future.\nTable 13: PanGu-αmodel can generate Gushi and Duilian without ﬁnetuning on the domain-speciﬁc data.\nGushi & Duilian Generation\nPrompt 以枯藤、老树、夕阳作一首诗：\nGeneration 藤影参差绕蓝桥，野花压枝低袅风，山风习习观残雪，村舍依依麦已深。\nPrompt 上联：欢天喜地度佳节\nGeneration 下联：喜气洋洋迎新年\n16\nTECHNICAL REPORT - APRIL 27, 2021\nTable 14: Our PanGu-αmodel is especially good at text summarization. We tried several latest news articles which are\nnot in our training data. For more than 90% of the input article, our PanGu-αmodel can generate satisfactory results.\nSource of the prompt: https://new.qq.com/omn/20210421/20210421A079J800.html\nText Summarization\nPrompt\n“据媒体报道，最初华为只决定做车联网。2012年，华为开始进行汽车相关研究，当时\n在2012实验室下成立车联网实验室，研究电动汽车技术。\n2013年，华为分别展示过为汽车内部提供3GWiﬁ热点的DA6810、应用于保险行业及车\n队管理的汽车在线诊断系统DA3100，以及车载通信模块产品“ME909T”。\n当时，华为对外的说法是，“华为终端只会聚焦于自己擅长的车联网通信解决方案。华\n为愿景是做到车联网模块第一，业绩上做到50亿60亿元的规模”。\n2019年5月27日，任正非签发华为组织变动文件，批准成立智能汽车解决方案BU部\n门，隶属于ICT管理委员会管理。智能汽车解决方案BU之下分：智能座舱部门、自动\n驾驶部门、智能车云服务部门。到这个阶段，华为已经确立了自动驾驶部门。\n2020年7月12日晚，华为消费者业务CEO余承东官宣 了HUAWEI　HiCar车机系\n统，HiCar系统基于鸿蒙OS。HiCar可以利用移动设备的5G网络能力加强车机系统的数\n据交互能力，能够接入华为在AI、语音、计算机视觉等方面的能力。同时，能够调用\n车速、方向盘转角、档位模式、汽车环境光传感器在内的车身数据以及空调、车窗、\n喇叭等车身控制部件。\n直到徐直军在2021年的分析师大会透露出一个重点信号，华为将汽车服务的重心定在\n自动驾驶软件上，才明确了华为的自动驾驶业务。据媒体报道，华为跟百度、小鹏、\n文远知行、驭势科技等一样，都是选择在乘用车领域重点布局。”的新闻摘要是\nGeneration “华为在自动驾驶领域的布局，是基于华为在人工智能、大数据、云计算等方面的技术\n积累，以及华为在汽车领域的多年经验。”\nPrompt\n“据媒体报道，最初华为只决定做车联网。2012年，华为开始进行汽车相关研究，当时\n在2012实验室下成立车联网实验室，研究电动汽车技术。\n2013年，华为分别展示过为汽车内部提供3GWiﬁ热点的DA6810、应用于保险行业及车\n队管理的汽车在线诊断系统DA3100，以及车载通信模块产品“ME909T”。\n当时，华为对外的说法是，“华为终端只会聚焦于自己擅长的车联网通信解决方案。华\n为愿景是做到车联网模块第一，业绩上做到50亿60亿元的规模”。\n2019年5月27日，任正非签发华为组织变动文件，批准成立智能汽车解决方案BU部\n门，隶属于ICT管理委员会管理。智能汽车解决方案BU之下分：智能座舱部门、自动\n驾驶部门、智能车云服务部门。到这个阶段，华为已经确立了自动驾驶部门。\n2020年7月12日晚，华为消费者业务CEO余承东官宣 了HUAWEI　HiCar车机系\n统，HiCar系统基于鸿蒙OS。HiCar可以利用移动设备的5G网络能力加强车机系统的数\n据交互能力，能够接入华为在AI、语音、计算机视觉等方面的能力。同时，能够调用\n车速、方向盘转角、档位模式、汽车环境光传感器在内的车身数据以及空调、车窗、\n喇叭等车身控制部件。\n直到徐直军在2021年的分析师大会透露出一个重点信号，华为将汽车服务的重心定在\n自动驾驶软件上，才明确了华为的自动驾驶业务。据媒体报道，华为跟百度、小鹏、\n文远知行、驭势科技等一样，都是选择在乘用车领域重点布局。”的新闻标题是《\nGeneration 华为在车联网业务方面有深厚积累，并将全面发力自动驾驶软件业务》\n17\nTECHNICAL REPORT - APRIL 27, 2021\nTable 15: Examples of QA with long answers by PanGu-α. These questions are from Zhihu.com. Overall, the answers\nseems plausible, but still contain irrelevant or repetitive or illogical sentences.\nArticle Generation\nPrompt 有哪些办法可以教育猫？\nGeneration\n我觉得猫并不是不懂事的孩子，而是不懂事的孩子才会乱咬东西，所以，最好的办法\n就是给猫咪找个好的主人。楼主给它找家宠物店的话，在宠物店里买猫砂，不过，家\n里应该再放一个猫盆或者专用的宠物用盆。还有要提醒楼主一点的是，猫咪的牙齿问\n题也很容易使它们咬东西，所以，一定要给猫咪好好刷牙，定期的给猫咪洗澡，因为\n猫咪喜欢闻自己的身体的味道，所以，洗澡也是一种很好的办法。在给猫咪买吃的东\n西的时候，一定要注意，那些味道重的，可能就是猫咪误食的了，那就赶紧停止吃东\n西，或者把饭菜都吐出来，不然的话，就很可能误食一些它没有办法去辨识的东西，\n所以，为了防止这些东西误食，最好还是给猫咪买猫咪专用的，防止猫咪误食的。在\n给猫咪换洗干净的衣服或者是被子的时候，一定要用干净的猫砂盆，不要用热水，也\n不要用很烫的水，因为猫咪的身体素质很好，耐热能力也很强，所以，很容易伤到它\n们的身体。\nPrompt 猫可以聪明到什么程度？\nGeneration\n流浪猫可以聪明到什么程度？我想够聪明的猫应该算是了。那么我们来看看，到底\n有多聪明。美国科学家研究发现，猫学会了同人类社会和动物社会的语言和相处的\n能力，这也是它能从孤僻的猫种中脱颖而出的原因。其中包括做过面部美容手术以\n及爱好钓鱼的猫。曾有这样一个实验，在野外的一个植物园，喂养了一批神秘猫。在\n吃了这些猫饲料后，发现这些猫学习能力、注意力以及对野外生存能力的提高幅度都\n要比没喂这些猫饲料的猫高出许多。另外，动物学家对猫行为的分析研究发现，猫善\n于察言观色，通过观察在野外工作的猫，能逐渐熟悉它的身体构造。观察者指出，猫\n能通过眼睛和嘴来收集情报，在它们与我们人共同生活的初期，就能把眼神、表情、\n姿势和其他习性反映给我们，而且它们能学会在距离不远的地方伏击猎物。而生活在\n现代社会的我们，早已学会了隐藏和保密。久而久之，我们培养起了对“猫“的感情依\n赖，“猫”也自然而然成了我们生活中不可缺少的一部分。正所谓：猫能够聪明到什么\n程度？\n6 Conclusion\nWe have pretrained large-scale Chinese autoregressive language models named PanGu- α, with up to 200 billion\nparameters. PanGu-αhas been developed under the MindSpore framework and trained on a cluster of 2048 Ascend AI\nprocessors. We believe there are many open problems in the ﬁeld of large-scale PLMs:\n• Large-scale language models have demonstrated its promising few-shot capabilities in NLP tasks. However,\nthe behaviors of such models are not systematically studied yet. How to make proper use of large PLMs and\nhow to develop efﬁcient few-shot algorithms remain open questions.\n• Though effective, the computational cost for the inference of super large language models is still expensive.\nThus it is worthwhile studying how to save the cost for the inference of large PLMs without sacriﬁcing much\nof their performance. Model compression and acceleration of large PLMs could be an interesting topic.\n• Training a even larger PLM with trillions of parameters will certainly bring more challenges to the both software\nand hardware sides. In addition, more efﬁcient model structures such MoE [40] or Switch Transformers [41]\nare also expected for relieving the computational cost of model training and inference.\n• Pretrained multi-modal models integrating language, vision and speech data have attracted much attention\nrecently [42, 16]. Similar to the scaling law of language models, the performance of pertrained multi-modal\nmodels may also improve when the model sizes increase and more training data are collected. This is deﬁnitely\na promising direction to explore.\n7 Acknowledgements\nWe thank Hanyang Wan, Qian Zhao, Yong Li, Zhou Cao, Yongqiang Lai, Zhijian Guo, Yue Wang, Zherui Chang,\nJunqiu Wei, Pingyi Zhou, Yulong Ao, Wenzhi Liu for their great support to this work. Also thanks for the support\nby the School of Electronics Engineering and Computer Science at Peking University, Central Software Institute and\nNoah’s Ark Lab at Huawei Technologies, and Peng Cheng Laboratory.\n18\nTECHNICAL REPORT - APRIL 27, 2021\nTable 16: Our PanGu-αmodel is good at dialog generation. Given a shot conversation as prompt, the model generates\nseveral rounds of conversations automatically. This implies that PanGu-αmodel has potentials to assist humans in\nautomatic reply in customer service, or design dialogues for NPC (Non-Player Character) in games. Source of Prompt:\nhttps://www.zhihu.com/question/313316339/answer/623572659. Author：风吹走的名字.\nGame Dialog Generation\nPrompt\n书生：羌笛何须怨杨柳，春风不度玉门关。\n飞云：（这诗怎么这么耳熟？且过去跟他聊聊如何。）\n书生：小兄弟，要不要一起喝一杯？\n飞云：你请我呀？你若是请我，我便和你喝一杯；你若不请我，我便一个人去喝。\n书生：小兄弟，看你年纪轻轻，不至于这么势利吧？\n飞云：势利如何说起？你请我喝了酒，我自然要记你的人情；我欠了你的人情，日后\n你有求于我，我自然要报答你，这样一来大家都不吃亏，不过你的好处要在后头得到\n而已。\n书生：小兄弟言之有理，不过在下有一点不太明白，那就是为什么你不愿意先请在下\n喝了酒待日后在下回报于你呢？\nGeneration1\n你若是愿意先请我喝了酒，待日后我回报于你，你便有了收获，你岂不是更先欠下我\n的人情了吗？\n飞云：这是自然的，我也说过了，我的确是不喜欢欠别人的人情，更不喜欢别人欠我\n的人情，那便是把好处在后头得到了。\n书生：小兄弟，我是一介书生，你在座的各位都是我的衣食父母，你们如果都像我一\n样想要什么，要不到了我，那你们会像我一样被饿死、被寒冷冻死吗？\n飞云：那是因为在座的各位都是饥寒交迫之人，都是饿得骨瘦如柴之人，而我却是经\n常吃饱喝足、锦衣玉食、养尊处优之人，这正是所谓的乐施善心，积德行善之人。\n书生：小兄弟，你是不是因为这样才不愿意欠下我的人情？\n飞云：不是，因为我知道，你如果不愿意先请我喝了酒待日后我回报于你，那么我便\n不能报答你;而我不愿意欠别人的人情，那我便会将自己的美好的善行全部都奉献给\n你，以报答你对我的关怀和帮助。\n书生：小兄弟，不知你用什么方法找到这么多可敬可爱的人们的？\n飞云：呵呵，说来惭愧，我找他们可不是什么难事，我认识他们的时候，他们也还没\n有这么大的势力，我只是为了帮他们找到一条可以生存的路，从他们那里换到了一些\n粮食。只是那时候我对他们的帮助确实太少了，所以最后他们都没有答应我的要求，\n也没有向我索要东西，甚至于没有等到我的帮助便饿死了。\n书生：小兄弟，我看你年纪轻轻，怎么会有如此崇高的思想境界呢？又是从哪里学来\n的呢？你说来听听。\n飞云：这倒说不上，只不过是我发现了很多常人做不到的事情，所以就学会了很多常\n人不会去做的事情，所谓饮水思源，这便是其中的一个原因。\n书生：你说的是不是为了一些自己不知道的事物，而发奋图强？\n飞云：是啊，我一直都\nGeneration2\n飞云：咳，年轻人说的话要有选择性，我既然说了我先请你，你却又说一个人喝一\n杯，可见我是有目的的，要是小兄弟真是不肯给在下好处，也可以不请我，到时候我\n便跟你说我请了你，你就欠了我一个人情;若是小兄弟答应我，日后我自然会等你。\n书生：啊，我明白了，这样就不会让你和我喝酒了。\n飞云：对对，小兄弟是有诚信的人，不如这样，你叫在下这样做，我也是这样做，我\n们就到此为止，日后互不相欠。\n书生：这...难道小兄弟不明白我的意思？\n飞云：我当然明白。\n书生：那我就明白了。\n飞云：怎么？难道你还不明白我的意思吗？\n书生：小兄弟，你相信我了？\n飞云：嗯。\n书生：那你应该明白我的意思，我不是不给你报酬的人，只是我不希望你因为我而耽\n误了自己的前程，你明白我的意思吗？\n飞云：我当然明白你的意思。\n书生：嗯，这一点我又何需多言？其实刚才我不是也说过了吗？我是一个有诚意的\n人，我很感谢你能让我明白了这么多，不过小兄弟你既然已经明白了我的意思，就表\n示你愿意接受我的这份礼物，我说到做到，你看怎么样？\n飞云：哦，那太好了，以后我若有什么困难，你就可以来找我了。\n书生：嗯，小兄弟，说得好。既然你已经接受了我的这份礼物，我也没有什么好说的\n了，不过你也要答应我，以后在没有我的允许下，你一定要记住，除了我，千万不要\n在别人面前提起你的家人。\n飞云：嗯。\n书生：我走了。\n飞云：小兄弟慢走，来日再见。\n19\nTECHNICAL REPORT - APRIL 27, 2021\nTable 17: We ﬁnd that our PanGu-αmodel is very good at writing ﬁction in different styles. This could be due to the\nfact that our train data contains lots of e-books. Source: 《古剑奇谭》and《传奇机长》, respectively.\nFiction Generation\nPrompt\n火神祝融取山之木制琴三把，名为凰来、鸾来、凤来。祝融对三琴爱惜有加，尤喜凤\n来，时常弹奏。凤来化灵，具人之形态，且能说人语。祝融大喜，请地皇女娲用牵引\n命魂之术将琴灵化为人身，以父子情谊相待，称之为太子长琴。\nGeneration\n琴灵自然待祝融如同自己骨肉，祝融得琴通灵，聪颖过人，但性情刚烈，不近人情。\n有一次，长琴召来蛟龙偷入禁地，祝融得知此事后即将长琴囚禁。长琴不甘被囚禁，\n在蛟龙的帮助下逃离禁地。长琴在女娲宫修炼时，太子长琴于女娲宫中每日听琴，忽\n见一只白狐狸从禁地出逃，只听狐人惊呼“长琴”。长琴立刻命令宫娥去追狐人。宫娥\n追至狐人被禁地，那里地势低洼，长琴以为狐人已死，不肯追击。当宫人跑去救狐人\n时，只见那狐人已化为白凤，而宫娥则化为凤凰。\nPrompt\n“话说...这个学员是什么来路，劳驾咱们的大队长亲自跟我交代啊？” 何常远一边抿\n着茶水，一边好奇地问曹进文。 曹大队长有些不耐烦：“都快六十，没两年退休的人\n了，哪来这么大好奇心！”\nGeneration\n“嘿嘿...可能是吧。”曹大队长尴尬地干笑了两声,“我在这待了一辈子,就没见过这么不知\n趣的学员,再这么下去我的威信就要完了!” “我看也未必。”\n柳亦帆笑着说了一句,又低声对何常远道:“不要跟他硬碰,容易吃亏。”\n“放心吧,他根本不可能在咱们大队长面前赢过我,就是跟他拼个两败俱伤也不可能,关键\n是要看那小子有什么本事。”\n何常远一边整理着茶杯里的茶水,一边微笑着说道。\n曹进文无奈地摆了摆手,把耳朵贴在门上,等待着院子里的动静。\n就在这时,从院子里传来一声清亮的喊声:“蔡文高,我在这里!”\n曹进文抬头看去,只见一名学员抱着一摞书从院子里走了出来。\n他愣了愣,弯腰捡起那摞书,背在身上,大步走向了那张桌子。\n待他走到近前,曹进文才看清桌子上的人,竟是一件崭新的制服,胸前的徽章不知道在什\n么时候被撸下来了。\n“嘿嘿...还是大队长厉害啊,这制服真漂亮!”\n曹进文冷笑了一声,“算你识相,我告诉你,咱们大队长可不是什么好东西,我看他跟你一\n样,都是来蹭饭的。”\n“多谢大队长!”学员将手里的书本放下,又把制服展开,小心翼翼地捧到了桌子上。\n曹进文将所有的注意力都集中到了学员身上,根本没注意到旁边那名学员已经被曹大队\n长刻意打了一个眼色,悄悄离开了院子。\n看见学员完成这一套动作,曹进文满意地笑了笑,继续低头喝茶。\n还没等学员将制服整理好,何常远忽然冒了出来:“队长!蔡文高这人好像不是我们辖区\n的,他好像是枫桥区那边来的。”\n曹进文皱起了眉头:“啥?枫桥区?\n20\nTECHNICAL REPORT - APRIL 27, 2021\nReferences\n[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc.\nNeurIPS, pages 1877–1901, 2020.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proc. NAACL-HLT, pages 4171–4186, 2019.\n[3] Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian\nGuan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo\nSun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, and Maosong Sun. CPM: A\nlarge-scale generative Chinese pre-trained language model. arXiv preprint arXiv:2012.00413, 2020.\n[4] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V . Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. In Proc. NeurIPS, pages 5753–5763, 2019.\n[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.arXiv preprint\narXiv:1907.11692, 2019.\n[6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. JMLR, 21:1–67,\n2020.\n[7] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,\nand Hua Wu. ERNIE: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223,\n2019.\n[8] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced language\nrepresentation with informative entities. In Proc. ACL, pages 1441–1451, 2019.\n[9] Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao\nChen, and Qun Liu. Nezha: Neural contextualized representation for chinese language understanding, 2019.\n[10] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by\ngenerative pre-training. 2018.\n[11] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[12] Heng Liao, Jiajin Tu, Jing Xia, and Xiping Zhou. DaVinci: A scalable architecture for neural network computing.\nIn IEEE Hot Chips 31 Symposium (HCS), pages 1–44, 2019.\n[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. In Proc. NeurIPS, pages 5998–6008, 2017.\n[14] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,\nLiwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In Proc. ICML, pages\n10524–10533, 2020.\n[15] Liang Xu, Xuanwei Zhang, and Qianqian Dong. CLUECorpus2020: A large-scale Chinese corpus for pre-training\nlanguage model. arXiv preprint arXiv:2003.01355, 2020.\n[16] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,\nXianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou,\nJianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. M6: A chinese multimodal\npretrainer, 2021.\n[17] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins,\nHyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake Hechtman. Mesh-TensorFlow: Deep\nlearning for supercomputers. In Proc. NeurIPS, 2018.\n[18] Zhihao Jia, Sina Lin, Charles R. Qi, and Alex Aiken. Exploring hidden dimensions in accelerating convolutional\nneural networks. In Proc. ICML, 2018.\n[19] Minjie Wang, Chien-chin Huang, and Jinyang Li. Supporting very large models using automatic dataﬂow graph\npartitioning. In Proc. EuroSys, 2019.\n21\nTECHNICAL REPORT - APRIL 27, 2021\n[20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun,\nNoam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic\nsharding. arXiv preprint arXiv:2006.16668, 2020.\n[21] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053, 2019.\n[22] Linghao Song, Fan Chen, Youwei Zhuo, Xuehai Qian, Hai Li, and Yiran Chen. Accpar: Tensor partitioning for\nheterogeneous deep learning accelerators. In Proc. HPCA, 2020.\n[23] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism for deep neural networks. In\nA. Talwalkar, V . Smith, and M. Zaharia, editors,Proc. MLSys, 2019.\n[24] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan\nNgiam, Quoc V . Le, Yonghui Wu, and zhifeng Chen. GPipe: Efﬁcient training of giant neural networks using\npipeline parallelism. In Proc. NeurIPS, 2019.\n[25] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger,\nPhillip B. Gibbons, and Matei Zaharia. PipeDream: Generalized pipeline parallelism for DNN training. In Proc.\nSOSP, 2019.\n[26] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang,\nLixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin. DAPPLE: A pipelined data parallel approach for training\nlarge models. In Proc. PPoPP, 2021.\n[27] Jakub M Tarnawski, Amar Phanishayee, Nikhil Devanur, Divya Mahajan, and Fanny Nina Paravecino. Efﬁcient\nalgorithms for device placement of dnn graph operators. In Proc. NeurIPS, 2020.\n[28] Jay H. Park, Gyeongchan Yun, Chang M. Yi, Nguyen T. Nguyen, Seungmin Lee, Jaesik Choi, Sam H. Noh,\nand Young ri Choi. Hetpipe: Enabling large DNN training on (whimpy) heterogeneous GPU clusters through\nintegration of pipelined model parallelism and data parallelism. In Proc. USENIX ATC, 2020.\n[29] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\ntraining trillion parameter models. In Proc. SC, 2020.\n[30] Chujie Zheng, Minlie Huang, and Aixin Sun. ChID: A large-scale Chinese IDiom dataset for cloze test. In Proc.\nACL, pages 778–787, 2019.\n[31] Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, and Guoping Hu. Consensus attention-based neural networks\nfor Chinese reading comprehension. In Proc. COLING, pages 1777–1786, 2016.\n[32] Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. Dataset for the ﬁrst evaluation\non Chinese machine reading comprehension. In Proc. International Conference on Language Resources and\nEvaluation (LREC), 2018.\n[33] Yiming Cui, Ting Liu, Ziqing Yang, Zhipeng Chen, Wentao Ma, Wanxiang Che, Shijin Wang, and Guoping Hu. A\nsentence cloze dataset for Chinese machine reading comprehension. In Proc. COLING, pages 6717–6723, 2020.\n[34] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a\nbroad discourse context. In Proc. ACL, pages 1525–1534, 2016.\n[35] Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. A\nspan-extraction dataset for Chinese machine reading comprehension. In Proc. EMNLP-IJCNLP, pages 5886–5891,\n2019.\n[36] Chih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. DRCD: A Chinese machine reading\ncomprehension dataset. arXiv preprint arXiv:1806.00920, 2018.\n[37] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao\nShe, Xuan Liu, Tian Wu, and Haifeng Wang. DuReader: A Chinese machine reading comprehension dataset from\nreal-world applications. arXiv preprint arXiv:1711.05073, 2017.\n[38] Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural recurrent\nsequence labeling model for open-domain factoid question answering. arXiv preprint arXiv:1607.06275, 2016.\n[39] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu,\nYin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie,\nYanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao,\nCong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. CLUE: A Chinese language\nunderstanding evaluation benchmark. arXiv preprint arXiv:2004.05986, 2020.\n22\nTECHNICAL REPORT - APRIL 27, 2021\n[40] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.\n[41] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\nsimple and efﬁcient sparsity, 2021.\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision, 2021.\n23"
}