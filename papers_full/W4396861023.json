{
  "title": "From CNNs to Transformers in Multimodal Human Action Recognition: A Survey",
  "url": "https://openalex.org/W4396861023",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4295691908",
      "name": "Shaikh, Muhammad Bilal",
      "affiliations": [
        "Edith Cowan University"
      ]
    },
    {
      "id": "https://openalex.org/A3041805467",
      "name": "Islam, Syed Mohammed Shamsul",
      "affiliations": [
        "Edith Cowan University"
      ]
    },
    {
      "id": "https://openalex.org/A2744958498",
      "name": "Chai Douglas",
      "affiliations": [
        "Edith Cowan University"
      ]
    },
    {
      "id": "https://openalex.org/A2715660432",
      "name": "Akhtar Naveed",
      "affiliations": [
        "University of Melbourne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4234816175",
    "https://openalex.org/W2056339039",
    "https://openalex.org/W2971659033",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2053101950",
    "https://openalex.org/W2965815071",
    "https://openalex.org/W3088117316",
    "https://openalex.org/W2906430385",
    "https://openalex.org/W6754337694",
    "https://openalex.org/W6765307894",
    "https://openalex.org/W2274499208",
    "https://openalex.org/W3207758636",
    "https://openalex.org/W3008546095",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2527524734",
    "https://openalex.org/W1993991024",
    "https://openalex.org/W2224196924",
    "https://openalex.org/W3015377432",
    "https://openalex.org/W2963218601",
    "https://openalex.org/W3111490429",
    "https://openalex.org/W3025796084",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W2802503116",
    "https://openalex.org/W3011934803",
    "https://openalex.org/W3094480255",
    "https://openalex.org/W3113320078",
    "https://openalex.org/W3023633125",
    "https://openalex.org/W2940791683",
    "https://openalex.org/W2601271987",
    "https://openalex.org/W2944006115",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W3011747142",
    "https://openalex.org/W6704477683",
    "https://openalex.org/W4307539314",
    "https://openalex.org/W2425380982",
    "https://openalex.org/W2580299352",
    "https://openalex.org/W2914868535",
    "https://openalex.org/W3143375397",
    "https://openalex.org/W6810263219",
    "https://openalex.org/W2309561466",
    "https://openalex.org/W3175419009",
    "https://openalex.org/W4318715744",
    "https://openalex.org/W2912814345",
    "https://openalex.org/W6600983433",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W4282981352",
    "https://openalex.org/W2941719259",
    "https://openalex.org/W2942810189",
    "https://openalex.org/W2914340986",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2768642967",
    "https://openalex.org/W3037516378",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W3155322285",
    "https://openalex.org/W2317053768",
    "https://openalex.org/W4214910852",
    "https://openalex.org/W3193761636",
    "https://openalex.org/W2917819557",
    "https://openalex.org/W2294438834",
    "https://openalex.org/W2466991859",
    "https://openalex.org/W2904658540",
    "https://openalex.org/W3004554579",
    "https://openalex.org/W2186222003",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W2887051120",
    "https://openalex.org/W3140110584",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W4287777632",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W4246329541",
    "https://openalex.org/W4225531458",
    "https://openalex.org/W4287826895",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4297697565",
    "https://openalex.org/W2963177663",
    "https://openalex.org/W3213269132",
    "https://openalex.org/W4390736386",
    "https://openalex.org/W4306884028",
    "https://openalex.org/W3105232955",
    "https://openalex.org/W4253716310",
    "https://openalex.org/W2533370895",
    "https://openalex.org/W4361733981"
  ],
  "abstract": "Due to its widespread applications, human action recognition is one of the most widely studied research problems in Computer Vision. Recent studies have shown that addressing it using multimodal data leads to superior performance as compared to relying on a single data modality. During the adoption of deep learning for visual modelling in the past decade, action recognition approaches have mainly relied on Convolutional Neural Networks (CNNs). However, the recent rise of Transformers in visual modelling is now also causing a paradigm shift for the action recognition task. This survey captures this transition while focusing on Multimodal Human Action Recognition (MHAR). Unique to the induction of multimodal computational models is the process of ‚Äòfusing‚Äô the features of the individual data modalities. Hence, we specifically focus on the fusion design aspects of the MHAR approaches. We analyze the classic and emerging techniques in this regard, while also highlighting the popular trends in the adaption of CNN and Transformer building blocks for the overall problem. In particular, we emphasize on recent design choices that have led to more efficient MHAR models. Unlike existing reviews, which discuss Human Action Recognition from a broad perspective, this survey is specifically aimed at pushing the boundaries of MHAR research by identifying promising architectural and fusion design choices to train practicable models. We also provide an outlook of the multimodal datasets from their scale and evaluation viewpoint. Finally, building on the reviewed literature, we discuss the challenges and future avenues for MHAR.",
  "full_text": "From CNNs to Transformers in Multimodal Human Action\nRecognition: A Survey\nMUHAMMAD BILAL SHAIKH, DOUGLAS CHAI, and SYED MOHAMMED SHAMSUL\nISLAM, Edith Cowan University, Australia\nNAVEED AKHTAR,School of Computing and Information Systems, The University of Melbourne, Australia\nDue to its widespread applications, human action recognition is one of the most widely studied research\nproblems in Computer Vision. Recent studies have shown that addressing it using multimodal data leads to\nsuperior performance as compared to relying on a single data modality. During the adoption of deep learning\nfor visual modelling in the last decade, action recognition approaches have mainly relied on Convolutional\nNeural Networks (CNNs). However, the recent rise of Transformers in visual modelling is now also causing\na paradigm shift for the action recognition task. This survey captures this transition while focusing on\nMultimodal Human Action Recognition (MHAR). Unique to the induction of multimodal computational\nmodels is the process of ‚Äòfusing‚Äô the features of the individual data modalities. Hence, we specifically focus\non the fusion design aspects of the MHAR approaches. We analyze the classic and emerging techniques in\nthis regard, while also highlighting the popular trends in the adaption of CNN and Transformer building\nblocks for the overall problem. In particular, we emphasize on recent design choices that have led to more\nefficient MHAR models. Unlike existing reviews, which discuss Human Action Recognition from a broad\nperspective, this survey is specifically aimed at pushing the boundaries of MHAR research by identifying\npromising architectural and fusion design choices to train practicable models. We also provide an outlook of\nthe multimodal datasets from their scale and evaluation viewpoint. Finally, building on the reviewed literature,\nwe discuss the challenges and future avenues for MHAR.\nCCS Concepts: ‚Ä¢ General and reference ‚ÜíSurveys and overviews ; ‚Ä¢ Computing methodologies ‚Üí\nActivity recognition and understanding ; Scene understanding ; Neural networks .\nAdditional Key Words and Phrases: Multimodal, action recognition, fusion, deep learning, neural networks\nACM Reference Format:\nMuhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, and Naveed Akhtar. 2024. From\nCNNs to Transformers in Multimodal Human Action Recognition: A Survey. In . ACM, New York, NY, USA,\n23 pages. https://doi.org/X.X\n1 INTRODUCTION\nModality broadly refers to the mode in which information is perceived. Humans are able to see, feel,\nhear and so on, using their senses. Our ability to intelligently process this multimodal information\nis what makes us highly effective beings. Currently, the field of Artificial Intelligence (AI) is\ngenerally concerned with developing capabilities for individual data modalities that encode specific\ninformation about our surroundings. In this regard, in the form of recent breakthroughs such as\nGPT-4 [85] for multimodal data (image and text), ChatGPT for text, DALL-E 2 [ 96] and Stable\nDiffusion [100] for the visual data, we are already witnessing human-level intelligence of machines.\nHowever, the ultimate objective to achieve Artificial General Intelligence (AGI) demands much more.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\n¬© 2024 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00\nhttps://doi.org/X.X\n1\narXiv:2405.15813v1  [cs.CV]  22 May 2024\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nFig. 1. ( Left) Number of relevant publications in recent years, identified with the data collected from the\nWeb of Science. (Center) Categories of publication contributions to different sub-fields of Science - generated\nwith data from the Web of Science. (Right) Distribution as document type (data collected from Scopus).\nThe ability to handle and leverage information encoded in multimodal data is one of the basic needs\nof AGI. With the availability of increasingly powerful computational resources, this realization is\nfast re-writing the guiding principles for the research communities in different sub-fields of AI.\nIn the domain of Computer Vision, human action recognition is a long-standing problem [138].\nThis article follows Herath et al. [ 45], treating human action as ‚Äúthe most elementary human-\nsurrounding interaction with a meaning. ‚Äù Human action recognition is thus the automated labelling\nprocess of human actions within a given sequence of visual frames. Due to its widespread practical\napplications, ranging from safety [117] to healthcare [161], particularly for COVID [95] and many\nother downstream tasks [32, 35, 94], human action recognition has historically received significant\nattention within the vision research community. Recent years have also seen the emergence of\nnumerous multimodal approaches for this task, which define the research direction of Multimodal\nHuman Action Recognition (MHAR). Multimodal data fusion refers to the mixing of features from\ndata of various modalities. Fusion analysis has been extensively used in several domains including\nactivity recognition [79], 3D shape classification [83] and predicting human eye fixation [47], as\nevident through relevant works.\nPresently, MHAR systems have been explored in various real-world scenarios of indoor and\noutdoor settings that relate to the application domains of healthcare [13, 147], smart homes [68,\n81, 108], surveillance systems [10, 57, 123], social relations recognition [142] and numerous other\ndomains [119, 121]. It is evident from the existing literature that owing to the diversity of data,\nthe research direction of MHAR has its own unique challenges and opportunities when compared\nto conventional action recognition tasks [ 80]. To contextualize this, early research in human\naction recognition was limited to only analysing still color images or videos [40, 157]. The MHAR\nproblem demands accounting for different data modalities while providing the opportunity for\nimproved performance due to the availability of complementary information in different modalities.\nNevertheless, it is worth noting here that the typical challenges of classical action recognition, (e.g.,\nbackground clutter, partial object occlusion, viewpoint variations, lighting changes, and execution\nrate etc.) remain equally pertinent to MHAR techniques.\nThis survey focuses on MHAR approaches that use computer vision and signal-processing\ntechniques to recognize human actions. Generally, techniques based on multimodal data are\nmotivated based on the expectation to achieve better performance than unimodal data techniques.\nHowever, MHAR research has also shown other motivational sources in the form of the development\nof cost-effective sensing (e.g., ASUS Xtion [50], Microsoft Kinect [143], and Intel RealSense [15])\nand widespread gain in the computational power. Combined, these factors currently propel MHAR\n2\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nFig. 2. A typical multimodal fusion-based action recognition pipeline.\nresearch to an unprecedented pace. Accordingly, there has been a growing number of MHAR-related\npublications in the last few years - see Fig. 1 (left). Interestingly, these publications have contributed\nto diverse sub-fields of Science as their application domains - Fig. 1 (middle). In Fig 1, we also show\nthe Scopus article-type distribution against the query ‚Äúmultimodal action recognition\" to provide\nan overview of the publication trends in this highly active research direction.\nDue to the emerging popularity of the topic, several survey articles have examined MHAR from\ndifferent perspectives [2, 4, 8, 20, 41, 104, 118, 134, 136, 151, 152, 154‚Äì156]. However, this article\nis distinct from all previous studies, as we specifically focus and extensively cover the area of\nmultimodal fusion and learning in action recognition. To the best of our knowledge, no prior\ndetailed review article has thoroughly addressed this research direction. The specific contributions\nof this survey are summarized as follows.\n‚Ä¢To the best of our knowledge, this is the first comprehensive review of MHAR methods from\nthe perspective of multimodality, with a focus on CNNs and transformer-based techniques.\n‚Ä¢It provides a thorough review of fusion approaches beyond the typical early and late splits\nused in the implementation of multimodal action recognition.\n‚Ä¢It provides a comprehensive comparison of the existing methods and their performance on\nbenchmark datasets, along with insightful discussions.\n‚Ä¢It pays special attention to the more recent approaches for MHAR, thereby providing readers\nwith an accessible overview of the current state-of-the-art in MHAR.\nThe article is organized as follows: Section 2 discusses multimodal learning methods, covering the\noriginal CNN and Transformer architectures and their derivatives. Section 3 summarizes standard\nbenchmark datasets and their state-of-the-art. Fusion mechanisms in multimodal action recognition\nare explored in Section 4, highlighting recent advancements in various fusion strategies. Section 5\naddresses the challenges in current multimodal action recognition. Section 6 concludes the article.\n2 MULTIMODAL LEARNING METHODS\n2.1 Multimodal Learning\nBefore discussing multimodal learning, we must first understand unimodal learning, where a neural\nnetwork processes a single data type.\n3\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nGiven a dataset ùëá = {ùë•1,...,ùë• ùëõ,ùë¶1,...,ùë¶ ùëõ}, ùë•ùëñ is the i-th training example and ùë¶ùëñ is the true label of\nùë•ùëñ. Training on a single modality, say RGB images, can be formalized by the equation:\nùêø(ùê∂(ùúôùëö(ùëã)),ùë¶) (1)\nwhere ùúôùëö is a deep neural network tailored for that modality (like a CNN for images). Its parameters\nare represented by ‚äñùëö. ùê∂is a classifier that predicts the label based on the features extracted byùúôùëö.\nThis classifier typically uses one or more fully connected layers, and its parameters are denoted by\n‚äñùëê.\nWhen dealing with real-world problems, often a single data modality is not enough. For example,\nin video understanding tasks, both video frames (RGB) and audio can provide valuable information.\nBy leveraging multiple modalities, a model can potentially achieve better performance than relying\non a single one. A typical multimodal fusion-based system would follow a standard workflow (see\nFig. 2).\nTraining with multiple modalities can be formalized as:\nùêømulti = ùêø(ùê∂(ùúôaudio ‚äïùúôvideo),ùë¶) (2)\nhere, each modality, audio and video, has its respective deep neural network, ùúôaudio and ùúôvideo,\ndesigned to extract relevant features. The fusion operation, represented by ‚äï, can be a simple\nconcatenation, or more sophisticated operations like weighted sums or attention mechanisms.\n2.2 Learning Methods in CNNs\n2.2.1 Convolution Neural Networks (CNNs). CNN architectures found in the literature are diverse\nwhile sharing common elements. Primarily, they incorporate convolutional layers, where multiple\nkernels compute various feature maps based on input data. Generating a new feature map involves\nconvolving the input with a learned kernel and applying an activation function to the results. Shared\nkernels, applied across all spatial input locations, produce individual feature maps, simplifying\nmodel complexity and improving trainability. The feature value at a specific location (ùëñ,ùëó )in the\nùëò-th feature map of the ùëô-th layer, denoted as ùëßùëô\nùëñ,ùëó,ùëò, is given by the equation:\nùëßùëô\nùëñ,ùëó,ùëò = ùë§ùëô\nùëò\nùëá\nùë•ùëô\nùëñ,ùëó +ùëèùëô\nùëò (3)\nHere, the shared kernel ùë§ùëô\nùëò generates the feature map ùëßùëô\n:,:,ùëò. The activation function ùëé(¬∑)introduces\ncrucial non-linearities, determining the convolutional feature activation value ùëéùëô\nùëñ,ùëó,ùëò as follows:\nùëéùëô\nùëñ,ùëó,ùëò = ùëé(ùëßùëô\nùëñ,ùëó,ùëò) (4)\nThe pooling layer enhances shift-invariance by reducing feature map resolution. For each feature\nmap ùëéùëô\n:,:,ùëò, the pooling function, denoted as pool(¬∑), is applied as follows:\nùë¶ùëô\nùëñ,ùëó,ùëò = pool(ùëéùëô\nùëö,ùëõ,ùëò),‚àÄ(ùëö,ùëõ)‚àà Rùëñùëó (5)\nAfter multiple convolutional and pooling layers, fully connected layers may exist for high-level\nreasoning. The output layer, often employing the softmax operator for classification, serves as the\nfinal layer. CNN parameters, denoted by ùúÉ, including weight vectors and bias terms, are optimized\nby minimizing a task-specific loss function. For ùëÅ input-output pairs (ùë•(ùëõ),ùë¶(ùëõ)); ùëõ ‚àà[1,ùëÅ ], the\nCNN loss is calculated as:\nLmulti = 1\nùëÅ\nùëÅ‚àëÔ∏Å\nùëõ=1\nùúÜ(ùúÉ;ùë¶(ùëõ),ùëú(ùëõ)) (6)\n4\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nTable 1. Summary of CNN-based MHAR methods applied on standard benchmark public datasets using\nfusion methods. Abbreviations: Modality (S: Skeleton, IR: Infrared, P: Pose, D: Depth), Datasets (U: UTD-\nMHAD, UT: UT-Kinect, N1:NTU RGB+D 60, N2:NTU RGB+D 120, NW: NW-UCLA, T: Toyota SH, M: MSAR\nDaily, UCF: UCF51).\nRef. Modality Fusion U UT N1 N2 NW T M UCF\n[51] RGB+S Middle 95.12 97.56 - - - - - -\n[78] S+IR Early 93.3 - 70.8 78.3 - - - -\n[26] RGB+P Middle - - 95.5 93.5 86.3 60.8 - -\n[27] P+IR Late - - 91.6 - - - - -\n[124] RGB+P Middle - - 91.99 - - - - -\n[90] RGB+P Hybrid - - 90.04 - - - - -\n[25] RGB+P Late - - 92.2 - 90.1 54.2 - -\n[71] Heatmap+P Late 94.5 - 91.7 - - - - -\n[158] RGB+P Late 92.5 - 94.3 - - - - -\n[9] S+P Late - - 86.6 - - - - -\n[77] RGB+P Late - - 85.5 - - - - -\n[55] RGB+S+D Late 95.1 - - - - - - -\n[103] RGB+D Middle - - 74.9 - - - 97.5 -\n[105, 106] RGB+Audio Middle - - - - - - - 86.7\n2.2.2 CNN-based Learning Methods. Convolutional Neural Network (CNN)-based Multimodal\nHuman Activity Recognition (MHAR) approaches can leverage the potential of automated feature\nlearning, wherein varying modalities inform and interact with each other through concealed\nfeatures. For a comparison of these techniques on benchmark datasets, please refer to Table 1. Some\nsignificant studies are summarized below.\nIslam et al. [51] introduced a Hierarchical Multimodal Attention-based Human Activity Recogni-\ntion Algorithm (HAMLET), which uses a unique feature encoder and a multi-head self-attention\nmechanism for each modality to encode spatio-temporal features. HAMLET employs a multimodal\nself-attention-based fusion architecture, called Multimodal Atention-based Feature Fusion (MAT),\nwhich blends attention-based fused features with the use of sum (extracted unimodal features are\nsummed after applying multimodal attention) and concatenate (in this approach the attended mul-\ntimodal features are concatenated) operations. Action classification is done using a fully connected\nlayer that harnesses the computed multimodal features. Contrastingly, [78] developed an intuitive\nmethodology that blends different modalities using a matrix concatenation operation, transforming\nsignals into an image for classification via a 2D CNN.\nAnother distinctive approach put forth by [26], combines spatial embeddings of RGB images and\n3D poses using an attention mechanism for extracting superior discriminatory spatio-temporal\npatterns. Moreover, [27] has proposed the late fusion of feature vectors from infrared (IR) and\n3D pose modules, classified by a multi-layer perceptron. Notably, [124] introduced an innovative\nmethod using intermediate fusion among RGB and 3D pose information through a Multimodal\nTransfer Module (MMTM). Moreover, [77] suggests a multi-task framework for concurrent 2D and\n3D estimation from still images and HAR from video sequences.\nThe work proposed by [90] used a network architecture search-based approach that applies a\nprogressive algorithm for multi-fusion architecture search and the introduction of new fusion layers.\nThis technique executes an explicit fusion of video and pose modalities via a search algorithm.\n5\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nFig. 3. The Transformer, as originally proposed in [125], depicted through visualization [101].\nAlternatively, [25] introduced an approach that utilizes a separable spatio-temporal attention model\nand a pose-driven attention model, late fusing scores by averaging softmax scores.\nThe Evolution of Pose Estimation Maps (PEM), proposed by [71], employs spatial rank pooling to\naggregate the evolution of heatmaps as a body shape evolution image, and body-guided sampling\nfor aggregating the evolution of poses as a body pose evolution image. Complementary features of\nboth images are then probed through CNNs for action classification. Further, [9] has proposed a\nmethod that uses unstructured collections of spatio-temporal glimpses with distributed recurrent\ntracking. Additionally, [103] has presented a deep multimodal feature analysis-based learning\nmachine that applies mixed norms for component regularization and group selection for superior\nclassification performance. Lastly, [ 55] has used a 5-CNN-streams approach, based on Motion\nHistory Image (MHI), Front Depth Motion Maps (DMMs), Side DMM, Top DMM, and Skeleton\nimages, fused at the decision level for action classification.\nThese various approaches offer different methods of fusing multimodal information and capitaliz-\ning on their synergies for improved performance in Human Activity Recognition. They demonstrate\nversatility and innovation in this rapidly advancing field.\n2.3 Learning Methods in Transformers\nWe first introduce Transformers here and then illustrate the major trends seen across all Transformer-\nbased architectures that have also been adopted for action recognition.\n2.3.1 Transformers. Transformers, as introduced by Vaswani et al. [125], have redefined neural\nnetwork architectures, primarily due to their superior capability in capturing long-term sequential\ndata dependencies. The Transformer was first proposed as a remedy to some limitations of sequence\nmodeling architectures, originally designed to deal with whole sequences at once (see Fig. 3),\nallowing parallelization of some operations (as opposed to RNNs, which are sequential in nature),\nand reducing the locality bias of traditional networks (such as CNNs). The key innovation in\ntransformers is the self-attention mechanism, which allows the network to selectively focus on\ndifferent parts of the input sequence based on their relevance to the current task. Different from\n6\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nRNNs, which operate sequentially, and traditional networks like CNNs that have a locality bias,\nTransformers treat entire sequences in parallel, a major breakthrough enabled by the self-attention\nmechanism.\nThe essence of the self-attention mechanism is computing three vectors for every input element:\nquery, key, and value. The mechanism assigns weights to each input element based on its query\nvector‚Äôs similarity, then derives an output from a weighted sum of the value vectors. Additionally,\nTransformers have other modules such as: Multi-head attention: It focuses on multiple sequence\nparts simultaneously by generating multiple sets of query, key, and value vectors. The outputs are\nthen amalgamated using a linear operation.\nAttention(ùëÑ,ùêæ,ùëâ )= softmax\n\u0012ùëÑùêæùëá\n‚àöùëëùëò\n\u0013\nùëâ (7)\nMultiHead(ùëÑ,ùêæ,ùëâ )= Concat(‚Ñéùëíùëéùëë1,...,‚Ñéùëíùëéùëë ‚Ñé)ùëäùëÇ (8)\nwhere\n‚Ñéùëíùëéùëëùëñ = Attention(ùëÑùëäùëÑ\nùëñ ,ùêæùëä ùêæ\nùëñ ,ùëâùëäùëâ\nùëñ ) (9)\nand ùëäùëÑ\nùëñ , ùëäùêæ\nùëñ , ùëäùëâ\nùëñ , and ùëäùëÇ are the learnable weight matrices. Position-wise feedforward net-\nwork: This module applies individual feedforward networks to every sequence element, which\nenhances the model‚Äôs sequential data handling.\nPosition-wise FeedForward(ùë•)= max(0,ùë•ùëä1 +ùëè1)ùëä2 +ùëè2 (10)\nwhere ùë•, ùëä1, ùëä2, ùëè1, and ùëè2 are input and learnable parameters.\n2.3.2 Transformers-based Learning Methods. Transformer-based methods have also been explored\nfor multimodal action recognition and have shown promising results. The self-attention mechanism\nof the Transformer allows it to selectively attend to different modalities and capture their temporal\nand spatial relationships, leading to improved performance compared to traditional methods [37].\nAdditionally, the use of cross-modal attention can help the model fuse information from different\nmodalities and improve its overall accuracy [160]. Table. 2 presents summary on architectures of\nsome significant transformer-based works in MHAR and refers to various features like Normal-\nization (Norm.), Activation (Act.), Encoder configuration (Heads/Layers), Efficiency (Aggregation,\nRestriction, and Weight-sharing), Positional-Encoding (PE, with type and strategy), and Backbone\nnetworks.\nInitial methods such as VATNet [39], CBT [137], and ELR [93] set a precedent. Their successes\npaved the way for a surge in transformer-based approaches, revealing a shift from traditional\nCNN methods, which primarily hinged on spatial features. Most methods either use Pre or Post\nnormalization. When paired with activation, GeLU and ReLU seem to be dominant choices. The\nvaried combinations indicate exploration to pinpoint the most efficient configuration.\nThe hierarchy of encoder architecture varies across different methods. While some like Actor-T\n[38] showcase simpler design configurations, others like Video Swin [73] take a deep dive with\nrelatively complex design choices. This reflects the diverse depth of transformer models tailored\nfor action recognition. However, notable methods, such as TimeSformer [ 11], ViViT [7], VATT\n[5], MViT [30], SCT [149], and others, have indicated the inclusion of aggregation, restriction, and\nweight-sharing strategies for achieving computational efficiency in design.\nA majority of the methods, including ViViT [ 7], SCT [149], and VTN [139], adopt positional\nencoding mechanisms, with strategies ranging from Learnt (L) to Fixed (F) and from Absolute (A)\nto Relative (R). This underscores the essential role of temporal awareness in videos. For backbone\nchoices, transformer-based methods for MHAR leverage diverse CNN-based backbones to capture\nspatiotemporal features essential for video understanding. In particular, I3D [ 18] is commonly\n7\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nadopted, known for its efficacy in video-related tasks, as seen in models like VATNet [39], LapFormer\n[60], and Actor-T [38]. S3D [141] in CBT [137] efficiently separates spatial and temporal information,\nenhancing video content understanding. ResNet-50, utilized in LapFormer [60], LTT [53], PEMM\n[65], TRX [91], and GroupFormer [67], provides a strong foundation with residual connections,\naddressing the vanishing gradient problem. R(2+1)D [ 122], found in LTT [ 53], STiCA [89], and\nGroupFormer [67], decomposes 3D convolutions for efficient spatiotemporal processing. SlowFast\n[33], employed in PEMM [65] and CATE [116], captures both slow and fast motion features, suitable\nfor varied temporal dynamics. The use of HRNet [132] in Actor-T [38] emphasizes maintaining\nhigh-resolution representations for detailed visual information. Additionally, various methods,\nincluding TimeSformer [11], ViViT [7], FAST [145], VATT [5], MViT [30], SCT [149], CATE [116],\nVideo Swin [73], and VTN [139], employ custom linear layers, showcasing a flexible approach to\nmultimodal feature fusion within the Transformer architecture. Overall, the selection of CNN-based\nbackbones reflects a strategic choice to enhance the ability of the model to recognize human actions\nin diverse multimodal contexts.\nTransformers, despite their successes, struggle with challenges. A scarcity of large-scale multi-\nmodal datasets reduces their performance, making valid comparisons with CNN-based methods\ndifficult. Their intricate architecture necessitates high computational power and memory. This\nunderlines an acute need for streamlined transformer architectures. The success of transformer-\nbased models, driven by their flexibility and depth, has steered the action recognition domain\ninto an experimental phase. Their departures and incorporations from CNN techniques not only\nnotice their distinctive design philosophy but also underline the evolutionary trajectory of video\nunderstanding. However, addressing their inherent challenges remains pivotal for yielding their\nfull potential in real-world scenarios.\n3 DATASETS\nA multitude of datasets have been created to develop and evaluate multimodal human activity\nrecognition (MHAR) methods. In Table 3, a series of benchmark datasets are listed along with their\nattributes such as scale, number of categories and provided modalities. For RGB-based MHAR,\nUCF101 [112], HMDB51 [63], and Kinectis-400 [54] are widely used as benchmark datasets, along\nwith Kinetics-600 [16], Kinetics-700 [17], EPIC-KITCHENS-55 [24], and ActivityNet [29]. Addi-\ntionally, the datasets MSR-DailyActivity3D [131] and Northwestern-UCLA [130] are widely used\nfor depth-based MHAR. Moreover, for MHAR, large benchmark datasets suitable for fusion and\nco-learning of different modalities include NTU RGB+D [102], NTU RGB+D 120 [70], and MMAct\n[61]. The datasets UTD-MHAD [19], and PKU-MMD [69] are also popularly used.\nMHAR datasets are generally created using different sensor perspectives and settings. For in-\nstance, there are single-view action datasets, multi-view action datasets and multi-person/interaction\naction datasets. In the single-view action datasets, a specific single viewpoint is captured. In the\nmulti-view datasets, two or more viewpoints of a single action are captured in the frame sequences.\nIn multi-person/interaction action datasets, action/activity is normally performed between two\nor more people [152]. Additionally, action recognition datasets are normally available in different\nmodalities, namely RGB, depth videos, skeleton joint positions, inertial sensor signals, and IR. These\nmodalities often allow capturing different aspects of information for the same action scenario. As\ncan be seen in Table 3, the datasets also vary considerably in terms of the size and number of classes.\nAs there is no public platform similar to YouTube available for MHAR dataset generation, MHAR\ncurrently lacks large-scale benchmark datasets when compared to video-based action recognition.\nMHAR datasets contain continuous (Con) or segmented (Seg) videos as reported in Table 3. The\ncontinuous video datasets comprise more than one action in a single video or frame sequence. They\nare mainly used for localization, detection, and prediction of actions. The techniques covered in\n8\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nTable 2. Summary of significant Transformer-based architectures in MHAR. Abbreviations: Norm: Normaliza-\ntion, Act: Activation (Pre or Post), Encoder: number of Heads/Layers (‚Äú-‚Äù subsequent blocks and ‚Äú*‚Äù repeated\nmodules), Efficient: ‚ÄúX‚Äù for indicating Aggregation, Restriction and Weight-sharing, PE: Positional-Encoding\n(L: Learnt/F: Fixed, A:Absolute/R: Relative), T: Tokenization (Patch, Clip, and Frame levels), Backbone (ResNet\nand DenseNet is abbreviated). Modality: (Visual, Audio, or Textual), SS: Self-Supervision, NA: not available,\nX/‚Äú‚Äì‚Äù applies or not.\nModel Norm. Act. Encoder\n(H/L) Backbone PE T Modal. SS\nVATNet[39] Post ReLU 2/3 I3D [18]\nFaster R-CNN [98] FA P X ‚Äì ‚Äì ‚Äì\nCBT [137] Post GeLU 4-1/1-NA S3D [141] ‚Äì C X ‚Äì X X\nELR [93] Post ReLU 8/1 I3D [18] ‚Äì P X ‚Äì ‚Äì ‚Äì\nLapFormer [60] Post ReLU 4/3 RN-50 [43] FA P X ‚Äì ‚Äì ‚Äì\nLTT [53] Pre GeLU 8/1 R(2+1)D [122] LA F X ‚Äì ‚Äì ‚Äì\nActor-T [38] Post ReLU 1/1 I3D [18]\nHRNet [132] FA P X ‚Äì ‚Äì ‚Äì\nTimeSformer [11] Pre GeLU 12/12 Linear Layer LA P X - - -\nPEMM [65] Post GeLU 6/12*3 SlowFast [33]\nRN-50 [43] LA C X X ‚Äì X\nViViT [7] Pre GeLU 16/24 Linear Layer LA P X ‚Äì ‚Äì ‚Äì\nFAST [145] Pre GeLU NA/8 2 Linear Layers LA P X ‚Äì ‚Äì ‚Äì\nVATT [5] Pre GeLU 12/12 Linear Layer LA P X X X X\nMViT [30] Pre GeLU 1-2-4-8/\n1-2-11-2 Linear Layer LA P X ‚Äì ‚Äì ‚Äì\nSCT [149] Pre GeLU 4-8-8-8/\n6-1-1-4 Linear Layer LA P X ‚Äì ‚Äì ‚Äì\nCATE [116] NA NA 12/4 SlowFast [33] ‚Äì C X ‚Äì ‚Äì X\nTRX [91] Pre NA 1/1 RN-50 [43] FA F X ‚Äì ‚Äì ‚Äì\nSTiCA [89] Post ReLU 4/2 R(2+1)D-18 [122]\nRN-9 [43] LA C X X ‚Äì X\nGroupFormer [67] Post ReLU 8*4 I3D [18] LA F X ‚Äì ‚Äì ‚Äì\nVideo Swin [73] Pre GeLU 3-6-12-24/\n2-2-6-2 Linear Layer LR P X ‚Äì ‚Äì ‚Äì\nMAiVAR-T [107] Pre ReLU 3-6-12-24/\n2-2-6-2 TSN [133] - P X X ‚Äì ‚Äì\nVTN [139] Pre GeLU 12-12/12-3 Linear Layer LA P X ‚Äì ‚Äì ‚Äì\nthis article generally focus on segmented datasets, which have complete actions in individual data\ninstances. These datasets can be easily used for the evaluation of action recognition algorithms.\nBelow, we discuss different aspects of commonly used segmented datasets.\nThe CMU MoCap [46] is a graphics motion-capture database. It is one of the earliest sources\nof action data, that covers a variety of actions, including the interaction between two subjects,\nlocomotion with uneven terrains, sports and several other human actions. CMU MoCap is a\nsegmented dataset with 45 classes of action performed on 144 subjects containing 2,235 video\nsamples. This dataset has a single line of sight, and it contains RGB and skeleton modalities. The\nMSR-Action3D [131] is the first RGB+D action dataset captured by the Kinect sensors. It contains\n9\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nTable 3. Public multimodal datasets. Notations : Seg: Segmented, Con: Continuous, D: Depth, S: Skeleton,\nAu: Audio, Ac: Accelerometer, IR: Infrared, #: Number of.* action classes with> 50 samples. Note: Some RGB\ndatasets are considered as multimodal because different modalities are from it such as audio, skeleton, and\noptical flow as well as some representations from compressed video formats.\nDataset Year Data Source Type Modality #Actions #Subjects #Samples #Views\nCMU MoCap [46] '01 MoCap Seg RGB,S 45 144 2,235 1\nMSR-Action3D [131] '10 Kinect v1 Seg S,D 20 10 567 1\nRGBD-HuDaAct [82] '11 Kinect v1 Seg RGB+D 13 30 1,189 1\nHMDB-51 [63] '11 YouTube Seg RGB 51 - 6,766 1\nMSR-DailyActivity3D [131]'12 Kinect v1 Seg RGB,D,S 16 10 320 1\nUCF-101 [112] '12 YouTube Seg RGB 101 - 13,320 1\nUT-Kinect [140] '12 Kinect v1 Seg RGB,D,S 10 10 200 1\nSBU Kinect [146] '12 Kinect v1 Seg RGB,D,S 7 8 300 1\n3D Action Pairs [86] '13 Kinect v1 Seg RGB,D,S 12 10 360 1\nBerkeley MHAD [84] '13 MoCap + Kinect v1 Seg RGB,D,S,Au,Ac 12 12 660 4\nNW-UCLA [130] '14 Kinect v1 Seg RGB,D,S 10 10 1,475 3\nUTD-MHAD [19] '15 Kinect v1 + Inertial Seg RGB,D,S 27 8 861 1\nNTU RGB+D 60 [102] '16 Kinect v2 Seg RGB,D,S,IR 60 40 56,880 80\nPKU-MMD [69] '17 Kinect v1 Con RGB,D,S,IR 51 66 1,076 3\nActivityNet-200 [29] '16 YouTube Con RGB,X 200 - 19,994 1\nKinetics 400 [54] '17 YouTube Seg RGB 400 - 306,245 1\nKinetics 600 [16] '18 YouTube Seg RGB 600 - 495,547 1\nAVE [120] '18 YouTube Seg RGB,Audio 28 - 4,143 1\nEPIC Kitchen 55 [24] '18 GoPro Hero 5 Con RGB 149* 32 39,596 1\nNTU RGB+D 120 [70] '19 Kinect v2 Seg RGB,D,S,IR 120 106 114,480 155\nToyota-SH [25] '19 Kinect v1 Seg RGB,D,S 31 18 16,115 1\nKinetics 700 [17] '19 YouTube Seg RGB 700 - 650,317 1\nEPIC Kitchen 100 [23] '20 GoPro Hero 7 Con RGB,Flow 4,053 37 89,977 1\n20 actions performed by ten subjects. This dataset requires some post-processing to remove the\nbackground. In particular, the right arms and legs are known to be preferred in this dataset for\nperforming several related actions.\nThe MSR-DailyActivity3D dataset [131] is collected by Microsoft and Northwestern University,\nfocusing on daily activities in a living room. A total of 16 actions are performed by ten subjects\nwhile sitting on a sofa or standing close to a sofa. This dataset functions as a good baseline for\nmultimodal video analysis, providing a small number of classes. The dataset provides RGB, depth\nand skeleton data modalities.\nThe Berkeley Multimodal Human Action Database (Berkeley MHAD) [84] is the first dataset to\nbe captured with five different modalities. One optical MoCap system, four multi-view cameras,\ntwo Kinect v1 cameras, six wireless accelerometers, and four microphones are used in the creation\nof this dataset. Berkeley MHAD contains eleven actions performed by seven male and five female\nsubjects, each varying in terms of style and speed. These actions are divided into the categories\nof: (1) full body movement actions, e.g., jumping jacks, throwing; (2) high dynamics in the upper\nextremity actions, e.g., waving hands, clapping hands; and (3) high dynamics in the lower extremity\nactions, e.g., sitting down, standing up. Audio and accelerometer data are two rare modalities in\nthis dataset.\nThe NTU RGB+D 120 [70] dataset, as highlighted in Table 3, is currently one of the largest action\nrecognition datasets in terms of the number of samples per action. It has RGB, depth, skeleton, and IR\nmodalities, captured with Kinect sensor v2. The dataset has more than 114,000 action video samples\nand up to 8 million image frames. The dataset contains 120 classes of daily and health-related actions\nperformed by 106 subjects aged around 10 to 57 years, with different cultural backgrounds. Further,\n10\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nit consists of 155 different camera viewpoints. This dataset is a strong candidate for evaluating the\nscalability and multimodal nature of algorithms.\nThe Toyota-Smarthome (Toyota-SH) [25] dataset contains approximately 16,100 unscripted video\nsamples with 31 action classes performed by 16 subjects. This dataset is one of the most recent\naction recognition datasets, with three different scenes and seven camera viewpoints. Toyota-SH\ndataset provides several real-world challenges such as improvisational acting, camera framing,\ncomposite activities, multi-view and the same activity using different objects. A unique feature\nof this dataset is that its actions are performed by subjects who did not receive any information\nregarding how to perform them. This dataset provides RGB, depth and skeleton modalities.\n4 FUSION METHODS\nTo this end, recent works have started to explore different fusing methods to incorporate multi-\nsensory data streams effectively. Compared with typical CNNs, a Transformer is naturally appropri-\nate for multi-stream data fusion because of its nonspecific embedding and dynamically interactive\nattention mechanisms. Multimodal data fusion refers to the mixing of features from data of various\nmodalities. The objective of multimodal data fusion is to achieve better accuracy than a single\nmodality. Data fusion supports diversity, enhancing the uses, advantages and analysis of ways that\ncannot be achieved through a single modality. Merging different modalities, as in [144], have many\nbenefits such as enhanced signal-to-noise ratio, improved confidence, increased robustness and\nreliability, enhanced resolution, better precision and discrimination as well as robustness against\ninterference [3].\nIn the context of MHAR, the choice of formulation depends on data characteristics and specific\ntask requirements, necessitating experimentation and empirical evaluation to identify the most\neffective fusion strategy for a given multimodal dataset. Researchers explore diverse architectures\nand fusion methods to optimize accuracy and robustness in MHAR systems. Fusion techniques\ncan be deployed at different stages in the action recognition process to acquire combinations of\ndistinct information. These fusion approaches depend on the type of data and technique used for\naction recognition. Multimodal data fusion approaches could be classified into classical-machine-\nlearning-based and deep learning-based, where latter can be further divided into CNNs- and\nTransformers-based, according to architectural choices. Various fusion approaches are discussed in\nthe following subsections.\n4.1 Fusion in CNNs\nData fusion in deep-learning-based MHAR techniques can be applied at similar stages as in classical-\nmachine-learning-based approaches. In the context of fusion in MHAR, a common strategy involves\nemploying modality-specific networks, where separate CNNs process each modality independently,\nand their outputs are combined through fully connected layers or other fusion techniques. Ad-\nditionally, for video-based action recognition, 3D CNNs capture spatial and temporal features\nsimultaneously, offering a natural approach to fuse information across space and time for multiple\nmodalities.\nFor video-based multimodal action recognition, the utilization of 3D CNNs, which capture spa-\ntiotemporal information simultaneously, presents an effective approach. This mirrors the approach\noften employed in single-modality video action recognition [62] and provides a natural means of\nfusing information over both space and time.\nTo accommodate heterogeneous modalities, studies have adopted the practice of utilizing separate\nCNNs for each modality [129]. Subsequently, the outputs from these modality-specific networks are\ncombined using fusion layers, often comprising fully connected layers or other fusion techniques.\n11\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nthe incorporation of temporal convolution layers [18] directly models the temporal dynamics\nwithin the data. This is critical for recognizing the temporal evolution of actions, a key factor in\naccurate action recognition.\nHowever, depending on the nature of the deep neural network, data-level, feature-level and\ndecision-level, techniques are often referred to as ‚Äúearly‚Äù [34], ‚Äúmiddle‚Äù [99] or ‚Äúintermediate‚Äù [88],\nand ‚Äúlate‚Äù fusion. Table 1 presents some common deep-learning-based MHAR methods that use\ndifferent data fusion approaches, which are discussed in the following subsections.\n4.1.1 Early Fusion. The early-fusion approach captures information and combines it at the feature\nlevel [34]. In this case, features from different modality sources are concatenated in initial layers\ninto an aggregated feature, which will then be used by the later layers for classification. As this\naggregated feature consists of many features, it increases the training and classification time.\nHowever, these large aggregates and suitable learning techniques can offer much better recognition\nperformance in the end. In Fig. 4, the initial process of feature fusion is illustrated as an example of\nearly fusion.\n4.1.2 Middle Fusion. The middle-fusion approach merges the features extracted from raw data in\nthe entire neural network so that the higher layers have access to more global information [88, 99].\nA CNN-based operation is performed to compute the weights and extend the connectivity of all\nlayers. Middle-level fusion attempts to exploit the advantages of early and late fusion in a common\nframework.\n4.1.3 Late Fusion. The late-fusion approach indicates a combination of the action information\nat the deepest layers in the network i.e., after the classification. For example, a typical MHAR\nnetwork architecture consists of two separate CNN-based networks with shared parameters up to\nthe last convolution layer. The outputs of the last convolution layer of these two separate network\nstreams are processed to the fully connected layer. This step predicts the final output after fusing\nthe classification scores and considers individual class labels at the score layer [ 105]. Different\ndecision rules are deployed to fuse the scores at this stage. Although late fusion ignores some\ninteractions between modality, it adds more simplicity and flexibility in making final decisions\nwhen one or more modalities is missing. The late fusion approach has been relatively successful in\nmost MHAR architectures. As evident in Fig. 4, the stage of Probability fusion is an example of late\nfusion in deep-learning-based methods.\n4.2 Fusion in Transformers\nTransformer-based techniques are commonly used for multimodal data fusion. On the one hand, the\nfusion process typically involves concatenating input sequences from all modalities, or employing\na form of cross-attention mechanism (as shown in Fig. 5c). Attention mechanisms dynamically\nweigh the importance of features from various modalities, allowing the network to focus on task-\nrelevant information and effectively fuse data. The fusion process can occur at different stages of\nthe architecture, including early, middle, or late stages. In this paper, we discuss both the how and\nwhere aspects of the fusion process in Transformer-based multimodal data fusion techniques.\n4.2.1 Early Fusion. This subsection discusses Early Fusion techniques which involve methods in\nwhich the fusion occurs prior to input being supplied to the encoder.\nEncoder Fusion (EF). Prior to being fed into the encoder, token embeddings from different modal-\nities are concatenated either in a sequence-wise manner as in [66, 72, 115] (refer to Fig. 5a) or in a\nchannel-wise manner as in [31]. The former can be likened to the approach of BERT [28] in dealing\nwith pairs of language sentences. To differentiate the tokens belonging to each modality, [ 115]\n12\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nFig. 4. An example of multi-level fusion in deep-learning-based action recognition, where the red dotted lines\nrepresent four integration points corresponding to different multimodal fusion methods examined. For a\nspecific integration point, the network is duplicated for ùêæ different modalities, concatenate the features at\nthe integration point, and the network after the integration point remain unchanged. (adapted from [74])\n(a)\n (b)\n(c)\n (d)\nFig. 5. Four main types of performing multimodal fusion in Transformers (adapted from [101]).\nseparator tokens are used, akin to the [SEP] token in BERT, which originally indicates the start of a\nnew sentence but here signals that the succeeding tokens are from another modality. However,\nmost video transformers (VTs) indicate the modality of a given token by summing or concate-\nnating learned modality embeddings in a manner similar to the addition of position embeddings.\nEncoder fusion significantly increases the computational cost of the self-attention operation up\nto ùëÇ((ùëá1 +...+ùëáùëö)2),where ùëÄ is the number of modalities and ùëáùëö is the number of tokens in the\nùëö-th modality. It is also worth noting here that we not only explore how but also where the fusion\nis integrated into the architecture, namely at early, middle, or late stages.\nHierarchical Encoder Fusion (HEF). In multimodal learning, Encoder fusion can be performed\nhierarchically by initially enhancing modality-specific token embeddings on individual encoders,\n13\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nconcatenating their outputs, and sending them to a multimodal encoder [65, 87, 114] (see Fig. 5b).\nThis approach enables intra-modal information to be dealt with before modelling the inter-modal\npatterns, which is advantageous when handling modalities that are not closely related or accurately\naligned at the input level. Experimentation is necessary to determine when to do this. Furthermore,\nthe computational cost is lower than that of the previous encoder fusion by a constant factor,\ndepending on the number of layers in both the unimodal and the multimodal encoders. Although\nusing multiple encoders increases the number of parameters, this can be mitigated by weight\nsharing [65].\n4.2.2 Middle Fusion. This subsection discusses Middle Fusion techniques which involve methods\nin which the fusion occurs before combining the outputs of the different encoders.\nCross-Attention Fusion (CAF). In the process of modalities fusion using CA, one modality seeks\ninformation from another modality to provide context (refer to Fig. 5c). This simple idea and\nits flexibility have led to different uses in various studies. In [49, 59], separate encoder-decoders\nare proposed for each modality, and each cross-attends to text embeddings before combining the\noutputs of the different encoders. The study of [52] proposes only one stream that keeps augmenting\na small set of latent embeddings by repeatedly cross-attending to the same very long multimodal\ninput sequence of minimal embeddings. In this case, CA layers are interleaved with SA layers\nthat refine the cross-attended information. [159] proposes a three-stream Transformer, where the\ncentral one cross-attends to the other two, and these, in turn, attend to the embeddings generated\nby their respective opposite previous cross-attentions. In contrast, [14] also uses three streams, one\nper modality. However, the fusion is achieved within a master stream that substitutes its SA by\nasymmetric cross-attention over the other two simultaneously (concatenating both sets of keys\nand values).\nCo-Attention Fusion (CoAF). In contrast to the cross-attention fusion, co-attention involves parallel\naugmentation of the two modalities by attending to each other‚Äôs embeddings. In this approach, the\nCA sub-layer in each stream generates the queries from its own embeddings, whereas keys and\nvalues are obtained from the other stream (see Fig. 5d). The ViLBERT model [75] originally proposed\nthis method for images and language, where it has since been adopted by several video-related\nworks [22, 48, 113]. While some works entirely replaced SA with CA [ 22, 113], others retained\nit [48]. In contrast to encoder fusion, co-attention reduces the computational cost to ùëÇ((ùëá1ùëá2)2).\nAdditionally, [153] has advocated for co-attending to each other and self-attending to themselves\nas a means of maintaining intra-modal and inter-modal dynamics separately.\n4.2.3 Late Fusion. All the aforementioned strategies for fusing modalities offer various early and\nmiddle fusion options. However, an alternative approach is to perform a late fusion of modalities.\nThis involves running different modalities through parallel encoders and combining their outputs.\nIn classification problems, these outputs could be class score distributions [38], as is typically done\nfor Two-stream ConvNets [109]. Although this strategy may be suboptimal for Transformers, it\nmay still be useful when there is limited training data available. However, late fusion could be\nconsidered by concatenating the augmented aggregation token as proposed by [59].\nThere are two concatenation-based approaches, namely Encoder Fusion (EF) and Hierarchical\nEncoder Fusion (HEF), the former of which concatenates the modalities at an early stage while the\nlatter concatenates them at a middle stage, which is more computationally intensive but eliminates\nthe need to experimentally determine the fusion point. In contrast, CAF and CoAF are based on\ncross-attention mechanisms and are inherently limited to fusing only two modalities. Nonetheless,\nrecent studies have attempted to extend these methods to more than two modalities, as evidenced\nby the cumbersome approach proposed by [159] in their ActBERT model. Compared to EF and HEF,\n14\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nboth CAF and CoAF are more efficient, but they still suffer from the issue of identifying the optimal\nfusion point. However, middle fusion may be useful when the input modalities are asynchronous\nor differ in nature.\n5 CHALLENGES, FUTURE DIRECTIONS AND LIMITATIONS\nIn this section of the survey article, we will delve into the potential challenges, future directions\nassociated with the field under discussion and limitations of this survey. By exploring the challenges,\nwe aim to shed light on the hurdles that must be overcome to make further progress. Further, by\ndiscussing future directions, we aim to provide insights into the exciting new avenues for research\nand development that lie ahead. Finally, by acknowledging the limitations of this survey, we aim to\nprovide a balanced and realistic view of its current scope and limitations.\n5.1 Challenges and Future Directions\nFuture of Datasets: Extensive and comprehensive datasets hold significant importance for the\nadvancement of MHAR, particularly for MHAR methods based on deep learning. Several factors,\nsuch as the magnitude, diversity, applicability, and modality type, indicate its quality. Despite\nthe availability of numerous datasets that have significantly progressed the MHAR field, the\ndevelopment of novel benchmark datasets is still necessary to further promote research in this\ndomain. For instance, most existing multimodal datasets have been obtained in controlled settings\nwith voluntary participants performing actions. Consequently, gathering multimodal data from\nuncontrolled environments to establish substantial and challenging benchmarks for enhancing\nMHAR‚Äôs practical applications would be beneficial. Moreover, constructing large and complex\ndatasets for recognizing each person‚Äôs actions in crowded settings warrants further exploration.\nMultimodal Learning: Earlier discussions have outlined various multimodal learning methods,\nincluding multimodal fusion and cross-modality transfer learning, proposed for MHAR. Multimodal\ndata fusion, which can complement each other, has been shown to enhance MHAR performance\nwhile co-learning can address data deficiencies in certain modalities. Nevertheless, as [135] has\nhighlighted, several challenges impede the effectiveness of many existing multimodal methods,\nsuch as the risk of overfitting. This underscores the need to develop more effective strategies\nfor multimodal fusion and co-learning in MHAR. This challenge requires further exploration\nand innovation to develop advanced techniques for addressing the limitations of the existing\nstate-of-the-art multimodal methods.\nLabelled Data Scarcity in Unsupervised and Semi-Supervised Learning: The application\nof supervised learning methods, particularly deep learning-based ones, typically necessitates a\nsubstantial amount of labelled data for model training, which can be expensive to obtain. Conversely,\nunsupervised and semi-supervised learning techniques [6, 110, 111] have the potential to utilize\nunlabelled data to train models, thereby mitigating the requirement for extensive labelled datasets.\nSince unlabelled action samples are often more easily collected than labelled ones, unsupervised and\nsemi-supervised MHAR has emerged as a crucial research direction that merits further investigation.\nGeneralization: It is acknowledged that while some works have investigated the generalization\ncapabilities of Transformers, this issue remains open and understudied, particularly in the case of\nVTs. Although some studies have examined the generalization of Transformers in natural language\nprocessing, such as out-of-distribution (OOD) generalization [44] and cross-modal transfer learning\nwith minimal fine-tuning [76], the analysis of the generalization capabilities of VTs is scarce. To\nthe best of our knowledge, only one work has thoroughly investigated the topic of generalization\nin VTs [150].\n15\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\nSelf-Supervised Approaches: There is a need for further investigation about video self-supervised\ntasks and their applicability to VTs. While contrastive techniques dominate self-supervised ap-\nproaches, there are several untapped possibilities for utilizing self-supervised video tasks on VTs,\nincluding temporal consistency [ 36], interframe predictability [ 42], geometric transformations\n[58] and motion statistics [128]. Additionally, the recent emergence of self-supervised vision tasks\nlike SimCLR [21] and Barlow Twins [148] for images, or BraVE [97] for video, warrants further\nexploration for their application to VTs.\nOnline and Mobile MHAR: While some studies have explored the use of deep MHAR on mobile\ndevices, such as smartphones and watches, they still face significant limitations in online and\nmobile deployment [12, 64]. In these approaches, the model is trained offline on a remote server,\nand the mobile device only employs the trained model, which is neither real-time nor efficient\nfor incremental learning. Two potential solutions to address this challenge are to reduce the\ncommunication cost between mobile and server and to improve the computing ability of mobile\ndevices.\nCollaboration in Deep-Shallow MHAR: High computational requirements of deep models\nare often a bottleneck, rendering them unsuitable for wearable devices. Contrastingly, shallow\nneural networks (NN) and traditional pattern recognition (PR) methods are not able to achieve\nhigh-performance levels. Therefore, a middle ground needs to be found by developing lightweight\ndeep models that can still achieve high-performance levels while being efficient enough to run\non mobile devices. In this regard, the collaborative efforts of deep and shallow models have the\npotential to provide accurate and lightweight MHAR solutions. However, several issues need to be\naddressed, such as how to effectively share parameters between deep and shallow models. Moreover,\nthe existing offline training of deep models hinders real-time execution, necessitating the need for\nonline training approaches that can enhance the adaptability of the model to new environments.\nEffective Transfer Learning: The process of data annotation via transfer learning is executed\nthrough the utilization of labelled data from auxiliary domains [ 127]. Various factors related to\nhuman activity can be leveraged as supplementary information through deep transfer learning.\nThis approach poses several challenges, including the sharing of weights between networks, the\neffective exploitation of knowledge between activity-related domains, and the identification of\nrelevant domains. These issues require resolution in order to facilitate the effective application of\ntransfer learning methodologies.\nHybrid Sensor and Context-Aware MHAR: The comprehensive information obtained from\nhybrid sensors is of considerable utility in discerning fine-grained activities [ 126]. Particular\nemphasis must be placed on the recognition of such activities through the collaborative utilization\nof hybrid sensors. In contrast, context refers to any information that can be employed to describe\nthe circumstances surrounding an entity [1]. Contextual data sources, such as Wi-Fi, Bluetooth,\nand GPS, can provide valuable insight into environmental factors associated with a given activity.\nThe effective exploitation of contextual information can greatly enhance the ability to recognize\nboth user states and specific activities.\nBeyond Activity Recognition: The recognition of activities frequently represents the initial stage\nin a variety of applications. For example, professional skill assessment is necessary for fitness train-\ning, while smart home assistants are indispensable components of healthcare services. Early efforts\nin activity recognition include research on climbing assessment [56]. Recent investigations suggest\nthat leveraging the expertise of crowds can substantially facilitate the task [92]. Crowdsourcing\noffers a means of annotating unlabelled activities by utilizing the collective abilities of a large\ngroup. In addition to passive label acquisition, researchers may also develop more sophisticated\n16\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\nand privacy-preserving methodologies to collect valuable labels. As deep learning continues to\nadvance, activity recognition applications can be expanded beyond simple recognition tasks.\n5.2 Limitations\nOur study was limited to research papers published within the last 10 years (2012-2022) and only\nincluded papers in English, potentially excluding some studies in other languages. Only studies\nthat utilized visual data were considered and others, such as those using olfactory, infrared, or\ntactile data, were outside the scope of this research. A potential limitation is publication bias, which\nmay lead to an overestimation of the advantages of using multiple forms of data for analysis. The\nstudies reviewed employed different input methods and evaluated various methods for recognizing\nactions on different datasets, making direct comparison of results difficult. Additionally, not all\narticles provided statistical confidence intervals, making it challenging to compare their findings.\n6 CONCLUSION\nIn this survey, we have conducted a comprehensive analysis and synthesis of the primary advance-\nments and emerging trends in adapting Convolutional Neural Networks (CNNs) and Transformers\nfor multimodal recognition of human actions. Drawing upon the existing literature, we have de-\nvised a robust taxonomy for CNN and Transformer architectures based on their modality, intended\ntask and overall structure. Furthermore, we have investigated diverse techniques for embedding,\nencoding and fusing different multimodal representations to achieve more accurate recognition of\nhuman actions.\nIn addition, we have provided a comparative analysis of the leading approaches on different\ndatasets and suggested key design modifications necessary for improving their performance.\nMoreover, we have discussed the current trends and potential challenges associated with the\ndifferent components of the multimodal action recognition pipeline. Despite the considerable\nprogress that has been made, the potential of multimodal fusion for action recognition remains\nlargely unexplored, and there remain several significant challenges that need to be addressed.\nFinally, we express a keen interest in comparing Transformer architectures to CNNs for multimodal\nunderstanding, given the tremendous promise of global-based learning methods.\nACKNOWLEDGMENTS\nThis work is partially funded by Edith Cowan University (ECU) and the Higher Education Commis-\nsion (HEC) of Pakistan under Project #PM/HRDI-UESTPs/UETs-I/Phase-1/Batch-VI/2018. Naveed\nAkhtar is the recipient of an Australian Research Council Discovery Early Career Researcher Award\n(project number DE230101058) funded by the Australian Government.\nREFERENCES\n[1] Gregory D Abowd, Anind K Dey, Peter J Brown, et al. 1999. Towards a better understanding of context and context-\nawareness. In Int. symposium on handheld and ubiquitous computing . Springer, 304‚Äì307.\n[2] J.K. Aggarwal and Lu Xia. 2014. Human Activity Recognition from 3D Data: A Review. Pattern Recognition Letters 48\n(2014), 70‚Äì80.\n[3] Antonio A. Aguileta, Ramon F. Brena, Oscar Mayora, Erik Molino-Minero-Re, and Luis A. Trejo. 2019. Multi-Sensor\nFusion for Activity Recognition : a Survey. Sensors 19 (Sept. 2019), 3808.\n[4] Kashif Ahmad and Nicola Conci. 2019. How Deep Features Have Improved Event Recognition in Multimedia: A\nSurvey. ACM Transactions on Multimedia Computing Communication Applications 15, 2, Article 39 (jun 2019), 27 pages.\n[5] Hassan Akbari, Liangzhe Yuan, Rui Qian, et al. 2021. Vatt: Transformers for multimodal self-supervised learning\nfrom raw video, audio and text. Advances in Neural Information Processing Systems 34 (2021), 24206‚Äì24221.\n[6] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, et al. 2016. Unsupervised learning from narrated instruction\nvideos. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 4575‚Äì4583.\n17\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\n[7] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luƒçiƒá, and Cordelia Schmid. 2021. ViViT: A video\nvision transformer. In Proc. of the Int. Conf. on Computer Vision . IEEE, 6836‚Äì6846.\n[8] Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S Kankanhalli. 2010. Multimodal fusion for\nmultimedia analysis: a survey. Multimedia systems 16 (2010), 345‚Äì379.\n[9] Fabien Baradel, Christian Wolf, Julien Mille, et al. 2018. Glimpse Clouds: Human Activity Recognition from Unstruc-\ntured Feature Points. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 469‚Äì478.\n[10] Ganbayar Batchuluun, Dat Tien Nguyen, Tuyen Danh Pham, Chanhum Park, and Kang Ryoung Park. 2019. Action\nrecognition from thermal videos. IEEE Access 7 (2019), 103893‚Äì103917.\n[11] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is space-time attention all you need for video understand-\ning?. In Proc. of the Int. Conf. on Machine Learning , Vol. 2. 4.\n[12] Sourav Bhattacharya and Nicholas D Lane. 2016. From smart to deep: Robust activity recognition on smartwatches\nusing deep learning. In Proc. of the IEEE Int. Conf. on Pervasive Computing and Communication Workshops . IEEE, 1‚Äì6.\n[13] XB Bruce, Yan Liu, and Keith CC Chan. 2020. Vision-Based Daily Routine Recognition for Healthcare with Transfer\nLearning. Int. Journal of Biomedical and Biological Engineering 14 (2020), 178‚Äì186.\n[14] Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, and Richard Bowden. 2020. Multi-channel transformers for\nmulti-articulatory sign language translation. In European Conf. on Computer Vision . Springer, 301‚Äì319.\n[15] Monica Carfagni, Rocco Furferi, Lapo Governi, et al. 2019. Metrological and Critical Characterization of the Intel\nD415 Stereo Depth Camera. Sensors 19, 3 (Jan. 2019), 489.\n[16] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. 2018. A short note about\nkinetics-600. arXiv:1808.01340 (2018).\n[17] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. 2019. A short note on the kinetics-700 human\naction dataset. arXiv:1907.06987 (2019).\n[18] Joao Carreira and Andrew Zisserman. 2017. Quo Vadis, action recognition? a new model and the kinetics dataset. In\nProc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . 6299‚Äì6308.\n[19] C. Chen, R. Jafari, and N. Kehtarnavaz. 2015. UTD-MHAD: A multimodal dataset for human action recognition\nutilizing a depth camera and a wearable inertial sensor. In Proc. of the Int. Conf. on Image Processing . IEEE, 168‚Äì172.\n[20] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. 2017. A survey of depth and inertial sensor fusion for human\naction recognition. Multimedia Tools Applications 76 (2017), 4405‚Äì4425.\n[21] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive\nlearning of visual representations. In Proc. of the Int. Conf. on Machine Learning . PMLR, 1597‚Äì1607.\n[22] David Curto, Albert Clap√©s, Javier Selva, Sorina Smeureanu, Julio Junior, CS Jacques, David Gallardo-Pujol, Georgina\nGuilera, David Leiva, Thomas B Moeslund, et al . 2021. Dyadformer: A multi-modal transformer for long-range\nmodeling of dyadic interactions. In Proc. of the Int. Conf. on Computer Vision . IEEE, 2177‚Äì2188.\n[23] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, , Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. 2022. Rescaling Egocentric Vision. Int.\nJournal of Computer Vision 130, 1 (2022), 33‚Äì55.\n[24] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. 2018. Scaling Egocentric Vision: the\nEPIC-KITCHENS Dataset. In Proc. of the European Conf. on Computer Vision . Springer.\n[25] Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, and Gianpiero Francesca.\n2019. Toyota Smarthome: Real-World Activities of Daily Living. In Proc. of the IEEE Int. Conf. on Computer Vision .\n833‚Äì842.\n[26] Srijan Das, Saurav Sharma, Rui Dai, et al. 2020. VPN: Learning video-pose embedding for activities of daily living. In\nProc. of the European Conf. on Computer Vision . Springer, 72‚Äì90.\n[27] A. M. De Boissiere and R. Noumeir. 2020. Infrared and 3D skeleton feature fusion for RGB-D action recognition. IEEE\nAccess 8 (2020), 168297‚Äì168308.\n[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv:1810.04805 (2018).\n[29] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia et al. 2015. ActivityNet: A Large-Scale Video Benchmark\nfor Human Activity Understanding. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE,\n961‚Äì970.\n[30] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\n2021. Multiscale vision transformers. In Proc. of the Int. Conf. on Computer Vision . IEEE, 6824‚Äì6835.\n[31] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. 2019. Scene memory transformer for embodied agents\nin long-horizon tasks. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 538‚Äì547.\n[32] Hannes Fassold and Barnabas Takacs. 2019. Towards Automatic Cinematography and Annotation for 360¬∞ Video. In\nProc. of the ACM Int. Conf. on Interactive Experiences for TV and Online Video . ACM, 157‚Äì166.\n18\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\n[33] Christoph Feichtenhofer et al. 2019. Slowfast networks for video recognition. InProc. of the IEEE Int. Conf. on Computer\nVision. IEEE, 6202‚Äì6211.\n[34] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. 2016. Convolutional Two-Stream Network Fusion for\nVideo Action Recognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 1933‚Äì1941.\n[35] Yachuang Feng, Yuan Yuan, and Xiaoqiang Lu. 2017. Learning deep event models for crowd anomaly detection.\nNeurocomputing 219 (2017), 548‚Äì556.\n[36] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. 2017. Self-supervised video representation\nlearning with odd-one-out networks. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE,\n3636‚Äì3645.\n[37] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval.\nIn Proc. of the European Conf. on Computer Vision . Springer, 214‚Äì229.\n[38] Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, and Cees GM Snoek. 2020. Actor-transformers for group activity\nrecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 839‚Äì848.\n[39] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. 2019. Video action transformer network. In Proc.\nof the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 244‚Äì253.\n[40] Guodong Guo and Alice Lai. 2014. A Survey on still-image-based human action recognition. Pattern Recognition 47,\n10 (2014), 3343‚Äì3361.\n[41] Fei Han, Brian Reily, William Hoff, and Hao Zhang. 2017. Space-time representation of people based on 3D skeletal\ndata: a review. Journal of Vision Communication Image Representation 158 (2017), 85‚Äì105.\n[42] Tengda Han, Weidi Xie, and Andrew Zisserman. 2019. Video representation learning by dense predictive coding. In\nProc. of the Int. Conf. on Computer Vision Workshops . IEEE, 1483‚Äì1492.\n[43] Kaiming He et al. 2016. Deep residual learning for image recognition. In Proc. of the IEEE Int. Conf. on Computer\nVision and Pattern Recognition . 770‚Äì778.\n[44] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained\ntransformers improve out-of-distribution robustness. arXiv:2004.06100 (2020).\n[45] Samitha Herath, Mehrtash Harandi, and Fatih Porikli. 2017. Going deeper into action recognition: a survey. Image\nVision Computing 60 (2017), 4‚Äì21.\n[46] Jessica Hodgins. 2021. Carnegie Mellon University - CMU Graphics Lab - motion capture library. http://mocap.cs.\ncmu.edu/. (Accessed on 01/28/2021).\n[47] Yi Huang, Xiaoshan Yang, Junyu Gao, Jitao Sang, and Changsheng Xu. 2020. Knowledge-Driven Egocentric Multimodal\nActivity Recognition. ACM Transactions on Multimedia Computing Communication Applications 16, 4, Article 133\n(2020), 133 pages.\n[48] Vladimir Iashin and Esa Rahtu. 2020. A better use of audio-visual cues: Dense video captioning with bi-modal\ntransformer. arXiv:2005.08271 (2020).\n[49] Vladimir Iashin and Esa Rahtu. 2020. Multi-modal dense video captioning. In Proc. of the IEEE Int. Conf. on Computer\nVision and Pattern Recognition Workshops . IEEE, 958‚Äì959.\n[50] ASUSTeK Computer Inc. 2020. Xtion PRO LIVE| 3D Sensor | ASUS USA. https://www.asus.com/us/3D-Sensor/Xtion_\nPRO_LIVE/. Accessed: 07-01-2022.\n[51] Md Mofijul Islam and Tariq Iqbal. 2020. Hamlet: A hierarchical multimodal attention-based human activity recognition\nalgorithm. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 10285‚Äì10292.\n[52] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. 2021. Perceiver:\nGeneral perception with iterative attention. In Proc. of the Int. Conf. on Machine Learning . 4651‚Äì4664.\n[53] M Kalfaoglu, Sinan Kalkan, and A Aydin Alatan. 2020. Late temporal modeling in 3D CNN architectures with BERT\nfor action recognition. In European Conf. on Computer Vision . Springer, 731‚Äì747.\n[54] Will Kay, Joao Carreira, Karen Simonyan, et al. 2017. The kinetics human action video dataset. arXiv:1705.06950\n(2017).\n[55] Pushpajit Khaire, Praveen Kumar, and Javed Imran. 2018. Combining CNN streams of RGB-D and skeletal data for\nhuman activity recognition. Pattern Recognition Letters 115 (2018), 107 ‚Äì 116.\n[56] Aftab Khan, Sebastian Mellor, Eugen Berlin, et al. 2015. Beyond activity recognition: skill assessment from accelerom-\neter data. In Proc. of the Int. Joint Conf. on Pervasive and Ubiquitous Computing . 1155‚Äì1166.\n[57] Muhammad Attique Khan, Kashif Javed, Sajid Ali Khan, Tanzila Saba, Usman Habib, Junaid Ali Khan, and Aaqif Afzaal\nAbbasi. 2020. Human action recognition using fusion of multiview and deep features: an application to video\nsurveillance. Multimedia Tools and Applications (2020), 1‚Äì27.\n[58] Dahun Kim, Donghyeon Cho, and In So Kweon. 2019. Self-supervised video representation learning with space-time\ncubic puzzles. In Proc. of the AAAI Conf. on Artificial Intelligence , Vol. 33. 8545‚Äì8552.\n[59] Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang. 2018. Multimodal dual attention memory\nfor video story question answering. In Proc. of the European Conf. on Computer Vision . Springer, 673‚Äì688.\n19\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\n[60] Satoshi Kondo. 2021. Lapformer: surgical tool detection in laparoscopic surgical video using transformer architecture.\nComputer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization 9, 3 (2021), 302‚Äì307.\n[61] Quan Kong, Ziming Wu, Ziwei Deng, Martin Klinkigt, Bin Tong, and Tomokazu Murakami. 2019. MMAct: A large-scale\ndataset for cross modal human action understanding. In Proc. of the Int. Conf. on Computer Vision . IEEE, 8658‚Äì8667.\n[62] Bruno Korbar, Du Tran, and Lorenzo Torresani. 2019. SCSampler: Sampling Salient Clips from Video for Efficient\nAction Recognition. In Proc. of the IEEE Int. Conf. on Computer Vision . IEEE, 6231‚Äì6241.\n[63] Hilde Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. 2011. HMDB: A Large Video\nDatabase for Human Motion Recognition. In Proc. of the IEEE Int. Conf. on Computer Vision . IEEE, 2556‚Äì2563.\n[64] Nicholas D Lane, Petko Georgiev, and Lorena Qendro. 2015. Deepear: robust smartphone audio sensing in un-\nconstrained acoustic environments using deep learning. In Proc. of the Int. Joint Conf. on Pervasive and Ubiquitous\nComputing. 283‚Äì294.\n[65] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, and Yale Song. 2020. Parameter efficient multimodal\ntransformers for video representation learning. arXiv:2012.04124 (2020).\n[66] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020. Hero: Hierarchical encoder for\nvideo+ language omni-representation pre-training. arXiv:2005.00200 (2020).\n[67] Shuaicheng Li, Qianggang Cao, Lingbo Liu, Kunlin Yang, Shinan Liu, Jun Hou, and Shuai Yi. 2021. Groupformer:\nGroup activity recognition with clustered spatial-temporal transformer. In Proc. of the Int. Conf. on Computer Vision .\nIEEE, 13668‚Äì13677.\n[68] Daniele Liciotti, Michele Bernardini, Luca Romeo, and Emanuele Frontoni. 2020. A sequential deep learning application\nfor recognising human activities in smart homes. Neurocomputing 396 (2020), 501‚Äì513.\n[69] Chunhui Liu, Yueyu Hu, Yanghao Li, Sijie Song, and Jiaying Liu. 2017. PKU-MMD: A Large Scale Benchmark for\nContinuous Multi-Modal Human Action Understanding. arXiv 1903.11314 (2017).\n[70] Jun Liu, Amir Shahroudy, Mauricio Lisboa Perez, Gang Wang, Ling-Yu Duan, and Alex Kot Chichung. 2019. NTU\nRGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding. IEEE Transactions on Pattern Analysis\nMachine Intelligence (2019), 2684 ‚Äì 2701.\n[71] Mengyuan Liu and Junsong Yuan. 2018. Recognizing Human Actions as the Evolution of Pose Estimation Maps. In\nProc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 1159‚Äì1168.\n[72] Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical\ntransformer with momentum contrast for video-text retrieval. In Proc. of the Int. Conf. on Computer Vision . IEEE,\n11915‚Äì11925.\n[73] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2022. Video swin transformer. In\nProc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 3202‚Äì3211.\n[74] Xiang Long, Chuang Gan, Gerard de Melo, Xiao Liu, Yandong Li, Fu Li, and Shilei Wen. 2018. Multimodal Keyless\nAttention Fusion for Video Classification. In Proc. of the AAAI Conf. on Artificial Intelligence . AAAI Press, 7202‚Äì7209.\n[75] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. VilBERT: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. Advances in neural information processing systems 32 (2019).\n[76] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2021. Pretrained transformers as universal computation\nengines. arXiv:2103.05247 (2021).\n[77] D. C. Luvizon, D. Picard, and H. Tabia. 2018. 2D/3D Pose Estimation and Action Recognition Using Multitask Deep\nLearning. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 5137‚Äì5146.\n[78] Raphael Memmesheimer, Nick Theisen, and Dietrich Paulus. 2020. Gimme Signals: Discriminative signal encoding\nfor multimodal activity recognition. arXiv:2003.06156 (2020).\n[79] Xiongkuo Min, Guangtao Zhai, Ke Gu, and Xiaokang Yang. 2016. Fixation Prediction through Multimodal Analysis.\nACM Transactions on Multimedia Computing Communication Applications 13, 1, Article 6 (2016), 23 pages.\n[80] Farida Mohsen, Hazrat Ali, Nady El Hajj, and Zubair Shah. 2022. Artificial intelligence-based methods for fusion of\nelectronic health records and imaging data. Scientific Reports 12, 1 (2022), 1‚Äì16.\n[81] Mihai Nan, Alexandra Stefania Ghit,ƒÉ, Alexandru-Florin Gavril, Mihai Trascau, Alexandru Sorici, Bogdan Cramariuc,\nand Adina Magda Florea. 2019. Human Action Recognition for Social Robots. In Proc. of the Int. Conf. on Control\nSystems and Computer Science . IEEE, 675‚Äì681.\n[82] Bingbing Ni, Gang Wang, and Pierre Moulin. 2011. RGBD-HuDaAct: A color-depth video database for human daily\nactivity recognition. In Proc. of the IEEE Int. Conf. on Computer Vision Workshops . IEEE, 1147‚Äì1153.\n[83] Weizhi Nie, Qi Liang, Yixin Wang, Xing Wei, and Yuting Su. 2020. MMFN: Multimodal Information Fusion Networks\nfor 3D Model Classification and Retrieval. ACM Transactions on Multimedia Computing Communication Applications\n16, 4, Article 131 (2020), 22 pages.\n[84] F. Ofli, R. Chaudhry, G. Kurillo, R. Vidal, and R. Bajcsy. 2013. Berkeley MHAD: A comprehensive Multimodal Human\nAction Database. In Proc. of the Workshop on Applications of Comp. Vision . IEEE, Tampa, USA, 53‚Äì60.\n[85] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n20\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\n[86] Omar Oreifej and Zicheng Liu. 2013. HON4D: Histogram of Oriented 4D Normals for Activity Recognition from\nDepth Sequences. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 716‚Äì723.\n[87] Alexander Pashevich, Cordelia Schmid, and Chen Sun. 2021. Episodic transformer for vision-and-language navigation.\nIn Proc. of the IEEE Int. Conf. on Computer Vision . IEEE, 15942‚Äì15952.\n[88] Chirag I Patel, Sanjay Garg, Tanish Zaveri, Asim Banerjee, and Ripal Patel. 2018. Human action recognition using\nfusion of features for unconstrained video sequences. Computers & Electrical Engineering 70 (2018), 284‚Äì301.\n[89] Mandela Patrick, Po-Yao Huang, Ishan Misra, et al. 2021. Space-time crop & attend: Improving cross-modal video\nrepresentation learning. In Proc. of the Int. Conf. on Computer Vision . IEEE, 10560‚Äì10572.\n[90] Juan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pateux, et al. 2019. MFAS: Multimodal Fusion Architecture\nSearch. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 6966‚Äì6975.\n[91] Toby Perrett, Alessandro Masullo, Tilo Burghardt, Majid Mirmehdi, and Dima Damen. 2021. Temporal-relational\ncrosstransformers for few-shot action recognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern\nRecognition. IEEE, 475‚Äì484.\n[92] Dra≈æen Prelec, H Sebastian Seung, and John McCoy. 2017. A solution to the single-question crowd wisdom problem.\nNature 541, 7638 (2017), 532‚Äì535.\n[93] Didik Purwanto, Rizard Renanda Adhi Pramono, Chen, et al. 2019. Extreme low resolution action recognition with\nspatial-temporal multi-head self-attention and knowledge distillation. In Proc. of the Int. Conf. on Computer Vision\nWorkshops. IEEE, 961‚Äì969.\n[94] Mengshi Qi, Yunhong Wang, Jie Qin, et al. 2019. StagNet: An attentive semantic RNN for group activity and individual\naction recognition. IEEE Transactions on Circuits and Systems for Video Technology 30, 2 (2019), 549‚Äì565.\n[95] MD Abdur Rahman, M. Shamim Hossain, Nabil A. Alrajeh, and B. B. Gupta. 2021. A Multimodal, Multimedia\nPoint-of-Care Deep Learning Framework for COVID-19 Diagnosis. ACM Transactions on Multimedia Computing\nCommunication Applications 17, 1s, Article 18 (2021), 24 pages.\n[96] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\n2021. Zero-shot text-to-image generation. In Int. Conf. on Machine Learning . PMLR, 8821‚Äì8831.\n[97] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, et al. 2021. Broaden your views for self-supervised video learning.\nIn Proc. of the Int. Conf. on Computer Vision . IEEE, 1255‚Äì1265.\n[98] Shaoqing Ren et al. 2015. Faster R-CNN: Towards real-time object detection with region proposal networks. In Proc.\nof the NeurIPS , Vol. 28. 91‚Äì99.\n[99] Alina Roitberg, Tim Pollert, Monica Haurilet, et al. 2019. Analysis of Deep Fusion Strategies for Multi-Modal Gesture\nRecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops . IEEE, 198‚Äì206.\n[100] Robin Rombach, Andreas Blattmann, Dominik Lorenz, et al . 2022. High-resolution image synthesis with latent\ndiffusion models. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 10684‚Äì10695.\n[101] Javier Selva, Anders S Johansen, Sergio Escalera, Kamal Nasrollahi, Thomas B Moeslund, and Albert Clap√©s. 2022.\nVideo transformers: a survey. arXiv:2201.05991 (2022).\n[102] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. 2016. NTU RGB+D: a large scale dataset for 3D human\nactivity analysis. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition .\n[103] Amir Shahroudy, Tian-Tsong Ng, Yihong Gong, and Gang Wang. 2016. Deep multimodal feature analysis for action\nrecognition in RGB-D Videos. arXiv 1603.07120 (2016).\n[104] Muhammad Bilal Shaikh and Douglas Chai. 2021. RGB-D data-based action recognition: a review. Sensors 21, 12\n(2021), 4246.\n[105] Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, and Naveed Akhtar. 2022. MAiVAR:\nMultimodal Audio-Image and Video Action Recognizer. In International Conference on Visual Communications and\nImage Processing (VCIP) . IEEE, Suzhou, China, 1‚Äì5.\n[106] Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, and Naveed Akhtar. 2024. Multimodal\nFusion for Audio-Image and Video Action Recognition. Neural Computing and Applications (2024), 1‚Äì14.\n[107] Muhammad Bilal Shaikh, Douglas Chai, Syed Mohammed Shamsul Islam, and Naveed Akhtar. 2023. MAiVAR-T:\nMultimodal Audio-image and Video Action Recognizer using Transformers. In 11th European Workshop on Visual\nInformation Processing (EUVIP) . 1‚Äì6.\n[108] Muhammad Sharif, Muhammad Attique Khan, Farooq Zahid, et al. 2020. Human action recognition: a framework of\nstatistical weighted segmentation and rank correlation-based selection. Pattern Analysis and Applications 23 (2020),\n281‚Äì294.\n[109] Karen Simonyan and Andrew Zisserman. 2014. Two-stream Convolutional Networks for Action Recognition in\nVideos. In Proc. of the Int. Conf. on Neural Information Process. Systems (NIPS) , Vol. 1. MIT Press, 568‚Äì576.\n[110] Ankit Singh, Omprakash Chakraborty, Ashutosh Varshney, et al. 2021. Semi-supervised action recognition with\ntemporal contrastive learning. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE,\n10389‚Äì10399.\n21\nACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY Shaikh and Chai, et al.\n[111] Xiaolin Song, Sicheng Zhao, Jingyu Yang, et al . 2021. Spatio-temporal contrastive domain adaptation for action\nrecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 9787‚Äì9795.\n[112] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: A dataset of 101 human actions classes\nfrom videos in the wild. arXiv:1212.0402 (2012).\n[113] Rui Su, Qian Yu, and Dong Xu. 2021. STVGbert: A visual-linguistic transformer based framework for spatio-temporal\nvideo grounding. In Proc. of the Int. Conf. on Computer Vision . IEEE, 1533‚Äì1542.\n[114] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. 2019. Learning Video Representations using\nContrastive Bidirectional Transformer. arXiv 1906.05743 (2019).\n[115] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. VideoBERT: A joint model for\nvideo and language representation learning. In Proc. of the Int. Conf. on Computer Vision . IEEE, 7464‚Äì7473.\n[116] Chen Sun, Arsha Nagrani, Yonglong Tian, and Cordelia Schmid. 2021. Composable augmentation encoding for video\nrepresentation learning. In Proc. of the Int. Conf. on Computer Vision . IEEE, 8834‚Äì8844.\n[117] Han Sun and Yu Chen. 2022. Real-time Elderly Monitoring for Senior Safety by Lightweight Human Action Recognition.\nIn Proc. of the IEEE Int. Symposium on Medical Information and Communication Technology . IEEE, 1‚Äì6.\n[118] Zehua Sun, Qiuhong Ke, Hossein Rahmani, et al. 2022. Human Action Recognition From Various Data Modalities: A\nReview. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022), 1‚Äì20.\n[119] S. Susan, P. Agrawal, M. Mittal, and S. Bansal. 2019. New shape descriptor in the context of edge continuity. CAAI\nTransactions on Intelligence Technology 4 (2019), 101‚Äì109.\n[120] Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. 2018. Audio-visual event localization in uncon-\nstrained videos. In Proc. of the European Conf. on Computer Vision . Springer, 247‚Äì263.\n[121] Y. Tingting, W. Junqian, W. Lintai, and X. Yong. 2019. Three-stage network for age estimation. CAAI Transactions on\nIntelligence Technology 4 (2019), 122‚Äì126.\n[122] Du Tran, Heng Wang, Lorenzo Torresani, et al . 2018. A closer look at spatiotemporal convolutions for action\nrecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 6450‚Äì6459.\n[123] Amin Ullah, Khan Muhammad, Ijaz Ul Haq, and Sung Wook Baik. 2019. Action recognition using optimized deep\nautoencoder and CNN for surveillance data streams of non-stationary environments. Future Generation Computer\nSystems 96 (2019), 386‚Äì397.\n[124] Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L. Iuzzolino, and Kazuhito Koishida. 2020. MMTM: Multimodal\nTransfer Module for CNN Fusion. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE,\nVirtual, 13289‚Äì13299.\n[125] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you need. In Advances in Neural Information\nProcessing Systems , Vol. 30.\n[126] Praneeth Vepakomma, Debraj De, Sajal K Das, and Shekhar Bhansali. 2015. A-Wristocracy: Deep learning on wrist-\nworn sensing for recognition of user complex activities. In Proc. of the IEEE Int. Conf. on Wearable and Implantable\nBody Sensor Networks (BSN) . IEEE, 1‚Äì6.\n[127] Jindong Wang, Yiqiang Chen, Shuji Hao, et al . 2017. Balanced distribution adaptation for transfer learning. In\nProceddings of the IEEE Int. Conf. on Data Mining . IEEE, 1129‚Äì1134.\n[128] Jiangliu Wang, Jianbo Jiao, Linchao Bao, et al . 2019. Self-supervised spatio-temporal representation learning for\nvideos by predicting motion and appearance statistics. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern\nRecognition. IEEE, 4006‚Äì4015.\n[129] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan. 2012. Mining actionlet ensemble for action recognition with\ndepth cameras. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 1290‚Äì1297.\n[130] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. 2014. Cross-view action modeling, learning and\nrecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 2649‚Äì2656.\n[131] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. 2014. Cross-view action modelling, learning and\nrecognition. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . Columbus, USA, 2649‚Äì2656.\n[132] Jingdong Wang, Ke Sun, Tianheng Cheng, et al . 2020. Deep high-resolution representation learning for visual\nrecognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 43, 10 (2020), 3349‚Äì3364.\n[133] Limin Wang et al. 2016. Temporal segment networks: Towards good practices for deep action recognition. In Proc. of\nthe European Conf. on Computer Vision . Springer, 20‚Äì36.\n[134] Pichao Wang, Wanqing Li, Philip Ogunbona, Jun Wan, and Sergio Escalera. 2018. RGB-D-based human motion\nrecognition with deep learning: a survey. Computing Vision Image Understanding 171 (2018), 118‚Äì139.\n[135] Weiyao Wang, Du Tran, and Matt Feiszli. 2020. What makes training multi-modal classification networks hard?. In\nProc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 12695‚Äì12705.\n[136] Yang Wang. 2021. Survey on Deep Multi-Modal Data Analytics: Collaboration, Rivalry, and Fusion.ACM Transactions\non Multimedia Computing Communication Applications 17, 1s, Article 10 (2021), 25 pages.\n22\nFrom CNNs to Transformers in Multimodal Human Action Recognition: A Survey ACM Trans. Multimedia Comput. Commun. Appl. ‚Äô24, , NY\n[137] Zhen Wang, Shixian Luo, He Sun, et al . 2019. An efficient non-local attention network for video-based person\nre-identification. In Proc. of the Int. Conf. on Information Technology: IoT and Smart City . ACM, 212‚Äì217.\n[138] Zihao W. Wang, Vibhav Vineet, Francesco Pittaluga, et al. 2019. Privacy-Preserving Action Recognition Using Coded\nAperture Videos. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops . 1‚Äì10.\n[139] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. 2021. Rethinking and improving relative\nposition encoding for vision transformer. In Proc. of the Int. Conf. on Computer Vision . IEEE, 10033‚Äì10041.\n[140] L. Xia, C.C. Chen, and JK Aggarwal. 2012. View Invariant Human Action Recognition using Histograms of 3D Joints.\nIn Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 20‚Äì27.\n[141] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. 2017. Rethinking spatiotemporal feature\nlearning for video understanding. arXiv:1712.04851 1, 2 (2017), 5.\n[142] Tong Xu, Peilun Zhou, Linkang Hu, Xiangnan He, Yao Hu, and Enhong Chen. 2021. Socializing the Videos: A\nMultimodal Approach for Social Relation Recognition. ACM Transactions on Multimedia Computing Communication\nApplications 17, 1, Article 23 (2021), 23 pages.\n[143] Lin Yang, Longyu Zhang, Haiwei Dong, Abdulhameed Alelaiwi, and Abdulmotaleb El Saddik. 2015. Evaluating and\nimproving the depth accuracy of Kinect for Windows v2. IEEE Sensors 15 (2015), 4275‚Äì4285.\n[144] Guanghao Yin, Shouqian Sun, Dian Yu, Dejian Li, and Kejun Zhang. 2022. A Multimodal Framework for Large-\nScale Emotion Recognition by Fusing Music and Electrodermal Activity Signals. ACM Transactions on Multimedia\nComputing Communication Applications 18, 3, Article 78 (2022), 23 pages.\n[145] Bingyao Yu, Wanhua Li, Xiu Li, Jiwen Lu, and Jie Zhou. 2021. Frequency-aware spatiotemporal transformers for\nvideo inpainting detection. In Proc. of the Int. Conf. on Computer Vision . IEEE, 8188‚Äì8197.\n[146] Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay, et al. 2012. Two-person Interaction Detection Using Body-Pose\nFeatures and Multiple Instance Learning. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition\nWorkshops. IEEE, 28‚Äì35.\n[147] Abrar Zahin, Rose Qingyang Hu, et al . 2019. Sensor-based human activity recognition for smart healthcare: a\nsemi-supervised machine learning. In Proceedins of the Int. Conf. on Artificial Intelligence for Communications and\nNetworks. Springer, 450‚Äì472.\n[148] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St√©phane Deny. 2021. Barlow twins: self-supervised learning via\nredundancy reduction. In Int. Conf. on Machine Learning . PMLR, 12310‚Äì12320.\n[149] Xuefan Zha, Wentao Zhu, Lv Xun, Sen Yang, and Ji Liu. 2021. Shifted chunk transformer for spatio-temporal\nrepresentational learning. Advances in Neural Information Processing Systems 34 (2021), 11384‚Äì11396.\n[150] Chongzhi Zhang, Mingyuan Zhang, Zhang, et al. 2022. Delving deep into the generalization of vision transformers\nunder distribution shifts. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE, 7277‚Äì7286.\n[151] Hong Bo Zhang, Yi Xiang Zhang, Bineng Zhong, et al. 2019. A Comprehensive Survey of Vision-Based Human Action\nRecognition Methods. Sensors 19, 5 (2019), 1005.\n[152] Jing Zhang, Wanqing Li, Philip O. Ogunbona, et al. 2016. RGB-D-based action recognition datasets: a survey. Pattern\nRecognition 60 (2016), 86‚Äì105.\n[153] Mingxing Zhang, Yang Yang, Xinghan Chen, et al. 2021. Multi-stage aggregated transformer network for temporal\nlanguage localization in videos. In Proc. of the IEEE Int. Conf. on Computer Vision and Pattern Recognition . IEEE,\n12669‚Äì12678.\n[154] Wei Zhang, Ting Yao, Shiai Zhu, and Abdulmotaleb El Saddik. 2019. Deep learning‚Äìbased Multimedia Analytics: A\nReview. ACM Transactions on Multimedia Computing Communication Applications 15, 1s, Article 2 (2019), 26 pages.\n[155] Z. Zhang, X. Ma, R. Song, X. Rong, X. Tian, G. Tian, and Y. Li. 2017. Deep learning-based human action recognition: a\nsurvey. In Chinese Automation Congress (CAC) . IEEE, 3780‚Äì3785.\n[156] Fan Zhu, Ling Shao, Jin Xie, and Yi Fang. 2016. From handcrafted to learned representations for human action\nrecognition: a survey. Image Vision Computing 55 (2016), 42‚Äì52.\n[157] G. Zhu, L. Zhang, L. Mei, Jie Shao, Juan Song, and Peiyi Shen. 2016. Large-scale Isolated Gesture Recognition using\npyramidal 3D convolutional networks. In Proc. of the Int. Conf. on Pattern Recognition . IEEE, 19‚Äì24.\n[158] Jiagang Zhu, Wei Zou, Liang Xu, et al. 2018. Action Machine: Rethinking Action Recognition in Trimmed Videos.\narXiv 1812.05770 (2018).\n[159] Linchao Zhu and Yi Yang. 2020. ActBERT: Learning global-local video-text representations. In Proc. of the IEEE Int.\nConf. on Computer Vision and Pattern Recognition . IEEE, 8746‚Äì8755.\n[160] Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, and Ian Reid. 2017. Attend in Groups: A Weakly-Supervised\nDeep Learning Framework for Learning From Web Data. InProc. of the IEEE Int. Conf. on Computer Vision and Pattern\nRecognition. IEEE, 1878‚Äì1887.\n[161] Athanasia Zlatintsi, AC Dometios, Nikolaos Kardaris, et al. 2020. I-Support: a robotic platform of an assistive bathing\nrobot for the elderly population. Robotics and Autonomous Systems 126 (2020), 103451.\n23",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7382522821426392
    },
    {
      "name": "Action recognition",
      "score": 0.6078736782073975
    },
    {
      "name": "Modalities",
      "score": 0.6017809510231018
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5754323601722717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5520117282867432
    },
    {
      "name": "Transformer",
      "score": 0.5085586309432983
    },
    {
      "name": "Multimodal learning",
      "score": 0.501563549041748
    },
    {
      "name": "Paradigm shift",
      "score": 0.4874146282672882
    },
    {
      "name": "Deep learning",
      "score": 0.4708554148674011
    },
    {
      "name": "Machine learning",
      "score": 0.44564002752304077
    },
    {
      "name": "Data science",
      "score": 0.3877883553504944
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.3724755644798279
    },
    {
      "name": "Engineering",
      "score": 0.12224262952804565
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}