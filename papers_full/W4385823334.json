{
  "title": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic Weighting",
  "url": "https://openalex.org/W4385823334",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1566979378",
      "name": "Wang Hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2228097711",
      "name": "Zhao Shi-Wan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329146398",
      "name": "Zheng Xiguang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1940263886",
      "name": "Qin Yong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3209059054",
    "https://openalex.org/W2963091184",
    "https://openalex.org/W4296068818",
    "https://openalex.org/W4296070386",
    "https://openalex.org/W4372263384",
    "https://openalex.org/W4385572772",
    "https://openalex.org/W2963403924",
    "https://openalex.org/W2963609956",
    "https://openalex.org/W3016160783",
    "https://openalex.org/W2972394484",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W3161558238",
    "https://openalex.org/W4225956675",
    "https://openalex.org/W4300816538",
    "https://openalex.org/W3175863856",
    "https://openalex.org/W4382935141",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W4226198268",
    "https://openalex.org/W3202278141",
    "https://openalex.org/W2557915412",
    "https://openalex.org/W4223503417",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4385572795",
    "https://openalex.org/W4297570641",
    "https://openalex.org/W3196225973"
  ],
  "abstract": "Automatic Mean Opinion Score (MOS) prediction is crucial to evaluate the perceptual quality of the synthetic speech. While recent approaches using pre-trained self-supervised learning (SSL) models have shown promising results, they only partly address the data scarcity issue for the feature extractor. This leaves the data scarcity issue for the decoder unresolved and leading to suboptimal performance. To address this challenge, we propose a retrieval-augmented MOS prediction method, dubbed {\\bf RAMP}, to enhance the decoder's ability against the data scarcity issue. A fusing network is also proposed to dynamically adjust the retrieval scope for each instance and the fusion weights based on the predictive confidence. Experimental results show that our proposed method outperforms the existing methods in multiple scenarios.",
  "full_text": "RAMP: Retrieval-Augmented MOS Prediction via Confidence-based Dynamic\nWeighting\nHui Wang1, Shiwan Zhao†, Xiguang Zheng2,Yong Qin1,∗\n1Nankai University, China\n2Kuaishou Technology, China\nwanghui hlt@mail.nankai.edu.cn, zhaosw@gmail.com, zhengxiguang@kuaishou.com,\nqinyong@nankai.edu.cn\nAbstract\nAutomatic Mean Opinion Score (MOS) prediction is crucial to\nevaluate the perceptual quality of the synthetic speech. While\nrecent approaches using pre-trained self-supervised learning\n(SSL) models have shown promising results, they only partly\naddress the data scarcity issue for the feature extractor. This\nleaves the data scarcity issue for the decoder unresolved and\nleading to suboptimal performance. To address this challenge,\nwe propose a retrieval-augmented MOS prediction method,\ndubbed RAMP, to enhance the decoder’s ability against the data\nscarcity issue. A fusing network is also proposed to dynami-\ncally adjust the retrieval scope for each instance and the fusion\nweights based on the predictive confidence. Experimental re-\nsults show that our proposed method outperforms the existing\nmethods in multiple scenarios.\nIndex Terms: MOS prediction, speech assessment, retrieval-\naugmented method, confidence-based dynamic weighting\n1. Introduction\nEvaluation of synthetic speech typically involves the use of ob-\njective and subjective methods. Objective methods, such as Mel\nCepstral Distortion (MCD) and F0 Frame Error (FFE), require\nreference audio, making them impractical or even impossible\nto use in scenarios where reference audio is unavailable [1].\nOn the other hand, subjective methods, such as Mean Opin-\nion Score (MOS), rely on listening tests conducted by crowd-\nsourced listeners, which can be time and resource consuming.\nAutomatic MOS prediction using machine learning [2, 3, 4]\nhas recently gained popularity as it allows for quality evalua-\ntion that matches human perception with low time and resource\ncost. However, training of such methods still relies on manu-\nally scored data and often suffers from data scarcity due to lim-\nited budgets. To alleviate this problem, self-supervised learning\n(SSL) models [5, 6] trained on large-scale unlabeled data are\nemployed as the feature extractor, followed by a downstream\nMOS prediction decoder trained on small-scale data with la-\nbeled MOS scores [7, 8]. Recent works based on this SSL-based\nstructure [9, 10, 11] have outperformed the earlier works trained\nfrom scratch [2, 12, 13] in the MOS prediction tasks.\nExisting SSL-based frameworks, as depicted in Figure 1,\nmainly focus on improving the feature extractor based on the\nhelp of large-scale pre-training corpora to obtain better rep-\nresentations. To name a few, Yang et al. [11] fuse seven\npre-trained SSL models, as both the data and model config-\nurations used for pre-training can impact SSL model perfor-\nmance. Tseng et al. [9] demonstrate that domain adaptive pre-\ntraining (DAPT) can reduce domain mismatch between speech\n†Independent researcher.\n* Corresponding author.\nFigure 1: The SSL-based framework (bottom left) improves the\nfeature extractor with an SSL model and the proposed frame-\nwork (bottom right) augments the decoder by introducing a non-\nparametric model and a fusing network.\nin the pre-training corpus and fine-tuning MOS corpus. Saeki\net al. [10] concatenate as much information as possible from\nphonemes, raters, and other sources with the SSL representa-\ntion vector. Vioni et al. [14] include prosodic and linguistic\nfeatures as inputs to improve system performance.\nHowever, the decoder, which decodes the features into\nscores, is only trained with the MOS dataset. The data scarcity\nissue for the decoder remains unsolved, leading to subopti-\nmal performance. A few works attempt to improve the neu-\nral decoder. Tseng et al. [9] replace the linear layer of the\ndecoder with a DNN, but their results indicate that increasing\nthe decoder’s parameters does not necessarily improve perfor-\nmance. Chen et al. [15] design multi-task heads to predict both\nquality and intelligibility scores simultaneously. In summary,\nthese neural decoders can be classified as parametric methods\nby learning the mapping from representations to scores, which\ngenerally require large-scale labeled data to achieve good gen-\neralization ability and tend to suffer difficulties in adapting to\nnew domains with distribution shifts.\nGiven the powerful SSL-based feature extractor and the\nweak decoder, we share a similar hypothesis with kNN-LM\n[16] that the representation learning is easier than the predic-\ntion. Thus, we propose a retrieval-augmented MOS prediction\nmethod, dubbed RAMP, to enhance the decoder for address-\ning the data scarcity issue. Specifically, as shown in Figure 1,\nwe augment the decoder of the SSL-based framework by lin-\nearly interpolating its prediction score with thek-nearest neigh-\nbors (kNN) model. The kNN model, which is a non-parametric\nmethod, not only is good at memorizing rare patterns [17] but\nalso handles cross-domain issues in a flexible way [16].\nThe fusion of the parametric and non-parametric methods\nhas been successfully utilized in tasks such as language mod-\neling [16, 18] and machine translation [19]. However, they\ncombine the two methods by static weights. We argue that the\npredictive ability of parametric methods varies for different in-\narXiv:2308.16488v1  [eess.AS]  31 Aug 2023\nFigure 2: The illustration of the system flow at the inference time.\nstances due to the uneven distribution of data in the dataset. For\ninstance, the model may be more confident in predicting scores\nfor some instances than for others. In such cases, less help from\nnon-parametric methods may be needed. To this end, we pro-\npose a confidence-based dynamic weighting scheme to balance\nthe outcomes of the two methods. Furthermore, the number\nof retrievals of the kNN model is preset and sensitive to noise\n[20, 21]. To improve the robustness of the model, we automat-\nically predict the importance of the number of retrievals and\naverage the scores for different numbers.\nThe main contributions of this work are threefold:\n• We propose RAMP, a novel retrieval-augmented MOS pre-\ndiction method, to enhance the neural decoder in SSL-based\nframeworks.\n• We design a fusing network to dynamically adjust the re-\ntrieval scope for each instance and the fusion weights based\non the predictive confidence.\n• We have demonstrated the effectiveness of the method\nthrough extensive comparative and ablation experiments.\n2. The System Overview\nThe training process of our system consists of two stages. In\nthe first training stage, we fine-tune the SSL-based model (i.e.,\nthe feature extractor) with multi-task heads (i.e., the decoder).\nIn the second stage, we freeze the feature extractor and decoder\nto train the fusing network to combine results from the decoder\nand the kNN model. Note that we use the frozen feature extrac-\ntor on training data to construct the datastore.\nThe overall flow of our system at the inference time is pre-\nsented in Figure 2. During inference, the SSL model extracts\nthe feature representation for the input utterance. The features\nare fed into the parametric and non-parametric paths separately.\nThe corresponding outputs are then fused to get the final out-\nput. In the case of evaluating cross-domain audios, the steps are\nidentical, except replacing the datastore with the target domain\ndata without an additional fine-tuning stage.\n3. The Proposed Method\nThe proposed method consists of the parametric path, the non-\nparametric path, and the fusing network.\n3.1. Parametric path\nThe parametric path refers to the use of a neural network-based\ndecoder to handle representations. The decoder is designed as\na multi-task architecture consisting of a regression head and a\nclassification head, which are implemented using several linear\nlayers following SSL-MOS [7]. The purpose of introducing a\nclassification head is to guide the model during fine-tuning, as\nwell as to output the confidence of each score bin. Obtaining the\nconfidence of the parametric model is crucial for subsequent re-\nsult fusion. For the i-th instance (ui, si), we first map the score\nsi to the bin idbi. Then the model outputs a prediction scoreSp\nalong with a confidence probability distribution [c1, c2, ..., cn]\non n bins. The loss function is defined by:\nL = Lreg(ui, si) +αLcls(ui, bi), (1)\nwhere Lreg and Lcls are the MSE and cross entropy losses, for\nthe regression and classification heads, respectively. The α is a\nhyper-parameter that balances the two types of losses.\n3.2. Non-parametric path\nCompared to the parametric path, the non-parametric path di-\nrectly exploits the representations with a kNN model. The rep-\nresentation of the utterance being evaluated is used as the query\nto retrieve the most similar data instances from the datastore,\nalong with their corresponding distances and labels. This infor-\nmation is then used as part of the input to the fusing network.\nDatastore: Let f(·) be the SSL model that maps an utter-\nance u to its representation. For a training sample(ui, si) ∈ D,\nwe create a key-value pair (ki, vi), where the key ki = f(ui)\nis the representation vector of the utterance ui and the value\nvi = si is its target score. The set of all key-value pairs con-\nstructed from all training examples in D forms the datastore\n(K, V):\n(K, V) ={(f(ui), si)|(ui, si) ∈ D}. (2)\nInference: During testing, the SSL model generates a rep-\nresentation q for the input utterance, which is then utilized by\nthe kNN model to retrieve the k nearest neighbors, denoted\nby Nk, from the datastore based on a given distance function\nd(q, ·). The retrieved score Sr,k is then computed as follows:\nSr,k =\nX\n(ki,vi)∈Nk\nwikvi, (3)\nwhere the weight wik is the inverse of its distance. Thus, neigh-\nbors that are closer will exert more influence than those that are\nfarther away.\nTable 1: The performances of our framework along with two systems on BVCC and BC2019 test set.\n(a) The results of the in-domain experiments.\ndataset model U MSE↓ U LCC↑ U SRCC↑ U KTAU↑ S MSE↓ S LCC↑ S SRCC↑ S KTAU↑\nBVCC\nSSL-MOS 0.246 0.875 0.872 0.697 0.113 0.928 0.923 0.770\nDDOS 0.212 0.880 0.880 0.707 0.110 0.933 0.932 0.782\nRAMP 0.195 0.881 0.881 0.708 0.097 0.931 0.932 0.784\nBC2019\nSSL-MOS 0.253 0.901 0.871 0.690 0.098 0.980 0.970 0.871\nDDOS 0.169 0.914 0.887 0.710 0.052 0.976 0.955 0.848\nRAMP 0.188 0.916 0.891 0.717 0.053 0.987 0.987 0.926\n(b) The results of the cross-domain experiments.\ndataset model U MSE↓ U LCC↑ U SRCC↑ U KTAU↑ S MSE↓ S LCC↑ S SRCC↑ S KTAU↑\nBC2019\nSSL-MOS 3.187 0.527 0.549 0.403 2.976 0.590 0.655 0.569\nDDOS 1.331 0.678 0.694 0.502 1.119 0.766 0.797 0.637\nRAMP 0.658 0.826 0.780 0.587 0.493 0.929 0.907 0.772\nRAMP(np) 0.294 0.842 0.789 0.596 0.093 0.955 0.926 0.797\nSince kNN is sensitive to k, we improve its robustness by\nusing various values of k in the range [1, 2, . . . , K], where K\nis the hyperparameter. As a result, we obtain the retrieved score\ndistribution [Sr,1, Sr,2, . . . , Sr,K], as well as the retrieved dis-\ntance distribution [d1, d2, . . . , dK], where dk is the distance be-\ntween q and the k-th nearest neighbor.\n3.3. Fusing network\nWe introduce two lightweight networks,k-net and λ-net, to dy-\nnamically predict the probability of the retrieval scopek and the\nfusion weight distribution λ based on the predictive confidence\nfor each instance.\nk-net: The k-net is a lightweight net consisting of only two\nlinear layers which dynamically predicts the probability of each\nk for each instance. It takes as input the distance distribution\nd = [d1, d2, . . . , dK] of the retrieved neighbors and outputs the\nprobability distribution pknet over different values of k:\npknet = softmax(knet(d)). (4)\nTherefore, the final retrieved score can be computed as a\nweighted average of different Sr,k:\nSr =\nKX\nk=1\npknet(k)Sr,k. (5)\nλ-net: The λ-net has the same structure as k-net. To bal-\nance the outcomes of two paths, it takes the confidences as the\ninput, in addition to the distance distributiond. The confidences\ninclude the top- 8 values ctop from the confidence probability\ndistribution [c1, c2, ..., cn], and two confidences cSr and cSp of\nthe bins to which Sr and Sp belong. Thus, the weight and then\nthe final score can be computed as:\nw = (wp, wr) =softmax(λnet([d; ctop; cSr ; cSp ])), (6)\nS = wpSp + wrSr, (7)\nwhere, Sp and Sr are the scores of the parametric and non-\nparametric paths, respectively.\n4. Experiments\n4.1. Datasets and Metrics\nThe experiments in this paper use three corpora: BVCC [22],\nBC2019 [23], and SOMOS [24]. BVCC contains 7,106 English\nsamples from the Blizzard Challenges, the V oice Conversion\nChallenges, and published samples of ESPNet [25]. The ra-\ntio of training/development/test is 70%/15%/15%, respectively.\nBC2019 contains Mandarin TTS samples submitted to the 2019\nBlizzard Challenge [23]. There are 136 samples for training,\n136 samples for validation, and 540 samples for testing. We\nalso use SOMOS in the ablation study. It consists of 20K TTS\naudio files generated from several Tacotron-like acoustic mod-\nels [26] and an LPCNet vocoder [27].\nModel evaluation is performed at the utterance level and\nsystem level, denoted as ‘U ’ and ‘S ’, respectively. Mean\nsquare error (MSE) and various correlation coefficient met-\nrics are used. In particular, the Linear Correlation Coefficient\n(LCC), the Spearman Rank Correlation Coefficient (SRCC) and\nthe Kendall Tau Rank Correlation (KTAU) scores are evaluated.\nIn general, smaller errors and higher correlations indicate better\nmodel performance.\n4.2. Implementation Details\nWe use the published wav2vec2.0 [5] base model pre-trained on\nLibrispeech [28] as the feature extractor. In both training stages,\nthe models are trained for 1,000 epochs with a batch size of 4\nand a learning rate of 0.0001. Training will be stopped early\nwhen the loss does not decrease for 20 epochs. Gradient accu-\nmulation is used to simulate large batch. In the experiments, the\nlength of the score bin is 0.25. The α is set to 1 in equation 1.\nWe set K = 60for BVCC, K = 8for BC2019 in Table 1. The\nL2 distance is used in Section 3.2. We crop the long audio in\nthe SOMOS dataset due to memory limitations for all compared\nmethods.\n4.3. In-domain Analysis\nWe first compare the performance of our method RAMP with\nSSL-MOS [7] and DDOS [9]. SSL-MOS is one of the first sys-\ntems to employ the SSL model, which delivers high prediction\nperformance with an easy-to-use framework. DDOS is one of\nthe winning solutions of V oiceMOS Challenge 2022 [29], pro-\nviding competitive outcomes across many metrics.\nTable 1a shows the performance of three systems in BVCC\nand BC2019. On the BVCC test set, our system RAMP per-\nforms better than SSL-MOS in all metrics. Compared to DDOS,\nRAMP performs better in error metrics and similarly in correla-\ntion metrics. To evaluate the performance in BC2019, all three\nTable 2: The performance of two paths in three datasets: SOMOS (big), BVCC (medium), and BC2019 (small).\nU MSE↓ U LCC↑ U SRCC↑ U KTAU↑ S MSE↓ S LCC↑ S SRCC↑ S KTAU↑\nP NP P NP P NP P NP P NP P NP P NP P NP\nb 0.185 0.179 0.658 0.668 0.653 0.658 0.468 0.473 0.182 0.174 0.664 0.676 0.659 0.665 0.474 0.480\nm 0.200 0.201 0.874 0.880 0.876 0.880 0.702 0.708 0.106 0.101 0.917 0.934 0.915 0.934 0.756 0.787\ns 0.226 0.196 0.880 0.904 0.831 0.868 0.634 0.682 0.064 0.045 0.968 0.984 0.926 0.972 0.797 0.889\nFigure 3: The bars are the score distribution of the BVCC\ndataset, and the curve represents the weights of the paramet-\nric path learned by the fusion network for each score bin. This\nshows dynamic weights are assigned aligning with the data dis-\ntribution, indicating the effectiveness of the fusing net.\nmodels are first trained in BVCC and fine-tuned in BC2019 fol-\nlowing the same procedure of the V oiceMOS Challenge 2022 to\nmake a fair comparison. On the BC2019 test set, RAMP also\noutperforms SSL-MOS in all metrics. While DDOS outper-\nforms our system slightly in terms of error metrics, our system\nconsistently performs well across all six correlation metrics.\nMoreover, in order to investigate how the confidence-based\ndynamic weighting scheme combines the two paths for effi-\ncient prediction, we visualize the fusion weights for various in-\nstances. Figure 3 displays the score distribution of the BVCC\ndata and the weights of the parametric path predicted by the\nfusing network. The data exhibit a long-tailed distribution with\nrelatively small numbers of scores that are very low or very\nhigh. As mentioned in Section 1, this unbalanced distribution\nof data can cause the neural network to have different predic-\ntion capabilities for data located in different score intervals. For\nhead data, the neural network-based decoder (i.e., the paramet-\nric path) can predict their scores with high confidence, requiring\nless help from the non-parametric path. Thus, smaller weights\nare assigned to the non-parametric path and higher weights to\nthe parametric path. Conversely, for data at the tail, higher\nweights are assigned to the non-parametric path and smaller\nweights to the parametric path.\n4.4. Cross-domain Analysis\nThe results of cross-domain experiments are presented in Table\n1b. All systems are trained in BVCC and tested in the BC2019\ntest set without additional fine-tuning. As the results show,\nboth SSL-MOS and DDOS exhibit a sharp drop in performance\nacross domains. While our system can solve the cross-domain\nadaptation problem flexibly and excel in performance by updat-\ning the datastore without any additional training.\nTo further investigate the help from the non-parametric\npath for cross-domain settings. We create a new variant called\nRAMP(np), which only includes the non-parametric path. This\nresults in further improvements in performance, demonstrating\nthat the distribution shift undermines the mapping from repre-\nsentations to scores, while the kNN model still performs well\nby directly leveraging the representations.\n4.5. Ablation study\nWe first compare the performance of the parametric and non-\nparametric paths across different data scales. In Table 2, ‘P’ and\n‘NP’ denote the parametric and non-parametric paths, respec-\ntively. We conduct the experiments on three datasets: SOMOS,\nBVCC, and BC2019, which correspond to tens of thousands,\nthousands, and hundreds of scales, respectively. We can observe\nthat as the size of the dataset decreases, the non-parametric path\nboosts the performance more significantly. This demonstrates\nthat our proposed method is more applicable to low-resource\ntasks like most MOS prediction tasks.\nWe then conduct an ablation study on the fusing net. Vanilla\nkNN uses a fixedk for all instances and its performance is sensi-\ntive to the value of k. While our proposed fusing net specifies a\npredefined upper bound K of the retrieval range. Then for each\ninstance, it dynamically computes the weighted average score\nfrom the K kNN models, each trained with different numbers\nof nearest neighbors from 1 to K. The weight of each kNN is\nobtained through the network. We only show MSE and KTAU\nperformance on the utterance level since the trends of system-\nlevel are similar. The results show that the fusing net performs\nbetter on average and performs more consistently when chang-\ning the hyper-parameter K, demonstrating that the fusing net\ncan improve the accuracy and robustness of the model.\nTable 3: The performance of the vanillakNN and the fusing net.\nk/K-size U MSE↓ U KTAU↑\nVanilla Fusing Vanilla Fusing\n5 0.216 0.197 0.695 0.704\n10 0.203 0.199 0.703 0.704\n15 0.198 0.197 0.707 0.705\n30 0.197 0.194 0.708 0.708\n60 0.197 0.195 0.708 0.708\nmean 0.202 0.196 0.704 0.706\nvar 6.57E-05 3.80E-06 3.07E-05 4.20E-06\n5. Conclusions\nIn this paper, we propose RAMP, a novel retrieval-augmented\nMOS prediction method, to enhance the neural decoder for al-\nleviating the data scarcity issue in the SSL-based frameworks.\nWe also design a fusing network to dynamically adjust the re-\ntrieval scope and the fusion weights based on the predictive con-\nfidence. The experimental results show that the proposed mod-\nels perform well in both in-domain and cross-domain settings.\n6. Acknowledgements\nThis work has been supported by the National Key R&D Pro-\ngram of China through grant 2022ZD0116307 and NSF China\n(Grant No.62271270).\n7. References\n[1] M. Chinen, J. Skoglund, C. K. A. Reddy, A. Ragano, and\nA. Hines, “Using Rater and System Metadata to Explain Variance\nin the V oiceMOS Challenge 2022 Dataset,” in Proc. Interspeech\n2022, 2022, pp. 4531–4535.\n[2] C. Lo, S. Fu, W. Huang, X. Wang, J. Yamagishi, Y . Tsao, and\nH. Wang, “Mosnet: Deep learning-based objective assessment for\nvoice conversion,”Proc. Interspeech 2019, pp. 1541–1545, 2019.\n[3] B. Patton, Y . Agiomyrgiannakis, M. Terry, K. W. Wilson, R. A.\nSaurous, and D. Sculley, “Automos: Learning a non-intrusive\nassessor of naturalness-of-speech,” ArXiv, vol. abs/1611.09207,\n2016.\n[4] S. wei Fu, Y . Tsao, H.-T. Hwang, and H.-M. Wang, “Quality-Net:\nAn End-to-End Non-intrusive Speech Quality Assessment Model\nBased on BLSTM,” in Proc. Interspeech 2018, 2018, pp. 1873–\n1877.\n[5] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec\n2.0: A framework for self-supervised learning of speech repre-\nsentations,” Advances in neural information processing systems ,\nvol. 33, pp. 12 449–12 460, 2020.\n[6] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhut-\ndinov, and A. Mohamed, “Hubert: Self-supervised speech rep-\nresentation learning by masked prediction of hidden units,”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, vol. 29, pp. 3451–3460, 2021.\n[7] E. Cooper, W.-C. Huang, T. Toda, and J. Yamagishi, “General-\nization ability of mos prediction networks,” ICASSP 2022 - 2022\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 8442–8446, 2021.\n[8] A. Ragano, E. Benetos, M. Chinen, H. B. Martinez, C. K.\nReddy, J. Skoglund, and A. Hines, “A comparison of deep learn-\ning mos predictors for speech synthesis quality,” arXiv preprint\narXiv:2204.02249, 2022.\n[9] W.-C. Tseng, W.-T. Kao, and H. yi Lee, “DDOS: A MOS Predic-\ntion Framework utilizing Domain Adaptive Pre-training and Dis-\ntribution of Opinion Scores,” inProc. Interspeech 2022, 2022, pp.\n4541–4545.\n[10] T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and\nH. Saruwatari, “UTMOS: UTokyo-SaruLab System for V oice-\nMOS Challenge 2022,” in Proc. Interspeech 2022 , 2022, pp.\n4521–4525.\n[11] Z. Yang, W. Zhou, C. Chu, S. Li, R. Dabre, R. Rubino, and\nY . Zhao, “Fusion of Self-supervised Learned Models for MOS\nPrediction,” in Proc. Interspeech 2022, 2022, pp. 5443–5447.\n[12] Y . Leng, X. Tan, S. Zhao, F. Soong, X.-Y . Li, and T. Qin, “Mbnet:\nMos prediction for synthesized speech with mean-bias network,”\nin ICASSP 2021-2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP) . IEEE, 2021, pp.\n391–395.\n[13] W.-C. Huang, E. Cooper, J. Yamagishi, and T. Toda, “Ldnet: Uni-\nfied listener dependent modeling in mos prediction for synthetic\nspeech,” inICASSP 2022-2022 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2022,\npp. 896–900.\n[14] A. Vioni, G. Maniati, N. Ellinas, J. S. Sung, I. Hwang, A. Chala-\nmandaris, and P. Tsiakoulis, “Investigating content-aware neural\ntext-to-speech mos prediction using prosodic and linguistic fea-\ntures,” ArXiv, vol. abs/2211.00342, 2022.\n[15] Y .-W. Chen and Y . Tsao, “InQSS: a speech intelligibility and qual-\nity assessment model using a multi-task learning network,” in\nProc. Interspeech 2022, 2022, pp. 3088–3092.\n[16] U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis, “Generalization through memorization: Nearest neigh-\nbor language models,” in 8th International Conference on Learn-\ning Representations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020, 2020.\n[17] Z. Wan, Q. Liu, Z. Mao, F. Cheng, S. Kurohashi, and J. Li, “Res-\ncue implicit and long-tail cases: Nearest neighbor relation extrac-\ntion,” in Conference on Empirical Methods in Natural Language\nProcessing, 2022.\n[18] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval\naugmented language model pre-training,” inInternational confer-\nence on machine learning, 2020, pp. 3929–3938.\n[19] U. Khandelwal, A. Fan, D. Jurafsky, L. Zettlemoyer, and\nM. Lewis, “Nearest neighbor machine translation,” in Interna-\ntional Conference on Learning Representations, 2021.\n[20] X. Zheng, Z. Zhang, J. Guo, S. Huang, B. Chen, W. Luo, and\nJ. Chen, “Adaptive nearest neighbor machine translation,” inPro-\nceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 2: Short Papers), Aug.\n2021, pp. 368–374.\n[21] H. Jiang, Z. Lu, F. Meng, C. Zhou, J. Zhou, D. Huang, and J. Su,\n“Towards robust k-nearest-neighbor machine translation,” inCon-\nference on Empirical Methods in Natural Language Processing ,\n2022.\n[22] E. Cooper and J. Yamagishi, “How do V oices from Past Speech\nSynthesis Challenges Compare Today?” in Proc. 11th ISCA\nSpeech Synthesis Workshop (SSW 11), 2021, pp. 183–188.\n[23] Z. Wu, Z. Xie, and S. King, “The blizzard challenge 2019,” in\nProc. Blizzard Challenge Workshop, 2019.\n[24] G. Maniati, A. Vioni, N. Ellinas, K. Nikitaras, K. Klapsas, J. S.\nSung, G. Jho, A. Chalamandaris, and P. Tsiakoulis, “SOMOS:\nThe Samsung Open MOS Dataset for the Evaluation of Neural\nText-to-Speech Synthesis,” in Proc. Interspeech 2022, 2022, pp.\n2388–2392.\n[25] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watan-\nabe, T. Toda, K. Takeda, Y . Zhang, and X. Tan, “Espnet-tts:\nUnified, reproducible, and integratable open source end-to-end\ntext-to-speech toolkit,” 2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing, pp. 7654–7658, 2020.\n[26] Y . Wang, R. J. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio, Q. V . Le,\nY . Agiomyrgiannakis, R. A. J. Clark, and R. A. Saurous,\n“Tacotron: Towards end-to-end speech synthesis,” inInterspeech,\n2017.\n[27] J.-M. Valin and J. Skoglund, “Lpcnet: Improving neural speech\nsynthesis through linear prediction,” ICASSP 2019 - 2019 IEEE\nInternational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pp. 5891–5895, 2018.\n[28] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: An asr corpus based on public domain audio books,”\n2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 5206–5210, 2015.\n[29] W. C. Huang, E. Cooper, Y . Tsao, H.-M. Wang, T. Toda, and J. Ya-\nmagishi, “The V oiceMOS Challenge 2022,” inProc. Interspeech\n2022, 2022, pp. 4536–4540.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8060067296028137
    },
    {
      "name": "Weighting",
      "score": 0.7151213884353638
    },
    {
      "name": "Extractor",
      "score": 0.6271262168884277
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5700604319572449
    },
    {
      "name": "Scarcity",
      "score": 0.5610116720199585
    },
    {
      "name": "Machine learning",
      "score": 0.5423794984817505
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43236157298088074
    },
    {
      "name": "Scope (computer science)",
      "score": 0.41254010796546936
    },
    {
      "name": "Data mining",
      "score": 0.38974589109420776
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32365208864212036
    },
    {
      "name": "Engineering",
      "score": 0.0875641405582428
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Microeconomics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Process engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205237279",
      "name": "Nankai University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4401726859",
      "name": "Kuaishou (China)",
      "country": null
    }
  ]
}