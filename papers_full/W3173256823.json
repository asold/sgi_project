{
  "title": "Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains",
  "url": "https://openalex.org/W3173256823",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2154388925",
      "name": "Haojie Pan",
      "affiliations": [
        "Zhejiang Lab",
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2108884846",
      "name": "Chengyu Wang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2114674632",
      "name": "Minghui Qiu",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2947780596",
      "name": "Yichang Zhang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2116094297",
      "name": "Ya-liang Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097175565",
      "name": "Jun Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2604474127",
    "https://openalex.org/W2895531857",
    "https://openalex.org/W3155431623",
    "https://openalex.org/W2803023299",
    "https://openalex.org/W2604986524",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3043372854",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W4300514939",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4293659930",
    "https://openalex.org/W2913356191",
    "https://openalex.org/W3034560159",
    "https://openalex.org/W2962897020",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W3021805648",
    "https://openalex.org/W2970692876",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W2978120197",
    "https://openalex.org/W2953070460",
    "https://openalex.org/W2592691248",
    "https://openalex.org/W3104763958",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2586087033",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W2950938254",
    "https://openalex.org/W2996970889",
    "https://openalex.org/W2997336365",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2905933322",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2970105755",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2964222566",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2588860837",
    "https://openalex.org/W2995158783",
    "https://openalex.org/W2310102669",
    "https://openalex.org/W4288346515",
    "https://openalex.org/W2990735210",
    "https://openalex.org/W2964352358",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W2996834012",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2964078140",
    "https://openalex.org/W2976132230",
    "https://openalex.org/W3080308281",
    "https://openalex.org/W3107016329",
    "https://openalex.org/W2804582864",
    "https://openalex.org/W4300081896",
    "https://openalex.org/W2739879705",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2978573218",
    "https://openalex.org/W2996923239",
    "https://openalex.org/W3034342078",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W3036463250",
    "https://openalex.org/W3034608141",
    "https://openalex.org/W2472819217",
    "https://openalex.org/W2970505118",
    "https://openalex.org/W3174167721",
    "https://openalex.org/W4297813345",
    "https://openalex.org/W2743289088"
  ],
  "abstract": "Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, Jun Huang. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3026‚Äì3036\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n3026\nMeta-KD: A Meta Knowledge Distillation Framework for Language\nModel Compression across Domains\nHaojie Pan1,2‚àó, Chengyu Wang2‚àó, Minghui Qiu2‚Ä†, Yichang Zhang2, Yaliang Li2, Jun Huang2\n1 Zhejiang Lab\n2 Alibaba Group\n{haojie.phj, chengyu.wcy, minghui.qmh}@alibaba-inc.com\n{yichang.zyc, yaliang.li, huangjun.hj}@alibaba-inc.com\nAbstract\nPre-trained language models have been ap-\nplied to various NLP tasks with considerable\nperformance gains. However, the large model\nsizes, together with the long inference time,\nlimit the deployment of such models in real-\ntime applications. One line of model com-\npression approaches considers knowledge dis-\ntillation to distill large teacher models into\nsmall student models. Most of these stud-\nies focus on single-domain only, which ig-\nnores the transferable knowledge from other\ndomains. We notice that training a teacher\nwith transferable knowledge digested across\ndomains can achieve better generalization ca-\npability to help knowledge distillation. Hence\nwe propose a Meta-Knowledge Distillation\n(Meta-KD) framework to build a meta-teacher\nmodel that captures transferable knowledge\nacross domains and passes such knowledge\nto students. SpeciÔ¨Åcally, we explicitly force\nthe meta-teacher to capture transferable knowl-\nedge at both instance-level and feature-level\nfrom multiple domains, and then propose\na meta-distillation algorithm to learn single-\ndomain student models with guidance from the\nmeta-teacher. Experiments on public multi-\ndomain NLP tasks show the effectiveness and\nsuperiority of the proposed Meta-KD frame-\nwork. Further, we also demonstrate the capa-\nbility of Meta-KD in the settings where the\ntraining data is scarce.\n1 Introduction\nPre-trained Language Models (PLM) such as\nBERT (Devlin et al., 2019) and XLNet (Yang et al.,\n2019) have achieved signiÔ¨Åcant success with the\ntwo-stage ‚Äúpre-training and Ô¨Åne-tuning‚Äù process.\nDespite the performance gain achieved in various\nNLP tasks, the large number of model parameters\n‚àóH. Pan and C. Wang contributed equally to this work.\n‚Ä†M. Qiu is the corresponding author.\n(a) Learning from an in-domain teacher.\n(b) Learning from multiple teachers of varied domains.\n(c) Learning from meta-teacher with multi-domain knowledge.\nPhysicsTeacher\nPhysicsTeacherPhysicsEquationPhysicsStudentMathTeacher\nPhysicsEquationAll-purposeTeacher\nPhysicsEquationPhysicsStudent\nPhysicsStudent\nFigure 1: A motivation example of academic learning.\nA physics student may learn physics equations better\nwith a powerful all-purpose teacher.\nand the long inference time have become the bot-\ntleneck for PLMs to be deployed in real-time ap-\nplications, especially on mobile devices (Jiao et al.,\n2019; Sun et al., 2020; Iandola et al., 2020). Thus,\nthere are increasing needs for PLMs to reduce the\nmodel size and the computational overhead while\nkeeping the prediction accuracy.\nKnowledge Distillation (KD) (Hinton et al.,\n2015) is one of the promising ways to distill the\nknowledge from a large ‚Äúteacher‚Äù model to a small\n‚Äústudent‚Äù model. Recent studies show that KD can\nbe applied to compress PLMs with acceptable per-\nformance loss (Sanh et al., 2019; Sun et al., 2019b;\nJiao et al., 2019; Turc et al., 2019; Chen et al.,\n2020a). However, those methods mainly focus\non single-domain KD. Hence, student models can\nonly learn from their in-domain teachers, paying\nlittle attention to acquiring knowledge from other\ndomains. It has been shown that it is beneÔ¨Åcial\nto consider cross-domain information for KD, by\neither training a teacher using cross-domain data\nor multiple teachers from multiple domains (You\net al., 2017; Liu et al., 2019; Yang et al., 2020;\n3027\nPeng et al., 2020). Consider an academic scenario\nin Figure 1. A typical way for a physics student\nto learn physics equations is to directly learn from\nhis/her physics teacher. If we have a math teacher\nto teach him/her basic knowledge of equations, the\nstudent can obtain a better understanding of physics\nequations. This ‚Äúknowledge transfer‚Äù technique in\nKD has been proved efÔ¨Åcient only when two do-\nmains are close to each other (Hu et al., 2019). In\nreality, however, it is highly risky as teachers of\nother domains may pass non-transferable knowl-\nedge to the student model, which is irrelevant to\nthe current domain and hence harms the overall\nperformance (Tan et al., 2017; Wang et al., 2020).\nBesides, current studies Ô¨Ånd multi-task Ô¨Åne-tuning\nof BERT does not necessarily yield better perfor-\nmance across all the tasks (Sun et al., 2019a).\nTo address these issues, we leverage the idea of\nmeta-learning to capture transferable knowledge\nacross domains, as recent studies have shown that\nmeta-learning can improve the model generaliza-\ntion ability across domains (Finn et al., 2017; Javed\nand White, 2019; Yin, 2020; Ye et al., 2020). We\nfurther notice that meta-knowledge is also help-\nful for cross-domain KD. Re-consider the example\nin Figure 1. If we have an ‚Äúall-purpose teacher‚Äù\n(i.e., the meta-teacher) who has the knowledge of\nboth physics principles and mathematical equations\n(i.e., the general knowledge of the two courses), the\nstudent may learn physics equations better with the\nteacher, compared to the other two cases. Hence, it\nis necessary to train an ‚Äúall-purpose teacher‚Äù model\nfor domain-speciÔ¨Åc student models to learn.\nIn this paper, we propose the Meta-Knowledge\nDistillation (Meta-KD) framework, which facili-\nties cross-domain KD. Generally speaking, Meta-\nKD consists of two parts, meta-teacher learning\nand meta-distillation. Different from the K-way\nN-shot problems addressed in traditional meta-\nlearning (Vanschoren, 2018), we propose to train\na ‚Äúmeta-learner‚Äù as the meta-teacher, which learns\nthe transferable knowledge across domains so that\nit can Ô¨Åt new domains easily. The meta-teacher\nis jointly trained with multi-domain datasets to\nacquire the instance-level and feature-level meta-\nknowledge. For each domain, the student model\nlearns to solve the task over a domain-speciÔ¨Åc\ndataset with guidance from the meta-teacher. To\nimprove the student‚Äôs distillation ability, the meta-\ndistillation module minimizes the distillation loss\nfrom both intermediate layers, output layers, and\ntransferable knowledge, combined with domain-\nexpertise weighting techniques.\nTo verify the effectiveness of Meta-KD, we\nconduct extensive experiments on two NLP tasks\nacross multiple domains, namely natural language\ninference (Williams et al., 2018) and sentiment\nanalysis (Blitzer et al., 2007). Experimental re-\nsults show the effectiveness and superiority of the\nproposed Meta-KD framework. Moreover, we Ô¨Ånd\nour method performs well especially when i) the\nin-domain dataset is very small or ii) there is no\nin-domain dataset during the training of the meta-\nteacher. In summary, the contributions of this study\ncan be concluded as follows:\n‚Ä¢ To the best of our knowledge, this work is the\nÔ¨Årst to explore the idea of meta-teacher learn-\ning for PLM compression across domains.\n‚Ä¢ We propose the Meta-KD framework to ad-\ndress the task. In Meta-KD, the meta-teacher\ndigests transferable knowledge across do-\nmains, and selectively passes the knowledge\nto student models with different domain ex-\npertise degrees.\n‚Ä¢ We conduct extensive experiments to demon-\nstrate the superiority of Meta-KD and also\nexplore the capability of this framework in the\nsettings where the training data is scarce.\nThe rest of this paper is summarized as follows.\nSection 2 describes the related work. The detailed\ntechniques of the Meta-KD framework are pre-\nsented in Section 3. The experiments are reported\nin Section 4. Finally, we conclude our work and\ndiscuss the future work in Section 5. 1\n2 Related Work\nOur study is close to the following three lines of\nstudies, introduced below.\n2.1 Knowledge Distillation (KD)\nKD was Ô¨Årst proposed by (Hinton et al., 2015), aim-\ning to transfer knowledge from an ensemble or a\nlarge model into a smaller, distilled model. Most of\nthe KD methods focus on utilizing either the dark\nknowledge, i.e., predicted outputs (Hinton et al.,\n2015; Chen et al., 2020b; Furlanello et al., 2018;\nYou et al., 2017) or hints, i.e., the intermediate\n1The experimental code can be found in https:\n//github.com/alibaba/EasyTransfer/tree/\nmaster/scripts/metaKD.\n3028\nrepresentations (Romero et al., 2015; Yim et al.,\n2017; You et al., 2017) or the relations between\nlayers (Yim et al., 2017; Tarvainen and Valpola,\n2017) of teacher models. You et al. (2017) also\nÔ¨Ånd that multiple teacher networks together can\nprovide comprehensive guidance that is beneÔ¨Åcial\nfor training the student network. Ruder et al. (2017)\nshow that multiple expert teachers can improve the\nperformances of sentiment analysis on unseen do-\nmains. Tan et al. (2019) apply the multiple-teachers\nframework in KD to build a state-of-the-art mul-\ntilingual machine translation system. Feng et al.\n(2021) considers to build a model to automatically\naugment data for KD. Our work is one of the Ô¨Årst\nattempts to learn a meta-teacher model that digest\ntransferable knowledge from multiple domains to\nbeneÔ¨Åt KD on the target domain.\n2.2 PLM Compression\nDue to the massive number of parameters in PLMs,\nit is highly necessary to compress PLMs for ap-\nplication deployment. Previous approaches on\ncompressing PLMs such as BERT (Devlin et al.,\n2019) include KD (Hinton et al., 2015), param-\neter sharing (Ullrich et al., 2017), pruning (Han\net al., 2015) and quantization (Gong et al., 2014).\nIn this work, we mainly focus on KD for PLMs.\nIn the literature, Tang et al. (2019) distill BERT\ninto BiLSTM networks to achieve comparable re-\nsults with ELMo (Peters et al., 2018). Chen\net al. (2021) studies cross-domain KD to facilitate\ncross-domain knowledge transferring. Zhao et al.\n(2019) use dual distillation to reduce the vocabulary\nsize and the embedding size. DistillBERT (Sanh\net al., 2019) applies KD loss in the pre-training\nstage, while BERT-PKD (Sun et al., 2019b) distill\nBERT into shallow Transformers in the Ô¨Åne-tuning\nstage. TinyBERT (Jiao et al., 2019) further dis-\ntills BERT with a two-stage KD process for hidden\nattention matrices and embedding matrices. Ad-\naBERT (Chen et al., 2020a) uses neural architec-\nture search to adaptively Ô¨Ånd small architectures.\nOur work improves the prediction accuracy of com-\npressed PLMs by leveraging cross-domain knowl-\nedge, which is complementary to previous works.\n2.3 Transfer Learning and Meta-learning\nTL has been proved to improve the performance on\nthe target domain by leveraging knowledge from\nrelated source domains (Pan and Yang, 2010; Mou\net al., 2016; Liu et al., 2017; Yang et al., 2017). In\nmost NLP tasks, the ‚Äúshared-private‚Äù architecture\nis applied to learn domain-speciÔ¨Åc representations\nand domain-invariant features (Mou et al., 2016;\nLiu et al., 2017; Chen et al., 2018, 2019). Com-\npared to TL, the goal of meta-learning is to train\nmeta-learners that can adapt to a variety of different\ntasks with little training data (Vanschoren, 2018).\nA majority of meta-learning methods for include\nmetric-based (Snell et al., 2017; Pan et al., 2019),\nmodel-based (Santoro et al., 2016; Bartunov et al.,\n2020) and model-agnostic approaches (Finn et al.,\n2017, 2018; Vuorio et al., 2019). Meta-learning\ncan also be applied to KD in some computer vision\ntasks (Lopes et al., 2017; Jang et al., 2019; Liu\net al., 2020; Bai et al., 2020; Li et al., 2020). For\nexample, Lopes et al. (2017) record per-layer meta-\ndata for the teacher model to reconstruct a training\nset, and then adopts a standard training procedure\nto obtain the student model. In our work, we use\ninstance-based and feature-based meta-knowledge\nacross domains for the KD process.\n3 The Meta-KD Framework\nIn this section, we formally introduce the Meta-\nKD framework. We begin with a brief overview of\nMeta-KD. After that, the techniques are elaborated.\n3.1 An Overview of Meta-KD\nTake text classiÔ¨Åcation as an example. Assume\nthere are K training sets, corresponding to K do-\nmains. In the k-th dataset Dk = {X(i)\nk ,y(i)\nk }Nk\ni=1,\nX(i)\nk is the i-th sample 2 and y(i)\nk is the class label\nof X(i)\nk . Nk is the total number of samples in Dk.\nLet Mk be the large PLM Ô¨Åne-tuned on Dk. Given\nthe K datasets, the goal of Meta-KD is to obtain\nthe Kstudent models S1,¬∑¬∑¬∑ ,SK that are small in\nsize but has similar performance compared to the\nKlarge PLMs, i.e., M1,¬∑¬∑¬∑ ,MK.\nIn general, the Meta-KD framework can be di-\nvided into the following two stages:\n‚Ä¢ Meta-teacher Learning : Learn a meta-\nteacher Mover all domains\nK‚ãÉ\nk=1\nDk. The\nmodel digests transferable knowledge from\neach domain and has better generalization\nwhile supervising domain-speciÔ¨Åc students.\n‚Ä¢ Meta-distillation: Learn K in-domain stu-\ndents S1,¬∑¬∑¬∑ ,SK that perform well in their\n2X(i)\nk can be a sentence, a sentence pair or any other tex-\ntual units, depending on the task inputs.\n3029\nrespective domains, given only in-domain data\nDk and the meta-teacher Mas input.\nDuring the learning process of the meta-teacher,\nwe consider both instance-level and feature-level\ntransferable knowledge. Inspired by prototype-\nbased meta-learning (Snell et al., 2017; Pan et al.,\n2019), the meta-teacher model should memo-\nrize more information about prototypes. Hence,\nwe compute sample-wise prototype scores as\nthe instance-level transferable knowledge. The loss\nof the meta-teacher is deÔ¨Åned as the sum of classi-\nÔ¨Åcation loss across all Kdomains with prototype-\nbased, instance-speciÔ¨Åc weighting . Besides, it\nalso learns feature-level transferable knowledge by\nadding a domain-adversarial loss as an auxiliary\nloss. By these steps, the meta-teacher is more gen-\neralized and digests transferable knowledge before\nsupervising student models.\nFor meta-distillation, each sample is weighted\nby a domain-expertise score to address the meta-\nteacher‚Äôs capability for this sample. Thetransfer-\nable knowledge is also learned for the students from\nthe meta-teacher. The overall meta-distillation\nloss is a combination of the Mean Squared Er-\nror (MSE) loss from intermediate layers of both\nmodels (Sun et al., 2019b; Jiao et al., 2019), the\nsoft cross-entropy loss from output layers (Hinton\net al., 2015), and the transferable knowledge distil-\nlation loss, with instance-speciÔ¨Åc domain-expertise\nweighting applied.\n3.2 Meta-teacher Learning\nWe take BERT (Devlin et al., 2019) as our base\nlearner for text classiÔ¨Åcation due to its wide\npopularity. For each sample X(i)\nk , the input\nis: [CLS], tok(i)\nk,1, tok(i)\nk,2, ¬∑¬∑¬∑, [SEP], where\ntok(i)\nk,n is the n-th token in X(i)\nk . The last\nhidden outputs of this sequence is denoted as\nh[CLS],h(tok(i)\nk,1),h(tok(i)\nk,2),..,h (tok(i)\nk,N), where\nh(tok(i)\nk,j) represents the last layer embedding of\nthe j-th token in X(i)\nk , and N is the maximum se-\nquence length. For simplicity, we deÔ¨Åne h(X(i)\nk )\nas the average pooling of the token embeddings,\ni.e., h(X(i)\nk ) =‚àëN\nn=1 h(tok(i)\nk,n).\nLearning Instance-level Transferable Knowl-\nedge. To select transferable instances across do-\nmains, we compute a prototype score t(i)\nk for each\nsample X(i)\nk . Here, we treat the prototype repre-\nsentation for the m-th class of the k-th domain:\np(m)\nk = 1\n|D(m)\nk |\n‚àë\nX(i)\nk ‚ààD(m)\nk\nh(X(i)\nk ), where D(m)\nk\nis the k-th training set with the m-th class label.\nThe prototype score t(i)\nk is:\nt(i)\nk =Œ±cos(p(m)\nk ,h(X(i)\nk ))\n+ Œ∂\nK(k‚Ä≤Ã∏=k)‚àë\nk‚Ä≤=1\ncos(p(m)\nk‚Ä≤ ,h(X(i)\nk )),\nwhere cos is the cosine similarity function, Œ±is a\npre-deÔ¨Åned hyper-parameter and Œ∂ = 1‚àíŒ±\nK‚àí1. We\ncan see that the deÔ¨Ånition of the prototype score\nhere is different from previous meta-learning, as\nwe require that an instance X(i)\nk should be close\nto its class prototype representation in the embed-\nding space (i.e., p(m)\nk ), as well as the prototype rep-\nresentations in out-of-domain datasets (i.e., p(m)\nk‚Ä≤\nwith k‚Ä≤ = 1,¬∑¬∑¬∑ ,K,k ‚Ä≤ Ã∏= k). This is because\nthe meta-teacher should learn more from instances\nthat are prototypical across domains instead of in-\ndomain only. For the text classiÔ¨Åcation task, the\ncross-entropy loss of the meta-teacher is deÔ¨Åned us-\ning the cross-entropy loss with the prototype score\nas a weight assigned to each instance.\nLearning Feature-level Transferable Knowl-\nedge. Apart from the cross-entropy loss, we pro-\npose the domain-adversarial loss to increase the\nmeta-teacher‚Äôs ability for learning feature-level\ntransferable knowledge.\nFor each sampleX(i)\nk , we Ô¨Årst learn an|h(X(i)\nk )|-\ndimensional domain embedding of the true domain\nlabel d(i)\nk by mapping one-hot domain representa-\ntions to the embeddings, denoted as ED(X(i)\nk ). A\nsub-network is then constructed by:\nhd(X(i)\nk )) = tanh((h(X(i)\nk ) +ED(X(i)\nk ))W + b),\nwhere W and bare sub-network parameters. The\ndomain-adversarial loss for X(i)\nk is deÔ¨Åned as:\nLDA(X(i)\nk ) =‚àí\nK‚àë\nk=1\n1k=z(i)\nk\n¬∑log œÉ(hd(X(i)\nk )),\nwhere œÉis the K-way domain classiÔ¨Åer, and 1 is\nthe indicator function that returns 1 ifk= z(i)\nk , and\n0 otherwise. Here, z(i)\nk Ã∏= d(i)\nk is a false domain la-\nbel of X(i)\nk\n3. Hence, we deliberately maximize the\nprobability that the meta-teacher makes the wrong\n3For ease of implementation, we shufÔ¨Çe the domain labels\nof all instances in a mini-batch.\n3030\nEmbeddingEncoderEncoder......Encoder...\nEmbeddingEncoderEncoder\n......‚Ñí!\"\nTransferrableKnowledgeTransferrableKnowledge‚Ñí#!\"\nDomain 1\nDomain 2\nClass 1 SampleClass 1 CentroidClass 2 SampleClass 2 Centroid\nPrototype Score ùë°!(%)\nPredictedLabel #ùë¶!(%)\nDomain-expertise Weight ùúÜ!(%)\nInputInstance w. Label (ùë•!(%),ùë¶!(%))OutputLayer OutputLayer\nMeta-teacher ModelDomain-specific Student Model\nKD Loss\nFigure 2: An overview of meta-distillation and the neural architecture that we adopt for knowledge distillation.\npredictions of domain labels. We call hd(X(i)\nk )) as\nthe transferable knowledge for X(i)\nk , which is more\ninsensitive to domain differences.\nLet LCE(X(i)\nk ) be the normal cross-entropy loss\nof the text classiÔ¨Åcation task. The total loss of the\nmeta-teacher LMT is the combination of weighted\nLCE(X(i)\nk ) and LDA(X(i)\nk ), shown as follows:\nLMT =\n‚àë\nX(i)\nk ‚àà\nK‚ãÉ\nk=1\nDk\nt(i)\nk LCE(X(i)\nk ) +Œ≥1LDA(X(i)\nk )\n‚àëK\nk=1 |Dk|\n,\nwhere Œ≥1 is the factor to represent how the domain-\nadversarial loss contributes to the overall loss.\n3.3 Meta-distillation\nWe take BERT as our meta-teacher and use smaller\nBERT models as student models. The distillation\nframework is shown in Figure 2. In our work, we\ndistill the knowledge in the meta-teacher model\nconsidering the following Ô¨Åve elements: input em-\nbeddings, hidden states, attention matrices, output\nlogits, and transferable knowledge. The KD pro-\ncess of input embeddings, hidden states and atten-\ntion matrices follows the common practice (Sun\net al., 2019b; Jiao et al., 2019). Recall that Mand\nSk are the meta-teacher and the k-th student model.\nLet Lembd(M,Sk,X(i)\nk ), Lhidn(M,Sk,X(i)\nk ) and\nLattn(M,Sk,X(i)\nk ) be the sample-wise MSE loss\nvalues of input embeddings, hidden states and at-\ntention matrices of the two models, respectively.\nHere, Lembd(M,Sk,X(i)\nk ), Lhidn(M,Sk,X(i)\nk )\nand Lattn(M,Sk,X(i)\nk ) refer to the sum of MSE\nvalues among multiple hidden layers. We refer\ninterested readers to Jiao et al. (2019) for more\ndetails. Lpred(M,Sk,X(i)\nk ) is the cross-entropy\nloss of ‚Äúsoftened‚Äù output logits, parameterized by\nthe temperature (Hinton et al., 2015). A naive ap-\nproach to formulating the total KD loss Lkd is the\nsum of all previous loss functions, i.e.,\nLkd =\n‚àë\nX(i)\nk ‚ààDk\n(\nLembd(M,Sk,X(i)\nk )+\nLhidn(M,Sk,X(i)\nk ) +Lattn(M,Sk,X(i)\nk )+\nLpred(M,Sk,X(i)\nk )\n)\n.\nHowever, the above approach does not give spe-\ncial considerations to the transferable knowledge of\nthe meta-teacher. Let hM\nd (X(i)\nk ) and hS\nd(X(i)\nk ) be\nthe transferable knowledge of the meta-teacher and\nthe student model w.r.t. the input X(i)\nk . We further\ndeÔ¨Åne the transferable knowledge distillation loss\nLTKD(M,Sk,X(i)\nk ) as follows:\nLtkd(M,Sk,X(i)\nk ) =\n1\n|Dk|\n‚àë\nX(i)\nk ‚ààDk\nMSE\n(\nhM\nd (X(i)\nk )WM\nd ,hS\nd(X(i)\nk )\n)\nwhere WM\nd is a learnable projection matrix to\nmatch the dimension between hM\nd (X(i)\nk ) and\nhS\nd(X(i)\nk ), and MSE is the MSE loss function w.r.t.\nsingle element. In this way, we encourage student\nmodels to learn more transferable knowledge from\nthe meta-teacher.\nWe further notice that although the knowledge\nof the meta-teacher should be highly transferable,\nthere still exists the domain gap between the meta-\nteacher and domain-speciÔ¨Åc student models. In this\n3031\nwork, for each sample X(i)\nk , we deÔ¨Åne the domain\nexpertise weight Œª(i)\nk as follows:\nŒª(i)\nk = 1 +t(i)\nk\nexp(ÀÜy(i)\nk ‚àíy(i)\nk )2\n+1\n,\nwhere ÀÜy(i)\nk is the predicted result of X(i)\nk ‚Äôs class\nlabel. Here, the weight Œª(i)\nk is large when the meta-\nteacher model i) has a large prototype scoret(i)\nk and\nii) makes correct predictions on the target input, i.e.,\nÀÜy(i)\nk = y(i)\nk . We can see that the weight reÔ¨Çects how\nwell the meta-teacher can supervise the student on\na speciÔ¨Åc input. Finally, we derive the complete\nformulation of the KD loss L‚Ä≤\nkd as follows:\nL‚Ä≤\nkd =\n‚àë\nX(i)\nk ‚ààDk\nŒª(i)\nk\n(\nLembd(M,Sk,X(i)\nk )+\nLhidn(M,Sk,X(i)\nk ) +Lattn(M,Sk,X(i)\nk )+\nLpred(M,Sk,X(i)\nk )\n)\n+ Œ≥2Ltkd(M,Sk,X(i)\nk )\n)\n,\nwhere Œ≥2 is the transferable KD factor to represent\nhow the transferable knowledge distillation loss\ncontributes to the overall loss.\n4 Experiments\nIn this section, we conduct extensive experiments\nto evaluate the Meta-KD framework on two popular\ntext mining tasks across domains.\n4.1 Tasks and Datasets\nWe evaluate Meta-KD over natural language infer-\nence and sentiment analysis, using the following\ntwo datasets MNLI and Amazon Reviews. The\ndata statistics are in Table 1.\n‚Ä¢ MNLI (Williams et al., 2018) is a large-\nscale, multi-domain natural language infer-\nence dataset for predicting the entailment re-\nlation between two sentences, containing Ô¨Åve\ndomains (genres). After Ô¨Åltering samples with\nno labels available, we use the original devel-\nopment set as our test set and randomly sam-\nple 10% of the training data as a development\nset in our setting.\n‚Ä¢ Amazon Reviews (Blitzer et al., 2007) is\na multi-domain sentiment analysis dataset,\nwidely used in multi-domain text classiÔ¨Åca-\ntion tasks. The reviews are annotated as pos-\nitive or negative. For each domain, there are\n2,000 labeled reviews. We randomly split the\ndata into train, development, and test sets.\nDataset Domain #Train #Dev #Test\nMNLI\nFiction 69,613 7,735 1,973\nGov. 69,615 7,735 1,945\nSlate 69,575 7,731 1,955\nTelephone 75,013 8,335 1,966\nTravel 69,615 7,735 1,976\nBook 1,631 170 199\nAmazon DVD 1,621 194 185\nReviews Elec. 1,615 172 213\nKitchen 1,613 184 203\nTable 1: Statistics of the two datasets.\n4.2 Baselines\nFor the teacher side, to evaluate the cross-domain\ndistillation power of the meta-teacher model, we\nconsider the following models as baseline teachers:\n‚Ä¢ BERT-single: Train the BERT teacher model\non the target distillation domain only. If we\nhave Kdomains, then we will have KBERT-\nsingle teachers.\n‚Ä¢ BERT-mix: Train the BERT teacher on a\ncombination of K-domain datasets. Hence,\nwe have one BERT-mix model as the teacher\nmodel for all domains.\n‚Ä¢ BERT-mtl: Similar to the ‚Äúone-teacher‚Äù\nparadigm as in BERT-mix, but the teacher\nmodel is generated by the multi-task Ô¨Åne-\ntuning approach (Sun et al., 2019a).\n‚Ä¢ Multi-teachers: It uses K domain-speciÔ¨Åc\nBERT-single models to supervise K student\nmodels, ignoring the domain difference.\nFor the student side, we follow TinyBERT (Jiao\net al., 2019) to use smaller BERT models as our\nstudent models. In single-teacher baselines (i.e.,\nBERT-single, BERT-mix and BERT-mtl), we use\nTinyBERT-KD as our baseline KD approach. In\nmulti-teachers, because TinyBERT-KD does not\nnaturally support distilling from multiple teacher\nmodels, we implement a variant of the TinyBERT-\nKD process based on MTN-KD (You et al., 2017),\nwhich uses averaged softened outputs as the incor-\nporation of multiple teacher networks in the output\nlayer. In practice, we Ô¨Årst learn the representations\nof the student models by TinyBERT, then apply\nMTN-KD for output-layer KD.\n3032\nMethod Fiction Government Slate Telephone Travel Average\nBERTB-single 82.2 84.2 76.7 82.4 84.2 81.9\nBERTB-mix 84.8 87.2 80.5 83.8 85.5 84.4\nBERTB-mtl 83.7 87.1 80.6 83.9 85.8 84.2\nMeta-teacher 85.1 86.5 81.0 83.9 85.5 84.4\nBERTB-single\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 78.8 83.2 73.6 78.8 81.9 79.3\nBERTB-mix\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 79.6 83.3 74.8 79.0 81.5 79.6\nBERTB-mtl\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 79.7 83.1 74.2 79.3 82.0 79.7\nMulti-teachers MTN-KD‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 77.4 81.1 72.2 77.2 78.0 77.2\nMeta-teacher\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 80.3 83.0 75.1 80.2 81.6 80.0\nMeta-teacher Meta-distillation‚àí ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí ‚ÜíBERTS 80.5 83.7 75.0 80.5 82.1 80.4\nTable 2: Results over MNLI (with Ô¨Åve domains) in terms of accuracy (%). Here X\nA\n‚àí ‚ÜíY means it uses X as the\nteacher and Y as the student, with Aas the KD method, hereinafter the same.\nMethod Books DVD Electronics Kitchen Average\nBERTB-single 87.9 83.8 89.2 90.6 87.9\nBERTB-mix 89.9 85.9 90.1 92.1 89.5\nBERTB-mtl 90.5 86.5 91.1 91.1 89.8\nMeta-teacher 92.5 87.0 91.1 89.2 89.9\nBERTB-single\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 83.4 83.2 89.2 91.1 86.7\nBERTB-mix\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 88.4 81.6 89.7 89.7 87.3\nBERTB-mtl\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 90.5 81.6 88.7 90.1 87.7\nMulti-teachers MTN-KD‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 83.9 78.4 88.7 87.7 84.7\nMeta-teacher\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 89.9 84.3 87.3 91.6 88.3\nMeta-teacher Meta-distillation‚àí ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí ‚ÜíBERTS 91.5 86.5 90.1 89.7 89.4\nTable 3: Results over Amazon reviews (with four domains) in terms of accuracy (%).\n4.3 Implementation Details\nIn the implementation, we use the original BERTB\nmodel (L=12, H=768, A=12, Total Parame-\nters=110M) as the initialization of all of the teach-\ners, and use theBERTS model (L=4, H=312, A=12,\nTotal Parameters=14.5M) as the initialization of all\nthe students4.\nThe hyper-parameter settings of the meta-teacher\nmodel are as follows. We train 3-4 epochs with the\nlearning rate to be 2e-5. The batch size and Œ≥1\nare chosen from {16, 32, 48}and {0.1, 0.2, 0.5},\nrespectively. All the hyper-parameters are tuned on\nthe development sets.\n4https://github.com/huawei-noah/\nPretrained-Language-Model/tree/master/\nTinyBERT\nFor meta-distillation, we choose the hidden lay-\ners in {3, 6, 9, 12 }of the teacher models in the\nbaselines and the meta-teacher model in our ap-\nproach to learn the representations of the student\nmodels. Due to domain difference, we train stu-\ndent models in 3-10 epochs, with a learning rate\nof 5e-5. The batch size and Œ≥2 are tuned from {32,\n256}and {0.1, 0.2, 0.3, 0.4, 0.5}for intermediate-\nlayer distillation, respectively. Following Jiao et al.\n(2019), for prediction-layer distillation, we run the\nmethod for 3 epochs, with the batch size and learn-\ning rate to be 32 and 3e-5. The experiments are\nimplemented on PyTorch and run on 8 Tsela V100\nGPUs.\n3033\n4.4 Experimental Results\nTable 2 and Table 3 show the general testing perfor-\nmance over MNLI and Amazon Reviews of base-\nlines and Meta-KD, in terms of accuracy. From the\nresults, we have the following three major insights:\n‚Ä¢ Compared to all the baseline teacher models,\nusing the meta-teacher for KD consistently\nachieves the highest accuracy in both datasets.\nOur method can help to signiÔ¨Åcantly reduce\nmodel size while preserving similar perfor-\nmance, especially in Amazon review, we re-\nduce the model size to 7.5x smaller with only\na minor performance drop (from 89.9 to 89.4).\n‚Ä¢ The meta-teacher has similar performance\nas BERT-mix and BERT-mtl, but shows\nto be a better teacher for distillation,\nas Meta-teacher\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS and\nMeta-teacher Meta-distillation‚àí ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí ‚ÜíBERTS have bet-\nter performance than other methods. This\nshows the meta-teacher is capable of learn-\ning more transferable knowledge to help the\nstudent. The fact that Meta-teacher ‚ÜíMeta-\ndistillation has better performance than other\ndistillation methods conÔ¨Årms the effectiveness\nof the proposed Meta-KD method.\n‚Ä¢ Meta-KD gains more improvement on the\nsmall datasets than large ones, e.g. it improves\nfrom 86.7 to 89.4 in Amazon Reviews while\n79.3 to 80.4 in MNLI. This motivates us to\nexplore our model performance on domains\nwith few or no training samples\n4.5 Ablation Study\nWe further investigate Meta-KD‚Äôs capability with\nregards to different portion training data for both of\ntwo phases and explore how the transferable knowl-\nedge distillation loss contributes to Ô¨Ånal results.\n4.5.1 No In-domain Data during\nMeta-teacher Learning\nIn this set of experiments, we consider a special\ncase where we assume all the ‚ÄúÔ¨Åction‚Äù domain\ndata in MNLI is unavailable. Here, we train a\nmeta-teacher without the ‚ÄúÔ¨Åction‚Äù domain dataset\nand use the distillation method proposed in Jiao\net al. (2019) to produce the student model for the\n‚ÄúÔ¨Åction‚Äù domain with in-domain data during dis-\ntillation. The results are shown in Table 4. We\nÔ¨Ånd that KD from the meta-teacher can have large\nMethod Accuracy\nBERTB-s (Ô¨Åction) 82.2%\nMeta-teacher (w/o Ô¨Åction) 81.6%\nBERTB-s (Ô¨Åction)\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 78.8%\nBERTB-s (govern)\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 75.3%\nBERTB-s (telephone)\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 75.6%\nBERTB-s (slate)\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 77.1%\nBERTB-s (travel)\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 74.1%\nMeta-teacher\nTinyBERT-KD\n‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚ÜíBERTS 78.2%\nTable 4: Results under the setting where no in-domain\ndata used for meta-teacher learning on MNLI. Here,\n‚ÄúBERTB-s‚Äù refers to the ‚ÄúBERTB-single‚Äù method. The\ndistillation is performed on the ‚ÄúÔ¨Åction‚Äù domain data.\nWe report accuracy on the domain dataset.\n9.9%\n5.2%\n2.4%2.5%0.9%1.1%\nFigure 3: Improvement rate w.r.t different portion (sam-\nple rate) of training data in usage.\nimprovement, compared to KD from other out-\ndomain teachers. Additionally, learning from the\nout-domain meta-teacher has a similar performance\nto KD from the in-domain ‚ÄúÔ¨Åction‚Äù teacher model\nitself. It shows the Meta-KD framework can be\napplied in applications for emerging domains.\n4.5.2 Few In-domain Data Available during\nMeta-distillation\nWe randomly sample a part of the MNLI dataset as\nthe training data in this setting. The sample rates\nthat we choose include 0.01, 0.02, 0.05, 0.1 and\n0.2. The sampled domain datasets are employed\nfor training student models when learning from the\nin-domain teacher or the meta-teacher. The experi-\nmental results are shown in Figure 3, with results\nreported by the improvement rate in averaged ac-\ncuracy. The experimental results show that when\nless data is available, the improvement rate is much\nlarger. For example, when we only have 1% of the\noriginal MNLI training data, the accuracy can be\n3034\n0.2 0.4 0.6 0.8 1.0\nTransferable KD factor 2\n75\n80\n85\n90\n95Accuracy (%)\nBooks DVD Electronics Kitchen\nFigure 4: Model performance w.r.t. the transferable\nKD factor Œ≥2\nincreased by approximately 10% when the student\ntries to learn from the meta-teacher. It shows Meta-\nKD can be more beneÔ¨Åcial when we have fewer\nin-domain data.\n4.5.3 InÔ¨Çuence of the Transferable\nKnowledge Distillation Loss\nHere, we explore how the transferable KD fac-\ntor Œ≥2 affects the distillation performance over the\nAmazon Reviews dataset. We tune the value of Œ≥2\nfrom 0.1 to 1.0, with results are shown in Figure\n4. We Ô¨Ånd that the optimal value of Œ≥2 generally\nlies in the range of 0.2 - 0.5. The trend of accu-\nracy is different in the domain ‚ÄúDVD‚Äù is different\nfrom those of the remaining three domains. This\nmeans the beneÔ¨Åts from transferable knowledge of\nthe meta-teacher vary across domains.\n5 Conclusion and Future Work\nIn this work, we propose the Meta-KD framework\nwhich consists of meta-teacher learning and meta\ndistillation to distill PLMs across domains. Ex-\nperiments on two widely-adopted public multi-\ndomain datasets show that Meta-KD can train a\nmeta-teacher to digest knowledge across domains\nto help better teach in-domain students. Quantita-\ntive evaluations conÔ¨Årm the effectiveness of Meta-\nKD and also show the capability of Meta-KD in\nthe settings where the training data is scarce i.e.\nthere is no or few in-domain data. In the future,\nwe will examine the generalization capability of\nMeta-KD in other application scenarios and apply\nother meta-learning techniques to KD for PLMs.\nAcknowledgements\nThis work is supported by Open Research Projects\nof Zhejiang Lab (No. 2019KD0AD01/004). Any\nopinions, Ô¨Åndings, and conclusions or recommen-\ndations expressed in this material are those of the\nauthors and do not necessarily reÔ¨Çect those of the\nsponsor.\nReferences\nHaoli Bai, Jiaxiang Wu, Irwin King, and Michael R.\nLyu. 2020. Few shot network compression via cross\ndistillation. In AAAI, pages 3203‚Äì3210.\nSergey Bartunov, Jack W. Rae, Simon Osindero, and\nTimothy P. Lillicrap. 2020. Meta-learning deep\nenergy-based memory models. In ICLR.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classiÔ¨Åcation. In\nACL.\nCen Chen, Minghui Qiu, Yinfei Yang, Jun Zhou, Jun\nHuang, Xiaolong Li, and Forrest Sheng Bao. 2019.\nMulti-domain gated CNN for review helpfulness pre-\ndiction. In The World Wide Web Conference, pages\n2630‚Äì2636.\nCen Chen, Chengyu Wang, Minghui Qiu, Dehong Gao,\nLinbo Jin, and Wang Li. 2021. Cross-domain knowl-\nedge distillation for retrieval-based question answer-\ning systems. In The World Wide Web Conference.\nCen Chen, Yinfei Yang, Jun Zhou, Xiaolong Li, and\nForrest Sheng Bao. 2018. Cross-domain review\nhelpfulness prediction based on convolutional neu-\nral networks with auxiliary domain discriminators.\nIn NAACL-HLT, pages 602‚Äì607.\nDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang,\nBofang Li, Bolin Ding, Hongbo Deng, Jun Huang,\nWei Lin, and Jingren Zhou. 2020a. Adabert: Task-\nadaptive BERT compression with differentiable neu-\nral architecture search. In IJCAI, pages 2463‚Äì2469.\nDefang Chen, Jian-Ping Mei, Can Wang, Yan Feng,\nand Chun Chen. 2020b. Online knowledge distilla-\ntion with diverse peers. In AAAI, pages 3430‚Äì3437.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171‚Äì4186.\nLingyun Feng, Minghui Qiu, Yaliang Li, Hai-Tao\nZheng, and Ying Shen. 2021. Learning to augment\nfor data-scarce domain bert knowledge distillation.\nIn AAAI.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In ICML, pages 1126‚Äì1135.\nChelsea Finn, Kelvin Xu, and Sergey Levine. 2018.\nProbabilistic model-agnostic meta-learning. In\nNeurIPS, pages 9537‚Äì9548.\n3035\nTommaso Furlanello, Zachary Chase Lipton, Michael\nTschannen, Laurent Itti, and Anima Anandkumar.\n2018. Born-again neural networks. In ICML, pages\n1602‚Äì1611.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir D.\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization. CoRR.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015. Learning both weights and connections for\nefÔ¨Åcient neural network. In NeurIPS, pages 1135‚Äì\n1143.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nIn NIPS Deep Learning and Representation Learn-\ning Workshop.\nMengting Hu, Yike Wu, Shiwan Zhao, Honglei Guo,\nRenhong Cheng, and Zhong Su. 2019. Domain-\ninvariant feature distillation for cross-domain senti-\nment classiÔ¨Åcation. CoRR.\nForrest N. Iandola, Albert E. Shaw, Ravi Krishna, and\nKurt Keutzer. 2020. Squeezebert: What can com-\nputer vision teach NLP about efÔ¨Åcient neural net-\nworks? CoRR.\nYunhun Jang, Hankook Lee, Sung Ju Hwang, and\nJinwoo Shin. 2019. Metadistiller: Network self-\nboosting via meta-learned top-down distillation. In\nICML.\nKhurram Javed and Martha White. 2019. Meta-\nlearning representations for continual learning. In\nNeurIPS, pages 1818‚Äì1828.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2019. Tinybert: Distilling BERT for natural lan-\nguage understanding. CoRR.\nTianhong Li, Jianguo Li, Zhuang Liu, and Changshui\nZhang. 2020. Few sample knowledge distillation\nfor efÔ¨Åcient network compression. In CVPR, pages\n14627‚Äì14635.\nBenlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, and\nCho jui Hsieh. 2020. Metadistiller: Network self-\nboosting via meta-learned top-down distillation. In\nECCV.\nLinqing Liu, Huan Wang, Jimmy Lin, Richard Socher,\nand Caiming Xiong. 2019. Mkd: a multi-task knowl-\nedge distillation approach for pretrained language\nmodels. CoRR.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017.\nAdversarial multi-task learning for text classiÔ¨Åca-\ntion. In ACL, pages 1‚Äì10.\nRaphael Gontijo Lopes, Stefano Fenu, and Thad\nStarner. 2017. Data-free knowledge distillationfor\ndeep neural networks. CoRR.\nLili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,\nLu Zhang, and Zhi Jin. 2016. How transferable are\nneural networks in NLP applications? In EMNLP,\npages 479‚Äì489.\nSinno Jialin Pan and Qiang Yang. 2010. A survey on\ntransfer learning. IEEE Trans. Knowl. Data Eng. ,\npages 1345‚Äì1359.\nYingwei Pan, Ting Yao, Yehao Li, Yu Wang, Chong-\nWah Ngo, and Tao Mei. 2019. Transferrable pro-\ntotypical networks for unsupervised domain adapta-\ntion. In CVPR, pages 2239‚Äì2247.\nShuke Peng, Feng Ji, Zehao Lin, Shaobo Cui, Haiqing\nChen, and Yin Zhang. 2020. MTSS: learn from mul-\ntiple domain teachers and become a multi-domain\ndialogue expert. In AAAI, pages 8608‚Äì8615.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT, pages 2227‚Äì2237.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2015. Fitnets: Hints for thin deep nets. In\nICLR.\nSebastian Ruder, Parsa Ghaffari, and John G. Breslin.\n2017. Knowledge adaptation: Teaching to adapt.\nCoRR.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nBERT: smaller, faster, cheaper and lighter.CoRR.\nAdam Santoro, Sergey Bartunov, Matthew Botvinick,\nDaan Wierstra, and Timothy P. Lillicrap. 2016.\nMeta-learning with memory-augmented neural net-\nworks. In ICML, pages 1842‚Äì1850.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical networks for few-shot learning.\nIn NeurIPS, pages 4077‚Äì4087.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019a. How to Ô¨Åne-tune BERT for text classiÔ¨Åca-\ntion? In CCL, pages 194‚Äì206.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019b.\nPatient knowledge distillation for BERT model com-\npression. CoRR.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert:\na compact task-agnostic BERT for resource-limited\ndevices. In ACL, pages 2158‚Äì2170.\nBen Tan, Yu Zhang, Sinno Jialin Pan, and Qiang Yang.\n2017. Distant domain transfer learning. In AAAI,\npages 2604‚Äì2610.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-\nYan Liu. 2019. Multilingual neural machine transla-\ntion with knowledge distillation. In ICLR.\n3036\nRaphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga\nVechtomova, and Jimmy Lin. 2019. Distilling task-\nspeciÔ¨Åc knowledge from BERT into simple neural\nnetworks. CoRR.\nAntti Tarvainen and Harri Valpola. 2017. Mean teach-\ners are better role models: Weight-averaged consis-\ntency targets improve semi-supervised deep learning\nresults. In NeurIPs, pages 1195‚Äì1204.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\nCoRR.\nKaren Ullrich, Edward Meeds, and Max Welling. 2017.\nSoft weight-sharing for neural network compression.\nIn ICLR.\nJoaquin Vanschoren. 2018. Meta-learning: A survey.\nCoRR.\nRisto Vuorio, Shao-Hua Sun, Hexiang Hu, and\nJoseph J. Lim. 2019. Multimodal model-agnostic\nmeta-learning via task-aware modulation. In\nNeurIPS, pages 1‚Äì12.\nChengyu Wang, Minghui Qiu, Jun Huang, and Xi-\naofeng He. 2020. Meta Ô¨Åne-tuning neural language\nmodels for multi-domain text mining. In EMNLP,\npage 3094‚Äì3104.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL-\nHLT, pages 1112‚Äì1122.\nZe Yang, Linjun Shou, Ming Gong, Wutao Lin, and\nDaxin Jiang. 2020. Model compression with two-\nstage multi-teacher knowledge distillation for web\nquestion answering system. In WSDM, pages 690‚Äì\n698.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS, pages 5754‚Äì\n5764.\nZhilin Yang, Ruslan Salakhutdinov, and William W.\nCohen. 2017. Transfer learning for sequence tag-\nging with hierarchical recurrent networks. In ICLR.\nZhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin\nChen, Xiaoxiao Xu, Suhang Zheng, Feng Wang, Jun\nZhang, and Huajun Chen. 2020. Zero-shot text clas-\nsiÔ¨Åcation via reinforced self-training. In ACL, pages\n3014‚Äì3024.\nJunho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo\nKim. 2017. A gift from knowledge distillation:\nFast optimization, network minimization and trans-\nfer learning. In CVPR, pages 7130‚Äì7138.\nWenpeng Yin. 2020. Meta-learning for few-shot natu-\nral language processing: A survey. CoRR.\nShan You, Chang Xu, Chao Xu, and Dacheng Tao.\n2017. Learning from multiple teacher networks. In\nSIGKDD, pages 1285‚Äì1294.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019. Extreme language model compres-\nsion with optimal subwords and shared projections.\nCoRR.",
  "topic": "Zh√†ng",
  "concepts": [
    {
      "name": "Zh√†ng",
      "score": 0.6394218802452087
    },
    {
      "name": "Computer science",
      "score": 0.5901035070419312
    },
    {
      "name": "Natural language processing",
      "score": 0.5465937256813049
    },
    {
      "name": "Joint (building)",
      "score": 0.5004334449768066
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4895823895931244
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4422367811203003
    },
    {
      "name": "Macro",
      "score": 0.4340152144432068
    },
    {
      "name": "Meta-analysis",
      "score": 0.41300705075263977
    },
    {
      "name": "Linguistics",
      "score": 0.35232844948768616
    },
    {
      "name": "China",
      "score": 0.2652159035205841
    },
    {
      "name": "Engineering",
      "score": 0.15443262457847595
    },
    {
      "name": "History",
      "score": 0.13971173763275146
    },
    {
      "name": "Programming language",
      "score": 0.11927279829978943
    },
    {
      "name": "Philosophy",
      "score": 0.11204686760902405
    },
    {
      "name": "Internal medicine",
      "score": 0.11064726114273071
    },
    {
      "name": "Medicine",
      "score": 0.10833263397216797
    },
    {
      "name": "Archaeology",
      "score": 0.07870954275131226
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ]
}