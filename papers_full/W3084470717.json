{
  "title": "Critical Thinking for Language Models",
  "url": "https://openalex.org/W3084470717",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282320184",
      "name": "Betz, Gregor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2551184269",
      "name": "Voigt, Christian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2560525202",
      "name": "Richardson, Kyle",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2952984539",
    "https://openalex.org/W1509402209",
    "https://openalex.org/W2469525363",
    "https://openalex.org/W3021275949",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2987553933",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3101035550",
    "https://openalex.org/W2999635142",
    "https://openalex.org/W2150102617",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W1489439638",
    "https://openalex.org/W2588334501",
    "https://openalex.org/W2962790689",
    "https://openalex.org/W3015773279",
    "https://openalex.org/W3092597885",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2561292242",
    "https://openalex.org/W2475264043",
    "https://openalex.org/W2094855206",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W2950708443",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3091922147",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3103587923",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2752099845",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W1571008965",
    "https://openalex.org/W3006188107",
    "https://openalex.org/W2971107062"
  ],
  "abstract": "This paper takes a first step towards a critical thinking curriculum for neural auto-regressive language models. We introduce a synthetic corpus of deductively valid arguments, and generate artificial argumentative texts to train and evaluate GPT-2. Significant transfer learning effects can be observed: Training a model on three simple core schemes allows it to accurately complete conclusions of different, and more complex types of arguments, too. The language models generalize the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for NLU benchmarks. In particular, pre-training on the argument schemes raises zero-shot accuracy on the GLUE diagnostics by up to 15 percentage points. The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help language models to acquire a broad range of reasoning skills. The synthetic argumentative texts presented in this paper are a promising starting point for building such a \"critical thinking curriculum for language models.\"",
  "full_text": "Critical Thinking for Language Models\nGregor Betz† and Christian Voigt† and Kyle Richardson‡\n† Karlsruhe Institute of Technology, Karlsruhe, Germany\n{gregor.betz, christian.voigt}@kit.edu\n‡Allen Institute for AI, Seattle, W A, USA\n{kyler}@allenai.org\nAbstract\nThis paper takes a ﬁrst step towards a crit-\nical thinking curriculum for neural auto-\nregressive language models. We introduce\na synthetic corpus of deductively valid ar-\nguments, and generate artiﬁcial argumenta-\ntive texts to train and evaluate GPT-2. Sig-\nniﬁcant transfer learning effects can be ob-\nserved: Training a model on three simple\ncore schemes allows it to accurately com-\nplete conclusions of different, and more\ncomplex types of arguments, too. The\nlanguage models generalize the core argu-\nment schemes in a correct way. More-\nover, we obtain consistent and promising\nresults for NLU benchmarks. In particu-\nlar, pre-training on the argument schemes\nraises zero-shot accuracy on the GLUE di-\nagnostics by up to 15 percentage points.\nThe ﬁndings suggest that intermediary pre-\ntraining on texts that exemplify basic rea-\nsoning abilities (such as typically covered in\ncritical thinking textbooks) might help lan-\nguage models to acquire a broad range of\nreasoning skills. The synthetic argumen-\ntative texts presented in this paper are a\npromising starting point for building such\na “critical thinking curriculum for language\nmodels.”\n1 Introduction\nPre-trained autoregressive language models (LM)\nsuch as GPT-2 and GPT-3 achieve, remarkably,\ncompetitive results in a variety of language model-\ning benchmarks without task-speciﬁc ﬁne-tuning\n(Radford et al., 2019; Brown et al., 2020). Yet,\nit is also widely acknowledged that these mod-\nels struggle with reasoning tasks, such as natu-\nral language inference (NLI) or textual entailment\n(Askell, 2020). Actually, that doesn’t come as a\nsurprise, given the tendency of humans to com-\nmit errors in reasoning (Kahneman, 2011; Sun-\nstein and Hastie, 2015), their limited critical think-\ning skills (Paglieri, 2017), the resulting omnipres-\nence of fallacies and biases in texts and the fre-\nquently low argumentative quality of online de-\nbates (Hansson, 2004; Guia¸ su and Tindale, 2018;\nCheng et al., 2017). Neural language models are\nknown to pick up and reproduce normative bi-\nases (e.g., regarding gender or race) present in the\ndataset they are trained on (Gilburt, 2019), as well\nas other annotation artifacts (Gururangan et al.,\n2018); no wonder this happens with argumenta-\ntive biases and reasoning ﬂaws, too (Kassner and\nSchütze, 2020; Talmor et al., 2020). This diag-\nnosis suggests that there is an obvious remedy for\nLMs’ poor reasoning capability: make sure that\nthe training corpus contains a sufﬁcient amount of\nexemplary episodes of sound reasoning.\nIn this paper, we take a ﬁrst step towards the\ncreation of a “critical thinking curriculum” for\nneural language models. Critical thinking can be\nloosely deﬁned as “reasonable reﬂective thinking\nthat is focused on deciding what to believe or\ndo.” (Norris and Ennis, 1989) Generally speak-\ning, our study exploits an analogy between teach-\ning critical thinking to students and training lan-\nguage models so as to improve their reasoning\nskill. More speciﬁcally, we build on three key as-\nsumptions that are typically made in critical think-\ning courses and textbooks: First, there exist fun-\ndamental reasoning skills that are required for, or\nhighly conducive to, a large variety of more spe-\nciﬁc and advanced critical thinking skills (e.g.,\nFisher, 2001, p. 7). Second, drawing deductive\ninferences is one such basic ability (e.g., Fisher,\n2001, pp. 7–8). Third, reasoning skills are not\n(just) acquired by learning a theory of correct rea-\nsoning, but by studying lots of examples and doing\n“lots of good-quality exercises” (Lau and Chan,\n2020), typically moving from simple to more dif-\nﬁcult problems (e.g., Bowell and Kemp, 2014).\nThese insights from teaching critical thinking\ntranslate, with respect to our study, as follows.\narXiv:2009.07185v2  [cs.CL]  17 Dec 2020\nFirst of all, we design and build ‘lots of good-\nquality exercises’: a synthetic corpus of deduc-\ntively valid arguments which instantiate a variety\nof (syllogistic) argument schemes, and which are\nrendered as text paragraphs (Section 3). Next, we\nuse our synthetic argument text corpus to train and\nto evaluate GPT-2 (Section 4). The training, which\nmaximizes a causal language modeling objective,\ncan be conceived of as a generic, intermediary\npre-training in the spirit of STILTS (Phang et al.,\n2018).\nEvaluating the models’ ability to correctly\ncomplete conclusions of arguments, we observe\nstrong transfer learning effects/generalization\n(Section 5): Just training the models on a few\ncentral core schemes (generalized modus ponens,\ncontraposition and chain rule) allows them to ac-\ncurately complete conclusions of different types\nof arguments, too (e.g., complex argumentative\nforms that involve dilemma and de Morgan). The\nlanguage models appear to connect and generalize\nthe core argument schemes in a correct way. In ad-\ndition, the models are equally able to apply learned\nargument patterns beyond the training corpus’ do-\nmain. Tests with a simple manually authored ar-\ngument produce evidence that generic language\nmodeling skill facilitates the successful general-\nization of learned argument patterns.\nMoreover, we test the trained models on differ-\nent reasoning benchmarks. Because we are par-\nticularly interested in transfer learning effects, we\ndo so in a zero-shot set-up (i.e., evaluating our\nargumentation models on entirely unrelated NLU\ntasks, which follows recent work by Mitra et al.\n(2019); Shwartz et al. (2020); Ma et al. (2020)).\nWe obtain consistent and promising results for the\nGLUE diagnostics (Wang et al., 2018) and SNLI\n(Bowman et al., 2015) benchmarks (Section 5),\nﬁnding that training on core schemes clearly im-\nproves NLU skill. However, training on the argu-\nment corpus doesn’t affect the performance with\nregard to the semantically more demanding Ar-\ngument Reasoning Comprehension task (Haber-\nnal et al., 2018) or the critical thinking assessment\ncompiled in LogiQA (Liu et al., 2020).\nAll these transfer learning effects observed\nstrengthen the analogy between teaching critical\nthinking and training language models: A variety\nof reasoning skills are improved by generic, in-\ntermediary pre-training on high-quality texts that\nexemplify a basic reasoning skill, namely sim-\nple deductive argumentation. Obviously, drawing\ncorrect inferences is just one of the elementary\nskills typically covered in critical thinking courses\n(Fisher, 2001). Critical thinking involves more\nthan deduction. And it would hence, by analogy,\nbe unreasonable to expect that intermediary pre-\ntraining on the synthetic argument corpus sufﬁces\nto turn language models into accomplished rea-\nsoners. However, we have shown that argumen-\ntative texts (with valid syllogistic arguments) are\ncertainly a good starting point when building a\nmore comprehensive dataset for initial or interme-\ndiary pre-training that might help language models\nto acquire a broad range of reasoning skills. Or, to\nput it differently, the synthetic argumentative texts\nmight belong to the core of a “critical thinking cur-\nriculum for language models.” In the ﬁnal section,\nwe advance some ideas for complementing the ar-\ntiﬁcial argument corpus so as to further improve\nthe performance of LMs with regard to different\nreasoning benchmarks.\n2 Related Work\nTo our knowledge, this paper is, together with\nGontier et al. (2020), among the ﬁrst to show\nthat autoregressive language models like GPT-2\ncan learn to reason by training on a text corpus\nof correct natural language arguments. By con-\ntrast, previous work in this ﬁeld, described below,\nhas typically modeled natural language reasoning\nproblems as classiﬁcation tasks and trained neural\nsystems to accomplish them. For example, Schick\nand Schütze (2020a,b), using pattern verbaliza-\ntions, construct structured training data that is suit-\nable for training a masked language model with\nclassiﬁcation head, and thusly achieve remarkable\nNLU performance. This paper explores the oppo-\nsite route: We start with highly structured (syn-\nthetic) data, render it as unstructured, plain text\nand train a uni-directional language model on the\nsynthetic text corpus.\nOver and above the methodological novelty of\nour approach, we discuss, in the following, related\nreasoning benchmarks and explain what sets our\nsynthetic argument corpus apart from this work.\nRule reasoning in natural language Various\ndatasets have been developed for (deductive) rule\nreasoning in natural language. In these tasks, one\nor multiple rules, i.e. (generalized) conditionals,\nmust be applied to a fact base in order to deduc-\ntively infer a conclusion. Facts and conclusions\nare represented by atomic statements. Rule ap-\nplication closely resembles the conclusion com-\npletion task for generalized modus ponens and\ngeneralized modus tollens schemes described be-\nlow. However, we go beyond previous work in in-\nvestigating the ability of language models to in-\nfer conclusions that have a more complex logico-\nsemantic structure (e.g., existential or universal\nstatements).\nThe question answering bAbI dataset (Weston\net al., 2016) contains a task which involves apply-\ning very speciﬁc rules of the form “Xs are afraid of\nYs” to an instance (for example: “Mice are afraid\nof cats. Jerry is a mouse. What is Jerry afraid\nof? A:cats”). Equally simple, one-step rule ap-\nplications are tested in Richardson et al. (2020),\nand also contained in the QuaRTz dataset (Tafjord\net al., 2019).\nROPES (Lin et al., 2019) is a reading compre-\nhension task that involves applying background\nknowledge to a given situation (both being pre-\nsented as paragraph long text). Correct answers\ncan be inferred by one-step rule application; part\nof the challenge is to identify the relevant rule and\nfact in the text.\nRuleTaker, arguably the most general system\nfor natural rule reasoning in natural language so\nfar, is a transformer model that has been ﬁne-tuned\nto predict whether a conclusion can be inferred\nfrom a set of rules and facts, not all of which are\nnecessarily required to draw the conclusion (Clark\net al., 2020). Moreover, inferring the conclusion\nfrom the premise set might involve multiple in-\nference steps. The authors show that the trans-\nformer model can be trained to perform this task\nnearly ﬂawlessly and, moreover, to ‘explain’ its\ninferences by identifying relevant premises. They\nalso observe substantial transfer learning effects.\nPRover extends RuleTaker by a component for\nproof generation (Saha et al., 2020). Technically,\nthe QA head of the RoBERTa language model\n(Liu et al., 2019) is complemented by two ad-\nditional neural classiﬁers (for nodes and edges)\nthat are used to to construct proof chains. Saha\net al. (2020) show that PRover can construct valid\nproofs and outperforms RuleTaker in terms answer\naccuracy in a zero-shot setting.\nTraining on synthetic knowledge-graph data\n(such as \"Paris CapitalOf France\" and \"France\nHasCapital Paris\") from scratch , Kassner et al.\n(2020) ﬁnd that BERT is able to correctly infer\nnovel facts. This conﬁrms that language mod-\nels can, in principle, learn basic conceptual rules,\nwhich, e.g., express that a relation is symmetric or\nthat two terms are equivalent.\nBenchmarks for enthymematic reasoning An\n‘enthymeme’ is an argument whose premises are\nnot explicitly stated, e.g.: “Jerry is a mouse.\nTherefore, Jerry is afraid of cats.” The three tasks\ndescribed below involve such reasoning with im-\nplicit assumptions, whereas our synthetic argu-\nment corpus doesn’t: all premises are transparent\nand explicitly given.\nCommensense Transformers (COMET) are au-\ntoregressive language models for generating com-\nmonsense knowledge graphs (Bosselut et al.,\n2019). Being trained on seed data, the models are\nable to meaningfully relate subject phrases to ob-\nject phrases in terms of multiple binary relations\n(by doing the type of completion tasks we intro-\nduce in Section 4), and can thereby both repro-\nduce and extend a given knowledge graph. In par-\nticular, this includes generating statements about\ncausal relationships, which can be construed as\nenthymematic reasoning with commonsense back-\nground assumptions. For example, given the in-\nput \"PersonX is re-elected. As a result, PersonX\nwants\" the model generates as completions: \"to\nget a raise\", \"to go to ofﬁce\", \"to go home\", \"to\nmake a speech\", \"to celebrate\" – all of which\nare plausible ﬁll-ins. The implicit commonsense\npremises that underlie this (entyhmematic) infer-\nence are principles such as \"If someone has been\nre-elected, then they want to celebrate.\"\nThe Argument Reasoning Comprehension\n(ARC) dataset (Habernal et al., 2018) comprises\nsimple informal arguments. Each argument\ncontains two premises: whereas the ﬁrst premise\nis explicitly stated, there are two alternative\nformulations of the second premise. The task\nconsists in identifying which of these two alter-\nnative formulations is actually assumed in the\nargument. For example: “Miss America gives\nhonors and education scholarships. And since\n[scholarships would give women a chance to\nstudy |scholarships would take women from the\nhome], Miss America is good for women.” ARC\ntherefore assesses the ability to make implicit\npremises explicit. An adversarial ARC dataset\nthat eliminates clues in the original benchmark is\nalso available in Niven and Kao (2019).\nCLUTRR is a task generator for relational rea-\nsoning on kinship graphs (Sinha et al., 2019).\nCLUTTR takes a set of (conceptual) rules about\nfamily relations as given and constructs set-\ntheoretic possible worlds (represented as graphs)\nwhich instantiate these rules. In such a possible\n(kinship) world, a target fact and a set of base facts\nare identiﬁed such that the base facts together with\nthe rules deductively entail the target fact. The\ntask consists in inferring the target fact from the\nbase facts alone – the conceptual rules remain im-\nplicit. For example: “Kristin and her son Justin\nwent to visit her mother Carol on a nice Sunday\nafternoon. They went out for a movie together\nand had a good time. Q: How is Carol related to\nJustin? A: Carol is the grandmother of Justin.”\nSo, CLUTRR assesses entyhmematic deductive\nreasoning with implicit conceptual rules. Gon-\ntier et al. (2020) have trained a generative Trans-\nformer language model on a synthetic text corpus\n(with each argumentative text containing a story,\na proof chain and a conclusion from CLUTTR)\nand show that the language model does not only\nlearn to draw the correct conclusion (given an ar-\ngument with implicit commonsense premises), but\nalso seems to acquire the ability to generate valid\nproof chains.\nCritical thinking tasks LogiQA (Liu et al.,\n2020) is a collection of publicly available criti-\ncal thinking questions, used by the National Civil\nServants Examination of China to assess candi-\ndates’ critical thinking and problem solving skills.\nLogiQA covers tasks of various types: different\nkinds of natural language inference problems as\nwell as the identiﬁcation of implicit premises or\n(practical) instrumental reasoning. Its scope is\nmuch broader than our highly speciﬁc and care-\nfully designed argument corpus. The LogiQA\ntasks are shown to be hard for current AI systems,\nof which a ﬁne-tuned transformer model performs\nbest with an accuracy score of 35% – 50 percent-\nage points below human performance.\n3 An Artiﬁcial Argument Corpus\nThis section describes the construction of a syn-\nthetic corpus of natural language arguments used\nfor training and evaluating GPT-2.1\nThe corpus is built around eight simple, deduc-\ntively valid syllogistic argument schemes (top row\n1The corpus as well as the source code used to gen-\nerate it will be released at https://github.com/\ndebatelab/aacorpus.\nin Figure 1). These base schemes have been cho-\nsen because of their logical simplicity as well as\ntheir relevance in critical thinking and argument\nanalysis (Feldman, 2014; Bowell and Kemp, 2014;\nBrun and Betz, 2016). Each of these eight base\nschemes is manually varied in speciﬁc ways to cre-\nate further valid variants.\nNegation variants of base schemes (second row\nin Figure 1) are created by substituting a sub-\nformula with its negation and/or by applying du-\nplex negatio afﬁrmat.\nComplex predicates variants (third row in Fig-\nure 1) build on base schemes or their respective\nnegation variants and are obtained by substituting\natomic predicates with compound disjunctive or\nconjunctive ones.\nDe Morgan variants of base schemes (fourth\nrow in Figure 1) are ﬁnally derived by applying\nde Morgan’s law to the respective variants created\nbefore.\nWith 2-3 different versions for each of these\nvariations of a base scheme (parameter \"n\" in Fig-\nure 1), we obtain, all in all, 71 distinct hand-\ncrafted argument schemes. Obviously, some of\nthese schemes can be derived from others. For\nexample, generalized modus ponens and general-\nized contraposition (base schemes) entail a nega-\ntion variant of generalized modus tollens. Like-\nwise, generalized contraposition and hypothetical\nsyllogism 1 entail a ( negation variant of) hypo-\nthetical syllogism 2.\nIn view of their simplicity and prominence in\nnatural language argumentation, three of the eight\nbase schemes are marked as core schemes: gener-\nalized modus ponens, generalized contraposition,\nhypothetical syllogism 1.\nNatural language instances of the argument\nschemes can be created by means of a ﬁrst-order-\nlogic domain (with names and predicates) and nat-\nural language templates for the formal schemes. In\norder to obtain a large variety of realistic natural\nlanguage arguments, we have devised\n• a multi-stage templating process with\n• alternative templates at each stage and\n• multiple domains.\nAs shown in Figure 2, this process can be split into\nﬁve consecutive steps.\nIn step 1, the argument scheme, which serves\nas formal template for the natural language argu-\nment, is chosen.\ngeneralized \ncontraposition\nhypothetical \nsyllogism 1\nhypothetical \nsyllogism 2\nhypothetical \nsyllogism 3\ngeneralized \nmodus tollens\ndisjunctive \nsyllogism\ngeneralized \ndilemma\nbase_schemenegation_variantcomplex_predicates\n∀x Fx→Gx \nFa \n—— /uni21E9—— \nGa\n∀x Fx→¬Gx \nFa \n—— /uni21E9—— \n¬Ga\n∀x Fx∧Hx→Gx \nFa \nHa \n—— /uni21E9—— \nGa\n∀x Fx→¬Gx \n—— /uni21E9—— \n∀x Gx→¬Fx\n∀x Fx→Gx \n—— /uni21E9—— \n∀x ¬Gx →¬Fx\n∀x (Fx∧Hx)→¬Gx \n—— /uni21E9—— \n∀x Gx→¬(Fx ∧Hx)\n∀x Fx→Gx \n∀x Gx→Hx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→¬Gx \n∀x ¬Gx →Hx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx \n∀x Fx→Ix \n∀x Gx∧Ix→Hx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx \n∀x ¬Hx →¬Gx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→¬Gx \n∀x ¬Hx →Gx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→¬(Gx ∨Ix) \n∀x Hx→¬(Gx ∨Ix) \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx \n∃x Hx∧¬Gx \n—— /uni21E9—— \n∃x Hx∧¬Fx\n∀x ¬Fx →Gx \n∃x Hx∧¬Gx \n—— /uni21E9—— \n∃x Hx∧Fx\n∀x Fx→Gx \n∀x Fx→Ix \n∃x Hx∧¬(Gx ∧Ix) \n—— /uni21E9—— \n∃x Hx∧¬Fx\n∀x Fx→Gx∨Hx \n∀x Fx→¬Gx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx∨Hx \n∀x Gx→¬Fx \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx∨Hx∨Ix \n∀x Fx→¬Gx \n∀x Fx→¬Ix \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx∨Hx \n∀x Gx→Jx \n∀x Hx→Jx \n—— /uni21E9—— \n∀x Fx→Jx\n∀x Fx→Gx∨Hx \n∀x Jx→¬Gx \n∀x Jx→¬Hx \n—— /uni21E9—— \n∀x Fx→¬Jx\n∀x Fx→Gx∨Hx∨Ix \n∀x Gx→Jx \n∀x Hx→Jx \n—— /uni21E9—— \n∀x Fx→Jx∨Ix\n∀x Fx→Gx \n¬Ga \n—— /uni21E9—— \n¬Fa\n∀x Fx→¬Gx \nGa \n—— /uni21E9—— \n¬Fa\n∀x Fx→Gx∧Hx \n¬Ga \n—— /uni21E9—— \n¬Fa\nn=2 n=3 n=3 n=3 n=3 n=2 n=3 n=3\nn=3 n=2 n=3 n=3 n=3 n=2 n=3 n=3\nde_morgan\n∀x (¬Fx ∧¬Ix) →Gx \n∀x Gx → Hx \n—— /uni21E9—— \n∀x ¬(Fx ∨ Ix)→Hx\n∀x ¬(Fx ∨Hx)→Gx \n¬Fa \n¬Ha \n—— /uni21E9—— \nGa\n∀x (Fx∧Hx)→¬Gx \n—— /uni21E9—— \n∀x Gx→¬Fx ∨¬Hx\n∀x Fx→¬(Gx ∨Ix) \n∀x Hx→¬Gx ∧¬Ix \n—— /uni21E9—— \n∀x Fx→Hx\n∀x Fx→Gx \n∀x Fx→Ix \n∃x Hx∧(¬Gx ∨¬Ix) \n—— /uni21E9—— \n∃x Hx∧¬Fx\n∀x Fx∧Ix→Gx∨Hx \n∀x Gx→¬Fx ∨¬Ix \n—— /uni21E9—— \n∀x Fx∧Ix→Hx\n∀x Fx→¬(Gx ∧Hx) \n∀x ¬Gx →Jx \n∀x ¬Hx →Jx \n—— /uni21E9—— \n∀x Fx→Jx\n∀x Fx→Gx∧Hx \n¬Ga ∨¬Ha \n—— /uni21E9—— \n¬Fan=2 n=2 n=2 n=3 n=3 n=3 n=2 n=2\ngeneralized \nmodus ponens\nFigure 1: Syllogistic argument schemes used to create an artiﬁcial argument corpus.\nIn step 2, each sentence in the formal scheme\n(premises and conclusion) is individually replaced\nby a natural language pattern in accordance with a\nrandomly chosen template. For example, the for-\nmula “∀xFx →Gx” might be replaced by any of\nthe following natural language sentence schemes:\n• “Every F is a G.”\n• “Whoever is a F is also a G.”\n• “Being a G is necessary for being a F.”\n• “If someone is a F, then they are a G.”*\nSome of these patterns are not used for training,\nbut are reserved for generating an out-of-domain\ntest dataset (e.g., the template marked with an as-\nterisk in the above list).\nIn step 3, the entity- and property-placeholders\nin the resulting argument scheme are replaced\nargument-wise with names and predicates from a\ndomain. We hence obtain an instance of the for-\nmal argument scheme as premise-conclusion list.\nEach domain provides hundreds of entity-names,\nwhich can be paired with different binary predi-\ncates to create thousands of different unary predi-\ncates. The following example predicates illustrate\nthe domains used in this study:\n• Female Relatives: sister of Anna, grand-\ndaughter of Elsa, cousin of Sarah, . . .\n• Male Relatives: grandson of Ryan, nephew\nof Jim, cousin of Lee, . . .\n• Football Fans: supporter of Real Madrid CF,\nex-fan of Sevilla FC, member of SSC Napoli,\n. . .\n• Personal Care: regular consumer of Dove\nshampoo, infrequent user of L’Oreal sham-\npoo, loyal buyer of Redken shampoo, . . .\n• Chemical Ingredients: ingredient of Maypole\nSoap, ingredient of OASIS CREAM, ingredi-\nent of BB concealer, . . .\n• Dinosaurs*: contemporary of Megalosaurus,\npredator of Iguanodon, ancestor of Al-\nlosaurus, . . .\n• Philosophers*: teacher of Aeschines of\nNeapolis, pupil of Cratylus, reader of Dem-\nocritus, . . .\nDomains marked with an asterisk are used for test-\ning only, and not for training (see below and Sec-\ntion 4.2).\nIn step 4, the premises of the natural language\nargument are randomly re-ordered.\nIn step 5, the premise-conclusion list is packed\ninto a text paragraph by adding an argument in-\ntro, framing the premises, and adding an inference\nindicator. Again, multiple templates are available\nfor doing so, which yields a large variety of textual\nStep 1: choose \nformal argument \nscheme\n artiﬁcial argument corpus conﬁg ﬁle\ntopic-neutral \nformal argument \nschemes\ntopic-neutral \nNL templates for \nformal sentence \nschemes\nStep 2: choose & \nsubstitute NL \nschemes \nsentence-wise\nStep 3: construct \n& substitute \ndomain-speciﬁc \npredicates and \nnames\ndomain-speciﬁc \nNL names and \nbinary predicates\nStep 5: construct \n& apply \nargument \ntemplate\nargument-, \npremise-, and \ninference-\nindicators \nStep 4: \npermutate \npremises \nrandomly\n∀x Fx→¬Gx \nGa \n—— /uni21E9—— \n¬Fa\nNo F is a G. \na is a G. \n—— /uni21E9—— \nIt is false that a is a F.\n1. No sister of Lisa is a friend of \nChloe. \n2. Susan is a friend of Chloe. \n—— /uni21E9—— \n3. It is false that Susan is a sister \nof Lisa.\n1. Susan is a friend of Chloe. \n2. No sister of Lisa is a friend of \nChloe. \n—— /uni21E9—— \n3. It is false that Susan is a sister \nof Lisa.\nHere comes a perfectly valid \nargument: To begin with, Susan is \na friend of Chloe. Moreover, no \nsister of Lisa is a friend of Chloe. \nIn consequence, it is false that \nSusan is a sister of Lisa.\nFigure 2: Pipeline for creating natural language instances of argument schemes with multiple templating.\nrenderings of an argument.\nFollowing this pipeline, we generate natu-\nral language instances of each formal argument\nscheme, thus creating:\n1. a training set of argumentative texts, based on\nthe default domains and templates (TRAIN);\n2. an evaluation set of argumentative texts,\nbased on the default domains and templates,\nwhich are used for development (DEV);\n3. a test set of argumentative texts, based on the\ndefault domains and templates and used for\nﬁnal tests (TEST_OUT-OF-SAMPLE);\n4. a test set of argumentative texts, based on the\ndomains and templates reserved for testing\n(TEST_OUT-OF-DOMAIN).\nThis represents the artiﬁcial argument text cor-\npus we use to train and evaluate GPT-2.\n4 Experiments with GPT-2\nWe train and evaluate three compact versions of\nGPT-2 with 117M, 345M and 762M parameters\nrespectively using the implementation from Wolf\net al. (2019). We note that all of these models fall\nshort of the full-scale model with 1542M parame-\nters.2\n2The ﬁne-tuned models will be released throughhttps:\n//huggingface.co/models.\n4.1 Training\nFrom the training items in the Artiﬁcial Argu-\nment Corpus ( TRAIN) we sample three types of\ndifferently-sized training sets as follows (see also\nthe color pattern in Figure 1):\n• TRAIN01: all training items which are in-\nstances of a core scheme , i.e. generalized\nmodus ponens, generalized contraposition,\nhypothetical syllogism 1 (N=4.5K, 9K, 18K,\n36K)\n• TRAIN02: all training items which are in-\nstances of a base scheme (N=4.5K, 9K, 18K,\n36K)\n• TRAIN03: all training items in the corpus\n(N=4.5K, 9K, 18K, 36K)\nIn an attempt to avoid over-ﬁtting, we blend\nthe training arguments with snippets from Reuters\nnews stories (Lewis et al., 2004) and the standard-\nized Project Gutenberg Corpus (Gerlach and Font-\nClos, 2018), trying a mixing ratio of 1:1 and thus\ndoubling training size to N=9K, 18K, 36K, 72K.\n(We ﬁnd that ﬁne-tuning on the accordingly en-\nhanced argument corpus still increases the model’s\nperplexity on the Wiki103 dataset by a factor of\n1.5 (see Appendix B), which suggests to mix a\nhigher proportion of common texts into the train-\ning data in future work.) The three different ver-\nsions of GPT-2 are ﬁne-tuned (causal language\nmodeling objective, using default training scripts\nby Wolf et al. (2019)) on each of the 12 enhanced\ntraining sets (hyper-parameters are detailed in Ap-\npendix A). This gives us 36 ﬁne-tuned model ver-\nsions plus the three BASE models to evaluate. Un-\nless explicitly stated otherwise, we report results\nof 762M parameter model trained on 72K items.\n4.2 Testing\nConclusion Completion on Artiﬁcial Argument\nCorpus To test whether language models can\nreason correctly, we assess their ability to accu-\nrately complete conclusions of arguments in the\nartiﬁcial argument corpus. Here, we make use of\nthe fact that, by construction, the conclusion of ev-\nery argument in the corpus ends with a predicate\n(a property-term such as “sister of Chloe” or “sup-\nporter of Tottenham Hotspurs”), which is poten-\ntially preceded by a negator. First of all, as shown\nin Table 1, we test whether the model is able to\ncorrectly ﬁll in the ﬁnal predicate (task split). The\nsecond, more difﬁcult task consists in completing\nthe ﬁnal predicate plus, if present, the preceding\nnegator (task extended). With a third, adverserial\ntask we check how frequently the model wrongly\nadjoins the complement of the correct completion\nof the extended task (task inverted). Consider, for\nexample, the following argument:\nIt is not always easy to see who is re-\nlated to whom – and in which ways.\nThe following argument pertains to\nthis question: First premise: Every\nworkmate of Brad is a classmate of\nJames. Second premise: Every class-\nmate of James is not a classmate of\nTheodore. So, necessarily, everyone\nwho is a workmate of Brad is [not a ]E\n[classmate of Theodore.]S”\nIn the split task, we prompt the model with the\nargument, dropping []S, and check whether it gen-\nerates “classmate of Theodore”. In the extended\ntask, we prompt the model with the argument,\ndropping []E[]S, and check whether it generates\n“not a classmate of Theodore”. Finally, in the\ninverted task, we prompt the model as before\nand check whether it generates “a classmate of\nTheodore”.\nClearly, the higher the accuracy in the split and\nextended tasks, and the lower the accuracy in the\nTask Conclusion with\ncloze-style prompt\nComple-\ntion\nsplit Every F is a G G\nSome F is not a G G\na is a F or not a G G\nextended Every F is a G a G\nSome F is not a G not a G\na is a F or not a G not a G\ninverted Every F is a G not a G\nSome F is not a G not a G\na is a F or not a G not a G\nTable 1: Three conclusion completion tasks\ninverted task, the stronger the model’s reasoning\nperformance.\nBased on the artiﬁcial argument corpus (see\nSection 3), we generate and distinguish three dif-\nferent test datasets, each of which comprises the\nthree tasks described above, as follows:\n• out of sample : contains items from\nTEST_OUT-OF-SAMPLE, which share\ndomain and natural language templates with\nthe training data;\n• paraphrased: a sample of 100 items, ran-\ndomly drawn from TEST_OUT-OF-SAMPLE,\nwhich have been manually reformulated so as\nto alter the premises’ grammatical structure\nimposed by the natural language templates;\n• out of domain : contains items from\nTEST_OUT-OF-DOMAIN, which belong\nto different domains instantiate grammatical\npatterns other than the training data.\nTechnically, conclusion completions, in all\ntasks and tests, are generated by the language\nmodel with top-p nucleus sampling (p = 0.9).\nClassiﬁcation for NLU Benchmarks To inves-\ntigate transfer learning effects, we evaluate the\ntrained models on standard NLU benchmarks,\nsuch as GLUE AX and SNLI. These benchmark\ntasks are classiﬁcation problems. In the following,\nwe describe how we use the generative language\nmodels to perform such classiﬁcation.\nUsing simple templates, we translate each\nbenchmark entry into alternative prompts (e.g.,\ncontext and question) and/or alternative comple-\ntions (e.g., answers). Consider for example a\nGLUE-style problem given by two sentences “The\ngirl is eating a pizza.” and “The girl is eating food”\nand the question whether one entails, contradicts,\nor is independent of the other. We can construct\nthree prompts, corresponding to the three possible\nanswers (entail / contradict / independent):\nPrompt1: The girl is eating a pizza.\nTherefore,\nPrompt2: The girl is eating a pizza. This\nrules out that\nPrompt3: The girl is eating a pizza. This\nneither entails nor rules out that\nCompletion: the girl is eating food.\nIn this case, the correct match is obviously\nPrompt1–Completion. The ability of a language\nmodel to discern that “The girl is eating pizza” en-\ntails (and does not contradict) “The girl is eating\nfood” will be reﬂected in a comparatively low con-\nditional perplexity of Completion given Prompt1\nand a correspondingly high conditional perplexity\nof Completion given Prompt2 or Prompt3.\nLet us describe this procedure in more gen-\neral terms and consider a textual classiﬁcation\nproblem with categories k = 1 . . . N. To clas-\nsify a given input X, one constructs n alternative\nprompts p1, . . . pn and m alternative completions\nc1, . . . , cm (N = m·n), such that each pair(pi, cj)\ncorresponds to a class k of the classiﬁcation prob-\nlem, i.e.,\nL : (pi, cj) ↦→{1 . . . N}.\nIn the above pizza example, we have N = n =\n3 and m = 1 . Moreover, let PPL(c|p) refer\nto the conditional perplexity of the completion c\ngiven prompt p according to the language model\nL. Rather than directly using this conditional per-\nplexity as a prediction score (as for instance in\nShwartz et al., 2020), which doesn’t account for\nvarying ‘prima facie’ or ‘prior’ perplexities of al-\nternative completions, we consider the degree to\nwhich prompting the model L with p changes the\nthe perplexity of c, i.e.\nrelPPL(c, p) :=PPL(c|p)\nPPL(c) .\nIn analogy to Bayesian conﬁrmation theory, this\nmight be termed a (perplexity-based) relevance\nmeasure, as opposed to a measure of absolute con-\nﬁrmation (cf. Carnap, 1950, pp. 346-48). We now\nuse relevance perplexity as a score function to pre-\ndict the category of X:\ncategory(X) =L\n(\nargmin\n(pi,cj )\n(relPP(cj, pi))\n)\n.\n5 Results\nConclusion Completion on Artiﬁcial Argument\nCorpus Does the (ﬁne-tuned) GPT-2 model cor-\nrectly complete conclusions of natural language\narguments? Figure 3 displays the evaluation re-\nsults in an aggregated way. Each subplot visual-\nizes the accuracy of the models in the three com-\npletion tasks for a different test dataset (see Sec-\ntion 4.2), comparing the BASE model (points at\nthe very left) with the ﬁne-tuned models trained\non TRAIN01, TRAIN02, and TRAIN03 (in this or-\nder from left to right). The task-speciﬁc accuracy\nvalues are distinguished by line color.\nWe may observe, ﬁrst of all, that training on the\nargument corpus effectively improves conclusion-\ncompletion-skill. In all three test datasets, the ac-\ncuracy in the split and extended tasks increases as\nthe models are trained on more and more argu-\nment schemes, far exceeding the base model’s per-\nformance. Once the model has seen all schemes\n(TRAIN03), accuracy levels reach 100% for in-\ndomain and 70%-90% for out-of-domain tests.\nHowever, the TRAIN01 and TRAIN02 models do\nalso generate more incorrect completions than the\nBASE model ( inverted task). But the frequency\nof such incorrect completions increases much less\nthan the frequency of correct ones (the gap be-\ntween blue and gray curve widens), and it actu-\nally falls back to almost zero with the TRAIN03\nmodel. Out-of-domain performance of the models\n(right-hand plot) is qualitatively similar and only\nslightly less strong than in-domain performance\n(left-hand and middle plot). The models trained\non arguments from a given domain are able to ef-\nfectively exercise the reasoning skill thus acquired\nin other domains, and have hence gained topic-\nneutral, universal reasoning ability.\nThe strong performance of TRAIN01 models,\naveraged over all schemes, suggests that signiﬁ-\ncant transfer learning occurs and that training on\na few argument schemes positively affects perfor-\nmance on other schemes, too. To further investi-\ngate this issue, Table 2 contrasts (a) the models’\naccuracy on schemes they have not been trained\non – averaged over TRAIN01 and TRAIN02 mod-\nbase train01 train02 train03\nmodel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\ntest = out of sample\nbase train01 train02 train03\nmodel\ntest = paraphrased\nbase train01 train02 train03\nmodel\ntest = out of domain\ntask\nsplit\nextended\ninverted\nFigure 3: Accuracy of four model versions in three conclusion completion tasks and on different test datasets (out\nof sample, paraphrased, out of domain).\nBASE (a) schemes not in training data (TR01–02) (b) trained on schemes ( TR01–03)\nTask o-o-sample paraphr. o-o-domain o-o-sample paraphr. o-o-domain\nsplit 21.4 85.4 82.0 69.4 99.9 99.2 89.0\nextended 10.7 60.3 59.3 45.8 99.9 99.2 76.2\ninverted 1.5 16.9 18.0 22.1 0.0 0.0 3.2\nTable 2: Accuracy of models in three conclusion completion tasks and on different test datasets (out of sample,\nparaphrased, out of domain). Columns report, separately, the performance (a) on schemes the model has not been\ntrained on, and (b) on schemes that are covered by the model’s training data.\nels – with (b) their accuracy on schemes that are\ninstantiated in their respective training corpus –\naveraged over TRAIN01, TRAIN02, and TRAIN023\nmodels. The upshot is that trained models per-\nform way more strongly than the base model not\nonly on argument schemes they’ve been trained,\nbut also on those schemes they haven’t seen yet.\nWe take this to be a promising result as it strength-\nens the analogy between teaching critical think-\ning and training language models: generic inter-\nmediary pre-training on high-quality texts that ex-\nemplify a speciﬁc, basic reasoning skill – namely,\nsimple deductive argumentation – improves other,\nmore complex reasoning skills.\nFigure 4 gives further insights by differentiating\nevaluation results according to argument type. Its\nsubplots are arranged in a grid that mirrors the or-\nganisation of argument schemes in Figure 1. Each\nsubplot visualizes the ability of the models to cor-\nrectly complete arguments of the corresponding\nscheme (given the out-of-sample test dataset). Ac-\ncordingly, the left-hand plot in Figure 3 in effect\naverages all curves in Figure 4. Reported accu-\nracy values that fall within gray background areas\nare attained by models which have seen the cor-\nresponding scheme during training. Vice versa,\nthick lines on white background visualize model\nperformance on unknown schemes. Figure 4 re-\nveals, ﬁrst of all, that even the BASE models (only\npre-training, no ﬁne-tuning) display a signiﬁcant\nability to correctly complete conclusions of some\nkinds of arguments. For example, GPT-2-762M\nachieves 50% accuracy ( split task) in completing\ncontrapositions, 30% accuracy in completing gen-\neralized modus ponens, and still 20% accuracy in\ncompleting disjunctive syllogism and dilemma ar-\nguments. These ﬁndings further corroborate the\nhypothesis that NLMs learn (basic) linguistic and\nreasoning skills “on the ﬂy” by training on a large\ngeneric corpus (Radford et al., 2019).\nIn addition, the matrix plot (Figure 4) demon-\nstrates that some types of arguments are much\neasier to master, given training on the core and\npossibly base schemes, than others. For in-\nstance, complex_predicates variants of general-\nized modus ponens or de_morgan variants of gen-\neralized modus tollens seem to be easily mas-\ntered by the TRAIN01 model. In contrast, even\nthe TRAIN02 model, which has been ﬁne-tuned on\nall eight base schemes, struggles with the nega-\ntion_variants of generalized modus ponens (gen-\nerating substantially more incorrect than correct\ncompletions). All in all, the picture that emerges\nis plausible: Generalization towards novel types\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0base_scheme\nGeneralized\nmodus ponens\nGeneralized\nContraposition\nHypothetical\nSyllogism 1\nHypothetical\nSyllogism 2\nHypothetical\nSyllogism 3\nGeneralized\nmodus tollens\nDisjunctive\nSyllogism\nGeneralized\nDilemma\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0negation_variant\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0complex_predicates\nBASE TR01 TR02 TR03\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0de_morgan\nBASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\n BASE TR01 TR02 TR03\ntask: split task: extended task: inverted not trained on scheme trained on scheme\nFigure 4: Accuracy of conclusion completions (three tasks) for instances of different argument schemes (see\nFigure 1) and four model versions.\nof argument appears to be comparatively difﬁ-\ncult whenever the new scheme involves negations\n(compare 2nd and 4th row in Figure 4 with 3rd\nrow). This is consistent with the ﬁnding that some\nNLMs seemingly fail to understand simple nega-\ntion (Kassner and Schütze, 2020; Talmor et al.,\n2020).\nThe results reported so far suggest that reason-\ning skills acquired on (a subset of) the artiﬁcial\nargument corpus generalize rather well – both to\nother domains and other types of arguments. We\nhave further cross-checked these statistical ﬁnd-\nings by letting the models complete a conclusion\nof a simple manually authored argument:\n[Hermes] Every philosopher is mortal.\nHermes is not mortal. Therefore, Her-\nmes . . .\nThis text differs syntactically and semantically\nfrom any argument possibly contained in the arti-\nﬁcial argument corpus (where predicates have al-\nways the form “is/being a Y of X,” and no domain\ncovers philosophers or mortality). Obviously, it\nfollows that Hermes “is not a philosopher.” The\nargument instantiates generalized modus tollens ,\nwhich is not a core scheme in TRAIN01. Can\nTRAIN01-models nonetheless ﬁll out the unﬁn-\nished argument in a sensible way?\nTable 3 counts and compares the most frequent\n762M 117M\nCompletion TR01 BASE TR01\n. . . is not a philosopher. ⋆ 100 2 2\n. . . is immortal. = 0 12 0\n. . . is not a critic. ◦ 0 0 9\n. . . is mortal. † 0 8 0\n. . . is not mortal. = 0 6 0\n. . . is not Hermes. † 0 2 0\n. . . does not exist. ◦ 0 2 0\n. . . is not God. ◦ 0 2 0\n. . . is not a friend of Eckhardt.◦ 0 0 1\n. . . is not an expert of BSI Ar-\nsenal FC.\n◦ 0 0 1\n. . . is not a friend of Atalanta. ◦ 0 0 1\n. . . is not an infrequent user of\nNeutrogena shampoo.\n◦ 0 0 1\nothers 0 66 85\nTable 3: Absolute frequency of predicted completions\nfor the hand-written [Hermes] query by three different\nmodels. Completions are – relative to the premises –\nentailed (⋆), redundant ( =), contradictory ( †) or inde-\npendent (◦).\ncompletions generated by two TRAIN01 models\n(762M and 117M) and by the large untrained\nBASE model (762M). Exclusively the 762M-\nmodel trained on the core schemes reliably pre-\ndicts the correct conclusion. The large BASE\nmodel rather repeats a premise or even generate a\ncontradiction, whereas the small TRAIN01 model\n(117M) changes the topic. This is consistent with\nand illustrates our previous ﬁndings. Remarkably,\nalthough both the small and the large TRAIN01\nmodels have been ﬁne-tuned on precisely the same\narguments, only the large model seems to correctly\nrecognize the logical structure of the [Hermes] ar-\ngument. Generic language modeling skill, it is\nsuggested, facilitates the successful generalization\nof learned argument patterns beyond the templates\nused to create the synthetic training data.\nTo further understand transfer learning ef-\nfects, we next examine whether intermediary pre-\ntraining on the artiﬁcial argument corpus improves\nzero-shot performance in other NLP reasoning\ntasks (i.e., without task-speciﬁc ﬁne-tuning).\nGLUE AX The GLUE datasets (Wang et al.,\n2018) represent standard benchmarks for natural\nlanguage understanding (NLU). We evaluate our\nmodels’ NLU skill in terms of accuracy on the cu-\nrated GLUE diagnostics dataset (Figure 5).\nTraining on the artiﬁcial argument corpus sub-\nstantially boosts accuracy on the GLUE diagnos-\ntics. Accuracy increases by at least 5 and up to 17\npercentage points, depending on model size. Re-\nmarkably, training on the core scheme alone suf-\nﬁces to bring about these improvements.\nThis is a major ﬁnding and our clearest evidence\nso far that training on the AAC involves substantial\ntransfer learning effects.\nSNLI The SNLI dataset (Bowman et al., 2015)\nis another standard benchmark for NLI. Like the\nGLUE dataset, it consists in pairs of sentences\nwhich entail, contradict, or don’t bear on each\nother. The assessment of our models with re-\nspect to SNLI data proceeds in close analogy to\nthe GLUE benchmark.\nThe results, reported in Figure 5, are consistent\nwith, albeit less deﬁnite than our previous ﬁnd-\nings for the GLUE benchmark: First and foremost,\nﬁne-tuning on all schemes ( TRAIN03) improves\nthe performance by up to 8 percentage points.\nTraining on fewer schemes is slightly less effec-\ntive. However, it is only the small and medium\nsized model that proﬁt from ﬁne-tuning on the\nAAC; the SNLI performance of the 762M param-\neter model gets rather deteriorated. This might be\ndue to a coincidentally strong performance of the\ncorresponding BASE model (see Figure 7), or sug-\ngest that the large model, unlike the smaller ones,\nhas already learned during pre-training whatever is\nof relevance for SNLI in the AAC. (Further exper-\niments, preferably involving more model versions,\nare required to clarify this.)\nArgument Reasoning Comprehension Task\nThe Argument Reasoning Comprehension (ARC)\ntask (Habernal et al., 2018) assesses the ability to\nidentify a missing premise in an informally recon-\nstructed and not necessarily deductively valid ar-\ngument. It is a multiple-choice task where two al-\nternative sentences are provided, one of which is\nthe missing premise.\nWe design and apply speciﬁc templates to con-\nstruct prompts and completions, and calculate rel-\native perplexity as described in Section 4.2.\nAs shown in Figure 5, we ﬁnd no evidence of\ntransfer learning effects with respect to ARC.\nLogiQA LogiQA (Liu et al., 2020) is a col-\nlection of nearly 9,000 multiple-choice questions\n(four alternative answers each) used in critical\nthinking assessments. These questions span the\nwhole range of critical thinking tasks.\nWe design and apply speciﬁc templates to con-\nstruct prompts and completions (one prompt and\nfour completions per question), and use perplexity\nscores to predict classiﬁcations as described above\n(Section 4.2).\nAs can be seen from Figure 5, training on the\nartiﬁcial argument corpus has no effect whatsoever\non the ability of the models to handle the critical\nthinking tasks collected in LogiQA.\n6 Conclusion\nThis paper has taken a ﬁrst step towards the cre-\nation of a critical thinking curriculum for neural\nlanguage models. It presents a corpus of deduc-\ntively valid, artiﬁcial arguments, and uses this ar-\ntiﬁcial argument corpus to train and evaluate GPT-\n2. The observation of strong transfer learning ef-\nfects/generalization is its main ﬁnding: Training a\nmodel on a few central core schemes allows it to\naccurately complete conclusions of different types\nof arguments, too. The language models seem\nto connect and to generalize the core argument\ntrain01 train02 train03\nmodel\n15\n10\n5\n0\n5\n10\n15\n20\ngain in accuracy (rel. to base)\nGLUE AX\nmodel_size\n117M\n345M\n762M\ntrain01 train02 train03\nmodel\n15\n10\n5\n0\n5\n10\n15\n20\ngain in accuracy (rel. to base)\nSNLI\nmodel_size\n117M\n345M\n762M\ntrain01 train02 train03\nmodel\n15\n10\n5\n0\n5\n10\n15\n20\ngain in accuracy (rel. to base)\nARC Task\nmodel_size\n117M\n345M\n762M\ntrain01 train02 train03\nmodel\n15\n10\n5\n0\n5\n10\n15\n20\ngain in accuracy (rel. to base)\nLogiQA\nmodel_size\n117M\n345M\n762M\nFigure 5: Gains in accuracy due to ﬁne-tuning on the AAC (accuracy TRAIN model – accuracy BASE model) for\ndifferently sized models and different NLP benchmark tasks: the GLUE diagnostics data, the SNLI dataset, the\nargument reasoning comprehension (ARC) benchmark, and the LogiQA dataset.\nschemes in a correct way. Moreover, the models\nare equally able to apply learned argument pat-\nterns beyond the domain they have been trained\non, and there is evidence that generic language\nmodeling skill facilitates the successful general-\nization of learned argument patterns. These ﬁnd-\nings are consistent with previous work on rule rea-\nsoning (Clark et al., 2020). They suggest that there\nexist (learning-wise) fundamental reasoning skills\nin the sense that generic intermediary pre-training\non texts which exemplify these skills leads to spill-\nover effects and can improve performance on a\nbroad variety of reasoning tasks. The synthetic ar-\ngumentative texts might be a good starting point\nfor building such a “critical thinking curriculum\nfor language models.”\nMoreover, the trained models have been tested\non different reasoning benchmarks. We obtain\nclear and promising results for the GLUE and\nSNLI benchmarks. But training on the argument\ncorpus doesn’t affect the performance with re-\ngard to the semantically more demanding Argu-\nment Reasoning Comprehension task or the criti-\ncal thinking assessment compiled in LogiQA.\nOur work suggests different directions for ad-\nvancing the approach adopted in this paper and\nfurther improving the general reasoning skill of\nneural language models:\n• The syllogistic argument text corpus might\nbe complemented with corpora of argu-\nments that instantiate different kinds of cor-\nrect schemes , e.g., propositional inference\nschemes, modal schemes, argument schemes\nfor practical reasoning, complex argument\nschemes with intermediary conclusions or as-\nsumptions for the sake of the argument, etc.\n(Technically, we provide the infrastructure\nfor doing so, as all this might be achieved\nthrough adjusting the argument corpus con-\nﬁguration ﬁle.)\n• To succeed in NLI tasks, it doesn’t sufﬁce\nto understand ‘what follows.’ In addition,\na system needs to be able to explicitly dis-\ncern contradictions and non sequiturs (rela-\ntions of logical independence). This suggests\nthat the artiﬁcial argument corpus might be\nfruitfully supplemented with corpora of cor-\nrectly identiﬁed aporetic clusters (Rescher,\n1987) as well as corpora containing correctly\ndiagnosed fallacies.\n• In addition, the idea of curriculum learning\nfor ML (Bengio et al., 2009) might be given\na try. Accordingly, a critical thinking cur-\nriculum with basic exemplars of good rea-\nsoning would not only be used to ﬁne-tune a\npre-trained model, but would be employed as\nstarting point for training a language model\nfrom scratch.\nNatural language templating is a fundamental\ntechnique used throughout this paper: both in con-\nstructing the artiﬁcial argument corpus as well\nas in transforming the NLP benchmark datasets\ninto text that can be processed by language mod-\nels. The concrete templates applied have been de-\nsigned in a trial-and-error process. It is far from\nclear that these represent optimal choices for ef-\nfectively eliciting a language model’s skills. Still,\nfollowing (Jiang et al., 2020), it seems of great im-\nportance to gain a more systematic understanding\nof different templating strategies and their effects\non metrics based on accuracy and perplexity.\nIn conclusion, designing a critical thinking cur-\nriculum for neural language models seems to be\na promising and worthwhile research program to\npursue.\nA Appendix: Training Parameters\nWe train the models on 8 GPUs for 2 epochs with\nbatch size = 2, learning rate = 5 ×10−5, gradient\naccumulation steps = 2, and default parameters of\nthe HuggingFace implementation otherwise (Wolf\net al., 2019).\nB Appendix: Performance Metrics for\nDifferently Sized Training Sets\nFigure 6 displays accuracy values on conclusion\ncompletion tasks for models trained on differently\nsized datasets.\nFigure 7 reports perplexity and NLU accuracy\nmetrics for models trained on differently sized\ndatasets.\nReferences\nAmanda Askell. 2020. Gpt-3: Towards renais-\nsance models. In Daily Nous Blog: Philoso-\nphers On GPT-3.\nYoshua Bengio, Jérôme Louradour, Ronan Col-\nlobert, and Jason Weston. 2009. Curriculum\nlearning. In Proceedings of the 26th Annual In-\nternational Conference on Machine Learning ,\nICML ’09, pages 41–48, New York, NY , USA.\nACM.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap,\nChaitanya Malaviya, Asli Çelikyilmaz, and\nYejin Choi. 2019. Comet: Commonsense trans-\nformers for automatic knowledge graph con-\nstruction. In Proceedings of the 57th Annual\nMeeting of the Association for Computational\nLinguistics (ACL).\nTracey Bowell and Gary Kemp. 2014. Critical\nThinking: A Concise Guide, 4th edition edition.\nRoutledge, London.\nSamuel R. Bowman, Gabor Angeli, Christopher\nPotts, and Christopher D. Manning. 2015. A\nlarge annotated corpus for learning natural lan-\nguage inference. In Proceedings of the 2015\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP) . Association\nfor Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Win-\nter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot\nlearners.\nGeorg Brun and Gregor Betz. 2016. Analysing\npractical argumentation. In Sven Ove Hansson\nand Gertrude Hirsch-Hadorn, editors, The Ar-\ngumentative Turn in Policy Analysis. Reason-\ning about Uncertainty , pages 39–77. Springer,\nCham.\nRudolf Carnap. 1950. Logical Foundations of\nProbability. University of Chicago Press,\nChicago.\nJ. Cheng, M. Bernstein, C. Danescu-Niculescu-\nMizil, and J. Leskovec. 2017. Anyone can be-\ncome a troll: Causes of trolling behavior in on-\nline discussions. CSCW: Proceedings of the\nConference on Computer-Supported Coopera-\ntive Work. Conference on Computer-Supported\nCooperative Work, 2017, page 1217–1230.\nPeter Clark, Oyvind Tafjord, and Kyle Richard-\nson. 2020. Transformers as soft reasoners over\nlanguage. arXiv preprint arXiv:2002.05867v2.\nRichard Feldman. 2014. Reason and Argument .\nPearson, Harlow.\nAlec Fisher. 2001. Critical Thinking: An Intro-\nduction. Cambridge University Press, Cam-\nbridge.\nMartin Gerlach and Francesc Font-Clos. 2018. A\nstandardized project gutenberg corpus for sta-\ntistical analysis of natural language and quanti-\ntative linguistics. CoRR, abs/1812.08092.\nBen Gilburt. 2019. Examining gender bias in ope-\nnai’s gpt-2 language model.hackernoon.com.\nNicolas Gontier, Koustuv Sinha, Siva Reddy, and\nChristopher Pal. 2020. Measuring systematic\ngeneralization in neural proof generation with\ntransformers.\nFigure 6: Accuracy on three conclusion completion tasks as a function of training corpus size.\n0 9K 18K 36K 72K\nsize training set\n25\n30\n35\n40\n45\n50\n55perplexity\nPerplexity Wiki103\nmodel_size\n762M\n345M\n117M\ntrain\ntrain03\ntrain02\ntrain01\n0 9K 18K 36K 72K\nsize training set\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50accuracy\nGLUE AX\nmodel_size\n762M\n345M\n117M\ntrain\ntrain03\ntrain02\ntrain01\n0 9K 18K 36K 72K\nsize training set\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50accuracy\nSNLI\nmodel_size\n762M\n345M\n117M\ntrain\ntrain03\ntrain02\ntrain01\nFigure 7: Perplexity and NLI metrics as a function of training corpus size.\nRadu Cornel Guia¸ su and Christopher W Tindale.\n2018. Logical fallacies and invasion biology.\nBiology & philosophy, 33(5-6):34.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and\nNoah A Smith. 2018. Annotation artifacts in\nnatural language inference data. In Proceed-\nings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technolo-\ngies, Volume 2 (Short Papers), pages 107–112.\nIvan Habernal, Henning Wachsmuth, Iryna\nGurevych, and Benno Stein. 2018. The argu-\nment reasoning comprehension task: Identiﬁ-\ncation and reconstruction of implicit warrants.\nIn Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans,\nLouisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 1930–1940. Association\nfor Computational Linguistics.\nSven Ove Hansson. 2004. Fallacies of risk. Jour-\nnal of Risk Research, 7(3):353–360.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and\nGraham Neubig. 2020. How can we know\nwhat language models know? Transactions of\nthe Association for Computational Linguistics ,\n8:423–438.\nDaniel Kahneman. 2011. Thinking, fast and slow,\n1st edition. Farrar, Straus and Giroux, New\nYork.\nNora Kassner, Benno Krojer, and Hinrich Schütze.\n2020. Are pretrained language models sym-\nbolic reasoners over knowledge?\nNora Kassner and Hinrich Schütze. 2020.\nNegated and misprimed probes for pretrained\nlanguage models: Birds can talk, but cannot ﬂy.\nJoe Lau and Jonathan Chan. 2020. Critical think-\ning web. https://philosophy.hku.hk/think.\nD. D. Lewis, Y . Yang, T. Rose, and F. Li. 2004.\nRcv1: A new benchmark collection for text\ncategorization research. Journal of Machine\nLearning Research, 5:361–397.\nKevin Lin, Oyvind Tafjord, Peter Clark, and Matt\nGardner. 2019. Reasoning over paragraph ef-\nfects in situations. Proc. MRQA Workshop\n(EMNLP’19).\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan\nHuang, Yile Wang, and Yue Zhang. 2020.\nLogiqa: A challenge dataset for machine read-\ning comprehension with logical reasoning. In\nProceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2020, pages 3622–3628. ijcai.org.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoy-\nanov. 2019. Roberta: A robustly optimized bert\npretraining approach.\nKaixin Ma, Filip Ilievski, Jonathan Francis,\nYonatan Bisk, Eric Nyberg, and Alessan-\ndro Oltramari. 2020. Knowledge-driven\nself-supervision for zero-shot common-\nsense question answering. arXiv preprint\narXiv:2011.03863.\nArindam Mitra, Pratyay Banerjee, Kuntal Kumar\nPal, Swaroop Mishra, and Chitta Baral. 2019.\nHow additional knowledge can improve nat-\nural language commonsense question answer-\ning? arXiv preprint arXiv:1909.08855.\nTimothy Niven and Hung-Yu Kao. 2019. Probing\nneural network comprehension of natural lan-\nguage arguments. In Proceedings of the 57th\nAnnual Meeting of the Association for Com-\nputational Linguistics, pages 4658–4664, Flo-\nrence, Italy. Association for Computational Lin-\nguistics.\nSP Norris and RH Ennis. 1989. What is criti-\ncal thinking. The practitioner’s guide to teach-\ning thinking series: Evaluating critical think-\ning, pages 1–26.\nFabio Paglieri. 2017. A plea for ecological argu-\nment technologies. Philosophy & Technology,\n30(2):209–238.\nJason Phang, Thibault Févry, and Samuel R Bow-\nman. 2018. Sentence encoders on stilts: Sup-\nplementary training on intermediate labeled-\ndata tasks. arXiv preprint arXiv:1811.01088.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. Preprint.\nNicholas Rescher. 1987. Aporetic method in\nphilosophy. The Review of metaphysics ,\n41(2):283–297.\nKyle Richardson, Lawrence S. Moss, , and Ashish\nSabharwal. 2020. Probing natural language\ninference models through semantic fragments.\nAAAI’20.\nSwarnadeep Saha, Sayan Ghosh, Shashank Sri-\nvastava, and Mohit Bansal. 2020. Prover:\nProof generation for interpretable reasoning\nover rules.\nTimo Schick and Hinrich Schütze. 2020a. Ex-\nploiting cloze questions for few shot text clas-\nsiﬁcation and natural language inference.\nTimo Schick and Hinrich Schütze. 2020b. It’s not\njust size that matters: Small language models\nare also few-shot learners.\nVered Shwartz, Peter West, Ronan Le Bras, Chan-\ndra Bhagavatula, and Yejin Choi. 2020. Un-\nsupervised commonsense question answering\nwith self-talk.\nKoustuv Sinha, Shagun Sodhani, Jin Dong, Joelle\nPineau, and William L. Hamilton. 2019. Clutrr:\nA diagnostic benchmark for inductive reasoning\nfrom text. arXiv preprint arXiv:1908.06177v2.\nCass R Sunstein and Reid Hastie. 2015. Wiser:\ngetting beyond groupthink to make groups\nsmarter. Harvard Business Review Press,\nBoston.\nOyvind Tafjord, Matt Gardner, Kevin Lin, and\nPeter Clark. 2019. Quartz: An open-domain\ndataset of qualitative relationship questions.\nEMNLP/IJCNLP.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics – on what lan-\nguage model pre-training captures.\nAlex Wang, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman.\n2018. Glue: A multi-task benchmark and anal-\nysis platform for natural language understand-\ning. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 353–355.\nJ. Weston, A. Bordes, S. Chopra, and T. Mikolov.\n2016. Towards ai-complete question answer-\ning: A set of prerequisite toy tasks. ICLR.\nThomas Wolf, Lysandre Debut, Victor Sanh,\nJulien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf,\nMorgan Funtowicz, et al. 2019. Huggingface’s\ntransformers: State-of-the-art natural language\nprocessing. ArXiv, pages arXiv–1910.",
  "topic": "Argumentative",
  "concepts": [
    {
      "name": "Argumentative",
      "score": 0.9087969064712524
    },
    {
      "name": "Computer science",
      "score": 0.7195526361465454
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.6820337772369385
    },
    {
      "name": "Critical thinking",
      "score": 0.532053530216217
    },
    {
      "name": "Curriculum",
      "score": 0.5172308087348938
    },
    {
      "name": "Point (geometry)",
      "score": 0.4987668991088867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4905340075492859
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.489803284406662
    },
    {
      "name": "Language understanding",
      "score": 0.4517570734024048
    },
    {
      "name": "Argument map",
      "score": 0.4398009181022644
    },
    {
      "name": "Language model",
      "score": 0.4278929829597473
    },
    {
      "name": "Natural language processing",
      "score": 0.4227582812309265
    },
    {
      "name": "Core (optical fiber)",
      "score": 0.41176697611808777
    },
    {
      "name": "Linguistics",
      "score": 0.299872487783432
    },
    {
      "name": "Mathematics education",
      "score": 0.2963785231113434
    },
    {
      "name": "Argumentation theory",
      "score": 0.25112563371658325
    },
    {
      "name": "Epistemology",
      "score": 0.1906735599040985
    },
    {
      "name": "Psychology",
      "score": 0.18206635117530823
    },
    {
      "name": "Mathematics",
      "score": 0.11695769429206848
    },
    {
      "name": "Pedagogy",
      "score": 0.09535586833953857
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}