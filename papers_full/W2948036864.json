{
    "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
    "url": "https://openalex.org/W2948036864",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2225075515",
            "name": "Nazneen Fatema Rajani",
            "affiliations": [
                "Salesforce (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2158755771",
            "name": "Bryan McCann",
            "affiliations": [
                "Salesforce (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2095665791",
            "name": "Caiming Xiong",
            "affiliations": [
                "Salesforce (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1964982643",
            "name": "Richard Socher",
            "affiliations": [
                "Salesforce (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2891012317",
        "https://openalex.org/W2798431140",
        "https://openalex.org/W2005814556",
        "https://openalex.org/W2950761309",
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2804053041",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963028402",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W2791386785",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2963233086",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2250790822",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2889866549"
    ],
    "abstract": "Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932–4942\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n4932\nExplain Yourself!\nLeveraging Language Models for Commonsense Reasoning\nNazneen Fatema Rajani Bryan McCann Caiming Xiong Richard Socher\nSalesforce Research\nPalo Alto, CA, 94301\n{nazneen.rajani,bmccann,cxiong,rsocher}@salesforce.com\nAbstract\nDeep learning models perform poorly on\ntasks that require commonsense reasoning,\nwhich often necessitates some form of world-\nknowledge or reasoning over information not\nimmediately present in the input. We collect\nhuman explanations for commonsense reason-\ning in the form of natural language sequences\nand highlighted annotations in a new dataset\ncalled Common Sense Explanations (CoS-E).\nWe use CoS-E to train language models to\nautomatically generate explanations that can\nbe used during training and inference in a\nnovel Commonsense Auto-Generated Expla-\nnation (CAGE) framework. CAGE improves\nthe state-of-the-art by 10% on the challeng-\ning CommonsenseQA task. We further study\ncommonsense reasoning in DNNs using both\nhuman and auto-generated explanations in-\ncluding transfer to out-of-domain tasks. Em-\npirical results indicate that we can effectively\nleverage language models for commonsense\nreasoning.\n1 Introduction\nCommonsense reasoning is a challenging task for\nmodern machine learning methods (Zhong et al.,\n2018; Talmor et al., 2019). Explanations are a way\nto verbalize the reasoning that the models learn\nduring training. Common sense Question Answer-\ning (CQA) is a multiple-choice question answer-\ning dataset proposed for developing natural lan-\nguage processing (NLP) models with commons-\nsense reasoning capabilities (Talmor et al., 2019).\nAlthough these efforts have led to progress, it is\nstill unclear how these models perform reasoning\nand to what extent that reasoning is based on world\nknowledge. We collect human explanations for\ncommonsense reasoning built on top of CQA and\nintroduce them as Common Sense Explanations\n(CoS-E)1. CoS-E contains human explanations in\n1https://github.com/nazneenrajani/CoS-E\nQuestion: While eating a hamburger with friends,\nwhat are people trying to do?\nChoices: have fun, tasty, or indigestion\nCoS-E: Usually a hamburger with friends indicates a\ngood time.\nQuestion: After getting drunk people couldn’t\nunderstand him,it was because of his what?\nChoices: lower standards, slurred speech, or falling\ndown\nCoS-E: People who are drunk have difﬁculty speaking.\nQuestion: People do what during their time off from\nwork?\nChoices: take trips, brow shorter, or become hysterical\nCoS-E: People usually do something relaxing, such as\ntaking trips,when they don’t need to work.\nTable 1: Examples from our CoS-E dataset.\nthe form of both open-ended natural language ex-\nplanations as well as highlighted span annotations\nthat represent words selected by humans as impor-\ntant for predicting the right answer (see Table 1).\nTalmor et al. (2019) show that using Google\nsearch to extract context from top 100 result snip-\npets for each of the question and answer choices\ndoes not help much in improving the accuracy on\nCQA trained using even the state-of-the-art read-\ning comprehension model BiDAF++ (Seo et al.,\n2017) augmented with a self-attention layer and\nELMo representations (Peters et al., 2018).\nIn contrast, we leverage a pretrained language\nmodel to generate explanations that are useful for\ncommonsense reasoning. We propose Common-\nsense Auto-Generated Explanations (CAGE) as a\nframework for generating explanations for CQA.\nWe break down the task of commonsense reason-\ning into two phases. In the ﬁrst phase, we pro-\nvide a CQA example alongside the corresponding\nCoS-E explanation to a language model. The lan-\nguage model conditions on the question and an-\nswer choices from the example and is trained to\ngenerate the CoS-E explanation.\nIn the second phase, we use the language model\n4933\n…\n(a) One time-step of training a CAGE language model to gen-\nerate explanations from CoS-E. It is conditioned on the ques-\ntion tokens Q concatenated with the answer choice tokens\nA1, A2, A3 and previously generated tokens E1, . . . , Ei−1. It\nis trained to generate token Ei.\n…\nCSRM\n(b) A trained CAGE language model is used to generate ex-\nplanations for a downstream commonsense reasoning model\n(CSRM), which itself predicts one of the answer choices.\nFigure 1: An overview of CAGE trained on CoS-E and CQA.\nto generate explanations for each example in the\ntraining and validation sets of CQA. These CAGE\nexplanations are provided to a second common-\nsense reasoning model by concatenating it to the\nend of the original question, answer choices, and\noutput of the language model. The two-phase\nCAGE framework obtains state-of-the-art results\noutperforming the best reported baseline by 10%\nand also produces explanations to justify its pre-\ndictions. Figure 1 shows an overview of our pro-\nposed approach.\nIn summary, we introduce a new Common\nSense Explanations (CoS-E) dataset to study neu-\nral commonsense reasoning and provide a new\nmethod, CAGE for automatically generating ex-\nplanations that achieve a state-of-the-art accuracy\nof approximately 65% on CQA v1.0. We demon-\nstrate explanation transfer on two out-of-domain\ndatasets. Note that before our ﬁnal submission,\nthe organizers released a more challenging v1.11\nof CQA with 5 answer choices instead of 3 and so\nwe also included the new version in our results and\ndiscussions.\n2 Background and Related Work\nCommonsense reasoning Datasets that require\nmodels to learn to predict relations between situ-\nations or events in natural language have been in-\ntroduced in the recent past. The Story Cloze (also\nreferred to as ROC Stories) involves predicting the\ncorrect story ending from a set of plausible end-\nings (Mostafazadeh et al., 2016) while the Situ-\nations with Adversarial Generations (SW AG) in-\nvolves predicting the next scene based on an initial\nevent (Zellers et al., 2018). Language Modeling\nbased techniques such as the GPT and BERT mod-\nels get human-level performance on these datasets\n(Radford et al., 2018; Devlin et al., 2019). They\nhave been less successful on tasks that require\nclear understanding of how pronouns resolve be-\ntween sentences and how that interacts with world\nknowledge. For example, the Winograd Schemas\n(Winograd, 1972) and challenges derived from\nthat format (Levesque et al., 2012; McCann et al.,\n2018; Wang et al., 2018) have proven difﬁcult for\neven the most modern machine learning methods\n(Trinh and Le, 2018) to achieve near-human per-\nformance, but the emphasis on pronoun resolution\nin those challenges leaves room for exploration\nof other kinds of commonsense reasoning. CQA\nis a new dataset that consists of 9500 questions\nwith one correct answer and two distractor an-\nswers (Talmor et al., 2019). The authors claim that\nbecause all the answer choices are drawn from the\nsame source concept, the dataset requires models\nto actually infer from the question rather than take\nadvantage of distributional biases. We, however,\nobserved that the current state of this dataset has\ngender disparity with higher proportion of femi-\nnine pronouns used in negative context.\nThe authors show that the state-of-the-art lan-\nguage models perform very poorly compared to\nhuman participants on their dataset. Although,\nCQA introduces a benchmark for evaluating com-\nmonsense reasoning capabilities of models, it is\nstill unclear how and to what extent do models ac-\ntually do common-sense reasoning. CoS-E builds\non top of their benchmark, on the other hand, pro-\nvides data in the form of explanations that can be\nused to study and analyze as well as evaluate a\nmodel’s reasoning capabilities.\nNatural language explanations Lei et al.\n(2016) proposed an approach for rationale genera-\ntion for sentiment analysis by highlighting com-\nplete phrases in the input text that by itself is\nsufﬁcient to predict the desired output. Human-\ngenerated natural language explanations for clas-\nsiﬁcation data have been used in the past to train a\nsemantic parser that in turn generates more noisy\n4934\nlabeled data which can used to train a classiﬁer\n(Hancock et al., 2018). Camburu et al. (2018)\ngenerate explanations and predictions for the nat-\nural language inference problem (Camburu et al.,\n2018). However, the authors report that inter-\npretability comes at the cost of loss in perfor-\nmance on the popular Stanford Natural Language\nInference (Bowman et al., 2015) dataset. We ﬁnd\nthat, unlike for e-SNLI, explanations for CQA\nlead to improved performance in what Camburu\net al. (2018) would call the explain-predict setting.\nIn the multi-modal setting, Rajani and Mooney\n(2018) showed that visual explanations can be\nleveraged to improve performance of VQA (An-\ntol et al., 2015) and that an ensemble explanation\nis signiﬁcantly better than individual explanations\nusing both automated and human evaluations (Ra-\njani and Mooney, 2017).\nKnowledge Transfer in NLP Natural language\nprocessing has often relied on the transfer of\nworld-knowledge through pretrained word vec-\ntors like Word2Vec (Mikolov et al., 2013) and\nGloVe (Pennington et al., 2014). Contextualized\nword vectors (McCann et al., 2017; Peters et al.,\n2018) reﬁned these representations for particular\ninputs by using different forms of general encod-\ning. Language models trained from scratch on\nlarge amounts of data have made groundbreak-\ning success in this direction by carefully ﬁne-\ntuning for speciﬁc tasks (Dai and Le, 2015; Rad-\nford et al., 2018; Howard and Ruder, 2018; Devlin\net al., 2019). These models have the advantage\nthat only a few parameters need to be learned from\nscratch and thus perform surprisingly well even on\nsmall amounts of supervised data. Fine-tuned lan-\nguage models do not however work as well for di-\nrectly predicting answers for CQA (Talmor et al.,\n2019). In our work, we show how these ﬁne-\ntuned language models are more effective when\nleveraged to generate explanations and empirically\nprove that they also linguistically capture common\nsense.\n3 Common Sense Explanations (CoS-E)\nWe used Amazon Mechanical Turk (MTurk) to\ncollect explanations for our Common Sense Ex-\nplanations (CoS-E) dataset. The CQA dataset con-\nsists of two splits – the question token splitand\nthe random split. Our CoS-E dataset and all our\nexperiments use the more difﬁcult random split,\nwhich is the main evaluation split according to Tal-\n0 20 40 60 80 100\nPercent of Examples\nQuestion\nTrigram\nQuestion\nBigram\nAnswer or\nDistractor\nDistractor\nAnswer\n38.1\n60.0\n58.5\n35.0\n30.0\nFigure 2: Analysis of the CoS-E v1.0 dataset. Percent\nof the dataset that contains the answer, a distractor, ei-\nther, at least one bigram from the question, and at least\none trigram from the question.\nmor et al. (2019). We also release CoS-E for CQA\nv1.11.\nHuman participants are given the question and\nanswer choices along with the ground-truth an-\nswer choice. Turkers are prompted with the fol-\nlowing question: “Why is the predicted output the\nmost appropriate answer?” Annotators were in-\nstructed to highlight relevant words in the question\nthat justiﬁes the ground-truth answer choice and\nto provide a brief open-ended explanation based\non the highlighted justiﬁcation could serve as the\ncommonsense reasoning behind the question. We\ncollected these explanations for the CQA train-\nrandom-split and dev-random-split, which have a\nsize of 7610 and 950 for v1.0 and 9741 and 1221\nfor v1.11 respectively. Table 1 shows a random\nsample of examples from our CoS-E dataset with\nboth free-form explanations and highlighted text.\nFrom here on, we refer to the highlighted words as\nCoS-E-selected and the free-form explanation as\nCoS-E-open-ended.\nIn MTurk, it is difﬁcult to control the quality\nof open-ended annotations. So, we do some in-\nbrowser checks to avoid obviously bad explana-\ntions. Annotators cannot move forward if they do\nnot highlight any relevant words in the question or\nif the length of explanations is less than 4 words.\nWe also check that the explanation is not a sub-\nstring of the question or the answer choices with-\nout any other extra words. We collect these ex-\nplanations from only one annotator per example,\nso we also perform some post-collection checks to\ncatch examples that are not caught by our previ-\nous ﬁlters. We ﬁlter out explanations that could\nbe classiﬁed as a template. For example, expla-\nnations of the form “<answer> is the only option\nthat is [correct|obvious]” are deleted and then re-\nannotated.\nFigure 2 shows the distribution of explanations\ncollected in the CoS-E v1.0 dataset.58% of expla-\n4935\nnations from CoS-E contain the ground truth, but\nthe effectiveness of CoS-E is not constrained only\nto those examples. Our model obtains state-of-the-\nart results by using CoS-E only during training.\nEmpirical results show that even when using only\nthose explanations that donot have any word over-\nlap with any of the answer choices, performance\nexceeds that of baselines that do not use CoS-E\nat all. We also observed that a signiﬁcant pro-\nportion of the distractor choices are also present\nin the CoS-E dataset and on further analysis we\nfound that for those examples, annotators resorted\nto explaining by eliminating the wrong choices.\nThis indicates that it is difﬁcult even for humans to\nreason about many of the examples in CQA. Be-\ncause CoS-E uses crowd-sourcing, it also adds di-\nversity of perspective and in particular diverse rea-\nsoning on world knowledge to the CQA dataset.\nEven though many explanations remain noisy af-\nter quality-control checks, we ﬁnd that they are of\nsufﬁcient quality to train a language model that\ngenerates commonsense reasoning. We refer to\nSection 5 for more details on empirical results and\nablation analysis on CoS-E.\n4 Algorithm\nWe present Commonsense Auto-Generated Expla-\nnations (CAGE) and apply it to the CQA task.\nCAGE are generated by a language model and are\nused aas supplementary inputs to a classiﬁcation\nmodel. Each example in CQA consists of a ques-\ntion, q, three answer choices, c0, c1, c2, and a la-\nbeled answer a. Our CoS-E dataset adds a human\nexplanation eh for why a is the most appropriate\nchoice. The output of CAGE is a language model\ngenerated explanation e that is trained to be close\nto eh.\n4.1 Commonsense Auto-Generated\nExplanations (CAGE)\nIn order to supply CAGE to a classiﬁcation model,\nwe ﬁne-tune a language model (LM) to gener-\nate explanations from our CoS-E dataset. Our\nLM is the large, pre-trained OpenAI GPT (Rad-\nford et al., 2018) which is a multi-layer, trans-\nformer (Vaswani et al., 2017) decoder. GPT is\nﬁne-tuned on the combination of CQA and CoS-E\ndatasets, as shown in the left half of Figure 1. We\nexplore explanation generation in two settings –\n1) explain-and-then-predict (reasoning) (Figure 1)\nand 2) predict-and-then-explain (rationalization).\nReasoning This is our main approach and in this\nthe LM is ﬁne-tuned conditioned on the question,\nanswer choices and the human generated explana-\ntion and not the actual predicted label. So, the in-\nput context during training is deﬁned as follows:\nCRE = “q, c0, c1, or c2? commonsense says ”\nThe model is trained to generate explanationse ac-\ncording to a conditional language modeling objec-\ntive. The objective is to maximize:\n∑\ni\nlog P(ei|ei−k, . . . , ei−1, CRE; Θ)\nwhere k is the size of the context window (in our\ncase k is always greater than the length ofe so that\nthe entire explanation is within the context). The\nconditional probability P is modeled by a neural\nnetwork with parameters Θ conditioned on CRE\nand previous explanation tokens. We call this kind\nof explanation reasoning because they can be au-\ntomatically generated during inference to provide\nadditional context for commonsense question an-\nswering. In Section 5, we show that this approach\noutperforms the reported state-of-the-art on CQA\nby 10%. For the sake of completeness, we also\nexperimented with the reverse of this approach\nwherein the model ﬁrst makes the predictions and\nthen generates explanations based on those labels,\nwhich we call rationalization and is discussed be-\nlow.\nRationalization In rationalization, the LM\nmodel conditions on the predicted labels along\nwith the input to generate post-hoc rational-\nizations. So, during the ﬁne-tuning step, the\ninput context contains the output label and is\nconstructed as follows:\nCRA = “ q, c0, c1, or c2? a because ”\nThe training objective for the LM in rationaliza-\ntion is similar to that in reasoning except that in\nthis case, the model has access to the ground truth\nlabels to the input questions during training. Be-\ncause the language model is conditioned on the\npredicted label, the explanations cannot be con-\nsidered as common sense reasoning. Instead, they\noffer a rationalization that makes the model more\naccessible and interpretable. We ﬁnd that this ap-\nproach outperforms the current best model by 6%\nand also produces interestingly good quality ex-\nplanations as discussed in Section 5.\nFor CAGE, we generate sequences of maximum\nlength 20, use a batch size of 36, train for a maxi-\nmum of 10 epochs, selecting the best model based\n4936\non validation BLEU and perplexity scores. Learn-\ning rate was set to 1e−6, warmed up linearly with\nproportion 0.002 and weight decay 0.01.\n4.2 Commonsense Predictions with\nExplanations\nGiven either a human explanation from CoS-E or\nreasoning from a language model, we can then\nlearn to perform predictions on the CQA task.\nFor the classiﬁcation module of our proposed ap-\nproach, we adopt the widely popular BERT model\n(Devlin et al., 2019) which we refer to as just\nBERT. BERT can be ﬁne-tuned for multiple choice\nquestion answering by adding a simple binary\nclassiﬁer that takes as input the ﬁnal state corre-\nsponding to the the special [CLS] token placed\nat the start of all inputs to BERT models (Devlin\net al., 2019). We apply this same approach to\nthe CQA task. For each example in the dataset,\nwe construct three input sequences for ﬁne-tuning\nBERT. Each sequence is the concatenation of the\nquestion, a separator token [SEP], and one of the\nanswer choices. If the approach requires expla-\nnation from either CoS-E or automatically gener-\nated as in the CAGE, we concatenate the question,\n[SEP], the explanation, [SEP], and an answer\nchoice. For BERT, the explanations share the same\ninput representation as that of the questions. We\nalso experimented with the explanation sharing the\nsame representation as that of the answer choice\nbut found that the performance decreased slightly.\nWhen explanations are used only during train-\ning, the explanation variable is optional and the\nanswer choices directly follow the question dur-\ning evaluation. For all our experiments we used\na train batch size of 24, test batch size of 12, 10\ntraining epochs and maximum sequence length of\n50 for the baseline and 175 for all experiments in-\nvolving explanations. The right part of Figure 1\ngives an overview of the classiﬁcation module of\nour proposed approach.\n4.3 Transfer to out-of-domain datasets\nTransfer without ﬁne-tuning to out-of-domain\nNLP datasets is known to exhibit poor perfor-\nmance. For example, for the comparatively eas-\nier natural langauge inference task with ﬁxed la-\nbels, Bowman et al. (2015) show that the accuracy\ndropped by 25% when training on SNLI and eval-\nuating on SICK-E (Marelli et al., 2014). We study\ntransfer of natural language explanations from the\nCQA to SW AG (Zellers et al., 2018) and Story\nCloze Test (Mostafazadeh et al., 2016). Both the\ndatasets are multiple-choice like CQA and the au-\nthors publicize them as commonsense reasoning\nand inference tasks.\nWe use the GPT language model ﬁne-tuned on\nCQA train and dev sets to generate explanations\non the SW AG train and val sets (with 73546 and\n20006 instances respectively) and the Story Cloze\nSpring 2016 val and test sets (with 1870 instances\neach). We then train a BERT classiﬁer using the\ninput instances and generated explanations and\nevaluate on the SW AG and Story Cloze test sets.\n5 Experimental Results\nWe present results on the CQA dataset using\nvariations of our proposed Commonsense Auto-\nGenerated Explanations (CAGE). All our models\nare based on BERT, which also serves as our base-\nline without any CoS-E or CAGE. All our ablation\nanalysis is conducted on the CQA dev-random-\nsplit. We also show results for key models on the\nﬁnal test split.2\nMethod Accuracy (%)\nBERT (baseline) 63.8\nCoS-E-open-ended 65.5\nCAGE-reasoning 72.6\nTable 2: Results on CQA dev-random-split with CoS-E\nused during training.\nTable 2 shows results that compare a BERT\nbaseline that uses only the CQA inputs and the\nsame architecture but trained using inputs that\ncontain explanations from CoS-E during train-\ning. The BERT baseline model reaches64% accu-\nracy and adding open-ended human explanations\n(CoS-E-open-ended) alongside the questions dur-\ning training results in a 2% boost in accuracy.\nBy generating explanations as described in Sec-\ntion 4.1, we can give the commonsense question\nanswering model access to an explanation that is\nnot conditioned on the ground truth. These expla-\nnations (CAGE-reasoning) can be provided during\nboth training and validation and increases the ac-\ncuracy to 72%.\nTable 3 shows the results obtained on the CQA\ntest split. We report our two best models that\nrepresent using human explanations (CoS-E-open-\nended) for training only and using language model\nexplanations (CAGE-reasoning) during both train\nand test. We compare our approaches to the best\nreported models for the CQA task (Talmor et al.,\n2https://www.tau-nlp.org/csqa-leaderboard\n4937\nMethod Accuracy (%)\nRC (Talmor et al., 2019) 47.7\nGPT (Talmor et al., 2019) 54.8\nCoS-E-open-ended 60.2\nCAGE-reasoning 64.7\nHuman (Talmor et al., 2019) 95.3\nTable 3: Test accuracy on CQA v1.0. The addition\nof CoS-E-open-ended during training dramatically im-\nproves performance. Replacing CoS-E during training\nwith CAGE reasoning during both training and infer-\nence leads to an absolute gain of10% over the previous\nstate-of-the-art.\nMethod Accuracy (%)\nCoS-E-selected w/o ques 53.0\nCoS-E-limited-open-ended 67.6\nCoS-E-selected 70.0\nCoS-E-open-ended w/o ques 84.5\nCoS-E-open-ended* 89.8\nTable 4: Oracle results on CQA dev-random-split using\ndifferent variants of CoS-E for both training and valida-\ntion. * indicates CoS-E-open-ended used during both\ntraining and validation to contrast with CoS-E-open-\nended used only during training in Table 2.\n2019). We observe that using CoS-E-open-ended\nduring training improves the state-of-the-art by ap-\nproximately 6%.\nTalmor et al. (2019) experimented with using\nGoogle search of “question + answer choice” for\neach example in the dataset and collected 100 top\nsnippets per answer choice to be used as context\nfor their Reading Comprehension (RC) model.\nThey found that providing such extra data does\nnot improve accuracy. On the other hand, us-\ning CAGE-reasoning resulted in a gain of 10%\naccuracy over the previous state-of-the-art. This\nsuggests that our CoS-E-open-ended and CAGE-\nreasoning explanations provide far more useful in-\nformation than what can be achieved through sim-\nple heuristics like using Google search to ﬁnd rel-\nevant snippets. We observed that our models’ per-\nformance on test is lower than those on validation\nand this trend was conﬁrmed by the organizers of\nthe task.\nTo establish an oracle upper-bound on the per-\nformance, we also explored an experimental set-\nting in which human-generated explanations from\nCoS-E are provided during both training and val-\nidation. These results are summarized in Table 4.\nWe note that this is an unfair setting because the\nhuman that provided the explanation had access to\nthe ground truth answer; these results merely serve\nas an oracle for how much potential beneﬁt can\ncome from using CoS-E-open-ended. If the open-\nended human explanations (CoS-E-open-ended)\nare provided at inference time, performance jumps\nto approximately 90%. These results also motivate\nan attempt to automatically generate explanations\nthat establish the world knowledge needed to solve\nCQA. CAGE-reasoning is our attempt towards this\ngoal.\nTable 4 also contains results that use only the\nexplanation and exclude the original question from\nCQA denoted by ‘w/o question’. These variants\nalso use explanation during both train and valida-\ntion. For these experiments we give the explana-\ntion in place of the question followed by the an-\nswer choices as input to the model. When the\nexplanation consists of words humans selected as\njustiﬁcation for the answer (CoS-E-selected), the\nmodel was able to obtain 53% in contrast to the\n85% achieved by the open-ended human explana-\ntions (CoS-E-open-ended). Adding the question\nboosts performance for CoS-E-selected to 70%,\nagain falling short of almost 90% achieved by\nCoS-E-open-ended. We conclude then that our\nfull, open-ended CoS-E thus supply a signiﬁcant\nsource of information beyond simply directing the\nmodel towards the most useful information al-\nready in the question.\nMethod Accuracy (%)\nCAGE-reasoning 55.7\nBERT baseline 56.7\nCoS-E-open-ended 58.2\nTable 5: Test results on CQA v1.11.\nWe experimented with one ﬁnal setting in which\nwe only used open-ended explanations that did not\ncontain any word from any answer choices (23%.\nIn this setting, we call these “CoS-E-limited-open-\nended” explanations because these explanations\nare limited in the choice of words allowed. We\nobserve that even using these limited kind of ex-\nplanations improves over the BERT baseline in Ta-\nble 4, which suggests that the explanations are pro-\nviding useful information beyond just mentioning\nthe correct or incorrect answers.\nWe also evaluated our key models – CoS-E-\nopen-ended used during training only and the\nCAGE reasoning on the v1.11 of CQA that was re-\nleased before the ﬁnal submission. Table 5 shows\nthe results obtained on the more challenging CQA\nv1.11.\nCamburu et al. (2018) empirically show that\n4938\ntransferring explanations on the natural language\ninference (NLI) problem from SNLI to MultiNLI\nperforms very poorly and is still an open challeng-\ning problem. We study transfer of explanations on\ncommonsense reasoning tasks. The NLI problem\nhas a small ﬁxed set of pre-deﬁned labels unlike\nthe commonsense reasoning tasks such as CQA,\nSW AG and Story Cloze. Table 6 shows the results\nobtained by the BERT baseline without explana-\ntions and using our transferred explanations from\nCQA to SW AG and Story Cloze. We observed\nthat adding explanations led to a very small de-\ncrease (< 0.6%) in the performance compared to\nthe baseline for both tasks.\nMethod SW AG Story Cloze\nBERT 84.2 89.8\n+ expl transfer 83.6 89.5\nTable 6: Results for explanation transfer from CQA to\nout-of-domain SW AG and Sotry Cloze tasks.\n6 Analysis and Discussion\nIn Table 2, using CAGE-reasoning at both train\nand validation resulted in an accuracy of 72%,\nbut Table 4 shows that if CAGE-reasoning truly\ncaptured all information provided in CoS-E-open-\nended, performance would be 90%. This gap be-\ntween CAGE and CoS-E prompted further analy-\nsis.\nWe measure quality of CAGE using human\nevaluation and automated metrics. One of the met-\nrics is the BLEU score (Papineni et al., 2002),\nwhich measures syntactical precision by n-gram\noverlap. We also report perplexity, which pro-\nvides a token-level measure of how well the lan-\nguage models predict the next word. We ob-\ntained a peak BLEU score of 4.1 between CAGE-\nreasoning and CoS-E-open-ended and perplexity\nof 32. Language models that are not ﬁne-tuned\nachieve BLEU score of only 0.8. Though it is\nclearly beneﬁcial to ﬁne-tune the LM and empiri-\ncal results suggested that CAGE increased perfor-\nmance, these scores suggest that humans and LMs\nhave widely varying ways of providing useful ex-\nplanations.\nError analysis on the baseline BERT model\nthat does not use any explanations indicates that\nthe model performs poorly on questions that are\nlonger on an average and are more compositional.\nThe average length of such questions is 14 words\nas opposed to the average length of 13 words for\nquestions that the model using CAGE predicts in-\nQuestion: What could people do that involves talking?\nChoices: confession, carnival, state park\nCoS-E: confession is the only vocal action.\nReason people talk to each other\nRationale: people talk to people\nQuestion: A child wants to play, what would they likely want?\nChoices: play tag, breathe, fall down\nCoS-E: A child to play tag\nReason Children want to play tag, and they want to play tag with their\nfriends.\nRationale: Children want to play tag, what would they want to do?\nQuestion: They were getting ready for a really long hike, he put the food\nin his what?\nChoices: recycling center, house, backpack\nCoS-E: Backpacks are used on hikes\nReason a backpack is a place to store food and supplies.\nRationale: a backpack is used to carry food and supplies\nQuestion: You can do knitting to get the feeling of what?\nChoices: relaxation, your, arthritis\nCoS-E: Your are focusing on a repetitive task.\nReason knitting is the only thing that is relaxing.\nRationale: you can do knitting to get the feeling of what?\nTable 7: Random sample of explanations generated by\nhumans from CoS-E and our CAGE framework’s rea-\nsoning and rationalization approaches. Boldface indi-\ncates gold label. All the typos and grammatical errors\nare as they appear in the actual output sequence.\ncorrectly. Therefore, we can conclude that expla-\nnations help elucidate the longer and more com-\nplicated compositional questions.\nTable 7 shows a collection of examples from\nCQA, CoS-E, and CAGE samples. We ob-\nserve that CAGE-reasoning typically employs\na much simpler construction than CoS-E-open-\nended. Nonetheless, this simple declarative mode\ncan sometimes be more informative than CoS-E-\nopen-ended. CAGE achieves this by either pro-\nviding more explicit guidance (as in the ﬁnal ex-\nample of Table 7) or by adding meaningful context\n(as in the third example by introducing the word\n‘friends’). We observe that CAGE-reasoning con-\ntains at least one of the answer choices 43% of the\ntime, out of which it contains the model’s actual\npredicted answer choice 21% of the time. This\nsuggests that there is more to the effectiveness of\nCAGE-reasoning than directly pointing to the an-\nswer.\nQuestion: What is the main purpose of having a bath?\nChoices: cleanness, use water, exfoliation, hygiene, wetness\nExplanation: the only purpose of having a bath is to clean yourself.\nQuestion: Where can you store you spare linens near your socks?\nChoices: cabinet, chest, hospital, dresser drawers, home\nExplanation: dresser drawer is the only place that you can store linens.\nQuestion: Where do you ﬁnd the most amount of leafs?,\nChoices: forrest, ﬂoral arrangement, compost pile, ﬁeld, ground\nExplanation: the most likely place to ﬁnd leafs is in a garden.\nTable 8: Random sample of incorrectly predicted in-\nstances by CAGE-reasoning on CQA v1.11 dev-set.\nBold indicated ground-truth and underline indicates\nour CAGE’s prediction.\n4939\nWe also carried out human evaluations to\ncompare 400 examples of CoS-E and CAGE-\nreasoning. We asked human participants on Me-\nchanical Turk to guess the most appropriate an-\nswer choice based on only the explanation without\nthe question. This tests whether the explanation\nby itself is sufﬁcient for a human to arrive at the\nsame answer as the neural network. We found that\nTurkers were able to arrive at the same answer as\nthe model based on CAGE-reasoning 42% of the\ntime. This initially seemed low, but Turkers could\nonly arrive at the same answer as humans using\nonly CoS-E-open-ended 52% of the time\nFrom Table 7, we observed that CAGE-\nrationalization and CAGE-reasoning were often\nidentical or differed only in word ordering or\nby replacing one of the answer choices with an-\nother. Humans could predict the answer based\non just CAGE-rationalization 42% of the time,\nsame as CAGE-reasoning. Although CAGE-\nrationalizations seem to be better than CAGE-\nreasoning, we ﬁnd that it does not drastically im-\nprove the model’s language generating behavior\nwhich is what humans judge while trying to guess\nthe right answer without the actual question.\nEven though CoS-E and CAGE are noisy, they\nempirically perform well when used by down-\nstream models for CQA, but this is not the case for\nmisleading explanations. If we manually changed\na random sample of 50 examples to have adversar-\nial misleading explanations, performance dropped\nfrom 60% to 30%, well below the baseline of50%\nvalidation accuracy. For example, we changed the\nexplanation from “being able to use“ to “buying\nmore will alleviate stress“ for the question “If a\ncouple is having ﬁnancial issues, buying products\ncan lead to what“ with answer choices “economic\nboom”, “disagreements”, “being able to use”. Of\nthe 70% of the errors made by a model trained\non misleading explanations, 57% of them were\ninstead correctly answered by our model trained\nwith true CoS-E explanations. This demonstrates\nthe effectiveness of having well-informing expla-\nnations.\nCamburu et al. (2018) use human explanations\nto train a neural network model on the SNLI\ndataset (Bowman et al., 2015). However, they\nobtain explanations at the cost of accuracy. The\nauthors use the InferSent (Conneau et al., 2017)\nmodel for classiﬁcation and add a one-layer LSTM\nas the explanation decoder. They report a slight\ndrop in performance ( < 1%) when training on\nhuman explanations and testing by ﬁrst predict-\ning an answer and then generating explanations.\nThere is a further drop of approximately 2% ac-\ncuracy when their model generates explanations\nprior to predicting an answer based only on that\nexplanations. However, they also show that a\nbidirectional encoder with MLP-classiﬁer obtains\n96.83% accuracy when given only human expla-\nnations. CQA experiences a lift from explana-\ntions when e-SNLI performance appears to de-\ngrade with explanations. For CQA, humans are\nable to predict the right answer only about 52%\nof the time using only human explanations from\nCoS-E.\nOn the more challenging CQA v1.11, we ob-\nserved that our CoS-E model trained on human\nexplanations but evaluated without explanations\nobtains state-of-the-art performance, beating the\nBERT baseline by 1.5%. Surprisingly, we found\nthat our CAGE-reasoning model performs slightly\nworse than the baseline. However, during error\nanalysis we found that the language model expla-\nnations do not exhibit any obvious problems. Ta-\nble 8 shows some samples that CAGE predicts\nincorrectly. We observed that many of the in-\ncorrectly predicted instances had the correct an-\nswer in the generated explanation, such as “dresser\ndrawer” and “cleanness” in the ﬁrst two exam-\nples, but this information is not properly used by\nthe BERT classiﬁer. A more explicit method of\nguiding attention towards the relevant information\nin the explanations might be necessary for such\ncases. The model also frequently errs when the\nchoices seem semantically close such as “forest”\nand “compost pile” in the third example. In these\ncases, the classiﬁer often predicts the incorrect\nchoice on v1.11, but was able to predict the cor-\nrect choice on v1.0 when only 3 choices were pre-\nsented. This suggests that simply concatenating\nexplanations is unable to make sufﬁciently clear\nthe more difﬁcult cases of the newer version of\nCQA.\nTransferring the language model used to gener-\nate commonsense explanations to out-of-domain\ndatasets, SW AG and Story Cloze, led to slight\ndecrease in performance. Upon inspection, the\ngenerated explanations exhibited little grammati-\ncal or syntactical errors and often contained appar-\nently relevant information. Table 9 shows exam-\nples from both datasets and the corresponding gen-\n4940\nSW AG\nQuestion: Men are standing on motorbikes getting ready for a motocross competition.\nChoices: man places the ladders onto a fence and winds up a marching wall, high with hammer and a stone., man is talking to the camera and\nstanding on a podium., man stands outside in the ﬁeld going at arms of people and leading a long jumping calf in front., man drops\nthe javelin to the ground and jumps it very high.\nExplanation: man is talking to the camera and not the crowd.\nQuestion: The man examines the instrument in his hand.\nChoices: The person studies a picture of the man playing the violin., The person holds up the violin to his chin and gets ready., The person stops to\nspeak to the camera again., The person puts his arm around the man and backs away.\nExplanation: the person is holding the instrument in his hand.\nQuestion: The woman is seated facing the camera while another woman styles her hair.\nChoices: The woman in purple is wearing a blue dress and blue headband, using the pits to style her hair., The woman begins to cut the hair with her\nhair then serves it and begins brushing her hair and styling it., The woman puts some right braids on his., The woman continues to have\nher hair styled while turned away from the camera.\nExplanation: the woman is using the braids to trim her hair.\nStory Cloze (ROCStories)\nQuestion: My friends all love to go to the club to dance. They think it’s a lot of fun and always invite. I ﬁnally decided to tag\nalong last Saturday. I danced terribly and broke a friend’s toe.\nChoices: My friends decided to keep inviting me out as I am so much fun., The next weekend, I was asked to please stay home.\nExplanation: the next weekend, i would be asked to stay home\nQuestion: Ari spends $20 a day on pickles. He decides to make his own to save money. He puts the pickles in brine. Ari waits 2 weeks for his pickles\nto get sour.\nChoices: Ari opens the jar to ﬁnd perfect pickles., Ari’s pickles are sweet.\nExplanation: pickles are the only thing that can be found in a jar.\nQuestion: Gina sat on her grandpa’s bed staring outside. It was winter and his garden was dead until spring. Her grandpa had passed away so there\nwould be no one to tend it. The weeds would take over and strangle the ﬂowers.\nChoices: Gina asked her grandpa what kind of ﬂowers he liked best., Gina decided to go outside and pick some of the weeds.\nExplanation: the weeds would take over and strangle the ﬂowers.\nTable 9: Random sample of explanations generated by the language model ﬁne-tuned on CQA and transferred\nwithout further training to SW AG and Story Cloze. Bold indicates ground-truth.\nerated explanations. In the SW AG dataset, each\nquestion is a video caption from activity recogni-\ntion videos with choices about what might happen\nnext and the correct answer is the video caption of\nthe next scene. Generated explanations for SW AG\nappear to be grounded in the given images even\nthough the language model was not at all trained\non SW AG. Similarly, we found that for the Story\nCloze dataset, the explanations had information\npointing to the correct ending. Nonetheless, the\nclassiﬁer was unable to make use of this informa-\ntion to improve performance.\n7 Conclusion and Future Work\nWe introduced the Common Sense Explanations\n(CoS-E) dataset built on top of the existing Com-\nmonsenseQA dataset. We also proposed the\nnovel Commonsense Auto-Generated Explana-\ntions (CAGE) framework that trains a language\nmodel to generate useful explanations when ﬁne-\ntuned on the problem input and human explana-\ntions These explanations can then be used by a\nclassiﬁer model to make predictions. We empir-\nically show that such an approach not only results\nin state-of-the-art performance on a difﬁcult com-\nmonsense reasoning task, but also opens further\navenues for studying explanation as it relates to\ninterpretable commonsense reasoning. We also\nperformed comprehensive error analyses of lan-\nguage model explanations and evaluated explana-\ntion transfer to out-of-domain datasets.\nWhile CAGE focuses on generating explana-\ntions prior to predicting an answer, language mod-\nels for explanation might also be jointly trained to\npredict the answer. They might also be extended to\na broader set of tasks. With a sufﬁcient dataset of\nexplanations (analogous to CoS-E) for many tasks,\nit might be possible to ﬁne-tune a more general\nexplanatory language model that generates more\nuseful explanations for unseen tasks.\nWith deferral of explanation to neural models,\nit will be crucial in the future to study the ethical\nimplications of biases that are accumulated dur-\ning pretraining or ﬁne-tuning. Explanations must\nbe carefully monitored to ensure that they do not\nreinforce negative or otherwise harmful reasoning\nthat might then propagate into downstream mod-\nels. For example, in CQA we observed signiﬁcant\ngender disparity and bias with higher proportion of\nfemale pronouns used in negative contexts. This\nkind of bias has inevitably propagated into CoS-\nE and advise these datasets and trained models be\nused with that in mind.\nAcknowledgements\nWe would like to thank Melvin Gruesbeck for the\nillustration of CAGE in Figure 1. We also thank\nthe anonymous reviewers for their feedback.\n4941\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual Question An-\nswering. In International Conference on Computer\nVision (ICCV).\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP2015), pages 632–642.\nOana-Maria Camburu, Tim Rockt ¨aschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-SNLI:\nNatural Language Inference with Natural Language\nExplanations. In Advances in Neural Information\nProcessing Systems (NeurIPS2018), pages 9560–\n9572.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing (EMNLP2017), pages\n670–680.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Proceedings of the 28th\nInternational Conference on Neural Information\nProcessing Systems (NIPS2015), pages 3079–3087.\nMIT Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nBraden Hancock, Paroma Varma, Stephanie Wang,\nMartin Bringmann, Percy Liang, and Christopher\nR´e. 2018. Training classiﬁers with natural language\nexplanations. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL2018), volume 1, pages 1884–1895.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL2018),\npages 328–339.\nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\nRationalizing neural predictions. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing (EMNLP2016), pages\n107–117.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nThirteenth International Conference on the Princi-\nples of Knowledge Representation and Reasoning.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A SICK cure for the evaluation of\ncompositional distributional semantic models. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 216–223, Reykjavik, Iceland. European Lan-\nguage Resources Association (ELRA).\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL2016), pages 839–\n849, San Diego, California. Association for Compu-\ntational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual meeting on Association for Computa-\ntional Linguistics (ACL2002), pages 311–318. As-\nsociation for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP2014), pages 1532–1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving lan-\nguage understanding by generative pre-training.\nhttps://s3-us-west-2.amazonaws.com/openai-assets/\nresearch-covers/language-unsupervised/\nlanguage understanding paper.pdf.\n4942\nNazneen Fatema Rajani and Raymond Mooney. 2018.\nStacking with auxiliary features for visual question\nanswering. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), volume 1,\npages 2217–2226.\nNazneen Fatema Rajani and Raymond J. Mooney.\n2017. Ensembling visual explanations for vqa.\nIn Proceedings of the NIPS 2017 workshop\non Visually-Grounded Interaction and Language\n(ViGIL).\nMin Joon Seo, Aniruddha Kembhavi, Ali Farhadi,\nand Hannaneh Hajishirzi. 2017. Bidirectional at-\ntention ﬂow for machine comprehension. CoRR,\nabs/1611.01603.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NIPS2017), pages 5998–6008.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nTerry Winograd. 1972. Understanding natural lan-\nguage. Cognitive psychology, 3(1):1–191.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP2018),\npages 93–104.\nWanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou,\nJiahai Wang, and Jian Yin. 2018. Improving ques-\ntion answering by commonsense-based pre-training.\narXiv preprint arXiv:1809.03568."
}