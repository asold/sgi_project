{
  "title": "Semantic Labeling Using a Deep Contextualized Language Model",
  "url": "https://openalex.org/W3095464614",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2556203398",
      "name": "Trabelsi Mohamed",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103442353",
      "name": "Cao Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222944261",
      "name": "Heflin, Jeff",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2951621897",
    "https://openalex.org/W2407487288",
    "https://openalex.org/W2923890923",
    "https://openalex.org/W2740438624",
    "https://openalex.org/W2106895292",
    "https://openalex.org/W2982596739",
    "https://openalex.org/W2255747889",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2045638068",
    "https://openalex.org/W2948012237",
    "https://openalex.org/W2945465473",
    "https://openalex.org/W2108223890",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2898796029",
    "https://openalex.org/W2094728533",
    "https://openalex.org/W1973435495",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2042389627",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2084988263",
    "https://openalex.org/W2022166150",
    "https://openalex.org/W2795302121",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2017977879",
    "https://openalex.org/W2945127593",
    "https://openalex.org/W102708294",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1501251778",
    "https://openalex.org/W2522154031",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2991170427",
    "https://openalex.org/W2798991201",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2970103342",
    "https://openalex.org/W2132862423",
    "https://openalex.org/W196214544",
    "https://openalex.org/W3099965312",
    "https://openalex.org/W1532325895",
    "https://openalex.org/W2008896880",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W3004169498",
    "https://openalex.org/W2398606196"
  ],
  "abstract": "Generating schema labels automatically for column values of data tables has many data science applications such as schema matching, and data discovery and linking. For example, automatically extracted tables with missing headers can be filled by the predicted schema labels which significantly minimizes human effort. Furthermore, the predicted labels can reduce the impact of inconsistent names across multiple data tables. Understanding the connection between column values and contextual information is an important yet neglected aspect as previously proposed methods treat each column independently. In this paper, we propose a context-aware semantic labeling method using both the column values and context. Our new method is based on a new setting for semantic labeling, where we sequentially predict labels for an input table with missing headers. We incorporate both the values and context of each data column using the pre-trained contextualized language model, BERT, that has achieved significant improvements in multiple natural language processing tasks. To our knowledge, we are the first to successfully apply BERT to solve the semantic labeling task. We evaluate our approach using two real-world datasets from different domains, and we demonstrate substantial improvements in terms of evaluation metrics over state-of-the-art feature-based methods.",
  "full_text": "Semantic Labeling Using a Deep Contextualized Language\nModel\nMohamed Trabelsi\nmot218@lehigh.edu\nLehigh University\nBethlehem, PA, USA\nJin Cao\njin.cao@nokia-bell-labs.com\nNokia Bell Labs\nMurray Hill, NJ, USA\nJeff Heflin\nheflin@cse.lehigh.edu\nLehigh University\nBethlehem, PA, USA\nABSTRACT\nGenerating schema labels automatically for column values of data\ntables has many data science applications such as schema match-\ning, and data discovery and linking. For example, automatically\nextracted tables with missing headers can be filled by the predicted\nschema labels which significantly minimizes human effort. Further-\nmore, the predicted labels can reduce the impact of inconsistent\nnames across multiple data tables. Understanding the connection\nbetween column values and contextual information is an important\nyet neglected aspect as previously proposed methods treat each\ncolumn independently. In this paper, we propose a context-aware\nsemantic labeling method using both the column values and context.\nOur new method is based on a new setting for semantic labeling,\nwhere we sequentially predict labels for an input table with miss-\ning headers. We incorporate both the values and context of each\ndata column using the pre-trained contextualized language model,\nBERT, that has achieved significant improvements in multiple nat-\nural language processing tasks. To our knowledge, we are the first\nto successfully apply BERT to solve the semantic labeling task. We\nevaluate our approach using two real-world datasets from different\ndomains, and we demonstrate substantial improvements in terms\nof evaluation metrics over state-of-the-art feature-based methods.\nCCS CONCEPTS\nâ€¢ Information systems â†’Data mining; â€¢ Computing method-\nologies â†’Artificial Intelligence; Knowledge representation and\nreasoning.\nKEYWORDS\nsemantic labeling; pretrained language model; data table\n1 INTRODUCTION\nIn this era of Big Data, various datasets are publicly available for\nusers to explore vast amounts of information in multiple fields.\nAmong all types of publicly available datasets, data tables represent\nthe most prevalent form of data. A data table has multiple rows\nand columns. Each column can be seen as a variable described by a\nschema label in order to distinguish between the variables. Some\nof these data tables are pre-processed, for example, those found in\nrepositories such as UCI machine learning repository1, kaggle2, and\nOpenML [42]. Governments also store their data in a tabular for-\nmat, like data.gov3. In addition to that, vast amounts of information\nthat are related to scientific, political, and cultural topics, are found\non the Web. Some others require extraction such as those HTML\nâˆ—Mohamed Trabelsiâ€™s work was done during a summer internship in Nokia Bell Labs.\n1http://archive.ics.uci.edu/ml/index.php\n2https://www.kaggle.com/\n3https://www.data.gov/\ntables that are embedded in web pages and spreadsheet files. For\npre-processed tables, data providers describe their data tables using\nmetadata, and headerâ€™s names that semantically describe columns\nvalues. However, a large number of data tables do not follow meta-\ndata standards and naming standards for schema labels which leads\nto less informative columnâ€™s labels [ 10, 17, 18, 27]. For example,\nthe date of birth of a person is saved as Date of Birth in some data\ntables, and DOB in others. On the other hand, the extracted tables\ncan have missing or wrong headers names, as a result of automatic\ntable extraction.\nGiven an unseen data table, our objective is to generate a schema\nlabel for each column from a set of labels. Schema labels of datasets\nare used in multiple tasks such as data discovery [ 6, 7], schema\nmatching [32, 49] and data preparation and analysis [33]. Existing\nmethods generate schema labels solely on the basis of their content\nor data values, and thus ignore the contextual information of each\ncolumn when predicting schema labels. For example, both columns\nwith labels nationality and location can contain data values from\nthe class country, but the context of these two columns within the\ndata table, such as other columns in the data table, has the potential\nto solve the ambiguity when inferring the label. In addition to that,\nprior approaches [10, 16, 27] define a set of hand-crafted features\nfor each column using data values. These methods require a fea-\nture engineering phase to define, extract and validate a predefined\nset of specific features for the schema label generation task. The\nfeatures are then used to train a traditional supervised machine\nlearning algorithm or deep neural network architectures in order\nto predict the label of previously-unseen data values. In conclusion,\nmany prior methods decouple the feature extraction and model\nbuilding steps and require significant human effort to validate both\nphases. Other approaches [8, 9] integrate external knowledge bases\n(KB) to predict semantic labels, with a strong assumption that the\nvocabulary of data tables match the KB entities.\nIn order to overcome the limitations of prior methods, we pro-\npose a new context-aware semantic labeling method that incorpo-\nrates both data values and columnâ€™s context in order to infer the\nlabel. Our method presents a new setting for generating schema\nlabels in which the input is a table with missing schema labels or\nheaders, instead of the traditional setting that treats each column\nseparately. The overview of the framework, that is used in our\nmethod, is described in Figure 1. Given a previously-unseen table\nwith missing headers, we sequentially predict schema labels, and\nincorporate the already-predicted labels as context for next header\nprediction within the same table.\nDeep contextualized language models, like BERT [14] and Ro-\nBERTa [25], have been recently proposed to solve multiple nat-\nural language understanding [ 24, 45] and information retrieval\narXiv:2010.16037v1  [cs.LG]  30 Oct 2020\nMohamed Trabelsi, Jin Cao, and Jeff Heflin\nA1 A2 A3 A4 A5 A6 A7\nCristiano Ronaldo 02/05/1985 Portugal Juventus 29 forward 7\nLionel Messi 06/24/1987 Argentina Barcelona 34 forward 10\nSergio Ramos 03/30/1986 Spain Real Madrid 21 defense 4\nModelContextual \ninput\nA2= birth date\nA4= team\nA1= player\nA3= nationality\nA6= position\nA7= shirt number\nA5= Nb trophies\n(a) Predicting headers for a soccer players table with true labels player,\nbirth date, nationality, team, Nb trophies, position, shirt number\nA1 A2 A3 A4 A5 A6\nLewis Hamilton Mercedes Australian GP Australia 03/17/2019 2\nSebastian Vettel Ferrari Chinese GP China 04/14/2019 3\nLando Norris McLaren Monaco GP France 05/26/2019 11\nContextual \ninput Model\nA2= car\nA5= start date\nA3= tournament\nA1= driver\nA4= location\nA6= ranking\n(b) Predicting headers for a Formula 1 table with true labels driver, car,\ntournament, location, start date, ranking\nFigure 1: The general framework of our method. The contextual input block extracts a contextual representation for each\ncolumn using data values from all columns in a given table. The model block maps the contextual input of each column into\na probability distribution to predict the schema label of each column. The contextual input solves ambiguity of predictions.\nFor example, ğ´3 from Table (a), and ğ´4 from Table (b) have data values related to class country. The context of ğ´3 using other\nheaders, such as player, team, position, etc, and the context of ğ´4 that has driver, car, tournament, etc, can guide the model to\npredict nationality and location for ğ´3 (Table (a)) and ğ´4 (Table (b)), respectively.\n[11, 36, 47] tasks. Different from traditional word embeddings, the\npre-trained neural language models are contextual with the rep-\nresentation of a token is a function of the entire sentence. This is\nmainly achieved by the use of a self-attention structure called trans-\nformer [43]. Here, we integrate BERT into our proposed method,\ndenoted by SeLaB (Semantic Labeling with BERT), to solve the\nschema generation task. We train a single BERT model that makes\nan initial prediction for the columnâ€™s label using only data values,\nand then updates its prediction by incorporating both data values\nand predicted contexts of the column. SeLaB is trained end-to-end\nfor feature extraction and model building, which reduces the signif-\nicant human effort that is needed in prior methods, and gives the\nmodel the ability to capture specific features that are better than\nthe hand-crafted ones for semantic labeling. In addition to that, by\nincorporating the context, we are able to predict labels in a richer\nand more fine-grained set of vocabulary unlike the limited classes\nthat are used to describe the semantic labels. SeLaB doesnâ€™t assume\nthat the column values match an existing KB, and therefore SeLaB\ngeneralizes to table collections from multiple domains.\nIn summary, we make the following contributions:\nâ€¢We propose a new context-aware semantic labeling approach.\nOur method presents a new setting in which the input to\nour model is a data table with missing headers, and we se-\nquentially generate schema labels for each data table.\nâ€¢We demonstrate that by incorporating the predicted contexts\nof an attribute into the model, we can more accurately infer\nits context-aware schema label.\nâ€¢We are the first to integrate BERT into the semantic labeling\ntask. In particular, we incorporate data values and predicted\ncontexts using BERT, which is trained end-to-end for feature\nextraction and label prediction. This reduces human effort\nin the semantic labeling.\nâ€¢We experiment on two datasets (public and internal data\ntable corpus), and demonstrate that our new method outper-\nforms the state-of-the-art baselines, and generalizes to table\ncollections from multiple domains.\n2 RELATED WORK\n2.1 Semantic Labeling\nExisting approaches [16, 35, 41] in the semantic labeling consist of\nclassifying data values into a predefined set or categories known as\nsemantic labels. These approaches rely on a multiclass classification\nsetup where the labels are manually defined and curated. Commer-\ncial tools, like tableau4 and Trifacta5, are proposed for semantic\ntype detection. Only a limited set of semantic types are predicted\nusing these tools. Hulsebos et al. [ 16] extend the set of semantic\ntypes by considering 275 DBpedia [1] properties. These manually\ndefined concepts, like Birth place , Continent, and Product, represent\nthe semantic types that are frequently found in datasets. In order\nto infer the semantic type of a column using data values, the au-\nthors define four categories of features which are: global statistics,\ncharacter distributions, pretrained word embedding, and trained\nparagraph embedding. Each feature category has different perfor-\nmance and noise level, so that the authors propose a multi-input\nneural networks model, instead of simply concatenating all features,\nand feeding the resulting feature vector to a single-input neural\nnetwork. The multi-input neural networks model is composed of\nmultiple identical subnetworks without weights sharing. Each sub-\nnetwork consists of two fully connected hidden layers with batch\nnormalization, rectified linear unit (ReLU) activation functions, and\ndropout. Knowledge Base based methods [8, 9] integrate DBpedia\n[1] to predict semantic labels, where entities from DBpedia that\nmatch all the column cells are used as additional information for a\ngiven column values.\nSemantic types use a limited set of vocabulary, and can restrict\nthe number of categories that can be considered when inferring the\nlabel of a given column. In practice, the predefined set of semantic\ntypes may not apply for new datasets. Chen et al. [10] proposed a\nschema label generation task, in which the objective is to infer the\nschema label , and not only the semantic type. This setting can be\n4https://www.tableau.com/\n5https://www.trifacta.com/\nSemantic Labeling Using a Deep Contextualized Language Model\nseen as a multiclass classification task, where each columnâ€™s label\nin the training set represents a possible semantic label. Generating\nschema labels is more challenging as the number of possible la-\nbels is large compared to the predefined set of semantic types. The\nauthors extract hand-crafted features from data values to predict\nschema labels. The set of features include content and unique con-\ntent ratio [15], and the content histogram which is a 20-dimensional\nvector extracted using fast Fourier transform (FFT). Random forest\nclassifier is used to predict schema labels from the curated features.\nSchema matching is related to semantic type detection where the\nobjective is to find correspondence between attributes in different\nschemas. Existing data on the Web, such as WebTables [ 5], and\nknowledge bases, such as DBPedia [1] and Freebase [4], are used\nin schema matching. Syed et al. [40] use headers and data values\nto predict the class of a column in the target ontology or knowl-\nedge base. The data values provide additional information that can\ndisambiguate between the possible candidates. Limaye et al. [22]\nassociate one or more types from YAGO [37] with each attribute or\ncolumn in the table using a probabilistic graphical model. Another\nprobabilistic approach, that is based on the maximum likelihood\nhypothesis, is introduced by Venetis et al. [ 44]. The best label is\nchosen to maximize the probability of the values given the class\nlabel for a given column. The authors showed that class labels that\nare automatically extracted from the web provide more coverage\nfor columnâ€™s labeling than using manually created knowledge bases\nlike YAGO [37] and Freebase [4].\nMatching functions are used to infer the correct semantic la-\nbels for data values. Pham et al. [ 31] solve the semantic labeling\nas a combination of many binary classification problems. After\nextracting similarity metrics features from a pair of attributes, each\nfeature vector is given a True/False label, where True means that\nthe attributes have the same semantic type, and False indicates\nthat the attributes are not sharing the same semantic type. Logistic\nRegression and Random Forests are used to predict the matching\nscore. For the similarity metrics features, the authors investigated\nmultiple metrics, such as Jaccard similarity [26], cosine similarity\nof the product of term frequency (TF) and inverse document fre-\nquency (IDF), known as TF-IDF [ 26] , Kolmogorov-Smirnov test\n(KS test) [21], and Mann-Whitney test (MW test) [21]. Mueller and\nSmola [27] proposed a neural network embeddings for data values\nto predict the matching score of two sets of data values. The match-\ning score is estimated using the distance between the embeddings\nof two sets of data values. The score is adjusted using the output\nof another neural network to distinguish two columns that are\ndifferent but their data values are identically distributed.\nSemantic types prediction is formalized as a ranking problem\nin the approach proposed by Ramnandan et al. [34]. Training data\nvalues are considered as documents, and the previously-unseen\ndata values are considered as queries. So, in the prediction phase,\nthe objective is to extract the top ğ‘˜ candidate semantic labels for\nthe new data values by ranking semantic labels in decreasing order\nusing cosine similarity between query feature and every document\nfeature in training data. The authors distinguished between textual\nand numeric data. For textual data, the feature vector is a weighted\nbag of words with TF-IDF. For numerical data, the authors used\na statistical hypothesis testing to analyze the distribution of nu-\nmerical data values that corresponds to a given semantic label. The\nstatistical hypothesis test is performed between each sample in\nthe training data and the testing sample. The returned p-values\nare then ranked in descending order to predict the top ğ‘˜ candidate\nsemantic labels for the testing data values.\nOur proposed method is based on the multiclass classification\nsetting because the schema labels are easily collected from data table\ncorpus, unlike matching based strategy that requires additional\nhuman effort to define pairs of attributes that have similar semantic\ntype.\n2.2 BERT\nBERT [14] is a deep contextualized language model that contains\nmultiple layers of transformer [43] blocks. Each transformer block\nhas a multi-head self-attention structure followed by a feed-forward\nnetwork, and it outputs contextualized embeddings or hidden states\nfor each token in the input. BERT is trained on unlabeled data over\ntwo pre-training tasks which are the masked language model, and\nnext sentence prediction. Then, BERT can be used for downstream\ntasks on single text or text pairs using special tokens ([SEP] and\n[CLS]) that are added into the input. For single text classification,\nBERT special tokens, [CLS] and [SEP], are added to the beginning\nand the end of the input sequence, respectively. For applications that\ninvolve text pairs, BERT encodes the text pairs with bidirectional\ncross attention between the two sentences. In this case, the text\npair is concatenated using [SEP], and then treated by BERT as a\nsingle text.\nThe sentence pair classification setting is used to solve mul-\ntiple tasks in information retrieval including document retrieval\n[13, 29, 46], frequently asked question retrieval [ 36], passage re-\nranking [28], and table retrieval [11]. The single sentence setting is\nused for text classification [38, 48]. BERT takes the final hidden state\nhğœƒ of the first token [CLS] as the representation of the whole input\nsequence, where ğœƒ denotes the parameters of BERT. Then, a simple\nsoftmax layer, with parametersğ‘Š, is added on top of BERT to pre-\ndict the probability of a given label ğ‘™: ğ‘(ğ‘™ |hğœƒ)= softmax(ğ‘Šhğœƒ).\nThe parameters of BERT, denoted by ğœƒ, and the softmax layer pa-\nrameters ğ‘Š are fine-tuned by maximizing the log-probability of\nthe true label.\nThe organization of the rest of the paper is as follows. Section\n3 formalizes the semantic labeling as a multiclass classification\nproblem; Section 4 proposes a new context-aware semantic labeling\napproach that is based on the deep contextualized language model,\nBERT; and Section 5 illustrates data table collections that are used\nin our approach, and compares baselines and our algorithm in the\nsemantic labeling task.\n3 PROBLEM STATEMENT\nOur goal is to generate schema labels or semantic types for ta-\nbles columns using data values, and predicted contexts in order\nto resolve the ambiguity problem in the prediction phase. As we\nmentioned before, we use the multiclass classification setting to\nsolve schema labeling. The training data consists of a table corpus\nT= {ğ‘‡1,ğ‘‡2,...,ğ‘‡ ğ‘›}, with ğ‘›is the total number of data tables. Each\ntable ğ‘‡ğ‘˜ has a set of ğ‘šcolumns ğ´1,ğ´2,...,ğ´ ğ‘š, where each column\nğ´ğ‘– has a schema label ğ‘™ğ‘– (columnâ€™s header), and a set of data values\nğ‘‰ğ‘– = {ğ‘£1,ğ‘£2,...,ğ‘£ ğ‘Ÿ}, where ğ‘Ÿ is the number of rows in ğ‘‡ğ‘˜. The set\nMohamed Trabelsi, Jin Cao, and Jeff Heflin\nof all possible schema labels is denoted by ğ¿. Resolving ambiguity\nwhen predicting schema labels requires the whole table as input\nto the model, instead of only using independent columnâ€™s values.\nTherefore, our setting consists of table inputs that have missing\nheaders, and our objective is to predict schema labels for all columns\nof the input table.\nWe denote our proposed model byğ‘€ = ğ‘â—¦ğ¹, withğ¹is the feature\nextractor function (Contextual input block in Figure 1), andğ‘ is the\nclassification layer (Model block in Figure 1). The input to ğ‘€ is a\ntable ğ‘‡ğ‘˜ with missing schema labels, and the output of our model is\na sequence of predicted schema labels Ë†ğ´1, Ë†ğ´2,..., Ë†ğ´ğ‘š. Our method\nlearns both features and model simultaneously leading to significant\nreduction in humanâ€™s effort spent in the feature engineering phase.\n4 CONTEXT PREDICTION FOR SEMANTIC\nLABELING\nIn this section, we introduce our context-aware method for schema\nlabel generation. We formally define the contextual information\nof each column, which is combined with columnâ€™s data values to\nimprove the performance of semantic labeling.\n4.1 Columnâ€™s context\nThe set of data values ğ‘‰ğ‘– for a given column ğ´ğ‘– in a table ğ‘‡ğ‘˜ are not\nsufficient to have accurate schema label prediction. For example in\nFigure 1, both columns nationality (A3 in the left table) andlocation\n(A4 in the right table) contain values from class country, but they\nrefer to different labels. In this case, if we know that ğ´3 (in the\nleft table) occurs in a table that contains player, team, position, and\nbirth date attributes, hence it is more probable that ğ´3 is related\nto nationality rather than location. Therefore, we argue that the\nattributes ğ´1,ğ´2,...,ğ´ ğ‘–âˆ’1,ğ´ğ‘–+1,...,ğ´ ğ‘š provides a rich contextual\ninformation for ğ´ğ‘–. However, as we explained in our setting, the\ninput to our model is a table that has missing headers, which means\nthat we cannot directly incorporate the context into our model.\nTo solve that, we propose incorporating predicted context instead\nof the ground truth context. In other words, our model has two\npasses for predicting schema labels. During the first pass, given a\ntable ğ‘‡ğ‘˜ with missing headers, only data values are used to make\ninitial predictions for semantic labels, denoted by ğ´â€²\n1,ğ´â€²\n2,...,ğ´ â€²ğ‘š.\nThe initial predictions are context-free, as they only capture data\nvalues. For the second pass, we incorporate both data valuesğ‘‰ğ‘–, and\nthe predicted context ğ´â€²\n1,ğ´â€²\n2,...,ğ´ â€²\nğ‘–âˆ’1,ğ´â€²\nğ‘–+1,...,ğ´ â€²ğ‘š of ğ´ğ‘– to make\nthe final context-aware prediction, denoted by Ë†ğ´ğ‘–.\n4.2 Semantic labeling with BERT (SeLaB)\nWe incorporate data values and predicted contexts of a given at-\ntribute using the contextualized language model BERT. So, for our\nproposed model ğ‘€ = ğ‘ â—¦ğ¹, denoted by SeLaB, ğ¹ is equivalent to\nBERT with parameters ğœƒ, as we use the hidden state of [CLS] token\nfrom the last transformer block to compute the embedding of the\ninput sentence. ğ‘ denotes the softmax layer with parameters ğ‘Š\nthat is used to produce the probability distribution of a given se-\nquence over all schema labels from ğ¿. The general form of input to\nğ‘€ for an attribute ğ´ğ‘–, denoted by contextual input , is the sequence\n[CLS]+ğ‘‰ğ‘–+[SEP]+ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)+[SEP], where ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)is the pre-\ndicted context of ğ´ğ‘–. For first pass prediction, where ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)is\nAlgorithm 1 Training phase\n1: Input: tables collection T={ğ‘‡1,ğ‘‡2,...,ğ‘‡ ğ‘›}, set of labels ğ¿.\n2: for ğ‘‡ğ‘˜ in Tdo\n3: the schema labels of ğ‘‡ğ‘˜, ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘š, are available\n4: % First phase: Values based prediction phase\n5: for [ğ´ğ‘–,ğ‘‰ğ‘–]in ğ‘‡ğ‘˜ do\n6: input to BERT for ğ´ğ‘–: ğ¼1 (ğ´ğ‘–)= [ğ¶ğ¿ğ‘†]+ğ‘‰ğ‘– +[ğ‘†ğ¸ğ‘ƒ]+[ ğ‘†ğ¸ğ‘ƒ]\n7: compute values based probability, ğ‘â€²\nğ‘– = ğ‘€(ğ¼1 (ğ´ğ‘–))\n8: ğ´â€²\nğ‘– = argmaxğ‘™âˆˆğ¿ğ‘â€²\nğ‘–[ğ‘™]\n9: end for\n10: % Second phase: Compute contexts of each attribute\n11: for [ğ´ğ‘–,_]in ğ‘‡ğ‘˜ do\n12: Context of ğ´ğ‘–: ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–) = ğ´â€²\n1 +ğ´â€²\n2 +... +ğ´â€²\nğ‘–âˆ’1 +\nğ´â€²\nğ‘–+1 +... +ğ´â€²ğ‘š\n13: avoid true label leakage in context: remove ğ‘™ğ‘– from\nğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)if ğ‘™ğ‘– âˆˆğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)\n14: remove duplicates from ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)\n15: end for\n16: % Third phase: compute final predictions\n17: for [ğ´ğ‘–,ğ‘‰ğ‘–]in ğ‘‡ğ‘˜ do\n18: input to BERT for ğ´ğ‘–: ğ¼2 (ğ´ğ‘–) = [ğ¶ğ¿ğ‘†] +ğ‘‰ğ‘– + [ğ‘†ğ¸ğ‘ƒ] +\nğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)+[ ğ‘†ğ¸ğ‘ƒ]\n19: compute context-aware probability, Ë†ğ‘ğ‘– = ğ‘€(ğ¼2 (ğ´ğ‘–))\n20: Ë†ğ´ğ‘– = argmaxğ‘™âˆˆğ¿ Ë†ğ‘ğ‘–[ğ‘™]\n21: end for\n22: ğ‘™ğ‘œğ‘ ğ‘ (ğ‘‡ğ‘˜)=ğ¶ğ‘Ÿğ‘œğ‘ ğ‘ ğ¸ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ ([Ë†ğ‘1, Ë†ğ‘2,..., Ë†ğ‘ğ‘š], [ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘š])\n23: update ğ‘€parameters (ğœƒ,ğ‘Š )by minimizing ğ‘™ğ‘œğ‘ ğ‘ (ğ‘‡ğ‘˜)\n24: end for\n25: Output: A Trained context-aware model ğ‘€\nmissing, the input sequence form, denoted by only values , becomes\n[CLS]+ğ‘‰ğ‘–+[SEP]+[SEP]. Next, we describe the training and testing\nphases.\n4.2.1 Training phase. The steps of training phase are shown in\nAlgorithm 1. The inputs to training phase are: table corpus T=\n{ğ‘‡1,ğ‘‡2,...,ğ‘‡ ğ‘›}where semantic labels ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘š are available for\nall attributes ğ´1,ğ´2,...,ğ´ ğ‘š of a given table ğ‘‡ğ‘˜ âˆˆT, set of possible\nsemantic labelsğ¿, and pre-trained BERT model as a feature extractor\nğ¹. The compact notation of table ğ‘‡ğ‘˜, that is used in algorithms, is\nğ‘‡ğ‘˜ = [[ğ´1,ğ‘‰1],[ğ´2,ğ‘‰2],..., [ğ´ğ‘š,ğ‘‰ğ‘š]].\nThe training process has three phases. The first phase consists of\npredicting an initial label for each column using only values input\nform as shown in Lines 4â€“9 of Algorithm 1. The output of the first\nphase is a sequenceğ´â€²\n1,ğ´â€²\n2,...,ğ´ â€²ğ‘šof initial predicted labels. During\nthe second phase (Lines 10â€“15), we construct the predicted context\nğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)for each attribute ğ´ğ‘–, which is the set of predicted\nlabels {ğ´â€²\nğ‘—; ğ‘— âˆˆ[1,ğ‘š]\\{ğ‘–}}. In order to avoid the true label leakage\nin ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–), we remove ğ‘™ğ‘– from ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)if ğ‘™ğ‘– âˆˆğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–).\nWe also remove duplicates fromğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)as most of data tables\ncontain unique headers. The final phase (Lines 16â€“21) computes\nthe context-aware predictions by using contextual input form. The\noutput of ğ‘€ is the probability distribution Ë†ğ‘ğ‘– over all labels in\nğ¿, for every ğ´ğ‘– âˆˆğ‘‡ğ‘˜. These probability distributions are used to\ncalculate the cross entropy loss, and to update the parameters of ğ‘€\nas indicated in Lines 22â€“23. In addition to incorporating the context\nof column for schema labeling, our model has the ability to accept\ntwo forms of sequence inputs (only values and contextual input ),\nSemantic Labeling Using a Deep Contextualized Language Model\nwhich significantly reduces the number of parameters compared to\nthe case where a separate model is needed to handle each type of\ninput sequence.\nIn contrast to [31, 34] which have a pre-processing step to dis-\ntinguish between string and numerical attributes, our BERT-based\nfeature extractor ğ¹ is able to process string and numerical texts by\ntaking advantage of BERT tokenizers. In contrast to [10, 16] where\nthe feature extraction and model building steps are decoupled, our\nmodel ğ‘€ = ğ‘ â—¦ğ¹ is trained end-to-end to jointly optimize the\nfeature extractor ğ¹, and the classification layer ğ‘. Unlike [8, 9] that\nintegrate external KB in semantic labeling with a strong assump-\ntion that the columnâ€™s values match the KB entities, SeLaB needs\nonly BERT embeddings that is fine-tuned on target table corpus\nto extract the feature of each column, and therefore generalizes to\ndata tables from multiple domains. We train SeLaB for ğ¸epochs.\nAlgorithm 2 Testing phase\n1: Input: testing table ğ‘‡ğ‘˜, set of headers labels ğ¿, trained ğ‘€ model,\nBoolean: ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’_â„ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  and ğ‘¡ğ‘œğ‘ğ‘˜.\n2: First phase: Values based prediction phase\n3: Second phase: Compute contexts of each attribute\n4: % Third phase: compute final predictions\n5: ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  = âˆ…, ğ‘ ğ‘’ğ‘’ğ‘›_ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘  = âˆ…\n6: for ğ‘–ğ‘¡ in [1,ğ‘š] do\n7: for [ğ´ğ‘–,ğ‘‰ğ‘–]in ğ‘‡ğ‘˜ do\n8: input to BERT for ğ´ğ‘–: ğ¼2 (ğ´ğ‘–) = [ğ¶ğ¿ğ‘†] +ğ‘‰ğ‘– + [ğ‘†ğ¸ğ‘ƒ] +\nğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–)+[ ğ‘†ğ¸ğ‘ƒ]\n9: compute context-aware probability, Ë†ğ‘ğ‘– = ğ‘€(ğ¼2 (ğ´ğ‘–))\n10: Ë†ğ´ğ‘– = argmaxğ‘™âˆˆğ¿ Ë†ğ‘ğ‘–[ğ‘™], ğ‘ğ‘–ğ‘šğ‘ğ‘¥ = maxğ‘™âˆˆğ¿ Ë†ğ‘ğ‘–[ğ‘™]\n11: end for\n12: if ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’_â„ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  then\n13: â„,ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ = ğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  ({Ë†ğ‘1, Ë†ğ‘2,..., Ë†ğ‘ğ‘šâˆ’ğ‘–ğ‘¡+1 },\nğ‘¡ğ‘œğ‘ğ‘˜,ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  )\n14: Ë†ğ´â„ = ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™\n15: else\n16: â„= argmaxğ‘ âˆˆ[1,ğ‘šâˆ’ğ‘–ğ‘¡+1]ğ‘ğ‘ ğ‘šğ‘ğ‘¥\n17: end if\n18: ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (Ë†ğ´â„),\n19: ğ‘ ğ‘’ğ‘’ğ‘›_ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ (â„)\n20: ğ‘‡ğ‘˜.ğ‘‘ğ‘’ğ‘™ğ‘’ğ‘¡ğ‘’ ([ğ´â„,ğ‘‰â„])\n21: update contexts with new predicted attribute Ë†ğ´â„: replace ğ´â€²\nâ„ by Ë†ğ´â„\nin ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–), ğ‘– âˆˆ[1,ğ‘š]\\{ ğ‘ ğ‘’ğ‘’ğ‘›_ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘ }\n22: end for\n23: Output: A label for each header in the testing table.\n4.2.2 Testing phase. The steps of the testing phase are shown in\nAlgorithm 2. The inputs to the testing phase are: a testing table\nğ‘‡ğ‘˜ that has missing headers (ğ‘™1,ğ‘™2,...,ğ‘™ ğ‘š, are not available), set of\npossible semantic labels ğ¿, trained model ğ‘€, and two parameters\nğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’_â„ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  and ğ‘¡ğ‘œğ‘ğ‘˜ that we will describe later.\nThe testing process has three phases. The first and second phases\n(Lines 2â€“3) are similar to the training process, where initial pre-\ndictions are computed using only values input form, and then\nused to produce the context of each attribute. During the third\nphase, the final predicted labels for the testing data table are gen-\nerated sequentially as shown in Lines 4â€“22. For a given table ğ‘‡ğ‘˜,\ninitially all schema labels are missing , and the set of predicted\nattributes, denoted by ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  , is empty. Given that\nthe prediction is done sequentially,ğ‘špasses are needed to obtain\na predicted schema label for each column in ğ‘‡ğ‘˜. For the ğ‘—-th pass,\nthe ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  has ğ‘— âˆ’1 labeled headers, and ğ‘šâˆ’ğ‘— +1\ncolumns in ğ‘‡ğ‘˜, denoted by ğ‘†ğ‘—, are still missing the predicted labels.\nWe predict the probability distribution Ë†ğ‘ğ‘–, and a schema label Ë†ğ´ğ‘–\nfor each column ğ´ğ‘– âˆˆğ‘†ğ‘— using our model ğ‘€ with contextual in-\nput sequence. The confidence of prediction for ğ´ğ‘– âˆˆğ‘†ğ‘— is given\nby ğ‘ğ‘–ğ‘šğ‘ğ‘¥ = maxğ‘™âˆˆğ¿ Ë†ğ‘ğ‘–[ğ‘™]. The ğ‘¢ğ‘›ğ‘–ğ‘ğ‘¢ğ‘’_â„ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  is a Boolean variable\nthat we set toTrue to force the unique headers constraint for a given\ntable. When predicting duplicate headers is allowed, the column\nâ„, that we choose to predict from ğ‘†ğ‘— in the ğ‘—-th pass, is given by\nâ„= argmaxğ‘ âˆˆ[1,ğ‘šâˆ’ğ‘—+1]ğ‘ğ‘ ğ‘šğ‘ğ‘¥ as shown in Lines 15â€“16.\nAlgorithm 3 ğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘ \n1: Input: probability distributions Ë†ğ‘1, Ë†ğ‘2,..., Ë†ğ‘ğ‘™ğ‘š, ğ‘¡ğ‘œğ‘ğ‘˜, already predicted\nheaders set ğ‘ƒâ„\n2: ğ‘ğ‘˜\nğ‘– : select ğ‘¡ğ‘œğ‘ğ‘˜ probabilities of Ë†ğ‘ğ‘–, ğ‘– âˆˆ[1,ğ‘™ğ‘š]\n3: ğ´ğ‘˜\nğ‘– : ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ of ğ‘¡ğ‘œğ‘ğ‘˜ probabilities of Ë†ğ‘ğ‘–, ğ‘– âˆˆ[1,ğ‘™ğ‘š]\n4: repeat\n5: ğ‘ = argmaxğ‘ âˆˆ[1,ğ‘™ğ‘š]ğ‘ğ‘˜ğ‘  [1]\n6: ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ = ğ´ğ‘˜ğ‘[1]\n7: ğ‘ğ‘˜ğ‘ [1 :ğ‘™ğ‘š âˆ’1]= ğ‘ğ‘˜ğ‘ [2 :ğ‘™ğ‘š], ğ‘ğ‘˜ğ‘ [ğ‘™ğ‘š]= âˆ’1\n8: ğ´ğ‘˜ğ‘[1 :ğ‘™ğ‘š âˆ’1]= ğ´ğ‘˜ğ‘[2 :ğ‘™ğ‘š], ğ´ğ‘˜ğ‘[ğ‘™ğ‘š]= âˆ’1\n9: until ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ âˆ‰ ğ‘ƒâ„ or ğ‘ğ‘˜\nğ‘– contains only âˆ’1, for ğ‘– âˆˆ[1,ğ‘™ğ‘š]\n10: if ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ âˆˆğ‘ƒâ„ then\n11: return ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ from first pass in ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘ğ‘¡ loop\n12: end if\n13: Output: ğ‘: chosen column for prediction , ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™\nOn the other hand, when unique headers constraint is required\nfor a given data table, we propose a routine, calledğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  ,\nthat resolves the duplicate headers problem as shown in Algo-\nrithm 3. The inputs to this routine are: the probability distribu-\ntions Ë†ğ‘ğ‘– for ğ´ğ‘– âˆˆğ‘†ğ‘—, ğ‘¡ğ‘œğ‘ğ‘˜ which denotes the number of top con-\nfidences per attribute that are used to find the label, and the set\nğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  that contains the ğ‘— âˆ’1 semantic labels that\nare already assigned to ğ‘— âˆ’1 columns of ğ‘‡ğ‘˜. The objective of the\nfunction ğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  is to find the label ğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ with the\nhighest confidence value, with respect to the unique headers con-\nstraint that requiresğ‘â„ğ‘œğ‘ ğ‘’ğ‘›_ğ‘™ğ‘ğ‘ğ‘’ğ‘™ âˆ‰ ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  . For time\ncomplexity efficiency, we limit the depth of search by choosing\nğ‘¡ğ‘œğ‘ğ‘˜ << |ğ¿|. By limiting the depth of search, ğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  can\nproduce a duplicate header. In this case, we use a heuristic that re-\nturns the label that corresponds to the maximum confidence score.\nğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  is called in Lines 12â€“14 of Algorithm 2.\nWe remove the chosen column â„ from ğ‘†ğ‘— to obtain ğ‘†ğ‘—+1 (the\ncolumns of ğ‘‡ğ‘˜ that are still missing labels after the ğ‘—-th pass) , and\nwe add the chosen columnâ„to ğ‘ ğ‘’ğ‘’ğ‘›_ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘›ğ‘  set, and the predicted\nlabel Ë†ğ´â„ to ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘’ğ‘‘ _ğ‘ğ‘¡ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘’ğ‘  set (Lines 18â€“20). We finish the\nğ‘—-th pass by updating ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ (ğ´ğ‘–),where ğ´ğ‘– âˆˆğ‘†ğ‘—+1, using the\npredicted label Ë†ğ´â„ from the ğ‘—-th pass as shown in Line 21. The\nobjective of the context update step is to replace the only values\npredicted label by the contextual input inferred schema label, as the\nlatter is more accurate than the former. For the ğ‘—-th pass, we select\nthe best schema label from ğ‘šâˆ’ğ‘—+1 predicted labels. The increase\nin the number of predictions is justified by the sequential nature of\nMohamed Trabelsi, Jin Cao, and Jeff Heflin\nthe testing algorithm where context is updated in each pass, and\nthe most confident prediction is selected.\n5 EVALUATION\n5.1 Data collections\n5.1.1 WikiTables. This dataset is composed of the WikiTables cor-\npus [2] which contains over 1.6ğ‘€ tables that are extracted from\nWikipedia. Since a lot of tables have unexpected formats, we pre-\nprocess tables so that we only keep tables that have enough content\nwith at least 3 columns and 50 rows. We further filter the columns\nwhose schema labels appear less than 10 times in the table corpus,\nas there are not enough data tables that can be used to train the\nmodel to recognize these labels. We experiment on 15,252 data\ntables, with a total number of columns equal to 82,981. The total\nnumber of schema labels is equal to 1088.\n5.1.2 Log Tables from Network Equipment. Our work is motivated\nby the business need to automatically generate schema labels for\nthe data tables extracted from log files of network equipment. Net-\nwork logs files contain computer-generated event records, such as\nauthentication attempts, process assessment calls and information\noutput of network equipment, and are instrumental for network\nperformance monitoring and fault diagnosis.\nFor the purpose of schema label auto-generation, we shall uti-\nlize the existing data tables that have already been collected in an\ninternal platform used by network care engineers from parsing log\nfiles. In the current pipeline, engineers design a parser for each\ntype of log files, and these parsers generate the tables. We have\ncollected 329 tables from this platform with logs files coming from\nproducts on wireless equipment such as base station, Radio Access\nNetwork and Radio Network Controllers. To evaluate our methods\non header prediction, we removed tables that have less than 10 rows\nand cleaned up columns that have mostly invalid values (such as\nNULL, empty string, or NA). The remaining set contains 248 tables.\nThe number of rows of these tables have a very skewed distribution\nwith quantiles being 138 (25%), 551 (50%) and 1954 (75%), while\nthe number of columns ranges from 3 to 48 columns with many\ntables having in the neighbourhood of 10 columns. For our purpose,\nwe focus on 87 headers from these tables that have more than 3\ninstances.\nFigure 2 shows the cumulative frequency distribution for the\nheaders from the two datasets, from the most to the least popular.\nThere is a small set of labels that are much more frequently occur-\nring in WikiTables. One reason that the labels in log tables are more\nscattered is because the tables are manually collected from diverse\nproducts as we would like to understand the performance of our\nalgorithm in various situations.\n5.2 Baselines\nWe compare the performance of our proposed model with feature-\nbased baselines [10, 16], and a variation of our model where only\ndata values (without context) are used. We describe the five cate-\ngories of features that are extracted from the data values of each col-\numn. There are five categories of features as shown in Table 1. Four\ncategories are previously used in feature-based methods: global sta-\ntistics [10, 16], character distributions [10, 16], word embeddings\nFigure 2: Cumulative Frequency Distribution of Headers\n[16], and paragraph embeddings [16]. To obtain more fine-grained\nembedding, we also propose a character-based generative model to\nproduce character embeddings for each value.\nTable 1: Features categories used in feature based method\n(total dimension equals to 2213)\nID Category type Dimension\n1 Global statistics 52\n2 Character distributions 960\n3 Character embeddings 400\n4 Word embeddings 401\n5 Paragraph embeddings 400\n5.2.1 Global statistics. We combine the global statistics from [10]\nand [16] into one category that has 17 unique features with different\ndimensions as shown in Table 2. When concatenating the global\nstatistics features, the dimension of the resulting feature vector is\n52.\n5.2.2 Character distributions. The distribution of characters is com-\nputed for each column using 96 ASCII-printable characters. For each\ncharacter, 10 statistics (any, all, mean, variance, min, max, median,\nsum, kurtosis, skewness) are calculated based on the count of each\ncharacter in a value from the set of data values for the input column.\nConcatenating character distribution for all characters results in a\nfeature vector with dimension 960.\n5.2.3 Character embeddings. We train a character-level language\nmodel [39] to generate values from the table corpus T. Our gener-\native model has two character-based LSTM layers, and it is trained\non the next character generation task. Given a value ğ‘£ğ‘˜, which is a\nsequence of characters, our generative model produces a hidden\nstate for each character, and we use the hidden state from the second\nLSTM layer of the last character as the character embedding of ğ‘£ğ‘˜.\nThen as in [16], we compute the mean, mode, median and variance\nof character embeddings across all values ğ‘£ğ‘˜ âˆˆğ‘‰ğ‘– in a column ğ´ğ‘–.\nGiven that the dimension of character embedding is 100, and we\nhave 4 statistics, the resulting feature vector has dimension 400.\n5.2.4 Word embeddings. Pre-trained word embedding, such as\nGlove [30], is used to compute embedding for each value ğ‘£ğ‘˜ âˆˆğ‘‰ğ‘–.\nThen, as in character embedding, 4 statistics are computed for ğ‘‰ğ‘–.\nSemantic Labeling Using a Deep Contextualized Language Model\nTable 2: Global statistics features [10, 16] with a total dimension equals to 52\nID Length Description\n1 1 minimum value in columnâ€™s content\n2 1 maximum value in columnâ€™s content\n3 1 average value of columnâ€™s content\n4 1 standard deviation value of columnâ€™s content\n5 1 percentage of numeric cells in columnâ€™s content\n6 20 content histogram\n7 1 number of values in a column\n8 1 column entropy\n9 1 percentage of unique content\n10 1 percentage of values with numeric characters\n11 1 percentage of values with alphabetical characters\n12 2 mean and standard deviation of the number of numerical characters in columnâ€™s content\n13 2 mean and standard deviation of the number of alphabetical characters in columnâ€™s content\n14 2 mean and standard deviation of the number of special characters in columnâ€™s content\n15 2 mean and standard deviation of the number of words in columnâ€™s content\n16 4 None values statistics: count, percentage, has some None values(Boolean), has Only None values (Boolean)\n17 10 length of values statistics: any nonzero length (Boolean), all nonzero length (Boolean), sum, min, max,variance,\nmedian, mode, kurtosis, skewness\nThe dimension of embedding is 100, so that after computing statis-\ntics, the dimension of the concatenated feature vector equals 400.\nAs in [16], an additional binary feature is appended to the final\nword embedding feature vector, and it indicates if there is at least\none value from ğ‘‰ğ‘– that belongs to Glove vocabulary.\nThe use of pre-trained embedding is suitable for table collections,\nsuch as WikiTables, that have common values with the vocabulary\nof the pre-trained embedding. This is not the case for log tables from\nnetwork equipment, where the number of out of vocabulary (OOV)\ntokens is large, and this leads to poor performance for pre-trained\nword embedding. To solve this problem, we train a word embedding\non the table corpus T, where the sentences are rows and columns\nfrom ğ‘‡ğ‘˜ âˆˆT. Instead of using Glove, we train a fastText [3] model\nto produce word embeddings. The use of character-level n-grams in\nfastText allows word embeddings to be created even for terms that\nhave not been seen before, and reduces the negative effect of OOV\ntokens. Given that having long strings is common for log data, we\nuse BERT tokenizer to preprocess values before training fastText\nembeddings.\n5.2.5 Paragraph embeddings. Each column ğ´ğ‘– can be seen as a\nparagraph that contains the set of values ğ‘‰ğ‘– = {ğ‘£1,ğ‘£2,...,ğ‘£ ğ‘Ÿ}. The\nparagraph embedding, that is based on the distributed bag of words\n[20], is trained to map each column into an embedding with dimen-\nsion equals to 400.\n5.2.6 Sherlock. Hulsebos et al. [16] uses global statistics, character\ndistributions, word embeddings, and paragraph embeddings with a\nmulti-input neural networks architecture.\n5.2.7 All features. This baseline extends Sherlock [16] features by\nadding our character embeddings to cover three different levels of\nembeddings (character, word, and paragraph).\n5.2.8 BERT with only values. This baseline can be seen as a vari-\nation of our proposed method SeLaB, where only data values are\nused to predict schema labels. So, for training, the first phase pre-\ndictions are used to update the parameters of the model. For testing,\nthe first phase predictions are used to evaluate the performance of\nthe model. The input data values sequence to the model has only\nvalues input form.\nWe note that we do not compare to external KB based methods\n[8, 9] because there is a vocabulary mismatch between log tables\nfrom network equipment and DBpedia, and an important aspect of\nour evaluation is to show generalization to data tables from multiple\ndomains (not only tables from Wikipedia and Web tables).\n5.3 Experimental Setup\nIn our proposed model, we use the BERT-base-uncased for the\nfeature extractor ğ¹. For each column, we shuffle data values and\nrandomly select a subset of values to reduce the complexity of the\nmodel. Given that the majority of log tables have more rows than\nWikiTables, we randomly choose 200 values for each column in\na given log table, and 100 values for columns from WikiTables.\nIn general, shuffling the predicted context can reduce overfitting.\nFor WikiTables collection, the majority of tables that have both\nattributes home team and away team (these attributes have similar\ndata values), report home team column before away team column\n(same for birth date which occurs before death date ) for a left-to-\nright sequential order. In this case, we can resolve the ambiguity\nof predicting home team and away team by keeping the sequential\norder of the predicted context. We train our model for 10 epochs,\nand we set ğ‘¡ğ‘œğ‘ğ‘˜ of ğ‘ˆğ‘›ğ‘–ğ‘ğ‘¢ğ‘’ğ»ğ‘’ğ‘ğ‘‘ğ‘’ğ‘Ÿğ‘  routine to 5.\nThe model is implemented using PyTorch, with Nvidia GeForce\nGTX 1080 Ti. We use Adam [19] optimizer for gradient descent to\nminimize the cross entropy loss function and update the weights of\nour model. We report results for our method and baselines using a\nrandom split of the entire table corpus, where80% of tables are used\nfor training, and 20% for testing. For the baselines, BERT with only\nvalues is also trained for 10 epochs. For the feature-based baselines\n(except for Sherlock), random forest is trained for prediction.\nMohamed Trabelsi, Jin Cao, and Jeff Heflin\nMethod Name Macro-P Macro-R Macro-F Micro-F MRR\nGlobal statistics 7 Ã—10âˆ’3 10âˆ’2 6Ã—10âˆ’3 0.17 0.28\nCharacter distributions 0.14 0.10 0.10 0.38 0.48\nCharacter embeddings 0.38 0.33 0.33 0.54 0.64\nWord embeddings 0.18 0.16 0.15 0.47 0.58\nParagraph embeddings 0.23 0.19 0.19 0.43 0.54\nSherlock 0.45 0.45 0.42 0.66 0.75\nAll features 0.54 0.47 0.47 0.66 0.75\nBERT (only values) 0.46 0.45 0.43 0.64 0.73\nSeLaB w/o unique headers 0.55 0.52 0.50 0.72 0.80\nSeLaB 0.55 0.53 0.51 0.72 0.80\n(a) WikiTables\nMethod Name Macro-P Macro-R Macro-F Micro-F MRR\nGlobal statistics 0.16 0.16 0.15 0.54 0.64\nCharacter distributions 0.40 0.42 0.40 0.68 0.71\nCharacter embeddings 0.44 0.44 0.43 0.69 0.71\nWord embeddings 0.35 0.34 0.33 0.65 0.69\nParagraph embeddings 0.31 0.29 0.29 0.61 0.67\nSherlock 0.38 0.38 0.37 0.67 0.73\nAll features 0.50 0.49 0.49 0.71 0.72\nBERT (only values) 0.48 0.49 0.47 0.73 0.76\nSeLaB w/o unique headers 0.61 0.62 0.60 0.81 0.81\nSeLaB 0.62 0.63 0.61 0.81 0.81\n(b) Log tables\nTable 3: Semantic labeling results\n(a) WikiTables collection\n (b) Log tables collection\nFigure 3: Top-k accuracy results\n5.4 Experimental results\nWe evaluate the performance of SeLaB and baselines on the schema\nlabeling task using macro-averaged and micro-averaged precision\n(P), recall (R) and F-score of predictions on the testing set. In the\nmulticlass classification problem, the micro-average precision, re-\ncall and F-score are the same, so we only report the Micro-F score.\nWe also report the Mean Reciprocal Rank (MRR) [23], as the rank\nof the true class is an important measure for evaluation. In addition\nto that, we calculate the top-ğ‘˜ accuracy that shows the fraction of\ntesting samples where the true label is within the top ğ‘˜ predicted\nconfidences.\n5.4.1 Semantic labeling results. Table 3(a) shows the performance\nof different approaches on the WikiTables collection. We show\nthat our proposed method SeLaB outperforms the baselines for\nall evaluation metrics. The context, which is incorporated into\nour model, solves the ambiguity in predictions and leads to an in-\ncrease in evaluation metrics compared to baselines that generate\nschema labels solely on the basis of data values. Among all the\nfive categories of features (global statistics, character distributions,\ncharacter embeddings, word embeddings, paragraph embeddings)\nthat are used in the feature-based approaches, our character em-\nbeddings feature achieves higher performance for all evaluation\nmetrics. So, a generative model with a character granularity is able\nto capture the distributions of data values drawn from different\nvariables or attributes. Figure 3(a) shows the top-ğ‘˜accuracy results\nwhere our method SeLaB outperforms the baselines. BERT with\nonly values, Sherlock, and all features baselines have close perfor-\nmance. So, the BERT-based embedding, which is trained by using\nonly data values, is as good as the hand-crafted features. While\nextracting the hand-crafted features requires significant human\neffort to compute the global statistics, character distributions and\nthree types of embeddings (character, word, and paragraph), BERT\nembeddings are trained jointly with the classification layer with\nminimal preprocessing which reduces the human effort.\nSemantic Labeling Using a Deep Contextualized Language Model\nFigure 4: SeLaB top-1 accuracy for masked headers\nFigure 5: Top-k accuracy for string and numerical columns\nTable 3(b) and Figure 3(b) show the performance of different\napproaches on the Log tables from network equipment. Consistent\nwith WikiTables, our results on the log tables corpus show the\nimportance of a columnâ€™s context in improving the semantic labels\nprediction, especially for top-1 accuracy as shown in Figure 3(b).\nThe top-5 accuracies for SeLaB, BERT with only values, and all\nfeatures baseline are similar which indicates that the ambiguity\nin semantic labeling occurs mainly when predicting exact schema\nlabels. Semantic labeling results on WikiTables and Log tables show\nthat SeLaB achieves significant improvements in the evaluation\nmetrics of two data table collections from different domains, which\nsupports the generalization characteristic of our proposed method.\n5.4.2 Masked headers. We showed the performance of SeLaB where\nall semantic labels are missing for a given data table. This can be\nseen as an extreme case. A common scenario for tables extraction\nis to have a percentage of missing or false headers. To better un-\nderstand how SeLab deals with such tables, we randomly mask a\npercentage of headers from tables in the testing set, and we report\nthe top-1 accuracy function of the percentage of masked headers\nas shown in Figure 4. For each table, we run the masked headers\nfor five times with a randomly selected headers, and we compute\nmean and standard deviation (std) for each percentage of masked\nheaders. 100% masked headers means that all labels are missing\nwhich is the most difficult and the main setting for SeLaB.\nAs reported in Figure 4, the maximum std (vertical bar) is 0.01\nfor Log tables and 0.005 for WikiTables. Figure 4 shows that when\nthe percentage of masked headers decreases, there is an increase\nin the mean of top-1 accuracy for predicted masked headers. This\nmeans that the attributeâ€™s context is more accurate given that the\nlabels of the non-masked headers are groundtruth labels. However,\ncomparing the fully predicted context in100% masked headers with\nthe partially predicted context in 20% masked headers, there is only\na small improvement (mainly for WikiTables) which indicates that\nthe predicted context in the extreme case is as good as groundtruth\ncontext.\n5.4.3 String vs Numeric columns analysis. We evaluate the perfor-\nmance of SeLaB for two categories of columns which are numeric\nand string. As shown in Figure 5, we report top-ğ‘˜ accuracy of nu-\nmerical and string columns for Log tables and WikiTables. In both\ndatasets, semantic labeling of string columns outperforms numer-\nical columns. So, generating an exact schema label for numerical\ndata values is more ambiguous than string values, as numerical\ncolumns contain similar values.\n5.4.4 Predicted labels examples. To better understand how SeLaB\nworks, we show examples of predicted schema labels from Wik-\niTables testing set in Table 4. Each row corresponds to a testing\ntable where we show the ground truth attributes, first phase pre-\ndictions, and the final predicted schema labels. For example, for\nthe first row, there are three wrong predictions ( year instead of\nseason, division instead of section, and finish instead of position)\nfrom first phase predictions which are based only on data values.\nAfter incorporating context for each attribute, SeLaB updates the\npredicted label for each column, and the new context-aware seman-\ntic labels that match the ground truth labels are shown in bold in\nTable 4. For the first example, we obtain the third phase predictions\nwhich are identical to the ground truth labels after three corrections\nfrom context-aware representation for each column. For the sixth\nrow, the tableâ€™s attributes contain home team and away team . Both\nattributes are predicted home team after first phase predictions.\nSeLaB learns that away team occurs with home team so that the\npredictions are corrected after the third phase.\n5.4.5 Effect of the Number of Training Samples. To understand\nhow the number of training samples for each semantic label affects\nthe accuracy of SeLaB predictions in the test data, in Figure 6, for\neach label in the testing set, we plot the indicator values of correct\nSeLaB prediction against the number of samples for each label in\nthe training set as black circles (i.e. 1 indicates the predicted column\nheader is the same as the ground truth). Local smoothing [12] was\nperformed to obtain the average accuracy curve for SeLaB predic-\ntions (yellow line). As a reference, we also added a similar local\nsmoothing curve representing the accuracy curve from predictions\nobtained using â€œBert with only valuesâ€ (pink line).\nFigure 6 clearly demonstrates that overall speaking, as the num-\nber of samples for each label in the training set increases, both SeLaB\nand â€œBert with only valuesâ€ are performing better. However, SeLaB\nappears to perform much better when there is sufficient number\nMohamed Trabelsi, Jin Cao, and Jeff Heflin\nTable attributes First phase predictions Third phase predictions\nseason, level, division, section, position year, level, division, division, finish season, level, division,section, position\nyear, title, developer, publisher, setting, platform year, title, developer, developer, setting, system year, title, developer,publisher, setting, system\npos, rider, bike, pts rank, rider, bike, pts pos, rider, bike, pts\npos, class, no, team, drivers, car, laps, qual pos pos, group, rank, driver, driver, car, deaths, rank pos,class, no, entrant,drivers, car,laps, grid\nsenator, party, years, term, electoral history representative, party, years, wins, electoral history representative, party, years,term, electoral history\ndate, time, home team, away team date, time, home team, home team date, time, home team, away team\nsite, location, year, description site, province, year, description name, location, year, description\nland area, latitude, longitude area, latitude, geographic coordinate system land area, latitude,longitude\ntitle, director, cast, genre, notes film, role, cast, genre, notes role, director, cast, genre, notes\ncounty, location, exit number, destinations, notes county, location, notes, notes, notes county, location, exit, destinations, notes\nTable 4: Example of predicted schema labels from Wikitables testing set\n(a) Log tables\n (b) WikiTables\nFigure 6: Accuracy vs. Number of training samples per label\nof samples per label (e.g. more than 20 instances). There is a slight\ndip for the SeLab curve for those columns when the corresponding\nnumber of samples in the training set becomes the largest. Upon\nclose inspection, it does not seem to imply that the performance\nof SeLaB is deteriorating. For example, in Wikitables, the column\nheader with the largest number of instances is in fact a generic\nlabel name (with 3064 instances in training data). In the testing set,\nSeLaB sometimes predicts name as player, swimmer, representative,\netc, depending on the context of the table, thus potentially yielding\nmore appropriate header names.\n6 CONCLUSIONS\nWe have shown that a context-aware model that combines data\nvalues and columnâ€™s context outperforms approaches that predict\nsemantic labels only on the basis of data values. Our method has\nbeen evaluated on two real-world datasets from multiple domains:\nWikiTables extracted from Wikipedia and Log tables from network\nequipment. We have shown that the attributeâ€™s predicted context,\nwhich is incorporated into our model SeLaB, solves the ambiguity\nin semantic labels predictions. Our model is trained end-to-end\nfor both feature extraction and label prediction which reduces the\nhuman effort in semantic labeling.\nFuture work includes looking at how to incorporate metadata of\neach data table, such as table caption and description, into SeLaB,\nand how to select the best subset of data values for each column to\nimprove semantic labeling results.\nSemantic Labeling Using a Deep Contextualized Language Model\nREFERENCES\n[1] SÃ¶ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,\nand Zachary Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In Proceed-\nings of the 6th International The Semantic Web and 2nd Asian Conference on Asian\nSemantic Web Conference . Springer-Verlag, Berlin, Heidelberg, 722â€“735.\n[2] Chandra Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL: Entity\nLinking in Web Tables. InInternational Semantic Web Conference .\n[3] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. En-\nriching Word Vectors with Subword Information. Transactions of the Association\nfor Computational Linguistics 5 (2017), 135â€“146.\n[4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.\n2008. Freebase: A Collaboratively Created Graph Database for Structuring Human\nKnowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on\nManagement of Data . Association for Computing Machinery, 1247â€“1250.\n[5] Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang.\n2008. WebTables: Exploring the Power of Tables on the Web. Proc. VLDB Endow.\n1, 1 (Aug. 2008), 538â€“549.\n[6] R. Castro Fernandez, Z. Abedjan, F. Koko, G. Yuan, S. Madden, and M. Stonebraker.\n2018. Aurum: A Data Discovery System. In2018 IEEE 34th International Conference\non Data Engineering (ICDE) . 1001â€“1012.\n[7] R. Castro Fernandez, E. Mansour, A. A. Qahtan, A. Elmagarmid, I. Ilyas, S. Mad-\nden, M. Ouzzani, M. Stonebraker, and N. Tang. 2018. Seeping Semantics: Linking\nDatasets Using Word Embeddings for Data Discovery. In 2018 IEEE 34th Interna-\ntional Conference on Data Engineering (ICDE) . 989â€“1000.\n[8] Jiaoyan Chen, Ernesto JimÃ©nez-Ruiz, Ian Horrocks, and Charles A. Sutton. 2019.\nLearning Semantic Annotations for Tabular Data. In IJCAI.\n[9] Jiaoyan Chen, Ernesto Jimenez--Ruiz, Ian Horrocks, and Charles Sutton. 2019.\nColNet: Embedding the Semantics of Web Tables for Column Type Prediction. In\nProceedings of the Thirty-Third AAAI Conference on Artificial Intelligence .\n[10] Zhiyu Chen, Haiyan Jia, Jeff Heflin, and Brian D. Davison. 2018. Generating\nSchema Labels through Dataset Content Analysis. In Companion Proceedings\nof the The Web Conference 2018 (Lyon, France). International World Wide Web\nConferences Steering Committee, 1515â€“1522.\n[11] Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. 2020.\nTable Search Using a Deep Contextualized Language Model. In Proceedings of the\n43rd International ACM SIGIR Conference on Research and Development in Infor-\nmation Retrieval (Virtual Event, China). Association for Computing Machinery,\nNew York, NY, USA, 589â€“598.\n[12] William S. Cleveland and Susan J. Devlin. 1988. Locally Weighted Regression:\nAn Approach to Regression Analysis by Local Fitting. J. Amer. Statist. Assoc. 83,\n403 (1988), 596â€“610. http://www.jstor.org/stable/2289282\n[13] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Development in Information Retrieval . 4.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT.\n[15] J. Eberius, K. Braunschweig, M. Hentsch, M. Thiele, A. Ahmadov, and W. Lehner.\n2015. Building the Dresden Web Table Corpus: A Classification Approach. In2015\nIEEE/ACM 2nd International Symposium on Big Data Computing (BDC) . 41â€“50.\n[16] Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind Satya-\nnarayan, Tim Kraska, Ã‡agatay Demiralp, and CÃ©sar Hidalgo. 2019. Sherlock: A\nDeep Learning Approach to Semantic Data Type Detection. In Proceedings of\nthe 25th ACM SIGKDD International Conference on Knowledge Discovery & Data\nMining. Association for Computing Machinery, 1500â€“1508.\n[17] Anuj Jaiswal, David J. Miller, and Prasenjit Mitra. 2013. Schema Matching and\nEmbedded Value Mapping for Databases with Opaque Column Names and Mixed\nContinuous and Discrete-Valued Data Fields. ACM Trans. Database Syst. 38, 1\n(April 2013), 34.\n[18] Jaewoo Kang and Jeffrey F. Naughton. 2003. On Schema Matching with Opaque\nColumn Names and Data Values. InProceedings of the 2003 ACM SIGMOD Interna-\ntional Conference on Management of Data . Association for Computing Machinery.\n[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-\nmization. In 3rd International Conference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings .\n[20] Quoc Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and\nDocuments. In Proceedings of the 31st International Conference on International\nConference on Machine Learning - Volume 32 . 1188â€“1196.\n[21] E. L. Lehmann and Joseph P. Romano. 2005.Testing statistical hypotheses. Springer,\nNew York.\n[22] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and\nSearching Web Tables Using Entities, Types and Relationships.Proc. VLDB Endow.\n3, 1â€“2 (Sept. 2010), 1338â€“1347.\n[23] Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Found. Trends Inf.\nRetr. 3, 3 (March 2009), 225â€“331.\n[24] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-Task\nDeep Neural Networks for Natural Language Understanding. InProceedings of the\n57th Annual Meeting of the Association for Computational Linguistics . 4487â€“4496.\n[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. ArXiv abs/1907.11692 (2019).\n[26] Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze. 2008. Intro-\nduction to Information Retrieval . Cambridge University Press, USA.\n[27] J. Mueller and A. Smola. 2019. Recognizing Variables from Their Data via Deep\nEmbeddings of Distributions. In 2019 IEEE International Conference on Data\nMining (ICDM) . 1264â€“1269.\n[28] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\nArXiv abs/1901.04085 (2019).\n[29] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-Stage\nDocument Ranking with BERT. ArXiv abs/1910.14424 (2019).\n[30] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. InProceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP) . Association for\nComputational Linguistics, 1532â€“1543.\n[31] Minh Pham, Suresh Alse, Craig A. Knoblock, and Pedro A. Szekely. 2016. Seman-\ntic Labeling: A Domain-Independent Approach. In International Semantic Web\nConference.\n[32] Erhard Rahm and Philip A. Bernstein. 2001. A Survey of Approaches to Automatic\nSchema Matching. The VLDB Journal 10, 4 (Dec. 2001), 334â€“350.\n[33] Vijayshankar Raman and Joseph M. Hellerstein. 2001. Potterâ€™s Wheel: An Inter-\nactive Data Cleaning System. In Proceedings of the 27th International Conference\non Very Large Data Bases . Morgan Kaufmann Publishers Inc., 381â€“390.\n[34] S. K. Ramnandan, Amol Mittal, Craig A. Knoblock, and Pedro A. Szekely. 2015.\nAssigning Semantic Labels to Data Sources. In ESWC.\n[35] Natalia Ruemmele, Yuriy Tyshetskiy, and Alex Collins. 2018. Evaluating ap-\nproaches for supervised semantic labeling. In TheWebConf Workshop: Linked\nData on the Web (LDOW) .\n[36] Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and Sadao Kurohashi. 2019.\nFAQ Retrieval Using Query-Question Similarity and BERT-Based Query-Answer\nRelevance. In Proceedings of the 42nd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval . Association for Computing\nMachinery, 1113â€“1116.\n[37] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core\nof semantic knowledge. In WWW â€™07 .\n[38] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to Fine-Tune\nBERT for Text Classification?. InChinese Computational Linguistics , Maosong Sun,\nXuanjing Huang, Heng Ji, Zhiyuan Liu, and Yang Liu (Eds.). Springer International\nPublishing, Cham, 194â€“206.\n[39] Ilya Sutskever, James Martens, and Geoffrey Hinton. 2011. Generating Text with\nRecurrent Neural Networks. In Proceedings of the 28th International Conference\non International Conference on Machine Learning . Omnipress, Madison, WI, USA,\n1017â€“1024.\n[40] Zareen Syed, Tim Finin, Varish Mulwad, and Anupam Joshi. 2010. Exploiting a\nWeb of Semantic Data for Interpreting Tables. InProceedings of the Second Web\nScience Conference.\n[41] Isabel Valera and Zoubin Ghahramani. 2017. Automatic Discovery of the Sta-\ntistical Types of Variables in a Dataset. In Proceedings of the 34th International\nConference on Machine Learning , Vol. 70. PMLR, 3521â€“3529.\n[42] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2014. OpenML:\nNetworked Science in Machine Learning. SIGKDD Explor. Newsl. 15, 2 (June 2014),\n49â€“60.\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Processing Systems 30 , I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.). 5998â€“6008.\n[44] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius PaÅŸca, Warren Shen, Fei\nWu, Gengxin Miao, and Chung Wu. 2011. Recovering Semantics of Tables on the\nWeb. Proc. VLDB Endow. 4, 9 (June 2011), 528â€“538.\n[45] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel\nBowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP . Association\nfor Computational Linguistics, 353â€“355.\n[46] Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple Applications of BERT\nfor Ad Hoc Document Retrieval. ArXiv abs/1903.10972 (2019).\n[47] Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, and\nJimmy Lin. 2019. Applying BERT to Document Retrieval with Birch. In\nEMNLP/IJCNLP.\n[48] S. Yu, J. Su, and D. Luo. 2019. Improving BERT-Based Text Classification With\nAuxiliary Sentence and Domain Knowledge.IEEE Access 7 (2019), 176600â€“176612.\n[49] Benjamin Zapilko, MatthÃ¤us Zloch, and Johann Schaible. 2012. Utilizing Regu-\nlar Expressions for Instance-Based Schema Matching. In Proceedings of the 7th\nInternational Conference on Ontology Matching - Volume 946 . 240â€“241.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8717188835144043
    },
    {
      "name": "Schema matching",
      "score": 0.7419379353523254
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.6492398977279663
    },
    {
      "name": "Column (typography)",
      "score": 0.618364155292511
    },
    {
      "name": "Natural language processing",
      "score": 0.5419938564300537
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5137404203414917
    },
    {
      "name": "Information retrieval",
      "score": 0.42194628715515137
    },
    {
      "name": "Table (database)",
      "score": 0.4210616946220398
    },
    {
      "name": "Semantic matching",
      "score": 0.41923993825912476
    },
    {
      "name": "Matching (statistics)",
      "score": 0.3607766628265381
    },
    {
      "name": "Data mining",
      "score": 0.3136671185493469
    },
    {
      "name": "Data integration",
      "score": 0.13291266560554504
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Frame (networking)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I186143895",
      "name": "Lehigh University",
      "country": "US"
    }
  ],
  "cited_by": 8
}