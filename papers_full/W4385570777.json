{
  "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
  "url": "https://openalex.org/W4385570777",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3170892139",
      "name": "Alex Mallen",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2784921365",
      "name": "Akari Asai",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2692068246",
      "name": "Victor Zhong",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2115737366",
      "name": "Rajarshi Das",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2039793395",
      "name": "Daniel Khashabi",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2949695381",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W4385573569",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W4205694376",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4385574183",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W3201233724",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4302305884",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2397288399",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4225992558"
  ],
  "abstract": "Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, Hannaneh Hajishirzi. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9802–9822\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nWhen Not to Trust Language Models: Investigating Effectiveness of\nParametric and Non-Parametric Memories\nAlex Mallen∗♢ Akari Asai∗♢ Victor Zhong♢ Rajarshi Das♢\nDaniel Khashabi♠ Hannaneh Hajishirzi♢♡\n♢University of Washington ♠Johns Hopkins University\n♡Allen Institute for AI\n{atmallen,akari,vzhong,rajarshi,hannaneh}@cs.washington.edu\ndanielk@jhu.edu\nAbstract\nDespite their impressive performance on di-\nverse tasks, large language models (LMs) still\nstruggle with tasks requiring rich world knowl-\nedge, implying the difficulty of encoding a\nwealth of world knowledge in their param-\neters. This paper aims to understand LMs’\nstrengths and limitations in memorizing factual\nknowledge, by conducting large-scale knowl-\nedge probing experiments on two open-domain\nentity-centric QA datasets: POPQA, our new\ndataset with 14k questions about long-tail enti-\nties, and EntityQuestions, a widely used open-\ndomain QA dataset. We find that LMs struggle\nwith less popular factual knowledge, and that\nretrieval augmentation helps significantly in\nthese cases. Scaling, on the other hand, mainly\nimproves memorization of popular knowledge,\nand fails to appreciably improve memorization\nof factual knowledge in the long tail. Based\non those findings, we devise a new method\nfor retrieval augmentation that improves per-\nformance and reduces inference costs by only\nretrieving non-parametric memories when nec-\nessary.1\n1 Introduction\nLarge language models (LMs; Brown et al. 2020;\nRaffel et al. 2020) have been shown to be compet-\nitive on diverse NLP tasks, including knowledge-\nintensive tasks that require fine-grained memoriza-\ntion of factual knowledge (Chowdhery et al., 2022;\nYu et al., 2022). Meanwhile, LMs have also been\nshown to have limited memorization for less fre-\nquent entities (Kandpal et al., 2022), are prone\nto hallucinations (Shuster et al., 2021), and suf-\nfer from temporal degradation (Kasai et al., 2022;\nJang et al., 2022). Incorporating non-parametric\nknowledge (i.e., retrieved text chunks) largely helps\naddress those issues stemming from reliance on\nLMs’ parametric knowledge—knowledge stored\n1Our code and data are available at https://github.\ncom/AlexTMallen/adaptive-retrieval.\nNot memorized in parameters  \n⇨  use retrieval\nMemorized in parameters\n⇨  don't use retrieval\nWhat is Kathy\nSaltzman's occupation?\nWhat is the capital of\nLouisiana?\nFigure 1: Relationship between subject entity popularity\nin a question and GPT-3 performance in open-domain\nQA, with and without retrieved passages. Adaptive\nRetrieval only retrieves when necessary (orange bars)\nbased on the heuristically-decided threshold (red line).\nin their parameters (Izacard et al., 2022b)—but it\nis unclear whether it is strictly superior or comple-\nmentary to parametric knowledge. Understanding\nwhen we should not trust LMs’ outputs is also\ncrucial to safely deploying them in real-world ap-\nplications (Kadavath et al., 2022).\nThis work conducts a large-scale knowledge\nprobing of LMs on factual knowledge memoriza-\ntion, to understand when we should and should not\nrely on LMs’ parametric knowledge, and how scal-\ning and non-parametric memories (e.g., retrieval-\naugmented LMs) can help. In particular, we aim to\naddress the following research questions:\n(RQ1) How much factual knowledge is memo-\nrized by LMs and what factors affect the\nmemorization? (Section 4)\n(RQ2) To what extent can non-parametric memo-\nries alleviate the shortcomings of paramet-\nric memories of LMs? (Section 5)\n(RQ3) Can we build a system to adaptively com-\nbine non-parametric and parametric mem-\nories? (Section 6)\nWe hypothesize that factual knowledge fre-\nquently discussed on the web is easily memorized\n9802\nby LMs, while the knowledge that is less discussed\nmay not be well captured and thus they require re-\ntrieving external non-parametric memories. We\nevaluate ten large LMs of three families (i.e., GPT-\nNeo, OPT, and GPT-3) with varying scales on\nthe open-domain question answering (QA) task\nin a zero- or few-shot prompting manner. We\nconstruct a new dataset, POPQA, consisting of\n14k questions to cover factual information in the\nlong tail that might have been missed in popular\nQA datasets (Kwiatkowski et al., 2019). We use\nWikipedia page views as a measure of popularity\nand convert knowledge triples from Wikidata, with\ndiverse levels of popularity, into natural language\nquestions, anchored to the original entities and re-\nlationship types. We also use EntityQuestions (Sci-\navolino et al., 2021), an open-domain QA dataset\nwith a long-tail distribution.\nOn both datasets, LMs’ memorization ( RQ1)\nis often limited to the popular factual knowledge\nand even GPT-3davinci-003 fails to answer the\nmajority of the long-tail questions. Moreover, on\nsuch questions, scaling up models does not signifi-\ncantly improve the performance (e.g., for the 4,000\nleast popular questions in POPQA, GPT-j 6B has\n16% accuracy and GPT-3 davinci-003 has 19%\naccuracy). This also suggests that we can predict\nif LMs memorize certain knowledge based on the\ninformation presented in the input question only.\nWe next investigate whether a semi-parametric\napproach that augments LMs with retrieved evi-\ndence can mitigate the low performance on ques-\ntions about less popular entities ( RQ2). Non-\nparametric memories largely improve performance\non long-tail distributions across models. Specif-\nically, we found that retrieval-augmented LMs\nare particularly competitive when subject entities\nare not popular: a neural dense retriever (Izac-\nard et al., 2022a)-augmented GPT-neo 2.7B out-\nperforms GPT-3 davinci-003 on the 4,000 least\npopular questions. Surprisingly, we also find that\nretrieval augmentation can hurt the performance of\nlarge LMs on questions about popular entities as\nthe retrieved context can be misleading.\nAs a result, we devise a simple-yet-effective\nretrieval-augmented LM method, Adaptive Re-\ntrieval, which adaptively combines parametric\nand non-parametric memories based on popularity\n(RQ3). This method further improves performance\non POPQA by up to 10%, while significantly re-\nducing the inference costs, especially with larger\nLMs (e.g., reducing GPT-3 API costs by half), in-\ndicating the potential for future research in more\nefficient and powerful retrieval-augmented LMs.\n2 Related Work\nParametric and non-parametric knowledge.\nPetroni et al. (2019) demonstrate that large pre-\ntrained LMs such as BERT (Devlin et al., 2019)\nmemorize the significant amount of world knowl-\nedge in their parameters (parametric knowledge),\nand Roberts et al. (2020) show that fine-tuned\nT5 without any reference documents (closed-\nbook QA) can achieve competitive performance\non open-domain QA. More recent and power-\nful LMs (Brown et al., 2020; Chowdhery et al.,\n2022) further improve performance on diverse\nknowledge-intensive tasks, leveraging their strong\nparametric memories (Kandpal et al., 2022; Yu\net al., 2022). However, relying solely on their\nparameters to encode a wealth of world knowl-\nedge requires a prohibitively large number of pa-\nrameters and the knowledge can become obsolete\nquickly (Kasai et al., 2022; Jang et al., 2022). Re-\ncent work shows that augmenting LMs with non-\nparametric memories (i.e., retrieved text chunks)\nenables much smaller models to match the per-\nformance of larger models (Izacard et al., 2022b;\nKhandelwal et al., 2020; Min et al., 2022), although\nChen et al. (2022) and Longpre et al. (2021) show\nthat even those models can ignore non-parametric\nknowledge and rely on parametric knowledge.\nUnderstanding memorization. Several prior\nwork establishes a positive relationship between\nstring frequency in pre-training corpora and memo-\nrization (Carlini et al., 2022; Razeghi et al., 2022).\nConcurrent to our work, Kandpal et al. (2022)\nshow that the co-occurrence of the question and\nanswer entities in pretraining corpora has a positive\ncorrelation with models’ QA accuracy on popu-\nlar open-domain QA benchmarks such as Natural\nQuestions (Kwiatkowski et al., 2019). This work,\ninstead, attempts to predict memorization using the\nvariables available in the input question only and\nuses popularity to obtain a proxy for how frequently\nan entity is likely to be discussed on the web. Im-\nportantly, by constructing a new dataset, we can\nconduct fine-grained controlled experiments across\na wide range of popularities, allowing the investiga-\ntion of hypotheses that might have been missed in\nprior analysis using existing open QA datasets. We\nfurther analyze the effectiveness and limitations of\n9803\nWikipedia pageview  \n(Kathy Saltzman, \noccupation, politician) \n(Louisiana, capital of, \nBaton Rouge) \nQ: What is the capital \nof Louisiana? \nA: Baton Rouge \nQ: What is Kathy \nSaltzman’s occupation? \nA: Politician \nKnowledge triples \nfrom Wikidata \n2. Converting triples to questions1. Sampling factual knowledge\n3. Collect popularity \nFigure 2: POPQA is created by sampling knowledge\ntriples from Wikidata and converting them to natural\nlanguage questions, followed by popularity calculation.\nretrieval-augmented LMs and introduce Adaptive\nRetrieval. Prior work investigates the effectiveness\nof deciding when to use non-parametric memories\nat the token level in k-nn LM (He et al., 2021).\nThis work is the first work to study the effective-\nness of deciding whether to retrieve for each query\nand show their effectiveness in retrieval-augmented\nLM prompting.\n3 Evaluation Setup\nWe evaluate LMs’ ability to memorize factual\nknowledge through closed-book QA tasks with few-\nshot samples. We evaluate LMs on our new dataset,\nPOPQA (Figure 2), and EntityQuestions, both of\nwhich have long-tail distributions (Figure 3).\n101 103 105 107\nPopularity\n0.0\n0.2\n0.4\n0.6Density\nNQ\nEntityQuestions\nPopQA\nFigure 3: Distribution of subject entity popularity for En-\ntityQuestions, POPQA, and for NQ-open for reference.\nDetails on NQ entities can be found in Appendix A.\n3.1 Focus and Task\nFocus: factual knowledge. Among diverse types\nof world knowledge, this work focuses on factual\nknowledge (Adams, 2015) of entities—knowledge\nabout specific details of the target entities. We\ndefine factual knowledge as a triplet of ( subject,\nrelationship, object) as in Figure 2 left.\nTask format: open-domain QA. We formulate\nthe task as open-domain QA (Roberts et al., 2020):\ngiven a question, a model predicts an answer with-\nout any pre-given ground-truth paragraph.2 As in\nKandpal et al. (2022), we study few-shot settings\nand prompt LMs without any parameter updates,\ninstead of fine-tuning them on QA datasets such as\nin Roberts et al. (2020).\nMetrics: accuracy. We mark a prediction as cor-\nrect if any substring of the prediction is an exact\nmatch of any of the gold answers.\n3.2 Dimensions of Analysis\nWe hypothesize that factual knowledge that is less\nfrequently discussed on the web may not be well-\nmemorized by LMs. Previous research often uses\nthe term frequency of object entities in pretraining\ncorpora to understand memorization (Févry et al.,\n2020; Kandpal et al., 2022; Razeghi et al., 2022).\nInstead, we investigate whether it’s possible to pre-\ndict memorization based on the input information\nonly, and then apply the findings for modeling im-\nprovements, unlike prior analyses. Therefore, our\nwork focuses on the other two variables in a fac-\ntual knowledge triple: the subject entity and the\nrelationship type.\nSubject entity popularity. We use the popular-\nity of the entities measured by Wikipedia monthly\npage views as a proxy for how frequently the enti-\nties are likely to be discussed on the web, instead\nof using the occurrence of entities or strings in the\npretraining corpus (Carlini et al., 2022; Kandpal\net al., 2022; Razeghi et al., 2022). Calculating fre-\nquencies over large pretraining corpora requires\nmassive computations to link entities over billions\nof tokens, or can result in noisy estimations.3 Our\ninitial studies show that this is much cheaper4 and\naligns well with our intuition.\nRelationship type. We also consider the relation-\nship types as key factors for factual knowledge\nmemorization. For example, even given the same\ncombinations of the subject and object entities,\nmodel performance can depend on the relationship\ntypes; relationship types widely discussed can be\neasier to be memorized, while types that are less\ndiscussed may not be memorized much.\n2Some work conducts knowledge probing of encoder-\nonly models by filling out [MASK] tokens (Petroni et al.,\n2019). We use decoder-only models and thus do not use\nthis fill-in-the-blank scheme.\n3Moreover, several recent models like GPT-3 do not release\ntheir pretraining corpora, and it is an open question whether\nthe frequencies in pretraining corpora reflect the frequencies\nin their private corpora.\n4We can get page views by calling Wikipedia API.\n9804\n3.3 Benchmarks\nPOPQA. In our preliminary studies, we found that\nexisting common open-domain QA datasets such as\nNatural Questions (NQ; Kwiatkowski et al. 2019)\nare often dominated by subject entities with high\npopularity, and it is often hard to identify relation-\nship types due to diverse question surface forms.\nTo enable a fine-grained analysis of memorization\nbased on the aforementioned analysis dimensions,\nwe construct POPQA, a new large-scale entity-\ncentric open-domain QA dataset about entities with\na wide variety of popularity, as shown in Figure 3.\nTo construct POPQA, we randomly sample\nknowledge triples of 16 diverse relationship types\nfrom Wikidata and convert them into natural lan-\nguage questions, using a natural language template\n(depicted in Figure 2). We verbalize a knowledge\ntriple (S, R, O) into a question that involves sub-\nstituting the subject S into a template manually\nwritten for the relationship type R. The full list\nof templates is found in Table 2 of the Appendix.\nThe set of acceptable answers to the question is the\nset of entities E such that (S, R, E) exists in the\nknowledge graph. We tried various templates and\nfound that the results were fairly robust to the tem-\nplates. Since POPQA is grounded to a knowledge\nbase, links to Wikidata entities allow for reliable\nanalysis of popularity and relationship types.\nEntityQuestions. We test on another popular open-\ndomain QA dataset, EntityQuestions (Sciavolino\net al., 2021), which also covers a long-tail entity\ndistribution. They use Wikipedia hyperlink counts\nas a proxy of the frequency of entities and sample\nknowledge triples from WikiData, from the fre-\nquency distributions. Unlike POPQA, EntityQues-\ntions doesn’t provide entity annotations, so we only\nuse 82% of the questions, where the mention of the\nsubject entity has a unique match with a Wikidata\nentity.\n4 Memorization Depends on Popularity\nand Relationship Type\nWe evaluate a range of LMs with varying num-\nbers of parameters, to quantify how much factual\nknowledge they memorize and how different fac-\ntors affect those memorization behaviors (RQ1).\n4.1 Experimental Setup\nModels. We evaluate ten models with a varying\nscale of model size: OPT (Zhang et al. 2022; 1.3,\n2.7, 6.7, and 13 billion), GPT-Neo (Black et al.\n2022; 1.3, 2.7, 6, and 20 billion), and GPT-3\n(Brown et al. 2020; davinci-002, davinci-003)\non our benchmark without any fine-tuning.5\nInstructions and demonstrations. We use a sim-\nple template “Q: <question> A:” to format all\nof our questions for generative prediction. More\nsophisticated instructions were attempted in prelim-\ninary experiments but they did not improve upon\nthe simple template significantly enough to merit\nusing them, especially given that they may overfit\nto the model. While we use zero-shot prompting\nfor GPT-3 to reduce API costs, 6 we use 15-shot\nprompting for all GPT-neo and OPT models.\n4.2 Results\nOverall model performance. The top left col-\numn of Figure 4 illustrates the overall performance\non POPQA. As shown, even without using in-\ncontext examples, larger LMs exhibit reasonable\nperformance: GPT-3 achieves 35% accuracy, and\nGPT-Neo 20B achieves 25% accuracy. This indi-\ncates that large LMs memorize factual knowledge\nin their parameters to some extent. This section ex-\namines which types of knowledge are better mem-\norized and what factors influence memorization.\nSubject entity popularity predicts memorization.\nFigure 4 (bottom) shows that there is a positive cor-\nrelation between subject entity popularity and mod-\nels’ accuracy for almost all relationship types. This\nsupports our hypothesis that subject entity popu-\nlarity can be a reliable indicator of LMs’ factual\nknowledge memorization. In general, the correla-\ntions between subject entity popularity and accu-\nracy are stronger for larger LMs; GPT-3 003 shows\nthe highest positive correlation (roughly 0.4) while\nGPT-Neo-1.3B shows relatively weak positive cor-\nrelations (approximately 0.1).\nRelationship types affects memorization. We\nfind that models have a higher average performance\nfor some relationship types than for others. While\nthis is evidence that factual knowledge of some rela-\ntionship types are more easily memorized than oth-\ners, we also observe that questions of certain rela-\ntionship types can be easily guessed without mem-\norizing the knowledge triple. Specifically, certain\nrelationship types (e.g., nationalities) allow models\n5We did not explore widely-used encoder-decoder models\nsuch as T5, as their supervised pretraining consists of QA.\n6Using 15-shot prompts for GPT-3 would cost upwards of\n$3000 for the combination of vanilla, Contriever, BM25, and\nGenRead evaluations on davinci-002 and davinci-003.\n9805\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nAggregate\n(n=14267)\nAggregate\n(n=14267)\noccupation\n(n=532)\nauthor\n(n=1514)\ndirector\n(n=1999)\ncountry\n(n=838)\ncapital of\n(n=363)\ncapital\n(n=645)\nreligion\n(n=338)\nsport\n(n=547)\nRelation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCorrelation between\nlog(popularity) and accuracy\nGPT-neo 1.3B\nOPT 1.3B\nGPT-neo 2.7B\nOPT 2.7B\nGPT-j 6B\nOPT 6.7B\nOPT 13B\nGPT-neox 20B\nGPT-3 davinci 002\nGPT-3 davinci 003\nFigure 4: Per relationship type (n = the number of questions) results onPOPQA by model, showing overall accuracy\nand the correlation between accuracy and log popularity. We uniformly bin the questions by log (popularity), then\nreport the correlation between the bin center and the bin’s accuracy. We see thatboth subject entity popularity\nand relationship type are strong predictors of memorization across models. The correlation with popularity\nexists across relationship types and is stronger for larger LMs. We show a representative subset of relationship\ntypes and the complete results are in Figures 16 and 17 in Appendix C.1, including results on EntityQuestions.\nto exploit surface-level artifacts in subject entity\nnames (Poerner et al., 2020; Cao et al., 2021). Addi-\ntionally, models often output the most dominant an-\nswer entities for questions about relationship types\nwith fewer answer entities (e.g., red for the color\nrelationship type). In Figure 4, relationships with\nlower correlation (e.g., country, sport) often shows\nhigher accuracy, indicating that on those relation-\nship types, models may exploit surface-level clues.\nOn the other hand, for relationship types with rel-\natively low accuracy (e.g., occupation, author, di-\nrector), larger LMs often show a high correlation.\nFurther details are in Appendix C.1.\nScaling may not help with tail knowledge. As\nseen in the left column of Figure 4, there are clear\noverall performance improvements with scale on\nthe POPQA dataset. However, Figure 5 shows\nthat on both POPQA and EntityQuestions, most\nof scaling’s positive effect on parametric knowl-\nedge comes from questions with high popularity.\nSpecifically, for the questions about the entities\nwhose log10 (popularity) is larger than 4, there\nis an improvement in accuracy as model size in-\ncreases (red and yellow lines), while performance\non questions with lower popularity remains rela-\ntively constant (blue and green lines). For the 4,000\nleast popular questions, GPT-Neo 6B, 20B, and\nGPT-3 davinci-003 have 15%, 16%, and 19%\naccuracy, respectively.\nThis somewhat dampens prior works’ findings\nthat scaling up models significantly improves their\nfactual knowledge memorization (Roberts et al.,\n2020; Kandpal et al., 2022). We hypothesize that\n1.3B 2.7B 6B 20B ~175B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nPopQA\nGPT-neo\nOPT\nGPT-3 002\nGPT-3 003\n102\n103\n104\n105\nPopularity\n1.3B 2.7B 6B 20B ~175B\nModel parameters\n0.0\n0.2\n0.4\n0.6Accuracy\nEntityQuestions\n102\n103\n104\n105\nPopularity\nFigure 5: POPQA scaling results, broken down by\nquestion popularity level. Scaling mostly improves\nmemorization of more popular factual knowledge.\nError bars are 95% confidence intervals.\n9806\nthis is because their evaluations are often conducted\non QA datasets with popular entities. In sum, scal-\ning lowers the threshold of popularity for knowl-\nedge to be reliably memorized, but is not projected\nto move the threshold far into the long tail for prac-\ntical model scales.\nRelationship type results breakdown. Figure 6\nprovides a closer look at the relationship between\npopularity, accuracy, and relationship type; it\nshows model accuracy over the popularity distri-\nbutions for director and country. For the first two\ntypes, we can see a clear positive trend between\npopularity and accuracy across models, and as the\nmodel size gets larger, the LMs memorize more.\nOn the other hand, in the “country” relationship\ntype, no models show trends, while overall the ac-\ncuracy is high, indicating the LMs often exploit\nartifacts to answer less popular questions. We\nshow example models’ predictions in Appendix\nSection C.3.\n5 Non-parametric Memory Complements\nParametric Memory\nOur analysis indicates that even the current state-\nof-the-art LMs struggle with less popular subjects\nor certain relationship types, and increasing the\nmodel size does not lead to further performance\nimprovements. In light of this, we extend our anal-\nysis to non-parametric sources of knowledge, as\noutlined in Section ( RQ2). Specifically, we in-\nvestigate the effectiveness of retrieval-augmented\nLMs (Borgeaud et al., 2022; Lewis et al., 2020),\nwhich leverage non-parametric memories (i.e., re-\ntrieved text) to improve performance.\n5.1 Experimental Setup\nAugmenting input. In this work, we try a simple\nretrieval-augmented LM approach, where we run\nan off-the-shelf retrieval system off-line to retrieve\ncontext from Wikipedia relevant to a question,7 and\nthen we concatenate the retrieved context with the\noriginal question. Although increasing the context\nsize often leads to performance gains (Izacard and\nGrave, 2021; Asai et al., 2022), we only use the top\none retrieved paragraph for simplicity.\nRetrieval models. We use two widely-used re-\ntrieval systems: BM25 (Robertson et al., 2009)\n630 POPQA and 26 EntityQuestions questions had popu-\nlarity less than the smallest popularity bin, and are excluded\nto avoid showing results for small sample sizes.\n7We use Wikipedia dump from December 2018.\n0.0\n0.5\n1.0\nGPT-neo 1.3B OPT 13B\ndirector\nGPT-3 davinci-003\n0.0\n0.5\n1.0\ncountry\nFigure 6: Memorization versus popularity for three mod-\nels and the relationship types with the largest and small-\nest correlations. Within a relationship type, generally,\nthere is a monotonically increasing link between pop-\nularity and performance, except for “country”. Error\nbars show Wilson 95% confidence intervals.\nand Contriever (Izacard et al., 2022a). BM25 is a\nstatic term-based retriever without training, while\nContriever is pretrained on large unlabeled corpora,\nfollowed by fine-tuning on MS MARCO (Bajaj\net al., 2016). We also experiment with aparametric\naugmentation method, GenRead (Yu et al., 2022),\nwhich prompts LMs to generate rather than retrieve\na contextual document to answer a question. We\nuse the ten LMs in Section 4, resulting in 40 LMs\nand retrieval-augmented LMs.\nGPT-neo 1.3B GPT-neo 2.7B GPT-j 6B GPT-3 davinci 003\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Accuracy\nVanilla\nGenRead\nBM25\nContriever\nFigure 7: POPQA accuracy of LMs augmented with\nBM25, Contriever, GenRead, and unassisted (vanilla).\nRetrieving non-parametric memories significantly\nimproves the performance of smaller models. Com-\nplete results on POPQA are found in Figure 13. Enti-\ntyQuestions results are in Figure 14 of the Appendix.\n5.2 Results\nRetrieval largely improves performance. Fig-\nure 7 shows that augmenting LMs with non-\nparametric memories significantly outperforms\nunassisted vanilla LMs. A much smaller LM (e.g.,\nGPT-Neo 2.7B) augmented by the Contriever re-\ntrieval results outperforms vanilla GPT-3. Large\nLMs such as GPT-3 also enjoy the benefits of non-\nparametric memories. Contriever gives 7% accu-\nracy gains on top of GPT-3 davinci-003. Gen-\n9807\nRead shows little-to-no performance improvement\nover vanilla parametric knowledge for smaller mod-\nels, while the technique shows sizeable gains for\nGPT-3, especially davinci-003. In addition to its\nlimited effectiveness with smaller LMs, GenRead\nhas potentially prohibitive inference time costs,\nwith GPT-NeoX 20B taking 70 seconds per query.\nVanilla GenRead BM25 Contriever\n2\n 1\n 0 1 2\n0.2\n0.4\n0.6\n0.8Accuracy\nPopQA\n2\n 1\n 0 1 2 3\nRelative Popularity\n0.2\n0.3\n0.4\n0.5Accuracy\nEntityQuestions\nFigure 8: GPT-3 davinci-003 accuracy versus rela-\ntive popularity (how popular a question is relative to\nother questions of its relationship type). Retrieval-\naugmented LMs (dashed) outperform LMs’ para-\nmetric memory (solid) for less popular entities, while\nparametric memory is competitive for more popu-\nlar entities. Relative popularity is defined as the log-\npopularity of a question, normalized by the mean and\nstandard deviation of log-popularity for the question’s\nrelationship type (smaller for less popular entities).8 Fig-\nure 17 shows per-relationship results.\nNon-parametric memories are effective for less\npopular facts. How does retrieval augmentation\nlead to such significant improvements? Figure 8\nshows the relationship between the entity popular-\nity and models’ QA performance. It can be seen\nthat retrieval-augmented LMs guided by Contriever\nor BM25 have a clear advantage over unassisted\nvanilla LMs, especially on less popular entities, re-\nsulting in a significant performance gain. Overall,\nContriever-guided LMs outperform BM25-based\nones on POPQA, while the BM25-based models\nperform better on the least popular entities, consis-\ntent with the findings from Sciavolino et al. (2021).\nOn the other hand, for more popular entities, para-\nmetric knowledge shows equal or higher accuracy,\nindicating that the state-of-the-art LMs have al-\n8Error bars show Wilson 95% confidence intervals. Bins\nwith less than 40 samples have been excluded to avoid showing\nresults with exceedingly wide errorbars.\nContriever-augmented LM\nsucceeded failed\nLM succeeded 0.83 (24%) 0.14 (10%)\nLM failed 0.88 (17%) 0.11 (49%)\nTable 1: The recall@1 of Contriever for questions\nthat GPT-3 davinci-003 answered correctly and in-\ncorrectly with and without retrieval on POPQA. The\npercent of questions falling in each category is shown in\nparentheses. For 10% of questions, retrieval is harm-\nful due to low-quality retrieved text (0.14 recall@1).\nready memorized the answers, and augmenting in-\nput with retrieved-context doesn’t help much or\neven hurts the performance. Interestingly, Gen-\nRead generally outperforms vanilla LMs despite\nrelying on LMs’ parametric memory. This demon-\nstrates the effectiveness of elicitive prompting (Wei\net al., 2022; Sun et al., 2022) as observed in prior\nwork. However, like vanilla LMs, GenRead shows\nlow performance on less popular entities.\nNon-parametric memories can mislead LMs.\nWe conduct an in-depth analysis of why retrieval-\naugmented models suffer in more popular entities.\nWe hypothesize that retrieval results may not al-\nways be correct or helpful, and can mislead LMs.\nTo test this hypothesis, we group the questions\nbased on two axes: whether unassisted GPT-3\ndavinci-003 predict correctly or not, and whether\nretrieval-augmented predictions are correct or not.\nFor each of the four categories, we calculate re-\ncall@1 (whether a gold answer is included in the\ntop 1 document; Karpukhin et al. 2020).\nTable 1 shows recall@1 for each group with\npercentages of the questions falling into each of\nthe categories. For 10% of questions, retrieval-\naugmentation causes the LM to incorrectly answer\na question it could otherwise answer correctly. We\nfound that on those questions, recall@1 is signifi-\ncantly lower than the overall recall@1 (0.14 vs 0.42\noverall), indicating that failed retrieval can result\nin performance drops. Conversely, for the 17% of\nquestions for which retrieval causes the LM to cor-\nrectly answer a question it would otherwise have\nfailed to answer, the recall@1 is 0.88. We include\nexamples of both cases in Appendix Section C.3.\n6 Adaptive Retrieval: Using Retrieval\nOnly Where It Helps\nWhile incorporating non-parametric memories\nhelps in long-tail distributions, powerful LMs have\n9808\nalready memorized factual knowledge for popular\nentities, and retrieval augmentation can be harm-\nful. As outlined in ( RQ3), can we achieve the\nbest of both worlds? We propose a simple-yet-\neffective method, Adaptive Retrieval, which de-\ncides when to retrieve passages only based on in-\nput query information and augments the input with\nretrieved non-parametric memories only when nec-\nessary. We show that this is not only more powerful\nthan LMs or retrieval-augmented LMs always re-\ntrieving context, but also more efficient than the\nstandard retrieval-augmented setup.\n6.1 Method\nAdaptive Retrieval is based on our findings: as the\ncurrent best LMs have already memorized more\npopular knowledge, we can use retrieval only when\nthey do not memorize the factual knowledge and\nthus need to find external non-parametric knowl-\nedge. In particular, we use retrieval for questions\nwhose popularity is lower than a threshold (popu-\nlarity threshold), and for more popular entities, do\nnot use retrieval at all.\nUsing a development set, the threshold is cho-\nsen to maximize the adaptive accuracy, which we\ndefine as the accuracy attained by taking the predic-\ntions of the retrieval-augmented system for ques-\ntions below the popularity threshold and the pre-\ndictions based on parametric knowledge for the\nrest. We determine the popularity threshold inde-\npendently for each relationship type.\nGPT-neo 2.7B GPT-j 6B GPT-neox 20B GPT-3 davinci 003\n0.0\n0.2\n0.4\n0.6Accuracy\nVanilla\nBM25\nAdaptive Vanilla/BM25\nFigure 9: POPQA performance of GPT-neo models and\nGPT3 davinci-003, with different retrieval methods.\nAdaptive Retrieval robustly outperforms approaches\nthat always retrieve, especially for larger LMs.\n6.2 Results\nAdaptive Retrieval improves performance.\nFigure 9 shows the results when we adaptively re-\ntrieve non-parametric memories based on the per-\nrelationship type thresholds. We can see that adap-\ntively retrieving non-parametric memories is effec-\ntive for larger models. The best performance on\nPOPQA is using GPT-3 davinci-003 adaptively\nwith GenRead and Contriever, yielding 46.5% ac-\ncuracy, 5.3% higher than any non-adaptive method.\n1.3B 2.7B 6B 20B ~175B\nModel parameters\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Proportion using retrieval\nAdaptive Vanilla/BM25\nAdaptive Vanilla/Contriever\nAdaptive GenRead/BM25\nAdaptive GenRead/Contriever\nGPT-neo\nGPT-3 davinci 003\nFigure 10: The proportion of questions for which vari-\nous models use retrieval in the Adaptive Retrieval setup\non POPQA. When using Adaptive Retrieval, small mod-\nels must still rely on non-parametric memory for most\nquestions, while larger models have more reliable para-\nmetric memories enabling them to use retrieval less\noften.\nThe threshold shifts with LM scale. While\nAdaptive Retrieval shows performance gains for\nlarger models, smaller models do not realize the\nsame benefits; as shown in Figure 9, the per-\nformance gain from Adaptive Retrieval is much\nsmaller when we use models smaller than 10 bil-\nlion. Why does this happen? Figure 10 shows\nthat smaller LMs almost always retrieve, indicating\nthat there are not many questions for which small\nLMs’ parametric knowledge is more reliable than\nnon-parametric memory. In contrast, large models\ntypically retrieve much less. For example, GPT-\n3 davinci-003 only retrieves for 40% of ques-\ntions when paired with BM25, and even the much\nsmaller GPT-neox 20B does not retrieve documents\non more than 20% of the questions. On EntityQues-\ntions (Appendix Figure 15) all of the LMs retrieve\nmuch more, as the questions are mostly about less\npopular entities.\nAdaptive Retrieval reduces inference-time costs.\nWe also found that Adaptive Retrieval improves\nefficiency; if we know we do not need to retrieve\ndocuments, we can skip retrieval components and\nthe input length becomes shorter, which improves\nlatency in both retrieval and language model com-\nponents. Figure 11 shows the inference latency of\nGPT-J 6B and GPT-neox 20B, and API costs of\nGPT-3. Especially for larger LMs, concatenating\nretrieved context results in significantly increased\nlatency (e.g., for GPT-J 6B, the inference time la-\ntency almost doubles). Adaptive retrieval enables\nreducing inference time up to 9% from standard\n9809\nGPT-j 6B GPT-neox 20B0.0\n2.5\n5.0\n7.5Latency (sec)\nVanilla\nBM25\nAdaptive Vanilla/BM25\nGPT-3 davinci 0030\n1\n2\n3Cost ($/1000 queries)\nFigure 11: POPQA latency for large GPT-neo models\nthat were run on our machines, and API costs for GPT3.\nAdaptive retrieval reduces latency and API costs.\nGPT-3 davinci 0030.00\n0.25\n0.50\n0.75Accuracy\nVanilla\nBM25\nAdaptive Vanilla/BM25\nGPT-3 davinci 0030\n1\n2\n3Cost ($/1000 queries)\nFigure 12: Accuracy and cost savings of Adaptive Re-\ntrieval for EntityQuestions. Despite EntityQuestions’s\nlack of popular entities (see Figure 3), Adaptive Re-\ntrieval is able to reduce API costs by 15% while main-\ntaining equivalent performance to retrieval only.\nretrieval. We also observe cost reduction on Enti-\ntyQuestions, as shown in Figure 12.\n7 Discussion and Conclusions\nThis work conducts large-scale knowledge prob-\ning to examine the effectiveness and limitations\nof relying on LMs’ parameters to memorize fac-\ntual knowledge and to understand what factors\naffect factual knowledge memorization. Our re-\nsults show that memorization has a strong corre-\nlation with entity popularity and that scaling up\nmodels on long-tail distributions may only provide\nmarginal improvements. We also demonstrate that\nnon-parametric memories can greatly aid LMs on\nthese long-tail distributions, but can also mislead\nLMs on questions about well-known entities, as\npowerful LMs have already memorized them in\ntheir parameters. Based on those findings, we de-\nvise simple-yet-effective Adaptive Retrieval, which\nonly retrieves when necessary, using a heuristic\nbased on entity popularity and relationship types.\nOur experimental results show that this method\nis not only more powerful than LMs or previous\nretrieval-augmented LMs but also more efficient.\nLimitations\nThis work focuses on entity-centric factual knowl-\nedge and demonstrates that LMs’ memorization is\nheavily affected by the popularity of the entities\nand the aspect of the entities being asked in the\nquestions. It is important to emphasize that for\nrunning controlled experiments, we have relied on\ntwo synthetic datasets, and the extent to which our\nresults apply to naturally occurring factual knowl-\nedge has not been firmly established. While we can\nbe fairly confident about the relationship between\nscaling, retrieval, popularity, relationship type, and\nperformance for the kinds of knowledge studied\nhere, the effectiveness of Adaptive Retrieval will\ndepend on many details of the question answering\npipeline. Moreover, our work depends on a defini-\ntion of popularity that is time-dependent and may\nnot perfectly reflect how frequently entities are dis-\ncussed on the web. Wikipedia page views are one\npossible definition of popularity for which we ob-\nserve our results, and we invite others to improve\nupon it in future work. Further research can expand\nupon this simple approach, perhaps drawing on in-\nsights from Kadavath et al. (2022) to improve the\neffectiveness of Adaptive Retrieval.\nIt is an open question if the same findings are\napplicable to other types of world knowledge such\nas commonsense. We conjecture that the concept\nof the subject topic (entity), as well as the aspect\n(relationship type), can be applied with some mi-\nnor modifications, which future work can quantify\nmemorization following our scheme.\nEthical Considerations\nRecent work (Huang et al., 2022) shows that LMs\nmemorize personal information available on the\nweb, which has significant security issues. Our\nevaluation focuses on the memorization of general\nentity-centric knowledge, but our findings can be\napplicable to those areas. Our findings suggest\nthat LMs are likely to have less reliable knowledge\nof minority groups. Parrish et al. (2022) established\nthat models often rely on stereotypes to answer in\nuncertain cases, so our results indicate that LMs\nare likely to rely on stereotypes disproportionately\nfor minority groups. Future work could investi-\ngate whether retrieval augmentation reduces bias\nin these cases.\nAcknowledgements\nWe thank the UW NLP group members for their\nhelpful discussions, and Joongwon Kim, Wenya\nWang, and Sean Welleck for their insightful feed-\nback on this paper. This research was supported\nby NSF IIS-2044660, ONR N00014-18-1-2826,\n9810\nONR MURI N00014- 18-1-2670, and Allen Dis-\ntinguished Award. AM is funded by a Goldwater\nScholarship and AA is funded by the IBM PhD\nFellowship.\nReferences\nNancy E Adams. 2015. Bloom’s taxonomy of cognitive\nlearning objectives. Journal of the Medical Library\nAssociation.\nAkari Asai, Matt Gardner, and Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nknowledge-intensive NLP tasks. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nProceedings of the 39th International Conference on\nMachine Learning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning systems.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramèr, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models.\nHung-Ting Chen, Michael JQ Zhang, and Eunsol Choi.\n2022. Rich knowledge sources bring complex knowl-\nedge conflicts: Recalibrating models to reflect con-\nflicting evidence. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. PaLM: Scaling language\nmodeling with pathways.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nPaolo Ferragina and Ugo Scaiella. 2010. TAGME:\non-the-fly annotation of short text fragments (by\nwikipedia entities). In Proceedings of the 19th ACM\ninternational conference on Information and knowl-\nedge management.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning.\nJunxian He, Graham Neubig, and Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.\n2022. Are large pre-trained language models leaking\nyour personal information? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022a. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\n9811\nGrave. 2022b. Few-shot learning with retrieval aug-\nmented language models.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022. Language models (mostly)\nknow what they know.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime QA: What’s the answer right now?\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-\ntau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.\n2022. Nonparametric masked language model.\nAlicia Parrish, Angelica Chen, Nikita Nangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel Bowman. 2022. BBQ:\nA hand-built bias benchmark for question answering.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2022. Recitation-augmented language\nmodels.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\n9812\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2022. Generate\nrather than retrieve: Large language models are\nstrong context generators.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An\nopen bilingual pre-trained model.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\n9813\nAppendix\nA Details of P OPQA Constructions\nList of the relationship types and templates. In\nthis work, we use the following 16 relationship\ntypes, and the authors of this paper manually an-\nnotated templates to verbalize knowledge triple to\nnatural language questions. We show the final list\nof the templates used to create POPQA in Table 2.\nFigure 3 shows the distribution of subject pop-\nularity of POPQAand EntityQuestions versus the\npopular NQ benchmark. NQ may have multiple\nentities so the distribution of the least popular en-\ntity per question is shown. Subject entities from\nNQ were extracted using TagMe (Ferragina and\nScaiella, 2010) on the NQ-open development set\nwith a score threshold of 0.22. TagMe returns the\ntitle of a Wikidata entity which can be directly used\nto find popularity.\nRelationship Template\noccupation What is [subj] ’s occupation?\nplace of birth In what city was [subj] born?\ngenre What genre is [subj]?\nfather Who is the father of [subj] ?\ncountry In what country is [subj] ?\nproducer Who was the producer of [subj] ?\ndirector Who was the director of [subj] ?\ncapital of What is [subj] the capital of?\nscreenwriter Who was the screenwriter for [subj] ?\ncomposer Who was the composer of [subj] ?\ncolor What color is [subj] ?\nreligion What is the religion of [subj] ?\nsport What sport does [subj] play?\nauthor Who is the author of [subj] ?\nmother Who is the mother of [subj] ?\ncapital What is the capital of [subj] ?\nTable 2: Full list of the manually annotated templated\nused for POPQAcreations. [subj] denotes a place-\nholder for subject entities.\nKnowledge triples sampling. In the construc-\ntion of the POPQAdataset, knowledge triples are\nsampled with higher weight given to more popular\nentities, otherwise, the distribution would be dom-\ninated by the tail and we would not have enough\nhigh-popularity entities to complete our analysis.\nSpecifically, when considering whether to sample a\nparticular knowledge triple, we include the knowl-\nedge triple if and only if f >exp(8R − 6), where\nR ∼ U(0, 1) is a unit uniform pseudo-random\nnumber and f is the exact match term frequency of\nthe subject entity’s aliases in an 800 MB random\nsample of C4. To increase diversity, once 2000\nknowledge triples of a particular relation type have\nbeen sampled, they are no longer sampled.\nB Experimental Details\nComputational resources and API costs. GPT-\n3 API usage totaled to $275. We ran 14,282\nquestions through two GPT-3 davinci models us-\ning four different methods: vanilla experiments\ncost $13 ($0.46 per 1000 questions), Contriever-\naugmented experiments cost $88 ($3.08 per 1000\nquestions), BM25-augmented experiments cost $81\n($2.80 per 1000 questions), and GenRead experi-\nments cost $93 ($3.25 per 1000 questions).\nTo run experiments using LMs larger than\ntwo billion parameters, we use a single V100\nV olta GPU with 32GB GPU memories. We use\nint8bit (Zeng et al., 2022) quantization with OPT\n13 billion and GPT-Neo 20 billion models to make\nthem fit our GPUs. In our preliminary experiments\nusing GPT-Neo 6 billion, we did not observe a no-\ntable performance drop by using the quantization.\nConstructing few-shot contexts. For POPQA,\nwe sample few-shot examples stratified by relation-\nship type to diversify the samples: for each of the\n15 relationship types other than the one in the test\nquestion, we sample one random question-answer\npair to include in the context. For EntityQuestions,\nwe take a simple random sample of 15 question-\nanswer pairs because there are more than 16 rela-\ntionship types.\nDetails of deciding thresholds. We 75% of\nPOPQAto determine a popularity threshold for\neach relation type. Using brute force search, we\nselect the threshold to maximize the adaptive accu-\nracy, which we define as the accuracy attained by\ntaking the predictions of the retrieval-augmented\nsystem for questions below the popularity threshold\nand the predictions based on parametric knowledge\nfor the rest.\nWe then evaluate adaptive accuracy using the\nlearned thresholds on the remaining 25% of\nPOPQA, and repeat with 100 different random\nsplits and take the mean to obtain the reported adap-\ntive accuracy measurement.\n9814\nGPT-neo 1.3B\nOPT 1.3B\nGPT-neo 2.7B\nOPT 2.7BGPT-j 6BOPT 6.7BOPT 13B\nGPT-neox 20B\nGPT-3 davinci 002GPT-3 davinci 003\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Accuracy\nVanilla\nGenRead\nBM25\nContriever\nFigure 13: Accuracy by LMs and retrieval-augmented\nLMs on POPQA. This is an extension of Figure 7\nGPT-neo 1.3BGPT-neo 2.7B\nGPT-j 6B OPT 13B\nGPT-neox 20B\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Accuracy\nGPT-3 davinci 003\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Accuracy\nVanilla\nGenRead\nBM25\nContriever\nFigure 14: Accuracy by LMs and retrieval-augmented\nLMs on EntityQuestions.\n1.3B 2.7B 6B 20B ~175B\nModel parameters\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Proportion using retrieval\nAdaptive Vanilla/BM25\nAdaptive Vanilla/Contriever\nAdaptive GenRead/BM25\nAdaptive GenRead/Contriever\nGPT-neo\nGPT-3 davinci 003\nFigure 15: The proportion of questions for which Adap-\ntive Retrieval uses retrieval versus model size for Enti-\ntyQuestions.\nC Detailed Results\nC.1 LM results\nFull results of per-relationship type accuracy\nand correlation. Figure 16 shows the full result\nof per-relationship type accuracy for all relation-\nship types in POPQA. Figure 17 shows the correla-\ntions for all relation types. Figures 19 and 18 show\nthe same results for the EntityQuestions dataset.\nNegative correlations of capital on EntityQues-\ntions. As shown in Figure 19, the capital relation-\nship types on in EntityQuestions, while onPOPQA,\nthis relationship shows relatively high correlations.\nWe found that in EntityQuestions, this capital rela-\ntionship type has many low-popularity questions\nwhose answers are included in subject entity names\n(e.g., subject=\"canton of Marseille-Belsunce\", ob-\nject=\"Marseille\"). This causes performance to have\na U-shaped relationship with popularity for the cap-\nital relationship type, so if most of the questions\nsampled come from the top half of popularity, the\nlinear correlation will be positive, and vice versa.\nC.2 Retrieval-augmented LM results\nOverall performance of retrieval-augmented\nLMs. Figure 13 shows the overall performance of\n40 LMs and retrieval-augmented LMs on POPQA.\nRetrieval-augmentation largely improves perfor-\nmance across different LMs, and much smaller\nmodels (GPT-Neo 1.3B) can perform on per with\nGPT-3. Figure 14 shows the results on EntityQues-\ntions. Due to computational and time constraints,\nwe were only able to run vanilla and Contriever\nresults for most models.\nAdaptive Retrieval for EntityQuestions. Fig-\nure 15 shows the proportion of questions above the\nretrieval threshold for various models using Adap-\ntive Retrieval on EntityQuestions. Because Enti-\ntyQuestions has a large quantity of low-popularity\nquestions, models (especially smaller ones) must\nrely heavily on retrieval.\nFull results on all relationship types. Figure 20\nshows the full results on POPQA of the retrieval-\naugmented LMs and unassisted LMs on 16 rela-\ntionship types using three different LMs as back-\nbones. Figure 21 shows these results for GPT-3\ndavinci-003 on EntityQuestions.\nC.3 Qualitative Results\nTable 3 shows several examples onPOPQA, where\nGPT-3 davinci-003 answers correctly while the\nContriever-augmented version fails to answer.\nAlong with the low recall@1 of 0.14 for this group,\nTable 3 suggests that the most common reason re-\ntrieval can be harmful is that it retrieves a document\nabout a mistaken entity, such as a person with the\nsame name as the subject, or an entity that simply\nis not relevant to the question (as in the case of\n“Noel Black”).\n9815\noccupation\n(n=532)\nauthor\n(n=1514)\ndirector\n(n=1999)\ncountry\n(n=838)\ncapital of\n(n=363)\ncapital\n(n=645)\nreligion\n(n=338)\nsport\n(n=547)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nfather\n(n=570)\nplace of birth\n(n=584)\ncomposer\n(n=978)\nproducer\n(n=1520)\ngenre\n(n=1619)\nmother\n(n=187)\nscreenwriter\n(n=1999)\ncolor\n(n=34)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-neo 1.3B\nOPT 1.3B\nGPT-neo 2.7B\nOPT 2.7B\nGPT-j 6B\nOPT 6.7B\nOPT 13B\nGPT-neox 20B\nGPT-3 davinci 002\nGPT-3 davinci 003\nFigure 16: Accuracy on PopQA for all relationship types and models. This is an extension of Figure 4.\noccupation\n(n=532)\nauthor\n(n=1514)\ndirector\n(n=1999)\ncountry\n(n=838)\ncapital of\n(n=363)\ncapital\n(n=645)\nreligion\n(n=338)\nsport\n(n=547)\n0.0\n0.2\n0.4\n0.6\n0.8\nCorrelation between\nlog(popularity) and accuracy\nfather\n(n=570)\nplace of birth\n(n=584)\ncomposer\n(n=978)\nproducer\n(n=1520)\ngenre\n(n=1619)\nmother\n(n=187)\nscreenwriter\n(n=1999)\ncolor\n(n=34)\n0.0\n0.2\n0.4\n0.6\n0.8\nCorrelation between\nlog(popularity) and accuracy\nGPT-neo 1.3B\nOPT 1.3B\nGPT-neo 2.7B\nOPT 2.7B\nGPT-j 6B\nOPT 6.7B\nOPT 13B\nGPT-neox 20B\nGPT-3 davinci 002\nGPT-3 davinci 003\nFigure 17: Correlations on PopQA for all relationship types and models. This is an extension of Figure 4.\ngenre\n(n=739)\nnotable\nwork\n(n=192)\noccupation\n(n=878)\nplace of\nbirth\n(n=891)\neducated\nat\n(n=874)\nheadquarters\nlocation...\n(n=917)\ncreator\n(n=623)\nlocation\nof formati...\n(n=736)\ncountry\n(n=837)\ncountry\nof origin\n(n=672)\nmanufa\nturer\n(n=861)\nlocation\n(n=891)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAaccuracy\nperformer\n(n=644)\nposition\nplayed on ...\n(n=846)\nauthor\n(n=756)\nowned by\n(n=872)\nlanguage\nof work or...\n(n=429)\nspouse\n(n=876)\nchild\n(n=856)\nlocated\nin the adm...\n(n=791)\nrecord\nlabel\n(n=658)\nfounded by\n(n=439)\ncapital\n(n=685)\nplace of\ndeath\n(n=882)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nGPT-neo 1.3B\nGPT-neo 2.7B\nGPT-j 6B\nOPT 13B\nGPT-neox 20B\nGPT-3 davinci 003\nFigure 18: Accuracy on EntityQuestions for all relationship types and models.\n9816\ngenre\n(n=739)\nnotable\nwork\n(n=192)\noccupation\n(n=878)\nplace of\nbirth\n(n=891)\neducated\nat\n(n=874)\nheadquarters\nlocation...\n(n=917)\ncreator\n(n=623)\nlocation\nof formati...\n(n=736)\ncountry\n(n=837)\ncountry\nof origin\n(n=672)\nmanufa\nturer\n(n=861)\nlocation\n(n=891)\n0.0\n0.2\n0.4\n0.6\n0.8\nCorrelation between\nlog(popularity) and accuracy\nperformer\n(n=644)\nposition\nplayed on ...\n(n=846)\nauthor\n(n=756)\nowned by\n(n=872)\nlanguage\nof work or...\n(n=429)\nspouse\n(n=876)\nchild\n(n=856)\nlocated\nin the adm...\n(n=791)\nrecord\nlabel\n(n=658)\nfounded by\n(n=439)\ncapital\n(n=685)\nplace of\ndeath\n(n=882)\n0.2\n0.0\n0.2\n0.4\n0.6\nCorrelation between\nlog(popularity) and accuracy\nGPT-neo 1.3B\nGPT-neo 2.7B\nGPT-j 6B\nOPT 13B\nGPT-neox 20B\nGPT-3 davinci 003\nFigure 19: Correlations on EntityQuestions for all relationship types and models.\n0.0\n0.5\n1.0\noccupation author director country capital of capital religion sport\n0.0\n0.5\n1.0\nproducer mother place of birth composer genre father color screenwriter\n0.0\n0.5\n1.0\noccupation author director country capital of capital religion sport\n0.0\n0.5\n1.0\nproducer mother place of birth composer genre father color screenwriter\n0.0\n0.5\n1.0\noccupation author director country capital of capital religion sport\n103 106\nPopularity\n0.0\n0.5\n1.0Accuracy\nproducer\nVanilla\nGenRead\nBM25\nContriever\n103 106\nmother\n103 106\nplace of birth\n103 106\ncomposer\n103 106\ngenre\n103 106\nfather\n103 106\ncolor\n103 106\nscreenwriter\nGPT-neo 1.3BOPT 13BGPT-3 DaVinci 003\nFigure 20: Accuracy for 3 models on POPQA versus popularity as shown in Figure 8 broken down by relationship\ntype. Popularity bins with less than 5 samples are excluded to avoid cluttering the figures with noisy results that\nhave wide error bars.\n9817\n0.0\n0.5\n1.0\ngenre notable work occupation place of birth educated at\nheadquarters\nlocation creator\nlocation of\nformation\n0.0\n0.5\n1.0\ncountry country of origin manufacturer location performer\nposition played\non team / speciality author owned by\n103 106\nPopularity\n0.0\n0.5\n1.0Accuracy\nlanguage of work\nor name\nVanilla\nGenRead\nBM25\nContriever\n103 106\nspouse\n103 106\nchild\n103 106\nlocated in the\nadministrative territorial entity\n103 106\nrecord label\n103 106\nfounded by\n103 106\ncapital\n103 106\nplace of death\nFigure 21: Accuracy versus popularity for GPT-3 davinci-003 on EntityQuestions broken down by relationship\ntype. Popularity bins with less than 5 samples are excluded.\nTable 4 shows several examples on POPQA,\nwhere GPT-3davinci-003 answers correctly only\nwhen augmented with Contriever. The recall@1\nfor this case is 0.88, which is significantly higher\nthan the overall recall. Note that in the second\nexample, the retrieval caused the LM to answer cor-\nrectly, but only by coincidence: the subject entity\n“Pierre” actually refers to the city in South Dakota,\nnot the Basketball player. Otherwise, retrieval ap-\npears to be helpful because it provides the relevant\ninformation directly.\n9818\nQuestion (answer) LM Retrieval-LM Retrieval\nWho was the director of\nBlack? (Sanjay Leela\nBhansali)\nThe director of\nBlack is Sanjay\nLeela Bhansali.\n✓\nNoel Black ✗ Noel Black (June 30, 1937 – July 5, 2014) was an Amer-\nican film and television director, screenwriter, and pro-\nducer. Black was born in Chicago, Illinois. He won\nawards at the 1966 ... and subsequently concentrated on\ndirecting for television, occasionally directing films such\nas \"Private School\".\nWho was the producer\nof The Faculty? (Robert\nRodriguez)\nThe Faculty\nwas produced\nby Elizabeth\nAvellan and\nRobert Ro-\ndriguez. ✓\nThe producer of\nThe Faculty was\nElizabeth Avel-\nlan. ✗\nThe Faculty is a 1998 American science fiction teen\nhorror film written by Kevin Williamson, directed by\nRobert Rodriguez, and starring Elijah Wood,... The\nfilm was released on Christmas Day, 1998, by Miramax\nFilms through Dimension Films. ... The film has devel-\noped a cult following in the years since its release.\nWhat is Michael\nShelley’s occupation?\n(singer-songwriter)\nMichael Shelley\nis a singer-\nsongwriter and\nmusician. ✓\nMichael Shelley\nis an American\napplied mathe-\nmatician. ✗\nMichael Shelley (mathematician) Michael J. Shelley\n(born August 17, 1959) is an American applied mathe-\nmatician who works on the modeling and simulation of\ncomplex systems arising in physics and biology. This\nhas included free-boundary problems in fluids and mate-\nrials science... He is also the co-founder and co-director\nof the Courant Institute’s Applied Mathematics Lab.\nIn what city was Zijah\nSokolovi´c born? (Sara-\njevo)\nZijah Sokolovi´c\nwas born in\nSarajevo ✓\nZijah Sokolovi´c\nwas born in\nOrahovac,\nKingdom ✗\nAli Sokol ... (born 8 May 1921 in Orahovac, Kingdom\nof Serbs, Croats and Slovenes, died 23 September 1974)\nwas a Yugoslav pulmonologist . Ali Sokol was born into\nan agricultural family. He was the third of four children\nof father and mother Hatixhes Solomon. It is unknown\nthe exact date of birth but the most reliable date is May\n8 year in 1921.\nTable 3: Qualitative examples of the questions where only retrieval-augmented LMsfail to answer correctly. The\nblue underlined text indicates the sub-strings matching the gold answers in the retrieved context.\n9819\nQuestion (answer) LM Retrieval-LM Retrieval\nWhat genre is Un-\nknown? (fantasy)\nUnknown is not\na specific genre\nof music. It\ncould refer to ✗\nUnknown is a\npulp fantasy fic-\ntion magazine.\n✓\nUnknown (magazine) Unknown (also known as Un-\nknown Worlds) was an American pulp fantasy fiction\nmagazine, published from 1939 to 1943 by Street &\nSmith, and edited by John W. Campbell. ... The leading\nfantasy magazine in the 1930s was \"Weird Tales\", which\nfocused on shock and horror.\nIn what country is\nPierre? (United States)\nPierre is in\nFrance. ✗\nPierre is from\nthe United\nStates. ✓\nJonas Farley Pierre (born May 30, 1980) is an American\nprofessional basketball player. A center, he has played\nwith three teams in the National Basketball League of\nCanada (NBL). Pierre last played with the Saint John\nMill Rats in Canada. ... He then joined Lincoln Uni-\nversity in Missouri for his third year. In the summer of\n2003, he signed a letter of intent to play with Nicholls\nState in the NCAA Division I.\nWho was the producer\nof The Cocoanuts?\n(Walter Wanger)\nThe Cocoanuts\nwas produced\nby Florenz\nZiegfeld. ✗\nThe Cocoanuts\nwas produced\nfor Paramount\nPictures by\nWalter Wanger,\nwho ✓\nThe Cocoanuts is a 1929 musical comedy film starring\nthe Marx Brothers. Produced for Paramount Pictures by\nWalter Wanger, who is not credited, the film stars the\nfour Marx Brothers, Oscar Shaw, Mary Eaton, and Mar-\ngaret Dumont. It was the first sound film to credit more\nthan one director (Robert Florey and Joseph Santley),\nand was adapted to the screen by Morrie Ryskind from\nthe George S. Kaufman Broadway musical play. ...\nWho was the director of\nThe White Suit? (Lazar\nRistovski)\nThe White Suit\nwas directed\nby Sachin\nKundalkar. ✗\nLazar Ristovski\n✓\nIn 1999 \"The White Suit\" an auteur film by Ristovski\n(director, writer, lead actor, and producer) was at the\nCannes Film Festival in the Critics Week program. \"The\nWhite Suit\" was the Serbian entry for the 1999 Academy\nAwards. Lazar Ristovski is the sole owner of Zillion\nFilm Company In 2006, he made a small appearance\nin the James Bond film \"Casino Royale\". He played\nCaruso in the 2004 movie \"King of Thieves\". He starred\nas Ðor ¯de in the award-winning 2009 film \"St. George\nShoots the Dragon\".\nTable 4: Qualitative examples of the questions where only retrieval-augmented LMssuccessfully answer correctly.\nThe blue underlined text indicates the sub-strings matching the gold answers in the retrieved context.\n9820\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7: Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 7: Ethical Considerations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3.3\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3.3\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe license is in our GitHub repository, which will be linked to from the abstract in the non-anonymous\nversion.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nOur dataset only contains data from Wikidata, which is widely used for NLP experiments and is\nalready publicly available.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nOur dataset only contains data from Wikidata, which is widely used for NLP experiments and is\nalready publicly available.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nAppendix A\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9821\nC □\u0013 Did you run computational experiments?\nSections 4, 5, and 6\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSections 4.1 and 5.1, Appendix B\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSections 4, 5, and 6\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSections 4, 5, and 6, Appendix C\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix A\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9822",
  "topic": "Parametric statistics",
  "concepts": [
    {
      "name": "Parametric statistics",
      "score": 0.7589941620826721
    },
    {
      "name": "Computer science",
      "score": 0.7349535226821899
    },
    {
      "name": "Statistics",
      "score": 0.09529361128807068
    },
    {
      "name": "Mathematics",
      "score": 0.08155626058578491
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}