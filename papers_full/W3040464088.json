{
  "title": "Integrating Multimodal Information in Large Pretrained Transformers",
  "url": "https://openalex.org/W3040464088",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5007498361",
      "name": "Wasifur Rahman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100656463",
      "name": "Md. Kamrul Hasan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040815133",
      "name": "Sangwu Lee",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5112033266",
      "name": "Amir Zadeh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055221257",
      "name": "Chengfeng Mao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5081398601",
      "name": "Louis‚ÄêPhilippe Morency",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059213806",
      "name": "Ehsan Hoque",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2947476638",
    "https://openalex.org/W2095176743",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2963710346",
    "https://openalex.org/W2798965674",
    "https://openalex.org/W2029996593",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2901272442",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W2465534249",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2883159046",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2808359495",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2738581557",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2075398069",
    "https://openalex.org/W2805662932",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2772633765",
    "https://openalex.org/W2787560479"
  ],
  "abstract": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP. Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream. While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication). Pre-trained models don't have the necessary components to accept two extra modalities of vision and acoustic. In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG). MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning. It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities. In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis. Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet. On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",
  "full_text": "Integrating Multimodal Information in Large Pretrained Transformers\nWasifur Rahman1, Md. Kamrul Hasan1*, Sangwu Lee1*, Amir Zadeh2,\nChengfeng Mao2, Louis-Philippe Morency2, Ehsan Hoque1\n1 - Department of Computer Science, University of Rochester, USA\n2 - Language Technologies Institute, SCS, CMU, USA\nechowdh2@ur.rochester.edu, mhasan8@cs.rochester.edu,\nslee232@u.rochester.edu,abagherz@cs.cmu.edu,\nchengfem@andrew.cmu.edu,morency@cs.cmu.edu,\nmehoque@cs.rochester.edu\nAbstract\nRecent Transformer-based contextual word\nrepresentations, including BERT and XLNet,\nhave shown state-of-the-art performance in\nmultiple disciplines within NLP. Fine-tuning\nthe trained contextual models on task-speciÔ¨Åc\ndatasets has been the key to achieving supe-\nrior performance downstream. While Ô¨Åne-\ntuning these pre-trained models is straight-\nforward for lexical applications (applications\nwith only language modality), it is not trivial\nfor multimodal language (a growing area in\nNLP focused on modeling face-to-face com-\nmunication). Pre-trained models don‚Äôt have\nthe necessary components to accept two ex-\ntra modalities of vision and acoustic. In\nthis paper, we proposed an attachment to\nBERT and XLNet called Multimodal Adapta-\ntion Gate (MAG). MAG allows BERT and XL-\nNet to accept multimodal nonverbal data dur-\ning Ô¨Åne-tuning. It does so by generating a\nshift to internal representation of BERT and\nXLNet; a shift that is conditioned on the vi-\nsual and acoustic modalities. In our experi-\nments, we study the commonly used CMU-\nMOSI and CMU-MOSEI datasets for multi-\nmodal sentiment analysis. Fine-tuning MAG-\nBERT and MAG-XLNet signiÔ¨Åcantly boosts\nthe sentiment analysis performance over pre-\nvious baselines as well as language-only Ô¨Åne-\ntuning of BERT and XLNet. On the CMU-\nMOSI dataset, MAG-XLNet achieves human-\nlevel multimodal sentiment analysis perfor-\nmance for the Ô¨Årst time in the NLP commu-\nnity. 1\n1 Introduction\nHuman face-to-face communication Ô¨Çows as a\nseamless integration of language, acoustic, and vi-\nsion modalities. In ordinary everyday interactions,\n1This paper has been accepted in Association for Compu-\ntational Linguistics (ACL) 2020 conference\n* - Equal contribution\nwe utilize all these modalities jointly to convey our\nintentions and emotions. Understanding this face-\nto-face communication falls within an increasingly\ngrowing NLP research area called multimodal lan-\nguage analysis (Zadeh et al., 2018b). The biggest\nchallenge in this area is to efÔ¨Åciently model the\nthree pillars of communication together. This gives\nartiÔ¨Åcial intelligence systems the capability to com-\nprehend the multi-sensory information without dis-\nregarding nonverbal factors. In many applications\nsuch as dialogue systems and virtual reality, this\ncapability is crucial to maintain the high quality of\nuser interaction.\nThe recent success of contextual word rep-\nresentations in NLP is largely credited to new\nTransformer-based (Vaswani et al., 2017) models\nsuch as BERT (Devlin et al., 2018) and XLNet\n(Yang et al., 2019). These Transformer-based mod-\nels have shown performance improvement across\ndownstream tasks (Devlin et al., 2018). However,\ntheir true downstream potential comes from Ô¨Åne-\ntuning their pre-trained models for particular tasks\n(Devlin et al., 2018). This is often done easily for\nlexical datasets which exhibit language modality\nonly. However, this Ô¨Åne-tuning for multimodal\nlanguage is neither trivial nor yet studied; simply\nbecause both BERT and XLNet only expect lin-\nguistic input. Therefore, in applying BERT and\nXLNet to multimodal language, one must either\n(a) forfeit the nonverbal information and Ô¨Åne-tune\nfor language, or (b) simply extract word represen-\ntations and proceed to use a state-of-the-art model\nfor multimodal studies.\nIn this paper, we present a successful framework\nfor Ô¨Åne-tuning BERT and XLNet for multimodal\ninput. Our framework allows the BERT and XL-\nNet core structures to remain intact, and only at-\ntaches a carefully designed Multimodal Adaptation\nGate (MAG) to the models. Using an attention\nconditioned on the nonverbal behaviors, MAG es-\narXiv:1908.05787v3  [cs.LG]  21 Nov 2020\nsentially maps the informative visual and acoustic\nfactors to a vector with a trajectory and magnitude.\nDuring Ô¨Åne-tuning, this adaptation vector modiÔ¨Åes\nthe internal state of the BERT and XLNet, allowing\nthe models to seamlessly adapt to the multimodal\ninput. In our experiments we use the CMU-MOSI\n(Zadeh et al., 2016) and CMU-MOSEI (Zadeh\net al., 2018d) datasets of multimodal language, with\na speciÔ¨Åc focus on the core NLP task of multimodal\nsentiment analysis. We compare the performance\nof MAG-BERT and MAG-XLNet to the above (a)\nand (b) scenarios in both classiÔ¨Åcation and regres-\nsion sentiment analysis. Our Ô¨Åndings demonstrate\nthat Ô¨Åne-tuning these advanced pre-trained Trans-\nformers using MAG yields consistent improvement,\neven though BERT and XLNet were never trained\non multimodal data.\nThe contributions of this paper are therefore sum-\nmarized as:\n‚Ä¢ We propose an efÔ¨Åcient framework for Ô¨Åne-\ntuning BERT and XLNet for multimodal lan-\nguage data. This framework uses a component\ncalled Multimodal Adaptation Gate (MAG)\nthat introduces minimal overhead to both the\nmodels.\n‚Ä¢ MAG-BERT and MAG-XLNet set new state\nof the art in both CMU-MOSI and CMU-\nMOSEI datasets, when compared to scenarios\n(a) and (b). For CMU-MOSI, MAG-XLNet\nachieves performance on par with reported\nhuman performance.\n2 Related Works\nThe studies in this paper are related to the following\nresearch areas:\n2.1 Multimodal Language Analyses\nMultimodal language analyses is a recent research\ntrend in natural language processing (Zadeh et al.,\n2018b) that helps us understand language from\nthe modalities of text, vision and acoustic. These\nanalyses have particularly focused on the tasks of\nsentiment analysis (Poria et al., 2018), emotion\nrecognition (Zadeh et al., 2018d), and personality\ntraits recognition (Park et al., 2014). Works in\nthis area often focus on novel multimodal neural\narchitectures (Pham et al., 2019; Hazarika et al.,\n2018) and multimodal fusion approaches (Liang\net al., 2018; Tsai et al., 2018).\nRelated to content in this paper, we discuss\nsome of the models in this domain including TFN,\nMARN, MFN, RMFN and MulT. Tensor Fusion\nNetwork (TFN) (Zadeh et al., 2017) creates a\nmulti-dimensional tensor to explicitly capture all\npossible interactions between the three modali-\nties: unimodal, bimodal and trimodal. Multi-\nattention Recurrent Network (MARN) (Zadeh et al.,\n2018c) uses three separate hybrid LSTM memories\nthat have the ability to propagate the cross-modal\ninteractions. Memory Fusion Network (Zadeh\net al., 2018a) synchronizes the information from\nthree separate LSTMs through a multi-view gated\nmemory. Recurrent Memory Fusion Network\n(RMFN) (Liang et al., 2018) captures the nuanced\ninteractions among the modalities in a multi-stage\nmanner, giving each stage the ability to focus\non a subset of signals. Multimodal Transformer\nfor Unaligned Multimodal Language Sequences\n(MulT) (Tsai et al., 2019) deploys three Transform-\ners ‚Äì each for one modality ‚Äì to capture the in-\nteractions with the other two modalities in a self-\nattentive manner. The information from the three\nTransformers are aggregated through late-fusion.\n2.2 Pre-trained Language Representations\nLearning word representations from large cor-\npora has been an active research area in NLP\ncommunity (Mikolov et al., 2013; Pennington\net al., 2014). Glove (Pennington et al., 2014) and\nWord2Vec (Mikolov et al., 2013) contributed to\nadvancing the state-of-the-art of many NLP tasks.\nA major setback of these word representations is\ntheir non-contextual nature. Recently, contextual\nlanguage representation models trained on large\ntext corpora have achieved state of the art results\non several NLP tasks including question answer-\ning, sentiment classiÔ¨Åcation, part-of-speech (POS)\ntagging and similarity modeling(Peters et al., 2018;\nDevlin et al., 2018). The Ô¨Årst two notable con-\ntextual representation based models were ELMO\n(Peters et al., 2018) and GPT (Radford et al., 2018).\nHowever, they only captured unidirectional context\nand therefore, missed more nuanced interactions\namong words of a sentence. BERT (Bidirectional\nEncoder Representations from Transformers) (De-\nvlin et al., 2018) outperforms both ELMO and GPT\nsince it can provide better representation through\ncapturing bi-directional context using Transform-\ners. XLNet(Dai et al., 2019) gives new contextual\nrepresentations through building an auto-regressive\nmodel capable of capturing all possible factoriza-\ntions of the input. Fine-tuning pretrained mod-\nels for BERT and XLNet has been a key factor in\nachieving state of the art performance for down-\nstream tasks. Even though previous works have\nexplored using BERT to model multimodal data\n(Sun et al., 2019), to the best of our knowledge, di-\nrectly Ô¨Åne-tuning BERT or XLNet for multimodal\ndata has not been explored in previous works.\n3 BERT and XLNet\nTo better understand the proposed multimodal\nframework in this paper, we Ô¨Årst present an\noverview of both the BERT and XLNet models.\nWe start by quickly formalizing the operations\nwithin Transformer and Transformer-XL models,\nfollowed by an overview of BERT and XLNet.\n3.1 Transformer\nTransformer is a non-recurrent neural architecture\ndesigned for modeling sequential data (Vaswani\net al., 2017). The superior performance of Trans-\nformer model is largely credited to a Multi-head\nSelf-Attention module. Using this module, each el-\nement of a sequence is attended by conditioning on\nall the other sequence elements. Figure 2 summa-\nrizes internal operations of a Transformer layer (for\nM such layers). Commonly, a Transformer uses\nan encoder-decoder paradigm. A stack of encoders\nis followed by a stack of decoders to map an input\nsequence to an output sequence. An additional em-\nbedding step with Positional Input Embedding is\napplied before the input goes through the stack of\nencoders and decoders.\n3.2 Transformer-XL\nTransformer-XL (Dai et al., 2019) is an extension\nof the Transformer which offers two improvements:\na) it enhances the capability of the Transformer\nto capture long-range dependencies (speciÔ¨Åcally\nfor the case of context fragmentation), and b) it\nimproves the capability to better predict Ô¨Årst few\nsymbols (which are often crucial for the rest of the\nsequence). It does so with a recurrence mechanism\ndesigned to pass context information from one seg-\nment to the next and a relative positional encoding\nmechanism to enable state reuse without causing\ntemporal confusion.\n3.3 BERT\nBERT is a successful language model that pro-\nvides rich contextual word representation (Devlin\net al., 2018). It follows an auto-encoding approach\n‚Äì masking out a portion of input tokens and pre-\ndicting those tokens based on all other non-masked\ntokens ‚Äì and thus learning a vector representation\nfor the masked out tokens in that process. We\nuse the variant of BERT used for Single Sentence\nClassiÔ¨Åcation Tasks. First, input embeddings are\ngenerated from a sequence of word-piece tokens\nby adding token embeddings, segment embeddings\nand position embeddings . Then multiple Encoder\nlayers are applied on top of these input embeddings.\nEach Encoder has a Multi-Head Attention layer and\na Feed Forward layer, each followed by a residual\nconnection with layer normalization. A special\n[CLS] token is appended in front of the input token\nsequence. So, for a N length input sequence, we\nget N+1 vectors from the last Encoder layer ‚Äì the\nÔ¨Årst of those vectors is used to predict the label\nof the input after that vector undergoes an afÔ¨Åne\ntransformation.\n3.4 XLNet\nXLNet (Yang et al., 2019) sets out to improve two\ncritical aspects of the BERT model: a) indepen-\ndence among the masked out tokens and b) pretrain-\nÔ¨Ånetune discrepancy in training vs inference, since\ninference inputs do not have masked out tokens.\nXLNet is an auto-regressive model and therefore,\nis free from the need of masking out certain tokens.\nHowever, auto-regressive models usually capture\nthe unidirectional context (either forward or back-\nward). XLNet can learn bidirectional context by\nmaximizing likelihood over all possible permuta-\ntions of factorization order. In essence, it randomly\nsamples multiple factorization orders and trains the\nmodel on each of those orders. Therefore, it can\nmodel input by taking all possible permutations\ninto consideration (in expectation).\nXLNet utilizes two key ideas from Transformer-\nXL (Dai et al., 2019): relative positioning and seg-\nment recurrence mechanism. Like BERT, it also\nhas a Input Embedder followed by multiple En-\ncoders. The Embedder converts the input tokens\ninto vectors after adding token embedding, segment\nembedding and relative positional embedding in-\nformation. Each encoder consists of a Multi-Head\nattention layer and a feed forward layer ‚Äì each\nfollowed by a residual addition and normalization\nlayer. The embedder output is fed into the encoders\nto get a contextual representation of input.\nAttentionGating\nShifting\nLexicalInputAcoustic InputVisual Inputùëç\" ùê¥\" ùëâ\"\nùêª\"\n&ùëç\"\nFigure 1: Multimodal Adaptation Gate (MAG) takes\nas input a lexical input vector, as well as its visual and\nacoustic accompaniments. Subsequently, an attention\nover lexical and nonverbal dimensions is used to fuse\nthe multimodal data into another vector, which is sub-\nsequently added to the input lexical vector (shifting).\n4 Multimodal Adaptation Gate (MAG)\nIn multimodal language, a lexical input is accom-\npanied by visual and acoustic information - simply\ngestures and prosody co-occurring with language.\nConsider a semantic space that captures latent con-\ncepts (positions in the latent space) for individual\nwords. In absence of multimodal accompaniments,\nthe semantic space is directly conditioned on the\nlanguage manifold. Simply put, each word falls\nwithin some part of this semantic space, depending\nonly on the meaning of the word in a linguistic\nstructure (i.e. sentence). Nonverbal behaviors can\nhave an impact on the meaning of words, and there-\nfore on the position of words in this semantic space.\nTogether, language and nonverbal accompaniments\ndecide on the new position of the word in the se-\nmantic space. In this paper, we regard to this new\nposition as addition of the language-only position\nwith a displacement vector; a vector with trajec-\ntory and magnitude that shifts the language-only\nposition of the word to the new position in light of\nnonverbal behaviors. This is the core philosophy\nbehind the Multimodal Adaptation Gate (MAG).\nA particularly appealing implementation of such\ndisplacement is studied in RA VEN (Wang et al.,\n2018), where displacements are calculated using\ncross-modal self-attention to highlight relevant non-\nverbal information. Figure 1 shows the studied\nMAG in this paper. Essentially, a MAG unit re-\nceives three inputs, one is purely lexical, one is\nvisual, and the last one is acoustic. Let the triplet\n(Zi,Ai,Vi) denote these inputs for ith word in a\nsequence. We break this displacement into bimodal\nfactors [Zi; Ai]and [Zi; Vi]by concatenating lex-\nical vector with acoustic and visual information\nrespectively and use them to produce two gating\nvectors gv\ni and ga\ni :\ngv\ni =R(Wgv[Zi; Vi]+bv) (1)\nga\ni =R(Wga[Zi; Ai]+ba) (2)\nwhere Wgv, Wga are weight matrices for visual and\nacoustic modality and bv and ba are scalar biases.\nR(x) is a non-linear activation function. These\ngates highlight the relevant information in visual\nand acoustic modality conditioned on the lexical\nvector.\nWe then create a non-verbal displacement vector\nHi by fusing together Ai and Vi multiplied by their\nrespective gating vectors:\nHi =ga\ni ‚ãÖ(WaAi) +gv\ni ‚ãÖ(WvVi) +bH (3)\nwhere Wa and Wv are weight matrices for acoustic\nand visual information respectively and bH is the\nbias vector.\nSubsequently, we use a weighted summation be-\ntween Zi and its nonverbal displacement Hi to cre-\nate a multimodal vector ¬ØZi:\n¬ØZi =Zi +Œ±Hi (4)\nŒ±=min( /parallel.alt1Zi/parallel.alt12\n/parallel.alt1Hi/parallel.alt12\nŒ≤,1) (5)\nwhere Œ≤is a hyper-parameter selected through the\ncross-validation process. /parallel.alt1Zi/parallel.alt12 and /parallel.alt1Hi/parallel.alt12 denote\nthe L2 norm of the Zi and Hi vectors respectively.\nWe use the scaling factorŒ±so that the effect of non-\nverbal shift Hi remains within a desirable range.\nFinally, we apply a layer normalization and dropout\nlayer to ¬ØZi.\nInput Embedder\n‚Ä¶.ùê∂ùêøùëÜ\nMultimodalAdaptation Gate\nMulti-Head Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\nùêø1 ùêø2 ùêøùëÅ\n‚Ä¶.ùê∏ùê∂ùêøùëÜùê∏1 ùê∏2 ùê∏ùëÅ\nùëç)*ùê¥)ùëâ)\nMultimodalAdaptation Gate\nùëç-*ùê¥-ùëâ-\nMultimodalAdaptation Gate\nùëç.*ùê¥.ùëâ.\nMulti-Head Attention\nFeed Forward\nAdd & Norm\nAdd & Norm\n(ùëÄ‚àíùëó)√ó\nùëó√ó\nÃÖùëç678* ÃÖùëç)* ÃÖùëç-* ÃÖùëç.*\nÃÖùëç6789 ÃÖùëç)9 ÃÖùëç-9 ÃÖùëç.9\n‚Ä¶.\nFigure 2: Best viewed zoomed in and in color. The\nTransformer architecture of BERT/XLNet with MAG\napplied at jth layer. We consider a total of M layers\nwithin the pretrained Transformer. MAG can be ap-\nplied at different layers of the pretrained Transformers.\n4.1 MAG-BERT\nMAG-BERT is a combination of MAG applied to\na certain layer of BERT network (Figure 2 demon-\nstrates the structure of MAG-BERT as well as\nMAG-XLNet). Essentially, at each layer, BERT\ncontains lexical vectors for ith word in the se-\nquence. For the same word, nonverbal accompa-\nniments are also available in multimodal language\nsetup. MAG essentially forms an attachment to the\ndesired layer in BERT; an attachment that allows\nfor multimodal information to leak into the BERT\nmodel and displace the lexical vectors. The oper-\nations within MAG allows for the lexical vectors\nwithin BERT to adapt to multimodal information\nby changing their positions within the semantic\nspace. Aside from the attachment of MAG, no\nchange is made to the BERT structure.\nGiven an N length language sequence L =\n[L1,L2,...L N ] carrying word-piece tokens, a\n[CLS] token is appended to L so that we can\nuse it later for class label prediction. Then,\nwe input L to the Input Embedder which out-\nputs E = [ECLS ,E1,E2,...E N ]after adding to-\nken, segment and position embeddings. Then,\nwe input E to the Ô¨Årst Encoding layer and\nthen apply j Encoders on it successively. After\nthat encoding process, we get the output Zj =\n[Zj\nCLS ,Zj\n1,Zj\n2,...Z j\nN ]which denotes the Lexical\nEmbeddings after jlayers of Encoding.\nFor injecting audio-visual information into these\nembeddings, we prepare a sequence of triplets\n[(Zj\ni ,Ai,Vi) ‚à∂ ‚àÄi ‚àà {CLS,[1,N]}]by pairing\nZj\ni with the corresponding (Ai,Vi). Each of these\ntriplets are passed through the Multimodal Adap-\ntation Gatewhich transforms the ith triplet into\n¬ØZj\ni ‚Äì a uniÔ¨Åed multimodal representation of the\ncorresponding Lexical Embedding.\nAs there exists M = 12 Encoder layers in our\nBERT model, we input ¬ØZj = [¬ØZj\n1, ¬ØZj\n2,... ¬ØZj\nN ]to\nthe next Encoder and apply M‚àíjEncoder layers\non it successively. At the end, we get ¬ØZM from\nthe Mth Encoder layer. As the Ô¨Årst element ¬ØZM\nCLS\nrepresents the [CLS] token, it has the information\nnecessary to make a class label prediction. There-\nfore, ¬ØZM\nCLS goes through an afÔ¨Åne transformation\nto produce a single real-value which can be used to\npredict a class label.\n4.2 MAG-XLNet\nLike MAG-BERT, MAG-XLNet also has the capa-\nbility of injecting audio-visual information at any\nof its layers using MAG. At each position jof any\nof its layer, it holds the lexical vector corresponding\nto that position. Utilizing the audio-visual infor-\nmation available for that position, it can invoke\nMAG to get an appropriately shifted lexical vector\nin multimodal space. Although it mostly follows\nthe general paradigm presented in Figure 2 ver-\nbatim, it uses the XLNet speciÔ¨Åc Embedder and\nEncoders. One other key difference is the position\nof the [CLS] token. Unlike BERT, the [CLS] to-\nken is appended at the right end of the input token\nsequence, and therefore in all the intermediate rep-\nresentations, the vector corresponding to the [CLS]\nwill be the rightmost one. Following the same logic,\nthe output from the Ô¨Ånal Encoding layer will be\n¬ØZM = [¬ØZM\n1 , ¬ØZM\n2 ,... ¬ØZM\nN , ¬ØZM\nCLS ]. The last item,\n¬ØZM\nCLS can be used for class label prediction after it\ngoes through an afÔ¨Åne transformation.\n5 Experiments\nIn this section we outline the experiments in this\npaper. We Ô¨Årst start by describing the datasets,\nfollowed by description of extracted features, base-\nlines, and experimental setup.\n5.1 CMU-MOSI Dataset\nCMU-MOSI (CMU Multimodal Opinion Senti-\nment Intensity) is a dataset of multimodal language\nspeciÔ¨Åcally focused on multimodal sentiment anal-\nysis (Zadeh et al., 2016). CMU-MOSI contains\n2199 video segments taken from 93 Youtube movie\nreview videos. The dataset has real-valued high-\nagreement sentiment intensity annotations in the\nrange [‚àí3,+3].\n5.2 Computational Descriptors\nFor each modality, the following computational\ndescriptors are available:\nLanguage: We transcribe the videos using\nYoutube API followed by manual correction.\nAcoustic: COV AREP (Degottex et al., 2014) is\nused to extract the following relevant features:\nfundamental frequency, quasi open quotient, nor-\nmalized amplitude quotient, glottal source param-\neters (H1H2, Rd, Rd conf), VUV , MDQ, the\nÔ¨Årst 3 formants, PSP, HMPDM 0-24 and HM-\nPDD 0-12, spectral tilt/slope of wavelet responses\n(peak/slope), MCEP 0-24.\nVisual: For the visual modality, the Facet library\n(iMotions, 2017) is used to extract a set of visual\nfeatures including facial action units, facial land-\nmarks, head pose, gaze tracking and HOG features.\nFor each word, we align all three modalities fol-\nlowing the convention established in (Chen et al.,\n2017). Firstly, the word alignment between lan-\nguage and audio is obtained using forced align-\nment (Yuan and Liberman, 2008). Afterwards, the\nboundary of each word denotes the co-occurring vi-\nsual and acoustic features (FACET and COV AREP).\nSubsequently, for each word, the co-occurring\nacoustic and visual features are averaged across\neach feature ‚Äì thus achieving Ai and Vi vectors\ncorresponding to word i.\n5.3 Baseline Models\nWe compare the performance of MAG-BERT and\nMAG-XLNet to a variety of state-of-the-art models\nfor multimodal language analysis. These models\nare trained using extracted BERT and XLNet word\nembeddings as their language input:\nTFN (Tensor Fusion Network)explicitly mod-\nels both intra-modality and inter-modality dy-\nnamics (Zadeh et al., 2017) by creating a multi-\ndimensional tensor that captures unimodal, bi-\nmodal and trimodal interactions across three modal-\nities.\nMARN (Multi-attention Recurrent Network)\nmodels view-speciÔ¨Åc interactions using hybrid\nLSTM memories and cross-modal interactions us-\ning a Multi-Attention Block (MAB) (Zadeh et al.,\n2018c).\nMFN (Memory Fusion Network)has three sepa-\nrate LSTMs to model each modality separately and\na multi-view gated memory to synchronize among\nthem (Zadeh et al., 2018a).\nRMFN (Recurrent Memory Fusion Network)\ncaptures intra-modal and inter-modal information\nthrough recurrent multi-stage fashion (Liang et al.,\n2018).\nMulT (Multimodal Transformer for Unaligned\nMultimodal Language Sequence)uses three sets\nof Transformers and combines their output in a\nlate fusion manner to model a multimodal se-\nquence (Tsai et al., 2019). We use the aligned\nvariant of the originally proposed model, which\nachieves superior performance over the unaligned\nvariant.\nWe also compare our model to Ô¨Åne-tuned BERT\nand XLNet using language modality only to mea-\nsure the success of the MAG framework.\n5.4 Experimental Design\nAll the models in this paper are trained using\nAdam (Kingma and Ba, 2014) optimizer with learn-\ning rates between {0.001,0.0001,0.00001}. We\nuse dropouts of {0.1,0.2,0.3,0.4,0.5}for train-\ning each model. LSTMs in TFN, MARN, MFN,\nRMFN, LFN use latent size of {16,32,64,128}.\nFor MulT, we use {3,5,7}layers in the network\nand {1,3,5}attention heads. All models use the\ndesignated validation set of CMU-MOSI for Ô¨Ånd-\ning best hyper-parameters.\nWe perform two different evaluation tasks on\nCMU-MOSI datset: i) Binary ClassiÔ¨Åcation, and\nii) Regression. We formulate it as a regression\nproblem and report Mean-absolute Error (MAE)\nand the correlation of model predictions with true\nlabels. Besides, we convert the regression outputs\ninto categorical values to obtain binary classiÔ¨Åca-\ntion accuracy (BA) and F1 score. Higher value\nmeans better performance for all the metrics except\nMAE. We use two evaluation metrics for BA and\nF1, one used in (Zadeh et al., 2018d) and one used\nin (Tsai et al., 2019).\n6 Results and Discussion\nTable 1 shows the results of the experiments in this\npaper. We summarize the observations from the\nresults in this table as following:\n6.1 Performance of MAG-BERT\nIn all the metrics across the CMU-MOSI dataset,\nwe observe that performance of MAG-BERT is su-\nperior to state-of-the-art multimodal models that\nuse BERT word embeddings. Furthermore, MAG-\nBERT also performs superior to Ô¨Åne-tuned BERT.\nThis essentially shows that the MAG component is\nallowing the BERT model to adapt to multimodal\ninformation during Ô¨Åne-tuning, thus achieving su-\nperior performance.\n6.2 Performance of MAG-XLNet\nA similar performance trend to MAG-BERT is\nalso observed for MAG-XLNet. Besides supe-\nrior performance than baselines and Ô¨Åne-tuned\nXLNet, MAG-XLNet achieves near-human level\nperformance for CMU-MOSI dataset. Further-\nmore, we train MulT using the Ô¨Åne-tuned XLNet\nembeddings and get the following performance:\n83.6/slash.left85.3,82.6/slash.left84.2,0.810,0.759 which is lower\nthan both MAG-XLNet and XLNet. It is notable\nthat the p-value for student t-test between MAG-\nXLNet and XLNet in Table 1 is lower than 10e‚àí5\nfor all the metrics.\nThe motivation behind the experiments reported\nin Table 1 is as follows: we extracted word embed-\ndings from pre-trained BERT and XLNet models\nand trained the baseline models using those embed-\ndings. Since BERT and XLNet are often perceived\nto provide better word embeddings than Glove, it\nis not fair to compare MAG-BERT/MAG-XLNet\nwith previous models trained with Glove embed-\ndings. Therefore, we retrain previous works us-\nTask Metric BA ‚Üë F1‚Üë MAE‚Üì Corr‚Üë\nOriginal (glove)\nTFN 73.9/‚Äì 73.4/‚Äì 0.970/‚Äì 0.633/‚Äì\nMARN 77.1/‚Äì 77.0/‚Äì 0.968/‚Äì 0.625/‚Äì\nMFN 77.4/‚Äì 77.3/‚Äì 0.965/‚Äì 0.632/‚Äì\nRMFN 78.4/‚Äì 78.0/‚Äì 0.922/‚Äì 0.681/‚Äì\nLFN 76.4/‚Äì 75.7/‚Äì 0.912/‚Äì 0.668/‚Äì\nMulT ‚Äì/83.0 ‚Äì/82.8 ‚Äì/0.871 ‚Äì/0.698\nBERT\nTFN 74.8/76.0 74.1/75.2 0.955 0.649\nMARN 77.7/78.9 77.9/78.2 0.938 0.691\nMFN 78.2/79.3 78.1/78.4 0.911 0.699\nRMFN 79.6/80.7 78.9/79.1 0.878 0.712\nLFN 79.1/80.2 77.3/78.1 0.899 0.701\nMulT 81.5/84.1 80.6/83.9 0.861 0.711\nBERT 83.5/85.2 83.4/85.2 0.739 0.782\nMAG-BERT 84.2/86.1 84.1 /86.0 0.712 0.796\nXLNet\nTFN 78.2/80.1 78.2/78.8 0.914 0.713\nMARN 78.3/79.5 78.8/79.6 0.921 0.707\nMFN 78.3/79.9 78.4/79.1 0.898 0.713\nRMFN 79.1/81.0 78.6/80.0 0.901 0.703\nLFN 80.2/82.9 79.1/81.6 0.862 0.701\nMulT 81.7/84.4 80.4/83.1 0.849 0.738\nXLNet 84.7/86.7 84.6/86.7 0.676 0.812\nMAG-XLNet 85.7/87.9 85.6 /87.9 0.675 0.821\nHuman 85.7/- 87.5/- 0.710 0.820\nTable 1: Sentiment prediction results on CMU-MOSI\ndataset. Best results are highlighted in bold. MAG-\nBERT and MAG-XLNet achieve superior performance\nthan the baselines and their language-only Ô¨Ånetuned\ncounterpart. BA denotes binary accuracy (higher is\nbetter, same for F1), MAE denotes Mean-absolute Er-\nror (lower is better), and Corr is Pearson Correlation\n(higher is better). For BA and F1, we report two num-\nbers: the number on the left side of ‚Äú/‚Äù is measures\ncalculated based on (Zadeh et al., 2018c) and the right\nside is measures calculated based on (Tsai et al., 2019).\nHuman performance for CMU-MOSI is reported as\n(Zadeh et al., 2018a).\nModel E 1 4 6 8 12 A /uni2295.big /uni2299.big\nMAG-XLNet 80.1 85.6 84.1 84.1 83.8 83.6 64.0 60.0 55.8\nTable 2: Results of variations of XLNet model: MAG\napplied at different layers of the XLNet model, input-\nlevel concatenation and addition of all modalities. ‚ÄúE‚Äù\ndenotes application of MAG immediately after embed-\nding layer of the XLNet and ‚ÄúA‚Äù denotes applying\nMAG after the embedding layer and all the subsequent\nEncoding layers. /uni2295.bigand /uni2299.bigdenote input-level addition\nand concatenation of all modalities respectively. MAG\napplied at initial layers performs better overall.\ning BERT/XLNet embeddings to establish a more\n# Spoken words +\nacoustic and visual behaviors\nGround\nTruth\nMAG-\nXLNet XLNet\n1\n‚ÄúAnd it really just lacked what made the other movies more enjoyable.‚Äù +\nFrustrated and disappointed tone -1.4 -1.41 -0.9\n2\n‚ÄúBut umm I liked it.‚Äù + Emphasis on tone +\npositive shock through sudden eyebrow raise 1.8 1.9 1.2\n3\n‚ÄúExcept their eyes are kind of like this welcome to the polar express.‚Äù +\ntense voice + frown expression -0.6 -0.6 0.8\n4\n‚ÄúStraight away miley cyrus acting miley cyrus, or lack of, she had this\nsame expression throughout the entire Ô¨Ålm‚Äù + sarcastic voice +\nfrustrated facial expression\n-1.0 -1.2 0.2\nTable 3: Examples from the CMU-MOSI dataset. The ground truth sentiment labels are between strongly negative\n(-3) and strongly positive (+3). For each example, we show the Ground Truth and prediction output of both\nthe MAG-XLNet and XLNet. XLNet seems to be replicating language modality mostly while MAG-XLNet is\nintegrating the non-verbal information successfully.\nfair comparison between proposed approach in\nthis paper, and previous work. Based on the in-\nformation from Table 1, we observe that MAG-\nBERT/MAG-XLNet models outperforms various\nbaseline models using BERT/XLNet/Glove models\nsubstantially.\n6.3 Adaptation at Different Layers\nWe also study the effect of applying MAG at dif-\nferent encoder layers of the XLNet. SpeciÔ¨Åcally,\nwe Ô¨Årst apply the MAG to the output of the embed-\nding layer. Subsequently, we apply the MAG to the\nlayer j ‚àà{1,4,6,8,12}of the XLNet. Then, we\napply MAG at all the XLNet layers. From Table 2,\nwe observe that earlier layers are more suitable for\napplication of MAG.\nWe believe that earlier layers allow for better\nintegration of the multimodal information, as they\nallow the word shifting to happen from the begin-\nning of the network. If the semantics of words\nshould change based on the nonverbal accompani-\nments, then initial layers should reÔ¨Çect the semantic\nshift, otherwise, those layers are only working uni-\nmodally. Besides, the higher layers of BERT learn\nmore abstract and higher-level information about\nthe syntactic and semantic structure of linguistic\nfeatures (Coenen et al., 2019). Since, the acoustic\nand visual information present in our model corre-\nsponds to each word in the utterance, it will be more\ndifÔ¨Åcult for the MAG to shift the vector extracted\nfrom a later layer since that vector‚Äôs information\nwill be very abstract in nature.\n6.4 Input-level Concatenation and Addition\nFrom Table 2, we see that both input-level concate-\nnation and addition of modalities perform poorly.\nFor Concatenation, we simply concatenate all the\nmodalities. For Addition, we add the audio and\nvisual information to the language embedding after\nmapping both of them to the language dimension.\nThese results demonstrate the rationale behind us-\ning an advanced fusion mechanism like MAG.\n6.5 Results on Comparable Datasets\nWe also perform experiments on the CMU-MOSEI\ndataset (Zadeh et al., 2018d) to study the generaliza-\ntion of our approach to other multimodal language\ndatasets. Unlike CMU-MOSI which has sentiment\nannotations at utterance level, CMU-MOSEI has\nsentiment annotations at sentence level. The exper-\nimental methodology for CMU-MOSEI is similar\nto the original paper. For the sake of comparison,\nwe sufÔ¨Åce2 to comparing the binary accuracy and\nf1 score for the top 3 models in Table 1. In BERT\ncategory, we compare the performance of MulT\n(with BERT embeddings), BERT and MAG-BERT\nwhich are respectively as follows: [83.5,82.9]for\nMulT, [83.9,83.9]for BERT, and[84.7,84.5]for\nMAG-BERT. Similarly for XLNET category, the\nresults for MulT (with XLNet embeddings), XLNet\nand MAG-XLNet are as follows: [84.1,83.7]for\nMulT, [85.4,85.2]for XLNet and [85.6,85.7]for\nMAG-XLNet. Therefore, superior performance of\n2Since Transformer based models take a long time to train\nfor CMU-MOSEI\nMAG-BERT and MAG-XLNet also generalizes to\nCMU-MOSEI dataset.\n6.6 Fine-tuning Effect\nWe study whether or not the superior performance\nof the MAG-BERT and MAG-XLNet is related to\nsuccessful Ô¨Ånetuning of the models, or related to\nother factors e.g. any transformer with architec-\nture like BERT or XLNet would achieve superior\nperformance regardless of being pretrained. By ran-\ndomly initializing the weights of BERT and XLNet\nwithin MAG-BERT and MAG-XLNet, we get the\nfollowing performance on BA for the CMU-MOSI:\n70.1 and 70.7 respectively. This indicates that the\nsuccess of the MAG-BERT and MAG-XLNet is\ndue to successful Ô¨Åne-tuning. Even on the larger\nCMU-MOSEI dataset we get BA of 76.8 and 78.4\nfor MAG-BERT and MAG-XLNet, which further\nsubstantiates the fact that Ô¨Åne-tuning is successful\nusing MAG framework.\n6.7 Qualitative Analysis\nIn Table 3, we present some examples where MAG-\nXLNet adjusted sentiment intensity properly by\ntaking into account nonverbal information. The\nexamples demonstrate that MAG-XLNET can suc-\ncessfully integrate the non-verbal modalities with\ntextual information.\nIn both Example-1 and Example-2, XLNet cor-\nrectly predicted the polarity of the displayed emo-\ntion. However, additional information was present\nin the acoustic and visual domain which XLNet\ncould not utlize. Given those information, MAG-\nXLNet could better predict the magnitude of emo-\ntion displayed in both cases.\nAlthough the emotion in the text of Example-3\ncan be portrayed as a bit positive, the tense voice\nand frown expression helps MAG-XLnet reverse\nthe polarity of predicted emotion. Similarly, the\ntext in Example-4 is mostly neutral, but MAG-\nXLNet can predict the negative emotion through\nthe sarcastic vocal and frustrated facial expression.\n7 Conclusion\nIn this paper, we introduced a method for efÔ¨Åciently\nÔ¨Ånetuning large pre-trained Transformer models\nfor multimodal language. Using a proposed Multi-\nmodal Adaptation Gate (MAG), BERT and XLNet\nwere successfully Ô¨Åne-tuned in presence of vision\nand acoustic modalities. MAG essentially poses\nthe nonverbal behavior as a vector with a trajectory\nand magnitude, which is subsequently used to shift\nlexical representations within the pre-trained Trans-\nformer model. A unique characteristic of MAG is\nthat it makes no change to the original structure of\nBERT or XLNet, but rather comes as an attachment\nto both models. Our experiments demonstrated the\nsuperior performance of MAG-BERT and MAG-\nXLNet. The code for both MAG-BERT and MAG-\nXLNet are publicly available here 3\nAcknowledgement\nThis research was supported in part by grant\nW911NF-15-1-0542 and W911NF-19-1-0029 with\nthe US Defense Advanced Research Projects\nAgency (DARPA) and the Army Research OfÔ¨Åce\n(ARO). Authors AZ and LM were supported by the\nNational Science Foundation (Awards #1750439\n#1722822) and National Institutes of Health. Any\nopinions, Ô¨Åndings, and conclusions or recommen-\ndations expressed in this material are those of the\nauthor(s) and do not necessarily reÔ¨Çect the views of\nUS Defense Advanced Research Projects Agency,\nArmy Research OfÔ¨Åce, National Science Founda-\ntion or National Institutes of Health, and no ofÔ¨Åcial\nendorsement should be inferred.\nReferences\nMinghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-\ntruÀásaitis, Amir Zadeh, and Louis-Philippe Morency.\n2017. Multimodal sentiment analysis with word-\nlevel fusion and reinforcement learning. In Proceed-\nings of the 19th ACM International Conference on\nMultimodal Interaction, pages 163‚Äì171. ACM.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda Vi¬¥egas, and Martin Watten-\nberg. 2019. Visualizing and measuring the geometry\nof bert. arXiv preprint arXiv:1906.02715.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2019. Transformer-xl: Attentive lan-\nguage models beyond a Ô¨Åxed-length context. arXiv\npreprint arXiv:1901.02860.\nGilles Degottex, John Kane, Thomas Drugman, Tuomo\nRaitio, and Stefan Scherer. 2014. Covarep‚Äîa col-\nlaborative voice analysis repository for speech tech-\nnologies. In 2014 ieee international conference\non acoustics, speech and signal processing (icassp),\npages 960‚Äì964. IEEE.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\n3https://github.com/WasifurRahman/\nBERT_multimodal_transformer\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nDevamanyu Hazarika, Soujanya Poria, Amir Zadeh,\nErik Cambria, Louis-Philippe Morency, and Roger\nZimmermann. 2018. Conversational memory net-\nwork for emotion recognition in dyadic dialogue\nvideos. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), volume 1, pages\n2122‚Äì2132.\niMotions. 2017. Facial expression analysis.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPaul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis-\nPhilippe Morency. 2018. Multimodal language anal-\nysis with recurrent multistage fusion. arXiv preprint\narXiv:1808.03920.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111‚Äì3119.\nSunghyun Park, Han Suk Shim, Moitreya Chatterjee,\nKenji Sagae, and Louis-Philippe Morency. 2014.\nComputational analysis of persuasiveness in social\nmultimedia: A novel dataset and multimodal predic-\ntion approach. In Proceedings of the 16th Interna-\ntional Conference on Multimodal Interaction, pages\n50‚Äì57. ACM.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532‚Äì1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nHai Pham, Paul Pu Liang, Thomas Manzini, Louis-\nPhilippe Morency, and Barnabas Poczos. 2019.\nFound in translation: Learning robust joint repre-\nsentations by cyclic translations between modalities.\narXiv preprint arXiv:1812.07809.\nSoujanya Poria, Amir Hussain, and Erik Cambria.\n2018. Multimodal Sentiment Analysis , volume 8.\nSpringer.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language understand-\ning paper. pdf.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Mur-\nphy, and Cordelia Schmid. 2019. Videobert: A joint\nmodel for video and language representation learn-\ning. arXiv preprint arXiv:1904.01766.\nYao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. 2019. Multimodal transformer for\nunaligned multimodal language sequences. arXiv\npreprint arXiv:1906.00295.\nYao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2018. Learning factorized multimodal representa-\ntions. arXiv preprint arXiv:1806.06176.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998‚Äì6008.\nYansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency. 2018.\nWords can shift: Dynamically adjusting word rep-\nresentations using nonverbal behaviors. arXiv\npreprint arXiv:1811.09362.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nJiahong Yuan and Mark Liberman. 2008. Speaker iden-\ntiÔ¨Åcation on the scotus corpus. Journal of the Acous-\ntical Society of America, 123(5):3878.\nAmir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency. 2017. Tensor\nfusion network for multimodal sentiment analysis.\narXiv preprint arXiv:1707.07250.\nAmir Zadeh, Paul Pu Liang, Navonil Mazumder,\nSoujanya Poria, Erik Cambria, and Louis-Philippe\nMorency. 2018a. Memory fusion network for multi-\nview sequential learning. In Thirty-Second AAAI\nConference on ArtiÔ¨Åcial Intelligence.\nAmir Zadeh, Paul Pu Liang, Louis-Philippe Morency,\nSoujanya Poria, Erik Cambria, and Stefan Scherer.\n2018b. Proceedings of grand challenge and work-\nshop on human multimodal language (challenge-\nhml). In Proceedings of Grand Challenge and Work-\nshop on Human Multimodal Language (Challenge-\nHML).\nAmir Zadeh, Paul Pu Liang, Soujanya Poria, Pra-\nteek Vij, Erik Cambria, and Louis-Philippe Morency.\n2018c. Multi-attention recurrent network for human\ncommunication comprehension. In Thirty-Second\nAAAI Conference on ArtiÔ¨Åcial Intelligence.\nAmir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency. 2016. Mosi: multimodal cor-\npus of sentiment intensity and subjectivity anal-\nysis in online opinion videos. arXiv preprint\narXiv:1606.06259.\nAmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency.\n2018d. Multimodal language analysis in the wild:\nCmu-mosei dataset and interpretable dynamic fu-\nsion graph. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , volume 1, pages 2236‚Äì\n2246.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8082244396209717
    },
    {
      "name": "Transformer",
      "score": 0.6959496140480042
    },
    {
      "name": "Language model",
      "score": 0.5748490691184998
    },
    {
      "name": "Modalities",
      "score": 0.5438007712364197
    },
    {
      "name": "Modality (human‚Äìcomputer interaction)",
      "score": 0.4806436598300934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46574509143829346
    },
    {
      "name": "Natural language processing",
      "score": 0.46152013540267944
    },
    {
      "name": "Sentiment analysis",
      "score": 0.43384116888046265
    },
    {
      "name": "Speech recognition",
      "score": 0.42739230394363403
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": []
}