{
  "title": "Chemformer: A Pre-Trained Transformer for Computational Chemistry",
  "url": "https://openalex.org/W4253877692",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2317361088",
      "name": "Ross Irwin",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2146233909",
      "name": "Spyridon Dimitriadis",
      "affiliations": [
        "AstraZeneca (Sweden)",
        "Linköping University"
      ]
    },
    {
      "id": "https://openalex.org/A2108660797",
      "name": "Jiazhen He",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A4220021479",
      "name": "Esben Bjerrum",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W3154464317",
    "https://openalex.org/W2911954305",
    "https://openalex.org/W3157265962",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3027819927",
    "https://openalex.org/W6628435738",
    "https://openalex.org/W3117879109",
    "https://openalex.org/W3030978062",
    "https://openalex.org/W4233253307",
    "https://openalex.org/W2912083425",
    "https://openalex.org/W6756224866",
    "https://openalex.org/W3025593963",
    "https://openalex.org/W6734561206",
    "https://openalex.org/W3030970797",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W6828894009",
    "https://openalex.org/W6609581451",
    "https://openalex.org/W2753921001",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4237622019",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3038856956",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W3035172872",
    "https://openalex.org/W4251572712",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2963587345",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2551217916",
    "https://openalex.org/W3171707284",
    "https://openalex.org/W2325811289",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W4287647552",
    "https://openalex.org/W4221074165",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W3007488165",
    "https://openalex.org/W3181403764",
    "https://openalex.org/W3191025211",
    "https://openalex.org/W4242073813",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4245562513",
    "https://openalex.org/W2604296437",
    "https://openalex.org/W2998367408",
    "https://openalex.org/W2994678679",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4230007416",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W2886791556",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4231666130"
  ],
  "abstract": "Transformer models coupled with Simplified Molecular Line Entry System (SMILES) have recently proven to be a powerful combination for solving challenges in cheminformatics. These models, however, are often developed specifically for a single application and can be very resource-intensive to train. In this work we present Chemformer model – a Transformerbased model which can be quickly applied to both sequence-to-sequence and discriminative cheminformatics tasks. Additionally, we show that self-supervised pre-training can improve performance and significantly speed up convergence on downstream tasks. On direct synthesis and retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top- 1 accuracy. We also improve on existing approaches for a molecular optimisation task and show that Chemformer can optimise on multiple discriminative tasks simultaneously. Models, datasets and code will be made available after publication.",
  "full_text": "Chemformer: A Pre-Trained Transformer\nfor Computational Chemistry\nRoss Irwin1, Spyridon Dimitriadis1,2, Jiazhen He1, and Esben Jannik Bjerrum1,*\n1Molecular AI, Discovery Sciences, R&D, AstraZeneca, Gothenburg, Sweden\n2Department of Computer and Information Science, Linköping University,\nLinköping, Sweden\nTransformer models coupled with Simpliﬁed Molecular Line Entry System (SMILES) have\nrecently proven to be a powerful combination for solving challenges in cheminformatics. These\nmodels, however, are often developed speciﬁcally for a single application and can be very\nresource-intensive to train. In this work we present Chemformer model – a Transformer-\nbased model which can be quickly applied to both sequence-to-sequence and discriminative\ncheminformatics tasks. Additionally, we show that self-supervised pre-training can improve\nperformance and signiﬁcantly speed up convergence on downstream tasks. On direct synthesis\nand retrosynthesis prediction benchmark datasets we publish state-of-the-art results for top-\n1 accuracy. We also improve on existing approaches for a molecular optimisation task and\nshow that Chemformer can optimise on multiple discriminative tasks simultaneously. Models,\ndatasets and code will be made available after publication.\n1 Introduction\nRecent years have witnessed an explosion in research applying neural network models to chemin-\nformatics tasks. Sequence-to-sequence models, such as the Transformer [1] and models based on\nthe Recurrent Neural Network (RNN) architecture [2,3], are well suited to tasks such as direct\nreaction prediction, retrosynthesis prediction and molecular optimisation. Applying molecules en-\ncoded using Simpliﬁed Molecular Line Entry System (SMILES) [4] to the Transformer model has\nproduced state-of-the-art results on benchmark datasets for these tasks [5–7]. Transformers have\nalso been successfully applied to discriminative tasks such as biological activity prediction (virtual\nscreening) [8] and molecular property prediction (QSAR modelling) [8–14]. Training Transformer\nmodels on SMILES strings, however, can be computationally expensive; a recently proposed model\nfor direct synthesis prediction requires two days of training [15]. Additionally, separate models\nmust be built, trained and tuned for each task, increasing the amount of eﬀort required by research\nteams.\nSelf-supervised learning using the Transformer has revolutionised Natural Language Processing\n(NLP) in recent years; large language models such as BERT [16], BART [17], GPT [18,19],\nUniLM [20] and T5 [21] have provided signiﬁcant improvements on key benchmark NLP tasks.\nPre-training these models – training on a large unlabelled dataset of text before ﬁne-tuning on the\ndataset of interest – has been shown to improve results on downstream tasks, especially when the\namount of data for ﬁne-tuning is limited. Furthermore, pre-training can also signiﬁcantly reduce\nthe amount of time required for ﬁne-tuning [16], thereby reducing computational costs and making\nstate-of-the-art models more accessible to those with limited computational resources.\n*Correspondence to: esben.bjerrum@astrazeneca.com\n1\nTransfer learning has also recently been shown to improve performance on reaction informatics\ntasks [22–27] and, separately, on discriminative tasks [8–10,14,28]. However, many of these\napproaches pre-train on a task-speciﬁc dataset, such as reaction informatics data. It is unclear how\nwell these models would be able to transfer their knowledge to other domains. Other approaches\nmake use of the encoder stack of the Transformer only, along with a fully-visible attention mask [8,\n9,11]. This makes it diﬃcult to apply these models to sequence-to-sequence tasks. In one study,\nembeddings from a self-supervisedAugmented Transformerwere used to build QSAR models [29],\nbut the pre-trained weights were not subsequently ﬁne-tuned.\nOne model, X-MOL [10], uses a Transformer encoder with a combined fully-visible and autore-\ngressive attention mask. This allows the model to be applied to both discriminative and sequence-\nto-sequence tasks. However, this is very resource intensive for the latter since the amount of\nmemory and computation required grows quadratically with the length of the sequence [1]. Ad-\nditionally, X-MOL does not approach pre-training from a language-modelling perspective and it\nexplores only a single pre-training task.\nTaking inspiration from NLP, we aim to address the resource challenges within computational\nchemistry by exploiting transfer learning to provide a model which can be quickly applied to di-\nverse tasks. The purpose of this work is therefore to use SMILES as a “language for Chemistry” [4]\nto provide a common data format on which we then apply Transformer-based language models.\nWe investigate the ability of self-supervised pre-training on a large dataset of unlabelled molecules\nto decrease convergence time for a number of sequence-to-sequence tasks, thereby improving the\nresults on these tasks when training time is limited. We explore a number of self-supervised\npre-training tasks and model architectures, and quantitatively compare their performance on\nboth sequence-to-sequence and discriminative downstream tasks. We show that, with the help\nof transfer learning, our models can achieve state-of-the-art results on four downstream datasets.\nAdditionally, we examine the ability of these models to ﬁne-tune on multiple discriminative tasks\nsimultaneously, further improving cheminformatics research eﬃciency.\n2 Methods\nChemformer is based on the BART language model, which uses both the encoder and decoder\nstacks of the Transformer. This makes it very suitable for sequence-to-sequence tasks such as\nreaction prediction and molecular optimisation. The BART model can also easily be applied to\ndiscriminative tasks by using only the encoder stack.\nThe Chemformer models were ﬁrstly trained in a self-supervised manner and the learnable\nweights were saved. These weights were then loaded separately for each downstream task of\ninterest and the task-speciﬁc ﬁne-tuning procedure took place. Figure 1 provides an overview of\nhow the pre-training and downstream ﬁne-tuning tasks are applied to the Chemformer model.\nIn order to investigate the importance of the number of learnable model parameters, we pre-\ntrained both a base model, Chemformer, and a larger model, Chemformer-Large. The Chemformer\nmodel uses the same hyperparameters as the original Transformer and contains approximately\n45 million learnable weights, whereas the Chemformer-Large model expands this to 230 million\nweights. Full details of the models can be found in Section 2.4.\n2.1 Pre-Training\n2.1.1 Dataset\nAn unlabelled dataset of approximately 100 million SMILES strings was used to pre-train the\nmodels. These molecules were randomly selected from roughly 1.5 billion molecules available\nfrom the publicly accessible ZINC-15 dataset [30] with the following constraints: reactivity set\nto reactive, purchasability set to annotated, molecular weight≤ 500 Daltons and LogP ≤ 5.\nTrain, validation and test splits were then randomly assigned, with training data taking 99% and\nvalidation and testing each assigned 0.5% of the 100 million molecules.\n2\nFigure 1: Illustration of the pre-training and ﬁne-tuning procedures for downstream tasks.\n2.1.2 Procedure\nThe pre-training procedure begins by converting each molecule in the batch to a non-canonical\nSMILES form which corresponds to the given molecule. SMILES strings are then randomly\nmodiﬁed, tokenised and embedded into a sequence of vectors. Sinusoidal positional embeddings [1]\nare added before the sequence is passed into the Transformer layers of the model. The modiﬁed\nsequence is passed to the bidirectional encoder, while the autoregressive decoder is asked to predict\nthe original SMILES sequence, given the same sequence right-shifted. A fully-connected layer is\napplied to the output of the decoder to produce a distribution over the model’s vocabulary and a\ncross-entropy loss function is used to train the model.\nFor the base Chemformer model, we investigated three SMILES modiﬁcation techniques in this\nwork: masking, augmentation and a combination of masking and augmentation. Due to resource\nconstraints, however, the Chemformer-Large model was pre-trained only on the combined task.\nFigure 2 illustrates example SMILES strings for all three pre-training tasks. Each of the tasks are\nimplemented as follows:\n• Masking Masking is conducted with the span masking algorithm used by the BART [17]\nmodel – short sequences of tokens within a SMILES string are randomly replaced by a single\n<MASK> token.\n• Augmentation The augmentation task is conducted similarly to the approach of the hetero-\nencoder model [31]; the input to the model is modiﬁed by randomly generating another\nSMILES string which corresponds to the same molecule as the output. This is done following\nthe SMILES enumeration technique [32] – permuting the atom order before generating a non-\ncanonicalSMILESform. Unlikemanycorruptiontasksusedforpre-trainingNLPmodels[16,\n17], this task is speciﬁc to the SMILES language.\n3\nFigure 2: Comparison of the three pre-training tasks for a caﬀeine molecule. On the left is the\nmodiﬁed SMILES given to the encoder, and on the right is the original SMILES which we train\nthe decoder to predict.\n• Combined Data for the combined task are created by ﬁrst augmenting and then masking\neach SMILES string. This task can be seen as a method of combining pre-training techniques\nfor both natural language and chemistry.\n2.2 Sequence-to-Sequence Fine Tuning\nAfter pre-training, the models were ﬁne-tuned on downstream datasets. For this work we inves-\ntigated three downstream sequence-to-sequence tasks: direct synthesis prediction, retrosynthesis\nprediction and molecular optimisation.\n2.2.1 Datasets\nFor the direct synthesis prediction task we made use of the benchmark USPTO-MIT dataset [33],\nwhich contains approximately 470,000 reactions originally extracted from patents [34]. We evalu-\nated performance on both USPTO Mixed, where reactants and reagents are assorted arbitrarily\nwithin the input string, and USPTO Separated, where reactants are reagents are split by a sep-\narator token. The USPTO-50K [35] dataset, which contains approximately 50,000 reactions, was\nused to benchmark Chemformer on the retrosynthesis prediction task.\nThe dataset [6] for the molecular optimisation task consists of a set of matched molecular\npairs (MMPs) extracted from ChEMBL [36], together with the property changes of the MMPs.\nThreemolecularproperties: logD,solubilityandclearance, areoptimisedsimultaneously. Property\nvalues for each molecule were predicted from models built using internal, experimental data. The\nproperty prediction models were used for both the construction of training data, and also for the\nevaluation of the generated molecules during testing. The dataset includes 160,831 train, 17,871\nvalidation and 19,856 test MMPs. Full details of the dataset and the models used to generate\nmolecular property predictions can be found in [6].\n2.2.2 Procedure\nSequence-to-sequence ﬁne-tuning is analogous to pre-training; inputs are passed to the encoder,\nright-shifted outputs along with the memory embeddings from the encoder are applied to the\ndecoder, and the decoder output embeddings are passed through a fully-connected layer to produce\na distribution over the model’s vocabulary. A cross-entropy loss function is used to train the model.\nFor the direct reaction prediction task, the model is given the reactants and asked to predict\nthe products, with the reverse being true for the retrosynthesis prediction task. Fine-tuning\nfor the molecular optimisation task is performed by preﬁxing the molecule to be optimised with\noptimisation tokens. For example, if we wish the solubility to be increased, the clearance to be\ndecreased, and the LogD to be left unchanged, we encode this into an optimisation using tokens\nin the model’s vocabulary. The model is then trained to predict the MMP output molecule given\nin the dataset.\n4\nIn addition to our novel pre-training tasks, we also introduce a novel SMILES augmentation\nscheme for downstream tasks which uses a tunable augmentation probability. Given a canonical\ninput-output pair of SMILES from the training set,(sin,sout) ∈Dtrain, we randomly augmentsin\nand sout independently with probabilitypaug. For sequence-to-sequence tasks we usepaug = 0.5\nthroughout, unless stated otherwise. Since the augmentations do not need to be pre-computed,\nwe can augment on-the-ﬂy, similarly to a previous study [37]. Thus, this approach has three key\nadvantages. Firstly, the augmentation probability can be tuned. Secondly, only the canonical\ndata needs to be stored, rather than every augmented version of the dataset. And, thirdly, the\nmodel sees a diﬀerent form of the same data every epoch, regardless of the number of epochs; we\nconjecture that this could improve the model’s ability to generalise to unseen data.\n2.3 Discriminative Fine-Tuning\nIn addition to sequence-to-sequence ﬁne-tuning, we also examined Chemformer’s application to\ndiscriminative tasks. In particular, we ﬁne-tuned on molecular property prediction and biological\nactivity tasks. Since we aim to improve eﬃciency in cheminformatics research, and since there\nshould be signiﬁcant synergy between tasks, we trained the models to optimise for multiple tasks\nsimultaneously – an approach known as “multi-task learning” [21,38]. Speciﬁcally, we trained\nmolecular property models to solve three property prediction tasks simultaneously, and trained\nbiological activity models to predict activity for 133 genes simultaneously, rather than having\nseparate models for each task.\n2.3.1 Datasets\nThe Chemformer model was applied to three molecular property datasets from MoleculeNet [39]:\nESOL, FreeSolvation and Lipophilicity, containing 1128, 642 and 4200 molecules, respectively.\nSince we are interested in optimising the model for all three tasks simultaneously, we ensure that\nall molecules which appear in more than one dataset appear only in the train set. After splitting\nthe remaining molecules we end up with train, validation and test splits corresponding to 75%, 10%\nand 15% of the dataset, respectively. We generated 20 diﬀerent random splits in these proportions.\nThe data was pre-processed by scaling the values in the training set to be between 0 and 1. Each of\nthe three datasets was scaled independently and the same scaling functions are used for validation\nand testing. Due to their size, the ESOL and FreeSolvation datasets were upsampled by factors\nof 2 and 3, respectively, during training.\nThe biological activity data were downloaded from the Exascale Compound Activity Prediction\nEngine (ExCAPE) database [40]. The data consists of the standardized, log-transformed activity\nvalues (pXC50 values) for chemical compounds against an array of protein targets. We selected\nthe subset of genes from the dataset which had biological activity readings for more than 1200\ncompounds. Additionally, we selected only genes which obtained a regression coeﬃcient over 0.4\nwhen a ridge regression model was applied to the compounds’ Morgan ﬁngerprints with radius 2.\nThe full list of the 133 included genes can be found in the supplementary information. The ﬁnal\ndataset contains 312,202 molecules with biological activity readings. Molecules for each gene were\nrandomly split into train, validation and test splits of 70%, 5% and 25%, respectively.\n2.3.2 Procedure\nUnlike sequence-to-sequence tasks, discriminative tasks only make use of the encoder stack of\nthe model. Firstly, the tokenised SMILES string of a molecule is preﬁxed with one or more\ntask tokens – gene symbols for biological activity prediction, or molecular properties for QSAR\nmodelling. This sequence of tokens is passed through the model’s embedding layer, followed by the\nmodel’s encoder. The output vector for each task token is then passed through a small multi-layer\nperceptron (MLP) head to produce either a class distribution vector or a single output number\nfor classiﬁcation and regression tasks, respectively. Since we only investigated regression tasks in\nthis work, a mean squared error loss function is applied to the MLP output for each task token.\n5\nChemformer Chemformer-Large\nModel Dimension 512 1024\nFeed-Forward 2048 4096\nLayers 6 8\nAttention Heads 8 16\nParameters 45M 230M\nTable 1: A comparison of the diﬀerences in hyperparameters and number of learnable weights\nbetween the two Chemformer model sizes we investigated.\nWe augmented the input SMILES string for discriminative tasks withpaug = 1.0 and, as with\nsequence-to-sequence tasks, this augmentation was performed on-the-ﬂy during training.\nFor each high-level task – biological activity prediction and molecular property prediction –\nChemformer models were trained simultaneously on all subtasks. The models were then evaluated\nseparately on each subtask, to facilitate easy comparison. Property prediction models were trained\non all 20 dataset splits and an average of the evaluation results was taken. Hyperparameters were\ntuned separately for each Chemformer model, and, to combat overﬁtting, the size (number of\nlayers, attention heads, model dimension and feed-forward dimension) of the randomly initialised\nmodel was also tuned. The full details of the tuned hyperparameters for each Chemformer model\ncan be found in the supplementary information.\nIn addition to the four diﬀerent base Chemformer models, we also trained Support Vector\nRegression (SVR) models as comparison baselines. 2048-bit Morgan ﬁngerprints with radius 2\nwere calculated for each molecule, and an SVR with a Tanimoto kernel was then applied. The\nSVR models were tuned, trained and evaluated on each subtask separately.\n2.4 Implementation Details\nThe Chemformer model was implemented using the PyTorch [41] and PyTorch Lightning [42]\nframeworks. We used the Transformer in thepre-norm layout – Layer Normalisation [43] is applied\nbefore the attention and feedforward blocks – and the GELU activation function [44] throughout.\nA comparison of the size of the Chemformer and Chemformer-Large models is shown in Table 1.\nEach model was pre-trained for 1,000,000 steps using 4 NVIDIA V100 GPUs with a batch size\nof 128 molecules per GPU. The original Transformer learning rate schedule was used, along with\n8000 linear warm-up steps. Pre-training took approximately 2.5 days for Chemformer and 6 days\nfor Chemformer-Large. The one-cycle learning rate schedule [45] was used for ﬁne-tuning, for both\nsequence-to-sequence and discriminative tasks. Additionally, we used the Adam optimiser [46] with\nparameters β1 = 0.9 and β2 = 0.999 for both pre-training and ﬁne-tuning on all tasks.\nChemformer’s vocabulary is constructed by applying regular expression matching (we use the\nsame regex as the Molecular Transformer [15]) to the canonical SMILES of the molecules in the\nChEMBL 27 [36]. There are 523 tokens in the vocabulary in total, including a large number of\nunused tokens which can be replaced with task-speciﬁc tokens, as required. Tokenisation and\naugmentation of SMILES was performed by extending the PySMILESUtils framework [47].\n3 Results\nWe evaluate the performance of Chemformer and Chemformer-Large models on three downstream\nsequence-to-sequence tasks: direct synthesis prediction, retrosynthesis prediction and molecular\noptimisation. Additionally, we investigate Chemformer’s ability to train simultaneously on multi-\nple downstream discriminative tasks. Speciﬁcally, we look at three molecular property prediction\ntasks and biological activity prediction for 133 genes.\n6\nModel Sequence-to-Sequence (%) Discriminative (Mean R2)\nDirect Retro Mol Opt Mol Prop Bioactivity\nRandom 91.1 50.8 73.1 0.680 0.480\nMask 91.2 52.1 75.0 0.843 0.603\nAugment 91.1 51.8 74.3 0.848 0.606\nCombined 91.8 53.6 72.2 0.857 0.631\nTable 2: Results on downstream tasks for a selection of pre-training approaches when ﬁne-tuning\nis limited to no more than 12 hours. TheRandom model uses randomly initialised weights rather\nthan weights learned during pre-training. For the molecular optimisation task we measure the\npercentage of generated molecules which fulﬁll thedesirableproperties. For discriminativedatasets\nwe report the meanR2 over all of the subtasks.\n3.1 Eﬀects of Transfer Learning\n3.1.1 Improvement in Performance\nTable 2 compares the downstream results for the three diﬀerent pre-trained models, as well as\na model with randomly initialised weights (no pre-training) on a selection of the tasks. The\ntraining time is limited to no more than 12 hours for each task. In particular, this corresponds\nto: 40 epochs for direct reaction prediction on the USPTO Separated dataset; 500 epochs for\nretrosynthesis prediction on the USPTO-50K dataset; 100 epochs for the molecular optimisation\ntask; 150 epochs of simultaneous ﬁne-tuning on the property prediction tasks; and the same for the\nbiological activity tasks. For sequence-to-sequence tasks, output SMILES are generated using the\nbeam search algorithm with a beam width of 10, and the top-1 prediction is used for evaluation.\nFrom Table 2 we can see that transfer learning provides a signiﬁcant improvement; pre-trained\nmodels beat the randomly initialised baseline for all datasets. We can also see that the Chem-\nformer model pre-trained on the combined task is the strongest performer. Other than molecular\noptimisation, the combined model performs best on all tasks. For molecular optimisation, the\nmodel pre-trained using only masking is the best performer, while thecombined model is unable\nto beat the model with no pre-training. We discuss possible explanations for this in more detail\nin Section 4.\nExamining the molecular property prediction tasks in more detail, we continue to see that\ntransfer learning provides a performance boost. Table 3 outlines the results of Chemformer models\non these tasks. The most signiﬁcant increase in performance from transfer learning is witnessed\non the lipophilicity task; the performance boost on the ESOL and free solvation datasets is more\nmodest. The table also compares the Chemformer models against an SVR baseline, trained as\ndescribed in Section 2. The SVR is able to beat the randomly initialised Chemformer model on\nthe lipophilicity task, but is otherwise outperformed by all other models across all tasks. The\ncombined model outperforms all others, including the SVR, on all three tasks.\nModel Lipophilicity ESOL Free Solvation\nR2 RMSE R2 RMSE R2 RMSE\nSVR 0.617 0.746 0.766 1.03 0.754 2.11\nRandom 0.398 0.946 0.855 0.803 0.786 1.89\nMask 0.736 0.621 0.903 0.657 0.889 1.37\nAugment 0.738 0.618 0.904 0.652 0.901 1.29\nCombined 0.754 0.598 0.910 0.633 0.908 1.23\nTable 3:R2 (higher is better) and RMSE (lower is better) downstream molecular property predic-\ntion results for Chemformer models pre-trained on diﬀerent tasks, as well as an SVR baseline. The\nRandom model uses randomly initialised weights rather than weights learned during pre-training.\nEach model was ﬁne-tuned on all three molecular property subtasks simultaneously.\n7\nFigure 3: Comparison of the performance of Chemformer models with that of an SVR baseline\nacross 133 bioactivity prediction tasks. Each dot corresponds to the bioactivity prediction result\nfor a single gene. If the dot is above the dashed line the Chemformer model is a better predictor\nfor that gene.\nFigure 3 provides a more detailed view of the results on the biological activity prediction tasks.\nThe performance of each Chemformer model is compared to that of the SVR for each of the 133\ntasks. While there is a lot of variation in the results for each gene – some tasks are challenging\nirrespective of the model – the improvement provided by transfer learning is clear. All three\npre-trained models perform signiﬁcantly better than the random initialised model, and, again,\nthe model pre-trained on the combined task is the strongest performer. However, despite the\nimprovement provided by pre-training, none of the Chemformer models are able to beat the SVR\nbaseline on average across all tasks. The full set of results on the 133 tasks can be found in the\nsupplementary information.\n3.1.2 Decreased Convergence Time\nIn addition to stronger performance on downstream tasks, transfer learning can also signiﬁcantly\nspeed up training convergence. Figure 4 illustrates the considerable eﬀect pre-training can have\non performance and convergence speed for the retrosynthesis task. Firstly, the Chemformer model\npre-trained on the combined task is able to outperform the existing SMILES-based state-of-the-\nart, the Augmented Transformer, with 20 epochs of ﬁne-tuning. This corresponds to fewer than\n30 minutes of training on one GPU. In addition to this, ﬁne-tuning for 50 epochs provides a better\ntop-1 result than 500 epochs of training from randomly initialised weights – an order of magnitude\ndiﬀerence in training time.\n3.2 Comparison with Existing Approaches\nAllowingthemodeltoﬁne-tuneforlongerthan12hoursimprovestheresultsfurtherformosttasks;\nin Table 4 we compare existing direct reaction prediction implementations against Chemformer\nand Chemformer-Large, ﬁne-tuned for 150 and 100 epochs, respectively. Additionally, Table 5\ncompares the Chemformer model, ﬁne-tuned for 500 epochs, and the Chemformer-Large model,\nﬁne-tuned for 200 epochs, against existing SMILES- and graph-based approaches on the USPTO-\n50K retrosynthesis dataset. All Chemformer models were pre-trained on the combined pre-training\ntask.\n8\nFigure 4: Comparison of the convergence on the USPTO-50K dataset of the randomly initialised\nChemformer model with that of the model pre-trained on the combined task. Each point shows\nthe result on the test dataset after a full training cycle for the speciﬁed number of epochs.\nFrom the results on the forward prediction datasets and the retrosynthesis prediction dataset\nwe can see that both Chemformer model sizes are able to outperform the existing SMILES-based\nstate-of-the-art on top-1 results. Chemformer-Large is also able to outperform the best graph-\nbased models on top-1 predictions. However, the tables also show that existing methods predict\nsigniﬁcantly more reactions correctly for top-5 and top-10 evaluation. We examine this eﬀect in\nmore detail in Section 4.\nIn Table 6 we compare the downstream molecular optimisation performance of a number of\npre-trained Chemformer models with existing implementations. In particular, we examine the\nperformance of all three pre-training tasks with base Chemformer models, after ﬁne-tuning for\n100 epochs, along with a Chemformer-Large model (pre-trained on the combined task) ﬁne-tuned\nfor 80 epochs. For the Transformer [6] and Transformer-R [7] benchmarks we use the published\nmodels, butexamineonlytop-1performance. Fromthetablewecanseethat, whileallChemformer\nmodels perform strongly in comparison to existing benchmarks, the smaller Chemformer models\noutperform the larger on the percentage of desirable molecules generated. The Transformer-R\nmodel, however, generates more molecules which meet the MMP-33 requirement. This metric\nmeasures the percentage of generated molecules for which, ﬁrstly, a single transformation has\nbeen applied to the starting molecule, and, secondly, the ratio between the number of heavy\natoms (non-hydrogen atoms) in the transformation and the number of heavy atoms in the entire\nmolecule is not greater than 0.33. All models we examined generated a very high proportion of\nvalid molecules, but the Chemformer models generated slightly more than existing approaches.\nModel Mixed Separated\nTop-1 Top-5 Top-10 Top-1 Top-5 Top-10\nMol Transformer [15] 88.6 94.2 - 90.4 95.3 -\nAug Transformer [5] 90.0 95.8 96.2 91.1 96.3 96.7\nChemformer 90.9 93.8 94.1 92.5 94.9 95.1\nChemformer-Large 91.3 93.7 94.0 92.8 94.9 95.0\nTable 4: Percentage of reactions predicted correctly in the forward direction from the USPTO\nMIT dataset. In theMixed dataset reactants and reagents are assorted arbitrarily, while in the\nSeparated dataset they are separated by an otherwise unused token.\n9\nModel Top-1 Top-5 Top-10\nSMILES-based\nSCROP [48] 43.7 65.2 68.7\nTwo-Way Transformer [49] 47.1 73.1 76.3\nAug Transformer [5] 48.3 73.4 77.4\nChemformer 53.6 61.1 61.7\nChemformer-Large 54.3 62.3 63.0\nGraph-based\nMEGAN [50] 48.1 78.4 86.1\nGLN [51] 52.5 75.6 83.7\nGraphRetro [52] 53.7 72.2 75.5\nTable 5: Percentage of retrosynthesis reactions predicted correctly on the USPTO-50K dataset on\na selection of SMILES- and graph-based approaches.\n4 Discussion\nThe downstream results presented in Section 3 show that the Chemformer model can be success-\nfully applied to both sequence-to-sequence and discriminative tasks. The results also that transfer\nlearning can provide a signiﬁcant boost to downstream performance and convergence speed. With\nthe exception of the molecular optimisation task, the model pre-trained on the combined task,\noutperforms all other Chemformer models, and, in some cases, outperforms the existing state-of-\nthe-art. This result suggests that valuable chemical information is contained in the weights of the\npre-trained Chemformer model.\n4.1 Sequence-to-Sequence Tasks\nDownstream results on sequence-to-sequence datasets show that our pre-trained Chemformer mod-\nels outperform not only their randomly initialised (no transfer learning) counterparts, but also the\ncurrent state-of-the-art models for a number of tasks. Speciﬁcally, Chemformer is able to beat\nthe existing state-of-the-art on top-1 prediction for direct synthesis and retrosynthesis prediction,\nand is able to produce more desirable molecules than existing approaches on the molecular opti-\nmisation task. However, for molecular optimisation, our Chemformer models used beam search\n(with a beam width of 10) to generate output molecules while the Transformer and Transformer-R\nbenchmarks used greedy search. Furthermore, these models contain fewer learnable parameters\nthan Chemformer and use a diﬀerent augmentation strategy. The randomly initialised Chem-\nformer model is able to generate more desirable molecules than the baselines, suggesting that\nthe performance of these existing models would improve with diﬀerent sampling or augmentation\ntechniques, or with more parameters.\nWhile the Chemformer model pre-trained on the combined task performed strongest on the\nModel Desirable MMP-33 Valid\nTransformer [6] 65.2 96.0 97.3\nTransformer-R [7] 70.2 99.0 98.4\nChemformer (Mask) 75.0 97.0 99.9\nChemformer (Augment) 74.3 97.8 99.9\nChemformer (Combined) 72.2 96.0 99.9\nChemformer-Large (Combined) 70.1 94.6 99.9\nTable 6: Percentage of top-1 generated molecules which fulﬁll the desirable properties, are matched\nmolecular pairs and are valid, for a selection of Chemformer models and existing implementations.\nThe pre-training tasks for the Chemformer models are shown in brackets.\n10\n(a) Top-1, 5 and 10 molecular accuracy\n (b) Percentage of replicated molecules\nFigure 5: Analysis of the impact of paug, the augmentation probabilty, on performance when\nﬁne-tuning Chemformer on USPTO-50K. The percentage of replicated molecules is determined by\ncalculating the proportion of molecules in the top-10 which are replicated, and taking an average\nover the whole dataset.\nreaction informatics tasks, the model pre-trained with only the masking task performs best on\nthe molecular optimisation dataset. This is a surprising result since, for the combined task, the\nmodel is required to solve both the masking and the augmentation pre-training tasks. One possible\nexplanation for this is that the combined task model overﬁts quickly on the molecular optimisation\ntask. This is supported by the lower performance of the Chemformer-Large model on the same\ntask. We therefore conjecture that with further hyperparameter tuning the performance of the\ncombined task model could be improved. In particular, more work is needed to determine the\noptimal number of epochs required for ﬁne-tuning.\nWhen comparing the performance on the forward synthesis and retrosynthesis prediction tasks,\nwe noted that the augmentation approach we employed resulted in stronger top-1 performance,\nbut that the top-5 and top-10 performance was weaker than existing methods. By analysing the\noutput of the beam search we found that the proportion of augmented forms of the same molecule\nin the beam outputs was signiﬁcantly larger in the models trained with augmentation than the\nmodel trained without augmentation. Figure 5a shows how the augmentation probability aﬀects\nthe top-1, top-5 and top-10 molecular accuracy for the USPTO-50K retrosynthesis prediction task.\nFine-tuning with no augmentation provides the lowest top-1 performance but the highest top-5\nand top-10 performance. Fine-tuning withpaug ∈{0.25,0.5,0.75,1.0}all lead to comparable top-1\nperformance, but top-5 and top-10 performance steadily decrease aspaug is increased. Figure 5b\nprovides an explanation for this eﬀect by examining the percentage of the ten beam outputs\nthat contain an augmented SMILES form of the same reactants. Filling the beam outputs with\naugmented forms results in a lower diversity when SMILES are converted back to molecules; this\ncauses the top-5 and top-10 results to converge towards the top-1. Our augmentation strategy\ntherefore creates a trade-oﬀ between an improvement in top-1 performance with a decrease in\ntop-5 and top-10.\nIn order to combat the detrimental eﬀect of augmentation on top-5 and top-10 results the\nbeam width could be increased signiﬁcantly. This would essentially counter the reduction in\nmolecular diversity by sampling more molecules. However, the amount of computation required\nscales linearly with the beam width; increasing the beam width from 10 to 50 would require ﬁve\ntimes as much computational resource. Alternatively, Levenshtein augmentation [53] could be\nused to ensure the input and generated SMILES sequences are similar. This would reduce the\nlikelihood of many SMILES forms being generated – speciﬁcally those which are dissimilar to the\ninput – therefore improving molecular diversity.\n11\n4.2 Discriminative Tasks\nIn addition to ﬁne-tuning on sequence-to-sequence tasks, we have also shown that it is possible\nto train Chemformer simultaneously on multiple discriminative tasks. For both the molecular\nproperty prediction tasks and the biological activity prediction tasks, the Chemformer model pre-\ntrained on the combined task shows strong performance in comparison to randomly initialised and\nSVR baselines.\nWhile the three pre-trained Chemformer models perform comparably on both sets of discrim-\ninative tasks, the randomly initialised model performs signiﬁcantly worse. A possible explanation\nfor this is that, without transfer learning, Chemformer overﬁts quickly on small datasets - the\nnumber of molecules per biological activity prediction task varies from 1241 to 5830, and the\nnumber of molecules per molecular property is no more than 4200. This explanation is supported\nby the observation that, for biological activity prediction, the optimal architecture found for the\nrandomly initialised model used only 6 million learnable parameters, in comparison to almost 20\nmillion for the pre-trained models. The randomly initialised model for property prediction uses\neven fewer parameters. Larger randomly initialised models were found to perform worse. These\nresults suggests that, in small-data regimes, pre-training is crucial for strong performance with\nTransformer models on discriminative tasks.\nOn the molecular property prediction tasks the results show that pre-trained Chemformer\nmodels are able to outperform the SVR baseline. On biological activity prediction, however,\neven the best performing Chemformer model, the combined task model, shows marginally lower\nperformance than the SVR. The SVR models are, however, trained on each activity prediction\ntask separately; meaning 133 models need to be maintained and productionised, in comparison\nwith a single Chemformer encoder. The performance of the SVR models may beneﬁt from this\nseparation, but more work is needed to determine the extent of this performance improvement.\nFor both sets of discriminative tasks, more experimentation is also required to compare our models\nwith existing baselines, including the use of additional molecular ﬁngerprinting algorithms.\nPrevious works [8–13] have also attempted to use Transformers for molecular property predic-\ntion. Many of these works publish stronger results than Chemformer on the three MoleculeNet\ntasks we investigated. For example, MolBERT publishes RMSEs of 0.531, 0.948 and 0.561 on\nESOL, Free Solvation and Lipophilicity, respectively. However, we do not produce a direct com-\nparison of these models with Chemformer for three reasons. Firstly, since we are constrained by\nthe use of simultaneous ﬁne-tuning, our dataset splits are diﬀerent. Secondly, we produce a single\nmodel for all tasks, while most existing baselines ﬁne-tune on each task separately. And, ﬁnally,\nmany existing models contain signiﬁcantly more learnable parameters than Chemformer; theMol-\nBERT model contains approximately 85 million parameters. Further improvements can be made\nto the Chemformer model by scaling up the size of the models and the size of the pre-training\ndataset. This is something we intend to investigate in future work.\n5 Conclusion\nIn this work we introduced the Chemformer model which makes use of the SMILES language\nfor application to diverse computational chemistry tasks. We investigated three diﬀerent self-\nsupervised pre-training techniques and applied these on a large dataset of unlabelled SMILES.\nFinally, we ﬁne-tuned the pre-trained Chemformer models on a selection of downstream tasks and\ncompared their performance to randomly initialised models and existing benchmarks.\nFrom the ﬁne-tuning results we presented three key conclusions can be drawn. Firstly, the\nChemformer model can be applied to a wide variety of downstream tasks, including both sequence-\nto-sequence and discriminative tasks, fairly easily. Secondly, self-supervised pre-training can im-\nprove convergence of the Chemformer model on downstream Cheminformatics tasks, and can\ntherefore signiﬁcantly improve results on these tasks when training time is limited. Finally, a\ncombination of transfer learning and our novel augmentation strategy is able to produce state-of-\nthe-art top-1 results on all downstream sequence-to-sequence tasks we examined.\n12\nGiven its ability to quickly ﬁne-tune on both sequence-to-sequence and discriminative chemin-\nformaticstasks, theproposedChemformermodelisasigniﬁcantsteptowardsagenerallyapplicable\ndeep learning model for computational chemistry.\nReferences\n[1] Vaswani, A.et al. Attention is all you need. InNIPS (2017).\n[2] Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural computation9, 1735–1780\n(1997).\n[3] Cho, K. et al. Learning phrase representations using rnn encoder–decoder for statistical\nmachine translation. InProceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 1724–1734 (2014).\n[4] Weininger, D. Smiles, achemicallanguageandinformationsystem.1.introductiontomethod-\nology and encoding rules.Journal of chemical information and computer sciences28, 31–36\n(1988).\n[5] Tetko, I. V., Karpov, P., Van Deursen, R. & Godin, G. State-of-the-art augmented nlp\ntransformer models for direct and single-step retrosynthesis. Nature communications 11,\n1–11 (2020).\n[6] He, J.et al. Molecular optimization by capturing chemist’s intuition using deep neural net-\nworks. Journal of cheminformatics13, 1–17 (2021).\n[7] He, J. et al. Transformer neural network for structure constrained molecular optimization.\nChemRxiv (2021).\n[8] Fabian, B.et al.Molecular representation learning with language models and domain-relevant\nauxiliary tasks. arXiv preprint arXiv:2011.13230(2020).\n[9] Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: Large-scale self-supervised pre-\ntraining for molecular property prediction.arXiv preprint arXiv:2010.09885(2020).\n[10] Xue, D.et al.X-mol: large-scale pre-training for molecular understanding and diverse molec-\nular analysis. bioRxiv (2020).\n[11] Wang, S., Guo, Y., Wang, Y., Sun, H. & Huang, J. Smiles-bert: large scale unsupervised\npre-training for molecular property prediction. InProceedings of the 10th ACM international\nconference on bioinformatics, computational biology and health informatics, 429–436 (2019).\n[12] Zhang, X.-C. et al. Mg-bert: leveraging unsupervised atomic representation learning for\nmolecular property prediction.Brieﬁngs in Bioinformatics(2021).\n[13] Maziarka, Ł.et al.Molecule attention transformer.arXiv preprint arXiv:2002.08264(2020).\n[14] Ross, J.et al.Do large scale molecular language representations capture important structural\ninformation? arXiv preprint arXiv:2106.09553(2021).\n[15] Schwaller, P.et al. Molecular transformer: a model for uncertainty-calibrated chemical reac-\ntion prediction. ACS central science5, 1572–1583 (2019).\n[16] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional\ntransformers for language understanding.arXiv preprint arXiv:1810.04805(2018).\n[17] Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, 7871–7880 (2020).\n13\n[18] Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understanding\nby generative pre-training (2018).\n[19] Radford, A.et al. Language models are unsupervised multitask learners (2019).\n[20] Dong, L.et al. Uniﬁed language model pre-training for natural language understanding and\ngeneration. arXiv preprint arXiv:1905.03197(2019).\n[21] Raﬀel, C. et al. Exploring the limits of transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research21, 1–67 (2020).\n[22] Bai, R.et al.Transfer learning: making retrosynthetic predictions based on a small chemical\nreaction dataset scale to a new level.Molecules 25, 2357 (2020).\n[23] Ishiguro, K., Ujihara, K., Sawada, R., Akita, H. & Kotera, M. Data transfer approaches to\nimprove seq-to-seq retrosynthesis.arXiv preprint arXiv:2010.00792(2020).\n[24] Wang, L., Zhang, C., Bai, R., Li, J. & Duan, H. Heck reaction prediction using a transformer\nmodel based on a transfer learning strategy.Chemical Communications56, 9368–9371 (2020).\n[25] Kreutter, D., Schwaller, P. & Reymond, J.-L. Predicting enzymatic reactions with a molecular\ntransformer. ChemRxiv (2020).\n[26] Zhang, Y. et al. Data augmentation and transfer learning strategies for reaction prediction\nin low chemical data regimes.Organic Chemistry Frontiers8, 1415–1423 (2021).\n[27] Pesciullesi, G., Schwaller, P., Laino, T. & Reymond, J.-L. Transfer learning enables the\nmolecular transformer to predict regio-and stereoselective reactions on carbohydrates.Nature\ncommunications 11, 1–8 (2020).\n[28] Li, X. & Fourches, D. Inductive transfer learning for molecular activity prediction: Next-gen\nqsar models with molpmoﬁt.Journal of Cheminformatics12, 1–15 (2020).\n[29] Karpov, P., Godin, G. & Tetko, I. V. Transformer-CNN: Swiss knife for QSAR modeling and\ninterpretation. Journal of Cheminformatics 12, 1–12 (2020). URL https://doi.org/10.\n1186/s13321-020-00423-w .\n[30] Sterling, T. & Irwin, J. J. Zinc 15–ligand discovery for everyone.Journal of chemical infor-\nmation and modeling55, 2324–2337 (2015).\n[31] Bjerrum, E. J. & Sattarov, B. Improving chemical autoencoder latent space and molecular\nde novo generation diversity with heteroencoders.Biomolecules 8, 131 (2018).\n[32] Bjerrum, E. J. SMILES Enumeration as Data Augmentation for Neural Network Modeling\nof Molecules. arXiv preprint arXiv:1703.07076(2017). URL http://arxiv.org/abs/1703.\n07076. 1703.07076.\n[33] Jin, W., Coley, C. W., Barzilay, R. & Jaakkola, T. Predicting organic reaction outcomes with\nweisfeiler-lehman network. In Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 2604–2613 (2017).\n[34] Schneider, N., Lowe, D. M., Sayle, R. A., Tarselli, M. A. & Landrum, G. A. Big data from\npharmaceutical patents: a computational analysis of medicinal chemists’ bread and butter.\nJournal of medicinal chemistry59, 4385–4402 (2016).\n[35] Schneider, N., Stieﬂ, N. & Landrum, G. A. What’s what: The (nearly) deﬁnitive guide\nto reaction role assignment. Journal of chemical information and modeling56, 2336–2346\n(2016).\n14\n[36] Mendez, D.et al.Chembl: towards direct deposition of bioassay data.Nucleic acids research\n47, D930–D940 (2019).\n[37] Kotsias, P.-C.et al.Directsteeringofdenovomoleculargenerationwithdescriptorconditional\nrecurrent neural networks.Nature Machine Intelligence2, 254–265 (2020).\n[38] Ruder, S. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098 (2017).\n[39] Wu, Z.et al.Moleculenet: a benchmark for molecular machine learning.Chemical science9,\n513–530 (2018).\n[40] Sturm, N. et al. Industry-scale application and evaluation of deep learning for drug target\nprediction. Journal of Cheminformatics12, 1–13 (2020).\n[41] Paszke, A.et al. Pytorch: An imperative style, high-performance deep learning library. In\nWallach, H.et al.(eds.) Advances in Neural Information Processing Systems 32, 8024–8035\n(Curran Associates, Inc., 2019).\n[42] Falcon, e. a., WA. Pytorch lightning. GitHub. Note:\nhttps://github.com/PyTorchLightning/pytorch-lightning3 (2019).\n[43] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer normalization.arXiv preprint arXiv:1607.06450\n(2016).\n[44] Hendrycks, D. & Gimpel, K. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016).\n[45] Smith, L. N. & Topin, N. Super-convergence: Very fast training of neural networks using large\nlearning rates. InArtiﬁcial Intelligence and Machine Learning for Multi-Domain Operations\nApplications, vol. 11006, 1100612 (International Society for Optics and Photonics, 2019).\n[46] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014).\n[47] Bjerrum, E., Rastemo, T., Irwin, R., Kannas, C. & Genheden, S. Pysmilesutils–enabling\ndeep learning with the smiles chemical language. ChemRxiv preprint 10.33774/chemrxiv-\n2021-kzhbs (2021).\n[48] Zheng, S., Rao, J., Zhang, Z., Xu, J. & Yang, Y. Predicting retrosynthetic reactions using\nself-corrected transformer neural networks. Journal of chemical information and modeling\n60, 47–55 (2019).\n[49] Kim, E., Lee, D., Kwon, Y., Park, M. S. & Choi, Y.-S. Valid, plausible, and diverse retrosyn-\nthesis using tied two-way transformers with latent variables.Journal of Chemical Information\nand Modeling61, 123–133 (2021).\n[50] Sacha, M., Błaż, M., Byrski, P., Włodarczyk-Pruszyński, P. & Jastrzębski, S. Molecule edit\ngraph attention network: Modeling chemical reactions as sequences of graph edits.arXiv\npreprint arXiv:2006.15426 (2020).\n[51] Dai, H., Li, C., Coley, C. W., Dai, B. & Song, L. Retrosynthesis prediction with conditional\ngraph logic network.arXiv preprint arXiv:2001.01408(2020).\n[52] Somnath, V. R., Bunne, C., Coley, C. W., Krause, A. & Barzilay, R. Learning graph models\nfor template-free retrosynthesis.arXiv preprint arXiv:2006.07038(2020).\n[53] Sumner, D., He, J., Thakkar, A., Engkvist, O. & Bjerrum, E. J. Levenshtein augmentation\nimproves performance of smiles based deep-learning synthesis prediction.ChemRxiv (2020).\n15",
  "topic": "Cheminformatics",
  "concepts": [
    {
      "name": "Cheminformatics",
      "score": 0.8725012540817261
    },
    {
      "name": "Discriminative model",
      "score": 0.7522097826004028
    },
    {
      "name": "Computer science",
      "score": 0.7487342357635498
    },
    {
      "name": "Machine learning",
      "score": 0.646836519241333
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6383235454559326
    },
    {
      "name": "Transformer",
      "score": 0.622909665107727
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5094407200813293
    },
    {
      "name": "Task (project management)",
      "score": 0.46260201930999756
    },
    {
      "name": "Engineering",
      "score": 0.15244802832603455
    },
    {
      "name": "Bioinformatics",
      "score": 0.14182424545288086
    },
    {
      "name": "Systems engineering",
      "score": 0.07777273654937744
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ]
}