{
  "title": "Differentially Private Language Models for Secure Data Sharing",
  "url": "https://openalex.org/W4385573108",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3213918378",
      "name": "Justus Mattern",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972202473",
      "name": "Zhijing Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2529371344",
      "name": "Benjamin Weggenmann",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224599527",
      "name": "Bernhard Schoelkopf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2035754532",
      "name": "Mrinmaya Sachan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4225713323",
    "https://openalex.org/W3197001178",
    "https://openalex.org/W2789226849",
    "https://openalex.org/W4229069570",
    "https://openalex.org/W2908109788",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W3169754167",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1465390462",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2887995258",
    "https://openalex.org/W2995465878",
    "https://openalex.org/W3193647133",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2963796896",
    "https://openalex.org/W4205228770",
    "https://openalex.org/W2955363520",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2742157926",
    "https://openalex.org/W2998378988",
    "https://openalex.org/W1992926795",
    "https://openalex.org/W3184238748",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W2167372639",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W2943912735",
    "https://openalex.org/W2621140322",
    "https://openalex.org/W2949387767",
    "https://openalex.org/W4249192582",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W3046518446",
    "https://openalex.org/W2963515066",
    "https://openalex.org/W2135930857",
    "https://openalex.org/W4287029337",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W2898778923",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3167530662",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2294904676",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W4241781926",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3167942460",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W2901147448",
    "https://openalex.org/W2966735522",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3104110119",
    "https://openalex.org/W2967880504",
    "https://openalex.org/W2986886197",
    "https://openalex.org/W2974738913",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3003815046",
    "https://openalex.org/W2889507104",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3045008227",
    "https://openalex.org/W4211134195",
    "https://openalex.org/W3090466896",
    "https://openalex.org/W1557833142",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4221150963",
    "https://openalex.org/W2963699739",
    "https://openalex.org/W2004915866",
    "https://openalex.org/W1985511977",
    "https://openalex.org/W4224318410",
    "https://openalex.org/W3154479671",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2963224817"
  ],
  "abstract": "To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4860–4873\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDifferentially Private Language Models for Secure Data Sharing\nJustus Mattern\nRWTH Aachen\njustus.mattern@rwth-aachen.de\nZhijing Jin\nMPI & ETH Zürich\nzjin@tue.mpg.de\nBenjamin Weggenmann\nSAP Security Research\nbenjamin.weggenmann@sap.com\nBernhard Schölkopf ∗\nMPI for Intelligent Systems\nbs@tue.mpg.de\nMrinmaya Sachan ∗\nETH Zürich\nmsachan@ethz.ch\nAbstract\nTo protect the privacy of individuals whose data\nis being shared, it is of high importance to de-\nvelop methods allowing researchers and com-\npanies to release textual data while providing\nformal privacy guarantees to its originators. In\nthe field of NLP, substantial efforts have been\ndirected at building mechanisms following the\nframework of local differential privacy, thereby\nanonymizing individual text samples before re-\nleasing them. In practice, these approaches\nare often dissatisfying in terms of the qual-\nity of their output language due to the strong\nnoise required for local differential privacy. In\nthis paper, we approach the problem at hand\nusing global differential privacy, particularly\nby training a generative language model in a\ndifferentially private manner and consequently\nsampling data from it. Using natural language\nprompts and a new prompt-mismatch loss, we\nare able to create highly accurate and fluent\ntextual datasets taking on specific desired at-\ntributes such as sentiment or topic and resem-\nbling statistical properties of the training data.\nWe perform thorough experiments indicating\nthat our synthetic datasets do not leak infor-\nmation from our original data and are of high\nlanguage quality and highly suitable for train-\ning models for further analysis on real-world\ndata. Notably, we also demonstrate that train-\ning classifiers on private synthetic data outper-\nforms directly training classifiers on real data\nwith DP-SGD.1\n1 Introduction\nRapid advancements in the field of deep learn-\ning and natural language processing (NLP) have\nenabled companies, public institutions and re-\nsearchers to extract information and gain knowl-\nedge from large-scale data generated by individu-\nals. In many cases, it is desirable to share such data\n∗ Equal Supervision.\n1Our code is available at https://github.com/\njustusmattern/private-datasets-with-llms .\nGPT-2 Prompt-Based  \nDP Finetuning \nSensitive Data\nShareable, Anonymous Data\nWrite a [sent.] review about a [prod.]: [Rev.] \nReviewProductSentiment\nCool novel!BookPositive\nDoesn't chargeElectronicsNegative\nReally useful!ElectronicsPositive\nReviewProductSentiment\nDumb deviceElectronicsNegative\nWhat a book!BookPositive\nGreat monitorElectronicsPositive\nGPT-2 Prompt-Based \nGeneration\nInternal:\nExternal:\nLLM\nDP LLM\nWrite a [sent.] review about a [prod.]: [Rev.] \nFigure 1: Main idea of our paper: To share potentially\nsensitive datasets with third parties, we train a language\nmodel (LM) on the sensitive data in a differentially\nprivate manner and consequently prompt the LM to\ngenerate synthetic samples with privacy guarantees.\nwith third parties, for example when analyses are\nperformed by external consultants or in order to\nprovide high quality benchmarks for the research\ncommunity. This, however, entails a variety of risks\nrelated to privacy that cannot merely be solved by\npseudonymization: A variety of deanonymization\nattacks enable the re-identification of individuals\nfrom tabular data such as movie ratings (Narayanan\nand Shmatikov, 2008), geolocation data (Lee et al.,\n2017) and notably also text (Koppel et al., 2009;\nShrestha et al., 2017; Fabien et al., 2020). It is there-\nfore highly desirable to develop anonymization\nmechanisms enabling secure data sharing, ideally\nwith mathematical privacy guarantees as granted by\ndifferential privacy (DP) (Dwork and Roth, 2014).\nExisting approaches anonymize every text sam-\nple individually by obtaining differentially pri-\nvate vector representations (Weggenmann and Ker-\nschbaum, 2018; Fernandes et al., 2019) or using\nsequence-to-sequence approaches that rewrite a\n4860\ngiven sample to eliminate user-revealing informa-\ntion (Shetty et al., 2018; Feyisetan et al., 2019a,\n2020a; Weggenmann et al., 2022), thereby follow-\ning local differential privacy. As pointed out by\nMattern et al. (2022), local DP requires a very high\ndegree of noise which often leads to incoherent\nlanguage and only little semantic overlap. The\nstrict requirements of local DP are, however, not\nnecessary if we assume that an entity aiming to\nshare data already has access to the full collection\nof user-written texts and only wants to release an\nanonymized version of it.\nIn this paper, inspired by recent advances demon-\nstrating the feasibility of training large language\nmodels (LLMs) in a differentially private manner\n(Li et al., 2021), we propose a globally differen-\ntially private data release mechanism relying on\nthe generation of a \"twin\" dataset of the original,\nsensitive user data from large language models.\nAs depicted in Figure 1, we train GPT-2 (Radford\net al., 2019) to generate texts of our original dataset\nbased on prompts inferred from the sample’s in-\ndividual attributes such as sentiment or topic. For\nfine-tuning, we use a differentially private optimiza-\ntion algorithm in order to protect the content of our\ntraining data. Subsequently, we sample from the\ntrained model to generate a large number of syn-\nthetic, anonymous texts, resulting in a verifiably\nprivate \"twin\" dataset. We carefully evaluate our\nproposed method using popular NLP datasets such\nas IMDb movie reviews or Amazon product re-\nviews. Here, we find that even after learning with\nstrong privacy guarantees such as ϵ= 3or ϵ= 8\nfrom only a very limited amount of training sam-\nples such as 25 or 50, our generated data is of high\nquality and the classifiers trained on it achieve ac-\ncuracies only ∼3% lower than those trained on the\nfull original dataset containing thousands of sam-\nples. Notably, we also find that transformer based\nclassification models trained on private data outper-\nform models trained on real data with differentially\nprivate optimization. Finally, we show that the dif-\nferentially private fine-tuning procedure effectively\nminimizes the risk of data leakage from language\nmodels that was previously discovered by Carlini\net al. (2021).\n2 Background\n2.1 Differential Privacy\nDifferential privacy (DP) is a formal notion of pri-\nvacy that is currently considered the state-of-the-art\nfor quantifying and limiting information disclosure\nabout individuals. It has been introduced by Dwork\net al. (2006a) under the name ϵ-indistinguishability\nwith the goal of giving semantic privacy by quan-\ntifying the risk of an individual that results from\nparticipation in data collection.\nIn the original, central model of DP, we con-\nsider adjacent datasets that differ by at most one\nrecord (i.e., one individual’s data). A differentially\nprivate query on both databases should yield match-\ning results with similar probabilities, i.e., answers\nthat are probabilistically indistinguishable. This is\nachieved via random mechanisms that return noisy\nquery results, thus masking the impact of each in-\ndividual.\nDefinition 1. Let ϵ >0 be a privacy parameter,\nand 0 ≤δ ≤1. A randomized mechanism M\non Xfulfills (ϵ,δ)-DP if for any pair of adjacent\ninputs x,x′∈X, and all sets of possible outputs\nZ ⊂supp M,\nPr [M(x) ∈Z] ≤eϵ ·Pr [M(x′) ∈Z]+ δ.\n(1)\nIn the local model (Duchi et al., 2013), noise is\nadded locally at the data source, before the data\nis collected and stored in a central database. A\nbasic example is randomized response (Warner,\n1965), where each survey participant either pro-\nvides a truthful or a random answer depending on\nthe flip of an (unbiased) coin. The local model\nmakes the strong assumption that any two inputs\nare considered adjacent, which often makes it diffi-\ncult to achieve a satisfying privacy-utility trade-off.\n2.2 Differentially Private Optimization\nAn important application of DP is privacy-\npreserving machine learning to protect the privacy\nof the training data. Typically, neural networks are\ntrained by optimizing a loss function using stochas-\ntic gradient descent (SGD) or a derived method\nsuch as Adam (Kingma and Ba, 2015), which itera-\ntively compute gradients of the loss function over\nbatches of samples from the training dataset. As\nshown by Song et al. (2013a); Bassily et al. (2014a);\nAbadi et al. (2016a), it is possible to implement a\ndifferentially private version of SGD (DP-SGD) by\nclipping the gradients and applying the Gaussian\nmechanism (Dwork and Roth, 2014): The latter\nworks by applying noise from an isotropic Gaus-\nsian distribution N(0,σ2I), where the standard\ndeviation σis derived based on the desired privacy\nparameters ϵand δ.\n4861\nTo achieve good privacy-utility trade-offs, it is\nimportant to accurately track the total privacy bud-\nget spent throughout the entire training. In the con-\ntext of DP, repeated executions of the same (here:\nGaussian) mechanism is referred to as composi-\ntion. Basic (Dwork et al., 2006b) and various more\nrefined, advanced composition theorems (Dwork\net al., 2010; Dwork and Rothblum, 2016; Bun and\nSteinke, 2016) have been stated in the literature that\naim at providing tight bounds for the overall pri-\nvacy budget. However, these advances still resulted\nin relatively loose bounds and thus large overall\nprivacy budgets over the course of highly itera-\ntive algorithms such as DP-SGD. Tight worst-case\nbounds for composition were derived by Kairouz\net al. (2015), however, it was shown to be compu-\ntationally infeasible to compute them in general\n(Murtagh and Vadhan, 2016).\nFor this reason, specific efforts have been made\nto find tighter bounds and accurate approximations\nfor the overall privacy loss: A first example that\nprovides substantial reduced upper bounds is the\nmoments accountant (Abadi et al., 2016a), which\nis closely related to Rényi DP (Mironov, 2017), a\ngeneralization of DP based on Rényi divergence.\nGaussian and f-DP (Dong et al., 2019) provide an\napproximation of the total budget using the central\nlimit theorem (CLT). Finally, Gopi et al. (2021);\nKoskela et al. (2020), inspired by Sommer et al.\n(2019), are able to compute the exact budget nu-\nmerically up to arbitrary precision by aggregating\nthe privacy loss random variable with fast Fourier\ntransform.\n3 Approach\nWe consider the following scenario to motivate\nour approach: an entity wants to implement NLP\npipelines to gain insights from internal data, e.g.,\nemails from customers. To seek advice and get sup-\nport for modeling the data and building pipelines,\nthe entity aims to share an excerpt of the internal\ndata with a third party such as a consultant or a\ngroup of researchers. In order to do this without\ncompromising the privacy of its customers, the aim\nis to synthesize a verifiably private “toy” dataset\nthat reflects the properties of the original data with-\nout leaking private information. On such a toy\ndataset, a third party could research how to best\nsolve the task at hand and train a model to perform\ninference on the actual internal data, without be-\ning able to access sensitive information about cus-\ntomers. Formally, we aim to achieve the following\ngoal: We consider a dataset consisting of a train-\ning set Dtrain and test set Dtest. Given Dtrain or a\nsubset of it, we want to train a generative model to\nsynthesize a dataset ‹Dtrain that does not leak infor-\nmation from the original Dtrain. Furthermore, the\nsynthesized dataset should share statistical prop-\nerties with the original one so that a classification\nmodel trained on ‹Dtrain performs as well as if it\nwas trained on Dtrain when making predictions\nabout Dtest.\nTo achieve this, we use the pretrained autore-\ngressive transformer model (Vaswani et al., 2017)\nGPT-2 (Radford et al., 2019) and use natural lan-\nguage prompts to enable the conditional generation\nof text based on desired textual attributes such as its\nsentiment, domain or genre provided in the prompt.\nFurthermore, we introduce a new training objective\nthat penalizes the generation of samples fitting an-\nother label to reduce the risk of faulty labeled sam-\nples in our synthetic dataset. Finally, we fine-tune\nour model using a differentially private optimizer\nto provide privacy guarantees for our training data\nand to prevent information leakage from our model\nwhen subsequently sampling our synthetic dataset.\n3.1 Conditional text generation with natural\nlanguage prompts\nAs we want to control specific textual attributes of\nour synthetic data, we need to train our model in a\nmanner that allows us to generate different types\nof texts corresponding to the desired attributes or\nlabels present in our dataset. We consider a text\nsample to correspond to a set of M attributes of\ninterest, namely A := {a1,a2,...,a M}, where\neach attribute aj can take on a set of categorical\nvalues Cj. In the case of product reviews, a1 could\nbe the sentiment of a review that can take on the\nvalues a1 ∈C1 = {Positive,Negative}and a2\ncan be the product category, so that a2 ∈C2 =\n{Books,Electronics,DVD,Kitchen}. Our goal\nis to learn a model p(x|a1,...,a M) in order to con-\ntrollably synthesize text samples according to our\ndesired attributes.\nA straightforward approach to realize this would\nbe to train a single generative model for all possi-\nble attribute value combinations. This approach\nis, however, highly memory-intensive, as it re-\nquires us to store the weights of a large number\nof models that grows exponentially with the num-\nber of categorical attributes. Following recent work\n4862\nWrite a [sentiment] review about a [product]:\n  \nFigure 2: Our template-based approach for generating\ntask instructions. A template consists of placeholders\nfor verbalizations of different attribute values.\n(Schick and Schütze, 2021a), we therefore train a\nsingle language model to conditionally generate\ntexts based on task instructions. Beyond reducing\nour memory needs, this approach allows us to lever-\nage our model’s pretraining knowledge and to per-\nform text generation with only very little training\nsamples (Schick and Schütze, 2021a). Our instruc-\ntions i(a1,..,a M) are formed using a template with\nplaceholders that are filled out with verbalizations\nv(aj) taking on different forms for different val-\nues of every attribute aj. An example of such an\ninstruction template is visualized in Figure 2.\nDuring the training stage, we use a differentially\nprivate optimizer to fine-tune our language model\nto generate each text sample within the original\ndataset based on the prompt corresponding to its\nindividual attributes. Subsequently, we can synthe-\nsize a new dataset by controllably sampling text\nbased on our desired attributes passed in the prompt.\nTo generate a private \"twin\" dataset, one might\nuse the same distribution of textual attributes as in\nthe original dataset. Alternatively, the instruction-\nbased approach allows us to control and change\nsuch ratios, for instance if we desire to debias our\noriginal data.\n3.2 Reducing faulty labels with\nprompt-mismatch objective\nThe standard training objective for autoregressive\nlanguage modeling is to minimize the negative log-\nlikelihood (NLL) of every token given its previ-\nous tokens. We incorporate the natural language\ninstructions (Radford et al., 2019; Brown et al.,\n2020) into this training objective. For every text\nsequence x and its corresponding attribute values\na := (a1,...,a M), we construct the concatenated\nsequence i(a)⊕x which prepends a corresponding\ntask instruction to each text sample. Let Ldenote\nthe length of this concatenated sequence and let wl\nbe the sequence’s l-th token. Our NLL loss is now\nNLL(i(a) ⊕x) =−\n∑\nwl∈i(a)⊕x\nlog p(wl|w<l) .\n(2)\nThis objective encourages the model to generate\ncorrect samples for a given instruction. However,\nit does not minimize the likelihood of generating\nwrong samples corresponding to another prompt\nand therefore attribute. This is specifically unfavor-\nable for our goal of generating synthetic training\ndatasets as every generated text having an error of\nthis kind corresponds to a wrongly labeled training\nsample. To address this, we extend the training\nobjective with a term penalizing the generation of a\ngiven sample for a wrong prompt. Specifically, let\nIwrong denote the set of all prompts not matching\nthe given attribute values a1,...,a M, specifically\nIwrong := {i(a1,..., aM) |aj ∈Cj \\{aj}}.\n(3)\nWe now define the overall training loss we are\naiming to minimize as\nLovr = NLL(i(a) ⊕x)\n− λ\n|Iwrong|\n∑\niw∈Iwrong\nNLL(iw ⊕x)) , (4)\nwhere λis the hyperparameter to balance the two\nlosses. Note that in practice, when the number of\npossible labels is high, this computation might be\ninefficient and the objective too complex for the\nmodel to realize. In this case, one might randomly\nsample a few class labels for the wrong prompt in\nevery training batch or penalize the generation for\nclass labels that are the most similar to the correct\none.\n4 Evaluation\nWe conduct extensive evaluation measuring the util-\nity and privacy of our generated data as well as the\nquality of its language. In this section, we describe\nthe datasets we use as well as our evaluation met-\nrics, implementation details and results.\n4.1 Datasets\nWe use two publicly available datasets that are\nwidely used for evaluating the performance of text\nclassification models:\n4863\nTable 1: Accuracy of classification models trained on synthetic data.\nIMDb Amazon\nSentiment Sentiment Product Category\n# Train Samples 25 50 5000 25 50 3000 25 50 3000\nBERT:\nϵ = 3 82.8% 88.3% 89.1% 85.2% 87.2% 88.5% 98.6% 98.7% 98.9%\nϵ = 8 86.0% 87.6% 89.1% 87.4% 85.9% 89.2% 98.5% 98.9% 98.9%\nϵ = ∞ 86.5% 87.6% 89.2% 89.2% 88.5% 89.2% 98.7% 98.8% 99.0%\nTF-IDF:\nϵ = 3 71.7% 78.3% 81.0% 69.5% 75.4% 79.1% 96.8% 97.0% 98.0%\nϵ = 8 76.4% 79.2% 82.6% 74.9% 74.5% 78.3% 96.8% 98.2% 98.2%\nϵ = ∞ 80.2% 79.0% 82.5% 75.2% 77.9% 79.7% 97.6% 97.9% 98.1%\n4.1.1 IMDb Movie Reviews\nThe IMDb movie review dataset2 consists of movie\nreviews written by various authors. We use the two\nbinary sentiment labels as attributes to condition\nour model on and use a random subset of 5,000\nreviews for training and evaluation each.\n4.1.2 Amazon Multi Domain Reviews\nThe Amazon multi domain review dataset was in-\ntroduced by Blitzer et al. (2007) and consists of\ntwo thousand product reviews from each of the\nfour product categories books, DVDs, electronics\nand kitchen appliances. Both binarized sentiment\nlabels and the product categories books and elec-\ntronics serve as attributes we consider. Our result-\ning training data consists of 3,000 training samples\nand 1,000 test samples.\n4.2 Implementation Details\nWe implement and train our language models us-\ning the PyTorch (Paszke et al., 2019) and Hug-\nging Face Transformers (Wolf et al., 2020) libraries\nand the 1.5B parameter implementation of GPT-\n2 (Radford et al., 2019). To fine-tune the lan-\nguage models, we employ the “privacy engine” of\nthe private-transformers3 package by Li et al.\n(2021). In line with their experiments, we also use\nDP-Adam (Dong et al., 2019; Bu et al., 2020), a\ndifferentially private version of the Adam (Kingma\nand Ba, 2015) optimizer. The privacy engine al-\nlows us to specify desired target privacy param-\neters ϵand δ, from which the standard deviation\nparameter σ for the Gaussian mechanism is de-\nrived using either Rényi DP (Mironov, 2017), the\nCLT (Dong et al., 2019), or the FFT accountant\n(Gopi et al., 2021). Following Li et al. (2021),\nwe set δ = 1\n2∗|Dtrain| and vary the parameter ϵ\n2https://datasets.imdbws.com/\n3https://github.com/lxuechen/private-\ntransformers\nTable 2: Accuracy of classification models trained on\nreal data.\nIMDb Amazon\nSentiment Sentiment Product\nBERT:\nϵ = 3 83.6% 79.5% 95.2%\nϵ = 8 86.7% 83.4% 96.6%\nϵ = ∞ 90.9% 91.2% 98.9%\nTF-IDF:\nϵ = ∞ 85.5% 75.8% 98.6%\nin our experiments. To obtain reliable results for\ntraining our generative models on small subsets of\nthe training samples, we sample three random sub-\nsets for every size and report averaged results from\nthese three experimental runs. We trained GPT-2\nover five epochs when using a differentially private\noptimizer and merely two epochs when using a\nnon-private optimizer, as the latter tended to overfit\nquickly on the small training set. To further miti-\ngate this, a smaller learning rate turned out to be\nmore effective for non-private optimization: While\nwe used a learning rate of 8e-6 with DP-Adam, we\nobtained the best results for non-private optimiza-\ntion with a learning rate of 5e-7. Lastly, we chose\nthe hyperparameter λ := 0.2. We generated our\nsynthetic datasets using the original distribution of\nsentiment and product category attributes, which\nwas 50 / 50 in all cases. To sample from GPT-2,\nwe use nucleus sampling (Holtzman et al., 2020)\nwith p = 0.8 across all experiments. Our results\nwere not obtained through an extensive hyperpa-\nrameter search but educated guesses over a couple\nof iterations to avoid large computational effort.\nAll experiments were performed using a NVIDIA\nTesla A100 GPU. With this setup, a training epoch\nover 1,000 text samples took approximately five\nminutes.\n4864\n4.3 Experimental Results\nAs stated previously, we aim to synthesize datasets\nthat (1) reflect properties of the original data and\ncan be used to train classifiers that perform similar\nto those trained on the original data, (2) are private\nand do not leak information from the original data\nand (3) are diverse and of high language quality.\nAccordingly, we perform experiments with metrics\nmaesuring these attributes and report our results in\nthe following:\n4.3.1 Data Utility\nTo measure the utility of our datasets, we train clas-\nsification models for each attribute on both our orig-\ninal data and the generated data and compare their\nperformances when evaluating them on our real\ntest data. Ideally, our anonymized twin datasets\nshould lead to classifiers that are as accurate as\nthose trained on our original data. To account for\nvarious settings including those with computational\nconstraints, we train a shallow support vector ma-\nchine classifier based on Tf-idf encodings as well\nas a deep BERT (Devlin et al., 2019) based clas-\nsifier with 110M parameters. Furthermore, as an\ninteresting baseline, we evaluate the performance\nof BERT trained on real data with differentially\nprivate optimization using the code from Li et al.\n(2021). The classification accuracies for models\ntrained on synthetic data are shown in Table 1 and\nclassification accuracies for models trained on real\ndata are shown in 2. In the following, we summa-\nrize our key findings:\nSynthetic data is almost on par with real data:\nThe performance of classification models trained\non generated data only drops minimally compared\nto those trained on real data. Across all three clas-\nsification tasks, for both datasets with privacy bud-\ngets of ϵ= 3and ϵ= 8, the accuracy of BERT and\nTf-Idf based models is never less than 3% of the\naccuracy obtained for real data when given access\nto all training samples (5,000 and 3,000 for IMDb\nand Amazon, respectively).\nClassifiers trained on synthetic data outperform\nprivate classifiers trained on real data: No-\ntably, when comparing the results of transformer\nbased classifiers trained on our synthetic data (Ta-\nble 1) to those trained on real data with differ-\nentially private optimization (Table 2), we find\nthat the former substantially outperforms the latter\nacross all tasks for both ϵ = 3 and ϵ = 8. This\nraises the question whether the intermediate step of\nprivate data generation should always be performed\nrather than training classifiers with DP-SGD.\nPrivate data generation shows high utility in\nfew-shot settings: Lastly, even when given only\nas little as 25 or 50 samples, GPT-2 can generate\ndatasets that lead to high-performing classifiers,\nwhich can most likely be attributed to the utilization\nof pretraining knowledge through our prompting\ntechniques. Therefore, beyond the anonymization\nof existing datasets, our method can be used to\nenlarge existing small datasets in a private manner.\n4.3.2 Data Privacy\nTo the best of our knowledge, methods aiming to\nmeasuring the privacy of textual data are an active\narea of research (Carlini et al., 2021; Brown et al.,\n2022) and there is no standardized and agreed upon\nway to do so. In our experiments, we follow Carlini\net al. (2021) by counting the number of instances in\nwhich our synthetic dataset contains samples that\nare extremely close to a sample from the training\ndata and can therefore be considered a duplicate:\nFor every sample xi from our training data used\nfor the language model and every xj ∈‹Dtrain, we\nmeasure the set of trigrams g3(xi), g3(xj). We\nconsider the two samples as duplicates if\n|g3(xi) ∪g3(xj)|≥ 2 ∗min(|g3(xi)|,|g3(xj)|)\nAs we hypothesize that duplicates are relatively\nrare, we double the generated data compared to\nour utility experiments and search for them within\n10,000 and 6,000 samples generated for the IMDb\nand Amazon dataset, respectively. Our results are\ndepicted in Table 3 and demonstrate the significant\nreduction of data leakage from privately trained\nmodels.\nIMDb Amazon\n# Samples 25 50 5000 25 50 3000\nϵ = 3 1 0 1 0 0 0\nϵ = 8 0 6 2 4 1 0\nϵ = ∞ 8 23 13 16 30 9\nTable 3: Number of duplicates from the training data\ngenerated by language models\n4.3.3 Language Quality\nAs a metric measuring the quality of our gener-\nated samples, we use the Mauve 4 (Pillutla et al.,\n4https://github.com/krishnap25/mauve\n4865\n2021) score to compute the similarity of the dis-\ntributions of Dtrain and the generated data ‹Dtrain\nfrom every trained model. As can be seen in Table\n4, higher ϵvalues tend to increase the quality mea-\nsured by Mauve, but overall seem not to be highly\nsignificant. As a reference, the Mauve score com-\nputed when comparing Dtrain and Dtest are 0.95\nfor IMDb and 0.94 for Amazon. Based on manual\ninspection, the quality of generated texts seems to\nbe very high. Mismatches between prompts and\ngenerated texts (e.g. a negative review generated\nfor a positive prompt) as well as incoherent gen-\nerations do occur, but very rarely. Excerpts of the\ngenerated data can be seen in Table 5, failure cases\ncan be found in Table 6 and 7 in the appendix.\nIMDb Amazon\n# Samples 25 50 5000 25 50 3000\nϵ = 3 0.81 0.83 0.81 0.82 0.81 0.83\nϵ = 8 0.82 0.81 0.81 0.82 0.81 0.82\nϵ = ∞ 0.81 0.85 0.84 0.84 0.85 0.82\nTable 4: Mauve scores measuring the similarity of gen-\nerated data and Dtrain.\n5 Related Work\n5.1 Text Anonymization\nSubstantial efforts have been made to enable\nthe privacy-preserving processing of textual data\nthrough both private textual vector representations\nand by transforming text into readable anonymous\nformats. Approaches from the former category ei-\nther aim at obtaining term frequency vectors using\ndifferentially private mechanisms (Weggenmann\nand Kerschbaum, 2018; Fernandes et al., 2019)\nor by using deep learning methods with adversar-\nial training objectives (Coavoux et al., 2018). In\nthe work by Qu et al. (2021), various local DP\nmechanisms are explored to obtain private BERT\nrepresentations.\nMethods aiming at rewriting texts in a privacy-\npreserving manner range from rule-based ap-\nproaches using human-engineered text perturba-\ntions (Mahmood et al., 2019; Bevendorff et al.,\n2019) as well as word replacements through the\nperturbation of individual word embeddings us-\ning differential privacy (Feyisetan et al., 2019b,\n2020b) to deep learning based approaches leverag-\ning sequence-to-sequence models. These sequence-\nto-sequence models can either incorporate adversar-\nial objectives penalizing the generation of author-\nrevealing information (Shetty et al., 2018; Xu et al.,\n2019) or integrate differential privacy in the text\nsampling process (Bo et al., 2021; Weggenmann\net al., 2022; Mattern et al., 2022).\nNotably, various papers proposing the inte-\ngration of differentially private mechanisms in\ndeep learning architectures (Krishna et al., 2021;\nBeigi et al., 2019a,b; Alnasser et al., 2021) have\nbeen shown to actually violate differential privacy\n(Habernal, 2021, 2022). While these works still\nrepresent important contributions due to their good\nempirical results, it should be noted that the design\nof NLP systems with DP guarantees is a task that is\nprone to errors and should be approached carefully.\n5.2 Differentially Private Language Model\nTraining\nAs generative language models have been shown\nto leak training data (Carlini et al., 2021) and the\nembeddings of discriminative models have been\nshown to contain sensitive information about a\ntext’s originator (Song and Raghunathan, 2020),\ndifferentially private optimizers such as DP-SGD\n(Song et al., 2013b; Bassily et al., 2014b) and\nDP-Adam (Abadi et al., 2016b; Kingma and Ba,\n2014) have been applied to a variety of NLP tasks.\nLarge-scale pretraining of BERT using DP-SGD\nhas shown to reap comparable masked language\nmodeling performance to non-private BERT (Anil\net al., 2021). For the tasks of text classification and\nnamed entity recognition, good performance has\nbeen obtained with BERT and DP-SGD, but only\nwith large privacy budgets of ϵ = 100 or higher.\nRecently, it has been demonstrated that with the\ncorrect choice of hyperparameters and fine-tuning\nobjectives aligned with the pretraining procedure,\nboth generative and discriminative language mod-\nels can achieve high performance in various tasks\neven with stricter privacy bounds (Li et al., 2021;\nYu et al., 2022). An active area of research is con-\ncerned with the empirical evaluation of a language\nmodel’s privacy (Brown et al., 2022) using meth-\nods such as membership inference attacks (Hayes\net al., 2019).\n5.3 Synthetic Data Generation:\nSynthetic data generation with privacy guarantees\nusing methods such as DP-GAN (Xie et al., 2018),\nPATE-GAN (Jordon et al., 2018) or various re-\nlated approaches has successfully been applied for\nstructured tabular or visual data (Torkzadehmahani\net al., 2019; Neunhoeffer et al., 2020; Chen et al.,\n2020). Beyond these methods, DPRP (Differen-\n4866\nTable 5: Exemplary generated samples from models trained with only 25 text samples. The texts were selected by\npicking a random sample from Dtest and finding the most similar one according to Sentence-BERT (Reimers and\nGurevych, 2019) within the generated data.\nAmazon (Category=Electronics, Sentiment=Positive)\nOriginal I’m not any expert in the computer world. But this product it is very easy to install. The range is very strong.\nI’m very happy with this product!!!\nϵ = 3 I really like this. It works great. The speed is a bit slow. But, after I learned that this is how a good\nkeyboard feels. I can’t say enough good things about this keyboard.\nϵ = 8 This is a good piece of equipment. It is small, has a high quality and a reasonable price. I used it to test the\nsound from my computer monitor. I am happy with the sound.\nϵ = ∞ I got this game to play on my PSP as a little side project. I’m quite happy with it. The graphics are pretty\ngood and the story line is pretty entertaining, as well.\nAmazon (Category=Books, Sentiment=Negative)\nOriginal Good Lord, people must be busier than I thought. If you haven’t been allowed to even think about your future\nafter retirement, then maybe you will get something out of this book. For the rest of us, this is drivel\nϵ = 3 I have been looking for an awesome, fun, easy to read book, like a classic from when I was a kid. I am very\ndisappointed. It is not a fun read. It really is a bore. I have read the sequel, and am also disappointed.\nThis is not one of those things that you read to pass the time. Not a good choice for a first book.\nϵ = 8 I would’ve done better with another book I read. The plot is interesting but the characters are bland and\nthe setting is really only a backdrop to the action and plot. It’s a shame as I enjoyed the book, but\nthis isn’t a good read.\nϵ = ∞ This book was boring, boring, and boring. I have been thinking about getting a new copy of this book ever\nsince I read it, but this one didn’t work for me at all. Not a bad idea, just not my cup of tea.\ntially Private Data Release via Random Projections)\n(Gondara and Wang, 2020) has been proposed as\na model free alternative for releasing small private\ndatasets that does not require training a genera-\ntive model. For the domain of text, synthetic data\ngeneration techniques have predominantly been de-\nveloped and evaluated without considering privacy\nguarantees (Anaby-Tavor et al., 2020; Schick and\nSchütze, 2021b). Merely the work presented by\nBommasani et al. (2019) is similar to our paper, but\ndoes not provide any quantitative results about the\nexperiments.\n6 Conclusion\nIn this paper, we explored the generation of syn-\nthetic datasets from differentially private language\nmodels as a solution for publicly sharing textual\ndata while protecting the privacy of users whose\ndata is being shared. Our experiments show that\nsynthetic data from differentially private language\nmodels is of high quality and is very well suited\nas training data for further tasks while significantly\nreducing the risk of leaking the original data. Our\napproach can be applied in a variety of use cases\nworking with sensitive data. An interesting chal-\nlenge for future work is the anonymization of mul-\ntimodal datasets consisting of tabular, visual and\ntext data.\nLimitations\nPrivacy Guarantee While differential privacy\nprovides a statistical privacy guarantee, one can\nnot be certain that a differentially private language\nmodel does not leak any sensitive information. As\nseen in our experiments, the differentially private\nmodels did leak some of their training data, even\nif significantly less than the non-private ones. This\ncan be a concern when dealing with training data\ncontaining names, telephone numbers or even pass-\nwords.\nSynthetic Data Quality As shown in Table 6 and\n7, our models did in rare cases produce incoherent\nlanguage or text samples that did not fit the desired\ncontrol attributes. This can limit the quality of the\ngenerated data.\n4867\nLimits of Controllable Generation The control-\nlability of multiple fine-grained textual attributes\nin text generation remains a difficult challenge Lyu\net al. (2021). We therefore need to assume that our\napproach will become less accurate the higher the\namount of textual attributes we want to consider.\nEthical Considerations\nData privacy is a highly important issue for the\nresponsible deployment of machine learning solu-\ntions. With our work, we directly contribute to this\nfield of research.\nAs our method relies on large pretrained lan-\nguage models, it should be noted that users deploy-\ning these technologies need to be aware of their\nundesirable, human-like biases (Sheng et al., 2019;\nAbid et al., 2021). Methods for reducing these\nharmful associations are actively being developed\nby the research community (Liang et al., 2021;\nSchick et al., 2021).\nAcknowledgments\nThis material is based in part upon works sup-\nported by the German Federal Ministry of Edu-\ncation and Research (BMBF): Tübingen AI Cen-\nter, FKZ: 01IS18039B; by the German Federal\nMinistry for Economic Affairs and Climate Ac-\ntion (BMWK) in the project Trade-EVs II, FKZ:\n01MV20006A; by the Machine Learning Cluster\nof Excellence, EXC number 2064/1 – Project num-\nber 390727645; by the John Templeton Foundation\n(grant #61156); by a Responsible AI grant by the\nHaslerstiftung; and an ETH Grant (ETH-19 21-1).\nZhijing Jin is supported by PhD fellowships from\nthe Future of Life Institute and Open Philanthropy,\nas well as the travel support from ELISE (GA no\n951847) for the ELLIS program. We also thank\nOpenAI Researcher Access Program for granting\nour team credits to their API.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H. Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016a. Deep learning with differential\nprivacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Secu-\nrity, CCS ’16, page 308–318, New York, NY , USA.\nAssociation for Computing Machinery.\nMartin Abadi, Andy Chu, Ian Goodfellow, H. Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016b. Deep learning with differential\nprivacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Secu-\nrity, CCS ’16, page 308–318, New York, NY , USA.\nAssociation for Computing Machinery.\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent anti-muslim bias in large language models.\nIn Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society, pages 298–306.\nWalaa Alnasser, Ghazaleh Beigi, and Huan Liu. 2021.\nPrivacy preserving text representation learning using\nbert. In Social, Cultural, and Behavioral Modeling,\npages 91–100, Cham. Springer International Publish-\ning.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):7383–7390.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert.\nRaef Bassily, Adam Smith, and Abhradeep Thakurta.\n2014a. Private empirical risk minimization: Efficient\nalgorithms and tight error bounds. In 2014 IEEE\n55th Annual Symposium on Foundations of Computer\nScience, pages 464–473. IEEE.\nRaef Bassily, Adam Smith, and Abhradeep Thakurta.\n2014b. Private empirical risk minimization: Efficient\nalgorithms and tight error bounds. In 2014 IEEE\n55th Annual Symposium on Foundations of Computer\nScience, pages 464–473.\nGhazaleh Beigi, Kai Shu, Ruocheng Guo, Suhang Wang,\nand Huan Liu. 2019a. I am not what i write: Privacy\npreserving text representation learning.\nGhazaleh Beigi, Kai Shu, Ruocheng Guo, Suhang Wang,\nand Huan Liu. 2019b. Privacy preserving text repre-\nsentation learning. In Proceedings of the 30th ACM\nConference on Hypertext and Social Media, HT ’19,\npage 275–276, New York, NY , USA. Association for\nComputing Machinery.\nJanek Bevendorff, Martin Potthast, Matthias Hagen, and\nBenno Stein. 2019. Heuristic authorship obfuscation.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 1098–\n1108, Florence, Italy. Association for Computational\nLinguistics.\nJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007.\nBiographies, Bollywood, boom-boxes and blenders:\nDomain adaptation for sentiment classification. In\nProceedings of the 45th Annual Meeting of the Asso-\nciation of Computational Linguistics, pages 440–447,\nPrague, Czech Republic. Association for Computa-\ntional Linguistics.\n4868\nHaohan Bo, Steven H. H. Ding, Benjamin C. M. Fung,\nand Farkhund Iqbal. 2021. ER-AE: Differentially\nprivate text generation for authorship anonymization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3997–4007, Online. Association for Computa-\ntional Linguistics.\nRishi Bommasani, Steven Wu, and Xanda Schofield.\n2019. Towards private synthetic text generation. In\nNeurIPS 2019 Machine Learning with Guarantees\nWorkshop.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy?\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nZhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su.\n2020. Deep learning with gaussian differential pri-\nvacy. Harvard data science review, 2020(23).\nMark Bun and Thomas Steinke. 2016. Concentrated\ndifferential privacy: Simplifications, extensions, and\nlower bounds. In Theory of Cryptography Confer-\nence, pages 635–658. Springer.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Úlfar\nErlingsson, Alina Oprea, and Colin Raffel. 2021. Ex-\ntracting training data from large language models. In\n30th USENIX Security Symposium (USENIX Security\n21), pages 2633–2650. USENIX Association.\nDingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz.\n2020. Gs-wgan: A gradient-sanitized approach for\nlearning differentially private generators. In Proceed-\nings of the 34th International Conference on Neu-\nral Information Processing Systems, NIPS’20, Red\nHook, NY , USA. Curran Associates Inc.\nMaximin Coavoux, Shashi Narayan, and Shay B. Co-\nhen. 2018. Privacy-preserving neural representations\nof text. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1–10, Brussels, Belgium. Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJinshuo Dong, Aaron Roth, and Weijie J Su. 2019.\nGaussian differential privacy. arXiv preprint\narXiv:1905.02383.\nJ. C. Duchi, M. I. Jordan, and M. J. Wainwright. 2013.\nLocal privacy and statistical minimax rates. In 2013\nIEEE 54th Annual Symposium on Foundations of\nComputer Science, pages 429–438.\nC. Dwork, F. McSherry, K. Nissim, and A. Smith. 2006a.\nCalibrating noise to sensitivity in private data anal-\nysis. In Theory of Cryptography Conference, pages\n265–284. Springer.\nCynthia Dwork, Krishnaram Kenthapadi, Frank McSh-\nerry, Ilya Mironov, and Moni Naor. 2006b. Our data,\nourselves: Privacy via distributed noise generation.\nIn Annual international conference on the theory and\napplications of cryptographic techniques, pages 486–\n503. Springer.\nCynthia Dwork and Aaron Roth. 2014. The Algorithmic\nFoundations of Differential Privacy, volume 9. Now\nPublishers Inc., Hanover, MA, USA.\nCynthia Dwork and Guy N Rothblum. 2016. Con-\ncentrated differential privacy. arXiv preprint\narXiv:1603.01887.\nCynthia Dwork, Guy N Rothblum, and Salil Vadhan.\n2010. Boosting and differential privacy. In 2010\nIEEE 51st Annual Symposium on Foundations of\nComputer Science, pages 51–60. IEEE.\nMaël Fabien, Esau Villatoro-Tello, Petr Motlicek, and\nShantipriya Parida. 2020. BertAA : BERT fine-\ntuning for authorship attribution. In Proceedings\nof the 17th International Conference on Natural Lan-\nguage Processing (ICON), pages 127–137, Indian\nInstitute of Technology Patna, Patna, India. NLP As-\nsociation of India (NLPAI).\nNatasha Fernandes, Mark Dras, and Annabelle McIver.\n2019. Generalised differential privacy for text docu-\nment processing. In International Conference on\nPrinciples of Security and Trust , pages 123–148.\nSpringer.\nOluwaseyi Feyisetan, Borja Balle, Thomas Drake, and\nTom Diethe. 2020a. Privacy- and Utility-Preserving\nTextual Analysis via Calibrated Multivariate Pertur-\nbations, page 178–186. Association for Computing\nMachinery, New York, NY , USA.\n4869\nOluwaseyi Feyisetan, Borja Balle, Thomas Drake, and\nTom Diethe. 2020b. Privacy-and utility-preserving\ntextual analysis via calibrated multivariate pertur-\nbations. In Proceedings of the 13th International\nConference on Web Search and Data Mining, pages\n178–186.\nOluwaseyi Feyisetan, Tom Diethe, and Thomas Drake.\n2019a. Leveraging hierarchical representations for\npreserving privacy and utility in text. In 2019 IEEE\nInternational Conference on Data Mining (ICDM),\npages 210–219.\nOluwaseyi Feyisetan, Tom Diethe, and Thomas Drake.\n2019b. Leveraging hierarchical representations for\npreserving privacy and utility in text. In 2019 IEEE\nInternational Conference on Data Mining (ICDM),\npages 210–219. IEEE.\nLovedeep Gondara and Ke Wang. 2020. Differentially\nprivate small dataset release using random projec-\ntions. In Proceedings of the 36th Conference on\nUncertainty in Artificial Intelligence (UAI), volume\n124 of Proceedings of Machine Learning Research,\npages 639–648. PMLR.\nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz.\n2021. Numerical composition of differential privacy.\nIn Advances in Neural Information Processing Sys-\ntems 34: Annual Conference on Neural Information\nProcessing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 11631–11642.\nIvan Habernal. 2021. When differential privacy meets\nnlp: The devil is in the detail. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1522–1528.\nIvan Habernal. 2022. How reparametrization trick broke\ndifferentially-private text representation learning.\nJamie Hayes, Luca Melis, George Danezis, and Emil-\niano De Cristofaro. 2019. Logan: Membership infer-\nence attacks against generative models. In Proceed-\nings on Privacy Enhancing Technologies (PoPETs),\nvolume 2019, pages 133–152. De Gruyter.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nJames Jordon, Jinsung Yoon, and Mihaela Van\nDer Schaar. 2018. Pate-gan: Generating synthetic\ndata with differential privacy guarantees. In Interna-\ntional conference on learning representations.\nPeter Kairouz, Sewoong Oh, and Pramod Viswanath.\n2015. The composition theorem for differential pri-\nvacy. In Proceedings of the 32nd International Con-\nference on Machine Learning , volume 37 of Pro-\nceedings of Machine Learning Research, pages 1376–\n1385, Lille, France. PMLR.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMoshe Koppel, Jonathan Schler, and Shlomo Argamon.\n2009. Computational methods in authorship attribu-\ntion. Journal of the American Society for information\nScience and Technology, 60(1):9–26.\nAntti Koskela, Joonas Jälkö, and Antti Honkela. 2020.\nComputing tight differential privacy guarantees using\nFFT. In The 23rd International Conference on Artifi-\ncial Intelligence and Statistics, AISTATS 2020, 26-28\nAugust 2020, Online [Palermo, Sicily, Italy], volume\n108 of Proceedings of Machine Learning Research,\npages 2560–2569. PMLR.\nSatyapriya Krishna, Rahul Gupta, and Christophe\nDupuy. 2021. ADePT: Auto-encoder based differ-\nentially private text transformation. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 2435–2439, Online. Association for\nComputational Linguistics.\nWei-Han Lee, Changchang Liu, Shouling Ji, Pra-\nteek Mittal, and Ruby B. Lee. 2017. Blind de-\nanonymization attacks using social networks. In Pro-\nceedings of the 2017 on Workshop on Privacy in the\nElectronic Society, WPES ’17, page 1–4, New York,\nNY , USA. Association for Computing Machinery.\nXuechen Li, Florian Tramèr, Percy Liang, and Tatsunori\nHashimoto. 2021. Large language models can be\nstrong differentially private learners.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understand-\ning and mitigating social biases in language models.\nIn International Conference on Machine Learning,\npages 6565–6576. PMLR.\nYiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy,\nBarnabás Póczos, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2021. StylePTB: A compositional\nbenchmark for fine-grained controllable text style\ntransfer. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2116–2138, Online. Association\nfor Computational Linguistics.\nAsad Mahmood, Faizan Ahmad, Zubair Shafiq, Pad-\nmini Srinivasan, and Fareed Zaffar. 2019. A girl\nhas no name: Automated authorship obfuscation us-\ning mutant-x. Proceedings on Privacy Enhancing\nTechnologies, 2019(4):54–71.\nJustus Mattern, Benjamin Weggenmann, and Florian\nKerschbaum. 2022. The limits of word level differ-\nential privacy.\nIlya Mironov. 2017. Rényi differential privacy. In 2017\nIEEE 30th Computer Security Foundations Sympo-\nsium (CSF), pages 263–275.\n4870\nJack Murtagh and Salil Vadhan. 2016. The complexity\nof computing the optimal composition of differen-\ntial privacy. In Theory of Cryptography Conference,\npages 157–175. Springer.\nArvind Narayanan and Vitaly Shmatikov. 2008. Robust\nde-anonymization of large sparse datasets. In 2008\nIEEE Symposium on Security and Privacy (sp 2008),\npages 111–125.\nMarcel Neunhoeffer, Steven Wu, and Cynthia Dwork.\n2020. Private post-gan boosting. In International\nConference on Learning Representations.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaid\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. Advances in Neural Information Process-\ning Systems, 34.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang,\nMichael Bendersky, and Marc Najork. 2021. Natural\nLanguage Understanding with Privacy-Preserving\nBERT, page 1488–1497. Association for Computing\nMachinery, New York, NY , USA.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021a. Few-shot\ntext generation with natural language instructions. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 390–\n402, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-Diagnosis and Self-Debiasing: A Proposal for\nReducing Corpus-Based Bias in NLP. Transactions\nof the Association for Computational Linguistics ,\n9:1408–1424.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRakshith Shetty, Bernt Schiele, and Mario Fritz. 2018.\nA4NT: Author attribute anonymity by adversarial\ntraining of neural machine translation. In 27th\nUSENIX Security Symposium (USENIX Security 18),\npages 1633–1650, Baltimore, MD. USENIX Associ-\nation.\nPrasha Shrestha, Sebastian Sierra, Fabio González,\nManuel Montes, Paolo Rosso, and Thamar Solorio.\n2017. Convolutional neural networks for authorship\nattribution of short texts. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 669–674, Valencia, Spain. Association\nfor Computational Linguistics.\nDavid M Sommer, Sebastian Meiser, and Esfandiar Mo-\nhammadi. 2019. Privacy loss classes: The central\nlimit theorem in differential privacy. Proceedings on\nprivacy enhancing technologies, 2019(2):245–269.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation Leakage in Embedding Models , page\n377–390. Association for Computing Machinery,\nNew York, NY , USA.\nShuang Song, Kamalika Chaudhuri, and Anand D Sar-\nwate. 2013a. Stochastic gradient descent with dif-\nferentially private updates. In 2013 IEEE Global\nConference on Signal and Information Processing ,\npages 245–248. IEEE.\nShuang Song, Kamalika Chaudhuri, and Anand D. Sar-\nwate. 2013b. Stochastic gradient descent with dif-\nferentially private updates. In 2013 IEEE Global\nConference on Signal and Information Processing ,\npages 245–248.\nReihaneh Torkzadehmahani, Peter Kairouz, and Bene-\ndict Paten. 2019. Dp-cgan: Differentially private\nsynthetic data and label generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\n4871\nStanley L Warner. 1965. Randomized response: A\nsurvey technique for eliminating evasive answer\nbias. Journal of the American Statistical Associa-\ntion, 60(309):63–69.\nBenjamin Weggenmann and Florian Kerschbaum. 2018.\nSynTF: Synthetic and differentially private term fre-\nquency vectors for privacy-preserving text mining.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\nSIGIR ’18, page 305–314, New York, NY , USA. As-\nsociation for Computing Machinery.\nBenjamin Weggenmann, Valentin Rublack, Michael An-\ndrejczuk, Justus Mattern, and Florian Kerschbaum.\n2022. DP-V AE: Human-readable text anonymization\nfor online reviews with differentially private varia-\ntional autoencoders. In Proceedings of the ACM Web\nConference 2022, WWW ’22, page 721–731, New\nYork, NY , USA. Association for Computing Machin-\nery.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nLiyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and\nJiayu Zhou. 2018. Differentially private generative\nadversarial network.\nQiongkai Xu, Lizhen Qu, Chenchen Xu, and Ran Cui.\n2019. Privacy-aware text rewriting. In Proceedings\nof the 12th International Conference on Natural Lan-\nguage Generation, pages 247–257, Tokyo, Japan.\nAssociation for Computational Linguistics.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\nSergey Yekhanin, and Huishuai Zhang. 2022. Differ-\nentially private fine-tuning of language models. In\nInternational Conference on Learning Representa-\ntions.\nA Failure Cases\nWe demonstrate manually selected failure cases in\nTables 6 and 7. Generated texts from Table 6 do not\nreflect the attributes passed in the prompt accurately\nand would therefore correspond to wrongly labeled\nsamples in our dataset. Table 7 features generated\ntexts that contain logical or language errors.\n4872\nTable 6: Failure cases in which the generated text does not fit the desired attributes. Model mistakes are marked in\nred.\n# Train Samples ϵ Desired Attributes Text\n25 3 Category=Books\nSentiment = Negative\nI have read and enjoyed many of your books, but I can’t recommend this\none highly enough. I am not a big fan of vampires. They don’t interest me\nat all This book, however, did. It was a fun, entertaining read. I loved the\ncharacters. I really didn’t care if I was going to be able to go back in time to\nhelp save the future, or if I would be in for the long haul. There was a lot of\ntension throughout the book. I was glad I was able to finish it. I’m glad that\nI was not as upset with it as many others were. I gave this book a solid 5 stars.\n50 3 Category=Books\nSentiment = Negative\nI was looking for a book on how to be a good leader and found it by chance.\nIt seems that no matter how bad you have been, how bad things have gotten,\nor how terrible a person you are, a leader can still turn it around.\n50 3 Category=Books\nSentiment = Positive\nI am not a huge fan of the Star Wars films but I really enjoyed this one.\nIt is the first time I’ve ever seen the Star Trek movies and I enjoyed\nthem as well. The plot is a bit different then you’d expect from a Star Trek\nstory but I enjoyed it all the same.\n50 8 Category=Electronics\nSentiment = Negative\nI have a brand new LG Electronics Vivid 5, and so far I’m very happy.\nThe screen has a nice clear picture and has great picture quality, the\nspeakers are good, the microphone is not great, the battery is adequate,\nand it’s nice and light. This is the first of the Vivid series I have bought, and\nI’m glad I got one.\nTable 7: Failure cases in which the generated text is incoherent or does not make sense. Model mistakes are marked\nin red.\n# Train Samples ϵ Desired Attributes Text\n50\n∞ Category=Electronics\nSentiment = Negative\nThe device itself, while very good, is bad. It seems like the software doesn’t\nlike to write out its own ID (it should) so the app doesn’t work. I’ve seen\nother users say the same thing and have had no problems. I haven’t bought another\nbluetooth device, but I’m pretty sure it has something to do with it.\n25 8 Category=Books\nSentiment = Positive\nI really enjoyed this book. I didn’t get the full explanation as I am only\na beginner but for beginners with the basic knowledge of a math and physics\nthe theory is very sound. 2.0 stars.\n4,000 ∞ Category=Books\nSentiment = Negative\nThis book is a compilation of some of my personal thoughts and ideas.\nThe author’s goal was to make me believe in witchcraft, but he\ncompletely failed.\n50 8 Category=Electronics\nSentiment = Positive\nThis is the best little portable guitar i’ve bought since the little portable\njuke box on a bike. The high quality quality quality of this product is great .\nI do wish i had bought a sooner the old jukebox but once you\nget it together its really nice to have.\n4873",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8449757099151611
    },
    {
      "name": "Differential privacy",
      "score": 0.8083447217941284
    },
    {
      "name": "Language model",
      "score": 0.545977771282196
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5251486301422119
    },
    {
      "name": "Generative model",
      "score": 0.5016775131225586
    },
    {
      "name": "Field (mathematics)",
      "score": 0.46596601605415344
    },
    {
      "name": "Private information retrieval",
      "score": 0.465934693813324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.459179162979126
    },
    {
      "name": "Synthetic data",
      "score": 0.45845264196395874
    },
    {
      "name": "Noise (video)",
      "score": 0.4446921646595001
    },
    {
      "name": "Machine learning",
      "score": 0.43872368335723877
    },
    {
      "name": "Data modeling",
      "score": 0.4294711947441101
    },
    {
      "name": "Data mining",
      "score": 0.3722074329853058
    },
    {
      "name": "Natural language processing",
      "score": 0.35792380571365356
    },
    {
      "name": "Generative grammar",
      "score": 0.3415601849555969
    },
    {
      "name": "Computer security",
      "score": 0.17285007238388062
    },
    {
      "name": "Database",
      "score": 0.13855350017547607
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}