{
  "title": "Using Large Language Models for Goal-Oriented Dialogue Systems",
  "url": "https://openalex.org/W4409729774",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2225160626",
      "name": "Leonid Legashev",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": "https://openalex.org/A2061933464",
      "name": "Alexander Shukhman",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": null,
      "name": "Vadim Badikov",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": "https://openalex.org/A5117276225",
      "name": "Vladislav Kurynov",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": "https://openalex.org/A2225160626",
      "name": "Leonid Legashev",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": "https://openalex.org/A2061933464",
      "name": "Alexander Shukhman",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": null,
      "name": "Vadim Badikov",
      "affiliations": [
        "Orenburg State University"
      ]
    },
    {
      "id": "https://openalex.org/A5117276225",
      "name": "Vladislav Kurynov",
      "affiliations": [
        "Orenburg State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4407362404",
    "https://openalex.org/W3216332495",
    "https://openalex.org/W3207535763",
    "https://openalex.org/W4389010441",
    "https://openalex.org/W4387316165",
    "https://openalex.org/W4387846655",
    "https://openalex.org/W4226162428",
    "https://openalex.org/W4392908891",
    "https://openalex.org/W4391305723",
    "https://openalex.org/W6798494020",
    "https://openalex.org/W4393035178",
    "https://openalex.org/W4229011615",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4391494845",
    "https://openalex.org/W6856439586",
    "https://openalex.org/W4402669899",
    "https://openalex.org/W4401555815",
    "https://openalex.org/W4405185387",
    "https://openalex.org/W4405282246",
    "https://openalex.org/W4405292086",
    "https://openalex.org/W4404685645",
    "https://openalex.org/W4404927256",
    "https://openalex.org/W4404765644",
    "https://openalex.org/W4404224824",
    "https://openalex.org/W4404047101",
    "https://openalex.org/W4403074495",
    "https://openalex.org/W4389520082",
    "https://openalex.org/W4401042590",
    "https://openalex.org/W4389524319",
    "https://openalex.org/W4402800989",
    "https://openalex.org/W4402670233",
    "https://openalex.org/W3045703328",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W4287075708"
  ],
  "abstract": "In the development of goal-oriented dialogue systems, neural network topic modeling and clustering methods are traditionally used to extract user intentions and operator response scenario blocks. The emergence of generative large language models allows one to radically change the approach to generate dialogue scenarios in the form of a graph with context preservation. In this article we analyzed seven popular large language models on prepared test prompts for Russian and English languages for intent mining and named entity recognition. The present study aimed to investigate the effectiveness of two methods for constructing dialogues in goal-oriented dialogue systems: the heuristic-based approach with additional training on labeled data and the prompt-based approach without such training. The primary objective was to evaluate the impact of incorporating labeled dialogue data on the quality of constructed dialogues, with a focus on dialogue context. The study emphasized the need for dialogue systems to consider the dialogue context in constructing goal-oriented dialogues. The two approaches were compared for the MultiWOZ 2.2 and MANTiS dialogue corpora on a locally deployed LLaMA model. The results showed that the LlaMA model without training on labeled dialogues achieved a BERTScore metric value of 0.75 for the MultiWOZ dataset and 0.72 for the MANTiS dataset, and the LlaMA model with training on labeled dialogues achieved a BERTScore metric value of 0.85 for the MultiWOZ dataset and 0.82 for the MANTiS dataset. This finding has practical implications for the development of more effective dialogue systems in the field of customer service that can engage users in more productive and meaningful machine-to-human interactions.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5738414525985718
    }
  ]
}