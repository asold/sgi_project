{
  "title": "Epilepsy seizure detection using Transformer",
  "url": "https://openalex.org/W4282826571",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2288740036",
      "name": "Hangyi Pan",
      "affiliations": [
        "Zhejiang Sci-Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A4282856228",
      "name": "Shuaikui Gong",
      "affiliations": [
        "Zhejiang Sci-Tech University"
      ]
    },
    {
      "id": "https://openalex.org/A2012314420",
      "name": "Fang Dong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2115365768",
      "name": "Lu-rong Jiang",
      "affiliations": [
        "Zhejiang Sci-Tech University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2588731307",
    "https://openalex.org/W2593745311",
    "https://openalex.org/W2795691199",
    "https://openalex.org/W2904559787",
    "https://openalex.org/W3209503092",
    "https://openalex.org/W2999435820"
  ],
  "abstract": "Electroencephalogram (EEG) is a general examination method for doctors to diagnose epilepsy, and it is also an important tool for studying brain activity. Due to the time-consuming and uncertainty of manually extracting features from EEG signals, this paper will be based on an end-to-end deep learning method different from the classic CNN and RNN network structure. This paper uses a relatively novel Transformer network structure to identify EEG whether the signal is epileptic. The experiment in this paper was carried out on the public CHBMIT data set, and finally, the average result of the five-fold cross-validation was 94.46%, the specificity was 93.97%, and the sensitivity was 94.96%. The experimental results show that the Transformer model has a higher performance improvement than the classic Resnet and Bi-LSTM networks, and it has greater potential in future epilepsy detection applications.",
  "full_text": "Highlights in Science, Engineering and Technology ICCIA 2022 \nVolume 1 (2022)  \n \n325 \nEpilepsy seizure detection using Transformer \nHangyi Pan 1, Shuaikui Gong 1, Fang Dong 2 and Lurong Jiang 1, * \n1 School of Information Science and Technology, Zhejiang Sci-Tech University, Hangzhou, China \n2 School of Information and Electrical Engineering, Zhejiang University City College, Hangzhou, \nChina \n*Corresponding author e-mail: jianglurong@zstu.edu.cn \nAbstract. Electroencephalogram (EEG) is a general examination method for  doctors to diagnose \nepilepsy, and it is also an important tool for studying brain activity. Due to the time- consuming and \nuncertainty of manually extracting features from EEG signals, this paper will be based on an end-to-\nend deep learning method differen t from the classic CNN and RNN network structure. This paper \nuses a relatively novel Transformer network structure to identify EEG whether the signal is epileptic. \nThe experiment in this paper was carried out on the public CHBMIT data set, and finally, the average \nresult of the five- fold cross-validation was 94.46%, the specificity was 93.97%, and the sensitivity \nwas 94.96%. The experimental results show that the Transformer model has a higher performance \nimprovement than the classic Resnet and Bi -LSTM networks, and it has greater potential in future \nepilepsy detection applications. \nKeywords: Electroencephalogram (EEG), Transformer, epilepsy detection. \n1. Introduction \nEpilepsy is a common chronic neurological disease caused by the abnormal discharge of human \nbrain neurons. In clinical manifestations, there will be sudden body cramps, unconscious movements, \nabsences, and other symptoms. At present, the diagnosis of epilepsy is mainly based on \nelectroencephalogram (EEG) to carry out research. The abnormal EEG sign als in epilepsy patients \nwill have different rhythm waves and abnormal amplitudes compared with the waveforms in the \nnormal period. \nBased on the abnormality of EEG signals  during epileptic seizures, many features obtained from \nEEG signal analysis can be used as indicators of epileptic seizures. EEG signal analysis methods can \ngenerally be divided into the time domain, frequency domain, and time-frequency domain. First, the \ntime domain analysis of EEG signals is based on the analysis and calculation of traditional statistical \nparameters based on the voltage amplitude of the EEG, and then studies the time -domain waveform \ndifference of EEG in the seizure and non-seizure period. E. Tessy et al. [1] extracted the two features \nof line length and energy in the time domain, and used some traditional algorithms k-nearest neighbor \n(KNN) classifier, linear discriminator analysis (LDA), quadratic discriminant analysis (QDA) is \nperformed to determine whether the EEG signal is a seizure signal. Second, the frequency domain  \nanalysis of EEG signals is based on the Fourier transform of the signal and extracts the frequency \ndomain components and frequency domain energy distribution of the signal as frequency domain \nfeatures for epilepsy seizure detection. Third, the time -frequency domain analysis of EEG signals \ncombines time-domain and frequency-domain information for analysis, including common analysis \nmethods such as The Short-time Fourier Transform (STFT), The Discrete Wavelet Transform (DWT), \nand Continuous Wavelet Transform  (CWT). Duo Chen et al. [2] used DWT to detect epileptic \nseizures and found that the reasonable setting of DWT has a greater impact on the performance of \nepileptic seizure detection. \nWith the popularity of deep learning in recent years, the method of using  neural networks to \nautomatically extract features and detect epilepsy seizure signals has been applied by many scholars. \nAmir H et al. [3] used a random forest to automatically select the features which are manually \nextracted, and then used a convolutiona l neural network (CNN) to replace the traditional machine \nlearning classifier. Mengni Zhou et al. [4] used Convolutional Neural Network (CNN) in the original \nHighlights in Science, Engineering and Technology ICCIA 2022 \nVolume 1 (2022)  \n \n326 \nEEG signal instead of manually extracted features, indicating that it is feasible to apply end -to-end \ndeep learning to epilepsy seizure detection. The convolutional neural network extracts only local \nfeatures and does not have temporal continuity. The Long Short Term Memory ( LSTM) network \nstructure may be better interpretive for sequential EEG signals. Shekokar KS et al. [5] used a three -\nlayer LSTM for epilepsy seizure detection. Minxing Geng et al. [6] deploy  the Stockwell \ntransformation on the original EEG signal and then feed the Bidirectional Long Short Term Memory \nnetwork (Bi-LSTM) for epilepsy seizure detection. But the LSTM network model always has limited \nmemory content, and the attention mechanism is a module different from CNN and LSTM. It filters \nglobal information to filter out a large amount of irrelevant content, and only obtains the content that \nyou care about. It is more in line with the mechanism of the human brain processing information. \nGoogle [7] proposed a new model Transformer based on Self-Attention and position embedding. \nBased on the above, this paper will use the Transformer model for epilepsy seizure detection based \non the public CHBMIT data set to prove the effectiveness of this model. \n2. Methods \n2.1 EEG data preprocessing \nThe public EEG dataset used in this paper was jointly contributed by Boston Children's Hospital \nand the Massachusetts Institute of Technology [8]. The data set contains 23 patient cases, and each \ncase contains 9-42 edf files which most of the files contain 23 channel signals, and these channels are \nnamed using the electrode positions of the international 10- 20 lead system name. This paper takes \ninto account the artifacts of eye movement and ECG and excludes the electrodes Fp1 and Fp2 near \nthe eyes and the electrodes FT9 and FT10 near the ears , as well as duplicate channels. According to \nthe start and end time of the epileptic seizures marked by experts, each seizure period is divided into \na 4s-length window, and the sliding step length of each segmentation is half of the window size to \nensure the continuity of the sample slices before and after. Because the time of seizures is small \nrelative to the time of non-seizures, to ensure the balance of positive and negative samples in the data \nset, this paper uses the negative sampling method to random ly sample the number of non- epilepsy \nsamples (negative) equivalent to the number of epileptic seizure samples (positive). This paper finally \ngot 10918 data samples, including 5428 positive samples and 5490 negative samples. \nThis paper does not use the traditional manual feature extraction method but uses the raw EEG \nsignal to synthesize the three-channel signal directly as the input of the neural network for end-to-end \nepilepsy detection. Compared with the manual feature extraction method, it can save a lot of time. \nBased on the effectiveness of frequency domain information, this paper combines the original signal, \nfrequency domain information, and filtered signal to form the three-channel EEG signal as the input \nof the neural network. The specific EEG signal preprocessing process is shown in Figure 1. \n \n \nFigure 1. EEG signal preprocessing. \n2.2 Transformer \nTransformer[7] is a popular model in recent years and has achieved good results in various tasks \nof NLP. In essence, Transformer is an en coder-decoder network structure that is often used to \nimplement sequence-to-sequence tasks. Since epileptic seizure detection is a two-class task, we only \nFilter Data\nFreq Data\nRaw Data\nFast Fourier\nTransformation\nButter\nBandpass FilterEEG\nHighlights in Science, Engineering and Technology ICCIA 2022 \nVolume 1 (2022)  \n \n327 \nneed to connect the encoder structure in the Transformer to a fully connected layer to complete our \nclassification task. The specific network structure is shown in Figure 2. \n \n \nFigure 2. Model structure. \n \nThe key module part in the Transformer encoder is multi -head attention. Its main function is to \ncalculate its attention score for the EEG vector after the input EEG signal is converted into a vector. \nFirst, calculate the query vector, key vector, and value vector by multiplying ğ‘Šğ‘Šğ‘„ğ‘„, ğ‘Šğ‘Šğ¾ğ¾ and ğ‘Šğ‘Šğ‘‰ğ‘‰ \nmatrices obtained by training, and then calculate the attention score of the EEG vector according to \nthe formula of the scaled dot product attention model. The formula is as follows: \nğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´ğ´(ğ‘„ğ‘„, ğ¾ğ¾, ğ‘‰ğ‘‰) = ğ‘ ğ‘ ğ´ğ´ğ‘ ğ‘ ğ´ğ´ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ï¿½ğ‘„ğ‘„ğ¾ğ¾ğ‘‡ğ‘‡\nï¿½ğ‘‘ğ‘‘ğ‘˜ğ‘˜\nï¿½ğ‘‰ğ‘‰ \nwhere the ğ‘‘ğ‘‘ğ‘˜ğ‘˜ in the formula represents the dimension of the key vector. \n3. Experimental results and analysis \nTo be able to show the superiority of the Transformer network structure, this paper compares \nTransformer with other classic CNN -based networks Resnet18, Resnet50, and Bi -LSTM. In this \npaper, five-fold cross-validation is performed on the preprocessed data set, using the SGD optimizer \nwith momentum, the loss function is cross -entropy, and the standard two -category index is used to \nevaluate the quality of the model, and calculate accuracy, sensitivity, and specificity according to the \nconfusion matrix in Figure 3(a). Among them, the accuracy rate refers to the proportion of the number \nof positive and negative samples that can be correctly identified, the sensitivity refers to the proportion \nof the number of samples that correctly identify epileptic seizures signals, and the spec ificity refers \nto the proportion of the number of samples that correctly identify non -seizure signals. The specific \nformula is shown in Figure 3(b). \n \n3 channel EEG embedding + \nposition embedding\nNormalization\nMulti-Head Attention\nFeed Forward\nFull Connection\nNormalization\nTransformer Encoder\nInput\nOutput\nHighlights in Science, Engineering and Technology ICCIA 2022 \nVolume 1 (2022)  \n \n328 \n \nFigure 3. Model evaluation methods. \n \nFrom the experimental results in Table 1, it can be found that in C NN-based Resnet18 and \nResnet50, the deep network performs better than the shallow network in all indicators, perhaps \nbecause the deep network extracts high -level features from three channels EEG signals. Compared \nwith the Resnet50, the Bi -LSTM model has a slightly lower accuracy rate, and the recognition rate \nof non-seizure samples is higher than that of epileptic seizure samples. On the Transformer, although \nit is lower in sensitivity than the Bi-LSTM, its accuracy and specificity are higher than other models, \nindicating that the Transformer-based epilepsy seizure detection method has greater potential. \n \nTable 1. Performance comparison of different models. \nModel AC SP SE \nResnet18 91.19 91.52 90.86 \nResnet50 92.78 93.32 92.26 \nBi-LSTM 92.52 88.22 97.73 \nTransformer 94.46 93.97 94.96 \n4. Conclusion \nThere are many epilepsy seizure detection methods based on EEG signals, but most of them are \nmanual extraction of EEG signal characteristics to detect whether the EEG signal is abnormal, \nbecause the manual extractio n process is  time-consuming. Therefore this paper uses an end -to-end \ndeep learning detection method. At the same time, few papers use the Transformer structure for EEG \nsignals, so we tried Transformer-based epilepsy seizure detection, and in the experiments of this paper, \nthe performance of the model was better than that of the network model based on CNN and LSTM. \nFinally, an average accuracy of 94.46%, 93.97% specificity, and 94.96% sensitivity was obtained in \nthe five-fold cross-validation. \nAcknowledgments \nThis research was supported by the \"Pioneer\" and \"Leading Goose\" R&D Program of Zhejiang \n(2022C03136), Agriculture and Social Development Scientific Research Project of  Hangzhou \n(20201203B145). \nReferences \n[1] E. Tessy, P. P. M. Shanir and S. Manafuddin, Time domain analysis of epileptic EEG for seizure detection. \n2016 International Conference on Next Generation Intelligent Systems (ICNGIS), 2016, pp. 1-4. \n[2] Chen Duo, Suiren Wan, Jing Xiang, Forrest Sheng Bao. A high-performance seizure detection algorithm \nbased on Discrete Wavelet Transform (DWT) and EEG. PloS one 12.3 (2017). \n[3] Ansari AH, Cherian PJ, Caicedo A, Naulaers G, De Vos M, Van Huffel S. Neonatal Seizure Detection \nUsing Deep Convolutional Neural Networks. Int J Neural Syst. 2019, 29(4). \nPredicted\nActual\nPositive Negative \nPositive TP FN\nNegative FP TN\nAccuracy(AC)\nSensitivity(SE)\nSpecificity(SP)\n%100Ã—+++\n+\nFPTNFNTP\nTNTP\n%100Ã—+FNTP\nTP\n%100Ã—+FPTN\nTN\n(a) (b)\nHighlights in Science, Engineering and Technology ICCIA 2022 \nVolume 1 (2022)  \n \n329 \n[4] Zhou M, Tian C, C ao R, et al. Epileptic Seizure Detection Based on EEG Signals and CNN. Front \nNeuroinform. 2018. \n[5] Shekokar, K.S. and Dour, S., Automatic epileptic seizure detection using LSTM networks, World Journal \nof Engineering, 2021.  \n[6] Geng M, Zhou W, Liu G, Li C, Zhang Y. Epileptic Seizure Detection Based on Stockwell Transform and \nBidirectional Long Short-Term Memory. IEEE Trans Neural Syst Rehabil Eng. 2020 Mar;28(3):573-580. \n[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz \nKaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International \nConference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, \nUSA, 6000â€“6010. \n[8] Information on https://physionet.org/content/chbmit/1.0.0/. ",
  "topic": "Epilepsy",
  "concepts": [
    {
      "name": "Epilepsy",
      "score": 0.7266942858695984
    },
    {
      "name": "Transformer",
      "score": 0.6987797021865845
    },
    {
      "name": "Computer science",
      "score": 0.6959546804428101
    },
    {
      "name": "Electroencephalography",
      "score": 0.6953330039978027
    },
    {
      "name": "Artificial intelligence",
      "score": 0.562046468257904
    },
    {
      "name": "Epileptic seizure",
      "score": 0.48851263523101807
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4535579979419708
    },
    {
      "name": "Machine learning",
      "score": 0.38535410165786743
    },
    {
      "name": "Speech recognition",
      "score": 0.3324185609817505
    },
    {
      "name": "Neuroscience",
      "score": 0.21319809556007385
    },
    {
      "name": "Psychology",
      "score": 0.16313117742538452
    },
    {
      "name": "Engineering",
      "score": 0.08892685174942017
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}