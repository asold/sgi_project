{
  "title": "Retrieval-Augmented Domain Adaptation of Language Models",
  "url": "https://openalex.org/W4385570162",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3035318654",
      "name": "Ben-Feng Xu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2122418258",
      "name": "Chunxu Zhao",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098983616",
      "name": "Jiang Wenbin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2098694214",
      "name": "Pengfei Zhu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2890520053",
      "name": "Songtai Dai",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2042434844",
      "name": "Chao Pang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2157903185",
      "name": "Zhuo Sun",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2938124241",
      "name": "Shuohuan Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097179453",
      "name": "Yu Sun",
      "affiliations": [
        "Baidu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3169565655",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W4385572749",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W2991316439",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W3200936406",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3114610051",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W4287888691",
    "https://openalex.org/W4285122897",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3169341408",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3135176278",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4308613717"
  ],
  "abstract": "Language models pretrained on general domain corpora usually exhibit considerable degradation when generalizing to downstream tasks of specialized domains. Existing approaches try to construct PLMs for each specific domains either from scratch or through further pretraining, which not only costs substantial resources, but also fails to cover all target domains at various granularity. In this work, we propose RADA, a novel Retrieval-Augmented framework for Domain Adaptation. We first construct a textual corpora that covers the downstream task at flexible domain granularity and resource availability. We employ it as a pluggable datastore to retrieve informative background knowledge, and integrate them into the standard language model framework to augment representations. We then propose a two-level selection scheme to integrate the most relevant information while alleviating irrelevant noises. Specifically, we introduce a differentiable sampling module as well as an attention mechanism to achieve both passage-level and word-level selection. Such a retrieval-augmented framework enables domain adaptation of language models with flexible domain coverage and fine-grained domain knowledge integration. We conduct comprehensive experiments across biomedical, science and legal domains to demonstrate the effectiveness of the overall framework, and its advantage over existing solutions.",
  "full_text": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023), pages 54‚Äì64\nJuly 13, 2023 ¬©2023 Association for Computational Linguistics\nRetrieval-Augmented Domain Adaptation of Language Models\nBenfeng Xu‚àó, Chunxu Zhao‚àó, Wenbin Jiang‚Ä†, PengFei Zhu\nSongtai Dai, Chao Pang, Zhuo Sun, Shuohuan Wang, Yu Sun\nBaidu Inc.\nAbstract\nLanguage models pretrained on general domain\ncorpora usually exhibit considerable degrada-\ntion when generalizing to downstream tasks of\nspecialized domains. Existing approaches try\nto construct PLMs for each speciÔ¨Åc domains ei-\nther from scratch or through further pretraining,\nwhich not only costs substantial resources, but\nalso fails to cover all target domains at various\ngranularity. In this work, we propose RADA,\na novel Retrieval-Augmented framework for\nDomain Adaptation. We Ô¨Årst construct a tex-\ntual corpora that covers the downstream task at\nÔ¨Çexible domain granularity and resource avail-\nability. We employ it as a pluggable datastore\nto retrieve informative background knowledge,\nand integrate them into the standard language\nmodel framework to augment representations.\nWe then propose a two-level selection scheme\nto integrate the most relevant information while\nalleviating irrelevant noises. SpeciÔ¨Åcally, we\nintroduce a differentiable sampling module as\nwell as an attention mechanism to achieve both\npassage-level and word-level selection. Such\na retrieval-augmented framework enables do-\nmain adaptation of language models with Ô¨Çexi-\nble domain coverage and Ô¨Åne-grained domain\nknowledge integration. We conduct compre-\nhensive experiments across biomedical, science\nand legal domains to demonstrate the effective-\nness of the overall framework, and its advan-\ntage over existing solutions.\n1 Introduction\nLanguage models pretrained on large scale of un-\nsupervised corpora are capable of producing pow-\nerful representations as well as providing satisfac-\ntory performance when Ô¨Ånetuned for general do-\nmain downstream tasks (Devlin et al., 2019; Brown\net al., 2020). However, as such representations are\nlearned from general domain distributions, their\n*Equal contribution.\n‚Ä†Correspondence to: jiangwenbin@baidu.com\ngeneralization performance are deteriorate on spe-\ncialized domains where distribution are much more\ndifferent, such as biomedicine, science, legal, etc.\nMany works thus trying to construct domain-\nspeciÔ¨Åc PLMs either from scratch or initialized\nfrom the original PLM, such as BioBERT (Lee\net al., 2020), LegalBERT (Chalkidis et al., 2020),\nSciBERT (Beltagy et al., 2019), etc. Through the\npretraining procedure, domain-speciÔ¨Åc knowledge\nare memorized and internalized into the parame-\nter of the PLMs, thus providing a better initializa-\ntion point for learning downstream tasks. How-\never, such attempts not only consume much more\ncosts, as it requires considerable computation to\npretrain a language model, but also can not cover\nadaptive needs at task- or even instance-level gran-\nularity, as domain-speciÔ¨Åc PLMs are preliminarily\ntrained with a Ô¨Åxed scale domain corpora but ap-\nplied for all domain tasks. In fact, domain can be\ndeÔ¨Åned at various granularity w.r.t. various appli-\ncations (Chronopoulou et al., 2022). For example,\nin the biomedical domain, texts might distribute\nacross academic publication, medical record or\nmedical-situated dialog, and exhibit completely dif-\nferent style and background knowledge.\nIn this paper, we novelly propose a Retrieval-\nAugmented framework for Domain Adaptation\n(RADA) to address the above challenges. Instead\nof internalizing a Ô¨Åxed corpora into PLM param-\neters via further pretraining, we construct and re-\ntrieve a datastore to dynamically enhance the learn-\ning of domain-speciÔ¨Åc downstream tasks. The Ô¨Çex-\nibility of such a retrieval-augmented framework is\nin two-fold: 1) On one hand, we no longer need\nto maintain multiple PLMs respectively for each\napplication domains, all domains simply share the\nsame checkpoint for initialization but with domain-\nspecialized datastore. 2) On the other hand, the\ndatastore only consists of most relevant background\npassages tailored for each speciÔ¨Åc task. Such task-\nlevel granularity provides best trade-off between\n54\nGeneralDomainPLM\nDomainAPLMDomainBPLMDomainCPLM\nDomainACorpusDomainBCorpusDomainCCorpus GeneralDomainPLM DomainBCorpusDomainACorpusDomainCCorpus\nDomainATaskDomainBTaskDomainCTask\nDomainATaskDomainBTaskDomainCTask\n(a)FurtherPretrainedDA (b)Retrieval-BasedDA\nFigure 1: Illustration of the motivated retrieval-augmented framework for language model domain adaptation.\nscale and coverage, we either do not need to collect\na very large corpora in order to effectively cover all\nsub-domains, nor concerned with its coverage for\nthis speciÔ¨Åc task. The illustration of the motivated\nframework compared to existing further pretraining\nsolution is given in Figure 1.\nThe core component of such a retrieval-\naugmented framework remains how to effectively\nintegrate the retrieved passages. One line of related\nworks is knowledge-enhanced PLMs (Zhang et al.,\n2019; Peters et al., 2019; Liu et al., 2020), they\nmostly resort to attention mechanism to directly\nincorporate the matched knowledge triples. Differ-\nently, we consider textual domain corpus instead of\nstructured knowledge in this paper as the latter only\nhas very limited coverage, especially for domain\nspecialized scenarios. As a consequence, we aim to\nintegrate retrieved passages that are less knowledge-\nintensive and might be mixed with noisy informa-\ntion that are irrelevant to solve the task. We ac-\ncordingly propose a two-level selection scheme:\npassage-level selection and word-level selection.\nAmong the multiple retrieved passages given by a\ncoarse-grained but efÔ¨Åcient retriever, we Ô¨Årst pro-\npose a differentiable sampling module that enables\nthe model to dynamically select the most helpful\nbackground passage. We then adopt an attention\nmechanism to weigh all words inside the selected\npassage to achieve more Ô¨Åne-grained integration.\nThe retrieval-augmentation interface works as a\npluggable module that needs no further modiÔ¨Åca-\ntion of the main encoding pipeline.\nWe conduct experiments on 7 popular tasks cov-\nering three different downstream domains and a\nwide range of task variety. The results and abla-\ntions demonstrate the effectiveness and advantage\nof the proposed framework. The contribution of\nthis paper is three-fold:\n‚Ä¢ Conceptual Contribution We novelly pro-\npose a retrieval-augmented framework for\ndomain-adaptation of PLMs. The framework\nis designed as a pluggable module, which not\nonly saves the costs to construct and maintain\nspecialized PLMs for each respective domain,\nbut also provides domain specialty at Ô¨Çexible\ntask-level granularity with no concern of the\ntrade-off between domain scale and coverage.\n‚Ä¢ Technical Contribution We propose a two-\nlevel selection scheme to achieve Ô¨Åne-grained\nintegration of retrieved passages while al-\nleviate noise distraction. SpeciÔ¨Åcally, we\nintroduce a differentiable sampling module\nfor passage-level selection and an attention\nmechanism to further assign word-level im-\nportance.\n‚Ä¢ Experimental Contribution We achieve con-\nsistent improvements across different domains\nincluding Science, Biomedicine and Legal.\nWhen combined with further pretraining, the\nimprovements of the proposed framework are\nmuch more signiÔ¨Åcant, surpassing competi-\ntive baseline of vanilla further pretraining.\n2 Related Work\nDomain Adaptation (Ramponi and Plank, 2020) is\nan important and widely investigated problem in\nthe area of NLP. In the current pretrain-Ô¨Ånetune\nparadigm, it becomes even more practical as\nmost of NLP systems start with a general-purpose\nPLM but applied into various downstream scenar-\nios (Guo and Yu, 2022). Many early works have\nfound that continued pretraining of PLM on domain\nspeciÔ¨Åc corpus could bring beneÔ¨Åts (Alsentzer\net al., 2019; Lee et al., 2020). Gururangan et al.\n(2020) take a step further to systematically inves-\ntigate the effect of further pretraining on multiple\ndomains, which they refer to as domain-adaptive\n55\nPLM PLM\nTa s kInput\nRetrievalAugmentationInterface\nDomainCorpus\nCross-AttentionCross-Attention\nAggregate\n‚®Å\nGumbelSoftmaxGradientApproximation\nùëÑ ùêæ&ùëâ ùêæ&ùëâ\nShared\nPost-Norm\nRA-Interface\n0.120.400.31\n0.120.400.310.250.100.22\nPassage-levelselection\nword-levelselection\nPassageA PassageB\nRetrieverùë•!,ùë•\",‚Ä¶,ùë•#,‚Ä¶,ùë•$\nAdaptedRepresentation\nFigure 2: The overall retrieval-augmented framework of RADA. The RA-Interface serves as a pluggable\nmodule to augment task-agnostic input representations with domain-specialized background passages.\npretraining (DAPT). Besides, they also propose\ntask-adaptive pretraining (TAPT) to perform fur-\nther pretraining also on task data, which further\nbrings signiÔ¨Åcant improvements.\nThere are mainly two concerns for these further\npretraining based methods. One is the cost to con-\nstruct and maintain considerable number of PLMs\nfor each speciÔ¨Åc domain, which can be quite a lot\nspreading over various application scenarios. The\nother is the complication to formally deÔ¨Åne the\nconcept of domain because domains exhibit hierar-\nchical taxonomy in practical scenarios (Reid et al.,\n2022). The scope of a domain should neither be too\nbroad, which results in loss of domain specialty, nor\nbe too narrow, which results in loss of generaliza-\ntion ability. Many recent works (Chronopoulou\net al., 2022; Gururangan et al., 2022; Li et al.,\n2022) thus propose to incorporate multiple domains\ninto one shared model, and introduce mixture-of-\nexperts module to allow different sub-domains also\nshare with each other. By contrast, RADA takes\na very different retrieval-based road to resolve the\nabove concerns.\nSeveral other works also try to advance domain\nadaptation from various perspectives. For example,\nYu et al. (2021) studied DAPT for the speciÔ¨Åc task\nof abstractive summarization. Xu et al. (2021)\npropose gradual Ô¨Ånetuning for domain adaptation.\nBai et al. (2021) investigates the trade-off between\nDAPT and data annotation under limited budget\nfor domain adaptation.\nAnother very related line of work is retrieval-\naugmented language models. kNN-LM (Khan-\ndelwal et al., 2020) retrieves for similar language\nmodeling probability distributions and interpolate\nthem with the current step of token generation.\nGuu et al. (2020) and Lee et al. (2019) propose\nto learn differentiable retriever to better select help-\nful passages. Lewis et al. (2020) propose a non-\nparametric retriever and combine it with a gen-\nerator model. Borgeaud et al. (2022) use atten-\ntion mechanism to integrate retrieved passages\nand also explore the datastore in a very large\nscale. Izacard et al. (2022) investigate the use-\nfulness of retrieval-augmentation in the context\nof few-shot prompting with LLMs. Although be-\ning technically related, most of these works focus\non general-purpose language understanding, espe-\ncially knowledge-intensive tasks. While in this\npaper, we focus on the challenge of LM domain\nadaptation, and also novelly propose a two-level se-\nlection scheme to further augment the framework.\n3 Methodology\nRADA is a general framework for task-agnostic do-\nmain adaptation, and can be readily integrated into\nthe standard pipeline of PLM deployment. In the\nfollowing section, we Ô¨Årst brief the construction of\ndomain specialized datastore in Section 3.1, and\nthe framework formulation in Section 3.2. We then\nintroduce the core component, a pluggable inter-\nface for retrieval-augmented domain adaptation in\nSection 3.3, and Ô¨Ånally brief the training usage in\nSection 3.4. The overall framework is illustrated in\nFigure 2.\n56\n3.1 Datastore Construction\nWe consider textual domain corpus as a necessary\nrecipe to adapt a general-domain language model.\nFrom their related sources, we collect domain spe-\ncialized passages {pi}D, and refer to them as the\ndatastore. In practical implementation, we truncate\nthe length of each passage to 256 tokens for sim-\nplicity. We then build an Elasticsearch service‚Ä†, for\nany task-speciÔ¨Åc training or test input sequence x,\nwe can retrieve for its Kmost relevant background\npassages {pk}K\nk=1. Although Elasticsearch only\nserves coarse-grained retrieving purpose, we can\nemploy it as an efÔ¨Åcient Ô¨Årst-stage retriever.\nFor each speciÔ¨Åc downstream task, the datastore\nis constructed at task-level granularity. And the re-\ntrieving of most relevant passages further achieves\ninstance-level granularity. This is in contrast with\nmany existing works where datastore is constructed\nand utilized at domain-level granularity, once a tar-\nget domain is determined, a Ô¨Åxed-scale datastore\nis collected and injected into PLM as an integral\nthrough further pretraining. As a result, RADA\nbrings better Ô¨Çexibility and save the difÔ¨Åculty to\ndeliberate the trade off between domain specialty\nand domain coverage.\n3.2 Retrieval-Augmented PLM\nIn a standard PLM-based framework, we Ô¨Årst take\nthe task input sequence x = {xi}and use the\npretrained encoder to produce contextualized rep-\nresentations:\nHx = encode(x) (1)\nwhere Hx ‚ààRl‚àód is the sequence level representa-\ntion for lwords and ddimensions. We accordingly\nrefer to hx\ni as the contextualized representation at\neach speciÔ¨Åc position i. The representations are\nthen fed into a task-tailored head module, such as\nclassiÔ¨Åcation, regression or sequence tagging, etc.\nIn RADA, accompanied with each task input, we\nsimilarly construct the representations for retrieved\npassages {pk}:\nHpk = encode(pk), 1 ‚â§k‚â§K (2)\nand we similarly get Hpk and hpk\nj . The purpose\nof the proposed RADA framework can be formu-\nlated as a universal interface that augments and\nupdates task representations hx\ni using Hpk, where\n‚Ä†https://github.com/elastic/\nelasticsearch\nHpk serves as informative, relevant and domain-\nspecialized knowledge source:\nÀúhi = RA-Interface(hx\ni ,{Hpk}K\nk=1) (3)\nÀúhi is then unchangeably used in the rest of the\ntask pipeline. The overall framework remains task-\nagnostic and can be readily integrated into both\npretraining and Ô¨Ånetuning stage. The interface can\nalso be easily extended to a more broad scenario\nwith different backbone networks other than Trans-\nformer.\n3.3 Retrieval Augmentation Interface\nWe elaborate the RA-Interface in this section.\nDifferent from many existing methods that resort to\nintegrate structured knowledge (Zhang et al., 2019;\nPeters et al., 2019), the retrieved textual domain\ncorpus are much less knowledge-intensive and thus\nmight contain irrelevant noise distractions. How-\never, Elasticsearch can only satisfy coarse-grained\nretrieving purpose, leaving it an important problem\nto effectively select and integrate actually helpful\ndomain knowledge.\nTo address such challenge, we propose a two-\nlevel selection scheme which performs discrete\nsampling at passage-level and attentively aggregate\nat word-level. Such selection strategy strengthens\nthe proposed interface with Ô¨Åne-grained selection\ncapability to integrate helpful knowledge while Ô¨Ål-\nter out its noisy parts. More speciÔ¨Åcally, we in-\ntroduce a gumbel sampling mechanism to select\nthe most helpful passage, along with an attention\nmechanism to assign distinguishable weights for\neach word insides the selected passage.\n3.3.1 Cross Attention\nIn order to cover both sequence-level and token-\nlevel downstream tasks, we would need to augment\nthe representation Hx at each speciÔ¨Åc position. For\nthe i-th token representation hxi, we calculate its\nrelevancy with respect to all other words from the\nretrieved passages using a canonical cross attention\nmechanism:\nqx\ni =WQhx\ni\nkpk\ni =WKhpk\nj\nvpk\ni =WV hpk\nj\n(4)\nŒ±k\nij =\nqx\ni kpk\nj‚àö\nd\n(5)\n57\nwhere Œ±k\nij denotes the attentive weight of j-th to-\nken from passage pk for current position hx\ni . Intu-\nitively, this attentive weight can naturally measure\neach retrieved word its importance and relevance.\nTo represent the usefulness of passages and their\ntokens at integral sequence level, we simply sum-\nming over all task input positions:\nŒ±k\nj =\n‚àë\ni\nŒ±k\nij (6)\n3.3.2 Differentiable Sampling\nTo alleviate the noise distraction in the retrieved\npassages, we Ô¨Årst sample the most relevant pas-\nsage, then integrate all its words according to re-\nspective attentive weights. Intuitively, the sampling\noperation should be an argmax operation across all\ncandidate passages:\nŒ±k =\n‚àë\nj\nŒ±k\nj (7)\nÀÜk= argmax\nk‚àà{1,2,...,K}\nŒ±k (8)\nNote that here we sum Œ±k\nj up to Œ±k only temporar-\nily for sampling most relevant passage, and will\nre-use Œ±k\nj later in Equation 12 to aggregate repre-\nsentations. The passage level selection operation in\nEquation 8, although intuitive, is not differentiable,\nand are not allowed in an end-to-end trainable neu-\nral network. To enable such a learning-to-select\nability inside the proposed framework, we nov-\nelly introduce the Gumbel-Softmax estimator (Jang\net al., 2017) to replace Equation 8:\nŒ≤k = exp((Œ±k + g)/œÑ)‚àë\nk exp((Œ±k + g)/œÑ) (9)\nwhere gis the noise sampled from Gumbel distri-\nbution:\ng‚àºGumbel(0,1) (10)\nThis is practically implemented as:\ng= ‚àílog(‚àílog(u))\nwhere u‚àºUniform(0,1) (11)\nThe resulting Œ≤distributed over k‚àà{1,2,...,K }\nis an approximate categorical distribution when\nthe temperature œÑ approaches 0. We follow the\nStraight-Through procedure (Jang et al., 2017) to\ndiscretize Œ≤into ÀÜkin the forward pass but use gum-\nbel approximation in the backward pass.\n3.3.3 Aggregation\nFor the selected passage ÀÜk, we apply a post-\nnormalization on the attentive weights:\nÀúŒ±\nÀÜk\nj =\nŒ±ÀÜk\nj\n‚àë\nj Œ±ÀÜk\nj\n,\n‚àë\nj\nÀúŒ±\nÀÜk\nj = 1 (12)\nWe then integrate the representations of passage ÀÜk\naccordingly:\nÀÜh=\n‚àë\nj\nÀúŒ±\nÀÜk\nj v\npÀÜk\nj (13)\nNote that in practical implementation, we use multi-\nhead attentions and concatenate all outputs together.\nWe then wrap ÀÜhwithin a residual connection and\nLayerNorm at each position ito produce the Ô¨Ånal\noutput representation:\nÀúhi = LayerNorm(WÀÜh+ hx\ni ) (14)\nwhere W is another feedforward layer that merges\nrepresentations from multiple attention heads.\n3.4 Retrieval-Augmented Training\nThe proposed RA-Interface serves as a plug-\ngable module that interacts with the representations\nin-place. It is also end-to-end trainable and does\nnot modify the Ô¨Ånal training objective. In practical\nusages, we can either directly apply such a retrieval-\naugmented framework in downstream Ô¨Ånetuning,\nor Ô¨Årst adapt it with a domain-speciÔ¨Åc further pre-\ntraining stage. We investigate both settings in the\nexperiments.\n4 Experiments\n4.1 Setup\nDataset We investigate the proposed framework\non a variety of domain-specialized downstream\ntasks. SpeciÔ¨Åcally, biomedical domain tasks in-\ncluding QIC (Intent ClassiÔ¨Åcation), QQR (Query-\nQuery Relevance), CMeEE (Named Entity Recog-\nnition), CMeIE (Information Extraction), scientiÔ¨Åc\ndomain tasks including SCIERC (Relation Classi-\nÔ¨Åcation) (Luan et al., 2018), ACL-ARC (Citation\nIntent ClassiÔ¨Åcation) (Jurgens et al., 2018) and le-\ngal domain task CAIL2019-SCM (Similar Case\nMatching) (Xiao et al., 2019). And these 4 biomed-\nical tasks are selected from the well established\nbenchmark CBLUE (Zhang et al., 2022). Detailed\nstatistics are listed in Table 4.\n58\nDomain Biomedical Science Legal Avg.\nMethod QIC QQR CMeEE CMeIE SCI ARC SCM\nFT 81.860.07 81.970.81 65.450.07 61.310.20 81.391.03 68.332.05 68.970.21 72.75\nRADA 82.430.21 82.180.74 65.550.18 60.070.49 79.760.48 68.872.86 69.700.46 72.87\nDAPT 81.740.16 82.060.77 65.570.12 61.420.12 81.360.46 73.144.23 69.100.35 73.50\nRADA‚Ä† 82.450.10 82.750.67 66.000.20 61.530.24 82.070.86 75.421.48 70.250.32 74.34\nTable 1: Main Results. FT refers to standard Ô¨Ånetuning. ‚Ä† denotes the RADA equipped with a pretraining stage.\nDomain Biomedical Science Legal Avg.\nMethod QIC QQR CMeEE CMeIE SCI ARC SCM\nFT 81.070.41 80.520.28 63.240.06 53.970.23 70.770.66 56.461.91 61.200.42 66.75\nDAPT 80.810.35 80.620.43 63.560.13 54.290.13 74.890.63 62.150.86 61.310.34 68.23\nRADA 81.160.19 81.060.33 63.620.22 54.480.14 73.960.56 62.592.80 61.700.11 68.36\nTable 2: Results under low resource scenario.\nDomain LANG Size Num. of Passages\nComputer English 6.2GB 4,602,628\nBiomedicineChinese 0.76GB 960,595\nLegal Chinese 3.7GB 2,794,605\nTable 3: Statistics of domain datastore.\nDatasetSCI ARCQIC QQR CMeEE CMeIESCM\nDomain Science Biomedical Legal\nTrain 3219 16886238 13500 13500 12905 5102\nDev 455 144 693 1500 1500 1434 1500\nTest 974 139 1955 1600 5000 3585 1500\nTable 4: Statistics of domain tasks. For the biomedical\ntasks, as we do not have access to the test label, we use\nthe released dev set for test, and split 10% of the train\nset for development.\nDatastore We collect domain specialized corpus\nat scale as datastore. For scientiÔ¨Åc corpus, we\nuse the S2ORC corpus constructed from semantic\nscholar (Lo et al., 2020), for biomedical and legal\ncorpus, we use in-house data regarding medical re-\nviews or legal documents crawled online. Detailed\nstatistics are listed in Table 3.\nTraining For retrieval-augmented pretraining,\nwe split 10% from the entire domain corpus as\npretraining pretext task data while the rest 90% as\npretraining datastore. The pretraining steps are set\nto 10k for both DAPT and RADA. For retrieval-\naugmented Ô¨Ånetuning, all corpus are considered as\ndatastore, but we only retrieval and keep the most\nrelevant passages for each task instances. In Elas-\nticSearch, we set the number of retrieved passages\nfor each input data as K = 10. For each task, we\nsearch batch size through {16,32}, learning rate\nthrough {1e‚àí5,2e‚àí5,5e‚àí5}‚Ä†, and set epoch\nto 10‚Ä†. We also run with 3 different random seeds,\nand accordingly report the average and standard\ndeviation. We use BERT (Devlin et al., 2019) as\nPLM for English tasks, and RoBERTa (Cui et al.,\n2020; Liu et al., 2019) for Chinese ones.\n4.2 Main Results\nTable 1 gives the main results for RADA. We can\nobserve two conclusions: 1) RADA outperforms\nstandard Ô¨Ånetuning on 4 out of 7 selected tasks;\n2) When combined with further pretraining, the\nproposed framework is further improved, signiÔ¨Å-\ncantly surpassing standard Ô¨Ånetuning and DAPT.\nSpeciÔ¨Åcally the absolute beneÔ¨Åts over FT baseline\nare +1.59.\nWe further investigate a low-resource setting by\nsubsampling 30% of the training set. This is a very\npractical scenario as we often need to deal with\ndomain tasks at a relatively low annotation cost.\nResults in Table 2 also demonstrate the effective-\nness of the proposed framework in such settings.\nOn 6 out of 7 tasks, RADA achieves the best per-\nformance.\n4.3 Ablations\nWe investigate the impact of various components\nin this section.\n‚Ä†For CMeIE, we search for {1e-5, 2e-5}.\n‚Ä†For CMeIE and CMeEE, we set to 35 and 15 respectively.\n59\nDomain Biomedical Science Legal Avg.\nMethod QIC QQR CMeEE CMeIE SCI ARC SCM\nFT 81.860.07 81.970.81 65.450.07 61.310.20 81.391.03 68.332.05 68.970.21 72.75\nRADA w/ Top 1 82.310.25 82.560.43 66.080.26 61.480.05 81.780.25 74.150.96 69.140.26 73.93\nRADA w/ DS 82.450.10 82.750.67 66.000.20 61.530.24 82.070.86 75.421.48 70.250.32 74.34\nTable 5: Effects of Differentiable Sampling (DS). Top 1 means we simply select the passage according to their\nsummed attention score.\nMethod ARC QIC QQR\nFT 68.33 2.05 81.860.07 81.970.81\nTAPT 72.432.36 82.140.04 82.710.57\nRADA w/ Trainset 70.65 0.38 82.210.21 82.520.46\nRADA (Full) 75.42 1.48 82.450.10 82.750.67\nTable 6: Results of using training data as datastore cor-\npus. Full means extra domain corpus are used.\n4.3.1 Scale of Datastore Corpus\nOne essential component of the proposed frame-\nwork is the datastore. We Ô¨Årst look into the effects\nof its scale. At pretraining stage, we Ô¨Åx the pretrain-\ning steps to 5,000, and accordingly set the scale\nof datastore to 128, 256, 512 and 1024. The re-\nsults are illustrated in Figure 3. We observe clear\ntrends of increasing performance w.r.t. increased\ndatastore scale. As a consequence, the proposed\nRADA framework can always beneÔ¨Åt from a larger\ndatastore.\n4.3.2 Training Data as Datastore\nWe further investigate the feasibility of directly\nusing downstream task data as datastore corpus.\nAs previous study has demonstrated, training data\nitself can also provide useful background informa-\ntion to an extent (Wang et al., 2022). Similarly, Gu-\nrurangan et al. (2020) have also used task training\ndata to perform further pretraining, which they re-\nferred to as TAPT. In Table 6, we provide the results\nwhen using training data in the proposed retrieval-\naugmented framework, and also reproduce TAPT\nfor more comprehensive comparison. The results\nshow that both TAPT and RADA with Trainset\ncan provide considerable beneÔ¨Åts, but are outper-\nformed by RADA (full). As for these two methods,\nthey perform comparably on the investigated three\ndatasets. We also Ô¨Ånd that training data is more use-\nful in sentence-level tasks (as included in the table),\nbut less helpful in other tasks such as classiÔ¨Åcation,\nsequence tagging, etc.\nFigure 3: Ablation on scale of corpus.\nFigure 4: Ablation on pretraining steps.\n4.3.3 Pretraining Steps\nWe have equipped RADA with a further pretraining\nstage with domain specialized data, and proved it to\nbe very effective in Table 1. In Figure 4, we further\nablate the effects of pretraining steps. We save and\nevaluate the intermediate checkpoint at respectively\n2.5k, 5k and 7.5k pretraining steps. The results ex-\nhibit clear trends that with more pretraining steps,\nRADA continually brings better adaptation perfor-\nmance.\n4.3.4 Differentiable Sampling\nOne key design of RADA is the two-level selection\nscheme. SpeciÔ¨Åcally, the differentiable sampling\nmodule based on Gumbel Sampling trick enables\nthe model to dynamically learn which passage to\nintegrate. In Table 5 we investigate this choice\nof design. We provide an alternative baseline,\nwhere instead of learning to sample, we simply\nsum the cross attention weights over all positions\nas passage-level score, and select the maximum one\nto integrate into the retrieval-augmented interface.\nWe refer to this as Top 1 selection strategy. The\nresults show that both methods can bring improve-\n60\n‚Ä¢InputTe x t :This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms, such as rewriting systems, dependency grammars , TAG , HPSG and LFG .‚Ä¢RelationLabel:HYPONYM-OF SamplingScoreùõΩ!\nRetrievedPassages(ESTo p10)\n1‚Ä¶Although we have presented supertaggingin the context of LTAG, it is applicable to other lexicalized grammar formalismssuch as CCG (Steedrnan1997), HPSG(Pollard and Sag 1987) , and LFG(Kaplan and Bresnan 1983)‚Ä¶ 0.0992\n2Scrambling in German poses a problem for most grammar formalisms. Neither Tree Adjoining Grammar (TAG, Joshi et al., 1975) nor even linear context-free rewriting systems(LCFRS, Weir, 1988) are powerful enough to deal with scrambling and the free word order in German (see Becker et al., 1992)‚Ä¶ 0.1737\n3‚Ä¶We would also expect that dependency grammarsMel‚Äôcukand Pertsov1987; Hudson 1984) and parsers (McDonald, Crammer, and Pereira 2005 ) could be trained and tested with little extra work on the dependenciesin CCGbank. Finally, we believe that existing methods for translating the Penn Treebank from scratch into other grammar formalisms will benefit from‚Ä¶ 0.1046\n‚Ä¶ ‚Ä¶ ‚Ä¶\n10‚Ä¶The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepenet al., 2014 (Oepenet al., , 2015 include corpora annotated with the PDT-TL, and dependenciesextracted from the HPSGgrammars Enju(Miyao, 2006) and the LinGOEnglish Reference Grammar(ERG; Flickinger, 2002) . Like the PDT-TL, projects based on CCG, HPSG, and other expressive grammarssuch as LTAG (Joshi and Vijay-Shanker, 1999) and LFG‚Ä¶ 0.1018\nFigure 5: Case study. Illustrated are top 10 retrieved passages using elasticsearch, and their sampling score produce\ninside the RA-Interface. Scores are re-normalized for better illustration.\nw/o RA w/ RA\nEfÔ¨Åciency 0.0083 sec/instance 0.0201 sec/instance\nTimes 2.4√ó 1.0√ó\nTable 7: Inference efÔ¨Åciency. Measured using a single\nRTX TITAN GPU.\nments, but the proposed differentiable sampling is\nmuch more effective.\n4.3.5 EfÔ¨Åciency Analysis\nOne potential concern for retrieval-augmented\nmethods is their efÔ¨Åciency. We therefore inves-\ntigate this factor in Table 7. We consider the infer-\nence efÔ¨Åciency at deployment time. For each task\ninput, we retrieve 10 background passages using\nElasticSearch, then compute and incorporate them\ninto the RA-Interface. Note that at deploy-\nment time, it is practical to encode and cache all\npassages from the datastore in advance, so we only\naccount for the time consumption starting from\nthe interface. The results show that, the retrieval-\naugmentation framework only brings acceptable\ntime increase. And the overall inference speed is\nmaintained at around 0.02 seconds per instance on\na production-level device.\n4.3.6 Qualitative Analysis\nWe provide qualitative analysis in Table 5. The\nexample is sampled from the SCI task, the target\nis to extract the relation between subject rewrit-\ning systemsand object grammar formalisms, i.e.,\nHYPONYM-OF. We can clearly see that elastic-\nsearch can only provide coarse-grained, shallow-\nsemantic retrieving capability based on keywords,\nsuch as grammar, formalisms, HPSG, etc.However,\nthe proposed differentiable sampling module can\nmore effectively identify the most helpful passage\nby reasoning over deep representations produced\nby shared encoders. From passage 2 with the high-\nest sampling score, we can indeed reason, under-\nstand and accordingly infer the actual relationship\nbetween the target entity pair.\n5 Conclusion\nIn this paper we propose a retrieval-augmented\nframework to novelly address the challenge of lan-\nguage model domain adaptation. We use domain\nspecialized corpus as datastore and retrieve from\nit for informative and helpful domain knowledge.\nThe key module of the framework is a retrieval-\naugmentation interface, where we design a two-\nlevel selection scheme to integrate the most rele-\nvant passage and its words while alleviating the\nnoise. The overall framework enables Ô¨Çexible do-\nmain coverage and Ô¨Åne-grained domain knowledge\nintegration. On a variety of downstream domains\nand tasks, we conduct comprehensive experiments\nand comparisons to demonstrate the effectiveness\nof the motivated framework and its components.\nIn the future, we hope to further extend the frame-\nwork to more scaled large language models and\nalso more challenging few-shot prompting setting.\n61\nLimitation\nWe summarize two limitations which also serve as\npromising directions to be explored in our future\nwork. RADA framework only considers textual\ndomain corpus as the datastore, although this has\ngreatly improve the coverage of domain knowl-\nedge as texts are always relatively easy to collect.\nHowever, it is widely investigated that structured\nknowledge such as knowledge base can also serve\nsimilar purpose. And such resources are generally\nin higher quality and are easier to match. There-\nfore, it would be beneÔ¨Åting to further integrate such\nresources at certain scenario where KB is available.\nThe other limitation regards to the scale of the\nRADA implementation. As large language models\nhave becoming increasingly powerful, they have\ndemonstrated quite impressive capability in mem-\norizing and recalling a wide range of background\nknowledge existed in the massive corpora they have\nbeen pretrained on. This trends of development nat-\nurally raises question for the proposed framework:\nwill it still be beneÔ¨Åcial when scale up to LLMs?\nand on what kinds of scenario does it brings best\nimprovements? These are very important questions\nto answer, and we can certainly expect them to be\nexplored in future works.\nEthical Statement\nWe evaluate the proposed method on established\nand publicly available datasets. There is also no\nhuman evaluation involved. This paper is not con-\ncerned with the above ethical risks. When the pro-\nposed framework is deployed into domain speciÔ¨Åc\nproduction, the domain adapted language models\nmight express ethical-related outputs, but just as\nany other language models do (Weidinger et al.,\n2021), and should be treated with according tech-\nniques to eliminate ethical risks such as bias, stereo-\ntypes.\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clini-\ncal BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop,\npages 72‚Äì78, Minneapolis, Minnesota, USA. Associ-\nation for Computational Linguistics.\nFan Bai, Alan Ritter, and Wei Xu. 2021. Pre-train\nor annotate? domain adaptation with a constrained\nbudget. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 5002‚Äì5015, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiÔ¨Åc\ntext. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 3615‚Äì3620, Hong Kong, China. Association\nfor Computational Linguistics.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Ro-\nman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals, Si-\nmon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving lan-\nguage models by retrieving from trillions of tokens.\nIn Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 2206‚Äì2240.\nPMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Advances\nin Neural Information Processing Systems, vol-\nume 33, pages 1877‚Äì1901. Curran Associates, Inc.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out\nof law school. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pages\n2898‚Äì2904, Online. Association for Computational\nLinguistics.\nAlexandra Chronopoulou, Matthew Peters, and Jesse\nDodge. 2022. EfÔ¨Åcient hierarchical domain adapta-\ntion for pretrained language models. In Proceedings\nof the 2022 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1336‚Äì1351, Seattle, United States. Association for\nComputational Linguistics.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin\nWang, and Guoping Hu. 2020. Revisiting pre-trained\nmodels for Chinese natural language processing. In\nProceedings of the 2020 Conference on Empirical\n62\nMethods in Natural Language Processing: Findings,\npages 657‚Äì668, Online. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171‚Äì4186, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nXu Guo and Han Yu. 2022. On the domain adaptation\nand generalization of pretrained language models: A\nsurvey. arXiv preprint arXiv:2211.03154.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2022. DEMix\nlayers: Disentangling domains for modular language\nmodeling. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5557‚Äì5576, Seattle, United\nStates. Association for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ¬¥c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don‚Äôt stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n8342‚Äì8360, Online. Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong\nPasupat, and Ming-Wei Chang. 2020. Realm:\nRetrieval-augmented language model pre-training.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML‚Äô20. JMLR.org.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nEric Jang, Shixiang Gu, and Ben Poole. 2017.\nCategorical reparameterization with gumbel-\nsoftmax. In International Conference on Learning\nRepresentations.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientiÔ¨Åc Ô¨Åeld through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391‚Äì406.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234‚Äì1240.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics, pages 6086‚Äì6096, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt√§schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural\nInformation Processing Systems, volume 33, pages\n9459‚Äì9474. Curran Associates, Inc.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A Smith, and Luke Zettle-\nmoyer. 2022. Branch-train-merge: Embarrassingly\nparallel training of expert language models. arXiv\npreprint arXiv:2208.03306.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-bert: En-\nabling language representation with knowledge graph.\nProceedings of the AAAI Conference on ArtiÔ¨Åcial\nIntelligence, 34(03):2901‚Äì2908.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The seman-\ntic scholar open research corpus. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4969‚Äì4983, On-\nline. Association for Computational Linguistics.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiÔ¨Åcation of enti-\nties, relations, and coreference for scientiÔ¨Åc knowl-\nedge graph construction. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3219‚Äì3232, Brussels,\nBelgium. Association for Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Robert Logan,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 43‚Äì54, Hong Kong, China.\nAssociation for Computational Linguistics.\n63\nAlan Ramponi and Barbara Plank. 2020. Neural un-\nsupervised domain adaptation in NLP‚ÄîA survey.\nIn Proceedings of the 28th International Conference\non Computational Linguistics, pages 6838‚Äì6855,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nMachel Reid, Victor Zhong, Suchin Gururangan, and\nLuke Zettlemoyer. 2022. M2d2: A massively multi-\ndomain language modeling dataset. arXiv preprint\narXiv:2210.07370.\nShuohang Wang, Yichong Xu, Yuwei Fang, Yang\nLiu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and\nMichael Zeng. 2022. Training data is more valu-\nable than you think: A simple and effective method\nby retrieving from training data. In Proceedings\nof the 60th Annual Meeting of the Association\nfor Computational Linguistics (V olume 1: Long\nPapers), pages 3170‚Äì3179, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifÔ¨Ån, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nChaojun Xiao, Haoxi Zhong, Zhipeng Guo, Cunchao\nTu, Zhiyuan Liu, Maosong Sun, Tianyang Zhang,\nXianpei Han, Zhen Hu, Heng Wang, et al. 2019.\nCail2019-scm: A dataset of similar case matching in\nlegal domain. arXiv preprint arXiv:1911.08962.\nHaoran Xu, Seth Ebner, Mahsa Yarmohammadi,\nAaron Steven White, Benjamin Van Durme, and\nKenton Murray. 2021. Gradual Ô¨Åne-tuning for low-\nresource domain adaptation. In Proceedings of the\nSecond Workshop on Domain Adaptation for NLP,\npages 214‚Äì221, Kyiv, Ukraine. Association for Com-\nputational Linguistics.\nTiezheng Yu, Zihan Liu, and Pascale Fung. 2021.\nAdaptSum: Towards low-resource domain adapta-\ntion for abstractive summarization. InProceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 5892‚Äì5904,\nOnline. Association for Computational Linguistics.\nNingyu Zhang, Mosha Chen, Zhen Bi, Xiaozhuan\nLiang, Lei Li, Xin Shang, Kangping Yin, Chuanqi\nTan, Jian Xu, Fei Huang, Luo Si, Yuan Ni, Guo-\ntong Xie, Zhifang Sui, Baobao Chang, Hui Zong,\nZheng Yuan, Linfeng Li, Jun Yan, Hongying Zan,\nKunli Zhang, Buzhou Tang, and Qingcai Chen.\n2022. CBLUE: A Chinese biomedical language un-\nderstanding evaluation benchmark. In Proceedings\nof the 60th Annual Meeting of the Association\nfor Computational Linguistics (V olume 1: Long\nPapers), pages 7888‚Äì7915, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 1441‚Äì1451, Florence, Italy. Association for\nComputational Linguistics.\n64",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8972488641738892
    },
    {
      "name": "Granularity",
      "score": 0.7550292611122131
    },
    {
      "name": "Construct (python library)",
      "score": 0.6600151062011719
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6544320583343506
    },
    {
      "name": "Domain adaptation",
      "score": 0.5681193470954895
    },
    {
      "name": "Language model",
      "score": 0.5419642925262451
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5282670259475708
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5108789801597595
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5105794072151184
    },
    {
      "name": "Natural language processing",
      "score": 0.4614497423171997
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4274018704891205
    },
    {
      "name": "Task (project management)",
      "score": 0.4229947626590729
    },
    {
      "name": "Information retrieval",
      "score": 0.3851465582847595
    },
    {
      "name": "Programming language",
      "score": 0.08721476793289185
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    }
  ],
  "cited_by": 3
}