{
  "title": "EBAT: Enhanced Bidirectional and Autoregressive Transformers for Removing Hairs in Hairy Dermoscopic Images",
  "url": "https://openalex.org/W4319878714",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5081320657",
      "name": "Youngchan Lee",
      "affiliations": [
        "Sun Moon University"
      ]
    },
    {
      "id": "https://openalex.org/A5011073269",
      "name": "Wonsang You",
      "affiliations": [
        "Sun Moon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281783336",
    "https://openalex.org/W2895214798",
    "https://openalex.org/W3115901151",
    "https://openalex.org/W2624950617",
    "https://openalex.org/W1991607872",
    "https://openalex.org/W2108900923",
    "https://openalex.org/W2124113591",
    "https://openalex.org/W1987207115",
    "https://openalex.org/W2170969730",
    "https://openalex.org/W2033075286",
    "https://openalex.org/W136214390",
    "https://openalex.org/W567006303",
    "https://openalex.org/W2315986677",
    "https://openalex.org/W2804405863",
    "https://openalex.org/W2759590086",
    "https://openalex.org/W3012192396",
    "https://openalex.org/W2586435665",
    "https://openalex.org/W2963270367",
    "https://openalex.org/W6755053353",
    "https://openalex.org/W2798365772",
    "https://openalex.org/W2982763192",
    "https://openalex.org/W2985764327",
    "https://openalex.org/W2992838650",
    "https://openalex.org/W4220791756",
    "https://openalex.org/W3158892156",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3206082266",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W6779669310",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6755808572",
    "https://openalex.org/W6745560452",
    "https://openalex.org/W3121732873",
    "https://openalex.org/W2070604790",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W3105938520",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2896194744",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4294643831"
  ],
  "abstract": "A great progress in deep learning technologies for skin cancer detection from dermoscopic images has been made for a decade. While its performance is vulnerable to a large amount of hairs densely covering the skin surface, the existing image processing methods frequently fail to remove hairs in hairy skin images. In this paper, we propose, as a deep learning approach to removing hairs, a generative image inpainting network where bidirectional autoregressive transformers (BATs) are employed to learn image features and are systematically integrated with convolutional neural networks (CNNs) in multiple spatial scales in order to reconstruct missing regions. Each patch split from a masked image is unfolded and processed through BATs, and re-folded to constitute diverse shapes of feature maps through kernel-based unfolding-folding operations. By introducing the multi-scale features extracted by collaborative learning of transformers and CNNs to the texture generator network, our method can effectively reconstruct minute details of local regions as well as global structure which might not be easily inferred from neighbor pixels in hairy skin images. Quantitative and qualitative evaluations show not only that our multi-scale dual-modality strategy is much robust to reconstruct hair-shaped missing regions compared to the existing transformer-based image inpainting method called BAT-Fill, but also that our framework outperforms the state-of-the-art image inpainting models in removing hairs from hairy dermoscopic images.",
  "full_text": "Received 1 January 2023, accepted 25 January 2023, date of publication 10 February 2023, date of current version 15 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.324391 1\nEBAT: Enhanced Bidirectional and Autoregressive\nTransformers for Removing Hairs in Hairy\nDermoscopic Images\nYOUNGCHAN LEE AND WONSANG YOU\n, (Member, IEEE)\nArtificial Intelligence and Image Processing Laboratory, Department of Information and Communication Engineering, Sun Moon University, Asan-si 31460,\nSouth Korea\nCorresponding author: Wonsang You (wyou@kaist.ac.kr)\nThis work was supported in part by the Support Program for Local Specialized Industry Promotion Business under Grant S3091196; in part\nby the Start-Up Growth Technology Development Project funded by the Ministry of SMEs and Startups (SME: Small and Medium-sized\nEnterprise) under Grant S3228660; in part by the Basic Science Research Program under Grant NRF-2022R1F1A1075204; in part by the\nBK21 FOUR (Fostering Outstanding Universities for Research); and in part by the Regional Innovation Strategy Project under Grant\n2021RIS-004 through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, South Korea.\nABSTRACT A great progress in deep learning technologies for skin cancer detection from dermoscopic\nimages has been made for a decade. While its performance is vulnerable to a large amount of hairs densely\ncovering the skin surface, the existing image processing methods frequently fail to remove hairs in hairy\nskin images. In this paper, we propose, as a deep learning approach to removing hairs, a generative image\ninpainting network where bidirectional autoregressive transformers (BATs) are employed to learn image\nfeatures and are systematically integrated with convolutional neural networks (CNNs) in multiple spatial\nscales in order to reconstruct missing regions. Each patch split from a masked image is unfolded and\nprocessed through BATs, and re-folded to constitute diverse shapes of feature maps through kernel-based\nunfolding-folding operations. By introducing the multi-scale features extracted by collaborative learning of\ntransformers and CNNs to the texture generator network, our method can effectively reconstruct minute\ndetails of local regions as well as global structure which might not be easily inferred from neighbor\npixels in hairy skin images. Quantitative and qualitative evaluations show not only that our multi-scale\ndual-modality strategy is much robust to reconstruct hair-shaped missing regions compared to the existing\ntransformer-based image inpainting method called BAT-Fill, but also that our framework outperforms the\nstate-of-the-art image inpainting models in removing hairs from hairy dermoscopic images.\nINDEX TERMS Hair removal, skin image, image inpainting, transformer, deep learning, generative\nadversarial networks.\nI. INTRODUCTION\nArtificial intelligence (AI) has fast leaped forward as assistive\nhealthcare technologies for medical doctors and patients,\nand it has been also applied for detecting a skin cancer\nfrom dermoscopic images with remarkable performance [1].\nOne of artifacts in skin image analyses is hairs which cover\neither widely or locally over the surface of skins [2]. Hairs\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Mingbo Zhao\n.\nmay seriously perturb analyzing lesion textures and result\nin reduced accuracy of a deep learning model predicting the\ncancer type corresponding to the skin lesion. Accordingly, the\npreprocessing stage of removing hairs from a dermoscopic\nimage is essential in the deep learning framework for skin\ncancer detection [3].\nThe hair removal in dermoscopic images can be understood\nas the problem of image inpainting that aims to reconstruct\nan incomplete image by filling missing parts naturally [4].\nTwo major image inpainting approaches to hair removal are\nVOLUME 11, 2023 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 14225\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\npresent; one is based on image processing, and the other is\nbased on machine learning.\nIn the image processing approach to hair removal, the\nDullrazor software was a pioneering software, introduced by\nLee et al., by which hair regions are detected from a grayscale\nskin image using morphological filtering and are restored\nthrough bilinear interpolation with neighbor pixels [5].\nE-shaver is its extension toward color skin images where edge\nfiltering and color averaging are employed for hair detection\nand inpainting respectively [6]. Those early algorithms for\nremoving hairs had been advanced using a diversity of\nimage inpainting techniques including partial differential\nequation and coherence transport [7], [8], [9], [10], [11],\n[12], [13], [14], [15], [16]. In particular, the fast marching\nmethod (FMM), where a skin region hidden by hairs is\nfilled progressively from its border, is one of the robust\nmethods for removing hairs [7], [17]. Since most skin image\ndiagnostic technologies have targeted human skin images\nwith few hairs, the image inpainting methods based on image\nprocessing have exhibited quite acceptable performance on\nsuch tractable human skin images despite the simplicity of\ntheir algorithms. However, their performance had not been\nverified for hairy skin images whose skin surface is covered\nwith lots of hairs in either quantitative or qualitative manner.\nOn the other hand, the machine learning approach to hair\nremoval had little advance compared to the image processing\napproach. It is obviously contrasted with the fast-growing\ntrend of deep learning models for image inpainting includ-\ning Shift-Net, DeepFill, GMCNN, PartialConv, LBAM\n[18], [19], [20], [21], [22], [23]. One practical reason for the\nunderdevelopment of machine learning-based hair removal\nmethods would be that an advanced hair removal algorithm\nwith large computational complexity was not necessary in\na moderate skin image with few hairs. The other reason\nis the difficulty in finding such a training dataset that\nconsists of pairs of original skin image with hairs and\nthe corresponding hairless image. To tackle the problem,\nTalavera-Martinez et al. first employed a convolutional neural\nnetwork model with simple encoder-decoder architecture to\nremove hairs in a skin image [3]. They exploited, as a training\ndataset, pairs of hairless skin images extracted from public\ndatasets as ground truth and their corresponding images with\nsimulated hairs as input data. Bardou et al. examined a\nvariational autoencoder model, where hairs are eliminated as\nnoise, that consequently can be trained without such a dataset\nas consisting of pairs of hairy and hairless skin images [24].\nLi et al. used DeepFill along with gated convolutions to allow\nfree-form image inpainting optimized to hair shapes [25].\nDespite the recent noteworthy advance in methodologies\nfor hair removal, the existing methods tend to be vulnerable\nto such a dermoscopic image in which hairs are densely\njammed and tangled, as illustrated in Section IV. The existing\nmethods based on either image processing or convolutional\nneural networks remove hairs from a skin image by filling a\nhair region progressively from its border using the properties\nof local skin texture [7], [25]. However, in such an extreme\nsituation as hairy skin images the surrounding neighbor pixels\nof a hair region are prone to severe contamination by other\nhairs covering the skin, which might consequently lead to\nthe reduced performance of reconstructing the skin texture\nhidden behind hairs.\nThe transformer can be taken into account as a solution\nto cope with such technical limitations that may be faced in\nhairy skin images. It was first introduced by Vaswani et al.\nin 2017 as a machine translation model that can learn the\nmeaning of a sentence through the attention mechanism\nwhich quantifies the contextual intra-relation of words in\na sentence [26]. The transformer has been successfully\nemployed mainly for machine translation, and has increas-\ningly applied for a wide range of computer vision problems\nincluding image inpainting [27].\nAn image inpainting model based on bidirectional and\nautoregressive transformers (BATs), so called BAT-Fill, was\nrecently introduced especially to reinforce the capability of\ngenerating diverse contents of missing region [28]. It has a\ncoarse-to-fine network architecture that is composed of the\ncoarse-structure generator based on transformers and the fine\ntexture generator based on generative adversarial network\n(GAN) [29]. We assessed the applicability of BAT-Fill, the\ntransformer-based image inpainting model, to removing hairs\nfrom dermoscopic images. As shown in Section IV, we found\nthat, although its qualitative performance in removing hairs\nwas superior to representative existing methods, fragments\nof hairs often remain incompletely eliminated in visual\ninspection and as a consequence the reconstructed skin\nimages used not to be so much as acceptable for clinical use.\nIn this study, we propose a multi-scale GAN framework,\ncalled EBAT (named in the sense of an enhanced network\nof BAT-Fill), where the image features encoded in multiple\nspatial scales by both transformers and convolutional neural\nnetwork (CNN) are jointly learnt to reconstruct fine skin\ntextures as well as global structure. While in BAT-Fill\nthe transformers produce a single low resolution image\nof coarse structure reconstructed from the corresponding\ndown-sampled input image, the transformers in EBAT gen-\nerate a set of multi-scale feature maps through kernel-based\nunfolding-folding operations instead of down-sampling.\nThe main contributions and novelties of this work can be\nsummarized as follows. First, the transformers are used to\nextract not coarse and diverse structures but multi-scale image\nfeatures with long-range dependency, which can enhance\nthe capability of reconstructing fine details and global\nstructure simultaneously. It is through patch-wise unfolding\nand downsized folding operations that a feature map as\nan output of BATs can be generated to have an arbitrary\nshape. Second, the multi-scale feature extractors based on\ntransformers and CNNs are unified and attached to the\nfine texture generator through multi-scale pathways, which\nallows collaborative learning with efficient information flows\nbetween two backbones. Third, both multi-scale feature\n14226 VOLUME 11, 2023\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nextractor and fine texture generator are jointly trained in\nan end-to-end fashion while in BAT-Fill the coarse-structure\ngenerator is completely separated from the fine texture\ngenerator in the process of training. Lastly, the inference\ntime was greatly reduced, compared to BAT-Fill, through\nbypassing the long-winded procedure of diverse contents\ngeneration involving millions of pixel-wise operations on\nCPUs.\nusing image patches instead of a down-sampled image as\nan input to transformers.\nII. RELATED WORKS\nA. CNN/GAN-BASED IMAGE INPAINTING\nIn this section, we review two representative deep learning\nmodels for image inpainting; one is Shift-Net as a CNN-\nbased framework, and the other is DeepFill as a generative\nadversarial network (GAN) based framework [18], [21].\nShift-Net is an extension of the U-net architecture where\nthe encoder features of the known region are shifted to the\ndecoder through a shift-connection layer and are employed\nto estimate the missing regions accurately [18]. The network\nis trained to minimize the guidance loss which is defined\nas the discrepancy between the predicted feature and the\nground-truth features of the missing regions.\nOn the other hand, DeepFill is a generative image inpaint-\ning framework where gated convolution and SN-PatchGAN\nare applied to enable using free-form masks with arbitrary\nshapes [21]. Unlike either vanila convolution or partial\nconvolution which uses hard-mask gating, the gated con-\nvolution lets free-form masks to be softly updated over\nlayers [20], [21]. SN-PatchGAN is a variant of vanilla GAN\nwhere the hinge loss of the discriminator is computed not\non a single output value (real or fake) but on all points\nof the output map, that enables the GAN framework to\nuse free-form masks. It makes a decisive difference with\nvanilla GAN that is designed based on a single rectan-\ngular mask. It also includes contextual attention modules\nto detect long-range dependencies between distant local\nregions.\nB. BAT-FILL: TRANSFORMER-BASED IMAGE INPAINTING\nAs illustrated in Figure 1, BAT-Fill, a transformer-based\nimage inpainting method, is composed of a diverse structure\ngenerator followed by a texture generator, which aims\nto achieve both diversity and accuracy [28]. A sequence\nof bidirectional autoregressive transformers (BATs) are\nused to reconstruct coarse but diverse structures from a\ndown-sampled masked image.\nThe transformers fill the missing regions pixel by pixel\nin an autoregressive manner, which in turn faciliates the\ndiversity of image generation. In addition to the autoregres-\nsive modeling, masked language modeling was also adopted,\nsimilar to BERT, so that it refers to bidirectional contextual\ndependency to better predict missing regions especially\nwhich have arbitrary shape and surrounding background of\nrough texture [30].\nFIGURE 1. The architecture of BAT-Fill as a transformer-based image\ninpainting method.A masked image is down-sampled and flattened as a\nword sequence including mask tokens. Missing regions corresponding to\nmask tokens are filled by the bidirectional autoregressive transformers\n(BATs), and reshaped into the down-sampled image size. The masked\nimage also passes through a convolutional network; its output feature\nmap is concatenated to the low resolution image reconstructed by BATs,\nand fed to the fine texture generator as a refinement network.\nOn the other hand, the texture generator, as the refinement\nnetwork following the diverse structure generator, is based\non generative adversarial learning to regenerate fine-grained\ndetails of image texture up-sampled from the low-resolution\nimage which is reconstructed by the diverse structure\ngenerator. It also takes advantage of unstained pixels of\nthe input image whose features are encoded by a separate\nencoder whose architecture resembles the contracting path\nof U-net [31]. The encoded feature maps are concatenated\nwith the reconstructed low resolution image from BATs, and\nthey are jointly fed to the decoding path of the texture gen-\nerator to be up-sampled in stages through spatially-adaptive\nnormalization and gated convolutions [21], [32]. The texture\ngenerator is completely separated from the diverse structure\ngenerator in its training process.\nIII. PROPOSED METHODS\nIn the proposed GAN framework, the generator has an\nencoder-decoder network architecture that is composed of\ntwo major parts: a multi-scale feature extractor and a fine\ntexture generator. As illustrated in Figure 2, it could be\nseen as a similar architecture as U-net but including dual\nencoding backbones [31]. The multi-scale feature extractor\nas the encoding part consists of both transformer and CNN\nbackbones that generate the multi-scale feature maps in\nwhich missing regions are filled throughout low and high\nresolutions. The fine texture generator as the decoding part\nintegrates both multi-scale feature maps from the transformer\nand CNN backbones to reconstruct the fine-grained textures\nof missing regions through the up-sampling pathways where\nthe context information of lower resolution feature maps are\nVOLUME 11, 2023 14227\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nFIGURE 2. The proposed image inpainting network for removing hairs from hairy dermoscopic images consisting of a multi-scale\nfeature extractor and a fine texture generator.In the transformer backbone of the multi-scale feature extractor, a masked image is split\ninto patches, and each patch is unfolded into a masked sequence including mask tokens denoted as [M], fed to BATs, and unfolded to have\nthe equivalent or down-sized shape by 2−1, 2−2, and 2−3 times. The masked image also passes through the CNN backbone and its features\nare encoded in multiple spatial scales. The multi-scale feature maps from both backbones are concatenated to the fine texture generator\nto reconstruct fine details of missing regions. The⊕ and ⊗ symbols represent element-wise addition and multiplication respectively.\npropagated to higher resolution layers. The discriminator, the\nother core element of the GAN framework, has the identical\nstructure to the one in BAT-Fill.\nA. MULTI-SCALE FEATURE EXTRACTOR\nThe multi-scale feature extractor is composed of a trans-\nformer backbone and a CNN backbone. The transformer\nbackbone consists of a sequence of BATs in the common\nmanner as the diverse structure generator in BAT-Fill.\nHowever, it aims to produce the multi-scale feature maps of\nthe input image whose missing regions are filled taking the\nlong-range dependency between distant regions into account\nand are readjusted to multiple spatial scales through the\noperations of pair-wise unfolding and down-sized folding,\nwhile the diverse structure generator in BAT-Fill aims\nto reconstruct the coarse and diverse structure of the\ndown-sampled input image.\n1) TRANSFORMER BACKBONE\nTo permit the generation of multi-scale feature maps, an input\nmasked image of 256 × 256 is not down-sampled but split\ninto 1024 non-overlapping patches of 8 ×8 size. Each patch is\nflattened into a sequence of length 192 = 8×8×3 with mask\ntokens corresponding to a missing region. The patch-wise\nreshaped input image of 192 ×1024 size is processed through\nBATs without position embedding [28]. The output feature\nvector corresponding to each patch is reshaped into the same\nsize of 8 × 8 as the input patch. However, as illustrated in\nFigure 3, the reconstructed output patches are not located in\ntheir original spatial positions but folded through a sliding\nkernel, and are consequently merged by summing all spatially\noverlapping values among blocks. The output shape resulted\nfrom kernel-based folding is determined by three parameters\nincluding kernel size in a spatial dimension (k ), striding (s),\npadding (p), and dilation (d ) as follows\nY = s(L − 1) + d(k − 1) − 2p + 1 (1)\nwhere Y and L denote the output size and the total number of\npatches respectively. Supposing that the 8 × 8 patches split\nfrom an input image of 256 × 256 size are folded using an\n8×8 kernel parameterized with stride of 4, padding of 2, and\ndilation of 1, the folded feature map has the down-sized shape\nof 128 × 128 based on the equation Y = 4 × (32 − 1) + 1 ×\n(8 − 1) − 2 × 2 + 1.\nWhile the conventional folding operation for vision\ntransformers has been designed to reconstruct the output\nfeature map with the same shape as the input image has, the\nfolding operation in the proposed method is more flexible so\nas to produce the different size of output feature map. The\nshape of the merged feature map is determined depending\non the settings of the sliding kernel; in other words, denser\nstriding between blocks leads to larger overlapping areas and\n14228 VOLUME 11, 2023\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nFIGURE 3. Pair-wise unfolding and down-sized folding operations for\nthe transformer backbone.A masked image of 256× 256 size is split into\n1,024 patches of 8× 8 size. Each patch is unfolded into a sequence of\nlength 192= 8 × 8 × 3 including mask tokens. After being processed\nthrough BATs, the output feature vectors are folded to build an equivalent\nor down-sized shape by controlling the parameters of stride (s), padding\n(p), and dilation (d).\nin turn a reduced size of the resulting output feature map.\nThe feature map with reduced size is expected to represent\nthe encoded features of coarse structure. On the other hand,\nthe resulting feature map will have exactly the same size\nas the input image if the maximum striding (as the original\ndistance between subsequent blocks) is applied not to allow\noverlapping areas between blocks.\n2) CNN BACKBONE\nThe CNN backbone, as the other encoding channel, is almost\nsimilar to the the contracting path of the U-net where\nthe input image is gradually down-sampled by the pooling\noperation with stride 2, however all vanilla convolutions are\nreplaced to the gated convolutions [21], [31]. As shown in\nFigure 2, an input feature map goes through two pathways\nof gated convolution; one is down-sampling convolution with\nstride 2 followed by batch normalization and leaky rectified\nlinear unit (Leaky ReLU) with negative slope of 0.2 as an\nactivation function, and the other consists of convolution\nwith stride 2, batch normalization, and sigmoid function,\nand two pathways are multiplied together [33]. The gated\nconvolution is designed to train an image inpainting model\nthrough soft gating where the mask is allowed to have gating\nvalues ranging from zero to one. The mechanism of the gated\nconvolution makes it feasible to use a mask with arbitrary\nshape rather than a rectangular mask when training the image\ninpainting network.\nB. FINE TEXTURE GENERATOR\nThe fine texture generator reconstructs fine-grained textures\nof the missing regions through phased operations that\nconsist of a concatenation of the up-sampled low-level\nfeature map with the feature maps conveyed from both\nthe transformer and CNN backbones of the multi-scale\nfeature extractor, followed by two gated convolutions and\nspatially adaptive denormalization (SPADE), a residual\nconnection of the up-sampled low-level feature map, and\nup-sampling by nearest neighbor unpooling, as illustrated in\nFigure 2 [32]. In the gated convolutions in the texture gener-\nator, Leaky ReLU were replaced with ReLU as an activation\nfunction [33].\nC. LOSS FUNCTION\nThe loss function for the GAN generator is given, similar to\nBAT-Fill, to be the combination of a L1 loss, a perceptual\nloss Lper , and an adversarial loss Ladv, which can be\nmathematically formulated as follows\nL = λ1L1 + λper Lper + λadvLadv. (2)\nwhere λ1, λper , and λadv are the weighting coefficients\ncorresponding to each loss which were set to be 1.0, 1.0, and\n0.2 as done in BAT-Fill [28].\nThe L1 loss is defined as the absolute difference between\nthe predicted output image and the corresponding ground\ntruth, while the perceptual loss Lper is defined to be the sum\nof absolute differences in feature maps of special layers in\na pretrained VGG-19 network between the predicted output\nimage and the ground truth [34]. On the other hand, the\nadversarial loss Ladv for the generator is defined based on\nWasserstein GAN to be the negative expectation of the\ndiscriminator output for the predicted output image as the\ngenerator aims to delude the discriminator into recognizing\nthe predicted image as the real sample [35].\nIV. EXPERIMENTS\nA. EXPERIMENTAL SETTINGS\n1) TRAINING CONFIGURATIONS\nThe proposed method was implemented in the Pytorch\nframework (version 1.10.1) based on the source codes of\nBAT-Fill, and was trained on 4 NVIDIA RTX A6000 GPUs\nwith CUDA (version 11.4) [28]. It was optimized using Adam\nsolver with a mini-batch size of 8 and a learning rate of\n2 × 10−4 [36].\nIn particular, hair-shaped masks were alternately employed\nalong with rectangular masks to train the proposed model as\nwell as BAT-Fill, in order to entice the model into increasing\nthe adaptability to hair-like shapes of missing regions in skin\nimages. To increase the complexity and heterogeneity of hair\npatterns, the hair-shaped masks were generated by stacking in\nrandomly chosen angles (among 0 ◦, 90 ◦, 180 ◦, 270 ◦) a few\nhair template masks where hair regions are segmented from\ndermoscopic images using LadderNet [37].\n2) TRAIN DATASETS\nTwo publicly available datasets of CelebA-HQ and\nISIC-2020 were separately used to train the proposed model\nVOLUME 11, 2023 14229\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nTABLE 1. Quantitative evaluations of the proposed framework with the state-of-the art image inpainting methods over ISIC 2020 hairless\ndermoscopic images.Shift-Net, DeepFill v2, and BAT-Fill were trained using the official source codes released in public. All the metrics were assessed in\ndifferent conditions of hair density: low (the mask ratio of 10%), high (30%), and random densities (whose mask ratios are randomly sampled ranging\nfrom 10% to 30%). A standard deviation was parenthesized under the corresponding mean, and the best performance was denoted inbold.\nand evaluate its performance compared to the other methods\nas described in Section IV-A4 [38], [39]. With the identical\nexperimental settings to BAT-Fill, CelebA-HQ which is a\nlarge human face dataset with 30,000 high quality images was\nused in this study to compare the performance of the proposed\nmethod with BAT-Fill. CelebA-HQ was split into 28,000 and\n2,000 for training and validation where 1,000 images chosen\nrandomly from the validation set were used for evaluation as\nwell.\nOn the other hand, the international skin imaging collabo-\nration (ISIC-2020) dataset consists of more than 33,000 der-\nmoscopic images acquired from over 2,000 patients including\nvarious types of skin lesion including melanoma (mel),\nseborrheic keratosis (sk) and nevus (nev) [39]. Adopting the\ngeneral process of training an image inpainting model using\na ground truth image and the synthetically generated mask\njointly, we built a training set of hairless dermoscopic images\nwhich were manually chosen from ISIC-2020 to train the\nproposed model along with pre-extracted hair-shaped masks\nexplained in Section IV-A1. The set of hairless images were\ndivided up into three subsets: 3,000 for training, 100 for\nvalidation, and 150 for evaluation. Dermoscopic images of\ndiverse resolutions and sizes in ISIC-2020 were reshaped and\ncropped into 256 × 256.\n3) TEST DATASETS\nFor quantitative evaluation, we built a test dataset including\n150 hairless skin images and the corresponding simulated\nimages with fake but realistic hairs, as an alternative\nsolution to the absence of such a dataset as the paired\nskin images with and without hairs. The simulated images\nwere generated by blending a hairless skin image with one\nor more stacked hair textures using the Poisson editing\nalgorithm [40]. Note that the hair textures were extracted\nfrom hairy skin images by the same method as described in\nSection IV-A1.\nFor qualitative evaluation, we built the other test dataset of\n30 hairy skin images chosen from ISIC-2020. For a test skin\nimage, the corresponding hair region mask was generated\nusing LadderNet to be used as an input to the common image\ninpainting pipeline [37].\n4) COMPARED METHODS\nThe proposed method was compared with a few state-of-the-\nart image inpainting models which were introduced in Sec-\ntions I and II, including FMM as an image processing based\napproach, Shift-Net as a CNN-based approach, DeepFillv2 as\na GAN-based approach, and BAT-Fill as a transformer-based\napproach [7], [18], [21], [28].\nThe default configuration for training the compared\nmethods were set to use the Adam optimizer and the\nmini-batch size of 8 [36]. As a few exceptions, Shift-Net\nwas trained with the mini-batch size of 1, and the diverse\nstructure generator of BAT-Fill was trained using Adam with\ndecoupled weight decay (AdamW) [41].\n5) EVALUATION METRICS\nAlthough there is no unanimous metric for quantitative\nevaluation in image inpainting as discussed in [42], four\nwell-known metrics were used to evaluate the proposed\nmethod on the test dataset of hairless skin images and\nsimulated hairy images described in Section IV-A3. Peak\nsignal-to-noise ratio (PSNR) and structural similarity index\n(SSIM) with the window size of 11 were adopted which are\nwidely used in image inpainting [43]. The Fréchet inception\nscore (FID) and the learned perceptual image patch similarity\n(LPIPS) are less widely used compared to PSNR and SSIM\nbut more pertinent to the perceptual quality and diversity of\ninpainting results respectively [44], [45].\nB. QUANTITATIVE EVALUATION\nWe first trained the proposed method and evaluated its\nperformance using the publicly available human face dataset\nCelebA-HQ that was previously used to train and test\nBAT-Fill [28]. Table 2 summarizes our quantitative evaluation\nresults on 1,000 human face images from CelebA-HQ in\nrespect of PSNR and SSIM. It should be noticed that the\nvalues for DeepFill v2 and BAT-Fill were quoted from\nthe experimental results reported by Yu et al. in 2021.\nInterestingly, the proposed method exhibited an increase\nof 7.98 dB in PSNR but a decrease of 0.105 in SSIM\ncompared to BAT-Fill when the mask ratios were randomly\n14230 VOLUME 11, 2023\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nFIGURE 4. Qualitative comparison of the proposed framework with the state-of-the-art image inpainting methods for removing hairs\nover the simulated hairy skin images from ISIC 2020.The images generated by FMM are seriously blurred impairing fine texture details,\nand Shift-Net and DeepFill v2 are not successful to get rid of hair marks. The images generated by BAT-Fill are much better but still includes\nhair fragments which are visually distinguishable. Our framework removes all hairs and accurately reconstructs skin and lesion textures.\nTABLE 2. Quantitative evaluations of the proposed framework with the\nstate-of-the art image inpainting methods over CelebA-HQ human face\nimages. The values for DeepFill v2 and BAT-Fill were copied from [28].\nBoth PSNR and SSIM were assessed for small mask ratios (ranging\nfrom 20 to 40%) and large make ratios (40-60%). The best performance\nwas denoted inbold.\ngiven ranging from 20% to 60%. It implies that the image\ngenerated by our framework is more congruous to its ground\ntruth even in large missing regions, compared to DeepFill\nand BAT-Fill despite accompanying a nonnegligible loss in\nstructural information.\nTo assess the proposed method in regard to remov-\ning hairs from dermoscopic images, our framework was\ntrained and tested using the ISIC-2020 dataset described in\nSection IV-A2 [39]. To analyze the effects of hair density on\nthe performance, the hairy images were simulated to have\neither low or high hair pixel densities (of 10% and 30% on\naverage) by stacking just one or three hair template masks\nas delineated in Section IV-A1. We carried out a paired\nt-test to verify the statistical significance of the difference in\nevaluation results between the proposed method and the other\nmethod.\nTable 1 shows the statistical evaluation results on the\npaired set of 150 hairless skin images and simulated hairy\nimages. Compared to the compared methods, PSNR and\nSSIM of our framework highly increased (with 2.6 dB and\n0.042 over the second best method respectively) while FID\nand LPIPS heavily decreased (with 60.6 and 0.09), which\nFIGURE 5. Qualitative comparison of the proposed framework with the\nstate-of-the-art image inpainting methods over CelebA-HQ with large\nmasks. The images generated by both our framework and BAT-Fill are\nmore photo-realistic compared to FMM and DeepFill v2, but no significant\ndifferences are found between two transformer-based methods.\nwere statistically significant ( p-value < 0.05 for Ours vs. all\nthe compared models). It indicates that our EBAT framework\noutperforms BAT-Fill as well as the other compared models,\non all the metrics relevant to either pixel-level accuracy or\nperceptual quality.\nWe measured both the number of trainable parameters\nand the inference time for the proposed method on a\nsingle NVIDIA RTX A6000 GPU and compared them with\nFMM and BAT-Fill. The average inference time on our\nproposed framework was 205 ms for one image sample,\nwhich is 1.5 times slightly slower than FMM (of 136 ms),\nwhile the inference time on BAT-Fill was 187.1 times\nVOLUME 11, 2023 14231\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nFIGURE 6. Qualitative comparison of the proposed framework with the state-of-the-art image inpainting methods for removing hairs\nover ISIC 2020 hairy skin images.FMM, Shift-Net, and DeepFill v2 suffer from the blurring artifact and a number of noticeable hair marks,\nsimilar with Figure 4 tested over simulated images. On the other hand, our proposed method removed most hairs in hairy skin images and\naccurately reconstructed minute texture details and global structure of skin lesions.\nextremely slower with 38,364 ms on average. Given the\nfact that the number of trainable parameters is 3.55 times\nlarger in the proposed framework (361.1M) compared to\nBAT-Fill (101.7M as the sum of 77.2M for the diverse\nstructure generator and 24.5M for the texture generator), it is\nextraordinary that our method is much faster than BAT-Fill.\nThe sluggish processing time might originate mainly from\nthe pixel-unit repetitive loops, in the top-K sampling process\nfor diverse creation of missing regions, which sample the\nmost likely pixel values from the outputs predicted from\nBAT.\nC. QUALITATIVE EVALUATION\nFigure 5 shows the visual comparison between our proposed\nframework and the compared image inpainting methods\nover the test dataset of CelebA-HQ. At first glance it is\nformidable to discriminate the visual differences in generated\nimages between our method and BAT-Fill while both\ntransformer-based methods are superior to DeepFill v2.\nOn the other hand, it can be observed, through such delicate\ndetails as hair color and eyebrow, that the image generated\nby our method is a bit more consistent with the ground truth\ncompared to the other methods.\nFigure 4 shows the qualitative comparison of removing\nhairs over the simulated hairy skin images with high hair\ndensity (30%) which were generated on skin cancer lesions\nincluding nevus as described in Section IV-A3. In Figure 6,\nwe also showed the qualitative results of removing hairs over\nauthentic dermoscopic images with either sparse hairs or\ndense hairs. Our method exhibited not only more enhanced\naccuracy in reconstructing fine texture details and global\nstructure of skin and lesions but also better perceptual quality\nin synthesizing photo-realistic images, compared to the other\nmethods whose image quality was noticeably deteriorated by\nblurry and deformed textures as well as incompletely erased\nhair stains.\n14232 VOLUME 11, 2023\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nTABLE 3. Results of user study.Each entry is a mean opinion score\n(MOS) for 26 image samples generated using the given methods. The\nscore is ranging from 1 (for the worst) to 5 (for the best), and the best\nscore was denoted inbold. A standard deviation was parenthesized\nunder the corresponding mean.\nTo evaluate the human-level perceived image quality\nwithout references, we obtained the mean opinion score\n(MOS) by asking 30 participant observers (24 males and\n6 females in their 20s) to assess the quality of 26 given\nimages with a score ranging from 1 (worst) to 5 (excellent).\nThe sample images used for the MOS acquisition were\nrandomly chosen from the output images generated over\nauthentic dermoscopic images with a large number of\nhairs. As summarized in Table 3, our method had the\nhighest average MOS ratings (4.18) which are proven\nstatistically significant compared to all the other methods\n(p-value < 0.05 for Ours vs. all the other methods).\nD. ABLATION STUDY\nTo figure out the effects of CNN and transformer backbones\nin the multi-scale feature extractor on the performance of\nremoving hairs, we conducted ablation studies by removing\none of two backbones. We compared our method with\ntwo ablations: (i) with CNN backbone only and (ii) with\ntransformer backbone only. The first ablation model (i) would\nbe seen as a network architecture similar to the U-net\nwhere the CNN backbone and the fine texture generator\nbehave as the encoding and decoding parts respectively [31].\nBy contrast, the convolutional image features from the CNN\nbackbone are no longer exploited in the fine texture generator\nin the second ablation model (ii), instead they are replaced\nwith the multi-scale feature maps produced through the\ntransformers.\nAs shown in Figure 7, the output images were distinctly\ndegraded when using both ablation models. Hairs remain\nincompletely eliminated which resulted in deteriorating the\nskin and lesion textures due to hair steins, although the\nCNN backbone and the transformer backbone seemed to be\nrelatively better in capturing the global structure information\nand the fine texture details with long-range dependency\nrespectively. On the other hand, the proposed method where\nboth CNN and transformer backbones are integrated was\nsuperior to both ablation models in reconstructing both global\nstructure as well as fine-grained textures.\nThe qualitative analysis for the ablation models is consis-\ntent with the quantitative evaluation results over simulated\nskin images as summarized in Table 4. The proposed\nmethod obviously improved all the metrics even in the\ncase of high hair density, which demonstrates the advantage\nof integrating both CNN and transformer backbones to\nextract a comprehensive set of image features relevant to\nglobal structure and fine-grained textures with long range\nFIGURE 7. Visual comparison of the proposed method with its\nablations for verifying the effect of integrated transformer and CNN\nbackbones. The use of integrated transformer and CNN backbones in the\nmulti-scale feature extractor results in the enhanced performance in\nremoving hairs and reconstructing the fine textures of skin and lesions.\nTABLE 4. Quantitative comparison of our proposed framework with its\nablations over a hair-simulated image dataset from ISIC 2020 hairless\ndermoscopic images.The ablation study was conducted for coarse hairs\n(of mask ratio 10%) or dense hairs (30%). A standard deviation was\nparenthesized under the corresponding mean, and the best performance\nwas denoted inbold.\ndependency that cannot be readily captured by convolutional\nlayers.\nV. CONCLUSION\nIn this study, we present a novel transformer-based generative\nimage inpainting framework, called EBAT, that achieves\naccurate and realistic reconstruction of skin texture by remov-\ning hairs from dermoscopic images. Our model is able to\nlearn both coarse structure and fine skin textures by extracting\nimage features with long-range dependency in multiple\nspatial scales using dual modalities of bidirectional autore-\ngressive transformers and convolutional neural networks. The\nmulti-scale feature extraction from transformers is facilitated\nVOLUME 11, 2023 14233\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\nby patch-wise unfolding and down-sized folding operations.\nOur framework is efficiently trained in an end-to-end manner\nthrough manifold pathways laid between multi-scale feature\nextractor and fine texture generator. The experimental results\non removing hairs in both simulated and authentic hairy\ndermoscopic images show that, in qualitative and quantitative\nperformance of removing hairs from dermoscopic images,\nour extensive transformer-based image inpainting framework\noutperforms not only the state-of-the-art image inpainting\nmodels but also the latest transformer-based method like\nBAT-Fill.\nDespite the novelties and improved applicability of our\nmodel to hairy skin images, it deserves to mention its\ntechnical limitations. It is still challenging to remove hairs\nlaid on skin lesions. We find that our method sometimes fails\nin removing hairs whose color is similar to either skin lesions\nor skin. The method should be improved to learn diverse\nfeatures of skin lesions.\nAs a future work, it is imperative to figure out the potential\neffects of hair removal on skin cancer classification. The hair\nremoval algorithm needs to be advanced toward reducing the\nvulnerability of skin cancer classification to hair removal,\ntaking into account the risk such that the texture of skin lesion\nmight be tainted by the process of removing hairs.\nACKNOWLEDGMENT\nThe authors thank Dr. Soo Ye Kim from Adobe Research,\nDr. Jaeyoung Choi from Hankuk University of Foreign Stud-\nies, Dr. Bumshik Lee from Chosun University, and Hyunjae\nZhang from F&D Partners Corporation, for insightful discus-\nsions on image inpainting and its industrial demands, and\nBumjin Park and Gyubin Lee from AIIP Lab, for assisting\ndata processing.\nREFERENCES\n[1] O. T. Jones, R. N. Matin, M. van der Schaar, K. P. Bhayankaram,\nC. K. I. Ranmuthu, M. S. Islam, D. Behiyat, R. Boscott, N. Calanzani,\nJ. Emery, H. C. Williams, and F. M. Walter, ‘‘Artificial intelligence\nand machine learning algorithms for early detection of skin cancer in\ncommunity and primary care settings: A systematic review,’’ Lancet Digit.\nHealth, vol. 4, no. 6, pp. e466–e476, Jun. 2022.\n[2] J. A. Salido, ‘‘Hair artifact removal and skin lesion segmentation of\ndermoscopy images,’’ Asian J. Pharmaceutical Clin. Res., vol. 11, no. 15,\np. 36, Oct. 2018.\n[3] L. Talavera-Martinez, P. Bibiloni, and M. Gonzalez-Hidalgo, ‘‘Hair\nsegmentation and removal in dermoscopic images using deep learning,’’\nIEEE Access, vol. 9, pp. 2694–2704, 2021.\n[4] J. A. A. Salido and C. Ruiz, ‘‘Using morphological operators and\ninpainting for hair removal in dermoscopic images,’’ in Proc. Comput.\nGraph. Int. Conf., Jun. 2017, pp. 1–6.\n[5] T. Lee, V. Ng, R. Gallagher, A. Coldman, and D. McLean, ‘‘Dullrazor ®:\nA software approach to hair removal from images,’’ Comput. Biol. Med.,\nvol. 27, no. 6, pp. 533–543, Nov. 1997.\n[6] K. Kiani and A. R. Sharafat, ‘‘E-shaver: An improved Dullrazor ® for\ndigitally removing dark and light-colored hairs in dermoscopic images,’’\nComput. Biol. Med., vol. 41, no. 3, pp. 139–145, Mar. 2011.\n[7] F. Bornemann and T. März, ‘‘Fast image inpainting based on coherence\ntransport,’’J. Math. Imag. Vis., vol. 28, no. 3, pp. 259–278, 2007.\n[8] F.-Y. Xie, S.-Y. Qin, Z.-G. Jiang, and R.-S. Meng, ‘‘PDE-based unsu-\npervised repair of hair-occluded information in dermoscopy images of\nmelanoma,’’ Comput. Med. Imag. Graph., vol. 33, no. 4, pp. 275–282,\n2009.\n[9] A. Huang, S.-Y. Kwan, W.-Y. Chang, M.-Y. Liu, M.-H. Chi, and\nG.-S. Chen, ‘‘A robust hair segmentation and removal approach for clinical\nimages of skin lesions,’’ in Proc. 35th Annu. Int. Conf. IEEE Eng. Med.\nBiol. Soc. (EMBC), Jul. 2013, pp. 3315–3318.\n[10] Q. Abbas, I. F. Garcia, M. Emre Celebi, and W. Ahmad, ‘‘A feature-\npreserving hair removal algorithm for dermoscopy images,’’ Skin Res.\nTechnol., vol. 19, no. 1, pp. e27–e36, Feb. 2013.\n[11] A. Nasonova, A. Nasonov, A. Krylov, I. Pechenko, A. Umnov, and\nN. Makhneva, ‘‘Image warping in dermatological image hair removal,’’\nin Proc. Int. Conf. Image Anal. Recognit., 2014, pp. 159–166.\n[12] D. Borys, P. Kowalska, M. Frackiewicz, and Z. Ostrowski, ‘‘A simple hair\nremoval algorithm from dermoscopic images,’’ in Proc. Int. Conf. Bioinf.\nBiomed. Eng., 2015, pp. 262–273.\n[13] J. Koehoorn, A. Sobiecki, P. Rauber, A. Jalba, and A. Telea, ‘‘Effcient and\neffective automated digital hair removal from dermoscopy images,’’ Math.\nMorphol.-Theory Appl., vol. 1, no. 1, pp. 1–17, Mar. 2016.\n[14] S. Pathan, K. G. Prabhu, and P. C. Siddalingaswamy, ‘‘Hair detec-\ntion and lesion segmentation in dermoscopic images using domain\nknowledge,’’ Med. Biol. Eng. Comput., vol. 56, no. 11, pp. 2051–2065,\nNov. 2018.\n[15] I. Zaqout, ‘‘An efficient block-based algorithm for hair removal in\ndermoscopic images,’’ Comput. Opt., vol. 41, no. 4, pp. 521–527,\n2017.\n[16] K. Zafar, S. O. Gilani, A. Waris, A. Ahmed, M. Jamil, M. N. Khan, and\nA. S. Kashif, ‘‘Skin lesion segmentation from dermoscopic images using\nconvolutional neural network,’’Sensors, vol. 20, no. 6, p. 1601, Mar. 2020.\n[Online]. Available: https://www.mdpi.com/1424-8220/20/6/1601\n[17] J. Koehoorn, A. C. Sobiecki, D. Boda, A. Diaconeasa, S. Doshi, S. Paisey,\nA. Jalba, and A. Telea, ‘‘Automated digital hair removal by threshold\ndecomposition and morphological analysis,’’ in Proc. Int. Symp. Math.\nMorphol. Appl. Signal Image Process., 2015, pp. 15–26.\n[18] Z. Yan, X. Li, M. Li, W. Zuo, and S. Shan, ‘‘Shift-Net: Image inpainting\nvia deep feature rearrangement,’’ in Proc. Int. Symp. Math. Morphology.\nAppl. Signal Image Process., 2018, pp. 3–19.\n[19] Y. Wang, X. Tao, X. Qi, X. Shen, and J. Jia, ‘‘Image inpainting via\ngenerative multi-column convolutional neural networks,’’ in Proc. Adv.\nNeural Inf. Process. Syst., Dec. 2018, pp. 331–340.\n[20] G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro,\n‘‘Image inpainting for irregular holes using partial convolutions,’’ in Proc.\nEur. Conf. Comput. Vis. (ECCV), 2018, pp. 89–105.\n[21] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. Huang, ‘‘Free-form image\ninpainting with gated convolution,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2019, pp. 4470–4479.\n[22] C. Xie, S. Liu, C. Li, M.-M. Cheng, W. Zuo, X. Liu, S. Wen,\nand E. Ding, ‘‘Image inpainting with learnable bidirectional atten-\ntion maps,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis., Oct. 2019,\npp. 8857–8866.\n[23] O. Elharrouss, N. Almaadeed, S. Al-Maadeed, and Y. Akbari, ‘‘Image\ninpainting: A review,’’Neural Process. Lett., vol. 51, no. 2, pp. 2007–2028,\n2019.\n[24] D. Bardou, H. Bouaziz, L. Lv, and T. Zhang, ‘‘Hair removal in dermoscopy\nimages using variational autoencoders,’’ Skin Res. Technol., vol. 28, no. 3,\npp. 445–454, May 2022.\n[25] W. Li, A. N. Joseph Raj, T. Tjahjadi, and Z. Zhuang, ‘‘Digital hair removal\nby deep learning for skin lesion segmentation,’’ Pattern Recognit., vol. 117,\nSep. 2021, Art. no. 107994.\n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 6000–6010.\n[27] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n‘‘Transformers in vision: A survey,’’ ACM Comput. Surv., vol. 54, no. 10s,\npp. 1–41, Jan. 2022.\n[28] Y. Yu, F. Zhan, R. Wu, J. Pan, K. Cui, S. Lu, F. Ma, X. Xie, and C.\nMiao, ‘‘Diverse image inpainting with bidirectional and autoregressive\ntransformers,’’ in Proc. 29th ACM Int. Conf. Multimedia, Oct. 2021,\npp. 69–78.\n[29] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‘‘Generative adversarial networks,’’\nCommun. ACM, vol. 63, pp. 139–144, Oct. 2020.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL-HLT, 2019, pp. 4171–4186.\n14234 VOLUME 11, 2023\nY. Lee, W. You: EBAT: Enhanced BATs for Removing Hairs in Hairy Dermoscopic Images\n[31] O. Ronneberger, P. Fischer, and T. Brox, ‘‘U-Net: Convolutional networks\nfor biomedical image segmentation,’’ in Medical Image Computing and\nComputer-Assisted Intervention—MICCAI 2015, W. M. W. Joachim and\nF. A. Nassir, and J. Hornegger, Eds. Cham, Switzerland: Springer, 2015,\npp. 234–241.\n[32] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, ‘‘Semantic image synthesis\nwith spatially-adaptive normalization,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2019, pp. 2332–2341.\n[33] V. Nair and G. E. Hinton, ‘‘Rectified linear units improve restricted\nBoltzmann machines,’’ in Proc. 27th Int. Conf. Mach. Learn., 2010,\npp. 807–814.\n[34] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ 2014, arXiv:1409.1556.\n[35] M. Arjovsky, S. Chintala, and L. Bottou, ‘‘Wasserstein GAN,’’ in Proc.\nAdv. Neural Inf. Process. Syst. 1 2017, pp. 1–15.\n[36] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[37] J. Zhuang, ‘‘LadderNet: Multi-path networks based on U-Net for medical\nimage segmentation,’’ 2018, arXiv:1810.07810.\n[38] T. Karras, T. Aila, S. Laine, and J. Lehtinen, ‘‘Progressive grow-\ning of GANs for improved quality, stability, and variation,’’ 2017,\narXiv:1710.10196.\n[39] V. Rotemberg et al., ‘‘A patient-centric dataset of images and metadata\nfor identifying melanomas using clinical context,’’ Sci. Data, vol. 8, p. 34,\nJan. 2021.\n[40] P. Pérez, M. Gangnet, and A. Blake, ‘‘Poisson image editing,’’ in Proc.\nACM SIGGRAPH, 2003, p. 313.\n[41] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2017, arXiv:1711.05101.\n[42] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, ‘‘Generative image\ninpainting with contextual attention,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., Jun. 2018, pp. 5505–5514.\n[43] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‘‘Image quality\nassessment: From error visibility to structural similarity,’’ IEEE Trans.\nImage Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.\n[44] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,\n‘‘GANs trained by a two time-scale update rule converge to a local\nnash equilibrium,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017,\npp. 6629–6640.\n[45] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, ‘‘The\nunreasonable effectiveness of deep features as a perceptual metric,’’\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , Jun. 2018,\npp. 586–595.\nYOUNGCHAN LEE received the B.S. degree\nin information and communications engineering\nfrom Sun Moon University, in 2022. He is\ncurrently pursuing the M.Sc. degree in information\nand communications engineering with the Artifi-\ncial Intelligence and Image Processing Laboratory\n(AIIP Lab). His research interests include image\ninpainting, computer vision, deep learning, and\nmedical imaging.\nWONSANG YOU (Member, IEEE) was born\nin Seoul, South Korea, in 1977. He received\nthe M.Sc. degree in engineering (specialized in\ncomputer vision and image processing) from the\nKorea Advanced Institute of Science and Technol-\nogy (KAIST), in 2008, and the Ph.D. degree in\nelectrical engineering and information technolo-\ngies (specialized in brain imaging data analysis)\nfrom Otto-von-Guericke University Magdeburg,\nin 2013. From 2009 to 2012, he worked as a\nResearch Assistant with the Leibniz Institute for Neurobiology, Magdeburg,\nGermany. From 2013 to 2019, he worked as a Staff Scientist with the Center\nfor the Developing Brain in Children’s National Hospital, Washington,\nDC, USA. He is currently an Assistant Professor with the Department\nof Information and Communication Engineering, Sun Moon University,\nSouth Korea. He is also the Director of the Artificial Intelligence and Image\nProcessing Laboratory (AIIP Lab). His research interests include computer\nvision, image analysis, deep learning, and medical imaging. He is also\nworking on image inpainting, super resolution, pose estimation, 2D-to-3D\nreconstruction, and image-to-text generation.\nVOLUME 11, 2023 14235",
  "topic": "Inpainting",
  "concepts": [
    {
      "name": "Inpainting",
      "score": 0.7880202531814575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7803671360015869
    },
    {
      "name": "Computer science",
      "score": 0.6896841526031494
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5926371216773987
    },
    {
      "name": "Deep learning",
      "score": 0.5541691184043884
    },
    {
      "name": "Autoregressive model",
      "score": 0.534114420413971
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5260153412818909
    },
    {
      "name": "Computer vision",
      "score": 0.5028223395347595
    },
    {
      "name": "Pixel",
      "score": 0.4774525761604309
    },
    {
      "name": "Kernel (algebra)",
      "score": 0.4661281108856201
    },
    {
      "name": "Transformer",
      "score": 0.46287277340888977
    },
    {
      "name": "Feature extraction",
      "score": 0.4188621938228607
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3162432909011841
    },
    {
      "name": "Mathematics",
      "score": 0.16040074825286865
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51926615",
      "name": "Sun Moon University",
      "country": "KR"
    }
  ]
}