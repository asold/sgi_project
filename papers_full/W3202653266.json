{
  "title": "Geometric Transformers for Protein Interface Contact Prediction",
  "url": "https://openalex.org/W3202653266",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5072705391",
      "name": "Alex Morehead",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A5100418423",
      "name": "Chen Chen",
      "affiliations": [
        "University of Missouri"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3035332875",
    "https://openalex.org/W3128976935",
    "https://openalex.org/W3128917876",
    "https://openalex.org/W2804651207",
    "https://openalex.org/W2980678340",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2970009611",
    "https://openalex.org/W3128355392",
    "https://openalex.org/W1964084224",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3167957225",
    "https://openalex.org/W2140538197",
    "https://openalex.org/W2242464395",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2885015404",
    "https://openalex.org/W2751808960",
    "https://openalex.org/W2056169996",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W3191962800",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2603552761",
    "https://openalex.org/W2992752586",
    "https://openalex.org/W3159719254",
    "https://openalex.org/W2792287754",
    "https://openalex.org/W1030883578",
    "https://openalex.org/W3133540795",
    "https://openalex.org/W3135883064",
    "https://openalex.org/W3157286395",
    "https://openalex.org/W3042283064",
    "https://openalex.org/W2963420686"
  ],
  "abstract": "Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer, a novel geometry-evolving graph transformer for rotation and translation-invariant protein interface contact prediction, packaged within DeepInteract, an end-to-end prediction pipeline. DeepInteract predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input. In rigorous benchmarks, DeepInteract, on challenging protein complex targets from the 13th and 14th CASP-CAPRI experiments as well as Docking Benchmark 5, achieves 14% and 1.1% top L/5 precision (L: length of a protein unit in a complex), respectively. In doing so, DeepInteract, with the Geometric Transformer as its graph-based backbone, outperforms existing methods for interface contact prediction in addition to other graph-based neural network backbones compatible with DeepInteract, thereby validating the effectiveness of the Geometric Transformer for learning rich relational-geometric features for downstream tasks on 3D protein structures.",
  "full_text": "Published as a conference paper at ICLR 2022\nGEOMETRIC TRANSFORMERS FOR PROTEIN\nINTERFACE CONTACT PREDICTION\nAlex Morehead, Chen Chen, & Jianlin Cheng\nDepartment of Electrical Engineering & Computer Science\nUniversity of Missouri\nColumbia, MO 65211, USA\n{acmwhb,chen.chen,chengji}@missouri.edu\nABSTRACT\nComputational methods for predicting the interface contacts between proteins\ncome highly sought after for drug discovery as they can signiﬁcantly advance\nthe accuracy of alternative approaches, such as protein-protein docking, protein\nfunction analysis tools, and other computational methods for protein bioinfor-\nmatics. In this work, we present the Geometric Transformer, a novel geometry-\nevolving graph transformer for rotation and translation-invariant protein inter-\nface contact prediction, packaged within DeepInteract, an end-to-end prediction\npipeline. DeepInteract predicts partner-speciﬁc protein interface contacts (i.e.,\ninter-protein residue-residue contacts) given the 3D tertiary structures of two pro-\nteins as input. In rigorous benchmarks, DeepInteract, on challenging protein com-\nplex targets from the 13th and 14th CASP-CAPRI experiments as well as Docking\nBenchmark 5, achieves 14% and 1.1% top L/5 precision (L: length of a protein\nunit in a complex), respectively. In doing so, DeepInteract, with the Geomet-\nric Transformer as its graph-based backbone, outperforms existing methods for\ninterface contact prediction in addition to other graph-based neural network back-\nbones compatible with DeepInteract, thereby validating the effectiveness of the\nGeometric Transformer for learning rich relational-geometric features for down-\nstream tasks on 3D protein structures.1\n1 I NTRODUCTION\nInteractions of proteins often reﬂect and directly inﬂuence their functions in molecular processes, so\nunderstanding the relationship between protein interaction and protein function is of utmost impor-\ntance to biologists and other life scientists. Here, we study the residue-residue interaction between\ntwo protein structures that bind together to form a binary protein complex (i.e., dimer), to better\nunderstand how these coupled proteins will function in vivo. Predicting where two proteins will\ninterface in silico has become an appealing method for measuring the interactions between proteins\nsince a computational approach saves time, energy, and resources compared to traditional methods\nfor experimentally measuring such interfaces (Wells & McClendon (2007)). A key motivation for\ndetermining these interface contacts is to decrease the time required to discover new drugs and to\nadvance the study of newly designed proteins (Murakami et al. (2017)).\nExisting approaches to interface contact prediction include classical machine learning and deep\nlearning-based methods. These methods traditionally use hand-crafted features to predict which\ninter-chain pairs of amino acid residues will interact with one another upon the binding of the two\nprotein chains, treating each of their residue pairs as being independent of one another. Recent\nwork on interface prediction (Liu et al. (2020)), however, considers the biological insight that the\ninteraction between two inter-chain residue pairs depends not only on the pairs’ features themselves\nbut also on other residue pairs ordinally nearby in terms of the protein complex’s sequence. As\nsuch, the problem of interface contact prediction became framed as one akin to image segmentation\n1Training and inference code as well as pre-trained models are available at\nhttps://github.com/BioinfoMachineLearning/DeepInteract\n1\narXiv:2110.02423v5  [cs.LG]  4 Mar 2022\nPublished as a conference paper at ICLR 2022\nFigure 1: A Mol* (Sehnal et al. (2021)) visualization of interacting protein chains (PDB ID: 3H11).\nor object detection, opening the door to innovations in interface contact prediction by incorporating\nthe latest techniques from computer vision.\nNonetheless, up to now, no works onpartner-speciﬁc protein interface contact prediction have lever-\naged two recent innovations to better capture geometric shapes of protein structures and long-range\ninteractions between amino acids important for accurate prediction of protein-protein interface con-\ntacts: (1) geometric deep learning for evolving proteins’ geometric representations and (2) graph-\nbased self-attention similar to that of Vaswani et al. (2017). Towards this end, we introduce Deep-\nInteract, an end-to-end deep learning pipeline for protein interface prediction. DeepInteract houses\nthe Geometric Transformer, a new graph transformer designed to exploit protein structure-speciﬁc\ngeometric properties, as well as a dilated convolution-based interaction module adapted from Chen\net al. (2021) to predict which inter-chain residue pairs comprise the interface between the two protein\nchains. In response to the exponential rate of progress being made in predicting protein structures\nin silico, we trained DeepInteract end-to-end using DIPS-Plus (Morehead et al. (2021)), to date the\nlargest feature-rich dataset of protein complex structures for machine learning of protein interfaces,\nto close the gap on a proper solution to this fundamental problem in structural biology.\n2 R ELATED WORK\nOver the past several years, geometric deep learning has become an effective means of automati-\ncally learning useful feature representations from structured data (Bronstein et al. (2021)). Previ-\nously, geometric learning algorithms like convolutional neural networks (CNNs) and graph neural\nnetworks (GNNs) have been used to model molecules and to predict protein interface contacts.\nSch¨utt et al. (2017) introduced a deep tensor neural network designed for molecular tasks in quan-\ntum chemistry. Fout et al. (2017) designed a siamese GNN architecture to learn weight-tied feature\nrepresentations of residue pairs. This approach, in essence, processes subgraphs for each residue in\neach complex and aggregates node-level features locally using a nearest-neighbors approach. Since\nthis partner-speciﬁc method derives its training dataset from Docking Benchmark 5 (DB5) (Vreven\net al. (2015)), it is ultimately data-limited. Townshend et al. (2019) represent interacting protein\ncomplexes by voxelizing each residue into a 3D grid and encoding in each grid entry the presence\nand type of the residue’s underlying atoms. This partner-speciﬁc encoding scheme captures static\ngeometric features of interacting complexes, but it is not able to scale well due to its requiring a\ncomputationally-expensive spatial resolution of the residue voxels to achieve good results.\nContinuing the trend of applying geometric learning to protein structures, Gainza et al. (2020)\ndeveloped MaSIF to perform partner-independent interface region prediction. Likewise, Dai &\nBailey-Kellogg (2021) do so with an attention-based GNN. These methods learn to perform binary\nclassiﬁcation of the residues in both complex structures to identify regions where residues from\nboth complexes are likely to interact with one another. However, because these approaches pre-\ndict partner-independent interface regions, they are less likely to be useful in helping solve related\ntasks such as drug-protein interaction prediction and protein-protein docking (Ahmad & Mizuguchi\n2\nPublished as a conference paper at ICLR 2022\n(2011)). Liu et al. (2021a) created a graph neural network for predicting the effects of mutations\non protein-protein binding afﬁnities, and, more recently, Costa et al. (2021) introduced a Euclidean\nequivariant transformer for protein docking. Both of these methods may beneﬁt from the availability\nof precise interface predictors by using them to generate contact maps as input features.\nTo date, one of the best result sets obtained by any model for protein interface contact prediction\ncomes from Liu et al. (2020) where high-order (i.e. sequential and coevolution-based) interactions\nbetween residues are learned and preserved throughout the network in addition to static geomet-\nric features initially embedded in the protein complexes. However, this work, like many of those\npreceding it, undesirably maintains the trend of reporting model performance in terms of the me-\ndian area under the receiver operating characteristic which is not robust to extreme class imbalances\nas often occur in interface contact prediction. In addition, this approach is data-limited as it uses\nthe DB5 dataset and its predecessors to derive both its training data and makes use of only each\nresidue’s carbon-alpha (Cα) atom in deriving its geometric features, ignoring important geometric\ndetails provided by an all-atom view of protein structures.\nOur work builds on top of prior works by making the following contributions:\n• We provide the ﬁrst example of graph self-attention applied to protein interface contact\nprediction, showcasing its effective use in learning representations of protein geometries to\nbe exploited in downstream tasks.\n• We propose the new Geometric Transformer which can be used for tasks on 3D protein\nstructures and similar biomolecules. For the problem of interface contact prediction, we\ntrain the Geometric Transformer to evolve a geometric representation of protein structures\nsimultaneously with protein sequence and coevolutionary features for the prediction of\ninter-chain residue-residue contacts. In doing so, we also demonstrate the merit of the\nrecently-released Enhanced Database of Interacting Protein Structures (DIPS-Plus) for in-\nterface prediction (Morehead et al. (2021)).\n• Our experiments on challenging protein complex targets demonstrate that our proposed\nmethod, DeepInteract, achieves state-of-the-art results for interface contact prediction.\n3 D ATASETS\nThe current opinion in the bioinformatics community is that protein sequence features still carry\nimportant higher-order information concerning residue-residue interactions (Liu et al. (2020)). In\nparticular, the residue-residue coevolution and residue conservation information obtained through\nmultiple sequence alignments (MSAs) has been shown to contain powerful information concerning\nintra-chain and even inter-chain residue-residue interactions as they yield a compact representation\nof residues’ coevolutionary relationships (Jumper et al. (2021)).\nKeeping this in mind, for our training and validation datasets, we chose to use DIPS-Plus (Morehead\net al. (2021)), to our knowledge the largest feature-rich dataset of protein complexes for protein in-\nterface contact prediction to date. In total, DIPS-Plus contains 42,112 binary protein complexes with\npositive labels (i.e., 1) for each inter-chain residue pair that are found within 6˚A of each other in the\ncomplex’s bound (i.e., structurally-conformed) state. The dataset contains a variety of rich residue-\nlevel features: (1) an 8-state one-hot encoding of the secondary structure in which the residue is\nfound; (2) a scalar solvent accessibility; (3) a scalar residue depth; (4) a 1 ×6 vector detailing each\nresidue’s protrusion concerning its side chain; (5) a 1 ×42 vector describing the composition of\namino acids towards and away from each residue’s side chain; (6) each residue’s coordinate number\nconveying how many residues to which the residue meets a signiﬁcance threshold, (7) a 1×27 vec-\ntor giving residues’ emission and transition probabilities derived from HH-suite3 (Steinegger et al.\n(2019)) proﬁle hidden Markov models constructed using MSAs; and (8) amide plane normal vectors\nfor downstream calculation of the angle between each intra-chain residue pair’s amide planes.\nTo compare the performance of DeepInteract with that of state-of-the-art methods, we select 32\nhomodimers and heterodimers from the test partition of DIPS-Plus to assess each method’s compe-\ntency in predicting interface contacts. We also evaluate each method on 14 homodimers and 5 het-\nerodimers with PDB structures publicly available from the 13th and 14th sessions of CASP-CAPRI\n(Lensink et al. (2019), Lensink et al. (2021)) as these targets are considered by the bioinformatics\n3\nPublished as a conference paper at ICLR 2022\nFigure 2: DeepInteract overview. Our proposed pipeline separates interface contact prediction into\ntwo tasks: (1) learning new node representations hA and hB for pairs of residue protein graphs and\n(2) convolving over hA and hB interleaved together to predict pairwise contact probabilities.\ncommunity to be challenging for existing interface predictors. For any CASP-CAPRI test complexes\nderived from multimers (i.e., protein complexes that can contain more than two chains), to represent\nthe complex we chose the pair of chains with the largest number of interface contacts. Finally, we\nuse the traditional 55 test complexes from the DB5 dataset (Fout et al. (2017); Townshend et al.\n(2019); Liu et al. (2020)) to benchmark each heteromer-compatible method.\nTo expedite training and validation and to constrain memory usage, beginning with all remaining\ncomplexes not chosen for testing, we ﬁltered out all complexes where either chain contains fewer\nthan 20 residues and where the number of possible interface contacts is more than 2562, leaving us\nwith an intermediate total of 26,504 complexes for training and validation. In DIPS-Plus, binary\nprotein complexes are grouped into shared directories according to whether they are derived from\nthe same parent complex. As such, using a per-directory strategy, we randomly designate 80% of\nthese complexes for training and 20% for validation to restrict overlap between our cross-validation\ndatasets. After choosing these targets for testing, we then ﬁlter out complexes from our training and\nvalidation partitions of DIPS-Plus that contain any chain with over 30% sequence identity to any\nchain in any complex in our test datasets. This threshold of 30% sequence identity is commonly used\nin the bioinformatics literature (Jordan et al. (2012), Yang et al. (2013)) to prevent large evolutionary\noverlap between a dataset’s cross-validation partitions. However, most existing works for interface\ncontact prediction do not employ such ﬁltering criteria, so the results reported in these works may\nbe over-optimistic by nature. In performing such sequence-based ﬁltering, we are left with 15,618\nand 3,548 binary complexes for training and validation, respectively.\n4 M ETHODS\n4.1 P ROBLEM FORMULATION\nSummarized in Figure 2, we designed DeepInteract, our proposed pipeline for interface contact pre-\ndiction, to frame the problem of predicting interface contacts in silico as a two-part task: The ﬁrst\npart is to use attentive graph representation learning to inductively learn new node-level represen-\ntations hA ∈RA×C and hB ∈RB×C for a pair of graphs representing two protein chains. The\nsecond part is to channel-wise interleave hA and hB into an interaction tensor I ∈RA×B×2C, where\nA ∈R and B ∈R are the numbers of amino acid residues in the ﬁrst and second input protein\nchains, respectively, and C ∈R is the number of hidden channels in both hA and hB. We use in-\nteraction tensors such as I as input to our interaction module, a convolution-based dense predictor\nof inter-graph node-node interactions. We denote each protein chain in an input complex as a graph\nG with edges E between the k-nearest neighbors of its nodes N, with nodes corresponding to the\nchain’s amino acid residues represented by theirCα atoms. In this setting, we let k= 20as we ob-\nserved favorable cross entropy loss on our validation dataset with this level of connectivity. We note\nthat this level of graph connectivity has also proven to be advantageous for prior works developing\n4\nPublished as a conference paper at ICLR 2022\nFigure 3: Geometric Transformer overview. Notably, our ﬁnal layer of the Geometric Transformer\nremoves the edge update path since, in our formulation of interface prediction, only graph pairs’\nnode representations hA and hB are directly used for the ﬁnal interface contact prediction.\ndeep learning approaches for graph-based protein representations (Fout et al. (2017); Ingraham et al.\n(2019)).\n4.2 G EOMETRIC TRANSFORMER ARCHITECTURE\nHypothesizing that a self-attention mechanism that evolves proteins’ physical geometries is a key\ncomponent missing from existing interface contact predictors, we propose the Geometric Trans-\nformer, a graph neural network explicitly designed for capturing and iteratively evolving protein\ngeometric features. As shown in Figure 3, the Geometric Transformer expands upon the existing\nGraph Transformer architecture (Dwivedi & Bresson (2021)) by introducing ( 1) an edge initializa-\ntion module, (2) an edge-wise positional encoding (EPE), and (3) a geometry-evolving conformation\nmodule employing repeated geometric feature gating (GFG) (see Sections A.6, A.7, and A.8 for ra-\ntionale). Moreover, the Geometric Transformer includes subtle architectural enhancements to the\noriginal Transformer architecture (Vaswani et al. (2017)) such as moving the network’s ﬁrst nor-\nmalization layer to precede any afﬁnity score computations for improved training stability (Hussain\net al. (2021)). To our knowledge, the Geometric Transformer is the ﬁrst deep learning model that\napplies multi-head attention to the task of partner-speciﬁc protein interface contact prediction. The\nfollowing sections serve to distinguish our new Geometric Transformer from other Transformer-like\narchitectures by describing its new neural network modules for geometric self-attention.\n4.2.1 E DGE INITIALIZATION MODULE\nTo accelerate its training, the Geometric Transformer ﬁrst embeds each edge e ∈E with the initial\nedge representation\ncij = φ1\ne([p1 ||p2 ||φmij\ne (mij ||λe) ||φf1\ne (f1) ||φf2\ne (f2) ||φf3\ne (f3) ||φf4\ne (f4)]) (1)\neij = φ2\ne(ρa\ne(ρg\ne(cij))) (2)\nwhere φi\ne refers to the i’th edge information update function such as a multi-layer perceptron; ||\ndenotes channel-wise concatenation; p1 and p2, respectively, are trainable one-hot vectors indexed\nby Pi and Pj, the positions of nodes iand nodes j in the chain’s underlying amino acid sequence;\nmij are any user-predeﬁned features for e(in our case the normalized Euclidean distances between\nnodes iand nodes j); λe are edge-wise sinusoidal positional encodings sin(Pi−Pj) for e; f1, f2, f3,\n5\nPublished as a conference paper at ICLR 2022\nFigure 4: Conformation module overview. The Geometric Transformer uses a conformation module\nin each layer to evolve proteins graphs’ geometric representations via repeated gating and a ﬁnal\nseries of residual connection blocks.\nand f4, in order, are the four protein-speciﬁc geometric features deﬁned in Section A.3; and ρa\ne and\nρg\ne are feature addition and channel-wise gating functions, respectively.\n4.2.2 C ONFORMATION MODULE\nThe role of the Geometric Transformer’s subsequent conformation module, as illustrated in Figure\n4, is for it to learn how to iteratively evolve geometric representations of protein graphs by applying\nrepeated gating to our initial edge geometric features f1, f2, f3, and f4. To do so, the conformation\nmodule updates eij by introducing the notion of a geometric neighborhood of edge e, treating eas\na pseudo-node. Precisely, Ek, the edge geometric neighborhood of e, is deﬁned as the 2nedges\nEk = {en1i,en2j |(n1,n2 ∈Nk) and (n1,n2 ̸= i,j)}, (3)\nwhere Nk ⊂N are the source nodes for incoming edges on edge e′ssource and destination nodes.\nThe intuition behind updating each edge according to its 2nnearest neighboring edges is that the\ngeometric relationship between a residue pair, described by their mutual edge’s features, can be\ninﬂuenced by the physical constraints imposed by proximal residue-residue geometries. As such, we\nuse these nearby edges during geometric feature updates. In the conformation module, the iterative\nprocessing of all geometric neighborhood features for edge ecan be represented as\nOij =\n∑\nk∈Ek\n[(φn\ne(en\nij,k) ⊙φfn\ne (fn)),∀n∈F] (4)\neij = 2×ResBlock2(φ5\ne(eij) + 2×ResBlock1(φ5\ne(eij) +Oij)), (5)\nwhere F are the indices of the geometric features{f1,f2,f3,f4}deﬁned in Section A.3; ⊙is element-\nwise multiplication; en\nij,k is neighboring edge ek’s representation after gating with fn−1; and 2 ×\nResBlocki represents the i’th application of two unique, successive residual blocks, each deﬁned\nas ResBlock(x) =φRes2\ne (φRes1\ne (x)) +x. Described in Section A.3, by way of their construction,\neach of our selected edge geometric features is translation and rotation invariant to the network’s\ninput space. As discussed in Section A.5, we couple these features with our choice of node-wise\npositional encodings (see Section 4.2.3) to attain canonical invariant local frames for each residue\nto encode the relative poses of features in our protein graphs. In doing so, we leverage many of the\nbeneﬁts of employing equivariant representations while reducing the large memory requirements\nthey typically induce, to yield a robust invariant representation of each input protein.\n4.2.3 R EMAINING TRANSFORMER INITIALIZATIONS AND OPERATIONS\nFor the initial node features used within the Geometric Transformer, we include each of DIPS-Plus’\nresidue-level features described succinctly in Section 3. Additionally, we append initial min-max\nnormalizations of each residue’s index in Pi to each node as node-wise positional encodings. For\n6\nPublished as a conference paper at ICLR 2022\nthe remainder of the Geometric Transformer’s operations, the network’s order of operations closely\nfollows the deﬁnitions given by Dwivedi & Bresson (2021) for the Graph Transformer, with an\nexception being that the ﬁrst normalization layer now precedes any afﬁnity score calculations.\n4.3 I NTERACTION MODULE\nUpon applying multiple layers of the Geometric Transformer to each pair of input protein chains,\nwe then channel-wise interleave the Geometric Transformer’s learned node representations hA and\nhB into I to serve as input to our interaction module, consisting of a dilated ResNet module adapted\nfrom Chen et al. (2021). The core residual network component in this interaction module consists\nof four residual blocks differing in the number of internal layers. Each residual block is comprised\nof several consecutive instance normalization layers and convolutional layers with 64 kernels of size\n3 ×3. The number of layers in each block represents the number of 2D convolution layers in the\ncorresponding component. The ﬁnal values of the last convolutional layer are added to the output\nof a shortcut block, which is a convolutional layer with 64 kernels of size 1 ×1. A squeeze-and-\nexcitation (SE) block (Hu et al. (2018)) is added at the end of each residual block to adaptively\nrecalibrate its channel-wise feature responses. Ultimately, the output of the interaction module is a\nprobability-valued A x B matrix that can be viewed as an inter-chain residue binding heatmap.\n5 E XPERIMENTS\n5.1 S ETUP\nFor all experiments conducted with DeepInteract, we used 2 layers of the graph neural network\nchosen for the experiment and 128 intermediate GNN and CNN channels to restrict the time required\nto train each model. For the Geometric Transformer, we used an edge geometric neighborhood of\nsize n = 2 for each edge such that each edge’s geometric features are updated by their 4-nearest\nincoming edges. In addition, we used the Adam optimizer (Kingma & Ba (2014)), a learning rate\nof 1e−3, a weight decay rate of 1e−2, a dropout (i.e., forget) rate of 0.2, and a batch size of 1. We\nalso employed 0.5-threshold gradient value clipping and stochastic weight averaging (Izmailov et al.\n(2018)). With an early-stopping patience period of 5 epochs, we observed most models converging\nafter approximately 30 training epochs on DIPS-Plus. For our loss function, we used weighted cross\nentropy with a positive class weight of 5 to help the network overcome the large class imbalance\npresent in interface prediction. All DeepInteract models employed 14 layers of our dilated ResNet\narchitecture described in Section 4.3 and had their top- kmetrics averaged over three separate runs,\neach with a different random seed (standard deviation of top- k metrics in parentheses). Prior to\nour experiments on the DB5 dataset’s 55 test complexes, we ﬁne-tuned each DeepInteract model\nusing the held-out 140 and 35 complexes remaining in DB5 for training and validation, respectively.\nEmploying a similar training conﬁguration as described above, in this context we used a lower\nlearning rate of 1e−5 to facilitate smoother transfer learning between DIPS-Plus and DB5.\n5.2 H YPERPARAMETER SEARCH\nTo identify our optimal set of model hyperparameters, we performed a manual hyperparameter\nsearch over the ranges of[1e−1,1e−2,1e−3,1e−4,1e−5,1e−6] and [1e−1,1e−2,1e−3,1e−4] for the\nlearning rate and weight decay rate, respectively. In doing so, we found a learning rate of1e−3 and a\nweight decay rate of 1e−2 to provide the lowest loss and the highest metric values on our DIPS-Plus\nvalidation dataset. We restricted our hyperparameter search to the learning rate and weight decay\nrate of our models due to the large computational and environmental costs associated with training\neach model. However, this suggests further improvements to our models could be found with a more\nextensive hyperparameter search over, for example, the models’ dropout rate.\n5.3 S ELECTION OF BASELINES\nWe considered the reproducibility and accessibility of a method to be the most important factors for\nits inclusion in our following benchmarks to encourage the adoption of accessible and transparent\nbenchmarks for future works. As such, we have included the methods BIPSPI (an XGBoost-based\nalgorithm) (Sanchez-Garcia et al. (2018)), DeepHomo (a CNN for homodimers) (Yan & Huang\n7\nPublished as a conference paper at ICLR 2022\nTable 1: The average top-kprecision on two types of DIPS-Plus test targets.\n16 (Homo) 16 (Hetero)\nMethod 10 L/10 L/5 10 L/10 L/5\nBI 0 0 0 0 .02 0 .02 0 .02\nDH 0.13 0 .12 0 .09\nCC 0.17 0 .16 0 .15\nDI (GCN) 0.22 (0.06) 0 .20 (0.07) 0 .18 (0.04) 0 .08 (0.01) 0 .08 (0.01) 0 .07 (0.02)\nDI (GT) 0.27 (0.06) 0 .24 (0.04) 0 .21 (0.04) 0 .10 (0.04) 0 .09 (0.04) 0 .08 (0.04)\nDI (GeoT w/o EPE) 0.28 (0.05) 0 .24 (0.01) 0 .23 (0.03) 0 .11 (0.05) 0 .10 (0.04) 0 .09 (0.03)\nDI (GeoT w/o GFG) 0.27 (0.08) 0 .24 (0.08) 0 .21 (0.08) 0 .10 (0.02) 0 .09 (0.02) 0 .09 (0.01)\nDI (GeoT) 0.25 (0.03) 0.25 (0.03) 0.23 (0.02) 0 .15 (0.04) 0 .14 (0.05) 0 .11 (0.04)\nTable 2: The average top-kprecision and recall on DIPS-Plus test targets of both types.\n32 (Both Types)\nMethod P@ 10 P@L/10 P@L/5 R@L R@L/2 R@L/5\nBI 0.01 0 .01 0 .01 0 .01 0 .004 0 .003\nDI (GCN) 0.15 (0.03) 0 .16 (0.01) 0 .12 (0.02) 0 .10 (0.02) 0 .06 (0.01) 0 .03 (0.003)\nDI (GT) 0.18 (0.05) 0 .16 (0.04) 0 .15 (0.04) 0 .13 (0.02) 0 .07 (0.01) 0 .04 (0.01)\nDI (GeoT w/o EPE) 0.19 (0.04) 0 .18 (0.03) 0 .16 (0.03) 0 .14 (0.02) 0 .08 (0.02) 0 .04 (0.02)\nDI (GeoT w/o GFG) 0.18 (0.05) 0 .16 (0.04) 0 .15 (0.04) 0 .14 (0.02) 0 .08 (0.02) 0 .04 (0.01)\nDI (GeoT) 0.20 (0.01) 0.19 (0.01) 0.17 (0.02) 0.15 (0.003) 0.09 (0.004) 0.04 (0.002)\n(2021)), and ComplexContact (a CNN for heterodimers) (Zeng et al. (2018)) since they are either\neasy to reproduce or simple for the general public to use to make predictions. Each method predicts\ninterfacing residue pairs subject to the (on average) 1:1000 positive-negative class imbalance im-\nposed by the biological sparsity of true interface contacts. We note that we also considered adding\nmore recent baseline methods such as those of Townshend et al. (2019) and Liu et al. (2020). How-\never, for both of these methods, we were not able to locate any provided source code or web server\npredictors facilitating the prediction of inter-protein residue-residue contacts for provided FASTA or\nPDB targets, so they ultimately did not meet our baseline selection criterion of reproducibility (e.g.,\nan ability to make new predictions). We also include two ablation studies (e.g., DI (GeoT w/o GFG))\nto showcase the effect of including network components unique to the Geometric Transformer.\nOur selection criterion for each baseline method consequently determined the number of complexes\nagainst which we could feasibly test each method, thereby restricting the size of our test datasets to\n106 complexes in total. In addition, not all baselines chosen were originally trained for both types\nof protein complexes (i.e., homodimers and heterodimers), so for these baselines we do not include\ntheir results for the type of complex for which they are not respectively designed.\nFor brevity, in all experiments, we refer to BIPSPI, DeepHomo, ComplexContact, and DeepInteract\nas BI, DH, CC, and DI, respectively. Further, we refer to the Graph Convolutional Network of\nKipf & Welling (2016), the Graph Transformer of Dwivedi & Bresson (2021), and the Geometric\nTransformer as GCN, GT, and GeoT, respectively. To assess models’ ability to correctly select\nresidue pairs in interaction upon binding of two given chains, all methods are scored using the\ntop-k precision and recall metrics (deﬁned in Section A.2) commonly used for intra-chain contact\nprediction (Chen et al. (2021)) as well as recommender systems (Jiang et al. (2020)), where k ∈\n{10,L/10,L/5,L/2}with Lbeing the length of the shortest chain in a given complex.\n5.4 D ISCUSSION\nTable 1 demonstrates that DeepInteract outperforms or achieves competitive results compared to ex-\nisting state-of-the-art methods for interface contact prediction on DIPS-Plus with both types of pro-\ntein complexes, homodimers (homo) where the two chains are of the same protein and heterodimers\n(hetero) where the two chains are of different proteins. Table 2 shows that, when taking both types\nof complexes into account, DeepInteract outperforms all other methods’ predictions on DIPS-Plus.\nSince future users of DeepInteract may want to predict interface contacts for either type of complex,\nwe consider a method’s type-averaged top-kmetrics as important metrics for which to optimize.\nLikewise, Tables 3 and 4 present the average top-kmetrics of DeepInteract on 19 challenging protein\ncomplexes (14 homodimers and 5 heterodimers) from the 13th and 14th rounds of the joint CASP-\nCAPRI meeting. In them, we once again see DeepInteract exceed the precision of state-of-the-art in-\n8\nPublished as a conference paper at ICLR 2022\nTable 3: The average top-kprecision on dimers from CASP-CAPRI 13 & 14.\n14 (Homo) 5 (Hetero)\nMethod 10 L/10 L/5 10 L/10 L/5\nBI 0 0 0 0 .04 0 0 .03\nDH 0.02 0 .02 0 .02\nCC 0.06 0 .08 0 .05\nDI (GCN) 0.12 (0.04) 0 .11 (0.03) 0.13 (0.02) 0 .10 (0.07) 0 .11 (0.08) 0 .09 (0.04)\nDI (GT) 0.08 (0.03) 0 .09 (0.05) 0 .08 (0.03) 0 .14 (0.02) 0 .14 (0.02) 0 .12 (0.03)\nDI (GeoT w/o EPE) 0.11 (0.01) 0 .12 (0.02) 0 .11 (0.01) 0 .18 (0.07) 0 .20 (0.09) 0 .18 (0.04)\nDI (GeoT w/o GFG) 0.10 (0.02) 0 .10 (0.02) 0 .09 (0.02) 0 .14 (0.03) 0 .17 (0.03) 0 .14 (0.02)\nDI (GeoT) 0.18 (0.05) 0.13 (0.03) 0 .11 (0.02) 0.30 (0.09) 0.31 (0.07) 0.24 (0.04)\nTable 4: The average top-kprecision and recall across all targets from CASP-CAPRI 13 & 14.\n19 (Both Types)\nMethod P@ 10 P@L/10 P@L/5 R@L R@L/2 R@L/5\nBI 0.01 0 0 .01 0 .02 0 .01 0 .001\nDI (GCN) 0.12 (0.04) 0 .10 (0.05) 0 .09 (0.04) 0 .11 (0.001) 0 .06 (0.01) 0 .02 (0.01)\nDI (GT) 0.10 (0.03) 0 .09 (0.03) 0 .08 (0.02) 0 .11 (0.02) 0 .06 (0.01) 0 .02 (0.01)\nDI (GeoT w/o EPE) 0.13 (0.02) 0 .14 (0.03) 0 .13 (0.02) 0 .12 (0.01) 0 .07 (0.01) 0 .03 (0.01)\nDI (GeoT w/o GFG) 0.11 (0.01) 0 .12 (0.02) 0 .10 (0.02) 0 .11 (0.01) 0 .06 (0.01) 0 .03 (0.01)\nDI (GeoT) 0.21 (0.01) 0.19 (0.01) 0.14 (0.01) 0.13 (0.02) 0.08 (0.01) 0.04 (0.003)\nTable 5: The average top-kprecision and recall on DB5 test targets.\n55 (Hetero)\nMethod P@ 10 P@L/10 P@L/5 R@L R@L/2 R@L/5\nBI 0 0 .002 0 .001 0 .003 0 .001 0 .0004\nCC 0.002 0 .003 0 .003 0 .007 0 .003 0 .001\nDI (GCN) 0.005 (0.002) 0 .006 (0.001) 0 .007 (0.001) 0 .013 (0.002) 0 .008 (0.001) 0 .003 (0.001)\nDI (GT) 0.008 (0.004) 0 .008 (0.005) 0 .008 (0.004) 0 .010 (0.005) 0 .006 (0.003) 0 .003 (0.002)\nDI (GeoT w/o EPE) 0.011 (0.004) 0 .009 (0.004) 0 .011 (0.002) 0 .018 (0.01) 0 .010 (0.004) 0 .0034 (0.002)\nDI (GeoT w/o GFG) 0.008 (0.001) 0 .008 (0.001) 0 .009 (0.002) 0 .014 (0.01) 0 .006 (0.002) 0 .003 (0.001)\nDI (GeoT) 0.013 (0.001) 0.009 (0.003) 0.011 (0.001) 0.018 (0.001) 0.010 (0.001) 0.0034 (0.001)\nterface contact predictors for both complex types. In particular, we see that combining DeepInteract\nwith the Geometric Transformer offers improvements to the majority of our top- kmetrics for both\nhomodimers and heterodimers compared to using either a GCN or a Graph Transformer-based GNN\nbackbone, notably for heteromeric complexes with largely asymmetric inter-chain geometries. Such\na result supports our hypothesis that the Geometric Transformer’s geometric self-attention mecha-\nnism can enable enhanced prediction performance for downstream tasks on geometrically-intricate\n3D objects such as protein structures, using interface contact prediction as a case study.\nFinally, in Table 5, we observe that, in predicting the interface contacts between unbound protein\nchains in the DB5 test dataset, the Geometric Transformer enables enhanced top- k precision and\nrecall (deﬁnition in A.2) compared to all other baseline methods, including GCNs and Graph Trans-\nformers paired with DeepInteract. Such as result conﬁrms, to a degree, the Geometric Transformer’s\nability to predict how the structural conformations occurring upon the binding of two protein chains\ninﬂuence which inter-chain residue pairs will interact with one another in the complex’sbound state.\n6 C ONCLUSION\nWe presented DeepInteract which introduces the geometry-evolving Geometric Transformer for pro-\ntein structures and demonstrates its effectiveness in predicting residue-residue interactions in protein\ncomplexes. We foresee several other uses of the Geometric Transformer in protein deep learning\nsuch as quaternary structure quality assessment and residue disorder prediction, to name a few. One\nlimitation of the Geometric Transformer’s current design is the high computational complexity as-\nsociated with its dot product self-attention mechanism, which we hope to overcome using efﬁcient\nalternatives to self-attention like that of the Nystr¨omformer (Xiong et al. (2021)).\n9\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nThe project is partially supported by two NSF grants (DBI 1759934 and IIS 1763246), one NIH grant\n(GM093123), three DOE grants (DE-SC0020400, DE-AR0001213, and DE-SC0021303), and the\ncomputing allocation on the Summit compute cluster provided by Oak Ridge Leadership Computing\nFacility (Contract No. DE-AC05-00OR22725).\nETHICS STATEMENT\nDeepInteract is designed to be used for machine learning of protein molecular data. It makes use of\nonly publicly available information concerning biomolecular structures and their interactions. Con-\nsequently, all data used to create DeepInteract models do not contain any personally identiﬁable\ninformation or offensive content. As such, we do not foresee negative societal impacts as a conse-\nquence of DeepInteract or the Geometric Transformer being made publicly available. Furthermore,\nfuture adaptions or enhancements to DeepInteract may beneﬁt the machine learning community and,\nmore broadly, the scientiﬁc community by providing meaningful reﬁnements to a transparent and\nextensible pipeline for geometric deep learning of protein-protein interactions.\nREPRODUCIBILITY STATEMENT\nTo enable this work to be reproducible by others, we have thoroughly documented two methods for\nrunning predictions with DeepInteract using a provided PyTorch Lightning model checkpoint (Fal-\ncon (2019)) and one method for training DeepInteract models in an associated GitHub repository.\nThe most convenient prediction method describes how to run DeepInteract as a platform-agnostic\nDocker container. The second method, for both training and prediction, details how users can piece-\nwise install DeepInteract and its data and software dependencies on a Linux-based operating system.\nREFERENCES\nShandar Ahmad and Kenji Mizuguchi. Partner-aware prediction of interacting residues in protein-\nprotein complexes from sequence data. PloS one, 6(12):e29104, 2011.\nMichael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Veli ˇckovi´c. Geometric deep learning:\nGrids, groups, graphs, geodesics, and gauges, 2021.\nChen Chen, Tianqi Wu, Zhiye Guo, and Jianlin Cheng. Combination of deep neural network with at-\ntention mechanism enhances the explainability of protein contact prediction. Proteins: Structure,\nFunction, and Bioinformatics , 89(6):697–707, 2021. doi: https://doi.org/10.1002/prot.26052.\nURL https://onlinelibrary.wiley.com/doi/abs/10.1002/prot.26052.\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\ndecoder with atrous separable convolution for semantic image segmentation. In Proceedings of\nthe European conference on computer vision (ECCV), pp. 801–818, 2018.\nTaco S. Cohen and Max Welling. Steerable cnns. CoRR, abs/1612.08498, 2016. URL http:\n//arxiv.org/abs/1612.08498.\nAllan Costa, Pranam Chatterjee, Manvitha Ponnapati, Suhas Bhat, Kalyan Palepu, Joseph Jacobson,\nand Iddo Drori. End-to-end euclidean equivariant transformers for protein docking. NeurIPS\nWorkshop on Learning Meaningful Representations of Life, 2021.\nBowen Dai and Chris Bailey-Kellogg. Protein Interaction Interface Region Prediction by Geometric\nDeep Learning. Bioinformatics, 03 2021. ISSN 1367-4803. doi: 10.1093/bioinformatics/btab154.\nURL https://doi.org/10.1093/bioinformatics/btab154. btab154.\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs,\n2021.\net al. Falcon, W A. Pytorch lightning. GitHub. Note: https://github.com/PyTorchLightning/pytorch-\nlightning, 3, 2019.\n10\nPublished as a conference paper at ICLR 2022\nAlex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph\nconvolutional networks. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett (eds.),Advances in Neural Information Processing Systems, volume 30,\npp. 6530–6539. Curran Associates, Inc., 2017. URL https://proceedings.neurips.\ncc/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf.\nFabian Fuchs, Daniel Worrall, V olker Fischer, and Max Welling. Se(3)-transformers: 3d roto-\ntranslation equivariant attention networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.\nBalcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33, pp.\n1970–1981. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/\npaper/2020/file/15231a7ce4ba789d13b722cc5c955834-Paper.pdf.\nPablo Gainza, Freyr Sverrisson, Frederico Monti, Emanuele Rodola, D Boscaini, MM Bronstein,\nand BE Correia. Deciphering interaction ﬁngerprints from protein molecular surfaces using geo-\nmetric deep learning. Nature Methods, 17(2):184–192, 2020.\nAlbert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gat-\ning mechanism of recurrent neural networks. In International Conference on Machine Learning,\npp. 3800–3809. PMLR, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.\n770–778, 2016.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 7132–7141, 2018.\nMd Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. Edge-augmented\ngraph transformers: Global self-attention is enough for graphs. arXiv preprint arXiv:2108.03348,\n2021.\nJohn Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for graph-\nbased protein design. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran As-\nsociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/\nf3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-\nson. Averaging weights leads to wider optima and better generalization. arXiv preprint\narXiv:1803.05407, 2018.\nJyun-Yu Jiang, Patrick H Chen, Cho-Jui Hsieh, and Wei Wang. Clustering and constructing user\ncoresets to accelerate large-scale top-k recommender systems. In Proceedings of The Web Con-\nference 2020, pp. 2177–2187, 2020.\nRafael A Jordan, EL-Manzalawy Yasser, Drena Dobbs, and Vasant Honavar. Predicting protein-\nprotein interface residues using local surface structural similarity. BMC bioinformatics, 13(1):\n1–14, 2012.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,\nKathryn Tunyasuvunakool, Russ Bates, Augustin ˇZ´ıdek, Anna Potapenko, et al. Highly accurate\nprotein structure prediction with alphafold. Nature, pp. 1–11, 2021.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nThomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907, 2016.\nMarc F Lensink, Guillaume Brysbaert, Nurul Nadzirin, Sameer Velankar, Rapha ¨el AG Chaleil,\nTereza Gerguri, Paul A Bates, Elodie Laine, Alessandra Carbone, Sergei Grudinin, et al. Blind\nprediction of homo-and hetero-protein complexes: The casp13-capri experiment.Proteins: Struc-\nture, Function, and Bioinformatics, 87(12):1200–1221, 2019.\n11\nPublished as a conference paper at ICLR 2022\nMarc F Lensink, Guillaume Brysbaert, Th´eo Mauri, Nurul Nadzirin, Sameer Velankar, Raphael AG\nChaleil, Tereza Clarence, Paul A Bates, Ren Kong, Bin Liu, et al. Prediction of protein as-\nsemblies, the next frontier: The casp14-capri experiment. Proteins: Structure, Function, and\nBioinformatics, 2021.\nXianggen Liu, Yunan Luo, Pengyong Li, Sen Song, and Jian Peng. Deep geometric representa-\ntions for modeling effects of mutations on protein-protein binding afﬁnity. PLoS computational\nbiology, 17(8):e1009284, 2021a.\nYi Liu, Hao Yuan, Lei Cai, and Shuiwang Ji. Deep learning of high-order interactions for protein\ninterface prediction. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, pp. 679–687, 2020.\nYi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message\npassing for 3d graph networks. arXiv preprint arXiv:2102.05013, 2021b.\nAlex Morehead, Chen Chen, Ada Sedova, and Jianlin Cheng. Dips-plus: The enhanced database of\ninteracting protein structures for interface prediction, 2021.\nYoichi Murakami, Lokesh P Tripathi, Philip Prathipati, and Kenji Mizuguchi. Network analysis and\nin silico prediction of protein–protein interactions with applications in drug discovery. Current\nopinion in structural biology, 44:134–142, 2017.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc,\nE. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32 , pp.\n8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nRuben Sanchez-Garcia, C O S Sorzano, J M Carazo, and Joan Segura. BIPSPI: a method for the\nprediction of partner-speciﬁc protein–protein interfaces.Bioinformatics, 35(3):470–477, 07 2018.\nISSN 1367-4803. doi: 10.1093/bioinformatics/bty647. URL https://doi.org/10.1093/\nbioinformatics/bty647.\nV´ıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural net-\nworks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con-\nference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp.\n9323–9332. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/\nsatorras21a.html.\nKristof T Sch¨utt, Farhad Arbabzadah, Stefan Chmiela, Klaus R M¨uller, and Alexandre Tkatchenko.\nQuantum-chemical insights from deep tensor neural networks.Nature communications, 8(1):1–8,\n2017.\nDavid Sehnal, Sebastian Bittrich, Mandar Deshpande, Radka Svobodov ´a, Karel Berka, V ´aclav\nBazgier, Sameer Velankar, Stephen K Burley, Jaroslav Ko ˇca, and Alexander S Rose. Mol*\nViewer: modern web app for 3D visualization and analysis of large biomolecular structures.\nNucleic Acids Research , 49(W1):W431–W437, 05 2021. ISSN 0305-1048. doi: 10.1093/nar/\ngkab314. URL https://doi.org/10.1093/nar/gkab314.\nMartin Steinegger, Markus Meier, Milot Mirdita, Harald V ¨ohringer, Stephan J Haunsberger, and\nJohannes S ¨oding. Hh-suite3 for fast remote homology detection and deep protein annotation.\nBMC bioinformatics, 20(1):1–15, 2019.\nRaphael Townshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-to-end learning on 3d protein\nstructure for interface prediction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,\nE. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems , volume 32,\npp. 15642–15651. Curran Associates, Inc., 2019. URLhttps://proceedings.neurips.\ncc/paper/2019/file/6c7de1f27f7de61a6daddfffbe05c058-Paper.pdf.\n12\nPublished as a conference paper at ICLR 2022\nGuido Van Rossum and Fred L. Drake. Python 3 Reference Manual . CreateSpace, Scotts Valley,\nCA, 2009. ISBN 1441412697.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.\nThom Vreven, Iain H Moal, Anna Vangone, Brian G Pierce, Panagiotis L Kastritis, Mieczyslaw\nTorchala, Raphael Chaleil, Brian Jim ´enez-Garc´ıa, Paul A Bates, Juan Fernandez-Recio, et al.\nUpdates to the integrated protein–protein interaction benchmarks: docking benchmark version 5\nand afﬁnity benchmark version 2. Journal of molecular biology, 427(19):3031–3041, 2015.\nJames A Wells and Christopher L McClendon. Reaching for high-hanging fruit in drug discovery at\nprotein–protein interfaces. Nature, 450(7172):1001–1009, 2007.\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and\nVikas Singh. Nystr\\” omformer: A nystr\\” om-based algorithm for approximating self-attention.\narXiv preprint arXiv:2102.03902, 2021.\nYumeng Yan and Sheng-You Huang. Accurate prediction of inter-protein residue–residue contacts\nfor homo-oligomeric protein complexes. Brieﬁngs in Bioinformatics, 2021.\nJianyi Yang, Ambrish Roy, and Yang Zhang. Protein–ligand binding site recognition using comple-\nmentary binding-speciﬁc substructure comparison and sequence proﬁle alignment. Bioinformat-\nics, 29(20):2588–2595, 2013.\nHong Zeng, Sheng Wang, Tianming Zhou, Feifeng Zhao, Xiufeng Li, Qing Wu, and Jinbo Xu.\nComplexcontact: a web server for inter-protein contact prediction using deep learning. Nucleic\nacids research, 46(W1):W432–W437, 2018.\n13\nPublished as a conference paper at ICLR 2022\nFigure 5: The network’s softmax contact probabilities (leftmost column), 0.5 positive probability-\nthresholded predictions (middle column), and ground-truth labels (rightmost column), respectively,\nfor PDB ID: 4HEQ (ﬁrst row) and 6TRI (second row), two of the complexes in our test datasets.\nA A PPENDIX\nA.1 S AMPLE INTERFACE CONTACT PREDICTIONS\nIn the ﬁrst row of Figure 5, we see predictions made by DeepInteract for a homodimer complex from\nour test partition of DIPS-Plus (i.e., PDB ID: 4HEQ). The leftmost image represents the softmax\ncontact probability map. The center image corresponds to the same contact map after having a 0.5\nprobability threshold applied to it such that residue pairs with at least a 50% probability of being\nin interaction with each other have their interaction probabilities rounded up to 1.0. The rightmost\nimage is the ground-truth contact map. Similarly, in the second row of Figure 5, we are shown\nthe cropped predictions made by DeepInteract for a CASP-CAPRI test heterodimer (i.e., PDB ID:\n6TRI).\nA.2 T OP-K TEST PRECISION AND RECALL OF BOTH COMPLEX TYPES IN DIPS-P LUS AND\nCASP-CAPRI\nFormally, our deﬁnitions of a model’s top-kprecision preck and recall reck, where Tposk represents\nthe number of true positive residue pairs selected from a model’s top-kmost probable pairs and Tpos\ncorresponds to the total number of true positive pairs in the complex, are\npreck = Tposk\nk (6)\nand\nreck = Tposk\nTpos\n. (7)\nAfter deﬁning top- krecall as such, in Tables 6 and 7 we provide the results of each model’s top- k\nrecall in the same set of experiments as given in Section 5.\nA.3 D EFINITION OF EDGE GEOMETRIC FEATURES\nSimilar to Ingraham et al. (2019), we construct a local reference frame (i.e., an orientation Oi) for\neach protein chain graph’s residues. Representing each residue by its Cartesian coordinates xi, we\nformally deﬁne\n14\nPublished as a conference paper at ICLR 2022\nTable 6: The average top-krecall on two types of DIPS-Plus test targets.\n16 (Homo) 16 (Hetero)\nMethod R@ L R@L/2 R@L/5 R@L R@L/2 R@L/5\nBI 0.01 0 0 0 .01 0 .01 0 .01\nDH 0.07 0 .04 0 .02\nCC 0.17 0 .12 0 .07\nDI (GCN) 0.14 (0.03) 0 .08 (0.01) 0 .04 (0.01) 0 .08 (0.02) 0 .05 (0.02) 0 .02 (0.01)\nDI (GT) 0.17 (0.01) 0 .10 (0.01) 0 .05 (0.01) 0 .09 (0.02) 0 .05 (0.02) 0 .03 (0.01)\nDI (GeoT w/o EPE) 0.18 (0.02) 0 .11 (0.01) 0 .05 (0.01) 0 .11 (0.03) 0 .07 (0.02) 0 .03 (0.02)\nDI (GeoT w/o GFG) 0.19 (0.04) 0 .11 (0.03) 0 .05 (0.02) 0 .09 (0.01) 0 .05 (0.02) 0 .03 (0.01)\nDI (GeoT) 0.19 (0.004) 0.12 (0.004) 0.06 (0.003) 0 .12 (0.003) 0 .07 (0.01) 0 .03 (0.01)\nTable 7: The average top-krecall on dimers from CASP-CAPRI 13 & 14.\n14 (Homo) 5 (Hetero)\nMethod R@ L R@L/2 R@L/5 R@L R@L/2 R@L/5\nBI 0.02 0 .01 0 0 .01 0 0\nDH 0.02 0 .01 0\nCC 0.03 0 .01 0 .01\nDI (GCN) 0.10 (0.01) 0 .07 (0.01) 0 .04 (0.02) 0 .08 (0.04) 0 .04 (0.02) 0 .02 (0.01)\nDI (GT) 0.10 (0.01) 0 .06 (0.01) 0 .02 (0.01) 0 .10 (0.01) 0 .05 (0.01) 0 .02 (0.01)\nDI (GeoT w/o EPE) 0.11 (0.01) 0 .07 (0.01) 0 .04 (0.01) 0 .12 (0.02) 0 .07 (0.01) 0 .03 (0.01)\nDI (GeoT w/o GFG) 0.10 (0.02) 0 .06 (0.01) 0 .03 (0.01) 0 .11 (0.02) 0 .07 (0.01) 0 .03 (0.01)\nDI (GeoT) 0.12 (0.03) 0.07 (0.01) 0.04 (0.01) 0.15 (0.02) 0.09 (0.01) 0.04 (0.01)\nui = xi −xi−1\n∥xi −xi−1∥, ni = ui ×ui+1\n∥ui ×ui+1∥, bi = ui −ui+1\n∥ui −ui+1∥. (8)\nwith ni being the unit vector normal to the plane formed by the rays ( xi−1 −xi) and (xi+1 −xi)\nand bi being the negative bisector of this plane. We then deﬁne Oi as\nOi = [bi ni bi ×ni]. (9)\nHaving deﬁned the orientation Oi for each residue that describes the local reference frame (xi, Oi).\nTo provide the Geometric Transformer with an alternative notion of residue-residue orientations, we\ndeﬁne the unit vector normal to the amide plane for residue ias\nUi = (xCαi −xCβi) ×(xCβi −xNi ) (10)\nwhere xCαi , xCβi, and xNi are the Cartesian coordinates of the residue’s carbon-alpha ( Cα),\ncarbon-beta (Cβ), and nitrogen (N) atoms, respectively.\nFinally, we relate the reference frames for residues iand j by describing their edge geometric fea-\ntures as\n(\nr(∥xj −xi∥), OT\ni\nxj −xi\n∥xj −xi∥, q(OT\ni Oj), a(Ui,Uj)\n)\n(11)\nwith the ﬁrst term r() being a distance encoding of 16 Gaussian RBFs spaced isotropically from\n0 to 20 ˚A, the second term describing the relative direction of xj with respect to reference frame\n(xi,Oi), the third term detailing an orientation encoding q() of the quaternion representation of the\nrotation matrix OT\ni Oj, representing each quaternion with respect to its vector of real coefﬁcients,\nand the fourth term a() representing the angle between the amide plane normal vectors Ui and Uj.\nOur deﬁnition of these edge geometric features makes use of the backbone atoms for each residue.\nAs such, the graph representation of protein chains we use with the Geometric Transformer encodes\nnot only residue-level geometric features but also those derived from an atomic view of protein\nstructures. We hypothesized this hybrid approach to modeling protein structure geometries would\nhave a noticeable downstream effect on interface contact prediction precision via the node and edge\nrepresentations learned by the Geometric Transformer. This hypothesis is conﬁrmed in Section 5.4.\n15\nPublished as a conference paper at ICLR 2022\nTable 8: The protein complexes selected from DIPS-Plus for testing interface contact predictors.\nPDB ID Chain 1 Chain 2 Type PDB ID Chain 1 Chain 2 Type\n1BHN B D Homo 1AON R S Hetero\n1KPT A B Homo 1BE3 D E Hetero\n1SDU A B Homo 1GK8 K M Hetero\n1UZN A B Homo 1OCZ R V Hetero\n2B4H A B Homo 1UW A A I Hetero\n2G30 C E Homo 3A6N A E Hetero\n2GLM E F Homo 3ABM D K Hetero\n2IUO D J Homo 3JRM H I Hetero\n3BXS A B Homo 3MG6 D E Hetero\n3CT7 B E Homo 3MNN C F Hetero\n3NUT A D Homo 3T1Y E H Hetero\n3RE3 B C Homo 3TUY D E Hetero\n4HEQ A B Homo 3VYG G H Hetero\n4LIW A B Homo 4A3D C L Hetero\n4OTA D F Homo 4CW7 G H Hetero\n4TO9 B D Homo 4DR5 G I Hetero\nTable 9: The CASP-CAPRI 13-14 protein complexes selected for testing interface contact predic-\ntors.\nPDB ID Chain 1 Chain 2 Type\n5W6L A B Homo\n6D2V A B Homo\n6E4B A B Homo\n6FXA C D Homo\n6HRH A B Homo\n6MXV A B Homo\n6N64 A B Homo\n6N91 A B Homo\n6NQ1 A B Homo\n6QEK A B Homo\n6UBL A B Homo\n6UK5 A B Homo\n6Y A2 A B Homo\n7CWP C D Homo\n6CP8 A C Hetero\n6D7Y A B Hetero\n6TRI A B Hetero\n6XOD A B Hetero\n7M5F A C Hetero\nA.4 P ROTEIN COMPLEXES SELECTED FOR TESTING\nTo facilitate reproducibility of the results presented in Section 5.4, Table 8 displays the PDB and\nchain IDs of DIPS-Plus protein complexes chosen for testing. Likewise, in Table 9, we provide the\nPDB and chain IDs of CASP-CAPRI 13-14 targets chosen for testing. These two tables describe\nprecisely which targets were selected and ultimately used in our RCSB-derived benchmarks. For\nfull data provenance, the targets we selected from the Docking Benchmark 5 dataset (Vreven et al.\n(2015)) for benchmarking are the same 55 protein heterodimers used for testing in works such as\nthat of Fout et al. (2017), Townshend et al. (2019), and Liu et al. (2020).\n16\nPublished as a conference paper at ICLR 2022\nA.5 I NVARIANCE OR EQUIVARIANCE ?\nIn our view, a natural question to ask concerning a deep learning architecture designed for a speciﬁc\ntask is whether equivariance to translations and rotations in R3 should be preferred over invariance\nto transformations in such a geometric space. The beneﬁts of employing equivariant representations\nin a deep learning architecture primarily include symmetry-preserving updates to type-1 tensors\nsuch as the coordinates representing an object in R3 and the derivation of invariant relative feature\nposes for type-0 features such as scalars (Cohen & Welling (2016)). However, equivariant represen-\ntations, particularly those derived with a self-attention mechanism, typically induce large memory\nrequirements for training and inference. In contrast, in the context of data domains such as ordered\nsets or proteins where there exists a canonical ordering of points, invariant representations may be\nadopted to simultaneously reduce memory requirements and provide many of the beneﬁts of using\nequivariant representations such as attaining these relative poses of type-0 features (Ingraham et al.\n(2019) and Jumper et al. (2021)). As such, in the context of the Geometric Transformer, we decided\nto pursue invariance over equivariance, to reduce the network’s effective memory requirements and\nto improve its learning efﬁciency and generalization capabilities (Bronstein et al. (2021)). How-\never, for applications such as protein-protein docking that may more directly rely on type-1 tensors\nfor network predictions (Costa et al. (2021)), designing one’s network architecture to preserve full\ntranslation and rotation equivariance in R3 is, in our perspective, a worthwhile research direction\nto pursue as many promising results on molecular datasets have already been demonstrated with\nequivariant neural networks such as SE(3)-Transformers (Fuchs et al. (2020)) and lightweight graph\narchitectures such as the Equivariant Graph Neural Network (Satorras et al. (2021)).\nA.6 R ATIONALE BEHIND THE NODE INITIALIZATION SCHEME\nDIPS-Plus residue-level features are initially embedded in our protein chain graphs to accelerate the\nnetwork’s training. However, we also initially append node-wise min-max positional encodings in\nour network’s operations. We do this to initialize the Geometric Transformer with information con-\ncerning the residue ordering of the chain’s underlying sequence as such ordering is important to un-\nderstanding downstream protein structural, interactional, and functional properties of each residue.\nA.7 R ATIONALE BEHIND THE EDGE INITIALIZATION MODULE ’S DESIGN\nFor the edge initializer module’s four protein geometric features, we sought to include enough geo-\nmetric information for the network to be able to uniquely determine the Euclidean positions of each\nnode’s neighboring nodes. For this reason, we adopt similar distance, direction, and orientation\ndescriptors as Ingraham et al. (2019). We concatenate the protein backbone-geometric features pro-\nvided by inter-residue distances, directions, and orientations with the angles between each residue\npair’s amide plane normal vectors. This is done ultimately to apply gating to each edges’ messages,\ndistances, directions, orientations, and amide angles separately to encourage the network to learn\nthe importance of speciﬁc channels in each of these input features. Gating is a technique that has\npreviously been shown to encourage neural networks to not become over-reliant on any particular\ninput feature (Gu et al. (2020)) and, as such, in the Geometric Transformer can be seen as a form of\nchannel-wise dropout for single feature sets. By also employing residual connections from original\nedge representations to gating-learned edge representations, the network module can operate more\nstably in the presence of multiple neural network layers (He et al. (2016)). Furthermore, in the edge\ninitialization module, we introduce edge-wise sinusoidal position encodings to provide the network\nwith a directional notion of residue-to-residue distances in protein chains’ underlying sequences.\nA.8 R ATIONALE BEHIND THE CONFORMATION MODULE ’S DESIGN\nThe conformation module’s design was inspired by SphereNet (Liu et al. (2021b)) and similar graph\nneural network architectures designed for learning on 3D graphs. What distinguishes our confor-\nmation module from the works of others is its introduction of the notion of 2n edge geometric\nneighborhoods when updating edge representations as well as its incorporation of geometric in-\nsights speciﬁc to large biomolecules such as proteins. Namely, by including the residue-residue\ndistances, residue-residue local reference frame directions and (quaternion) orientations, and amide\nplane-amide plane angles, the network is provided with enough information to ascertain the rela-\n17\nPublished as a conference paper at ICLR 2022\ntive coordinates of each neighboring residue from a given residue’s local reference frame (Liu et al.\n(2021b)), thereby ensuring the network’s capability of adequately learning from 3D structures.\nA.9 A LTERNATIVE NETWORKS WITHIN THE INTERACTION MODULE\nWe, like Liu et al. (2020), note that the task of interface prediction bears striking similarities to\ndense prediction tasks in computer vision (e.g., semantic segmentation). In this train of thought,\nwe experimented with several semantic segmentation models as replacements for our interaction\nmodule’s dilated ResNet, one namely being DeepLabV3Plus (Chen et al. (2018)). We observed\na strong propensity of such semantic segmentation models to identify interaction regions well but\nto do so with low pixel-wise precision. We hypothesize this is due to the downsampling and up-\nsampling methods often employed within such architectures that invariably degrade the original\ninput tensor’s representation resolution. We also experimented with several state-of-the-art Vision\nTransformer and MLP-based models for computer vision but ultimately found their algorithmic\ncomplexity, memory usage, or input shape requirements to be prohibitive for this task, since our test\ndatasets’ input protein complexes can vary greatly in size to contain between 20 residues and over\n2,000 residues in length. As such, for the design of DeepInteract’s interaction module, we experi-\nmented primarily with convolution-based architectures that do not employ such sampling techniques\nor pose limited input size constraints.\nA.10 H ARDWARE USED\nThe Oak Ridge Leadership Facility (OLCF) at the Oak Ridge National Laboratory (ORNL) is an\nopen science computing facility that supports HPC research. The OLCF houses the Summit compute\ncluster. Summit, launched in 2018, delivers 8 times the computational performance of Titan’s 18,688\nnodes, using only 4,608 nodes. Like Titan, Summit has a hybrid architecture, and each node contains\nmultiple IBM POWER9 CPUs and NVIDIA V olta GPUs all connected with NVIDIA’s high-speed\nNVLink. Each node has over half a terabyte of coherent memory (high bandwidth memory + DDR4)\naddressable by all CPUs and GPUs plus 800GB of non-volatile RAM that can be used as a burst\nbuffer or as extended memory. To provide a high rate of I/O throughput, the nodes are connected\nin a non-blocking fat-tree using a dual-rail Mellanox EDR InﬁniBand interconnect. We used the\nSummit compute cluster to train all our models.\nA.11 S OFTWARE USED\nIn addition, we used Python 3.8 (Van Rossum & Drake (2009)), PyTorch 1.7.1 (Paszke et al.\n(2019)), and PyTorch Lightning 1.4.8 (Falcon (2019)) to run our deep learning experiments.\nPyTorch Lightning was used to facilitate model checkpointing, metrics reporting, and dis-\ntributed data parallelism across 72 Tesla V100 GPUs. A more in-depth description of the\nsoftware environment used to train and predict with DeepInteract models can be found at\nhttps://github.com/BioinfoMachineLearning/DeepInteract.\n18",
  "topic": "CASP",
  "concepts": [
    {
      "name": "CASP",
      "score": 0.8740297555923462
    },
    {
      "name": "Computer science",
      "score": 0.6413506269454956
    },
    {
      "name": "Transformer",
      "score": 0.6216429471969604
    },
    {
      "name": "Protein structure prediction",
      "score": 0.48375701904296875
    },
    {
      "name": "Docking (animal)",
      "score": 0.44076213240623474
    },
    {
      "name": "Protein structure",
      "score": 0.40392133593559265
    },
    {
      "name": "Algorithm",
      "score": 0.4003335237503052
    },
    {
      "name": "Computational science",
      "score": 0.330214262008667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32958006858825684
    },
    {
      "name": "Electrical engineering",
      "score": 0.16834118962287903
    },
    {
      "name": "Engineering",
      "score": 0.150558739900589
    },
    {
      "name": "Voltage",
      "score": 0.14415106177330017
    },
    {
      "name": "Chemistry",
      "score": 0.13665363192558289
    },
    {
      "name": "Nursing",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76835614",
      "name": "University of Missouri",
      "country": "US"
    }
  ]
}