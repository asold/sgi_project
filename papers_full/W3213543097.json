{
  "title": "Discovering Explanatory Sentences in Legal Case Decisions Using Pre-trained Language Models",
  "url": "https://openalex.org/W3213543097",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5077762937",
      "name": "Jaromír Šavelka",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101664770",
      "name": "Kevin D. Ashley",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963105309",
    "https://openalex.org/W2964328740",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3092952717",
    "https://openalex.org/W4200270611",
    "https://openalex.org/W4301193485",
    "https://openalex.org/W3097295829",
    "https://openalex.org/W642225766",
    "https://openalex.org/W218952541",
    "https://openalex.org/W2984469754",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2242998196",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3173533009",
    "https://openalex.org/W3108764574",
    "https://openalex.org/W4248744670",
    "https://openalex.org/W3015773172",
    "https://openalex.org/W2962854673",
    "https://openalex.org/W136732505",
    "https://openalex.org/W2510307911",
    "https://openalex.org/W2971209824",
    "https://openalex.org/W3013904071",
    "https://openalex.org/W2923890923",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3108610837",
    "https://openalex.org/W2169511760",
    "https://openalex.org/W3116474736",
    "https://openalex.org/W2963656255",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W4241727697",
    "https://openalex.org/W3118038702",
    "https://openalex.org/W3134665270",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2973832122",
    "https://openalex.org/W1565746575",
    "https://openalex.org/W1974758710",
    "https://openalex.org/W3180395890",
    "https://openalex.org/W2155482025",
    "https://openalex.org/W3003740546",
    "https://openalex.org/W4226160860",
    "https://openalex.org/W2982596739",
    "https://openalex.org/W2121044470"
  ],
  "abstract": "Legal texts routinely use concepts that are difficult to understand. Lawyers elaborate on the meaning of such concepts by, among other things, carefully investigating how they have been used in the past. Finding text snippets that mention a particular concept in a useful way is tedious, time-consuming, and hence expensive. We assembled a data set of 26,959 sentences, coming from legal case decisions, and labeled them in terms of their usefulness for explaining selected legal concepts. Using the dataset we study the effectiveness of transformer models pre-trained on large language corpora to detect which of the sentences are useful. In light of models’ predictions, we analyze various linguistic properties of the explanatory sentences as well as their relationship to the legal concept that needs to be explained. We show that the transformer-based models are capable of learning surprisingly sophisticated features and outperform the prior approaches to the task.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4273–4283\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n4273\nDiscovering Explanatory Sentences in Legal Case Decisions Using\nPre-trained Language Modelss\nJaromir Savelka\nSchool of Computer Science\nCarnegie Mellon University\njsavelka@cs.cmu.edu\nKevin D. Ashley\nSchool of Law\nUniversity of Pittsburgh\nashley@pitt.edu\nAbstract\nLegal texts routinely use concepts that are dif-\nﬁcult to understand. Lawyers elaborate on the\nmeaning of such concepts by, among other\nthings, carefully investigating how have they\nbeen used in past. Finding text snippets that\nmention a particular concept in a useful way\nis tedious, time-consuming, and, hence, ex-\npensive. We assembled a data set of 26,959\nsentences, coming from legal case decisions,\nand labeled them in terms of their usefulness\nfor explaining selected legal concepts. Us-\ning the dataset we study the effectiveness of\ntransformer-based models pre-trained on large\nlanguage corpora to detect which of the sen-\ntences are useful. In light of models’ predic-\ntions, we analyze various linguistic properties\nof the explanatory sentences as well as their\nrelationship to the legal concept that needs to\nbe explained. We show that the transformer-\nbased models are capable of learning surpris-\ningly sophisticated features and outperform\nthe prior approaches to the task.\n1 Introduction\nWritten laws enacted by legislative bodies set forth\nthe collection of legally binding rules of conduct\n(e.g., rights, prohibitions, duties). Understand-\ning written laws is difﬁcult because the abstract\nrules must account for a variety of situations, even\nthose not yet encountered. Written laws communi-\ncate general standards and refer to classes of per-\nsons, acts, things, and circumstances (Hart, 1994,\np. 124). Therefore, legislators use vague (Endicott,\n2000), open textured (Hart, 1994) terms, abstract\nstandards (Endicott, 2014), principles, and values\n(Daci, 2010) to deal with the inherent uncertainty.\nFor example, let us focus on the two emphasized\nconcepts from the following written provision of\nlaw (29 U.S. Code § 203):\n“Enterprise” means the related activities per-\nformed [. . . ] for a common business purpose\n[. . . ].\nUnderstanding of the provision depends on un-\nderstanding the meaning of the two emphasized\nconcepts. Any doubts about the meaning may be\nremoved by explanation or interpretation (Mac-\nCormick and Summers, 1991). Even small dif-\nferences in understanding of a single concept may\nbe crucial for determining how a provision applies\nand what are its effects in a particular context.\nFor example, the meaning of the concept com-\nmon business purpose could be crucial in determin-\ning if two restaurants in different parts of the same\ncity, sharing a single owner, constitute an “enter-\nprise.” The explanation of the concept would in-\nvolve an investigation of how has it been referred to,\nexplained, interpreted, or applied in the past. This\nis an important step that enables a lawyer to come\nup with arguments in support of or against partic-\nular accounts of meaning (Šavelka and Harašta,\n2015; Savelka and Ashley, 2021).\nSearching through a database of legal documents\na lawyer may retrieve sentences such as the follow-\ning:\n1. Courts have held that a joint proﬁt motive is insufﬁcient\nto support a ﬁnding of common business purpose.\n2. The fact of common ownership of the two businesses\nclearly is not sufﬁcient to establish a common business\npurpose.\n3. The third test is “ common business purpose.”\nSome of these sentences are most likely useful for\nexplaining the concept (1 and 2) but others appear\nto have very little value (3). Manually reviewing\nsuch sentences is labor intensive.\nWe would like to rank more highly the sentences\nthe goal or effect of which is to elaborate upon the\nmeaning of the selected concept. These include,\nbut are not limited to, (i) deﬁnitional sentences\n(e.g., a sentence that provides a test for when the\nconcept applies), (ii) sentences that state explicitly\nin a different way what the concept means or state\nwhat it does not mean, (iii) sentences that provide\n4274\nan example, instance, or counterexample of the\nconcept, and (iv) sentences that show how a court\ndetermines whether something is such an example,\ninstance, or counterexample.\n2 Related and Prior Work\nIn prior work, we employed a variety of tradi-\ntional information retrieval (IR) measures and their\ncombinations, e.g., BM25, novelty, topic model-\ning (Savelka et al., 2019; Savelka, 2020; Šavelka\nand Ashley, 2021). These turned out to be remark-\nably successful in ﬁnding documents or their parts\n(e.g., paragraphs) that are likely to contain useful\nsentences. However, they fell short in perform-\ning ﬁner-grained evaluation of the sentences con-\ntained in those document parts. Using a learning-\nto-rank approaches on hand-crafted features led to\nonly moderate improvements (Šavelka and Ashley,\n2016; Savelka and Ashley, 2020). In this work,\nwe show that transformer-based pre-trained models\n(BERT family) are capable of such ﬁne-grained\nevaluation by learning to detect sophisticated se-\nmantic features in sentences themselves as well\nas in their relationships to the explained concepts.\nFurthermore, we show that many of these features\nare sensible to humans.\nThe models based on the BERT architecture have\nbeen successfully used in a variety of IR tasks. A\ncomprehensive survey of text ranking with trans-\nformers, such as BERT, is provided in (Lin et al.,\n2020). Several simple applications of BERT to\nad hoc document retrieval are presented in (Yang\net al., 2019). Successful applications of BERT for\nretrieval of short texts such as sentences are pre-\nsented in Yilmaz et al. (2019) and Rao et al. (2019).\nSimilar to the utilization of provisions of written\nlaw in this work, the authors of Mehrotra and Yates\n(2019) demonstrated the effectiveness of using a\nquery context in a re-ranking component based on\nBERT. In Nogueira et al. (2019) BERT is ﬁne-tuned\non query-retrieved document pairs as is done in this\nwork.\nThere are examples of successful applications of\nBERT on legal texts as well. A task of retrieving\nrelated case-law similar to a case decision a user\nprovides is tackled in Rossi and Kanoulas (2019).\nBERT was also proposed as one of the approaches\nto predict court decision outcomes given the facts\nof a case (Chalkidis et al., 2019). BERT has been\nsuccessfully used for classiﬁcation of legal areas\nof Supreme Court judgments (Howe et al., 2019).\nBERT was used to tackle the challenging task of\ncase law entailment (Rabelo et al., 2019; Wester-\nmann et al., 2020). BERT was also used in learning-\nto-rank settings, as is done in this work, for retrieval\nof legal news (Sanchez et al., 2020). Systematic\ninvestigation of BERT’s adaptation to the legal do-\nmain, resulting in a release of several legal-BERT\nmodels, was performed in (Chalkidis et al., 2020).\nRoBERTa (variation of BERT) model was used for\nclassiﬁcation of legal principles applied in court\ncase decisions (Gretok et al., 2020). The ability\nof pre-trained language models (RoBERTa) to gen-\neralize beyond the legal domain and dataset they\nwere trained on was analyzed in (Šavelka et al.,\n2020).\n3 Data Set\nWe downloaded the complete bulk data from the\nCaselaw access project1 which includes all ofﬁcial,\nbook-published U. S. cases from all federal and\nstate courts as well as from a number of territo-\nrial courts (President and of Harvard University,\n2018). The dataset comprises more than 6.7 mil-\nlion unique cases. For document indexing we used\na lemmatizer based on the so-called induced ripple-\ndown rules (Juršic et al., 2010). 2 Using the U.S.\ncase law sentence segmenter (Savelka et al., 2017)\nwe divided each case into individual sentences (0.8\nbillion).\nWe queried the system for sentences mentioning\n42 selected legal concepts (i.e., terms/phrases, such\nas “audiovisual work,” or “electronic signature”)\ncoming from provisions of the U.S. Code (the ofﬁ-\ncial collection of federal statutes).3 Given the con-\nstraints imposed by available resources, we made\nthe best effort to create a well-balanced dataset cov-\nering 20 different areas of legal regulation (26,959\nretrieved sentences in total).\nEleven law students classiﬁed the sentences in\nterms of four categories with respect to their utility\nfor explaining the legal concepts:\n1. High value – This category is reserved for\nsentences the goal of which is to elaborate on\nthe meaning of the concept.\n2. Certain value – Sentences that provide\n1A small portion of the dataset is available at case.law.\nThe complete dataset could be obtained upon entering into\nresearch agreement with LexisNexis.\n2http://lemmatise.ijs.si\n3https://www.law.cornell.edu/uscode/\ntext/\n4275\nFigure 1: The graph on the left shows the distribution\nof the labels across all the sentences retrieved for the\n42 selected concepts. The graph on the right presents\nthe distribution of the number of sentences retrieved for\neach concept.\ngrounds to draw some conclusions about the\nmeaning of the concept.\n3. Potential value– Sentences that provide ad-\nditional information over what is known from\nthe provision of law.\n4. No value– Sentences that do not provide any\nadditional information over what is known\nfrom the provision.\nAnnotators needed to be properly trained to deal\nwith this challenging task. We adopted multiple\nmeasures to ensure the annotations of the resulting\ndataset are of high-quality. The most important\none was a second-pass annotation performed by\ntwo annotators with a completed law degree (α=\n0.79).4\nFigure 1 shows the overall distribution of the\nlabels and the number of sentences associated with\neach concept/query. The Figure shows that the less\nvaluable categories, ‘no value’ and ‘potential value,’\nare dominant. For all the “larger” queries and al-\nmost all the “small” queries it holds that either the\n‘no value’ or the ‘potential value’ category is the\nmost numerous one. No matter the size, it is still\nthe case that some of the terms contain quite a con-\nsiderable number of more valuable sentences (e.g.,\n“audiovisual work” or “switchblade knife”) while\nothers are signiﬁcantly more limited in this respect\n(e.g., “essential step” or “hazardous liquid”). As\nthe dataset has not yet been released to the public\nwe are making it available with this paper.5\n4To measure inter-annotator agreement we used Krippen-\ndorff’sα (Krippendorff, 2011).\n5https://github.com/jsavelka/\nstatutory_interpretation\n4 Experiments\nIn this work, we use RoBERTa—a robustly op-\ntimized BERT pretraining approach (Liu et al.,\n2019)—as the starting point for the rankers. 6 Out\nof the available language models we chose to work\nwith the smaller roberta.base model that has 125\nmillion parameters. This choice was motivated by\nthe ability to iterate the experiments faster when\ncompared to working with roberta.large with 355\nmillion parameters.\nWe experiment with three different setups. In\nthe ﬁrst setup we ﬁne-tuned the base RoBERTa\nmodel on the task of classifying retrieved sentences\nin terms of their value for explaining the legal con-\ncepts. In prediction we then applied the model to\nclassify the sentences, that were not seen during\nﬁne-tuning, in terms of the four value categories\n(see Section 3). By applying softmax to the ﬁnal\nprediction layer we obtained the probability dis-\ntribution over the four possible classes. To obtain\nthe sentence’s score we compute an inner product\nbetween the class probability distribution and value\nweight vector (0,1,2,3)T . The motivation to use\nthe approach over considering only the predicted\nclass is to take into account the conﬁdence of the\nprediction. Henceforth, this model is referred to\nas BERT snt because it infers the usefulness of a\nsentence for explaining a legal concept from the\nsentence only.\nIn the second setup we ﬁne-tune the base\nRoBERTa model on the sentence pair classiﬁca-\ntion task. The model is provided a legal concept\nin place of the ﬁrst sentence and a retrieved sen-\ntence as the second one. The task of predicting\nsentence value is thus recast as predicting the re-\nlationship between the concept and the retrieved\nsentence. The goal is still to predict one of the four\nsentence value labels. As in the previous setup,\nwe applied softmax to the ﬁnal prediction layer to\nobtain a probability distribution over the classes.\nSentences’ scores are determined in the same way\nas well. Henceforth, this model is referred to as\nBERT qry2snt.\nThe third setup is similar to the second one. Here,\nwe again ﬁne-tune the base RoBERTa model on the\nsentence pair classiﬁcation task. Unlike in the sec-\nond setup, the model is ﬁne-tuned on the whole\nprovision of written law as the ﬁrst sentence and\nthe retrieved sentence as the second one. There-\n6https://github.com/pytorch/fairseq/\ntree/master/examples/roberta\n4276\nfore, in this experiment the task is understood as\nprediction of the relationship between the provision\nof law and the retrieved sentence. As in the two\nprevious experiments, softmax is applied to the ﬁ-\nnal prediction layer and the probability distribution\nover the classes is obtained. Henceforth, this model\nis referred to as BERT sp2snt.\nIn all the experiments, we ﬁne-tuned the base\nRoBERTa model, with a linear classiﬁcation layer\non top of the pooled output, for 10 epochs on the\ntraining splits of the selected datasets. We used the\nbatch size of 8 which is the maximum allowed by\nour hardware setup (1080Ti with 11GB) given we\nset the length of a sequence to 512 (maximum). As\noptimizer we use the Adam algorithm (Kingma and\nBa, 2014) with initial learning rate set to 4e−5. We\nstored models’ checkpoints after the end of each\ntraining epoch. The checkpoints are evaluated on\nthe validation set (see Section 4.1 for details). The\nmodel with the highest F1 on the validation set was\nthen selected as the one to make predictions on the\ntest sets.\n4.1 Evaluation\nSince the notion of relevance in this work is non-\nbinary, we use normalized discounted cumula-\ntive gain ( NDCG) to evaluate the performance\nof different approaches. An output of the pre-\nsented ranking algorithms for each concept/query\nqj has the form of an ordered tuple of sentences\nSj = (s1,s2,...,s n). We chose to evaluate the\nrankings at k= 10and 100 which means that the\ntuples produced by the algorithms are truncated to\nthe respective lengths. Note that the chosen values\nof kare higher than typical. Measuring at k= 100\nmay even appear somewhat extreme. However,\nlegal search differs from the general web search.\nAssuming a lawyer has conﬁdence in the query\n(based on seeing several relevant hits towards the\ntop of the results’ list), he or she might be inclined\nto inspect the results way beyond what would a\ntypical web search user do. For each query qj the\nNDCG at each kis then computed as:\nNDCG(Sj,k) = 1\nZjk\nk∑\ni=1\nrel(si)\nlog2(i+ 1)\nThe function rel(si) takes a sentence as an input\nand outputs its value in a numerical form. It is\ndeﬁned as follows:\nrel(si) =\n\n\n\n3 if si has high value\n2 if si has certain value\n1 if si has potential value\n0 if si has no value\nZjk is a normalizing quantity which is equal to\nNDCG(Sj,k) where Sj is the ideal ranking. In\nour case this would mean that all the si with ‘high\nvalue’ labels are at the beginning positions of the tu-\nple, followed by those with the ‘certain value,’ then\n‘potential value,’ and ﬁnally ‘no value’ sentences.\nWe used stratiﬁed sampling to distribute the\nqueries into six folds. There are many dimensions\nalong which the result lists associated with the in-\ndividual queries could be assessed. Two very im-\nportant ones are the size of the list (i.e., the number\nof retrieved sentences) and its richness. Richness\nis a term often used in technology assisted review\nin eDiscovery. It refers to the prevalence of respon-\nsive documents in a collection (result list in case\nof this work). We adapted the idea for this work\nby deﬁning a measure that describes the prevalence\nof valuable sentences in the dataset. First, we as-\nsigned a value to a sentence si depending on its\nlabel on a scale from 0 to 10:\nval(si) =\n\n\n\n10 if si has ‘high value’\n5 if si has ‘certain value’\n1 if si has ‘potential value’\n0 if si has ‘no value’\nThe reason why we used the scale of 0 to 10 is\nto overcome the dominance of the less valuable\nsentences. It is important to emphasize that these\nscores do not reﬂect the value ratio among sen-\ntences with different labels. In order to determine\nthe richness (R) of a concept/query qj we simply\ncomputed an average value of the sentence within\na results list associated with the concept/query:\nR(qj) = 1\nn\nn∑\ni=1\nval(si)\nQueries with over 550 retrieved sentences are\ndeemed large whereas the rest is considered small.\nFigure 1 (right) shows that this is where the long\ntail starts. For richness, we chose 2.0 as a cut-off\nscore. The sentences that fall below are dominated\nby low value sentences. The sentences that fall\n4277\nabove are quite rich in higher value sentences. This\nresulted in the four groups, i.e., small sparse (12\nqueries), small dense (18), large sparse (6), and\nlarge dense (6). Each of the six splits then contain\n2 SmSp, 3 SmDs, 1 LgSp, and 1 LgDs sentences.\nAll the systems are then evaluated using 6-fold\ncross-validation. In each iteration four folds are\nused as a training set, one as a validation set,\nand one as a test set. We obtained two scores\n(NDCG@10 and NDCG@100) for each of the 42\nconcepts/queries. We report the unweighted means\n(i.e., the size of the result list is not taken into ac-\ncount) of each score for the four groups determined\nby the stratiﬁed sampling as well as the Overall\nperformance.\nFor testing statistical signiﬁcance we employ the\nstrategy suggested by (Demšar, 2006) for testing\nk methods applied to N datasets. In our exper-\niments, we use the NDCG@100 of the Overall\ngroup as the evaluation metric to determine statisti-\ncal signiﬁcance. Demšar (2006) recommends the\nFriedman test (Friedman, 1937), a non-parametric\nequivalent of the repeated-measures ANOV A. The\nnull-hypothesis states that all the methods (i.e., the\nassessed ranker and the baselines) are equivalent.\nIn case the null-hypothesis is rejected, we can draw\na conclusion that some methods do differ. In or-\nder to learn which of them are different, a post-\nhoc test needs to be conducted. We use the Holm-\nBonferroni step down method (Holm, 1979) where\nthe comparisons are performed in sequential order\nfrom the most signiﬁcant hypotheses until a null-\nhypothesis that cannot be rejected is encountered.\n4.2 Baselines\nAs baselines we report the performance of a Ran-\ndom system on a large sample of repeated runs\n(for reference) as well as two simple methods\nbased on BM25. The ﬁrst method is the Okapi\nBM25 function (Robertson and Zaragoza, 2009)\napplied to query-sentence pairs. The second BM25-\nbased baseline (BM25-c) is a linear interpolation\nof BM25 applied to the query-sentence pair (s) and\nto the whole provision of written law-whole case\ndecision pair (c) it comes from (context). The two\nBM25 baselines are very close to what is typically\nused in many legal IR systems. Furthermore, they\nare very effective baselines that are often not easy\nto outperform. For comparison, we also report the\nperformance of the best systems from the prior\nwork. (Savelka et al., 2019; Savelka and Ashley,\n2020; Šavelka and Ashley, 2021)\n5 Results\nThe results of the experiments described in Sec-\ntion 4 are reported in Table 1 (group and over-\nall means). The top section of the table presents\nthe performance of the three baselines. The two\nBM25 baselines clearly outperform the Random\nsystem. Despite their similar performance they\nbeneﬁt from completely different phenomena. In-\ntuitively, BM25 ranks high sentences that contain\nmultiple mentions of the concept. In this work the\nmethod is optimized in such a way that the docu-\nments are not penalized for their length. Hence, the\nsystem would often prefer very long sentences. Ob-\nviously, such a simple approach works to a certain\nextent. BM25-c is a combination (linear) of the\nplain BM25 and another BM25 measure applied to\nthe whole text of a case (i.e., sentence’s context).\nHence, this system can additionally use the fact\nof the concept appearing many times within the\nwhole text. This is useful because a decision that\nmentions the term many times is more likely to con-\ntain useful sentences than a decision that mentions\nit just once. Apparently, the BM25-c is the most\ncompetitive of the three baselines.\nThe middle section of Table 1 shows the per-\nformance of the two best models from prior\nwork (Savelka et al., 2019; Savelka, 2020). The\nBMp+NW+LDA is a linear combination of BM25\napplied on a paragraph level, novelty measure, and\ntopic similarity measure. The RF-PWT is the ran-\ndom forest model trained on the 161 hand-crafted\nfeatures proposed in Savelka (2020); Savelka and\nAshley (2020). These models appear to be an im-\nprovement over the two baselines.\nThe performance of the methods based on the\npre-trained language models is very promising.\nEven the performance of the model that considers\njust the sentence itself (BERT snt) and completely\nignores the legal concept or the source provision\nshows promise. The statistical evaluation corrob-\norates the summary statistics reported in Table 1.\nThe strongest conclusion as to outperforming the\ntwo BM25 and the Random baseline can be reached\nfor the BERT sp2snt model (p = 0.0002). While\nfor the BERT qry2snt (p = 0.012) and BERT snt\n(p= 0.022) models the conclusion is not as strong,\nit still solidly supports the ﬁnding (especially con-\nsidering the relatively limited size of the dataset).\nThe models also appear to improve over the prior\n4278\nTable 1: The table shows the results of the experiments with pre-trained language models. The NDCG@10 and\nNDCG@100 are shown for the small sparse queries (SmSp), small dense queries (SmDs), large sparse queries\n(LgSp), large dense queries (LgDs), and all of them together (Overall).\nSmSp SmDs LgSp LgDs Overall\nMethod @10 @100 @10 @100 @10 @100 @10 @100 @10 @100\nRandom .38 ± .10 .67 ± .15 .52 ± .07 .76 ± .09 .25 ± .16 .29 ± .18 .47 ± .09 .48 ± .09 .43 ± .13 .63 ± .21\nBM25 .47 ± .13 .74 ± .11 .60 ± .18 .79 ± .11 .44 ± .21 .37 ± .22 .61 ± .17 .56 ± .12 .54 ± .18 .68 ± .20\nBM25-c .48 ± .12 .76 ± .09 .59 ± .17 .80 ± .11 .49 ± .14 .42 ± .17 .63 ± .19 .55 ± .13 .55 ± .16 .70 ± .18\nBMp+NW+LDA .55 ± .11 .78 ± .12 .64 ± .14 .82 ± .10 .58 ± .16 .56 ± .02 .65 ± .23 .62 ± .11 .61 ± .15 .74 ± .14\nRF-PWT .60 ± .16 .81 ± .11 .66 ± .12 .83 ± .10 .71 ± .17 .68 ± .08 .67 ± .10 .64 ± .09 .65 ± .14 .77 ± .12\nBERT snt .50 ± .18 .71 ± .17 .61 ± .14 .80 ± .11 .46 ± .24 .47 ± .21 .83 ± .15 .77 ± .12 .59 ± .20 .72 ± .18\nBERT qry2snt .59 ± .23 .76 ± .19 .72 ± .18 .85 ± .10 .64 ± .34 .50 ± .28 .86 ± .25 .77 ± .18 .69 ± .24 .77 ± .20\nBERT sp2snt .57 ± .19 .80 ± .12 .74 ± .15 .87 ± .07 .73 ± .12 .59 ± .18 .89 ± .16 .80 ± .14 .71 ± .19 .80 ± .14\nstate-of-the-art.\n6 Discussion\nWhile it should be apparent that the sentence de-\ntached from the legal concept (and the provision\nof law it is embedded in) does not provide reli-\nable grounds for determining its value, it appears\nthat the sentences themselves do carry some signal.\nThis is evidenced by the performance of the BERT\nsnt model that only considers sentences themselves.\nThis model outperforms the BM25-based baselines.\nInterestingly, the system correctly recognized that\nvery short pieces of text that do not form full gram-\nmatical sentences typically have very little value.\nFor example, the following sentences have been\nplaced at the bottom of their respective rankings\n(the explained context is highlighted in yellow):\nCommunication & Navigation Equipment\n[No value]\nB. Non-Disclosure of PreExisting Works\n[No value]\nFurthermore, it appears that the system relies on\nfeatures such as the presence of numbering in the\nsentence, complicated sentence structures, abrupt\nendings or starts of the sentences, and references,\nto recognize quotations of written provisions of law\nand assign a low value to such sentences:\nDerives independent economic value, actual or\npotential, from not being generally known to the\npublic or to other persons who can obtain eco-\nnomic value from its disclosure or use; and [f] (2)\n[No value]\nThis strategy makes sense in general. The quotation\ncould either be the citation of the source provision\n(‘no value’) or a citation of a different provision\n(high chance of different meaning and hence lower\nvalue of a sentence). However, there are situations\nin which the strategy does not work well.\nThe Electronic Communications Privacy Act of\n1986 (ECPA), Pub. L. 99-508, §101(a)(6)(C), 100\nStat. 1848, 1849 (1986), codiﬁed, as amended, at\n18 U.S.C. §2510(18) (1986), deﬁnes “aural trans-\nfer” to mean “a transfer containing the human\nvoice at any point between and including the point\nof origin and the point of reception.”\n[High value]\nThe ‘aural transfer’ is a rare example of a concept\nfor which there is a legal deﬁnition. As a result\nBERT snt underperforms the Random baseline on\nthis particular concept (NDGC@100 0.53 vs 0.62).\nBERT snt also seems to have developed a certain\ntendency to rank high sentences where something\nis claimed to be something else:\nScreen output is considered an audiovisual work\nthat falls within the subject matter of copyright.\n[High value]\nThis also appears to be a good strategy that works\nwell many times but not always. For example, the\nfollowing sentence is just ‘potential value’ because\nit uses “navigation equipment” in a different mean-\ning (avionics instead of seafaring):\nAvionics are aircraft radios and navigation equip-\nment.\n[Potential value]\nThe above examples demonstrate how the pre-\ntrained deep architecture detects very complex fea-\ntures. It would be quite difﬁcult for a human expert\nto hand-craft such features. While it is not difﬁcult\nto come up with features such as sentence length,\nit is far more difﬁcult to come up with features\ncapturing complicated sentence structures, abrupt\nendings, or subsumption. It is even more difﬁcult\nto ensure that all the relevant phenomena are con-\nsidered.\nBERT qry2snt models the relationship between\nthe legal concept and the retrieved sentences. It\nappears to perform better than the base BERT snt\n4279\nmodel. BERT qry2snt has access to the same kind\nof strategies as BERT snt, but since it does not\nignore the concept it can go further. For exam-\nple, there is a clear trend of ranking as very high\nsentences that contain the concept surrounded by\nquotation marks:\nThe ﬁrst subsection of that provision, entitled\n“Navigation Equipment,” requires tankers to pos-\nsess global positioning system (“GPS”) receivers,\nas well as two separate radar systems.\n[High value]\nWe believe the common meaning and general un-\nderstanding of the term “switchblade knife” is a\nknife in which the blade extends and is securely\nlocked open upon the pressing of a button or other\nmechanism.\n[High value]\nThis appears to be a viable strategy. However, there\ncould be instances where it does not work perfectly.\nBERT qry2snt appears to have the ability to rec-\nognize certain linguistic relationships between the\nterm of interest and other parts of a sentence. The\nfollowing sentences were not recognized as valu-\nable by BERT snt but they are correctly ranked very\nhigh by BERT qry2snt:\nAirplanes need wings to ﬂy, but that does not\nmean that all wing designs have independent eco-\nnomic value.\n[High value]\nAs explained above, the duty titles in this case do\nnot qualify as identifying particulars.\n[High value]\nAnd “motion pictures” are “ audiovisual works\nconsisting of a series of related images which,\nwhen shown in succession, impart an impression\nof motion, together with accompanying sounds, if\nany.”\n[High value]\nAll these examples seem to exhibit certain higher\nlevel patterns that are intuitively very appealing.\nRewriting the above sentences into such patterns\ncould look like this:\n[. . . ] NOUN_PHRASE have CONCEPT\n[. . . ] qualify [. . . ] NOUN_PHRASE [. . . ] CON-\nCEPT\nNOUN_PHRASE is deﬁned to be CONCEPT\n[. . . ]\n[. . . ] “NOUN_PHRASE” are “CONCEPT [. . . ]”\nThis is corroborated by the inspection of the model\nweights as applied to several sentences shown in\nFigure 2. The visualization was created using the\ntool published with (Vig, 2019). As mentioned ear-\nlier BERT is based on the transformer model from\n(Vaswani et al., 2017). An advantage of using the\nattention-based model is that it can be interpreted\nvia inspection of the weights assigned to different\ninput elements. As Vig (2019) warns one needs to\nbe very conservative with respect to drawing any\nconclusions. The three diagrams in Figure 2 show\nhow much attention the ﬁrst special tokens pay to\nthe individual words in the three input sequences.\nNote that the input sequences each consist of a\nterm of interest and a retrieved sentence. The rea-\nson why the ﬁrst special token is interesting is that\nthis token stands for the vector representing the\nsequence which is then fed into a classiﬁer. Hence,\nthe visualization provides some indication of what\ninﬂuences the representation that is being used in\nthe ﬁnal classiﬁcation step.\nAll three examples show that BERT qry2snt es-\ntablishes the relationship between the term of in-\nterest (ﬁrst part of the sequence) and its mention\nin the sentence. Additionally, the model attends to\nparts of the sentences that appear to be suggestive\nabout the higher value of a sentence (i.e., “to sat-\nisfy the common business purpose requirement”,\nthe quotation marks surrounding the digital mu-\nsical recording, or “that all . . . have independent\neconomic value”).\nFinally, the BERT sp2snt model that focuses on\nthe relationship between a written provision of law\nand a retrieved sentence appears to perform bet-\nter than the BERT qry2snt model. This may seem\nsomewhat surprising because BERT sp2snt does\nnot have access to the focused legal concept. On\nthe other hand, it is provided with the full provision\nof law. While BERT sp2snt appears to lack the abil-\nity of BERT qry2snt to detect the useful linguistic\npatterns attached to the legal concepts, it has the\nability to recognize the sentences with ‘no value’\nwith a high level of accuracy. For example, BERT\nqry2snt ranked the following sentences high:\nIn that article, a “wire communication” is deﬁned\nas “an aural transfer made in whole or in part\nthrough the use of facilities for the transmission\nof communications by the aid of wire, cable, or\nother like connection between the point of origin\nand the point of reception.” [No value]\nThe semiconductor chip product in turn is deﬁned\nas: the ﬁnal or intermediate form of any product–\n[No value]\nWhile these sentences appear to offer valuable deﬁ-\nnitions of the legal concepts, they merely quote the\nprovision of law, and thus have ‘no value.’ Overall,\nit appears that with respect to the NDCG scores, it\n4280\nFigure 2: The ﬁgure provides some indication of what input elements inﬂuence the representation that is being\nused in the ﬁnal classiﬁcation step. The model attends to parts of the sentences that really appear to be suggestive\nabout the higher value of a sentence (i.e., “to satisfy the common business purpose requirement”, the quotation\nmarks surrounding the digital musical recording, or “that all . . . have independent economic value”).\nis extremely important to make sure that sentences\nsuch as these do not appear at the top positions of\nthe rankings.\nFinally, to provide some concrete examples of\nthe rankings produced by the assessed models Fig-\nure 3 shows the distributions of labels of the top\n10 retrieved sentences as compared to the overall\ndistribution for two selected concepts (“navigation\nequipment” and “common business purpose”). The\nchanges in the distributions demonstrate how effec-\ntive the models can be.\nFigure 4 shows box and whisker plots augmented\nwith swarm plots of per query performance for the\nevaluated systems. Interestingly, it appears that the\nprogression starting from the BM25 method and\nending with the RF-PWT (i.e., the prior work ref-\nerenced above) mostly improves the performance\nby correcting the disastrous performance of the\nqueries on the left tail of the swarm plots. Despite\ncertain improvements happening at the right side\nas well, these are dwarfed by the events on the left.\nThe pre-trained language models ﬁne-tuned on\nthe task of sentence pair classiﬁcation are inter-\nesting because they no longer focus on the im-\nprovement of the lowest performing queries only.\nFigure 3: The top two graphs show the sentence value\ndistribution for the concept “navigation equipment.”\nThe graph on the left shows the overall distribution\nwhile the graph on the right shows the distribution of\nthe top ten sentences retrieved by BERT qry2snt. The\nbottom two graphs show the same for the concept of\n“common business purpose.”\n4281\nFigure 4: The ﬁgure shows scatter plots of the perfor-\nmance on the individual 42 queries measured in terms\nof NDCG@100 showing the progression from applica-\ntion of simpler similarity methods towards more com-\nplex learning-to-rank systems.\nThey also bring notable improvement to the queries\nwhere the performance had already been decent.\nThis is especially true for the BERT qry2snt model\nthat is completely oblivious to the source provision.\nHence, this model cannot address the requirement\nof “providing additional information” as well as\nthe requirement of “using the term of interest in the\nsame meaning.” Indeed, it appears that the model\nhas similar issues with a number of queries that\nthe BM25 method had. Yet, despite these notable\nissues the overall performance is comparable to (if\nnot better than) the RF-PWT.\nThe BERT sp2snt method uses the source pro-\nvision instead of the term of interest. On closer\ninspection one sees three large sparse queries that\nare not handled well by this method in Figure 4.\nThere are two reasons why a sentence could have\n‘no value.’ It either provides no additional informa-\ntion or it uses the term in a completely different\nmeaning. The three mishandled queries have many\nsentences that use the term in a different meaning.\nIt appears that BERT sp2snt learned to down-rank\nthe sentences that do not provide additional infor-\nmation quite reliably whereas it completely failed\nto learn to down-rank the sentences that use the\nterm in a different meaning. The data set may be\ntoo small for the method to capture this aspect.\n7 Conclusions and Future Work\nIn this work, we showed that pre-trained language\nmodels based on transformers can be ﬁne-tuned for\nthe special task of retrieving sentences for explain-\ning legal concepts. Speciﬁcally, we demonstrated\nthat a pre-trained RoBERTa base model, ﬁne-tuned\non three variations of the task, resulted in effec-\ntive ranking functions outperforming the BM25\nbaselines. The promising performance of BERT\nsnt reveals the interesting fact that sentences them-\nselves carry certain signal about their usefulness.\nThe even stronger performance of BERT qry2snt\nand BERT sp2snt points to the important interac-\ntions among a legal concept, the provision of law\nin which it is embedded, and retrieved sentences,\nthat both need to be accounted for in order to per-\nform well in this challenging task. The whole work\ndemonstrates the effectiveness of methods based\non pre-trained language models applied to a legal\ndomain task. This is important because advances\nin general NLP and ML do not always transfer in\na straightforward manner to specialized domains\nsuch as automatic processing of legal or medical\ntexts. Importantly, we ﬁll the gap in prior work by\nshowing that the transformer based methods are ca-\npable of ﬁne-grained evaluations of the individual\nsentences as to their usefulness.\nThe application of pre-trained language models\nto the task of discovering sentences explaining legal\nconcepts yielded promising results. At the same\ntime, the work is subject to limitations and leaves\nmuch room for improvement. Hence, we suggest\nseveral directions for future work:\n• Focus on diversity in addition to relevance to\nensure that the top results do not repeat the\nsame sentences.\n• Account for all three constituents, i.e., the\nlegal concept, the written provision of law,\nand retrieved sentences, simultaneously.\n• Investigate the effects of including the context\nof a retrieved sentence, i.e., the full text of a\ncase decision.\n• Invest more resources in developing and ex-\ntending the dataset.\n• Investigate retrieving sentences from other\ntypes of legal documents beyond court case\ndecisions (e.g., legislative histories, commen-\ntaries).\n• Perform an extrinsic evaluation of the system\nin the context of an end-to-end legal project.\n4282\nAcknowledgements\nThe ﬁrst author would like to acknowledge the Uni-\nversity of Pittsburgh as his home institution during\nthe time this work was conducted. This work was\nsupported in part by a National Institute of Jus-\ntice Graduate Student Fellowship (Fellow: Jaromir\nSavelka) Award # 2016-R2-CX-0010, “Recommen-\ndation System for Statutory Interpretation in Cy-\nbercrime,” and by a University of Pittsburgh Pitt\nCyber Accelerator Grant entitled “Annotating Ma-\nchine Learning Data for Interpreting Cyber-Crime\nStatutes.”\nReferences\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos\nAletras. 2019. Neural legal judgment prediction in\nenglish. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4317–4323.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert:“preparing the muppets for court’”.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\npages 2898–2904.\nJordan Daci. 2010. Legal principles, legal values and\nlegal norms: are they the same or different? Aca-\ndemicus International Scientiﬁc Journal , 02:109–\n115.\nJanez Demšar. 2006. Statistical comparisons of clas-\nsiﬁers over multiple data sets. Journal of Machine\nlearning research, 7(Jan):1–30.\nTimothy Endicott. 2000. Vagueness in Law. Oxford\nUniversity Press.\nTimothy Endicott. 2014. Law and Language the stan-\nford encyclopedia of philosophy. http://plato.\nstanford.edu/. Accessed: 2016-02-03.\nMilton Friedman. 1937. The use of ranks to avoid the\nassumption of normality implicit in the analysis of\nvariance. Journal of the american statistical associ-\nation, 32(200):675–701.\nEvan Gretok, David Langerman, and Wesley M Oliver.\n2020. Transformers for classifying fourth amend-\nment elements and factors tests. Legal Knowledge\nand Information Systems JURIX, pages 63–72.\nHerbert L. Hart. 1994. The Concept of Law , 2nd edi-\ntion. Clarendon Press.\nSture Holm. 1979. A simple sequentially rejective mul-\ntiple test procedure. Scandinavian journal of statis-\ntics, pages 65–70.\nJerrold Soh Tsin Howe, Lim How Khang, and Ian Ernst\nChai. 2019. Legal area classiﬁcation: A compara-\ntive study of text classiﬁers on singapore supreme\ncourt judgments. In Proceedings of the Natural Le-\ngal Language Processing Workshop 2019, pages 67–\n77.\nMatjaz Juršic, Igor Mozetic, Tomaz Erjavec, and Nada\nLavrac. 2010. Lemmagen: Multilingual lemmatisa-\ntion with induced ripple-down rules. Journal of Uni-\nversal Computer Science, 16(9):1190–1214.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability. Computing, 1:25.\nJimmy Lin, Rodrigo Nogueira, and Andrew Yates.\n2020. Pretrained transformers for text ranking: Bert\nand beyond. arXiv preprint arXiv:2010.06467.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nD. N. MacCormick and R. S. Summers. 1991. Inter-\npreting Statutes. Darmouth.\nSamarth Mehrotra and Andrew Yates. 2019. Mpii at\ntrec cast 2019: Incoporating query context into a bert\nre-ranker.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy Lin. 2019. Multi-stage document ranking\nwith bert. arXiv preprint arXiv:1910.14424.\nThe President and Fellows of Harvard University. 2018.\nCaselaw access project. https://case.law/.\nAccessed: 2018-12-21.\nJuliano Rabelo, Mi-Young Kim, and Randy Goebel.\n2019. Combining similarity and transformer meth-\nods for case law entailment. In Proceedings of the\nSeventeenth International Conference on Artiﬁcial\nIntelligence and Law, pages 290–296.\nJinfeng Rao, Linqing Liu, Yi Tay, Wei Yang, Peng\nShi, and Jimmy Lin. 2019. Bridging the gap be-\ntween relevance matching and semantic matching\nfor short text similarity modeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5373–5384.\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Now Publishers Inc.\nJulien Rossi and Evangelos Kanoulas. 2019. Le-\ngal search in case law and statute law. In Legal\nKnowledge and Information Systems: JURIX 2019:\nThe Thirty-second Annual Conference, volume 322,\npage 83. IOS Press.\n4283\nLuis Sanchez, Jiyin He, Jarana Manotumruksa, Dyaa\nAlbakour, Miguel Martinez, and Aldo Lipani. 2020.\nEasing legal news monitoring with learning to rank\nand bert. In European Conference on Information\nRetrieval, pages 336–343. Springer.\nJaromir Savelka. 2020. Discovering sentences for ar-\ngumentation about the meaning of statutory terms .\nPh.D. thesis, University of Pittsburgh.\nJaromír Šavelka and Kevin D Ashley. 2016. Extract-\ning case law sentences for argumentation about the\nmeaning of statutory terms. In Proceedings of\nthe Third Workshop on Argument Mining (ArgMin-\ning2016), pages 50–59.\nJaromír Savelka and Kevin D Ashley. 2020. Learning\nto rank sentences for explaining statutory terms. In\nASAIL@ JURIX.\nJaromír Šavelka and Kevin D Ashley. 2021. Legal in-\nformation retrieval for understanding statutory terms.\nArtiﬁcial Intelligence and Law, pages 1–45.\nJaromir Savelka and Kevin D. Ashley. 2021. On the\nrole of past treatment of terms from written laws in\nlegal reasoning. In New Developments in Legal Rea-\nsoning and Logic, pages 379–394. Springer.\nJaromir Šavelka and Jakub Harašta. 2015. Open tex-\nture in law, legal certainty and logical analysis of\nnatural language. In Logic in the Theory and Prac-\ntice of Lawmaking, pages 159–171. Springer.\nJaromir Savelka, Vern R Walker, Matthias Grabmair,\nand Kevin D Ashley. 2017. Sentence boundary de-\ntection in adjudicatory decisions in the united states.\nTraitement automatique des langues, 58(2):21–45.\nJaromır Šavelka, Hannes Westermann, and Karim\nBenyekhlef. 2020. Cross-domain generalization and\nknowledge transfer in transformers trained on legal\ndata.\nJaromir Savelka, Huihui Xu, and Kevin D Ashley. 2019.\nImproving sentence retrieval from case law for statu-\ntory interpretation. In Proceedings of the Seven-\nteenth International Conference on Artiﬁcial Intel-\nligence and Law, pages 113–122.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the transformer model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations , pages\n37–42.\nHannes Westermann, Jaromir Savelka, and Karim\nBenyekhlef. 2020. Paragraph similarity scoring and\nﬁne-tuned bert for legal information retrieval and en-\ntailment. In JSAI International Symposium on Arti-\nﬁcial Intelligence, pages 269–285. Springer.\nWei Yang, Haotian Zhang, and Jimmy Lin. 2019. Sim-\nple applications of bert for ad hoc document re-\ntrieval. arXiv preprint arXiv:1903.10972.\nZeynep Akkalyoncu Yilmaz, Wei Yang, Haotian\nZhang, and Jimmy Lin. 2019. Cross-domain mod-\neling of sentence-level evidence for document re-\ntrieval. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3481–3487.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7966547012329102
    },
    {
      "name": "Natural language processing",
      "score": 0.6853856444358826
    },
    {
      "name": "Transformer",
      "score": 0.6556290984153748
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6504662036895752
    },
    {
      "name": "Task (project management)",
      "score": 0.4783056080341339
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4630819261074066
    },
    {
      "name": "Meaning (existential)",
      "score": 0.45725008845329285
    },
    {
      "name": "Language model",
      "score": 0.42055124044418335
    },
    {
      "name": "Linguistics",
      "score": 0.3332228660583496
    },
    {
      "name": "Psychology",
      "score": 0.0847364068031311
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 10
}