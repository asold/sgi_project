{
    "title": "Self-attention in vision transformers performs perceptual grouping, not attention",
    "url": "https://openalex.org/W4382599796",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104967863",
            "name": "Paria Mehrani",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A2074551997",
            "name": "John K. Tsotsos",
            "affiliations": [
                "York University"
            ]
        },
        {
            "id": "https://openalex.org/A2104967863",
            "name": "Paria Mehrani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2074551997",
            "name": "John K. Tsotsos",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3035422918",
        "https://openalex.org/W3195282418",
        "https://openalex.org/W4281480780",
        "https://openalex.org/W2006063316",
        "https://openalex.org/W3025069833",
        "https://openalex.org/W2165728411",
        "https://openalex.org/W6796761347",
        "https://openalex.org/W6776793684",
        "https://openalex.org/W6792881003",
        "https://openalex.org/W2164084182",
        "https://openalex.org/W2020825481",
        "https://openalex.org/W248046807",
        "https://openalex.org/W2166206801",
        "https://openalex.org/W6794655914",
        "https://openalex.org/W2112845172",
        "https://openalex.org/W6794166682",
        "https://openalex.org/W2013894622",
        "https://openalex.org/W6769955919",
        "https://openalex.org/W6796931752",
        "https://openalex.org/W3137278571",
        "https://openalex.org/W2118615399",
        "https://openalex.org/W2792911100",
        "https://openalex.org/W6737912528",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2537084945",
        "https://openalex.org/W4291792830",
        "https://openalex.org/W6847279771",
        "https://openalex.org/W1998938084",
        "https://openalex.org/W2103147032",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W6799166919",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2139748113",
        "https://openalex.org/W2971464184",
        "https://openalex.org/W2952671852",
        "https://openalex.org/W6761473936",
        "https://openalex.org/W4213083558",
        "https://openalex.org/W2164364459",
        "https://openalex.org/W2040036684",
        "https://openalex.org/W2808580750",
        "https://openalex.org/W1971534008",
        "https://openalex.org/W6732958910",
        "https://openalex.org/W2156047020",
        "https://openalex.org/W6777058603",
        "https://openalex.org/W3176860045",
        "https://openalex.org/W2343204383",
        "https://openalex.org/W2095627201",
        "https://openalex.org/W4360995299",
        "https://openalex.org/W4212942565",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W6810938606",
        "https://openalex.org/W3200213953",
        "https://openalex.org/W6792566387",
        "https://openalex.org/W4282933246",
        "https://openalex.org/W2295763703",
        "https://openalex.org/W6795475546",
        "https://openalex.org/W4210433173",
        "https://openalex.org/W3170642968",
        "https://openalex.org/W4321504988",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W6685621245",
        "https://openalex.org/W2143237238",
        "https://openalex.org/W2171590216",
        "https://openalex.org/W6800217721",
        "https://openalex.org/W3089527440",
        "https://openalex.org/W2552164092",
        "https://openalex.org/W2152669853",
        "https://openalex.org/W6789425149",
        "https://openalex.org/W2038220177",
        "https://openalex.org/W4234561751",
        "https://openalex.org/W4251073084",
        "https://openalex.org/W3034236144",
        "https://openalex.org/W2040568291",
        "https://openalex.org/W119228284",
        "https://openalex.org/W2744872705",
        "https://openalex.org/W4289173098",
        "https://openalex.org/W177844970",
        "https://openalex.org/W1985393502",
        "https://openalex.org/W6795297120",
        "https://openalex.org/W3192770954",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2950225513",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W6802910155",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W6630875275",
        "https://openalex.org/W3145158551",
        "https://openalex.org/W3146620971",
        "https://openalex.org/W4295951228",
        "https://openalex.org/W6803680838",
        "https://openalex.org/W2025819347",
        "https://openalex.org/W6788477181",
        "https://openalex.org/W6799770928",
        "https://openalex.org/W6839026099",
        "https://openalex.org/W6680565713",
        "https://openalex.org/W6811002592",
        "https://openalex.org/W6800018832",
        "https://openalex.org/W4286218466",
        "https://openalex.org/W6802741100",
        "https://openalex.org/W3119948327",
        "https://openalex.org/W6963229073",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W3209987414",
        "https://openalex.org/W3213783001",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W3204538018",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2014886758",
        "https://openalex.org/W3145185940",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4225088380",
        "https://openalex.org/W3171087525",
        "https://openalex.org/W4384663241",
        "https://openalex.org/W2175160520",
        "https://openalex.org/W4214713996",
        "https://openalex.org/W4298171088",
        "https://openalex.org/W4299802238",
        "https://openalex.org/W4297812995",
        "https://openalex.org/W3205725084",
        "https://openalex.org/W2939218116",
        "https://openalex.org/W2612573399",
        "https://openalex.org/W2138046011",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W4287022992",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4306802685",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3143373604",
        "https://openalex.org/W2949847915",
        "https://openalex.org/W3151130473"
    ],
    "abstract": "Recently, a considerable number of studies in computer vision involve deep neural architectures called vision transformers. Visual processing in these models incorporates computational models that are claimed to implement attention mechanisms. Despite an increasing body of work that attempts to understand the role of attention mechanisms in vision transformers, their effect is largely unknown. Here, we asked if the attention mechanisms in vision transformers exhibit similar effects as those known in human visual attention. To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects. Additionally, whereas modern experimental findings reveal that human visual attention involves both feed-forward and feedback mechanisms, the purely feed-forward architecture of vision transformers suggests that attention in these models cannot have the same effects as those known in humans. To quantify these observations, we evaluated grouping performance in a family of vision transformers. Our results suggest that self-attention modules group figures in the stimuli based on similarity of visual features such as color. Also, in a singleton detection experiment as an instance of salient object detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms thought to be utilized in human visual attention. We found that generally, the transformer-based attention modules assign more salience either to distractors or the ground, the opposite of both human and computational salience. Together, our study suggests that the mechanisms in vision transformers perform perceptual organization based on feature similarity and not attention.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/nine.tnum June /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nOPEN ACCESS\nEDITED BY\nDirk Bernhardt-Walther,\nUniversity of Toronto, Canada\nREVIEWED BY\nStavros Tsogkas,\nSamsung AI Center Toronto, Canada\nKatherine Rebecca Storrs,\nJustus Liebig University Giessen, Germany\n*CORRESPONDENCE\nParia Mehrani\nparia/six.tnum/one.tnum@yorku.ca\nRECEIVED /zero.tnum/two.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/two.tnum June /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /two.tnum/nine.tnum June /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nMehrani P and Tsotsos JK (/two.tnum/zero.tnum/two.tnum/three.tnum) Self-attention\nin vision transformers performs perceptual\ngrouping, not attention.\nFront. Comput. Sci. /five.tnum:/one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Mehrani and Tsotsos. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nSelf-attention in vision\ntransformers performs perceptual\ngrouping, not attention\nParia Mehrani * and John K. Tsotsos\nDepartment of Electrical Engineering and Computer Science, Y ork University, Toronto, ON, Canada\nRecently, a considerable number of studies in computer vision i nvolve deep\nneural architectures called vision transformers. Visual proce ssing in these models\nincorporates computational models that are claimed to implemen t attention\nmechanisms. Despite an increasing body of work that attempts to understand\nthe role of attention mechanisms in vision transformers, the ir eﬀect is largely\nunknown. Here, we asked if the attention mechanisms in vision tr ansformers\nexhibit similar eﬀects as those known in human visual attention. To answer\nthis question, we revisited the attention formulation in the se models and found\nthat despite the name, computationally, these models perfor m a special class\nof relaxation labeling with similarity grouping eﬀects. Addi tionally, whereas\nmodern experimental ﬁndings reveal that human visual attenti on involves both\nfeed-forward and feedback mechanisms, the purely feed-forward ar chitecture of\nvision transformers suggests that attention in these model s cannot have the same\neﬀects as those known in humans. To quantify these observation s, we evaluated\ngrouping performance in a family of vision transformers. Our results suggest that\nself-attention modules group ﬁgures in the stimuli based on s imilarity of visual\nfeatures such as color. Also, in a singleton detection experime nt as an instance\nof salient object detection, we studied if these models exhibit similar eﬀects\nas those of feed-forward visual salience mechanisms thought to be utilized in\nhuman visual attention. We found that generally, the transfo rmer-based attention\nmodules assign more salience either to distractors or the grou nd, the opposite\nof both human and computational salience. Together, our study suggests that\nthe mechanisms in vision transformers perform perceptual org anization based on\nfeature similarity and not attention.\nKEYWORDS\nvision transformers, attention, similarity grouping, singleton detection, odd-one-out\n/one.tnum. Introduction\nThe Gestalt principles of grouping suggest rules that explain the tendency of perceiving\na uniﬁed whole rather than a mosaic pattern of parts. Gestaltists consider organizational\npreferences, or priors, such as symmetry, similarity, proximity, continuity and closure as\ngrouping principles that contribute to the perception of a whole. These principles which\nrely on input factors and the conﬁguration of parts can be viewed as biases that result in\nthe automatic emergence of ﬁgure and ground. To Gestalt psychologists, the perceptual\norganization of visual input to ﬁgure and ground was an early stage of interpretation prior to\nprocesses such as object recognition and attention. In fact, they posited that higher-level\nprocesses operate upon the automatically emerged ﬁgure. Some proponents of emergent\nintelligence go as far as to undermine the eﬀect of attention on perceptual organization. For\nexample, Rubin, known for his face-vase illusion, presented a paper in 1926 titled “On the\nNon-Existence of Attention\" (\nBerlyne, 1974).\nFrontiers in Computer Science /zero.tnum/one.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nDespite the traditional Gestalt view, modern experimental\nevidence suggests that in addition to low-level factors, higher-\nlevel contributions can aﬀect ﬁgure-ground organization.\nSpeciﬁcally, experimental ﬁndings suggest that attention is\nindeed real and among the higher-level factors that inﬂuence\nﬁgure-ground assignment (\nQiu et al., 2007 ; Poort et al., 2012 )\n(see Peterson, 2015 for review). Considering these discoveries\nand the enormous literature on attention (see Itti et al., 2005 , for\nexample), an interesting development in recent years has been the\nintroduction of deep neural architectures dubbed transformers\nthat claim to incorporate attention mechanisms in their hierarchy\n(\nVaswani et al., 2017 ). Transformers, originally introduced in the\nlanguage domain, were “based solely on attention mechanisms,\ndispensing with recurrence and convolutions entirely\"\n(\nVaswani et al., 2017 ).\nFollowing the success of transformers in the language\ndomain, Dosovitskiy et al. (2021) introduced the vision\ntransformer (ViT), a transformer model based on self-attention\nmechanisms that received a sequence of image patches as input\ntokens.\nDosovitskiy et al. (2021) reported comparable performance\nof ViT to convolutional neural networks (CNNs) in image\nclassiﬁcation and concluded, similar to (\nVaswani et al., 2017 ), that\nconvolution is not necessary for vision tasks. The reported success\nof vision transformers prompted a myriad of studies (\nBhojanapalli\net al., 2021 ; Caron et al., 2021 ; Dai et al., 2021 ; D’Ascoli et al.,\n2021; Liu et al., 2021 , 2022; Mahmood et al., 2021 ; Srinivas et al.,\n2021; Touvron et al., 2021 ; Wu B. et al., 2021 ; Wu H. et al., 2021 ;\nXiao et al., 2021 ; Yang et al., 2021 ; Yuan et al., 2021 ; Zhou et al.,\n2021; Bao et al., 2022 ; Guo et al., 2022 ; Han et al., 2022 ; Pan\net al., 2022 ; Park and Kim, 2022 ; Zhou D. et al., 2022 ). In most\nof these studies, the superior performance of vision transformers,\ntheir robustness (\nBhojanapalli et al., 2021 ; Mahmood et al., 2021 ;\nNaseer et al., 2021 ) and more human-like image classiﬁcation\nbehavior compared to CNNs ( Tuli et al., 2021 ) were attributed to\nthe attention mechanisms in these architectures. Several hybrid\nmodels assigned distinct roles of feature extraction and global\ncontext integration to convolution and attention mechanisms,\nrespectively, and reported improved performance over models\nwith only convolution or attention (\nDai et al., 2021 ; D’Ascoli\net al., 2021 ; Srinivas et al., 2021 ; Wu B. et al., 2021 ; Wu H. et al.,\n2021; Xiao et al., 2021 ; Guo et al., 2022 ). Hence, these studies\nsuggested the need for both convolution and attention in computer\nvision applications.\nA more recent study by\nZhou Q. et al. (2022), however,\nreported that hybrid convolution and attention models do not\n“have an absolute advantage\" compared to pure convolution\nor attention-based neural networks when their performance in\nexplaining neural activities of the human visual cortex from two\nneural datasets was studied. Similarly,\nLiu et al. (2022) questioned\nthe claims on the role of attention modules in the superiority\nof vision transformers by proposing steps to “modernize” the\nstandard ResNet (\nHe et al., 2016 ) into a new convolution-based\nmodel called ConvNeXt. They demonstrated that ConvNeXt with\nno attention mechanisms achieved competitive performance to\nstate-of-the-art vision transformers on a variety of vision tasks.\nThis controversy on the necessity of the proposed mechanisms\ncompared to convolution adds to the mystery of the self-attention\nmodules in vision transformers. Surprisingly, and to the best of our\nknowledge, no previous work directly investigated whether the self-\nattention modules, as claimed, implement attention mechanisms\nwith eﬀects similar to those reported in humans. Instead, the\nconclusions in previous studies were grounded on the performance\nof vision transformers vs. CNNs on certain visual tasks. As a\nresult, a question remains outstanding: Have we ﬁnally attained a\ndeep computational vision model that explicitly integrates visual\nattention into its hierarchy?\nAnswering this question is particularly important for advances\nin both human and computer vision ﬁelds. Speciﬁcally, in human\nvision sciences, the term attention has a long history (e.g.,\nBerlyne,\n1974; Tsotsos et al., 2005 ) and entails much confusion (e.g.,\nDi Lollo, 2018; Hommel et al., 2019 ; Anderson, 2023). In a review of\na book on attention ( Sutherland, 1998) says,“After many thousands\nof experiments, we know only marginally more about attention\nthan about the interior of a black hole”. More recently,\nAnderson\n(2023) calls attention a conceptually fragmented term, a term that is\nassumed to have one meaning is found to have many, and suggests\naid from mathematical language for theoretical clarity. The call for\na more formal approach to vision research has appeared several\ntimes (e.g.,\nZucker, 1981 ; Tsotsos, 2011 ; Anderson, 2023 ) but no\nbroadly accepted speciﬁcation of attention is available. The majority\nof words in any dictionary have multiple meanings, and a particular\nclass of words, homonyms, are spelled and pronounced the same\nyet diﬀer in meaning which is only distinguished by the context\nin which they are used. “Attention” is one such word, here we\nseek to understand the scope of its use in order to provide the\ncorrect context.\nTo complicate matters further, many kinds of visual attention\nhave been identiﬁed, the primary distinctions perhaps being that\nof overt and covert attention (with and without eye movements\nand viewpoint changes, respectively).\nTsotsos (2011) shows over\n20 kinds in his taxonomy, and other comprehensive reviews on\nthe topic such as\nDesimone and Duncan (1995), Pashler (1998),\nKastner and Ungerleider (2000), Itti et al. (2005), Styles (2006),\nKnudsen (2007), Nobre et al. (2014), Moore and Zirnsak (2017),\nand Martinez-Trujillo (2022) similarly cover many kinds, not all the\nsame. As Styles (2006) asserts, attention is not a unitary concept.\nIn addition, discussions of attention are always accompanied by\nconsideration of how attention can change focus; this dynamic\naspect does not appear in transformers at all.\nThe many descriptions of attention often conﬂate mechanism\nwith eﬀect while assuming that an exposition within some\nnarrow domain easily generalizes to all of cognitive behavior.\nOne might think that as long as the discussion remains within a\nparticular community, all can be controlled with respect to use of\nterminology. This is not the case. Machine learning approaches\nhave been already employed frequently in recent years in brain\nresearch by utilizing deep neural architectures as mathematical\nmodels of the brain (\nCadieu et al., 2014 ; Khaligh-Razavi and\nKriegeskorte, 2014 ; Kubilius et al., 2016 ; Eickenberg et al., 2017 ;\nZhuang et al., 2021 ). Therefore, it is only a matter of time before\nvision transformers with attention modules are used in human\nvision studies, if not already by the time of this publication. As\na result, it is imperative to understand how attention modules\nin vision transformers relate to attention mechanisms in the\nFrontiers in Computer Science /zero.tnum/two.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nhuman visual system to avoid adding further confusion to attention\nresearch in human vision sciences.\nSimilarly, on the computer vision side, a more engineering kind\nof discipline, we need to specify the requirements of a solution\nagainst which we test the results of any algorithm realization.\nBut the requirements of attention modules in vision transformers\nare not speciﬁed. They are only implied, through the use of\nthe term ‘attention’ and can be traced back to the studies that\nexplicitly motivated these modules, speciﬁcally, by the eﬀect of\nattention mechanisms in the human visual system (i.e.,\nVaswani\net al., 2017 → Kim et al., 2017 →Xu et al., 2015 ).\nOne might argue that from an engineering point of view, there\nis no need for these modules to remain faithful to their biological\ncounterparts, hence, there is no need for direct comparison between\nthe two systems. However, that train has already left the station.\nComputer vision has been using the term “attention” since the\nmid-1970’s, connected to both inspiration from and comparisons\nto human visual attention, and continuously to this day (there are\nmany reviews as evidence, e.g.,\nTsotsos and Rothenstein, 2011; Borji\nand Itti, 2012 ; Bylinskii et al., 2015 ). An expectation that a new\nmechanism can aﬀect amnesia for a whole ﬁeld is unwarranted. For\nexample,\nTan et al. (2021), Yue et al. (2021), Zhu et al. (2021), Paul\nand Chen (2022), and Panaetov et al. (2023) among others, have\nalready mentioned eﬀects of these modules as similar to those of\nattention in the human visual system.\nRegardless of whether one considers attention from a human\nvision perspective or a machine vision point of view, it is\nunprincipled to leave the term ill-deﬁned. Our goal in this paper\nis to contribute to an understanding of the function of the attention\nmodules in vision transformers by revisiting two of their aspects.\nFirst, we hope to show that transformers formulate attention\naccording to similarity of representations between tokens, and that\nthis results in perceptual similarity grouping, not any of the many\nkinds of attention in the literature. Second, because of their feed-\nforward architecture, vision transformers cannot be not aﬀected by\nfactors such as goals, motivations, or biases (also see\nHerzog and\nClarke, 2014 ). Such factors have played a role in attention models\nin computer vision for decades. Vision Transformers fall into the\nrealm of the traditional Gestalt view of automatic emergence of\ncomplex features.\nIn a set of experiments, we examined attention modules\nin various vision transformer models. Speciﬁcally, to quantify\nGestalt-like similarity grouping, we introduced a grouping dataset\nof images with multiple shapes that shared/diﬀered in various\nvisual feature dimensions and measured grouping of ﬁgures\nin these architectures. Our results on a family of vision\ntransformers indicate that the attention modules, as expected\nfrom the formulation, group image regions based on similarity.\nOur second observations indicates that if vision transformers\nimplement attention, it can only be in the form of bottom-\nup attention mechanisms. To test this observation, we measured\nthe performance of vision transformers in the task of singleton\ndetection. Speciﬁcally, a model that implements attention is\nexpected to almost immediately detect the pop-out, an item in the\ninput that is visually distinct from the rest of the items. Our ﬁndings\nsuggest that vision transformers perform poorly in that regard and\neven in comparison to CNN-based saliency algorithms.\nTo summarize, our observations and experimental results\nsuggest that “attention mechanisms” is a misnomer for\ncomputations implemented in so-called self-attention modules of\nvision transformers. Speciﬁcally, these modules perform similarity\ngrouping and not attention. In fact, the self-attention modules\nimplement a special class of in-layer lateral interactions that\nwere missing in CNNs (and perhaps this is the reason for their\ngenerally improved performance). Lateral interactions are known\nas mechanisms that counteract noise and ambiguity in the input\nsignal (\nZucker, 1978 ). In light of this observation, the reported\nproperties of vision transformers such as smoothing of feature\nmaps (\nPark and Kim, 2022 ) and robustness ( Mahmood et al., 2021 ;\nNaseer et al., 2021 ) can be explained. These observations lead to the\nconclusion that the quest for a deep computational vision model\nthat implements attention mechanisms has not come to an end yet.\nIn what follows, we will employ the terms attention\nand self-attention interchangeably as our focus is limited to\nvision transformers with transformer encoder blocks. Also, each\ncomputational component in a transformer block will be referred\nto as a module, for example, the attention module or the multi-layer\nperceptron (MLP) module. Both block and layer, then, will refer to\na transformer encoder block that consists of a number of modules.\n/two.tnum. Materials and methods\nIn this section, we ﬁrst provide a brief overview of vision\ntransformers followed by revisiting attention formulation and the\nrole of architecture in visual processing in these models. Then, we\nexplain the details of the two experiments we performed in this\nstudy.\n/two.tnum./one.tnum. Vision transformers\nFigure 1 provides an overview of Vision Transformer (ViT)\nand the various modules in its transformer encoder blocks. Most\nvision transformer models extend and modify or simply augment\na ViT architecture into a larger system. Regardless, the overall\narchitecture and computations in the later variants resemble those\nof ViT and each model consists of a number of stacked transformer\nencoder blocks. Each block performs visual processing of its input\nthrough self-attention, MLP and layer normalization modules.\nInput to these networks includes a sequence of processed image\ntokens (localized image patches) concatenated with a learnable class\ntoken.\nVision transformer variants can be grouped into three main\ncategories:\n1. Models that utilized stacks of transformer encoder blocks as\nintroduced in ViT but modiﬁed the training regime and reported\na boost in performance, such as DeiT (\nTouvron et al., 2021 ) and\nBEiT (Bao et al., 2022 ).\n2. Models that modiﬁed ViT for better adaptation to the\nvisual domain. For example, Liu et al. (2021) introduced an\narchitecture called Swin and suggested incorporating various\nscales and shifted local windows between blocks. A few other\nFrontiers in Computer Science /zero.tnum/three.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum\nThe ViT model architecture (\nDosovitskiy et al., /two.tnum/zero.tnum/two.tnum/one.tnum). First, each input image is split into local patches called tok ens. After linear embedding of the\ntokens, a numerical position embedding is added to each token. Aft er concatenating a learnable class embedding shown with an ast erisk to the input\nsequence, the combined embeddings are fed to L blocks of transfor mer encoders. The output of the ﬁnal encoder block is fed to a class iﬁcation\nhead in ViT. The zoomed-in diagram on the right demonstrates the var ious modules within a transformer encoder block. These modules consi st of\nnorm, multi-head self-attention and MLP.\nwork suggested changes to the scope of attention, for example,\nlocal vs. global (\nChen et al., 2021 ; Yang et al., 2021 ).\n3. Hybrid models that introduced convolution either as a\npreprocessing stage ( Xiao et al., 2021 ) or as a computational step\nwithin transformer blocks ( Wu H. et al., 2021 ).\nThe family of vision transformers that we studied in our\nexperiments includes ViT, DEiT, BEiT, Swin, and CvT. These\nmodels span all three categories of vision transformers as classiﬁed\nabove. For each model, we studied a number of pre-trained\narchitectures available on HuggingFace (\nWolf et al., 2020 ). Details\nof these architectures are outlined in Table 1.\n/two.tnum./one.tnum./one.tnum. Attention formulation\nIn transformers, the attention mechanism for a query and\nkey-value pair is deﬁned as:\nAttention(Q, K, V) = softmax( QKT\n√\ndk\n)V, (1)\nwhere Q, K, and V represent matrices of queries, keys and values\nwith tokens as rows of these matrices, and dk is the dimension of\nindividual key/query vectors. Multiplying each query token, a row\nof Q, in the matrix multiplication QKT is in fact a dot-product of\neach query with all keys in K. The output of this dot-product can\nbe interpreted as how similar the query token is to each of the key\ntokens in the input; a compatibility measure. This dot product is\nthen scaled by\n√\ndk and the softmax yields the weights for value\ntokens. Vaswani et al. (2017) explained the output of attention\nmodules as “a weighted sum of the values, where the weight\nassigned to each value is computed by a compatibility function of\nthe query with the corresponding key”. The same formulation was\nemployed in ViT while the compatibility function formulation is\nslightly modiﬁed in some vision transformer variants. Nonetheless,\nthe core of the compatibility function in all of these models is a dot-\nproduct measuring representation similarity.\nVaswani et al. (2017)\nreported improved performance when instead of a single attention\nfunction, they mapped the query, key and value tokens to h disjoint\nrepresentational learned spaces and computed attention in each\nspace called a head. Concatenation of the attention computed in\nindividual heads yields the output of the attention module that they\ncalled Multi-Head Attention module.\nIn transformer encoders, the building block of vision\ntransformers, the query, key and value have the same source\nand come from the output of the previous block. Hence, the\nattention modules in these blocks are called self-attention. In this\ncase, the attention formulation can be explained as a process\nthat results in consistent token representations across all spatial\npositions in the stimulus. Speciﬁcally, token representation and\nattention can be described as follows: each token representation\nsigniﬁes presence/absence of certain visual features, providing a\nvisual interpretation or label at that spatial position. The attention\nmechanism incorporates the context from the input into its\nprocess and describes the inter-token relations determined by\nthe compatibility function. As a result, Equation (1) shifts the\ninterpretation of a given token toward that of more compatible\nFrontiers in Computer Science /zero.tnum/four.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nTABLE /one.tnum The family of vision transformers studied in this work.\nModel Architecture name # layers # params Training dataset Fine-tuned\nViT ViT-base-patch16-224 12 86 M ImageNet-21k –\nDeiT\nDeiT-tiny-distilled-patch16-224 12 5 M ImageNet-1k ImageNet-1k\nDeiT-small-distilled-patch16-224 12 22 M ImageNet-1k ImageNet-1k\nDeiT-base-distilled-patch16-224 12 86 M ImageNet-1k ImageNet-1k\nBEiT\nBEiT-base-patch16-224 12 86 M ImageNet-21k ImageNet-1k\nBEiT-base-patch16-224-pt22k 12 86 M ImageNet-21k –\nBEiT-base-patch16-224-pt22k-ft22k 12 86 M ImageNet-21k ImageNet-21k\nCvT\nCvT-13 13 19.98 M ImageNet-1k –\nCvT-21 21 31.54 M ImageNet-1k –\nSwin\nSwin-tiny-patch4-window7-224 12 29 M ImageNet-1k –\nSwin-small-patch4-window7-224 12 50 M ImageNet-1k –\nFor each model, a number of architecture variations were studied. For all models, pre-trained architectures available on HuggingFace (Wolf et al., 2020 ) were utilized. Input resolution to all\npre-trained models was 224 × 224. The datasets used for training and ﬁne-tuning are speciﬁed. W hereas, DeiT and BEiT models use the same general architecture as ViT, Swin introduces\nmultiple scales and shifted windows to overcome the shortcomings of ﬁxed s ize and position in tokens for visual tasks. The CvT architectures ar e hybrid models combining convolution and\nself-attention mechanisms in each transformer encoder block.\ntokens in the input. The ﬁnal outcome of this process will be groups\nof tokens with similar representations.\nZucker (1978) referred to\nthis process as “Gestalt-like similarity grouping process”.\nIn Zucker (1978), the Gestalt-like similarity grouping process is\nintroduced as a type of relaxation labeling (RL) process. Relaxation\nlabeling is a computational framework for updating the possibility\nof a set of labels (or interpretations) for an object based on the\ncurrent interpretations among neighboring objects. Updates in RL\nare performed according to a compatibility function between labels.\nIn the context of vision transformers, at a given layer, each token\nis an object for which a feature representation (label) is provided\nfrom the output of the previous layer. A token representation is\nthen updated (the residual operation after the attention module)\naccording to a dot-product compatibility function deﬁned between\nrepresentations of neighboring tokens. In ViT, the entire stimulus\nforms the neighborhood for each token.\nZucker (1978) deﬁned two types of RL processes in low-\nlevel vision: vertical and horizontal. In horizontal processes,\nthe compatibility function deﬁnes interaction at a single level\nof abstraction but over multiple spatial positions. In contrast,\nvertical processes involve interaction in a single spatial position\nbut across various levels of abstraction. Although Zucker counts\nboth types of vertical and horizontal processes contributing to\nGestalt-like similarity grouping, self-attention formulation only ﬁts\nthe deﬁnition of horizontal relaxation labeling process and thus,\nimplements a special class of RL. As a ﬁnal note, while traditional\nRL relies on several iterations to achieve consistent labeling\nacross all positions, horizontal processes in vision transformers\nare limited to a single iteration and therefore, a single iteration of\nGestalt-like similarity grouping is performed in each transformer\nencoder block.\n/two.tnum./one.tnum./two.tnum. Transformer encoders are feed-forward\nmodels\nEven though the formulation of self-attention in vision\ntransformers suggests Gestalt-like similarity grouping, this alone\ndoes not rule out the possibility of performing attention in these\nmodules. We consider this possibility in this section.\nIt is now established that humans employ a set of mechanisms,\ncalled visual attention, that limit visual processing to sub-regions\nof the input to manage the computational intractability of the\nvision problem (\nTsotsos, 1990, 2017). Despite the traditional Gestalt\nview, modern attention research ﬁndings suggest a set of bottom-\nup and top-down mechanisms determine the target of attention.\nFor example, visual salience [“the distinct subjective perceptual\nquality which makes some items in the world stand out from\ntheir neighbors and immediately grab our attention” (\nItti, 2007 )]\nis believed to be a bottom-up and stimulus-driven mechanism\nemployed by the visual system to select a sub-region of the\ninput for further complex processing. Purely feed-forward (also\ncalled bottom-up) processes, however, were shown to be facing\nan intractable problem with exponential computational complexity\n(\nTsotsos, 2011). Additionally, experimental evidence suggests that\nvisual salience ( Desimone and Duncan, 1995 ) as well as other low-\nlevel visual factors could be aﬀected by feedback (also known as top-\ndown) and task-speciﬁc signals (\nFolk et al., 1992 ; Bacon and Egeth,\n1994; Kim and Cave, 1999 ; Yantis and Egeth, 1999 ; Lamy et al.,\n2003; Connor et al., 2004 ; Baluch and Itti, 2011 ; Peterson, 2015 ).\nIn other words, theoretical and experimental ﬁndings portray\nan important role for top-down and guided visual processing.\nFinally,\nHerzog and Clarke (2014) showed how a visual processing\nstrategy for human vision cannot be both hierarchical and strictly\nfeed-forward through an argument that highlights the role of visual\ncontext. A literature going back to the 1800’s extensively documents\nhuman attentional abilities (\nItti et al., 2005 ; Carrasco, 2011 ; Nobre\net al., 2014 ; Tsotsos, 2022; Krauzlis et al., 2023 ).\nModern understanding of visual attention in humans\nprovides a guideline to evaluate current computational models\nfor visual attention. Vision transformers are among more\nrecent developments that are claimed to implement attention\nmechanisms. However, it is evident that these models with\ntheir purely feed-forward architectures implement bottom-up\nmechanisms. Therefore, if it can be established that these models\nFrontiers in Computer Science /zero.tnum/five.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nimplement attention mechanisms, they can only capture the\nbottom-up signals that contribute to visual attention and not all\naspects of visual attention known in humans. These observations\ncall for a careful investigation of the eﬀect of attention on visual\nprocessing in these models.\n/two.tnum./two.tnum. Experiments\nIn our experiments, we will consider the output of the attention\nmodule in each model block (the green rectangle in\nFigure 1) before\nthe residual connection. In both experiments, we removed the class\ntoken from our analysis. Suppose that an attention module receives\nan input of size H × W × C, where H, W, and C represent height,\nwidth and feature channels. Then, the output, regardless of whether\nthe attention module is multi-head or not, will also be of size\nH ×W ×C. In what follows, the term attention map is used for each\nH×W component of the attention module output along each single\nfeature dimension c ∈ { 1, 2, . . ., C}. In other words, the values\ncomprising each attention map are obtained from the attention\nscores (Equation 1), along a single feature dimension. Also, feature\nchannel and hidden channel will be employed interchangeably.\nIt is important to emphasize that the attention maps we\nconsider for our experiments and evaluations diﬀer from those\noften visualized in the vision transformer literature. Speciﬁcally,\nin our evaluations, we consider what the model deems as salient,\nthe regions that aﬀect further processing in later model blocks.\nIn contrast, what is commonly called an attention map in\nprevious work (\nDosovitskiy et al., 2021 ) is computed for a token,\nusually the output token in vision transformers and by recursively\nbacktracking the compatibility of the token with other tokens to the\ninput layer (\nAbnar and Zuidema, 2020 ). Therefore, a diﬀerent map\ncan be plotted for the various class tokens in the model and these\nmaps are conditioned on the given token. One can interpret these\nmaps as regions of input that are most relevant to yielding the given\nclass token. Also, note that the compatibility (result of the softmax\nfunction in Equation 1) employed for this visualization, is only part\nof what (\nVaswani et al., 2017 ) called the attention score deﬁned\nEquation 1. Maps obtained with this approach do not serve our\ngoal: we seek to determine regions of the input that were considered\nas salient, as\nXu et al. (2015) put it, and were the focus of attention\nduring the bottom-up ﬂow of the signal in inference mode. These\nregions with high attention scores from Equation (1) are those\nthat aﬀect the visual signal through the residual connection (the +\nsign after the green rectangle in\nFigure 1). Hence, we evaluated the\noutput of the attention module in both experiments.\n/two.tnum./two.tnum./one.tnum. Experiment /one.tnum: similarity grouping\nTo quantify Gestalt-like similarity grouping in vision\ntransformers, we created a dataset for similarity grouping with\nexamples shown in\nFigure 2 and measured similarity grouping\nperformance in vision transformers mentioned in Section 2.1. As\nexplained earlier, the attention from Equation (1) signals grouping\namong tokens. Therefore, we measured similarity grouping by\nrecording and analyzing the output of attention modules in\nthese models.\n/two.tnum./two.tnum./one.tnum./one.tnum. Dataset\nEach stimulus in the dataset consists of four rows of ﬁgures\nwith features that diﬀer along a single visual feature dimension\nincluding hue, orientation, lightness, shape, orientation and size.\nEach stimulus is 224 × 224 pixels and contains two perceptual\ngroups of ﬁgures that alternate between the four rows. The values\nof the visual feature that formed the two groups in each stimulus\nwere randomly picked.\nIn some vision transformers, such as ViT, the token size\nand position are ﬁxed from input and across the hierarchy. This\nproperty has been considered a shortcoming in these models when\nemployed in visual tasks and various work attempted to address this\nissue (\nLiu et al., 2021 ). Since we included vision transformers that\nemploy ViT as their base architecture in our study, and in order to\ncontrol for the token size and position in our analysis, we created\nthe dataset such that each ﬁgure in the stimulus would ﬁt within a\nsingle token of ViT. In this case, each ﬁgure ﬁts a 16 × 16 pixels\nsquare positioned within ViT tokens. To measure the eﬀect of ﬁxed\ntokens on grouping, we created two other sets of stimuli. In the\nﬁrst set, we considered the center of every other token from ViT\nas a ﬁxed position for ﬁgures and generated stimuli with ﬁgures\nthat would ﬁt 32 × 32 pixels squares. In this case, each ﬁgure will\nbe relatively centered at a ViT token, but will span more than a\nsingle token. In the second set, we generated stimuli with ﬁgures\nthat were token-agnostic. We designed these stimuli such that the\nset of ﬁgures was positioned at the center of the image instead of\nmatching token positions, with each ﬁgure size ﬁtting a 37 × 37\npixels square.\nEach version of our grouping dataset consists of 600 images\nwith 100 stimuli per visual feature dimension, summing to a total\nof 1,800 stimuli for all three versions.\n/two.tnum./two.tnum./one.tnum./two.tnum. Evaluation and metrics\nFor a self-attention module that yields a H ×W ×C map, where\nH and W represent height and width and C the number of feature\nchannels, we ﬁrst normalized the attention maps across individual\nfeature channels so that attention scores are in the [0, 1] range.\nThen, we measured grouping along each feature channel based on\ntwo metrics:\n• Grouping index: Suppose Ag1 and Ag2 represent the average\nattention score of pixels belonging to ﬁgures in group 1 and\ngroup 2, respectively. We deﬁned the grouping index as:\nGI = ∥Ag1 − Ag2∥\nAg1 + Ag2\n. (2)\nThe grouping index GI varies in [0, 1], with larger values\nindicating better grouping of one group of ﬁgures in the\nstimulus along the feature channel.\n• Figure-background ratio: The overall performance of vision\ntransformers will be impacted if background tokens are\ngrouped with ﬁgure tokens (mixing of ﬁgure and ground).\nTherefore, we measured the ﬁgure-background attention\nratio as:\nAR = max( Ag1\nAbkg\n, Ag2\nAbkg\n), (3)\nFrontiers in Computer Science /zero.tnum/six.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /two.tnum\nSimilarity grouping stimuli examples. The stimuli in this dat aset consists of two groups deﬁned according to diﬀerence in one of hue, saturation,\nlightness, shape, orientation, size features. Each stimuli ha s four rows with alternating ﬁgures from the two groups. The values for the visual features\nthat deﬁne the groups are chosen randomly. The shape set in this da taset consists of rectangle, triangle, ellipse, star, rhombus, right triangles,\ntrapezoid, hexagon and square. Examples in this ﬁgure were picke d from the version in which each ﬁgure ﬁts within a /three.tnum/seven.tnum× /three.tnum/seven.tnum square.\nwhere Ag1, Ag2 represent the average attention for group\n1 and group 2 ﬁgures, respectively, and Abkg is the average\nscore of background. The attention ratio AR is positive and\nvalues larger than 1 indicate the attention score of at least\none group of ﬁgures is larger than that of the background\n(the larger the ratio, the less the mixing of ﬁgure and\nground). Note that the attention ratio AR signiﬁes the relative\nattention score assigned to ﬁgure and ground. Therefore,\nvalues close to 1 suggest similar attention scores assigned to\nﬁgure and ground, quite contrary to the expected eﬀect from\nattention mechanisms.\nFor each stimulus, we excluded all feature dimensions along\nwhich both Ag1 = 0 and Ag2 = 0 from our analysis. This happens\nwhen, for example, the feature channels represent green hues, and\nthe ﬁgures in the stimulus are ﬁgures of red and blue. Moreover,\nwhen analyzing AR, we excluded all channels with Abkg = 0 as our\ngoal was to investigate grouping of ﬁgure and ground when some\nattention was assigned to the background.\n/two.tnum./two.tnum./two.tnum. Experiment /two.tnum: singleton detection\nEvidence for similarity grouping does not disprove\nimplementation of attention in vision transformers. Since\nthese models are feed-forward architectures, investigating the\neﬀect of attention modules in their visual processing must be\nrestricted to bottom-up mechanisms of attention. Therefore,\nwe limited our study to evaluating the performance of these\nmodels in the task of singleton detection as an instance of saliency\ndetection (see\nBruce et al., 2015 ; Kotseruba et al., 2019 for a\nsummary of saliency research). Speciﬁcally, strong performance on\nsaliency detection would suggest that these models implement the\nbottom-up mechanisms deployed in visual attention.\nIn this experiment, we recorded the attention map of all blocks\nin vision transformers mentioned in Section 2.1. Following\nZhang\nand Sclaroﬀ (2013), we computed an average attention map for\neach transformer block by averaging over all the attention channels\nand considered the resulting map as a saliency map. Then, we\ntested if the saliency map highlights the visually salient singleton.\nAdditionally, we combined the feature maps obtained after the\nresidual operation of attention modules and evaluated saliency\ndetection performance for the average feature map. It is worth\nnoting that self-attention modules, and not the features maps, are\nexpected to highlight salient regions as the next targets for further\nvisual processing. Nonetheless, for a better understanding of the\nvarious representations and mechanisms in vision transformers, we\nincluded feature-based saliency maps in our study.\n/two.tnum./two.tnum./two.tnum./one.tnum. Dataset\nFor the singleton detection experiment, we utilized the\npsychophysical patterns (P 3) and odd-one-out (O 3) dataset\nintroduced by\nKotseruba et al. (2019). Examples of each set are\nshown in Figure 3. The P 3 dataset consists of 2,514 images of size\n1,024×1,024. Each image consists of ﬁgures on a regular 7 × 7 grid\nwith one item as the target that is visually diﬀerent in one of color,\norientation or size from other items in the stimulus. The location of\nthe target is chosen randomly. The O 3 dataset includes 2,001 images\nwith the largest dimension set to 1,024. In contrast to the grouping\nand P3 datasets whose stimuli were synthetic images, the O 3 dataset\nconsists of natural images. Each image captures a group of objects\nthat belong to the same category with one that stands out (target)\nfrom the rest (distractors) in one or more visual feature dimensions\n(color, texture, shape, size, orientation, focus and location). The\nO3 with natural images provides the opportunity to investigate the\nperformance of the vision transformer models in this study on the\nsame type of stimuli those were trained. Both P 3 and O3 datasets are\npublicly available and further details of both datasets can be found\nin\nKotseruba et al. (2019).\n/two.tnum./two.tnum./two.tnum./two.tnum. Metrics\nWe followed Kotseruba et al. (2019) to measure singleton\ndetection performance in vision transformers. We employed their\npublicly available code for the computation of metrics they used\nto study traditional and deep saliency models. The number of\nﬁxation and saliency ratio were measured for P 3 and O 3 images,\nrespectively, as explained below.\n• Number of ﬁxations:\nKotseruba et al. (2019) used the number\nof ﬁxations required to detect pop-out as a proxy for salience.\nSpeciﬁcally, they iterated through the maxima of the saliency\nmap until the target was detected or a maximum number\nof iterations was reached. At each iteration that resembles\na ﬁxation of the visual model on a region of input, they\nsuppressed the ﬁxated region with a circular mask before\nmoving the ﬁxation to the next maxima. Lower number of\nFrontiers in Computer Science /zero.tnum/seven.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /three.tnum\nSamples of stimuli from P /three.tnumand O /three.tnumdatasets introduced by\nKotseruba et al. (/two.tnum/zero.tnum/one.tnum/nine.tnum) are illustrated. These datasets consist of stimuli with sin gletons in\nvarious feature dimensions. (A) Examples from the psychophysical patterns (P /three.tnum) dataset. The singletons in this dataset are deﬁned according t o color,\norientation and size. (B) Examples from the odd-one-out (O /three.tnum) dataset with singletons in color, size, texture, shape and orien tation feature dimensions.\nﬁxations indicates higher relative salience of the target to that\nof distractors.\n• Saliency ratio:\nKotseruba et al. (2019) employed the ratio\nof the maximum saliency of the target vs. the maximum\nsaliency of the distractors. They also measured the ratio of\nthe maximum saliency of the background to the maximum\nsaliency of the target. These two ratios that are referred to\nas MSRtarg and MSRbg determine if the target is more salient\nthan the distractors or the background, respectively. Ideally,\nMSRtarg is >1 and MSRbg is <1.\n/three.tnum. Results\n/three.tnum./one.tnum. Experiment /one.tnum: similarity grouping\nEach vision transformer in our study consists of a stack of\ntransformer encoder blocks. In this experiment, our goal was to\ninvestigate similarity grouping in attention modules in transformer\nencoder blocks. We were also interested in changes in similarity\ngrouping over the hierarchy of transformer encoders. Therefore,\nfor each vision transformer, we took the following steps: We\nﬁrst isolated transformer encoders in the model and computed\nthe grouping index ( GI) and attention ratio ( AR) per channel as\nexplained in Section 2.2.1.2. Then, we considered the mean GI and\nAR per block as the representative index and ratio of the layer.\nFigure 4A shows the mean GI for the architecture called “ViT-\nbase-patch16-224” in Table 1 over all layers of the hierarchy. The\nGI is plotted separately according to the visual feature that diﬀered\nbetween the groups of ﬁgures. This plot demonstrates that GI for\nall blocks of this model across all tested feature dimensions is\ndistinctly larger than 0, suggesting similarity grouping of ﬁgures\nin all attention modules of this architecture. Interestingly, despite\nsome variations in the ﬁrst block, all layers have relatively similar\nGI. Moreover, the grouping indices for all feature dimensions are\nclose, except for hue with GI larger than 0.6 in the ﬁrst block,\nindicating stronger grouping among tokens based on this visual\nfeature.\nFigure 4B depicts the mean AR for the same architecture, ViT-\nbase-patch16-224, for all the encoder blocks. Note that all curves in\nthis plot are above the AR = 1 line denoted as a dashed gray line,\nindicating that all attention modules assign larger attention scores\nto at least one group of ﬁgures in the input vs. the background\ntokens. However, notable is the steep decline in the mean AR across\nthe hierarchy. This observation conﬁrms the previous reports of\nsmoother attention maps in higher stages of the hierarchy (\nPark\nand Kim, 2022 ) with similar attention assigned to ﬁgure and\nbackground tokens.\nFigure 5 shows the mean GI for all the architectures from\nTable 1 separately based on the visual feature that deﬁned the\ngroups in the input. All models, across all their layers, with some\nexceptions, demonstrate mean GI that are distinctly larger than\n0. The exceptions include the ﬁrst layer of all BEiT architectures\nand Swin-small-patch4-window7-224, and the last block of CvT-\n13 and CvT-21. Interestingly, BEiT and Swin architectures jump\nin their mean GI in their second block. Even though DeiT and\nBEiT architectures utilized the same architecture as ViT but trained\nthe model with more modern training regimes, both models\ndemonstrate modest improvement over ViT-base-patch16-224.\nPlots in\nFigure 6 depict the mean AR over all the\narchitectures. Interestingly, ViT-base-patch16-224 is the only\narchitecture whose mean AR for the ﬁrst block is the largest\nin its hierarchy and unanimously for all visual features.\nAmong the three DeiT architectures (tiny, small, and base),\nDeiT-tiny-distilled-patch16-224, demonstrates larger mean AR\nFrontiers in Computer Science /zero.tnum/eight.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /four.tnum\nMean grouping index and attention ratio for the ViT-base-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum architecture over all stimuli but separated according to the visual features\nthat deﬁned the groups of ﬁgures in the input. (A) The mean grouping index is larger than /zero.tnum./two.tnum for all layers of the model across all visual features,\nsuggesting perceptual grouping based on similarity in this archi tecture. (B) The attention ratio of larger than /one.tnum for all transformer encoder blocks of\nViT-base-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum indicates larger scores are assigned toﬁgure tokens. However, the steep decline in the AR ratio in the hierarchy demonstrates\nmixing of ﬁgure and background tokens due to similar attention scor es. (A) Mean grouping index (GI). (B) Mean attention ratio (AR).\nratios. Comp ared to ViT, DeiT-tiny-distilled-patch16-224 has\nfar fewer parameters and the comparable mean AR for this\narchitecture with Vit conﬁrms the suggestion of\nTouvron et al.\n(2021) that an eﬃcient training regime in a smaller model\ncould result in performance gain against a larger model. Results\nfrom\nFigure 6 are also interesting in that all of Swin and CvT\narchitectures that are claimed to adapt transformer models to the\nvision domain, have relatively small mean AR over their hierarchy.\nThese results show that these models mix ﬁgure and background\ntokens in their attention score assignments, an observation that\ndeserves further investigation in a future work.\nFinally,\nFigure 7 summarizes the mean grouping index GI\nfor the DeiT-base-distilled-patch16-224 architecture over the three\nversions of the grouping dataset as explained in Section 2.2.1.1.\nThese results demonstrate similar grouping index over all three\nversions, suggesting little impact of token position and size relative\nto ﬁgures in the input.\n/three.tnum./two.tnum. Experiment /two.tnum: singleton detection\nGenerally, in saliency experiments, the output of the model\nis considered for performance evaluation. In this study, however,\nnot only we were interested in the overall performance of vision\ntransformers (the output of the last block), but also in the\ntransformation of the saliency signal in the hierarchy of these\nmodels. Examining the saliency signal over the hierarchy of\ntransformer blocks would provide valuable insights into the role of\nattention modules in saliency detection. Therefore, we measured\nsaliency detection in all transformer blocks.\n/three.tnum./two.tnum./one.tnum. The P/three.tnumdataset results\nFollowing\nKotseruba et al. (2019), to evaluate the performance\nof vision transformer models on the P 3 dataset, we measured the\ntarget detection rate at 15, 25, 50, and 100 ﬁxations. Chance level\nperformance for ViT-base-patch16-224, as an example, would be\n6, 10, 20, and 40% for 15, 25, 50, and 100 ﬁxations, respectively\n(masking after each ﬁxation explained in Section 2.2.2 masks an\nentire token). Although these levels for the various models would\ndiﬀer due to diﬀerences in token sizes and incorporating multiple\nscales, these chance level performances from ViT-base-patch16-224\ngive a baseline for comparison.\nFigure 8 demonstrates the performance of saliency maps\nobtained from attention and feature maps of all ViT-base-patch16-\n224 blocks. These plots clearly demonstrate that the feature-\nbased saliency maps in each block outperform those computed\nfrom the attention maps. This is somewhat surprising since\nas explained in Section 2.2.2, if vision transformers implement\nattention mechanisms, attention modules in these models are\nexpected to highlight salient regions in the input for further visual\nprocessing. Nonetheless, plots in\nFigure 8 tell a diﬀerent story,\nnamely that feature maps are preferred options for applications\nthat require singleton detection. Comparing target detection rates\nacross color, orientation and size for both attention and feature\nmaps demonstrate higher rates in detecting color targets compared\nto size and orientation. For all three of color, orientation and size,\nthe target detection rates peak at earlier blocks for attention-based\nsaliency maps and decline in later blocks, with lower than chance\nperformance for most blocks. This pattern is somewhat repeated in\nfeature-based saliency maps with more ﬂat curves in the hierarchy,\nespecially for a larger number of ﬁxations.\nSimilar detection rate patterns were observed in other vision\ntransformer models. However, due to limited space, we refrain\nfrom reporting the same plots as in\nFigure 8 for all the vision\ntransformer models that we studied. These plots can be found in\nthe\nSupplementary material. Here, for each model, we report the\nmean target detection rate over all blocks and the detection rate\nfor the last block of each model for both attention and feature-\nbased saliency maps. These results are summarized in\nFigures 9,\n10 for the last and mean layer target detection rates, respectively.\nConsistent with the observations from ViT-base-patch16-224 in\nFrontiers in Computer Science /zero.tnum/nine.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /five.tnum\nMean grouping index for all the architectures from\nTable /one.tnumplotted separately for each visual feature that deﬁned the per ceptual grouping of ﬁgures in\nthe input. For comparison purposes, we plotted the grouping index of all models within the same range on the x-axis. Accordingly, we labeled the\nﬁrst and last block on the horizontal axis. These plots demonstrate that all model s perform similarity grouping of ﬁgures based on low-level visual\nfeatures such as hue and orientation. Except for the ﬁnal block of CvT models, all layers of all architectures have mean GI higher than /zero.tnum. The legend at\nthe top applies to all plots.\nFigure 8, the feature-based saliency maps outperform attention-\nbased ones in Figure 9 and in general have higher detection rates\nthan the chance levels stated earlier. The attention-based saliency\nmaps, across most of the models, fail to perform better than\nchance. Generally, all models have higher detection rates for color\ntargets, repeating similar results reported by\nKotseruba et al. (2019).\nInterestingly, Swin architectures that incorporate multiple token\nscales, perform poorly in detecting size targets with both feature\nand attention-based saliency maps.\nResults for mean target detection rates over all blocks in\nFigure 10 are comparable to those of last layer detection rates,\nexcept for a shift to higher rates. Speciﬁcally, all models are\nmore competent at detecting color targets and that the feature-\nbased saliency maps look more appropriate for singleton detection.\nIn Swin architectures, the mean detection rate of feature-based\nsaliency maps are relatively higher for size targets than that of other\nmodels. This observation, together with the last layer detection\nrate of Swin models for size targets suggest that incorporating\nmultiple scales in vision transformers improves representing ﬁgures\nof various sizes but the eﬀect fades higher in the hierarchy.\nIn summary, the attention maps in vision transformers were\nexpected to reveal high salience for the target vs. distractors.\nNonetheless, comparing the detection rate of attention-based\nsaliency maps in vision transformers at 100 ﬁxations with those\nof traditional and deep saliency models reported by\nKotseruba\net al. (2019) suggest that not only do the attention modules in\nvision transformers fail to highlight the target, but also come\nshort of convolution-based deep saliency models with no attention\nmodules. Although the feature-based saliency maps in vision\ntransformers showed promising results in target detection rates\nFrontiers in Computer Science /one.tnum/zero.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /six.tnum\nMean attention ratio for all the architectures from\nTable /one.tnumplotted separately for each visual feature that deﬁned the per ceptual grouping of ﬁgures in\nthe input. Similar to Figure /five.tnum, and for ease of comparison, we plotted the AR for all models within the same range on the x-axis. Interestingly, Swin\nand CvT, two models that adapted ViT to the visual domain, have r elatively smaller attention ratios compared to the rest of the a rchitectures,\nsuggesting that incorporating scale and shifting the token position in Swin and convolution in CvT architectures results in mixing o f ﬁgure and\nbackground representations and consequently attention scores. A mong the other architectures that use ViT as the underlying mode l, the attention\nratio plots are somewhat similar to those of Figure /four.tnumB, that is, larger attention ratios in earlier blocks with a declin e in the hierarchy.\nrelative to attention-based maps, in comparison with convolutional\nsaliency models (see\nKotseruba et al., 2019 , their Figure 3),\nthose performed relatively similar to convolution-based models.\nTogether, these results suggest that contrary to the expectation,\nthe proposed attention mechanisms in vision transformers are\nnot advantageous vs. convolutional computations in representing\nvisual salience.\n/three.tnum./two.tnum./two.tnum. The O/three.tnumdataset results\nWe measured the maximum saliency ratios MSRtarg and MSRbg\nfor feature and attention-based saliency maps of all blocks of\nvision transformers in\nTable 1. These ratios are plotted in Figure 11,\ndemonstrating poor performance of all models in detecting the\ntarget in natural images of the O 3 dataset. We acknowledge that\nwe expected improved performance of vision transformers on\nthe O 3 dataset with natural images compared to the results on\nsynthetic stimuli of the P 3 dataset. However, whereas MSRtarg\nratios larger than 1 are expected (higher salience of target vs.\ndistractors), in both feature and attention-based saliency maps, the\nratios were distinctly below 1 across all blocks of all models, with\nthe exception of later blocks of two BEiT architectures. Notable\nare the feature-based ratios of ViT-base-patch16-224 with peaks in\nearlier blocks and a steep decrease in higher layers. In contrast, all\nthree BEiT architectures show the opposite behavior and perform\npoorly in earlier blocks but correct the ratio in mid-higher stages\nof processing.\nThe MSRbg ratios illustrated in\nFigure 11 follow a similar theme\nas MSRtarg ratios. Even though MSRbg ratios <1 suggest that the\ntarget is deemed more salient than the background, most of these\nFrontiers in Computer Science /one.tnum/one.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /seven.tnum\nEach token in ViT-based architectures has a ﬁxed position and si ze across the hierarchy of transformer encoders. This property i s noted as a\nshortcoming of some vision transformers. To control for position and si ze of tokens in these models, we designed our grouping dataset accord ing to\nthe ViT model tokens such that each ﬁgure in the stimulus would ﬁt w ithin and positioned inside the model /one.tnum/six.tnum× /one.tnum/six.tnum tokens. To test for the eﬀect of\nﬁgure size, we introduced a version of grouping dataset with ﬁgures centered at every other ViT token but larger in size such that eac h ﬁgure would\nﬁt a /three.tnum/two.tnum× /three.tnum/two.tnum square. We also introduced a third version where ﬁgures in thestimuli were token-agnostic. In the third version, the set of ﬁgur es occupy\nthe center of image and each ﬁgure ﬁts within a /three.tnum/seven.tnum× /three.tnum/seven.tnum square. We tested the grouping performance of the DeiT-base-distilled-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum\narchitecture over all three versions of the dataset. Note that D eiT-base-distilled-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum utilizes an identical architecture as\nViT-base-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum with a diﬀerent training regime. Our results over the various visual features in the dataset demonstr ate comparable results\nover the three versions of the dataset, suggesting no signiﬁcant e ﬀect of token position or size in grouping in vision transformers.\nmodels have MSRbg ratios larger than 1 in their hierarchy. Among\nall models, feature-based saliency of BEiT and Swin architectures\nhave the best overall performance.\nFor a few randomly selected images from the O 3 dataset,\nFigures 12–14 demonstrate the attention-based saliency map of the\nblock with best MSRtarg ratio for each model. Each saliency map in\nthese ﬁgures is scaled to the original image size for demonstration\npurposes. Interestingly, saliency maps in\nFigure 13 show how the\nsame BEiT model with varying training result in vastly diﬀerent\nattention-based maps.\nTo summarize, for a bottom-up architecture that is claimed to\nimplement attention mechanisms, we expected a boost in saliency\ndetection compared to convolution-based models with no explicit\nattention modules. Our results on the O 3 dataset, however, point\nto the contrary, speciﬁcally in comparison with the best ratios\nreported in\nKotseruba et al. (2019) for MSRtarg and MSRbg at 1.4\nand 1.52, respectively. These results, together with the proposal\nof\nLiu et al. (2022) for a modernized convolution-based model with\ncomparable performance to vision transformers, overshadow the\nclaim of attention mechanisms in these models.\nFrontiers in Computer Science /one.tnum/two.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /eight.tnum\nTarget detection rate of the ViT-base-patch/one.tnum/six.tnum-/two.tnum/two.tnum/four.tnum model for /one.tnum/five.tnum,/two.tnum/five.tnum, /five.tnum/zero.tnum, and /one.tnum/zero.tnum/zero.tnum ﬁxations on images of the P/three.tnumdataset. Legend on top applies to all\nplots. For this model with /one.tnum/six.tnum× /one.tnum/six.tnum pixels tokens, each masking after a ﬁxation masks almost an entire token. Therefore, chance performance will be at\n/six.tnum, /one.tnum/zero.tnum, /two.tnum/zero.tnum, and /four.tnum/zero.tnum% for /one.tnum/five.tnum, /two.tnum/five.tnum, /five.tnum/zero.tnum, and /one.tnum/zero.tnum/zero.tnum ﬁxations. Comparing the plots of the left column for attention-based saliency maps vs. those on the right\nobtained from feature-based saliency maps indicates superior p erformance of feature-based maps for salient target detection. This is interesting in\nthat modules claimed to implement attention mechanisms are ex pected to succeed in detecting visually salient ﬁgures in the input. Overall, for both\nattention and feature-based maps, color targets have higher dete ction rates vs. orientation and size, the conditions in which perf ormance is mainly at\nchance level for all ﬁxation thresholds and across all blocks in th e ViT hierarchy. Additionally, in both attention and feature-b ased maps, performance\npeaks in earlier blocks and declines in later layers, suggestin g multiple transformer encoder blocks mix representations acros s spatial locations such\nthat the model cannot detect the visually salient target almost i mmediately or even by chance.\nFrontiers in Computer Science /one.tnum/three.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /nine.tnum\nTarget detection rate of the last block of vision transformers inv estigated in this work for /one.tnum/five.tnum, /two.tnum/five.tnum, /five.tnum/zero.tnum, and /one.tnum/zero.tnum/zero.tnum ﬁxations on stimuli fromthe P /three.tnumdataset.\nThe chance level performance of the ViT model is plotted as dashe d lines with matching colors for each ﬁxation threshold. Similar t o the observation\nof ViT, feature-based maps outperform attention-based maps an d generally at rates higher than chance. Color targets are easier t o detect for both\nmap types. Interestingly, both Swin architectures struggle to d etect size targets in both attention and feature-based maps, de spite incorporating\nmultiple scales in their model.\n/four.tnum. Discussion\nOur goal in this work was to investigate if the self-attention\nmodules in vision transformers have similar eﬀects to human\nattentive visual processing. Vision transformers have attracted\nmuch interest in the past few years partly due to out-performing\nCNNs in various visual tasks, and in part due to incorporating\nmodules that were claimed to implement attention mechanisms.\nSpeciﬁcally, the origins of attention mechanisms in transformers\ncould be traced back to the work by\nXu et al. (2015), where they\nFrontiers in Computer Science /one.tnum/four.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum/zero.tnum\nAverage target detection rate over all blocks of vision transformer s for /one.tnum/five.tnum, /two.tnum/five.tnum, /five.tnum/zero.tnum, and /one.tnum/zero.tnum/zero.tnum ﬁxations on stimuli from the P/three.tnumdataset. Compared to\ndetection rates in\nFigure /nine.tnum, mean detection rates are higher for all models in all conditions (c olor, size, and orientation), indicating superior\nperformance of earlier transformer blocks compare to the ﬁnal bl ock in these models. In line with results in Figures /eight.tnum, /nine.tnum, color targets are easier to\ndetect and that generally, feature-based maps outperform atte ntion-based maps in salient target detection.\nintroduced an attention-based model for image captioning. Xu\net al. (2015) motivated modeling attention in their network\nby reference to attention in the human visual system and its\neﬀect that “allows for salient features to dynamically come to\nthe forefront as needed”, especially in the presence of clutter in\nthe input. In light of these observations, a curious question to\nask is if these computational attention mechanisms have similar\neﬀects as their source of inspiration. Despite some previous\nattempts (\nNaseer et al., 2021 ; Tuli et al., 2021 ; Park and Kim,\n2022), the role and eﬀect of the attention modules in vision\nFrontiers in Computer Science /one.tnum/five.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum/one.tnum\nTarget and background saliency ratios ( MSRtarg and MSRbg) for all vision transformer architectures on natural images of th e O /three.tnumdataset. Even though\nthese models were trained on natural images, they fail in assignin g higher saliency to the target object vs. the distractors or backgr ound (smaller than\n/one.tnumMSRtarg and larger than /one.tnumMSRbg ratios) in both feature and attention-based maps. For easier comp arison, we also plotted MSRtarg and MSRbg for\nthe top-/three.tnum best performing deep saliency models from\nKotseruba et al. (/two.tnum/zero.tnum/one.tnum/nine.tnum). These results compared to the reported ratios in Kotseruba et al. (/two.tnum/zero.tnum/one.tnum/nine.tnum)\nfor traditional and deep convolution-based saliency models suggest that the proposed attention mechanisms do not enhance the perform ance of\nvision transformers when the goal is to detect the visually salie nt object in the stimulus.\ntransformers have been largely unknown. To give a few examples,\nin a recent work,\nLi et al. (2023) studied the interactions of the\nattention heads and the learned representations in multi-head\nattention modules and reported segregation of representations\nacross heads. (\nAbnar and Zuidema, 2020 ) investigated the eﬀect\nof various approaches for visualizing attention map as an\ninterpretability step and with their attention rollout approach\noften employed for this purpose.\nGhiasi et al. (2022) visualized\nthe learned representations in vision transformers and found\nsimilarity to those of CNNs. In contrast,\nCaron et al. (2021)\nand Raghu et al. (2021) reported dissimilarities in learned\nrepresentations across the hierarchy of vision transformers and\nCNNs.\nCordonnier et al. (2020) as well as some others ( D’Ascoli\net al., 2021 ) suggested attention mechanisms as a generalized form\nof convolution. The quest to understand the role and eﬀect of\nattention modules in transformers is still ongoing as these models\nare relatively new and the notable variations in ﬁndings (for\nexample, dis/similarity to CNNs) adds to its importance. Yet, and\nto the best of our knowledge, none of these studies investigated\nif the computations in self-attention modules would have similar\neﬀects on visual processing as those discovered with visual attention\nin humans.\nFrontiers in Computer Science /one.tnum/six.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum/two.tnum\nThe attention-based saliency map of the block with the best MSRtarg ratio for ViT and DeiT models on a select few images from the O /three.tnumdataset. In\neach map, saliency varies from blue (least salient) to yellow (m ost salient). Each saliency map in these ﬁgures is scaled to the original image size for\ndemonstration purposes.In this work, we studied two aspects of processing in\nvision transformers: the formulation of attention in self-attention\nmodules, and the overall bottom-up architecture of these deep\nneural architectures. Our investigation of attention formulation\nin vision transformers suggested that these modules perform\nGestalt-like similarity grouping in the form of horizontal relaxation\nlabeling whereby interactions from multiple spatial positions\ndetermine the update in the representation of a token. Additionally,\nFrontiers in Computer Science /one.tnum/seven.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum/three.tnum\nThe attention-based saliency map of the block with the best MSRtarg ratio for BEiT models on a select few images from the O /three.tnumdataset. In each map,\nsaliency varies from blue (least salient) to yellow (most salien t). Each saliency map in these ﬁgures is scaled to the original im age size for\ndemonstration purposes. An interesting observation is how the var iants of the same model with diﬀering training regimes result i n vastly diﬀerent\nattention-based saliency maps.\ngiven previous evidence on the role of feedback in human visual\nattention (\nFolk et al., 1992 ; Bacon and Egeth, 1994 ; Desimone and\nDuncan, 1995 ; Kim and Cave, 1999 ; Yantis and Egeth, 1999 ; Lamy\net al., 2003 ; Connor et al., 2004 ; Baluch and Itti, 2011 ; Peterson,\n2015), we argued that if vision transformers implement attention\nmechanisms, those can only be in the form of bottom-up and\nstimulus-driven visual salience signals.\nTesting a family of vision transformers on a similarity\ngrouping dataset suggested that the attention modules in these\narchitectures perform similarity grouping and that the eﬀect\ndecays as hierarchical level increases in the hierarchy especially\nbecause more non-ﬁgure tokens are grouped with ﬁgures in\nthe stimulus over multiple transformer encoder blocks. Most\nsurprising, however, were our ﬁndings in the task of singleton\ndetection as a canonical example of saliency detection. With both\nsynthetic and natural stimuli, vision transformers demonstrated\nsub-optimal performance in comparison with traditional and deep\nconvolution-based saliency models.\nThe P 3O3 dataset was designed according to psychological\nand neuroscience ﬁndings on human visual attention.\nFrontiers in Computer Science /one.tnum/eight.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nFIGURE /one.tnum/four.tnum\nThe attention-based saliency map of the block with the best MSRtarg ratio for CvT and Swin models on a select few images from the O /three.tnumdataset. In\neach map, saliency varies from blue (least salient) to yellow (m ost salient). Each saliency map in these ﬁgures is scaled to the original image size for\ndemonstration purposes.Kotseruba et al. (2019) demonstrated a gap between human\nperformance and traditional/CNN-based saliency models in\nsingleton detection tasks. The fact that\nKotseruba et al. (2019)\nreported that training CNN-based saliency models on these\nstimuli did not improve their performance, hints on a more\nfundamental diﬀerence between the two systems. Several other\nFrontiers in Computer Science /one.tnum/nine.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nworks have provided evidence on the lack of human equivalence\nin deep neural networks (\nGhodrati et al., 2014 ; Dodge and\nKaram, 2017 ; Kim et al., 2018 ; Geirhos et al., 2019 ; Horikawa\net al., 2019 ; Hu et al., 2019 ; RichardWebster et al., 2019 ; Wloka\nand Tsotsos, 2019 ; Baker et al., 2020 ; Lonnqvist et al., 2021 ;\nRicci et al., 2021 ; Xu and Vaziri-Pashkam, 2021a ,b, 2022;\nAyzenberg and Lourenco, 2022 ; Feather et al., 2022 ; Fel et al.,\n2022; Vaishnav et al., 2022 ; Zerroug et al., 2022 ; Zhou Q. et al.,\n2022) on various aspects of visual processing. The claim of\nimplementing attention mechanisms in vision transformers oﬀered\nthe possibility that these models might be more human-like.\nThis impression was conﬁrmed in the work of\nTuli et al. (2021)\nwho reported that vision transformers are more human-like\nthan CNNs based on performance on the Stylized ImageNet\ndataset (\nGeirhos et al., 2019 ). Our work, however, adds to the\nformer collection of studies and reveals a gap between human\nvisual attention and the mechanisms implemented in vision\ntransformers.\nThis work can be further extended in several directions. For\nexample, even though\nKotseruba et al. (2019) found training CNN-\nbased saliency models on the O 3 dataset did not improve their\nsaliency detection performance, an interesting experiment is to\nﬁne-tune vision transformers on the O 3 dataset and evaluate the\nchange or lack of change in their saliency detection performance.\nAdditionally, incorporating vertical visual processes into the\nformulation in Equation (1) is another avenue to explore in\nthe future.\nTo conclude, not only does our deliberate study of attention\nformulation and the underlying architecture of vision transformers\nsuggest that these models perform perceptual grouping and do\nnot implement attention mechanisms, but also our experimental\nevidence, especially from the P 3O3 datasets conﬁrms those\nobservations. The mechanisms implemented in self-attention\nmodules of vision transformers can be interpreted as lateral\ninteractions within a single layer. In some architectures, such as\nViT, the entire input deﬁnes the neighborhood for these lateral\ninteractions, in some others (\nYang et al., 2021 ) this neighborhood\nis limited to local regions of input. Although Liu et al. (2022)\nfound similar performance in a modernized CNNs, the ubiquity\nof lateral interactions in the human and non-human primate\nvisual cortex (\nStettler et al., 2002 ; Shushruth et al., 2013 ) suggest\nthe importance of these mechanisms in visual processing. Our\nobservation calls for future studies to investigate whether vision\ntransformers show the eﬀects that are commonly attributed\nto lateral interactions in the visual cortex such as crowding,\ntilt illusion, perceptual ﬁlling-in, etc. (\nLin et al., 2022 ). Self-\nattention in vision transformers performs perceptual organization\nusing feature similarity grouping, not attention. Additionally,\nconsidering Gestalt principles of grouping, vision transformers\nimplement a narrow aspect of perceptual grouping, namely\nsimilarity, and other aspects such as symmetry and proximity\nseem problematic for these models. The term attention has a\nlong history going back to the 1800’s and earlier (see\nBerlyne,\n1974) and in computer vision to 1970’s (for examples, see Hanson\nand Riseman, 1978 ). With decades of research on biological\nand computational aspects of attention, the confusion caused by\ninappropriate use of terminology and technical term conﬂation\nhas already been problematic. Therefore, we remain with the\nsuggestion that even though vision transformers do not perform\nattention as claimed, they incorporate visual mechanisms in deep\narchitectures that were previously absent in CNNs and provide\nnew opportunities for further improvement of our computational\nvision models.\nData availability statement\nThe original contributions presented in the study are included\nin the article/ Supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nPM and JT developed the theoretical formalisms, analyzed\nthe data, and wrote the manuscript. PM contributed to the\nimplementation, designed, and carried out the experiments.\nAll authors contributed to the article and approved the\nsubmitted version.\nFunding\nThis research was supported by several sources for which\nthe authors are grateful: Air Force Oﬃce of Scientiﬁc Research\n[grant numbers FA9550-18-1-0054 and FA9550-22-1-0538\n(Computational Cognition and Machine Intelligence, and\nCognitive and Computational Neuroscience Portfolios)],\nthe Canada Research Chairs Program (grant number 950-\n231659), and the Natural Sciences and Engineering Research\nCouncil of Canada (grant numbers RGPIN-2016-05352\nand RGPIN-2022-04606).\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nSupplementary material\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fcomp.\n2023.1178450/full#supplementary-material\nFrontiers in Computer Science /two.tnum/zero.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nReferences\nAbnar, S., and Zuidema, W. (2020). “Quantifying attention ﬂo w in transformers, ” in\nProceedings of the 58th Annual Meeting of the Association for Computati onal Linguistics,\n4190–4197.\nAnderson, B. (2023). Stop paying attention to “attention”. Wiley Interdiscip. Rev.\nCogn. Sci. 14, e1574. doi: 10.1002/wcs.1574\nAyzenberg, V., and Lourenco, S. (2022). Perception of an obje ct’s global shape is\nbest described by a model of skeletal structure in human infan ts. Elife. 11, e74943.\ndoi: 10.7554/eLife.74943\nBacon, W. F., and Egeth, H. E. (1994). Overriding stimulus-dr iven attentional\ncapture. Percept. Psychophys. 55, 485–496. doi: 10.3758/BF03205306\nBaker, N., Lu, H., Erlikhman, G., and Kellman, P. J. (2020). Local features and global\nshape information in object classiﬁcation by deep convolution al neural networks.\nVision Res. 172, 46–61. doi: 10.1016/j.visres.2020.04.003\nBaluch, F., and Itti, L. (2011). Mechanisms of top-down attent ion. Trends Neurosci.\n34, 210–224. doi: 10.1016/j.tins.2011.02.003\nBao, H., Dong, L., Piao, S., and Wei, F. (2022). “BEit: BERT pre -training of image\ntransformers, ” inInternational Conference on Learning Representations .\nBerlyne, D. E. (1974). “Attention, ” in Handbook of Perception ., Chapter 8, eds E. C.\nCarterette, and M. P. Friedman (New York, NY: Academic Press ).\nBhojanapalli, S., Chakrabarti, A., Glasner, D., Li, D., Unterthi ner, T., and Veit,\nA. (2021). “Understanding robustness of transformers for i mage classiﬁcation, ” in\nProceedings of the IEEE/CVF International Conference on Computer Vision (IC CV),\n10231–10241.\nBorji, A., and Itti, L. (2012). State-of-the-art in visual a ttention modeling. IEEE\nTrans. Pattern Anal. Mach. Intell . 35, 185–207. doi: 10.1109/TPAMI.2012.89\nBruce, N. D., Wloka, C., Frosst, N., Rahman, S., and Tsotsos, J . K. (2015). On\ncomputational modeling of visual saliency: examining what’s r ight, and what’s left.\nVision Res. 116, 95–112. doi: 10.1016/j.visres.2015.01.010\nBylinskii, Z., DeGennaro, E. M., Rajalingham, R., Ruda, H., Zha ng, J., and Tsotsos,\nJ. K. (2015). Towards the quantitative evaluation of visual at tention models. Vision Res.\n116:258–268. doi: 10.1016/j.visres.2015.04.007\nCadieu, C. F., Hong, H., Yamins, D. L., Pinto, N., Ardila, D., S olomon,\nE. A., et al. (2014). Deep neural networks rival the representa tion of primate\nit cortex for core visual object recognition. PLoS Comput. Biol . 10, e1003963.\ndoi: 10.1371/journal.pcbi.1003963\nCaron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Boja nowski, P., et al. (2021).\n“Emerging properties in self-supervised vision transformers, ” in Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV) 9650–9660.\nCarrasco, M. (2011). Visual attention: the past 25 years. Vision Res. 51, 1484–1525.\ndoi: 10.1016/j.visres.2011.04.012\nChen, C.-F. R., Fan, Q., and Panda, R. (2021). “Crossvit: cro ss-attention multi-\nscale vision transformer for image classiﬁcation, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 357–366.\nConnor, C. E., Egeth, H. E., and Yantis, S. (2004). Visual att ention: bottom-up versus\ntop-down. Curr. Biol. 14, R850–R852. doi: 10.1016/j.cub.2004.09.041\nCordonnier, J.-B., Loukas, A., and Jaggi, M. (2020). “On the relationship between\nself-attention and convolutional layers, ” in Eighth International Conference on Learning\nRepresentations-ICLR 2020, number CONF .\nDai, Z., Liu, H., Le, Q. V., and Tan, M. (2021). “CoAtNet: marr ying convolution\nand attention for all data sizes, ” in Advances in Neural Information Processing Systems,\nVol. 34 , eds M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan\n(Curran Associates, Inc.), 3965–3977.\nD’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S., Biro li, G., and Sagun, L.\n(2021). “ConViT: improving vision transformers with soft co nvolutional inductive\nbiases, ” inProceedings of the 38th International Conference on Machine Learning, Vol.\n139, eds M. Meila, and T. Zhang (PMLR), 2286–2296.\nDesimone, R., and Duncan, J. (1995). Neural mechanisms of se lective visual\nattention. Ann. Rev. Neurosci . 18, 193–222. doi: 10.1146/annurev.ne.18.030195.001205\nDi Lollo, V. (2018). Attention is a sterile concept; iterative re entry is a fertile\nsubstitute. Conscious. Cogn. 64:45–49. doi: 10.1016/j.concog.2018.02.005\nDodge, S., and Karam, L. (2017). “A study and comparison of hum an and deep\nlearning recognition performance under visual distortions, ” in 2017 26th International\nConference on Computer Communication and Networks (ICCCN) , 1–7.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2021). “An image is worth 16x16 words: transformer s for image recognition\nat scale, ” inInternational Conference on Learning Representations .\nEickenberg, M., Gramfort, A., Varoquaux, G., and Thirion, B. (2 017).\nSeeing it all: convolutional network layers map the function of t he human\nvisual system. Neuroimage. 152, 184–194. doi: 10.1016/j.neuroimage.2016.\n10.001\nFeather, J., Leclerc, G., Ma ¸dry, A., and McDermott, J. H. (202 2). Model metamers\nilluminate divergences between biological and artiﬁcial neur al networks. bioRxiv, pages\n2022–05. doi: 10.32470/CCN.2022.1147-0\nFel, T., Rodriguez, I. F. R., Linsley, D., and Serre, T. (2022). “Harmonizing the object\nrecognition strategies of deep neural networks with humans , ” in Advances in Neural\nInformation Processing Systems , eds A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, K.\nFolk, C. L., Remington, R. W., and Johnston, J. C. (1992). Invo luntary covert\norienting is contingent on attentional control settings. J. Exp. Psychol. Hum. Percept.\nPerform. 18, 1030. doi: 10.1037/0096-1523.18.4.1030\nGeirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann , F. A., and Brendel,\nW. (2019).“Imagenet-trained CNNs are biased towards textu re; increasing shape\nbias improves accuracy and robustness, ” in International Conference on Learning\nRepresentations.\nGhiasi, A., Kazemi, H., Borgnia, E., Reich, S., Shu, M., Goldbu lm, A., et al. (2022).\nWhat do vision transformers learn? A visual exploration. arXiv [Preprint]. arXiv:\n2212.06727. Available online at: https://arxiv.org/pdf/2212.06727.pdf\nGhodrati, M., Farzmahdi, A., Rajaei, K., Ebrahimpour, R., an d Khaligh-Razavi,\nS.-M. (2014). Feedforward object-vision models only tolerate small image variations\ncompared to human. Front. Comput. Neurosci . 8, 74. doi: 10.3389/fncom.2014.00074\nGuo, J., Han, K., Wu, H., Tang, Y., Chen, X., Wang, Y., et al. (202 2).“Cmt:\nconvolutional neural networks meet vision transformers, ” i n 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 12165–12175.\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., et al. (20 22). “A survey on\nvision transformer, ” inIEEE Transactions on Pattern Analysis and Machine Intelligence ,\nVol. 45 (IEEE), 87–110.\nHanson, A., and Riseman, E. (1978). Computer Vision Systems: Papers from the\nWorkshop on Computer Vision Systems . Amherst, MA: Held at the University of\nMassachusetts, Academic Press.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” in 2016 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2016 (Las Vegas, NV: IEEE Computer Society), 770–778.\nHerzog, M. H., and Clarke, A. M. (2014). Why vision is not both h ierarchical and\nfeedforward. Front. Comput. Neurosci . 8, 135. doi: 10.3389/fncom.2014.00135\nHommel, B., Chapman, C. S., Cisek, P., Neyedli, H. F., Song, J.-H ., and Welsh, T.\nN. (2019). No one knows what attention is. Attent. Percept. Psychophys . 81, 288–2303.\ndoi: 10.3758/s13414-019-01846-w\nHorikawa, T., Aoki, S. C., Tsukamoto, M., and Kamitani, Y. (2 019). Characterization\nof deep neural network features by decodability from human br ain activity. Sci. Data 6,\n1–12. doi: 10.1038/sdata.2019.12\nHu, B., Khan, S., Niebur, E., and Tripp, B. (2019). “Figure-gro und representation in\ndeep neural networks, ” in 2019 53rd Annual Conference on Information Sciences and\nSystems (CISS) (IEEE), 1–6.\nItti, L. (2007). Visual salience. Scholarpedia. 2, 3327. doi: 10.4249/scholarpedia.3327\nItti, L., Rees, G., and Tsotsos, J. K. (2005). Neurobiology of Attention . Elsevier.\nKastner, S., and Ungerleider, L. G. (2000). Mechanisms of visu al attention in the\nhuman cortex. Annu. Rev. Neurosci . 23, 315–341. doi: 10.1146/annurev.neuro.23.1.315\nKhaligh-Razavi, S.-M., and Kriegeskorte, N. (2014). Deep su pervised, but not\nunsupervised, models may explain it cortical representation. PLoS Comput. Biol . 10,\ne1003915. doi: 10.1371/journal.pcbi.1003915\nKim, J., Ricci, M., and Serre, T. (2018). Not-so-clevr: learni ng same-diﬀerent\nrelations strains feedforward neural networks. Interface Focus 8, 20180011.\ndoi: 10.1098/rsfs.2018.0011\nKim, M.-S., and Cave, K. R. (1999). Top-down and bottom-up att entional control:\non the nature of interference from a salient distractor. Percept. Psychophys . 61,\n1009–1023. doi: 10.3758/BF03207609\nKim, Y., Denton, C., Hoang, L., and Rush, A. M. (2017). “Struc tured attention\nnetworks, ” inInternational Conference on Learning Representations .\nKnudsen, E. I. (2007). Fundamental components of attention. Annu. Rev. Neurosci .\n30, 57–78. doi: 10.1146/annurev.neuro.30.051606.094256\nKotseruba, I., Wloka, C., Rasouli, A., and Tsotsos, J. K. (2019 ). “Do saliency models\ndetect odd-one-out targets? New datasets and evaluations, ” in British Machine Vision\nConference (BMVC).\nKrauzlis, R. J., Wang, L., Yu, G., and Katz, L. N. (2023). What is attention? Wiley\nInterdiscip. Rev. Cogn. Sci . 14, e1570. doi: 10.1002/wcs.1570\nKubilius, J., Bracci, S., and Op de Beeck, H. P. (2016). Deep ne ural networks as a\ncomputational model for human shape sensitivity. PLoS Comput. Biol . 12, e1004896.\ndoi: 10.1371/journal.pcbi.1004896\nLamy, D., Tsal, Y., and Egeth, H. E. (2003). Does a salient distr actor\ncapture attention early in processing? Psychonom. Bull. Rev . 10, 621–629.\ndoi: 10.3758/BF03196524\nFrontiers in Computer Science /two.tnum/one.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nLi, Y., Wang, J., Dai, X., Wang, L., Yeh, C.-C. M., Zheng, Y., e t al. (2023). How does\nattention work in vision transformers? A visual analytics at tempt. IEEE Transact. Vis.\nComp. Graph. doi: 10.1109/TVCG.2023.3261935\nLin, Y.-S., Chen, C.-C., and Greenlee, M. W. (2022). The role of la teral modulation\nin orientation-speciﬁc adaptation eﬀect. J. Vis. 22, 13–13. doi: 10.1167/jov.22.2.13\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (202 1). “Swin transformer:\nhierarchical vision transformer using shifted windows, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 10012–10022.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., an d Xie, S. (2022). “A\nconvnet for the 2020s, ” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , 11976–11986.\nLonnqvist, B., Bornet, A., Doerig, A., and Herzog, M. H. (2021 ). A comparative\nbiology approach to dnn modeling of vision: a focus on diﬀerences , not similarities. J.\nVis. 21, 17–17. doi: 10.1167/jov.21.10.17\nMahmood, K., Mahmood, R., and van Dijk, M. (2021). “On the rob ustness of vision\ntransformers to adversarial examples, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) , 7838–7847.\nMartinez-Trujillo, J. (2022). Visual attention in the prefron tal cortex. Ann. Rev.\nVision Sci. 8, 407–425. doi: 10.1146/annurev-vision-100720-031711\nMoore, T., and Zirnsak, M. (2017). Neural mechanisms of selec tive visual attention.\nAnnu. Rev. Psychol . 68, 47–72. doi: 10.1146/annurev-psych-122414-033400\nNaseer, M., Ranasinghe, K., Khan, S., Hayat, M., Khan, F., an d Yang, M.-H.\n(2021). Intriguing properties of vision transformers. Adv. Neural Inf. Process. Syst . 34,\n23296–23308.\nNobre, A. C., Nobre, K., and Kastner, S. (2014). The Oxford Handbook of Attention .\nOxford University Press.\nPan, Z., Zhuang, B., He, H., Liu, J., and Cai, J. (2022). “Less is more: Pay less attention\nin vision transformers, ” inProceedings of the AAAI Conference on Artiﬁcial Intelligence ,\nVol. 36 (AAAI Press), 2035–2043.\nPanaetov, A., Daou, K. E., Samenko, I., Tetin, E., and Ivanov , I. (2023). “Rdrn:\nrecursively deﬁned residual network for image super-resoluti on, ” inComputer Vision-\nACCV 2022, eds L. Wang, J. Gall, T.-J. Chin, I. Sato, and R. Chellappa (Cham: Spr inger\nNature Switzerland), 629–645.\nPark, N., and Kim, S. (2022). “How do vision transformers wor k?, ”In International\nConference on Learning Representations .\nPashler, H. (ed). (1998). Attention, 1st Edn . Psychology Press.\nPaul, S., and Chen, P.-Y. (2022). Vision transformers are rob ust learners. Proc. AAAI\nConf. Artif. Intell . 36, 2071–2081. doi: 10.1609/aaai.v36i2.20103\nPeterson, M. A. (2015). “Low-level and high-level contributi ons to ﬁgure-ground\norganization, ” in The Oxford Handbook of Perceptual Organization , ed J. Wagemans\n(Oxford University Press), 259–280.\nPoort, J., Raudies, F., Wannig, A., Lamme, V. A., Neumann, H. , and Roelfsema, P.\nR. (2012). The role of attention in ﬁgure-ground segregation in areas v1 and v4 of the\nvisual cortex. Neuron 75, 143–156. doi: 10.1016/j.neuron.2012.04.032\nQiu, F. T., Sugihara, T., and Von Der Heydt, R. (2007). Figure -ground\nmechanisms provide structure for selective attention. Nat. Neurosci . 10, 1492–1499.\ndoi: 10.1038/nn1989\nRaghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dos ovitskiy, A. (2021). Do\nvision transformers see like convolutional neural networks? Adv. Neural Inf. Process.\nSyst. 34, 12116–12128.\nRicci, M., Cadène, R., and Serre, T. (2021). Same-diﬀerent c onceptualization:\na machine vision perspective. Curr. Opin. Behav. Sci . 37, 47–55.\ndoi: 10.1016/j.cobeha.2020.08.008\nRichardWebster, B., Anthony, S. E., and Scheirer, W. J. (201 9). Psyphy:\na psychophysics driven evaluation framework for visual recogn ition. IEEE\nTrans. Pattern Anal. Mach. Intell . 41, 2280–2286. doi: 10.1109/TPAMI.2018.\n2849989\nShushruth, S., Nurminen, L., Bijanzadeh, M., Ichida, J. M., Vanni, S.,\nand Angelucci, A. (2013). Diﬀerent orientation tuning of nea r-and far-\nsurround suppression in macaque primary visual cortex mirrors t heir tuning\nin human perception. J. Neurosci . 33, 106–119. doi: 10.1523/JNEUROSCI.2518-\n12.2013\nSrinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., a nd Vaswani, A. (2021).\n“Bottleneck transformers for visual recognition, ” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) , 16519–16529.\nStettler, D. D., Das, A., Bennett, J., and Gilbert, C. D. (2002). Lateral connectivity\nand contextual interactions in macaque primary visual cortex . Neuron 36, 739–750.\ndoi: 10.1016/S0896-6273(02)01029-2\nStyles, E. (2006). The Psychology of Attention . Psychology Press.\nSutherland, S. (1988). Feature selection. Nature 392, 350.\nTan, A., Nguyen, D. T., Dax, M., Nießner, M., and Brox, T. (202 1). Explicitly\nmodeled attention maps for image classiﬁcation. Proc. AAAI Conf. Artif. Intell . 35,\n9799–9807. doi: 10.1609/aaai.v35i11.17178\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., a nd Jegou, H. (2021).\n“Training data-eﬃcient image transformers and distillation through attention, ” in\nProceedings of the 38th International Conference on Machine Learning , Vol. 139, eds\nM. Meila, and T. Zhang (PMLR), 10347–10357.\nTsotsos, J. K. (1990). Analyzing vision at the complexity level. Behav. Brain Sci . 13,\n423–445. doi: 10.1017/S0140525X00079577\nTsotsos, J. K. (2011). A Computational Perspective on Visual Attention . MIT Press.\nTsotsos, J. K. (2017). Complexity level analysis revisited: Wha t can 30 years of\nhindsight tell us about how the brain might represent visual in formation? Front.\nPsychol. 8, 1216. doi: 10.3389/fpsyg.2017.01216\nTsotsos, J. K. (2022). When we study the ability to attend, wha t exactly are we trying\nto understand? J. Imaging 8, 212. doi: 10.3390/jimaging8080212\nTsotsos, J. K., Itti, L., and Rees, G. (2005). “A brief and selec tive history of attention, ”\nin Neurobiology of Attention , eds L. Itti, G. Rees, and J. K. Tsotsos (Academic Press).\nTsotsos, J. K., and Rothenstein, A. (2011). Computational mo dels of visual attention.\nScholarpedia 6, 6201. doi: 10.4249/scholarpedia.6201\nTuli, S., Dasgupta, I., Grant, E., and Griﬃths, T. (2021). “Are co nvolutional neural\nnetworks or transformers more like human vision?, ” in Proceedings of the Annual\nMeeting of the Cognitive Science Society, Vol. 43 .\nVaishnav, M., Cadene, R., Alamia, A., Linsley, D., VanRullen, R. , and Serre,\nT. (2022). Understanding the computational demands underlyi ng visual reasoning.\nNeural Comput. 34, 1075–1099. doi: 10.1162/neco_a_01485\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is All you need, ” in Advances in Neural Information Processing\nSystems, Vol. 30, eds I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fer gus, S.\nVishwanathan,et al. (Curran Associates, Inc.)\nWloka, C., and Tsotsos, J. K. (2019). Flipped on its head: deep lear ning-based\nsaliency ﬁnds asymmetry in the opposite direction expected for s ingleton search of\nﬂipped and canonical targets. J. Vis. 19, 318–318. doi: 10.1167/19.10.318\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., et al.\n(2020). “Transformers: state-of-the-art natural language processing, ” in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Process ing: System\nDemonstrations (Association for Computational Linguistics), 38–45.\nWu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., et al. (2021 ). “Visual\ntransformers: where do transformers really belong in vision mo dels?, ” in Proceedings\nof the IEEE/CVF International Conference on Computer Vision (ICCV) , 599–609.\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., et al. (2 021). “CvT:\nintroducing convolutions to vision transformers, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) , 22–31.\nXiao, T., Singh, M., Mintun, E., Darrell, T., Dollar, P., and Girsh ick, R. (2021).\n“Early convolutions help transformers see better, ”. in Advances in Neural Information\nProcessing Systems, Vol. 34, eds M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang ,\nand J. W. Vaughan (Curran Associates, Inc.), 30392–30400.\nXu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., et al. (2015). “Show,\nattend and tell: neural image caption generation with visual at tention, ” inProceedings\nof the 32nd International Conference on Machine Learning , Vol. 37, F. Bach, and D. Blei\n(Lille: PMLR), 2048–2057.\nXu, Y., and Vaziri-Pashkam, M. (2021a). Examining the codin g strength of object\nidentity and nonidentity features in human occipito-tempora l cortex and convolutional\nneural networks. J. Neurosci. 41, 4234–4252. doi: 10.1523/JNEUROSCI.1993-20.2021\nXu, Y., and Vaziri-Pashkam, M. (2021b). Limits to visual repr esentational\ncorrespondence between convolutional neural networks and th e human brain. Nat.\nCommun. 12, 2065. doi: 10.1038/s41467-021-22244-7\nXu, Y., and Vaziri-Pashkam, M. (2022). Understanding trans formation tolerant\nvisual object representations in the human brain and convolut ional neural networks.\nNeuroimage 263, 119635. doi: 10.1016/j.neuroimage.2022.119635\nYang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., et al. ( 2021). “Focal attention\nfor long-range interactions in vision transformers, ” in Advances in Neural Information\nProcessing Systems, Vol. 34, eds M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang ,\nand J. W. Vaughan (Curran Associates, Inc.), 30008–30022.\nYantis, S., and Egeth, H. E. (1999). On the distinction betwe en visual\nsalience and stimulus-driven attentional capture. J. Exp. Psychol . 25, 661.\ndoi: 10.1037/0096-1523.25.3.661\nYuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., et al. (2021). “Tokens-to-\ntoken ViT: training vision transformers from scratch on Ima geNet, ” inProceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV) , 558–567.\nYue, X., Sun, S., Kuang, Z., Wei, M., Torr, P. H., Zhang, W., et al. (2021). “Vision\ntransformer with progressive sampling, ” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 387–396.\nZerroug, A., Vaishnav, M., Colin, J., Musslick, S., and Serre, T. (2022). “A benchmark\nfor compositional visual reasoning, ” in Thirty-sixth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track .\nZhang, J., and Sclaroﬀ, S. (2013). “Saliency detection: a boole an map approach, ” in\nProceedings of the IEEE International Conference on Computer Vision , 153–160.\nFrontiers in Computer Science /two.tnum/two.tnum frontiersin.org\nMehrani and Tsotsos /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fcomp./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/eight.tnum/four.tnum/five.tnum/zero.tnum\nZhou, D., Yu, Z., Xie, E., Xiao, C., Anandkumar, A., Feng, J., et al. (2022).\n“Understanding the robustness in vision transformers, ” in Proceedings of the 39th\nInternational Conference on Machine Learning , Vol. 162, eds K. Chaudhuri, S. Jegelka,\nL. Song, C. Szepesvari, G. Niu, and S. Sabato (PMLR), 27378–273 94.\nZhou, H.-Y., Lu, C., Yang, S., and Yu, Y. (2021). “ConvNets vs . transformers:\nwhose visual representations are more transferable?, ” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) Workshops , 2230–2238.\nZhou, Q., Du, C., and He, H. (2022). Exploring the brain-like prope rties of\ndeep neural networks: a neural encoding perspective. Mach. Intell. Res . 19, 439–455.\ndoi: 10.1007/s11633-022-1348-x\nZhu, M., Hou, G., Chen, X., Xie, J., Lu, H., and Che, J. (2021). “ Saliency-guided\ntransformer network combined with local embedding for no-re ference image quality\nassessment, ” in Proceedings of the IEEE/CVF International Conference on Computer\nVision, 1953–1962.\nZhuang, C., Yan, S., Nayebi, A., Schrimpf, M., Frank, M. C., Di Carlo, J.\nJ., et al. (2021). Unsupervised neural network models of the ven tral visual\nstream. Proc. Nat. Acad. Sci. U. S. A . 118, e2014196118. doi: 10.1073/pnas.2014\n196118\nZucker, S. (1981). “Computer vision and human perception, ” in\nProceedings of the 7th International Joint Conference on Artiﬁcial Int elligence,\n1102–1116.\nZucker, S. W. (1978). Vertical and horizontal processes in low level vision. Comp.\nVision Syst. 187–195. Available online at: https://shop.elsevier.com/books/computer-\nvision-systems/hanson/978-0-12-323550-3\nFrontiers in Computer Science /two.tnum/three.tnum frontiersin.org"
}