{
  "title": "How Context Affects Language Models' Factual Predictions",
  "url": "https://openalex.org/W3021533447",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2748829762",
      "name": "Petroni, Fabio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221722488",
      "name": "Lewis, Patrick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225993286",
      "name": "Piktus, Aleksandra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226861708",
      "name": "Rocktäschel, Tim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223768303",
      "name": "Wu Yuxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290203155",
      "name": "Miller, Alexander H.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745510070",
      "name": "Riedel, Sebastian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2983088655",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2964348592",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3089102176",
    "https://openalex.org/W2989588035",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963430447",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3021191241",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2973362636",
    "https://openalex.org/W2923890923",
    "https://openalex.org/W2973957133",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W2984270004",
    "https://openalex.org/W3022176767",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2909970382",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2889729765",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2985008383",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2984354699",
    "https://openalex.org/W2945329331",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2962718483",
    "https://openalex.org/W3082274269"
  ],
  "abstract": "When pre-trained on large unsupervised textual corpora, language models are able to store and retrieve factual knowledge to some extent, making it possible to use them directly for zero-shot cloze-style question answering. However, storing factual knowledge in a fixed number of weights of a language model clearly has limitations. Previous approaches have successfully provided access to information outside the model weights using supervised architectures that combine an information retrieval system with a machine reading component. In this paper, we go a step further and integrate information from a retrieval system with a pre-trained language model in a purely unsupervised way. We report that augmenting pre-trained language models in this way dramatically improves performance and that the resulting system, despite being unsupervised, is competitive with a supervised machine reading baseline. Furthermore, processing query and context with different segment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classifier to determine whether the context is relevant or not, substantially improving BERT's zero-shot cloze-style question-answering performance and making its predictions robust to noisy contexts.",
  "full_text": "Automated Knowledge Base Construction (2020) Conference paper\nHow Context Aﬀects Language Models’ Factual Predictions\nFabio Petroni1 fabiopetroni@fb.com\nPatrick Lewis1,2 plewis@fb.com\nAleksandra Piktus1 piktus@fb.com\nTim Rockt¨ aschel1,2 rockt@fb.com\nYuxiang Wu2 yuxiang.wu.18@ucl.ac.uk\nAlexander H. Miller1 ahm@fb.com\nSebastian Riedel1,2 sriedel@fb.com\n1Facebook AI Research\n2University College London\nAbstract\nWhen pre-trained on large unsupervised textual corpora, language models are able to\nstore and retrieve factual knowledge to some extent, making it possible to use them di-\nrectly for zero-shot cloze-style question answering. However, storing factual knowledge in a\nﬁxed number of weights of a language model clearly has limitations. Previous approaches\nhave successfully provided access to information outside the model weights using super-\nvised architectures that combine an information retrieval system with a machine reading\ncomponent. In this paper, we go a step further and integrate information from a retrieval\nsystem with a pre-trained language model in a purely unsupervised way. We report that\naugmenting pre-trained language models in this way dramatically improves performance\nand that the resulting system, despite being unsupervised, is competitive with a super-\nvised machine reading baseline. Furthermore, processing query and context with diﬀerent\nsegment tokens allows BERT to utilize its Next Sentence Prediction pre-trained classiﬁer\nto determine whether the context is relevant or not, substantially improving BERT’s zero-\nshot cloze-style question-answering performance and making its predictions robust to noisy\ncontexts.\n1. Introduction\nPre-trained language models such as BERT [Devlin et al., 2019] and RoBERTa [Liu et al.,\n2019] enabled state-of-the-art in many downstream NLP tasks [Wang et al., 2018a, 2019, Wu\net al., 2019]. These models are trained in an unsupervised way from large textual collection\nand recent work [Petroni et al., 2019, Jiang et al., 2019, Talmor et al., 2019, Devlin et al.,\n2019] has demonstrated that such language models can store factual knowledge to some\nextent. However, considering the millions of documents and facts in Wikipedia 1 and other\ntextual resources, it unlikely that a language model with a ﬁxed number of parameters is\nable to reliably store and retrieve factual knowledge with suﬃcient precision [Guu et al.,\n2020].\nOne way to get around this is to combine machine reading with an information retrieval\n(IR) system [Chen et al., 2017, Guu et al., 2020]. Given a question, the IR system retrieves\n1. https://en.wikipedia.org/wiki/Wikipedia:Statistics\narXiv:2005.04611v1  [cs.CL]  10 May 2020\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\nrelevant contexts which are subsequently processed by a reading component. In the case of\nDrQA [Chen et al., 2017], the retriever is ﬁxed and the reading component is trained based\non retrieved contexts, whereas in REALM [Guu et al., 2020] the IR component is trained\nalongside the reader during both pre-training and subsequent ﬁne-tuning.\nIn this paper, we go a step further and forego supervised ﬁne-tuning. Instead, we\nconsider the purely unsupervised case of augmenting a language model with retrieved con-\ntexts at test time. We demonstrate that augmenting pre-trained language models with\nsuch retrieved contexts dramatically improves unsupervised cloze-style question answering,\nreaching performance that is on par with the supervised DrQA approach. In addition to\nbeing unsupervised, using a pre-trained language model like BERT instead of a trained\nmachine reading component has several other advantages. Since BERT is not an extractive\nQA model, it is able to utilize contexts that contain relevant information but do not contain\nthe answer span directly. More importantly, we ﬁnd that via the next-sentence prediction\nobjective BERT is able to ignore noisy or irrelevant contexts.\nIn summary, we present the following core ﬁndings: i) augmenting queries with rel-\nevant contexts dramatically improves BERT and RoBERTa performance on the LAMA\nprobe [Petroni et al., 2019], demonstrating unsupervised machine reading capabilities of\npre-trained language models; ii) fetching contexts using an oﬀ-the-shelf information retrieval\nsystem is suﬃcient for BERT to match the performance of a supervised open-domain QA\nbaseline; iii) BERT’s next-sentence prediction pre-training strategy is a highly eﬀective un-\nsupervised mechanism in dealing with noisy and irrelevant contexts. The code and data to\nreproduce our analysis will be made publicly available.\n2. Related Work\nLanguage Models and Probes With the recent success of pre-trained language models\nlike BERT [Devlin et al., 2019] and its variants [Liu et al., 2019, Seo et al., 2019, Raﬀel\net al., 2019, Lewis et al., 2019a], it becomes increasingly important to understand what these\nmodels learn. A variety of “probes” have been developed to analyse the syntactic structures,\nsuch as syntax trees [Marvin and Linzen, 2018, Hewitt and Manning, 2019, Vig and Belinkov,\n2019], negative polarity items [Warstadt and Bowman, 2019, Warstadt et al., 2019], semantic\nfragments [Richardson et al., 2019], function words [Kim et al., 2019], and many other\nlinguistic phenomena [Tenney et al., 2019a,b, De Cao et al., 2020]. To measure the factual\nknowledge present in these pre-trained language models, Petroni et al. [2019] propose the\nLAMA benchmark which tests the models with cloze-style questions constructed from\nknowledge triples. Jiang et al. [2019] later extends LAMA by automatically discovering\nbetter prompts, Kassner and Sch¨ utze [2019] add negated statements, Poerner et al. [2019]\nﬁlter out easy-to-guess queries, and Richardson and Sabharwal [2019], Talmor et al. [2019],\nBisk et al. [2019] develop further probes for textual reasoning. Pre-trained language models\nhave also been ﬁne-tuned and adapted to be used as information retrieval systems [Yang\net al., 2019b, Yilmaz et al., 2019, Seo et al., 2019].\nOpen-Domain QA Open-domain QA aims at answering questions without explicitly\nknowing which documents contain relevant information. Open-domain QA models often\ninvolve a retriever to ﬁnd relevant documents given a question, and a reader to produce\nthe answers [Chen et al., 2017]. Works in this areas mostly focus on enhancing retrieval\nHow Context Affects Language Models’ Factual Predictions\nquality [Choi et al., 2017, Wang et al., 2018b,c, Lin et al., 2018, Min et al., 2018, Lee et al.,\n2018, 2019, Das et al., 2019, Xiong et al., 2019], improving answer aggregation [Clark and\nGardner, 2018, Wang et al., 2018c, Lee et al., 2018, Pang et al., 2019], and accelerating\nthe whole pipeline [Seo et al., 2019]. Recently, Guu et al. [2020] show that augmenting\nlanguage model pre-training with a knowledge retriever induces performance gains on open-\ndomain QA tasks. Our work diﬀers from previous works in open-domain QA in two ways:\ni) we consider a fully unsupervised setting using a pre-trained language model and an oﬀ-\nthe-shelve information retrieval system, ii) our aim is to assess the prediction of factual\nknowledge in this setup rather than to improve open-domain question answering in general.\n3. Methodology\nGiven a cloze-style question q with an answer a, we assess how the predictions from a\nlanguage model change when we augment the input with contexts c. In this section, we\ndescribe the datasets we use to source ( q, a) pairs, as well as various methods of generating\ncontext documents c.\n3.1 Datasets\nCorpus Relation Statistics\n#Facts #Rel\nGoogle-RE\nbirth-place 2937 1\nbirth-date 1825 1\ndeath-place 765 1\nTotal 5527 3\nT-REx\n1-1 937 2\nN-1 20006 23\nN-M 13096 16\nTotal 34039 41\nSQuAD Total 305 -\nTable 1: Statistics for the LAMA data.\nWe use the LAMA2 probe in our experi-\nments [Petroni et al., 2019], a collection of\ncloze-style questions about real world re-\nlational facts with a single token answer.\nEach question is accompanied by snippets\nof text from Wikipedia that are likely to\nexpress the corresponding fact. Although\nthere are several cloze-style QA datasets\n(some listed in Section 2) we decided to\nuse LAMA because: (1) the nature of the\nLAMA data is aligned with the relational\nknowledge focus or our analysis ( i.e., given\na subject and a relation predict the ob-\nject) and (2) each data point is aligned\nby construction with relevent contextual in-\nformation. We consider the Google-RE 3\n(3 relations, 5527 facts), T-REx [Elsahar\net al., 2018] (41 relations, 34039 facts) and\nSQuAD [Rajpurkar et al., 2016] (305 questions manually translated in cloze-style format)\nsubsets of the probe. More detailed statistics for the LAMA data considered are reported\nin Table 1. For the RoBERTa results, we trim the LAMA dataset (by about 15%) such\nthat all answers are in the model’s vocabulary, so BERT and RoBERTanumbers in this\npaper should not be directly compared as they consider slightly diﬀerent subsets of the\ndata.\n2. https://github.com/facebookresearch/LAMA\n3. https://code.google.com/archive/p/relation-extraction-corpus\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\n3.2 Baselines\nWe consider DrQA [Chen et al., 2017], a popular system for open-domain question an-\nswering. The overall pipeline consists of two phases: (1) a TF-IDF document retrieval\nstep, where the model ﬁnds relevant paragraphs from Wikipedia and (2) a machine com-\nprehension step to extract the answer from those paragraphs. The machine comprehension\ncomponent is trained with supervision on SQuAD v1.1 [Rajpurkar et al., 2016]. In order to\napply DrQA to the LAMA probe, we take inspiration from [Levy et al., 2017] and map\neach cloze-style template to a natural question template ( e.g., “X was born in [Mask]” to\n“Where was X born?”). We constrain the predictions of DrQA to single-token answers as\nin Petroni et al. [2019]. Our results for DrQA and BERT are directly comparable with\nthe other baselines in Petroni et al. [2019].\n3.3 Language Models\nAmong the constellation of language models that have been proposed in recent years we\nconsider BERT [Devlin et al., 2019] since it is one of the post popular and widely used\nat the time of writing. 4 Moreover, the large cased version of the BERT model is the best\nperforming LM on the LAMA probe among those considered in Petroni et al. [2019]. We ad-\nditionally consider the large version of the RoBERTamodel [Liu et al., 2019]. Both BERT\nand RoBERTa have been trained on corpora that include Wikipedia. While BERT uses\ntwo pre-training strategies, Mask Language Modelling (MLM) and Next Sentence Prediction\n(NSP), RoBERTa considers only the MLM task. We produce a probability distribution\nover the uniﬁed vocabulary of Petroni et al. [2019] for the masked token in each cloze-style\nquestions and report the average precision at 1 (P@1).\n3.4 Contexts\nWe enrich cloze statements with diﬀerent types of contextual information. We explicitly\ndistinguish cloze question q and context c in the input according to the model. For BERT,\nwe use diﬀerent segment embeddings, index 0 forq and 1 for c, and insert the separator token\n(i.e., [SEP]) in between. For RoBERTa, which is not equipped with segment embeddings,\nwe use the end of sentence ( eos) token to separate q and c. We addidionally performed\nsome experiments without this clear separation of query and context, but considering them\nas concatenated in a single segment (or wihtout the eos token in between). The input is\ntruncated to 512 tokens.\n3.4.1 Oracle Contexts\nWe provide an oracle-based ( ora) context in order to assess the capability of LMs to\nexploit context that we know is relevant to the entity in the question. Concretely, we use\nthe Wikipedia snippet accompanying each example in the LAMA probe, truncated to at\nmost ﬁve sentences. This context often contains the true answer and always contains related\ntrue information.\n4. https://huggingface.co/models\nHow Context Affects Language Models’ Factual Predictions\nLAMA Relation open domain sourced context\nB B-adv B-gen DrQA B-ret B-ora\nGoogle-RE\nbirth-place 16.1 14.5 8.5 48.6 43.5 70.6\nbirth-date 1.4 1.4 1.4 42.9 43.1 98.1\ndeath-place 14.0 12.6 6.0 38.4 35.8 65.1\nTotal 10.5 9.5 5.3 43.3 40.8 78.0\nT-REx\n1-1 74.5 74.5 71.3 55.2 81.2 91.1\nN-1 34.2 33.8 32.7 30.4 47.5 67.3\nN-M 24.3 23.6 23.8 15.4 32.0 52.4\nTotal 32.3 31.8 31.1 25.8 43.1 62.6\nSQuAD 17.4 17.4 15.8 37.5 34.3 61.7\nweighted average 30.5 30.0 29.0 27.2 42.8 63.6\nTable 2: Mean precision at one (P@1) for the DrQA baseline, BERT-large on context-free\ncloze questions ( B) and on adversarial ( B-adv), generated ( B-gen), retrieved ( B-ret)\nand oracle ( B-ora) context-enriched questions on the relational LAMA probe. The fully\nunsupervised B-ret is competitive with the supervised DrQA system and is dramatically\nbetter than the context-free baseline. We weight the average per number of relations (3\nfor Google-RE, 41 for T-REx and we consider SQuAD as a single contribution). Pairwise\nsign tests per relation show statistically signiﬁcant diﬀerences (p-value below 1e-5) between:\nB-ret and all other results; B-ora and all other results.\n3.4.2 Sourcing Relevant Contexts\nRelevant context is often not available and must be automatically sourced by the model [Chen\net al., 2017, Clark and Gardner, 2018]. In this scenario, we consider two possible approaches:\nusing an information retrieval engine (ret) or generating the context with an autoregressive\nLM (gen) [Radford et al., 2019]. For the retrieval case, we use the ﬁrst paragraph from\nDrQA’s retrieval system as context. For the generative case, taking inspiration from the\nstudy of Massarelli et al. [2019], we consider a 1.4B parameters autoregressive language\nmodel trained on CC-NEWS [Liu et al., 2019]. This model has been shown to generate\nmore factual text with respect to others trained on diﬀerent corpora, including Wikipedia.\nFor each question inLAMA, we use the natural question template as preﬁx to condition the\ngeneration, and generate ﬁve sentences using the delayed beam search strategy [Massarelli\net al., 2019]. These results may be quite related to the entity in the query, though they\nmay not always be completely factual.\n3.4.3 Adversarial Contexts\nWe provide an uninformative context in order to test the ability of the model to ignore\nirrelevant context that is not useful for answering the query. We do this by randomly\nsampling an oracle context from a diﬀerent question that has the same relation type but\na diﬀerent answer a′. This results in a context document that refers to a diﬀerent subject\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\n1 2 3 4 5 6 7 8 9 100\n10\n20\n30\n40\n50\n60\n70\nk: number of retrieved paragraphs\nrecall accuracy\nGoogle-RE\nT-REx\nSQuAD\n(a) Percentage of times the answer appears\nin the top-k retrieved paragraphs by DrQA.\nWe use k=1 for our experiments as a single\nparagraph can already contain a large num-\nber of tokens.\nP@1\nanswer\nin ctx B-adv B-gen B-ret B-ora\nbetter\npresent 0.9 4.6 14.0 32.6\nabsent 2.4 2.5 3.2 1.4\nTotal 3.3 7.0 17.2 34.0\nworse\npresent 0.6 2.0 2.4 3.5\nabsent 3.1 6.2 3.9 0.1\nTotal 3.7 8.2 6.3 3.6\n# better rel. 11 13 34 39\n(b) For T-REx, we report the percentage of time the\nmodel changes its output for thebetter or worse when\nthe context is provided, grouped by the presence or\nabsence of the answer in the provided context. B-\nret and B-ora scored higher than the context-free\nmodel on most relations.\nFigure 1\nentity but contains a distracting and semantically plausible answer a′. Table 4 shows some\nexamples of adversarial contexts.\n4. Results\nThe main results of our analysis are summarized in Table 2. It shows the mean precision\nat one (P@1) for the DrQA baseline and BERT-large on the LAMA probe enriched with\ndiﬀerent kinds of contextual information. Enriching cloze-style questions with relevant\ncontext dramatically improves the performance of BERT:B-ora obtains ×7.4 improvement\non Google-RE, ×1.9 on T-REx and ×3.5 on SQuAD with respect to using context-free\nquestions (B). This clearly demonstrates BERT’s ability to successfully exploit the provided\ncontext and act as a machine reader model. Remarkably, no ﬁne-tuning is required to trigger\nsuch behaviour.\nWhen we rely on TF-IDF retrieved context ( B-ret), BERT still performs much better\nthan in the no context setting. Overall, B-ret results are comparable with DrQA on\nGoogle-RE and SQuAD and much higher on T-REx. This is particularly surprising given\nthat B-ret, unlike DrQA, did not receive any supervision for this task. Pairwise sign tests\nacross relations show that the improvements for B-ret and B-ora are indeed statistically\nsigniﬁcant (p-value below 1e-5).\nFigure 1a shows the recall of the IR system, which demonstrates that the answer is not\npresent in many of the retrieved contexts, though often the context is still related to the\nsame topic. Table 1b reports a detailed analysis of whether the answer is present in retrieved\ncontexts and how that aﬀects the model’s predictions. We observe that most of the gain\nof B-ret comes from cases in which the context contained the answer. However, there are\nalso cases where the context does not explicitly mention the answer but BERT is still able\nHow Context Affects Language Models’ Factual Predictions\n-10\n 0\n 10\n 20\n 30\nADV GEN RET ORA\nP@1 difference\nBERT two segments\n-0.5 -1.5\n12.3 33.1\nBERT one segment\n-12.4 -3.9\n6.9 27.2\nRoBERTa two sentences\n-5.2 -0.9\n10.9 31\nRoBERTa one sentence\n-8.5 -2.9\n9.4 31.5\n-10\n 0\n 10\n 20\n 30\nADV GEN RET ORA\nP@1 difference\nBERT two segments\nBERT one segment\nRoBERTa two sentences\nRoBERTa one sentence\nFigure 2: For each type of context considered, we report the change in P@1 relative to zero\ncontext, averaging results across relations. For each model we consider a concatenation of\nquestion and context as well as separating the two using separator tokens ( BERT) or end\nof sentence tokens (RoBERTa). Separation dramatically improves both model’s ability to\nignore poor context and improves BERT’s performance in the presence of good context.\nto utilize the related context to help select the correct answer. Note that an extractive\napproach (such as DrQA) would have provided an incorrect answer (or no answer) for\nthose cases.\n4.1 Adversarial Robustness\nThe B-adv column in Table 2 shows the LAMA P@1 results for BERT for adversarial\ncontexts. BERT is very robust, dropping only 0.5 P@1 on average from the zero context\nbaseline. However, as shown in Figure 2, this strong performance only occurs when the\ncontext and question are processed as two segments using BERT’s separator tokens. Using\nonly one segment (that is, simply concatenating the input query and the context) leads to a\nsevere drop of 12.4 P@1 for BERT (a 40.7% relative drop in performance). We also observe\na consistent improvement in performance from one segment to two for retrieved and oracle\ncontexts.\nOne possible reason for this phenomenon resides in the Next Sentence Prediction (NSP)\nclassiﬁer of BERT, learned with self-supervision during pretraining by training the model\nto distinguish contiguous ( i.e., “next sentence” pairs) from randomly sampled blocks of\ntext. We hypothesize that the MLM task might be inﬂuenced by the NSP’s output. Thus,\nBERT might learn to not condition across segments for masked token prediction if the NSP\nscore is low, thereby implicitly detecting irrelevant and noisy contexts. A result that seems\nin line with this hypothesis is that RoBERTa, which does not use NSP, is more vulnerable\nto adversarial contexts and the diﬀerence between one and two sentences (for RoBERTa\nseparated by the eos token) is much smaller.\nTo further investigate this hypothesis, we calculate the number of (c, q) pairs classiﬁed by\nBERT as “next sentence” pairs in LAMA for the diﬀerent context strategies. These results\nare shown in Table 3. We see that for B-ret, B-gen and B-ora, NSP classiﬁcations are\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\n(% Next Sentence) B-adv B-gen B-ret B-ora\nGoogle-RE 10.4 95.1 88.9 98.4\nT-REx 14.0 97.0 89.7 94.5\nSQuAD 11.9 96.4 93.1 99.3\nTable 3: Percentage of examples classiﬁed as ‘next sentences’ according to BERT’s NSP\nclassiﬁer for the diﬀerent context types. The low number of ‘next sentence’ classiﬁcations\nfor B-adv shows the model is able to recognize that adversarial contexts are unrelated and\nthus limit its inﬂuence on modeling the masked token in the query.\n10□5 10□4 10□3 10□2 10□1 100\n∥PLM (a|q) − PLM (a|q + cadversary )∥\n0.1\n0.2\n0.3\n0.4\n0.5\nPNSP (q + cadversary )\n10□5 10□4 10□3 10□2 10□1 100\n∥PLM (a|q) − PLM (a|q + cretrieved)∥\n0.80\n0.85\n0.90\n0.95\n1.00\nPNSP (q + cretrieved)\n10□4 10□3 10□2 10□1 100\n∥P(a|q) − P(a|q + cgenerated)∥\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00\nPNSP (q + cgenerated)\nFigure 3: NSP Probabilities vs the change in LM probability upon appending contexts\nfrom the adversary (left), retrieval (mid), and generation (right) systems. At higher NSP\nprobabilities, we see a higher larger increase to the probability mass placed on the correct\nanswer in the presence of the context. That is, the more relevant that BERT thinks the\ncontext is, the more we see an increase to the likelihood of the true answer. This is exactly\nwhat we would want to see if we had hand-trained a relevance system ourselves, yet this\ninstead emerges naturally from BERT’s NSP pre-training loss.\nhigh, suggesting BERT ﬁnds the segments to be contiguous, and hence useful to condition\nupon. However, for B-adv, very few (c, q) pairs are classiﬁed as “next sentences”, suggesting\nBERT may condition on them less. Additional evidence for our NSP adversarial robustness\nhypothesis is given in Figure 3. Here we compute the absolute diﬀerence in probability that\nBERT places on the correct answer upon including context ||PLM(a|q) −PLM(a|q + c)||,\nand plot it against NSP probability PNSP (q, c). We see that for adversarial, retrieved and\ngenerated contexts, increasing NSP probability is associated with greater change in true\nanswer probability upon including context. 5\n5. Each context method has diﬀerent NSP statistics, ( e.g. the generated contexts have very high NSP\nprobabilities on average) but the trend is consistent—higher NSP scores co-occur with greater changes\nin correct answer probability\nHow Context Affects Language Models’ Factual Predictions\n4.2 Generated context\nBy generating context from a LM we aim at assessing the performance of a solution purely\nbased on knowledge implicitly stored in the parameters of the underlying neural networks.\nAlthough the overall results of B-gen are lower than the context-free baseline, some inter-\nesting insights emerge from our analysis. First, generated context improves performance\nfor 13 relations and overall for 7% of the questions on T-REx (Table 1b). This demon-\nstrates that autoregressive language models can generate relevant context and potentially\nserve as unsupervised IR systems. They do, of course, generate also irrelevant or factually\nwrong information. What is interesting is that BERT associates high NSP probabilities\nwith generated contexts—for BERT, the generation is always a plausible continuation of\nthe question. This inhibits the selective behaviour of BERT with respect to the context,\nand hurts performance when the generation is noisy, irrelevant or wrong.\nTable 4 shows three examples for the generation of BERT-large for adversarial, gener-\nated, retrieved and oracle context-enriched questions.\n5. Discussion\nIn this section we discuss some of our ﬁndings and their implications.\nRe-examining NSP The Next Sentence Prediction task has been extensively explored\n[Devlin et al., 2019, Liu et al., 2019, Yang et al., 2019c, Lan et al., 2019] with the ap-\nparent consensus that it is not helpful for downstream ﬁne-tuning accuracy. Our ﬁndings,\nin contrast, suggest that it is important for robust exploitation of retrieved context for\nunsupervised tasks. Basing design decisions with a limited set of downstream uses when\ndesigning general purpose pre-trained models may well us lead to less ﬂexible models. As\na community, we should continue to strive for greater diversity in our criteria and possible\nuse-cases for assessing such models [Talmor et al., 2019].\nPractical Takeaways Section 4 shows that BERT has a very diﬀerent behaviour when\ninputs are processed with one or two segments. Practitioners should thus ensure that\nthey thoroughly ablate segmentation options. The consistent improvement upon including\nretrieved context also suggests that it may be possible to get performance boosts in many\nother tasks by the trivial incorporation of retrieved documents, even when such documents\nare not strictly required for the task. We leave investigating this for future work.\nComparison with DrQA We demonstrate that BERT with retrieved context and no\nﬁne-tuning performs on par with DrQA on the LAMA probe, but it is worth discussing\nthis comparison further. Firstly, it is encouraging that an unsupervised system performs\njust as well as a system that requires signiﬁcant supervision such as DrQA. We further\nnote that LMs are abstractive models, whereas DrQA is extractive, conﬁned to returning\nanswers that are spans of retrieved context. However, it is worth stating that LAMA\nonly requires single token answers. Generating an arbitrarly long sequence of contiguous\ntokens from bidirectional LMs like BERT and RoBERTa is not trivial, but extractive\nQA models handle such cases by considering spans of text of varying lengths. Finally,\nwhile we have chosen DrQA as our baseline to compare to recent work, there exist several\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\nQuery Predictions\n[P101] Allan Sandage works in the field of . engineering [-3.1]\nadv\nq [SEP] According to Gould, classical Darwinism encompasses three\nessential core commitments: Agency, the unit of selection, which for\nCharles Darwin was the organism, upon which natural selection ... [0.0]\npsychology [-2.8]\neconomics [-3.4]\nanthropology [-3.5]\ngen\nq [SEP] How many hours a week does he work? Does he get paid? How\nmuch does he get paid? How much does he get paid? He does not have\na car. [1.0]\nﬁnance [-2.1]\nengineering [-3.4]\nadvertising [-3.4]\nret\nq [SEP] In 1922 John Charles Duncan published the ﬁrst three variable\nstars ever detected in an external galaxy, variables 1, 2, and 3, in the\nTriangulum Galaxy (M33). These were followed up by Edwin ... [1.0]\nastronomy [-0.0]\nphysics [-5.5]\nobservation [-7.3]\nora\nq [SEP] He currently works at the Institute of Astronomy in Cambridge;\nhe was the Institute’s ﬁrst director.Educated at the University of Cam-\nbridge, in 1962 he published research with Olin Eggen and Allan ... [1.0]\nastronomy [-0.0]\nphysics [-4.0]\ngalaxies [-5.5]\n[P279] Interleukin 6 is a subclass of . proteins [-0.2]\nadv\nq [SEP]First built in 1893 by Chinese residents of Nagasaki with the\nsupport of the Qing Dynasty government, the shrine was designed to\nserve as a place of worship and learning for the Chinese ... [0.0]\nproteins [-0.2]\nprotein [-3.1]\nDNA [-3.7]\ngen\nq [SEP]Okay, let’s get this out of the way. The Interleukin 6 (IL-6) is\nan interleukin-6 receptor (IL-6R) that plays a key role in the immune\nsystem. Intra-leukin-6 (IL-6R) is an interleukin-6 receptor (IL ... [1.0]\nproteins [-0.6]\nreceptors [-1.6]\nantibodies [-2.2]\nret\nq [SEP]In particular, the increase in levels of IL-6 (interleukin 6), a\nmyokine, can reach up to one hundred times that of resting levels. De-\npending on volume, intensity, and other training factors, the IL ... [1.0]\ninsulin [-1.9]\nIL [-2.1]\nproteins [-2.4]\nora\nq [SEP]It is a cardiac hypertrophic factor of 21.5 kDa and a protein\nmember of the IL-6 cytokine family. This protein heterodimerizes with\ninterleukin 6 signal transducer to form the type II oncostatin M ... [1.0]\nproteins [-0.7]\nprotein [-1.5]\ninsulin [-2.4]\n[P413] Giacomo Tedesco plays in position . center [-2.2]\nadv\nq [SEP]On July 31, 2009 he was traded from the Tigers to the Seattle\nMariners along with fellow pitcher Luke French for veteran pitcher Jarrod\nWashburn. On July 31, 2009 he was traded from the Tigers to ... [0.03]\ncenter [-1.5]\ncentre [-2.4]\nforward [-2.6]\ngen\nq [SEP]How much does he play? He can play fullback, wing or centre. He\ncan also play on the wing. Tedesco can also play in the halves. Tedesco\ncan play in the halves. [1.0]\nfullback [-1.4]\ncentre [-2.1]\nwing [-3.6]\nret\nq [SEP]Giovanni Tedesco has two brothers who are also football play-\ners, Salvatore (formerly of Perugia and Lucchese) and Giacomo, who is\nplaying for Reggina. [1.0]\nmidﬁelder [-1.2]\nforward [-1.8]\nmidﬁeld [-2.3]\nora\nq [SEP]Giacomo Tedesco (born February 1, 1976 in Palermo) is a former\nItalian football (soccer) midﬁelder. Giacomo Tedesco (born February 1,\n1976 in Palermo) is a former Italian football (soccer) midﬁelder ... [1.0]\nmidﬁelder [-0.7]\nforward [-2.2]\ndefender [-2.4]\nTable 4: Examples of generation for BERT-large. We report the top three tokens predicted\nwith the associated log probability (in square brackets) for adversarial ( adv), generated\n(gen), retrieved ( ret) and oracle ( ora) context-enriched questions. NSP probability (in\nsquare brackets) reported at the end of each statement.\nHow Context Affects Language Models’ Factual Predictions\nmore sophisticated supervised open-domain QA models that outperform it on a variety of\nopen-domain QA tasks [Lee et al., 2019, Yang et al., 2019a, Guu et al., 2020].\nUnsupervised Question Answering Our work is part of growing body of work that\ndemonstrate that unsupervised question answering is not only possible, but beginning to\nreach and even outperform some standard supervised baselines. Radford et al. [2019] and\nLewis et al. [2019b] demonstrate non-trivial performance on CoQA [Reddy et al., 2019] and\nSQuAD [Rajpurkar et al., 2016] respectively, and [Yadav et al., 2019] achieve SoTA results\nusing an unsupervised method for multi-choice QA on ARC [Clark et al., 2018]. Taken\ntogether, these recent ﬁndings suggest that powerful and ﬂexible unsupervised QA systems\ncould soon be a reality, bringing with them many advantages including avoiding biases that\noften plague smaller datasets by incorporating knowledge from much larger corpora and\ngreater abilities to combine and abstract pieces of information from diﬀerent sources.\n6. Conclusion\nWe demonstrated a simple technique to greatly improve factual unsupervised cloze QA by\nproviding context documents as additional inputs. We used oracle documents to establish an\nupper bound to this improvement, and found that using oﬀ-the-shelf information retrieval\nis suﬃcient to achieve performance on par with the supervised DrQA system. We also\ninvestigated how brittle language models’ factual predictions were to noisy and irrelevant\ncontext documents, and found that BERT, when featurized appropriately, is very robust.\nWe provide evidence that this robustness stems from the Next Sentence Prediction pre-\ntraining task.\nReferences\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: rea-\nsoning about physical commonsense in natural language. CoRR, abs/1911.11641, 2019.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer\nopen-domain questions. ACL, 2017.\nEunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-ﬁne question answering for long documents. In Proceedings\nof the 55th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/\nanthology/P17-1020.\nChristopher Clark and Matt Gardner. Simple and eﬀective multi-paragraph reading com-\nprehension. ACL, 2018.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\nSchoenick, and Oyvind Tafjord. Think you have solved question answering? try arc,\nthe AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/\nabs/1803.05457.\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. Multi-\nstep retriever-reader interaction for scalable open-domain question answering. In ICLR\n(Poster). OpenReview.net, 2019.\nNicola De Cao, Michael Schlichtkrull, Wilker Aziz, and Ivan Titov. How do decisions emerge\nacross layers in neural models? interpretation with diﬀerentiable masking. arXiv preprint\narXiv:2004.14992, 2020.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training\nof deep bidirectional transformers for language understanding. NAACL-HLT, 2019.\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare,\nElena Simperl, and Frederique Laforest. T-rex: A large scale alignment of natural lan-\nguage with knowledge base triples. LREC, 2018.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\nRetrieval-augmented language model pre-training. arXiv:2002.08909, 2020.\nJohn Hewitt and Christopher D. Manning. A structural probe for ﬁnding syntax in word\nrepresentations. In NAACL-HLT (1), pages 4129–4138. Association for Computational\nLinguistics, 2019.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what\nlanguage models know? CoRR, abs/1911.12543, 2019.\nNora Kassner and Hinrich Sch¨ utze. Negated lama: Birds cannot ﬂy. arXiv preprint\narXiv:1911.03343, 2019.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia, Alex Wang, Tom McCoy, Ian Tenney,\nAlexis Ross, Tal Linzen, Benjamin Van Durme, Samuel R. Bowman, and Ellie Pavlick.\nProbing what diﬀerent NLP tasks teach machines about function word comprehension. In\n*SEM@NAACL-HLT, pages 235–249. Association for Computational Linguistics, 2019.\nZhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and\nRadu Soricut. Albert: A lite bert for self-supervised learning of language representations.\nArXiv, abs/1909.11942, 2019.\nJinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, and Jaewoo Kang. Ranking\nparagraphs for improving answer recall in open-domain question answering. In EMNLP,\npages 565–569. Association for Computational Linguistics, 2018.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly su-\npervised open domain question answering. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics, pages 6086–6096, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL\nhttps://www.aclweb.org/anthology/P19-1612.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\nvia reading comprehension. CoNLL, 2017.\nHow Context Affects Language Models’ Factual Predictions\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv\npreprint arXiv:1910.13461, 2019a.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel. Unsupervised question answering\nby cloze translation. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4896–4910, Florence, Italy, July 2019b. Association for\nComputational Linguistics. doi: 10.18653/v1/P19-1484. URL https://www.aclweb.\norg/anthology/P19-1484.\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised open-\ndomain question answering. In ACL (1), pages 1736–1745. Association for Computational\nLinguistics, 2018.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\nRebecca Marvin and Tal Linzen. Targeted syntactic evaluation of language models. In\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Process-\ning, Brussels, Belgium, October 31 - November 4, 2018, pages 1192–1202, 2018. URL\nhttps://aclanthology.info/papers/D18-1151/d18-1151.\nLuca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt¨ aschel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies aﬀect the\nveriﬁability of generated text. arXiv preprint arXiv:1911.03587, 2019.\nSewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. Eﬃcient and robust ques-\ntion answering from minimal context over documents. In ACL (1), pages 1725–1735.\nAssociation for Computational Linguistics, 2018.\nLiang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Lixin Su, and Xueqi Cheng. HAS-QA:\nhierarchical answer spans model for open-domain question answering. In AAAI, pages\n6875–6882. AAAI Press, 2019.\nFabio Petroni, Tim Rockt¨ aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. Language models as knowledge bases? EMNLP, 2019.\nNina Poerner, Ulli Waltinger, and Hinrich Sch¨ utze. Bert is not a knowledge base\n(yet): Factual knowledge vs. name-based reasoning in unsupervised qa. arXiv preprint\narXiv:1911.03681, 2019.\nAlec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI Blog, 2019.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. CoRR, abs/1910.10683, 2019.\nF. Petroni, P. Lewis, A. Piktus, T. Rockt¨aschel, Y. Wu, A. H. Miller, S. Riedel\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. EMNLP, 2016.\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational ques-\ntion answering challenge. Transactions of the Association for Computational Linguistics,\n7:249–266, March 2019. doi: 10.1162/tacl a 00266. URL https://www.aclweb.org/\nanthology/Q19-1016.\nKyle Richardson and Ashish Sabharwal. What does my QA model know? devising con-\ntrolled probes using expert knowledge. CoRR, abs/1912.13337, 2019.\nKyle Richardson, Hai Hu, Lawrence S. Moss, and Ashish Sabharwal. Probing natural\nlanguage inference models through semantic fragments. CoRR, abs/1909.07521, 2019.\nMin Joon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur P. Parikh, Ali Farhadi, and Hannaneh\nHajishirzi. Real-time open-domain question answering with dense-sparse phrase index.\nIn ACL (1), pages 4430–4441. Association for Computational Linguistics, 2019.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. olmpics - on what\nlanguage model pre-training captures. CoRR, abs/1912.13283, 2019.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. BERT rediscovers the classical NLP pipeline.\nIn ACL (1), pages 4593–4601. Association for Computational Linguistics, 2019a.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Na-\njoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick.\nWhat do you learn from context? probing for sentence structure in contextualized word\nrepresentations. In ICLR (Poster). OpenReview.net, 2019b.\nJesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer\nlanguage model. Second BlackboxNLP Workshop on Analyzing and Interpreting Neural\nNetworks for NLP, 2019.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bow-\nman. GLUE: A multi-task benchmark and analysis platform for natural language un-\nderstanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages 353–355, Brussels, Belgium, November\n2018a. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL\nhttps://www.aclweb.org/anthology/W18-5446.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-\npurpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d’ Alch´ e-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Pro-\ncessing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu\nChang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R 3: Reinforced ranker-reader for\nopen-domain question answering. In AAAI, pages 5981–5988. AAAI Press, 2018b.\nHow Context Affects Language Models’ Factual Predictions\nShuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,\nTim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-\nranking in open-domain question answering. In ICLR (Poster). OpenReview.net, 2018c.\nAlex Warstadt and Samuel R Bowman. Linguistic analysis of pretrained sentence encoders\nwith acceptability judgments. arXiv preprint arXiv:1901.03438, 2019.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Hagen Blix, Yining Nie, Anna Alsop,\nShikha Bordia, Haokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason Phang, Anhad Mo-\nhananey, Phu Mon Htut, Paloma Jeretic, and Samuel R. Bowman. Investigating bert’s\nknowledge of language: Five analysis methods with npis. In EMNLP/IJCNLP (1), pages\n2877–2887. Association for Computational Linguistics, 2019.\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. Zero-\nshot entity linking with dense entity retrieval. arXiv preprint arXiv:1911.03814, 2019.\nWenhan Xiong, Mo Yu, Xiaoxiao Guo, Hong Wang, Shiyu Chang, Murray Campbell, and\nWilliam Yang Wang. Simple yet eﬀective bridge reasoning for open-domain multi-hop\nquestion answering. Second Workshop on Machine Reading for Question Answering, 2019.\nVikas Yadav, Steven Bethard, and Mihai Surdeanu. Alignment over heterogeneous embed-\ndings for question answering. In Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages 2681–2691, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1274. URL\nhttps://www.aclweb.org/anthology/N19-1274.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and\nJimmy Lin. End-to-end open-domain question answering with BERTserini. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages 72–77, Minneapolis, Minnesota, June\n2019a. Association for Computational Linguistics. doi: 10.18653/v1/N19-4013. URL\nhttps://www.aclweb.org/anthology/N19-4013.\nWei Yang, Haotian Zhang, and Jimmy Lin. Simple applications of BERT for ad hoc docu-\nment retrieval. CoRR, abs/1903.10972, 2019b.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.\nIn Advances in neural information processing systems, pages 5754–5764, 2019c.\nZeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, and Jimmy Lin.\nApplying BERT to document retrieval with birch. In EMNLP/IJCNLP (3), pages 19–24.\nAssociation for Computational Linguistics, 2019.",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.5984157919883728
    },
    {
      "name": "Computer science",
      "score": 0.49104833602905273
    },
    {
      "name": "Linguistics",
      "score": 0.41770192980766296
    },
    {
      "name": "Natural language processing",
      "score": 0.37622445821762085
    },
    {
      "name": "Cognitive science",
      "score": 0.34563592076301575
    },
    {
      "name": "Psychology",
      "score": 0.30280929803848267
    },
    {
      "name": "History",
      "score": 0.17450380325317383
    },
    {
      "name": "Philosophy",
      "score": 0.15621298551559448
    },
    {
      "name": "Archaeology",
      "score": 0.04535916447639465
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ]
}