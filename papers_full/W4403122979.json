{
    "title": "Embers of autoregression show how large language models are shaped by the problem they are trained to solve",
    "url": "https://openalex.org/W4403122979",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2758499626",
            "name": "R. Thomas McCoy",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2222916931",
            "name": "Shun-Yu Yao",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2143871442",
            "name": "Dan Friedman",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2290416963",
            "name": "Mathew D. Hardy",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2122351653",
            "name": "Thomas L. Griffiths",
            "affiliations": [
                "Princeton University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4318919287",
        "https://openalex.org/W4385430086",
        "https://openalex.org/W3031914912",
        "https://openalex.org/W3210923133",
        "https://openalex.org/W4384154861",
        "https://openalex.org/W2134145060",
        "https://openalex.org/W3090441522",
        "https://openalex.org/W2087946919",
        "https://openalex.org/W2077877335",
        "https://openalex.org/W2161271547",
        "https://openalex.org/W2066261007",
        "https://openalex.org/W2973893193",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W1968024655",
        "https://openalex.org/W76845108",
        "https://openalex.org/W6838865847",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3193875190",
        "https://openalex.org/W4287854950",
        "https://openalex.org/W4385573636",
        "https://openalex.org/W4309217888",
        "https://openalex.org/W4362655426",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4401042580",
        "https://openalex.org/W4317463334",
        "https://openalex.org/W4385570579",
        "https://openalex.org/W4328049044",
        "https://openalex.org/W4386576705",
        "https://openalex.org/W2118373646",
        "https://openalex.org/W1966678693",
        "https://openalex.org/W2789352267",
        "https://openalex.org/W4378942694",
        "https://openalex.org/W4388787315",
        "https://openalex.org/W3213728903",
        "https://openalex.org/W4391215636",
        "https://openalex.org/W4385571633",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W3092323704",
        "https://openalex.org/W4306412326",
        "https://openalex.org/W4401123261",
        "https://openalex.org/W2001598681",
        "https://openalex.org/W4387950058",
        "https://openalex.org/W1996359725",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W2132684680",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2164418233",
        "https://openalex.org/W4322720178",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4303648904",
        "https://openalex.org/W2963091658",
        "https://openalex.org/W2889936086",
        "https://openalex.org/W2053727512",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2092919341",
        "https://openalex.org/W4392935324",
        "https://openalex.org/W4287658955",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that to develop a holistic understanding of these systems, we must consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts, we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. Using this approach—which we call the teleological approach—we identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. To test our predictions, we evaluate five LLMs (GPT-3.5, GPT-4, Claude 3, Llama 3, and Gemini 1.0) on 11 tasks, and we find robust evidence that LLMs are influenced by probability in the hypothesized ways. Many of the experiments reveal surprising failure modes. For instance, GPT-4’s accuracy at decoding a simple cipher is 51% when the output is a high-probability sentence but only 13% when it is low-probability, even though this task is a deterministic one for which probability should not matter. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system—one that has been shaped by its own particular set of pressures.",
    "full_text": null
}