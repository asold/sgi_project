{
    "title": "LLM4PM: A Case Study on Using Large Language Models for Process Modeling in Enterprise Organizations",
    "url": "https://openalex.org/W4402092309",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Ziche, Clara",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222761036",
            "name": "Apruzzese, Giovanni",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4312911974",
        "https://openalex.org/W1843425376",
        "https://openalex.org/W4367394488",
        "https://openalex.org/W4390694561",
        "https://openalex.org/W3151685851",
        "https://openalex.org/W3089448520",
        "https://openalex.org/W4283653080",
        "https://openalex.org/W4386321037",
        "https://openalex.org/W4392781032",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4220816940",
        "https://openalex.org/W2029228521",
        "https://openalex.org/W4386431794",
        "https://openalex.org/W4378938713",
        "https://openalex.org/W4206908609",
        "https://openalex.org/W1188413835"
    ],
    "abstract": null,
    "full_text": "LLM4PM: A case study on using Large\nLanguage Models for Process Modeling in\nEnterprise Organizations\nClara Ziche and Giovanni Apruzzese\nUniversity of Liechtenstein\nAbstract. We investigate the potential of using Large Language Mod-\nels (LLM) to support process model creation in organizational contexts.\nSpecifically, we carry out a case study wherein we develop and test an\nLLM-based chatbot, PRODIGY (PROcess moDellIng Guidance for You),\nin a multinational company, the Hilti Group. We are particularly inter-\nested in understanding how LLM can aid (human) modellers in creating\nprocess flow diagrams. To this purpose, we first conduct a preliminary\nuser study (n=10) with professional process modellers from Hilti, inquir-\ning for various pain-points they encounter in their daily routines. Then,\nwe use their responses to design and implement PRODIGY. Finally, we\nevaluate PRODIGY by letting our user study’s participants use PRODIGY,\nand then ask for their opinion on the pros and cons of PRODIGY. We\ncoalesce our results in actionable takeaways. Through our research, we\nshowcase the first practical application of LLM for process modelling in\nthe real world, shedding light on how industries can leverage LLM to\nenhance their Business Process Management activities.\n1 Introduction\nOrganizations perform business processes to deliver value-adding outcomes to\ntheir customers. Hence, Business Process Management (BPM) capabilities, such\nas process modeling, are a pivotal task in modern enterprises [3]. However, de-\nspite decades of efforts [7], process modeling still remains a costly activity due\nto, e.g., the difficulty of providing clear, up-to-date and easy-to-retrieve docu-\nmentation [3] to those tasked to carry out such activities—the process modelers.\nInspired by recent developments in artificial intelligence (AI), such as large\nlanguage models (LLM), researchers have proposed various techniques that can\nfacilitate BPM-related tasks (e.g., [6]). Indeed, LLM can elaborate large col-\nlections of documents. Hence, by receiving an input from a given user, LLM\ncan quickly produce an output that (i) accounts for existing documentation,\nwhile simultaneously (ii) answering the request of the user—i.e., a human. Yet,\nwe found no evidence of practical applications of LLM for BPM in real contexts\nand, in particular, for process modeling. Hence, there is a need to investigate the\neffectiveness of such automation in industry [15]. Here, we tackle this challenge\nand showcase how a large enterprise, Hilti, can benefit from a LLM-powered\nchatbot—which we developed ad-hoc for Hilti—for BPM.\narXiv:2407.17478v1  [cs.HC]  1 Jul 2024\n2 Clara Ziche and Giovanni Apruzzese\nCONTRIBUTIONS. We present a (the first) real-world case study show-\ncasing the application of LLM for Process Modeling in operational contexts.\nSpecifically, to advance the state of the art on BPM, we:\n• describe the problems faced by the considered organization , Hilti, providing\nevidence of the necessities of modern enterprises ( §2);\n• carry out a requirement analysis by conducting interviews with Hilti’s employ-\nees, shedding light on the pain-points of professional process modelers ( §3.1);\n• use our interviews as a scaffold to develop an original LLM-based chatbot ,\nPRODIGY (Fig. 1), designed to support Hilti’s process modelers ( §3.2);\n• evaluate the ability of PRODIGY to generate practical value by (i) having Hilti’s\nemployees use PRODIGY and (ii) collecting and analysing their feedback (§3.3).\nOur results (§4) show that PRODIGY is generally well-received, and identify room\nfor improvement. We also derive lessons learned that future work can use to\ndrive practical deployment of LLM-based technologies ( §5).\nIndustrial Secret and Ethics:In this paper, we describe some elements pertaining\nto the internal processes of Hilti: we have been granted permission to share such\ninformation. Furthermore, we carry out our user studies ethically: our institutions\nare aware of this research, our participants have been informed of their rights and\nof the purpose of their contributions, and we have their consent to post them.\nPRODIGY\n(LLM Chatbot)\nPlease model a\ncustomer complaint\nhandling process\nProcess Modeller\n(human)\nCustomer: Submit complaint\nSupport Team:Receive complaint\nInvestigate complaint\nResolve complaint\nCustomer: Confirm resolution\nCustomer: Submit complaint\nSupport Team:Receive complaint\nInvestigate complaint\nEscalate complaint\nManager: Review complaint\nResolve complaint\nCustomer: Confirm resolution\nFig. 1: An exemplary usage ofPRODIGY – Our LLM-powered chatbot can fulfill various BPM-\nrelated tasks. Its most appreciated functionality is generating an output for a complete process\nmodel (if copy-pasted into the open-source tool BPMN Sketch Miner [10])\n2 Organizational Context and Problem Statement\nOur case organization, Hilti Group, is a multinational company that was founded\nin 1941 in Schaan, Liechtenstein. It is a world market leader in fastening and\ndemolition technology for construction professionals and provides tools, technolo-\ngies, software and services to the global construction industry. In 2023, Hilti’s\nworkforce consists of about 33.000 employees in more than 120 countries, mak-\ning it a highly diverse, distributed organization that operates in complex and\ncompetitive markets all over the world. The size, complexity and business model\nof Hilti make it an ideal use case for testing the capabilities of LLM for process\nmodeling: BPM is essential to ensure cooperation and consistent outcomes within\nHilti’s ecosystem; furthermore, it is crucial for Hilti to optimize customer-facing\nprocesses. Hence, a smooth process modelling is pivotal for Hilti.\nA case study on LLM for Process Modeling in Enterprise Organizations 3\nChallenge. Hilti has an extensive and heterogeneous documentation land-\nscape which adds to the intrinsically complex nature of process modeling. Hilti’s\nemployees spend abundant time searching through such documentation (our\ninterviews revealed an average of ∼40 minutes of search before modelling a pro-\ncess). Hence, to improve the productivity of their process modellers, and to ac-\ntively explore innovative technologies, Hilti is interested in novel solutions that\nfacilitate the routines of their employees.\nTechnological Gap. LLM-based solutions are common (in 2024). However,\nexisting techniques cannot be applied to Hilti’s use case. This is because of the\nconfidential nature of Hilti’s documents: publicly available models (e.g., Chat-\nGPT) should not be able to access Hilti’s data to shape their responses; fur-\nthermore, even interacting with certain LLM (or their APIs) from within Hilti’s\nnetworks triggers warnings, preventing a reliable usage of these solutions—which\nare leveraged also by renown prior work, such as [4,6,11,12,2,18].\nOUR GOAL. We seek to design, develop and evaluate an LLM-based solution\nthat facilitates the job of Hilti’s process modellers. The development of such\na solution should be driven by Hilti’s distinctive organizational’s context—\nincluding its employee’s viewpoint, and its existing documentation.\n3 Research and Methods\nInspired by Peffers et al. [14], we followed a Design Science Research (DSR)\nprocess consisting of four phases depicted in Fig. 2. 1 DSR is appropriate given\nour goal of examining LLMs for process modelling in organizations, as DSR\nemphasizes the creation of innovative solutions (in our case, PRODIGY) while\nalso considering the context in which these solutions will be applied.\nConduct\nUser Study\nReview\nLiterature\nDefine\nProblem\nConduct\nInterviews\nDefine\nRequirements\nDevelop\nArtifact\nUse\nArtifact\nShare\nLearnings\nArtifact Definition Artifact Implementation\nArtifact\nEvaluation Dissemination\nBirth ofPRODIGY Operating\nModel\ncommunicate\ndevise\nvalidate\nFig. 2: Method. We rely on design science research to design, develop, and deploy our artifact.\nDuring the implementation ofPRODIGY, we also devise an “operating model” (which we validate in the\nevaluation of PRODIGY) through which we explain how PRODIGY should be used in real organizations.\n3.1 Artifact Definition\nAs a preliminary step, we carried out a systematic literature review [20] which\nwe used as a foundation to investigate the state of the art and define the scope\n1 Background: DSR is a methodology that focuses on creating and evaluating arti-\nfacts to solve complex problems. Such procedure is rooted on the coming together\nof people, organizations and technology, with the ultimate intention of “extending\nthe boundaries of human and organizational capabilities” [8].\n4 Clara Ziche and Giovanni Apruzzese\nof our project (see §2). Then, we carried out structured interviews [5] meant to\nidentify pain-points and desiderata by professional process modelers 2 working\nfor Hilti. We found an agreement with 10 employees, summarised in Table 1.\nThe complete questionnaire is provided in our repository [1]. Among the most\nrelevant questions, we ask: “what challenges do you experience when modelling\nprocesses?”, “how helpful is existing documentation when you model processes?”\nand “what would you like to see in a new AI artifact that supports process\nmodelling?”; we also provide a list of functionalities for the AI artifact and ask\nto rate them on a 1–5 scale, as well as potential concerns. Finally, we inquire\nabout the time spent looking for, and reviewing, existing documentation.\nTable 1: Overview of process modellers.Our participants pertain to various geographical lo-\ncations of Hilti, and have diverse backgrounds. [Demographics] Each participant has 5–25 years\nof experience in BPM, and they are within 26–60 years of age. Seven hold a MSc. degree. The\nmale:female ratio is 6:4. They all have “above average” or “advanced” computer knowledge, and all\nhave a basic understanding of LLM. Five perform process modelling activities at least weekly.\n# Job Title Functional AreaLocation\n1 Business Process Excellence Manager Corporate Schaan, FL\n2 Global Process Manager Communications Schaan, FL\n3 Business Process Excellence Manager Quality ManagementSchaan, FL\n4 Business Process Excellence Senior ManagerCorporate Schaan, FL\n5 Business Process Excellence Expert Corporate Schaan, FL\n6 Regional Process Manager Customer Service Plano, US\n7 Regional Process Manager Logistics Kaufering, GER\n8 Business Process Excellence Lead Corporate Schaan, FL\n9 Global Process Manager Repair Schaan, FL\n10Global Process Manager Repair Schaan, FL\n3.2 Artifact Implementation\nWe use the results of our interviews alongside those of our investigation of the\nstate of the art to define the requirements of our technical artifact, i.e., the LLM-\nbased chatbot PRODIGY. To develop PRODIGY, we rely on Botpress, a platform to\nbuild custom AI chatbots powered by GPT-based LLMs; for our prototype ver-\nsion of PRODIGY, we used GPT-3.5 Turbo, which we found provided satisfactory\nperformance while also requiring less resources to generate an output.\nA crucial aspect of PRODIGY is its reliance on the BPMN Sketch Miner\ntool [10]. The syntax for this tool is entirely text-based, human-readable and\nlight in terms of token consumption, making it appropriate for our case study.\nTherefore, we use few-shot prompting to teach PRODIGY to provide an output\nthat matches the format expected by BPMN Sketch Miner. This output serves as\nthe input for the model generation and transformation pipeline of BPMN Sketch\nMiner [9]. Such a design choice enables users of PRODIGY to directly paste the\nAI outputs into the online tool and get their model visualized (see Fig. 1).\nFurthermore, we have leveraged retrieval-augmented generation (RAG) [13]\nto embed Hilti’s documentation into PRODIGY. Such documentation included:\nprocess descriptions from Hilti’s internal documentation repository (anonymised);\nand information about Hilti’s process management, and how to model processes\n2 This is in stark contrast with a closely related work that does not carry out any user\nstudy [12], thereby preventing to fully capture the organizational context.\nA case study on LLM for Process Modeling in Enterprise Organizations 5\nat Hilti (taken verbatim from the learning platform for Hilti’s process modellers).\nThese procedures enabled us to instill some knowledge about Hilti’s processes\nin PRODIGY—a functionality that was heavily endorsed by our interviewees.\nDemonstration: We recorded a video [1] showing the functionalities of PRODIGY.\n3.3 Artifact Evaluation\nWe conducted a user study with our artifact and process modellers. Our aim\nwas to answer evaluative questions on the quality of PRODIGY for Hilti.\nFirst, the process modellers tested all functionalities of PRODIGY by creat-\ning custom prompts, with the intention of simulating their routine tasks. Their\ninputs and the corresponding AI-generated outputs are fully observable in our\nrepository [1]. Then, we carried out semi-structured interviews during which the\nparticipants answered 28 questions. Among these, we ask to give an 1–5 rating\nto the statement “Using PRODIGY would make it easier for me to do process\nmodeling tasks.” The complete questionnaire is provided in our repository [1].\n3.4 Dissemination and Communication of the Results\nTo conclude our DSR process, we formalized our learnings and made them ac-\ncessible to interested parties. We documented our observations, analyzed our\nfindings, identified lessons learned, stated limitations, and recommended direc-\ntions for future work. We shared our learnings within Hilti Group and the wider\nBPM community in academia and practice—some companies reached out to us\nand expressed their interest about the development process of PRODIGY.\n4 Key Findings and Lessons Learned\nWe first summarise the major results of our user studies, and then outline our\nproposed “operating model” for our developed LLM-based chatbot, PRODIGY.\nConfidentiality Statement: To protect the privacy of the participants to our user\nstudies, we cannot reveal the full transcript of their interviews. However, we are able\nto answer questions about their generic viewpoint on some specific issues.\n4.1 Preliminary Interviews: what do Hilti process modellers want?\nThese open interviews lasted for 60 minutes, and the results shed light on the\npain-points and desiderata of our participants. We found that, before modelling\na process, 60% spend between 15–60 minutes searching for documentation; and\nalso 60% spend between 5–60 minutes toreview such documentation. As a matter\nof fact, 90% state that it is “extremely important” that an LLM-based chatbot\nhas access to Hilti’s documentation; however, we also found that, on a 1–10\nrating (low to high) scale, the average usefullness of current Hilti’s documen-\ntation is 6.7—indicating helpfulness, but with huge margins for improvement.\nNonetheless, with respect to AI-related concerns, some stated that “humans may\nmisinterpret the AI’s outputs” or “AI may negatively impact collaboration with\ncolleagues” or even about accountability (“the mindset that [the machine] does\neverything and we no longer have to worry about it is dangerous”).\n6 Clara Ziche and Giovanni Apruzzese\nTAKEAWAYS. After analysing all our responses, we identified two design\nobjectives which we used as basis to develop our LLM-based chatbot,PRODIGY.\n– The chatbot should support process modellers in creating BPMN models.\nIn doing so, the chatbot should hint at the larger picture, i.e., emphasize\nand guide in purpose, usage, and value creation of the resulting models.\n– The chatbot should be able to access and utilize existing documentation,\nand hence be aware of organizational specifics. Such knowledge should drive\nthe formulation of the output, which will be tailored to the organization.\nThe name PRODIGY stands for “PROcess moDellIng Guidance for You”.\n4.2 Evaluation: what do Hilti process modellers say about PRODIGY?\nAfter letting our process modellers use PRODIGY, we collected their feedback via\n90-minutes long semi-structured interviews; one participant to the preliminary\ninterviews did not provide feedback since they were not available, so we obtained\nresponses from nine employees. The general opinion was positive . Five of\nour participants asserted that they would usePRODIGY on a daily or weekly basis\n(i.e., whenever they have to carry out process-modeling duties). Moreover, six\nparticipants asserted that PRODIGY would speed-up their tasks (three remained\nneutral), and eight believe that PRODIGY makes their tasks easier (one remained\nneutral). Finally, we report in Fig. 3 the participants’ perception on the func-\ntionalities we integrated in PRODIGY, showing great appreciation.\nFig. 3:Helpfulness ofPRODIGY’s functionalities.According to our participants, most of our imple-\nmented features are helpful—especially for creating process modelsand supporting human requests.\n4.3 Operating model: how should PRODIGY be used in practice?\nDuring our implementation, we devised an operating model that describes how\nPRODIGY should be leveraged by real organizations; we have further refined our\nmodel (shown in Fig. 4) after receiving the feedback by our interviewees.\nA case study on LLM for Process Modeling in Enterprise Organizations 7\nAt a high-level, our model emphasizes continuous improvement through reg-\nular evaluations, feedback, and updates. To this end, our model delineates the\ninteraction between a Governance Team (i.e., the set of employees within a com-\npany that oversee the development and maintenance of PRODIGY) and a Process\nModeller (i.e., the end-users of PRODIGY). These two actors work collaboratively\nto ensure that the system performs well over time. For instance, the Process\nModeller should be familiar with existing documentation and with the specific\nprocess, and scrutinize the response of PRODIGY accordingly; they should also be\nwilling to provide feedback (collected in a dedicated repository) and receive guid-\nance from the Governance Team—who must, in turn, define clear performance\nindicators for PRODIGY and periodically review the performance of prodigy (e.g.,\nby analysing logs [19]) and apply updates if needed; as well as ensure that ex-\nisting documentation is properly embedded in PRODIGY (in a timely manner).\nPRODIGY\nprocess\ndocumentation\nperformance\nindicators\nprocess\nmodel\nBPM project\nprompt response\nrepository\nGovernance Team\nprovide guidance\ndefine\nimprove\nevaluate\nmonitor\ngenerate\ncheck \nusage in\nstore in\ninclude depend\nembed\nrevise\nsubmit\nProcess Modeler\nFig. 4: Operating model of PRODIGY – We visualize the interactions between the Governance\nTeam (e.g., developers and managers) of a given organization with the Process Modeller(i.e., the\nend-users of PRODIGY) that ensure a smooth operation of PRODIGY for real-world deployments.\n5 Significance and Relevance in Research and Practice\nBesides our key findings we underscore three orthogonal aspects of our research.\nThe perspective of process modellers in organizations. We coalesce\nthe responses—not pertaining to AI—of our preliminary interviews, and de-\nrive an original framework representing dynamics of process modellers’ issues at\nHilti. This is instructive because, during our literature analysis, we found some\nworks mentioning pitfalls of process modeling (e.g., [16]) but without accounting\nfor context. Our framework (displayed in Fig. 5, and described in the caption of\nFig. 5) attempts to rectify this shortcoming, providing guidance for future work.\nEvaluating LLM-/AI-based solutions. Upon further analysing the re-\nsults of our evaluation interview, we have found that the reception of PRODIGY\nby our process modellers was highly dependant on their expectations and over-\nall attitude towards AI and IT innovation. Indeed, some participants had “lower\nexpectations” and provided prompts that were “more aligned” to the expected\n8 Clara Ziche and Giovanni Apruzzese\ninput of PRODIGY—and these participants ratedPRODIGY more positively. In con-\ntrast, participants who were expecting that PRODIGY would “do their work for\nthem” by issuing a single (and typically poorly phrased and/or unclear) prompt\nwere more skeptical of PRODIGY’s helpfulness. These results underscore the im-\nportance of (i) accounting for each end-user’s expectations while evaluating the\nperformance of operational AI-based solutions; as well as(ii) educating end-users\non the potential (and limitations) of AI-based tools.\nThe role played by higher education. With this paper, we (also) seek\nto bridge three domains: industrial practice, scholarly literature, and higher-\neducation institutions [17]. This research has been predominantly carried by\nClara Ziche for her MSc. thesis, during which she was working part-time at\nHilti. The development of PRODIGY was driven by following the guidelines of\nprior academic literature, and the resulting artifact was appreciated by Hilti as\nwell as by other companies that witnessed its capabilities. On this note, we find\nit instructive to trace the timeline of this research by outlining the path followed\nby Clara Ziche to bring our findings to light. In Sept–Dec 2023, after attending\nthe BPM’23 conference, Clara investigated the state of the art and designed\nthe interviews for the requirement analysis. In Jan 2024, Clara carried out the\ninterviews, and began familiarizing with current LLM technologies. In Feb 2024,\nClara developed PRODIGY and designed the questionnaire for its evaluation—\nwhich took place in March 2024. Disseminations occurred in Apr–May 2024.\nTechnical feasibility. After having collected the input from experts, the develop-\nment of PRODIGY took only two weeks from a MSc. student in Information Systems.\nembeddedness in business context\nspecific modeling task\nModel value\nPrep\nphase\nModel\nusage\nModel\ncreation\nPurpose: strategic directive\nPurpose: value creation\nFig. 5: Issues of process modellers.Our framework has three levels with two-way transitions be-\ntween each level—each having its own set of issues. Model value→Model creation:the organization\nmust communicate a clear strategic directive for process model creation, balancing the cost/ben-\nefit of model creation and usage [issue: during the “Prep phase”, process modellers find existing\ncommunication to lack clarity, leading to time waste and unproductive discussions among various\nstakeholders]. Model creation→Model value: While creating the model, process modelers should\nhave a clear vision of “who and how” is going to use the model (which is what leads to the model\nbecoming valuable) [issue: lack of clarity and/or poor documentation may lead to process models\nrepresenting “standalone exercises” which do not bring any value to the company.]\nA case study on LLM for Process Modeling in Enterprise Organizations 9\n6 Discussion: Scope and Limitations\nWe showcased an exemplary application of an LLM-based chatbot that can as-\nsist process modellers in a large organization, Hilti. In doing so, we have carried\nout a twofold user study with 10 employees of Hilti, and developed an original\nartifact, PRODIGY. Our research has a number of limitations. For instance, we\ndo not claim that our findings can apply to other organizations—irrespective of\ntheir similarity to Hilti. Moreover, we cannot claim that even our own findings\ncan apply to the entirety of Hilti: The participants of our user study are mostly\nbased in Liechtenstein, and therefore cover the global headquarters perspective\nrather than regional and local perspectives. Furthermore,PRODIGY uses GPT-3.5\nTurbo (which is not privacy-compliant), and it relies on BPMN Sketch Miner:\nif such a tool is taken down, the output of PRODIGY may lose its immediate use-\nfulness. Finally, even our own participants have pointed out some shortcomings\nof PRODIGY, such as a poor “knowledge” of Hilti’s documentation. Such a result,\nhowever, was expected: the documents that PRODIGY has access to (with RAG)\nare just a drop in the deluge of files and logs included in Hilti’s databases (and\nwe, as researchers, do not have complete access to such data).\n7 Conclusions\nWe have presented the first case study showcasing how LLM can be used for\nprocess modeling in large enterprises—specifically, Hilti Group. We follow DSR\nguidelines and develop an original LLM-based chatbot, PRODIGY, which we test\nwith professional process modellers from Hilti. Our findings revealed that end-\nusers appreciate the functionalities of PRODIGY. However, concerns were raised\nabout the poor alignment of PRODIGY’s output with Hilti’s specifications. Such\na shortcoming underscores the importance of integrating LLM-based solutions\nwith the organization’s documentation—which is a task outside the responsibili-\nties of process modellers. Hence, deployment of similar solutions in real contexts\nshould be done with the support of the organization’s governance team: it is\nunrealistic to expect that “off the shelf” solutions work properly to drive the\nprocess modeling routines of complex and large organizations (see Fig. 6).\nProcess\nModeler\nLLM\nissue request(prompt)\nprovide response\n(output)\nLearn from\neach other\nreceive / processfeedback\nimprove LLM\nORGANIZATION\nGovernance\nTeam\nGuide\nlearning\nProcess\nModeler\nLLM\nissue request(prompt)\nprovide response\n(output)\nContext agnostic Context specific\nFig. 6: Takeaway. LLMs are hardly usable for process modeling in a context-agnostic setting [left].\nDeployment of LLM in organizations for process modeling should follow a context-specific approach,\nin which the governance team ensures that LLM and end-users “learn from each other” [right].\n10 Clara Ziche and Giovanni Apruzzese\nAcknowledgements.We thank Hilti for enabling and funding this research;\nand the participants to our user study for their contributions and feedback.\nReferences\n1. Our repository. https://github.com/Nouronihar/BPM24_LLM4PM\n2. Bellan, P., Dragoni, M., Ghidini, C.: Extracting business process entities and re-\nlations from text using pre-trained language models and in-context learning. In:\nInternational Conference on Enterprise Design, Operations, and Computing (2022)\n3. Dumas, M., La Rosa, M., Mendling, J., Reijers, H.A., et al.: Fundamentals of\nbusiness process management. Springer (2018)\n4. Fill, H.G., Fettke, P., K¨ opke, J.: Conceptual modeling and large language models:\nimpressions from first experiments with chatgpt. EMISAJ (2023)\n5. Franch, X., Palomares, C., Quer, C., Chatzipetrou, P., Gorschek, T.: The state-of-\npractice in requirements specification: an extended interview study at 12 compa-\nnies. Requirements Engineering (2023)\n6. Grohs, M., Abb, L., Elsayed, N., Rehse, J.R.: Large language models can accom-\nplish business process management tasks. In: Int. Conf. BPM (2023)\n7. Herbst, J., Karagiannis, D.: An inductive approach to the acquisition and adapta-\ntion of workflow models. In: IJCAI (1999)\n8. Hevner, A.R., March, S.T., Park, J., Ram, S.: Design science in information systems\nresearch. MIS quarterly (2004)\n9. Ivanchikj, A., Serbout, S., Pautasso, C.: From text to visual bpmn process models:\nDesign and evaluation. In: ACM/IEEE MoDELS (2020)\n10. Ivanchikj, A., Serbout, S., Pautasso, C.: Live process modeling with the bpmn\nsketch miner. Software and systems modeling (2022)\n11. Klievtsova, N., Benzin, J.V., Kampik, T., Mangler, J., Rinderle-Ma, S.: Conversa-\ntional process modelling: state of the art, applications, and implications in practice.\nIn: International Conference on Business Process Management (2023)\n12. Kourani, H., Berti, A., Schuster, D., van der Aalst, W.M.: Process modeling with\nlarge language models. arXiv:2403.07541 (2024)\n13. Lewis, P., et al.: Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems (2020)\n14. Peffers, K., Tuunanen, T., Rothenberger, M.A., Chatterjee, S.: A design science\nresearch methodology for information systems research. JMIS (2007)\n15. Plattfaut, R., Borghoff, V., Godefroid, M., Koch, J., Trampler, M., Coners, A.:\nThe critical success factors for robotic process automation. Comp. Ind. (2022)\n16. Rosemann, M.: Potential pitfalls of process modeling: part a. BPM Journal (2006)\n17. Senkus, P., Berniak-Wozny, J., Gabryelczyk, R., Napieraj, A., Podobi´ nska-Staniec,\nM., Sli˙ z, P., Szelkagowski, M.: Bridging the gap: An evaluation of business process\nmanagement education and industry expectations–the case of poland. In: Interna-\ntional Conference on Business Process Management (2023)\n18. Sola, D., van der Aa, H., Meilicke, C., Stuckenschmidt, H.: Activity recommenda-\ntion for business process modeling with pre-trained language models. In: European\nSemantic Web Conference (2023)\n19. Stein Dani, V., Leopold, H., van der Werf, J.M.E., Lu, X., Beerepoot, I., Koorn,\nJ.J., Reijers, H.A.: Towards understanding the role of the human in event log\nextraction. In: International Conference on Business Process Management (2021)\n20. Vom Brocke, J., Simons, A., Riemer, K., Niehaves, B., Plattfaut, R., Cleven, A.:\nStanding on the shoulders of giants: Challenges and recommendations of literature\nsearch in information systems research. CAIS (2015)"
}