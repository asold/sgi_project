{
  "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
  "url": "https://openalex.org/W4385572258",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4296696864",
      "name": "Suet-Ying Lam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102522522",
      "name": "Qingcheng Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2242716642",
      "name": "Kexun Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2678749619",
      "name": "Chenyu You",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128960775",
      "name": "Rob Voigt",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2968619018",
    "https://openalex.org/W6816406768",
    "https://openalex.org/W1980862600",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W2921890305",
    "https://openalex.org/W4304697829",
    "https://openalex.org/W2115867364",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3098952151",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4280539892",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W76724602",
    "https://openalex.org/W3021102630",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W2075806525",
    "https://openalex.org/W4296564631",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2007495516",
    "https://openalex.org/W2804751826",
    "https://openalex.org/W2031878513",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W3167227435",
    "https://openalex.org/W3034685497",
    "https://openalex.org/W3103816537",
    "https://openalex.org/W4248365693",
    "https://openalex.org/W3099772776"
  ],
  "abstract": "While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson & Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at https://github.com/zkx06111/llm_priming.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 9493–9506\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Are Partially Primed in Pronoun Interpretation\nSuet-Ying Lam†∗ Qingcheng Zeng‡∗ Kexun Zhang♦∗ Chenyu You♣ Rob Voigt‡\n† Department of Linguistics, UMass Amherst\n‡ Department of Linguistics, Northwestern University\n♦ Department of Computer Science, UCSB\n♣ Department of Electrical Engineering, Yale University\nAbstract\nWhile a large body of literature suggests that\nlarge language models (LLMs) acquire rich lin-\nguistic representations, little is known about\nwhether they adapt to linguistic biases in a\nhuman-like way. The present study probes\nthis question by asking whether LLMs display\nhuman-like referential biases using stimuli and\nprocedures from real psycholinguistic experi-\nments. Recent psycholinguistic studies suggest\nthat humans adapt their referential biases with\nrecent exposure to referential patterns; closely\nreplicating three relevant psycholinguistic ex-\nperiments from Johnson and Arnold (2022) in\nan in-context learning (ICL) framework, we\nfound that InstructGPT adapts its pronominal\ninterpretations in response to the frequency\nof referential patterns in the local discourse,\nthough in a limited fashion: adaptation was\nonly observed relative to syntactic but not se-\nmantic biases. By contrast, FLAN-UL2 fails to\ngenerate meaningful patterns. Our results pro-\nvide further evidence that contemporary LLMs\ndiscourse representations are sensitive to syn-\ntactic patterns in the local context but less so\nto semantic patterns. Our data and code are\navailable at https://github.com/zkx06111/\nllm_priming.\n1 Introduction\nWhile neural network models, and particularly pre-\ntrained large language models have shown excel-\nlent performance at particular language processing\ntasks, many questions remain about the extent to\nwhich models optimized for such performance en-\ncode, as a side effect, human-like linguistic knowl-\nedge and cognitive biases. We know that they do to\nsome extent; existing work has shown, for example,\nthat neural models encode aspects of human-like\nlong-distance number agreement ( Gulordava et al. ,\n2018), incremental syntactic state ( Futrell et al. ,\n∗ ∗Equal contribution by alphabetical order. Correspon-\ndence to qcz@u.northwestern.edu\n2019), and syntactic generalization more broadly\n(Hu et al. , 2020). In this paper, we examine whether\nFLAN-UL2 ( Tay et al. , 2023) and InstructGPT\n(Ouyang et al. , 2022), two representative LLMs,\ndisplay adaptation in pronoun interpretation when\nexposed to consistent referential patterns in the lo-\ncal discourse context.\nCompared with syntactic or lexical knowledge,\nrepresenting referential knowledge is possibly\nmore complex; we know from psycholinguistic\nstudies that human referential interpretation inte-\ngrates multiple levels of linguistic structure. Hu-\nmans do not interpret ambiguous pronouns at ran-\ndom but are guided by both syntactic and semantic\ninformation. It is well-established that absent other\ncues humans prefer a syntactic subject in choos-\ning the antecedent of the ambiguous pronoun, i.e.,\nsubject bias(Ariel, 1990; Brennan, 1995). In ex-\nample (1), she is more likely to be interpreted as\nthe subject Ada than the non-subject Eva ¶, even\nthough both referents are possible antecedents for\nthe pronoun.\n(1) Ada 1 talked with Eva 2. She 1...\nPeople are also sensitive to the semantic struc-\nture of the sentence when choosing an antecedent\nfor an ambiguous pronoun, in addition to syntac-\ntic information. In a transfer event that depicts a\ntransfer-of-possession from one entity (the source)\nto another (the goal), they prefer the goal refer-\nent (Ada in (2), Eva in (3)) over the source refer-\nent (Eva in (2), Ada in (3)) to be the antecedent\n(Arnold, 2001, 1998), i.e., goal bias:\n(2) Goal-source (gs) verb:\nAda1 received a letter from Eva 2. She 1...\n(3) Source-goal (sg) verb:\nAda1 sent a letter to Eva 2. She 2...\n¶Indices of pronouns in examples indicate the preferred\nreferent.\n9493\nIn sum, people exhibit sensitivity to both syn-\ntax (subject bias) and semantics (goal bias) during\npronoun interpretation. Importantly, these levels of\nlinguistic structure are frequently entwined since\nboth can inﬂuence referential interpretation. At\ntimes these inﬂuences may push in different direc-\ntions: for instance, Ada in [3] is both the syntactic\nsubject and the semantic source.\nBuilding on a long tradition investigating prefer-\nences in pronoun interpretation, recent psycholin-\nguistic studies have probed into the deeper question\nof the origin of these biases. One hypothesis is that\nreferential biases come from linguistic experience:\nwhen a bias occurs very frequently, people will\ntend to adapt this more frequent referential pattern,\nboth in the immediate exposure as well as more\nlarge-scale past experience (\nArnold, 1998, 2001).\nRecent evidence has provided support to this idea\nby demonstrating that recent exposure to certain\nreferential patterns did change people’s referen-\ntial biases. In a series of psycholinguistic studies,\nJohnson and Arnold (2022) show that after reading\nnumerous stories that consistently show a partic-\nular referential bias, e.g., always referring to the\nnon-subject or source referent, people did have a\nstronger preference for these primed referents.\nGiven this line of psycholinguistic research, the\ncurrent study investigates the extent to which LLMs\nadapt and vary referential biases in pronoun inter-\npretation through exposure to referential patterns\nin the local context. To do so we replicated actual\npsycholinguistic experiments from Johnson and\nArnold (2022) in LLMs using an ICL framework\nand asked whether the responses of LLMs display\nadaptation from exposure to referential patterns\nlike human experimental participants. Comparing\nsyntactically-motivated to semantically-motivated\nexposure conditions will allow us to ﬁrst exam-\nine whether LLMs display human-like subject bias\nand goal bias and further understand the extent to\nwhich LLMs make use of local frequency changes\nin discourse representations in these categories.\nIn-context learning refers to LLMs’ ability to\nlearn from demonstrations written in natural lan-\nguage prompts. Compared to previous work that\nhas largely examined the encoding of such dis-\ncourse knowledge using zero-shot inference ( Upad-\nhye et al. , 2020), ICL is particularly suitable for\nexperimental simulation since it replicates the natu-\nralistic context in which later responses draw upon\nexposure to previous examples.\nForeshadowing our results, we ﬁnd that Instruct-\nGPT can adapt and thus vary its syntactic bias from\nexposure to referential patterns in the local context,\nbut the same is not true for semantic bias. Given\nthat InstructGPT still exhibits a goal bias in spite\nof local discourse priming, we argue this suggests\nLLMs only encode partial semantic knowledge in\nreferential processing. To sum up, our contribu-\ntions can be summarized as follows:\n1) We extended a discourse understanding eval-\nuation to state-of-the-art LLMs from a new\nperspective, asking whether LLM’s referential\nbias can be modiﬁed by exposure to particular\nreferential patterns, like how humans adapt\nreferential bias from experience.\n2) To the best of our knowledge, we are the ﬁrst\nstudy that replicates actual psycholinguistic\nexperiments using the ICL framework and\ncompares LLMs’ behaviors with real human\nparticipants.\n3) We present results in this context showing fur-\nther evidence that InstructGPT can acquire\nabstract syntactic knowledge in referential in-\nterpretation to some extent, but not semantic\nknowledge.\n2 Related Work\nA growing body of literature has suggested that\nLLMs encode rich representations of linguistic\nstructure at various levels, including aspects of syn-\ntax, semantics, and reference encoded throughout\ntheir representations ( Tenney et al. , 2019). One\nof the most well-documented lines of this work\ndemonstrates that these models can acquire diverse\nelements of syntactic knowledge ( Gulordava et al. ,\n2018; Futrell et al. , 2019; Hu et al. , 2020).\nThis capacity for encoding linguistic understand-\ning extends to priming effects with psycholinguistic\nanalogies to humans. At the syntactic level, Sin-\nclair et al. (2022) explored structural priming in\nvarious autoregressive LLMs and found priming\neffects despite a clear dependence on semantic in-\nformation. At the semantic level, using English\nlexical stimuli in BERT ( Devlin et al. , 2019), Misra\net al. (2020) found that BERT does display evi-\ndence of sensitivity to semantic priming, though\nthis is localized to more unconstrained contexts and\nonly certain semantic relations.\nIn analyses of LLMs’ linguistic understanding\nmodeled on psycholinguistic experiments, however,\n9494\nthe question of discourse knowledge remains rel-\natively under-explored. Recent existing work has\npresented partially conﬂicting accounts in this area,\nin particular with regard to how LLMs may or may\nnot exhibit human-like biases in pronoun interpreta-\ntion. For example, Davis and van Schijndel (2020)\ncompared LSTM LMs and Transformer LMs be-\nhaviors and internal representations in dealing with\nimplicit causality verbs, ﬁnding that surprisingly\n(contrary to humans) implicit causality only inﬂu-\nences Transformer LMs’ behavior for reference,\nbut inﬂuences neither model for syntactic attach-\nment. Sorodoc et al. (2020) also compared LSTM\nLMs and Transformer LMs in coreference resolu-\ntion corpora, ﬁnding that although LMs are much\nbetter at grammar, they also captured referential\naspects to some extent.\nThis existing work replicates sets of individ-\nual stimuli from psycholinguistic experiments in\nisolation; by contrast, our work takes a more\nbehaviorally-oriented approach by replicating the\nstimuli, procedure, and even experimental design\nof a full set of human psycholinguistic studies. We\ndo this to ask a further question: beyond exhibit-\ning baseline human-like referential biases in pro-\nnoun interpretation, do LLMs display adaptation\nto the frequency of referential patterns in the lo-\ncal context like humans? This question is impor-\ntant because adapting to a referential bias over the\ncourse of an experiment requires a sustained rep-\nresentation of the frequency of the pattern, which\ninvolves a higher-level understanding of the dis-\ncourse structure. Humans have shown abilities of\nthis kind of adaptation at multiple linguistic levels.\nFor instance, exposure to syntactic structures con-\nsecutively affects humans’ choice of structures in\nboth the short term and long term via priming ef-\nfects (e.g., Branigan et al. 2005 ; Chang et al. 2000 ).\nExposure to phonological patterns can also guide\nhumans’ segmentation patterns (e.g., Saffran et al.\n1996a,b). At the semantic and pragmatic levels,\nhumans can also adapt as listeners to speakers’ vari-\nable choices of uncertainty expressions ( Schuster\nand Degen , 2020).\n3 Methods\nIn this work, we aim to replicate three experiments\nfrom Johnson and Arnold (2022), transferring their\ndesigns, procedures, and stimuli as faithfully as\npossible to the LLM context using in-context learn-\ning.\n3.1 Source Experiments\nWe ﬁrst brieﬂy summarize the experimental setup\nemployed by Johnson and Arnold (2022). In each\nexperimental setting, participants heard a series\nof two-sentence stories in which the ﬁrst sentence\ncontained two characters with gender-marked ﬁrst\nnames (Matt or Will for men, Liz or Ana for\nwomen). For each story, participants answered\na content question to check comprehension, and\nthen a reference question to check pronominal in-\nterpretation. In order to lower the ceiling and keep\nparticipants from falling into a pattern of simply\nanswering \"yes,\" reference questions were equally\nsplit between default and non-default phrasings\n(e.g. between the subject/non-subject interpreta-\ntions in Experiment 1a and 2a and between the\nsource/goal interpretations in Experiment 2b). Fig-\nure 1 illustrates sample stimuli and the procedure\nof the three experiments conducted.\nIn each experiment participants were ﬁrst shown\na series of stories with exposure reference ques-\ntions to establish a referential pattern; in these, the\ncharacters had different genders so pronoun inter-\npretation was unambiguous. After the exposure\nphase (20 stories in Experiment 1, 12 stories in\nExperiment 2), further stories with unambiguous\nexposure questions were intermixed with 12 sto-\nries accompanied by critical reference questions;\nin these stories, the characters had the same gen-\nder, so pronoun interpretation was ambiguous and\nrequired reliance on discourse cues.\nAcross experiments, there were two key condi-\ntions that were manipulated for exposures. Under\na subject exposure condition, the unambiguous in-\ntended referents of all exposure questions are sub-\njects of the preceding clauses; in the corresponding\nnon-subject exposure condition, they are the ob-\njects of the preceding clauses. Similarly, under a\ngoal exposure condition the unambiguous intended\nreferents of all exposure questions are goal refer-\nents while in a source exposure condition they are\nsource referents.\nAiming to transfer these experiments as faith-\nfully as possible to the LLM context, we used this\nexperimental paradigm to evaluate the LLM by pro-\nviding the model with the full text of each story\nprompt and content/reference question, then gen-\nerated tokens in response which we interpreted as\nanswers.\nFor clarity, we use the same experiment numbers\nand identiﬁers as in Johnson and Arnold (2022).\n9495\nExperiment 1:\nStory: Will went running with Ana. She needed some \nwater.\nContent question: What did they do?\nReference question: Did Will need water?\njoint-action story without clear semantic bias, \nsubject vs. non-subject exposure\nExperiment 2a:\nStory: Will and Liz were watching TV. Will took the \nremote from Liz, and then she went to get a beer.\nContent question: What were Will and Liz doing?\nReference question: Did Will go to get a beer?\ntransfer event with semantic goal bias, subject vs. \nnon-subject exposure\nExperiment 2b:\nStory: Will and Ana were going ice skating. Will \nbrought the skates to Ana, and then she put them on.\nContent question: What did Will bring to Ana?\nReference question: Did Ana put her skates on?\ntransfer event with semantic goal bias, goal vs. \nsource exposure\nExperiment 1:\nStory: Matt is having dinner with Will. He wanted \nsome chicken.\nContent question: What did they do?\nReference question: Did Will want some chicken?\nExperiment 2:\nStory: Will and Matt were watching a movie. Will took \nthe popcorn from Matt, and then he drank some soda.\nContent question: What did Will take from Matt?\nReference question: Did Matt drink some soda?\nIntermixed Same as exposures above.\nFigure 1: Illustrated Experimental Procedure. Closely following the setup with human participants in Johnson and\nArnold (2022), LLMs were primed via exposure to story/question pairs with unambiguous referents, and tested for\ntheir responses on ambiguous target pairs.\nNote that we did not replicate experiments 1b and\n1c, which investigated whether people are still sen-\nsitive to referential patterns using different types of\nreferential expressions (e.g., third-person names\nand ﬁrst- and second-person pronouns). While\nthese experiments provided insights into the linguis-\ntic structure at which people generalize referential\nbiases, they were less relevant to the objective of\nthis study and thus were not included.\nExperiment 1a In this experiment, all story\nprompts contain \"joint-action\" verbs using \"with\"\nin the form \"X did something with Y .\" Since these\nverbs lack a clear semantic bias (e.g., Arnold et al.\n2018), this context allows us to evaluate LLMs’\nsensitivity to syntax-based biases in discourse by\nasking whether exposure to only subject-bias or\nobject-bias examples will inﬂuence following an-\nswers on the ambiguous critical items. If LLMs are\nsensitive to syntax-based referential patterns, we\nexpect more subject responses under the subject ex-\nposure condition and more non-subject responses\nunder the non-subject exposure condition.\nExperiment 2a Experiment 2a forms a bridge\nbetween adaptation to syntactic and semantic con-\nstraints. Are LLMs able to track patterns in both\ncategories, for instance learning an exposure bias\nin one category while ignoring variation in the\nother? In this experiment, all story prompts con-\ntain source/goal verbs like \"give\" and \"receive,\" but\nthese are distributed equally throughout exposures.\nThe manipulation remains the same as Experiment\n1a, in which LLMs are exposed to consistent and\nunambiguous subject interpretations in the subject\nexposure condition and non-subject interpretations\nin the non-subject exposure condition.\nExperiment 2b This experiment focuses solely\non source/goal biases, in which all story prompts\ncontain source/goal words, but the unambiguous\nexposure items are manipulated to contain only\nsource references in the source exposure condition\nand only goal references in the goal exposure con-\ndition.\n3.2 In-context Learning\nWe propose that since these experiments rely on\nshort-term learning effects of exposure in an ex-\nperimental context, they can only be effectively\nsimulated with LLMs by using in-context learn-\ning recursively. Speciﬁcally, for each question, the\nmodel is provided access to all previous items in an-\nswering a new question, including its own previous\nresponses. This is intended to mirror the process\nof human experimental participants making judg-\nments in the light of recent exposure to input and\n9496\nFigure 2: The ICL Simulation Framework. Black text is provided to the models as prompts, red text is generated by\nthe model. With the last step’s output appended to previous prompts, we ask LLMs the next question.\ntheir own past responses.\nWe manually checked the correctness of LLM\nresponses to content questions intended to check\ncomprehension, and as in the human experimenta-\ntion context removed answers for which the LLMs\nprovided incorrect answers. The answers to critical\ntarget questions were recorded for further statistical\nanalysis. Our ICL procedure is shown in Figure 2.\n3.3 Models and Experimental Settings\nWe used text-davinci-003 from the OpenAI API\nand open-sourced FLAN-UL2 as the LLMs of inter-\nest. Though these models are of course not exhaus-\ntive of the current landscape of LLMs, they provide\nsome diversity since they differ in both structure\n(decoder-only vs. encoder-decoder) and parameter\ncount (175B vs. 20B). To introduce more random-\nness into the experiment and allow enough sample\nsizes for statistical comparisons across conditions,\nwe made slight modiﬁcations to the temperature\nhyperparameter on each run. Speciﬁcally, we as-\nsigned each run a random and unique temperature\nvalue between 0.2 to 1.0.\nWe also attempted to simulate ‘participants’ us-\ning a natural language prompt to approach different\nspeaker identities following a similar methodol-\nogy to\nAher et al. (2022). We developed prompts\nwith slots for titles, names, and country of origin\nto establish different character backgrounds sim-\nulating native English speakers from the United\nStates, Britain, and Australia, following the partici-\npants’ demographics in Johnson and Arnold (2022).\nHowever, these prompts did not induce greater di-\nversity in responses than temperature modiﬁcation,\nso only results using temperature modiﬁcation are\npresented below. We present a further analysis for\nboth methodologies in Appendix B.\nIn the end, we simulated 24 ‘participants’ each\nin Experiment 1a and Experiment 2a, and 60 ‘par-\nticipants’ in Experiment 2b. We included more\nin Experiment 2b because this experiment has a\nlower response variability and thus needs more\ndata points for the statistical analysis.\n3.4 Measures\nFollowing the analytic approach of Johnson and\nArnold (2022), we used a regression-based ap-\nproach to analyze whether the responses of LM\nare consistent with the subject or gogoalal bias of\nthe context for each experiment. We can then com-\npare our ﬁndings with theirs by asking whether\nthe effect of the main predictors is the same. In\nall experiments, predictors included exposure type\n(unambiguous exposures to subject/non-subject or\ngoal/source), reference question type (whether the\nreference question is asked about the subject/non-\nsubject or goal/source), and the interaction ef-\nfect between them. Experiment 2 included verb\ntype, as well as its two-way and three-way interac-\ntion effects with the other two predictors as addi-\n9497\nExposure Subject Object\n100%\n38%\n89%\n0%\n0%\n25%\n50%\n75%\n100%\nSubj Q Obj Q\nReference Question Condition\n% Subject Responses\nExp. 1a − LLMa\n96.6%\n71.5%\n78.1%\n60.1%\n0%\n25%\n50%\n75%\n100%\nSubj Q Obj Q\nReference Question Condition\n% Subject Responses\nExp. 1a − Johnson & Arnold (2022) b\nFigure 3: Subject responses of LLM (left) and human participants (right)in Experiment 1a, showing the\npercentage of subject responses for each type of reference question (Subj Q: subject reference question; Obj Q:\nobject reference question), grouped by exposure type (subject exposure: light blue; object exposure: dark blue).\ntional predictors. Given our small dataset, the re-\nsults were analyzed using Bayesian mixed-effects\nBernoulli logistic regression models in the R pack-\nage brms (Bürkner, 2017) instead of a frequentist\nmodel. We report a Bayesian equivalent p-value\n(p_MAP) computed with the R package bayestestR\n(Makowski et al. , 2019) to offer a straightforward\ninterpretation of the results. Details of models are\nprovided in Appendix 1.\n4 Results\n4.1 FLAN-UL2\nWe found that FLAN-UL2 was not capable of gen-\nerating meaningful output for analysis under this\ndesign. First, FLAN-UL2 showed a much higher\nfalse rate in answering content questions versus\nInstructGPT: while InstructGPT replied 100% cor-\nrectly to these questions, FLAN-UL2 replied to\nonly 57% correctly. Second, for the ambiguous\ntarget items, FLAN-UL2 answered ‘yes’ 100% of\nthe time, indicating an extremely strong bias to-\nwards simply answering ‘yes’ and producing no\nmeaningful variation. By contrast, InstructGPT an-\nswered ‘yes’ to target questions 68% of the time,\nsuggesting some amount of ‘yes’ bias but to a much\nweaker degree. From these ﬁndings we concluded\nthat FLAN-UL2 did not produce sufﬁciently clean\noutputs for analysis; therefore, in the following sec-\ntions, we will focus on results from InstructGPT.\n4.2 Experiment 1a\nExperiment 1a asks whether LLMs are sensitive\nto the frequency of referential patterns when sub-\nject referents are preferred. Figure 3 compares the\nsubject responses of our results with Johnson and\nArnold (2022)’s results. In both LLM and human\ndata, we saw fewer subject responses in object ex-\nposure than in subject exposure. This is conﬁrmed\nby the main effect of exposure type ( p_MAP < .001)\nrevealed in the statistical analyses. There was also\na main effect of response question type ( p_MAP\n= .009). As seen from the ﬁgure, there were more\nsubject responses in the subject than object ref-\nerence questions in both human and LLM data.\nThis is because InstructGPT did answer ‘yes’ more\nfrequently in general, which led to more subject\ninterpretations when the question asked about the\nsubject (where answering ‘yes’ indicates a subject\ninterpretation), and fewer subject interpretations\nwhen the question was asked about the non-subject\n(where answering ‘yes’ indicates a non-subject in-\nterpretation).\nHowever, we did not ﬁnd any interaction ef-\nfect between exposure type and reference question\ntype. While Johnson and Arnold (2022) found that\nexposure type has a signiﬁcant effect for subject-\nreferent questions but only a marginal effect for\nthe nonsubject-referent questions, the exposure ef-\nfect was signiﬁcant for both question types in our\n9498\nexposure subject object\n100.0% 100.0%\n50.0%\n86.1%\n5.6%\n19.4%\n0.0%\ngs sg\nsubj Q nonsubj Q subj Q nonsubj Q\n0%\n25%\n50%\n75%\n100%\nQuestion type\n% Subject responses\nExp 2a − LLMa\n94.3%\n85.1%\n90.6%\n74.3%\n83.3%\n73.0%71.3%\n46.2%\ngs sg\nsubj Q nonsubj Q subj Q nonsubj Q\n0%\n25%\n50%\n75%\n100%\nQuestion type\n% Subject responses\nExp 2a − Johnson & Arnold (2022)b\nFigure 4: Subject responses of LLM (left)and human participants (right)in Experiment 2a. The proportion of\nsubject responses is plotted against question type faceting by verb type, comparing goal-subject verbs (gs) like\n\"receive,\" where the subject is the goal referent) to subject-goal verbs (sg) like \"send,\" where the subject is the\nsource referent.\nresults. InstructGPT is sensitive to exposure type\neven for nonsubject-referent questions, as reﬂected\nin Figure 3: there were no subject responses when\nnon-subject reference questions were asked under\nthe non-subject exposure condition. This suggests\nthat LLM may even be more sensitive to the ex-\nposure effect than humans. Overall, Experiment 1\nshows that LLM can indeed learn from recent ex-\nposure to syntactically-oriented referential patterns,\nin a relatively more non-subject-biased way.\n4.3 Experiment 2a\nWe ask in Experiment 2a if the sensitivity to sub-\nject reference patterns observed in Experiment 1a\npersists independent of semantic variability in the\nlocal context. Figure 4 illustrates the responses of\nLLM and human participants by exposure, refer-\nence question, and verb type. The behavior of In-\nstructGPT is in line with what Johnson and Arnold\n(2022) found in human participants for Experiment\n2a: as in Experiment 1, statistical analyses revealed\na signiﬁcant main effect of referent question type\n(p_MAP = .034), despite a marginally signiﬁcant\neffect of exposure type ( p_MAP = .085). Instruct-\nGPT did understand the pronoun as subject ref-\nerents more after subject exposure and said ‘yes’\nmore often such that there were more subject in-\nterpretations when the critical question was asked\nabout the subject. InstructGPT also showed a goal\nbias: there was a main effect of verb type ( p_MAP\n= .001), such that it referred to the subject more\nwith gs verbs (where the subject is the goal refer-\nent) than sg verbs (where the subject is the source\nreferent). We observed no interaction effect.\nIn spite of these similarities, we still observe\ndifferences between LLM and human participants.\nNotice in Figure 4 that there is a larger difference\nbetween gs and sg verbs in InstructGPT than in\nhuman participants while keeping other conditions\nconstant, suggesting that LLMs may have a larger\ngoal bias than humans.\n4.4 Experiment 2b\nExperiment 2b examines whether InstructGPT is\nstill sensitive to exposure to referential patterns that\nexhibit consistent preferences for a source or goal\nreferent. Figure 5 compares the results from In-\nstructGPT and human participants. Whereas John-\nson and Arnold (2022) reported signiﬁcant main\neffects from exposure type, reference question type,\nand verb type, as well as a marginal interaction ef-\nfect between verb type and exposure, we did not\nﬁnd any effect from these predictors, but only an\ninteraction effect between verb type and question\ntype ( p_MAP < .001). We further examined the\neffect of question type and exposure type for each\nverb type, but no signiﬁcant effect was found for ei-\nther predictor. These results suggest that exposure\n9499\nexposure goal source\n100% 100%\n68%\n100%\n49%\n33%\ngs sg\ngoal Q source Q goal Q source Q\n0%\n25%\n50%\n75%\n100%\nQuestion type\n% Goal responses\nExp 2b − LLM\n94.35%\n82.49%\n76.84%\n55.37%\n37.85% 37.85%\n12.99%\ngs sg\ngoal Q source Q goal Q source Q\n0%\n25%\n50%\n75%\n100%\nQuestion type\n% Goal responses\nExp 2b − Johnson & Arnold (2022)\nFigure 5: Goal responses of LLM (left)and human participants (right)in Experiment 2b.\ndid not necessarily change InstructGPT’s behav-\niors: for goal questions, both the goal and source\nexposure conditions led to 100% goal responses.\nReference question type also did not affect Instruct-\nGPT’s responses, because it almost always inter-\nprets the pronoun as the goal referent under subject\nexposure. The lack of any signiﬁcant effects, and\nindeed our observed universal goal interpretations\nto goal-focused reference questions, suggest that\nInstructGPT displays an extreme goal bias in pro-\nnoun interpretation.\n5 Discussion\nThe present study examined whether and to what\nextent LLMs display adaptation for pronominal in-\nterpretation after exposure to referential patterns\nby replicating three psycholinguistic experiments.\nExposed to the same stimuli and study design, and\nanalyzed with the same statistical procedures as\nthe source experiments, we asked whether LLMs\nshow human-like behaviors and compared the per-\nformance of LLMs-simulated participants with hu-\nman participants.\nWe ﬁrstly found a difference between the capac-\nity of contemporary models to replicate human psy-\ncholinguistic experimental designs in an ICL frame-\nwork. While InstructGPT was able to correctly\nanswer all the presented comprehension-check con-\ntent questions, FLAN-UL2 was not. Both mod-\nels displayed at least some bias towards answer-\ning ‘yes’ in ambiguous cases, but in the case of\nFLAN-UL2 this bias resulted in 100% ‘yes’ an-\nswers, rendering meaningful variation impossible\nto ascertain. This could be a result of structural dif-\nferences between the models (decoder-only in the\ncase of InstructGPT, encoder-decoder in the case of\nFLAN-UL2), or perhaps more likely a question of\nsimple model size (175B for InstructGPT vs. 20B\nfor FLAN-UL2).\nExperiments 1a and 2a examined whether LLMs\nadapt their syntactic bias from recent exposure to\nreferential patterns like humans, without and with\nthe presence of possibly confounding semantic goal\nbias. We found that LLM’s referential biases are\nindeed sensitive to such exposure in both experi-\nments. In addition, LLM did exhibit a goal bias in\nExperiment 2a, in line with previous studies which\nargue for LLM’s ability to exhibit human-like se-\nmantic bias in pronoun interpretation (e.g., Davis\nand van Schijndel 2020 ).\nExperiment 2b examined whether LLM can\nadapt and vary their semantic bias from exposures.\nIn this context, in contrast with the previous two\nexperiments, exposure type did not affect LLM\nbehavior at all. This raises the question of why\nLLM would be sensitive to exposure to only refer-\nential patterns that exhibit consistent syntactic bias\nbut not semantic bias. An immediate and intuitive\nanswer would be that LLM is unable to fully repre-\nsent semantic knowledge in referential processing.\nHowever, given that LLM did display a human-like\ngoal bias in pronoun interpretation independent of\nexposure, this explanation seems unlikely.\n9500\nWe suggest that our work provides evidence that\nLLM only partially represents the semantic knowl-\nedge involved in referential processing for two rea-\nsons.\nFirst, adapting referential biases on the basis of\nexposure in the local context may require repre-\nsentations of higher-level knowledge than merely\nexhibiting a bias towards certain referents. While\nthe latter may only require knowledge of which\nfeatures associated with a referent are more fre-\nquent or likely in general, adaption requires a sus-\ntained awareness of referential pattern frequency as\nit changes in the local discourse context. Though\nrepresenting knowledge about semantic relations\nwas observed as early as the analogical reasoning\ntask in Word2Vec ( Mikolov et al. , 2013), human-\nlike extraction of abstract information like a persis-\ntent discourse state is more challenging. The model\nmay only be able to identify thematic roles (source\nor goal) of a referent and associate them with pro-\nnoun interpretation, but not to identify a consistent\npattern of thematic roles across a discourse. If so,\nthis would explain the strong goal bias we observed\nin Experiment 2.\nSecond, it is possible that LLM has an extremely\nstrong goal bias that masks the inﬂuence of expo-\nsures. If so, this suggests that LLM over-represents\nthe semantic knowledge encoded in pronoun inter-\npretation.\nIn either case, our results suggest that LLM\ndo not encode semantic knowledge in a fully\nhuman-like way, even though they do demonstrate\nsome human-like capacities for semantic under-\nstanding. Although we believe this gap can be\nmitigated via instruction ﬁne-tuning or chain-of-\nthought prompting (\nWei et al. , 2022), these re-\nsults still suggest we should consider incorpo-\nrating semantically-informed objectives into self-\nsupervised pre-training to a greater extent.\n6 Conclusions and Future Work\nBy replicating a series of psycholinguistic experi-\nments as closely as possible using in-context learn-\ning, this paper pioneered whether LLMs would\nadapt pronominal interpretation behaviors in a\nhuman-like way given exposure to referential pat-\nterns in the local discourse context. Our work sug-\ngests paths forward for replicating psycholinguistic\nexperiments in a more faithful way that allows for\ncomparisons between human and LLMs’ behav-\niors.\nLimitations\nSeveral main limitations exist in our study in its\ncurrent form. First, our reported results only sim-\nulated experimental participants by manipulating\nthe temperature hyperparameter. We compared this\napproach with natural language prompting for Ex-\nperiment 1, but that prompting did not increase\n\"participant\" diversity, so it was abandoned. More-\nover, approaches for simulating psycholinguistic\nexperimental \"participants\" could go far beyond\nwhat was tried here; our prompting method was\nrelatively limited, and more detailed prompting\ncould be included in future experimental simula-\ntions. Second, making a direct comparison with\nactual psycholinguistic experiments might not be\nthe only method to investigate LLMs’ discourse\ncapacity. A comprehensive list of discourse prob-\ning tasks might play a similar role despite a differ-\nent way ( Koto et al. , 2021). Third, this study is\nstrictly behavioral: limited by both computational\nresources and obscure mechanisms of in-context\nlearning, we do not dive into models’ internal rep-\nresentations in our analyses.\nAcknowledgement\nWe are very grateful to Jennifer Arnold for shar-\ning the stimuli and design of Johnson and Arnold\n(2022) with us.\nReferences\nGati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.\n2022. Using large language models to simulate mul-\ntiple humans .\nMira Ariel. 1990. Accessing noun-phrase antecedents .\nRoutledge.\nJennifer E Arnold. 1998. Reference form and discourse\npatterns. Stanford University.\nJennifer E Arnold. 2001. The effect of thematic roles on\npronoun use and frequency of reference continuation.\nDiscourse processes , 31(2):137–162.\nJennifer E Arnold, Iris M Strangmann, Heeju Hwang,\nSandra Zerkle, and Rebecca Nappa. 2018. Linguistic\nexperience affects pronoun interpretation. Journal of\nMemory and Language , 102:41–54.\nDale J Barr, Roger Levy, Christoph Scheepers, and\nHarry J Tily. 2013. Random effects structure for\nconﬁrmatory hypothesis testing: Keep it maximal.\nJournal of memory and language , 68(3):255–278.\n9501\nHolly P Branigan, Martin J Pickering, and Janet F\nMcLean. 2005. Priming prepositional-phrase attach-\nment during comprehension. Journal of Experimen-\ntal Psychology: Learning, Memory, and Cognition ,\n31(3):468.\nSusan E Brennan. 1995. Centering attention in\ndiscourse. Language and Cognitive processes ,\n10(2):137–167.\nPaul-Christian Bürkner. 2017. brms: An r package for\nbayesian multilevel models using stan. Journal of\nstatistical software , 80:1–28.\nFranklin Chang, Gary S Dell, Kathryn Bock, and\nZenzi M Grifﬁn. 2000. Structural priming as im-\nplicit learning: A comparison of models of sentence\nproduction. Journal of psycholinguistic research ,\n29(2):217–230.\nForrest Davis and Marten van Schijndel. 2020. Dis-\ncourse structure interacts with reference but not\nsyntax in neural language models. arXiv preprint\narXiv:2010.04887.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural language models as psycholinguistic subjects:\nRepresentations of syntactic state . In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers) , pages 32–42, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless green\nrecurrent networks dream hierarchically . In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1195–1205, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels\n. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , pages\n1725–1744, Online. Association for Computational\nLinguistics.\nElyce D Johnson and Jennifer E Arnold. 2022. The fre-\nquency of referential patterns guides pronoun com-\nprehension. Journal of Experimental Psychology:\nLearning, Memory, and Cognition .\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021.\nDiscourse probing of pretrained language models .\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 3849–3864, Online. Association for Computa-\ntional Linguistics.\nDominique Makowski, Mattan S Ben-Shachar, and\nDaniel Lüdecke. 2019. bayestestr: Describing ef-\nfects and their uncertainty, existence and signiﬁcance\nwithin the bayesian framework. Journal of Open\nSource Software , 4(40):1541.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositionality .\nIn Advances in Neural Information Processing Sys-\ntems, volume 26. Curran Associates, Inc.\nKanishka Misra, Allyson Ettinger, and Julia Rayz. 2020.\nExploring BERT’s sensitivity to lexical cues using\ntests from semantic priming . In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020 ,\npages 4625–4635, Online. Association for Computa-\ntional Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback .\nJenny R Saffran, Richard N Aslin, and Elissa L Newport.\n1996a. Statistical learning by 8-month-old infants.\nScience, 274(5294):1926–1928.\nJenny R Saffran, Elissa L Newport, and Richard N Aslin.\n1996b. Word segmentation: The role of distributional\ncues. Journal of memory and language , 35(4):606–\n621.\nSebastian Schuster and Judith Degen. 2020. I know\nwhat you’re probably going to say: Listener adapta-\ntion to variable use of uncertainty expressions. Cog-\nnition, 203:104285.\nArabella Sinclair, Jaap Jumelet, Willem Zuidema, and\nRaquel Fernández. 2022. Structural Persistence in\nLanguage Models: Priming as a Window into Ab-\nstract Language Representations . Transactions of the\nAssociation for Computational Linguistics , 10:1031–\n1050.\nIonut-Teodor Sorodoc, Kristina Gulordava, and Gemma\nBoleda. 2020. Probing for referential information in\nlanguage models . In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 4177–4189, Online. Association for\nComputational Linguistics.\n9502\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier\nGarcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Siamak Shakeri, Dara Bahri, Tal Schuster,\nHuaixiu Steven Zheng, Denny Zhou, Neil Houlsby,\nand Donald Metzler. 2023. Ul2: Unifying language\nlearning paradigms .\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline . CoRR,\nabs/1905.05950.\nShiva Upadhye, Leon Bergen, and Andrew Kehler. 2020.\nPredicting reference: What do language models learn\nabout discourse models? In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) , pages 977–982, Online.\nAssociation for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models .\nA Statistical Information\nFor each experiment, we analyzed referent choice\n(Subject = 1, Non-subject = 0) using a mixed-\neffects Bernoulli regression model from the R pack-\nage brms (Bürkner, 2017), with the maximal ran-\ndom structure justiﬁed by design ( Barr et al. , 2013).\nPredictors are coded in the same way as in Johnson\nand Arnold (2022). All models were speciﬁed with\na weakly informative prior using the Cauchy dis-\ntribution with center 0 and scale 2.5. Models were\nﬁtted using six chains, each with 4,000 iterations\nof which the ﬁrst 1,000 are warmup to calibrate the\nsampler, resulting in 18,000 posterior examples.\nThe model for Experiment 1a included question\ntype (QtypeC, sum-coded: Subject = 0.5, Non-\nsubject = -0.5) and exposure type (PC, effects-\ncoded: Subject-biased = 0.51, Object-biased = -\n0.49) as ﬁxed predictors, random intercepts for\nparticipants and items, random slopes of question\ntype, exposure type and their interaction for items,\nand a random slope of question type for the partici-\npant. Exposure type was not included as a random\nslope for participants because the condition does\nnot vary within participants.\nbrm (Rc ~ QtypeC*PC +\n(1+QtypeC*PC|ID)+\n(1+QtypeC|Subject),\ndata=(e1a,Exposure!=\"None\"),\nfamily=\"bernoulli\",\nchains=6,\niter=4000,\nwarmup=1000,\ncontrol =\nlist(adapt_delta = 0.95),\nprior=\nc(set_prior (\"cauchy(0,2.5)\")))\nThe model for Experiment 2a included question\ntype (QtypeC, sum-coded: Subject = 0.5, Non-\nsubject = -0.5), exposure type (PC, sum-coded:\nSubject-biased = 0.5, Object-biased = -0.5), and\nverb type (Vc, sum-coded: gs-verb = 0.5, sg-verb\n= -0.5) as ﬁxed predictors, random intercepts for\nparticipants and items, random slopes of question\ntype, exposure type and their interaction for items,\nand a random slope of question type for the par-\nticipant. As in Experiment 1a, exposure type was\nnot included as a random slope for participants\nbecause the condition does not vary within partic-\nipants. Similarly, verb bias was not included as a\nrandom slope for items here because it does not\nvary within items.\nbrm (Rc ~ QtypeC*PC*Vc +\n(1+PC*Qtypec|Item)+\n(1+QtypeC*Vc|Subject),\ndata=e2a,\nfamily=\"bernoulli\", chains=6,\niter=4000, warmup=1000,\ncontrol = list(adapt_delta = 0.98),\ncores = 6,\nprior=\nc(set_prior (\"cauchy(0,2.5)\")))\nThe model for Experiment 2b included question\ntype (QtypeC, sum-coded: Goal = 0.5, Source =\n-0.5), exposure type (PC, sum-coded: Goal-biased\n= 0.5, Source-biased = -0.5), and verb type (Vc,\nsum-coded: gs-verb = 0.5, sg-verb = -0.5) as ﬁxed\npredictors. The random effect structure was the\nsame as that of Experiment 2a.\nbrm (Rc ~ PC*Vc*QtypeC\n+(1+PC*QtypeC|Item)\n+(1+Vc*QtypeC|Subject),\ndata=e2b,\nfamily=\"bernoulli\",\nchains=6,\niter=4000,\nwarmup=1000,\ncontrol =\nlist(adapt_delta = 0.999),\ncores = 6,\nprior=\nc(set_prior (\"cauchy(0,2.5)\")))\n9503\nB Temperature vs. Prompt\nOur prompt-based simulation of multiple partic-\nipants embedded names, countries, preﬁxes, and\ngenders into a carrier sentence: {Preﬁx + Name} is\na native English speaker living in {Country}. {Gen-\nder} is asked in a psycholinguistic experiment to\nanswer the following questions. For example, Mr.\nSmith is a native English speaker living in Eng-\nland. He is asked in a psycholinguistic experiment\nto answer the following questions.\nSpeciﬁcally, we calculated the variance and ran\nthe Levene’s test for any signiﬁcant difference be-\ntween humans and InstructGPT in each experiment.\nIn experiment 1, the human responses’ variance is\n0.055, the temperature-based responses’ variance\nis 0.024, and the prompt-based responses’ vari-\nance is 0.017. Human responses were signiﬁcantly\nhigher than both (Temperature-based: p = .049 ;\nPrompt-based: p = .007 ). Yet, temperature-based\nwere not signiﬁcantly higher than prompt-based.\nDue to the limitation of API pricing, we only ran\ntemperature-based in the following experiments. In\nexperiment 2a, the human responses’ variance is\n0.034 and the temperature-based responses’ vari-\nance is 0.043. Yet, Levene’s test did not reveal any\nsigniﬁcant difference. In Experiment 2b, the human\nresponses’ variance is 0.020 and the temperature-\nbased responses’ variance is 0.008 ( p < .001 ).\nWe used different techniques to introduce ran-\ndomness and include more experimental data in\nour experiments. We realized these were not well-\ndesigned prompts to elicit different linguistic back-\ngrounds. Given the lack of investigation on sim-\nulating multiple participants in psycholinguistics\nstudies, we recognize this as a future direction of\npossible work.\n9504\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLast section without the number\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe introduction is in the ﬁrst section.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nExperiments were described in section 3.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nIn the method section, we described our parameters.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9505\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. We did not do a hyperparameter search.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nWe provide detailed results in the results section.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe provide existing packages’ information in the methods section.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9506",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.5857795476913452
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5781570076942444
    },
    {
      "name": "Priming (agriculture)",
      "score": 0.578105092048645
    },
    {
      "name": "Pronoun",
      "score": 0.5643409490585327
    },
    {
      "name": "Computer science",
      "score": 0.5431600213050842
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.4885793626308441
    },
    {
      "name": "Contrast (vision)",
      "score": 0.47505638003349304
    },
    {
      "name": "Psychology",
      "score": 0.4544726312160492
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.452911376953125
    },
    {
      "name": "Cognitive psychology",
      "score": 0.34490227699279785
    },
    {
      "name": "Natural language processing",
      "score": 0.3278498649597168
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2837457060813904
    },
    {
      "name": "History",
      "score": 0.08322125673294067
    },
    {
      "name": "Biology",
      "score": 0.06480148434638977
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Germination",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}