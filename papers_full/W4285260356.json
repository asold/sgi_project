{
  "title": "Benchmarking Post-Hoc Interpretability Approaches for Transformer-based Misogyny Detection",
  "url": "https://openalex.org/W4285260356",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2049312926",
      "name": "Giuseppe Attanasio",
      "affiliations": [
        "Polytechnic University of Turin",
        "Bocconi University"
      ]
    },
    {
      "id": "https://openalex.org/A2203505503",
      "name": "Debora Nozza",
      "affiliations": [
        "Bocconi University"
      ]
    },
    {
      "id": "https://openalex.org/A2608969421",
      "name": "Eliana Pastor",
      "affiliations": [
        "Polytechnic University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A310222905",
      "name": "Dirk Hovy",
      "affiliations": [
        "Bocconi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3120706522",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W3023690688",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W3029801575",
    "https://openalex.org/W3114738563",
    "https://openalex.org/W3101155149",
    "https://openalex.org/W3034475796",
    "https://openalex.org/W3174490235",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W3172794097",
    "https://openalex.org/W3173813266",
    "https://openalex.org/W2493343568",
    "https://openalex.org/W2980350050",
    "https://openalex.org/W3099954305",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W3212603771",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W3173173856",
    "https://openalex.org/W3105090453",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2962816513",
    "https://openalex.org/W3117696238",
    "https://openalex.org/W4287026833",
    "https://openalex.org/W2806872289",
    "https://openalex.org/W3085380432",
    "https://openalex.org/W4285183888",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3138819813",
    "https://openalex.org/W3154048149",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W3029245643",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2996507500",
    "https://openalex.org/W3168175209",
    "https://openalex.org/W4285152678",
    "https://openalex.org/W3114511942",
    "https://openalex.org/W2955358972",
    "https://openalex.org/W3115397159",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W3031696024",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W3174150157",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W3174882277",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W4287718305",
    "https://openalex.org/W3035371891",
    "https://openalex.org/W2804878416",
    "https://openalex.org/W2953646920",
    "https://openalex.org/W3176614973",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W3034282334",
    "https://openalex.org/W3116662254",
    "https://openalex.org/W2962772482"
  ],
  "abstract": "Transformer-based Natural Language Processing models have become the standard for hate speech detection. However, the unconscious use of these techniques for such a critical task comes with negative consequences. Various works have demonstrated that hate speech classifiers are biased. These findings have prompted efforts to explain classifiers, mainly using attribution methods. In this paper, we provide the first benchmark study of interpretability approaches for hate speech detection. We cover four post-hoc token attribution approaches to explain the predictions of Transformer-based misogyny classifiers in English and Italian. Further, we compare generated attributions to attention analysis. We find that only two algorithms provide faithful explanations aligned with human expectations. Gradient-based methods and attention, however, show inconsistent outputs, making their value for explanations questionable for hate speech detection tasks.",
  "full_text": "Proceedings of NLP Power! The First Workshop on Efﬁcient Benchmarking in NLP, pages 100 - 112\nMay 26, 2022 ©2022 Association for Computational Linguistics\nBenchmarking Post-Hoc Interpretability Approaches for\nTransformer-based Misogyny Detection\nGiuseppe Attanasio1,2, Debora Nozza1, Eliana Pastor2, Dirk Hovy1\n1Bocconi University, Milan, Italy\n2Politecnico di Torino, Turin, Italy\n{giuseppe.attanasio3,debora.nozza,dirk.hovy}@unibocconi.it,\neliana.pastor@polito.it\nAbstract\nWarning: This paper contains examples of lan-\nguage that some people may find offensive.\nTransformer-based Natural Language Process-\ning models have become the standard for hate\nspeech detection. However, the unconscious\nuse of these techniques for such a critical\ntask comes with negative consequences. Vari-\nous works have demonstrated that hate speech\nclassifiers are biased. These findings have\nprompted efforts to explain classifiers, mainly\nusing attribution methods. In this paper, we\nprovide the first benchmark study of inter-\npretability approaches for hate speech detec-\ntion. We cover four post-hoc token attribu-\ntion approaches to explain the predictions of\nTransformer-based misogyny classifiers in En-\nglish and Italian. Further, we compare gen-\nerated attributions to attention analysis. We\nfind that only two algorithms provide faithful\nexplanations aligned with human expectations.\nGradient-based methods and attention, how-\never, show inconsistent outputs, making their\nvalue for explanations questionable for hate\nspeech detection tasks.\n1 Introduction\nThe advent of social media has proliferated hate-\nful content online – with severe consequences for\nattacked users even in real life. Women are often\nattacked online. A study by Data & Society 1 of\nwomen between 15 to 29 years showed that 41%\nself-censored to avoid online harassment. Of those,\n21% stopped using social media, 13% stopped go-\ning online, and 4% stopped using their mobile\nphone altogether. These numbers demonstrate the\nneed for automatic misogyny detection systems for\nmoderation purposes.\n1https://www.datasociety.net/pubs/oh/\nOnline_Harassment_2016.pdf\nYou are a smart woman\n∆P (10−2) -0.1 1.1 -0.0 0.8 -47.6\nG 0.11 0.10 0.09 0.25 0.27\nIG -0.17 0.18 -0.09 -0.35 -0.20\nSHAP 0.00 -0.14 -0.04 -0.03 0.78\nSOC 0.07 -0.13 0.03 0.03 0.52\nTable 1: Explanations generated by benchmarked meth-\nods. A fine-tuned BERT wrongly classifies the text as\nmisogynous. Darker colors indicate higher importance.\nVarious Natural Language Processing (NLP)\nmodels have been proposed to detect and mitigate\nmisogynous content (Basile et al., 2019; Indurthi\net al., 2019; Lees et al., 2020; Fersini et al., 2020a;\nSafi Samghabadi et al., 2020; Attanasio and Pastor,\n2020; Guest et al., 2021; Attanasio et al., 2022).\nHowever, several papers already demonstrated that\nhate speech detection models suffer from unin-\ntended bias, resulting in harmful predictions for\nprotected categories (e.g., women). Table 1 (top\nrow) reports a very simple sentence that a state-\nof-the-art NLP model misclassifies as misogynous\ncontent.\nThis issue shows the need to understand the ra-\ntionale behind a given prediction. A mature litera-\nture on model interpretability with applications to\nNLP-specific approaches exists (Ross et al., 2021;\nSanyal and Ren, 2021; Rajani et al., 2019, inter-\nalia).2 As explanations become part of legal regu-\nlations (Goodman and Flaxman, 2017), a growing\nbody of work has focused on the evaluation of\nexplanation approaches (Nguyen, 2018; Hase and\nBansal, 2020; Nguyen and Martínez, 2020; Jacovi\nand Goldberg, 2020, inter-alia). However, little\nguidance on which interpretability method suits\n2We refer the reader to Danilevsky et al. (2020) and Mad-\nsen et al. (2021) for a recent, thorough perspective on explana-\ntion methods for NLP models.\n100\nbest to the sensible context of misogyny identifi-\ncation has been given. For instance, some expla-\nnations in Table 1 hint to which token is wrongly\ndriving the classification and even highlight a po-\ntential bias of the model. But not all of them.\nWe bridge this gap. We benchmark interpretabil-\nity approaches to explain state-of-the-art Trans-\nformer classifiers on the task of automatic misog-\nyny identification. We cover two benchmark Twit-\nter datasets for misogyny detection in English and\nItalian (Fersini et al., 2018, 2020b). We focus on\nsingle-instance, post-hoc input attribution methods\nto measure the importance of each token for pre-\ndicting the instance label. Our benchmark suite\ncomprises gradient-based methods (Gradients (Si-\nmonyan et al., 2014) and Integrated Gradients (Sun-\ndararajan et al., 2017)), Shapley values-based meth-\nods (SHAP (Lundberg and Lee, 2017)), and in-\nput occlusion (Sampling-And-Occlusion (Jin et al.,\n2020)). We evaluate explanations in terms of plausi-\nbility and faithfulness (Jacovi and Goldberg, 2020).\nTable 1 reports an example of token-wise contribu-\ntion computed with these methods. Furthermore,\nwe study attention-based visualizations and com-\npare them to token attribution methods searching\nfor any correlation. To our knowledge, this is\nthe first benchmarking study of feature attribution\nmethods used to explain Transformer-based misog-\nyny classifiers.\nOur results show that SHAP and Sampling-And-\nOcclusion provide plausible and faithful explana-\ntions and are consequently recommended for ex-\nplaining misogyny classifiers’ outputs. We also\nfind that, despite their popularity, gradient- and\nattention-based methods do not provide faithful ex-\nplanations. Outputs of gradient-based explanation\nmethods are inconsistent, while attention does not\nprovide any useful insights for the classification\ntask.\nContributions We benchmark four post-hoc ex-\nplanation methods on two misogyny identification\ndatasets across two languages, English and Ital-\nian. We evaluate explanations in terms of plausi-\nbility and faithfulness. We demonstrate that not\nevery token attribution method provides reliable\ninsights and that attention cannot serve as explana-\ntion. Code is available at https://github.c\nom/MilaNLProc/benchmarking-xai-m\nisogyny.\n2 Benchmarking suite\nIn the following, we describe the scope (§2.1) of\nour benchmarking study, the included methods\n(§2.2), and the evaluation criteria (§2.2).\n2.1 Scope\nWe consider local explanation methods (Lipton,\n2018; Guidotti et al., 2019). Given a classification\nmodel, a data point, and a target class, these meth-\nods explain the probability assigned to the class by\nthe model. Global explanations provide model- or\nclass-wise explanations and are hence out of the\nscope of this work.\nAmong local explanation methods, we focus on\npost-hoc interpretability, i.e., we explain classifi-\ncation models that have already been trained. We\nleave out inherently interpretable models (Rudin,\n2019) as they do not find widespread use in NLP-\ndriven practical applications.\nWe restrict our study to input attribution meth-\nods. In Transformer-based language models, inputs\ntypically correspond to the tokens’ input embed-\ndings (Madsen et al., 2021). We, therefore, refer to\ntoken attribution methods to generate a contribu-\ntion score for each input token (or word, resulting\nby some aggregation of sub-word token contribu-\ntions).\n2.2 Methods\nWe benchmark three families of input token at-\ntribution methods. First, we derive token contri-\nbution using gradient attribution. These methods\ncompute the gradient of the output with respect to\neach of the inputs. We compute simple gradient\n(G) (Simonyan et al., 2014) and integrated gradi-\nents (IG) (Sundararajan et al., 2017). Then, we\nattribute inputs using approximated Shapley val-\nues (SHAP) (Lundberg and Lee, 2017). Finally,\nfollowing the literature on input perturbation via\nocclusion, we impute input contributions using\nSampling-And-Occlusion (SOC) (Jin et al., 2020).\nSee appendix A.2 for all implementation details.\nAttention There is an open debate of whether\nattention is explanation or not (Jain and Wallace,\n2019; Wiegreffe and Pinter, 2019; Bastings and\nFilippova, 2020). Our benchmarking study pro-\nvides a perfect test-bed to understand if attention\naligns with attribution methods. We compare stan-\ndard self-attention with effective attention (Brunner\net al., 2020; Sun and Marasovi ´c, 2021). Further,\nwe measure attribution between input tokens and\n101\nDataset # Train # Test Hate % F1\nAMI-EN 4,000 1,000 45% 68.78\nAMI-IT 5,000 1,000 47% 79.79\nTable 2: Summary of datasets in terms of the number\nof training, validation and test tweets, percentage of\nhateful records within the training split, and F1-score of\nBERT models on test sets.\nhidden representations using Hidden Token Attri-\nbution (HTA) (Brunner et al., 2020).\n2.3 Evaluation criteria\nWe use plausibility and faithfulness as evaluation\ncriteria (Jacovi and Goldberg, 2020). A “plausi-\nble” explanation should align with human beliefs.\nIn our context, the provided explanation artifacts\nshould convince humans that highlighted words are\nresponsible for either misogynous speech or not.3\nA “faithful” explanation is a proxy for the true “rea-\nsoning” of the model. Gradient attributions are\ncommonly considered faithful explanations as gra-\ndients provide a direct, mathematical measure of\nhow variations in the input influences output. For\nthe remaining attribution approaches, we measure\nfaithfulness under the linearity assumption (Jacovi\nand Goldberg, 2020), i.e., the impact of certain\nparts of the input is independent of the rest. In our\ncase, independent units correspond to input tokens.\nFollowing related work (Jacovi et al., 2018; Feng\net al., 2018; Serrano and Smith, 2019, inter-alia),\nwe evaluate faithfulness by erasing input tokens and\nmeasuring the variation on the model prediction.\nIdeally, faithful interpretations highlight tokens that\nchange the prediction the most.\n2.4 Data\nAutomatic misogyny identification is the binary\nclassification task to predict whether a text is\nmisogynous or not. 4 We focus on two recently-\nreleased datasets for misogynous content identifi-\ncation in English and Italian, released as part of the\nAutomatic Misogyny Identification (AMI) shared\ntasks (Fersini et al., 2018, 2020b). Both datasets\nhave been collected via keyword-based search on\nTwitter. Table 2 reports the dataset statistics.\n3In this study, the human expectation corresponds to the\nauthors’.\n4Characterizing misogyny is a much harder task, possibly\nmodeling complex factors such as shaming, objectification, or\nmore. Here, we simplify the task to focus on benchmarking\ninterpretability.\n3 Experimental setup\nAmong the Transformer-based models, we focus\non BERT (Devlin et al., 2019) due to its widespread\nusage. We fine-tuned pre-trained BERT-based mod-\nels on the AMI-EN and AMI-IT datasets. We report\nfull details on the training in appendix A.1. Table 2\nreports the macro-F1 performance of BERT models\non the test splits.\nWe explain BERT outputs on both tweets from\ntest sets5 and manually-generated data. On real\ndata, we address two questions: 1) Is it right for the\nright reason?, i.e., we assess if the model relies on\na plausible set of tokens; 2) What is the source of\nerror?, i.e., we aim to identify tokens that wrongly\ndrive the classification outcome. By explaining\nmanually-defined texts, we can probe for model\nbiases.\nTables 3-6 report token contributions computed\nwith benchmarked approaches (§2.2). We report\ncontributions for individual tokens.6 We define ta-\nble contents as follows. Separately by explanation\nmethod, we first generate raw contributions and\nthen L1-normalize the vector. Finally, we use a\nlinear color scale between solid blue (assigned for\ncontribution -1), white (contribution 0), and solid\nred (contribution 1). For all reported examples,\nwe explain the misogynous class. Hence, posi-\ntive contributions indicate tokens pushing towards\nthe misogynous class, while negative contributions\npush towards the non-misogynous one. Lastly, the\nsecond top row reports the variation on the probabil-\nity assigned by the model when the corresponding\ntoken is erased (∆P).\n4 Discussion\nError analysis Table 3 shows the explanations\nfor a tweet incorrectly predicted as misogynous.\nIG, SHAP, and SOC assign a negative contribu-\ntion to the word boy. This matches our expecta-\ntions since the target of the hateful comment is the\nmale gender. These explanations are thus plausi-\nble. Still, the tweet is classified as misogynous.\nThe tokens pu and ##ssy mainly drive the predic-\ntion to the misogynous class, as revealed by all\nexplainers (SHAP and SOC in a clearer way). Ex-\n5We rephrase and explain rephrased versions of tweets to\nprotect privacy.\n6While several work average sub-word contributions for\nout-of-vocabulary words, there is no general agreement on\nwhether this brings meaningful results. Indeed, an average\nwould assume a model that leverages tokens as a single unit,\nwhile there is no clear evidence of that.\n102\nYou pu ##ssy boy\n∆P (10−2) -0.3 -0.2 -35.6 0.8\nG 0.11 0.19 0.32 0.18\nIG 0.26 0.00 0.14 -0.60\nSHAP -0.03 0.52 0.28 -0.17\nSOC -0.01 0.03 0.51 -0.14\nTable 3: Example from AMI-EN test set, anonymyzed\ntext on first row. Ground truth: non misogynous.\nPrediction: misogynous (P = 0.78).\nplanations suggest the model is failing to assign\nthe proper importance to the targeted gender of the\nhateful comment. These plausible explanations are\nalso faithful. Removing the term boy increases the\nprobability of the misogynous class while omitting\ntokens pu and ##ssy decrease it.\nWe further analyze the term p*ssy and its role\nas a source of errors. Almost all tweets of the test\nset containing the term p*ssy are labeled by the\nmodel as misogynous. The false-positive rate on\nthis set of tweets is 0.93 compared to the 0.49 of the\noverall test set. Similar considerations apply to En-\nglish words typically associated with misogynous\ncontent as b*tch and wh*re.\nIs it right for the right reason? Table 4 shows\nthe explanation of a correctly predicted misogy-\nnous tweet. Gradient, SHAP, and SOC explana-\ntions assign a high positive contribution to slurs\n(b*tch, s*ck, and d*ck). These explanations align\nwith human expectations. However, not all slurs\nimpact the classification outcome. Explanations\non b*tch are faithful but they are not for s*ck and\nd*ck. Differently, IG does not highlight any token\nwith a positive contribution. This goes against ex-\npectations as the predicted class is misogynous and\ntherefore we cannot draw conclusions.\nUnintended bias We study explanations to\nsearch for errors caused by unintended bias,\na known phenomenon affecting models for\nmisogynous identification. A model suffering\nfrom unintended bias performs better (or worse)\nwhen texts mention specific identity terms (e.g.,\nwoman) (Dixon et al., 2018).\nTable 1 reports the non-misogynous text \"You\nare a smart woman\" incorrectly labeled as misog-\nynous. SHAP, SOC, and, to a lesser extent, Gra-\ndient explanations indicate the term woman as re-\nsponsible for the prediction. This result matches\nwith recent findings on the unintended bias of hate-\nful detection models (Nozza et al., 2019; Dixon\net al., 2018; Borkan et al., 2019) and therefore\nexplanations are plausible. Removing the term\nwoman causes a drop of 0.48 to the probability of\nthe misogynous class. This validates the insight\nprovided by the explanations. Similar to the previ-\nous examples, the explanation of IG is difficult to\ninterpret.\nTable 5 shows another example of unintended\nbias. The text “Ann is in the kitchen” is incorrectly\nlabeled as misogynous. Gradients, SHAP, and SOC\nassign the highest positive contribution to the (com-\nmonly) female name Ann. Interestingly, the second\nmost important word for Gradients and SHAP is\nkitchen, reflecting stereotypes learned by the clas-\nsification model (Fersini et al., 2018). These expla-\nnations are faithful: the model prediction drops by\na significant 0.40 and 0.24 when erasing the tokens\nAnn and kitchen, respectively. We substitute the\nname Ann with David, a common male name. We\nobserve that the prediction and the explanations\ndrastically change. The text is correctly assigned to\nthe non-misogynous class and IG, SHAP, and SOC\nassign a high negative contribution to the word\nDavid. The all-positive contributions of Gradients\ndo not provide useful insights.\nBias due to language-specific expressions Table\n6 (left) shows an example of incorrectly predicted\nmisogynous text in Italian: \"p*rca p*ttana che gran\npezzo di f*ga\" (\"holy sh*t what a nice piece of\n*ss\"). The expression \"p*rca p*ttana\" (literally pig\nsl*t) is a taboo interjection commonly used in the\nItalian language and does not imply misogynous\nspeech.\nThe interpretation of the gradient explanation is\nhard since all contributions are positive and asso-\nciated with the misogynous class. All explanation\nmethods assign a positive contribution to the word\nf*ga (*ss). SHAP, SOC, and, to a lesser extent\nIG, indicate that the main reason behind the non-\nmisogynous prediction is the term p*rca. The bias\nof the model towards this expression was firstly\nexposed in (Nozza, 2021) and it thus validates IG,\nSHAP, and SOC explanations as plausible. When\none of the two terms of the expression is removed,\nthe probability increases significantly. This sug-\ngests that explanations by IG, SHAP, and SOC are\nfaithful. Further, we inspect the behavior of expla-\nnation methods when we erase one of the terms.\nWe omit the word p*rca and we report its expla-\nnations on Table 6 (right). The text is correctly\nassigned to the misogynous class and the word\n103\ns*ck a d*ck and choke you b*tch\n∆P (10−2) -0.02 0.2 0.8 0.3 -0.1 0.03 -13.4\nG 0.10 0.08 0.14 0.07 0.08 0.10 0.25\nIG -0.14 -0.16 -0.08 -0.05 -0.20 -0.22 -0.16\nSHAP 0.24 -0.03 0.07 -0.05 0.05 -0.06 0.50\nSOC 0.20 -0.02 0.26 -0.02 0.07 0.00 0.29\nTable 4: Example from AMI-EN test set, anonymyzed text on first row. Ground truth: misogynous. Prediction:\nmisogynous (P = 0.90).\nAnn is in the kitchen David is in the kitchen\n∆P (10−2) -40.4 15.4 12.7 -12.6 -24.3 -1.0 8.0 -1.3 -5.8 -6.7\nG 0.25 0.16 0.08 0.10 0.21 0.19 0.18 0.09 0.09 0.28\nIG -0.15 0.18 0.12 -0.33 -0.22 -0.36 0.14 0.09 -0.25 -0.17\nSHAP 0.27 -0.31 -0.15 -0.01 0.27 -0.29 -0.38 -0.19 -0.05 0.09\nSOC 0.28 -0.19 -0.06 0.10 0.07 -0.25 -0.11 -0.03 0.04 0.05\nTable 5: Manually-generated example. Text starts with a female (left) and male (right) name. Ground truth (both):\nnon-misogynous. Prediction: misogynous (P = 0.53) (left), non-misogynous (P = 0.14) (right).\np*rca p*ttana che gran pezzo di f*ga p*ttana che gran pezzo di f*ga\n∆P (10−2) 94.7 79.7 -0.8 -0.6 0.3 -0.7 -0.6 1.0 -2.3 -1.3 0.4 0.3 -22.9\nG 0.17 0.15 0.06 0.07 0.11 0.07 0.13 0.20 0.08 0.10 0.14 0.08 0.21\nIG -0.25 -0.10 -0.09 -0.16 -0.04 0.21 0.13 -0.12 -0.03 -0.25 0.11 0.17 0.32\nSHAP -0.69 -0.01 0.01 0.05 0.05 0.05 0.14 0.15 0.10 0.13 0.10 0.10 0.43\nSOC -0.56 -0.07 0.00 0.04 0.05 -0.05 0.22 0.00 0.05 0.07 0.04 -0.12 0.57\nTable 6: Manually-generated example. Complete text (left) and text without initial “p*rca” (right). Non-literal trans-\nlation: “holy sh*t what a nice piece of *ss”. Ground truth (both): misogynous. Prediction: non-misogynous\n(P = 0.03) (left), misogynous (P = 0.97) (right).\nf*ga (*ss) has the highest positive contribution for\nall the approaches.\n4.1 Is attention explanation?\nWe follow up on the open debate on attention\nused as an explanation, providing examples on the\nmisogyny identification task. Figure 1 shows self-\nattention maps in our fine-tuned BERT at different\nlayers and heads for the already discussed sentence\n“You are a smart woman”. Based on our previous\nanalysis (§4), we know that the model has an unin-\ntended bias towards the token “woman”.\nWe cannot infer the same information from at-\ntention maps. Raw attention weights differ sig-\nnificantly for different layers and heads. In this\nexample, there is a vertical pattern (Kovaleva et al.,\n2019) on the token “a” in layer 3 (Figure 1a). How-\never, the pattern disappears from heads in the same\nlayer (Figure 1b) and from the same head on deeper\nlayers, where, instead, a block pattern characterizes\n“smart” and “woman” (Figure 1c). This variabil-\nity hinders interpretability as no unique behavior\nemerges. Effective Attention (Brunner et al., 2020)\nis based on attention and shares the same issue. 7\nThese results further motivate the idea that attention\ngives only a local perspective on token contribu-\ntion and contextualization (Bastings and Filippova,\n2020). However, this does not provide any use-\nful insight for the classification task. To further\nvalidate this limited scope, we use Hidden Token\nAttribution (Brunner et al., 2020) and measure the\ncontribution of each input token (i.e., its first-layer\ntoken embedding) to hidden representations. On\nlower layers, there is a marked diagonal contribu-\ntion, meaning that tokens mainly contribute to their\nown representation. Interestingly, on the upper lay-\ners, a strong contribution to “smart” and “woman”\nappears for all the tokens in the sentence. Different\npatterns between HTA and attention suggest that,\neven in the locality of a layer and a single head, at-\ntention weights do not measure token contribution.\nWe observed similar issues on other examples\nand for Italian models (see appendix B). We there-\n7In most of our experiments, Effective Attention brings no\nperceptually different maps than simple Attention. The two\nmethods are hence equivalent for local attention inspection.\n104\n(a) Layer 3, Head 1\n (b) Layer 3, Head 3\n(c) Layer 10, Head 1\n (d) Layer 10, Head 3\nFigure 1: Attention (left), Effective Attention (center), and Hidden Token Attribution (right) maps at different layers\nin fine-tuned BERT. Lighter colors indicate higher weights. Sentence: “You are a smart woman”.\nfore cannot consider attention as a plausible nor\na faithful explanation method and discourage the\nuse of attention to explain BERT-based misogyny\nclassifiers.\n5 Related Work\nFew works applied interpretability approaches to\nhate speech detection. Wang (2018) proposes an\nadaptation of explainability techniques for com-\nputer vision to visualize and understand the CNN-\nGRU classifier for hate speech (Zhang et al., 2018).\nMosca et al. (2021) study both local and global\nexplanations. They use Shapley values (Lund-\nberg and Lee, 2017) to quantify feature impor-\ntance on a local level and feature space exploration\nfor a global explanation. Risch et al. (2020) ana-\nlyze multiple attribution-based explanation meth-\nods for offensive language detection. The analy-\nsis includes an interpretable model (Naïve Bayes),\nmodel-agnostic methods based on surrogate models\n(LIME (Ribeiro et al., 2016), layer-wise relevance\npropagation (LRP) (Bach et al., 2015), and a self-\nexplanatory model (LSTM with an attention mech-\nanism). SHAP explainer is applied (Wich et al.,\n2020) to investigate the impact of political bias on\nhate speech classification. Sample-And-Occlusion\n(SOC) explanation algorithm has been used in its\nhierarchical version in different papers for showing\nthe results of hate speech detection (Nozza, 2021;\nKennedy et al., 2020).\nIn this paper, we specifically focus on hate\nspeech against women. In this context, Godoy and\nTommasel (2021) apply SHAP to derive global ex-\nplanations with the aim of exploring unintended\nbias of Random Forest-based misogyny classifier.\nWhile growing efforts are made for evaluat-\ning interpretability approaches for NLP models\n(Atanasova et al., 2020; DeYoung et al., 2020;\nPrasad et al., 2021; Nguyen, 2018; Hase and\nBansal, 2020; Nguyen and Martínez, 2020; Jacovi\nand Goldberg, 2020), the evaluation is not domain-\nspecific. Therefore, the benchmarking miss to con-\nsider specific sensitive problems and biases that\nare proper of the hate speech domain on which the\nexplanation validation must focus. This paper fills\nthis gap by focusing on post-hoc feature attribution\nexplanation methods on individual predictions for\nthe task of hate speech against women.\n6 Conclusion\nIn this paper, we benchmarked different explain-\nability approaches on Transformer-based models\nfor the task of hate speech detection against women\nin English and Italian. We focus on post-hoc\nfeature attribution methods applied to fine-tuned\nBERT models. Our evaluation demonstrated that\nSHAP and SOC provide plausible and faithful\nexplanations and are consequently recommended\nfor explaining misogyny classifiers’ outputs. In\ncontrast, gradient- and attention-based approaches\nfailed in providing reliable explanations.\nAs future work, we plan to add to the bench-\nmarking suite a systematic evaluation involving\nhuman annotators. We also plan to include recently\nintroduced token attribution methods (Sikdar et al.,\n2021) as well as new families of approaches, like\n105\nnatural language explanations (Rajani et al., 2019;\nNarang et al., 2020) and input editing (Ross et al.,\n2021). Finally, we will assess explanations of the\nmost problematic data subgroups (Goel et al., 2021;\nPastor et al., 2021; Wang et al., 2021).\nAcknowledgments\nWe would like to thank the anonymous reviewers\nand area chairs for their suggestion to strengthen\nthe paper. This research is partially supported\nby funding from the European Research Council\n(ERC) under the European Union’s Horizon 2020\nresearch and innovation program (No. 949944, IN-\nTEGRATOR), and by Fondazione Cariplo (grant\nNo. 2020-4288, MONICA). DN, and DH are mem-\nbers of the MilaNLP group, and of the Data and\nMarketing Insights Unit at the Bocconi Institute for\nData Science and Analysis. EP is member of the\nDataBase and Data Mining Group (DBDMG) at\nPolitecnico di Torino. GA did part of the work as a\nmember of the DBDMG and is currently a member\nof MilaNLP. Computing resources were provided\nby the SmartData@PoliTO center on Big Data and\nData Science.\nEthical Considerations\nWe explain BERT-based classifiers using a con-\ntrolled subset of a large, fast-growing collection\nof explanation methods available in the literature.\nWhile replicating our experiments with different\napproaches, or on different data samples, from dif-\nferent datasets or explaining different models, we\ncannot exclude that some people may find the ex-\nplanations offensive or stereotypical. Further, re-\ncent work has demonstrated gradient-based expla-\nnations are manipulable (Wang et al., 2020), ques-\ntioning the reliability of this widespread category\nof methods.\nWe, therefore, advocate for responsible use of\nthis benchmarking suite (or any product derived\nfrom it) and suggest pairing it with human-aided\nevaluation. Moreover, we encourage users to con-\nsider this work as a starting point for model debug-\nging (Nozza et al., 2022) and the included explana-\ntion methods as baselines for future developments.\nReferences\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3256–3274, Online. Association for\nComputational Linguistics.\nGiuseppe Attanasio, Debora Nozza, Dirk Hovy, and\nElena Baralis. 2022. Entropy-based attention regular-\nization frees unintended bias mitigation from lists. In\nFindings of the Association for Computational Lin-\nguistics: ACL2022. Association for Computational\nLinguistics.\nGiuseppe Attanasio and Eliana Pastor. 2020. PoliTeam\n@ AMI: Improving sentence embedding similarity\nwith misogyny lexicons for automatic misogyny iden-\ntificationin italian tweets. In Valerio Basile, Danilo\nCroce, Maria Maro, and Lucia C. Passaro, editors,\nEVALITA Evaluation of NLP and Speech Tools for\nItalian - December 17th, 2020 , pages 48–54. Ac-\ncademia University Press.\nSebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wo-\njciech Samek. 2015. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PLOS ONE, 10(7):1–46.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.\n2019. SemEval-2019 task 5: Multilingual detection\nof hate speech against immigrants and women in\nTwitter. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation, pages 54–63, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use attention\nas explanation when we have saliency methods? In\nProceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 149–155, Online. Association for Com-\nputational Linguistics.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum\nThain, and Lucy Vasserman. 2019. Nuanced met-\nrics for measuring unintended bias with real data for\ntext classification. In Companion Proceedings of\nThe 2019 World Wide Web Conference, WWW ’19,\npage 491–500, New York, NY , USA. Association for\nComputing Machinery.\nGino Brunner, Yang Liu, Damian Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Wat-\ntenhofer. 2020. On identifiability in transformers. In\n8th International Conference on Learning Represen-\ntations, ICLR 2020. OpenReview.net.\nMarina Danilevsky, Kun Qian, Ranit Aharonov, Yan-\nnis Katsis, Ban Kawas, and Prithviraj Sen. 2020. A\nsurvey of the state of explainable AI for natural lan-\nguage processing. In Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Association\n106\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 447–459, Suzhou, China. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark to\nevaluate rationalized NLP models. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4443–4458, Online.\nAssociation for Computational Linguistics.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classification. In Proceed-\nings of the 2018 AAAI/ACM Conference on AI, Ethics,\nand Society, AIES ’18, page 67–73, New York, NY ,\nUSA. Association for Computing Machinery.\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\nPathologies of neural models make interpretations\ndifficult. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3719–3728, Brussels, Belgium. Association\nfor Computational Linguistics.\nElisabetta Fersini, Debora Nozza, and Giulia Boifava.\n2020a. Profiling Italian misogynist: An empirical\nstudy. In Proceedings of the Workshop on Resources\nand Techniques for User and Author Profiling in Abu-\nsive Language, pages 9–13, Marseille, France. Euro-\npean Language Resources Association (ELRA).\nElisabetta Fersini, Debora Nozza, and Paolo Rosso.\n2018. Overview of the EV ALITA 2018 task on au-\ntomatic misogyny identification (AMI). volume 12,\npage 59, Turin, Italy. CEUR.org.\nElisabetta Fersini, Debora Nozza, and Paolo Rosso.\n2020b. AMI @ EV ALITA2020: Automatic misog-\nyny identification. In Proceedings of the 7th eval-\nuation campaign of Natural Language Processing\nand Speech tools for Italian (EVALITA 2020), Online.\nCEUR.org.\nDaniela Godoy and Antonela Tommasel. 2021. Is my\nmodel biased? exploring unintended bias in misog-\nyny detection tasks. In AIofAI’21: 1st Workshop on\nAdverse Impacts and Collateral Effects of Artificial\nIntelligence Technologies, pages 97–111.\nKaran Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary\nTaschdjian, Mohit Bansal, and Christopher Ré. 2021.\nRobustness gym: Unifying the NLP evaluation land-\nscape. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies: Demonstrations, pages 42–55, Online. As-\nsociation for Computational Linguistics.\nBryce Goodman and Seth Flaxman. 2017. European\nunion regulations on algorithmic decision-making\nand a “right to explanation”. AI magazine, 38(3):50–\n57.\nElla Guest, Bertie Vidgen, Alexandros Mittos, Nishanth\nSastry, Gareth Tyson, and Helen Margetts. 2021. An\nexpert annotated dataset for the detection of online\nmisogyny. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 1336–1350,\nOnline. Association for Computational Linguistics.\nRiccardo Guidotti, Anna Monreale, Salvatore Ruggieri,\nFranco Turini, Fosca Giannotti, and Dino Pedreschi.\n2019. A survey of methods for explaining black box\nmodels. ACM Computing Surveys, 51(5):93:1–93:42.\nXiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov.\n2020. Explaining black box predictions and unveil-\ning data artifacts through influence functions. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5553–\n5563, Online. Association for Computational Lin-\nguistics.\nPeter Hase and Mohit Bansal. 2020. Evaluating explain-\nable AI: Which algorithmic explanations help users\npredict model behavior? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5540–5552, Online. Association\nfor Computational Linguistics.\nVijayasaradhi Indurthi, Bakhtiyar Syed, Manish Shri-\nvastava, Nikhil Chakravartula, Manish Gupta, and\nVasudeva Varma. 2019. FERMI at SemEval-2019\ntask 5: Using sentence embeddings to identify hate\nspeech against immigrants and women in Twitter.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation , pages 70–74, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable NLP systems: How should we\ndefine and evaluate faithfulness? In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4198–4205, On-\nline. Association for Computational Linguistics.\nAlon Jacovi, Oren Sar Shalom, and Yoav Goldberg.\n2018. Understanding convolutional neural networks\nfor text classification. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 56–65,\nBrussels, Belgium. Association for Computational\nLinguistics.\n107\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot Explanation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 3543–3556, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXisen Jin, Zhongyu Wei, Junyi Du, Xiangyang Xue, and\nXiang Ren. 2020. Towards hierarchical importance\nattribution: Explaining compositional semantics for\nneural sequence models. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nBrendan Kennedy, Xisen Jin, Aida Mostafazadeh Da-\nvani, Morteza Dehghani, and Xiang Ren. 2020. Con-\ntextualizing hate speech classifiers with post-hoc ex-\nplanation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5435–5442, Online. Association for Computa-\ntional Linguistics.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, et al. 2020. Captum: A unified and\ngeneric model interpretability library for PyTorch.\narXiv preprint arXiv:2009.07896.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for Com-\nputational Linguistics.\nAlyssa Lees, Jeffrey Sorensen, and Ian Kivlichan. 2020.\nJigsaw @ AMI and HaSpeeDe2: Fine-Tuning a Pre-\nTrained Comment-Domain BERT Model. In Pro-\nceedings of Seventh Evaluation Campaign of Natu-\nral Language Processing and Speech Tools for Ital-\nian. Final Workshop (EVALITA 2020), Bologna, Italy.\nCEUR.org.\nZachary C. Lipton. 2018. The mythos of model in-\nterpretability: In machine learning, the concept of\ninterpretability is both important and slippery.Queue,\n16(3):31–57.\nScott M Lundberg and Su-In Lee. 2017. A unified\napproach to interpreting model predictions. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 30. Curran Associates, Inc.\nAndreas Madsen, Siva Reddy, and Sarath Chandar. 2021.\nPost-hoc Interpretability for Neural NLP: A Survey.\narXiv preprint arXiv:2108.04840.\nEdoardo Mosca, Maximilian Wich, and Georg Groh.\n2021. Understanding and interpreting the impact of\nuser context in hate speech detection. In Proceedings\nof the Ninth International Workshop on Natural Lan-\nguage Processing for Social Media, pages 91–102,\nOnline. Association for Computational Linguistics.\nSharan Narang, Colin Raffel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\nWT5?! Training Text-to-Text Models to Explain their\nPredictions. arXiv preprint arXiv:2004.14546.\nAn-phi Nguyen and María Rodríguez Martínez. 2020.\nOn quantitative aspects of model interpretability.\narXiv preprint arXiv:2007.07584.\nDong Nguyen. 2018. Comparing automatic and human\nevaluation of local explanations for text classification.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 1069–1078, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nDebora Nozza. 2021. Exposing the limits of zero-shot\ncross-lingual hate speech detection. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 907–914, Online.\nAssociation for Computational Linguistics.\nDebora Nozza, Federico Bianchi, , and Dirk Hovy. 2022.\nPipelines for Social Bias Testing of Large Language\nModels. In Proceedings of the First Workshop on\nChallenges & Perspectives in Creating Large Lan-\nguage Models. Association for Computational Lin-\nguistics.\nDebora Nozza, Claudia V olpetti, and Elisabetta Fersini.\n2019. Unintended bias in misogyny detection. In\nIEEE/WIC/ACM International Conference on Web\nIntelligence, WI ’19, page 149–155, New York, NY ,\nUSA. Association for Computing Machinery.\nEliana Pastor, Luca de Alfaro, and Elena Baralis. 2021.\nLooking for trouble: Analyzing classifier behavior\nvia pattern divergence. In Proceedings of the 2021\nInternational Conference on Management of Data ,\npage 1400–1412, New York, NY , USA. Association\nfor Computing Machinery.\nGrusha Prasad, Yixin Nie, Mohit Bansal, Robin Jia,\nDouwe Kiela, and Adina Williams. 2021. To what ex-\ntent do human explanations of model behavior align\nwith actual model behavior? In Proceedings of the\nFourth BlackboxNLP Workshop on Analyzing and\nInterpreting Neural Networks for NLP, pages 1–14,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942, Florence, Italy. Association for\nComputational Linguistics.\n108\nMarco Túlio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"Why Should I Trust You?\": Ex-\nplaining the Predictions of Any Classifier. In Pro-\nceedings of the 22nd ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, San Francisco, CA, USA, August 13-17, 2016 ,\npages 1135–1144. ACM.\nJulian Risch, Robin Ruff, and Ralf Krestel. 2020. Offen-\nsive language detection explained. In Proceedings\nof the Second Workshop on Trolling, Aggression and\nCyberbullying, pages 137–143, Marseille, France.\nEuropean Language Resources Association (ELRA).\nAlexis Ross, Ana Marasovi´c, and Matthew Peters. 2021.\nExplaining NLP models via minimal contrastive edit-\ning (MiCE). In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n3840–3852, Online. Association for Computational\nLinguistics.\nCynthia Rudin. 2019. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine\nIntelligence, 1(5):206–215.\nNiloofar Safi Samghabadi, Parth Patwa, Srinivas PYKL,\nPrerana Mukherjee, Amitava Das, and Thamar\nSolorio. 2020. Aggression and misogyny detection\nusing BERT: A multi-task approach. In Proceedings\nof the Second Workshop on Trolling, Aggression and\nCyberbullying, pages 126–131, Marseille, France.\nEuropean Language Resources Association (ELRA).\nSoumya Sanyal and Xiang Ren. 2021. Discretized in-\ntegrated gradients for explaining language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10285–10299, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nSofia Serrano and Noah A. Smith. 2019. Is attention in-\nterpretable? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2931–2951, Florence, Italy. Association for\nComputational Linguistics.\nSandipan Sikdar, Parantapa Bhattacharya, and Kieran\nHeese. 2021. Integrated directional gradients: Fea-\nture interaction attribution for neural NLP models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 865–878,\nOnline. Association for Computational Linguistics.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In 2nd International Conference on Learning\nRepresentations, ICLR 2014.\nKaiser Sun and Ana Marasovi´c. 2021. Effective atten-\ntion sheds light on interpretability. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4126–4135, Online. Association\nfor Computational Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning - Volume 70, ICML’17, page 3319–3328.\nJMLR.org.\nCindy Wang. 2018. Interpreting neural network hate\nspeech classifiers. In Proceedings of the 2nd Work-\nshop on Abusive Language Online (ALW2) , pages\n86–92, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJunlin Wang, Jens Tuyls, Eric Wallace, and Sameer\nSingh. 2020. Gradient-based analysis of NLP mod-\nels is manipulable. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n247–258, Online. Association for Computational Lin-\nguistics.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng\nZou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui\nZheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li,\nChong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai,\nJun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan,\nYuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin\nZhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong\nPeng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,\nXipeng Qiu, and Xuanjing Huang. 2021. TextFlint:\nUnified multilingual robustness evaluation toolkit for\nnatural language processing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 347–355, Online. Association\nfor Computational Linguistics.\nMaximilian Wich, Jan Bauer, and Georg Groh. 2020.\nImpact of politically biased data on hate speech clas-\nsification. In Proceedings of the Fourth Workshop\non Online Abuse and Harms, pages 54–64, Online.\nAssociation for Computational Linguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is not\nnot explanation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 11–20, Hong Kong, China. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\n109\nZiqi Zhang, David Robinson, and Jonathan A. Tep-\nper. 2018. Detecting hate speech on twitter using\na convolution-GRU based deep neural network. In\nThe Semantic Web - 15th International Conference,\nESWC 2018, Heraklion, Crete, Greece, June 3-7,\n2018, Proceedings, volume 10843 of Lecture Notes\nin Computer Science, pages 745–760. Springer.\n110\nA Experimental setup\nA.1 Training hyper-parameters\nAll our experiments use the Hugging\nFace transformers library (Wolf et al.,\n2020). We base our models and to-\nkenizers on the bert-base-cased\ncheckpoint for English tasks and on the\ndbmdz/bert-base-italian-cased\ncheckpoint for Italian. We pre-process and\ntokenize our data using the standard pre-trained\nBERT tokenizer, with a maximum sequence length\nof 128 and right padding. We train all models\nfor 3 epochs with a batch size of 64, a linearly\ndecaying learning rate of 5 · 10−5 and 10% of the\ntotal training step as a warmup, and full precision.\nWe use 10% of training data for validation. We\nevaluate the model every 50 steps on the respective\nvalidation set. At the end of the training, we use\nthe checkpoint with the best validation loss. We\nre-weight the standard cross-entropy loss using\nthe inverse of class frequency to account for class\nimbalance.\nA.2 Explanation methods\nWe used the Captum library (Kokhlikyan et al.,\n2020) with default parameters to compute gradients\n(G) and integrated gradients (IG). Following (Han\net al., 2020), for IG we multiply gradients by input\nword embeddings. For Shapley values estimation\n(SHAP), we use the shap library 8 with Partition-\nSHAP as approximation method. For Sampling-\nAnd-Occlusion (SOC), we used the implementation\nassociated with Kennedy et al. (2020).9 Please re-\nfer to our repository ( https://github.com\n/MilaNLProc/benchmarking-xai-mis\nogyny) for further technical details.\nA.3 Attention maps\nWe used attention weights provided by the trans-\nformers library for visualization. We implemented\nEffective Attention and Hidden Token Attribution\nfollowing Brunner et al. (2020). We release the\nimplementation on our repository.\nB Attention plots\nFigure 2 shows attention visualizations for the\nsentence “p*rca p*ttana che gran pezzo di f*ga”\n8https://github.com/slundberg/shap\n9https://github.com/BrendanKennedy/co\nntextualizing-hate-speech-models-with-ex\nplanations\n(Non-literal translation: “ holy sh*t what a nice\npiece of *ss ”). As discussed in §4 ( Bias due to\nlanguage-specific expressions), the text is mis-\nclassified as non-misogynous and most of ex-\nplanation methods correctly highlight the Italian\ninterjection “p*rca p*ttana”.\nSimilar to results reported in §2.2, we cannot find\nuseful insights in attention plots. Attention in layer\n3 has a diagonal pattern in head 1, and a diagonal\npattern in head 3 on the word che (“what”). How-\never, these patterns disappear in layer 10 where\nattention is focused on p*rca. At layer 10, HTA\nis more spread than attention, suggesting that the\nlatter measures only a local token contribution.\n111\n(a) Layer 3, Head 1\n (b) Layer 3, Head 3\n(c) Layer 10, Head 1\n (d) Layer 10, Head 3\nFigure 2: Attention (left), Effective Attention (center), and Hidden Token Attribution (right) maps at different layers\nin fine-tuned BERT. Lighter colors indicate higher weights. Sentence: “p*rca p*ttana che gran pezzo di f*ga”,\nnon-literal translation: “holy sh*t what a nice piece of *ss”.\n112",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9263993501663208
    },
    {
      "name": "Computer science",
      "score": 0.7241575717926025
    },
    {
      "name": "Benchmarking",
      "score": 0.652925968170166
    },
    {
      "name": "Transformer",
      "score": 0.5958965420722961
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5400041341781616
    },
    {
      "name": "Natural language processing",
      "score": 0.47329702973365784
    },
    {
      "name": "Machine learning",
      "score": 0.46677130460739136
    },
    {
      "name": "Security token",
      "score": 0.4664183557033539
    },
    {
      "name": "Attribution",
      "score": 0.4500739872455597
    },
    {
      "name": "Speech recognition",
      "score": 0.3657398223876953
    },
    {
      "name": "Psychology",
      "score": 0.18379855155944824
    },
    {
      "name": "Computer security",
      "score": 0.08905023336410522
    },
    {
      "name": "Social psychology",
      "score": 0.08657252788543701
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71209653",
      "name": "Bocconi University",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I177477856",
      "name": "Polytechnic University of Turin",
      "country": "IT"
    }
  ],
  "cited_by": 15
}