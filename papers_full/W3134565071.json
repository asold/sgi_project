{
    "title": "SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation",
    "url": "https://openalex.org/W3134565071",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Yun, Boxiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967442656",
            "name": "Wang Yan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2367696820",
            "name": "Chen, Jieneng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2336363575",
            "name": "Wang Huiyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2115343774",
            "name": "Shen Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1990794095",
            "name": "Li Qingli",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2951839332",
        "https://openalex.org/W2991616716",
        "https://openalex.org/W3087334746",
        "https://openalex.org/W3011934240",
        "https://openalex.org/W2970777192",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W3132503749",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2083125248",
        "https://openalex.org/W3088873647",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2981108991",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2150990614"
    ],
    "abstract": "Hyperspectral imaging (HSI) unlocks the huge potential to a wide variety of applications relied on high-precision pathology image segmentation, such as computational pathology and precision medicine. Since hyperspectral pathology images benefit from the rich and detailed spectral information even beyond the visible spectrum, the key to achieve high-precision hyperspectral pathology image segmentation is to felicitously model the context along high-dimensional spectral bands. Inspired by the strong context modeling ability of transformers, we hereby, for the first time, formulate the contextual feature learning across spectral bands for hyperspectral pathology image segmentation as a sequence-to-sequence prediction procedure by transformers. To assist spectral context learning procedure, we introduce two important strategies: (1) a sparsity scheme enforces the learned contextual relationship to be sparse, so as to eliminates the distraction from the redundant bands; (2) a spectral normalization, a separate group normalization for each spectral band, mitigates the nuisance caused by heterogeneous underlying distributions of bands. We name our method Spectral Transformer (SpecTr), which enjoys two benefits: (1) it has a strong ability to model long-range dependency among spectral bands, and (2) it jointly explores the spatial-spectral features of HSI. Experiments show that SpecTr outperforms other competing methods in a hyperspectral pathology image segmentation benchmark without the need of pre-training. Code is available at https://github.com/hfut-xc-yun/SpecTr.",
    "full_text": "SpecTr: Spectral Transformer for Hyperspectral\nPathology Image Segmentation\nBoxiang Yun1, Yan Wang1\f, Jieneng Chen2, Huiyu Wang2,\nWei Shen3, and Qingli Li 1\n1East China Normal University 2Johns Hopkins University\n3Shanghai Jiaotong University\nwyanny.9@gmail.com\nAbstract. Hyperspectral imaging (HSI) unlocks the huge potential to\na wide variety of applications relied on high-precision pathology image\nsegmentation, such as computational pathology and precision medicine.\nSince hyperspectral pathology images beneﬁt from the rich and detailed\nspectral information even beyond the visible spectrum, the key to achieve\nhigh-precision hyperspectral pathology image segmentation is to felici-\ntously model the context along high-dimensional spectral bands. Inspired\nby the strong context modeling ability of transformers, we hereby, for\nthe ﬁrst time, formulate the contextual feature learning across spectral\nbands for hyperspectral pathology image segmentation as a sequence-to-\nsequence prediction procedure by transformers. To assist spectral context\nlearning procedure, we introduce two important strategies: (1) a sparsity\nscheme enforces the learned contextual relationship to be sparse, so as\nto eliminates the distraction from the redundant bands; (2) a spectral\nnormalization, a separate group normalization for each spectral band,\nmitigates the nuisance caused by heterogeneous underlying distributions\nof bands. We name our method Spectral Transformer (SpecTr), which\nenjoys two beneﬁts: (1) it has a strong ability to model long-range de-\npendency among spectral bands, and (2) it jointly explores the spatial-\nspectral features of HSI. Experiments show that SpecTr outperforms\nother competing methods in a hyperspectral pathology image segmen-\ntation benchmark without the need of pre-training. Code is available\nat https://github.com/hfut-xc-yun/SpecTr.\nKeywords: Hyperspectral image segmentation· Pathology · Transformer.\n1 Introduction\nHyperspectral imaging (HSI) is a technique that analyzes how a wide spectrum\nof light interacts with observed materials, measuring the amount of light that is\nemitted, reﬂected or transmitted from a target [9], instead of assigning primary\ncolors (red, green, blue) to each pixel. Detailed spectral information is presented\nin hundreds of narrow and contiguous spectral bands. Since diﬀerent molecules’\nresponses to light are diﬀerent, utilizing HSI, spectral information across bands\narXiv:2103.03604v1  [eess.IV]  5 Mar 2021\n2 B. Yun et al.\nacquires biochemical properties invisible to the naked eye from both stained\nand unstained histological specimens, which brings opportunities for digital and\ncomputational pathology and precision medicine.\nWith the surge of deep neural networks, hyperspectral pathology image seg-\nmentation is undergoing a transition from traditional machine learning methods\n[6] to deep learning based methods [8]. Traditional methods designed spatial fea-\nture extraction methods such as watershed, wavelet, local binary pattern, and\nspectral feature mapping methods such as spectral unmixing, band selection.\nBut these methods suﬀer from the low eﬃciency of feature fusion, limited rep-\nresentation learning ability. High-dimensional spectral bands in HSIs are highly\ncorrelated [1], exploring the wide range of band relationships is critical to achieve\nhigh-precision hyperspectral pathology image segmentation results. Deep learn-\ning, especially the u-shape architecture, has become the de-facto standard and\nachieved tremendous success for various medical image segmentation tasks. How-\never, u-shape networks, no matter 2D or 3D, have limited ability to model long-\nrange context along the rich spectral dimension, which restricts these methods\nfrom achieving good performance on hyperspectral pathology image segmenta-\ntion [16,13].\nInspired from transformers’ strong ability in modeling long-range context\n[5,18,2,14], we hereby, for the ﬁrst time, formulate context-aware spectral band\nrepresentation learning as a sequence-to-sequence prediction problem, and in-\nstantiate it by transformers. Moreover, to assist the spectrum context modeling\nprocess, two important schemes are introduced. First, since standard transform-\ners learn dense correlations among all bands, the learned contextual representing\nmay be polluted by irrelevant bands. We thereby introduce an important sparsity\nscheme [4] into transformers to ensure the sparsity of the learned correlations.\nSecond, a spectral normalization, a separate group normalization for each band,\nis proposed to ease the nuisance caused by heterogeneous underlying distribu-\ntions of bands. Our transformers are embedded into the encoder part of a u-shape\nsegmentation network, with the input of convolutional features at each spatial lo-\ncation. The convolutional features are equipped with spatial information, which\nare then entangled with spectral information encoded by transformers, forming\na joint spatial-spectralrepresentation learning framework. We term the proposed\nmethod as Spectral Transformer (SpecTr). Experimental results show that the\ntwo introduced schemes are vital for spectral contextual representation learning.\nOur SpecTr achieves signiﬁcant better hyperspectral pathology segmentation re-\nsults than other competitors, including CNN-based self-attention segmentation\nmethods.\n2 Related Work\nHyperspectral medical image segmentationTraditional hyperspectral med-\nical image segmentation methods are mainly dedicated to extract hand-designed\nspectral and spatial features from hyperspectral data, and fuse features from\nthe two domains [6]. Deep learning methods become ubiquitous in the ﬁeld of\nSpecTr: Spectral Transformer 3\n……\nSpectrum embedding\nNormMulti-Head Attention\nNormMLP\n+\n+\nTransformer Encoder\nSparsity constraint\nconv 3X3SpectralNormReLU\nconv 3X3x3Group NormReLU\nConcatenateUp-sampleConv3x3, SN, ReLUConvolution+AverageConv3x3x3, BN, ReLUTransformerEncoder\nMax-pool \n! \"#\nFig. 1.The architecture of SpecTr. Follow the u-shape architecture, our SpecTr em-\nploys 2D convolutional kernel, spectral norm, and transformers with the sparsity con-\nstraint in the encoder. The decoder and skip connections are the same as 3D U-Net.\nmedical image segmentation, which also beneﬁt hyperspectral medical image\nsegmentation. 2D CNN is explored to segment tongue tumor from HSIs, with\nan additional layer to select important bands from HSIs [13]. Since HSIs are 3D\nvolumes, it is intuitive to use 3D CNNs to simultaneously learn spatial-spectral\ninformation. Hyper-Net [16] ﬁrst extracted 16 informative bands from an image,\nthen fed the image into a dual-path module 3D CNN, which achieved promising\nresults for hyperspectral pathology image segmentation.\nTransformer Transformer [15] was ﬁrst proposed for NLP tasks, it becomes\nstate-of-the-art methods in some computer vision tasks [5,18,10]. Transformer,\nbased solely on attention mechanisms, is famous for its sequence modeling abil-\nity. Parmar et al. [10] performed conditional image generation with the Image\nTransformer, and the generated images look more natural than other methods.\nViT [5] showed that applying a pure transformer to sequences of image patches\ncan achieve favorable performance in image classiﬁcation tasks. SETR [18] lever-\nages merits from both transformer and CNN, where transformer is served as the\nencoder while CNN is the decoder. Very recently, TransUNet [2], which also\ncombines transformer and CNN, obtaining promising results for some medical\nsegmentation tasks. Our SpecTr is the ﬁrst model to learn the sequence represen-\ntation of spectra for hyperspectral pathology image segmentation, and explores\nthe context along spectral dimension which is preferable to segmentation.\n3 Methodology\nMathematically, let Z ∈ RW×H×L denote the 3D volume of a hyperspectral\npathology image, where W×H is the spatial resolution, and Lis the number of\nspectral bands. The goal of image segmentation is to predict the per-pixel label\nmap ˆY ∈{0,1}W×H, indicating where the target is in Z. Let D= {Zi,Yi}N\ni=1\nbe our training set, where Yi denotes the per-pixel annotation for image Zi.\n4 B. Yun et al.\n550 nm\n700 nm\n850 nm\nFig. 2.Probability density functions (PDF) of diﬀerent spectral images. Each PDF on\nthe left is plotted by average ﬁve hyperspectral pathology images. Example spectral\nimages of diﬀerent wavelengths from one HSI are shown on the right.\nAn overview of the model, which follows the u-shape architecture, is depicted in\nFig. 1. Image Z is ﬁrst decomposed into a sequence of spectral images in order\nof wavelength. Then the sequence of spectral images is fed into an alternating\nprocess of depth-wise convolution, spectral normalization and transformers with\nsparsity constraint to produce a sequence of spatial-spectral contextual feature\nmaps with the same sequence length. Since the ﬁrst resolution layer lacks seman-\ntic feature for each spectral location, transformers are added after the second,\nthird and fourth resolution layers. Then the sequence of feature maps are con-\ncatenated and then gradually decoded to generate the segmentation map. Skip\nconnections are employed, which from layers of equal resolution in the encoder\nretain essential high-resolution features to the decoder.\n3.1 Spectral Normalization\nIn the encoder, the decomposed spectral images are fed into depth-wise convo-\nlution [12], which applies a single ﬁlter to each input spectral image. Depth-wise\nconvolution can learn spectrum embedding while keeping spectrum independent.\nA normalization processing is usually applied after convolution for reducing in-\nternal covariate shift, and therefore helping improve feature discrimination ca-\npability. Diﬀerent spectral images have heterogeneous distributions, as shown in\nFig. 2. When training deep networks that directly integrate all heterogeneous\nspectral images, we hypothesize this distribution mismatch among diﬀerent spec-\ntral images adversely aﬀects context learning among bands, and causes perfor-\nmance degradation.\nTo this end, we propose Spectral Normalization (SN) to address this problem.\nMore speciﬁcally, let F(Z; Θ) ∈RW′×H′×L′×C denote the feature map produced\nafter a certain depth-wise convolutional layer, where C is the number of feature\nchannels, and Θ is the parameter of all previous layers. We assign an individual\ngroup normalization layer for fs(Z; Θ) ∈RW′×H′×C, which is the feature map\nat spectral location s, to eﬀectively tackle the inter-spectrum discrepancy.\nSpecTr: Spectral Transformer 5\n3.2 Transformer with Sparsity Constraint\nSpatial-spectral Embedding The feature vector hx,s(Z; Θ) ∈RC at each\nspatial location x and spectral location son the feature map F(Z; Θ) represents\na spatial-spectral feature element. The feature element is learned by 2D spatial\nconvolution, and will be fed into a transformer to learn spectral context. Followed\nby [5,18], a linear projection matrix E ∈RC×D is learned to map the feature\nelement hx,s into a latent D-dimensional embedding space. We omit parameters\nfor notational simplicity. To encode the spectral location information, we learn a\nspeciﬁc embedding ps for every location s, which is added to the spatial-spectral\nembedding to form the sequential input:z0 = [hx,1E+p1; hx,2E+p2; ...; hx,L′ E+\npL′ ]\nTransformer The transformer consists of multiheaded self-attention (MSA),\nMLP blocks, layer normalization (LN) and residual connections. The output of\nthe transformer can be computed by:\nz′\n1 = MSA(LN(z0)) + z0, (1)\nz1 = MLP(LN(z′\n1)) + z′\n1. (2)\nTransformer with Sparsity The attention distribution of each head in MSA\nis predicted typically using the softmax normalization function, which results\nin non-zero weights for all context bands. Our purpose is to learn a meaningful\ncontext among bands for segmentation tasks, i.e., to learn useful bands and get\nrid of noisy or redundant bands. Instead of using softmax, we employ αentmax\nas proposed in [4]:\nAtt(Q,K,V) = π\n(QK⊤\n√\nd\n)\nV, (3)\nπ(X)ij = α -entmax(xi)j. (4)\nIf α = 1, it is exactly softmax mapping. If α >1, it moves away from softmax\ntowards sparse mappings. Ifα= 2, a complete sparse mappings are obtained. We\nset randomly-initialized α values for all heads and the values oscillate between\n1 and 2 by jointly optimized with other parameters of the network.\n4 Experimental Results\n4.1 Experimental Setup\nDataset: We use multi-dimensional choledoch dataset for cholangiocarcinoma\ndiagnosis [17], where 514 scenes with high quality labels are selected for training\nand testing. When using microscopy hyperspectral imaging system to capture\nimages, the light transmitted from the choledoch tissue slice was collected by the\nmicroscope with the objective lens of 20 ×. The wavelength is from 550 nm to\n1000 nm, which ends up with 60 spectral bands for each scene. The image size\nof a single band image of the microscopy hyperspectral date cube is 1280 ×1024.\nWe randomly split all scenes into 401 for training and 113 for testing.\n6 B. Yun et al.\nTable 1.Performance comparison in DSC (%), IoU (%), Hausdorﬀ Distance (HD) on\ncholangiocarcinoma segmentation. Bold denotes the best results for each metric.\nMethod Mean DSC (Median, Max, Min) ↑ IoU ↑ HD ↓\nHSI UNet [13] 62.92 (66.71, 99.93, 12.59) 49.48 41.31\nHSI Hyper-net [16] 69.74 (75.05, 99.95, 4.185) 56.91 36.51\n3D UNet [3] 72.19 (74.98, 99.95, 7.548) 59.37 35.37\n2D UNet [11] 65.80 (66.52, 93.87, 17.97) 50.91 36.46\nAttn UNet [7] 71.05 (73.38, 98.43, 19.01) 57.46 34.32\nUNet++ [19] 68.63 (70.70, 96.26, 17.09) 54.45 35.95\nSpecTr (Ours) 75.21 (77.92, 99.48, 16.44) 62.44 31.60\nImplementation Details and Evaluation Metric: Similar as [16], since the\nentire hyperspectral pathology image is too large to be fed into the model, each\nis downsampled four times spatially, divided into 196 ×196 ×60 image cubes.\nFor data augmentation, we adopt online rotation whose degree is less than 90 ◦\nand vertical or horizontal ﬂipping with probability of 0 .2 during training for all\nthe methods. We set the learning rate of each parameter group using a cosine\nannealing schedule, with the initial learning rate to be 0.0003, and weight decay is\n5×10−4. We use Adam optimizer. The batch size is 1, and the maximum number\nof training epoch is 75 unless otherwise speciﬁed. Experiments are conducted\nusing a single Nvidia Tesla V100-PCIE. All components in our SpecTr do\nnot need pre-training. Multiple evaluation metrics, including Dice-Sørensen\nsimilarity coeﬃcient (DSC), Intersection of Union (IoU), Hausdorﬀ Distance\n(HD) are computed for evaluation purpose.\n4.2 Comparison between SpecTr and Other Methods\nWe conduct comparisons between SpecTr and six competitors: 1) HSI UNet [13],\n2) HSI Hyper-net [16], 3) 3D UNet [3], 4) 2D UNet [11], 5) Attention UNet [7],\nand 6) UNet++ [19]. The ﬁrst two competitors are state-of-the-arts for medi-\ncal HSI segmentation. HSI UNet incorporated a spectral selection layer in the\nnetwork to select a subset of spectra that leaded to optimal performance. HSI\nHyper-net ﬁrst adopted a spectrum selection module to select the most im-\nportant bands out of all bands, and then trained a 3D Hyper-net to conduct\nsegmentation. We follow the same settings as illustrated in the paper [13,16].\nThe other four methods are de-facto standards for various medical image seg-\nmentation tasks. For 2D networks, we simply feed all spectral bands as the input\nchannels to train the networks.\nResults are summarized in Table 1. Our SpecTr performs much better than\nother competitors w.r.t. all evaluation metrics. In particular, SpecTr outperforms\nthe two state-of-the-art hyperspectral medical image segmentation networks,i.e.,\nHSI UNet and HSI Hyper-net by 12 .29% and 5 .47% in DSC. To make a fair\ncomparison with the popular medical image segmentation networks, we change\ntheir batch normalization into group normalization. The 3D UNet with batch\nSpecTr: Spectral Transformer 7\n79.0537.6279.4063.6266.5561.0083.01\n43.8648.9155.7239.1563.4140.6077.83\nImageLabelHSI Hyper3D UNet2D UNetAttn UNetUNet++SpecTr\nHSI UNetHSI Hyper3D UNet2D UNetAttn UNetUnet++SpecTr20406080100DSC(%)\nHSI UNet\nFig. 3.Top: box plots on DSCs of SpecTr and all competing methods. Bottom: quali-\ntative comparisons of cholangiocarcinoma segmentation. The spectrum of the image is\n625 mn. Numbers on the bottom are segmentation DSCs (%).\nnormalization obtains only 47 .35% in DSC. We also change 2D convolution in\nAttention UNet to 3D convolution, and obtains 68 .28% in DSC. The top ﬁgure\nin Fig. 3 shows comparison results by box plots. We also illustrate segmentation\nresults in the bottom ﬁgure of Fig. 3 for qualitative comparison. We can see that\ncompared with other methods, SpecTr can output more accurate segmentation\nresults, which are more robust to the complicated background.\n4.3 Discussion on Spectral Band Context\nFig. 4 (a) plots the average intensity values versus spectral band index for all\ncancer and normal regions in the testing set. The average intensity shows a dis-\ncriminative region around the 20th spectral band. In Fig. 4 (b), we visualize\na subset of the averaged attention heads which are sparse in the ﬁrst trans-\nformer (after down-sampling the spectrum by a factor of 2). In particular, head\n5 in the encoder self-attention layer becomes one of the sparsest head. Trans-\nformer successfully learns the attention spectra, i.e., the 5-10 spectral locations\nin normal-head8, cancer-head8 and cancer-head5 (corresponding to 10-20 spec-\ntral bands); 13-17 spectral locations in normal-head1 (26-34 spectral bands).\nThese two strongly correlated clusters are discriminative for segmentation. In-\nterestingly, in normal-head5, the 25-30 spectral locations (50-60 spectral bands)\nare also important, which suggests that though the intensity values within this\nrange is similar, they may still contain discriminative information. The ﬁnal seg-\nmentation masks are obtained via averaging probability maps from all spectral\nimages (see the purple arrow in Fig. 1), which enforces the above-mentioned at-\ntention heads in transformers to project all queries into the discriminative keys.\nThus, it leads to the vertical patterns in attention heads.\n8 B. Yun et al.\nCancer\nNormal\nHead 8, !=1.27Head 5, !=1.67Head 1, !=1.45\nCancerNormal\n(a) (b)Spectral band index0   5   10  15  20 25 30      0   5   10  15  20 25 30      0   5   10  15  20 25 30      \n302520 151050\n* * *\n302520 151050* *\n… …\n… …\n0 1020 3040 50600.100.150.200.250.300.350.400.450.50Relative Brightness\nFig. 4.(a) Plots of the intensity values versus spectral band index for ALL cancer and\nnormal regions in the testing set. (b) Visualization of learned attention heads.\nTable 2.Ablation on the design of SpecTr.\nDepthwise Conv Transformer Sparsity SN DSC (%)E2 E3 E4\n✓ ✓ ✓ ✓ ✓ ✓ 75.21\n✓ ✓ ✓ ✓ ✓ 72.79\n✓ ✓ ✓ ✓ 72.17\n✓ ✓ ✓ ✓ 72.39\n✓ ✓ ✓ ✓ 73.21\n✓ ✓ ✓ ✓ ✓ 70.66\n✓ ✓ ✓ ✓ ✓ 73.28\n✓ 70.40\n4.4 Ablation Study\nWe conduct ablation experiments to analyze the inﬂuence of diﬀerent designs and\ncomponents for SpecTr. As shown in Table 2, w/o depth-wise convolution means\nwe replace depth-wise convolution with 3D convolution in SpecTr. Transformer\n(E2-E4) indicates whether the transformer is added after the second, third, and\nfourth resolution layers in the encoder. We show the segmentation results of dif-\nferent variants. Using 3D convolution in the encoder leads to a performance drop\ncompared with depth-wise convolution. Using transformer E2 leads to 73 .21%,\nwhich is better than using transformer E3 or E4. This may due to the reason\nthat identiﬁcation of cancer areas in pathology images does not require highly\nsemantic information. Thus, the highly semantic information in spatial dimen-\nsion does not help too much in learning long-range context of the spectrum.\nBesides, Sparsity and spectral normalization are two vital schemes needed to be\nlearned with transformers to achieve better segmentation performances.\n5 Conclusion\nIn this paper, we present Spectral Transformer (SpecTr) for hyperspectral pathol-\nogy image segmentation, which employs transformers to learn the contextual fea-\nture across spectral bands. Two vital schemes are introduced to assist context\nlearning: (1) A sparsity scheme is adopted to learn context-dependent sparsity\nSpecTr: Spectral Transformer 9\npatterns, and improve the model’s performance and interpretability. (2) A spec-\ntral normalization method is proposed, to conduct separate normalization for the\nfeature map on each spectral location and eliminate the interference caused by\ndistribution mismatch among spectral images. We evaluated SpecTr on cholan-\ngiocarcinoma segmentation dataset. Experiments show the superiority of the\nproposed method for hyperspectral pathology image segmentation.\nReferences\n1. Chang, C., Du, Q., Sun, T., Althouse, M.L.G.: A joint band prioritization and\nband-decorrelation approach to band selection for hyperspectral image classiﬁca-\ntion. IEEE Trans. Geosci. Remote. Sens. 37(6), 2631–2641 (1999)\n2. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\nCoRR abs/2102.04306 (2021)\n3. C ¸ i¸ cek,¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-\nnet: Learning dense volumetric segmentation from sparse annotation. In: MICCAI\n(2016)\n4. Correia, G.M., Niculae, V., Martins, A.F.T.: Adaptively sparse transformers. In:\nInui, K., Jiang, J., Ng, V., Wan, X. (eds.) EMNLP-IJCNLP (2019)\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nICLR (2021)\n6. Lu, C., Mandal, M.: Toward automatic mitotic cell detection and segmentation in\nmultispectral histopathological images. IEEE J. Biomed. Health Informatics18(2),\n594–605 (2014)\n7. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. In: MIDL (2018)\n8. Ortega, S., Halicek, M.T., Fabelo, H., Guerra, R., L´ opez, C., Lejeune, M., Godtlieb-\nsen, F., Callic´ o, G.M., Fei, B.: Hyperspectral imaging and deep learning for the\ndetection of breast cancer cells in digitized histological images. In: Medical Imag-\ning: Digital Pathology. SPIE Proceedings, vol. 11320, p. 113200V (2020)\n9. Paoletti, M., Haut, J., Plaza, J., Plaza, A.: Deep learning classiﬁers for hyperspec-\ntral imaging: A review. ISPRS Journal of Photogrammetry and Remote Sensing\n158, 279–317 (2019)\n10. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: ICML (2018)\n11. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: MICCAI (2015)\n12. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Mobilenetv2: In-\nverted residuals and linear bottlenecks. In: CVPR (2018)\n13. Trajanovski, S., Shan, C., Weijtmans, P.J.C., Brouwer de Koning, S.G., Ruers,\nT.J.M.: Tongue tumor detection in hyperspectral images using deep learning se-\nmantic segmentation. IEEE Trans. Biomedical Engineering (2020)\n14. Valanarasu, J.M.J., Oza, P., Hacihaliloglu, I., Patel, V.M.: Medical transformer:\nGated axial-attention for medical image segmentation. CoRR abs/2102.10662\n(2021)\n10 B. Yun et al.\n15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: NIPS (2017)\n16. Wang, Q., Sun, L., Wang, Y., Zhou, M., Hu, M., Chen, J., Wen, Y., Li, Q.: Identi-\nﬁcation of melanoma from hyperspectral pathology image using 3d convolutional\nnetworks. IEEE Trans. Medical Imaging 40(1), 218–227 (2021)\n17. Zhang, Q., Li, Q., Yu, G., Sun, L., Zhou, M., Chu, J.: A multidimensional chole-\ndoch database and benchmarks for cholangiocarcinoma diagnosis. IEEE Access 7,\n149414–149421 (2019)\n18. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang,\nT., Torr, P.H.S., Zhang, L.: Rethinking semantic segmentation from a sequence-\nto-sequence perspective with transformers. CoRR abs/2012.15840 (2020)\n19. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep Learning in Medical Im-\nage Analysis and Multimodal Learning for Clinical Decision Support, pp. 3–11.\nSpringer (2018)"
}