{
  "title": "HRFormer: High-Resolution Transformer for Dense Prediction",
  "url": "https://openalex.org/W3206810688",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2374254801",
      "name": "Yuan, Yuhui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185291080",
      "name": "Fu Rao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347503218",
      "name": "Huang Lang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352737055",
      "name": "Lin Weihong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1851812986",
      "name": "Zhang Chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186661828",
      "name": "Chen Xi-lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350795573",
      "name": "Wang Jingdong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2965853874",
    "https://openalex.org/W2963741402",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3156109214",
    "https://openalex.org/W3203166921",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W3151130473",
    "https://openalex.org/W3212756788",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2295744361",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W3202511134",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3176892444",
    "https://openalex.org/W3034742259",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W3096653763",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2561196672",
    "https://openalex.org/W2953139137",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W3170227631",
    "https://openalex.org/W3203925315",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2969825080",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3117707723"
  ],
  "abstract": "We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Transformer that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet), along with local-window self-attention that performs self-attention over small non-overlapping image windows, for improving the memory and computation efficiency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the High-Resolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer parameters and $30\\%$ fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer.",
  "full_text": "HRFormer: High-Resolution Transformer for Dense\nPrediction\nYuhui Yuan124 Rao Fu1 Lang Huang3 Weihong Lin4 Chao Zhang3 Xilin Chen12\nJingdong Wang5∗\n1University of Chinese Academy of Sciences 2Institute of Computing Technology, CAS\n3Peking University 4Microsoft Research Asia 5Baidu\nAbstract\nWe present a High-Resolution Transformer (HRFormer) that learns high-resolution\nrepresentations for dense prediction tasks, in contrast to the original Vision Trans-\nformer that produces low-resolution representations and has high memory and\ncomputational cost. We take advantage of the multi-resolution parallel design\nintroduced in high-resolution convolutional networks (HRNet [46]), along with\nlocal-window self-attention that performs self-attention over small non-overlapping\nimage windows [21], for improving the memory and computation efﬁciency. In\naddition, we introduce a convolution into the FFN to exchange information across\nthe disconnected image windows. We demonstrate the effectiveness of the High-\nResolution Transformer on both human pose estimation and semantic segmentation\ntasks, e.g., HRFormer outperforms Swin transformer [ 27] by 1.3 AP on COCO\npose estimation with 50% fewer parameters and 30% fewer FLOPs. Code is\navailable at: https://github.com/HRNet/HRFormer.\n1 Introduction\nVision Transformer (ViT) [ 13] shows promising performance on ImageNet classiﬁcation tasks.\nMany follow-up works boost the classiﬁcation accuracy through knowledge distillation [42], adopting\ndeeper architecture [43], directly introducing convolution operations [16, 48], redesigning input image\ntokens [54], and etc. Besides, some studies attempt to extend the transformer to address broader\nvision tasks such as object detection [4], semantic segmentation [63, 37], pose estimation [51, 23],\nvideo understanding [61, 2, 30], and so on. This work focuses on the transformer for dense prediction\ntasks, including pose estimation and semantic segmentation.\nVision Transformer splits an image into a sequence of image patches of size16 ×16, and extracts the\nfeature representation of each image patch. Thus, the output representations of Vision Transformer\nlose the ﬁne-grained spatial details that are essential for accurate dense predictions. The Vision\nTransformer only outputs a single-scale feature representation, and thus lacks the capability to handle\nmulti-scale variation. To mitigate the loss of feature granularity and model the multi-scale variation,\nwe present High-Resolution Transformer (HRFormer) that contains richer spatial information and\nconstructs multi-resolution representations for dense predictions.\nThe High-Resolution Transformer is built by following the multi-resolution parallel design that is\nadopted in HRNet [ 46]. First, HRFormer adopts convolution in both the stem and the ﬁrst stage\nas several concurrent studies [ 11, 50] also suggest that convolution performs better in the early\nstages. Second, HRFormer maintains a high-resolution stream through the entire process with parallel\nmedium- and low-resolution streams helping boost high-resolution representations. With feature\nmaps of different resolutions, thus HRFormer is capable to model the multi-scale variation. Third,\n∗Corresponding author.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2110.09408v3  [cs.CV]  7 Nov 2021\nMHSA\nMHSA\nMHSA\nMHSA\n1 × 1 conv.\n3 × 3 DW conv.\n1 × 1 conv.\n(a)\n(b)\nFigure 1: Illustrating the HRFormer block. The HRFormer block is composed of (a) local-window self-\nattentionm and (b) feed-forward network (FFN) with depth-wise convolution. The local-window self-attention\nscheme is inspired by the interlaced sparse self-attention [56, 21].\nHRFormer mixes the short-range and long-range attention via exchanging multi-resolution feature\ninformation with the multi-scale fusion module.\nAt each resolution, the local-window self-attention mechanism is adopted to reduce the memory\nand computation complexity. We partition the representation maps into a set of non-overlapping\nsmall image windows and perform self-attention in each image window separately. This reduces\nthe memory and computation complexity from quadratic to linear with respect to spatial size. We\nfurther introduce a 3 ×3 depth-wise convolution into the feed-forward network (FFN) that follows\nthe local-window self-attention, to exchange information between the image windows which are\ndisconnected in the local-window self-attention process. This helps to expand the receptive ﬁeld and\nis essential for dense prediction tasks. Figure 1 shows the details of an HRFormer block.\nWe conduct experiments on image classiﬁcation, pose estimation, and semantic segmentation tasks,\nand achieve competitive performance on various benchmarks. For example, HRFormer-B gains\n+1.0% top-1 accuracy on ImageNet classiﬁcation over DeiT-B [ 42] with 40% fewer parameters\nand 20% fewer FLOPs. HRFormer-B gains 0.9% AP over HRNet-W48 [41] on COCO val set\nwith with 32% fewer parameters and 19% fewer FLOPs. HRFormer-B + OCR gains +1.2% and\n+2.0% mIoU over HRNet-W48 + OCR [55] with 25% fewer parameters and slightly more FLOPs on\nPASCAL-Context test and COCO-Stuff test, respectively.\n2 Related work\nVision Transformers. With the success of Vision Transformer (ViT) [ 13] and the data-efﬁcient\nimage transformer (DeiT) [42], various techniques are proposed to improve the ImageNet classiﬁca-\ntion accuracy of Vision Transformer [12, 43, 48, 16, 54, 17, 5, 27, 22, 40]. Among the very recent\nadvancements, the community has veriﬁed several effective improvements such as multi-scale feature\nhierarchies and incorporating convolutions.\nFor example, the concurrent works MViT [14], PVT [47], and Swin [27] introduce the multi-scale\nfeature hierarchies into transformer following the spatial conﬁguration of a typical convolutional\narchitecture such as ResNet-50. Different from them, our HRFormer incorporates the multi-scale fea-\nture hierarchies through exploiting the multi-resolution parallel design inspired by HRNet. CvT [48],\nCeiT [53], and LocalViT [25] propose to enhance the locality of transformer via inserting depth-wise\nconvolutions into either the self-attention or the FFN. The purpose of the inserted convolution within\nour HRFormer is different, apart from enhancing the locality, it also ensures information exchange\nacross the non-overlapping windows.\nSeveral previous studies [ 36, 19] have proposed similar local self-attention schemes for image\nclassiﬁcation. They construct the overlapped local windows following the strided convolution,\nresulting in heavy computation cost. Similar to [21, 44, 27], we propose to apply the local-window\nself-attention scheme to divide the input feature map into non-overlapping windows. Then we apply\nthe self-attention within each window independently so as to improve the efﬁciency signiﬁcantly.\nThere are several concurrently-developed works [63, 37] use the Vision Transformer to address the\ndense predict tasks such as semantic segmentation. They have shown that increasing the spatial\nresolution of the representations output by the Vision Transformer is important for semantic segmen-\n2\nconv.\nblock\ntrans.\nblock\nstrided\nconv.\nupsample\nFigure 2: Illustrating the High-Resolution Transformer architecture. The multi-resolution parallel trans-\nformer modules are marked with light blue color areas. Each module consists of multiple successive multi-\nresolution parallel transformer blocks. The ﬁrst stage is constructed with convolution block and the remained\nthree stages are constructed with transformer block.\ntation. Our HRFormer provides a different path to address the low-resolution problem of the Vision\nTransformer via exploiting the multi-resolution parallel transformer scheme.\nHigh-Resolution CNN for Dense Prediction. The high-resolution convolutional schemes have\nachieved great success on both pose estimation and semantic segmentation tasks. In the development\nof high-resolution convolutional neural networks, the community has developed three main paths in-\ncluding: (i) applying dilated convolutions to remove some down-sample layers [6, 52], (ii) recovering\nhigh-resolution representations from low-resolution representations with decoders [38, 1, 31, 32], and\n(iii) maintaining high-resolution representations throughout the network [46, 15, 39, 64, 45, 59, 20].\nOur HRFormer belongs to the third path, and retains the advantages of both vision transformer and\nHRNet [46].\n3 High-Resolution Transformer\nMulti-resolution parallel transformer. We follow the HRNet [46] design and start from a high-\nresolution convolution stem as the ﬁrst stage, gradually adding high-to-low resolution streams one by\none as new stages. The multi-resolution streams are connected in parallel. The main body consists of\na sequence of stages. In each stage, the feature representation of each resolution stream is updated\nwith multiple transformer blocks independently and the information across resolutions is exchanged\nrepeatedly with the convolutional multi-scale fusion modules.\nFigure 2 illustrates the overall HRFormer architecture. The design of convolutional multi-scale fusion\nmodules exactly follows HRNet. We illustrate the details of the transformer block in the following\ndiscussion and more details are presented in Figure 1.\nLocal-window self-attention. We divide the feature maps X ∈RN×D into a set of non-overlapping\nsmall windows: X →{X1, X2, ··· , XP }, where each window is of size K ×K. We perform\nmulti-head self-attention (MHSA) within each window independently. The formulation of multi-head\nself-attention on the p-th window is given as:\nMultiHead(Xp) = Concat[head(Xp)1, ··· , head(Xp)H] ∈RK2×D, (1)\nhead(Xp)h = Softmax\n[\n(XpWh\nq )(XpWh\nk)T\n√\nD/H\n]\nXpWh\nv ∈RK2×D\nH , (2)\nˆXp = Xp + MultiHead(Xp)Wo ∈RK2×D\nH , (3)\nwhere Wo ∈RD×D, Wh\nq ∈R\nD\nH ×D, Wh\nk ∈R\nD\nH ×D, and Wh\nv ∈R\nD\nH ×D for h ∈{1, ··· , H}.\nH represents the number of heads, D represents the number of channels, N represents the input\nresolutions, and ˆXp represents the output representation of MHSA. We also apply the relative position\nembedding scheme introduced in the T5 model [35] to incorporate the relative position information\ninto the local-window self-attention.\nWith MHSA aggregates information within each window, we merge them to compute the output\nXMHSA:\n{ˆX1, ˆX2, ··· , ˆXP }\nMerge\n−−−−→XMHSA. (4)\nThe left part of Figure 1 illustrates how local-window self-attention updates the 2D input representa-\ntions, where the multi-head self-attention operates within each window independently.\n3\nTable 1: The architecture conﬁguration of HRFormer. LSA: local-window self-attention, FFN-DW:\nfeed-forward network with a 3 × 3 depth-wise convolution, (M1, M2, M3, M4): the number of modules,\n(B1, B2, B3, B4): the number of blocks, (W1, W2, W3, W4): the size of windows, (H1, H2, H3, H4): the\nnumber of heads, (R1, R2, R3, R4): the MLP expansion ratios.\nRes. Stage 1 Stage 2 Stage 3 Stage 4\n4×\n\n\n1 ×1,64\n3 ×3,64\n1 ×1,256\n\n×B1×M1\n[ LSA,W1,H1\nFFN-DW,R1\n]\n×B2×M2\n[ LSA,W1,H1\nFFN-DW,R1\n]\n×B3×M3\n[ LSA,W1,H1\nFFN-DW,R1\n]\n×B4×M4\n8×\n[ LSA,W2,H2\nFFN-DW,R2\n]\n×B2×M2\n[ LSA,W2,H2\nFFN-DW,R2\n]\n×B3×M3\n[ LSA,W2,H2\nFFN-DW,R2\n]\n×B4×M4\n16×\n[ LSA,W3,H3\nFFN-DW,R3\n]\n×B3×M3\n[ LSA,W3,H3\nFFN-DW,R3\n]\n×B4×M4\n32×\n[ LSA,W4,H4\nFFN-DW,R4\n]\n×B4×M4\nTable 2: HRFormer instances. HRFormer-T, HRFormer-S, and HRFormer-B represents tiny, small, and base\nHRFormer model, respectively.\nModel #modules\n(M1, M2, M3, M4)\n#blocks\n(B1, B2, B3, B4)\n#channels\n(C1, C2, C3, C4)\n#heads\n(H1, H2, H3, H4)\nHRFormer-T (1,1,3,2) (2,2,2,2) (18,36,72,144) (1,2,4,8)\nHRFormer-S (1,1,4,2) (2,2,2,2) (32,64,128,256) (1,2,4,8)\nHRFormer-B (1,1,4,2) (2,2,2,2) (78,156,312,624) (2,4,8,16)\nFFN with depth-wise convolution. Local-window self-attention performs self-attention over the\nnon-overlapping windows separately. There is no information exchange across the windows. To\nhandle this issue, we add a 3 ×3 depth-wise convolution in between the two point-wise MLPs that\nform the FFN in Vision transformer: MLP(DW-Conv.(MLP())). The right part of Figure 1 shows\nan example of how FFN with 3 ×3 depth-wise convolution updates the 2D input representations.\nRepresentation head designs. As shown in Figure 2, the output of HRFormer consists of four\nfeature maps of different resolutions. We illustrate the details of the representation head designs for\ndifferent tasks as following: (i) ImageNet classiﬁcation, we send the four-resolution feature maps into\na bottleneck and the output channels are changed to 128, 256, 512, and 1024 respectively. Then, we\napply the strided convolutions to fuse them and output a feature map of the lowest resolution with2048\nchannels. Last, we apply a global average pooling operation followed by the ﬁnal classiﬁer. (ii) pose\nestimation, we only apply the regression head over the highest resolution feature map. (iii) semantic\nsegmentation, we apply the semantic segmentation head over the concatenated representations, which\nare computed by ﬁrst upsampling all the low-resolution representations to the highest resolution and\nthen concatenate them together.\nFigure 3: Illustrating that FFN with 3 × 3 depth-wise\nconvolution connects the non-overlapping windows.\nInstantiation. We illustrate the overall architec-\nture conﬁguration of HRFormer in Table 1. We\nuse (M1, M2, M3, M4) and (B1, B2, B3, B4)\nto represent the number of modules and the\nnumber of blocks of {state 1, stage 2, stage 3,\nstage4}, respectively. We use(C1, C2, C3, C4),\n(H1, H2, H3, H4) and (R1, R2, R3, R4) to rep-\nresent the number of channels, the number of\nheads and the MLP expansion ratios in trans-\nformer block associated with different resolu-\ntions. We keep the ﬁrst stage unchanged fol-\nlowing the original HRNet and use the bottle-\nneck as the basic building block. We apply the\ntransformer blocks in the other stages and each\ntransformer block consists of a local-window\nself-attention followed by an FFN with 3 ×3\n4\nFigure 4: Example results of HRFormer-B on COCO pose estimation val: containing occlusion,\nmultiple persons, viewpoint and appearance change.\nFigure 5: Example results of HRFormer-B + OCR on Cityscapes val (left one), COCO-Stuff test\n(middle two), and PASCAL-Context test (right two).\ndepth-wise convolution. We have not included the convolutional multi-scale fusion modules in Table 1\nfor simplicity. In our implementation, we set the size of the windows on four resolution streams\nas (7, 7, 7, 7) by default. Table 2 illustrates the conﬁguration details of three different HRFormer\ninstances with increasing complexities, where the MLP expansion ratios (R1, R2, R3, R4) are set as\n(4, 4, 4, 4) for all models and are not shown.\nAnalysis. The beneﬁts of 3 ×3 depth-wise convolution are twofold: one is enhancing the locality and\nthe other one is enabling the interactions across windows. We illustrate how the FFN with depth-wise\nconvolution is capable to expand the interactions beyond the non-overlapping local windows and\nmodel the relations between them in Figure 3. Therefore, based on the combination of the local-\nwindow self-attention and the FFN with 3 ×3 depth-wise convolution, we can build the HRFormer\nblock that improves the memory and computation efﬁciency signiﬁcantly.\n4 Experiments\n4.1 Human Pose Estimation\nTraining setting. We study the performance of HRFormer on the COCO [26] human pose estimation\nbenchmark, which contains more than 200K images and 250K person instances labeled with 17\nkeypoints. We train our model on COCO train 2017 dataset, including 57K images and 150K\nperson instances. We evaluate our approach on the val 2017 set and test-dev 2017, containing 5K\nimages and 20K images, respectively.\nWe follow most of the default training and evaluation settings of mmpose [8]2, and change the\noptimizer from Adam to AdamW. For the training batch size, we choose256 for HRFormer-T and\nHRFormer-S and 128 for HRFormer-B due to limited GPU memory. Each HRFormer experiment on\nCOCO pose estimation task takes 8×32G-V100 GPUs.\nResults. Table 3 reports the comparisons on COCO val set. We compare HRFormer to the\nrepresentative convolutional method such as HRNet [41] and several recent transformer methods,\nincluding PRTR [23], TransPose-H-A6 [51], and TokenPose-L/D24 [24]. HRFormer-B gains 0.9%\nwith 32% fewer parameters and 19% fewer FLOPs when compared to HRNet-W48 with an input\nsize of 384 ×288. Therefore, our HRFormer-B already achieves 77.2% w/o using any advanced\ntechniques such as UDP [20] and DARK[59]. We believe that our HRFormer-B could achieve better\nresults by exploiting either UDP or DARK scheme. We also report the comparisons on COCO\ntest-dev set in Table 4. Our HRFormer-B outperforms HRNet-W48 by around 0.7% with fewer\nparameters and FLOPs. Figure 4 shows some example results of human pose estimation on COCO\nval set.\n2https://github.com/open-mmlab/mmpose, Apache License 2.0\n5\nTable 3: Comparison on the COCO pose estimation val set. The number of parameters and\nFLOPs for the pose estimation network are measured w/o considering neither human detection nor\nkeypoint grouping. All results are based on ImageNet pretraining. −means the numbers are not\nprovided in the original paper.\nMethod input size #param. FLOPs AP AP 50 AP75 APM APL AR\nHRNet-W32 [41] 256 × 192 28 .5M 7.1G 74.4 90 .5 81 .9 70 .8 81 .0 78 .9\nHRNet-W32 [41] 384 × 288 28 .5M 16.0G 75.8 90 .6 82 .7 71 .9 82 .8 81 .0\nHRNet-W48 [41] 256 × 192 63 .6M 14.6G 75.1 90 .6 82 .2 71 .5 81 .8 80 .4\nHRNet-W48 [41] 384 × 288 63 .6M 32.9G 76.3 90 .8 82 .9 72 .3 83 .4 81 .2\nPRTR [23] 512 × 384 57 .2M 37.8G 73.3 89 .2 79 .9 69 .0 80 .9 80 .2\nTransPose-H-A6 [51] 256 × 192 17 .5M 21.8G 75.8 − − − − 80.8\nTokenPose-L/D24 [24] 256 × 192 27 .5M 11.0G 75.8 90 .3 82 .5 72 .3 82 .7 80 .9\nHRFormer-T 256 × 192 2 .5M 1.3G 70.9 89 .0 78 .4 67 .2 77 .8 76 .6\nHRFormer-T 384 × 288 2 .5M 1.8G 72.4 89 .3 79 .0 68 .2 79 .7 77 .9\nHRFormer-S 256 × 192 7 .8M 2.8G 74.0 90 .2 81 .2 70 .4 80 .7 79 .4\nHRFormer-S 384 × 288 7 .8M 6.2G 75.6 90 .3 82 .2 71 .6 82 .5 80 .7\nHRFormer-B 256 × 192 43 .2M 12.2G 75.6 90 .8 82 .8 71 .7 82 .6 80 .8\nHRFormer-B 384 × 288 43 .2M 26.8G 77.2 91 .0 83 .6 73 .2 84 .2 82 .0\nTable 4: Comparison on the COCO pose estimation test-dev set. The number of parameters\nand FLOPs for the pose estimation network are measured w/o considering neither human detection\nnor keypoint grouping. All results are based on ImageNet pretraining.\nMethod input size #param. FLOPs AP AP 50 AP75 APM APL AR\nHRNet-W32 [41] 384 × 288 28 .5M 16.0G 74.9 92 .5 82 .8 71 .3 80 .9 80 .1\nHRNet-W48 [41] 384 × 288 63 .6M 32.9G 75.5 92 .5 83 .3 71 .9 81 .5 80 .5\nPRTR [23] 512 × 384 57 .2M 37.8G 72.1 90 .4 79 .6 68 .1 79 .0 79 .4\nTransPose-H-A6 [51] 256 × 192 17 .5M 21.8G 75.0 92 .2 82 .3 71 .3 81 .1 −\nTokenPose-L/D24 [24] 384 × 288 29 .8M 22.1G 75.9 92 .3 83 .4 72 .2 82 .1 80 .8\nHRFormer-S 384 × 288 7 .8M 6.2G 74.5 92 .3 82 .1 70 .7 80 .6 79 .8\nHRFormer-B 384 × 288 43 .2M 26.8G 76.2 92 .7 83 .8 72 .5 82 .3 81 .2\n4.2 Semantic Segmentation\nCityscapes. The Cityscapes dataset [ 9] is for urban scene understanding. There are a total of 30\nclasses and only 19 classes are used for parsing evaluation. The dataset contains 5K high-quality\npixel-level ﬁnely annotated images and 20K coarsely annotated images. The ﬁnely annotated 5K\nimages are divided into 2, 975 train images, 500 val images and 1, 525 test images. We set the\ninitial learning rate as 0.0001, weight decay as 0.01, crop size as 1024 ×512, batch size as 8, and\ntraining iterations as 80K by default. Each HRFormer + OCR experiment on Cityscapes takes 8×\n32G-V100 GPUs.\nTable 5 reports the results on Cityscapes val. We choose to use HRFormer + OCR as our semantic\nsegmentation architecture. We compare our method with several well-known Vision Transformer\nbased methods [63, 37] and CNN based methods [6, 62, 55]. Speciﬁcally, SETR-PUP and SETR-\nMLA use the ViT-Large [13] as the backbone. DPT-Hybrid uses the ViT-Hybrid [13] that consists of\na ResNet-50 followed by 12 transformer layers. Both ViT-Large and ViT-Hybrid are initialized with\nthe weights pre-trained on ImageNet-21K, where both of them achieve around 85.1% top1 accuracy\non ImageNet. DeepLabv3 [6] and PSPNet [62] are based on dilated ResNet-101 with output stride 8.\nAccording to the fourth column of Table 5, HRFormer + OCR achieves competitive performance\noverall. For example, HRFormer-B + OCR achieves comparable performance with SETR-PUP while\nsaving 70% parameters and 50% FLOPs.\n6\nTable 5: Comparison with the recent SOTA on semantic segmentation tasks. We report the\nmIoUs on Cityscapes val, PASCAL-Context test, COCO-Stuff test, and ADE 20K val. The\nnumber of parameters and FLOPs are measured on the image size of 1024 ×1024, and the output\nlabel map size of 19 ×1024 ×1024. All results are evaluated with multi-scale testing. ‡: the results\nare obtained with extra pre-training on ADE20K.\nMethod #params. FLOPs Cityscapes PASCAL-Context COCO-Stuff ADE20K\nTransformer backbone\nSETR-PUP [63] 317.8M 2326.7G 82.2 55 .3 − 50.1\nSETR-MLA [63] 309.5M 2138.6G − 55.8 − 50.3\nSwin-S + UperNet [27] 81.16M 1036.50G − − − 49.5\nSwin-B + UperNet [27] 121.18M 1187.90G − − − 49.7\nPVT-Large + Semantic FPN [47] 65.1M −G − − − 43.5\nCNN backbone\nDeeplabv3 [7] 87.1M 1394.0G 80.7 54 .1 − −\nPSPNet [62] 68.0M 1028.8G 80.0 54 .0 43 .3 −\nHRNet-W48 + OCR [55] 74.5M 924.7G − 56.2 40 .5 45 .7\nCNN+Transformer backbone\nDPT-Hybrid [37] 124.0M 1231.5G − 60.5‡ − 49.0\nHRFormer-B + OCR 56.2M 1119.9G 82.6 58 .5 43 .3 50 .0\nHRFormer-B + OCR + SegFix [57] 56.2M 1119.9G 83.2 − − −\nPASCAL-Context. The PASCAL-Context dataset [29] is a challenging scene parsing dataset that\ncontains 59 semantic classes and 1 background class. The train set and test set consist of 4, 998\nand 5, 105 images respectively. We set the initial learning rate as 0.0001, weight decay as 0.01,\ncrop size as 520 ×520, batch size as 16, and training iterations as 60K by default. We report the\ncomparisons on the ﬁfth column of Table 5. Accordingly, HRFormer-B + OCR gains 1.1%, 1.5%\nover HRNet-W48 + OCR, SETR-MLA with fewer parameters and FLOPs, respectively. Notably,\nDPT-Hybrid achieves the best performance through extra pre-training the models on ADE20K in\nadvance. Each HRFormer + OCR experiment on PASCAL-Context takes 8×32G-V100 GPUs.\nCOCO-Stuff. The COCO-Stuff dataset [3] is a challenging scene parsing dataset that contains 171\nsemantic classes. The train set and test set consist of 9K and 1K images respectively. We set the\ninitial learning rate as 0.0001, weight decay as 0.01, crop size as 520 ×520, batch size as 16, and\ntraining iterations as 60K by default. We report the comparisons on the last column of Table 5 and\nHRFormer-B + OCR outperforms the previous best-performing HRNet-W48 + OCR by nearly 2%.\nEach HRFormer + OCR experiment on COCO-Stuff takes 8×32G-V100 GPUs. Figure 5 shows\nsome example results on Cityscapes, PASCAL-Context, and COCO-Stuff.\n4.3 ImageNet Classiﬁcation\nTraining setting. We conduct the comparisons on ImageNet-1K, which consists of 1.28M train\nimages and 50K val images with 1000 classes. We train all models with batch size 1024 for 300\nepochs with AdamW [ 28] optimizer, cosine decay learning rate schedule, weight decay as 0.05,\nand a bag of augmentation policies, including rand augmentation [ 10], mixup [ 60], cutmix [ 58],\nand so on. HRFormer-T and HRFormer-S require 8 ×32G-V100 GPUs and HRFormer-B requires\n32 ×32G-V100 GPUs.\nResults. We compare HRFormer to some representative CNN methods and vision transformer\nmethods in Table 6, where all methods are trained on ImageNet-1K only. The results of ViT-Large\nwith larger dataset such as ImageNet-21K not included for fairness. According to Table 6, HRFormer\nachieves competitive performance. For example, HRFormer-B gains 1.0% over DeiT-B while saving\nnearly 40% parameters and 20% FLOPs.\n7\nTable 6: Comparisons on ImageNet-1K val.\nMethod image size #param. FLOPs Top-1 acc.\nResNet-18 [18] 224 × 224 11 M 1.8G 69.8\nResNet-50 [18] 224 × 224 26 M 4.1G 78.5\nResNet-101 [18] 224 × 224 45 M 7.9G 79.8\nHRNet-W18 [46] 224 × 224 21 .3M 4.0G 76.8\nHRNet-W32 [46] 224 × 224 41 .2M 8.3G 78.5\nHRNet-W48 [46] 224 × 224 77 .5M 16.1G 79.3\nRegNetY-4G [34] 224 × 224 21 M 4.0G 80.0\nRegNetY-8G [34] 224 × 224 39 M 8.0G 81.7\nRegNetY-16G [34] 224 × 224 84 M 16.0G 82.9\nViT-B/16 [13] 224 × 224 86 M 55.4G 77.9\nViT-L/16 [13] 224 × 224 307 M 190.7G 76.5\nDeiT-T [42] 224 × 224 5 M 1.3G 72.2\nDeiT-S [42] 224 × 224 22 M 4.6G 79.8\nDeiT-B [42] 224 × 224 86 M 17.5G 81.8\nDeiT-B\n⚗ [42] 384 × 384 86 M 55.4G 83.4\nConformer-T [33] 224 × 224 23 .5M 5.2G 81.3\nConformer-S [33] 224 × 224 37 .7M 10.6G 83.4\nConformer-B [33] 224 × 224 83 .3M 23.3G 84.1\nPVT-T [47] 224 × 224 13 .2M 1.9G 75.1\nPVT-S [47] 224 × 224 24 .5M 3.8G 79.8\nPVT-M [47] 224 × 224 44 .2M 6.7G 81.2\nPVT-L [47] 224 × 224 61 .4M 9.8G 81.7\nSwin-T [27] 224 × 224 29 M 4.5G 81.3\nSwin-S [27] 224 × 224 50 M 8.7G 83.0\nSwin-B [27] 224 × 224 88 M 15.4G 83.5\nSwin-B [27] 384 × 384 88 M 47G 84.5\nHRFormer-T 224 × 224 8 .0M 1.8G 78.5\nHRFormer-S 224 × 224 13 .5M 3.6G 81.2\nHRFormer-B 224 × 224 50 .3M 13.7G 82.8\nTable 7: Study of the 3×3 depth-wise convolution in FFN. We report the top1 acc., mIoU, and AP\non ImageNet val, PASCAL-Context test, and COCO pose estimation val, respectively. Results on\nPASCAL-Context are evaluated with single-scale testing. The number of parameters and FLOPs are\nmeasured on ImageNet.\nMethod #param. FLOPs ImageNet PASCAL-Context COCO\nFFN w/o 3×3 DW-Conv. 7.9M 1.76G 77.83 46 .84 66 .88\nFFN w/ 3× 3 DW-Conv. 8.0M 1.83G 78.48 49 .74 70 .92\n4.4 Ablation Experiments\nInﬂuence of 3 ×3 depth-wise convolution within FFN We study the inﬂuence of the 3 ×3 depth-\nwise convolution within FFN based on HRFormer-T in Table 7. We observe that applying 3 ×3\ndepth-wise convolution in FFN signiﬁcantly improves the performance on multiple tasks, including\nImageNet classiﬁcation, PASCAL-Context segmentation, and COCO pose estimation. For example,\nHRFormer-T + FFN w/ 3×3 depth-wise convolution outperforms HRFormer-T + FFN w/o 3×3\ndepth-wise convolution by 0.65%, 2.9% and 4.04% on ImageNet, PASCAL-Context and COCO,\nrespectively.\nInﬂuence of shifted window scheme & 3×3 depth-wise convolution within FFN based on Swin-\nT. We compare our method with the shifted windows scheme of Swin transformer [27] in Table 8.\nFor fair comparisons, we construct a Intra-Window transformer architecture following the same\n8\nTable 8: Inﬂuence of shifted window scheme & 3×3 depth-wise convolution within FFN based on\nSwin-T.\nMethod 3× 3 depth-wise convolution in FFN #param. FLOPs ImageNet top1 acc.\nSwin-T \u0017 28.3M 4.5G 81.3\nSwin-T \u0013 28.5M 4.6G 82.2\nIntraWin-T \u0017 28.3M 4.5G 80.2\nIntraWin-T \u0013 28.5M 4.6G 82.3\nTable 9: Shifted window scheme v.s. 3×3 depth-wise convolution within FFN based on HRFormer-T.\nshifted\nwindow scheme\n3×3 depth-wise\nconvolution within FFN #param. FLOPs ImageNet\ntop1 acc.\nPASCAL-Context\nmIoU\nCOCO\nAP\n\u0017 \u0013 8.0M 1.8G 78.5 49 .7 70 .9\n\u0013 \u0017 7.9M 1.6G 76.6 43 .3 67 .3\narchitecture conﬁgurations of Swin-T [ 27] except that we do not apply shifted windows scheme.\nWe see that applying 3×3 depth-wise convolution within FFN improves both Swin-T and Intrawin-\nT. Surprisingly, when equipped with 3 ×3 depth-wise convolution within FFN, Intrawin-T even\noutperforms Swin-T.\nShifted window scheme v.s. 3 ×3 depth-wise convolution within FFN based on HRFormer-\nT. In Table 9, we compare the 3 ×3 depth-wise convolution within FFN scheme to the shifted\nwindow scheme based on HRFormer-T. According to the results, we see that applying 3×3 depth-\nwise convolution within FFN signiﬁcantly outperforms applying shifted window scheme across all\ndifferent tasks.\nComparison to ViT, DeiT & Swin on pose estimation. We report the COCO pose estimation\nresults based on the two well-known transformer models, including ViT-Large [13], DeiT-B\n⚗ [42] and\nSwin-B [27] in Table 10. Notably, both ViT-Large and Swin-B‡are pre-trained on ImageNet21K in\nadvance and then ﬁnetuned on ImageNet1K and achieve85.1% and 86.4% top-1 accuracy respectively.\nDeiT-B\n⚗ is trained on ImageNet 1K for 1000 epochs and achieves 85.2% top-1 accuracy. We\napply deconvolution modules to upsample the output representations of the encoder following the\nSimpleBaseline [49] for three methods. The number of parameters and FLOPs are listed on the fourth\nand ﬁfth columns of Table 10. According to the results in Table 10, we see that our HRFormer-B\nachieves better performance than all three methods with fewer parameters and FLOPs.\nComparison to HRNet. We compare our HRFormer to the convolutional HRNet with almost the\nsame architecture conﬁgurations via replacing all the transformer blocks with the conventional basic\nblock consisting of two 3 ×3 convolutions. Table 11 shows the comparison results on ImageNet,\nPASCAL-Context, and COCO. We observe that HRFormer signiﬁcantly outperforms HRNet under\nvarious conﬁgurations with much less model and computation complexity. For example, HRFormer-T\noutperforms HRNet-T by 2.0%, 1.5%, and 1.6% on three tasks while requiring only around 50%\nparameters and FLOPs, respectively. In summary, HRFormer achieves better performance via\nexploiting the beneﬁts of transformers such as content-dependent dynamic interactions.\n5 Conclusion\nIn this work, we present the High-Resolution Transformer (HRFormer), a simple yet effective trans-\nformer architecture, for dense prediction tasks, including pose estimation and semantic segmentation.\nThe key insight is to integrate the HRFormer block, which combines local-window self-attention\nand FFN with depth-wise convolution to improve the memory and computation efﬁciency, with\nthe multi-resolution parallel design of the convolutional HRNet. Besides, HRFormer also beneﬁts\nfrom adopting convolution in the early stages and mixing short-range and long-range attention with\nmulti-scale fusion scheme. We empirically verify the effectiveness of our HRFormer on both pose\nestimation and semantic segmentation tasks.\n9\nTable 10: Comparisons to ViT & DeiT on COCO pose estimation val. ‡marks the methods\npretrained on ImageNet-22K.\nMethod image size #param. FLOPs COCO\nViT-Large‡ 256 × 192 308 .5M 60.1G 69.2\nDeiT-B\n⚗ 256 × 192 90 .0M 17.9G 69.0\nSwin-B‡ 256 × 192 93 .2M 17.6G 74.3\nHRFormer-B 256 × 192 43 .2M 12.2G 75.6\nTable 11: Comparisons to HRNet. We report the top1 acc., mIoU, and AP on ImageNet val,\nPASCAL-Context test, and COCO pose estimation val, respectively. Results on PASCAL-Context\nare based on single-scale testing. The number of parameters and FLOPs are measured on ImageNet.\nMethod #param. FLOPs ImageNet PASCAL-Context COCO\nHRNet-T 15.6M 2.7G 76.5 47 .8 69 .3\nHRFormer-T 8.0M 1.8G 78.5 49 .3 70 .9\nHRNet-S 24.5M 5.0G 78.7 52 .3 73 .1\nHRFormer-S 13.5M 3.6G 81.2 53 .8 74 .0\nHRNet-B 85.3M 20.3G 81.4 55 .2 75 .1\nHRFormer-B 50.3M 13.7G 82.8 58 .5 75 .6\n6 Appendix\nMore Visualization Results. We present additional visualizations of the example results of our\nmethod on both pose estimation and semantic segmentation tasks.\nFigure 6 shows more pose estimation results of HRFormer-B on COCO val. Figure 7 shows more\nsemantic segmentation results on Cityscapes val, PASCAL-Context test and COCO-Stuff test.\nAblation of window sizes. We report the results with different window sizes at different resolutions\non semantic segmentation tasks and we will add more results if necessary. We use(W1, W2, W3, W4)\nto represent the window sizes associated with feature maps with different resolutions with stride\n4, 8, 16, 32. We choose larger window sizes for higher resolution branches, thus, we have W1 >\nW2 > W3 > W4. According to these results, we can see that applying larger windows improves the\nperformance, and applying different window sizes at different resolutions makes no big difference.\nTable 12: Inﬂuence of the size of windows (W1, W2, W3, W4) in HRFormer-B on PASCAL\nContext.\nMethod (W1, W2, W3, W4) #param. FLOPs ss result (ms result)\nHRFormer-B + OCR\n(7,7,7,7) 56.0M 1051G 56.3(57.3)\n(9,9,9,9) 56.0M 1064G 57.4(58.5)\n(11,11,11,11) 56.1M 1069G 56.6(57.6)\n(13,13,13,13) 56.1M 1083G 57.0(58.1)\n(15,15,15,15) 56.2M 1120G 57.5(58.5)\n(15,13,11,9) 56.1M 1094G 56.9(57.9)\n(21,17,13,9) 56.2M 1148G 56.9(57.9)\n(17,15,13,11) 56.2M 1113G 57.5(58.5)\nReferences\n[1] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional\nencoder-decoder architecture for image segmentation. PAMI, 39(12):2481–2495, 2017.\n[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\nvideo understanding? In ICML, July 2021.\n10\nFigure 6: Visualization of the pose estimation results based on HRFormer-B on COCO val.\n[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in\ncontext. In CVPR, 2018.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213–229,\n2020.\n[5] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. arXiv preprint arXiv:2103.14899, 2021.\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE transactions on pattern analysis and machine intelligence,\n40(4):834–848, 2017.\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\n[8] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://\ngithub.com/open-mmlab/mmpose, 2020.\n[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In CVPR, 2016.\n[10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical\nautomated data augmentation with a reduced search space. In CVPRW, pages 702–703, 2020.\n[11] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and\nattention for all data sizes. arXiv preprint arXiv:2106.04803, 2021.\n[12] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent\nSagun. Convit: Improving vision transformers with soft convolutional inductive biases. arXiv\npreprint arXiv:2103.10697, 2021.\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[14] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104.11227,\n2021.\n[15] Damien Fourure, Rémi Emonet, Élisa Fromont, Damien Muselet, Alain Trémeau, and Christian\nWolf. Residual conv-deconv grid network for semantic segmentation. In BMVC, 2017.\n11\n[16] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve\nJegou, and Matthijs Douze. Levit: A vision transformer in convnet’s clothing for faster inference.\nIn ICCV, pages 12259–12269, October 2021.\n[17] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in\ntransformer. In NeurIPS, 2021.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, pages 770–778, 2016.\n[19] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image\nrecognition. In ICCV, pages 3464–3473, 2019.\n[20] Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. The devil is in the details: Delving into\nunbiased data processing for human pose estimation. In CVPR, June 2020.\n[21] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang.\nInterlaced sparse self-attention for semantic segmentation. arXiv preprint arXiv:1907.12273,\n2019.\n[22] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, and Jiashi Feng.\nToken labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on\nimagenet. arXiv preprint arXiv:2104.10858, 2021.\n[23] Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose recognition\nwith cascade transformers. In CVPR, pages 1944–1953, 2021.\n[24] Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou Yang, Shu-Tao Xia, and Erjin\nZhou. Tokenpose: Learning keypoint tokens for human pose estimation. arXiv preprint\narXiv:2104.03516, 2021.\n[25] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing\nlocality to vision transformers. arXiv preprint arXiv:2104.05707, 2021.\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV,\n2014.\n[27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021.\n[28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[29] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler,\nRaquel Urtasun, and Alan Yuille. The role of context for object detection and semantic\nsegmentation in the wild. In CVPR, 2014.\n[30] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network.\narXiv preprint arXiv:2102.00719, 2021.\n[31] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose\nestimation. In ECCV, pages 483–499, 2016.\n[32] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose\nestimation. In ECCV, pages 483–499, 2016.\n[33] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei Wang, Jianbin Jiao, and Qixiang\nYe. Conformer: Local features coupling global representations for visual recognition. arXiv\npreprint arXiv:2105.03889, 2021.\n[34] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Design-\ning network design spaces. In CVPR, pages 10428–10436, 2020.\n[35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. JMLR, 21(140):1–67, 2020.\n[36] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and\nJonathon Shlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909,\n2019.\n12\n[37] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.\nIn ICCV, pages 12179–12188, October 2021.\n[38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In MICCAI, pages 234–241, 2015.\n[39] Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. NeurIPS, 29:4053–4061,\n2016.\n[40] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish\nVaswani. Bottleneck transformers for visual recognition. In CVPR, pages 16519–16529, 2021.\n[41] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning\nfor human pose estimation. In CVPR, pages 5693–5703, 2019.\n[42] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[43] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.\nGoing deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.\n[44] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and\nJonathon Shlens. Scaling local self-attention for parameter efﬁcient visual backbones. In CVPR,\npages 12894–12904, 2021.\n[45] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning\nfor visual recognition. PAMI, 2020.\n[46] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep high-resolution\nrepresentation learning for visual recognition. PAMI, 2019.\n[47] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[48] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[49] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and\ntracking. In ECCV, 2018.\n[50] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollár, and Ross Girshick. Early\nconvolutions help transformers see better. arXiv preprint arXiv:2106.14881, 2021.\n[51] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Transpose: Towards explainable human\npose estimation by transformer. arXiv preprint arXiv:2012.14214, 2020.\n[52] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv\npreprint arXiv:1511.07122, 2015.\n[53] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating\nconvolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021.\n[54] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and\nShuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet.\narXiv preprint arXiv:2101.11986, 2021.\n[55] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic\nsegmentation. arXiv preprint arXiv:1909.11065, 2019.\n[56] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet:\nObject context network for scene parsing. arXiv preprint arXiv:1809.00916, 2018.\n[57] Yuhui Yuan, Jingyi Xie, Xilin Chen, and Jingdong Wang. Segﬁx: Model-agnostic boundary\nreﬁnement for segmentation. In ECCV, pages 489–506. Springer, 2020.\n[58] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon\nYoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In\nICCV, pages 6023–6032, 2019.\n13\n[59] Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce Zhu. Distribution-aware coordinate\nrepresentation for human pose estimation. In CVPR, pages 7093–7102, 2020.\n[60] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n[61] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng Gao.\nMulti-scale vision longformer: A new vision transformer for high-resolution image encoding.\nICCV 2021, 2021.\n[62] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In CVPR, pages 2881–2890, 2017.\n[63] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021.\n[64] Yisu Zhou, Xiaolin Hu, and Bo Zhang. Interlinked convolutional neural networks for face\nparsing. In ISNN, pages 222–231, 2015.\n14\nFigure 7: Visualization of the semantic segmentation results based on HRFormer-B + OCR on\nCityscapes val, PASCAL-Context test, and COCO-Stuff test.\n15",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7465100288391113
    },
    {
      "name": "Computer science",
      "score": 0.7436566352844238
    },
    {
      "name": "FLOPS",
      "score": 0.68455570936203
    },
    {
      "name": "Computation",
      "score": 0.6602872610092163
    },
    {
      "name": "Segmentation",
      "score": 0.5674668550491333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5280167460441589
    },
    {
      "name": "High resolution",
      "score": 0.472078800201416
    },
    {
      "name": "Computer vision",
      "score": 0.34296125173568726
    },
    {
      "name": "Algorithm",
      "score": 0.26563262939453125
    },
    {
      "name": "Parallel computing",
      "score": 0.2389203906059265
    },
    {
      "name": "Engineering",
      "score": 0.09043088555335999
    },
    {
      "name": "Voltage",
      "score": 0.08588555455207825
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Remote sensing",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}