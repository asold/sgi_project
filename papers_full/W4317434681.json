{
    "title": "When Geometric Deep Learning Meets Pretrained Protein Language Models",
    "url": "https://openalex.org/W4317434681",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2001693504",
            "name": "Fang Wu",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A2097366902",
            "name": "Jinbo Xu",
            "affiliations": [
                "Toyota Technological Institute at Chicago"
            ]
        },
        {
            "id": "https://openalex.org/A2913816252",
            "name": "Dragomir Radev",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2109255866",
            "name": "Yu Tao",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1976666280",
        "https://openalex.org/W2948990653",
        "https://openalex.org/W3185087522",
        "https://openalex.org/W3186179742",
        "https://openalex.org/W2161062388",
        "https://openalex.org/W3005407385",
        "https://openalex.org/W4225405705",
        "https://openalex.org/W3002639525",
        "https://openalex.org/W2099438806",
        "https://openalex.org/W1512387364",
        "https://openalex.org/W2114850508",
        "https://openalex.org/W2784883284",
        "https://openalex.org/W2301595689",
        "https://openalex.org/W2110505462",
        "https://openalex.org/W3110563343",
        "https://openalex.org/W3202105508",
        "https://openalex.org/W2914721378",
        "https://openalex.org/W2751808960",
        "https://openalex.org/W3036737467",
        "https://openalex.org/W2606780347",
        "https://openalex.org/W3105738938",
        "https://openalex.org/W2171641243",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2995514860",
        "https://openalex.org/W3099688644",
        "https://openalex.org/W2957874522",
        "https://openalex.org/W2617750324",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2905812122",
        "https://openalex.org/W3199799076",
        "https://openalex.org/W2969996838",
        "https://openalex.org/W3042283064",
        "https://openalex.org/W2114162221",
        "https://openalex.org/W3083386021",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W2920053903",
        "https://openalex.org/W3015490653",
        "https://openalex.org/W2594725344",
        "https://openalex.org/W2895234710",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2583907533",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W2943495267",
        "https://openalex.org/W4210997669",
        "https://openalex.org/W3131204112",
        "https://openalex.org/W2527566079",
        "https://openalex.org/W2980272550",
        "https://openalex.org/W3154275519",
        "https://openalex.org/W3213430222",
        "https://openalex.org/W4221096048",
        "https://openalex.org/W3134992635",
        "https://openalex.org/W2534288757",
        "https://openalex.org/W2170463736",
        "https://openalex.org/W1030883578",
        "https://openalex.org/W2092285329",
        "https://openalex.org/W3113626021",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2059117301",
        "https://openalex.org/W2117451312",
        "https://openalex.org/W4223581484",
        "https://openalex.org/W3108423942",
        "https://openalex.org/W3166372986",
        "https://openalex.org/W2951690294",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W4281478251",
        "https://openalex.org/W2953524386",
        "https://openalex.org/W3155206330",
        "https://openalex.org/W4281626490",
        "https://openalex.org/W3010387158",
        "https://openalex.org/W2739999456",
        "https://openalex.org/W3203710967",
        "https://openalex.org/W3164046276",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4285483966",
        "https://openalex.org/W4287828570",
        "https://openalex.org/W4287325738",
        "https://openalex.org/W4280543433",
        "https://openalex.org/W3081836708",
        "https://openalex.org/W4288419263",
        "https://openalex.org/W3196791236",
        "https://openalex.org/W2909727437",
        "https://openalex.org/W4221147818",
        "https://openalex.org/W3208574069",
        "https://openalex.org/W4221149941",
        "https://openalex.org/W4287724045",
        "https://openalex.org/W4366001157",
        "https://openalex.org/W4296060337",
        "https://openalex.org/W3037888463",
        "https://openalex.org/W4281614398",
        "https://openalex.org/W2788775653",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W4225726305",
        "https://openalex.org/W4206367183",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W4287586570"
    ],
    "abstract": "Abstract Geometric deep learning has recently achieved great success in non-Euclidean domains, and learning on 3D structures of large biomolecules is emerging as a distinct research area. However, its efficacy is largely constrained due to the limited quantity of structural data. Meanwhile, protein language models trained on substantial 1D sequences have shown burgeoning capabilities with scale in a broad range of applications. Nevertheless, no preceding studies consider combining these different protein modalities to promote the representation power of geometric neural networks. To address this gap, we make the foremost step to integrate the knowledge learned by well-trained protein language models into several state-of-the-art geometric networks. Experiments are evaluated on a variety of protein representation learning benchmarks, including protein-protein interface prediction, model quality assessment, protein-protein rigid-body docking, and binding affinity prediction, leading to an overall improvement of 20\\% over baselines and the new state-of-the-art performance. Strong evidence indicates that the incorporation of protein language models' knowledge enhances geometric networks' capacity by a significant margin and can be generalized to complex tasks.",
    "full_text": "When Geometric Deep Learning Meets Pretrained\nProtein Language Models\nFang Wu  (  fw2359@columbia.edu )\nColumbia University https://orcid.org/0000-0001-7240-3915\nJinbo Xu  (  jinboxu@gmail.com )\nToyota Technological Institute at Chicago https://orcid.org/0000-0001-7111-4839\nDragomir Radev  (  dragomir.radev@yale.edu )\nYale University\nYu Tao  (  taoyu928@sjtu.edu.cn )\nShanghai Jiao Tong University\nBrief Communication\nKeywords:\nDOI: https://doi.org/10.21203/rs.3.rs-2469268/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nSpringer Nature 2023 L ATEX template\nWhen Geometric Deep Learning Meets\nPretrained Protein Language Models\nF ang W u1†, Y u T ao3†, Dragomir Radev2 and Jinbo Xu1,4*\n1Institute of AI Industry Research, T singhua University , Haidian\nStreet, Beijing, 100084, China.\n2Department of Computer Science, Y ale University , New Haven,\n06511, Connecticut, United States.\n3School of Electronic, Information and Electrical Engineering,\nShanghai Jiao T ong University , Shanghai, 200240, China.\n4T oyota T echnological Institute at Chicago, Chicago, 60637,\nIllinois, United States.\n*Corresponding author(s). E-mail(s): jinboxu@gmail.com;\nContributing authors: fw2359@columbia.com;\ntaoyu928@sjtu.edu.cn; dragomir.radev@yale.edu;\n†These authors contributed equally to this work.\nAbstract\nGeometric deep learning has recently achieved great success in non-\nEuclidean domains, and learning on 3D structures of large biomolecules\nis emerging as a distinct research area. However, its eﬃcacy is largely\nconstrained due to the limited quantity of structural data. Meanwhile,\nprotein language models trained on substantial 1D sequences have shown\nburgeoning capabilities with scale in a broad range of applications. Nev-\nertheless, no preceding studies consider combining these diﬀerent protein\nmodalities to promote the representation power of geometric neural net-\nworks. T o address this gap, we make the foremost step to integrate\nthe knowledge learned by well-trained protein language models into\nseveral state-of-the-art geometric networks. Experiments are evaluated\non a variety of protein representation learning benchmarks, including\nprotein-protein interface prediction, model quality assessment, protein-\nprotein rigid-body docking, and binding aﬃnity prediction, leading to\nan overall improvement of 20% over baselines and the new state-of-\nthe-art performance. Strong evidence indicates that the incorporation\n1\nSpringer Nature 2023 L ATEX template\n2 Article Title\nof protein language models’ knowledge enhances geometric networks’\ncapacity by a signiﬁcant margin and can be generalized to complex tasks.\nKeywords: Geometric Deep Learning, Language Model, Protein\nRepresentation Learning\n1 Introduction\nMacromolecules ( e.g., proteins, RNAs, or DNAs) are essential to biophysical\nprocesses. While they can be represented using lower-dimensional represen-\ntations such as linear sequences (1D) or chemical bond graphs (2D), a\nmore intrinsic and informative form is the three-dimensional geometry [\n97].\n3D shapes are critical to not only understanding the physical mechanisms\nof action but also answering a number of questions associated with drug\ndiscovery and molecular design [\n86]. As a consequence, tremendous efforts\nin structural biology have been devoted to deriving insights from their\nconformations [\n53, 55, 96].\nWith the rapid advances of deep learning (DL) techniques, it has been an\nattractive challenge to represent and reason about macromolecules’ structures\nin the 3D space. In particular, different sorts of 3D information including bond\nlengths and dihedral angles play an essential role. In order to encode them,\na number of 3D geometric graph neural networks (GGNNs) or CNNs [\n9, 42,\n44, 45, 81, 94, 95] have been proposed, and simultaneously achieve several\ncrucial properties of Euclidean geometry such as E(3) or SE(3) equivariance\nand symmetry . Notably , they are important constituents of geometric deep\nlearning (GDL), an umbrella term that generalizes networks to Euclidean or\nnon-Euclidean domains [\n5].\nMeanwhile, the anticipated growth of sequencing promises unprecedented\ndata on natural sequence diversity . The abundance of 1D amino acid sequences\nhas spurred increasing interest in developing protein language models at the\nscale of evolution, such as the series of ESM [\n54, 60, 71] and ProtT rans [ 25].\nThese protein language models are capable of capturing information about sec-\nondary and tertiary structures and can be generalized across a broad range of\ndownstream applications. T o be explicit, they have recently been demonstrated\nwith strong capabilities in uncovering protein structures [\n54], predicting the\neffect of sequence variation on function [ 60], learning inverse folding [ 40] and\nmany other general purposes [ 71].\nDespite the fruitful progress in protein language models, no prior stud-\nies have considered enhancing GGNNs’ ability by leveraging the knowledge\nof those protein language models. This is nontrivial because compared to\nsequence learning, 3D structures are much harder to obtain and thus less preva-\nlent. Consequently , learning on the structure of proteins leads to a reduced\namount of training data. F or example, the SAbDab database [\n23] merely has\n3K antibody-antigen structures without duplicate. The SCOPe database [ 16]\nSpringer Nature 2023 L ATEX template\nArticle Title 3\nSAbDab structures\n(~3,000)\nSCOPe structures\n(~226,000)\nUniParc sequences \n(250 million)\nUniRef50 sequences \n(12 million)\nE  G  R  L  T  V  Y  C  …\nProtein Sequence \nProtein Language Models\n(e.g., ESM-2, MSA-Transformer) \n3D Protein Graph\nSequential Models \nY\nC\nE\nV\nT\nL\nR\nG\nGeometric NNs\ne.g., GVP -GNN, EGNN, \nSE(3)-Transformer, \nSchnet , DimeNet\nFig. 1 : Illustration of our proposed framework to strengthen GGNNs with\nknowledge of protein language models. The protein sequence is őrst forwarded\ninto a pretrained protein language model to extract per-residue representa-\ntions, which are then used as the node feature in 3D protein graphs for GGNNs.\nhas 226K annotated structures, and the SIFTS database [\n89] comprises around\n220K annotated enzyme structures. These numbers are orders of magni-\ntude lower than the data set sizes that can inspire major breakthroughs\nin the deep learning community . In contrast, while the Protein Data Bank\n(PDB) [\n12] possesses approximately 182K macromolecule structures, databases\nlike Pfam [ 61] and UniParc [ 8] contains more than 47M and 250M protein\nsequences respectively .\nIn addition to the data size, the beneőt of protein sequence to struc-\nture learning also has solid evidence and theoretical support. Remarkably ,\nthe idea that biological function and structures are documented in the statis-\ntics of protein sequences selected through evolution has a long history [\n2, 98].\nThe unobserved variables that decide a protein’s őtness, including structure,\nfunction, and stability , leave a record in the distribution of observed natural\nsequences [\n35]. Those protein language models use self-supervision to unlock\nthe information encoded in protein sequence variations, which is also beneőcial\nfor GGNNs.\nSpringer Nature 2023 L ATEX template\n4 Article Title\nAccordingly , in this paper, we propose to promote the capacity of GGNNs\nwith the knowledge learned by protein language models (see Figure 1). The\nimprovements come from two major lines. Firstly , GGNNs can beneőt from\nthe information that emerges in the learned representations of those protein\nlanguage models on fundamental properties of proteins, including secondary\nstructures, contacts, and biological activity . This kind of knowledge may be\ndifficult for GGNNs to be aware of and learn in a speciőc downstream task.\nT o conőrm this claim, we conduct a toy experiment to demonstrate that con-\nventional graph connectivity mechanisms prevent existing GGNNs from being\ncognizant of residues’ absolute and relative positions in the protein sequence.\nSecondly and more intuitively , protein language models serve as an alternative\nway of enriching GGNNs’ training data and allow GGNNs to be exposed to\nmore different families of proteins, which thereby greatly strengthens GGNNs’\ngeneralization capability .\nW e examine our hypothesis across a wide range of benchmarks, containing\nmodel quality assessment, protein-protein interface prediction, protein-protein\nrigid-body docking, and ligand binding affinity prediction. Extensive exper-\niments show that the incorporation and combination of pretrained protein\nlanguage models’ knowledge signiőcantly improve GGNNs’ performance for\nvarious problems, which require distinct domain knowledge. By utilizing the\nunprecedented view into the language of protein sequences provided by power-\nful protein language models, GGNNs promise to augment our understanding\nof a vast database of poorly understood protein structures. Our work pro-\nvides a new perspective on protein representation learning and hopes to shed\nlight on how to bridge the gap between the thriving geometric deep learning\nand mature protein language models, which independently leverage different\nmodalities of proteins.\n2 Experiments\nOur toy experiments illustrate that existing GGNNs are unaware of the posi-\ntional order inside the protein sequences. T aking a step further, we show in\nthis section that incorporating knowledge learned by large-scale protein lan-\nguage models can robustly enhance GGNN’s capacity in a wide variety of\ndownstream tasks.\n2.1 T asks and Datasets\n• Model Quality Assessment (MQA) aims to select the best\nstructural model of a protein from a large pool of candidate\nstructures and is an essential step in structure prediction [\n17].\nF or a number of recently solved but unreleased structures, struc-\nture generation programs produce a large number of candidate\nstructures. MQA approaches are evaluated by their capability\nof predicting the GDT-TS score of a candidate structure com-\npared to the experimentally solved structure of that target. Its\nSpringer Nature 2023 L ATEX template\nArticle Title 5\ndatabase is composed of all structural models submitted to the\nCritical Assessment of Structure Prediction (CASP) [\n52] over\nthe last 18 years. The data is split temporally by competition\nyear. MQA is similar to the Protein Structure Ranking (PSR)\ntask introduced by T ownshend et al. [\n86].\n• Protein-protein Rigid-body Docking (PPRD) computa-\ntionally predicts the 3D structure of a protein-protein complex\nfrom the individual unbound structures. It assumes that no\nconformation change within the proteins happens during bind-\ning. W e leverage Docking Benchmark 5.5 (DB5.5) [\n91] as the\ndatabase. It is a gold standard dataset in terms of data quality\nand contains 253 structures.\n• Protein-protein Interface (PPI) investigates whether two\namino acids will contact when their respective proteins bind. It\nis an important problem in understanding how proteins inter-\nact with each other, e.g., antibody proteins recognize diseases\nby binding to antigens. W e use the Database of Interacting\nProtein Structures (DIPS), a comprehensive dataset of protein\ncomplexes mined from the PDB [\n85], and randomly select 15K\nsamples for evaluation.\n• Ligand Binding Aﬃnity (LBA) is an essential task for drug\ndiscovery applications. It predicts the strength of a candidate\ndrug molecule’s interaction with a target protein. Speciőcally , we\naim to forecast pK = −log10 K, where K is the binding affinity\nin Molar units. W e use the PDBbind database [\n57, 93], a curated\ndatabase containing protein-ligand complexes from the PDB\nand their corresponding binding strengths. The protein-ligand\ncomplexes are split such that no protein in the test dataset has\nmore than 30% or 60% sequence identity with any protein in\nthe training dataset.\n2.2 Experimental Setup\nW e evaluate our proposed framework on the instances of several state-of-the-\nart geometric networks, using Pytorch [\n66] and PyG [ 28] on four standard\nprotein benchmarks. F or MQA, PPI, and LBA, we use backbones that have\nbeen carefully described in Section\n5, i.e., GVP-GNN, EGNN, and Molformer.\nF or PPRD, we utilize the state-of-the-art deep learning model, EquiDock [ 32],\nas the backbone. It approximates the binding pockets and obtains the docking\nposes using keypoint matching and alignment. F or more experimental details,\nplease refer to Appendix\nA.\nSpringer Nature 2023 L ATEX template\n6 Article Title\nT able 1: Results on MQA.\nModel PLM\nModel Quality Assessment\nSpearman Correlation ↑ Pearson’s Correlation ↑ Kendall Rank ↑\nMean Global Mean Global Mean Global\nGVP-GNN ✗ 0.4144 ±0.010 0.6910 ±0.008 0.5235 ±0.013 0.6875 ±0.006 0.2960 ±0.010 0.4959 ±0.004\n✓ 0.6121 ±0.017 0.8492 ±0.015 0.7399 ±0.017 0.8544 ±0.009 0.4530 ±0.008 0.6798 ±0.014\nEGNN ✗ 0.4249 ±0.016 0.7341 ±0.015 0.5315 ±0.008 0.7336 ±0.018 0.3004 ±0.013 0.5344 ±0.011\n✓ 0.5642 ±0.013 0.8436 ±0.012 0.6925 ±0.006 0.8456 ±0.015 0.4105 ±0.014 0.6558 ±0.006\nMolformer ✗ 0.1238 ±0.011 0.3921 ±0.004 0.1969 ±0.004 0.3901 ±0.012 0.0841 ±0.010 0.2696 ±0.005\n✓ 0.2424 ±0.015 0.6516 ±0.009 0.3850 ±0.011 0.6210 ±0.014 0.1681 ±0.012 0.4579 ±0.007\n2.3 Results\nResults are reported with mean ± standard deviation over three repeated\nruns, where the best performance is in bold. The column of ’PLM’ indicates\nwhether the protein language model is used.\n2.3.1 Single-protein Representation T ask\nF or MQA, we document Spearman correlation ( RS), Pearson’s correlation\n(RP), and Kendall rank correlation ( KR) (see T able\n1). The introduction of\nprotein language models has brought a signiőcant average increase of 32.63%\nand 55.71% in global and mean RS, of 34.66% and 58.75% in global and mean\nRP, and of 43.21% and 63.20% in global and mean KR respectively . With the\naid of language models, GVP-GNN achieves the optimal global RS, global RP,\nand KR of 84.92%, 85.44%, and 67.98% separately .\nApart from that, we provide a full comparison with all existing approaches\nin T able\n2. W e elect R W plus [ 99], ProQ3D [ 88], V oroMQA [ 64], SBROD [ 47],\n3DCNN [ 86], 3DGNN [ 86], 3DOCNN [ 65], DimeNet [ 51], GraphQA [ 24], and\nGBPNet [ 6] as the baselines. Performance is recorded in T able 2, where the\nsecond best is underlined. It can be concluded that even if GVP-GNN is not\nthe best architecture, it can largely outperform existing methods including\nthe state-of-the-art no-pretraining method set by A ykent and Xia [\n6] ( i.e.,\nGBPNet) and the state-of-the-art pretraining results set by Jing et al. [ 45] and\nset the new state-of-the-art if it is enhanced by the protein language model.\n2.3.2 Protein-protein Representation T asks\nF or PPRD, we report three items as the measurements: the complex root mean\nsquared deviation (RMSD), the ligand RMSD, and the interface RMSD (see\nT able\n3). The interface is determined with a distance threshold less than 8Å.\nIt is noteworthy that, unlike the EquiDock paper, we do not apply the Kab-\nsch algorithm to superimpose the receptor and the ligand. Contrastingly , the\nreceptor protein is őxed during evaluation. All three metrics decrease consid-\nerably with improvements of 11.61%, 12.83%, and 31.01% in complex, ligand,\nand interface median RMSD, respectively . Notably , we also report the result\nof EquiDock, which is őrst pretrained on DIPS and then őne-tuned on DB5.\nIt can be discovered that DIPS-pretrained EquiDock still performs worse than\nSpringer Nature 2023 L ATEX template\nArticle Title 7\nT able 2: Comparison of performance on MQA. Models are sorted by the year they are released.\nModel PLM\nModel Quality Assessment\nSpearman Correlation ↑ Pearson’s Correlation ↑ Kendall Rank ↑\nMean Global Mean Global Mean Global\nR W plus [99]1 ✗ 0.167 0.056 0.192 0.033 0.137 0.011\nProQ3D [ 88]1 ✗ 0.432 0.772 0.444 0.796 0.304 594\nV oroMQA [64]1 ✗ 0.419 0.651 0.412 0.651 0.291 0.505\nSBROD [ 47]1 ✗ 0.413 0.569 0.431 0.551 0.291 0.393\n3DOCNN [ 65]2 ✗ 0.432 0.796 0.444 0.772 0.304 0.594\nDimeNet [ 51]1 ✗ 0.351 0.625 0.302 0.614 0.285 0.431\n3DCNN [ 86]2 ✗ 0.431 ±0.013 0.789 ±0.017 0.557 ±0.011 0.780 ±0.016 0.308 ±0.010 0.592 ±0.016\n3DGNN [ 86]2 ✗ 0.411 ±0.006 0.750 ±0.018 0.500 ±0.012 0.747 ±0.018 0.278 ±0.005 0.547 ±0.016\nGVP-GNN [ 44]3 ✗ 0.414 ±0.010 0.691 ±0.008 0.523 ±0.013 0.687 ±0.006 0.296 ±0.010 0.495 ±0.004\nGraphQA [ 24]1 ✗ 0.379 0.820 0.357 0.821 0.331 0.618\nGBPNet [ 6]1 ✗ 0.517 0.856 0.612 0.853 0.372 0.656\nGVP-GNN ✓ 0.612 ±0.017 0.849 ±0.015 0.739 ±0.017 0.854 ±0.009 0.453 ±0.008 0.679 ±0.014\n1These results are taken from A ykent and Xia [ 6].\n2These results are taken from T ownshend et al. [ 86].\n3These results are re-produced.\nSpringer Nature 2023 L ATEX template\n8 Article Title\nT able 3: Performance of PPRD on DB5.5 T est Set. Models with ♣are directly\ntrained and tested on DB5, while EquiDock with ♠is őrst pretrained on DIPS\nand then őne-tuned on the DB5 training set.\nModel PLM\nProtein-protein Rigid-body Docking\nComplex RMSD ↓ Ligand RMSD ↓ Interface RMSD ↓\nMedian Mean Std Median Mean Std Median Mean Std\nEquiDock\n✗♣ 16.88 17.11 5.33 40.35 37.97 12.94 16.19 37.97 4.47\n✗♠ 15.02 14.31 5.28 36.82 35.95 13.18 14.37 35.68 4.12\n✓♣ 14.92 13.14 4.59 35.17 33.48 14.34 11.17 33.48 4.38\nT able 4: Results on LBA.\nModel PLM\nLigand Binding Aﬃnity\nSequence Identity (30%)\nRMSD↓ Pearson’s Correlation ↑ Spearman Correlation ↑ Kendall Rank ↑\nGVP-GNN ✗ 1.6480 ±0.014 0.2138 ±0.013 0.1648 ±0.009 0.1107 ±0.012\n✓ 1.4556 ±0.011 0.5373 ±0.010 0.5078 ±0.005 0.3495 ±0.009\nEGNN ✗ 1.4929 ±0.012 0.4891 ±0.017 0.4725 ±0.008 0.3291 ±0.014\n✓ 1.4033 ±0.013 0.5655 ±0.016 0.5448 ±0.005 0.3790 ±0.007\nMolformer ✗ 1.9107 ±0.018 0.4618 ±0.014 0.4104 ±0.011 0.2812 ±0.019\n✓ 1.6028 ±0.020 0.5351 ±0.017 0.5372 ±0.015 0.3758 ±0.016\nSequence Identity (60%)\nGVP-GNN ✗ 1.5438 ±0.015 0.6608 ±0.012 0.6668 ±0.0010 0.4797 ±0.014\n✓ 1.5137 ±0.019 0.6680 ±0.010 0.6716 ±0.008 0.4786 ±0.012\nEGNN ✗ 1.5928 ±0.020 0.6274 ±0.013 0.6271 ±0.017 0.4498 ±0.014\n✓ 1.5595 ±0.022 0.6445 ±0.015 0.6463 ±0.019 0.4656 ±0.019\nMolformer ✗ 1.8610 ±0.018 0.5528 ±0.016 0.5309 ±0.015 0.3738 ±0.017\n✓ 1.5926 ±0.024 0.6524 ±0.018 0.6528 ±0.016 0.4367 ±0.011\nEquiDock equipped with pretrained language models. This strongly demon-\nstrates that structural pretraining for GGNNs may not beneőt GGNNs more\nthan pretrained protein language models.\nF or PPI, we record AUROC as the metric in Figure\n2. It can be found that\nAUROC increases for 6.93%, 14.01%, and 22.62% for GVP-GNN, EGNN, and\nMolformer respectively . It is worth noting that Molformer falls behind EGNN\nand GVP-GNN originally in this task. But after injecting knowledge learned\nby protein language models, Molformer achieves competitive or even better\nperformance than EGNN or GVP-GNN. This indicates that protein language\nmodels can realize the potential of GGNNs to the full extent and greatly narrow\nthe gap between different geometric deep learning architectures. The results\nmentioned above are amazing because, unlike MQA, PPRD and PPI study the\ngeometric interactions between two proteins. Though existing protein language\nmodels are all trained on single protein sequences, our experiments show that\nSpringer Nature 2023 L ATEX template\nArticle Title 9\nT able 5: Comparison of performance on LBA with 30% sequence identity . Models are sorted by the year they are released.\nModel PLM\nLigand Binding Aﬃnity\nSequence Identity (30%)\nRMSD↓ Pearson’s Correlation ↑ Spearman Correlation ↑ Kendall Rank ↑\nDeepAffinity [ 48]1 ✗ 1.893 ±0.650 0.415 0.426 Ð\nCormorant [ 4]2 ✗ 1.568 ±0.012 0.389 0.408 Ð\nLSTM [ 11]3 ✗ 1.985 ±0.006 0.165 ±0.006 0.152 ±0.024 Ð\nT APE [ 68]3 ✗ 1.890 ±0.035 0.338 ±0.044 0.286 ±0.124 Ð\nProtT rans [25]3 ✗ 1.544 ±0.015 0.438 ±0.053 0.434 ±0.058 Ð\n3DCNN [ 86]1 ✗ 1.414 ±0.021 0.550 0.553 Ð\nGNN [ 86]1 ✗ 1.570 ±0.025 0.545 0.533 Ð\nMaSIF [ 31]3 ✗ 1.484 ±0.018 0.467 ±0.020 0.455 ±0.014 Ð\nDGA T [63]2 ✗ 1.719 ±0.047 0.464 0.472 Ð\nDGIN [ 63]2 ✗ 1.765 ±0.076 0.426 0.432 Ð\nDGA T-GCN [63]2 ✗ 1.550 ±0.017 0.498 0.496 Ð\nGVP-GNN [ 44]4 ✗ 1.648 ±0.014 0.213 ±0.013 0.164 ±0.009 0.110 ±0.012\nEGNN [ 73]4 ✗ 1.492 ±0.012 0.489 ±0.017 0.472 ±0.008 0.329 ±0.014\nHoloProt [ 79]3 ✗ 1.464 ±0.006 0.509 ±0.002 0.500 ±0.005 Ð\nGBPNet [ 6]2 ✗ 1.405 ±0.009 0.561 0.557 Ð\nEGNN ✓ 1.403 ±0.013 0.565 ±0.016 0.544 ±0.005 0.379 ±0.007\n1These results are taken from T ownshend et al. [ 86].\n2These results are taken from A ykent and Xia [ 6].\n3These results are copied from Somnath et al. [ 79].\n4These results are re-produced.\nSpringer Nature 2023 L ATEX template\n10 Article Title\n/uni0000002a/uni00000039/uni00000033/uni00000010/uni0000002a/uni00000031/uni00000031 /uni00000028/uni0000002a/uni00000031/uni00000031 /uni00000030/uni00000052/uni0000004f/uni00000049/uni00000052/uni00000055/uni00000050/uni00000048/uni00000055\n/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000013\n/uni00000013/uni00000011/uni0000001b/uni00000018\n/uni00000013/uni00000011/uni0000001c/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000018\n/uni00000014/uni00000011/uni00000013/uni00000013\n/uni00000024/uni00000038/uni00000035/uni00000032/uni00000026\n/uni00000013/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000018\n/uni00000013/uni00000011/uni0000001b/uni00000015/uni0000001c/uni00000017\n/uni00000013/uni00000011/uni0000001a/uni0000001a/uni00000014/uni0000001c\n/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000019/uni0000001c\n/uni00000013/uni00000011/uni0000001c/uni00000017/uni00000018/uni00000019 /uni00000013/uni00000011/uni0000001c/uni00000017/uni00000019/uni00000018\n/uni00000033/uni00000055/uni00000052/uni00000057/uni00000048/uni0000004c/uni00000051/uni00000010/uni00000053/uni00000055/uni00000052/uni00000057/uni00000048/uni0000004c/uni00000051/uni00000003/uni0000002c/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000048/uni00000003/uni00000033/uni00000055/uni00000048/uni00000047/uni0000004c/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000005a/uni00000011/uni00000052/uni00000011/uni00000003/uni00000033/uni0000002f/uni00000030\n/uni0000005a/uni00000011/uni00000003/uni00000033/uni0000002f/uni00000030\nFig. 2 : Results of PPI.\nthe evolution information hidden in unpaired sequences can also be valuable\nto analyze the multi-protein environment.\n2.3.3 Protein-molecules Representation T ask\nF or LBA, we compare RMSD, RS, RP, and KR in T able\n4. The incorpo-\nration of protein language models produces a remarkably average decline of\n11.26% and 6.15% in RMSD for 30% and 60% identity , an average increase of\n51.09% and 9.52% in RP for the 30% and 60% identity , an average increment\nof 66.60% and 8.90% in RS for the 30% and 60% identity , and an average\nincrement of 68.52% and 6.70% in KR for the 30% and 60% identity . It can be\nseen that the improvements in the 30% sequence identity is higher than that in\nthe less restrictive 60% sequence identity . This conőrms that protein language\nmodels beneőt GGNNs more when the unseen samples belong to different pro-\ntein domains. Moreover, contrasting PPRD or PPI, LBA studies how proteins\ninteract with small molecules. Our outcome demonstrates that rich protein\nrepresentations encoded by protein language models can also contribute to the\nanalysis of protein’s reaction to other non-protein drug-like molecules.\nIn addition, we compare thoroughly with all existing approaches for LBA\nin T able\n5. W e select a broad range of models including DeepAffinity [ 48],\nCormorant [ 4], LSTM [ 11], T APE [ 68], ProtT rans [ 25], 3DCNN [ 86], GNN [ 86],\nMaSIF [ 31], DGA T [ 63], DGIN [ 63], DGA T-GCN [ 63], HoloProt [ 79], and\nGBPNet [ 6] as the baseline. W e report the comparison in T able 5, where the\nsecond best is underlined. It is clear that even if EGNN is a median-level\narchitecture, it can achieve the best RMSD and the best Pearson’s correlation\nwhen enhanced by protein language models, beating a group of strong baselines\nincluding HoloProt [\n79] and GBPNet [ 6].\nSpringer Nature 2023 L ATEX template\nArticle Title 11\n/uni00000014/uni00000013\n/uni00000013\n/uni00000014/uni00000013\n/uni00000014\n/uni00000014/uni00000013\n/uni00000015\n/uni00000014/uni00000013\n/uni00000016\n/uni00000014/uni00000013\n/uni00000017\n/uni00000014/uni00000013\n/uni00000018\n/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b10\n6\n/uni0000000c\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000033/uni00000048/uni00000044/uni00000055/uni00000056/uni00000052/uni00000051/uni00000003/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000002a/uni00000039/uni00000033\n/uni00000028/uni0000002a/uni00000031/uni00000031\n/uni00000030/uni00000032/uni0000002f/uni00000029/uni00000032/uni00000035/uni00000030/uni00000028/uni00000035\nFig. 3 : Performance of GGNNs on MQA with ESM-2 at different scales.\n2.4 Scale of Protein Language Models\nIt has been observed that as the size of the language model increases, there are\nconsistent improvements in tasks like structure prediction [\n54]. Here we con-\nduct an additional ablation study to investigate the effect of protein language\nmodels’ sizes on GGNNs. Speciőcally , we explore different ESM-2 with the\nparameter numbers of 8M, 35M, 150M, 650M, and 3B. Results are plotted in\nFigure\n3, which veriőes that scaling the protein language model is advantageous\nfor GGNNs.\n3 Conclusion\nIn this study , we investigate a problem that has been long ignored by exist-\ning geometric deep learning methods for proteins. That is, how to employ the\nabundant protein sequence data for 3D geometric representation learning. T o\nanswer this question, we propose to leverage the knowledge learned by exist-\ning advanced pre-trained protein language models, and use their amino acid\nrepresentations as the initial features. W e conduct a variety of experiments\nsuch as protein-protein docking and model quality assessment to demonstrate\nthe efficacy of our approach. Our work provides a simple but effective mecha-\nnism to bridge the gap between 1D sequential models and 3D geometric neural\nnetworks, and hope to throw light on how to combine information encoded in\ndifferent protein modalities.\n4 Method\nAs discussed before, learning on 3D structures cannot beneőt from these large\namounts of sequential data. Due to this fact, model sizes of those GGNNs are\ntherefore limited or overőtting may occur [\n39]. On the contrary , it can be seen,\ncomparing the number of protein sequences in the UniProt database [ 19] to\nthe number of known structures in the PDB, over 1700 times more sequences\nSpringer Nature 2023 L ATEX template\n12 Article Title\nthan structures. More importantly , the availability of new protein sequence\ndata continues to far outpace the availability of experimental protein structure\ndata, only increasing the need for accurate protein modeling tools.\nTherefore, it is straightforward to assist GGNNs with pretrained protein\nlanguage models. T o this end, we feed amino acid sequences into those protein\nlanguage models, where the state-of-the-art ESM-2 [\n54] is adopted in our case,\nand extract the per-residue representations, denoted as h′ ∈RN×ψP LM. Here\nψPLM = 1280. Then h′ can be added or concatenated to the per-atom feature\nh. F or residue-level graphs, h′ immediately replaces the original h as the input\nnode features.\nNotably , incompatibility exists between the experimental structure and its\noriginal amino acid sequence. That is, structures stored in the PDB őles are\nusually incomplete and some strings of residues are missing due to inevitable\nrealistic issues [\n22]. They , therefore, do not perfectly match the corresponding\nsequences ( i.e., F AST A sequence). There are two choices to address this mis-\nmatch. On the one hand, we can simply use the fragmentary sequence as the\nsubstitute for the integral amino acid sequence and forward it into the protein\nlanguage models. On the other hand, we can leverage a dynamic program-\nming algorithm provided by Biopython [\n18] to implement pairwise sequence\nalignment and abandon residues that do not exist in the PDB structures. It is\nempirically discovered that no big difference exists between them, so we adopt\nthe former processing mechanism for simplicity .\n5 Sequence Recovery Analysis\nIt is commonly acknowledged that protein structures maintain much more\ninformation than their corresponding amino acid sequences. And for decades\nlong, it has been an open challenge for computational biologists to predict\nprotein structure from its amino acid sequence [\n49, 76]. Though the advance-\nment of Alphafold (AF) [ 46], as well as RosettaF old [ 7], have made a huge step\nin alleviating the limitation brought by the number of available experimen-\ntally determined protein structures [\n40], neither AF nor its successors such as\nAlphafold-Multimer [ 26], IgF old [ 72], and HelixF old [ 92] are a panacea. Their\npredicted structures can be severely inaccurate when the protein is orphan and\nlacks multiple sequence alignment (MSA) as the template. As a consequence,\nit is hard to conclude that protein sequences can be perfectly transformed to\nthe structure modality by current tools and be used as extra training resources\nfor GGNNs.\nMoreover, we argue that even if conformation is a higher-dimensional repre-\nsentation, the prevailing learning paradigm may forbid GGNNs from capturing\nthe knowledge that is uniquely preserved in protein sequences. Recall that\nGGNNs are mainly diverse in their patterns to employ 3D geometries, the\ninput features include distance [\n74], angles [ 50, 51], torsion, and terms of other\norders [ 56]. The position index hidden in protein sequences, however, is usually\nneglected when constructing 3D graphs for GGNNs. Therefore, in this section,\nSpringer Nature 2023 L ATEX template\nArticle Title 13\nTask 1: Absolute Position Recognition \nTask 2: Relative Position Estimation \nr -ball graph KNN graph \nSequential Order\n1\n2\n3\n8\na. b.\n2\n6\n0 3\n4\n3\n8\nFig. 4 : (a) Protein residue graph construction. Here we draw graphs in 2D\nfor better visualization but study 3D graphs for GGNNs. (b) T wo sequence\nrecovery tasks. The őrst requires GGNNs to predict the absolute position\nindex for each residue in the protein sequence. The second aims to forecast the\nminimum distance of each amino acid to the two sides of the protein sequence.\nT able 6: Results of two residue position identiőcation tasks.\nModels Graph Type Absolute Position Recognition Relative Position Estimation\nAccuracy (%) ↑ RMSE ↓\nGVP-GNN r-ball graph 0.157 ±0.002 392.38 ±3.41\nKNN graph 0.158 ±0.003 392.38 ±4.05\nEGNN r-ball graph 0.150 ±0.005 412.70 ±2.36\nKNN graph 0.131 ±0.004 403.86 ±1.77\nMolformer FC graph 0.148 ±0.007 270.69 ±4.53\nwe design a toy trial to examine whether GGNNs can succeed in recovering\nthis kind of positional information.\n5.1 Protein Graph Construction\nHere the structure of a protein can be represented as an atom-level or residue-\nlevel graph G= (V, E), where Vand E= (eij) correspond to the set of N nodes\nand M edges respectively . Nodes have their 3D coordinates as x ∈RN×3 as well\nas the initial ψh-dimension roto-translational invariant features h ∈RN×ψh\n(e.g., atom types and electronegativity , residue classes). Normally , there are\nthree types of options to construct connectivity for molecules: r-ball graphs,\nfully-connected (FC) graphs, and K-nearest neighbors (KNN) graphs. In our\nsetting, nodes are linked to K = 10 nearest neighbors for KNN graphs, and\nedges include all atom pairs within a distance cutoff of 8Å for r-ball graphs.\n5.2 Recovery from Graphs to Sequences\nSince most prior studies choose to establish 3D protein graphs based on purely\ngeometric information and ignore their sequential identities, it provokes the\nfollowing position identity question:\nCan existing GGNNs identify the sequential position order only from\ngeometric structures of proteins?\nSpringer Nature 2023 L ATEX template\n14 Article Title\nT o answer this question, we formulate two categories of toy tasks (see\nFigure 4). The őrst one is a classiőcation task, where models are asked to\ndirectly predict the position index ranging from 1 to N, the residue number of\neach protein. This task adopts accuracy as the metric and expects models to\ndiscriminate the absolute position of the amino acid within the whole protein\nsequence.\nIn addition to that, we propose the second task to focus on the relative\nposition of each residue, where models are required to predict the minimum\ndistance of residue to the two sides of the given protein. W e use the root mean\nsquared error (RMSE) as the metric. This task aims to examine the capability\nof GGNNs to distinguish which segment the amino acid belongs to ( i.e., the\ncenter section of the protein or the end of the protein).\n5.3 Experimental Setting\nW e adopt three technically distinct and broadly accepted architectures of\nGGNNs for empirical veriőcation. T o be speciőc, GVP-GNN [\n44, 45] extends\nstandard dense layers to operate on collections of Euclidean vectors, performing\nboth geometric and relational reasoning on efficient representations of macro-\nmolecules. EGNN [\n73] is a translation, rotation, reŕection, and permutation\nequivariant GNN without expensive spherical harmonics. Molformer [95]\nemploys the self-attention mechanism for 3D point clouds while guarantees\nSE(3)-equivariance.\nW e exploit a small non-redundant subset of high-resolution structures from\nthe PDB. T o be speciőc, we use only X-ray structures with resolution <3.0Å,\nand enforce a 60% sequence identity threshold. This results in a total of 2643,\n330, and 330 PDB structures for the train, validation, and test sets, respec-\ntively . Experimental details, the summary of the database, and the description\nof these GGNNs are elaborated in Appendix\nA.\n5.4 Results and Analysis\nT able\n6 documents the overall results, where metrics are labeled with ↑/↓if\nhigher/lower is better, respectively . It can be found that all GGNNs fail to\nrecognize either the absolute or the relative positional information encoded in\nthe protein sequences with an accuracy lower than 1% and a high RMSE.\nThis phenomenon stems from the conventional ways to build graph con-\nnectivity , which usually excludes sequential information. T o be speciőc, unlike\ncommon applications of GNNs such as citation networks [\n75, 83], social net-\nworks [ 27, 36], knowledge graphs [ 15], molecules do not have explicitly deőned\nedges or adjacency . On the one hand, r-ball graphs utilize a cut-off distance,\nwhich is usually set as a hyperparameter, to determine the particle connections.\nBut it is hard to guarantee a cut-off to properly include all crucial node inter-\nactions for complicated and large molecules. On the other hand, FC graphs\nthat consider all pairwise distances will cause severe redundancies, dramati-\ncally increasing the computational complexity especially when proteins consist\nSpringer Nature 2023 L ATEX template\nArticle Title 15\nof thousands of residues. Besides, GGNNs also easily get confused by exces-\nsive noise, leading to unsatisfactory performance. As a remedy , KNN becomes\na more popular choice to establish graph connectivity for proteins [\n29, 32, 80].\nHowever, all of them take no account of the sequential information and require\nGGNNs to learn this original sequential order during training.\nThe lack of sequential information can yield several problems. T o begin\nwith, residues are unaware of their relative positions in the proteins. F or\ninstance, two residues can be close in the 3D space but distant in the sequence,\nwhich can mislead models to őnd the correct backbone chain. Secondly , accord-\ning to the characteristics of the MP mechanism, two residues in a protein with\nthe same neighborhood are expected to share similar representations. Never-\ntheless, the role of those two residues can be signiőcantly separate [\n62] when\nthey are located at different segments of the protein. Thus, GGNNs may be\nincapable of differentiating two residues with the same 1-hop local structures.\nThis restriction has already been distinguished by several works [\n42, 100], but\nnone of them make a strict and thorough investigation. Admittedly , sequen-\ntial order may only be necessary for certain tasks. But this toy experiment\nstrongly indicates that the knowledge monopolized by amino acid sequences\ncan be lost if GGNNs only learn from protein structures.\n6 Related W ork\n6.1 Geometric Deep Learning for Proteins\nGigantic molecules ( i.e., macromolecules) populate a cell, providing it with\nirreplaceable functions for life. And past few years have witnessed growing\nattention in learning on their 3D structures. Early work adopts graph kernels\nand support vector machines to classify enzymes based on their conforma-\ntions [\n14]. Later, inspired by the booming development of computer vision,\nprotein tertiary structures are represented as 3D density maps with 3DCNNs\nto address a host of problems such as protein binding site prediction [\n43],\nenzyme classiőcation [ 3], protein-ligand binding affinity [ 67], protein quality\nassessment [ 21], and protein-protein interaction interface identiőcation [ 85].\nAt the same time, due to the fact that molecules can be naturally modeled\nas graphs in real-world studies, GGNNs have emerged as the mainstream line to\nlearn directly from the protein spatial neighboring graphs. They show promis-\ning capacity in many tasks including protein interface prediction [\n29], protein\ndesign [ 42, 45, 81], protein quality assessment [ 9], function prediction [ 34], and\nin more challenging ones like protein folding [ 39].\nMost existing GGNNs rely on the widely adopted message passing (MP)\nparadigm [ 20, 33, 56] to aggregate local neighborhood information for the\nupdate of node features. Their divergence mainly lies in how to exploit different\ntypes of 3D information such as bond lengths or dihedral angles [\n41, 50, 51, 56].\nMoreover, equivariance is regarded as a ubiquitous property for molecular sys-\ntems, and plenty of evidence has proven the effectiveness to integrate such\nSpringer Nature 2023 L ATEX template\n16 Article Title\ninductive bias into GGNNs for modeling 3D geometry [ 4, 10, 30, 84]. Never-\ntheless, the potential of GGNNs is largely underestimated and cannot be fully\nreleased owing to the sparsity of structure data.\n6.2 Protein Language Modeling\nA large body of work has focused on protein language modeling in individual\nprotein families, solving problems like functional nanobody design [\n78] and pro-\ntein sequence generation [ 87]. This success has triggered a prospective trend to\nmodel large-scale databases of protein sequences rather than families of related\nsequences, where unsupervised learning becomes the preferred option. T o be\nexplicit, Bepler and Berger [\n11] combine unsupervised sequence pretraining\nwith structural supervision to generate sequence embeddings. Alley et al. [ 1]\nand Heinzinger et al. [ 38] demonstrate that LSTM language models are able to\ncapture certain biological properties. In the meantime, Rao et al. [ 68] evaluated\na variety of protein language models across a panel of benchmarks conclud-\ning that small LSTMs and T ransformers fall well short of features from the\nbioinformatics pipeline. Rives et al. [\n71] start to model protein sequences with\nself-attention, illustrating that T ransformer-based protein language models can\nseize accurate information of structure and function in their representations.\nA combination of model scale and architecture improvements has been\ncritical to recent successes in protein language modeling. Elnaggar et al.\n[\n25] analyze a diversity of T ransformer variants. Rives et al. [ 71] prove that\nlarge T ransformer models can achieve state-of-the-art features across various\ntasks. Vig et al. [\n90] discover that speciőc attention heads of pretrained T rans-\nformers have immediate correlations with protein contacts. Moreover, Rao\net al. [\n69] őnd that the combination of multiple attention heads is more accu-\nrate than Potts models in contact prediction, even if using a single sequence\nfor inference.\nF or the sake of better capturing the biochemical knowledge, a group of\ntypical pretraining objectives is explored including next amino acid predic-\ntion [\n1, 25], masked language modeling (MLM) [ 37], contrastive predictive\ncoding [ 58], and conditional generation [ 59]. Besides that, Sturmfels et al. [ 82]\nand Sercu et al. [ 77] study alternative objectives with sets of sequences for\nsupervision. Sturmfels et al. [ 82] extend the unsupervised language modeling to\nforecast the position-speciőc scoring matrix (PSSM). Apart from approaches\nthat merely learn in the whole sequence space, multiple sequence alignment\n(MSA)-based methods leverage the sequences within a protein family to seize\nthe conserved and variable regions of homologous sequences [\n13, 60, 70].\nData A vailability\nThe data of model quality assessment, protein-protein interface prediction, and\nligand affinity prediction is available by\nhttps://www.atom3d.ai/. The data of\nprotein-protein rigid-body docking can be downloaded directly from the official\nrepository of Equidock\nhttps://github.com/octavian-ganea/equidock_public.\nSpringer Nature 2023 L ATEX template\nArticle Title 17\nCode availability\nThe code repository is stored at\nhttps://github.com/smiles724/bottleneck.\nAuthors’ contributions\nF.W. and J.X. led the research. F.W. contributed technical ideas. F.W. and\nY.T. developed the proposed method. F.W., D.R., and Y.T. performed the\nanalysis. J.X. and D.R. provided evaluation and suggestions. All authors\ncontributed to the manuscript.\nAcknowledgments\nThis work is supported in part by the Institute of AI Industry Research at\nT singhua University and the Molecule Mind.\nReferences\n[1] Ethan C Alley , Grigory Khimulya, Surojit Biswas, Mohammed\nAlQuraishi, and George M Church. Uniőed rational protein engineering\nwith sequence-based deep representation learning. Nature methods , 16\n(12):1315ś1322, 2019.\n[2] DANIÈLE Altschuh, AM Lesk, AC Bloomer, and A Klug. Correlation\nof co-ordinated amino acid substitutions with function in viruses related\nto tobacco mosaic virus. Journal of molecular biology , 193(4):693ś707,\n1987.\n[3] Afshine Amidi, Shervine Amidi, Dimitrios Vlachakis, V asileios Mega-\nlooikonomou, Nikos Paragios, and Evangelia I Zacharaki. Enzynet:\nenzyme classiőcation using 3d convolutional neural networks on spatial\nrepresentation. PeerJ, 6:e4750, 2018.\n[4] Brandon Anderson, T ruong Son Hy , and Risi Kondor. Cormorant:\nCovariant molecular neural networks. Advances in neural information\nprocessing systems , 32, 2019.\n[5] Kenneth Atz, F rancesca Grisoni, and Gisbert Schneider. Geometric deep\nlearning on molecular representations. Nature Machine Intelligence , 3\n(12):1023ś1032, 2021.\n[6] Sarp A ykent and Tian Xia. Gbpnet: Universal geometric representation\nlearning on protein structures. In Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining , pages 4ś14, 2022.\n[7] Minkyung Baek, F rank DiMaio, Ivan Anishchenko, Justas Dauparas,\nSergey Ovchinnikov, Gyu Rie Lee, Jue W ang, Qian Cong, Lisa N Kinch,\nSpringer Nature 2023 L ATEX template\n18 Article Title\nR Dustin Schaeffer, et al. Accurate prediction of protein structures\nand interactions using a three-track neural network. Science, 373(6557):\n871ś876, 2021.\n[8] Amos Bairoch, Rolf Apweiler, Cathy H W u, Winona C Barker, Brigitte\nBoeckmann, Serenella F erro, Elisabeth Gasteiger, Hongzhan Huang,\nRodrigo Lopez, Michele Magrane, et al. The universal protein resource\n(uniprot). Nucleic acids research , 33(suppl_1):D154śD159, 2005.\n[9] F ederico Baldassarre, David Menéndez Hurtado, Arne Elofsson, and Hos-\nsein Azizpour. Graphqa: protein model quality assessment using graph\nconvolutional networks. Bioinformatics, 37(3):360ś366, 2021.\n[10] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P\nMailoa, Mordechai Kornbluth, Nicola Molinari, T ess E Smidt, and Boris\nKozinsky . E (3)-equivariant graph neural networks for data-efficient and\naccurate interatomic potentials. Nature communications , 13(1):1ś11,\n2022.\n[11] T ristan Bepler and Bonnie Berger. Learning protein sequence\nembeddings using information from structure. arXiv preprint\narXiv:1902.08661, 2019.\n[12] Helen M Berman, John W estbrook, Zukang F eng, Gary Gilliland, T ala-\npady N Bhat, Helge W eissig, Ilya N Shindyalov, and Philip E Bourne.\nThe protein data bank. Nucleic acids research , 28(1):235ś242, 2000.\n[13] Surojit Biswas, Grigory Khimulya, Ethan C Alley , Kevin M Esvelt, and\nGeorge M Church. Low-n protein engineering with data-efficient deep\nlearning. Nature methods , 18(4):389ś396, 2021.\n[14] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vish-\nwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function\nprediction via graph kernels. Bioinformatics, 21(suppl_1):i47śi56, 2005.\n[15] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Este-\nvam R Hruschka, and T om M Mitchell. T oward an architecture for\nnever-ending language learning. In Twenty-Fourth AAAI conference on\nartiﬁcial intelligence , 2010.\n[16] John-Marc Chandonia, Naomi K F ox, and Steven E Brenner. Scope:\nclassiőcation of large macromolecular structures in the structural classi-\nőcation of proteinsÐextended database. Nucleic acids research , 47(D1):\nD475śD481, 2019.\n[17] Jianlin Cheng, Myong-Ho Choe, Arne Elofsson, Kun-Sop Han, Jie Hou,\nAli HA Maghrabi, Liam J McGuffin, David Menéndez-Hurtado, Kliment\nSpringer Nature 2023 L ATEX template\nArticle Title 19\nOlechnovič, T orsten Schwede, et al. Estimation of model accuracy in\ncasp13. Proteins: Structure, Function, and Bioinformatics , 87(12):1361ś\n1377, 2019.\n[18] Peter JA Cock, Tiago Antao, Jeffrey T Chang, Brad A Chapman,\nCymon J Cox, Andrew Dalke, Iddo F riedberg, Thomas Hamelryck, F rank\nKauff, Bartek Wilczynski, et al. Biopython: freely available python tools\nfor computational molecular biology and bioinformatics. Bioinformatics,\n25(11):1422ś1423, 2009.\n[19] UniProt Consortium. Uniprot: a hub for protein information. Nucleic\nacids research , 43(D1):D204śD212, 2015.\n[20] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai,\nRobert J Ragotte, Lukas F Milles, Basile IM Wicky , Alexis Courbet,\nRob J de Haas, Neville Bethel, et al. Robust deep learning based protein\nsequence design using proteinmpnn. bioRxiv, 2022.\n[21] Georgy Derevyanko, Sergei Grudinin, Y oshua Bengio, and Guillaume\nLamoureux. Deep convolutional networks for quality assessment of\nprotein folds. Bioinformatics, 34(23):4046ś4053, 2018.\n[22] Kristina Djinovic-Carugo and Oliviero Carugo. Missing strings of\nresidues in protein crystal structures. Intrinsically disordered proteins ,\n3(1):e1095697, 2015.\n[23] James Dunbar, Konrad Krawczyk, Jinwoo Leem, T erry Baker, Angelika\nF uchs, Guy Georges, Jiye Shi, and Charlotte M Deane. Sabdab: the\nstructural antibody database. Nucleic acids research , 42(D1):D1140ś\nD1146, 2014.\n[24] Stephan Eismann, Raphael JL T ownshend, Nathaniel Thomas, Milind\nJagota, Bowen Jing, and Ron O Dror. Hierarchical, rotation-equivariant\nneural networks to select structural models of protein complexes. Pro-\nteins: Structure, Function, and Bioinformatics , 89(5):493ś501, 2021.\n[25] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\nY u W ang, Llion Jones, T om Gibbs, T amas F eher, Christoph Angerer,\nMartin Steinegger, et al. Prottrans: towards cracking the language of\nlife’s code through self-supervised deep learning and high performance\ncomputing. arXiv preprint arXiv:2007.06225 , 2020.\n[26] Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova,\nAndrew Senior, Tim Green, Augustin Žídek, Russ Bates, Sam Blackwell,\nJason Yim, et al. Protein complex prediction with alphafold-multimer.\nBioRxiv, pages 2021ś10, 2022.\nSpringer Nature 2023 L ATEX template\n20 Article Title\n[27] W enqi F an, Y ao Ma, Qing Li, Y uan He, Eric Zhao, Jiliang T ang, and\nDawei Yin. Graph neural networks for social recommendation. In The\nworld wide web conference , pages 417ś426, 2019.\n[28] Matthias F ey and Jan Eric Lenssen. F ast graph representation learning\nwith pytorch geometric. arXiv preprint arXiv:1903.02428 , 2019.\n[29] Alex F out, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein\ninterface prediction using graph convolutional networks. Advances in\nneural information processing systems , 30, 2017.\n[30] F abian F uchs, Daniel W orrall, V olker Fischer, and Max W elling. Se\n(3)-transformers: 3d roto-translation equivariant attention networks.\nAdvances in Neural Information Processing Systems , 33:1970ś1981,\n2020.\n[31] Pablo Gainza, F reyr Sverrisson, F rederico Monti, Emanuele Rodola,\nD Boscaini, MM Bronstein, and BE Correia. Deciphering interaction őn-\ngerprints from protein molecular surfaces using geometric deep learning.\nNature Methods , 17(2):184ś192, 2020.\n[32] Octavian-Eugen Ganea, Xinyuan Huang, Charlotte Bunne, Y atao Bian,\nRegina Barzilay , T ommi Jaakkola, and Andreas Krause. Independent\nse (3)-equivariant models for end-to-end rigid protein docking. arXiv\npreprint arXiv:2111.07786 , 2021.\n[33] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley , Oriol Vinyals, and\nGeorge E Dahl. Neural message passing for quantum chemistry . In\nInternational conference on machine learning , pages 1263ś1272. PMLR,\n2017.\n[34] Vladimir Gligorijević, P Douglas Renfrew, T omasz Kosciolek,\nJulia Koehler Leman, Daniel Berenberg, T ommi V atanen, Chris Chan-\ndler, Bryn C T aylor, Ian M Fisk, Hera Vlamakis, et al. Structure-based\nprotein function prediction using graph convolutional networks. Nature\ncommunications, 12(1):1ś14, 2021.\n[35] Ulrike Göbel, Chris Sander, Reinhard Schneider, and Alfonso V alen-\ncia. Correlated mutations and residue contacts in proteins. Proteins:\nStructure, Function, and Bioinformatics , 18(4):309ś317, 1994.\n[36] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation\nlearning on large graphs. Advances in neural information processing\nsystems, 30, 2017.\nSpringer Nature 2023 L ATEX template\nArticle Title 21\n[37] Liang He, Shizhuo Zhang, Lijun W u, Huanhuan Xia, F usong Ju,\nHe Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, Pan Deng, et al. Pre-\ntraining co-evolutionary protein representation via a pairwise masked\nlanguage model. arXiv preprint arXiv:2110.15527 , 2021.\n[38] Michael Heinzinger, Ahmed Elnaggar, Y u W ang, Christian Dallago,\nDmitrii Nechaev, Florian Matthes, and Burkhard Rost. Modeling aspects\nof the language of life through transfer-learning protein sequences. BMC\nbioinformatics, 20(1):1ś17, 2019.\n[39] Pedro Hermosilla and Timo Ropinski. Contrastive representation\nlearning for 3d protein structures. arXiv preprint arXiv:2205.15675 ,\n2022.\n[40] Chloe Hsu, Robert V erkuil, Jason Liu, Zeming Lin, Brian Hie, T om\nSercu, Adam Lerer, and Alexander Rives. Learning inverse folding from\nmillions of predicted structures. bioRxiv, 2022.\n[41] Ilia Igashov, Nikita Pavlichenko, and Sergei Grudinin. Spherical convolu-\ntions on molecular graphs for protein model quality assessment. Machine\nLearning: Science and Technology , 2(4):045005, 2021.\n[42] John Ingraham, Vikas Garg, Regina Barzilay , and T ommi Jaakkola.\nGenerative models for graph-based protein design. Advances in neural\ninformation processing systems , 32, 2019.\n[43] José Jiménez, Stefan Doerr, Gerard Martínez-Rosell, Alexander S Rose,\nand Gianni De F abritiis. Deepsite: protein-binding site predictor using\n3d-convolutional neural networks. Bioinformatics, 33(19):3036ś3042,\n2017.\n[44] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL T own-\nshend, and Ron Dror. Learning from protein structure with geometric\nvector perceptrons. arXiv preprint arXiv:2009.01411 , 2020.\n[45] Bowen Jing, Stephan Eismann, Pratham N Soni, and Ron O Dror. Equiv-\nariant graph neural networks for 3d macromolecular structure. arXiv\npreprint arXiv:2106.03843 , 2021.\n[46] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael\nFigurnov, Olaf Ronneberger, Kathryn T unyasuvunakool, Russ Bates,\nAugustin Žídek, Anna Potapenko, et al. Highly accurate protein\nstructure prediction with alphafold. Nature, 596(7873):583ś589, 2021.\n[47] Mikhail Karasikov, Guillaume Pagès, and Sergei Grudinin. Smooth\norientation-dependent scoring function for coarse-grained protein quality\nassessment. Bioinformatics, 35(16):2801ś2808, 2019.\nSpringer Nature 2023 L ATEX template\n22 Article Title\n[48] Mostafa Karimi, Di W u, Zhangyang W ang, and Y ang Shen. Deepaffinity:\ninterpretable deep learning of compoundśprotein affinity through uniőed\nrecurrent and convolutional neural networks. Bioinformatics, 35(18):\n3329ś3338, 2019.\n[49] Lisa N Kinch, R Dustin Schaeffer, Andriy Kryshtafovych, and Nick V\nGrishin. T arget classiőcation in the 14th round of the critical assessment\nof protein structure prediction (casp14). Proteins: Structure, Function,\nand Bioinformatics , 89(12):1618ś1632, 2021.\n[50] Johannes Klicpera, Shankari Giri, Johannes T Margraf, and Stephan\nGünnemann. F ast and uncertainty-aware directional message passing for\nnon-equilibrium molecules. arXiv preprint arXiv:2011.14115 , 2020.\n[51] Johannes Klicpera, Janek Groß, and Stephan Günnemann. Directional\nmessage passing for molecular graphs. arXiv preprint arXiv:2003.03123 ,\n2020.\n[52] Andriy Kryshtafovych, T orsten Schwede, Maya T opf, Krzysztof Fidelis,\nand John Moult. Critical assessment of methods of protein struc-\nture prediction (casp)Ðround xiii. Proteins: Structure, Function, and\nBioinformatics, 87(12):1011ś1020, 2019.\n[53] Jaechang Lim, Seongok Ryu, Kyubyong Park, Y o Joong Choe, Jiyeon\nHam, and W oo Y oun Kim. Predicting drugśtarget interaction using a\nnovel graph neural network with 3d structure-embedded graph represen-\ntation. Journal of chemical information and modeling , 59(9):3981ś3988,\n2019.\n[54] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, W ent-\ning Lu, Allan dos Santos Costa, Maryam F azel-Zarandi, T om Sercu, Sal\nCandido, et al. Language models of protein sequences at the scale of\nevolution enable accurate structure prediction. bioRxiv, 2022.\n[55] Yi Liu, Hao Y uan, Lei Cai, and Shuiwang Ji. Deep learning of high-order\ninteractions for protein interface prediction. In Proceedings of the 26th\nACM SIGKDD international conference on knowledge discovery & data\nmining, pages 679ś687, 2020.\n[56] Yi Liu, Limei W ang, Meng Liu, Y uchao Lin, Xuan Zhang, Bora Oztekin,\nand Shuiwang Ji. Spherical message passing for 3d molecular graphs. In\nInternational Conference on Learning Representations , 2021.\n[57] Zhihai Liu, Y an Li, Li Han, Jie Li, Jie Liu, Zhixiong Zhao, W ei Nie,\nY uchen Liu, and Renxiao W ang. Pdb-wide collection of binding data:\ncurrent status of the pdbbind database. Bioinformatics, 31(3):405ś412,\n2015.\nSpringer Nature 2023 L ATEX template\nArticle Title 23\n[58] Amy X Lu, Haoran Zhang, Marzyeh Ghassemi, and Alan Moses. Self-\nsupervised contrastive learning of protein representations by mutual\ninformation maximization. BioRxiv, 2020.\n[59] Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Nam-\nrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher.\nProgen: Language modeling for protein generation. arXiv preprint\narXiv:2004.03497, 2020.\n[60] Joshua Meier, Roshan Rao, Robert V erkuil, Jason Liu, T om Sercu,\nand Alex Rives. Language models enable zero-shot prediction of the\neffects of mutations on protein function. Advances in Neural Information\nProcessing Systems , 34:29287ś29303, 2021.\n[61] Jaina Mistry , Sara Chuguransky , Lowri Williams, Matloob Qureshi,\nGustavo A Salazar, Erik LL Sonnhammer, Silvio CE T osatto, Lisanna\nPaladin, Shriya Raj, Lorna J Richardson, et al. Pfam: The protein\nfamilies database in 2021. Nucleic acids research , 49(D1):D412śD419,\n2021.\n[62] Ryan Murphy , Balasubramaniam Srinivasan, Vinayak Rao, and Bruno\nRibeiro. Relational pooling for graph representations. In International\nConference on Machine Learning , pages 4663ś4673. PMLR, 2019.\n[63] Thin Nguyen, Hang Le, Thomas P Quinn, T ri Nguyen, Thuc Duy Le, and\nSvetha V enkatesh. Graphdta: Predicting drugśtarget binding affinity\nwith graph neural networks. Bioinformatics, 37(8):1140ś1147, 2021.\n[64] Kliment Olechnovič and Česlovas V enclovas. V oromqa: Assessment of\nprotein structure quality using interatomic contact areas. Proteins:\nStructure, Function, and Bioinformatics , 85(6):1131ś1145, 2017.\n[65] Guillaume Pagès, Benoit Charmettant, and Sergei Grudinin. Pro-\ntein model quality assessment using 3d oriented convolutional neural\nnetworks. Bioinformatics, 35(18):3313ś3319, 2019.\n[66] Adam Paszke, Sam Gross, F rancisco Massa, Adam Lerer, James Brad-\nbury , Gregory Chanan, T revor Killeen, Zeming Lin, Natalia Gimelshein,\nLuca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library . Advances in neural information processing systems , 32,\n2019.\n[67] Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and\nDavid Ryan Koes. Proteinśligand scoring with convolutional neural net-\nworks. Journal of chemical information and modeling , 57(4):942ś957,\n2017.\nSpringer Nature 2023 L ATEX template\n24 Article Title\n[68] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Y an Duan, Peter\nChen, John Canny , Pieter Abbeel, and Y un Song. Evaluating protein\ntransfer learning with tape. Advances in neural information processing\nsystems, 32, 2019.\n[69] Roshan Rao, Joshua Meier, T om Sercu, Sergey Ovchinnikov, and Alexan-\nder Rives. T ransformer protein language models are unsupervised\nstructure learners. Biorxiv, 2020.\n[70] Roshan M Rao, Jason Liu, Robert V erkuil, Joshua Meier, John Canny ,\nPieter Abbeel, T om Sercu, and Alexander Rives. Msa transformer.\nIn International Conference on Machine Learning , pages 8844ś8856.\nPMLR, 2021.\n[71] Alexander Rives, Joshua Meier, T om Sercu, Siddharth Goyal, Zeming\nLin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma,\net al. Biological structure and function emerge from scaling unsupervised\nlearning to 250 million protein sequences. Proceedings of the National\nAcademy of Sciences , 118(15):e2016239118, 2021.\n[72] Jeffrey A Ruffolo and Jeffrey J Gray . F ast, accurate antibody struc-\nture prediction from deep learning on massive set of natural antibodies.\nBiophysical Journal , 121(3):155aś156a, 2022.\n[73] Vıctor Garcia Satorras, Emiel Hoogeboom, and Max W elling. E (n)\nequivariant graph neural networks. In International conference on\nmachine learning , pages 9323ś9332. PMLR, 2021.\n[74] Kristof Schütt, Pieter-Jan Kindermans, Huziel Enoc Sauceda F elix, Ste-\nfan Chmiela, Alexandre Tkatchenko, and Klaus-Robert Müller. Schnet:\nA continuous-őlter convolutional neural network for modeling quantum\ninteractions. Advances in neural information processing systems , 30,\n2017.\n[75] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gal-\nligher, and Tina Eliassi-Rad. Collective classiőcation in network data.\nAI magazine , 29(3):93ś93, 2008.\n[76] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick,\nLaurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR\nNelson, Alex Bridgland, et al. Protein structure prediction using multiple\ndeep neural networks in the 13th critical assessment of protein structure\nprediction (casp13). Proteins: Structure, Function, and Bioinformatics ,\n87(12):1141ś1148, 2019.\n[77] T om Sercu, Robert V erkuil, Joshua Meier, Brandon Amos, Zeming Lin,\nCaroline Chen, Jason Liu, Y ann LeCun, and Alexander Rives. Neural\nSpringer Nature 2023 L ATEX template\nArticle Title 25\npotts model. bioRxiv, 2021.\n[78] Jung-Eun Shin, Adam J Riesselman, Aaron W Kollasch, Conor McMa-\nhon, Elana Simon, Chris Sander, Aashish Manglik, Andrew C Kruse,\nand Debora S Marks. Protein design and variant prediction using\nautoregressive generative models. Nature communications , 12(1):1ś11,\n2021.\n[79] Vignesh Ram Somnath, Charlotte Bunne, and Andreas Krause. Multi-\nscale representation learning on proteins. Advances in Neural Informa-\ntion Processing Systems , 34:25244ś25255, 2021.\n[80] Hannes Stärk, Octavian Ganea, Lagnajit Pattanaik, Regina Barzilay ,\nand T ommi Jaakkola. Equibind: Geometric deep learning for drug\nbinding structure prediction. In International Conference on Machine\nLearning, pages 20503ś20521. PMLR, 2022.\n[81] Alexey Strokach, David Becerra, Carles Corbi-V erge, Albert Perez-Riba,\nand Philip M Kim. F ast and ŕexible protein design using deep graph\nneural networks. Cell systems , 11(4):402ś411, 2020.\n[82] Pascal Sturmfels, Jesse Vig, Ali Madani, and Nazneen F atema Rajani.\nProőle prediction: An alignment-based pre-training task for protein\nsequence models. arXiv preprint arXiv:2012.00195 , 2020.\n[83] Jie T ang, Jing Zhang, Limin Y ao, Juanzi Li, Li Zhang, and Zhong\nSu. Arnetminer: extraction and mining of academic social networks.\nIn Proceedings of the 14th ACM SIGKDD international conference on\nKnowledge discovery and data mining , pages 990ś998, 2008.\n[84] Nathaniel Thomas, T ess Smidt, Steven Kearnes, Lusann Y ang, Li Li,\nKai Kohlhoff, and Patrick Riley . T ensor őeld networks: Rotation-and\ntranslation-equivariant neural networks for 3d point clouds. arXiv\npreprint arXiv:1802.08219 , 2018.\n[85] Raphael T ownshend, Rishi Bedi, Patricia Suriana, and Ron Dror. End-\nto-end learning on 3d protein structure for interface prediction. Advances\nin Neural Information Processing Systems , 32, 2019.\n[86] Raphael JL T ownshend, Martin Vögele, Patricia Suriana, Alexander\nDerry , Alexander Powers, Yianni Laloudakis, Sidhika Balachandar,\nBowen Jing, Brandon Anderson, Stephan Eismann, et al. Atom3d: T asks\non molecules in three dimensions. arXiv preprint arXiv:2012.04035 ,\n2020.\n[87] Jeanne T rinquier, Guido Uguzzoni, Andrea Pagnani, F rancesco Zamponi,\nand Martin W eigt. Efficient generative modeling of protein sequences\nSpringer Nature 2023 L ATEX template\n26 Article Title\nusing simple autoregressive models. Nature communications, 12(1):1ś11,\n2021.\n[88] Karolis Uziela, David Menéndez Hurtado, Nanjiang Shu, Björn W allner,\nand Arne Elofsson. Proq3d: improved model quality assessments using\ndeep learning. Bioinformatics, 33(10):1578ś1580, 2017.\n[89] Sameer V elankar, José M Dana, Julius Jacobsen, Glen V an Ginkel,\nPaul J Gane, Jie Luo, Thomas J Oldőeld, Claire O’Donovan, Maria-\nJesus Martin, and Gerard J Kleywegt. Sifts: structure integration with\nfunction, taxonomy and sequences resource. Nucleic acids research , 41\n(D1):D483śD489, 2012.\n[90] Jesse Vig, Ali Madani, Lav R V arshney , Caiming Xiong, Richard Socher,\nand Nazneen F atema Rajani. Bertology meets biology: interpreting\nattention in protein language models. arXiv preprint arXiv:2006.15222 ,\n2020.\n[91] Thom V reven, Iain H Moal, Anna V angone, Brian G Pierce, Panagiotis L\nKastritis, Mieczyslaw T orchala, Raphael Chaleil, Brian Jiménez-García,\nPaul A Bates, Juan F ernandez-Recio, et al. Updates to the integrated\nproteinśprotein interaction benchmarks: docking benchmark version 5\nand affinity benchmark version 2. Journal of molecular biology , 427(19):\n3031ś3041, 2015.\n[92] Guoxia W ang, Xiaomin F ang, Zhihua W u, Yiqun Liu, Y ang Xue, Yingfei\nXiang, Dianhai Y u, F an W ang, and Y anjun Ma. Helixfold: An effi-\ncient implementation of alphafold2 using paddlepaddle. arXiv preprint\narXiv:2207.05477, 2022.\n[93] Renxiao W ang, Xueliang F ang, Yipin Lu, and Shaomeng W ang. The\npdbbind database: Collection of binding affinities for protein- lig-\nand complexes with known three-dimensional structures. Journal of\nmedicinal chemistry , 47(12):2977ś2980, 2004.\n[94] Xiao W ang, Sean T Flannery , and Daisuke Kihara. Protein docking\nmodel evaluation by graph neural networks. Frontiers in Molecular\nBiosciences, page 402, 2021.\n[95] F ang W u, Qiang Zhang, Dragomir Radev, Jiyu Cui, W en Zhang,\nHuabin Xing, Ningyu Zhang, and Huajun Chen. 3d-transformer: Molec-\nular representation with transformer in 3d space. arXiv preprint\narXiv:2110.01191, 2021.\n[96] Zhenqin W u, Bharath Ramsundar, Evan N F einberg, Joseph Gomes,\nCaleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande.\nMoleculenet: a benchmark for molecular machine learning. Chemical\nSpringer Nature 2023 L ATEX template\nArticle Title 27\nscience, 9(2):513ś530, 2018.\n[97] Minkai Xu, Lantao Y u, Y ang Song, Chence Shi, Stefano Ermon, and Jian\nT ang. Geodiff: A geometric diffusion model for molecular conformation\ngeneration. arXiv preprint arXiv:2203.02923 , 2022.\n[98] Charles Y anofsky , Virginia Horn, and Deanna Thorpe. Protein structure\nrelationships revealed by mutational analysis. Science, 146(3651):1593ś\n1594, 1964.\n[99] Jian Zhang and Y ang Zhang. A novel side-chain orientation depen-\ndent potential derived from random-walk reference state for protein fold\nselection and structure prediction. PloS one , 5(10):e15386, 2010.\n[100] Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan,\nAurelie Lozano, Payel Das, and Jian T ang. Protein representation learn-\ning by geometric structure pretraining. arXiv preprint arXiv:2203.06125 ,\n2022.\nSpringer Nature 2023 L ATEX template\n28 Article Title\nAppendix A Experimental Details\nA.1 Dataset Description\nA.1.1 Sequence Position Identiﬁcation\nW e use a subset of 3243 high-resolution structures from the PDB and adopt\na random split of 2643/330/330 for train/val/test. The distribution of the\nnumber of residues is plotted in Figure\nA1. F or the train set, the maximum\nand the minimum number of residues are 6248 and 43 separately . The mean\nand standard deviation of the number of residues are 389.9 and 385.7. F or the\nvalidation set, the maximum and the minimum number of residues are 9999\nand 59 separately . The mean and standard deviation of the number of residues\nare 454.3 and 772.0. F or the test set, the maximum and the minimum number\nof residues are 2326 and 59 separately . The mean and standard deviation of\nthe number of residues are 383.2 and 286.2.\n0 2000 4000 6000 8000 10000\n0\n100\n200\n300\n400\nFrequency\nFrequency Histogram of Sequence Length\ntrain\nval\ntest\nFig. A1 : The histogram of the sequence length\nA.1.2 Model Quality Assessment\nThe Critical Assessment of Structure Prediction (CASP) [\n52] is a long-running\ninternational competition held biennially , of which CASP13 is the most recent\nSpringer Nature 2023 L ATEX template\nArticle Title 29\nthat addresses the protein structure prediction problem by withholding newly\nsolved experimental structures. Mirroring the setup of the competition, we\nfollow T ownshend et al. [\n86] and split the decoy sets based on target and\nreleased year. W e choose CASP11 as the test set, as the targets in CASP12-13\nare not fully released yet. This leads to a dataset split of 25400/2800/16014\nfor train/val/test.\nA.1.3 Protein-protein Rigid-body Docking.\nW e use the DB5.5 database, which is obtained from\nhttps://zlab.umassmed.\nedu/benchmark/. It is randomly partitioned in train/val/test splits of sizes\n203/25/25. It is worth mentioning that DB5.5 also includes unbound protein\nstructures, however, which mostly show rigid structures.\nA.1.4 Protein-protein Interface Prediction\nW e adopt a part of the DIPS database and use a data split of 12216/1526/1526\nfor train/val/test. Each complex is an ensemble, with the bound ligand and\nbound receptor structures forming 2 distinct sub-units of the ensemble. W e\nthen deőne the neighboring amino acids as those with any α-carbon within 8Å\nof one another. These neighbors are then included as the positive samples, with\nall other residues as negatives. At prediction time, we attempt to re-predict\nwhich possible residues are positive or negative. In other words, we desire to\ndetermine whether each residue is located in the binding pocket. AUROC of\nthose predictions is used as the metric to evaluate performance.\nA.1.5 Ligand Aﬃnity Prediction\nPDBbind contains X-ray structures of proteins bound to small molecules and\npeptide ligands, W e use the dataset mined from PBDbind by T ownshend\net al. [\n86], which has two splits based on 30% and 60% sequence identity\nthresholds, respectively . Splitting using 30% sequence identity results in train/-\nval/test of 3507/466/490, while splitting using 60% sequence identity results\nin train/val/test of 3678/460/460.\nA.2 Backbone Architecture\nF or tasks that only require the predictive model to output a scalar for the\nprotein/complex or each residue including model quality assessment, protein-\nprotein interface prediction, and ligand binding affinity prediction, we select\nGVP-GNN, EGNN, and Molformer as the backbone architecture. F or more\ncomplicated tasks that require more complex computational processes such as\nprotein-protein rigid-body docking, we use speciőc models to address them like\nEquidock.\nGVP-GNN.\nGVP-GNN [\n44, 45] is an equivariant GNN in which all node and edge\nembeddings are tuples (s, V) of scalar feature and geometric vector features.\nSpringer Nature 2023 L ATEX template\n30 Article Title\nMessage and update functions are parameterized by geometric vector per-\nceptrons (GVPs) ś modules mapping between tuples (s, V) while preserving\nrotation equivariance. Its computational process is described in Algorithm 1,\nwhere s and V correspond to the node embedding h and coordinates x\nseparately . :\nAlgorithm 1 GVP-GNN\n1: Input: Scalar and vector features (s, V) ∈Rn ×Rν×3.\n2: Output: Scalar and vector features (s′, V′) ∈Rn ×Rµ×3.\n3: h←max(ν,µ )\n4: Vh ←WhV ∈Rh×3\n5: Vµ ←WµVh ∈Rµ×3\n6: sh ←∥Vh∥2 (row-wise) ∈Rh\n7: vµ ←∥Vµ∥2 ( row-wise) ∈Rµ\n8: sh+n ←concat (sh, s) ∈Rh+n\n9: sm ←Wm sh+n + b ∈Rm\n10: s′ ←σ(sm) ∈Rm\n11: V′ ←σ+ (vµ) ⊙Vµ (row-wise multiplication) ∈Rµ×3\n12: Return: (s′, V′)\nAt its core, GVP-GNN consists of two separate linear transformations Wm\nand Wh for the scalar and vector features, followed by nonlinearities σ,σ +. An\nadditional linear transformation Wµ is inserted before the vector nonlinearity\nto control the output dimensionality independently of the number of norms\nextracted. W e adopt a 5-layer GVP-GNN with a dropout rate of 0.7 and a\nReLU activation function. The number of radial bases in the edge embedding\nis 16 and the node dimension is set as (100, 16). All implementation codes are\ndownloaded from the official repository in\nhttps://github.com/drorlab/gvp.\nEGNN.\nEGNN [\n73] achieves equivariance without expensive high-order representations\nin intermediate layers and also realizes competitive performance. Its Equiv-\nariant Graph Convolutional Layer (EGCL) takes the set of node embedding\nhl = {hl\ni}i=1,...,N , the coordinate embeddings xl = {xl\ni}i=1,...,N and edge infor-\nmation E = (eij) as input, and then outputs a transformation on hl+1 and\nSpringer Nature 2023 L ATEX template\nArticle Title 31\nxl+1. Concisely , The equations that deőne this layer are described as follows:\nmij = φe\n(\nhl\ni, hl\nj,\n\nxl\ni −xl\nj\n\n2\n,a ij\n)\n,\nxl+1\ni = xl\ni + C\n∑\nj̸=i\n(\nxl\ni −xl\nj\n)\nφx(mij) ,\nmi =\n∑\nj̸=i\nmij,\nhl+1\ni = φh\n(\nhl\ni, mi\n)\n,\n(A1)\nwhere hl\ni ∈Rnf is the nf_dimensional embedding of node vi at layer l. aij are\nthe edge attributes. φe and φh are the edge and node operations respectively\nwhich are commonly approximated by Multi-layer Perceptrons (MLPs). φx :\nRnf →R1 is the function that takes the edge embedding mij as input from the\nprevious edge operation and outputs a scalar value. C is chosen to be 1/ ∥Ni∥,\nwhich divides the sum by its number of neighboring (connected) elements. W e\nchoose a 4-layer EGNN with a Siwsh activation function as a non-linearity .\nThe number of the dimension for edges is 16 and residue connections are\nused. All implementation codes are downloaded from the official repository\nin\nhttps://github.com/vgsatorras/egnn.\nMolformer.\nMolformer [\n95] is a variant of T ransformer that employs a heterogeneous self-\nattention layer to differentiate the interactions between multi-level nodes. Here\nwe use a weaker version of Molformer. T o be explicit, we do not extract any\nsort of motifs from either protein or small molecules. Besides, we abandon the\nmulti-scale self-attention mechanism and the Attentive F arthest Point Sam-\npling (AFPS) for more efficient computations and only use the global features.\nEven though we pick up a simpliőed form of Molformer, its performance is\ncompetitive with or even outperforms EGNN and GVP-GNN on all tasks. The\nMolformer architecture has 2 layers, 4 heads, a hidden-layer dimension of 1280,\nand a dropout rate of 0.1. All implementation codes are downloaded from the\nofficial repository in\nhttps://github.com/smiles724/molformer.\nEquiDock.\nEquidock [\n32] predicts the rotation and translation to place on of the proteins\nat the right docked position relative to the second protein. It adopts an Inde-\npendent E(3)-Equivariant Graph Matching Network (IEGMN), which extends\nboth Graph Matching Networks (GMN) and EGNN. It performs node coor-\ndinate and feature embedding updates for an input pair of protein graphs\nand uses inter- and inter-node messages, as well as E(3)-equivariant coordi-\nnate updates. The backbone IEGMN has 5 layers and no dropout. It uses\nLeakyReLU as the activation function. and does not use distance as an edge\nfeature. All implementation codes are downloaded from the official repository\nin\nhttps://github.com/octavian-ganea/equidock_public.\nSpringer Nature 2023 L ATEX template\n32 Article Title\nA.3 T raining Details\nW e run all experiments on 2 A100 GPUs, each with a memory of 80G. F or\nMQA, PPRD and PPI, we use residue-level graphs for protein representation\nlearning. F or PPRD, models are trained using Adam with a learning rate of\n2e-4 and early stopping with patience of 30 epochs. F or MQA, PPI, and LBA,\nmodels are trained using Adam with a learning rate of 1e-4 and early stopping\nwith patience of 8 epochs. A Plateau learning rate scheduler is applied with a\nfactor of 0.6, patience of 5, and a minimum learning rate of 5e-7. The batch\nsize is 32 if no out-of-memory (OOM) error is not triggered, otherwise, we\nadopt a batch size of 16. The maximum epoch is set as 200.\nF or PPRD, we randomly assign the roles of ligands and receptors during\ntraining. F or PPI, as mentioned before, we formulate interface as the residues\nwhose least distances to their counterpart protein are within 8 Å. F or LBA,\nwe only use the residues within a distance of 6 Å from the ligand ( i.e., the\npocket) following T ownshend et al. [\n86]. W e build heterogeneous molecular\ngraphs where nodes for proteins are residue-level and nodes for ligands are\natom-level. Here we do not distinguish different atom types and simply regard\nall atoms as the same group, which is denoted as the new ’LIG’ pseudo residue\nclass.\nF or the protein language model, we use the ESM-2 with a parameter size of\n650M and 33 layers as the default one. It is trained on UR50/D2021_04 and\nhas an embedding dimension of 1280. W e extract per-residue representations\nas the input for each task. F or the ablation study , we adopt ESM-2 with\nparameter sizes of 8M, 35M, 150M, and 3B that are all trained on the same\nUR50/D2021_04 dataset with different layers of 6, 12, 30, and 36 respectively .\nF or more details, please visit the official website of ESM in\nhttps://github.\ncom/facebookresearch/esm.\nAppendix B Limitations\nIn spite of our successful conőrmation that protein language models can pro-\nmote geometric deep learning, there are several limitations and extensions of\nour framework left open for future investigation. First, our work only exam-\nines the efficacy of the state-of-the-art protein language model, ESM-2. It\nshould be more convincing if more types of protein language models like\nMSA-T ransformer and ProtT rans are veriőed. Second, our 3D protein graphs\nare residue-level. W e believe atom-level protein graphs also beneőt from our\napproach, but its increase in performance needs further exploration."
}