{
    "title": "In-House Knowledge Management Using a Large Language Model: Focusing on Technical Specification Documents Review",
    "url": "https://openalex.org/W4392361835",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2955805447",
            "name": "Jooyeup Lee",
            "affiliations": [
                "Korea Electric Power Corporation (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A2146805440",
            "name": "Woo-Yong Jung",
            "affiliations": [
                "Korea Electric Power Corporation (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A2106391387",
            "name": "Seungwon Baek",
            "affiliations": [
                "Korea Institute of Civil Engineering and Building Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2955805447",
            "name": "Jooyeup Lee",
            "affiliations": [
                "Korea Electric Power Corporation (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A2146805440",
            "name": "Woo-Yong Jung",
            "affiliations": [
                "Korea Electric Power Corporation (South Korea)"
            ]
        },
        {
            "id": "https://openalex.org/A2106391387",
            "name": "Seungwon Baek",
            "affiliations": [
                "Korea Institute of Civil Engineering and Building Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4200547267",
        "https://openalex.org/W2030051975",
        "https://openalex.org/W2114630980",
        "https://openalex.org/W6761887019",
        "https://openalex.org/W2080060217",
        "https://openalex.org/W1973276668",
        "https://openalex.org/W2003500890",
        "https://openalex.org/W1995132632",
        "https://openalex.org/W2076383389",
        "https://openalex.org/W2063152008",
        "https://openalex.org/W1963978108",
        "https://openalex.org/W3094054110",
        "https://openalex.org/W4389672268",
        "https://openalex.org/W4361002760",
        "https://openalex.org/W2095659828",
        "https://openalex.org/W2012854775",
        "https://openalex.org/W2974708838",
        "https://openalex.org/W6809396288",
        "https://openalex.org/W3210245152",
        "https://openalex.org/W2083588432",
        "https://openalex.org/W2015130098",
        "https://openalex.org/W2146685020",
        "https://openalex.org/W2038053623",
        "https://openalex.org/W2128868530",
        "https://openalex.org/W3190730109",
        "https://openalex.org/W4384981928",
        "https://openalex.org/W4312198711",
        "https://openalex.org/W4360980513",
        "https://openalex.org/W4386187806",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4381480279",
        "https://openalex.org/W3199761064",
        "https://openalex.org/W4296965999",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W6837789219",
        "https://openalex.org/W4220674484",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2944016842",
        "https://openalex.org/W4280534475"
    ],
    "abstract": "In complex construction projects, technical specifications have to be reviewed in a short period of time. Even experienced engineers find it difficult to review every detail of technical specifications. In addition, it is not easy to transfer experienced knowledge to junior engineers. With the technological innovation of large language models such as ChatGPT, a fine-tuned language model is proposed as an effective solution for the automatic review of technical specification documents. Against this backdrop, this study examines the in-house technical specification documents that are not publicly available. Then, two fine-tuned large language models, GPT-3 and LLaMA2, are trained to answer questions related to technical specification documents. The results show that the fine-tuned LLaMA2 model generally outperforms the fine-tuned GPT-3 model in terms of accuracy, reliability, and conciseness of responses. In particular, the fine-tuned LLaMA2 model suppressed hallucinogenic effects better than the fine-tuned GPT-3 model. Based on the results, this study discussed the applicability and limitations of a fine-tuned large language model for in-house knowledge management. The results of this study are expected to assist practitioners in developing a domain-specific knowledge management solution by fine-tuning an open-source large language model with private datasets.",
    "full_text": null
}