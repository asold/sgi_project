{
    "title": "Transformer based on channel-spatial attention for accurate classification of scenes in remote sensing image",
    "url": "https://openalex.org/W4295782942",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2315016661",
            "name": "Jingxia Guo",
            "affiliations": [
                "Baotou Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A1967839315",
            "name": "Nan JIA",
            "affiliations": [
                "Baotou Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2541458894",
            "name": "Jinniu Bai",
            "affiliations": [
                "Baotou Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A2315016661",
            "name": "Jingxia Guo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967839315",
            "name": "Nan JIA",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2541458894",
            "name": "Jinniu Bai",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3105392204",
        "https://openalex.org/W2980346985",
        "https://openalex.org/W3083219173",
        "https://openalex.org/W2770844153",
        "https://openalex.org/W3122050197",
        "https://openalex.org/W2478271901",
        "https://openalex.org/W2959701797",
        "https://openalex.org/W3022140654",
        "https://openalex.org/W1157380099",
        "https://openalex.org/W2592962403",
        "https://openalex.org/W2940726923",
        "https://openalex.org/W3175037443",
        "https://openalex.org/W3168462391",
        "https://openalex.org/W2897086142",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W3095319281",
        "https://openalex.org/W3043257208",
        "https://openalex.org/W3210372881",
        "https://openalex.org/W3115994626",
        "https://openalex.org/W2997240345",
        "https://openalex.org/W3113877107",
        "https://openalex.org/W3039010446",
        "https://openalex.org/W2944971001",
        "https://openalex.org/W3128592650",
        "https://openalex.org/W3207506706",
        "https://openalex.org/W4214910799",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6828894009",
        "https://openalex.org/W1989316905",
        "https://openalex.org/W2515866431",
        "https://openalex.org/W2621526417",
        "https://openalex.org/W2746325398",
        "https://openalex.org/W2783165089",
        "https://openalex.org/W2991682101",
        "https://openalex.org/W2988666519",
        "https://openalex.org/W2766049824",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3040156639",
        "https://openalex.org/W2620429297",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W3204002518",
        "https://openalex.org/W3105577662"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports\nTransformer based \non channel‑spatial attention \nfor accurate classification of scenes \nin remote sensing image\nJingxia Guo*, Nan Jia & Jinniu Bai*\nRecently, the scenes in large high‑resolution remote sensing (HRRS) datasets have been classified \nusing convolutional neural network (CNN)‑based methods. Such methods are well ‑suited for spatial \nfeature extraction and can classify images with relatively high accuracy. However, CNNs do not \nadequately learn the long‑distance dependencies between images and features in image processing, \ndespite this being necessary for HRRS image processing as the semantic content of the scenes in these \nimages is closely related to their spatial relationship. CNNs also have limitations in solving problems \nrelated to large intra‑class differences and high inter‑class similarity. To overcome these challenges, \nin this study we combine the channel‑spatial attention (CSA) mechanism with the Vision Transformer \nmethod to propose an effective HRRS image scene classification framework using Channel ‑Spatial \nAttention Transformers (CSAT). The proposed model extracts the channel and spatial features of HRRS \nimages using CSA and the Multi‑head Self‑Attention (MSA) mechanism in the transformer module. \nFirst, the HRRS image is mapped into a series of multiple planar 2D patch vectors after passing to \nthe CSA. Second, the ordered vector is obtained via the linear transformation of each vector, and \nthe position and learnable embedding vectors are added to the sequence vector to capture the \ninter‑feature dependencies at a distance from the generated image. Next, we use MSA to extract \nimage features and the residual network structure to complete the encoder construction to solve \nthe gradient disappearance problem and avoid overfitting. Finally, a multi‑layer perceptron is used \nto classify the scenes in the HRRS images. The CSAT network is evaluated using three public remote \nsensing scene image datasets: UC‑Merced, AID, and NWPU‑RESISC45. The experimental results show \nthat the proposed CSAT network outperforms a selection of state‑of‑the‑art methods in terms of \nscene classification.\nThe rapid development of remote sensing technology and the continuous increase in the number of satellites has \nprovided a wealth of data sources for ground  surveys1–3, as well as a solid foundation for the interpretation of \ncomplex and high-resolution remote sensing images. Remote sensing image classification has been extensively \nresearched, laying the foundation for the effective analysis of practical applications such as urban  planning4, \ngeospatial object  detection 5,6, vegetation  mapping7, and environmental  monitoring8. Remote sensing image \nclassification is generally divided into three categories: pixel-, object-, and scene-level 9. With the continuous \nimprovement in remote sensing image resolution, and an increasing number of images containing different \ntarget categories, pixel- and object- classification methods can no longer meet the requirements for the accurate \nclassification of complex remote sensing images. Accordingly, researchers have recently adopted a method to \nautomatically classify images with a specific semantic label based on the content of the entire image (i.e., remote \nsensing image scene classification) to extract high-level semantic information from remote sensing  images10 and \nprovide auxiliary references to understand what appears in the images. However, scene classification capable of \naccounting for image semantics is still a challenging task due to the complex spatial distribution of objects in \nscenes and the diverse types of land  cover11.\nThe primary goal of high-resolution remote sensing (HRRS) image scene classification is to correctly classify \na given remote sensing image according to its content (e.g., commercial, industrial, or residential areas)12. Clas-\nsification performance largely depends on features that accurately represent the scene in the image, and thus the \nOPEN\nBaotou Medical College, Baotou 014040, Inner Mongolia, China. *email: 102008168@btmc.edu.cn; baijinniu@163.\ncom\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nextraction of features that describe an image more accurately has become a primary research focus. Recently, \nconvolutional neural networks (CNNs) have been widely utilized in scene classification because they are capable \nof extracting high-level semantic feature representations for scene  classification13–15. However, the spatial rela-\ntionships between features in HRRS images are complex, and there is a large amount of redundant information. \nConsequently, it is difficult to directly extract the features that reflect the key information of the image content. \nThe human visual attention mechanism involves obtaining detailed information of the target region by scanning \nthe global area. Similarly, the attention mechanism in CNNs simulates the way humans understand and perceive \nimages by assigning different weights to global features, highlighting key local features, and suppressing invalid \nfeatures. For example, Park et al.16 proposed a simple and effective bottleneck attention module (BAM) that can be \nintegrated into any CNNs architecture. It focuses on high-level semantic features by allocating feature weights to \ninput images through an effective combination of spatial- and channel-independent paths. Woo et al.17 proposed \na convolutional block attention module (CBAM), which is a lightweight general attention module. CBAM infers \nthe attention map along the spatial and channel dimensions and then assigns weights to features, which can be \nseamlessly integrated into CNN architectures. In addition, Yu et al. 18 adopted an improved channel attention \nmechanism to enhance features at different levels. Furthermore, Tong et al.19 introduced an attentional mecha-\nnism in the channel dimension that adaptively enhanced the weights of important feature channels and inhibited \nsecondary feature channels. Remote sensing images not only contain rich channel information but also spatial \ninformation. To fully extract their features, Ma et al.20 proposed an adaptive multi-scale spatial attention module \n(AMSA) and an adaptive multi-scale channel attention module (AMCA) based on image characteristics by adopt-\ning adaptive expansion rate selection strategy (ADR-SS), so as to increase the diversity of extracted features. Zhu \net al.21 designed an adaptive spatial attention module (ASA-Module) and an adaptive channel attention module \n(ACA-Module) to strengthen spatial features from both larger- with smaller-sized targets and spectral features \namong channels. Zhu et al.22 designed a spatial attention module (SA-module) and a channel attention module \n(CA-module), Ma et al.23 designed a local spatial attention module (LSA-module) and a global channel atten-\ntion module (GCA-module), which not only highlight the advantages of spatial resolution and channel features \nbut also reduced the difference between features through the interaction between the two modules. Li et al. 24 \nadopted an enhanced attention mechanism to prompt the beneficial information in both spatial and channel \ndimensions to push the model to capture discriminative regions as much as possible. Guo et al. 25 proposed a \nglobal–local attention network (GLANet), which assigns different weights to different channels through global \nbranch learning, and local branch learning to improve relevant spatial attention regions and weaken background \nregions. These previous studies demonstrate that channel and spatial attention mechanisms play a certain role in \nenhancing the main features, and decisions are made on this basis according to the needs of the model. Based on \nthis property, our method first adopts the channel-spatial attention mechanism to focus on the key information \nin the image and form an attention map, which is used as the input to the encoder.\nAlthough CNNs excel in spatial feature extraction and can achieve relatively high classification accuracy, \nthere are some limitations. First, the receptive field of CNNs is limited by the size of the convolution kernel, \nwhich introduces difficulties in capturing global information. Second, CNNs are not suitable for mining long-\nrange dependencies inside image scenes. The potential spatial topological relationship can be readily ignored, \nand CNNs still have certain restrictions in processing large and small intraclass differences. Third, increasing \nthe number CNN layers can extract more features and increasing the size of the convolution kernel can obtain \na larger receptive field; however, this will greatly increase the complexity of the model and lead to the gradient \ndisappearance problem.\nRecently,  transformers26 have been applied in various vision tasks due to their excellent ability to capture \nlong-range dependencies and sequence-based image modeling. For example, the Vision Transformer (ViT) \ndemonstrates that the standard transformer architecture can achieve state-of-the-art performance in image \n classification27. On this basis, Bazi et al.28 applied the standard transformer structure to HRRS image classification \nand achieved better classification accuracy than through other advanced classification models. In addition, Deng \net al.29 proposed CTNet, which mines the semantic features in HRRS scene images through ViT and extracts \nlocal structural features in HRRS scene images via a CNN. Finally, these two features are combined to classify \nHRRS scenes. However, when a transformer processes an image, the image must be divided into patches, which \nlimits the ability of the model to learn the overall image characteristics. Therefore, Li et al.30 proposed the remote \nsensing transformer (TRS), which uses self-attention integrated into a residual neural network (ResNet), Multi-\nHead Self-Attention (MHSA) layers instead of spatial convolutions, and concatenates multiple pure transformer \nmodule encoders to improve the attention-dependent representation learning performance. Ma et al.31 proposed \na homo- heterogenous transformer learning (HHTL) framework for HRRS scene classification according to the \ncharacteristics of the transformer to divide the image into multiple patches.\nTo address the aforementioned problems, we designed a new HRRS image scene classification framework \nbased on a channel-spatial attention transformer (CSAT). First, the HRRS scene image is divided into patches, \nand local fine-grained features are extracted via the BAM attention mechanism to enhance the spatial and chan-\nnel features, thereby reducing redundant information in the image and improving the ability to obtain local \ninformation. Then, the image is transformed into a sequence via linear transformation, during which embedding \npositions are added to the patches to preserve positional information. Finally, the transformer module was used \nas an encoder to extract the image features, which can effectively learn global and local context information. \nThe experimental results for three public datasets used for HRRS image scene classification demonstrate the \neffectiveness of the proposed algorithm.\nIn this paper, we choose transformer instead of CNNs for feature extraction for the following reasons. First, \ntransformer is a new encoder-decoder architecture that relies on an attention mechanism to characterize the \nglobal dependencies between its input and  output32, overcoming the convolutional inductive bias of CNNs on \nthe overall input data insufficient grasp. Furthermore, CNNs cannot effectively extract long-distance dependent \n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nfeatures between global  data33. Second, the feature maps generated by the self-attention mechanism in trans-\nformer do not have the same spatial constraints as convolution calculations, can handle a larger receptive field \nthan conventional  convolutions34,35, and can effectively obtain global information. It is beneficial to explore the \nglobal contextual knowledge hidden in HRRS scenarios. To obtain global information, CNN needs to expand the \nreceptive field, use larger convolution kernels, and stack deeper convolution layers. However, as the number of \nlayers increases, the amount of information will be exhausted, and dimensional disasters may occur. Third, the \nself-attention mechanism layer in transformer will have a large amount of calculation for high-resolution input \ndata, while being more suitable for processing data scenarios with smaller spatial dimensions. Therefore, it is \npossible to process only small feature maps instead of the entire feature map space, which will inevitably result \nin a relatively small receptive field, which, however, is larger than the receptive field of the convolution kernel of \nthe convolution  operation36. This processing method is more suitable for mining more useful information from \ncomplex HRRS scene images.\nThe main contributions of this study can be summarized as follows.\n• A modified Transformer network model titled CSAT is designed to complete the HRRS scene classification \ntask. This method uses the channel-spatial attention mechanism and self-attention mechanisms to extract \nfeature information and avoid the loss of feature information.\n• The channel-spatial attention mechanism helps improve the network’s ability to obtain local information. It \nfocuses on fine-grained features in patches according to two independent paths (channel and space), which \nmitigates the effects of small differences between classes and large differences within classes.\n• HRRS produces various scenario categories and rich scenario information. The core process of the network \nis a multi-head self-attention encoder block, which successfully handles the long-range dependence of the \nspatial information in HRRS images.\n• The CSAT network introduces the transformer structure as an encoder to avoid the dimension disaster caused \nby too many layers and enhance the global modeling ability of the network.\n• Our proposed CSAT network is interpretable and offers an enhanced capability in extracting HRRS image \nfeatures and generalizations.\n• We propose a CSAT learning scheme that combines the contributions mentioned above. Experiments were \nconducted on three different scene classification datasets, and the results demonstrate that the proposed \nmethod outperforms state-of-the-art methods in terms of scene classification.\nMethods\nThe overall architecture of the CSAT is shown in Fig. 1, where the image input is sliced into evenly sized patches \nand sequential patches are fed into the CSA module to infer the attention patch (a detailed explanation is pre -\nsented in the following section). The attention patches are then transformed into a vector of patch embeddings \nvia flattening and linear projection. The embedding position is added to this projection and the category identity \nis sent as input to the transformer encoder along with the patch embedding vector. After a multi-layer perceptron \n(MLP) classifier is used for classification, the probability values  P1,  P2, …,  Pn are obtained for each category.\nFigure 1.  Structure of the channel-spatial attention transformer (CSAT) based on the transformer and channel-\nspatial attention module.\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nFigure 2.  Channel-spatial attention mechanism (CSAM).\nFigure 3.  Channel attention module (CAM).\nLong‑range‑dependent feature extraction of high‑resolution remote sensing images. Chan‑\nnel‑spatial attention mechanism for HRRS feature extraction. Attention mechanisms play an important role in \nthe human visual system by enabling the neural network to selectively focus on salient parts, removing redun-\ndant information, and efficiently extracting important features from images such as those obtained from HRRS.\nAttention mechanisms are widely used in computer vision tasks. For HRRS images with complex spatial infor-\nmation, we introduced a spatial-channel attention mechanism called the bottleneck attention module (BAM)16 \n(Fig. 2). This mechanism consists of two independent attention modules, the channel attention module (CAM) \nand the spatial attention module (SAM). To emphasize or suppress the information in a remote sensing image, \nCAM uses the inter-channel relationship and SAM uses the features of different spatial locations, respectively. \nAfter acquiring the attention maps separately, the \"residual structure\" proposed by  ResNet37 was used to gener-\nate the refined feature map F\n′\n . The process by which the CAM and SAM image feature extraction is performed \nis described in detail below.\n1. The CAM process (shown in Fig. 3) is described as follows.\n  Because each channel contains a specific feature response, CAM exploits the relationships between chan-\nnels to aggregate the feature maps in each channel and generates the attention map M c(F) . First, the feature \nmap F ∈ RC×H×W  is passed through the global AvgPool to obtain F c\navg for 1 × 1 × C channels, and then it \nenters the MLP with a hidden layer. To reduce the number of parameters, the activation size of the hidden \nlayer was set as R C/r×1×1, where r is the compression rate. Second, to adjust the scale of the CAM output, F c\navg \nis obtained as M c(F) ∈ RC×1×1 after entering the batch normalization (BN) layer of the MLP . The CAM was \ncomputed as:\nwhere W 0 ∈ R C/r×C,b0 ∈ R C/r,W 1 ∈ R C×C/r, andb1 ∈ R C.\n2. The SAM process (shown in Fig. 4) is described as follows.\nThe SAM generates spatial attention maps M s(F) , which are used to emphasize or suppress features in dif-\nferent spatial locations. First, the input feature map F ∈ RC×H×W  is subject to a 1 × 1 convolution operation, \nand then the F dimension is reduced to RC/r×H×W  to integrate and compress F across the channel dimensions, \nwhere r = 16 is consistent with the setting found in the  literature16. Second, we adopted two 3 × 3 convolutions to \n(1)\nM c(F) = BN\n(\nMLP\n(\nAvgPool(F)\n))\n,\n= BN\n(\nW 1\n(\nW 0AvgPool(F) + b0\n)\n+ b1\n)\n,\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nextract useful information from the context, and then adjusted the spatial attention map dimension to R1×H×W \nthrough a 1 × 1 convolution. Finally, to adjust the scale of the SAM output, M s(F)∈ R1×H×W was obtained after \nthe BN layer. The SAM is described as:\nwhere f i×i represents an i × i convolution operation with i =1, 3.\nTransformer encoder for HRRS classification. The  transformer18 mainly relies on the self-attention \nmechanism to construct a global dependency model between the input and output. Additionally, a standard \ntransformer module usually includes multi-head self-attention (MSA), MLP , and layer norm (LN)38. Our trans-\nformer encoder was composed of three standard transformer modules, with the MSA module as the core part \nof the transformer module. In contrast to CNNs, which use convolutional and pooling layers to obtain feature \ninformation, the data used to extract features predominantly contains local information, and the ability to cap-\nture global information is poor. These conditions are not conducive to obtaining global spatial information in \nHRRS images or semantic information between images. The transformer extracts global features based on the \nattention mechanism and learns long-range dependencies, which helps encode patches according to global con-\ntextual information and captures the information between ordered patches, thus improving the performance of \nglobal feature extraction from HRRS images.\nSpecifically, the patches are first flattened, then positional embedding is added to the patch embedding vector \n Zn to maintain the spatial location information between the input patches. Then, the learnable embedding vector \nfor category classification is input to the transformer encoder along with  Zn. At this point, a set of sequential \npatches are input to the transformer encoder, as shown in Fig. 1. Two important components of the transformer \nmodule are MSA and the MLP , which are computed via Eqs. (3) and (4), respectively.\nThe latter consists of two fully connected (FC) layers and an activation function, the Gaussian Error Linear \nUnit (GeLU). Residual connections are used in both components in the transformer module, and each compo-\nnent is preceded by an LN.\nAmong the above components, the MSA block is the central part of the transformer module, which can \nlearn rich semantic features from patches of sequence of size n, capture internal data correlations, and establish \ndependencies among different features.\nSpecifically, each element in the input sequence Z is multiplied by three learnable weight matrices  WQKV, \nwhich are composed of three values: Q , K and V  (“query” , “key” , and “value” of dimensions  Dq,  Dk and  Dv, \nrespectively). These values can be calculated as:\nTo determine the correlation between elements in the sequence, the dot product between the Q-vector of that \nelement and the K-vectors of the other elements was calculated, and the result determined the relative importance \nof the patches in the sequence. The softmax function was used to calculate the weights of V . Subsequently, the \nvalue of each patch embedding vector was multiplied by the output of the softmax function to obtain the patches \nwith higher attention, which were calculated according to\nMSA uses the previous operation multiple times to perform multiple dot-product attention calculations for \nQ, K, and V (i.e., h times), and then connects the results of these attentions via Eq. (7), which is the MSA process.\n(2)M s(F) = BN\n(\nf1×1\n3\n(\nf3×3\n2\n(\nf3×3\n1\n(\nf1×1\n0 (F)\n))))\n,\n(3)z\n′\nl = MSA\n(\nLN\n(\nzl−1\n))\n+ zl−1,l = 1, 2...L ,\n(4)z l = MLP\n(\nLN\n(\nz\n′\nl\n))\n+ z\n′\nl ,l = 1, 2...\n(5)[Q,K,V] = zWQKV ,W QKV ∈ Rd×3D k.\n(6)Attention(Q,K,V) = softmax\n(QK T\n√Dk\n)\nV.\nFigure 4.  Spatial attention module (SAM).\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nEach result of these parallel attention calculations is called a head, which are defined as:\nwhere W Q\ni ∈ Rd ×Dq,W K\ni ∈ Rd ×Dk,W V\ni ∈ Rd ×Dv,andW O ∈ Rh×Dv×d .\nThe weights extracted via MSA are sent to the MLP layer, where an LN is performed before entering the MLP \n layer38. The LN serves not only to reduce the training time, but also to mitigate the effects of gradient disappear-\nance and explosion. The MLP layer is composed of two fully connected (FC) layers, and the nonlinearity between \nthe layers is called the activation of the  GELU39, which is calculated as:\nwhere �(x) is the standard Gaussian cumulative distribution function and erf(x) =\n∫ x\n0 e−t2\ndt.\nDetails of the patch size. An initial HRRS image of size (w,h,o) was mapped onto a set of images of \nsize (l,l,o), which were first divided into patches of size (p,p,o ) with sample data of size (m,p,p,o), where p is \nthe height and width of the sample and m = (l× l)/(p × p) is the number of samples. Second, the patches are \npassed through the CSAM module to redistribute the weights of the channel and spatial information. Because \nthe CSAM does not change the shape of the input feature map, the shape of the output sample data remained \n(m,p,p,o). Finally, the output of m samples were flattened, and the 2D patch of size (p,p,o) with a sequence was \nentered into the transformer encoder after adding position encoding and category identification. The size of p \nwas the same as the size of the patch designed by  Dosovitskiy26. The purpose of this design strategy was to verify \nthat the CSAM module can focus on the local features of the ordered patches before entering the transformer \nencoder, thereby improving the performance of the model, as described later.\nExperimental results and analysis\nDataset description. In our experiments, three public remote-sensing datasets were used: the University \nof California Merced Land Use Dataset (hereafter “UCM”), the Aerial Image Dataset (hereafter “ AID”), and the \nNorthwestern Polytechnical University NWPU-RESISC45 Dataset (hereafter “NWPU”). The characteristics of \neach dataset are listed in Table 1. AID and NWPU are large-scale datasets.\nTraining details. We conducted all the experiments on a Dell Precision station with the following technical \nspecification: an Intel(R) Xeon(R) Silver 4216@2.10 GHz central processing unit (CPU) with 64 GB of RAM and \nan NVIDIA RTX A4000 graphical processing unit (GPU) with a 16-GB memory. The code used for the experi-\nments was implemented using PyTorch, an open-source deep neural network library written in Python. We used \nthe Adam e optimizer with an initial learning rate of 0.001 and a weight decay of 0.00001. All experiments were \ntrained using 300 epochs. We set the size of the UCM, NWPU, and AID datasets to 224 × 224 and the batch size \nto 64. We used 12 transformer blocks as encoders, the number of MSA headers in the transformer block was 12, \nthe patch size was 16, and the ImageNet1K pre-trained parameters were used in the encoders.\nComparison with the state‑of‑the‑art methods. The main purpose of this study is to demonstrate \nthat channel and spatial attention mechanisms optimize the transformer, which can improve the network per -\nformance. We used the overall accuracy as the evaluation criterion for this model, and all the experiments results \nused in the comparison were obtained from the literature.\nUCM dataset. The experimental results are listed in Table 2. The \"–\" in Table 2 indicates that the model did not \ncomplete the experiment at 50% or 80% of the training rate (the other two datasets are presented in the same \nform). When the training rate was 50%,  APDCNet47 used a trainable pooling operation method to improve the \ntraining effect, achieving an accuracy of 95.01 ± 0.43%. Our CSAT achieved an accuracy of 95.72 ± 0.23%, which \nwas 0.61% higher than that of APDCNet. When the training rate was 80%, our method achieved an accuracy \nof 97.86 ± 0.16%, which was 0.81% higher than that of RADC-Net 46 (which uses residual dense connectivity), \n0.76% higher than that of the fine-tuned and pre-trained  GoogLeNet41 model. ViT-Base26,30 and ViT-Large26,30 \nwere classified with patch sizes of 16 and 32, respectively, and their accuracies were 95.81% and 96.06%, respec-\ntively. Compared to these two methods, our CSAT method achieved an 2.05% and 1.8% improvement, respec-\ntively, in terms of accuracy. This not only proves the effectiveness of our method, but also demonstrates that \n(7)MultiHead (Q,K,V) = concat( Head 1,··· , Head h)W O.\n(8)Head i= Attention\n(\nQW Q\ni ,KW K\ni ,VW V\ni\n)\n,\n(9)GELU = X �(x) = X · 1\n2\n[\n1 + erf\n( X√\n2\n)]\n,\nTable 1.  Characteristics of the dataset.\nDataset Number of classes Number of images/class Image size Tota l Publishing organization Ref\nUCM 21 100 256 × 256 2100 United States Geological Survey 40\nAID 30 220–420 600 × 600 10,000 Wuhan University 41\nNWPU 45 700 256 × 256 31,500 Northwestern Polytechnical University 42\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nthe optimized transformer outperforms certain state-of-the-art (SOTA) methods. The confusion matrix for the \nUCM test set is shown in Fig. 5.\nAID dataset. AID datasets are better than UCM datasets for testing the model performance, as there are more \ntypes and numbers of AID datasets than there are UCM datasets. The experimental results are shown in Table 3. \nAt a 20% AID training rate, the accuracy of CSAT was 92.55 ± 0.28%, which was 0.67% higher than that of \nViT-Large and 1.39% higher than that of ViT-Base. At a 50% AID training rate, the accuracy of CSAT was \n95.44 ± 0.17, which was 0.97% higher than that of D-CNN with AlexNet, 0.13% higher than that of ViT-Large, \nand 1% higher than that of ViT-Base. The experimental results show that the CSAT performs better on the AID \ndataset. The confusion matrix for the AID test set is shown in Fig. 6.\nNWPU dataset. The NWPU dataset has more remote sensing images but is more difficult to train than AID \nand UCM datasets. The experimental results are listed in Table 4. At an NWPU training rate of 10%, the accuracy \nof CSAT was 89.70 ± 0.18%, which was 2.11% and 0.54% higher than that of ViT-Base and ViT-Large, respec-\ntively. At a training rate of 20%, the accuracy of CSAT was 93.06 ± 0.16%, which was 2.19% and 1.12% higher \nthan that of ViT-Base and ViT-Large, respectively. The experimental results show that the proposed method \nperforms well on the NWPU dataset. The confusion matrix for the NWPU in the test set is shown in Fig. 7.\nThe proposed the CSAT network combines the CSA mechanism to optimize the transformer. As shown in \nTables 2, 3 and 4, this method was validated on the UCM, AID, and NWPU datasets and outperformed some \nexisting SOTA models. In addition, the advantages of CSAT compared with some other methods were not \nsignificant. For example,  TRS29 cleverly used ResNet to develop an upgraded version of MHSA, and integrated \ntransformers into CNNs. Based on the basic CNN,  CTNet30 delicately develops an enhanced version of the CNN-\nbased network.  HHTL31 carefully designed the patch before it was input to the transformers and subtly fused \nthem after feature extraction. Some methods improved the classification performance by adding operations, \ne.g., multi-scale, spatial attention, and feature aggregation. However, our CSAT network did not include these \nadvanced skills. In the future, we will attempt to introduce some targeted operations into our CSAT network to \nimprove its performance in terms of HRRS scene classification.\nTraining and testing time and parameters. The training and testing time can directly reflect the efficiency of \nthe model and the time cost of running the model. We use the tqdm package to compare the time required for \ntraining and testing the model. As shown in Table 5, the efficiency and time costs of all methods are acceptable. \nCSAT takes longer to train and test an epoch than ResNet-101 or ResNet-152; however, it outperforms both of \nthem in terms of accuracy. Compared with the SE-Net model, its training time and parameters are higher than \nthose of CSAT. However, the parameters and Flops of the CSAT model are not optimal among models. The rea-\nsons for this analysis are as follows. The transformer embedded in the CSAT is the MSA block. It mainly obtains \nlong-term context information from the HRRS scene by measuring the relationship between the HRRS scene \npatches, which further increases the amount of computation. However, we found that the CSAT model has a \nslight advantage over the ViT-Base model in terms of time cost, parameter quantity, and Flops, which means that \nembedding the BAM module into the CSAT model is effective.\nAblation study for the proposed CSAT. In the ablation study, we explored how the components of \nthe CSAT model affected its performance. To obtain more convincing results, we selected three datasets (i.e., \nAID, NWPU, and UCM) with different resolutions for the ablation experiments. The training rates of the AID, \nNWPU, and UCM datasets were chosen to be 50%, 20%, and 80%, respectively.\nTable 2.  Classification results of the UCM dataset.\nMethod\nTraining ratio\n50% 80%\nGoogLeNet41 92.70 ± 0.60 94.31 ± 0.89\nAlexNet41 93.98 ± 0.67 95.02 ± 0.81\nVGGNet-1641 94.14 ± 0.69 95.21 ± 1.20\nTEX-Net with  VGG43 94.22 ± 0.50 95.31 ± 0.69\nSPP with  AlexNet44 94.77 ± 0.46 96.67 ± 0.94\nD-CNN with VGGNet-1645 – 96.67 ± 0.94\nD-CNN with  AlexNet45 – 97.42 ± 1.79\nRADC-Net46 94.79 ± 0.42 97.05 ± 0.48\nAPDCNet47 95.01 ± 0.43 97.05 ± 0.43\nFine-tuned  GoogLeNet48 86.02 ± 0.81 97.10\nViT-Base26,30 93.57 95.81\nViT-Large26,30 94.00 96.06\nT2T-ViT-1231,49 95.68 ± 0.61 97.81 ± 0.49\nCSAT (ours) 95.72 ± 0.23 97.86 ± 0.16\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nInfluence of other attention blocks. We changed the components in the CSAM, which contained the spatial and \nchannel attention modules, as described previously in this paper. We set up the ablation experimental deploy-\nment with the channel attention module, and spatial attention module, and replaced CSAM with the channel \nbottleneck attention module (CBAM). The experimental results are shown in Table 6, in which the results with \nthree sets of experiments are compared. We found that with the channel attention module alone, the classifi-\ncation accuracy of the AID, NWPU, and UCM datasets decreased by 5.08%, 7.26%, and 2.12%, respectively. \nWith the spatial attention module alone, the classification accuracy of the AID, NWPU, and UCM datasets \ndecreased by 6.01%, 8.13%, and 3.09%, respectively. Finally, in the case where the CSAM components were \nreplaced with CBAM, the classification accuracy of the AID, NWPU, and UCM datasets decreased by 3.13%, \n4.12%, and 1.07%, respectively. The experimental results show that the CSAM component effectively improves \nthe ViT network performance in the overall model structure.\nInfluence of patch size. In this experiment, we changed the size of the 2D flattening patch sequence and set the \npatch size to 16 and 32. According to the  literature20, the patch size should be 14, 16, and 32, but due to hardware \nlimitation, the experimental setting was limited to the smallest patch size (i.e., 16). The corresponding results, \nwhich indicate that the number of linear embedding sequences input to the encoder module is inversely pro-\nportional to patch  size20, are shown in Table 7. The smaller the patch size, the more pieces are cut; the larger the \npatch size, the fewer pieces are cut. When the model uses a smaller patch size, the computation is more expensive \nbecause the sequence length increases. In this experiment, when the patch size was 32, the accuracy of the AID, \nNWPU, and UCM datasets decreased by 2.26%, 1.96%, and 2.78%, respectively. This verifies that a large patch \nsize reduces the linear embedding sequence and affects the accuracy.\nFigure 5.  Confusion matrix for the UCM dataset for a training ratio of (a) 50% and (b) 80% linear assessments.\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nVisual attention map. To gain insight into the impact of the CSAM components on the proposed model \nperformance, we provide some visualization examples, which are shown in Fig. 8. Figure 8a,b show the original \nscene image, the image without CSAM components, and the image with CSAM components, respectively. The \narea highlighted in the feature map indicates a greater significance assigned to the classification of that area. Five \nscenes were selected, namely, harbor, golf course, tennis court, storage tank, and airplane. These results suggest \nthat the network structure with CSAM components extracts the discriminative information of these scenes more \naccurately. The main target in each scene was more accurately captured, which greatly promoted scene classifica-\ntion and reflected the positive effect of CSAM components on the overall network structure.\nConclusion\nIn this paper, a CSAT network combined with a channel-spatial attention (CSA) mechanism to optimize the \nViT network is proposed for HRRS image scene classification. The proposed network first processes the HRRS \nimage into a linear embedding sequence, uses the CSA module to extract fine-grained image features, and then \nenters the transformer encoder. There, the core MSA module resolves the long-range dependencies between \nHRRS images while discarding convolutional operations to avoid information loss caused by the irregular pro-\ncessing of typical convolutional kernels during classification. Overall, the proposed CSAT network combines \nthe CSAM module, MSA, linear mapping, regularization, activation function, and other operations and utilizes \nthe residual structure to form the encoder blocks. To improve the performance of the CSAT network, multiple \nencoder blocks were stacked to form the main model structure. We conducted two sets of experiments using \nthree public datasets to validate the effectiveness of the proposed model. The first set of experiments defined \nthe training ratio of the dataset, and the proposed model was then compared with a selection of existing SOTA \nclassification methods, including ViT-Base, ViT-Large, RADC-NET, and GoogLeNet. The experimental results \nshow that the proposed CSAT model outperforms the SOTA methods. One of the components of the CSAT \nnetwork is the CSA component. In this component, we selected the BAM module, which divided the attention \nprocess into two independent parts (i.e., the channel and spatial attention modules), and fused the attention \nweights of these two levels in parallel. The BAM module is a simple and effective attention module, which could \nbe integrated into existing network architectures as a plug-and-play module with higher  flexibility16. In the second \nset of experiments, we analyzed the effect of the CSA component on the model performance by removing the \nspatial and channel attention modules for comparison, as well as replacing the CSA component with the CBAM \ncomponent. The experimental results indicated that CSA was more beneficial to the performance of the CSAT \nmodel. By changing the hyperparameter patch size, the experimental results illustrated that a patch size of 36 \ndecreases the number of linearly embedded sequences and affects the classification accuracy.\nIn conclusion, the classification results of the CSAT network for the UCM, AID, and NWPU datasets signifi-\ncantly outperformed a selection of existing SOTA methods, thereby illustrating the effectiveness of our proposed \nnetwork. However, our proposed method still has some limitations: the method validation is based on public \ndatasets and lacks real data application. In future work, we will explore the application of real remote sensing \nimages based on deep learning  methods55. For example, the proposed CSAT network will be applied to crop \nidentification, a field where deep learning is widely used. According to a literature  survey56, deep learning is \nrarely used for particular special crops, such as medicinal plants. Due to the lack of data sets for such crops and \ntheir low coverage, the relative fragmentation of acreage compared to conventional crops such as large maize \nTable 3.  Classification accuracy of AID dataset.\nMethod\nTraining ratio\n20% 50%\nGoogLeNet41 83.44 ± 0.40 86.39 ± 0.55\nAlexNet41 86.86 ± 0.47 89.53 ± 0.31\nVGGNet-1641 86.59 ± 0.29 89.64 ± 0.36\nTEX-Net with  VGG43 87.32 ± 0.37 90.00 ± 0.33\nSPP with  AlexNet44 87.44 ± 0.45 91.45 ± 0.38\nD-CNN with  AlexNet45 85.62 ± 0.10 94.47 ± 0.12\nRADC-Net46 88.12 ± 0.43 92.35 ± 0.19\nMobileNet50 88.53 ± 0.17 90.91 ± 0.18\nSPP-Net44 87.44 ± 0.45 91.45 ± 0.38\nFussion by  addition51 – 91.87 ± 0.36\nViT-Base26,30 91.16 94.44\nViT-Large26,30 91.88 95.13\nT2T-ViT-1231,49 90.09 ± 0.08 93.82 ± 0.55\nPiT-S31,52 90.51 ± 0.57 94.17 ± 0.36\nPVT-Medium31,53 92.13 ± 0.45 95.28 ± 0.23\nCSAT (ours) 92.55 ± 0.28 95.44 ± 0.17\n10\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nand wheat, introduces certain difficulties into the research. Therefore, capturing the characteristic information \nof the limited medicinal plant data through the proposed CSAT network will be part of our future work. Such \nresearch will play an important role in achieving the sustainable use of medicinal plant resources, the coordinated \ndevelopment of economic and social resources, and the conservation of the ecological balance.\nFigure 6.  Confusion matrix for the AID dataset for a training ratio of (a) 20%, and (b) 50% linear assessments.\n11\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nTable 4.  Classification accuracy of the NWPU dataset.\nMethod\nTraining ratio\n10% 20%\nGoogLeNet41 76.19 ± 0.38 78.48 ± 0.26\nAlexNet41 76.69 ± 0.21 79.85 ± 0.13\nVGGNet-1641 76.47 ± 0.18 79.79 ± 0.15\nSPP with  AlexNet44 82.13 ± 0.30 84.64 ± 0.23\nD-CNN with  AlexNet45 85.56 ± 0.20 87.24 ± 0.12\nRADC-Net46 85.72 ± 0.25 87.63 ± 0.28\nMobileNet50 80.32 ± 0.16 83.26 ± 0.17\nSPP-Net44 82.13 ± 0.30 84.64 ± 0.23\nViT-Base26,30 87.59 90.87\nViT-Large26,30 89.16 91.94\nT2T-ViT-1231,49 84.91 ± 0.30 89.43 ± 0.23\nPiT-S31,52 85.85 ± 0.18 89.91 ± 0.19\nPVT-Medium31,53 87.40 ± 0.36 91.36 ± 0.09\nCSAT (ours) 89.70 ± 0.18 93.06 ± 0.16\nFigure 7.  Confusion matrix for the NWPU dataset for a training ratio of (a) 10%, and (b) 20% linear \nassessments.\n12\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nFigure 7.  (continued)\nTable5.  Training and testing time and parameter comparison between different models.\nMethod\nUCM (50%)\nAccuracy Train (s/epoch) Test (s/epoch) Parameters (M) Flops (G)\nResNet-10137 92.47 16.1 6.9 46.0 7.6\nResNet-15210 92.95 23.5 9.3 60.0 11.0\nSE-Net54 95.38 49.7 23.6 146.0 42.0\nViT-Base26 93.57 25.9 10.4 86.4 17.5\nCSAT (our) 95.72 25.3 10.19 85.99 16.88\nTable 6.  Influence of CSAM.\nAID (50%) NWPU (20%) UCM (80%)\nChannel attention 90.36 85.80 95.74\nSpatial attention 89.42 84.93 94.77\nCBAM + Transformer 92.31 88.94 96.79\nCSAT (ours) 95.44 93.06 97.86\n13\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\nData availability\nThe three public remote sensing scene image datasets used in this study are available in the following sites: (1) \nUC-Merced:http:// weegee. vision. ucmer ced. edu/ datas ets/ landu se. html; (2) AID: http:// capta in- whu. github. io/ \nAID/; (3) NWPU-RESISC45: http://  www. escie nce. cn/ people/ Junwe iHan/ NWPU- RESIS C45. html. The author \ncommit to sharing the raw data and materials upon acceptance of the Stage 2 manuscript.\nCode availability\nThe datasets generated and analyzed during the current study are available in the gjx2017/csam repository, https:// \ngithub. com/ gjx20 17/ csam. The authors commit to sharing all codes upon acceptance of the Stage 2 manuscript.\nReceived: 24 April 2022; Accepted: 5 September 2022\nReferences\n 1. Wang, Q. et al. Ship detection based on fused features and rebuilt YOLOv3 networks in optical remote-sensing images. Int J. Remote \nSens. 42, 520–536 (2021).\n 2. Liu, H. et al. DE-Net: Deep encoding network for building extraction from high-resolution remote sensing imagery. Remote Sens. \n11, 2380 (2019).\n 3. Ren, Y ., Yu, Y . & Guan, H. DA-CapsUNet: A dual-attention capsule U-net for road extraction from remote sensing imagery. Remote \nSens. 12, 2866 (2020).\n 4. Huang, X., Chen, H. & Gong, J. Angular difference feature extraction for urban scene classification using ZY-3 multi-angle high-\nresolution satellite imagery. ISPRS J. Photogramm. Remote Sens. 135, 127–141 (2018).\n 5. Han, W . et al. Methods for small, weak object detection in optical high-resolution remote sensing images: A survey of advances \nand challenges. IEEE Geosci. Remote Sens. 14, 11737–11749 (2021).\n 6. Li, K. et al. Object detection in optical remote sensing images: A survey and a new benchmark, arXiv2019, arXiv: 1909. 00133 v1 \n(2019).\n 7. Alsharrah, S. A. et al. Use of shadow for enhancing mapping of perennial desert plants from high-spatial resolution multispectral \nand panchromatic satellite imagery. J. Appl Remote Sens. 10, 1–15 (2016).\nTable 7.  Influence of patch size.\nPatch size AID (50%) NWPU (20%) UCM (80%)\n16 95.44 ± 0.17 93.06 ± 0.16 97.86 ± 0.14\n32 93.18 ± 0.23 91.10 ± 0.24 95.08 ± 0.19\nFigure 8.  Original scene images and the corresponding attention maps.\n14\nVol:.(1234567890)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\n 8. Ghazouani, F ., Farah, I. R. & Solaiman, B. A. Multi-level semantic scene interpretation strategy for change interpretation in remote \nsensing imagery. IEEE Trans. Geosci. Remote Sens. 57, 8775–8795 (2019).\n 9. Cheng, G. et al. Remote sensing image scene classification meets deep learning: Challenges, methods, benchmarks, and opportuni-\nties. IEEE J. Sel. Top Appl Earth Obs. Remote Sens. 13, 3735–3756 (2020).\n 10. Zhang, X. & Du, S. A Linear Dirichlet Mixture Model for decomposing scenes: Application to analyzing urban functional zonings. \nRemote Sens. Environ. 169, 37–49 (2015).\n 11. Gong, C., Han, J. & Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE 105, 1865–1883 \n(2017).\n 12. Ma, L. et al. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 152, \n166–177 (2019).\n 13. Wan, H. et al. Lightweight channel attention and multiscale feature fusion discrimination for remote sensing scene classification. \nIEEE Access 9, 94586–94600 (2021).\n 14. Mei, S. et al. Remote sensing scene classification using sparse representation-based framework with deep feature fusion. IEEE J. \nSel. Top Appl Earth Obs. Remote Sens. 14, 5867–5878 (2021).\n 15. Yuan, Y ., Fang, J., Lu, X. & Feng, Y . Remote sensing image scene classification using rearranged local features. IEEE Trans. Geosci. \nRemote Sens 57, 1779–1792 (2019).\n 16. Park, J., Woo, S., Lee, J.Y . & Kweon, I.S. BAM: Bottleneck Attention Module, arXiv2018, arXiv: 1807. 06514 v2 (2018).\n 17. Woo, S., Park, J., Lee, J.Y . & Kweon, I.S. CBAM: Convolutional Block Attention Module, arXiv 2018, arXiv: 1807. 06521 v1 (2018).\n 18. Yu, D.et al. Hierarchical attention and bilinear fusion for remote sensing image scene classification. IEEE J. Sel. Top Appl Earth \nObs. Remote Sens. 13, 6372–6383 (2020).\n 19. Tong, W .et al. Channel-attention-based densenet network for remote sensing image scene classification. IEEE J. Sel. Top Appl. \nEarth Obs. Remote Sens. 13, 4121–4132 (2020).\n 20. Ma, W . et al. A multi-scale progressive collaborative attention network for remote sensing fusion classification. IEEE Trans. Neural \nNetw. Learn Syst. 1–15 (2021).\n 21. Zhu, H. et al. A spatial-channel progressive fusion ResNet for remote sensing classification. Inf. Fusion 70, 72–87 (2021).\n 22. Zhu, H. et al. A dual–branch attention fusion deep network for multiresolution remote–sensing image classification. Inf. Fusion  \n58, 116–131 (2020).\n 23. Ma, W . et al. A spatial-channel collaborative attention network for enhancement of multiresolution classification. Remote Sens 13, \n106 (2020).\n 24. Li, F .et al. An Augmentation attention mechanism for high-spatial-resolution remote sensing image scene classification. IEEE J. \nSel. Top Appl Earth Obs. Remote Sens, 13, 3862–3878 (2020).\n 25. Guo, Y . et al. Global-local attention network for aerial scene classification. IEEE Access 7, 67200–67212 (2019).\n 26. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale, arXiv2020, arXiv: 2010. 11929 \n(2020).\n 27. Yi, T., Dehghani, M., Bahri, D. & Metzler, D. Efficient transformers: A Survey, arXiv2020, arXiv: 2009. 06732 v2 (2020).\n 28. Bazi, Y . et al. Vision transformers for remote sensing image classification. Remote Sens. 13, 516 (2021).\n 29. Deng, P ., Xu, K. & Huang, H. When CNNs meet vision transformer: A joint frame work for remote sensing scene classification. \nIEEE Geosci. Remote Sens. Lett. 19, 1–5 (2021).\n 30. Li, J., Zhang, J. & Zhao, H. TRS: Transformers for remote sensing scene classification. Remote Sens. 13, 4143 (2021).\n 31. Ma, J.et al. Homo–heterogenous transformer learning framework for RS scene classification. IEEE J. Sel. Top Appl Earth Obs. \nRemote Sens. 15, 2223–2239 (2022).\n 32. Vaswani, A. et al. Attention Is All Y ou Need, arXiv 2017, arXiv: 1706. 03762 v5 (2017).\n 33. d’ Ascoli, S., Touvron, H., & Leavitt, M. L. et al. Convit: Improving vision transformers with soft convolutional inductive biases, \narXiv2021, arXiv: 2103. 10697 v2 (2021).\n 34. Cordonnier, J.B., Loukas, A., & Jaggi, M. On the relationship between self-attention and convolutional layers, arXiv:2019, arXiv: \n1911. 03584(2019).\n 35. Bello, I. et al. Attention augmented convolutional networks, Proceedings of the IEEE/CVF international conference on computer \nvision (ICCV), Soul, Korea (South), 27 Oct.-2 Nov. pp. 3286–3295 (2019).\n 36. Ramachandran, P ., Parmar, N., Vaswani, A. et al. Stand-alone self-attention in vision models, arXiv:2019, arXiv: 1906. 05909 v1(2019).\n 37. He, K.M., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition, IEEE Conference on Computer Vision and \nPattern Recognition (CVPR), Las Vegas, NV , USA, 27–30 June pp. 770–778 (2016).\n 38. Ba, J.L., Kiros, J.R. & Hinton, G.E. Layer normalization, arXiv2016, arXiv: 1607. 06450 (2016).\n 39. Hendrycks, D. & Gimpel, K. Gaussian Error Linear Units (GELUs), arXiv2016, arXiv: 1606. 08415 v4 (2016).\n 40. Y ang, Y . & Newsam, S. Geographic image retrieval using local invariant features. IEEE Trans. Geosci. Remote Sens. 51, 818–832 \n(2013).\n 41. Xia, G. et al. AID: A benchmark data set for performance evaluation of aerial scene classification. IEEE Trans. Geosci. Remote Sens. \n55, 3965–3981 (2017).\n 42. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv2014, arXiv:  1409. \n1556(2014).\n 43. Anwer, R. M. et al. Binary patterns encoded convolutional neural networks for texture recognition and remote sensing scene \nclassification. ISPRS J. Photogramm. Remote Sens. 138, 74–85 (2018).\n 44. Han, X., Zhong, Y ., Cao, L. & Zhang, L. Pre-Trained AlexNet Architecture with pyramid pooling and supervision for high spatial \nresolution remote sensing image scene classification. Remote Sens. 848 (2017).\n 45. Cheng, G. et al. When deep learning meets metric learning: Remote sensing image scene classification via learning discriminative \nCNNs. IEEE Trans. Geosci. Remote Sens. 56, 2811–2821 (2018).\n 46. Bi, Q. et al. RADC-Net: A residual attention based convolution network for aerial scene classification. Neurocomputing 377, 345–359 \n(2020).\n 47. Bi, Q. et al. APDC-Net: Attention pooling-based convolutional network for aerial scene classification. IEEE Geosci. Remote Sens. \nLett. 17, 1603–1607 (2019).\n 48. Gong, C. et al. Remote sensing image scene classification using bag of convolutional features. IEEE Geosci. Remote Sens. Lett. 14, \n1735–1739 (2017).\n 49. Yuan, L. et al. Tokens-to-token vit: Training vision transformers from scratch on imagenet, Proceedings of the IEEE/CVF Interna‑\ntional Conference on Computer Vision (ICCV).11–17 Oct., pp. 558–567 (2021).\n 50. Pan, H. et al. A new image recognition and classification method combining transfer learning algorithm and MobileNet model \nfor welding defects. IEEE Access 8, 119951–119960 (2020).\n 51. Chaib, S., Liu, H. & Gu, Y . Deep feature fusion for VHR remote sensing scene classification. IEEE Trans. Geosci. Remote Sens. 55, \n4775–4784 (2017).\n 52. Heo, B. et al. Rethinking spatial dimensions of vision transformers, Proceedings of the IEEE/CVF International Conference on \nComputer Vision (ICCV).11–17 Oct., pp. 11936–11945 (2021).\n 53. Wang, W . et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions, Proceedings of the \nIEEE/CVF International Conference on Computer Vision (ICCV). 11–17 Oct., pp. 568–578 (2021).\n15\nVol.:(0123456789)Scientific Reports |        (2022) 12:15473  | https://doi.org/10.1038/s41598-022-19831-z\nwww.nature.com/scientificreports/\n 54. Hu, J. et al. Squeeze-and-excitation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), Salt Lake City, UT, USA, 18–22 Jun., pp. 7132–7141(2018).\n 55. Ma, L. et al. Deep learning in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm. Remote Sens. 152, \n166–177 (2019).\n 56. Guo, J. et al. Application of remote sensing technology in medicinal plant resources. Chi. J. Chin. Mater. Med. 46, 4689–4697 (2021).\nAcknowledgements\nThis research was funded by National Natural Science Foundation of China, grant number: M1942003, Natural \nScience Foundation of Inner Mongolia Autonomous Region, grant number:2021LHMS08014. I would like to \nthank Nan Jia for their suggestions on revision during this period. At the same time, I would like to thank Edit-\nage for helping with the English language.\nAuthor contributions\nJ.G. completed the investigation and wrote paper. J.G. and N.J. designed the methodology, J.G., N.J., analyzed \nthe data. J.G., N.J. and J.B. conducted the experiment. All authors have read and agreed to the published version \nof the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.G. or J.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022"
}