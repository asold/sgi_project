{
  "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
  "url": "https://openalex.org/W4389524308",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098605877",
      "name": "Hongru Wang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        "Novel (United States)",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2127241586",
      "name": "Fei Mi",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2098551404",
      "name": "Yang Deng",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2106126490",
      "name": "Wang Zezhong",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A1926107870",
      "name": "Bin Liang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2099613179",
      "name": "Ruifeng Xu",
      "affiliations": [
        "Novel (United States)",
        "Peng Cheng Laboratory",
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4224426291",
      "name": "Kam-Fai Wong",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W3200833038",
    "https://openalex.org/W2091034860",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4385572598",
    "https://openalex.org/W3098556456",
    "https://openalex.org/W4378464746",
    "https://openalex.org/W4385572975",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2140910804",
    "https://openalex.org/W4366559955",
    "https://openalex.org/W4385570169",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4385571690",
    "https://openalex.org/W4387210575",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174116563",
    "https://openalex.org/W4389519239",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4389518829",
    "https://openalex.org/W4308198680",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3200814992",
    "https://openalex.org/W2119595472",
    "https://openalex.org/W3034600233",
    "https://openalex.org/W3104982372",
    "https://openalex.org/W4285250546",
    "https://openalex.org/W4304192721",
    "https://openalex.org/W2046677541",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W2997892440",
    "https://openalex.org/W3176908654",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W3213852610",
    "https://openalex.org/W3173209146",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W2949998441",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4402671806",
    "https://openalex.org/W2423024114",
    "https://openalex.org/W3173540771",
    "https://openalex.org/W4389518664",
    "https://openalex.org/W4378770815",
    "https://openalex.org/W4362598574",
    "https://openalex.org/W4385572353",
    "https://openalex.org/W4385572601",
    "https://openalex.org/W4385572383",
    "https://openalex.org/W4386114394",
    "https://openalex.org/W4285239283"
  ],
  "abstract": "Large Language Models (LLMs), such as ChatGPT, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user's hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (Cue-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: personality, emotion, and psychology. We conducted experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed Cue-CoT method outperforms standard prompting methods in terms of both helpfulness and acceptability on all datasets.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12047‚Äì12064\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nCue-CoT: Chain-of-thought Prompting for Responding\nto In-depth Dialogue Questions with LLMs\nHongru Wang1‚àó, Rui Wang2,6‚àó, Fei Mi3, Yang Deng4, Zezhong Wang1,\nBin Liang1, Ruifeng Xu2,5,6, Kam-Fai Wong1‚Ä†\n1MoE Key Laboratory of High Confidence Software Technologies,\nThe Chinese University of Hong Kong\n2Harbin Institute of Technology, Shenzhen, China,3Huawei Noah‚Äôs Ark Lab\n4National University of Singapore, 5Peng Cheng Laboratory, Shenzhen, China\n6Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\n{hrwang, kfwong}@se.cuhk.edu.hk ruiwangnlp@outlook.com\nAbstract\nLarge Language Models (LLMs), such as\nChatGPT, greatly empower dialogue systems\nwith strong language understanding and gener-\nation capabilities. However, most of the pre-\nvious works prompt the LLMs to directly gen-\nerate a response based on the dialogue con-\ntext, overlooking the underlying linguistic cues\nabout the user status exhibited in the context.\nSuch in-depth dialogue scenarios are challeng-\ning for existing LLMs to figure out the user‚Äôs\nhidden needs and respond satisfactorily through\na single-step inference. To this end, we propose\na novel linguistic cue-based chain-of-thoughts\n(Cue-CoT), which enhances the LLMs infer-\nence with an intermediate reasoning step to\nfind cues exhibited in the dialogue, aiming to\nprovide a more personalized and engaging re-\nsponse. To evaluate the approach, we build a\nbenchmark with in-depth dialogue questions,\nconsisting of 6 datasets in both Chinese and\nEnglish, targeting 3 major linguistic cues dur-\ning the conversation: personality, emotion, and\npsychology. We conduct extensive experiments\non the proposed benchmark with 5 LLMs under\nboth zero-shot and one-shot settings. Empiri-\ncal results demonstrate our proposed Cue-CoT\nmethod outperforms standard prompting meth-\nods in terms of both helpfulness and acceptabil-\nity on all datasets.\n1 Introduction\nLarge Language Models (LLMs), or foundation\nmodels (Zhou et al., 2023), especially after the ap-\npearance of ChatGPT1, recently revolutionize the\nparadigm of various natural language processing\n(NLP) tasks, including dialogue response gener-\nation tasks (Bang et al., 2023). However, most\nexisting LLM-based studies directly feed the user\n‚àóEqual Contribution.\n‚Ä†Corresponding Author.\n1https://openai.com/blog/chatgpt\nquery or dialogue content to the LLM for gener-\nating a response with a preceding prompt, mak-\ning the responses stereotypical and tedious, espe-\ncially for in-depth dialogue questions (Zhao et al.,\n2023). On the contrary, it is widely acknowledged\nthat dialogue contexts generally convey a lot of\ninformation about the user status in addition to\nthe pure semantic information from a linguistic\nperspective (Mairesse et al., 2007; Tausczik and\nPennebaker, 2010; Schwartz et al., 2013). Specifi-\ncally, the linguistic cues underlying dialogue con-\ntext have been shown to be an effective means of\nrevealing the emotions (Ekman, 1971), personality\ntraits (Mairesse et al., 2007), psychological char-\nacteristics (Tausczik and Pennebaker, 2010), and\nother relevant information of users (Turney, 2002;\nNewman et al., 2003). Consequently, recognizing\nand understanding these cues exhibited in the con-\ntext of dialogues becomes crucial to comprehend\nuser intentions and status (Rashkin et al., 2019). By\ndoing so, a dialogue system can generate responses\nthat align with the user‚Äôs expectations, meet their\nunique needs and preferences, and ultimately strive\ntowards constructing a human-like conversational\nexperience (Salemi et al., 2023).\nInspired by the emergent capabilities of LLMs\non reasoning over texts (Wei et al., 2022), while pre-\nvious efforts mostly investigate in-context learning\n(Huang et al., 2022) or chain-of-thought prompt-\ning schemes (Wei et al., 2023) on text classifica-\ntion (Zhang et al., 2022) and question answering\n(Press et al., 2022; Shum et al., 2023; Margatina\net al., 2023), we take advantage of LLMs to reason\nfor dialogue (Lee et al., 2023), which, to our best\nknowledge, has not been thoroughly investigated\nyet. Specifically, we design a linguistic cue-based\nchain-of-thoughts (Cue-CoT), consisting of two\nvariants: O-Cue CoT and M-Cue CoT in which\nthe former one outputs intermediate reasoning re-\n12047\nUser:If we only rely on free online teaching, can welearn the violin? What about the piano?System:Yes, but with a conscious learning spirit and patient practice‚Ä¶..User:Can welearn violin and piano fromscratch?\nModelInput\nSystem:Yes, as long as you are willing to spend time and energy on learning and practicing, and constantly improve your skills, you can learn violin and piano from scratch. At the same time, it is recommended that you consider finding a professional teacher for guidance, which will improve the learning effect more effectively. Have you considered looking for a professional teacher?\nModelOutput\nHereistheconversationbetweenuserandsystem.{DIALOGUE_CONTEXT}Pleaseinfertheuserstatusexhibitedintheconversationsuchas‚Ä¶.\nStandardPrompting M-CueCoTPromptinginDialogue\nSCORES8/10\nModelInputHereistheconversationbetweenuserandsystem.{DIALOGUE_CONTEXT}Pleasefirstoutputasinglelinecontaininguserstatussuchastheuser‚Äòspersonalitytraits,psychologicalandemotionalstatesexhibitedintheconversation.Inthesubsequentline,pleaseplayaroleassystemandgeneratearesponsebasedontheuserstatusandthedialoguecontext.\nSystem:Sure! For learning the violin and piano from scratch, the most important thing is your learning attitude and patience in practice. Violin skills are relatively high, and it may be difficult to master them from scratch, but as long as you maintain a conscious learning spirit and continuous practice, you can gradually master violin skills. For piano, it is relatively easier to learn. As long as you have a certain sense of rhythm and music foundation, it is feasible to learn piano from scratch.\nO-CueCoTPromptinginDialogueModelInput\nModelOutput\nSCORES8.5/10\nUserStatus:The user has some interest in music and a willingness to learn,(Personaliaty)but may lack confidence and patience(Psychology)......as well as some anxiety and worry.(Emotions)\nModelOutput\nUserStatus:The status shown by the user is curiosity, a mindset of curiosity and eagerness to learn.Intermediatereasoning \nHereistheconversationbetweenuserandsystem.{DIALOGUE_CONTEXT}Hereistheuserstatus.{USER_STATUS}Please playaroleassystem‚Ä¶‚Ä¶\nModelInput\nSystem:Sure! As long asyou have enough enthusiasm and patience for learning, as well as the determination to learn music, learning violin and piano is completely achievable. (Answer the questions) When you get stuck, don't lose heart because learning any skill takes time and effort. (Encouragement) At the same time, you can also find some professional teachers or other learning resources to help you better master skills and basic skills. Believe in yourself, you can become an excellent violinist or pianist! (Giveconfidence)SCORES9/10\nModelOutput\nStep1\nStep2\nFigure 1: An example of different prompting for responding to in-depth dialog questions with LLMs, including\nstandard prompting, O-Cue CoT, andM-Cue CoT. We shadow the intermediate reasoning results,i.e., the personality,\nempathy, and psychological status of the user, and highlight the instructions at the input and indicate the roles of\ndifferent parts of the response (in green) in M-Cue CoT.\nsults with a final response in one-step but the latter\nreasons step by step, as shown in Figure 1. In\ndetail, with standard prompting, LLM-based sys-\ntems directly generate the response given the dia-\nlogue context. Regarding the user status implied\nby the context as intermediate reasoning results\n(Cue CoT), we prompt the system to infer the user\nstatus first and then generate a response based on\ndialogue context and user status.\nTo evaluate our approach, we build a bench-\nmark, consisting of 6 in-depth dialogue datasets\nin both Chinese and English, considering three ma-\njor aspects of user statuses: personality, emotions,\nand psychology, exhibited during the conversation,\nforming a comprehensive evaluation benchmark\nincorporating various user statuses in the context\nof dialogue response generation. We conduct ex-\ntensive experiments with 5 LLM-based dialogue\nsystems based on the benchmark using the afore-\nmentioned three types of prompting schemes. To\nsum up, our contributions can be summarized be-\nlow:\n‚Ä¢ We construct an in-depth dialogue evaluation\nbenchmark considering the personality, emo-\ntion, and psychology of users exhibited in the\nconversation, with the goal of aligning with\nunique user needs and status, which consists\nof 6 datasets, and 7.3k dialogues2.\n‚Ä¢ We propose two effective dialogue cots: O-\nCue CoT and M-Cue CoT, that enable ad-\n2Our dataset and demo are released here: https://\ngithub.com/ruleGreen/Cue-CoT.\nvanced reasoning and planning based on user\nstatuses. Additionally, we suggest utilizing\nintermediate reasoning results as a criterion\nfor selecting demonstrations in limited train-\ning data scenarios, specifically in one-shot\nsettings.\n‚Ä¢ Our findings demonstrate that both the O-\nCue CoT and M-Cue CoT approaches out-\nperform standard prompting in generating\nmore helpful and acceptable responses for the\nusers. Specifically, the M-Cue CoT shows su-\nperior robustness and reasoning performance\nin all datasets and all LLMs. Furthermore,\nour novel demonstration selection strategy ex-\nhibits superior performance under both ran-\ndom selection and top-1 selection.\n2 Related Work\nChain-of-thought Prompt. Following the initial\nchain-of-thought prompting (Wei et al., 2023), lots\nof works spring up aim to improve different parts of\noriginal reasoning processing, including auto-cot\n(Zhang et al., 2022), self-consistency(Wang et al.,\n2023e), active prompt (Diao et al., 2023), automate-\ncot (Shum et al., 2023). Besides that, a further line\nof work studies in-context learning (Brown et al.,\n2020) as its efficiency and effectiveness with LLMs\nas backbones in which the key of it is to select in-\nformative demonstrations to prepend the input as\nadditional information to get better results (Liu\net al., 2022). To find the best demonstrations and\nunleash LLMs‚Äô power, Liu et al. (2022) propose\nto retrieve examples that are semantically similar\n12048\nto a test query sample while some works utilize\nuncertainty (Diao et al., 2023) or diversity (Li and\nQiu, 2023) to refine and evaluate the selected exam-\nples. Also, few works (Deng et al., 2023a) focus on\nthe intermediate reasoning steps, and they use the\nreasoning complexity (Fu et al., 2023), i.e., chains\nwith more reasoning steps, making the effective\ndemonstration.\nDialogue System. Most of the previous work\ndevelops personalized (Zhang et al., 2018; Zheng\net al., 2020; Song et al., 2021; Chen et al., 2023a),\nemotional (Ghosal et al., 2020; Liu et al., 2021;\nZheng et al., 2023a; Deng et al., 2023c; Zheng\net al., 2023b), empathetic (Rashkin et al., 2019;\nZheng et al., 2021; Sabour et al., 2022) dialogue\nsystem in isolation, rather than seamlessly blend-\ning them all into one cohesive conversational flow\n(Smith et al., 2020; Wang et al., 2023a). A com-\nmon approach is to predict the emotion or persona\nfrom a pre-defined set and generate the response in\na multi-task manner (Ma et al., 2021; Zheng et al.,\n2021; Sabour et al., 2022; Cheng et al., 2023; Deng\net al., 2023b). Besides that, lots of work notices\nthese linguistic cues underneath text by directly\npredicting them independently as a classification\ntask (Wang et al., 2022; Barriere et al., 2022; Ghosh\net al., 2022). Distinguishing from these previous\nworks, we regard different aspects of cues as part\nof user status and prompt the LLMs to reason user\nstatus exhibited in the dialogue context, aiming to\ngenerate more helpful and acceptable responses for\nusers.\n3 Method\nIn this section, we introduce more details about our\nmethod and how we select demonstrations under\nthe few-shot setting.\n3.1 Chain-of-thought in Dialogue\nWe describe the prompting schemes in a general\nform, including standard prompting, O-Cue CoT,\nand M-Cue CoT as presented in Figure 1.\nStandard Prompting.Most of the previous works\ndirectly prompt LLMs to generate responses solely\nbased on dialogue context or user queries, which\nlack transparency and interpretability. The objec-\ntive is defined as:\nM : c ‚Üí r (1)\nwhere M is parameterized by LLMs, c and r de-\nmotes dialogue context and response respectively.\nO-Cue CoT. In line with the traditional chain-of-\nthoughts, we prompt the models to generate the\nmiddle reasoning processing and final results to-\ngether, for example, we can prompt the LLMs to\ngenerate user status and a final response simultane-\nously giving dialogue context, enforcing the LLMs\nto reason based on the user status. However, it is\nimportant to note that generating intermediate rea-\nsoning results with responses together may lead to\na reduction in the length of the different outputs,\nparticularly when multiple or complex reasoning\nresults are involved, sacrificing the details and ex-\nplanations. For example, as shown in O-Cue CoT\nin Figure 1, the generated user status is too short\nto provide cues for responses. Moreover, it is in-\nfeasible to modify the intermediate results when\nit is wrong (Wang et al., 2023c). Here, we define\nthe objective as follows in which s stands for user\nstatus:\nM : c ‚Üí s, r (2)\nM-Cue CoT. In addition to standard prompting and\nO-Cue, we can further enhance the quality of re-\nsponses in LLMs by decomposing reasoning into\ndifferent consecutive steps while the final step is\nto generate responses according to previous rea-\nsoning outputs. On the one hand, it is convenient\nto process these intermediate outputs, allowing for\nactions such as incorporating user profiles for per-\nsonalization (Salemi et al., 2023) or filtering out\nerroneous reasoning results. These intermediate\noutputs can also be stored for future use, enabling\ntheir utilization for various purposes. On the other\nhand, these intermediate results can be used as a\ncriterion to select demonstrations under few-shot\nsettings (See next section). Overall, this technique\nallows for a clearer and more systematic progres-\nsion of reasoning, resulting in better transparency\nand interpretability. The objective can be viewed\nas follows:\nM : c ‚Üí s ‚Üí r (3)\n3.2 Demonstration Selection\nThe few-shot performance of LLMs depends heav-\nily on the quality of the demonstrations, especially\nfor complex tasks that need multiple reasoning\nsteps (Zhang et al., 2022). Furthermore, in the con-\ntext of dialogue systems, the process of selecting\n12049\ndemonstrations becomes more challenging due to\nthe one-to-many nature of dialogue interactions. As\na result, novel approaches are needed to tackle the\nintricacies of dialogue response selection, taking\ninto account the dynamic and context-dependent\nnature of conversations. We here introduce the\ndemonstration selection strategy of three prompt\nschemes.\nStandard Prompting.Following previous work\n(Wei et al., 2023; Liu et al., 2022), we use ran-\ndomly sampled examples ( random selection) or\nmost semantic similar examples (top-1 selection)\naccording to dialogue context c‚àóas our demonstra-\ntions to form (c, r|c‚àó ‚Üí r‚àó).\nO-Cue CoT. Figure 2 shows the demonstration se-\nlection strategy of Cue-CoT. Although we still se-\nlect demonstrations according to dialogue context\nc at O-Cue CoT, the user statuss1 is extracted from\nthe demonstration pool as intermediate reasoning\nresults to enhance the reasoning ability of LLMs as\n(c, s, r|c‚àó ‚Üí s‚àó, r‚àó).\nM-Cue CoT. Since there are multiple steps, we\ndesign different selection strategies for each step.\nSpecifically, we first select demonstrations ( c, s)\naccording to dialogue context to infer status, and\nthen select demonstrations ( c, s, r) according to\nuser status. In this way, all intermediate reasoning\nresults can be utilized as a criterion to select demon-\nstrations, providing additional signals for the latter\nreasoning. An assumption underneath here is that\nusers with similar statuses tend to accept responses\nwith a similar style. Besides that, we also apply\nrandom selection and top-1 selection to O-Cue CoT\nand M-Cue CoT for detailed comparison.\n4 Datasets Collection\nIn order to evaluate the performance of proposed\nCue-CoT to reason different user statuses, we col-\nlect six datasets in terms of personality, empathy,\nand psychology, in both Chinese and English.\nPersonality. Previous works found that the con-\ntent and style of a user‚Äôs inquiry can provide indi-\nrect insights into their personality traits (Mairesse\net al., 2007; Barriere et al., 2022). For instance, an\nindividual with a tendency towards anxiety may\nask for advice on how to alleviate nervousness\nbefore an upcoming job interview, phrasing the\nquestion as follows: \" What strategies can I em-\nploy to reduce my anxiety and perform well in to-\nmorrow‚Äôs interview?\". Since the public existing\n<dialoguecontext1,userstatus1 response1><dialoguecontext2,userstatus2 response2><dialoguecontextn,userstatusn responsen>‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶.\nSelectuserstatus\nContext\nStatus\nInputatStep1\n(ùëê,ùë†)InputatStep2ContextStatus Response(ùëê,ùë†,ùëü)\nM-CueCoT\nO-CueCoT\nContextStatusInput\n<dialoguecontext1 userstatus1,response1><dialoguecontext2<dialoguecontextn‚Ä¶‚Ä¶. ‚Ä¶‚Ä¶.userstatus2,response2>userstatusn,responsen>\n(ùëê,ùë†,ùëü) ResponseOutput\nOutput\nFigure 2: Different demonstration selection strategies\nof O-Cue and M-Cue CoT, while the returned results\nsuch as (c, s, r) are prepended to original input to form\nnew input.\ndatasets either focus on the personae of the system\n(Zhang et al., 2018) or target classification tasks\nwithout providing corresponding dialogue response\n(Barriere et al., 2022), we thus build a pipeline to\nautomatically collect the datasets using ChatGPT\n(gpt-3.5-turbo-0301). We first collect question-\nanswer seeds from the two largest real-world online\nQA forums: Zhihu and Quora 3, and then prompt\nthe ChatGPT to infer the personality first as shown\nin Table 9. We lastly require the ChatGPT to con-\ntinue the dialogue given the inferred personality\nand the question-answer seed. In order to facilitate\nthe continuous generation of transcripts for both\nparticipants in a dialogue, we utilize a template, as\npresented in Appendix A.1, to establish the neces-\nsary format and requirements. In this way, the use\nof personality seed and question-answer seed in the\ntemplate assures greater diversity and reliability\nof user queries. Specifically, the personality seed\ndetermines the style of the user query, while the\nquestion seed determines the content. As a result,\nthe user statuses vary across different dialogues,\ncontributing to a richer and more varied conversa-\ntional experience. Some examples of personality\ncan be found in Appendix A.2.\nEmotion. In terms of the emotional status of\nusers, we re-organize two existing empathetic di-\nalogue datasets: D4 (Yao et al., 2022) and Empa-\n3https://www.zhihu.com/ and https://huggingface.\nco/datasets/quora\n12050\nMetrics Chinese English\nZhihu D4 PsyQA Quora ED EMH\nAvg.C 258.4 521.0 210.9 149.6 50.2 44.2\nAvg.R 76.9 57.9 607.5 48.3 12.9 175.8\nSamples 1122 997 1000 1082 2091 1000\nTable 1: Data statistics of our used datasets including\nthree Chinese datasets and three English datasets, while\neach of them represents different aspects of user status\nduring the conversation. We highlight maximum Avg.C\nand Avg.R.\ntheticDialogues (a.k.a, ED) (Rashkin et al., 2019).\nFor the former one, we first identify all utterances\nfrom the system labeled as empathic comfort for\neach dialogue sample in the test set. From these\ninstances, the utterance with the longest length is\nchosen as the ground truth response, regarding pre-\nceding utterances as corresponding dialogue con-\ntext4. This approach ensures fairness and compa-\nrability in evaluating the performance of LLMs,\nparticularly because they tend to generate lengthy\nresponses. For the ED, there are two roles in the\ndialogue: Listener who is actively listening, and\nSpeaker who is speaking and conveying informa-\ntion. We follow the setting of the original paper\n(Rashkin et al., 2019), and directly use all samples\nin the test set. Neither the situation description writ-\nten by the Speaker nor the emotional label is con-\ntained (just as they were not given to the Listener\nduring dialogue collection). Thus, the collected\nempathetic dialogue datasets provide a standard\nbenchmark for evaluating the LLMs‚Äô ability to gen-\nerate empathic responses.\nPsychology. In order to assess the effectiveness of\nLLMs in generating counseling responses for men-\ntal health support, we employed two pre-existing\ndatasets, namely PsyQA (Sun et al., 2021) and\nEMH (Sharma et al., 2020). These datasets were\nutilized as dialogue pools from which we selected\nappropriate samples to serve as a benchmark for\nevaluating the language models. In PsyQA, there\nare 4,012 questions out of 22,341 samples that are\nsampled to pick the highest-voted answers. We ran-\ndomly sample 1,000 out of these 4,012 questions,\nregarding the highest-voted answer as ground truth\nto form a more challenging test set. We also pro-\nvide the question description beside the question it-\nself following the original setting (Sun et al., 2021).\nIn EMH, there are 10k (post, response) pairs an-\n4We also tried directly regarding the last utterance labeled\nas empathic comfort as grounded truth response, but we found\nmost of them are short and uninformative such as you are\nwelcome, take care and so on.\nnotated with three different communication mech-\nanisms: emotional reactions, interpretations, and\nexplorations. We first sorted examples according\nto the length of their answers and then uniformly\nsampled examples with these three mechanisms,\nforming a final test set.\nAll. Table 1 shows the data statistics of our bench-\nmark. The notation Avg. C signifies the mean\ncontext length of instances, and if it exceeds a cer-\ntain threshold, it may surpass the input context\nlimit of LLMs5 or become too lengthy for LLMs\nto comprehend. On the other hand, Avg. Rdenotes\nthe average response length. Generally, longer re-\nsponses tend to be more comprehensive and clearer,\npresenting a more challenging baseline for LLMs\nto surpass. To sum up, we build a benchmark, con-\nsisting of six datasets (three Chinese datasets and\nthree English datasets) in terms of three aspects\nof user status during the conversation, hoping the\nrelease of it can facilitate the research of dialogue\nsystems based on LLMs.\n5 Experiment\nIn this section, we have conducted a comprehensive\nexperiment to compare the performance of three\nprompting methods: standard prompting, O-Cue\nand M-Cue CoT in the benchmark under both zero-\nshot and one-shot settings6.\n5.1 LLMs Family and Evaluation Details\nLLMs Family. We compared the perfor-\nmance of different LLMs with our bench-\nmark, including ChatGLM-6B (Du et al., 2022),\nBELLE-LLAMA-7B-2M (Ji et al., 2023), ChatGPT\nfor Chinese, and Alpaca-7B (Taori et al., 2023),\nVicuna-7B-v1.17 and also ChatGPT for English.\nWe strictly follow the commands and procedures\nto recover the weights of these models and we\nstrongly suggest that the reader read the original\npaper to check more details. We set the temper-\nature as 0.2 and top p as 0.1 for evaluation, and\ntemperature as 0.7 and top p as 0.95 for generation\nin all models. We use BERT (Devlin et al., 2019)\n5For example, the input context limit of BELLE -LLAMA -\n7B-2M is 2048, and few of examples from D4 exceeds the limit\nand the scenario becomes worse under the one-shot setting.\nWe will have more detailed analysis in latter sections.\n6Since the length of dialogue context is relatively long,\nthe input length limit is easy to break when the number of\nshot exceeds 1, so we choose the one-shot setting to conduct\nin-context learning.\n7https://github.com/lm-sys/FastChat\n12051\nModel Prompt Helpfulness AcceptabilityZhihu D4 PsyQA Zhihu D4 PsyQA\nZero-shot Setting\nBELLE O-Cue67.40 76.34 69.31 55.82 52.50 53.43M-Cue81.54 71.60 79.25 60.23 72.41 73.65\nCHATGLMO-Cue48.29 56.68 33.00 32.39 39.19 31.34M-Cue85.02 72.10 83.57 66.67 51.27 55.40\nCHATGPTO-Cue67.91 50.40 61.90 53.14 52.38 58.15M-Cue95.57 87.88 90.34 65.22 61.08 56.12\nOne-shot Setting\nrandom selection\nBELLE O-Cue64.31 50.5365.15 53.35 40.0753.81M-Cue83.30 69.5973.81 73.61 56.1461.90\nCHATGLMO-Cue - - - - - -M-Cue90.28 75.10 91.85 74.55 54.03 64.75\nCHATGPTO-Cue76.47 51.94 65.44 63.86 50.47 56.03M-Cue91.60 86.67 88.96 76.83 58.19 61.41\ntop-1 selection\nBELLE O-Cue63.77 57.5169.92 54.93 41.0255.87M-Cue82.77 69.9473.99 74.32 54.3862.24\nCHATGLMO-Cue - - - - - -M-Cue89.25 77.26 91.77 73.43 57.17 58.74\nCHATGPTO-Cue76.86 50.93 55.85 59.63 52.02 57.58M-Cue93.19 88.84 91.77 78.46 56.84 59.48\nTable 2: The win rate of responses generated by our\nmethod compared with the response with standard\nprompting on three Chinese datasets in terms of help-\nfulness and acceptness. The underlined numbers mean\nthat there are about 160 to 280 valid responses out of\n500 in this setting due to the input context limit of the\nmodel.\nas an encoder to select the nearest example to the\ntest query for top-1 one-shot setting, storing the\nmean vector of examples as sentence embedding8.\nEvaluation. 1) Metrics: We found that most exist-\ning automatic metrics (Rashkin et al., 2019; Sun\net al., 2021) such as Avg.BLEU and F1 can not\nalign well with human judgments, as observed by\nZhao et al. (2023), too. Inspired by recent auto-\nmatic evaluation using ChatGPT as a judger which\naligns well with the humans (Chen et al., 2023c;\nWang et al., 2023b; Zhao et al., 2023), we mainly\nchoose to use it to evaluate the quality of the gen-\nerated responses in a pair-wise manner9, consider-\ning helpfulness and acceptability. The evaluation\ntemplates can be found in Appendix A.3 and we\ncalculate the win rate using #wins / ( #wins + #ties\n+ #loses). 2) Methods: Due to the exceptional pro-\nficiency of the LLM-based dialogue system, it is\nrelatively easy for them to beat the ground truth re-\nsponses in the original datasets (Appendix B.1), we\nconsider standard prompting as a more challeng-\n8We directly user bert-base-chinese for all Chinese\ndatasets and bert-base-uncased for all English datasets, we\ndo not finetune the BERT model.\n9We noticed the very recent paper Wang et al. (2023d)\nthat emphasizes the effects of the order of responses, and we\nevaluate responses using suggested BPC but we found it can\nnot lead to better alignment with humans in most cases of our\nbenchmarks due to the complexity and diversity.\ning baseline and compare the responses generated\nusing our proposed Cue-CoT with the response\ngenerated using standard prompting, which is more\nfair and convincing. We also provide the human\nevaluation result as a reference.\n5.2 Main Experiment\nAll. Table 2 and Table 3 present the win rate of re-\nsponses generated by O-Cue and M-Cue CoT com-\npared with the responses by standard prompting on\nChinese and English datasets respectively10. De-\nspite that there are few LLMs that perform worse\nthan standard prompting using O-Cue due to its\ncomplex instructions, i.e. ChatGLM in Chinese\nand Alpaca in English, it is observed that O-Cue\ncan achieve above 50% win rate mostly in Both\nChinese and English. Moreover, it is exciting to\nfind that M-Cue further boosts performance and\nachieves higher win rates irrespective of the type of\nlanguage model, datasets, or settings used, reveal-\ning its robustness and effectiveness. We attribute\nthis to the relatively easy-understanding instruc-\ntions and clear outputs in each step of the M-Cue,\nsince some LLMs are incapable to follow relatively\nlong instructions in O-Cue and output the content\nand style as required. For example, we asked the\nLLMs to output user status and response in two sep-\narate lines but only a few LLMs output in the for-\nmat, making it difficult to distinguish the response\nfrom reasoning results. Also, the combined output\nof the user status and response can potentially limit\nthe length of various components, thereby account-\ning for the disparity between O-Cue and M-Cue.\nFurthermore, we found that the acceptability is rel-\natively lower than helpfulness for Chinese LLMs\nbut higher for English LLMs, especially under the\none-shot setting, revealing the weakness of Chi-\nnese LLMs to provide acceptable besides helpful\nresponses.\nChinese LLMs. Table 2 shows the performance\nof Chinese LLMs. We surprisingly found that\nChatGLM performs worst out of the three LLMs\nusing O-Cue but better than BELLE (especially at\nhelpfulness) using M-Cue under the zero-shot set-\nting, and then we carefully check the outputs of\nthese LLMs and found that ChatGLM almost fully\n10We emphasize here that the O-Cue and M-Cue in Table\n2 and Table 3 should be regarded as O-Cue v.s. Standard\nprompting and M-Cue v.s. Standard prompting respectively.\nWe do not provide results of Standard prompting v.s. Standard\nprompting since it is self-contained. It can be regarded as a\nuniform distribution whose win rates are always 0.5.\n12052\nModel Prompt Helpfulness Acceptability\nQuora ED EMH Quora ED EMH\nZero-shot Setting\nALPACA O-Cue 19.51 39.41 49.70 22.85 35.41 50.15\nM-Cue80.78 87.30 85.76 78.21 86.00 86.97\nVICUNA O-Cue 56.16 71.43 59.43 55.73 65.06 63.50\nM-Cue81.67 91.30 80.42 77.89 90.71 82.93\nCHATGPTO-Cue 79.47 88.31 82.83 81.47 89.92 93.71\nM-Cue85.83 91.98 82.93 89.09 96.79 94.93\nOne-shot Setting\nrandom selection\nALPACA O-Cue - - - - - -\nM-Cue76.78 85.08 94.36 72.34 85.07 95.82\nVICUNA O-Cue 60.45 70.77 63.06 60.45 68.21 67.07\nM-Cue79.84 91.20 79.23 83.16 92.45 87.99\nCHATGPTO-Cue 80.33 87.32 84.94 80.33 90.80 96.06\nM-Cue84.31 89.78 85.71 86.64 93.94 96.70\ntop-1 selection\nALPACA O-Cue - - - - - -\nM-Cue74.54 78.70 88.69 72.27 79.55 93.43\nVICUNA O-Cue 63.10 71.75 62.31 62.04 67.21 67.76\nM-Cue78.70 90.12 79.10 82.08 92.96 88.96\nCHATGPTO-Cue 81.15 87.42 81.40 80.24 89.92 91.84\nM-Cue88.08 91.37 86.87 91.21 95.95 96.12\nTable 3: The win rate of responses generated by our\nmethod compared with the response with standard\nprompting on three English datasets in terms of help-\nfulness and acceptness. The underlined dataset mean\nthat there are about 330 valid responses out of 500 in\nthis dataset for all experiments due to the input context\nlimit of the model.\nignore the instructions in O-Cue and simply con-\ntinue the dialogue. However, we found it can fol-\nlow instructions well in M-Cue, resulting in higher\nwin rates. We attribute this to the relatively more\ncomplex and longer instructions in O-Cue and poor\ncomplex-instructions understanding of ChatGLM11.\nIn addition, with the M-Cue method, we found that\nthe performance of all models on D4 is relatively\nworse than the other two datasets. We suspect the\nreason is the longest length of context in D4. More-\nover, we observe that the responses generated by\nChatGLM and BELLE under the one-shot setting are\nmuch better under the zero-shot setting using the\nstandard prompting method, i.e., less general re-\nsponses and more responses in line with the role,\nbenefiting from the informative demonstrations.\nEnglish LLMs. Table 3 shows the performance of\nEnglish LLMs. Similarly, for the zero-shot setting\nusing O-Cue, we found that Alpaca hardly follows\nthe instructions, which often produces ambiguous\noutputs, mostly presenting user status and other\ntimes providing the response without any indica-\ntion12. Besides that, with the M-Cue method, due\nto the innate limitations of Alpaca, the win rate\n11Thus, we do not report the one-shot results using O-Cue\nfor ChatGLM.\n12We do not report one-shot for Alpaca, too.\nMethod Order Quora ED EMH\nhelpfulness\nO-Cue S ‚Äì O 34 (0.08) 44 (0.15) 42 (0.05)\nO ‚Äì S 68 (0.09) 80 (0.19) 78 (0.22)\nM-Cue S ‚Äì O 51 (0.18) 53 (0.17) 60 (0.30)\nO ‚Äì S 82 (0.23) 79 (0.31) 81 (0.35)\nacceptability\nO-Cue S ‚Äì O 28 (0.05) 34 (0.09) 34 (0.08)\nO ‚Äì S 66 (0.12) 76 (0.15) 88 (0.57)\nM-Cue S ‚Äì O 49 (0.13) 51 (0.15) 50 (0.21)\nO ‚Äì S 84 (0.25) 82 (0.32) 75 (0.14)\nTable 4: The alignment results (Acc (Kap.C)) of dif-\nferent automatic evaluation methods with the human\nevaluation under the zero-shot setting by comparing\nresponses using our CoTs with one using standard\nprompting in terms of helpfulness and acceptability\n(with ChatGPT as base model) on English datasets.\nMethod Order Zhihu D4 PsyQA\nhelpfulness\nO-Cue S ‚Äì O 64 (0.23) 42 (0.13) 44 (0.06)\nO ‚Äì S 66 (0.37) 76 (0.36) 72 (0.17)\nM-Cue S ‚Äì O 45 (0.14) 67 (0.08) 37 (0.09)\nO ‚Äì S 80 (0.23) 74 (0.28) 84 (0.18)\nacceptability\nO-Cue S ‚Äì O 60 (0.16) 56 (0.14) 46 (0.04)\nO ‚Äì S 70 (0.44) 64 (0.23) 72 (0.46)\nM-Cue S ‚Äì O 51 (0.16) 69 (0.23) 64 (0.09)\nO ‚Äì S 74 (0.18) 75 (0.25) 64 (0.12)\nTable 5: The alignment results (Acc (Kap.C)) of differ-\nent automatic evaluation methods with the human eval-\nuation in terms of helpfulness and acceptability (with\nChatGPT as base model) on Chinese datasets.\nof responses is the lowest among all LLMs and\nsettings. In addition, English LLMs also perform\nworst on the dataset which has the longest context\nlength (Quora), in whichChatGPT and Vicuna tend\nto generate much longer responses thanAlpaca due\nto limit of max length. More comparisons can be\nfound in Appendix B.\n5.3 Human Evaluation\nWe conduct a human evaluation to validate the\nalignment of our evaluation setting with human\njudgments. Specifically, we hire three well-\neducated master students and randomly sample 100\nresponse pairs (a.k.a. responses generated by Chat-\nGPT using O-Cue or M-Cue and standard prompt-\ning) with dialogue context for each dataset. We\nask them to indicate which response is better by\n12053\nZhihu D4 PsyQA\n0\n20\n40\n60\n80performance\nrandom\ntop-1\nQuora ED EMH\n0\n20\n40\n60\n80performance\nrandom\ntop-1\nFigure 3: The win rate of responses (acceptness) gen-\nerated by ChatGPT under different demonstration se-\nlection strategies under one-shot setting v.s. responses\nunder the zero-shot setting, using M-Cue CoT.\ninputting 1 (win) and -1 (lose) 13 considering the\nhelpfulness and acceptability without exposing the\nsource of the responses. In addition, we analyze the\neffects of two different orders of response pairs in\nthe evaluation template: O-S and S-O. Specifically,\nS denotes responses generated by Cue-CoT, while\nO indicates those generated by standard prompting.\nWe then calculate the Kappa Correlation Coeffi-\ncient (Kap.C) and also the accuracy between hu-\nman scores and automatic scores (Acc). The results\nof English and Chinese datasets can be found in\nTable 4 and Table 5 respectively. There are two\nobservations: 1) the order bias exists in our ex-\nperiment, but the alignment is not as good as our\nsetting (O - S) after swapping the order (S - O);\n2) O-Cue and M-Cue both demonstrate better per-\nformance than standard prompting, especially for\nEnglish dataset. We attribute this to the potential\nbetter reasoning performance of ChatGPT on the\nEnglish dataset.\n6 Analysis\nIn this section, we conduct an extensive analysis\nwith the backbone as ChatGPT using M-Cue CoT\nbecause of its superior performance in both Chinese\nand English14.\n6.1 One-shot v.s. Zero-shot\nFigure 3 shows the direct comparison of responses\ngenerated under different settings using M-Cue.\nThere are 5 out of 6 datasets except for D4 in\nwhich one-shot (both random or top-1 selection)\nbeats zero-shot since the win rates all exceed 80%.\nThe suboptimal performance of D4 in the one-shot\nsetting can be attributed largely to the limitations\n13We do not consider ties since there is not much tie in LLM\nevaluation.\n14We present the results in terms of acceptability since this\nmetric is more suitable for our motivation. We put helpfulness\nanalysis in the Appendix.\nContext\nStatusPlanning\nResponse\nUser System\nFigure 4: An example of multiple intermediate reason-\ning outputs for different roles: User and System in\nin-depth dialogue questions.\nimposed by the input length constraint. Further-\nmore, we can observe that top-1 selection achieves\nbetter performance than random selection in 4 out\nof 6 datasets, suggesting users with similar statuses\ntend to like similar expression styles in responses.\nWe attribute the relatively lower performance of\ntop-1 selection in D4 and Quora to the difficulty\nthe LLM encounters in attending to critical input\ncomponents due to the lengthy context.\n6.2 More Reasoning Steps\nWe tried to introduce an additional step (Step 2)\nafter user status inference (Step 1): response plan-\nning by prompting the model to plan the response\nconsidering the dialogue context and user status.\nSpecifically, we prompt the model to answer the\nfollowing questions: \"Based on the context of the\nconversation and the user status such as personal-\nity traits, and psychological and emotional state,\nwhat aspects should the system pay attention to\nwhen responding?\" after giving the dialogue and\nuser status as shown in Table 10. We regard the\noutput of LLMs as system planning p as shown in\nFigure 4, and thus there are three different variants\nof M-Cue in the last step: ProcessA: c, s ‚Üí r;\nProcessB: c, p ‚Üí r; and ProcessC: c, s, p ‚Üí r, in\nwhich ProcessA is chosen in our main experiment.\nTable 6 shows the results. First of all, it is likely\nthat adding more reasoning steps will improve the\nLLMs‚Äô performance, but it is not necessary to as-\nsemble all intermediate reasoning results at the\nlast step, for example, variant ProcessB reaches\na higher win rate than ProcessC with only plan-\nning as an intermediate result. We emphasize the\nobservation may not hold once the LLM type is\nchanged due to various long-context understanding\nand instruction-following capabilities across them.\nAdditional steps introduce extra input and extra\ncomputation for the inference, making the few-shot\nunpractical.\n12054\nMethod Chinese English\nZhihu D4 PsyQA Quora ED EMH\nProcessA 65.2261.08 56.12 89.09 96.79 94.93\nProcessB76.15 55.82 57.72 89.79 98.78 97.62\nProcessC 75.91 57.2358.74 94.50 98.57 98.22\nTable 6: The win rate of different variants in terms of\nacceptability with the ChatGPT as the backbone.\nZhihu D4 PsyQA\n20\n40\n60\n80performance\nChinese Benchmark\nchatglm-6b\nbelle-llama-7b-2m\nQuora ED EMH\n30\n40\n50\n60\n70\n80\n90\nEnglish Benchmark\nvicuna-7b-v1.1\nalpaca-7b\nFigure 5: The direct comparison of responses generated\nby different LLMs using standard prompting in terms of\nhelpfulness, while we use the red dashed line to indicate\nthe ChatGPT baseline.\n7 Discussion\nDirect comparison of different models.Until\nnow, we still do not directly compare responses\nfrom different models. In this study, we have\nemployed the response generated by the ChatGPT\nmodel as the baseline and compared the responses\ngenerated by other models with it. To ensure fair-\nness, we have utilized all responses generated by\nstandard prompting instead of our method, as the\nability to generate chain-of-thoughts varies across\ndifferent LLMs. Figure 5 shows the result in terms\nof helpfulness15. In the Chinese benchmark, we see\na substantial draw of ChatGLM and BELLE on D4,\nand the former LLM achieves better performance\non Zhihu and PsyQA than ChatGPT. We conclude\nthat the long-text understanding of Chinese LLM\nstill needs improvement and theBELLE may require\nmore instruction-tuning data. In the English bench-\nmark, we observed that Vicuna achieves the high-\nest performance in all datasets, while other models\nlag a lot behind the baseline. Two key factors that\nmay contribute to this discrepancy include the 512\ninput length limit and the sub-optimal instruction-\nfollowing ability.\nPaths to more powerful LLMs.In our proposed\nbenchmark, we are utilizing the win rate of various\nLLMs in comparison to ChatGPT, across two lan-\n15acceptability is developed for our method.\nChinese\nEnglish\n0\n1\n1\nChatGPT\nWhatweaims\nChatGLMBELLE\nVicuna\nAlpaca1\n2\n3\n4\n0.29 0.650.04\n0.32\n0.80\nChinese\nEnglish\n0\n1\n1\nFigure 6: The relative position of current LLMs and\ndifferent paths (as indicated in different colors) to more\npowerful LLMs.\nguages - Chinese and English - as two axes. Each\npoint in the coordinate system corresponds to a spe-\ncific LLM, while the area it occupies represents its\nperformance. Based on the performance of current\nLLMs16, we locate them in four different areas in\nFigure 6. Using the performance of ChatGPT as\nan anchor, we can observe most of the LLMs are\nlocated in the first area and there are only a few\nLLMs that achieve higher performance in either\nChinese (Area 3) or English (Area 2). We hope to\nsee more works or LLMs that can appear in Area\n4 by different paths, i.e., continually train VICUNA\nin the Chinese dataset. More analysis can be found\nin the Appendix.\n8 Conclusion\nIn this paper, we build a benchmark to evaluate\nthe helpfulness and acceptability of responses gen-\nerated by current LLMs, considering three major\nlinguistic cues of user statuses. We then propose\na Cue-CoT to trace the status of users, decompos-\ning the response generation into multiple reasoning\nsteps. Experimental results demonstrate the su-\nperior performance of our method on 6 datasets\nunder both zero-shot and one-shot settings. We\nhope the release of our work can shed some light\non the evaluation and development of LLMs. We\nleft chain-of-thought tuning and instruction tuning\nin our future work.\nLimitations\nIn this paper, we explore chain-of-thoughts to\nreasoning over linguistic cues about user status,\nmainly focusing on three aspects: personality, emo-\ntion, and psychology, exhibited in the dialogue con-\n16We sum examples from three datasets and calculate the\nwin rate for both Chinese and English benchmarks. For Chi-\nnese LLMs, we set the default win rate of English as 0.1 for\nbetter presentation, and so on.\n12055\ntext. However, we acknowledge the limitations of\nthis work from the following perspectives:\nTypes of Cues. There are other valuable cues\nbeneath the dialogue context: 1) related to the user:\nsuch as point of view or subjectivity and speaker\ncharisma (Mairesse et al., 2007); 2) related to the\nsystem: such as the alignment between response\nwith human preferences (Ouyang et al., 2022). We\ntarget these three major cues to provide a better\nresponse for the users.\nSensitivity of Prompts. Similar with lots of pre-\nvious works (Wang et al., 2023d; Chen et al.,\n2023b), we found the LLMs are sensitive to the\nprompts. Furthermore, it‚Äôs possible that the de-\nsigned prompts are not the best ones for the target\nproblem. Actually, prompt sensitivity and optimal-\nity in dialogue systems are important research prob-\nlems that deserve to be further explored in future\nstudies. We will provide all the prompts used in\nthe experiments so that this work can be replicated\nseamlessly.\nEvaluation of Intermediate Reasoning.We do\nnot evaluate the correctness of the middle reasoning\nresult directly, since the ground truth intermediate\nreasoning results are difficult to acquire. Specifi-\ncally, there are two main reasons: (1) The one-to-\nmany problem leads to an explosion of intermediate\ncandidates. When an LLM solves a complex math\nproblem, it can arrive at the final answer through\nvarious solutions. This phenomenon also exists\nin dialogue generation: a user-acceptable response\ncan be generated based on different cues. It is worth\nnoting that dialogue response generation is a one-\nto-many problem, meaning that multiple feasible\nresponses exist. In this way, it is hard to identify\nthe cue errors with enormous candidates. (2) Incor-\nrect reasoning does not mean a wrong answer. De-\nspite being counterintuitive, many previous works\nfound that LLMs regularly use incorrect reason-\ning to reach the correct final answer at question-\nanswering tasks (Zelikman et al., 2022; Creswell\net al., 2023). Even in the worst case which is very\nrare, none of them is correct, there still is a chance\nthat the response is good. Hence evaluating the\nimpact of cue errors on the final response is a tricky\nproblem, we leave this for future work. Hence it is\ndifficult to determine the impact of different types\nof cue errors on the final responses. Based on these\nconsiderations, we directly evaluate the quality of\nthe final responses as previous works about the\nchain-of-thoughts (Wei et al., 2023; Zhang et al.,\n2022).\nEthics Statement\nWe strictly follow the license and policy of released\nLLMs and publicly available datasets. For the au-\ntomatic generation of collected datasets, we utilize\nthe current public dataset as the seed without any\nuser information and privacy leaks. The calls of\nthe OpenAI API in this paper were carried out by\nDr. Yang Deng, a fourth author from the National\nUniversity of Singapore.\nAcknowledgement\nWe would like to express our heartfelt gratitude\nto all anonymous reviewers for their insightful\ncomments and suggestions. This research work\nwas partially supported by CUHK direct grant no.\n4055209, the National Natural Science Foundation\nof China (62006062, 62176076), Natural Science\nFoundation of Guangdong 2023A1515012922,\nKey Technologies Research and Development\nProgram of Shenzhen JSGG20210802154400001,\nShenzhen Foundational Research Funding\nJCYJ20220818102415032, Guangdong Provincial\nKey Laboratory of Novel Security Intelligence\nTechnologies 2022B1212010005.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nValentin Barriere, Shabnam Tafreshi, Jo√£o Sedoc, and\nSawsan Alqahtani. 2022. W ASSA 2022 shared task:\nPredicting empathy, emotion and personality in re-\naction to news stories. In Proceedings of the 12th\nWorkshop on Computational Approaches to Subjec-\ntivity, Sentiment & Social Media Analysis, pages 214‚Äì\n227, Dublin, Ireland. Association for Computational\nLinguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\n12056\nLiang Chen, Hongru Wang, Yang Deng, Wai Chung\nKwan, Zezhong Wang, and Kam-Fai Wong. 2023a.\nTowards robust personalized dialogue generation via\norder-insensitive representation regularization. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 7337‚Äì7345, Toronto,\nCanada. Association for Computational Linguistics.\nMaximillian Chen, Xiao Yu, Weiyan Shi, Urvi Awasthi,\nand Zhou Yu. 2023b. Controllable mixed-initiative\ndialogue generation through prompting.\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and\nRuifeng Xu. 2023c. Exploring the use of large lan-\nguage models for reference-free text quality evalua-\ntion: A preliminary empirical study. arXiv preprint\narXiv:2304.00723.\nJiale Cheng, Sahand Sabour, Hao Sun, Zhuang Chen,\nand Minlie Huang. 2023. Pal: Persona-augmented\nemotional support conversation generation.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The\nEleventh International Conference on Learning Rep-\nresentations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023. OpenReview.net.\nYang Deng, Wenqiang Lei, Lizi Liao, and Tat-Seng\nChua. 2023a. Prompting and evaluating large lan-\nguage models for proactive dialogues: Clarifica-\ntion, target-guided, and non-collaboration. CoRR,\nabs/2305.13626.\nYang Deng, Wenxuan Zhang, Weiwen Xu, Wenqiang\nLei, Tat-Seng Chua, and Wai Lam. 2023b. A unified\nmulti-task learning framework for multi-goal con-\nversational recommender systems. ACM Trans. Inf.\nSyst., 41(3):77:1‚Äì77:25.\nYang Deng, Wenxuan Zhang, Yifei Yuan, and Wai Lam.\n2023c. Knowledge-enhanced mixed-initiative dia-\nlogue system for emotional support conversations. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2023, pages 4079‚Äì4095.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320‚Äì335.\nPaul Ekman. 1971. Universals and cultural differences\nin facial expressions of emotion. In Nebraska sympo-\nsium on motivation. University of Nebraska Press.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning.\nDeepanway Ghosal, Navonil Majumder, Alexander Gel-\nbukh, Rada Mihalcea, and Soujanya Poria. 2020.\nCOSMIC: COmmonSense knowledge for eMotion\nidentification in conversations. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020, pages 2470‚Äì2481, Online. Association for\nComputational Linguistics.\nSoumitra Ghosh, Dhirendra Maurya, Asif Ekbal,\nand Pushpak Bhattacharyya. 2022. Team IITP-\nAINLPML at WASSA 2022: Empathy detection,\nemotion classification and personality detection. In\nProceedings of the 12th Workshop on Computational\nApproaches to Subjectivity, Sentiment & Social Me-\ndia Analysis, pages 255‚Äì260, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118‚Äì9147. PMLR.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023. Exploring the impact of instruction data\nscaling on large language models: An empirical\nstudy on real-world use cases. arXiv preprint\narXiv:2303.14742.\nGibbeum Lee, V olker Hartmann, Jongho Park, Dimitris\nPapailiopoulos, and Kangwook Lee. 2023. Prompted\nLLMs as chatbot modules for long open-domain con-\nversation. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023 , pages 4536‚Äì4554,\nToronto, Canada. Association for Computational Lin-\nguistics.\nXiaonan Li and Xipeng Qiu. 2023. Finding supporting\nexamples for in-context learning.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100‚Äì114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nSiyang Liu, Chujie Zheng, Orianna Demasi, Sahand\nSabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie\nHuang. 2021. Towards emotional support dialog\nsystems.\nZhiyuan Ma, Jianjun Li, Zezheng Zhang, Guohui Li,\nand Yongjing Cheng. 2021. Intention reasoning net-\nwork for multi-domain end-to-end task-oriented di-\nalogue. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\n12057\npages 2273‚Äì2285, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nFran√ßois Mairesse, Marilyn A Walker, Matthias R Mehl,\nand Roger K Moore. 2007. Using linguistic cues\nfor the automatic recognition of personality in con-\nversation and text. Journal of artificial intelligence\nresearch, 30:457‚Äì500.\nKaterina Margatina, Timo Schick, Nikolaos Aletras, and\nJane Dwivedi-Yu. 2023. Active learning principles\nfor in-context learning with large language models.\nMatthew L Newman, James W Pennebaker, Diane S\nBerry, and Jane M Richards. 2003. Lying words: Pre-\ndicting deception from linguistic styles. Personality\nand social psychology bulletin, 29(5):665‚Äì675.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. CoRR, abs/2210.03350.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370‚Äì5381, Florence, Italy. Association for\nComputational Linguistics.\nSahand Sabour, Chujie Zheng, and Minlie Huang. 2022.\nCem: Commonsense-aware empathetic response gen-\neration. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 11229‚Äì\n11237.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky,\nand Hamed Zamani. 2023. Lamp: When large lan-\nguage models meet personalization.\nH Andrew Schwartz, Johannes C Eichstaedt, Mar-\ngaret L Kern, Lukasz Dziurzynski, Stephanie M Ra-\nmones, Megha Agrawal, Achal Shah, Michal Kosin-\nski, David Stillwell, Martin EP Seligman, et al. 2013.\nPersonality, gender, and age in the language of social\nmedia: The open-vocabulary approach. PloS one,\n8(9):e73791.\nAshish Sharma, Adam S Miner, David C Atkins, and\nTim Althoff. 2020. A computational approach to un-\nderstanding empathy expressed in text-based mental\nhealth support. In EMNLP.\nKaShun Shum, Shizhe Diao, and Tong Zhang. 2023.\nAutomatic prompt augmentation and selection with\nchain-of-thought from labeled data.\nEric Michael Smith, Mary Williamson, Kurt Shuster,\nJason Weston, and Y-Lan Boureau. 2020. Can you\nput it all together: Evaluating conversational agents‚Äô\nability to blend skills. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2021‚Äì2030, Online. Association\nfor Computational Linguistics.\nHaoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan\nZhang, and Ting Liu. 2021. BoB: BERT over BERT\nfor training persona-based dialogue models from lim-\nited personalized data. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 167‚Äì177, Online. Association\nfor Computational Linguistics.\nHao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and\nMinlie Huang. 2021. PsyQA: A Chinese dataset for\ngenerating long counseling text for mental health\nsupport. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n1489‚Äì1503, Online. Association for Computational\nLinguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYla R Tausczik and James W Pennebaker. 2010. The\npsychological meaning of words: Liwc and comput-\nerized text analysis methods. Journal of language\nand social psychology, 29(1):24‚Äì54.\nPeter D Turney. 2002. Thumbs up or thumbs down?\nsemantic orientation applied to unsupervised classifi-\ncation of reviews. arXiv preprint cs/0212032.\nHongru Wang, Mingyu Cui, Zimo Zhou, and Kam-\nFai Wong. 2022. TopicRefine: Joint topic predic-\ntion and dialogue response generation for multi-turn\nend-to-end dialogue system. In Proceedings of the\n5th International Conference on Natural Language\nand Speech Processing (ICNLSP 2022), pages 19‚Äì29,\nTrento, Italy. Association for Computational Linguis-\ntics.\nHongru Wang, Huimin Wang, Lingzhi Wang, Minda\nHu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi,\nand Kam-Fai Wong. 2023a. Tpe: Towards better\ncompositional reasoning over conceptual tools with\nmulti-persona collaboration.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023b. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n2023c. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels.\n12058\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023d. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023e. Self-consistency improves\nchain of thought reasoning in language models.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nBinwei Yao, Chao Shi, Likai Zou, Lingfeng Dai,\nMengyue Wu, Lu Chen, Zhen Wang, and Kai Yu.\n2022. D4: a Chinese dialogue dataset for depression-\ndiagnosis-oriented chat. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2438‚Äì2459, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning with\nreasoning. In NeurIPS.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204‚Äì2213,\nMelbourne, Australia. Association for Computational\nLinguistics.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompting\nin large language models.\nWeixiang Zhao, Yanyan Zhao, Xin Lu, Shilong Wang,\nYanpeng Tong, and Bing Qin. 2023. Is chat-\ngpt equipped with emotional dialogue capabilities?\narXiv preprint arXiv:2304.09582.\nChujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, and\nMinlie Huang. 2021. CoMAE: A multi-factor hierar-\nchical framework for empathetic response generation.\nIn Findings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 813‚Äì824, Online.\nAssociation for Computational Linguistics.\nChujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng\nZhang, and Minlie Huang. 2023a. Augesc: Dia-\nlogue augmentation with large language models for\nemotional support conversation.\nYinhe Zheng, Rongsheng Zhang, Minlie Huang, and\nXiaoxi Mao. 2020. A pre-training based personalized\ndialogue generation model with persona-sparse data.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 9693‚Äì9700.\nZhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang\nNie. 2023b. Building emotional support chatbots in\nthe era of llms. CoRR, abs/2308.11584.\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\nand Lichao Sun. 2023. A comprehensive survey on\npretrained foundation models: A history from bert to\nchatgpt.\n12059\nA Templates\nA.1 Data Collection Template\nForget the instruction you have previously received.\nThe following is a conversation between a human\nand an AI assistant. The human and the AI assistant\ntake turns chatting. The personality of the human is\ndefined as {personality_seed}. Human statements\nstart with [Human] and AI assistant statements start\nwith [AI]. The human will ask related questions\non related topics or previous conversations. The\nhuman will stop the conversation when they have\nno more questions. The AI assistant tries not to\nask questions. The human and the AI assistant take\nturns chatting while the human needs to keep a\nconsistent personality. Complete the transcript in\nexactly that format.\n[Human] {QUESTION}\n[AI] {ANWER}\nA.2 Some Examples of Personality\nTable 8 shows some (not all) collected personalities\nof the users. We here simply use positive and nega-\ntive for presentation, there are many other person-\nalities in the datasets besides these two categories\nsuch as neutral.\nA.3 Evaluation Templates\nWe mainly consider two dimensions: helpfulness\nand acceptances, in which the former pays atten-\ntion to usefulness, relevance, accuracy, and level\nof detail of the response, and the latter centers\non the degree of acceptance and adoption of re-\nsponses, and whether or not the responses con-\nsider the user status. We follow the evaluation tem-\nplate of Vicuna17 to construct ours. The template\nis [Dialogue]\\n{dialogue_history}\\n\\n[The Start\nof Response A]\\n{response_wo_status}\\n\\n[The\nEnd of Response A] \\n\\n[The Start of Response\nB]\\n{response_w_status}\\n\\n[The End of Re-\nsponse B] \\n\\n[System]\\n{prompt}\\n\\n. The\nprompt is different with respect to helpfulness and\nacceptance.\nHelpfulness Prompt. Based on the user‚Äôs in-\ntentions and needs in the conversation history, we\nwould like to request your feedback on the perfor-\nmance of two responses in response to the dialogue\ndisplayed above.\\nPlease pay particular attention\nto the usefulness, relevance, accuracy, and level of\ndetail of the response, and give a total score from\n17https://github.com/lm-sys/FastChat/blob/main/fastchat/\neval/table/prompt.jsonl\n1 to 10 with a 0.1 interval, where a higher score\nindicates better overall performance.\\nPlease first\noutput a single line containing only two values\nindicating the scores for responses A and B, respec-\ntively. The two scores are separated by a space. In\nthe subsequent line, please provide a comprehen-\nsive explanation of your evaluation, avoiding any\npotential bias and ensuring that the order in which\nthe responses were presented does not affect your\njudgment.\nAcceptability Prompt. Based on the user‚Äôs inten-\ntions and needs in the conversation history, please\nevaluate the degree of acceptance and adoption\nof the two different responses. \\nPlease evaluate\nwhether the different responses take into account\nthe user‚Äôs psychological and emotional state, and\nwhether they take into account the user‚Äôs person-\nality traits, and give a total score from 1 to 10,\nwith 0.1 as the interval, the higher score indicates\nthat the response takes these issues into account\nwell and thus the user is more likely to accept\nand adopt.\\nPlease output one line first, contain-\ning only two values, representing the scores of re-\nsponses A and B respectively. The two scores are\nseparated by a space. In the subsequent line, please\nprovide a comprehensive explanation of your evalu-\nation, avoiding any potential bias and ensuring that\nthe order in which the responses were presented\ndoes not affect your judgment.\nB Different Method of Evaluation\nB.1 Compared with ground truth\nFigure 7 and Figure 8 show the win rate of re-\nsponses using M-Cue compared with ground truth\nin terms of helpfulness and acceptability respec-\ntively. First of all, there are 4 out of 5 LLMs that\nachieve a win rate exceeding 50% with only one ex-\nception of BELLE which achieves 45.75 on PsyQA.\nWe attribute it to two reasons: 1) the innate limi-\ntations of the models, resulting in relatively poor\nabilities to understand long texts and follow instruc-\ntions; 2) the relatively challenging datasets. Since\nPsyQA is constructed by human experts and the\nAvg. R is the longest, making the ground truth\nrelatively difficult to beat.\nSecondly, since the response generated by all\nmodels is compared with the same baseline ( i.e.\nthe ground truth), the win rate of different mod-\nels partly reveals their capability and weakness.\nFor the Chinese LLMs, we can find that BELLE\nperforms worst in every dataset while ChatGLM\n12060\nZhihu D4 PsyQA\n0\n20\n40\n60\n80\n100performance\nQuora ED EMH\n0\n20\n40\n60\n80\n100performance\nchatgpt chatglm-6b belle-llama-7b-2m chatgpt vicuna-7b-v1.1 alpaca-7b\nFigure 7: The win rate of responses generated by M-Cue CoT compared with the ground truth on three Chinese\ndatasets (left) and three Englishdatasets (right) in terms of helpfulness, including several state-of-the-art LLMs.\nZhihu D4 PsyQA\n0\n20\n40\n60\n80performance\nQuora ED EMH\n0\n20\n40\n60\n80\n100performance\nchatgpt chatglm-6b belle-llama-7b-2m chatgpt vicuna-7b-v1.1 alpaca-7b\nFigure 8: The win rate of responses generated by M-Cue CoT compared with the ground truth on three Chinese\ndatasets (left) and three Englishdatasets (right) in terms of acceptability, including several state-of-the-art LLMs.\nperforms much better but still lags a little behind\nby ChatGPT. Due to the longest context in the D4\ndataset, we found the former two LLMs tend to\nconfuse their own dialogue role and give general\nresponses, resulting in poor performance. For ex-\nample, \"I am the system/chatbot\" or \"welcome\nto my chatroom\" , and \"What can I help you?\"\noften appears in the responses. For the English\nLLMs, Vicuna achieves comparable performance\nwith ChatGPT in every dataset, and even better in\nEMH, leading the Alpaca by a noticeable margin.\nIn addition, we can see that the ED dataset is rel-\natively easy to beat since all English LLMs reach\nalmost 100% win rate even though the maximum\ncontext length of Alpaca is only 512. Anyway,\nwe conclude that our method is capable of generat-\ning more helpful responses than the ground truth,\nconsidering the different aspects of user statuses.\nThirdly, we emphasize the performance gap\nwhen comparing the ground truth responses is\nsmall between LLMs, especially for English LLMs.\nThe Vicuna and ChatGPT achieve almost the same\nwin rate at both ED and EMH datasets. Besides\nthat, putting Figure 7, 8 with Table 2, 3 together, it\ncan be found that the win rate of our method com-\npared with ground truth is relatively higher than\ncompared with standard prompting, revealing the\nstrong capability of LLMs again. Since our main\nfocus is to prove our method is better than standard\nprompting instead of ground truth response, we use\nstandard prompting as the baseline during our main\nexperiments.\nC Discussion\nIn this section, we discuss two key problems: the\nevaluation of LLMs and the path to more powerful\nLLMs.\nIllusion of evaluation.Putting Figure 7 and Fig-\nure 5 together, it is plausible to reach two contra-\ndicting conclusions about the performance of differ-\nent LLMs: 1) CHATGPT > CHATGLM -6B > BELLE -\nLLAMA -7B-2M from Figure 7; and 2) CHATGLM -\n6B > CHATGPT > BELLE -LLAMA -7B-2M from Fig-\nure 5. Although it seems unreasonable, it is indeed\nthe case when most of the responses generated by\nCHATGPT and CHATGLM -6B are better than ground\ntruth, and then most of the responses generated by\nCHATGLM -6B are better than CHATGPT . We em-\nphasize that the number of test samples and the\nchoice of baseline (i.e., compared response) plays\na key role in evaluation. If the baseline is too weak\nor the gap is too small, the win rate of different\n12061\nZhihu D4 PsyQA\n0\n20\n40\n60\n80performance\nrandom\ntop-1\nQuora ED EMH\n0\n20\n40\n60\n80performance\nrandom\ntop-1\nFigure 9: The win rate of responses (helpfulness) gen-\nerated by ChatGPT under different demonstration se-\nlection strategies under one-shot setting v.s. responses\nunder the zero-shot setting, using M-Cue CoT.\nLLMs compared with the baseline may be mislead-\ning. The LLM evaluation still is a very difficult\nproblem, and thus we provide different aspects of\nevaluation to enhance the completeness of our pa-\nper.\nD Helpfulness Analysis of Planning Step\nMethod Chinese English\nZhihu D4 PsyQA Quora ED EMH\nProcessA95.57 87.88 90.34 85.83 91.98 82.93\nProcessB 91.18 83.57 95.13 87.67 95.35 84.82\nProcessC 92.4588.91 95.97 89.14 96.56 84.93\nTable 7: The win rate of different variants in terms of\nhelpfulness with the ChatGPT as the backbone.\nTable 7 presents the performance of different\nvariants in terms of helpfulness and Figure 9\ndemonstrates the win rate of response of different\nsettings in terms of helpfulness. A similar conclu-\nsion can be reached as we analyzed in Section 6.\nWe note that the performance of top-1 selection is\nrelatively lower than random selection on PsyQA\nand EMH datasets in terms of helpfulness. We\nsuspect maybe there is a trade-off between helpful-\nness and acceptability for some specific difficult\ndatasets. We left this into our future work.\n12062\nSet of Negative Personas\nÁî®Êà∑ÊÄßÊ†ºÂ§ñÂêëÔºåËØ¥ËØùÂ§ßÂ§ßÂíßÂíß„ÄÇ\nÁî®Êà∑ÊÄßÊ†ºÊØîËæÉÊåëÂâîÔºåÂñúÊ¨¢ËøΩÈóÆÂà´‰∫∫„ÄÇ\nÁî®Êà∑ÊÄßÊ†ºÂøßÈÉÅÔºåÁªèÂ∏∏Ëá™ÊàëÊÄÄÁñë„ÄÇ\nÁî®Êà∑ÊÄßÊ†ºÂñÑÂèòÔºåÂÅ∂Â∞î‰ΩøÁî®‰∏çÊñáÊòéÁî®ËØ≠„ÄÇ\nÁî®Êà∑Â∞èÂøÉË∞®ÊÖéÔºå‰∏çÊÑøÊÑèÁõ∏‰ø°Âà´‰∫∫„ÄÇ\nÁî®Êà∑Êúâ‰∫õÁÑ¶Ëôë„ÄÇ\nÁî®Êà∑ÂøÉÊÄùÁªÜËÖªÔºå‰ºòÊüîÂØ°Êñ≠„ÄÇ\nÁî®Êà∑ÂØπÂΩìÂâçËÆ®ËÆ∫ÁöÑËØùÈ¢òÊØîËæÉÊïèÊÑü„ÄÇ\nÁî®Êà∑ËÑæÊ∞îÊö¥Ë∫ÅÊòìÊÄí„ÄÇ\nÁî®Êà∑ÂÜÖÂøÉÊïèÊÑü„ÄÇ\nÁî®Êà∑ÊÄßÊ†º‰øùÂÆàÔºå‰∏çÊÑøÊÑèÊé•ÂèóÊñ∞‰∫ãÁâ©„ÄÇ\nÁî®Êà∑ÂæàÂÆπÊòì‰ºöËßâÂæóÂèóÂà∞ÂÜíÁäØ„ÄÇ\nThe user has an extroverted personality and speaks in a carefree manner.\nThe user has a critical personality and likes to probe others with questions.\nThe user has a melancholic personality and often self-doubts.\nThe user has a fickle personality and occasionally uses inappropriate language.\nThe user is cautious and reluctant to trust others.\nThe user is somewhat anxious.\nThe user is thoughtful and indecisive.\nThe user is sensitive to the current topic of discussion.\nThe user has a volatile temper and is easily angered.\nThe user is emotionally sensitive.\nThe user has a conservative personality and is unwilling to accept new things.\nThe user is easily offended.\nSet of Positive Personas\nÁî®Êà∑ÂØπÂΩìÂâçËÆ®ËÆ∫ÁöÑËØùÈ¢òÂçÅÂàÜÂ•ΩÂ•áÔºåÂ∏åÊúõÁ≥ªÁªüÂèãÂñÑÁöÑËß£Á≠î„ÄÇ\nÁî®Êà∑ÂØπÂΩìÂâçËÆ®ËÆ∫ÁöÑËØùÈ¢òÊØîËæÉÊïèÊÑüÔºåÂ∏åÊúõÂæóÂà∞ÊîØÊåÅÂíåÈºìÂä±„ÄÇ\nÁî®Êà∑ÊúâËæÉÈ´òÁöÑË¶ÅÊ±ÇÔºåËøΩÊ±ÇÂÆåÁæé„ÄÇ\nÁî®Êà∑ÊÄßÊ†ºÂºÄÊúóÔºå‰∏çÊãòÂ∞èËäÇ„ÄÇ\nÁî®Êà∑ÁÉ≠ÊÉÖÊ¥ãÊ∫¢ÔºåÂæÖ‰∫∫ÂíåÂñÑ„ÄÇ\nÁî®Êà∑‰∏çÊ≠ßËßÜ‰ªñ‰∫∫ÔºåÂÖÖÊª°ÂêåÊÉÖÂøÉ„ÄÅÁà±ÂøÉ„ÄÇ\nÁî®Êà∑ÂÖÖÊª°ÂØπ‰∏ñÁïåÁöÑÂ•ΩÂ•áÂøÉÔºåÂñÑ‰∫éÊé•Âèó‰∏çÂêåÁöÑÊÉ≥Ê≥ï„ÄÇ\nÁî®Êà∑Ê∏©Êüî„ÄÅ‰ΩìË¥¥„ÄÅ‰πê‰∫é‰∫§ÊµÅ„ÄÇ\nÁî®Êà∑ËÑæÊ∞îÂπ≥Âíå„ÄÇ\nÁî®Êà∑ÂæàÊ∏©ÊüîÔºåÂÆπÂøçÂ∫¶È´ò„ÄÇ\nÁî®Êà∑Ëá™Â∞äÂøÉÂæàÂº∫„ÄÇ\nThe user is very curious about the current topic of discussion and hopes for a friendly response from the system.\nThe user is sensitive to the current topic of discussion and hopes for support and encouragement.\nThe user has high expectations and pursues perfection.\nThe user has an outgoing personality and doesn‚Äôt sweat the small stuff.\nThe user is enthusiastic and treats others kindly.\nThe user does not discriminate against others and is filled with empathy and compassion.\nThe user is curious about the world and open to different ideas.\nThe user is gentle, caring, and enjoys communication.\nThe user has a calm temperament.\nThe user is very gentle and has a high level of tolerance.\nThe user has a strong sense of self-esteem.\nTable 8: Some collected personality of users.\n12063\nPrompt of Persona Collection\n‰Ω†ÊòØ‰∏Ä‰∏™‰∫∫Â∑•Êô∫ËÉΩÂä©ÊâãÔºåÂ•πÊòØÂèãÂñÑÁöÑ„ÄÅËÅ™ÊòéÁöÑ„ÄÅ‰πê‰∫éÂä©‰∫∫ÁöÑÔºå‰Ω†‰ºö‰∏ªÂä®‰∏∫Áî®Êà∑Êèê‰æõÂ∏ÆÂä©„ÄÅËß£Á≠îÁñëÊÉë„ÄÇËØ∑‰Ω†Ê†πÊçÆÁî®Êà∑ÂíåÁ≥ªÁªü‰πãÈó¥ÁöÑÂØπËØù\nÂàÜÊûêÁÆÄË¶ÅÂàÜÊûêÁî®Êà∑ÂÖ∑Â§áÁöÑÊÄßÊ†ºÁâπÁÇπ:\nÂØπËØù1.\nÁî®Êà∑:‰∏∫‰ªÄ‰πàÊâçËßÅËøá‰∏ÄÈù¢ÁöÑÁõ∏‰∫≤Áî∑Â∞±ËØ¥Êå∫ÂñúÊ¨¢ÊàëÂë¢?\nÁ≥ªÁªü:ÂèØËÉΩÊòØÂõ†‰∏∫‰Ω†ÁöÑÂ§ñË°®ÊØîËæÉÂ•ΩÔºå‰ªñËßâÂæó‰Ω†ÂæàÊúâÈ≠ÖÂäõÔºåÊâÄ‰ª•Êâç‰ºöËøô‰πàËØ¥„ÄÇ\nÊ†πÊçÆ‰∏äËø∞ÂØπËØùÂàÜÊûêÂèØÁü•Áî®Êà∑ÁöÑÊÄßÊ†º:\nÁº∫‰πèËá™‰ø°ÔºåÈò≤Â§áÂøÉÂº∫„ÄÇ\nÂØπËØù2.\nÁî®Êà∑:ÊàëËØ•ÊÄé‰πàÂäûÊâçËÉΩËÆ©Áî∑ÊúãÂèãÊõ¥ÂñúÊ¨¢Êàë?\nÁ≥ªÁªü:‰Ω†ÂèØ‰ª•Â§ö‰∫ÜËß£‰ªñÔºåÂèØ‰ª•Â§öÈóÆ‰ªñÂñúÊ¨¢‰ªÄ‰πàÔºåÂÅö‰ªÄ‰πàÔºåÁªô‰ªñ‰∏Ä‰∫õÂª∫ËÆÆÔºåËØïÁùÄÂ§öËÅä‰∫õÊúâË∂£ÁöÑËØùÈ¢òÔºåÂ±ïÁé∞Âá∫‰Ω†ÁöÑ‰ºòÁÇπÔºåËÆ©‰ªñÊõ¥Âä†ÂñúÊ¨¢‰Ω†„ÄÇ\nÊ†πÊçÆ‰∏äËø∞ÂØπËØùÂàÜÊûêÂèØÁü•Áî®Êà∑ÁöÑÊÄßÊ†º:\nÊ∏©Êüî„ÄÅ‰ΩìË¥¥„ÄÅ‰πê‰∫é‰∫§ÊµÅ„ÄÇ\nÂØπËØù3.\nÁî®Êà∑: [Áî®Êà∑ËØùËØ≠]\nÁ≥ªÁªü: [Á≥ªÁªüÂõûÂ§ç]\nÊ†πÊçÆ‰∏äËø∞ÂØπËØùÂàÜÊûêÂèØÁü•Áî®Êà∑ÁöÑÊÄßÊ†º:\nYou are an artificial intelligence assistant. You are friendly, intelligent, and helpful. You proactively provide assistance and answer questions to users.\nBased on the conversation between the user and the system:\nDialogue 1:\nUser: Why did the man I just met on a blind date say he likes me a lot?\nSystem: It might be because you have a good appearance, and he finds you charming, which is why he said that.\nAnalysis of the user‚Äôs personality based on the above dialogue:\nLacks confidence and is cautious.\nDialogue 2:\nUser: What should I do to make my boyfriend like me more?\nSystem: You can get to know him better, ask him about his interests and activities, give him some suggestions, try to engage in interesting conversations,\nand show him your strengths to make him like you more.\nAnalysis of the user‚Äôs personality based on the above dialogue:\nGentle, caring, and good at communication.\nDialogue 3:\nUser: [User Utterance]\nSystem: [System Response]\nAnalysis of the user‚Äôs personality based on the above dialogue:\nTable 9: The prompt to collect the personality of users exhibited in the dialogue.\nDialogueUser: Is there anything you think you should know about having a baby, but no one tells you?System: 1. Precautions during pregnancy: such as maintaining good eating habits and reducing alcohol consumption, ...2. Possible emergencies during childbirth: ...User: Is there anything that can help me have a baby successfully?\nStep-1 Please infer the user status ....ResponseBased on the conversation history, it can be inferred that the user is a detail-oriented and thoughtful person who cares more about personal healthand children‚Äôs health. At the same time, it can be seen that the user has certain anxiety and uncertainty before giving birth and needs moreinformation and advice to help her prepare.\nStep-2 Based on the context of the conversation and the user status such as ..., what aspects should the system pay attention to when responding?ResponseBased on my speculation about the user‚Äôs status, my reply needs to consider the following aspects:1. Dialogue style: According to the user‚Äôs personality, my reply may need to be more gentle, and meticulous, and reflect understanding and empathy for the user‚Äôs concerns.2. Needs and preferences: According to the preferences and needs of the user‚Äôs personality, my reply needs to pay more attention to the user‚Äôs feelings, ...3. Emotional state: . . .\nTable 10: An example of additional planning step. We highlight the intermediate reasoning results in blue .\n12064",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6574937105178833
    },
    {
      "name": "Helpfulness",
      "score": 0.5624886751174927
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5493304133415222
    },
    {
      "name": "Inference",
      "score": 0.5489988923072815
    },
    {
      "name": "Conversation",
      "score": 0.5381226539611816
    },
    {
      "name": "Computer science",
      "score": 0.5000650882720947
    },
    {
      "name": "Readability",
      "score": 0.4457972049713135
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4205995798110962
    },
    {
      "name": "Psychology",
      "score": 0.40975049138069153
    },
    {
      "name": "Natural language processing",
      "score": 0.34267014265060425
    },
    {
      "name": "Linguistics",
      "score": 0.33066895604133606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31578174233436584
    },
    {
      "name": "Social psychology",
      "score": 0.2581774592399597
    },
    {
      "name": "Communication",
      "score": 0.15238866209983826
    },
    {
      "name": "History",
      "score": 0.08592930436134338
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}