{
  "title": "Stress Test Evaluation of Transformer-based Models in Natural Language Understanding Tasks",
  "url": "https://openalex.org/W3006552058",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5043133456",
      "name": "Carlos Aspillaga",
      "affiliations": [
        "Pontificia Universidad Católica de Chile"
      ]
    },
    {
      "id": "https://openalex.org/A5087152717",
      "name": "Andrés Carvallo",
      "affiliations": [
        "Pontificia Universidad Católica de Chile"
      ]
    },
    {
      "id": "https://openalex.org/A5058447624",
      "name": "Vladimir Araujo",
      "affiliations": [
        "Pontificia Universidad Católica de Chile"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1603015195",
    "https://openalex.org/W2617685061",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2962685628",
    "https://openalex.org/W2957941633",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W3137695714",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2965570258",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963491027",
    "https://openalex.org/W1600009974",
    "https://openalex.org/W2963260436",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2020278455",
    "https://openalex.org/W2296076036",
    "https://openalex.org/W2949202705",
    "https://openalex.org/W2799007037",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963394326",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2146241755",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W48507986"
  ],
  "abstract": "There has been significant progress in recent years in the field of Natural Language Processing thanks to the introduction of the Transformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have shown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand their actual behavior under different scenarios, showing that these models are taking advantage of clues or failures of datasets and that slight perturbations on the input data can severely reduce their performance. In contrast, recent models have not been systematically tested with adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three Transformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NLI) and Question Answering (QA) tasks to know if they are more robust or if they have the same flaws as their predecessors. As a result, our experiments reveal that RoBERTa, XLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks. Nevertheless, they are still very fragile and demonstrate various unexpected behaviors, thus revealing that there is still room for future improvement in this field.",
  "full_text": "Stress Test Evaluation of Transformer-based Models\nin Natural Language Understanding Tasks\nCarlos Aspillaga*†, Andr´es Carvallo†‡, Vladimir Araujo*†‡\n†Pontiﬁcia Universidad Cat´olica de Chile, Santiago, Chile\n‡IMFD, Santiago, Chile\n{cjaspill, afcarvallo, vgaraujo}@uc.cl\nAbstract\nThere has been signiﬁcant progress in recent years in the ﬁeld of Natural Language Processing thanks to the introduction of the Trans-\nformer architecture. Current state-of-the-art models, via a large number of parameters and pre-training on massive text corpus, have\nshown impressive results on several downstream tasks. Many researchers have studied previous (non-Transformer) models to understand\ntheir actual behavior under different scenarios, showing that these models are taking advantage of clues or failures of datasets and that\nslight perturbations on the input data can severely reduce their performance. In contrast, recent models have not been systematically tested\nwith adversarial-examples in order to show their robustness under severe stress conditions. For that reason, this work evaluates three\nTransformer-based models (RoBERTa, XLNet, and BERT) in Natural Language Inference (NLI) and Question Answering (QA) tasks\nto know if they are more robust or if they have the same ﬂaws as their predecessors. As a result, our experiments reveal that RoBERTa,\nXLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks. Nevertheless, they are\nstill very fragile and demonstrate various unexpected behaviors, thus revealing that there is still room for future improvement in this ﬁeld.\nKeywords:adversarial evaluation, stress tests, natural language inference, natural language understanding, question answering\n1. Introduction\nDeep learning has allowed for solving several problems re-\nlated to natural language processing (NLP), even outper-\nforming human performance in some tasks, such as multi-\nlabel classiﬁcation (Tsoumakas and Katakis, 2007), doc-\nument screening (Carvallo and Parra, 2019), named en-\ntity recognition (Nadeau and Sekine, 2007), among others.\nHowever, previous research has shown that neural networks\nare powerful enough to memorize the training data, which\nlimits their ability to generalize or to really understand the\ntasks they are dealing with (Zhang et al., 2017).\nOne way to test NLP models is by using adversarial tests,\nwhich implies an intentional perturbation of the input sen-\ntence to confuse a model into making wrong predictions.\nThis methodology has shown that models are still weak\n(Belinkov and Bisk, 2018; Iyyer et al., 2018; Ribeiro et al.,\n2018; Ebrahimi et al., 2018). Other researchers have also\nshown that language models can “falsely” solve the task.\nIn other words, they might be taking advantage of dataset\nfailures or artifacts on the input sentences in order to guess\nthe answer (Gururangan et al., 2018; Agrawal et al., 2016;\nLevy et al., 2015). These evaluations, also known as “stress\ntests”, have been performed on classic models based on\nrecurrent networks (RNN). However, Transformer-based\nmodels such as RoBERTa (Liu et al., 2019), XLNet (Yang\net al., 2019) and BERT (Devlin et al., 2018), which are\nstate-of-the-art for NLU tasks, have not been systematically\nevaluated under severe stress conditions. Only BERT has\nbeen tested with similar objectives as ours (Hsieh et al.,\n2019; Jin et al., 2019; Niven and Kao, 2019), but not in\na systematic way as here nor in the same scenarios.\nIn this work, we focus on three language models based\non the state-of-the-art Transformer architecture (RoBERTa,\nXLNet and BERT), with the aim of carrying out a stress test\n* Equal contribution, listing order is random.\nevaluation on two natural language understanding (NLU)\ntasks. On the one hand, Natural Language Inference (NLI),\nalso known as recognizing textual entailment (RTE) which\nconsists of ﬁnding semantic relations between a premise\nsentence and an associated hypothesis, by classifying if\nthey are entailed, in contradiction or in neutral relation-\nship. On the other hand, we apply stress tests on a question-\nanswering (QA) task, also known as machine reading com-\nprehension (MRC) which consists ofpredicting the answer\nto a question given a paragraph.\nThe evaluation of the NLI task was performed using the\nMultiNLI dataset (Williams et al., 2018) following the\nmethodology of Naik et al. (2018). For the QA task we\nused the SQuAD dataset (Rajpurkar et al., 2016) and adver-\nsarial techniques introduced by Jia and Liang (2017). We\nalso developed a new adversarial dataset for SQuAD, using\ntechniques inspired on Belinkov and Bisk (2018)1.\nAll our test procedures try to prove the strength of the\nmodels, by distracting, confusing or proving their compe-\ntence. Experiments show that all models are affected by\nstress tests, but on Transformer-based models, the adver-\nsaries have smaller impact compared to previous models\nbased on RNNs. This behavior could be explained by the\nlarge number of parameters and their prior training. Nev-\nertheless, in this work we not only measure the impact on\nperformance of various adversarial or noisy conditions, but\nalso reveal that in some cases the state-of-the-art models\nbehave in strange and unexpected ways.\nWe provide detailed quantitative analysis on all the per-\nformed tests, and in some cases we report representative\nexamples via inspection of the attention matrices that these\nmodels produce during inference when tested under adver-\nsarial test scenarios.\n1we released the dataset at https://github.com/\ncaspillaga/noisy-squad\narXiv:2002.06261v2  [cs.AI]  27 Mar 2020\n2. Transformer for Natural Language\nUnderstanding\nThe Transformer (Vaswani et al., 2017) is a deep learning\narchitecture originally proposed for neural machine trans-\nlation applications. The main idea behind this model is the\nmulti-head self-attention, the ability to attend to different\nparts and aspects of the input sequence to compute a con-\ntextual representation of it, at increasing levels of abstrac-\ntion (layers). This architecture allows surpassing long-term\ndependency problems that are common on Recurrent Neu-\nral Networks (RNN) models, and adding the possibility of\nbeing highly parallelizable.\nEarly works such as GPT (Radford and Sutskever, 2018)\nand BERT (Devlin et al., 2018) proposed variants of the\nTransformer architecture for language modeling (Bengio\net al., 2001). These works show that the representations\nlearned on large-scale language modeling datasets are ef-\nfective for downstream sentence-level tasks (i.e. NLI) and\ntoken-level tasks (i.e. QA) via ﬁne-tuning. However, com-\npared to RNNs, no systematic evaluation of robustness and\nfailure modes for these kind of models (specially the most\nrecent variants) have been performed in previous works.\nIn this work, we evaluate three state-of-the-art models on\ntheir large version: BERT (Devlin et al., 2018), which\nwas the ﬁrst model to introduce bidirectional representation\nin the Transformer encoder and masked modeling, XLNet\n(Yang et al., 2019) that proposed the permutation model-\ning to prevent the corruption of the input with masks, and\nRoBERTa (Liu et al., 2019), which can be seen as a BERT\noptimization that includes additional pre-training and hy-\nperparameter improvements.\nWe use the HuggingFace python library (Wolf et al., 2019),\nwhich includes pre-trained models, in order to ﬁne-tune\neach model to a classiﬁer for the NLI task and a regres-\nsor for the QA task. We used the hyperparameters speciﬁed\nin the original paper for each model, to achieve an accuracy\nclose to the ones reported for each task.\nAdditionally, we include pre-Transformer baselines as a\ncomparison reference. These models rely on the LSTM\narchitecture (Hochreiter and Schmidhuber, 1997) and are\ntask-dependent. However, our analysis and discussion are\nmainly about experiments on Transformer-based models.\n3. NLI Task Description\n3.1. Task\nThe MultiNLI corpus (Williams et al., 2018) is a crowd-\nsourced collection of 433k sentence pairs annotated with\ntextual entailment information from a broad range of gen-\nres. In this task, given a premise, the model has to deter-\nmine whether a hypothesis is true (entailment), false (con-\ntradiction), or undetermined (neutral).\n3.2. Baselines\nAs a baseline to evaluate stress test performance for this\ntask, we chose the winner of RepEval 2017 Shared Task\n(Nangia et al., 2017), which proposed a model of stacked\nBiLSTMs with residual connections (Nie and Bansal,\n2017). Also, we used the baseline proposed in the original\npaper (Williams et al., 2018) of the dataset, which consists\nof a standard BiLSTM.\n4. QA Task Description\n4.1. Task\nSQuAD, the Stanford Question Answering Dataset (Ra-\njpurkar et al., 2016) is a widely used Question Answer-\ning benchmark that consists of a collection of English\nWikipedia paragraphs with more than 100k associated\nquestion-answer pairs generated via crowdsourcing. The\ntask is designed in a way that the solution to each question\nis literally contained in the corresponding paragraph, so the\ntask is to predict the answer text span in the corresponding\npassage. We use SQuAD v1.1 instead of SQuAD v2.0 to\nallow comparability with previous work.\n4.2. Baselines\nTo be consistent with previous work, we used BiDAF (Seo\net al., 2016) and Match-LSTM (Wang and Jiang, 2016) as\nbaselines to compare stress tests against Transformer-based\nmodels. BiDAF consists of embedding, attention and mod-\neling layers with a BiLSTM, that outputs a vector with in-\nformation of the context and the query, and ﬁnally an out-\nput layer with probabilities indicating where the answer\nstarts and ends in the context text. In the case of Match-\nLSTM, the model is an architecture that remembers impor-\ntant word-level matching results to get better predictions of\nthe answers.\n5. Experiments\n5.1. NLI Task Evaluation\nOur experiments on the MultiNLI dataset closely follow the\nNaik et al. (2018) procedure, which conducted a stress test\nevaluation of several models of the RepEval 2017 Shared\nTask. Below we describe each test set 2 used in this work\nand Table 1 shows some examples, however for further de-\ntails of the sets construction we refer the readers to the work\nby Naik et al. (2018).\n5.1.1. Distraction Test\nThe distraction test explores the model robustness after a\ntext with a clear “True” value is added.\n• One way to evaluate this is by decreasing the lexical\nsimilarity between premise and hypothesis. On the one\nhand, the word overlap set adds a tautology (“and true\nis true”) at the end of each hypothesis sentence. On\nthe other hand, the length mismatch set adds ﬁve times\nthe same tautology to each premise.\n• We can also evaluate this by the inclusion of strong\nnegations. The negation set is quite similar to the pre-\nvious ones, but in this case, the tautology added to the\nhypothesis includes negation words (“and false is not\ntrue”).\n5.1.2. Noise Test\nThis test veriﬁes the model strength against noisy data, in\nterms of spelling errors. It has two types of permutations on\n2We use the sets provided by the authors to avoid discrepancy\nduring the procedure. abhilasharavichander.github.\nio/NLI_StressTest\nTest Set Premise Hypothesis\nWord\nOverlap Then he ran. He ran like an athlete and true is true.\nLength\nMismatch\nThen he ran and true is true and true is true\nand true is true and true is true and true is true. He ran like an athlete.\nNegation Then he ran. He ran like an athlete and false is not true.\nSpelling\nErrors Then he ran. He ran like an athleet.\nAntonymy The Joint Venture had justiﬁed itself by failure. The Joint Venture had justiﬁed itself by success.\nNumerical\nReasoning Adam spent 1/6 of his lifetime in adolescence. Adam spent less than 1/6 of his lifetime in adolescence.\nTable 1: Examples of stress tests for the NLI task.\nOriginal W. Overlap Negation L. Mismatch S. Error\n0\n20\n40\n60\n80\n100Accuracy (%)\nRoBERTa\nXLNet\nBERT\nS-BiLSTM\nBiLSTM\nFigure 1: Accuracy results in the development set and ad-\nversarial sets: word overlap, negation, length mismatch and\nspelling error. Only matched partition is shown.\na word randomly selected from the hypothesis: swap of ad-\njacent characters within the word, and random substitution\nof a character next to it on the English keyboard. Note that\nonly one substitution is performed for the entire sentence.\n5.1.3. Competence Test\nThe competence test consists of two evaluation sets to mea-\nsure the reasoning ability of the models.\n• Understanding of antonymy relationships. This set in-\ncludes sentences that result in contradiction simply by\nusing an antonym in some adjectives or nouns.\n• Numerical reasoning ability of a model. This evalua-\ntion includes statements of simple algebraic problems\nwith solutions aspremises. The entailed, contradictory\nand neutral hypotheses were generated through the use\nof heuristic rules.\n5.2. NLI Task Results\nTable 2 shows the results of the performed tests. It can\nbe seen that all models decrease their accuracy in all eval-\nuations. However, Transformer-based models show more\nrobustness in some tests. The analysis of the results of the\nmodels in each stress test is shown on the following sec-\ntions.\n5.2.1. Models Performance on Distraction Test\nFigure 1 shows a bar graph of the “matched” partition of the\nevaluation sets on the different types of distraction tests. As\nmentioned in a previous section, the distraction tests allow\nus to check the robustness in two different ways.\nOn the one hand, the effect of introducing negation words\ndrops the models performance below 60% of accuracy,\nclose to the baselines. We checked the model predictions\non the negation test v/s the development set and we found\nthat BERT and XLNet obtained 93% and 91% of E-N (en-\ntailment predicted as neutral) error respectively. In contrast,\nRoBERTa obtained 85% of N-E error (neutral predicted as\nentailment). This could occur due to the introduction of\nextra negation words (“false” and “not”).\nOn the other hand, the decrease of lexical similarity by\nword overlap and length mismatch evaluation shows:\n• In the ﬁrst case ( word overlap set), the Transformer-\nbased models reach around 60% accuracy, which is\napproximately 20% less than in the development set.\nWe found a similar behavior with the previous set\n(negation), where BERT and XLNet obtained 83%\nand 61% of E-N error respectively. It also stands out\nthat RoBERTa achieved 89% of N-E error.\n• In the second case (length mismatch), the models per-\nformed better than expected, because they reached al-\nmost the same accuracy as in the development set. We\nhypothesize that these results may be due to thelength\nmismatch set modifying the premise sentence instead\nof the hypothesis as in the negation of the word over-\nlap sets, which suggests that in order to answer, the\nmodel is paying more attention to that sentence.\nTo verify the results on the length mismatch set, we ex-\ntended the evaluation by testing the addition of the tautol-\nogy “and true is true” in thehypothesis or in thepremises N\ntimes (where N = 1..5). Figure 2 shows the performance\nof XLNet in these tests, likewise we observed similar be-\nhavior on the other models. We noticed that the inclusion\nof the distractions to the premise sentence does not affect\nthe model performance. However, when we add the tau-\ntology a single time (which is equivalent to the word over-\nlap test) to the hypothesis sentence, the performance drops\nabout 20%, and the more repetitions we add, the more accu-\nracy increases, almost reaching the same performance ob-\nModel\nOriginal\nDev\nDistraction Test Noise Test Competence Test\nWord\nOverlap Negation Length\nMismatch\nSpelling\nError Antonymy Numerical\nReasoningM MM M MM M MM M MM M MM M MM\nRoBERTa 90.0 89.7 64.3 62.3 59.0 58.5 87.5 88.2 85.3 85.7 63.9 59.2 64.9[28.5] [30.5] [34.4] [34.8] [2.8] [1.7] [5.2] [4.5]\nXLNet 89.2 89.1 71.0 68.9 60.0 59.5 87.2 87.5 83.5 83.7 74.7 70.9 63.9[20.4] [22.7] [32.7] [33.2] [1.9] [1.8] [6.4] [6.1]\nBERT 86.0 86.1 61.2 56.8 57.3 57.6 83.7 84.6 79.5 79.8 64.6 59.2 56.8[28.8] [34.0] [33.4] [33.1] [2.7] [1.7] [7.6] [7.3]\nS-BiLSTM 74.2 74.8 47.2 47.1 39.5 40.0 48.2 47.3 51.1 49.8 15.1 19.3 21.2[36.4] [37.0] [46.8] [46.5] [35.0] [36.8] [31.1] [33.4]\nBiLSTM 70.2 70.8 57.0 58.5 51.4 51.9 49.7 51.2 65.0 65.1 13.2 9.8 31.3[18.8] [17.4] [26.8] [26.7] [29.2] [27.7] [7.4] [8.1]\nTable 2: Classiﬁcation accuracy (%) of Transformer-based models and baselines. Both genre-matched (M) and mismatched\n(MM) sets were evaluated. Values in brackets represent the percentage of reduction with respect to the original dev set.\n0 1 2 3 4 5\nTautology Repetition\n0\n20\n40\n60\n80\n100Accuracy (%)\nPremise modification\nHypothesis modification\nFigure 2: Accuracy (%) of XLNet after the addition of dif-\nferent number of tautologies in hypothesis or premises.\ntained in the development set. We also checked the atten-\ntion weights, and did not identify anomalous behavior.\nThe unexpected result in accuracy indicates that the lexical\nsimilarity is not a strong enough signal to generate distrac-\ntion in this type of model, as they can discern the tautolo-\ngies anyway. Moreover, the model seems to pay more atten-\ntion to the hypothesis sentence in order to respond, without\ndiscarding the premise. However, the distraction evaluation\nindicates that these Transformer-based models are fragile\nto adversarial attacks that include strong negation words.\n5.2.2. Models Performance on Noise Test\nThe noise test with the spelling error set exhibits that\nTransformer-based models perform very well. They only\nlose between 2 to 5 percentage points in accuracy with\nrespect to the development set. The results suggest that\nthe multi-head self-attention mechanism of these models is\nvery effective at recovering the global information from the\ncorrupted sentence.\nHowever, the adversarial attacks of this set only modify one\nword of the hypothesis. This explains why there is no sud-\nden drop in performance in models, even for the BiLSTM-\nbased models.\n5.2.3. Models Performance on Competence Test\nAs we supposed, Transformer-based models work quite\nwell in this evaluation task. In the case of the antonymy\ntest, the models exceeded baselines by approximately 50\npercentage points in accuracy. This is probably because\nTransformers were pre-trained on a diverse and big corpus,\nallowing them to adequately represent the majority of the\nwords of the dictionary. XLNet and BERT were trained\nwith BookCorpus and Wikipedia, so we expected better\naccuracy of RoBERTa which used additional data. How-\never, XLNet outperformed others by at least 10 percentage\npoints, suggesting that permutation modeling could help\ncapture antonymy relationships better.\nFurthermore, the results on the numerical reasoning eval-\nuation show a lower performance for all models. In this\ntask, XLNet and RoBERTa have similar accuracy but have\ndifferent behavior. On the one hand, XLNet specialized\nin classifying “entailment” samples, achieving 90% in that\nclass. On the other hand, RoBERTa specialized in “neu-\ntral” category, obtaining 89% of correct answers. In both\ncases, the remaining classes achieved less than 74% of ac-\ncuracy (the model ﬁnds it hard to distinguish between those\nclasses). These results indicate that Transformer-based\nmodels trained in the NLI task have serious difﬁculties in\nnumerical reasoning and that they take different strategies\nto solve the task.\nFor both evaluations, we also explored the attention weights\nvia the BertViz library (Vig, 2019). Appendix B shows a\nbrief analysis of some speciﬁc cases on all the mentioned\nTransformer-based models.\n5.2.4. Annotation Artifacts Exploitation Test\nGururangan et al. (2018) found that MultiNLI dataset has\nannotation artifacts. It means that crowd workers who par-\nticipated in the creation of the data, adopted heuristics to\ngenerate the hypothesis in an easy and fast way. For in-\nstance, they usually use some keywords such as “not”,\n“never”, etc. to create negation sentences.\nTo evaluate if Transformer-based models leverage the arti-\nfacts, we tested the models by removing the premise sen-\ntence in the development set. In other words, the models\nMatched Mismatched\nMajority Class 35.4 35.2\nRoBERTa 35.2 35.8\nXLNet 35.4 35.8\nBERT 35.5 35.7\nS-BiLSTM 45.2 45.4\nBiLSTM 37.4 38.3\nTable 3: Performance (%) of premise-unaware text mod-\nels on MultiNLI development set. Greater accuracy means\nmore exploitation of artifacts, thus smaller numbers mean\nthe models performed best.\nare unaware of the premises of the dataset.\nTable 3 shows the results of this experiment. It is possi-\nble to see that Transformer-based models perform similar\nto the majority class3, which denotes an unbiased guess of\nthe models. In contrast, BiLSTM-based models show sig-\nniﬁcant proportion of correctly classiﬁed samples without\neven looking at the premise (which is an undesirable be-\nhavior). This result demonstrates that Transformer-based\nmodels are in fact learning to take into account and relate\nthe two sentences of the NLI task in order to choose the\ncorrect answer, which is consistent with the ﬁndings in Sec-\ntion 5.2.1.\n5.3. QA Task Evaluation\nOne of our test scenarios was taken from Jia and Liang\n(2017), which intentionally adds a new adversarial sen-\ntence at the end of SQuAD passages of the development\nset. These sentences are especially designed (via differ-\nent strategies) to act as a decoy to confuse the model. The\nother test scenario is inspired on Belinkov and Bisk (2018).\nAlthough originally proposed for a different task, we repli-\ncated the 5 types of noise proposed by the authors, and ap-\nplied them on the development set of SQuAD.\n5.3.1. Adversarial Sentence Tests\nIn Jia and Liang (2017), the authors proposed 4 strategies\nto create a sentence especially designed to confuse models\nby pretending to be the correct answer to a speciﬁc ques-\ntion, although they are unrelated with the question. This\nadversarial sentence is concatenated to the corresponding\nparagraph provided at test time. The 4 strategies proposed\nwere:\n• AddOneSent: Adjectives and nouns of the question\nare replaced by antonyms. Named entities and num-\nbers are replaced by their nearest word in GloVe (Pen-\nnington et al., 2014). This modiﬁed question is then\nturned into declarative form (using a set of manually\ndeﬁned rules) and a fake answer of the same type as\nthe original answer is inserted. Finally the sentence is\nmanually checked and ﬁxed via crowdsourcing.\n• AddSent: Identical to AddOneSent but generating\nmultiple candidate sentences (adversaries) and keep-\ning only the one that induces the biggest error when\ntested on a speciﬁc model.\n3The majority class is used as a baseline of random guessing.\nArticle: Super Bowl 50\nContext: Peyton Manning became the ﬁrst quarterback ever\nto lead two different teams to multiple Super Bowls. He is\nalso the oldest quarterback ever to play in a Super Bowl at\nage 39. The past record was held by John Elway, who led\nthe Broncos to victory in Super Bowl XXXIII at age 38 and\nis currently Denver’s Executive Vice President of Football\nOperations and General Manager. Quarterback Jeff Dean\nhad jersey number 37 in Champ Bowl XXXIV .\nQuestion: What is the name of the quarterback who was 38\nin Super Bowl XXXIII?\nOriginal prediction:John Elway\nPrediction after adversarial phrase is added:Jeff Dean\nFigure 3: An example of an AddOneSent adversarial sam-\nple. This example was taken from Jia and Liang (2017). In\nthis case we can see that the model correctly answered the\noriginal question, but after the inclusion of the adversarial\nsentence (in italic blue), the model fails (answer in red).\n• AddAny: The adversarial sentence is generated by\nsampling random words and successively replacing\nthem by elements from a sampled set of20 words each\ntime. Words are selected from this set by using a crite-\nrion that tries to minimize the conﬁdence of the model\non the correct answer. The 20-word set is sampled\nfrom a list of common words plus the words from the\nquestion. This process is repeated iteratively 6 times\nfor each adversarial phrase.\n• AddCommon: Identical to AddAny, but in this case\nthe 20-word set is sampled from the list of common\nwords directly.\n5.3.2. Noise Tests\nAlthough originally proposed for a different task, we repli-\ncated the 5 types of noise introduced by Belinkov and Bisk\n(2018). In each experiment, a speciﬁc noise type was ap-\nplied to each word in the passage of SQuAD’s develop-\nment set. The question was kept unchanged, and the an-\nswers were adapted to preserve consistency with the mod-\niﬁed passage. In contrast to the noise tests performed in\nthe NLI setting (Section 5.1.2), the scenario tested here is\nsigniﬁcantly more aggressive because it introduces noise to\nevery word in the reference text.\nThe 5 noise types tested are:\n• Natural Noise: Words are replaced by real typing er-\nrors of people. To automate this, we used a collection\nof word corrections performed by people in web plat-\nforms that keep track of edits history (Max and Wis-\nniewski, 2010; Zesch, 2012; Wisniewski et al., 2013;\nˇSebesta et al., 2017).\n• Swap Noise: For each word in the text, one ran-\ndom pair of consecutive characters is swapped (e.g.\nexpression →exrpession).\n• Middle Random Noise: For each word in the text,\nall characters are shufﬂed, except for the ﬁrst and last\ncharacters. (e.g. expression →esroxiespn).\n• Fully Random Noise: For each word in the text,\nall characters are shufﬂed (e.g. expression →\nrsnixpoees).\n• Keyboard Typo Noise: For each word in the text,\none character is replaced by an adjacent character in\ntraditional English keyboards (e.g. expression →\nexprwssion).\n5.4. QA Task Results\nSimilarly to the observations for the NLI experiments, for\nQA it is clear that the performance of all models is affected\nby the stress tests, with Transformer-based models being\nthe most robust in all the cases analyzed. Detailed results\ncan be found in Table 4.\n5.4.1. Results on Adversarial Sentence Tests\nFigure 5 shows a bar graph that compares the accuracy of\nthe tested models under the different adversarial strategies.\nWhen we analyze the results of the AddOneSent experi-\nments, we notice an accuracy reduction between 18.1%\nand 21.7% for the Transformer-based models, and greater\nthan 39.5% for non-Transformer models. In spite of show-\ning greater robustness in comparison with their counterpart,\nTransformer-based models still suffer from a signiﬁcant im-\npact on performance, which elucidates a clear opportunity\nfor future improvements on these kind of models. The same\nphenomenon is observed for AddSent adversaries, but more\npronounced (as expected, since AddSent tests the worst\ncase for each candidate question). We see accuracy re-\nductions ranging from 27.7% and 32.2% for Transformer-\nbased models, and greater than 54.6% for non-Transformer\nmodels.\nWe notice that as the model is more powerful in the main\ntask (accuracy in the unmodiﬁed SQuAD v1.1 development\nset), it also achieves greater robustness. This conclusion is\nhopeful because other works have asserted that more pow-\nerful models could justify their performance on their higher\nmemorization capabilities (Zhang et al., 2017). These ex-\nperiments, in contrast, indicate that the models are improv-\ning their reading capabilities in a balanced fashion.\nInterestingly, AddAny and AddCommon adversaries show\nthat those strategies are very model-speciﬁc, as evidenced\nby the fact that Transformer-based models only reduce their\naccuracy in small degree when tested against adversaries\nwhere other architectures failed. These results are relevant\nbecause, as reported by Jia and Liang (2017), those adver-\nsaries (and especially AddAny) turned to be very effective\nwhen trying to mislead the models that they were targeting.\nThis cross-check between different model’s adversaries for\nAddAny is consistent with the results reported by Jia and\nLiang (2017), although in the case of Transformer-based\nmodels, the before-mentioned behavior is even more pro-\nnounced. For the case of AddCommon, in the other hand,\nthis tests were not reported in previous work nor analyzed\nby the authors that proposed these adversaries, thus this\nﬁnding is especially relevant.\nFurther details on the results of every experiment performed\ncan be found in Appendix A. Also in Appendix C we per-\nform a more qualitative analysis of the attention matrices\nthat these models produce during inference.\nArticle: Genghis Khan\nContext: (...) Maluqi, a tsuretd lteneitnau, was given cmm-\nnoad of the Monogl focres angisat the Jin dytasny whlie\nGneghis Kahn was ftgniihg in Ctneral Aais, and Stbuaui\nand Jbee were aeolwld to prusue the Great Raid itno the\nCuaucsas and Kaiven Rus’, an idea tehy had peestrned to the\nKaaghn on tehir own ieivtnitia. Whlie grnniatg his gneaelrs\na gerat dael of amotonuy in mkiang canommd diesscion, Gn-\nhgeis Kahn also epecxted uvwannrieg layolty from them.\nQuestion: Who was delegated command of the Mongol\nforces against the Jin dynasty?\nAnswer: Maluqi\nFigure 4: A QA adversarial example after the introduction\nof Middle Random noise. Note that only the context (and\nthe answer, accordingly) is modiﬁed, but not the question.\n5.4.2. Results on Noise Tests\nAs shown in Figure 6, all ﬁve types of noise have a signif-\nicant negative impact on accuracy on all the tested models.\nThe accuracy reduction is more prominent than on Adver-\nsarial Sentence tests (Section 5.4.1) due to the aggressive-\nness of the strategies tested here.\nSwap Noise has a signiﬁcant impact on accuracy, between\n46.3% and 59.0% (for the Transformer-based models) and\nof 70.1% for Match-LSTM, although only a single pair\nof characters per word are altered. Performance is only\nslightly better than when using Middle Random Noise (and\nin that scenario, all the characters are shufﬂed, except for\nthe ﬁrst and last ones). We hypothesize that this is due to the\nfact that by introducing this change, the resulting tokeniza-\ntion differ signiﬁcantly from the original ones and are also\nvery different from the ones seen in training or ﬁne-tuning,\nand thus the model is not prepared to answer accurately.\nNote also that, in absolute terms, under Middle Random\nnoise, the model is still able to correctly answer one in four\nquestions, even though the text is severely transformed (ex-\nample in Figure 4).\nAnother unexpected pattern that these tests showed is the\nfact that for Transformer-based models, the Keyboard Typo\nnoise is more challenging to deal with than Swap Noise .\nThis ﬁnding is especially intriguing becauseKeyboard Typo\nnoise corrupts only one character for each word, and Swap\nNoise corrupts two. For this reason, this result is opposed\nto what we expected and reveals that swapping operations\naffect these models less than replacement operations. This\neffect may be caused by the fact that the tokenized repre-\nsentation of words withswapped characters might be closer\nto the original one (in the embedding space of each model),\nor maybe it is because this kind of noise might be more\nfrequent in real misspellings than keyboard typos, so the\nmodels were more exposed to this kind of noise during pre-\ntraining. Further study is required to ﬁnd out which phe-\nnomenon is the dominant one in this case, but this analysis\nis out of the scope of this work.\nSimilarly to what was reported in Belinkov and Bisk\n(2018), Natural Noise is signiﬁcantly more straightforward\nto overcome than the other four tested noise types, even\nconsidering that in the dataset we built for Natural Noise,\nwe forcefully replaced every word by a noisy version of it\nModel Original\nDev\nConcatenative Adversaries Noise Adversaries\nAddOne-\nSent AddSent AddAny Add-\nCommon Swap Middle\nRandom\nFully\nRandom\nKeyboa-\nrd Typo Natural\nRoBERTa 85.8 70.3 61.5 77.3 84.3 46.1 32.2 3.3 30.4 54.9\n[18.1] [28.3] [9.9] [1.7] [46.3] [62.5] [96.2] [64.6] [36.0]\nXLNet 85.2 67.7 61.6 78.8 83.0 43.0 31.9 4.4 27.2 57.4\n[20.5] [27.7] [7.5] [2.6] [49.5] [62.6] [94.8] [68.1] [32.6]\nBERT 82.5 64.6 55.9 71.4 81.1 33.8 28.6 5.5 23.1 47.7\n[21.7] [32.2] [13.5] [1.7] [59.0] [65.3] [93.3] [72.0] [42.2]\nMatch-LSTM 60.8 30.0 24.8 35.7 52.5 17.8 20.2 4.1 9.4 19.7\n[50.7] [59.2] [41.3] [13.7] [70.7] [66.8] [93.3] [84.5] [67.6]\nTable 4: Exact match (%) of Transformer-based models and baselines on the SQuAD v1.1 dev set. AddAny and AddCom-\nmon report the worst accuracy after running against all the alternative adversarial datasets of that speciﬁc type. For fair\ncomparison, experiments on the adversaries generated for the model itself are excluded in those two speciﬁc cases. Values\nin brackets represent the percentage of reduction with respect to the original dev set.\nOriginal AddOneSent AddSent AddAny* AddCommon*\n0\n20\n40\n60\n80\n100Accuracy (%)\nRoBERTa\nXLNet\nBERT\nBiDAF\nMatch-LSTM\nFigure 5: Accuracy results in the adversarial sets proposed\nby Jia and Liang (2017). AddAny* and AddCommon* re-\nport the worst accuracy after running against all the alter-\nnative adversarial datasets of that speciﬁc type. For fair\ncomparison, experiments on the adversaries generated for\nthe model itself are excluded in those two speciﬁc cases.\n(when real typing errors were available). It is natural to\nthink that in real scenarios, misspelled words will appear\nat a much lower rate than in this test. Thus this result can\nbe seen as a kind of lower-bound estimator for performance\non Natural Noise in real scenarios. When we compare the\nresult of the Natural Noise experiments with those of the\nSwap Noise experiments, we hypothesize that the gap in\nfavor to Natural Noise is because, during the pre-training\nphase, the model observed this type of noise (in real occur-\nrences) and was, therefore, able to learn useful represen-\ntations both for well-written words and for versions with\ncommon misspellings.\n6. Related Work and Discussion\nPrior work (Smith, 2012) discusses the importance of eval-\nuation frameworks that allow characterizing model suc-\ncesses and failures. During previous years, several ap-\nproaches to test NLP models have been proposed on var-\nious tasks, showing that most of the time, predictions are\nmemorized without really understanding the real meaning\nof utterances (Zhang et al., 2017).\nOriginal Swap\nNoise\nMiddle\nRandom\nNoise\nFully\nRandom\nNoise\nKeyboard\nTypo\nNoise\nNatural\nNoise\n0\n20\n40\n60\n80\n100Accuracy (%)\nRoBERTa\nXLNet\nBERT\nMatch-LSTM\nFigure 6: Accuracy results in SQuAD when the models are\nexposed to noise tests. It is clear that all noise types heav-\nily affect the performance of all the models. Further com-\nparative analysis (Section 5.4.2) show some interesting and\nunexpected ﬁndings in these results.\nEarly research demonstrated that NLP models are fragile\nto input perturbations. Some attempts at performing stress\ntests on machine translation systems demonstrated that by\nadding small perturbations on the input text, the general\nperformance of language models could be profoundly af-\nfected (Belinkov and Bisk, 2018; Ribeiro et al., 2018;\nEbrahimi et al., 2018). In the same line, the inspiring work\nof Jia and Liang (2017) proposed an evaluation procedure\nfor language models using the SQuAD dataset. They used\nSQuAD samples, concatenating adversarial sentences at the\nend of the paragraph that contains the answer, and showed\nthat 14 open-source models failed when these changes are\nintroduced.\nOther relevant ﬁndings reveal that models take advantage of\nlexical cues of the dataset, allowing them to solve the prob-\nlem falsely. Gururangan et al. (2018) observed that some\nNLI datasets have annotation artifacts that models exploit to\npredict the answer without even seeing the rest of the sen-\ntence. The same problem was found in the Visual Question\nAnswering (VQA) ﬁeld. Agrawal et al. (2016) analyzed the\nbehavior of three models based on CNN, LSTM, and atten-\ntion mechanism by adding adversaries only to the caption\nof the image, obtaining that most of the times models were\npaying attention to the text and not the image at inference\ntime.\nThe success of language models based on the Transformer\narchitecture in tasks such as machine translation (Vaswani\net al., 2017; Vaswani et al., 2018), text summarization\n(Kroening et al., 2008), reading comprehension (Dehghani\net al., 2018), among others, motivated new research. Recent\nworks have performed adversarial testing of BERT in Ar-\ngument Reasoning Comprehension Task (Niven and Kao,\n2019). They have shown that tested against adversaries,\nBERT outperforms BiLSTM and Bag of Vectors baselines,\nbut still has trouble with logic understanding. Furthermore,\nJin et al. (2019) showed that BERT is the language model\nthat best performs under adversary attacks when compared\nto CNN and LSTM in terms of success rate and perturba-\ntion rate, preservation of semantic content, and efﬁciency\nfor text classiﬁcation tasks. Hsieh et al. (2019) also stud-\nied BERT and compared it with recurrent architectures, in-\nspecting the attention matrices of the models and proposing\nan algorithm to generate adversaries focusing on distracting\nmodels but not humans.\nAlthough there is considerable progress in this area, it can\nbe seen that this article differentiates from previous works\nby systematically evaluating adversaries, artifacts and vari-\nous severe stress conditions on the state-of-the-art language\nmodels based on Transformer (BERT and the models that\ncame after it), in order to verify their language comprehen-\nsion capabilities and generalization power.\nAs a ﬁnal thought, to use these models in real-world ap-\nplications, the reader must take the conclusions exposed in\nthis work carefully, as some of the noise types and adver-\nsaries are far more aggressive than what can be expected\nin real scenarios. Rather than deﬁning a realistic test sce-\nnario, the purpose of this work was to study these models\nrobustness under severe stress conditions to elucidate their\nstrengths and weaknesses, and in some cases quantify an\nupper bound in the impact that noise or misleading informa-\ntion can have in them. Additionally, some of the adversarial\ndatasets of this work could be used to improve the robust-\nness of the models through an adversarial training process,\nas they can be seen as an exaggerated version of common\ntypographical errors made by humans.\n7. Conclusion\nWe conducted a stress test evaluation for Transformer-\nbased language models in NLI and QA tasks. In general,\nour experiments indicate that applying stress tests inﬂu-\nenced the performance of all models, but as expected, more\nrecent models such as XLNet and RoBERTa are more ro-\nbust, showing a better response to this evaluation.\nIn the NLI task, we veriﬁed that the distraction test signiﬁ-\ncantly reduces the performance of all models, especially in\nthe negation test. However, tests on noise examples show\nthat models are somewhat robust, possibly because they\nwere pre-trained in a huge corpus that may have had nat-\nural noise. Due to the same reason, the models show good\nperformance for antonymy relationship. Besides, the anno-\ntation artifacts test showed that these models take both sen-\ntences into account to perform the entailment task and do\nnot take advantage of the artifacts contained in the dataset.\nMoreover, in the QA task, experiments revealed that all\nmodels suffer in performance when tested with adversarial\nor noisy samples. Despite this, Transformer-based models\nturned out to be more robust than their predecessors. We\ncompared Transformer-based models against each and ob-\nserved that while improving in the main task, models also\nimproved in their robustness in a balanced way. We also\nnoticed that some adversaries are model-speciﬁc, as they\naffect one model but not the rest. Speciﬁcally, in the noise\ntests, we observed that the robustness trend also holds,\nbut noticed some unexpected behavior in relative analy-\nsis, as some types of noise affect the models more severely\nthan others, thus revealing speciﬁc weak points across all\nTransformer-based models that did not seem evident at ﬁrst\nsight.\nWe consider this evaluation to be valuable to the commu-\nnity because it exhibits some strengths and weaknesses of\nthe state-of-the-art models. We argue that it is vital that\nmodels pass behavioral checks to ensure proper perfor-\nmance in extreme scenarios, where data failures are not be-\ning considered. Taking this into consideration, we see that\nthere is still room for future improvements on Transformer-\nbased models.\n8. Acknowledgements\nWe would like to thank Alvaro Soto and Denis Parra for\nhelpful comments. We are also grateful to the anonymous\nreviewers for their valuable feedback on an earlier ver-\nsion of this paper. This work has been partially funded\nby Millennium Institute for Foundational Research on Data\n(IMFD).\n9. Bibliographical References\nAgrawal, A., Batra, D., and Parikh, D. (2016). Analyz-\ning the behavior of visual question answering models.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pages 1955–\n1960, Austin, Texas, November. Association for Compu-\ntational Linguistics.\nBelinkov, Y . and Bisk, Y . (2018). Synthetic and natural\nnoise both break neural machine translation. In Interna-\ntional Conference on Learning Representations.\nBengio, Y ., Ducharme, R., and Vincent, P. (2001). A neu-\nral probabilistic language model. In T. K. Leen, et al.,\neditors, Advances in Neural Information Processing Sys-\ntems 13, pages 932–938. MIT Press.\nCarvallo, A. and Parra, D. (2019). Comparing word em-\nbeddings for document screening based on active learn-\ning.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J.,\nand Kaiser, Ł. (2018). Universal transformers. arXiv\npreprint arXiv:1807.03819.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2018). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805.\nEbrahimi, J., Lowd, D., and Dou, D. (2018). On adver-\nsarial examples for character-level neural machine trans-\nlation. In Proceedings of the 27th International Con-\nference on Computational Linguistics , pages 653–663,\nSanta Fe, New Mexico, USA, August. Association for\nComputational Linguistics.\nGururangan, S., Swayamdipta, S., Levy, O., Schwartz, R.,\nBowman, S., and Smith, N. A. (2018). Annotation arti-\nfacts in natural language inference data. In Proceedings\nof the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana, June. Associa-\ntion for Computational Linguistics.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-\nterm memory. Neural computation, 9(8):1735–1780.\nHsieh, Y .-L., Cheng, M., Juan, D.-C., Wei, W., Hsu, W.-\nL., and Hsieh, C.-J. (2019). On the robustness of self-\nattentive models. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguis-\ntics, pages 1520–1529, Florence, Italy, July. Association\nfor Computational Linguistics.\nIyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L.\n(2018). Adversarial example generation with syntacti-\ncally controlled paraphrase networks. In Proceedings of\nthe 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages\n1875–1885, New Orleans, Louisiana, June. Association\nfor Computational Linguistics.\nJia, R. and Liang, P. (2017). Adversarial examples for eval-\nuating reading comprehension systems. In Proceedings\nof the 2017 Conference on Empirical Methods in Nat-\nural Language Processing , pages 2021–2031, Copen-\nhagen, Denmark, September. Association for Computa-\ntional Linguistics.\nJin, D., Jin, Z., Zhou, J. T., and Szolovits, P. (2019). Is\nbert really robust? a strong baseline for natural language\nattack on text classiﬁcation and entailment.\nKroening, D., Sharygina, N., Tonetta, S., Tsitovich, A., and\nWintersteiger, C. M. (2008). Loop summarization us-\ning abstract transformers. In International Symposium\non Automated Technology for Veriﬁcation and Analysis,\npages 111–125. Springer.\nLevy, O., Remus, S., Biemann, C., and Dagan, I. (2015).\nDo supervised distributional methods really learn lexical\ninference relations? In Proceedings of the 2015 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, pages 970–976, Denver, Colorado, May–June.\nAssociation for Computational Linguistics.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\n(2019). Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMax, A. and Wisniewski, G. (2010). Mining naturally-\noccurring corrections and paraphrases from Wikipedia’s\nrevision history. In Proceedings of the Seventh confer-\nence on International Language Resources and Evalu-\nation (LREC’10), Valletta, Malta, May. European Lan-\nguages Resources Association (ELRA).\nNadeau, D. and Sekine, S. (2007). A survey of named en-\ntity recognition and classiﬁcation. Lingvisticae Investi-\ngationes, 30(1):3–26.\nNaik, A., Ravichander, A., Sadeh, N., Rose, C., and Neu-\nbig, G. (2018). Stress test evaluation for natural lan-\nguage inference. In The 27th International Conference\non Computational Linguistics (COLING), Santa Fe, New\nMexico, USA, August.\nNangia, N., Williams, A., Lazaridou, A., and Bowman, S.\n(2017). The RepEval 2017 shared task: Multi-genre nat-\nural language inference with sentence representations.\nIn Proceedings of the 2nd Workshop on Evaluating Vec-\ntor Space Representations for NLP, pages 1–10, Copen-\nhagen, Denmark, September. Association for Computa-\ntional Linguistics.\nNie, Y . and Bansal, M. (2017). Shortcut-stacked sentence\nencoders for multi-domain inference. In Proceedings of\nthe 2nd Workshop on Evaluating Vector Space Represen-\ntations for NLP , pages 41–45, Copenhagen, Denmark,\nSeptember. Association for Computational Linguistics.\nNiven, T. and Kao, H.-Y . (2019). Probing neural network\ncomprehension of natural language arguments. arXiv\npreprint arXiv:1907.07355.\nPennington, J., Socher, R., and Manning, C. (2014).\nGlove: Global vectors for word representation. In Pro-\nceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages 1532–\n1543, Doha, Qatar, October. Association for Computa-\ntional Linguistics.\nRadford, A. and Sutskever, I. (2018). Improving language\nunderstanding by generative pre-training.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. (2016).\nSQuAD: 100,000+ questions for machine comprehen-\nsion of text. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing,\npages 2383–2392, Austin, Texas, November. Associa-\ntion for Computational Linguistics.\nRibeiro, M. T., Singh, S., and Guestrin, C. (2018). Seman-\ntically equivalent adversarial rules for debugging NLP\nmodels. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume\n1: Long Papers), pages 856–865, Melbourne, Australia,\nJuly. Association for Computational Linguistics.\nˇSebesta, K., Bed ˇrichov´a, Z., ˇSormov´a, K., ˇStindlov´a, B.,\nHrdliˇcka, M., Hrdli ˇckov´a, T., Hana, J., Petkevi ˇc, V .,\nJel´ınek, T., ˇSkodov´a, S., Jane ˇs, P., Lund ´akov´a, K., Sk-\noumalov´a, H., Sl ´adek, ˇS., Pierscieniak, P., Toufarov ´a,\nD., Straka, M., Rosen, A., N ´aplava, J., and Pol ´aˇckov´a,\nM. (2017). CzeSL grammatical error correction dataset\n(CzeSL-GEC). LINDAT/CLARIN digital library at the\nInstitute of Formal and Applied Linguistics ( ´UFAL),\nFaculty of Mathematics and Physics, Charles University.\nSeo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\n(2016). Bidirectional attention ﬂow for machine com-\nprehension. ArXiv, abs/1611.01603.\nSmith, N. A. (2012). Adversarial evaluation for models of\nnatural language. arXiv preprint arXiv:1207.0245.\nTsoumakas, G. and Katakis, I. (2007). Multi-label clas-\nsiﬁcation: An overview. International Journal of Data\nWarehousing and Mining (IJDWM), 3(3):1–13.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017).\nAttention is all you need. In Advances in neural infor-\nmation processing systems, pages 5998–6008.\nVaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,\nA. N., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner,\nN., Parmar, N., et al. (2018). Tensor2tensor for neural\nmachine translation. arXiv preprint arXiv:1803.07416.\nVig, J. (2019). A multiscale visualization of at-\ntention in the transformer model. arXiv preprint\narXiv:1906.05714.\nWang, S. and Jiang, J. (2016). Machine comprehen-\nsion using match-lstm and answer pointer. ArXiv,\nabs/1608.07905.\nWilliams, A., Nangia, N., and Bowman, S. (2018). A\nbroad-coverage challenge corpus for sentence under-\nstanding through inference. In Proceedings of the 2018\nConference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers) , pages 1112–\n1122. Association for Computational Linguistics.\nWisniewski, K., Sch ¨one, K., Nicolas, L., Vettori, C.,\nBoyd, A., Meurers, D., Abel, A., and Hana, J.\n(2013). MERLIN: An online trilingual learner\ncorpus empirically grounding the European Ref-\nerence Levels in authentic learner data. URL\nhttps://www.ukp.tu-darmstadt.de/data/\nspelling-correction/rwse-datasets.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-\nicz, M., and Brew, J. (2019). Huggingface’s transform-\ners: State-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR., and Le, Q. V . (2019). Xlnet: Generalized autore-\ngressive pretraining for language understanding. arXiv\npreprint arXiv:1906.08237.\nZesch, T. (2012). Measuring contextual ﬁtness using error\ncontexts extracted from the Wikipedia revision history.\nIn Proceedings of the 13th Conference of the European\nChapter of the Association for Computational Linguis-\ntics, pages 529–538, Avignon, France, April. Associa-\ntion for Computational Linguistics.\nZhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.\n(2017). Understanding deep learning requires rethinking\ngeneralization.\nAppendix A: Detailed Results on SQuAD Tests\nIn Table 5, we report the detailed results of the experiments performed on the adversarial versions of the SQuAD dataset\nusing the adversaries proposed by Jia and Liang (2017). In all the experiments, each model was trained/ﬁne-tuned on the\noriginal SQuAD v1.1 training set, and tested on each one of the generated adversarial datasets. As a result, we see that all\nmodels are affected by these adversarial samples, but also found that some adversaries are model-speciﬁc because they do\nnot affect all models as much as they affect the model they are targeting.\nModel under Evaluation\nMatch- BERT - XLNet- RoBERTa-\nTargeted Model LSTM BiDAF Large Large Large\nOriginal (for reference only) 60.8 75 .5 82 .5 85 .2 85 .8\nAddOneSent 30.0 45 .7 64 .6 67 .7 70 .3\nAddSent\nMatch-LSTM Single 24.8 40 .3 62 .8 64 .6 67 .8\nMatch-LSTM Ensemble 24.2 40 .2 62 .1 64 .4 68 .0\nBiDAF Single 25.7 34 .3 62 .3 64 .4 67 .7\nBiDAF Ensemble 25.9 38 .3 61 .7 64 .0 67 .6\nBERT-Base 26.3 − 60.8 63 .3 66 .6\nBERT-Large 26.9 − 55.9 62 .8 66 .0\nXLNet-Large 27.1 − 61.6 61 .6 66 .6\nRoBERTa-Large 27.6 − 60.9 63 .2 61 .5\nAddAny\nMatch-LSTM Single 38.3 57 .1 73 .8 78 .8 78 .8\nMatch-LSTM Ensemble 26.1 50 .4 70 .7 78 .0 77 .2\nBiDAF Single 43.8 4 .8 72 .2 79 .6 78 .4\nBiDAF Ensemble 34.7 25 .0 68 .8 79 .1 76 .3\nAddCommon\nMatch-LSTM Single 55.8 − 82.1 83 .6 84 .6\nMatch-LSTM Ensemble 44.7 − 81.4 83 .0 84 .6\nBiDAF Single 56.7 41 .7 80 .9 83 .4 84 .8\nBiDAF Ensemble 52.8 − 79.9 82 .0 83 .2\nTable 5: Adversarial examples transferability between models. Each row measures accuracy (%) on adversarial ex-\namples designed to attack one particular model. Each column reports the test results of one particular model on all the\nadversarial datasets.\nThe information from the ﬁrst two columns was obtained by running the ofﬁcial implementations used by Jia and Liang\n(2017). The results are slightly different from the original work because the original weights were not available.\nAppendix B: Attention-level Results of NLI Task\nAntonymy Evaluation\nFor this analysis, we took a representative adversarial example where a word in the sentence was replaced by its antonym.\nThe model is asked to decide if there is a contradiction, neutral, or entailment relationship between them. We expect the\nmodel to connect the attention between the replaced words to predict the correct answer. Assume the following pair of\nsentences:\nI saw that daylight was coming, and heard the people sleeping up.\nI saw that daylight was coming, and heard the people waking up.\nIn this representative example for testing antonyms, we computed the attentions produced by XLNet, RoBERTa, and BERT.\nWe checked the layers and heads where a clear attention pattern was present between the word and its antonym, as shown\nin Figures 7 - 9. Within this particular case, for XLNet, we saw that only 2.86% of the total attention heads and layers had\nthis pattern. For RoBERTa, this number was 2.60%, and for BERT 1.56%. On the other hand, for all models, most of the\nattention was paid to separators and all words from the reference sentence without distinction (Figure 10).\nFigure 7: XLNet antonym test\nFigure 8: RoBERTa antonym test\nFigure 9: BERT antonym test\n Figure 10: Failed antonym test\nNumerical Reasoning Evaluation\nFor samples of numerical reasoning for NLI, the expectation is that the model should pay attention to words like ”more”\nor ”less” to check if there is a change in numerical references. Assume the following pair of sentences:\nThe next day Bob took the test and with this grade, included the new average, was more than 48.\nThe next day Bob took the test and with this grade, included the new average, was 78.\nNevertheless, for this testing example, the premise includes ”more than 48” and the hypothesis replaces this last part\nby ”78”, but all the models (XLNet, RoBERTa and BERT) incorrectly predicted ”contradiction”. We observed that the\nexpected pattern (shown in Figures 11- 13) is a very infrequent pattern for all models (for XLNet it appeared in 5.20% of\nthe cases, for RoBERTa in only 4.42% and for BERT this percentage was 1.30%). For other cases, they focused on sentence\nseparators (as shown in Fig 14).\nFigure 11: XLNet numerical test\nFigure 12: RoBERTa numer. test\nFigure 13: BERT numerical test\nFigure 14: Failed numerical test\nAppendix C: Attention-level Results of QA Task\nQA task attention-level evaluation\nFor the QA task, we manually inspected failure cases to see the amount of attention the model paid to the introduced\nadversaries versus to the correct answer. Here we show one representative example of a ”what” question:\nQuestion: What company took over Edison Machine works?.\nAnswer: General Electric.\nAdversary: Stark Industries took over Calpine Device Well.\nIn this particular example, with the question ”What company took over Edison Machine works?” , the correct answer was\n”General Electric”, and the artiﬁcially introduced adversary was ”Stark Industries”, appended at the end of the context of\nthe original sample.\nAll models fell into the same trap. It can be seen in Figures 15- 17 that they paid attention to the wrong answer. In this case,\nthis pattern appeared in 52% of the layer-heads of XLNet, 60% in the case of RoBERTa, and 30% on BERT. Nevertheless,\nwhile checking the level of certainty of each model in the predicted wrong answer for this example, XLNet had a 43.3%\ncertainty probability, 75.5 % BERT, and the most mistaken was RoBERTa with a 99.9% certainty probability for predicting\nthe wrong answer (which is consistent with the sharpness of attention in Figure 16). This behavior provides evidence that\nthe three models behave slightly different and that increased accuracy in the main task (before adversarial evaluation) is no\ndirect indicator of increased robustness in all cases, but only in the average case.\nFigure 15: XLNet SQuAD\n Figure 16: RoBERTa SQuAD\n Figure 17: BERT SQuAD",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7897517681121826
    },
    {
      "name": "Computer science",
      "score": 0.7769469022750854
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6489468812942505
    },
    {
      "name": "Inference",
      "score": 0.5614284873008728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5168778896331787
    },
    {
      "name": "Adversarial system",
      "score": 0.48594462871551514
    },
    {
      "name": "Language model",
      "score": 0.4816388189792633
    },
    {
      "name": "Natural language",
      "score": 0.42686140537261963
    },
    {
      "name": "Natural language understanding",
      "score": 0.42366015911102295
    },
    {
      "name": "Architecture",
      "score": 0.4199143946170807
    },
    {
      "name": "Natural language processing",
      "score": 0.40629732608795166
    },
    {
      "name": "Machine learning",
      "score": 0.40007439255714417
    },
    {
      "name": "Voltage",
      "score": 0.15708637237548828
    },
    {
      "name": "Engineering",
      "score": 0.10797694325447083
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}