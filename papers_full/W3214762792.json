{
  "title": "Multi-View Stereo with Transformer",
  "url": "https://openalex.org/W3214762792",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2035661069",
      "name": "Zhu Jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128362379",
      "name": "Peng Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999987658",
      "name": "Li Wanqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359230951",
      "name": "Shen Haifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1947874271",
      "name": "Zhang Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112489698",
      "name": "Lei Jianjun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2519683295",
    "https://openalex.org/W2926429807",
    "https://openalex.org/W2982169158",
    "https://openalex.org/W2085905957",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W2129404737",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3204354488",
    "https://openalex.org/W2205172244",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3166285241",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2992718396",
    "https://openalex.org/W2963073398",
    "https://openalex.org/W3034564916",
    "https://openalex.org/W3034530552",
    "https://openalex.org/W2128052895",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W1964057156",
    "https://openalex.org/W2604231069",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3108346482",
    "https://openalex.org/W3170262190",
    "https://openalex.org/W2738551266",
    "https://openalex.org/W3102132650",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W1539230104",
    "https://openalex.org/W2997111976",
    "https://openalex.org/W2985066174",
    "https://openalex.org/W3034600477",
    "https://openalex.org/W2967693513",
    "https://openalex.org/W3204267695",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "This paper proposes a network, referred to as MVSTR, for Multi-View Stereo (MVS). It is built upon Transformer and is capable of extracting dense features with global context and 3D consistency, which are crucial to achieving reliable matching for MVS. Specifically, to tackle the problem of the limited receptive field of existing CNN-based MVS methods, a global-context Transformer module is first proposed to explore intra-view global context. In addition, to further enable dense features to be 3D-consistent, a 3D-geometry Transformer module is built with a well-designed cross-view attention mechanism to facilitate inter-view information interaction. Experimental results show that the proposed MVSTR achieves the best overall performance on the DTU dataset and strong generalization on the Tanks &amp; Temples benchmark dataset.",
  "full_text": "Multi-View Stereo with Transformer\nJie Zhu1 Bo Peng1 Wanqing Li2 Haifeng Shen3 Zhe Zhang1 Jianjun Lei1*\n1 Tianjin University 2 University of Wollongong 3 AI Labs, Didi Chuxing\n{3016207561,bpeng,zz300,jjlei}@tju.edu.cn wanqing@uow.edu.au shenhaifeng@didiglobal.com\nAbstract\nThis paper proposes a network, referred to as MVSTR,\nfor Multi-View Stereo (MVS). It is built upon Transformer\nand is capable of extracting dense features with global con-\ntext and 3D consistency, which are crucial to achieving re-\nliable matching for MVS. Speciﬁcally, to tackle the problem\nof the limited receptive ﬁeld of existing CNN-based MVS\nmethods, a global-context Transformer module is ﬁrst pro-\nposed to explore intra-view global context. In addition, to\nfurther enable dense features to be 3D-consistent, a 3D-\ngeometry Transformer module is built with a well-designed\ncross-view attention mechanism to facilitate inter-view in-\nformation interaction. Experimental results show that the\nproposed MVSTR achieves the best overall performance on\nthe DTU dataset and strong generalization on the Tanks &\nTemples benchmark dataset.\n1. Introduction\nAs a fundamental problem in computer vision, multi-\nview stereo (MVS) has received great interest because of\nits wide range of applications, such as robot navigation, au-\ntonomous driving, and augmented reality. Given calibrated\nmulti-view images, MVS aims to recover a 3D model of the\nobserved scene. For decades, MVS has been studied ex-\ntensively to improve the quality of the recovered 3D mod-\nels. While traditional methods [3, 10, 11, 26, 28, 32] have\nmade great progress, there are still some intractable prob-\nlems, such as incomplete reconstruction and limited scala-\nbility.\nRecently, learning-based MVS methods [6, 7, 12, 23, 24,\n30, 31, 33–39] have shown superior performance over tra-\nditional counterparts on MVS benchmarks [14, 19]. These\nlearning-based methods make use of convolutional neural\nnetworks (CNNs) to infer a depth map for each view, and\ncarry out a separate multi-view depth fusion process to re-\nconstruct 3D point clouds. For depth map inference, they\ngenerally ﬁrst utilize a 2D CNN to extract dense features\n*Corresponding author.\nFigure 1. Visual comparison between the CNN-based state-of-the-\nart MVS methods [7, 30, 37] and the proposed Transformer-based\nMVSTR.\nof each view separately, and then perform robust feature\nmatching to regress the depth map. However, these meth-\nods often suffer from matching ambiguities and mismatches\nin challenging regions, such as texture-less areas or non-\nLambertian surfaces. One of the main reasons is that dense\nfeatures extracted by CNNs with a limited receptive ﬁeld\nare difﬁcult to capture global context. The lack of global\ncontext usually leads to local ambiguities in untextured or\ntexture-less regions, thus reducing the robustness of match-\ning. Although some recent works [31, 34] try to obtain\nlarge context using deformable convolution or multi-scale\ninformation aggregation, the solution of mining the global\ncontext in each view has not been explored yet for MVS.\nBesides, in previous methods, the feature of each view is\nextracted independently from other views. These indepen-\ndently extracted features are hardly optimal for 3D recon-\nstruction. For MVS, due to the widespread existence of\nnon-Lambertian surfaces, the features without being 3D-\nconsistent at the same 3D position may vary considerably\nacross different views, which leads to mismatches. There-\nfore, exploring a way to access 3D-consistent features is\ncritical for robust and reliable matching in MVS.\nTo cope with the aforementioned problems, a novel\nTransformer-based network MVSTR is proposed in this pa-\nper. Motivated by the long-range modeling capabilities of\nself-attention in Transformer, a global-context Transformer\n1\narXiv:2112.00336v1  [cs.CV]  1 Dec 2021\nis designed so that intra-view global context is effectively\nacquired. In addition, a 3D-geometry Transformer is con-\nstructed with cross-view attention. With the well-designed\ncross-view attention mechanism, the feature of each view\nwill be guided by the information from other views to im-\nprove 3D consistency. The proposed MVSTR achieves reli-\nable matching in challenging regions, and thus obtains more\naccurate and complete reconstruction results than the CNN-\nbased methods, as shown in Figure 1.\nThe main contributions of this paper are as follows:\n1) A new MVS network built upon Transformer, termed\nMVSTR, is proposed. To our best knowledge, this is the\nﬁrst Transformer architecture for MVS.\n2) A global-context Transformer is proposed to explore\nthe intra-view global context.\n3) To acquire 3D-consistent features, a 3D-geometry\nTransformer with the well-designed cross-view attention\nmechanism is proposed to efﬁciently enable inter-view in-\nformation interaction for extraction of multi-view features.\n4) The proposed method outperforms the state-of-the-art\nmethods on the DTU dataset [14] and achieves robust gen-\neralization on the Tanks & Temples benchmark dataset [19].\n2. Related Work\n2.1. Traditional MVS Methods\nIn the last decades, traditional MVS methods have been\nthoroughly researched and made great progress. Furukawa\net al. [10] implemented MVS as a procedure of match, ex-\npansion, and ﬁlter. Campbell et al. [3] designed a discrete\nlabel Markov Random Field (MRF) optimization to remove\noutliers of depth maps. Tolaet al. [28] proposed an efﬁcient\nMVS method, which utilizes DAISY descriptors for match-\ning to reduce computational complexity of large-scale MVS\nreconstruction. Additionally, a diffusion-like propagation\nscheme based on the patchmatch stereo algorithm [1] is em-\nployed in [11] to fully utilize parallel acceleration capabili-\nties of GPUs. An MVS system named COLMAP [26] is de-\nveloped to jointly estimate scene depth, surface normal, and\npixel-wise view selection. More recently, Xu et al. [32] im-\nproved depth map quality by utilizing structured region in-\nformation and multi-scale geometric consistency. Although\nthese traditional methods have greatly advanced the devel-\nopment of MVS, their robustness and reliability have been\nchallenged by multiple issues such as low-texture regions,\nillumination changes, and reﬂections.\n2.2. Learning-Based MVS Methods\nInspired by the great success of CNNs in computer vi-\nsion, many learning-based MVS methods have been pro-\nposed recently. Yao et al. [36] proposed MVSNet to in-\nfer the depth map for large-scale MVS reconstruction. The\nMVSNet builds the variance-based cost volume with fea-\ntures extracted by a 2D CNN and performs cost regular-\nization with a 3D CNN to regress the depth map. In or-\nder to reduce the huge memory consumption of the 3D\nCNN, the cost volume is regularized with gated recurrent\nunits (GRUs) [8] in [37]. Besides, Wei et al. [31] pro-\nposed AA-RMVSNet to further improve the performance\nof MVS reconstruction by utilizing adaptive aggregation\nand long short-term memory (LSTM) [13]. To develop a\ncomputationally efﬁcient network, some works adopt multi-\nstage strategy to achieve MVS reconstruction. Yuet al. [39]\ndesigned Fast-MVSNet, which ﬁrstly forms a sparse cost\nvolume to infer an initial sparse depth map and then opti-\nmizes the sparse depth map gradually. Gu et al. [12] con-\nstructed a cascade cost volume in a coarse-to-ﬁne manner\nto recover a high-resolution depth map. Cheng et al. [7]\nbuilt adaptive thin volumes in multiple stages to progres-\nsively increase depth resolution and precision. Moreover,\nby adopting the pyramid structure, Yanget al. [35] designed\nCVP-MVSNet to iteratively reﬁne depth maps. Wang et\nal. [30] introduced the traditional patchmatch stereo algo-\nrithm [1] into a learning-based coarse-to-ﬁne framework.\nTo further improve the performance of MVS reconstruction,\nMa et al. [24] utilized an epipolar-assembling module to\nregress a coarse depth map and developed an entropy-based\nstrategy for reﬁnement. Though the learning-based meth-\nods have achieved great success, they still suffer from match\nambiguities and mismatches in challenging regions proba-\nbly due to insufﬁcient global context and 3D-inconsistent\nrepresentations.\n2.3. Transformer in Vision Related Tasks\nThanks to the powerful capabilities of modeling long-\nrange dependencies, Transformer [29] and its variants [16,\n18, 20] have greatly advanced the development of natural\nlanguage processing. Recently, Transformer has been in-\ncreasingly explored in various vision tasks. Dosovitskiy et\nal. [9] proposed a vision Transformer to boost the perfor-\nmance of image classiﬁcation. Carion et al. [4] presented\nan end-to-end object detection network based on Trans-\nformer. Besides, Transformers are utilized for homogra-\nphy estimation, relative pose estimation, and visual local-\nization in [27]. A pre-trained image processing Transformer\nis introduced for super-resolution, denoising, and deraining\nin [5]. In addition, Li et al. [21] proposed to take the ad-\nvantages of both CNN and Transformer for stereo disparity\nestimation. Liu et al. [22] developed a hierarchical vision\nTransformer, known as Swin Transformer, for a broad range\nof vision tasks.\nThis paper exploits the Transformer architecture for the\nMVS task. The proposed MVSTR takes full advantages\nof Transformer to enable features to be extracted under the\nguidance of global context and 3D geometry, which brings\nsigniﬁcant improvement on reconstruction results.\n2\nFigure 2. The overall architecture of the proposed MVSTR.\n3. Proposed Method\n3.1. Overall Architecture\nThe overall architecture of the proposed MVSTR is il-\nlustrated in Figure 2. Given one reference image Ir and\nN source images\n{\nIi\ns\n}N\ni=1, local features are ﬁrst extracted\nusing 2D CNNs and mapped into sequences after posi-\ntional encoding and ﬂattening. For the feature of each view,\nthe global-context Transformer module is built to explore\nthe intra-view global context. To acquire 3D-consistent\ndense features, a 3D-geometry Transformer module is built\nthrough cross-view attention. With the well-designed cross-\nview attention mechanism, information interaction across\nmultiple views is efﬁciently explored. In the proposed\nMVSTR, the two modules are alternated Z times so that\nthe transformed feature of each view is capable of effec-\ntively perceiving both intra-view global context and inter-\nview 3D geometry. Finally, together with the transformed\nfeatures and local features, a depth map is generated by a\nwidely-used coarse-to-ﬁne regression strategy [12].\n3.2. Global-Context Transformer\nTo acquire dense features with intra-view global\ninformation, the proposed global-context Transformer\nmodule utilizes multi-head self-attention for long-\nrange dependency learning. Before being fed into\nthe global-context Transformer module, reference fea-\nture Fr = [ Fr,1,Fr,2,...,F r,j] and source features{\nFi\ns =\n[\nFi\ns,1,Fi\ns,2,...,F i\ns,j\n]}N\ni=1 extracted from 2D\nCNNs, are ﬁrst supplemented with the learnable 2D\npositional encodings P = [P1,P2,...,P j] for each pixel,\nwhere j denotes the number of pixels of the CNN feature\nmap of each view. It is worth noted that the positional\nencodings P are the same for all views. Then, the features\nof the reference view and source views with position\ninformation are ﬂattened into sequences Xr and\n{\nXi\ns\n}N\ni=1,\nwhich are expressed by:\nXr = [P1 + Fr,1,P2 + Fr,2,...,P j + Fr,j] (1)\nXi\ns =\n[\nP1 + Fi\ns,1,P2 + Fi\ns,2,...,P j + Fi\ns,j\n]\n(2)\nThe structure of the global-context Transformer module\nis illustrated as Figure 3, the core of which is the Trans-\nformer Layer-S. For each view, an individual Transformer\nLayer-S is applied to explore the intra-view global context.\nGiven Xr as the input, Transformer Layer-S is expressed\nas:\nQr = Kr = Vr = Xr (3)\nX′\nr = Concat (LN (MSA (Qr,Kr,Vr)) ,Xr) (4)\nCr = LN (FFN (X′\nr)) +Xr (5)\n3\nFigure 3. Global-context Transformer module.\nwhere Concat(,) denotes the operation of concatenation,\nLN(·) denotes the layer normalization. MSA (Qr,Kr,Vr)\ndenotes the multi-head self-attention with query Qr, key\nKr, and value Vr. The multi-head self-attention enables\neach pixel to establish dependencies with all other pixels\nwithin the view. FFN(·) denotes a fully connected feed-\nforward network, which is adopted to improve the ﬁtting\nability of the model. Cr represents the context-aware fea-\nture of the reference view. Similarly, given Xi\ns as the in-\nput to Transformer Layer-S, the context-aware feature of\nthe corresponding source view Ci\ns is also obtained in the\nsame way.\nThe global-context Transformer module is capable of\nexploring the global context within each view. With the\ncontext-aware features, the local ambiguities in large un-\ntextured or texture-less areas will be reduced.\n3.3. 3D-Geometry Transformer\nTo acquire dense features with 3D consistency, a 3D-\ngeometry Transformer module is proposed to effectively fa-\ncilitate the information interaction across multiple views.\nEmploying a cross-view attention mechanism, Transformer\nLayer-Cr is ﬁrst constructed to obtain 3D-consistent ref-\nerence feature Tr by enabling the reference view to ac-\ncess information in all source views. Then, based on the\n3D-consistent reference feature Tr, Transformer Layer-C s\nis constructed to acquire features being 3D-consistent with\nsource views by exploiting the information in Tr.\nThe details of the 3D-geometry Transformer module\nwith the cross-view attention mechanism are shown in Fig-\nure 4. The context-aware features Cr and\n{\nCi\ns\n}N\ni=1 gener-\nFigure 4. 3D-geometry Transformer module.\nated from the global-context Transformer module, are fed\ninto the 3D-geometry Transformer module. To integrate\nsource-view information with the reference view, N cross-\nview attentions are ﬁrst employed, each of which is utilized\nto enhance Cr with Ci\ns. These N cross-view attentions are\ndenoted as Transformer Layer-Cr, which is expressed as:\nQr = Cr,Ki\ns = Vi\ns = Ci\ns (6)\nCs,i\nr = Concat\n(\nLN\n(\nMCA\n(\nQr,Ki\ns,V i\ns\n))\n,Cr\n)\n(7)\n˜Cs,i\nr = LN\n(\nFFN\n(\nCs,i\nr\n))\n+ Cr (8)\nwhere MCA\n(\nQr,Ki\ns,V i\ns\n)\n) denotes the multi-head cross-\nattention with query Qr, key Ki\ns, and value Vi\ns . ˜Cs,i\nr de-\nnotes the feature of the reference view enhanced by Ci\ns.\nBased on Transformer Layer-C r,\n{\n˜Cs,i\nr\n}N\ni=1\nare ob-\ntained by enhancing Cr with\n{\nCi\ns\n}N\ni=1. Then, an aver-\nage operation is adopted to fuse the enhanced features{\n˜Cs,i\nr\n}N\ni=1\nto acquire the 3D-consistent feature Tr of the\nreference view, which is formulated as:\nTr = 1\nN\nN∑\ni=1\n˜Cs,i\nr (9)\nSubsequently, Transformer Layer-Cs is constructed with\nadditional Ncross-view attentions, which are utilized to en-\nhance\n{\nCi\ns\n}N\ni=1 with the 3D-consistent featureTr of the ref-\nerence view, resulting in 3D-consistent features\n{\nTi\ns\n}N\ni=1 of\nsource views. The Transformer Layer-Cs is formulated as:\nQi\ns = Ci\ns,Kr = Vr = Tr (10)\n4\nCr,i\ns = Concat\n(\nLN\n(\nMCA\n(\nQi\ns,Kr,Vr\n))\n,Ci\ns\n)\n(11)\nTi\ns = LN\n(\nFFN\n(\nCr,i\ns\n))\n+ Ci\ns (12)\nwhere MCA\n(\nQi\ns,Kr,Vr\n)\ndenotes the multi-head cross-\nattention with query Qi\ns, key Kr, and value Vr. Ti\ns rep-\nresents the 3D-consistent feature of i-th source view.\nBeneﬁcial from the designed mechanism, 3D-consistent\nfeatures are obtained for the reference view and source\nviews. With these 3D-consistent features, mismatches in\nnon-Lambertian surfaces are expected to be efﬁciently alle-\nviated, hence, to improve the 3D reconstruction.\n3.4. Loss Function and Implementation\nLoss Function. Similar to the existing coarse-to-ﬁne\nMVS works, smoothl1 loss is applied at each scale to super-\nvise depth estimation results of different resolutions [12].\nThe loss function of MVSTR can be formulated as:\nLoss =\nM∑\nm=1\nαm · Lm (13)\nwhere M refers to total number of scales in the proposed\nnetwork and is set to 3. Lm and αm refers to the loss and\nits corresponding loss weight at scale m, respectively. In\nparticular, m= 1represents the coarsest scale whilem= 3\nrepresents the ﬁnest scale. With mincreasing from 1 to 3,\nαm is set to 0.5, 1.0, and 2.0, respectively.\nImplementation. The 2D CNN which is adopted for\nextracting feature from individual view, is an eight-layer ar-\nchitecture similar to the one used in MVSNet [36]. In par-\nticular, for the sake of computing efﬁciency, the batch nor-\nmalization layer and the ReLU activation are replaced with\na uniﬁed in-place activated batch normalization layer [2],\nwhich brings nearly 40% memory savings. The output for\neach view is a 32-channel feature map downsized by four\ncompared with the input image. Note that the weights of\nthe 2D CNNs are shared among multiple views. In addition,\nthe global-context Transformer module and 3D-geometry\nTransformer module are alternatively stacked Z = 4times.\nFor the global-context Transformer module, the weights\nof Transformer Layer-S are shared among multiple views.\nSimilarly, for both Transformer Layer-Cr and Transformer\nLayer-Cs, the weights of cross-view attentions are shared\namong multiple views. For all Transformer layers, a lin-\nearized multi-head attention [16] is adopted, in which the\nnumber of heads is set to 4. Besides, the coarse-to-ﬁne\ndepth regression consists of cost volume pyramid construc-\ntion and 3D CNN regularization of 3 scales. For cost vol-\nume pyramid construction, the transformed features are ﬁrst\nutilized to construct cost volume at the coarsest scale via\ndifferentiable homography warping [36] and average group-\nwise correlation [33] in which the number of groups is set to\n8. At larger scales, the transformed features are ﬁrst upsam-\npled by bilinear interpolation and fused with the features\nMethod Acc.(mm) Comp.(mm) Overall(mm)\nTraditional\nFuru [10] 0.613 0.941 0.777\nTola [28] 0.342 1.190 0.766\nCamp [3] 0.835 0.554 0.695\nGipuma [11] 0.283 0.873 0.578\nColmap [26] 0.400 0.644 0.532\nLearning-based\nSurfaceNet [15] 0.450 1.040 0.745\nMVSNet [36] 0.396 0.527 0.462\nR-MVSNet [37] 0.383 0.452 0.417\nP-MVSNet [23] 0.406 0.434 0.420\nPoint-MVSNet [6] 0.342 0.411 0.376\nCIDER [33] 0.417 0.437 0.427\nFast-MVSNet [39] 0.336 0.403 0.370\nCasMVSNet [12] 0.325 0.385 0.355\nUCS-Net [7] 0.338 0.349 0.344\nCVP-MVSNet [35] 0.296 0.406 0.351\nPV A-MVSNet [38] 0.379 0.336 0.357\nPatchmatchNet [30] 0.427 0.277 0.352\nAA-RMVSNet [31] 0.376 0.339 0.357\nEPP-MVSNet [24] 0.413 0.296 0.355\nMVSTR(Ours) 0.356 0.295 0.326\nTable 1. Quantitative results of different methods on the DTU eval-\nuation set (lower is better).\nof the corresponding scale in the 2D CNNs by a convolu-\ntional layer with a 1×1 ﬁlter. Then, ﬁner cost volumes are\nconstructed with the fused features in the same manner as\nthe coarsest scale. The number of depth hypotheses and the\ncorresponding depth intervals are set to be the same as those\nin CasMVSNet [12]. For 3D CNN cost regularization, 3D\nU-Nets [25] without shared weights are applied at 3 scales.\nSimilar to the 2D CNNs, the batch normalization layer and\nthe ReLU activation in 3D U-Nets are replaced with the in-\nplace activated batch normalization layer. Finally, the soft\nargmin operation [17] is used to regress depth maps at dif-\nferent scales. The proposed method is implemented using\nPyTorch on a machine with a GPU of NVIDIA GeForce\nGTX 1080Ti and a CPU of Intel Core i9-9900K processor\n@3.60 GHz. During training, the number of source images\nN is set to 2, and the resolution of input images is set to 640\n× 512. The network is optimized by Adam with β1 = 0.9\nand β2 = 0.999 for 16 epochs. The initial learning rate is\nset to 1 × 10−3 and reduced by half at the 10th, 12th, and\n14th epochs.\n4. Experimental Results\n4.1. Experimental Settings\nThe MVS datasets including DTU [14] and Tanks &\nTemples [19] are used to evaluate the proposed MVSTR.\nDTU is an indoor dataset, which consists of 124 different\nscenes. Each scene is covered by 49 or 64 views under 7 dif-\nferent lighting conditions. Tanks & Temples is a benchmark\ndataset with both indoor and outdoor scenes. It contains two\n5\nFigure 5. Visual comparison with state-of-the-art methods [7, 30, 37] of scan33 and scan13 on the DTU evaluation dataset. The ﬁrst and\nthird rows show the point clouds reconstructed by the corresponding methods while the second and fourth rows show those zoomed-in\nlocal areas marked with red rectangle boxes.\ngroups called the intermediate and the advanced ones. The\ntwo groups of Tanks & Temples are used for generalization\nveriﬁcation of the proposed method.\nDuring testing on the DTU evaluation set, the number\nof source images N is set to 4, and the resolution of input\nimages is set to 1152 × 864. When verifying generalization\non the Tanks & Temples benchmark dataset, the number of\nsource images N is set to 6, the input resolution is set to\n1920 × 1056 for images with original resolution of 1920 ×\n1080, and 2048 × 1056 for images with original resolution\nof 2048 × 1080, which is the same as [12].\nSimilar to the previous MVS methods [12, 30, 36], post-\nprocessing steps including photometric depth map ﬁltering,\ngeometric depth map ﬁltering, and depth fusion are used to\n6\nF-score Intermediate Group Advanced Group\nFam. Franc. Horse Light. M60 Pan. Play. Train Mean Audi. Ballr. Courtr. Museum Palace TempleMean\nMVSNet [36] 55.99 28.55 25.07 50.79 53.96 50.86 47.90 34.69 43.48 - - - - - - -\nR-MVSNet [37]69.96 46.65 32.59 42.95 51.88 48.80 52.00 42.38 48.40 12.55 29.09 25.06 38.68 19.14 24.96 24.91\nP-MVSNet [23]70.04 44.64 40.22 65.20 55.08 55.17 60.37 54.29 55.62 - - - - - - -\nPoint-MVSNet [6]61.79 41.15 34.20 50.79 51.97 50.85 52.38 43.06 48.27 - - - - - - -\nCIDER [33] 56.79 32.39 29.89 54.67 53.46 53.51 50.48 42.85 46.76 12.77 24.94 25.01 33.64 19.18 23.15 23.12\nFast-MVSNet [39]65.18 39.59 34.98 47.81 49.16 46.20 53.27 42.91 47.39 - - - - - - -\nCasMVSNet [12]76.37 58.45 46.26 55.81 56.11 54.06 58.18 49.51 56.84 19.81 38.46 29.10 43.87 27.36 28.11 31.12\nUCS-Net [7] 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89 54.83 - - - - - - -\nCVP-MVSNet [35]76.50 47.74 36.34 55.12 57.28 54.28 57.43 47.54 54.03 - - - - - - -\nPV A-MVSNet [38]69.36 46.80 46.01 55.74 57.23 54.75 56.70 49.06 54.46 - - - - - - -\nPatchmatchNet [30]66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81 53.15 23.69 37.73 30.04 41.80 28.31 32.29 32.31\nMVSTR(Ours)76.92 59.82 50.16 56.73 56.53 51.22 56.58 47.48 56.93 22.83 39.04 33.87 45.46 27.95 27.97 32.85\nTable 2. Quantitative results of different methods on the Tanks & Temples benchmark dataset (higher is better).\nFigure 6. Visual comparison with state-of-the-art methods [12, 30, 37] of Courtroom and Auditorium on the Tanks & Temples benchmark\ndataset. The top row is the error visualization of Courtroom and the bottom row is the error visualization of Auditorium. The darker in the\nmap the bigger the error in the point cloud.\ngenerate 3D point clouds. The standard evaluation proto-\ncol [14] is adopted to evaluate the performance of recon-\nstructing 3D point clouds. In particular, the accuracy and\nthe completeness of the reconstructed point clouds are cal-\nculated using the ofﬁcial MATLAB code provided by DTU.\nFurther, the average of the accuracy and the completeness\nis expressed as the overall score. For the Tanks & Temples\nbenchmark dataset, reconstructed point clouds are uploaded\nonline to calculate F-score.\n4.2. Evaluation on the DTU Dataset\nTo demonstrate the effectiveness of the proposed\nmethod, quantitative and qualitative experiments on the\nDTU dataset are conducted. Table 1 reports the results and\ncomparison with different methods. It can be seen from\nthe table that the proposed MVSTR achieves the best over-\nall performance with competitive accuracy and complete-\nness. In particular, compared with the multi-stage CNN-\nbased methods ( i.e., FastMVSNet [39], CasMVSNet [12],\nUCS-Net [7], CVP-MVSNet [35], PatchmatchNet [30], and\nEPP-MVSNet [24]), the proposed MVSTR achieves obvi-\nous improvement in terms of the overall score. In addition,\nthe proposed MVSTR outperforms the methods with RNN-\nbased cost regularizations ( i.e., R-MVSNet [37] and AA-\nRMVSNet [31]) in terms of accuracy, completeness, and\nthe overall score.\nVisual comparison of the results achieved by different\nmethods are shown in Figure 5. It can be seen that the\nproposed MVSTR reconstructs more complete point clouds\nwith well-preserved structure. Specially, compared with\nPatchmatchNet [30], the proposed MVSTR obtains denser\nreconstruction results with fewer outliers. It is mainly be-\ncause that the proposed MVSTR is capable of acquiring\ncontext-aware and 3D-consistent features, which help re-\nduce matching ambiguities and mismatches in challenging\nregions to further improve the reconstruction quality.\n4.3. Evaluation on the Tanks & Temples Benchmark\nIn order to verify the generalization of the proposed\nmethod, the model trained on the DTU training dataset\nwithout any ﬁne-tuning is utilized to test on the Tanks &\nTemples benchmark dataset. Table 2 shows the quantita-\n7\nMethods Acc.(mm) Comp.(mm) Overall(mm)\nw/o GCT 0.369 0.297 0.333\nw/o 3GT 0.363 0.298 0.331\nMVSTR 0.356 0.295 0.326\nTable 3. Ablation study of the two Transformer modules (lower is\nbetter).\nNumber of Transformers Acc.(mm) Comp.(mm) Overall(mm)\n0 0.374 0.305 0.340\n2 0.368 0.289 0.329\n4 0.356 0.295 0.326\n6 0.359 0.300 0.330\nTable 4. Ablation study of the number of Transformers (lower is\nbetter).\nMethod GPU Memory(MB) Run-time(s) Overall(mm)\nMVSNet [36] 10632 1.435 0.551\nCasMVSNet [12] 5667 0.459 0.355\nPatchmatchNet [30] 2323 0.417 0.374\nMVSTR(Ours) 3879 0.818 0.326\nTable 5. Comparison of GPU memory, run-time and overall per-\nformance between the proposed method and other state-of-the-art\nlearning-based methods (lower is better).\ntive results of the reconstructed point clouds obtained by\nthe proposed MVSTR and other state-of-the-art methods on\nboth the intermediate and the advanced groups. For a fair\ncomparison, the methods that are only trained on the DTU\ntraining dataset are listed in the table. It can be seen from\nthe table that the proposed MVSTR achieves state-of-the-art\nperformance in the intermediate group. For the advanced\ngroup, which is more difﬁcult due to complex geomet-\nric layouts and camera trajectories, the proposed MVSTR\nobtains the highest mean F-score among all the methods,\nwhich demonstrates the strong generalization of the pro-\nposed MVSTR.\nThe error maps of the point clouds reconstructed by dif-\nferent methods are visualized in Figure 6. In contrast to\nthese state-of-the-art methods, the proposed MVSTR recon-\nstructs more accurate and complete point clouds (see the\nred box in Figure 6 (d)). This mainly beneﬁts from the fea-\ntures with global context and 3D consistency extracted by\nthe proposed MVSTR, which are more suitable to achieve\nreliable matching on such a difﬁcult benchmark dataset.\n4.4. Ablation Study\nTransformer Modules.To evaluate the contribution of\nthe proposed global-context Transformer module and 3D-\ngeometry Transformer module, ablation studies are con-\nducted. Experimental results are shown in Table 3, where\n“w/o GCT” refers to the variant removing the global-\ncontext Transformer module from MVSTR, and “w/o 3GT”\nrefers to the variant removing the 3D-geometry Transformer\nmodule from MVSTR. It can be seen from the table that\nboth the modules contribute to the improvement of the ac-\ncuracy and completeness of the reconstructed point clouds.\nBeneﬁting from combining the two modules, the proposed\nMVSTR achieves state-of-the-art performance on the DTU\nevaluation set.\nThe Number of Transformers. To investigate the\nimpact of the number Z of Transformers, Transformers\n(i.e., the global-context Transformer module and the 3D-\ngeometry Transformer module) are repeated different times.\nExperimental results on the DTU evaluation set are shown\nin Table 4. It can be seen from the table that the overall per-\nformance improves with the increase of the number until it\nreaches 4. However, when the number increases to 6, the\nperformance declines probably because of a larger number\nof parameters which are more difﬁcult for training process.\n4.5. Complexity Analysis\nMVSNet [36], CasMVSNet [12] and PatchmatchNet\n[30] are compared with the proposed MVSTR in terms of\nmemory and run-time performance. For a fair comparison,\na ﬁxed input size of 1152 × 864 is used to evaluate the\ncomputational cost on a single GPU of NVIDIA GeForce\nGTX 1080Ti. As shown in Table 5, the proposed MVSTR\nachieves 63.5% memory savings and 43.0% run-time reduc-\ntion compared with MVSNet [36]. Though there is no com-\nputational savings compared to PatchmatchNet [30], the\nproposed MVSTR achieves obvious improvement on the\nquality of reconstructed point clouds with acceptable extra\nmemory and run-time.\n5. Conclusion\nIn this paper, a new MVS network built upon Trans-\nformer, termed MVSTR, is proposed. Compared with the\nexisting CNN-based MVS networks, the proposed MVSTR\nenables features to be extracted under the guidance of global\ncontext and 3D geometry via the proposed global-context\nTransformer and 3D-geometry Transformer modules, re-\nspectively. Beniﬁcial from the two modules, dense fea-\ntures acquired by the proposed MVSTR are more suitable\nfor reliable matching, especially in texture-less regions and\nnon-Lambertian surfaces. Extensive experimental results\ndemonstrate that the proposed MVSTR outperforms other\nCNN-based state-of-the-art methods. For real-time appli-\ncations, the proposed MVSTR still needs to be further im-\nproved on efﬁciency due to the limitation of time complex-\nity.\n8\nReferences\n[1] Michael Bleyer, Christoph Rhemann, and Carsten Rother.\nPatchMatch Stereo-stereo matching with slanted support\nwindows. In British Machine Vision Conference (BMVC) ,\nvolume 11, pages 1–11, 2011. 2\n[2] Samuel Rota Bulo, Lorenzo Porzi, and Peter Kontschieder.\nIn-place activated batchnorm for memory-optimized training\nof DNNs. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5639–5647, 2018. 5\n[3] Neill DF Campbell, George V ogiatzis, Carlos Hern ´andez,\nand Roberto Cipolla. Using multiple hypotheses to improve\ndepth-maps for multi-view stereo. In European Conference\non Computer Vision (ECCV), pages 766–779, 2008. 1, 2, 5\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision (ECCV), pages 213–229, 2020. 2\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 12299–12310, 2021. 2\n[6] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based\nmulti-view stereo network. In International Conference on\nComputer Vision (ICCV), pages 1538–1547, 2019. 1, 5, 7\n[7] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran\nLi, Ravi Ramamoorthi, and Hao Su. Deep stereo using adap-\ntive thin volume representation with uncertainty awareness.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 2524–2534, 2020. 1, 2, 5, 6, 7\n[8] Kyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using RNN\nencoder-decoder for statistical machine translation. In Con-\nference on Empirical Methods in Natural Language Process-\ning (EMNLP), 2014. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In International Conference on Learning Representa-\ntions (ICLR), 2021. 2\n[10] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and\nrobust multiview stereopsis. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 32(8):1362–1376, 2009.\n1, 2, 5\n[11] Silvano Galliani, Katrin Lasinger, and Konrad Schindler.\nMassively parallel multiview stereopsis by surface normal\ndiffusion. In International Conference on Computer Vision\n(ICCV), pages 873–881, 2015. 1, 2, 5\n[12] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong\nTan, and Ping Tan. Cascade cost volume for high-resolution\nmulti-view stereo and stereo matching. In Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n2495–2504, 2020. 1, 2, 3, 5, 6, 7, 8\n[13] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural Computation, 9(8):1735–1780, 1997. 2\n[14] Rasmus Jensen, Anders Dahl, George V ogiatzis, Engin Tola,\nand Henrik Aanæs. Large scale multi-view stereopsis evalu-\nation. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 406–413, 2014. 1, 2, 5, 7\n[15] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu\nFang. SurfaceNet: An end-to-end 3D neural network for\nmultiview stereopsis. In International Conference on Com-\nputer Vision (ICCV), pages 2307–2315, 2017. 5\n[16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFranc ¸ois Fleuret. Transformers are RNNs: Fast autoregres-\nsive transformers with linear attention. InInternational Con-\nference on Machine Learning (ICML) , pages 5156–5165,\n2020. 2, 5\n[17] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter\nHenry, Ryan Kennedy, Abraham Bachrach, and Adam Bry.\nEnd-to-end learning of geometry and context for deep stereo\nregression. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 66–75, 2017. 5\n[18] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efﬁcient transformer. In International Confer-\nence on Learning Representations (ICLR), 2020. 2\n[19] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and Temples: Benchmarking large-scale\nscene reconstruction. ACM Transactions on Graphics ,\n36(4):1–13, 2017. 1, 2, 5\n[20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set Transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks. In International Conference on Machine Learning\n(ICML), pages 3744–3753, 2019. 2\n[21] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding,\nFrancis X Creighton, Russell H Taylor, and Mathias Un-\nberath. Revisiting stereo depth estimation from a sequence-\nto-sequence perspective with transformers. In International\nConference on Computer Vision (ICCV), pages 6197–6206,\n2021. 2\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin Transformer:\nHierarchical vision transformer using shifted windows. In\nInternational Conference on Computer Vision (ICCV), pages\n10012–10022, 2021. 2\n[23] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei\nLuo. P-MVSNet: Learning patch-wise matching conﬁdence\naggregation for multi-view stereo. In International Confer-\nence on Computer Vision (ICCV), pages 10452–10461, 2019.\n1, 5, 7\n[24] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei\nChen, and Fan Yu. EPP-MVSNet: Epipolar-assembling\nbased depth prediction for multi-view stereo. In Interna-\ntional Conference on Computer Vision (ICCV), pages 5732–\n5740, 2021. 1, 2, 5, 7\n[25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical Image Com-\nputing and Computer Assisted Intervention (MICCAI), pages\n234–241, 2015. 5\n9\n[26] Johannes L Sch ¨onberger, Enliang Zheng, Jan-Michael\nFrahm, and Marc Pollefeys. Pixelwise view selection for\nunstructured multi-view stereo. In European Conference on\nComputer Vision (ECCV), pages 501–518, 2016. 1, 2, 5\n[27] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. LoFTR: Detector-free local feature matching\nwith transformers. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 8922–8931, 2021. 2\n[28] Engin Tola, Christoph Strecha, and Pascal Fua. Efﬁcient\nlarge-scale multi-view stereo for ultra high-resolution im-\nage sets. Machine Vision and Applications, 23(5):903–920,\n2012. 1, 2, 5\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neu-\nral Information Processing Systems (NeurIPS), pages 5998–\n6008, 2017. 2\n[30] Fangjinhua Wang, Silvano Galliani, Christoph V ogel, Pablo\nSpeciale, and Marc Pollefeys. PatchmatchNet: Learned\nmulti-view patchmatch stereo. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 14194–\n14203, 2021. 1, 2, 5, 6, 7, 8\n[31] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and\nGuoping Wang. AA-RMVSNet: Adaptive aggregation re-\ncurrent multi-view stereo network. In International Confer-\nence on Computer Vision (ICCV) , pages 6187–6196, 2021.\n1, 2, 5, 7\n[32] Qingshan Xu and Wenbing Tao. Multi-scale geometric con-\nsistency guided multi-view stereo. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 5483–\n5492, 2019. 1, 2\n[33] Qingshan Xu and Wenbing Tao. Learning inverse depth re-\ngression for multi-view stereo with correlation cost volume.\nIn AAAI Conference on Artiﬁcial Intelligence (AAAI) , vol-\nume 34, pages 12508–12515, 2020. 1, 5, 7\n[34] Youze Xue, Jiansheng Chen, Weitao Wan, Yiqing Huang,\nCheng Yu, Tianpeng Li, and Jiayu Bao. MVSCRF: Learn-\ning multi-view stereo with conditional random ﬁelds. In In-\nternational Conference on Computer Vision (ICCV) , pages\n4312–4321, 2019. 1\n[35] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu.\nCost volume pyramid based depth inference for multi-view\nstereo. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4877–4886, 2020. 1, 2, 5, 7\n[36] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\nQuan. MVSNet: Depth inference for unstructured multi-\nview stereo. In European Conference on Computer Vision\n(ECCV), pages 767–783, 2018. 1, 2, 5, 6, 7, 8\n[37] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang,\nand Long Quan. Recurrent MVSNet for high-resolution\nmulti-view stereo depth inference. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 5525–\n5534, 2019. 1, 2, 5, 6, 7\n[38] Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang,\nYisong Chen, Guoping Wang, and Yu-Wing Tai. Pyramid\nmulti-view stereo net with self-adaptive view aggregation.\nIn European Conference on Computer Vision (ECCV), pages\n766–782, 2020. 1, 5, 7\n[39] Zehao Yu and Shenghua Gao. Fast-MVSNet: Sparse-to-\ndense multi-view stereo with learned propagation and Gauss-\nNewton reﬁnement. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1949–1958, 2020. 1, 2,\n5, 7\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7669579982757568
    },
    {
      "name": "Computer science",
      "score": 0.7505240440368652
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49380168318748474
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.45717814564704895
    },
    {
      "name": "Engineering",
      "score": 0.13280993700027466
    },
    {
      "name": "Electrical engineering",
      "score": 0.09022566676139832
    },
    {
      "name": "Cartography",
      "score": 0.0764453113079071
    },
    {
      "name": "Geography",
      "score": 0.07108765840530396
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 21
}