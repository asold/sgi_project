{
  "title": "Explainable Vision Transformers for Vein Biometric Recognition",
  "url": "https://openalex.org/W4395447364",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5095887512",
      "name": "Rocco Albano",
      "affiliations": [
        "Roma Tre University"
      ]
    },
    {
      "id": "https://openalex.org/A5055179380",
      "name": "Lorenzo Giusti",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A5035586529",
      "name": "Emanuele Maiorana",
      "affiliations": [
        "Roma Tre University"
      ]
    },
    {
      "id": "https://openalex.org/A5044915500",
      "name": "Patrizio Campisi",
      "affiliations": [
        "Roma Tre University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2140959843",
    "https://openalex.org/W2103601368",
    "https://openalex.org/W1523682477",
    "https://openalex.org/W1984959895",
    "https://openalex.org/W2136696710",
    "https://openalex.org/W1991620112",
    "https://openalex.org/W2084489067",
    "https://openalex.org/W6716978396",
    "https://openalex.org/W2990985175",
    "https://openalex.org/W2945197573",
    "https://openalex.org/W3215676125",
    "https://openalex.org/W4384699184",
    "https://openalex.org/W2528491735",
    "https://openalex.org/W6734862562",
    "https://openalex.org/W4292875392",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4226275002",
    "https://openalex.org/W4226108798",
    "https://openalex.org/W4323065905",
    "https://openalex.org/W2785451509",
    "https://openalex.org/W2884114979",
    "https://openalex.org/W2888322626",
    "https://openalex.org/W2803380720",
    "https://openalex.org/W4312097426",
    "https://openalex.org/W2009856449",
    "https://openalex.org/W2037829978",
    "https://openalex.org/W2810660001",
    "https://openalex.org/W3010583178",
    "https://openalex.org/W3173915460",
    "https://openalex.org/W3021558250",
    "https://openalex.org/W3178510125",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4388108499",
    "https://openalex.org/W4324381667",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W6842483350",
    "https://openalex.org/W2602244503",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3212194243",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W4298005495",
    "https://openalex.org/W4382934573",
    "https://openalex.org/W4312933694",
    "https://openalex.org/W4385127856",
    "https://openalex.org/W4360993699",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W4385732922",
    "https://openalex.org/W4223635542",
    "https://openalex.org/W4378746207",
    "https://openalex.org/W4285018316",
    "https://openalex.org/W6847249830",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1976468890",
    "https://openalex.org/W1994616650",
    "https://openalex.org/W2798405286",
    "https://openalex.org/W4226325015",
    "https://openalex.org/W4292958758",
    "https://openalex.org/W4307473288",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2417903943",
    "https://openalex.org/W2951025380"
  ],
  "abstract": "In the field of deep learning, understanding the rationale behind an automatic system’s decisions is essential for building users’ trust and ensuring accountability. In this regard, explainable artificial intelligence (XAI) recently emerged as a valuable tool to offer insights into a model behavior. The present study focuses on vein-based biometric recognition, investigating techniques allowing to identify which regions of a wrist-vein image are mostly exploited to carry out a verification process. Toward this aim, our research exploits vision transformers (ViTs), which rely on self-attention mechanisms to automatically detect and exploit the input parts with the content deemed most relevant for its further processing. Two distinct wrist-vein pattern datasets, namely PUT-wrist and FYO-wrist, are employed to fine-tune the considered models. Their behavior is interpreted by analyzing the attention maps generated when applying the trained networks to vein-pattern images, investigating which regions are exploited to decide a user’s identity. The proposed approach testifies that the performed recognition process can improve when a ViT focuses on areas with significant vein pattern content, achieving verification performance surpassing state-of-the-art methods in open-set scenarios, while promoting transparency through explainability.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2022.DOI\nExplainable Vision Transformers for Vein\nBiometric Recognition\nROCCO ALBANO1, LORENZO GIUSTI2 EMANUELE MAIORANA1 AND PATRIZIO CAMPISI1\n1Department of Industrial, Electronic, and Mechanical Engineering, Roma Tre University Via Vito V olterra 62, 00146 Rome, Italy\n(e-mail: rocco.albano@uniroma3.it)\n2Department of Computer, Control and Management Engineering, Sapienza University of Roma Via Ariosto 25, 00185 Rome, Italy\n(e-mail: lorenzo.giusti@uniroma1.it)\nCorresponding author: Rocco Albano (e-mail: rocco.albano@uniroma3.it).\nABSTRACT In the field of deep learning, understanding the rationale behind an automatic system’s\ndecisions is essential for building users’ trust and ensuring accountability. In this regard, explainable\nartificial intelligence (XAI) recently emerged as a valuable tool to offer insights into a model behavior. The\npresent study focuses on vein-based biometric recognition, investigating techniques allowing to identify\nwhich regions of a wrist-vein image are mostly exploited to carry out a verification process. Toward\nthis aim, our research exploits vision transformers (ViTs), which rely on self-attention mechanisms to\nautomatically detect and exploit the input parts with the content deemed most relevant for its further\nprocessing. Two distinct wrist-vein pattern datasets, namely PUT-wrist and FYO-wrist, are employed to fine-\ntune the considered models. Their behavior is interpreted by analyzing the attention maps generated when\napplying the trained networks to vein-pattern images, investigating which regions are exploited to decide a\nuser’s identity. The proposed approach testifies that the performed recognition process can improve when\na ViT focuses on areas with significant vein pattern content, achieving verification performance surpassing\nstate-of-the-art methods in open-set scenarios, while promoting transparency through explainability.\nINDEX TERMS Biometric Recognition, Vein Biometrics, Wrist Vein Biometrics, Explainable AI, Vision\nTransformers.\nI. INTRODUCTION\nBiometric recognition systems leverage individuals’ unique\nphysiological or behavioral attributes to perform people iden-\ntification or verification, revolutionizing several applications\nwith security-related requirements [1]. One notable sub-\ndomain of this research field regards hand vein patterns that\ncan be captured by exposing palms, wrists, or fingers to\nimaging systems relying on infrared radiation and allow the\nrecognition of individuals based on the unique characteristics\nof the acquired subcutaneous traits [2], [3]. Renowned for\nits robustness against spoofing attacks [4], vein recognition\nstands out as a highly secure biometric modality [5]. Tra-\nditionally, vein recognition has been dominated by feature\nengineering and conventional algorithms like local binary\npattern (LBP) and Gabor filters [6]–[9]. The advent of deep\nlearning pushed the performance of vein-based biometric\nrecognition systems, making convolutional neural networks\n(CNNs) the standard choice for extracting hierarchical fea-\ntures from raw vein images [10]–[12]. However, the black-\nbox nature of these models still represents a downside affect-\ning their reliability. Nowadays, there is a growing demand\nfor transparent systems that may empower users to com-\nprehend why specific decisions are made, thereby fostering\nconfidence in their fairness and unbiasedness [13]–[15].\nWithin the context of vein pattern recognition, the need\nto gain insights into the aspects considered by deep learning\napproaches when producing their decisions is highly rele-\nvant. Traditional approaches commonly relied on segmen-\ntation processes to generate vessel skeletons that were then\nemployed for recognition, thus offering a solid basis for\nwhich characteristics were used for recognition. Given the\nlack of investigations on their interpretability, an analogous\nawareness still needs to be made available for deep learn-\ning approaches. Here, we analyze these aspects, prioritizing\nexplainability as a path towards interpretability and trying\nto shed some light on the features that mainly contribute\nto producing a decision in vein-based recognition systems\nrelying on deep learning approaches.\nIn more detail, we here resort to vision transformers\n(ViTs), a key paradigm that recently emerged within the land-\nscape of deep learning architectures [16], to reach the desired\ngoal. Differently from CNNs, ViTs perform vision tasks by\nVOLUME 1, 2022 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Explainable Vision Transformers for Vein Biometric Recognition\ndividing images into sequences of non-overlapping patches,\nwhich are then fed into transformer blocks [17]. In addition\nto being often able to learn more discriminative features\ncompared to alternatives such as CNNs, ViTs represent an\napproach allowingin-model explainability, that is, integrating\nexplainability mechanisms within the learning process by\nresorting to attention-based processes [18]. Such an approach\ndiffers from post-model explainability, which involves an-\nalyzing the model after it has been trained, with in-model\nparadigms providing end-to-end transparency, allowing one\nto understand not only the final decisions but also the process\nthat led to those decisions, and typically providing greater\ngeneralization and robustness, thus enhancing trust in the\nmodel [19].\nWhile early research investigated the potential of ViTs\nfor vein-based biometric recognition [20], a comprehensive\nstudy elucidating robust explainability mechanisms tailored\nfor ViTs and applied to this task is still missing in the liter-\nature. The present paper represents a step in this direction,\nanalyzing the attention maps generated by ViTs applied to\nvein images when performing user verification. In detail, the\nstate of the art on the specific modality here considered,\ni.e., wrist vein patterns and the use of ViTs for biometric\nrecognition and explainability, is presented in Section II. The\nViT-based framework used for wrist-vein biometric recog-\nnition is described in Section III. The setup employed in\nthe performed experimental tests is outlined in Section IV,\nwhile the obtained results are presented in Section V. Some\nconclusions are eventually drawn in Section VI.\nII. RELATED WORKS\nAs already mentioned, vein-based biometric recognition was\nhistorically approached by resorting to feature engineering\nand conventional machine learning algorithms, before CNNs\nkicked in and significantly outperformed traditional pro-\ncessing techniques [21]–[24]. With specific regard to wrist-\nvein biometric recognition [25], early attempts focused on\nextracting the vessel patterns in the treated images to derive\nrepresentations based on handcrafted characteristics fed to\nclassic machine learning algorithms. For instance, support\nvector machines (SVMs) with LBP inputs were used in [26]\nto train a classifier, then used as feature extractors in a closed-\nset verification system, achieving an equal error rate (EER)\nat 1.3% when comparing samples from different acquisition\nsessions of the PUT wrist-vein database [27]. Such verifica-\ntion performance was obtained by performing tests on the\nsame subjects employed to train the SVM classifier, with\nlimited generalizability capacity. On the other side, open-\nset verification conditions were considered on [28], where\nthe comparison between vein patterns was performed using\ncorrelations, as well as in [29], where scale-invariant feature\ntransform (SIFT) characteristics were extracted from vein\npatterns and compared to estimate the similarity between\nsamples. However, the verification rates achieved in open-set\nscenarios, where recognition performance is computed over\nsubjects other than those used for training the employed so-\nlution or no training is required, are typically worse but more\ngeneralizable than those achievable in closed-set scenarios.\nIn fact, EERs at 9.3% and 15.9% were respectively obtained\nin [28] and [29] on the PUT database. Handcrafted features\nwere also employed to perform wrist-vein identification in\n[30], where rank-1 accuracies at 84.0% and 93.1% were\nrespectively achieved on the PUT and FYO [31] datasets.\nDeep learning approaches were instead applied to wrist-\nvein images in [32], where an EER=2.1% on PUT was\nobtained in closed-set verification when training a ResNet152\nnetwork [33], then employed to extract features used as\ninput to a further logistic regression classifier. Closed-set\nconditions were also considered in [34], where a lightweight\nnetwork was designed to extract discriminative wrist-vein\nfeatures, achieving EERs at 1.2% on PUT and 1.84% on\nFYO when testing on the same subjects employed to train\nthe used model. Open-set verification conditions have been\ninstead considered in [35], where a siamese approach was\nemployed to train a CNN, achieving an F1 score at 84.7%.\nAlso, ViTs were applied to wrist-vein images, as in [20]\nwhere an accuracy at 99.5% on PUT was obtained in iden-\ntification, yet not investigating any explainability aspect nor\nthe generalizability of ViT on verification tasks.\nIn general, while deep learning approaches notably out-\nperformed classic machine learning algorithms for wrist-vein\nbiometrics in identification scenarios they are still not as\nefficient for verification, where networks should be employed\nas feature extractors. Furthermore, deep learning reliability\nfor wrist-vein verification in open-set scenarios must be prop-\nerly explored. Given that these latter conditions are closer to\nreal-life verification, in which a solution is designed to be\noptimized over a certain dataset yet then applied to different\nsubjects, we here resort to open-set experimental scenarios\nto conduct our analysis in the hope of deriving more general\noutcomes than what closed-set conditions could allow.\nIn more detail, within the context of explainable artificial\nintelligence (XAI), our intent here is to shed some light\non the decision-making processes of deep-learning-based\nwrist-vein biometric verification, thus trying to demystify\nthe operations of approaches relying on neural networks\nand understand their rationale [36]. Unfortunately, XAI\napplications in biometric recognition are still at an infant\nstage [37], with most of the studies so far presented mainly\nfocused on presentation attack detection (PAD) for facial bio-\nmetrics [38]. Among the investigations that tried to look into\nthe aspects exploited by neural networks applied to biometric\nrecognition during their decision-making process, the vast\nmajority relied on post-model methods. These approaches,\nessentially visual toolkits, elucidate predictions of an already\ntrained model [39]. A notable technique in this category\nis the gradient-weighted class activation mapping (Grad-\nCAM) approach, prominently featured in studies analyzing\nnetwork decisions [40]. The only work devoted explicitly to\nthe explainability of vein recognition [41], to the best of our\nknowledge, actually adopted such post-model paradigm, em-\nploying the local interpretable model-agnostic explanations\n2 VOLUME 1, 2022\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Gender-Specific Explainable Vision Transformers for Vein Biometric Recognition\n(LIME) method [42] to visually dissect a custom CNN areas\nof focus on palm vein images. However, the obtained visual\nresults are coarse and hardly highlight areas resembling the\nvein patterns in the images.\nTo achieve better explainability, it is preferable to consider\nmethods that jointly provide predictions and explanations. In\nthis regard, ViTs achieved remarkable results for vision tasks.\nSpecifically, ViTs segment images into fixed-size patches\nand process them through multiple layers of self-attention\nmechanisms, capturing long-range dependencies [16], [17].\nGiven their ability to capture complex patterns, ViTs have\nalready been exploited for biometric recognition, especially\nfor gait [43], [44]. Regarding vein biometrics, in addition to\n[20] where ViTs were applied to wrist-vein images, in [45]\nand [46] the authors have applied ViTs to finger-vein traits,\nwhile a multi-scale transformer was applied on palm-vein\ndata in [47]. Yet, all the studies mentioned above only used\nViTs to extract discriminative features from the considered\nimages, with no reference to the associated explainability\naspects and without investigating the derived attention maps.\nAlthough there is an ongoing debate about the effec-\ntiveness of generic attention mechanisms as explainability\ntools [48], [49], there is also a consensus on the efficacy of\nViT attention maps in providing relevant insights on models\nexplainability for visual tasks [50], [51]. This kind of analysis\nproved effective in other fields such as bio-medicine [52]. For\ninstance, ViT attention maps provided relevant insights for\nexplainable COVID-19 screening in [18]. A comprehensive\nstudy in [53] also delves into explainable transfer learning\nfor ViTs applied to chest X-rays. Similar works within the\nbiometric recognition field are still rare, with a notable ex-\nample presented in [54], where the authors employed ViTs to\nguide fingerprint embedding using minutiae matching, and\nemphasized the decision process of ViTs through salience\nmaps. The present study aims at providing an analogous con-\ntribution considering wrist-vein biometrics, trying to achieve\nhigh recognition performance and provide an insightful and\ntransparent understanding of the underlying biometric fea-\ntures that drive accurate verification.\nIII. EXPLAINABLE VEIN BIOMETRIC RECOGNITION\nInitially crafted for natural language processing tasks,\nViTs [16] recently emerged as a groundbreaking approach in\ncomputer vision, outperforming traditional CNNs in a variety\nof vision tasks, especially when dealing with large-scale\ndata [55]. The core concept of ViTs can be mathematically\ndistilled into the following components.\nTokenization: Analogous to the tokenization of words in\ntext, ViTs dissect images into fixed-size and non-overlapping\npatches. An image I of dimensions H × W × C is divided\ninto patches of size P × P × C, resulting in N = H·W\nP·P\npatches. Each patch is subsequently flattened and linearly\nembedded into a vector uij ∈ Rd. Formally, if vij ∈ RP2·C\ndenotes the vector corresponding to the patch at row i and\ncolumn j of the original image, we have uij = vij We,\nwhere We ∈ R(P2·C)×d is a learnable embedding matrix.\nFurthermore, a special <cls> token is appended to the begin-\nning of this sequence, serving as an aggregate representation\nfor downstream tasks, especially classification.\nPositional Embedding:By design, the transformer archi-\ntecture does not provide any order or position that captures\nthe original context provided by the spatial arrangement of\nthe patches. To overcome this limitation, a learnable posi-\ntional embedding vector pij is added to its corresponding\ntoken, with xij = uij + pij being the final patch embedding\ninfused with positional information.\nTransformer Layers: To capture both local and global\ninformation from an image I, a ViT receives as input the\nsequence X ∈ R(1+N2)×d composed of a vector ucls that\ncontains the embedding of the <cls> token, followed by the\nsequence of the patch embeddingsxij. Each layer consists of\na self-attention mechanism [56] followed by a feed-forward\nnetwork. For a single head of attention, the attention weights\nα and output oij are computed as:\nQ = X WQ\nK = X WK\nV = X WV\nα = softmax\n\u0012QKT\n√\nd\n\u0013\nO = αV,\nwhere WQ, WK and WV are learnable weight matrices.\nThe attention maps indicate which areas of the original\nimage are highlighted during the process and mostly used\nfor the desired task, that is, classification during the training\nphase. After L ViT layers, the output corresponding to the\n<cls> token, denoted as ocls, is processed by a linear layer\nfor classification as ˆy = softmax(oclsWcls + bcls). Cross-\nentropy is typically employed as a loss function over the\ncomputed probabilities to drive network training using the\nback-propagation algorithm.\nAttention Maps:As previously illustrated, attention maps\nprovide information about which regions of the input image\nare of particular interest to the model. These maps serve as\na compass, guiding our understanding of where the model\nfocuses when making a prediction. To construct such maps\nin the context of ViTs, one typically examines the attention\nweights, specifically from the last layer of the transformer.\nThe rationale behind this choice is that the latter layers\ncapture higher-level, more abstract features that directly in-\nfluence the model’s final decision. To provide an output\nthat is robust to the model fluctuations, we consider a ViT\nequipped with multiple attention heads. Each head assigns\ndistinct attention weights to different input regions during\nits operation. To derive the final attention maps, we average\nthe attention weights across all heads. Mathematically, for\na multi-head attention mechanism with h heads, the final\nattention map A can be thus formulated as:\nA = 1\nh\nhX\ni=1\nA(L)\ni (1)\nVOLUME 1, 2022 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Explainable Vision Transformers for Vein Biometric Recognition\nFIGURE 1: Illustrative representation of the Vision Transformer (ViT) [16] tailored for explainable vein biometric recognition.\nwhere A(L)\ni is the attention weight matrix for the i-th head\nat the last transformer layer L. Once obtained, this averaged\nattention map is upsampled and overlaid on the original\nimage to visualize the regions that the ViT paid attention\nto during the biometric recognition task. Such visualizations\nnot only serve as an explanatory tool, demystifying the ViT\nbehavior, but also play a pivotal role in diagnosing potential\nbiases or shortcomings in the model focus and, by exten-\nsion, its decision-making process. A visual depiction of the\nprocessing applied to the considered vein pattern images is\nshown in Figure 1,. In contrast, the details of the processing\nperformed to derive the desired attention maps are given in\nAlgorithm 1.\nIt is worth noticing that the ViT attention weight matrices,\nand therefore the attention maps, are computed before deriv-\ning the final inner representation of the input or performing\nclassification, differently from class-dependent post-model\nexplainability tools such as Grad-CAM. Therefore, attention\nmaps may provide insights on the most relevant parts of an\nimage even when a ViT is used only as a feature extractor, as\nit happens in our case when verification, and not identifica-\ntion, is carried out in the experimental tests.\nIV. EXPERIMENTAL SETUP\nThe datasets used in the performed tests are presented in Sec-\ntion IV-A, while the employed training and testing strategies\nare respectively outlined in Sections IV-B and IV-C.\nA. DATASET\nWe applied ViTs to the wrist-vein patterns in two public\ndatasets, namely PUT [27] and FYO [31].\nThe PUT wrist-vein database comprises 1200 images,\nwith acquisitions taken from the right and left hands of 50\nsubjects. A total of 4 images were captured for each wrist\nduring 3 acquisition sessions separated in time. Given the low\nCharacteristic FYO PUT-wrist\nSubjects 160 50\nTraits 2 2\nSamples per Session 1 4\nSessions 2 3\nTotal Images 640 1200\nTrain, Validation, and Test Split\nTrain Class 260 75\nTrain Images/Class 25 120\nValidation Class 260 75\nValidation Images/Class 5 12\nTest Class 60 25\nTest Images/Class 2 12\nTABLE 1: Employed wrist-vein dataset characteristics.\ncorrelation between each subject’s right and left wrist-vein\npatterns, each wrist is considered a class for a total of 100 dis-\ntinct classes. The samples within this dataset experience low\ncontrast between the vein traits and the background, being,\ntherefore, particularly hard to be processed effectively. To\nenhance the visibility of the traits, contrast limited adaptive\nhistogram equalization (CLAHE) [57] is typically applied\nto the images of this dataset. CLAHE is a widely used\nimage enhancement method that adjusts the contrast of an\nimage by redistributing pixel intensities, characterized by two\nparameters, i.e., clip limit and grid size, here, respectively set\nat 5.0 and (8, 8). The results of this preprocessing to images\nfrom the PUT database can be seen in Figure 2.\nThe FYO wrist-vein database contains images taken from\nboth hands of 160 subjects, collected using a medical vein\nfinder in a controlled environment, with an image captured\nduring each of two separate acquisition sessions for each\nparticipant. As for PUT, the vein patterns from the two wrists\nof each subject are considered as two separate classes, for a\ntotal of 320 classes. Given that images in FYO have better\n4 VOLUME 1, 2022\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Gender-Specific Explainable Vision Transformers for Vein Biometric Recognition\nAlgorithm 1Get Attention Maps\nInput: Image I of dimensions H × W × C\nOutput: Predicted class ˆy, Attention Map A(I)\nRequire: Patch size P\nprocedure TOKENIZATION (I : Image)\nP = Divide I into a gird of N patches. ▷ N= H×W\nP×P\nvi = vec(Pi,j) ∀i, j ▷ Seq. of RP2·C vectors\nui = vi · We. ▷ We ∈ R(P2·C) × d\nreturn u. ▷ Patch Embeddings ∈ RN×d\nend procedure\nprocedure POS. EMBEDDING (u : patch embeddings)\nxi = ui + pi ∀i\nX(0) = [ucls, x1, . . . ,xN]\nreturn X(0)\nend procedure\nprocedure SELF ATTENTION (X(l))\nQ(l) = X(l) · W(l)\nQ\nK(l) = X(l) · W(l)\nK\nV(l) = X(l) · W(l)\nV\nA(l) = softmax\n\u0012\nQ(l)·(K(l))\n⊤\n√\nd\n\u0013\n. ▷ Attention Maps\nO(l) = A(l) · V(l)\nreturn O(l), A(l)\nend procedure\nu = Tokenization(I)\nX(0) = embedding(u)\nfor l = 1to L do\nˆX\n(l)\n= LN(X(l)) ▷ Layer Norm.\nO(l), A(l) = MHA(ˆX\n(l)\n) ▷ Multi-head Attention.\n˜X\n(l)\n= X(l) + O(l)\nX(l+1) = ˜X\n(l)\n+ FF(LN(˜X\n(l)\n)) ▷ 2-Layer MLP.\nend for\nA = 1/h P\nh A(L)\nh .\nreturn A\nquality than those in PUT, applying CLAHE to the images in\nthis dataset is typically not required. A summary of the main\ncharacteristics of the employed datasets is given in Table 2.\nGiven the limited amount of data in both databases,\ndata augmentation was employed to increase the number\nof samples available to train the employed ViT models. In\nmore detail, we leveraged a suite of basic image processing\noperations to generate augmented instances for each class,\nthus enhancing the diversity of the training samples. The\naugmentations were applied as follows: (1) Horizontal Flip\nwith a probability p = 0.5; (2) Rotation with a probability\np = 0.7 bounded by a maximum left rotation of −10◦ and\na maximum right rotation of +10◦; (3) Random Contrast\nadjustments, initiated with a probability p = 0.5, with a\nfactor range between 0.7 and 1.3. (4) Random Illumination\nvariations, activated with a probability p = 0.5, regulated\nby a factor ranging from 0.7 to 1.3. The PUT dataset was\naugmented from 12 to 132 images per class, while the FYO\nFIGURE 2: Original wrist-vein patterns (left) vs their\nCLAHE-enhanced representations (right) from the PUT-\nWrist vein dataset.\ndataset from 2 to 30 images per class.\nB. ARCHITECTURE AND TRANSFER LEARNING\nTwo different ViT configurations were used in the performed\ntests, the first exploiting patch sizes ofP ×P =16x16 and the\nsecond with patch sizes of P × P =32x32. This choice was\nmade to evaluate whether the employed patch size affects the\nmodel recognition performance in this task.\nThe imported architectures were pretrained on the\nImageNet-1K dataset, which contains 1 million images la-\nbeled in 1,000 categories, thus providing a solid knowledge\nbase for our model. The used ViTs are characterized by h =\n12 heads, feeding inputs to a multi-layer perceptron (MLP),\nproducing representations with 768 coefficients to model\nmore complex relationships among the extracted features\nbefore performing classification. For regularization purposes,\nwe replaced the last linear layer of the ViT model with a\ncombination of two linear layers, interspersed with a dropout\nlayer, to increase its generalization capability.\nTo investigate the properties of networks with different\ndiscriminative characteristics, we fine-tuned the considered\nViTs on the available wrist-vein datasets varying the number\nof training epochs, namely 20, 40, 60, and 100. As an\noptimizer, we used stochastic gradient descent (SGD) [58]\nwith momentum at 0.9, a starting learning rate of 0.05,\nand cosine decay without an initial warm-up. For all the\nexperiments, the batch size was set to 128. All tests were\nperformed on an 4 x NVIDIA ® Tesla V100 GPUs with\n5,120 CUDA cores and 32GB GPU memory, on a personal\ncomputing platform with an Intel ® Xeon® Gold 5218 CPU\n@ 2.30GHz CPU using Ubuntu 18.04.6 LTS. The model was\nimplemented in PyTorch [59] by building on top of TIMM\nlibrary [60]. PyTorch, NumPy, SciPy, and Joblib are available\nunder the BSD and Matplotlib under the PSF licenses. TIMM\nis available under the Apache 2.0 license.\nVOLUME 1, 2022 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Explainable Vision Transformers for Vein Biometric Recognition\nFIGURE 3: In the left graph, one can see the trend of EER obtained from the two models at different training epochs while on\nthe right is the evolution of AUC for the PUT dataset.\nFIGURE 4: In the left graph, one can see the trend of EER obtained from the two models at different training epochs, while on\nthe right is the evolution of AUC for the FYO dataset.\nC. TESTING\nAs mentioned in our tests, we considered a verification\nscenario in which a user asks the system to be recognized\nbased on comparing a probe sample to the data provided\nduring an enrolment phase. To assess the generalizability of\nViTs and their suitability for real-world scenarios, open-set\nconditions were considered when estimating the achievable\nrecognition performance. In more detail, we performed a\n10-fold cross-validation by excluding, at each iteration, 25\nclasses from PUT and 60 from FYO when training the\nemployed ViTs. After a ViT is fine-tuned, it is applied as\na feature extractor to the images belonging to the classes\nexcluded from training. The representations thus generated\nare then compared to compute pairwise 1-vs-1 similarity\nscores so that distributions of genuine scores are created by\ncomparing images belonging to the same class. In contrast,\nthe distributions of impostor scores are obtained by com-\nparing samples belonging to different test subjects. Setting\na threshold and comparing it against the obtained scores\nallows us to decide whether the pairwise comparison samples\nstem from the same user. Only the original images, not the\naugmented ones, are included in the test set to estimate the\nachievable performance.\nV. EXPERIMENTAL RESULTS\nAs mentioned in the previous section, tests were conducted\nby fine-tuning the considered ViT models for increasing\nnumbers of epochs, and adopting the trained networks as\nfeature extractors on a disjoint set of subjects. Figures 3 and\n4 show the results obtained in the performed 10-fold cross-\nvalidation tests, in terms of mean and standard deviation\nequal error rate (EER), with ViT models trained for different\nnumbers of epochs. Also, the trend of the area under the\ncurve (AUC) of the receiver operating characteristic (ROC),\nobtained by plotting (1-FRR) vs FAR, is reported. As for the\nPUT database, the results in Figure 3 are referred to images\nprocessed with CLAHE, which is instead not employed for\nFYO since it does not produce improvements, given that the\noriginal samples are already characterized by proper contrast\nand sharpness.\nThe ViT configuration resulting in the best values of aver-\nage EER and AUC for the PUT database relies on 16 ×16\n6 VOLUME 1, 2022\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Gender-Specific Explainable Vision Transformers for Vein Biometric Recognition\n(a)\n (b)\n (c)\n (d)\n (e)\nFIGURE 5: Attention maps of ViTs trained for different numbers of epochs, for two different images of the same wrist from\nthe PUT dataset. Warmer colors signify higher attention concentration. Column (a) shows the original images, column (b) the\nattention maps obtained after fine-tuning for 20 epochs, column (c) 40 epochs, column (d) 60 epochs, column (e) 100 epochs.\n(a)\n (b)\n (c)\n (d)\n (e)\nFIGURE 6: Attention maps of ViTs trained for different numbers of epochs for two images of the same wrist from the PUT\ndataset. Warmer colors signify higher attention concentration. Column (a) shows the original images, column (b) the attention\nmaps obtained after fine-tuning for 20 epochs, column (c) 40 epochs, column (d) 60 epochs, column (e) 100 epochs.\npatches and fine-tuned for 100 epochs. As for the FYO\ndataset, the ViT configuration leading to the lowest average\nEER and highest average AUC uses a patch size of 32 × 32\nand fine-tuned for 40 epochs. The obtained results, therefore,\ntestify that the selected patch size may significantly im-\npact the achievable recognition rate, especially for the FYO\ndataset, whose better image quality allows it to perform better\nthan PUT. In more detail, the best average EERs obtained in\nopen-set verification is 5.2% for PUT and 2.3% for FYO.\nFigures 5 and 6 show the attention maps associated with\ndistinct images of two subjects from the PUT database,\ncreated by ViTs fine-tuned for different numbers of epochs. It\nis worth remarking that such maps refer to subjects used dur-\ning performance evaluation and, therefore, are not included\nin the training dataset employed to fine-tune the ViTs. As\ncan be seen, regions with higher relevance (warmer colors)\nsignificantly overlay with areas containing wrist vessels, thus\ntestifying the effectiveness of the considered solutions and\nalso their generalizability, since they are able to localize\nrelevant vessel regions also on images from subjects other\nthan those seen during fine-tuning. Furthermore, it can also\nbe appreciated that ViTs trained for more significant numbers\nof epochs are more effective since they reduce the spurious\nregions on which attention is placed.\nThe attention maps associated with distinct images of two\nsubjects from the FYO database, created by ViTs fine-tuned\nfor different numbers of epochs, are instead shown in Fig-\nures 7 and 8. As for the images from PUT, the attention maps\nproperly focus on areas containing relevant vessel contents,\nwhich are, therefore, those mostly considered to decide on\nthe presented subjects.\nAdditional tests were performed to assess the effectiveness\nand the effects of using CLAHE on images from the PUT\ndatabase. Figure 9 reports the performance achievable on\nPUT when training ViTs on images either preprocessed or\nnot with CLAHE. The best average EER achieved when\navoiding CLAHE is 7.3%, with a notable worsening from\nthe 5.2% obtained with CLAHE. Moreover, Figure 10 offers\nVOLUME 1, 2022 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Explainable Vision Transformers for Vein Biometric Recognition\n(a)\n (b)\n (c)\n (d)\n (e)\nFIGURE 7: Attention maps of ViTs trained for different numbers of epochs, for two different images of the same wrist from\nthe FYO dataset. Warmer colors signify higher attention concentration. Column (a) shows the original images, column (b) the\nattention maps obtained after fine-tuning for 20 epochs, column (c) 40 epochs, column (d) 60 epochs, column (e) 100 epochs..\n(a)\n (b)\n (c)\n (d)\n (e)\nFIGURE 8: Attention maps of ViTs trained for different numbers of epochs, for two different images of the same wrist from\nthe FYO dataset. Warmer colors signify higher attention concentration. Column (a) shows the original images, column (b) the\nattention maps obtained after fine-tuning for 20 epochs, column (c) 40 epochs, column (d) 60 epochs, column (e) 100 epochs.\nFIGURE 9: EERs of ViT models trained on PUT wrist vein\nimages either with or without CLAHE preprocessing.\na visual comparison of the effects of CLAHE on the images\nprocessed by ViTs. The maps there reported show that, due to\nthe limited contrast in the original images, networks trained\non data not preprocessed with CLAHE might find it hard\nto pay attention to areas with relevant vessel content, with\nconsequences on the achievable recognition performance.\nFine-tuning ViTs on images with enhanced contrast may\nalso be beneficial when applying the trained network on not-\nenhanced images. Specifically, models fine-tuned on images\npreprocessed with CLAHE produce more informative atten-\ntion maps when applied to both the original and enhanced\nimages of subjects, as shown in Figure 11. Conversely,\nleveraging the original images for fine-tuning does not allow\nViT to focus appropriately on the most relevant areas of the\ninputs when enhanced images are fed to the trained network.\nResorting to attention maps for explainability also offers\ninteresting insights into the effects of CLAHE on PUT wrist-\nvein images.\nEventually, for comparison purposes, we report in Table\n2 the recognition performance achievable in open-set verifi-\ncation with the proposed approach based on ViT, and with\n8 VOLUME 1, 2022\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Gender-Specific Explainable Vision Transformers for Vein Biometric Recognition\n(a)\n (b)\n (c)\nFIGURE 10: Attention maps for images with or without\nCLAHE. Column (a) shows an image from PUT prepro-\ncessed with CLAHE, column (b) the corresponding attention\nmap created by a ViT-16 model trained on CLAHE images,\ncolumn (c) the attention map created on the original image\nby a ViT-16 model trained on original images.\n(a)\n (b)\n (c)\nFIGURE 11: Effects of CLAHE on ViTs trained on PUT.\nColumn (a) shows an image from PUT with (top) and\nwithout (bottom) CLAHE preprocessing. Column (b) shows\nthe attention maps obtained applying the trained models on\nthe original image, while column (c) shows the attention\nmaps obtained from the CLAHE-preprocessed images. For\ncolumns (b) and (c), the first row refers to ViTs trained on the\nCLAHE-preprocessed PUT dataset, the second row to ViTs\ntrained without CLAHE.\na state of the art approach relying on ResNet152 [33], as\nproposed in [32]. As for tests with ViTs, a 10-fold cross\nvalidation was performed by fine-tuning, at each iteration,\na pretrained ResNet152 on a subset of the considered vein\ndatabases, and then using the trained model as feature ex-\ntractor for the disjoint dataset upon which the achievable\nverification rates are estimated. The models are trained for\n1000 training epochs using early stopping with a patience\nof 10 epochs. As optimizer, we used SGD with a moment\nof 0.9 and a learning rate of 0.0005, with batch size at\n128. It can be seen that ViTs outperform ResNet152 in both\nthe considered datasets, testifying the effectiveness of the\nproposed solution in extracting discriminative characteristics\nfrom vein patterns. It is worth mentioning that far better\nresults were reported in [32] using ResNet158, yet closed-set\nDATASET MODEL CLAHE EER (%) AUC (%)\nPUT\nViT-16 ✓ 5.2 ± 0.6 97 .4 ± 0.3\nX 10.2 ± 1.3 94 .3 ± 0.7\nResNet152 ✓ 15.5 ± 2.1 92 .5 ± 2.1\nX 16.1 ± 3.5 90 .7 ± 3.1\nFYO ViT-32 X 2.3 ± 0.7 98 .2 ± 0.2\nResNet152 5.0 ± 1.6 98 .9 ± 0.5\nTABLE 2: Recognition performance comparison between\napproaches based on ViTs and ResNet152 on the PUT-Wrist\ndataset.\nverification conditions were there used, testing the models on\nthe same subjects exploited for training. Results quite similar\nto those in [32] are obtained with ResNet152 in our tests\nunder the same closed-set conditions, yet open-set represents\na much more challenging scenario.\nVI. CONCLUSIONS\nIn this paper, we demonstrated the effectiveness of using\nViTs for vein biometric recognition and exploited the atten-\ntion maps produced by ViTs when processing their inputs to\nargue for the explainability of the obtained results. We thus\nprovide insights into the employed models’ inner behavior\nwhen making their decisions.\nTests on the PUT and FYO wrist-vein datasets testified that\nthe considered approach based on ViT can outperform state-\nof-the-art alternatives for open-set verification, with the best\nEERs obtained on the two datasets respectively at 5.2% and\n2.3%. More interestingly, we have demonstrated for the first\ntime in literature that such models’ decisions are taken mostly\nby focusing on image regions containing significant venous\ncontents.\nFor the PUT database, we also proved the effectiveness of\npreprocessing the available wrist-vein images with CLAHE\nto enhance their quality and, consequently, the achievable\nperformance by also providing examples of the differences\nin processing original or enhanced images through ViT at-\ntention maps.\nIn conclusion, with this work, we evaluated the explain-\nability of wrist-vein biometric recognition by resorting to the\nattention maps produced when employing ViTs for image\nprocessing. The results provide interesting insights into the\ndecision-making process for vascular biometric recognition,\nthus fostering transparency and trust in the performed meth-\nods and promoting responsible AI deployment. Conducting\ntests in open-set verification scenarios further increases the\nrobustness and generalizability of the obtained outcomes.\nREFERENCES\n[1] A. K. Jain, A. Ross, and S. Prabhakar, “An introduction to biometric recog-\nnition,” IEEE Transactions on circuits and systems for video technology,\nvol. 14, no. 1, pp. 4–20, 2004.\n[2] Y . Ding, D. Zhuang, and K. Wang, “A study of hand vein recognition\nmethod,” in IEEE International Conference Mechatronics and Automation,\n2005.\n[3] A. Uhl, C. Busch, S. Marcel, and R. Veldhuis, Handbook of Vascular\nBiometrics. Springer, 2020.\nVOLUME 1, 2022 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Explainable Vision Transformers for Vein Biometric Recognition\n[4] L. Wang and G. Leedham, “Near-and far-infrared imaging for vein pattern\nbiometrics,” in IEEE International Conference on Video and Signal Based\nSurveillance, 2006.\n[5] D. Mulyono and H. S. Jinn, “A study of finger vein biometric for personal\nidentification,” in International Symposium on Biometrics and Security\nTechnologies, 2008.\n[6] N. Miura, A. Nagasaka, and T. Miyatake, “Extraction of finger-vein\npatterns using maximum curvature points in image profiles,” IEICE Trans-\nactions on Information and Systems, vol. E90-D, no. 8, p. 1185–1194,\n2007.\n[7] J. Liu and Y . Zhang, “Palm-dorsa vein recognition based on two-\ndimensional fisher linear discriminant,” in International Conference on\nImage Analysis and Signal Processing, 2011.\n[8] J. Wang, H. Li, G. Wang, M. Li, and D. Li, “Vein recognition based on (2D)\n2FPCA,” International Journal of Signal Processing, Image Processing and\nPattern Recognition, vol. 6, no. 4, pp. 323–332, 2013.\n[9] H. T. Van, C. M. Duong, G. Van Vu, and T. H. Le, “Palm vein recognition\nusing enhanced symmetry local binary pattern and sift features,” in 19th\nInternational Symposium on Communications and Information Technolo-\ngies (ISCIT), 2019.\n[10] J. M. Song, W. Kim, and K. R. Park, “Finger-vein recognition based on\ndeep DenseNet using composite image,” IEEE Access, vol. 7, pp. 66845–\n66863, 2019.\n[11] Z. K. J. Jasim, A. H. Mohammed, L. Elwiya, B. D. Al-Jabbari, and H. Al-\nhaji, “Human identification with finger vein image using deep learning,” in\n5th International Symposium on Multidisciplinary Studies and Innovative\nTechnologies (ISMSIT), 2021.\n[12] R. Hernández-García, E. H. Salazar-Jurado, R. J. Barrientos, F. M. Castro,\nJ. Ramos-Cózar, and N. Guil, “From synthetic data to real palm vein iden-\ntification: a fine-tuning approach,” in IEEE 13th International Conference\non Pattern Recognition Systems (ICPRS), 2023.\n[13] D. Castelvecchi, “Can we open the black box of AI?,” Nature News,\nvol. 538, no. 7623, p. 20, 2016.\n[14] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable\nmachine learning,” arXiv, 2017.\n[15] K. Santosh and C. Wall, “Trustworthy and explainable ai for biometrics,”\nin AI, Ethical Issues and Explainability—Applied Biometrics, pp. 29–46,\nSpringer, 2022.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Un-\nterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit,\nand N. Houlsby, “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv, 2021.\n[17] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” ACM computing surveys (CSUR),\nvol. 54, no. 10, pp. 1–41, 2022.\n[18] A. K. Mondal, A. Bhattacharjee, P. Singla, and A. Prathosh, “xvitcos:\nexplainable vision transformer based covid-19 screening using radiogra-\nphy,” IEEE Journal of Translational Engineering in Health and Medicine,\nvol. 10, pp. 1–10, 2021.\n[19] A. Holzinger, A. Saranti, C. Molnar, P. Biecek, and W. Samek, “Ex-\nplainable ai methods - a brief overview,” in International Workshop on\nExtending Explainable AI Beyond Deep Models and Classifiers, pp. 13–\n38, 2022.\n[20] R. Garcia-Martin and R. Sanchez-Reillo, “Vision transformers for vein\nbiometric recognition,” IEEE Access, vol. 11, pp. 22060–22080, 2023.\n[21] W. Liu, W. Li, L. Sun, L. Zhang, and P. Chen, “Finger vein recognition\nbased on deep learning,” in 12th IEEE conference on industrial electronics\nand applications (ICIEA), 2017.\n[22] W. Kim, J. M. Song, and K. R. Park, “Multimodal biometric recognition\nbased on convolutional neural network by the fusion of finger-vein and\nfinger shape using near-infrared (NIR) camera sensor,” Sensors, vol. 18,\nno. 7, p. 2296, 2018.\n[23] K. Shaheed, H. Liu, G. Yang, I. Qureshi, J. Gou, and Y . Yin, “A systematic\nreview of finger vein recognition techniques,” Information, vol. 9, no. 9,\np. 213, 2018.\n[24] K. Sundararajan and D. L. Woodard, “Deep learning for biometrics: A\nsurvey,” ACM Computing Surveys (CSUR), vol. 51, no. 3, pp. 1–34, 2018.\n[25] F. Marattukalam, D. Cole, P. Gulati, and W. H. Abdulla, “On wrist vein\nrecognition for human biometrics,” in Asia-Pacific Signal and Information\nProcessing Association Annual Summit and Conference (APSIPA ASC),\n2022.\n[26] A. Das, U. Pal, M. A. Ferrer Ballester, and M. Blumenstein, “A new\nwrist vein biometric system,” in IEEE Symposium on Computational\nIntelligence in Biometrics and Identity Management (CIBIM), 2014.\n[27] R. Kabacinski and K. Kowalski, “Vein pattern database and benchmark\nresults,” Electronic Letters, vol. 47, no. 20, pp. 1127–1128, 2011.\n[28] O. Nikisins, T. Eglitis, A. Anjos, and S. Marcel, “Fast cross-correlation\nbased wrist vein recognition algorithm with rotation and translation com-\npensation,” in IEEE International Workshop on Biometrics and Forensics\n(IWBF), 2018.\n[29] R. Garcia-Martin and R. Sanchez-Reillo, “Wrist vascular biometric recog-\nnition using a portable contactless system,” Sensors, vol. 20, no. 5, 2020.\n[30] F. O. Babalola, Ö. Toygar, and Y . Bitirim, “Wrist vein recognition by\nfusion of multiple handcrafted methods,” in 3rd International Congress\non Human-Computer Interaction, Optimization and Robotic Applications\n(HORA), 2021.\n[31] Önsen Toygar, F. O. Babalola, and Y . Bitirim, “Fyo: A novel multimodal\nvein database with palmar, dorsal and wrist biometrics,” IEEE Access,\nvol. 8, pp. 82461–82470, 2020.\n[32] R. Garcia-Martin and R. Sanchez-Reillo, “Deep learning for vein biomet-\nric recognition on a smartphone,” IEEE Access, vol. 9, pp. 98812–98832,\n2021.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in IEEE conference on computer vision and pattern recogni-\ntion, 2016.\n[34] T.-V . Nguyen, S.-J. Horng, D.-T. Vu, H. Chen, and T. Li, “Lawnet: A\nlightweight attention-based deep learning model for wrist vein verification\nin smartphones using rgb images,” IEEE Transactions on Instrumentation\nand Measurement, vol. 72, pp. 1–10, 2023.\n[35] F. Marattukalam, W. Abdulla, D. Cole, and P. Gulati, “Deep learning-based\nwrist vascular biometric recognition,” Sensors, vol. 23, no. 6, 2023.\n[36] A. Arrieta et al., “Explainable artificial intelligence (XAI): Concepts, tax-\nonomies, opportunities and challenges toward responsible ai,” Information\nFusion, vol. 58, 2020.\n[37] P. C. Neto, T. Gonçalves, J. R. Pinto, W. Silva, A. F. Sequeira, A. Ross,\nand J. S. Cardoso, “Explainable biometrics in the age of deep learning,”\narXiv, 2022.\n[38] R. Ramachandra and C. Busch, “Presentation attack detection methods\nfor face recognition systems: A comprehensive survey,” ACM Computing\nSurveys (CSUR), vol. 50, no. 1, pp. 1–37, 2017.\n[39] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional\nnetworks,” in 13th European Conference on Computer Vision, 2014.\n[40] R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Ba-\ntra, “Grad-cam: Visual explanations from deep networks via gradient-\nbased localization,” in IEEE International Conference on Computer Vision\n(ICCV), 2017.\n[41] Y .-Y . Chen, S.-Y . Jhong, C.-H. Hsia, and K.-L. Hua, “Explainable AI: a\nmultispectral palm-vein identification system with new augmentation fea-\ntures,” ACM Transactions on Multimedia Computing, Communications,\nand Applications (TOMM), vol. 17, no. 3, pp. 1–21, 2021.\n[42] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should i trust you?\nexplaining the predictions of any classifier,” arXiv, 2016.\n[43] J. N. Mogan, C. P. Lee, K. M. Lim, and K. S. Muthu, “Gait-vit: Gait\nrecognition with vision transformer,” Sensors, vol. 22, no. 19, p. 7362,\n2022.\n[44] P. Delgado-Santos, R. Tolosana, R. Guest, F. Deravi, and R. Vera-\nRodriguez, “Exploring transformers for behavioural biometrics: A case\nstudy in gait recognition,” Pattern Recognition, vol. 143, p. 109798, 2023.\n[45] Z. Zhao, H. Zhang, Z. Chen, and J. Yang, “Transfinger: Transformer\nbased finger tri-modal biometrics,” in Chinese Conference on Biometric\nRecognition, 2022.\n[46] X. Li and B.-B. Zhang, “Fv-vit: Vision transformer for finger vein recog-\nnition,” IEEE Access, vol. 11, pp. 75451–75461, 2023.\n[47] H. Qin, C. Gong, Y . Li, X. Gao, and M. A. El-Yacoubi, “Label\nenhancement-based multiscale transformer for palm-vein recognition,”\nIEEE Transactions on Instrumentation and Measurement, vol. 72, pp. 1–\n17, 2023.\n[48] S. Jain and B. Wallace, “Attention is not explanation,” arXiv, 2019.\n[49] S. Wiegreffe and Y . Pinter, “Attention is not not explanation,” arXiv, 2019.\n[50] W. Yan et al., “Survey on explainable AI: From approaches, limitations\nand applications aspects,” Human-Centered Intelligent Systems, vol. 3,\np. 161–188, 2023.\n[51] J. An and J. Inwhee, “Attention map-guided visual explanations for deep\nneural networks,” Applied Sciences, vol. 12, no. 8, 2022.\n[52] B. de Vries et al., “Explainable artificial intelligence (XAI) in radiology\nand nuclear medicine: a literature review,” Frontiers on Medicine, vol. 10,\nno. 1180773, 2023.\n10 VOLUME 1, 2022\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAlbano, Giusti, Maiorana, Campisi: Gender-Specific Explainable Vision Transformers for Vein Biometric Recognition\n[53] M. Usman, T. Zia, and A. Tariq, “Analyzing transfer learning of vision\ntransformers for interpreting chest radiography,” Journal of digital imag-\ning, vol. 35, no. 6, pp. 1445–1462, 2022.\n[54] S. A. Grosz, J. J. Engelsma, R. Ranjan, N. Ramakrishnan, M. Aggarwal,\nG. G. Medioni, and A. K. Jain, “Minutiae-guided fingerprint embeddings\nvia vision transformers,” arXiv, 2022.\n[55] K. Han et al., “A survey on vision transformer,” IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 45, no. 1, pp. 87–110,\n2022.\n[56] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, 2017.\n[57] A. M. Reza, “Realization of the contrast limited adaptive histogram\nequalization (CLAHE) for real-time image enhancement,” Journal of VLSI\nsignal processing systems for signal, image and video technology, vol. 38,\npp. 35–44, 2004.\n[58] H. Robbins and S. Monro, “A stochastic approximation method,” The\nannals of mathematical statistics, pp. 400–407, 1951.\n[59] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in py-\ntorch,” in Conference on Neural Information Processing Systems (NIPS),\n2017.\n[60] R. Wightman, “Pytorch image models,” GitHub repository, 2019.\nROCCO ALBANO, IEEE Student Member, is\na PhD student at the University of Roma Tre,\nspecializing in neural networks focusing on atten-\ntion mechanisms and explainable AI for biometric\nrecognition. With a master’s degree in informa-\ntion and communication technology engineering\nand a bachelor’s degree in electrical engineering\nfrom the University of Roma Tre, Rocco com-\nbines a solid academic foundation with a passion\nfor advancing artificial intelligence technologies.\nHis current research aims to improve biometric recognition capabilities\nthrough innovative applications of neural networks and attention mecha-\nnisms. Rocco’s work exemplifies a commitment to pushing the boundaries\nof AI in transparent and interpretable ways.\nLORENZO GIUSTI is Ph.D. student in Data\nScience at La Sapienza, University of Rome, spe-\ncialized in geometric and topological deep learn-\ning. His research journey includes a significant\nstint as a visiting Ph.D. student at the University\nof Cambridge and as a research scientist intern\nat NASA’s Jet Propulsion Laboratory, where he\nled a project on Martian terrain modeling using\nspacecraft imagery and Neural Radiance Fields.\nAt CERN, he innovated in anomaly detection for\nparticle accelerators. Lorenzo holds a Master’s in Data Science from La\nSapienza with a focus on deep learning and a Bachelor’s in Computer\nScience and Engineering from Roma Tre University, specializing in quantum\ncomputing.\nEMANUELE MAIORANA, IEEE Senior Mem-\nber, received the Ph.D. degree with European Doc-\ntorate Label from Roma Tre University, Rome,\nItaly, in 2009. He is currently an Assistant Profes-\nsor at the Department of Industrial, Electronic, and\nMechanical Engineering at Roma Tre University.\nHis research interests are in the area of digital\nsignal and image processing, with specific empha-\nsis on biometric recognition. He was the General\nChair of the 9th IEEE International Workshop\non Biometrics and Forensics (IWBF) 2021 and the General Chair of the\n16th IEEE International Workshop on Information Forensics and Security\n(WIFS) 2024. He is an Associate Editor of the IEEE TRANSACTIONS ON\nINFORMATION FORENSICS AND SECURITY .\nPATRIZIO CAMPISI, IEEE Fellow, received the\nPh.D. degree in electrical engineering from Roma\nTre University, Rome, Italy. He is a Full Professor\nat the Department of Industrial, Electronics, and\nMechanical Engineering at Roma Tre University.\nHis current research interests are in the area of bio-\nmetrics and secure multimedia communications.\nHe is the Vice President for Publications for the\nIEEE Biometrics Council. He was a member of\nthe IEEE Certified Biometric Program Learning\nSystem Committee. He was the IEEE SPS Director for Student Services\nfrom 2015 to 2017 and the Chair of the IEEE Technical Committee on\nInformation Forensics and Security from 2017 to 2018. He was the General\nChair of the 26th European Signal Processing Conference EUSIPCO 2018,\nItaly, the 7th IEEE Workshop on Information Forensics and Security (WIFS)\n2015, Italy, and the 12th ACM Workshop on Multimedia and Security\n2010, Italy. He was an Associate Editor and a Senior Associate Editor\nof the IEEE SIGNAL PROCESSING LETTERS and an Associate Editor\nof the IEEE TRANSACTIONS ON INFORMATION FORENSICS AND\nSECURITY . He was the Editor-in-Chief of the IEEE TRANSACTIONS ON\nINFORMATION FORENSICS AND SECURITY (2018-2021).\nVOLUME 1, 2022 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3393558\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Biometrics",
  "concepts": [
    {
      "name": "Biometrics",
      "score": 0.8306853771209717
    },
    {
      "name": "Computer science",
      "score": 0.6414114236831665
    },
    {
      "name": "Computer vision",
      "score": 0.5594644546508789
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5265206098556519
    },
    {
      "name": "Transformer",
      "score": 0.4637255072593689
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32726627588272095
    },
    {
      "name": "Engineering",
      "score": 0.17514395713806152
    },
    {
      "name": "Electrical engineering",
      "score": 0.09007656574249268
    },
    {
      "name": "Voltage",
      "score": 0.06265702843666077
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I119003972",
      "name": "Roma Tre University",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    }
  ]
}