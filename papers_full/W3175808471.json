{
  "title": "Do Language Models Perform Generalizable Commonsense Inference?",
  "url": "https://openalex.org/W3175808471",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2138353447",
      "name": "Peifeng Wang",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A13399620",
      "name": "Filip Ilievski",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2656641051",
      "name": "Muhao Chen",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3048585918",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W3176825161",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3119212036",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2985671014",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2748256906",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2898617749",
    "https://openalex.org/W2509019445",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3174202502",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W3103252405",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3106484161",
    "https://openalex.org/W2963219906"
  ],
  "abstract": "Inspired by evidence that pretrained language models (LMs) encode commonsense knowledge, recent work has applied LMs to automatically populate commonsense knowledge graphs (CKGs).However, there is a lack of understanding on their generalization to multiple CKGs, unseen relations, and novel entities.This paper analyzes the ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction.Our experiments with these three aspects show that: (1) LMs can adapt to different schemas defined by multiple CKGs but fail to reuse the knowledge to generalize to new relations.(2) Adapted LMs generalize well to unseen subjects, but less so on novel objects.Future work should investigate how to improve the transferability and induction of commonsense mining from LMs. 1",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3681–3688\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3681\nDo Language Models Perform Generalizable Commonsense Inference?\nPeifeng Wang1,2, Filip Ilievski 2, Muhao Chen 1,2, Xiang Ren 1,2\n1Department of Computer Science, University of Southern California\n2Information Sciences Institute, University of Southern California\n{peifengw,muhaoche,xiangren}@usc.edu, ilievski@isi.edu\nAbstract\nInspired by evidence that pretrained language\nmodels (LMs) encode commonsense knowl-\nedge, recent work has applied LMs to auto-\nmatically populate commonsense knowledge\ngraphs (CKGs). However, there is a lack of\nunderstanding on their generalization to mul-\ntiple CKGs, unseen relations, and novel enti-\nties. This paper analyzes the ability of LMs\nto perform generalizable commonsense infer-\nence, in terms of knowledge capacity, transfer-\nability, and induction. Our experiments with\nthese three aspects show that: (1) LMs can\nadapt to different schemas deﬁned by multiple\nCKGs but fail to reuse the knowledge to gen-\neralize to new relations. (2) Adapted LMs gen-\neralize well to unseen subjects, but less so on\nnovel objects. Future work should investigate\nhow to improve the transferability and induc-\ntion of commonsense mining from LMs.1\n1 Introduction\nLarge-scale commonsense knowledge graphs\n(CKGs), like ConceptNet (Speer et al., 2017) and\nATOMIC (Sap et al., 2019), store structured knowl-\nedge that can beneﬁt various knowledge-driven\napplications. Given the usefulness of CKGs, but\nalso their inability to ﬂexibly provide information,\n(Paulheim, 2018), recent work has paid much at-\ntention to populating CKGs with commonsense\nknowledge mined from pretrained language models\n(LMs) (Wang et al., 2020c; Bosselut et al., 2019).\nEnhancing the knowledge of CKGs is essential\nto support reasoning on downstream tasks (Talmor\net al., 2019; Wang et al., 2020b; Young et al., 2018).\nThe task of completing CKGs has typically been\nposed as commonsense knowledge inference, where\nthe goal is to predict the object of a fact triplet,\ngiven its subject and a relation (predicate) (Petroni\n1The code is avaiable at https://github.com/\nwangpf3/LM-for-CommonsenseInference.\n0. Single CKG1. Multi-task 2. Transfer Learning 3. Low-resource\nFigure 1: Unlike previous studies that adapt LM on one\nsingle CKG (0), we investigate LM’s three aspects of\ngenerlizability: (1) knowledge capacity by multi-task\nlearning, (2) transferability by transfer learning and (3)\ninduction by controlled low-resource learning.\net al., 2019; Bosselut et al., 2019). Commonsense\ninference techniques, such as COMET (Bosse-\nlut et al., 2019), typically ﬁne-tune an LM, like\nGPT (Radford et al., 2018), over the training set\nfrom a single CKG. While such methods are able\nto dynamically enhance the completeness of CKGs,\ntheir application so far has been limited to the re-\nlation set of the source (training) CKG (Da et al.,\n2021). In addition, the generated object concepts\nare found to be largely biased towards the ones in\nthe training set (Wang et al., 2020a). It remains\nunclear to which extent LMs can generalize to mul-\ntiple CKGs, new relations, and novel objects. To\nthis end, we pose the question: do language models\nperform generalizable commonsense inference?\nTo answer this question, we study three aspects\nof the LM generalizability for commonsense infer-\nence, namely: knowledge capacity, transferability,\nand induction. To measure the knowledge capac-\nity ability of LMs, we examine whether LMs can\nbe adapted to multiple CKGs simultaneously, and\ntested on each of the CKGs. We test their transfer-\nability by assessing whether an initial adaptation\nof a LM on multiple source CKGs can reduce the\neffort on further adapting it to a new CKG. The\ninductive power of LMs is measured by varying\nthe overlap between the objects in the training and\ntest splits of a CKG. The overview of our analysis\nis depicted in Figure 1. Our results show that LMs\nare able to infer knowledge for multiple CKGs si-\nmultaneously without loss of performance on the\n3682\ntarget inference task, though the transferability of\nknowledge across tasks is limited. In addition, we\nobserve that the inductive power of LMs for com-\nmonsense inference relies heavily on whether an\nobject is observed during training.\n2 Analysis Setup\nTo shed light on the LM’s generalizalibility for\ncommonsense inference, we investigate: whether\nLMs have the capability to adapt to multiple CKGs\n(Q1: capacity), whether LMs can reuse the knowl-\nedge learned from source CKGs to efﬁciently adapt\nto a target CKG (Q2: transferability), and whether\nLMs can predict unseen objects or mainly repeat\nthe observed ones ( Q3: induction ). In this Sec-\ntion, we deﬁne the task, the CKGs we consider, our\nexperimental settings, and relate to prior studies.\n2.1 Task Formulation\nFollowing Hwang et al. (2020); Da et al. (2021),\nwe formalize commonsense inference as a task\nof predicting the object of a triplet, given a pair\nof (subject, relation) as input. The subject s\nand the object o are both expressed as free-form\nphrases, while the relation r is a predeﬁned rela-\ntion type from the CKG. A training example from\nConceptNet could have (go to a concert,\nMotivatedByGoal) as input, and listen\nto music as output. Assuming that a CKG is\ngiven, the goal is to leverage the commonsense\ntriplets in the CKG as training examples to adapt\nthe LM for commonsense inference.\n2.2 CKG Datasets\nWe consider three large and popular CKGs, with\ndifferent foci:(1) ConceptNet’s broad set of com-\nmonsense knowledge includes taxonomic (e.g.,\nIsA), utility (e.g., UsedFor), and temporal\nknowledge (e.g., HasPrerequisite). It com-\nbines crowdsourced knowledge with that from\nexisting sources, such as WordNet. We use its\nConceptNet-100K subset, collected by Li et al.\n(2016). (2) TupleKB (Dalvi Mishra et al., 2017)\nfocuses on scientiﬁc commonsense knowledge like\n(salt, dissolve in, water). It is con-\nstructed through an information extraction pipeline.\n(3) ATOMIC(Sap et al., 2019) has social common-\nsense knowledge about causes and effects of every-\nday events, and mental states (e.g., xIntent) of\ntheir participants. It is created by crowdsourcing.\nAs indicated by Jastrzebski et al. (2018), a\nlarge proportion of the subjects in the test set\nof ConceptNet-100K overlap with its training set,\nwhile TupleKB does not provide an ofﬁcial split.\nThus, we (re-)split these two datasets to ensure that\nthe subjects of testing triplets do not appear in the\ntraining set. This criterion is also consistent with\nhow the ATOMIC dataset is constructed.\n2.3 Experimental Settings\nMulti-task LearningTo answer Q1, we adapt an\nLM with balanced training data from ConceptNet,\nTupleKB, and ATOMIC. We sample 8 triplets from\neach dataset to form one training batch.\nTransfer LearningTo provide insight into Q2, we\nadopt transfer learning under a leave-one-out strat-\negy. In this setting, we adapt an LM on two of the\nthree CKGs, and then we further adapt it on the\nthird target CKG. Moreover, we study the data efﬁ-\nciency of this transfer learning by down-sampling\neach training set to x = {1, 20, 50}%, in order to\nsee whether the LM can adapt to the target CKG\nwith less training effort. Fine-tuning on data as\nsmall as 1% training set may suffer from instability,\nand results may change dramatically given a new\nsplit of training data (Gao et al., 2020). To control\nthe randomness, we re-sample the 1% training data\n5 times with a ﬁxed set of random seeds and report\nthe average performance instead.\nControlled Low-resource LearningTo answer\nQ3, we design a controlled experiment, where we\nﬁrst split the training set into two disjoint subsets\ndepending on whether the triplets in the original\ntraining set contain objects that exist in the test set\nor not. We denote the subset where the objects of\nthe triplets appear in testing data as Ω. We sam-\nple x = {0, 25, 50, 100}% of the training triplets\nin Ω for adapting the LM. During the evaluation,\nwe also separate the test set into two disjoint sub-\nsets, according to whether the objects are seen in\nthe original full training set. The results on these\ntwo split test sets are reported separately for each\nadapted LM.\nEvaluation ProtocolFor each (subject, relation)\npair in the test set, we treat all their objects as\nground truth references for evaluating the model\ninference. We report scores for commonly used\nautomatic evaluation metrics for text generation:\nBLEU (Papineni et al., 2002), ROUGE (Lin, 2004),\nand METEOR (Banerjee and Lavie, 2005), which\nare shown to be consistent with human judge-\nments (Hwang et al., 2020). During experiments,\nwe observe a high correlation among these differ-\n3683\nAdaptation methodInput Learnable params\nZero-shot (ZS) (s, r) N/A\nZS+demo (s′, r, o′, s, r) N/A\nFine-tuning (FT) (s, r) Transformer (LM)\nFT+demo (s′, r, o′, s, r) Transformer (LM)\nAdapter tuning (AT)(s, r) Adapter\nTable 1: Methods for using LMs to conduct common-\nsense inference. “+demo” means prepending a demon-\nstration triplet (s\n′\n, r, o\n′\n) before the input tuple.\nent metrics and choose to report METEOR in the\nmain text and other metrics in the appendix.\n2.4 Connections to Prior Studies\nEarlier works (Li et al., 2016; Jastrzebski et al.,\n2018; Davison et al., 2019) poses the CKG com-\npletion task as triplet classiﬁcation, where the\ngoal is to score the plausibility of a complete\ntriplet. COMET (Bosselut et al., 2019) is the ﬁrst\nto cast this task as commonsense inference with\nLMs. Follow-up contributions utilize COMET as\na commonsense provider in various downstream\ntasks (Bosselut and Choi, 2021; Ammanabrolu\net al., 2021; Chakrabarty et al., 2020), thus provid-\ning evidence for LM’s generalization to previously\nunseen scenarios. Further efforts include Hwang\net al. (2020), which show that the quality of the\ntraining triplets is a key factor of adapting LMs,\nand (Da et al., 2021), which investigates how to\nlearn COMET in a few-shot learning setting. Mean-\nwhile, the study by Wang et al. (2020a) indicates\nthe limited generalization of COMET. Ma et al.\n(2021) also adapt LMs simultaneously on multiple\nCKGs, albeit their goal is to improve downstream\nperformance rather than CKG inference. In this pa-\nper, we aim to provide a more comprehensive study\nof a LM’s generalizability for CKG inference.\n3 Method\nWhile a set of pretrained LMs exists, we adopt\na widely used generative model, GPT2 (Radford\net al., 2019), as our baseline LM. The investigation\nof other generative LMs is orthogonal to our analy-\nsis. We experiment with its largest version, GPT2-\nXL, which contains 48 transformer layers (Vaswani\net al., 2017), ensuring sufﬁcient capacity for stor-\ning knowledge acquired during its pretraining. We\nintroduce our experimental method as follows.\nCommonsense Inference with LMsGiven a train-\ning triplet (s,r,o), we represent s and o as sequences\nof tokens, xs and xo, which is trivial given that they\nare already expressed as phrases. As for the rela-\ntion r, we convert it by using a template taken from\nthe literature (Davison et al., 2019) into a natural-\nlanguage phrase xr, e.g., IsA is converted to “is a”.\nThis has been shown to facilitate efﬁcient adapta-\ntion of LMs (Da et al., 2021). Note that we do not\nexplicitly provide the LMs with the information\nabout the source CKG of the triplet as input (e.g.,\nprepending a related special token to the triplet).\nAdapting LMs with Commonense Knowledge\nThe training objectives for adapting LMs is to maxi-\nmize the probability of generating the object phrase\nxo given the tuple (xs, xr). During inference, we\nadopt greedy decoding to obtain the predicted ob-\nject from the adapted LM.\nThere have been various techniques devel-\noped for adapting pretrained LMs to downstream\ntasks (Howard and Ruder, 2018; Chen et al., 2020).\nMoreover, previously only the vanillaFine-tuning,\ni.e., updating the whole LM architecture during\ntraining, has been employed to adapt LMs for com-\nmonsense inference (Bosselut et al., 2019; Hwang\net al., 2020; Da et al., 2021). To obtain comprehen-\nsive results that are not speciﬁc to one particular\nway of ﬁne-tuning, here we investigate two more\nalternatives, each of which has their own advantage\nwhen considered in different contexts.\nFine-tuning with Demonstration (FT+demo)\nCombining the ideas of ﬁne-tuning and in-context\nlearning (Brown et al., 2020), this technique (Gao\net al., 2020) adds a demonstration to each input\nas additional context and ﬁne-tunes the whole LM\nas usual. Incorporating demonstrations is shown\nto boost performance when the amount of training\ndata is extremely limited. In our case, a demon-\nstration is a top-1 training triplet (s\n′\n, r, o\n′\n), ranked\naccording to the cosine similarity between the em-\nbedding of the input tuple (s, r) and the embed-\ndings of the training tuples with the same relation\ntype r. The tuple embeddings are given by a pre-\ntrained Sentence-BERT (Reimers and Gurevych,\n2019). For instance, a demonstration ( go to\nrestaurant, UsedFor, eat out) would be\nadded before the input (go to pub, UsedFor).\nWith the demonstrated triplets, the LM could learn\nto understand the schema of the CKG instead of\nsimply learning the knowledge from the training\ndata.\nAdapter Tuning (AT)Unlike ﬁne-tuning, adapter\ntuning (Houlsby et al., 2019) ﬁxes the entire LM\nand adds one trainable adapter right before the skip\nconnection in each transformer layer of the LM,\n3684\nZS ZS+ FT FT+d AT0.00\n0.05\n0.10\n0.15\n0.20METEOR\n0.05\n0.08\n0.18 0.18 0.170.17 0.19\n0.17\nConceptNet\nSingle-task\nMulti-task\nZS ZS+ FT FT+d AT0.0\n0.1\n0.2\n0.3\n0.03\n0.09\n0.22\n0.26\n0.210.21\n0.26\n0.20\nTupleKB\nSingle-task\nMulti-task\nZS ZS+ FT FT+d AT0.0\n0.1\n0.2\n0.3\n0.14 0.16\n0.29 0.29 0.290.28 0.28 0.28\nATOMIC\nSingle-task\nMulti-task\nFigure 2: Results (METEOR) forknowledge capacityof LMs. ”FT+d” refers to FT+demo.We ﬁnd no notable performance\ndrop for any method trained in the multi-task setting.\n1 20 50 100\n0\n10\n20ConceptNet\n10.24\n14.36 15.97 17.68\n10.04\n13.89 15.91 17.61\n10.78\n15.68 17.51 18.29\n10.39\n15.22 17.13 18.08\n9.88\n14.60 15.62 16.33\n2.99 4.67 5.68 6.42\nFT FT (multi-task) FT+d FT+d (multi-task) AT AT (multi-task)\n1 20 50 100\n0\n20TupleKB\n13.13\n18.28 21.01 22.50\n12.81\n18.23 21.02 22.16\n12.63\n20.82 23.34 25.96\n13.31\n20.39 23.54 26.11\n13.40\n17.71 19.23 21.54\n6.91 7.06 9.64 10.75\n1 20 50 100\nTraining Proportion (%)\n0\n20\n40ATOMIC\n25.79 28.19 28.76 29.38\n24.42\n28.16 28.73 29.29\n25.03 28.17 28.58 28.87\n25.24 28.30 28.59 28.8825.50 28.50 28.77 29.32\n17.94\n22.14 23.68 23.64\nFigure 3: Results (METEOR) for LM transferability.\n”FT+d” refers to FT+demo. Across datasets, we do not ob-\nserve that adapting to the source CKGs would enable the LMs\nto adapt to the target CKG better or more easily.\n0 25 50 100\n0\n10\n20ConceptNet\n14.92\n17.38 18.70\n20.66\n11.15 11.11 10.34 9.76\n16.09\n18.87 20.72 22.27\n12.22 11.56 11.34 10.63\n15.84 17.74 18.75 20.30\n11.25 10.83 10.59 9.73\nFT (seen)\nFT (unseen)\nFT+d (seen)\nFT+d (unseen)\nAT (seen)\nAT (unseen)\n0 25 50 100\n0\n20TupleKB\n8.06\n17.68 19.78 21.92\n4.84 5.08 5.13 5.09\n8.67\n19.92\n23.12\n26.44\n5.27 6.21 6.03 5.827.96\n17.33 19.11 21.41\n4.34 5.02 4.97 5.08\n0 25 50 100\nProportion of training set with testing objects (%)\n0\n20ATOMIC\n21.25\n25.56 27.01 27.70\n19.79 17.63 17.20 16.63\n20.52\n25.63 26.93 27.56\n19.01 17.39 16.88 16.31\n20.77\n26.45 27.61 27.90\n19.41 18.16 17.46 16.60\nFigure 4: Results (METEOR) for LMinduction. ”FT+d”\nrefers to FT+demo. All the methods perform better on pre-\ndicting facts that contain seen objects, while the performance\ndegrades when less objects are seen during training.\nwhich is more parameter-efﬁcient. Each adapter\nis a two-layer bottleneck network with a skip-\nconnection internally. Following Houlsby et al.\n(2019), the parameters of the bottleneck network\nare initialized close to zero so that the adapter ap-\nproximates an identity function from the beginning.\nWe compare to two additional baselines, both\nusing GPT2-XL in a zero-shot setting: Zero-shot\n(ZS) is fed with the same input as Fine-tuning,\nwhile zero-shot with demonstrations ( ZS+demo)\ncombines the input plus demonstration, as in the\nFT+demo method. By investigating all these meth-\nods, we aim to understand the inﬂuence of different\nadaptation techniques on the models’ performance.\nTable 1 summarizes the set of methods which we\nconsider in this paper.\n4 Results and Discussion\nKnowledge Capacity (Q1)The results that quan-\ntify the knowledge capacity of LMs for common-\nsense inference over multiple CKGs with ME-\nTEOR scores are shown in Figure 2. The com-\nplete results including other metrics can be found\nin the appendix. All adaptation methods perform\nconsiderably better than the zero-shot baselines,\nindicating the beneﬁt of adaptation. There is no\nclear distinction between the adaptation methods,\nthough FT+demo performs slightly better than the\nothers across CKGs. Most importantly, we ﬁnd no\nnotable performance drop for any method in the\nmulti-task training setup despite the challenge that\nthere is limited overlap between these CKGs. Only\n10.0% of the facts from ATOMIC can be found\nin ConceptNet (Hwang et al., 2020) while 8.4%\nof the facts from ConceptNet can be found in Tu-\npleKB (Dalvi Mishra et al., 2017) 2. This indicates\nthe prominent capacity of LMs to simultaneously\nadapt to different CKGs. Nevertheless, the results\nreveal that learning different CKGs jointly do not\ninterfere with each other positively (via knowledge\nsharing) or negatively (due to overﬁtting).\nTransferability (Q2)Figure 3 shows the obtained\nresults regarding the transferability of LMs. Across\ndifferent CKGs and for any training data size, we\nobserve no indications that adapting to the source\nCKGs enhances the performance on the target\nCKG. On the contrary, adapting from source CKGs\n2We also try to breakdown the results by relation types and\ndo not observe correlation between the relation-wise perfor-\nmance and the extent of overlap.\n3685\neven hurts the performance of the Adapter-tuning\nmethod, revealing that this method overﬁts to the\nsource CKGs. Overall, we conclude that LMs can-\nnot reuse the knowledge learned from the source\nCKGs to improve the performance on the target\nCKG or achieve the same performance with less\ntraining data. Thus, we call for future study on\ndeveloping more effective adaptation methods.\nInduction (Q3)The results in Figure 4 show that\nwithout down-sampling (x = 100%), all methods\nperform much better on predicting facts that con-\ntain seen objects, and their performance degrades\nmore when less object entities are seen to training.\nMeanwhile, the performance on facts with unseen\nobjects stays roughly unaffected. This indicates a\nkey limitation of the LMs: they adapt notably better\non seen objects. Since the training set and test set\ndo not share subjects, we conclude that the general-\nizability of the LM is largely dependent on ﬁnding\nthe relationship between unseen subjects and ob-\nserved objects. We thus posit that a novel strategy\nfor adapting LMs while retaining the knowledge\nacquired during pre-training is necessary for bet-\nter generalizability. Promising directions here are\npreﬁx tuning (Li and Liang, 2021) or including an\nadditional objective during adaptation which would\nencourage the generation of novel objects.\n5 Conclusion\nThis work conducted a focused study of three as-\npects of the generalizability of LMs for common-\nsense inference: knowledge capacity, transferabil-\nity, and induction. We experiment with ﬁve meth-\nods of using a generative LM and three represen-\ntative CKGs. Despite their capability to accommo-\ndate multiple CKGs, we have observed that LMs\nhave limited ability to transfer knowledge across\nCKGs. Moreover, their adaptation relies heavily\non whether the objects to predict are seen during\ntraining. These ﬁndings help our understanding\nof LMs’ adaptation behavior on commonsense in-\nference, and highlight the need for future work to\nimprove their transferability and induction.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. This material is based upon\nwork sponsored by the DARPA MCS program un-\nder Contract No. N660011924033 with the United\nStates Ofﬁce Of Naval Research.\nReferences\nPrithviraj Ammanabrolu, Wesley Cheung, William\nBroniec, and Mark O Riedl. 2021. Automated sto-\nrytelling via causal, commonsense plot ordering. In\nProceedings of the 35th AAAI Conference on Artiﬁ-\ncial Intelligence (AAAI).\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization , pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nAntoine Bosselut and Yejin Choi. 2021. Dynamic\nknowledge graph construction for zero-shot com-\nmonsense question answering. In Proceedings of\nthe 35th AAAI Conference on Artiﬁcial Intelligence\n(AAAI).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nTuhin Chakrabarty, Smaranda Muresan, and Nanyun\nPeng. 2020. Generating similes effortlessly like a\npro: A style transfer approach for simile generation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6455–6469, Online. Association for Computa-\ntional Linguistics.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7870–7881, Online. As-\nsociation for Computational Linguistics.\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and\nAntoine Bosselut. 2021. Understanding few-shot\n3686\ncommonsense knowledge models. arXiv preprint\narXiv:2101.00297.\nBhavana Dalvi Mishra, Niket Tandon, and Peter Clark.\n2017. Domain-targeted, high precision knowledge\nextraction. Transactions of the Association for Com-\nputational Linguistics, 5:233–246.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA , volume 97 of\nProceedings of Machine Learning Research , pages\n2790–2799. PMLR.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. arXiv\npreprint arXiv:2010.05953.\nStanislaw Jastrzebski, Dzmitry Bahdanau, Seyedarian\nHosseini, Michael Noukhovitch, Yoshua Bengio,\nand Jackie Cheung. 2018. Commonsense mining as\nknowledge base completion? a study on the impact\nof novelty. In Proceedings of the Workshop on Gen-\neralization in the Age of Deep Learning, pages 8–16,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nXiang Li, Aynaz Taheri, Lifu Tu, and Kevin Gimpel.\n2016. Commonsense knowledge base completion.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1445–1455, Berlin, Germany.\nAssociation for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for genera-\ntion. arXiv preprint arXiv:2101.00190.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nKaixin Ma, Filip Ilievski, Jonathan Francis, Yonatan\nBisk, Eric Nyberg, and Alessandro Oltramari. 2021.\nKnowledge-driven Data Construction for Zero-shot\nEvaluation in Commonsense Question Answering.\nIn 35th AAAI Conference on Artiﬁcial Intelligence.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nHeiko Paulheim. 2018. How much is a triple? esti-\nmating the cost of knowledge graph creation. In\nProceedings of the 17th International Semantic Web\nConference.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A. Smith, and Yejin Choi. 2019.\nATOMIC: an atlas of machine commonsense for\nif-then reasoning. In The Thirty-Third AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2019, The\nThirty-First Innovative Applications of Artiﬁcial In-\ntelligence Conference, IAAI 2019, The Ninth AAAI\nSymposium on Educational Advances in Artiﬁcial\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA,\nJanuary 27 - February 1, 2019 , pages 3027–3035.\nAAAI Press.\n3687\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence, Febru-\nary 4-9, 2017, San Francisco, California, USA ,\npages 4444–4451. AAAI Press.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nCunxiang Wang, Jinhang Wu, Luxin Liu, and Yue\nZhang. 2020a. Commonsense knowledge graph rea-\nsoning by selection or generation? why? arXiv\npreprint arXiv:2008.05925.\nHaoyu Wang, Muhao Chen, Hongming Zhang, and\nDan Roth. 2020b. Joint constrained learning for\nevent-event relation extraction. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 696–706,\nOnline. Association for Computational Linguistics.\nPeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro\nSzekely, and Xiang Ren. 2020c. Connecting the\ndots: A knowledgeable path generator for common-\nsense question answering. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 4129–4140, Online. Association for Computa-\ntional Linguistics.\nTom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,\nSubham Biswas, and Minlie Huang. 2018. Aug-\nmenting end-to-end dialogue systems with common-\nsense knowledge. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 4970–4977. AAAI Press.\n3688\nBLEU-2 ROUGE-L METEOR\nsingle-task multi-task single-task multi-task single-task multi-task\nZero-shot 0.0069 NA 0.1009 NA 0.0506 NA\nZS+demo 0.0284 NA 0.1281 NA 0.0787 NA\nAdapter-tuning 0.1289 0.1279 0.2598 0.2560 0.1739 0.1706\nFine-tuning 0.1325 0.1286 0.2629 0.2575 0.1775 0.1749\nConceptNetFT+demo 0.1333 0.1398 0.2678 0.2738 0.1795 0.1851\nZero-shot 0.0017 NA 0.0999 NA 0.0263 NA\nZS+demo 0.0099 NA 0.2748 NA 0.0869 NA\nAdapter-tuning 0.1383 0.1323 0.3785 0.3627 0.2094 0.2010\nFine-tuning 0.1371 0.1388 0.3985 0.3812 0.2151 0.2122\nTupleKBFT+demo 0.1699 0.1698 0.4902 0.4714 0.2622 0.2580\nZero-shot 0.0436 NA 0.2523 NA 0.1419 NA\nZS+demo 0.0808 NA 0.2233 NA 0.1572 NA\nAdapter-tuning 0.2161 0.2035 0.4008 0.3890 0.2913 0.2832\nFine-tuning 0.2125 0.2057 0.3982 0.3908 0.2913 0.2843\nATOMICFT+demo 0.2111 0.2070 0.3915 0.3868 0.2887 0.2800\nTable 2: Results of all the evaluation metrics for the knowledge capacity experiments.\nA Appendix\nA.1 Dataset Statistics\n[h]\nTrain Dev Test\nConceptNet100k 79,770 10,203 10,027\nTupleKB 98,674 12,357 12,427\nATOMIC 578,002 64,902 71,127\nTable 3: CKG Dataset Statistics.\nA.2 Implementation Details\nThe GPT2-XL language model we adopted in this\nwork has 1558M parameters in total. We train\nall the models on a V100 GPU. As for hyper-\nparameters, we adopt the commonly-used learning\nrate (1e-5) and batch size (16) for adapting GPT2,\nexcept that in the multi-task learning setting, the\nbatch size is 24 (8 samples from each CKG).\nA.3 Additional Results",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8170292377471924
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.7376305460929871
    },
    {
      "name": "Inference",
      "score": 0.7211287617683411
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6754472851753235
    },
    {
      "name": "Transferability",
      "score": 0.6547112464904785
    },
    {
      "name": "Generalization",
      "score": 0.622678279876709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5776249766349792
    },
    {
      "name": "ENCODE",
      "score": 0.5555475354194641
    },
    {
      "name": "Natural language processing",
      "score": 0.43609732389450073
    },
    {
      "name": "Machine learning",
      "score": 0.38160523772239685
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.2129729688167572
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}