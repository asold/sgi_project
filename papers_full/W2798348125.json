{
  "title": "Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data",
  "url": "https://openalex.org/W2798348125",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2799062162",
      "name": "Adithya Pratapa",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2583693142",
      "name": "Gayatri Bhat",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2162966668",
      "name": "Monojit Choudhury",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2013710467",
      "name": "Sunayana Sitaram",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2148686933",
      "name": "Sandipan Dandapat",
      "affiliations": [
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A1965970590",
      "name": "Kalika Bali",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2990897930",
    "https://openalex.org/W2149778059",
    "https://openalex.org/W2034585809",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2962920413",
    "https://openalex.org/W1541518680",
    "https://openalex.org/W2407467516",
    "https://openalex.org/W2097606805",
    "https://openalex.org/W1605308203",
    "https://openalex.org/W2014962660",
    "https://openalex.org/W2139645402",
    "https://openalex.org/W44716798",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2002019621",
    "https://openalex.org/W2525997790",
    "https://openalex.org/W2295078202",
    "https://openalex.org/W4297813410",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W2338893019",
    "https://openalex.org/W2593383075",
    "https://openalex.org/W2252095989",
    "https://openalex.org/W2145867197",
    "https://openalex.org/W4237155282",
    "https://openalex.org/W2250548009",
    "https://openalex.org/W1970026646",
    "https://openalex.org/W2251149908",
    "https://openalex.org/W2571593272",
    "https://openalex.org/W2560970694",
    "https://openalex.org/W1590009191",
    "https://openalex.org/W2148708890",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W1969754958",
    "https://openalex.org/W2236357521",
    "https://openalex.org/W2963293280",
    "https://openalex.org/W2108015785",
    "https://openalex.org/W2807157666"
  ],
  "abstract": "Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan Dandapat, Kalika Bali. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018.",
  "full_text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1543–1553\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n1543\nLanguage Modeling for Code-Mixing:\nThe Role of Linguistic Theory based Synthetic Data\nAdithya Pratapa1 Gayatri Bhat2∗ Monojit Choudhury1 Sunayana Sitaram1\nSandipan Dandapat3 Kalika Bali1\n1 Microsoft Research, Bangalore, India\n2 Language Technology Institute, Carnegie Mellon University\n3 Microsoft R&D, Hyderabad, India\n1{t-pradi, monojitc, sunayana.sitaram, kalikab}@microsoft.com,\n2gbhat@andrew.cmu.edu, 3sadandap@microsoft.com\nAbstract\nTraining language models for Code-mixed\n(CM) language is known to be a difﬁ-\ncult problem because of lack of data com-\npounded by the increased confusability\ndue to the presence of more than one lan-\nguage. We present a computational tech-\nnique for creation of grammatically valid\nartiﬁcial CM data based on the Equiva-\nlence Constraint Theory. We show that\nwhen training examples are sampled ap-\npropriately from this synthetic data and\npresented in certain order (aka training\ncurriculum) along with monolingual and\nreal CM data, it can signiﬁcantly reduce\nthe perplexity of an RNN-based language\nmodel. We also show that randomly gener-\nated CM data does not help in decreasing\nthe perplexity of the LMs.\n1 Introduction\nCode-switching or code-mixing (CM) refers to the\njuxtaposition of linguistic units from two or more\nlanguages in a single conversation or sometimes\neven a single utterance.1 It is quite commonly ob-\nserved in speech conversations of multilingual so-\ncieties across the world. Although, traditionally,\nCM has been associated with informal or casual\nspeech, there is evidence that in several societies,\nsuch as urban India and Mexico, CM has become\nthe default code of communication (Parshad et al.,\n2016), and it has also pervaded written text, espe-\ncially in computer-mediated communication and\nsocial media (Rijhwani et al., 2017).\n∗Work done during author’s internship at Microsoft Re-\nsearch\n1According to some linguists, code-switching refers to\ninter-sentential mixing of languages, whereas code-mixing\nrefers to intra-sentential mixing. Since the latter is more gen-\neral, we will use code-mixing in this paper to mean both.\nIt is, therefore, imperative to build NLP tech-\nnology for CM text and speech. There have\nbeen some efforts towards building of Automatic\nSpeech Recognition Systems and TTS for CM\nspeech (Li and Fung, 2013, 2014; Gebhardt, 2011;\nSitaram et al., 2016), and tasks like language\nidentiﬁcation (Solorio et al., 2014; Barman et al.,\n2014), POS tagging (Vyas et al., 2014; Solorio\nand Liu, 2008), parsing and sentiment analy-\nsis (Sharma et al., 2016; Prabhu et al., 2016; Rudra\net al., 2016) for CM text. Nevertheless, the accura-\ncies of all these systems are much lower than their\nmonolingual counterparts, primarily due to lack of\nenough data.\nIntuitively, since CM happens between two (or\nmore languages), one would typically need twice\nas much, if not more, data to train a CM sys-\ntem. Furthermore, any CM corpus will contain\nlarge chunks of monolingual fragments, and rel-\natively far fewer code-switching points, which are\nextremely important to learn patterns of CM from\ndata. This implies that the amount of data required\nwould not just be twice, but probably 10 or 100\ntimes more than that for training a monolingual\nsystem with similar accuracy. On the other hand,\napart from user-generated content on the Web and\nsocial media, it is extremely difﬁcult to gather\nlarge volumes of CM data because (a) CM is rare\nin formal text, and (b) speech data is hard to gather\nand even harder to transcribe.\nIn order to circumvent the data scarcity issue,\nin this paper we propose the use of linguistically-\nmotivated synthetically generated CM data (as\na supplement to real CM data) for development\nof CM NLP systems. In particular, we use the\nEquivalence Constraint Theory (Poplack, 1980;\nSankoff, 1998) for generating linguistically valid\nCM sentences from a pair of parallel sentences\nin the two languages. We then use these gener-\nated sentences, along with monolingual and little\n1544\namount of real CM data to train a CM Language\nModel (LM). Our experiments show that, when\ntrained following certain sampling strategies and\ntraining curriculum, the synthetic CM sentences\nare indeed able to improve the perplexity of the\ntrained LM over a baseline model that uses only\nmonolingual and real CM data.\nLM is useful for a variety of downstream NLP\ntasks such as Speech Recognition and Machine\nTranslation. By deﬁnition, it is a discriminator be-\ntween natural and unnatural language data. The\nfact that linguistically constrained synthetic data\ncan be used to develop better LM for CM text is,\non one hand an indirect statistical and task-based\nvalidation of the linguistic theory used to generate\nthe data, and on the other hand an indication that\nthe approach in general is promising and can help\nsolve the issue of data scarcity for a variety of NLP\ntasks for CM text and speech.\n2 Generating Synthetic Code-mixed Data\nThere is a large and growing body of linguis-\ntic research regarding the occurrence, syntac-\ntic structure and pragmatic functions of code-\nmixing in multilingual communities across the\nworld. This includes many attempts to explain\nthe grammatical constraints on CM, with three of\nthe most widely-accepted being the Embedded-\nMatrix (Joshi, 1985; Myers-Scotton, 1993, 1995),\nthe Equivalence Constraint(EC) (Poplack, 1980;\nSankoff, 1998) and the Functional Head Con-\nstraint (DiSciullo et al., 1986; Belazi et al., 1994)\ntheories.\nFor our experiments, we generate CM sentences\nas per the EC theory, since it explains a range of\ninteresting CM patterns beyond lexical substitu-\ntion and is also suitable for computational model-\ning. Further, in a brief human-evaluation we con-\nducted, we found that it is representative of real\nCM usage. In this section, we list the assumptions\nmade by the EC theory, brieﬂy explain the theory,\nand then describe how we generate CM sentences\nas per this theory.\n2.1 Assumptions of the EC Theory\nConsider two languages L 1 and L 2 that are be-\ning mixed. The EC Theory assumes that both\nlanguages are deﬁned by context-free grammars\nG1 and G 2. It also assumes that every non-\nterminal category X 1 in G1 has a corresponding\nnon-terminal category X2 in G2 and that every ter-\nminal symbol (or word) w 1 in G 1 has a corre-\nsponding terminal symbol w 2 in G 2. Finally, it\nassumes that every production rule in L1 has a cor-\nresponding rule in L2 - i.e, the non-terminal cate-\ngories on the left-hand side of the two rules cor-\nrespond to each other, and every category/symbol\non the right-hand side of one rule corresponds to\na category/symbol on the right-hand side of the\nother rule.\nAll these correspondences must also hold vice-\nversa (between languages L 2 and L1), which im-\nplies that the two grammars can only differ in the\nordering of categories/symbols on the right-hand\nside of any production rule. As a result, any sen-\ntence in L1 has a corresponding translation in L 2,\nwith their parse trees being equivalent except for\nthe ordering of sibling nodes. Fig.1(a) and (b)\nillustrate one such sentence pair in English and\nSpanish and their parse-trees. The EC Theory de-\nscribes a CM sentence as a constrained combina-\ntion of two such equivalent sentences.\nWhile the assumptions listed above are quite\nstrong, they do not prevent the EC Theory from\nbeing applied to two natural languages whose\ngrammars do not correspond as described above.\nWe apply a simple but effective strategy to recon-\ncile the structures of a sentence and its translation\n- if any corresponding subtrees of the two parse\ntrees do not have equivalent structures, we col-\nlapse each of these subtrees to a single node. Ac-\ncounting for the actual asymmetry between a pair\nof languages will certainly allow for the genera-\ntion of more CM variants of any L 1-L2 sentence\npair. However, in our experiments, this strategy\nretains most of the structural information in the\nparse trees, and allows for the generation of up to\nthousands of CM variants of a single sentence pair.\n2.2 The Equivalence Constraint Theory\nSentence production. Given two monolingual\nsentences (such as those introduced in Fig.1), a\nCM sentence is created by traversing all the leaf\nnodes in the parse tree of either of the two sen-\ntences. At each node, either the word at that\nnode or at the corresponding node in the other\nsentence’s parse is generated. While the traver-\nsal may start at any leaf node, once the produc-\ntion enters one constituent, it will exhaust all the\nlexical slots (leaf nodes) in that constituent or its\nequivalent constituent in the other language before\nentering into a higher level constituent or a sister\n1545\n(a) SE\nVPE\nPPE\nNPE\nNNE\nhouseJJE\nwhiteDTE\na\nINE\nin\nVBZE\nlives\nNPE\nPRPE\nShe\n(b) SS\nVPS\nPPS\nNPS\nJJS\nblancaNNS\ncasaDTS\nuna\nINS\nen\nVBZS\nvive\nNPS\nPRPS\nElle\n(c) S VPPPNPJJ*whiteNNS\ncasaDTS\nuna\nINS\nen\nVBZE\nlives\nNPS\nPRPS\nElle\n(d) S VPPPNPS\nJJS\nblancaNNS\ncasaDTS\nuna\nINE\nin\nVBZE\nlives\nNPS\nPRPS\nElle\nFigure 1: Parse trees of a pair of equivalent (a) English and (b) Spanish sentences, with corresponding\nhierarchical structure (due to production rules), internal nodes (non-terminal categories) and leaf nodes\n(terminal symbols), and parse trees of (c) incorrectly code-mixed and (d) correctly code-mixed variants\nof these sentences (as per the EC theory).\nconstituent. (Sankoff, 1998) This guarantees that\nthe parse tree of a sentence so produced will have\nthe same hierarchical structure as the two mono-\nlingual parse trees (Fig. 1(c) and (d)).\nThe EC theory also requires that any mono-\nlingual fragment that occurs in the CM sentence\nmust occur in one of the monolingual sentences (in\nthe running example, the fragment una blanca\nwould be disallowed since it does not appear in the\nSpanish sentence).\nSwitch-point identiﬁcation. To ensure that the\nCM sentence does not at any point deviate from\nboth monolingual grammars, the EC theory im-\nposes certain constraints on its parse tree. To this\nend and in order to identify the code-switching\npoints in a generated sentence, nodes in its parse\ntree are assigned language labels according to the\nfollowing rules: All leaf nodes are labeled by the\nlanguages of their symbols. If all the children of\nany internal node share a common label, the inter-\nnal node is also labeled with that language. Any\nnode that is out of rank-order among its siblings\naccording to one language is labeled with the other\nlanguage. (See labeling in Fig.1(c) and (d)) If any\nnode acquires labels of both languages during this\nprocess (such as the node marked with an asterisk\nin Fig.1(c)), the sentence is disallowed as per the\nEC theory. In the labeled tree, any pair of adjacent\nsibling nodes with contrasting labels are said to be\nat a switch-point (SP).\nEquivalence constraint. Every switch-point\nidentiﬁed in the generated sentence must abide by\nthe EC. Let U →U1U2...Un and V →V1V2...Vn\nbe corresponding rules applied in the two mono-\nlingual parse trees, and nodes Ui and Vi+1 be ad-\njacent in the CM parse tree. This pair of nodes\nis a switch-point, and it only abides by the EC if\nevery node in U1...Ui has a corresponding node\nin V1...Vi. This is true for the switch-point in\nFig.1(d), and indicates that the two grammars are\n‘equivalent’ at the code-switch point. More im-\nportantly, it shows that switching languages at this\npoint does not require another switch later in the\nsentence. If every switch-point in the generated\nsentence abides by the EC, the generated sentence\nis allowed by the EC theory.\n2.3 System Description\nWe assume that the input to the generation model\nis a pair of parallel sentences in L 1 and L2, along\nwith word level alignments. For our experiments,\nL1 and L2 are English and Spanish, and Sec 3.2\ndescribes how we create the input set. We use\nthe Stanford Parser (Klein and Manning, 2003) to\nparse the English sentence.\nProjecting parses. We use the alignments to\nproject the English parse tree onto the Spanish\nsentence in two steps: (1) We ﬁrst replace every\nword in the English parse tree with its Spanish\nequivalent (2) We re-order the child nodes of each\ninternal node in the tree such that their left-to-right\norder is as in the Spanish sentence. For instance,\nafter replacing every English word in Fig.1(a) with\nits corresponding Spanish word, we interchange\nthe positions of casa and blanca to arrive Fig.1(b).\nFor a pair of parallel sentences that follow all the\nassumptions of the EC theory, these steps can be\nperformed without exception and result in the cre-\nation of a Spanish parse tree with the same hierar-\nchical structure as the English parse.\nWe use various techniques to address cases in\nwhich the grammatical structures of the two sen-\ntences deviate. English words that are unaligned to\nany Spanish words are replaced by empty strings.\n(See Fig.2 wherein the English word she has no\nSpanish counterpart, since this pronoun is dropped\nin the Spanish sentence.) Contiguous word se-\nquences in one sentence that are aligned to the\n1546\n(a) SE\nVPE\nVPE\nNPE\nPRPE\nit\nVBE\ndo\nMDE\nwill\nNPE\nNNPE\nShe\n(b) SE\nVPE\nNPE\nPRPE\nit\nMD+VBE\ndowill\nNPE\nNNPE\nShe\n(c) SS\nVPS\nMD+VBS\nhar´aNPS\nPRPS\nlo\nNPS\nNNPS\n<>\nFigure 2: (a) The parse of an English sentence\nas per Stanford CoreNLP. This parse is projected\nonto the parallel Spanish sentence Lo har´a and\nmodiﬁed during this process, to produce corre-\nsponding (b) English and (c) Spanish parse trees.\nsame word(s) in the other language are collapsed\ninto a single multi-word node, and the entire sub-\ntree between these collapsed nodes and their clos-\nest common ancestor is ﬂattened to accommo-\ndate this change (example in Fig.2). While these\nchanges do result in slightly unnatural or simpli-\nﬁed parse trees, they are used very sparingly since\nEnglish and Spanish have very compatible gram-\nmars.\nGenerating CS sentences. The number of CS\nsentences that can be produced by combining a\ncorresponding pair of English and Spanish sen-\ntences increases exponentially with the length of\nthe sentences. Instead of generating these sen-\ntences exhaustively, we use the parses to construct\na ﬁnite-state automaton that succinctly captures\nthe acceptable CS sentences. Since the CS sen-\ntence must have the same hierarchical structure as\nthe monolingual sentences, we construct the au-\ntomaton during a post-order traversal of the mono-\nlingual parses. An automaton is constructed at\neach node by (1) concatenating the automatons\nconstructed at its child nodes, (2) splitting states\nand removing transitions to ensure that the EC the-\nory is not violated. The last automaton to be con-\nstructed, which is associated with the root node,\naccepts all the CS sentences that can be generated\nusing the monolingual parses. We do not provide\nthe exact details of automaton construction here,\nbut we plan to release our code in the near future.\n3 Datasets\nIn this work, we use three types of language data:\nmonolingual data in English and Spanish (Mono),\nreal code-mixed data (rCM), and artiﬁcial or gen-\nerated code-mixed data (gCM). In this section, we\ndescribe these datasets and their CM properties.\nWe begin with description of some metrics that we\nshall use for quantiﬁcation of the complexity of a\nCM dataset.\n3.1 Measuring CM Complexity\nThe CM data, both real and artiﬁcial, can vary\nin the their relative usage and ordering of L1 and\nL2 words, and thereby, signiﬁcantly affect down-\nstream applications like language modeling. We\nuse the following metrics to estimate the amount\nand complexity of code-mixing in the datasets.\nSwitch-point (SP): As deﬁned in the last sec-\ntion, switch-points are points within a sentence\nwhere the languages of the words on the two sides\nare different. Intuitively, sentences that have more\nnumber of SPs are inherently more complex. We\nalso deﬁne the metric SP Fraction (SPF) as the\nnumber of SP in a sentence divided by the total\nnumber of word boundaries in the sentence.\nCode mixing index (CMI): Proposed by Gam-\nback and Das (2014, 2016), CMI quantiﬁes the\namount of code mixing in a corpus by accounting\nfor the language distribution as well as the switch-\ning between them. Let N be the number of lan-\nguage tokens, xan utterance; let tLi be the tokens\nin language Li, P be the number of code switch-\ning points in x. Then, the Code mixed index per\nutterance, Cu(x) for xcomputed as follows,\nCu(x) = (N(x) −maxLi∈L{tLi }(x)) +P(x)\nN(x) (1)\nNote that all the metrics can be computed at the\nsentence level as well as at the corpus level by av-\neraging the values for all the sentences in a corpus.\n3.2 Real Datasets\nWe chose to conduct all our experiments on\nEnglish-Spanish CM tweets because English-\nSpanish CM is well documented (Solorio and\nLiu, 2008), is one of the most commonly mixed\nlanguage pairs on social media (Rijhwani et al.,\n2017), and a couple of CM tweet datasets are read-\nily available (Solorio et al., 2014; Rijhwani et al.,\n2017).\n1547\nDataset # Tweets # Words CMI SPF\nMono\nEnglish 100K 850K (48K) 0 0\nSpanish 100K 860K (61K) 0 0\nrCM\nTrain 100K 1.4M (91K) 0.31 0.105\nValidation 100K 1.4M (91K) 0.31 0.106\nTest-17 83K 1.1M (82K) 0.31 0.104\nTest-14 13K 138K (16K) 0.12 0.06\ngCM 31M 463M (79K) 0.75 0.35\nTable 1: Size of the datasets. Numbers in paren-\nthesis show the vocabulary size, i.e., the no. of\nunique words.\nFor our experiments, we use a subset of the\ntweets collected by Rijhwani et al. (2017) that\nwere automatically identiﬁed as English, Span-\nish or English-Spanish CM. The authors provided\nus around 4.5M monolingual tweets per language,\nand 283K CM tweets. These were already dedu-\nplicated and tagged for hashtags, URLs, emoti-\ncons and language labels automatically through\nthe method proposed in the paper. Table 1 shows\nthe sizes of the various datasets, which are also de-\nscribed below.\nMono: 50K tweets were sampled for Spanish\nand English from the entire collection of monolin-\ngual tweets. The Spanish tweets were translated\nto English and vice versa, which gives us a total of\n100K monolingual tweets in each language. We\nshall refer to this dataset as Mono. The sampling\nstrategy and reason for generating translations will\nbecome apparent in Sec. 3.3.\nrCM: We use two real CM datasets in our ex-\nperiment. The 283K real CM tweets provided by\nRijhwani et al. (2017) were randomly divided into\ntraining, validation and test sets of nearly equal\nsizes. Note that for most of our experiments, we\nwill use a very small subset of the training set con-\nsisting of 5000 tweets as train data, because the\nfundamental assumption of this work is that very\nlittle amount of CM data is available for most lan-\nguage pairs (which is in fact true for most pairs\nbeyond some very popularly mixed languages like\nEnglish-Spanish). Nevertheless, the much larger\ntraining set is required for studying the effect of\nvarying the amount of real CM data on our mod-\nels. We shall refer to this training dataset as\nrCM. The test set with 83K tweets will be re-\nferred to as Test-17. We also use another dataset of\nFigure 3: Average number of gCM sentences (y-\naxis) vs mean input sentence length (x-axis)\nEnglish-Spanish CM tweets for testing our mod-\nels which was released during the language la-\nbeling shared task at the Workshop on “Compu-\ntational Approaches to Code-switching, EMNLP\n2014” (Solorio et al., 2014). We mixed the train-\ning, validation and test datasets released during\nthis shared task to construct a set of 13K tweets,\nwhich we shall refer to as Test-14. The two test\ndatasets are tweets that were collected three years\napart, and therefore, will help us estimate the ro-\nbustness of the language models. As shown in Ta-\nble 1, these datasets are quite different in terms\nof CMI and average number of SP per tweet. For\ncomputing the CMI and SP, we used a English-\nSpanish LID to language tag the words. In fact,\n9500 tweets in the Test-14 dataset are monolin-\ngual, but we chose to retain them because it re-\nﬂects the real distribution of CM data. Further,\nTest-14 also has manually annotated language la-\nbels, which will be helpful while conducting an\nin-depth analysis of the models.\n3.3 Synthetic Code-Mixed Data\nAs described in the previous section, we use par-\nallel monolingual sentences to generate grammat-\nically valid code mixed sentences. The entire pro-\ncess involves the following four steps.\nStep 1: We created the parallel corpus by gen-\nerating translations for all the monolingual En-\nglish and Spanish tweets (4.5M each) using the\nBing Translator API. 2 We have found, that the\ntranslation quality varies widely across different\nsentences. Thus, we rank the translated sen-\ntences using Pseudo Fuzzy-match Score (PFS)\n2https://www.microsoft.com/en-\nus/translator/translatorapi.aspx\n1548\n(He et al., 2010). First, the forward translation\nengine (eg. English-to-Spanish) translates mono-\nlingual source sentence s into target t. Then the\nreverse translation system (eg. Spanish-English)\ntranslates target t into pseudo source s′. Equa-\ntion 2 computes the PFS between sand s′.\nPFS = EditDistance(s,s′)\nmax(|s|,|s′|) (2)\nAfter manual inspection, we decided to select\ntranslation pairs whose PFS ≤0.7. The edit dis-\ntance is based on Wagner and Fischer (1974).\nStep 2: We used the fast align toolkit3\n(Dyer et al., 2013), to generate the word align-\nments from these parallel sentences.\nStep 3: The constituency parses for all the\nEnglish tweets were obtained using the Stanford\nPCFG parser (Klein and Manning, 2003).\nStep 4:Using the parallel sentences, alignments\nand parse trees, we apply the Equivalent constraint\ntheory (Sec 2.2) to generate all syntactically valid\nCM sentences while allowing for lexical substitu-\ntion.\nWe randomly selected 50K monolingual Span-\nish and English tweets whose PFS ≤ 0.7. This\ngave us 200K monolingual tweets in all ( Mono\ndataset) and the total amount of generated CM\nsentences from these 100K translation pairs was\n31M, which we shall refer to as gCM. Note that\neven though we consider the Mono and gCM\nas two separate sets, in reality the EC model\nalso generates the monolingual sentences; further,\nexistence of gCM presumes existence of Mono.\nHence, we also use Mono as part of all training\nexperiments which use gCM.\nWe would also like to point out that the choice\nof experimenting with a much smaller set of\ntweets, only 50K per language, was made because\nthe number of generated tweets even from this\nsmall set of monolingual tweet pairs is almost pro-\nhibitively large to allow experimentation with sev-\neral models and their respective conﬁgurations.\n4 Approach\nLanguage modeling is a very widely researched\ntopic (Rosenfeld, 2000; Bengio et al., 2003; Sun-\ndermeyer et al., 2015). In recent times, deep learn-\ning has been successfully employed to build ef-\nﬁcient LMs (Mikolov et al., 2010; Sundermeyer\net al., 2012; Arisoy et al., 2012; Che et al., 2017).\n3https://github.com/clab/fast align\nBaheti et al. (2017) recently showed that there is\nsigniﬁcant effect of the training curriculum, that is\nthe order in which data is presented to an RNN-\nbased LM, on the perplexity of the learnt English-\nSpanish CM language model on tweets. Along\nsimilar lines, in this study we focus our experi-\nments on training curriculum, especially regarding\nthe use of gCM data during training, which is the\nprimary contribution of this paper.\nWe do not attempt to innovate in terms of the\narchitecture or computational structure of the LM,\nand use a standard LSTM-based RNN LM (Sun-\ndermeyer et al., 2012) for all our experiments. In-\ndeed, there are enough reasons to believe that CM\nlanguage is not fundamentally different from non-\nCM language, and therefore, should not require an\naltogether different LM architecture. Rather, the\ndifference arises in terms of added complexity due\nto the presence of lexical items and syntactic struc-\ntures from two linguistic systems that blows up the\nspace of valid grammatical and lexical conﬁgura-\ntions, which makes it essential to train the models\non large volumes of data.\n4.1 Training Curricula\nBaheti et al. (2017) showed that rather than ran-\ndomly mixing the monolingual and CM data dur-\ning training, the best performance is achieved\nwhen the LM is ﬁrst trained with a mixture of\nmonolingual texts from both languages in nearly\nequal proportions, and ending with CM data. Mo-\ntivated by this ﬁnding, we deﬁne the following ba-\nsic training curricula (“X |Y” indicates training\nthe model ﬁrst with data X and then data Y):\n(1) rCM, (2) Mono, (3) Mono |rCM,\n(4a) Mono |gCM, (4b) gCM |Mono,\n(5a) Mono |gCM |rCM,\n(5b) gCM |Mono |rCM\nCurricula 1-3 are baselines, where gCM data is\nnot used. Note that curriculum 3 is the best case\naccording to Baheti et al. (2017). Curricula 4a and\n4b help us examine how far generated data can\nsubstitute real data. Finally, curricula 5a and 5b\nuse all the data, and we would expect them to per-\nform the best.\nNote that we do not experiment with other po-\ntential combinations (e.g., rCM|gCM |Mono) be-\ncause it is known (and we also see this in our ex-\nperiments) that adding rCM data at the end always\nleads to better models.\n1549\nFigure 4: Scatter plot of fractional increase in\nword frequency in gCM (y-axis) vs original fre-\nquency (x-axis).\n4.2 Sampling from gCM\nAs we have seen in Sec 3.3 (Fig. 3), in the EC\nmodel, a pair of monolingual parallel tweets gives\nrise to a large number (typically exponential in the\nlength of the tweet) of CM tweets. On the other\nhand, in reality, only a few of those tweets would\nbe observed. Further, if all the generated sentences\nare used to train an LM, it is not only computation-\nally expensive, it also leads to undesirable results\nbecause the statistical properties of the distribution\nof the gCM corpus is very different from real data.\nWe see this in our experiments (not reported in\nthis paper for paucity of space), and also in Fig 4,\nwhere we plot the ratio of the frequencies of the\nwords in gCM and Mono corpora (y-axis) against\ntheir original frequencies in Mono (x-axis). We\ncan clearly see that the frequencies of the words\nare scaled up non-uniformly, the ratios varying be-\ntween 1 and 500,000 for low frequency words.\nIn order to reduce this skew, instead of select-\ning the entire gCM data, we propose three sam-\npling techniques for creating the training data from\ngCM:\nRandom: For each monolingual pair of parallel\ntweets, we randomly pick a ﬁxed number, k, of\nCM tweets. We shall refer to the resultant training\ncorpus as χ-gCM.\nCMI-based: For each monolingual pair of par-\nallel tweets, we randomly pick k CM tweets and\nbucket them using CMI (in 0.1 intervals). Thus,\nin this case we can deﬁne two different curric-\nula, where we present the data in increasing or\ndecreasing order of CMI during training, which\nwill be represented by the notations ↑-gCM and\n↓-gCM respectively.\nSPF-based: For each monolingual pair of par-\nallel tweets, we randomly pick kCM tweets such\nthat the SPF distribution (section 3.1) of these\ntweets is similar to that of rCM data (as estimated\nfrom the validation set). This strategy will be re-\nferred to as ρ-gCM.\nThus, depending on the gCM sampling strategy\nused, curricula 4a-b and 5a-b can have three differ-\nent versions each. Note that since CMI for Mono\nis 0, ↑-gCM is not meaningful for 4b and 5b and\nsimilarly, ↓-gCM not for 4a and 5a.\n5 Experiments and Results\nFor all our experiments, we use a 2 layered RNN\nwith LSTM units and hidden layer dimension of\n100. While training, we use sampled softmax with\n5000 samples instead of a full softmax to speed\nup the training process. The sampling is based on\nthe word frequency in the training corpus. We use\nmomentum SGD with a learning rate of 0.002. We\nhave used the CNTK toolkit for building our mod-\nels.4 We use a ﬁxed k=5 (from each monolingual\npair) for sampling the gCM data. We observed the\nperformance on ↑-gCM to be the best when trained\ntill CMI 0.4 and similarly on ↓-gCM when trained\nfrom 1.0 to 0.6.\n5.1 Results\nTable 2 presents the perplexities on validation,\nTest-14 and Test-17 datasets for all the models\n(Col. 3, 4 and 5). We observe the following\ntrends: (1) Model 5(b)- ρ has the least perplex-\nity value (signiﬁcantly different from the second\nlowest value in the column, p < 0.00001 for a\npaired t-test). (2) There is 55 and 90 point re-\nduction in perplexity on Test-17 and Test-14 sets\nrespectively from the baseline experiment 3, that\ndoes not use gCM data. Thus, addition of gCM\ndata is helpful. (3) Only the 4a and 4b models are\nworse than 3, while 5a and 5b models are better.\nHence, rCM is indispensable, even though gCM\nhelps. (4) SPF based sampling performs signiﬁ-\ncantly better (again p< 0.00001) than other sam-\npling techniques.\nTo put these numbers in perspective, we also\ntrained our model on 50k monolingual English\ndata, which gave a PPL of 264. This shows that\nthe high PPL values our models obtain are due\nto the inherent complexity of modeling CM lan-\nguage. This is further substantiated by the PPL\n4https://www.microsoft.com/en-us/cognitive-toolkit/\n1550\nID Training curriculum Overall PPL Avg. SP PPL\nValid Test-17 Test-14 Valid Test-17 Test-14\n1 rCM 1995 2018 1822 5598 5670 8864\n2 Mono 1588 1607 892 23378 23790 26901\n3 Mono |rCM 1029 1041 861 4734 4824 7913\n4(a)-χ Mono |χ-gCM 1749 1771 1119 5752 5869 6065\n4(a)-↑ Mono |↑-gCM 1852 1872 1208 9074 9167 8803\n4(a)-ρ Mono |ρ-gCM 1599 1618 1116 6534 6618 7293\n4(b)-χ χ-gCM |Mono 1659 1680 903 20634 21028 20300\n4(b)-↓ ↓-gCM |Mono 1900 1917 973 28422 28722 25006\n4(b)-ρ ρ-gCM |Mono 1622 1641 871 26191 26710 22557\n5(a)-χ Mono |χ-gCM |rCM 1026 1038 836 4317 4386 5958\n5(a)-↑ Mono |↑-gCM |rCM 1045 1058 961 4983 5078 6861\n5(a)-ρ Mono |ρ-gCM |rCM 999 1011 830 4736 4829 6807\n5(b)-χ χ-gCM |Mono |rCM 1006 1019 790 4878 4987 7018\n5(b)-↓ ↓-gCM |Mono |rCM 1012 1025 800 5396 5489 7476\n5(b)-ρ ρ-gCM |Mono |rCM 976 986 772 4810 4912 6547\nTable 2: Perplexity of the LM Models on all tweets and only on SP (right block).\nRL 3 5(a)-χ 5(a)-ρ 5(a)-↑ 5(b)-↓ 5(b)-χ 5(b)-ρ\n1 13222128151371714017137611349413077\n2 2201 2120 2064 2078 2155 2256 2108\n3 970 926 902 896 914 966 911\n4 643 594 567 575 573 608 571\n5 574 540 509 517 502 553 503\n6 593 545 529 543 520 566 529\n≥7 507 465 444 460 431 479 440\nTable 3: Perplexities of minor language runs for\nvarious run lengths on Test-17.\n# rCM 0.5K 1K 2.5K 5K 10K 50K\n3 1238 1186 1120 1041 991 812\n5(b)-ρ 1181 1141 1068 986 951 808\nTable 4: Perplexity variation on Test-17 with\nchanges in amount of rCM train data. Similar\ntrends for other models (left for paucity of space)\nvalues computed only at the code-switch points,\nwhich are shown in Table 2, col. 6, 7 and 8. Even\nfor the best model, which in this case is 5(a)-χ,\nPPL is four times higher than the overall PPL on\nTest-17.\nRun length: The complexity of modeling CM\nis also apparent from Table 3, which reports the\nperplexity value of the 3 and 5 models for mono-\nlingual fragments of various run lengths. We de-\nﬁne run lengthas the number of words in a max-\nimal monolingual fragment or run within a tweet.\nIn our analysis, we only consider runs of the em-\nbedded language, deﬁned as the language that has\nfewer words. As one would expect, model 5(a)-\nχperforms the best for run length 1 (recall that it\nhas lowest PPL at SP), but as the run length in-\ncreases, the models sampling the gCM data us-\nSample size (k) 1 2 5 10\n# tweets 93K 184K 497K 952K\n5(b)-ρ 1081 1053 986 1019\nTable 5: Variation of PPL on Test-17 with gCM\nsample size k. Similar trends for other models.\ning CMI (5(a)- ↑and 5(b)- ↓) are better than the\nrandomly sampled ( χ) models. Run length 1 are\ntypically cases of word borrowing and lexical sub-\nstitution; higher run length segments are typically\nan indication of CM. Clearly, modeling the shorter\nruns of the embedded language seems to be one of\nthe most challenging aspect of CM LM.\nSigniﬁcance of Linguistic Constraints: To\nunderstand the importance of the linguistic con-\nstraints imposed by EC on generation of gCM, we\nconducted an experiment where a synthetic CM\ncorpus was created by combining random contigu-\nous segments from the monolingual tweets such\nthat the generated CM tweets’ SPF distribution\nmatched that of rCM. When we replaced gCM by\nthis corpus in5(b)-ρ, the PPL on test-17 was 1060,\nwhich is worse than the baseline PPL.\nEffect of rCM size: Table 4 shows the PPL\nvalues for models 3 and 5(b)- ρwhen trained with\ndifferent amounts of rCM data, keeping other pa-\nrameters constant. As expected, the PPL drops for\nboth models as rCM size increases. However, even\nwith high rCM data, gCM does help in improv-\ning the LM until we have 50k rCM data (compa-\nrable to monolingual, and an unrealistic scenario\nin practice), where the returns of adding gCM\nstarts diminishing. We also observe that in gen-\n1551\neral, model 3 needs twice the amount of rCM data\nto perform as well as model 5(b)-ρ.\nEffect of gCM size: In our sampling methods\non gCM data, we ﬁxed our sample size, kas 5 for\nconsistency and feasibility of experiments. To un-\nderstand the effect of k(and hence the size of the\ngCM data), we experimented withk= 1, 2, and 10\nkeeping everything else ﬁxed. Table 5 reports the\nresults for the models 3 and 5(b)- ρ. We observe\nthat unlike rCM data, increasing gCM data or k\ndoes not necessarily decrease PPL after a point.\nWe speculate that there is trade-off between kand\nthe amount of rCM data, and also probably be-\ntween these and the amount of monolingual data.\nWe plan to explore this further in future.\n6 Related Work\nWe brieﬂy describe the various types of ap-\nproaches used for building LM for CM text.\nBilingual models: These models combine data\nfrom monolingual data sources in both languages\n(Weng et al., 1997). Factored models : Geb-\nhardt (2011) uses Factored Language Models for\nrescoring n-best lists during ASR decoding. The\nfactors used include POS tags, CS point prob-\nability and LID. In Adel et al.(2014b; 2014a;\n2013) RNNLMs are combined with n-gram based\nmodels, or converted to backoff models, giv-\ning improvements in perplexity and mixed error\nrate. Models that incorporate linguistic con-\nstraints: Li and Fung (2013) use inversion con-\nstraints to predict CS points and integrates this\nprediction into the ASR decoding process. Li\nand Fung (2014) integrates Functional Head con-\nstraints (FHC) for code-switching into the Lan-\nguage Model for Mandarin-English speech recog-\nnition. This work uses parsing techniques to re-\nstrict the lattice paths during decoding of speech\nto those permissible under the FHC theory. Our\nmethod instead imposes grammatical constraints\n(EC theory) to generate synthetic data, which can\npotentially be used to augment real CM data. This\nallows ﬂexibility to deploy any sophisticated LM\narchitecture and the synthetic data generated can\nalso be used for CM tasks other than speech recog-\nnition. Training curricula for CM: Baheti et al.\n(2017) show that a training curriculum where an\nRNN-LM is trained ﬁrst with interleaved monolin-\ngual data in both languages followed by CM data\ngives the best results for English-Spanish LM. The\nperplexity of this model is 4544, which then re-\nduces to 298 after interpolation with a statistical\nn-gram LM. However, these numbers are not di-\nrectly comparable to our work because the datasets\nare different. Our work is an extension of this ap-\nproach showing that adding synthetic data further\nimproves results.\nWe do not know of any work that uses syntheti-\ncally generated CM data for training LMs.\n7 Conclusion\nIn this paper, we presented a computational\nmethod for generating synthetic CM data based\non the EC theory of code-mixing, and showed\nthat sampling text from the synthetic corpus\n(according to the distribution of SPF found in\nreal CM data) helps in reduction of PPL of the\nRNN-LM by an amount which is equivalently\nachieved by doubling the amount of real CM data.\nWe also showed that randomly generated CM\ndata doesn’t improve the LM. Thus, the linguistic\ntheory based generation is of crucial signiﬁcance.\nThere is no unanimous theory in linguistics on\nsyntactic structure of CM language. Hence, as a\nfuture work, we would like to compare the useful-\nness of different linguistic theories and different\nconstraints within each theory in our proposed\nLM framework. This can also provide an indirect\nvalidation of the theories. Further, we would like\nto study sampling techniques motivated by natural\ndistributions of linguistic structures.\nAcknowledgements\nWe would like to thank the anonymous\nreviewers for their valuable suggestions.\nReferences\nHeike Adel, K. Kirchhoff, N. T. Vu, D. Telaar, and\nT. Schultz. 2014a. Combining recurrent neural net-\nworks and factored language models during decod-\ning of code-switching speech. In INTERSPEECH,\npages 1415–1419.\nHeike Adel, K Kirchhoff, N T Vu, D Telaar, and\nT Schultz. 2014b. Comparing approaches to con-\nvert recurrent neural networks into backoff language\nmodels for efﬁcient decoding. In INTERSPEECH,\npages 651–655.\nHeike Adel, N T Vu, and T Schultz. 2013. Combina-\ntion of recurrent neural networks and factored lan-\nguage models for code-switching language model-\ning. In ACL (2), pages 206–211.\n1552\nEbru Arisoy, Tara N Sainath, Brian Kingsbury, and\nBhuvana Ramabhadran. 2012. Deep neural network\nlanguage models. In Proceedings of the NAACL-\nHLT 2012 Workshop: Will We Ever Really Replace\nthe N-gram Model? On the Future of Language\nModeling for HLT, pages 20–28. Association for\nComputational Linguistics.\nAshutosh Baheti, Sunayana Sitaram, Monojit Choud-\nhury, and Kalika Bali. 2017. Curriculum design for\ncode-switching: Experiments with language iden-\ntiﬁcation and language modeling with deep neural\nnetworks. In Proc. of ICON-2017, Kolkata, India,\npages 65–74.\nUtsab Barman, Amitava Das, Joachim Wagner, and\nJennifer Foster. 2014. Code mixing: A challenge\nfor language identiﬁcation in the language of social\nmedia. In The 1st Workshop on Computational Ap-\nproaches to Code Switching, EMNLP 2014.\nHedi M Belazi, Edward J Rubin, and Almeida Jacque-\nline Toribio. 1994. Code switching and x-bar the-\nory: The functional head constraint. Linguistic in-\nquiry, pages 221–237.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nTong Che, Yanran Li, Ruixiang Zhang, R Devon\nHjelm, Wenjie Li, Yangqiu Song, and Yoshua Ben-\ngio. 2017. Maximum-likelihood augmented discrete\ngenerative adversarial networks. arXiv preprint\narXiv:1702.07983.\nA.-M. DiSciullo, Pieter Muysken, and R. Singh. 1986.\nGovernment and code-mixing. Journal of Linguis-\ntics, 22:1–24.\nChris Dyer, Victor Chahuneau, and N A. Smith. 2013.\nA simple, fast, and effective reparameterization of\nibm model 2. In Proceedings of NAACL-HLT 2013,\npages 644–648. Association for Computational Lin-\nguistics.\nB. Gamback and A Das. 2014. On measuring the com-\nplexity of code-mixing. In Proc. of the 1st Workshop\non Language Technologies for Indian Social Media\n(Social-India).\nB. Gamback and A Das. 2016. Comparing the level of\ncode-switching in corpora. In Proc. of the 10th In-\nternational Conference on Language Resources and\nEvaluation (LREC).\nJan Gebhardt. 2011. Speech recognition on english-\nmandarin code-switching data using factored lan-\nguage models.\nYifan He, Yanjun Ma, Andy Way, and Josef Van Gen-\nabith. 2010. Integrating n-best smt outputs into a\ntm system. In Proceedings of the 23rd International\nConference on Computational Linguistics: Posters,\npages 374–382. Association for Computational Lin-\nguistics.\nA. K. Joshi. 1985. Processing of Sentences with In-\ntrasentential Code Switching. In D. R. Dowty,\nL. Karttunen, and A. M. Zwicky, editors, Natural\nLanguage Parsing: Psychological, Computational,\nand Theoretical Perspectives, pages 190–205. Cam-\nbridge University Press, Cambridge.\nD Klein and CD Manning. 2003. Accurate unlexi-\ncalized parsing. In Proceedings of the 41st annual\nmeeting of the association for computational lin-\nguistics. Association of Computational Linguistics.\nYing Li and P Fung. 2013. Improved mixed lan-\nguage speech recognition using asymmetric acoustic\nmodel and language model with code-switch inver-\nsion constraints. In ICASSP, pages 7368–7372.\nYing Li and P Fung. 2014. Language modeling with\nfunctional head constraint for code switching speech\nrecognition. In EMNLP.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nCarol Myers-Scotton. 1993. Duelling Lan-\nguages:Grammatical structure in Code-switching.\nClarendon Press, Oxford.\nCarol Myers-Scotton. 1995. A lexically based model\nof code-switching. In Lesley Milroy and Pieter\nMuysken, editors, One Speaker, Two Languages:\nCross-disciplinary Perspectives on Code-switching,\npages 233–256. Cambridge University Press, Cam-\nbridge.\nRana D. Parshad, Suman Bhowmick, Vineeta Chand,\nNitu Kumari, and Neha Sinha. 2016. What is India\nspeaking? Exploring the “Hinglish” invasion. Phys-\nica A, 449:375–389.\nShana Poplack. 1980. Sometimes Ill start a sentence in\nSpanish y termino en espaol. Linguistics, 18:581–\n618.\nAmeya Prabhu, Aditya Joshi, Manish Shrivastava, and\nVasudeva Varma. 2016. Towards sub-word level\ncompositions for sentiment analysis of hindi-english\ncode mixed text. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 2482–2491.\nShruti Rijhwani, R Sequiera, M Choudhury, K Bali,\nand C S Maddila. 2017. Estimating code-switching\non Twitter with a novel generalized word-level lan-\nguage identiﬁcation technique. In ACL.\nRonald Rosenfeld. 2000. Two decades of statistical\nlanguage modeling: Where do we go from here?\nProceedings of the IEEE, 88(8):1270–1278.\nKoustav Rudra, S Rijhwani, R Begum, K Bali,\nM Choudhury, and N Ganguly. 2016. Understand-\ning language preference for expression of opinion\n1553\nand sentiment: What do Hindi-English speakers do\non Twitter? In EMNLP, pages 1131–1141.\nDavid Sankoff. 1998. A formal production-based ex-\nplanation of the facts of code-switching. Bilingual-\nism: language and cognition, 1(01):39–50.\nA. Sharma, S. Gupta, R. Motlani, P. Bansal, M. Srivas-\ntava, R. Mamidi, and D.M Sharma. 2016. Shallow\nparsing pipeline for hindi-english code-mixed social\nmedia text. In Proceedings of NAACL-HLT.\nSunayana Sitaram, Sai Krishna Rallabandi, Shruti Ri-\njhwani, and Alan W Black. 2016. Experiments with\ncross-lingual systems for synthesis of code-mixed\ntext. In 9th ISCA Speech Synthesis Workshop.\nThamar Solorio and Yang Liu. 2008. Part-of-speech\ntagging for english-spanish code-switched text. In\nProc. of EMNLP.\nThamar Solorio et al. 2014. Overview for the ﬁrst\nshared task on language identiﬁcation in code-\nswitched data. In 1st Workshop on Computational\nApproaches to Code Switching, EMNLP, pages 62–\n72.\nMartin Sundermeyer, Hermann Ney, and Ralf Schl¨uter.\n2015. From feedforward to recurrent lstm neural\nnetworks for language modeling. IEEE Transac-\ntions on Audio, Speech, and Language Processing,\n23(3):517–529.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. Lstm neural networks for language model-\ning. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association.\nYogarshi Vyas, S Gella, J Sharma, K Bali, and\nM Choudhury. 2014. POS Tagging of English-\nHindi Code-Mixed Social Media Content. In Proc.\nEMNLP, pages 974–979.\nRobert A Wagner and Michael J Fischer. 1974. The\nstring-to-string correction problem. Journal of the\nACM (JACM), 21(1):168–173.\nFuliang Weng, H Bratt, L Neumeyer, and A Stolcke.\n1997. A study of multilingual speech recognition.\nIn EUROSPEECH, volume 1997, pages 359–362.\nCiteseer.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6677930355072021
    },
    {
      "name": "Linguistics",
      "score": 0.5755718946456909
    },
    {
      "name": "Computational linguistics",
      "score": 0.5680376887321472
    },
    {
      "name": "Code (set theory)",
      "score": 0.5325034856796265
    },
    {
      "name": "Code-mixing",
      "score": 0.5223099589347839
    },
    {
      "name": "Natural language processing",
      "score": 0.49869441986083984
    },
    {
      "name": "Mixing (physics)",
      "score": 0.4870786964893341
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3951171040534973
    },
    {
      "name": "Programming language",
      "score": 0.3728331923484802
    },
    {
      "name": "Philosophy",
      "score": 0.1659938395023346
    },
    {
      "name": "Code-switching",
      "score": 0.15223726630210876
    },
    {
      "name": "Physics",
      "score": 0.11663204431533813
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.07344040274620056
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}