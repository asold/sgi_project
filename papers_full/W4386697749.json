{
  "title": "A foundation model for generalizable disease detection from retinal images",
  "url": "https://openalex.org/W4386697749",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5087083706",
      "name": "Yukun Zhou",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5021802360",
      "name": "Mark A. Chia",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5028939056",
      "name": "Siegfried Wagner",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5039273153",
      "name": "Murat Seçkin Ayhan",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5041777120",
      "name": "Dominic J. Williamson",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5003439740",
      "name": "Robbert Struyven",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5048259658",
      "name": "Timing Liu",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5073094674",
      "name": "Moucheng Xu",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5057549396",
      "name": "Mateo Gende",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5023795910",
      "name": "Peter Woodward-Court",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5031053936",
      "name": "Yuka Kihara",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5073405229",
      "name": "Naomi E. Allen",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A5060591968",
      "name": "John Gallacher",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A5076322041",
      "name": "Thomas J. Littlejohns",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A5005781424",
      "name": "Tariq Aslam",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A5075704676",
      "name": "Paul N. Bishop",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A5033873486",
      "name": "Graeme C. Black",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A5085779691",
      "name": "Panagiotis I. Sergouniotis",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A5031321430",
      "name": "Denize Atan",
      "affiliations": [
        "University of Bristol"
      ]
    },
    {
      "id": "https://openalex.org/A5029898835",
      "name": "Andrew D. Dick",
      "affiliations": [
        "University of Bristol"
      ]
    },
    {
      "id": "https://openalex.org/A5015690005",
      "name": "Cathy Williams",
      "affiliations": [
        "University of Bristol"
      ]
    },
    {
      "id": "https://openalex.org/A5036785197",
      "name": "Sarah Barman",
      "affiliations": [
        "Kingston University"
      ]
    },
    {
      "id": "https://openalex.org/A5067571294",
      "name": "Jennifer H. Barrett",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A5107303443",
      "name": "Sarah Mackie",
      "affiliations": [
        "University of Leeds"
      ]
    },
    {
      "id": "https://openalex.org/A5082582215",
      "name": "Tasanee Braithwaite",
      "affiliations": [
        "St Thomas' Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5091381850",
      "name": "Roxana O. Carare",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A5002940859",
      "name": "Sarah Ennis",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A5080984588",
      "name": "Jane Whitney Gibson",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A5072541133",
      "name": "Andrew Lotery",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A5068083311",
      "name": "Jay Self",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A5019521414",
      "name": "Usha Chakravarthy",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5048960834",
      "name": "Ruth Hogg",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5080626306",
      "name": "Euan Paterson",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5031466793",
      "name": "Jayne V. Woodside",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5018227698",
      "name": "Tünde Pető",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5054848879",
      "name": "Gareth J. McKay",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5050397593",
      "name": "Bernadette McGuinness",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5060343222",
      "name": "Paul J. Foster",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5015881286",
      "name": "Konstantinos Balaskas",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5056428570",
      "name": "Anthony P. Khawaja",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5083903120",
      "name": "Nikolas Pontikos",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5048391596",
      "name": "Jugnoo S. Rahi",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5018960148",
      "name": "Gerassimos Lascaratos",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5067795081",
      "name": "Praveen J. Patel",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5107459387",
      "name": "Michelle Chan",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5086349230",
      "name": "Sharon Chua",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5001428225",
      "name": "Alexander Day",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5022224665",
      "name": "Parul Desai",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5041956661",
      "name": "Cathy Egan",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5010153509",
      "name": "Marcus Fruttiger",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5026703795",
      "name": "David F. Garway‐Heath",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5068050684",
      "name": "Alison J. Hardcastle",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5069175872",
      "name": "Peng T. Khaw",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5041946054",
      "name": "Tony Moore",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5066434934",
      "name": "Sobha Sivaprasad",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5035391535",
      "name": "Nicholas G. Strouthidis",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5078832824",
      "name": "Dhanes Thomas",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5016588157",
      "name": "Adnan Tufail",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5105819907",
      "name": "Ananth C. Viswanathan",
      "affiliations": [
        "Moorfields Eye Hospital",
        "Moorfields Eye Hospital NHS Foundation Trust",
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5109568597",
      "name": "Bal Dhillon",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5078119193",
      "name": "Tom MacGillivray",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5081850154",
      "name": "Cathie Sudlow",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5071827492",
      "name": "Véronique Vitart",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A5014733082",
      "name": "Alex S. F. Doney",
      "affiliations": [
        "University of Dundee"
      ]
    },
    {
      "id": "https://openalex.org/A5048954811",
      "name": "Emanuele Trucco",
      "affiliations": [
        "University of Dundee"
      ]
    },
    {
      "id": "https://openalex.org/A5092860037",
      "name": "Jeremy A. Guggeinheim",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A5013343091",
      "name": "James P. Morgan",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A5101683595",
      "name": "Christopher J. Hammond",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A5002448265",
      "name": "Katie Williams",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A5016454136",
      "name": "Pirro G. Hysi",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A5072357388",
      "name": "Simon Harding",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A5081186911",
      "name": "Yalin Zheng",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A5059386391",
      "name": "Robert Luben",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5027846238",
      "name": "Philip J. Luthert",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5088378966",
      "name": "Zihan Sun",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5023564113",
      "name": "Martin McKibbin",
      "affiliations": [
        "Leeds Teaching Hospitals NHS Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5101730044",
      "name": "Eoin O’Sullivan",
      "affiliations": [
        "King's College Hospital NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5015558687",
      "name": "Richard A. Oram",
      "affiliations": [
        "University of Exeter"
      ]
    },
    {
      "id": "https://openalex.org/A5107942330",
      "name": "Mike Weedon",
      "affiliations": [
        "University of Exeter"
      ]
    },
    {
      "id": "https://openalex.org/A5042801829",
      "name": "Christopher G. Owen",
      "affiliations": [
        "University of London"
      ]
    },
    {
      "id": "https://openalex.org/A5077337083",
      "name": "Alicja R. Rudnicka",
      "affiliations": [
        "University of London"
      ]
    },
    {
      "id": "https://openalex.org/A5078498803",
      "name": "Naveed Sattar",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A5039042181",
      "name": "David Steel",
      "affiliations": [
        "Newcastle University"
      ]
    },
    {
      "id": "https://openalex.org/A5086462179",
      "name": "Irene Stratton",
      "affiliations": [
        "Gloucestershire Hospitals NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5072458761",
      "name": "Robyn J. Tapp",
      "affiliations": [
        "St George's, University of London"
      ]
    },
    {
      "id": "https://openalex.org/A5004345635",
      "name": "Max Yates",
      "affiliations": [
        "University of East Anglia"
      ]
    },
    {
      "id": "https://openalex.org/A5061751725",
      "name": "Axel Petzold",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5081957210",
      "name": "Savita Madhusudhan",
      "affiliations": [
        "Royal Liverpool University Hospital",
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A5069743881",
      "name": "André Altmann",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5029283104",
      "name": "Aaron Lee",
      "affiliations": [
        "The Retina Center",
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5084515381",
      "name": "Eric J. Topol",
      "affiliations": [
        "Scripps (United States)",
        "Scripps Institution of Oceanography"
      ]
    },
    {
      "id": "https://openalex.org/A5074056939",
      "name": "Alastair K. Denniston",
      "affiliations": [
        "University Hospitals Birmingham NHS Foundation Trust"
      ]
    },
    {
      "id": "https://openalex.org/A5033449704",
      "name": "Daniel C. Alexander",
      "affiliations": [
        "University College London"
      ]
    },
    {
      "id": "https://openalex.org/A5003259452",
      "name": "Pearse A. Keane",
      "affiliations": [
        "University College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205164650",
    "https://openalex.org/W3007935259",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W2895763047",
    "https://openalex.org/W2976398475",
    "https://openalex.org/W2886281300",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W4291023040",
    "https://openalex.org/W343636949",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W3006354677",
    "https://openalex.org/W3091908957",
    "https://openalex.org/W2810823899",
    "https://openalex.org/W7075662349",
    "https://openalex.org/W2147844305",
    "https://openalex.org/W2810823800",
    "https://openalex.org/W7074026734",
    "https://openalex.org/W2126126493",
    "https://openalex.org/W4248116761",
    "https://openalex.org/W2927432035",
    "https://openalex.org/W4206961805",
    "https://openalex.org/W4281254305",
    "https://openalex.org/W4379878309",
    "https://openalex.org/W3043108808",
    "https://openalex.org/W4200331648",
    "https://openalex.org/W3102174132",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4286267143",
    "https://openalex.org/W2557738935",
    "https://openalex.org/W2788633781",
    "https://openalex.org/W4220912223",
    "https://openalex.org/W2895486342",
    "https://openalex.org/W3036224891",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2977650145",
    "https://openalex.org/W2073244572",
    "https://openalex.org/W1969496006",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W1594870591",
    "https://openalex.org/W2093690346",
    "https://openalex.org/W2090496521",
    "https://openalex.org/W4283456219",
    "https://openalex.org/W2581082771",
    "https://openalex.org/W2972243697",
    "https://openalex.org/W3028394061",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4282048668",
    "https://openalex.org/W2903117925",
    "https://openalex.org/W3191934798",
    "https://openalex.org/W2905493020",
    "https://openalex.org/W2996651271",
    "https://openalex.org/W2762741128",
    "https://openalex.org/W4281695693",
    "https://openalex.org/W4285387118",
    "https://openalex.org/W2962858109"
  ],
  "abstract": null,
  "full_text": "156 | Nature | Vol 622 | 5 October 2023\nArticle\nA foundation model for generalizable \ndisease detection from retinal images\nYukun Zhou1,2,3 ✉, Mark A. Chia2,4, Siegfried K. Wagner2,4, Murat S. Ayhan1,2,4, \nDominic J. Williamson1,2,4, Robbert R. Struyven1,2,4, Timing Liu2, Moucheng Xu1,3, \nMateo G. Lozano2,5, Peter Woodward-Court1,2,6, Yuka Kihara7,8, UK Biobank Eye & Vision \nConsortium*, Andre Altmann1,3, Aaron Y . Lee7,8, Eric J. Topol9, Alastair K. Denniston10,11, \nDaniel C. Alexander1,12 & Pearse A. Keane2,4 ✉\nMedical artificial intelligence (AI) offers great potential for recognizing signs of health \nconditions in retinal images and expediting the diagnosis of eye diseases and systemic \ndisorders1. However, the development of AI models requires substantial annotation \nand models are usually task-specific with limited generalizability to different clinical \napplications\n2. Here, we present RETFound, a foundation model for retinal images that \nlearns generalizable representations from unlabelled retinal images and provides a \nbasis for label-efficient model adaptation in several applications. Specifically, RETFound \nis trained on 1.6 million unlabelled retinal images by means of self-supervised learning \nand then adapted to disease detection tasks with explicit labels. We show that adapted \nRETFound consistently outperforms several comparison models in the diagnosis and \nprognosis of sight-threatening eye diseases, as well as incident prediction of complex \nsystemic disorders such as heart failure and myocardial infarction with fewer labelled \ndata. RETFound provides a generalizable solution to improve model performance and \nalleviate the annotation workload of experts to enable broad clinical AI applications \nfrom retinal imaging.\nMedical artificial intelligence (AI) has achieved significant progress in \nrecent years with the notable evolution of deep learning techniques1,3,4. \nFor instance, deep neural networks have matched or surpassed the \naccuracy of clinical experts in various applications5, such as referral \nrecommendations for sight-threatening retinal diseases6 and pathol-\nogy detection in chest X-ray images7. These models are typically devel-\noped using large volumes of high-quality labels, which requires expert \nassessment and laborious workload1,2. However, the scarcity of experts \nwith domain knowledge cannot meet such an exhaustive requirement, \nleaving vast amounts of medical data unlabelled and unexploited.\nSelf-supervised learning (SSL) aims to alleviate data inefficiency by \nderiving supervisory signals directly from data, instead of resorting \nto expert knowledge by means of labels8–11. SSL trains models to per-\nform ‘pretext tasks’ for which labels are not required or can be gener-\nated automatically. This process leverages formidable amounts of \nunlabelled data to learn general-purpose feature representations that \nadapt easily to more specific tasks. Following this pretraining phase, \nmodels are fine-tuned to specific downstream tasks, such as classifica-\ntion or segmentation. The SSL model has outperformed supervised \nlearning-based transfer learning (for example, pretraining the models \nwith ImageNet12 and categorical labels) in various computer vision \ntasks, even when the SSL models are fine-tuned with smaller amounts \nof data13,14. Besides this label efficiency, SSL-based models perform \nbetter than supervised models when tested on new data from different \ndomains15,16. The combined qualities of strong generalization capac-\nity of representations, and high performance achieved by fine-tuned \nmodels in many downstream tasks, indicate the great potential of SSL in \nmedical AI in which data are abundant and healthcare tasks are diverse \nbut labels are scarce1,8.\nColour fundus photography (CFP) and optical coherence tomogra-\nphy (OCT) are the most common imaging modalities in ophthalmology \nand such retinal images accumulate quickly in routine clinical practice. \nIn addition to illustrating clinical features associated with ocular dis-\neases, these images also provide valuable insights into systemic dis-\neases, a field that has recently been termed ‘oculomics’17,18. For example, \nthe optic nerve and inner retinal layers provide a non-invasive view of \ncentral nervous system tissue19–21, and thus a window into neurodegen-\neration. Similarly, retinal vascular geometry provides insights into other \nvascular organ systems22–25, such as the heart and kidneys. Although \nseveral studies have shown that SSL can increase performance for indi-\nvidual ocular disease detection tasks, such as the diagnosis of diabetic \nmacular oedema26, age-related macular degeneration (AMD)27 and \nreferable diabetic retinopathy28–30, there has been limited work dem-\nonstrating the ability of a single SSL pretrained model to generalize to a \ndiverse range of complex tasks. Progress has probably been hampered \nby the challenges involved with curating a large repository of retinal \nhttps://doi.org/10.1038/s41586-023-06555-x\nReceived: 5 December 2022\nAccepted: 18 August 2023\nPublished online: 13 September 2023\nOpen access\n Check for updates\n1Centre for Medical Image Computing, University College London, London, UK. 2NIHR Biomedical Research Centre at Moorfields Eye Hospital NHS Foundation Trust, London, UK. 3Department \nof Medical Physics and Biomedical Engineering, University College London, London, UK. 4Institute of Ophthalmology, University College London, London, UK. 5Department of Computer \nScience, University of Coruña, A Coruña, Spain. 6Institute of Health Informatics, University College London, London, UK. 7Department of Ophthalmology, University of Washington, Seattle, WA, \nUSA. 8Roger and Angie Karalis Johnson Retina Center, University of Washington, Seattle, WA, USA. 9Department of Molecular Medicine, Scripps Research, La Jolla, CA, USA. 10Academic Unit of \nOphthalmology, University of Birmingham, Birmingham, UK. 11University Hospitals Birmingham NHS Foundation Trust, Birmingham, UK. 12Department of Computer Science, University College \nLondon, London, UK. *A list of authors and their affiliations appears at the end of the paper. ✉e-mail: yukun.zhou.19@ucl.ac.uk; p.keane@ucl.ac.uk\nNature | Vol 622 | 5 October 2023 | 157\nimages with extensive linkage to several relevant disease outcomes. \nMoreover, the capabilities of different SSL approaches (contrastive SSL \nversus generative SSL) and the interpretability of SSL models in retinal \nimaging, remain relatively under-explored. Developing an understand-\ning of the specific features that SSL models learn during training is an \nimportant step for safe and reliable translation to clinical practice.\nIn this work, we present a new SSL-based foundation model for retinal \nimages (RETFound) and systematically evaluate its performance and \ngeneralizability in adapting to many disease detection tasks. A foun-\ndation model is defined as a large AI model trained on a vast quantity \nof unlabelled data at scale resulting in a model that can be adapted to \na wide range of downstream tasks31,32. Here we construct RETFound \nfrom large-scale unlabelled retinal images by means of SSL and use it to \npromote the detection of many diseases. Specifically, we develop two \nseparate RETFound models, one using CFP and the other using OCT, \nby means of an advanced SSL technique (masked autoencoder15) suc-\ncessively on natural images (ImageNet-1k) followed by retinal images \nfrom the Moorfields diabetic image dataset (MEH-MIDAS) and public \ndata (totalling 904,170 CFPs and 736,442 OCTs). We adapt RETFound \nto a series of challenging detection and prediction tasks by fine-tuning \nRETFound with specific task labels, and then validate its performance. \nWe consider first the diagnostic classification of ocular diseases, \nincluding diabetic retinopathy and glaucoma; second, ocular disease \nprognosis, specifically conversion of contralateral (‘fellow’) eyes to \nneovascular (‘wet’) AMD in a 1-year time period and, finally, oculomic \nchallenges, specifically the 3-year prediction of cardiovascular dis-\neases (ischaemic stroke, myocardial infarction and heart failure) and a \nneurodegenerative disease (Parkinson’s disease). RETFound achieves \nconsistently superior performance and label efficiency in adapting to \nthese tasks, compared to state-of-the-art competing models, including \nthat pretrained on ImageNet-21k with traditional transfer learning. \nWe also probe the interpretation of disease detection performance of  \nRETFound with qualitative results and variable-controlling experi-\nments, showing that salient image regions reflect established know-\nledge from ocular and oculomic literature. Finally, we make RETFound \npublicly available so others can use it as the basis for their own down-\nstream tasks, facilitating diverse ocular and oculomic research.\nFigure 1 gives an overview of the construction and application of \nRETFound. For construction of RETFound, we curated 904,170 CFP \nin which 90.2% of images came from MEH-MIDAS and 9.8% from  \nKaggle EyePACS33, and 736,442 OCT in which 85.2% of them came from \nMEH-MIDAS and 14.8% from ref. 34. MEH-MIDAS is a retrospective data-\nset that includes the complete ocular imaging records of 37,401 patients \nwith diabetes who were seen at Moorfields Eye Hospital between  \nJanuary 2000 and March 2022. After self-supervised pretraining on these \nretinal images, we evaluated the performance and generalizability of \nRETFound in adapting to diverse ocular and oculomic tasks. We selected \npublicly available datasets for the tasks of ocular disease diagnosis. \nDetails are listed in Supplementary Table 1. For the tasks of ocular disease \nprognosis and systemic disease prediction, we used a cohort from the \nMoorfields AlzEye study (MEH-AlzEye) that links ophthalmic data of \n353,157 patients, who attended Moorfields Eye Hospital between 2008 \nand 2018, with systemic disease data from hospital admissions across  \nthe whole of England35. We also used UK Biobank36 for external evalua-\ntion in predicting systemic diseases. The validation datasets used for \nocular disease diagnosis are sourced from several countries, whereas \nsystemic disease prediction was solely validated on UK datasets due to \nlimited availability of this type of longitudinal data. Our assessment of \ngeneralizability for systemic disease prediction was therefore based on \nmany tasks and datasets, but did not extend to vastly different geograph-\nical settings. Details of the clinical datasets are listed in Supplementary  \nTable 2 (data selection is introduced in the Methods section).\nWe compared the performance and label efficiency of RETFound  \nagainst three pretrained comparison models: SL-ImageNet, \nSSL-ImageNet and SSL-Retinal. All models use differing pretraining \nstrategies but have the same model architecture as well as fine-tuning \nprocesses for downstream tasks (architecture details are introduced \nin the Methods section). SL-ImageNet uses traditional transfer learn-\ning, that is, pretraining the model by means of supervised learning on \nImageNet-21k (about 14 million natural images with categorical labels); \nSSL-ImageNet pretrains the model by means of SSL on ImageNet-1k \n(about 1.4 million natural images) and SSL-Retinal pretrains the model \nusing SSL on retinal images from scratch. RETFound uses the weights \nof SSL-ImageNet as a baseline before extending to retinal images \n(equivalent to pretraining the model by means of SSL successively on \nnatural images followed by retinal images). The pretraining schemat-\nics are shown in Extended Data Fig. 1. Furthermore, we explored the \nperformance of using different SSL strategies, that is, generative SSL \nversus contrastive SSL approaches, by substituting the primary SSL \ntechnique (that is, masked autoencoder) for SimCLR16, SwAV37, DINO38 \nand MoCo-v3 (ref. 14) within the RETFound framework, respectively. \nWe reported internal and external evaluation results for these mod-\nels. The models were adapted to each task with labelled training data, \nand evaluated on both held-out internal test sets, as well as external \ndatasets completely distinct from the training data (details are listed \nin the Methods section). Model performance was reported using the \narea under the receiver operating curve (AUROC) and area under the \nprecision-recall curve (AUPR). We calculated P values with the two-sided \nt-test between RETFound and the most competitive comparison model \nfor each task to check for significance.\nOcular disease diagnosis\nWe included eight publicly available datasets to verify the model’s \nperformance on several ocular diseases and imaging conditions (Fig. 2). \nRETFound generally achieved the best performance in most datasets \nand SL-ImageNet ranked second, as shown in Fig. 2a. For instance, on \ndiabetic retinopathy classification, RETFound achieved AUROC of  \n0.943 (95% confidence interval (CI) 0.941, 0.944), 0.822 (95% CI 0.815, \n0.829) and 0.884 (95% CI 0.88, 0.887), respectively, on Kaggle APTOS-\n2019, IDRID39 and MESSIDOR-2 (refs. 40,41) datasets, significantly out-\nperforming SL-ImageNet (all P < 0.001). The superior performance can \nalso be observed for glaucoma and the classification of many diseases. \nThe AUPR results of RETFound were also significantly higher than the \ncompared groups (Extended Data Fig. 2a). For external evaluation, \nwe evaluated the performance of RETFound on diabetic retinopathy \ndatasets (Kaggle APTOS-2019, IDRID and MESSIDOR-2), which were both \nlabelled on the basis of the five-stage International Clinical Diabetic \nRetinopathy Severity scale. We conducted cross evaluation among the \nthree datasets, that is, fine-tuned models on one dataset and evaluated \nthem on the others. RETFound achieved the best performance in all \ncross evaluations, as shown in Fig. 2b. For instance, when fine-tuned \non Kaggle APTOS-2019, RETFound achieved AUROC of 0.822 (95% CI \n0.815, 0.829) and 0.738 (95% CI 0.729, 0.747), respectively, on IDRID \nand MESSIDOR-2 datasets, statistically significantly higher than \nSL-ImageNet (P < 0.001) on IDRID and SSL-ImageNet (P < 0.001) on \nMESSIDOR-2. The AUPR results of all groups were low but RETFound \nachieved significantly higher performance (Extended Data Fig. 2b). All \nquantitative results are listed in Supplementary Table 3.\nOcular disease prognosis\nFor 1-year prognosis of fellow eye converting to wet-AMD, we evalu-\nated the internal performance on data from AlzEye (Fig. 2c). With CFP \nas the input modality, RETFound showed the best performance with \nan AUROC of 0.862 (95% CI 0.86, 0.865), significantly outperform-\ning the comparison groups (P < 0.001). The runner-up SL-ImageNet \nachieved an AUROC of 0.83 (95% CI 0.825, 0.836). With OCT, RETFound \nscored the highest AUROC of 0.799 (95% CI 0.796, 0.802), showing a \nstatistically significantly higher AUROC (P < 0.001) than SSL-Retinal. \n158 | Nature | Vol 622 | 5 October 2023\nArticle\nThe AUPR results of RETFound are highest with CFP and comparable \nto SSL-Retinal with OCT (Extended Data Fig. 2c).\nSystemic diseases prediction\nWe organized four oculomic tasks to evaluate the model perfor -\nmance in predicting the incidence of systemic diseases with retinal \nimages (Fig. 3). Although the overall performance was limited in these  \nchallenging tasks, RETFound has shown significant improvement in \ninternal evaluation for both CFP and OCT, as shown in Fig. 3a. For the \nprediction of myocardial infarction with CFP, RETFound achieved \nAUROC of 0.737 (95% CI 0.731, 0.743). SSL-Retinal scored the second-best \nperformance but was significantly worse than RETFound (P < 0.001). \nThe confusion matrix (Extended Data Table 1) shows that RETFound \nachieved the highest sensitivity of 0.7 and specificity of 0.67. Likewise, \nRETFound also ranked first for prediction of heart failure, ischaemic \nstroke and Parkinson’s disease with AUROCs of 0.794 (95% CI 0.792, \n0.797), 0.754 (95% CI 0.752, 0.756) and 0.669 (0.65, 0.688), respectively. \nRETFound also performed significantly better than the other models \nwhen using OCT as the input modality. It achieved significantly higher \nAUPR results in all tasks (Extended Data Fig. 3a). External evaluation \non the UK Biobank (Fig. 3b) showed that RETFound and SSL-Retinal \nperformed similarly in prediction of ischaemic stroke. For tasks of \nmyocardial infarction, heart failure and Parkinson’s disease, RETFound \nachieved the best performance both with CFP and OCT. RETFound also \nshowed significantly higher AUPR in most tasks when it was externally \nevaluated on UK Biobank (Extended Data Fig. 3b).\nLabel efficiency for disease detection\nLabel efficiency refers to the amount of training data and labels \nrequired to achieve a target performance level for a given downstream \ntask, which indicates the annotation workload for medical experts.  \nRETFound showed superior label efficiency across various tasks (Fig. 4).  \nFor heart failure prediction, RETFound outperformed the other \npretraining strategies using only 10% of labelled training data, \ndemonstrating the potential of this approach in alleviating data short-\nages. RETFound similarly showed superior label efficiency for diabetic \nretinopathy classification and myocardial infarction prediction. Fur-\nthermore, RETFound showed consistently high adaptation efficiency \n(Extended Data Fig. 4), suggesting that RETFound required less time in \nadapting to downstream tasks. For example, RETFound can potentially \nsave about 80% of the training time required to achieve convergence \nfor the task of predicting myocardial infarction, leading to significant \nreductions in computational costs (for example, credits on Google \nCloud Platform) when appropriate mechanisms such as early stopping  \nare used.\nSSL strategies for RETFound\nWe explored the performance of different SSL strategies, that is, gen-\nerative SSL (for example, masked autoencoder) and contrastive SSL \n(for example, SimCLR, SwAV, DINO and MoCo-v3), in the RETFound \nframework. As shown in Fig. 5, RETFound with different contrastive \nSSL strategies showed decent performance in downstream tasks. For \ninstance, RETFound with DINO achieved AUROC of 0.866 (95% CI 0.864, \n0.869) and 0.728 (95% CI 0.725, 0.731), respectively, on wet-AMD prog-\nnosis (Extended Data Fig. 5) and ischaemic stroke prediction (Fig. 5), \noutperforming the baseline SL-ImageNet (Supplementary Tables 3 \nand 4). This demonstrates the effectiveness of RETFound framework \nwith diverse SSL strategies. Among these SSL strategies, the masked \nautoencoder (primary SSL strategy for RETFound) performed signifi-\ncantly better than the contrastive learning approaches in most disease \ndetection tasks (Fig. 5 and Extended Data Fig. 5). All quantitative results \nare listed in Supplementary Table 4.\nModel interpretation\nT o gain insights into the inner-workings of RETFound leading to its \nsuperior performance and label efficiency in downstream tasks, \nwe performed qualitative analyses of the pretext task used for \nself-supervised pretraining and task-specific decisions of RETFound \nCFP\nOCT\nPublic\ndatasets\nPublic\ndatasets\nMEH-\nAlzEye\n• Diabetic retinopathy\n•G laucoma\n•M ulticlass disease\nInternal External\nOcular disease prognosis\n•F ellow eye converts to wet-AMD\nInternal\nStage 1: Self-supervision on retinal images Stage 2: Supervised /f_ine-tuning for clinical tasks\nRETFound\nMEH-MIDAS +\npublic datasets\nOcular disease diagnosis\nMEH-\nAlzEye\nUK\nBiobank\nOculomics: prediction of systemic disease\n•I schaemic stroke\n• Myocardial infarction\n• Heart failure\n• Parkinson’s disease\nInternal External\nFig. 1 | Schematic of development and evaluation of the foundation models (RETFound).  Stage one constructs RETFound by means of SSL, using CFP and OCT \nfrom MEH-MIDAS and public datasets. Stage two adapts RETFound to downstream tasks by means of supervised learning for internal and external evaluation.\nNature | Vol 622 | 5 October 2023 | 159\n(Extended Data Fig. 6). The pretext task of RETFound allows models \nto learn retina-specific context, including anatomical structures and \ndisease lesions. As shown in Extended Data Fig. 6a, RETFound was able \nto reconstruct major anatomical structures, including the optic nerve \nand large vessels on CFP, and the nerve fibre layer and retinal pigment \nepithelium on OCT, despite 75% of the retinal image being masked. \nThis demonstrates that RETFound has learned to identify and infer \nthe representation of disease-related areas by means of SSL, which \ncontributes to performance and label efficiency in downstream tasks. \nOn top of the reconstruction-based interpretation, we further used an \nadvanced explanation tool (RELPROP42) to visualize the salient regions \nof images conducive to classifications made by fine-tuned models in \ndownstream tasks (Extended Data Fig. 6b). For ocular disease diagnosis, \nwell-defined pathologies were identified and used for classification, \nsuch as hard exudates and haemorrhage for diabetic retinopathy and \nparapapillary atrophy for glaucoma. For oculomic tasks, we observed \n0.6\n0.8\n1.0\nAUROC\nDiabetic retinopathy\nAPTOS-2019 \n0.6\n0.8\n1.0\nDiabetic retinopathy\nIDRID \n0.6\n0.8\n1.0\nDiabetic retinopathy\nMESSIDOR-2 \n0.6\n0.8\n1.0\nGlaucoma\nGlaucoma fundus \n0.6\n0.8\n1.0\nAUROC\nGlaucoma\nPAPILA \n0.6\n0.8\n1.0\nMulticategory\nRetina \n0.6\n0.8\n1.0\nMulticategory\nJSIEC \n0.6\n0.8\n1.0\nMulticategory \nOCTID\n0.6\n0.8\n1.0\nAUROC\nFine-tune on APTOS-2019\nEvaluate on IDRID \n0.6\n0.8\n1.0\nFine-tune on IDRID\nEvaluate on APTOS-2019 \n0.6\n0.8\n1.0\nFine-tune on MESSIDOR-2\nEvaluate on IDRID \nSL-ImageNetSSL-ImageNet\nSSL-RetinalRE\nTFound\n0.6\n0.8\n1.0\nAUROC\nFine-tune on APTOS-2019\n Evaluate on MESSIDOR-2 \nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.6\n0.8\n1.0\nFine-tune on IDRID\n Evaluate on MESSIDOR-2 \nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.6\n0.8\n1.0\nFine-tune on MESSIDOR-2\n Evaluate on APTOS-2019 \n0.6\n0.8\n1.0\nWet-AMD, CFP\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.6\n0.8\n1.0\nWet-AMD, OCT\na\nb\nP < 0.001\nP < 0.001\nP < 0.001\nP < 0.001\nP = 0.003\nP < 0.001 P < 0.001\nP = 0.009\nP < 0.001\nP < 0.001\nP < 0.001\nP = 0.026 P < 0.001\nP < 0.001\nP < 0.001\nP < 0.001\ncAUROCAUROC\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRE\nTFound\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nFig. 2 | Performance on ocular disease diagnostic classification. a, Internal \nevaluation. Models are adapted to each dataset by fine-tuning and internally \nevaluated on hold-out test data in the tasks of diagnosing ocular diseases, such \nas diabetic retinopathy and glaucoma. The disease category and dataset \ncharacteristics are listed in Supplementary Table 1. b , External evaluation. \nModels are fine-tuned on one diabetic retinopathy dataset and externally \nevaluated on the others. c, Performance on ocular disease prognosis. The \nmodels are fine-tuned to predict the conversion of fellow eye to wet-AMD in \n1 year and evaluated internally. RETFound performs best in all tasks. For each \ntask, we trained the model with five different random seeds, determining the \nshuffling of training data, and evaluated the models on the test set to get five \nreplicas. We derived the statistics with the five replicas. The error bars show \n95% CI and the bar centre represents the mean value of the AUROC. We compare \nthe performance of RETFound with the most competitive comparison model to \ncheck whether statistically significant differences exist. P value is calculated \nwith the two-sided t -test and listed in the figure.\n160 | Nature | Vol 622 | 5 October 2023\nArticle\nthat anatomical structures associated with systemic conditions, such \nas the optic nerve on CFP and nerve fibre layer and ganglion cell layer \non OCT, were highlighted as areas that contributed to the incidence \nprediction of systemic diseases (Extended Data Fig. 6b).\nRobustness to age distribution shifts\nFor ageing-associated systemic diseases, clinically relevant anatomi-\ncal structures alter with both ageing43,44 and disease progression19,20,22. \nRETFound was trained to identify general structure alterations for \ndetection of systemic diseases (Extended Data Fig. 6b). T o further verify \nthe extent to which models can learn anatomical structure changes, \nrespectively, relating to ageing and disease progression, we evaluated \nperformance of the models when using four different control groups \nwith varying ages (mean ages 66.8, 68.5, 70.4 and 71.9 years) versus \na fixed disease group (mean age 72.1 years) in the task of myocardial \ninfarction. As shown in Extended Data Fig. 7, the models showed bet-\nter performance when the age difference is larger, indicating that age \nis indeed a confounder for studying ageing-associated diseases. The  \ncontribution of age can be demonstrated by the extreme case in which \nthe age difference between cohorts is maximal (5.3 years in our sce-\nnario), at which point a simple logistic regression with the input of age \nachieved an AUROC of 0.63, surpassing SSL-ImageNet and SL-ImageNet. \nWhen the age difference decreased, the models clearly outperformed \nthe logistic regression. We observed that RETFound kept stable per-\nformance even when the age difference decreased, which suggested \nthat RETFound well identified the disease-related anatomical structure \nalteration and used the information for predicting systemic diseases.\n0.6\n0.8\n1.0\nAUROC\nMyocardial infarction, CFP\n0.6\n0.8\n1.0\nHeart failure, CFP\n0.6\n0.8\n1.0\nIschaemic stroke, CFP\n0.6\n0.8\n1.0\nParkinson’s disease, CFP\n0.6\n0.8\n1.0\nAUROC\nMyocardial infarction, OCT\n0.6\n0.8\n1.0\nHeart failure, OCT\n0.6\n0.8\n1.0\nIschaemic stroke, OCT\n0.6\n0.8\n1.0\nParkinson’s disease, OCT\n0.4\n0.6\n0.8\n1.0\nAUROC\nMyocardial infarction, CFP\n0.4\n0.6\n0.8\n1.0\nHeart failure, CFP\n0.4\n0.6\n0.8\n1.0\nIschaemic stroke, CFP\n0.4\n0.6\n0.8\n1.0\nParkinson’s disease, CFP\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.4\n0.6\n0.8\n1.0\nAUROC\nMyocardial infarction, OCT\nSL\n-ImageNet\nSSL-ImageNet\nSSL-RetinalRETFound\n0.4\n0.6\n0.8\n1.0\nHeart failure, OCT\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.4\n0.6\n0.8\n1.0\nIschaemic stroke, OCT\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\n0.4\n0.6\n0.8\n1.0\nParkinson’s disease, OCT\na\nb\nP < 0.001 P < 0.001 P < 0.001\nP < 0.001\nP < 0.001P < 0.001\nP < 0.001\nP < 0.001\nP < 0.001 P < 0.001 P = 0.202 P = 0.003\nP = 0.043 P < 0.001\nP = 0.451 P = 0.085\nSL-ImageNetSSL-ImageNet\nSSL-\nRetinal\nRETFound\nSL-ImageNe\nt\nSSL-ImageNet\nSSL-RetinalRETFound\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\nSL-ImageNetSSL-ImageNet\nSSL-RetinalRETFound\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nAUROCAUROC\nFig. 3 | Performance on 3-year incidence prediction of systemic diseases  \nwith retinal images. a, Internal evaluation. Models are adapted to curated \ndatasets from MEH-AlzEye by fine-tuning and internally evaluated on hold-out \ntest data. b, External evaluation. Models are fine-tuned on MEH-AlzEye and \nexternally evaluated on the UK Biobank. Data for internal and external \nevaluation are described in Supplementary Table 2. Although the overall \nperformances are not high due to the difficulty of tasks, RETFound achieved \nsignificantly higher AUROC in all internal evaluations and most external \nevaluations. For each task, we trained the model with five different random \nseeds, determining the shuffling of training data, and evaluated the models on \nthe test set to get five replicas. We derived the statistics with the five replicas. \nThe error bars show 95% CI and the bar centre represents the mean value of the \nAUROC. We compare the performance of RETFound with the most competitive \ncomparison model to check whether statistically significant differences exist. \nP value is calculated with the two-sided t -test and listed in the figure.\nNature | Vol 622 | 5 October 2023 | 161\nDiscussion\nThis work introduces a new SSL-based foundation model, RETFound, \nand evaluates its generalizability in adapting to diverse downstream \ntasks. After training on large-scale unlabelled retinal images using an \nadvanced SSL technique (masked autoencoder), RETFound can be  \nefficiently adapted to a broad range of disease detection tasks, resulting \nin significant performance improvements for detecting ocular diseases \nand predicting cardiovascular and neurodegenerative diseases. It is a \nmedical foundation model that has been developed and assessed, and \nshows considerable promise for leveraging such multidimensional data \nwithout constraints of enormous high-quality labels.\nRETFound enhances the performance of detecting ocular diseases \nby learning to identify disease-related lesions. Ocular diseases are \ndiagnosed by the presence of well-defined pathological patterns, such \nas hard exudates and haemorrhages for diabetic retinopathy. These \nfeatures involve abnormal variations in colour or structure, showing \nvisible differences from the surrounding retina. RETFound can identify \ndisease-related patterns and correctly diagnose ocular diseases (for \nexample, myopia and diabetic retinopathy cases in Extended Data \nFig. 6b). In Fig.  2, we observe that RETFound ranks first in various  \ntasks, followed by SL-ImageNet. SL-ImageNet pretrains the model \nusing supervised learning on ImageNet-21k, which contains 14 million \nimages with 21,000 categories of natural objects with diverse shapes \nand textures, such as zebras and oranges. Such diverse characteristics \nallow models to learn abundant low-level features (for example, lines, \ncurves and edges) to identify the boundary of abnormal patterns, thus \nimproving disease diagnosis when the model adapts to medical tasks. \nIn this paper, we demonstrate that by using SSL successively on natural \nimages and unlabelled retinal images, a generalizable foundation model \n(RETFound) can be developed to further improve ocular disease diag-\nnosis and prognosis, even outperforming the powerful SL-ImageNet.\nRETFound learns retina-specific context by SSL on unlabelled retinal \ndata to improve the prediction of systemic health states. RETFound \nand SSL-Retinal rank top 2 in both internal and external evaluation in \npredicting systemic diseases by using SSL on unlabelled retinal images \n(Fig. 3). In pretraining RETFound learns representations by perform-\ning a pretext task involving the reconstruction of an image from its \nhighly masked version, requiring the model to infer masked informa-\ntion with limited visible image patches. Solving such a pretext task \nin retinal images allows the model to learn retina-specific context, \nincluding anatomical structures such as the optic nerve and retinal \nnerve fibre layer (Extended Data Fig. 6a) that are potential markers \nin retinal images for neurodegenerative diseases and cardiovascular \ndiseases17,19,21,45. The confusion matrix shows that RETFound achieves \nthe highest sensitivity (Extended Data Table 1), indicating that more \nindividuals with a high risk of systemic diseases are identified. The \nevaluation on oculomic tasks demonstrates the use of retinal images \nfor incidence prediction and risk stratification of systemic diseases, \nsignificantly promoted by RETFound.\nCompared to SSL-Retinal and SSL-ImageNet, RETFound shows con-\nsistently better performance for disease detection (Figs. 2 and 3 and \nSupplementary Table 3), thus demonstrating SSL on retinal and natural  \nimages is complementary to developing the powerful foundation \nmodel. The strategy of combining natural images and medical data in \nRETFound SSL-Retinal SSL-ImageNet SL-ImageNet\n10 20 50 90 100\nPercentage of training data\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nHeart failure, CFP\n10 20 50 90 100\nPercentage of training data\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDiabetic retinopathy MESSIDOR-2\n10 20 50 90 100\nPercentage of training data\n0.5\n0.6\n0.7\n0.8\nAUROC\nMyocardial infarction, CFP\n10 20 50 90 100\nPercentage of training data\n0.5\n0.6\n0.7\n0.8\n0.9\nAUROC\nDiabetic retinopathy IDRID\n50% data\n45% data\nFig. 4 | Label efficiency in exemplary applications. Label efficiency measures \nthe performance with different fractions of training data to understand the \namount of data required to achieve a target performance level. The dashed \ngrey lines highlight the difference in training data between RETFound and the \nmost competitive comparison model. RETFound performs better than the \ncomparison groups with 10% of training data in 3-year incidence prediction of \nheart failure and myocardial infarction with modality of CFP and comparable to \nother groups with 45% of data in diabetic retinopathy MESSIDOR-2 and 50% of \ndata on IDRID. The 95% CI of AUROC are plotted in colour bands and the centre \npoints of the bands indicate the mean value of AUROC.\n0.4\n0.6\n0.8\n1.0\nAUROC\nDiabetic retinopathy\nAPTOS-2019 \n0.4\n0.6\n0.8\n1.0\nDiabetic retinopathy\nIDRID \n0.4\n0.6\n0.8\n1.0\nAUROC\nIschaemic stroke, CFP\n0.4\n0.6\n0.8\n1.0\nHeart failure, CFP\nSwAVSimCLRMoCo-v\n3\nDINO MAE\nP < 0.001\nP < 0.001\nP < 0.001P < 0.001\nSwAVSimCLRMoCo-v\n3\nDINO MAE\nSwAVSimCLRMoCo-v3\nDINO MAE SwAVSimCLRMoCo-v3\nDINO MAE\nAUROCAUROC\nFig. 5 | Comparison of different SSL strategies in RETFound framework on \nexemplar applications.  We show AUROC of predicting diabetic retinopathy, \nischaemic stroke and heart failure by the models pretrained with different SSL \nstrategies, including the masked autoencoder (MAE), SwAV, SimCLR, MoCo-v3 \nand DINO. The data for systemic disease tasks come from the MEH-AlzEye \ndataset. RETFound with MAE achieved significantly higher AUROC in most \ntasks. The corresponding quantitative results for the contrastive SSL approaches \nare listed in Supplementary Table 4. For each task, we trained the model with \nfive different random seeds, determining the shuffling of training data, and \nevaluated the models on the test set to get five replicas. We derived the \nstatistics with the five replicas. The error bars show 95% CI and the bar centre \nrepresents the mean value of the AUPR. We compare the performance of \nRETFound with the most competitive comparison model to check whether \nstatistically significant differences exist. P value is calculated with the \ntwo-sided t-test and listed in the figure.\n162 | Nature | Vol 622 | 5 October 2023\nArticle\nmodel development has also been validated in other medical fields, such \nas chest X-rays6 and dermatology imaging46. We also conducted calibra-\ntion analyses for prediction models in oculomic tasks, which examines \nthe agreement between predicted probabilities and real incidence.  \nA well-calibrated model can provide a meaningful and reliable disease \nprediction as the predicted probability indicates the real likelihood \nof disease occurrence, enabling the risk stratification of diseases47,48.  \nWe observed that RETFound was better calibrated compared to other \nmodels and showed the lowest expected calibration error in the reliabil-\nity diagram (Extended Data Fig. 8). This verifies that RETFound gener-\nates reliable predicted probabilities, rather than overconfident ones.\nThe experiments show that both modalities of CFP and OCT have \nunique ocular and systemic information encoded that is valuable \nin predicting future health states. For ocular diseases, some image \nmodalities are commonly used for a diagnosis in which the specific \nlesions can be well observed, such as OCT for wet-AMD. However, such \nknowledge is relatively vague in oculomic tasks as (1) the markers for \noculomic research on different modalities are under exploration and \n(2) it requires a fair comparison between many modalities with identi-\ncal evaluation settings. In this work, we investigate and compare the \nefficacy of CFP and OCT for oculomic tasks with identical training and \nevaluation details (for example, train, validation and/or test data split-\nting is aligned by anonymous patient IDs). We notice that the models \nwith CFP and OCT achieve unequal performances in predicting systemic \ndiseases (Fig. 3 and Supplementary Table 3), suggesting that CFP and \nOCT contain different levels of information for oculomic tasks. For \ninstance, in 3-year incidence prediction of ischaemic stroke, RETFound \nwith CFP performs better than with OCT on both MEH-AlzEye (inter-\nnal evaluation) and UK Biobank (external evaluation). For the task of \nParkinson’s disease, RETFound with OCT shows significantly better \nperformance in internal evaluation. These observations may indicate \nthat various disorders of ageing (for example, stroke and Parkinson’s \ndisease) manifest different early markers on retinal images. A practical \nimplication for health service providers and imaging device manu -\nfacturers is to recognize that CFP has continuing value, and should \nbe retained as part of the standard retinal assessment in eye health \nsettings. This observation also encourages oculomic research to inves-\ntigate the strength of association between systemic health with the \ninformation contained in several image modalities.\nThere is a significant fall in performance when adapted models are \ntested against new cohorts that differ in the demographic profile, and \neven on the imaging devices that were used (external evaluation phase). \nThis phenomenon is observed both in the external evaluation of ocular \ndisease diagnosis (Fig. 2b) and systemic disease prediction (Fig. 3b). \nFor example, the performance on ischaemic stroke drops (RETFound’s \nAUROC decreases by 0.16 with CFP and 0.19 with OCT). In the challeng-\ning oculomic tasks, the age and ethnicity profile of the internal and \nexternal validation cohorts (MEH-AlzEye and UK Biobank) as well as the \nimaging devices are significantly different (Supplementary Table 2), \nand this is likely to be reflected in the drop in performance when exter-\nnally evaluated in the UK Biobank cohort. Compared to other models, \nRETFound achieves significantly higher performance in external evalu-\nation in most tasks (Fig. 3b) as well as different ethnicities (Extended \nData Figs. 9–11), showing good generalizability.\nWe observe that RETFound maintains competitive performance for \ndisease detection tasks, even when substituting various contrastive \nSSL approaches into the framework (Fig. 5 and Extended Data Fig. 5). \nIt seems that the generative approach using the masked autoencoder \ngenerally outperforms the contrastive approaches, including SwAV, \nSimCLR, MoCo-v3 and DINO. However, it is worth noting that assert-\ning the superiority of the masked autoencoder requires caution, given \nthe presence of several variables across all models, such as network \narchitectures (for example, ResNet-50 for SwAV and SimCLR, Trans-\nformers for the others) and hyperparameters (for example, learning \nrate scheduler). Our comparison demonstrates that the combination of \npowerful network architecture and complex pretext tasks can produce \neffective and general-purpose medical foundation models, aligning \nwith the insights derived from large language models in healthcare49,50. \nFurthermore, the comparison further supports the notion that the \nretinal-specific context learned from the masked autoencoder’s pre-\ntext task, which includes anatomical structures such as the optic nerve \nhead and retinal nerve fibre layer (as shown in Extended Data Fig. 6a), \nindeed provides discriminative information for the detection of ocular \nand systemic diseases.\nWe believe that research on medical foundation models, such as \nRETFound, has the potential to democratize access to medical AI and \naccelerate progress towards widespread clinical implementation.  \nT o this end, foundation models must learn powerful representations \nfrom enormous volumes of medical data (1.6 million retinal images in \nour case), which is often only accessible to large institutions with effi-\ncient dataset curation workflows. Also, SSL pretraining of foundation \nmodels requires many computational resources to achieve training \nconvergence. We used eight NVIDIA T esla A100 (40 GB) graphical pro-\ncessing units (GPUs) on the Google Cloud Platform, requiring 2 weeks \nof developing time. By contrast, the data and computational require-\nments required to fine-tune RETFound to downstream tasks are com-\nparatively small and therefore more achievable for most institutions. \nWe required only one NVIDIA T esla T4 (16 GB) GPU, requiring about \n1.2 h with a dataset of 1,000 images. Moreover, foundational models \noffer the potential to raise the general quality of healthcare AI models. \nTheir adoption may help avoid superficially impressive models that \nrarely affect clinical care. These poorly generalizable models consume \nsignificant resources and can feed scepticism about the benefits of \nAI in healthcare. By making RETFound publicly available, we hope to \naccelerate the progress of AI in medicine by enabling researchers to \nuse our large dataset to design models for use in their own institutions \nor to explore alternative downstream applications.\nAlthough this work systematically evaluates RETFound in detecting \nand predicting diverse diseases, there are several limitations and chal-\nlenges requiring exploration in future work. First, most data used to \ndevelop RETFound came from UK cohorts, therefore it is worth explor-\ning the impact of introducing a larger dataset by incorporating retinal \nimages worldwide, with more diverse and balanced data distribution. \nSecond, although we study the performance with modalities of CFP \nand OCT, the multimodal information fusion between CFP and OCT \nhas not been investigated, which might lead to further improvement \nin performance. Finally, some clinically relevant information, such as \ndemographics and visual acuity that may work as potent covariates for \nocular and oculomic research, has not been included in SSL models.  \nCombining these, we propose to further enhance the strength of  \nRETFound in subsequent iterations by introducing even larger quan-\ntities of images, exploring further modalities and enabling dynamic \ninteraction across multimodal data. While we are optimistic about \nthe broad scope of RETFound to be used for a range of AI tasks, we \nalso acknowledge that enhanced human–AI integration is critical to \nachieving true diversity in healthcare AI applications.\nIn conclusion, we have verified the efficacy and efficiency of  \nRETFound in adapting to diverse healthcare applications, showing \nhigh performance and generalizability in detecting ocular diseases and \nsignificant improvement in predicting systemic diseases. By overcom-\ning current barriers to clinical AI applications—notably, the extent of \nlabelled data and limited performance and generalizability—SSL-based \nfoundation models open the door to accelerated, data-efficient devices \nthat may transform care for patients with ocular or systemic diseases.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nNature | Vol 622 | 5 October 2023 | 163\nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-023-06555-x.\n1. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine. Nat. Med. \nhttps://doi.org/10.1038/s41591-021-01614-0 (2022).\n2. Willemink, M. J. et al. Preparing medical imaging data for machine learning. Radiology \n295, 4–15 (2020).\n3. Topol, E. J. High-performance medicine: the convergence of human and artificial \nintelligence. Nat. Med. 25, 44–56 (2019).\n4. Yu, K.-H., Beam, A. L. & Kohane, I. S. Artificial intelligence in healthcare. Nat. Biomed. Eng. \n2, 719–731 (2018).\n5. Liu, X. et al. A comparison of deep learning performance against health-care \nprofessionals in detecting diseases from medical imaging: a systematic review and \nmeta-analysis. Lancet Digit. Health 1, e271–e297 (2019).\n6. De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal \ndisease. Nat. Med. 24, 1342–1350 (2018).\n7. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest X-ray images \nvia self-supervised learning. Nat. Biomed. Eng. https://doi.org/10.1038/s41551-022-00936-9 \n(2022).\n8. Krishnan, R., Rajpurkar, P. & Topol, E. J. Self-supervised learning in medicine and \nhealthcare. Nat. Biomed. Eng. https://doi.org/10.1038/s41551-022-00914-1 (2022).\n9. Doersch, C., Gupta, A. & Efros, A. A. Unsupervised visual representation learning by \ncontext prediction. In Proc. 2015 IEEE International Conference on Computer Vision (eds \nIkeuchi, K. et al.) 1422–1430 (IEEE, 2015).\n10. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature \n616, 259–265 (2023).\n11. Jing, L. & Tian, Y. Self-supervised visual feature learning with deep neural networks:  \na survey. IEEE Trans. Pattern Anal. Mach. Intell. 43, 4037–4058 (2021).\n12. Deng, J. et al. ImageNet: a large-scale hierarchical image database. In Proc. 2009 IEEE \nConference on Computer Vision and Pattern Recognition (eds Essa, I., Kang, S. B. & \nPollefeys, M.) 248–255 (IEEE, 2009).\n13. Chen, T., Kornblith, S., Swersky, K., Norouzi, M. & Hinton, G. Big self-supervised models \nare strong semi-supervised learners. In Proc. 34th International Conference on Neural \nInformation Processing Systems (ed. Ranzato, M.) 22243–22255 (Neurips, 2020).\n14. Chen, X., Xie, S. & He, K. An empirical study of training self-supervised vision transformers. \nIn Proc. 2021 IEEE/CVF International Conference on Computer Vision (eds Hassner, T. et al.) \n9640–9649 (IEEE, 2021).\n15. He, K. et al. Masked autoencoders are scalable vision learners. In Proc. 2022 IEEE/CVF \nConference on Computer Vision and Pattern Recognition (eds Dana, K. et al.) 16000–\n16009 (IEEE, 2022).\n16. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. A simple framework for contrastive \nlearning of visual representations. In Proc. 37th International Conference on Machine \nLearning (eds Iii, H. D. & Singh, A.) 1597–1607 (PMLR, 2020).\n17. Wagner, S. K. et al. Insights into systemic disease through retinal imaging-based \noculomics. Transl. Vis. Sci. Technol. 9, 6 (2020).\n18. Cheung, C. Y. et al. A deep-learning system for the assessment of cardiovascular disease \nrisk via the measurement of retinal-vessel calibre. Nat. Biomed. Eng. 5, 498–508 (2021).\n19. Mutlu, U. et al. Association of retinal neurodegeneration on optical coherence tomography \nwith dementia: a population-based study. JAMA Neurol. 75, 1256–1263 (2018).\n20. Thomson, K. L., Yeo, J. M., Waddell, B., Cameron, J. R. & Pal, S. A systematic review and \nmeta-analysis of retinal nerve fiber layer change in dementia, using optical coherence \ntomography. Alzheimers Dement. 1, 136–143 (2015).\n21. Ko, F. et al. Association of retinal nerve fiber layer thinning with current and future \ncognitive decline: a study using optical coherence tomography. JAMA Neurol. 75, \n1198–1205 (2018).\n22. McGeechan, K. et al. Meta-analysis: retinal vessel caliber and risk for coronary heart \ndisease. Ann. Intern. Med. 151, 404–413 (2009).\n23. Wong, T. Y. & Mitchell, P. Hypertensive retinopathy. N. Engl. J. Med. 351, 2310–2317 (2004).\n24. Günthner, R. et al. Impaired retinal vessel dilation predicts mortality in end-stage renal \ndisease. Circ. Res. https://doi.org/10.1161/CIRCRESAHA.118.314318 (2019).\n25. Diaz-Pinto, A. et al. Predicting myocardial infarction through retinal scans and minimal \npersonal information. Nat. Mach. Intell. 4, 55–61 (2022).\n26. Azizi, S. et al. Robust and efficient medical imaging with self-supervision. Nat. Biomed. \nEng. 7, 756–779 (2023)\n27. Li, X., Jia, M., Islam, M. T., Yu, L. & Xing, L. Self-supervised feature learning via exploiting \nmulti-modal data for retinal disease diagnosis. IEEE Trans. Med. Imaging 39, 4023–4033 \n(2020).\n28. Burlina, P., Paul, W., Liu, T. Y. A. & Bressler, N. M. Detecting anomalies in retinal diseases \nusing generative, discriminative, and self-supervised deep learning. JAMA Ophthalmol. \n140, 185–189 (2022).\n29. Holmberg, O. G. et al. Self-supervised retinal thickness prediction enables deep learning \nfrom unlabelled data to boost classification of diabetic retinopathy. Nat. Mach. Intell. 2, \n719–726 (2020).\n30. Truong, T., Mohammadi, S. & Lenga, M. How transferable are self-supervised features in \nmedical image classification tasks? In Proc. 2021 Machine Learning for Health (eds Roy, S. \net al.) 54–74 (PMLR, 2021)\n31. Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at \nhttps://arxiv.org/abs/2108.07258 (2021).\n32. Wiggins, W. F. & Tejani, A. S. On the opportunities and risks of foundation models for \nnatural language processing in radiology. Radiol. Artif. Intell. 4, e220119 (2022).\n33. Gulshan, V. et al. Development and validation of a deep learning algorithm for detection \nof diabetic retinopathy in retinal fundus photographs. J. Am. Med. Assoc. 316, 2402–2410 \n(2016).\n34. Kermany, D. S. et al. Identifying medical diagnoses and treatable diseases by \nimage-based deep learning. Cell 172, 1122–1131.e9 (2018).\n35. Wagner, S. K. et al. AlzEye: longitudinal record-level linkage of ophthalmic imaging and \nhospital admissions of 353 157 patients in London, UK. BMJ Open 12, e058552 (2022).\n36. Bycroft, C. et al. The UK Biobank resource with deep phenotyping and genomic data. \nNature 562, 203–209 (2018).\n37. Caron, M. et al. Unsupervised learning of visual features by contrasting cluster \nassignments. In Proc. 34th International Conference on Neural Information Processing \nSystems (ed. Ranzato, M.) 9912–9924 (Neurips, 2020).\n38. Caron, M. et al. Emerging properties in self-supervised vision transformers. In Proc. 2021 \nIEEE/CVF International Conference on Computer Vision (eds Hassner, T. et al.) 9650–9660 \n(IEEE, 2021).\n39. Porwal, P. et al. IDRiD: diabetic retinopathy - segmentation and grading challenge. Med. \nImage Anal. 59, 101561 (2020).\n40. Abràmoff, M. D. et al. Automated analysis of retinal images for detection of referable \ndiabetic retinopathy. JAMA Ophthalmol. 131, 351–357 (2013).\n41. Decencière, E. et al. Feedback on a publicly distributed image database: the Messidor \ndatabase. Image Anal. Stereol. 33, 231–234 (2014).\n42. Chefer, H., Gur, S. & Wolf, L. Transformer interpretability beyond attention visualization.  \nIn Proc. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (eds \nForsyth, D. et al.) 782–791 (IEEE, 2021).\n43. Sung, K. R. et al. Effects of age on optical coherence tomography measurements of healthy \nretinal nerve fiber layer, macula, and optic nerve head. Ophthalmology 116, 1119–1124 (2009).\n44. Wong, T. Y., Klein, R., Klein, B. E. K., Meuer, S. M. & Hubbard, L. D. Retinal vessel diameters \nand their associations with age and blood pressure. Invest. Ophthalmol. Vis. Sci. 44, \n4644–4650 (2003).\n45. Hanssen, H., Streese, L. & Vilser, W. Retinal vessel diameters and function in \ncardiovascular risk and disease. Prog. Retin. Eye Res. 91, 101095 (2022).\n46. Esteva, A. et al. Dermatologist-level classification of skin cancer with deep neural \nnetworks. Nature 542, 115–118 (2017).\n47. Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. On calibration of modern neural networks. \nIn Proc. 34th International Conference on Machine Learning (eds Precup, D. & Teh, Y. W.) \nVol. 70, 1321–1330 (PMLR, 2017).\n48. Ayhan, M. S. et al. Expert-validated estimation of diagnostic uncertainty for deep neural \nnetworks in diabetic retinopathy detection. Med. Image Anal. 64, 101724 (2020).\n49. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 \n(2023).\n50. Singhal, K. et al. Towards expert-level medical question answering with large language \nmodels. Preprint at https://arxiv.org/abs/2305.09617 (2023).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nUK Biobank Eye & Vision Consortium\nNaomi Allen13, John E. J. Gallacher13, Thomas Littlejohns13, Tariq Aslam14, Paul Bishop14, \nGraeme Black14, Panagiotis Sergouniotis14, Denize Atan15, Andrew D. Dick15, Cathy Williams15, \nSarah Barman16, Jenny H. Barrett17, Sarah Mackie17, Tasanee Braithwaite18, Roxana O. Carare19, \nSarah Ennis19, Jane Gibson19, Andrew J. Lotery19, Jay Self19, Usha Chakravarthy20, \nRuth E. Hogg20, Euan Paterson20, Jayne Woodside20, Tunde Peto20, Gareth Mckay20, \nBernadette Mcguinness20, Paul J. Foster2,4, Konstantinos Balaskas2,4, Pearse A. Keane2,4, \nAnthony P . Khawaja2,4, Nikolas Pontikos2,4, Jugnoo S. Rahi2,4, Gerassimos Lascaratos2, \nPraveen J. Patel2, Michelle Chan2, Sharon Y . L. Chua2, Alexander Day2, Parul Desai2, \nCathy Egan2, Marcus Fruttiger2, David F. Garway-Heath2, Alison Hardcastle2, \nSir Peng T . Khaw2, Tony Moore2, Sobha Sivaprasad2, Nicholas Strouthidis2, Dhanes Thomas2, \nAdnan Tufail2, Ananth C. Viswanathan2, Alastair K. Denniston10,11, Bal Dhillon21, \nTom Macgillivray21, Cathie Sudlow21, Veronique Vitart21, Alexander Doney22, \nEmanuele Trucco22, Jeremy A. Guggeinheim23, James E. Morgan23, Chris J. Hammond24, \nKatie Williams24, Pirro Hysi24, Simon P . Harding25, Yalin Zheng25, Robert Luben4, \nPhil Luthert4, Zihan Sun4, Martin McKibbin26, Eoin O’Sullivan27, Richard Oram28, \nMike Weedon28, Chris G. Owen29, Alicja R. Rudnicka29, Naveed Sattar30, David Steel31, \nIrene Stratton32, Robyn Tapp33, Max M. Yates34, Axel Petzold35 & Savita Madhusudhan36\n13University of Oxford, Oxford, UK. 14University of Manchester, Manchester, UK. 15University  \nof Bristol, Bristol, UK. 16Kingston University, London, UK. 17University of Leeds, Leeds, UK.  \n18St Thomas’ Hospital, London, UK. 19University of Southampton, Southampton, UK. 20Queens \nUniversity Belfast, Belfast, UK. 21University of Edinburgh, Edinburgh, UK. 22University of \nDundee, Dundee, UK. 23Cardiff University, Cardiff, UK. 24King’s College London, London,  \nUK. 25University of Liverpool, Liverpool, UK. 26Leeds Teaching Hospitals NHS Trust, Leeds, UK. \n27King’s College Hospital NHS Foundation Trust, London, UK. 28University of Exeter, Exeter,  \nUK. 29University of London, London, UK. 30University of Glasgow, Glasgow, UK. 31Newcastle \nUniversity, Newcastle, UK. 32Gloucestershire Hospitals NHS Foundation Trust, Gloucester,  \nUK. 33St George’s University of London, London, UK. 34University of East Anglia, Norwich, UK. \n35UCL Institute of Neurology, London, UK. 36Royal Liverpool University Hospital, Liverpool, UK. \nArticle\nMethods\nDatasets for developing RETFound\nWe curate large collections of unannotated retinal images for SSL, total-\nling 904,170 CFPs and 736,442 OCT scans. Of these, 815,468 (90.2%) \nCFPs and 627,133 (85.2%) OCTs are from Moorfields Diabetic imAge \ndataSet (MEH-MIDAS), and 88,702 (9.8%) CFPs are Kaggle EyePACS and \n109,309 (14.8%) OCTs that come from ref. 34. MEH-MIDAS is a retrospec-\ntive dataset that includes the complete ocular imaging records of 37,401 \npatients (16,429 female, 20,966 male and six unknown) with diabetes \nwho were seen at Moorfields Eye Hospital, London, UK between 2000 \nand 2022. The age distribution has a mean value of 64.5 and standard \ndeviation of 13.3. The ethnicity distributes diversly: British (13.7%), \nIndian (14.9%), Caribbean (5.2%), African (3.9%), other ethnicity (37.9%) \nand not stated (24.4%). MEH-MIDAS includes various imaging devices, \nsuch as topcon 3DOCT-2000SA (T opcon), CLARUS (ZEISS) and Triton  \n(T opcon). EyePACS includes images devices of Centervue DRS  \n(Centervue), Optovue iCam (Optovue), Canon CR1/DGi/CR2 (Canon) and \nT opcon NW (T opcon). Reference 34 contains images from SPECTRALIS  \n(Heidelberg).\nData for ocular disease diagnosis\nWe evaluate the model performance on three different categories of \ndisease detection tasks. The first category of tasks involves diagnostic \nclassification of ocular diseases with publicly available ophthalmic data. \nFor diabetic retinopathy diagnosis, Kaggle APTOS-2019 (India), IDRID \n(India) and MESSIDOR-2 (France) are used. The labels for diabetic retin-\nopathy are based on the International Clinical Diabetic Retinopathy \nSeverity scale, indicating five stages from no diabetic retinopathy to \nproliferative diabetic retinopathy. For glaucoma, PAPILA51 (Spain) and \nGlaucoma Fundus52 (South Korea) are included. Glaucoma Fundus and \nPAPILA have three categorical labels, non-glaucoma, early glaucoma \n(suspected glaucoma) and advanced glaucoma. For datasets with sev-\neral diseases, JSIEC53 (China), Retina and OCTID54 (India) are included. \nJSIEC includes 1,000 images with 39 categories of common referable \nfundus diseases and conditions. Retina has labels of normal, glaucoma, \ncataract and retina disease. OCTID includes 470 OCT scans with labels \nof normal, macular hole, AMD, central serous retinopathy and diabetic \nretinopathy. The grading protocols for the public datasets are summa-\nrized as: IDRiD, two medical experts provided adjudicated consensus \ngrades; MESSIDOR-2, adjudicated by a panel of three retina specialists \nin accordance with a published protocol55; APTOS-2019, Kaggle dataset \nwith limited information but possibly a single clinician grader; PAPILA, \nlabelling and segmentation by two experts following extensive clinical \nexamination and testing procedure including a retrospective clinical \nrecord review; Glaucoma Fundus, agreement of two specialists based \non visual fields and extensive imaging and JSIEC, labelled by ophthal-\nmologists and confirmed by senior retina specialists. Disagreements \nresolved by panel of five senior retina specialists were as follows: Retina, \ndetails not available and OCTID, describes image labelling based on \nthe diagnosis of retinal clinical experts but does not specify duplicate \nadjudication. The details of datasets, such as imaging devices, country \nand label category, are listed in Supplementary Table 1.\nData for disease prognosis and prediction\nFor disease prognosis of fellow eye converting to wet-AMD in 1 year, we \nuse data from the Moorfields AlzEye study (MEH-AlzEye). MEH-AlzEye \nis a retrospective cohort study linking ophthalmic data of 353,157 \npatients, who attended Moorfields Eye Hospital between 2008 and \n2018, with systemic health data from hospital admissions across the \nwhole of England. Systemic health data are derived from Hospital \nEpisode Statistics (HES) data relating to admitted patient care, with \na focus on cardiovascular disease and all-cause dementia. Diagnostic  \ncodes in HES admitted patient care are reported according to the tenth \nrevision of the ICD (International Statistical Classification of Diseases)56. \nIn line with previous reports, we selected the study cohort using ICD \ncode: stroke (I23-I24), myocardial infarction (I21-I22), heart failure \n(I50) and Parkinson’s disease (G20). Among 186,651 patients with HES, \n6,504 patients are diagnosed with wet-AMD in at least one eye, 819 \npatients have retinal imaging within 1 year before their fellow eyes con-\nvert to wet-AMD and 747 patients with their fellow eyes not converting  \nwet-AMD, after excluding other eye diseases. The final category of \ntasks studies the 3-year prediction of systemic diseases, with a focus \non cardiovascular and neurodegenerative dysfunctions, using the \nMEH-AlzEye and UK Biobank. The UK Biobank includes 502,665 UK \nresidents aged between 40 and 69 years who are registered with the \nNational Health Service. Among all participants, 82,885 get CFP and \nOCT examinations and a total of 171,500 retinal images are collected. \nFor each patient, we only include the retinal image from the left eye in \none visit, to avoid potential bias by inconsistent individual visits. For \ninternal evaluation, we split the patient groups into training, validation \nand test sets at a ratio of 55:15:30%. The training set is used to revise \nmodel parameters to achieve objective function. The validation set is \nfor monitoring training converge and checkpoint selection. The test \nset is used to test the saved model checkpoint and evaluate the internal  \nperformance. For external validation, all patient data are used for \nevaluating the saved model checkpoint. The detailed data flowcharts \nare listed in Supplementary Figs. 1–5.\nData processing and augmentation for SSL\nFor CFP image preprocessing, we use AutoMorph57, an automated reti-\nnal image analysis tool, to exclude the background and keep the retinal \narea. All images are resized to 256 × 256 with cubic interpolation. For \nOCT, we extract the middle slices and resize them to 256 × 256. We fol-\nlow the same data augmentation as the masked autoencoder in model \ntraining, including random crop (lower bounds 20% of the whole image \nand upper bounds 100%) and resizing the cropped patches to 224 × 224, \nrandom horizontal flipping and image normalization.\nRETFound architecture and implementation\nWe use a specific configuration of the masked autoencoder15, which \nconsists of an encoder and a decoder. The architecture detail is shown \nin Supplementary Fig. 6. The encoder uses a large vision Transformer58 \n(ViT-large) with 24 Transformer blocks and an embedding vector size \nof 1,024, whereas the decoder is a small vision Transformer (Vit-small) \nwith eight Transformer blocks and an embedding vector size of 512. The \nencoder takes unmasked patches (patch size of 16 × 16) as input and \nprojects it into a feature vector with a size of 1,024. The 24 Transformer \nblocks, comprising multiheaded self-attention and multilayer percep-\ntron, take feature vectors as input and generate high-level features. \nThe decoder inserts masked dummy patches into extracted high-level \nfeatures as the model input and then reconstructs the image patch \nafter a linear projection. In model training, the objective is to recon-\nstruct retinal images from the highly masked version, with a mask \nratio of 0.75 for CFP and 0.85 for OCT. The batch size is 1,792 (8 GPUs ×  \n224 per GPU). The total training epoch is 800 and the first 15 epochs \nare for learning rate warming up (from 0 to a learning rate of 1 × 10−3). \nThe model weights at the final epoch are saved as the checkpoint for \nadapting to downstream tasks.\nAdaptation to downstream tasks\nIn adapting to downstream tasks, we only need the encoder (ViT-large) \nof the foundation model and discard the decoder. The encoder  \ngenerates high-level features from retinal images. A multilayer per-\nceptron takes the features as input and outputs the probability of \ndisease categories. The category with the highest probability will be \ndefined as the final classification. The number of categories decides \nthe neuron of the final layer of the multilayer perceptron. We include \nlabel smoothing to regulate the output distribution thus prevent -\ning overfitting of the model by softening the ground-truth labels \nin the training data. The training objective is to generate the same  \ncategorical output as the label. The batch size is 16. The total training \nepoch is 50 and the first ten epochs are for learning rate warming up \n(from 0 to a learning rate of 5 × 10−4), followed by a cosine annealing \nschedule (from learning rates of 5 × 10−4 to 1 × 10−6 in the rest of the 40 \nepochs). After each epoch training, the model will be evaluated on \nthe validation set. The model weights with the highest AUROC on the \nvalidation set will be saved as the model checkpoint for internal and  \nexternal evaluation.\nContrastive SSL implementation\nWe replace the primary SSL approach (that is, masked autoencoder) \nwith SimCLR16, SwAV37, DINO38 and MoCo-v3 (ref. 14) in the RETFound \nframework to produce variants of the pretrained model for comparison. \nFor SSL training with each contrastive learning approach, we follow the \nrecommended network architectures and hyperparameter settings \nfrom the published papers for optimal performance. We first load the \npretrained weights on ImageNet-1k to the models and further train the \nmodels with 1.6 million retinal images with each contrastive learning \napproach to obtain pretrained models. We then follow the identical \nprocess of transferring the masked autoencoder to fine-tune those \npretrained models for the downstream disease detection tasks.\nExplanations for fine-tuned models\nWe use RELPROP42 specified for Transformer-based networks. The \nmethod uses layer-wise relevance propagation to compute relevancy \nscores for each attention head in each layer and then integrates them \nthroughout the attention graph, by combining relevancy and gradient \ninformation. As a result, it visualizes the areas of input images that lead \nto a certain classification. RELPROP has been shown to outperform \nother well-known explanation techniques, such as GradCam59.\nComputational resources\nSSL typically benefits from a large batch size for training and extract-\ning context from data, which requires powerful GPUs for computa-\ntion. We use eight NVIDIA T esla A100 (40 GB) on the Google Cloud \nPlatform. It takes about 14 days to develop RETFound. We allocate an \nequal computational cost to each SSL approach for pretraining. For \nfine-tuning RETFound to downstream tasks, we use NVIDIA T esla T4 \n(16 GB). Fine-tuning takes about 70 min for every 1,000 images.\nEvaluation and statistical analysis\nAll task performances are evaluated by the classification metrics known \nas AUROC and AUPR, computed from the receiver operating charac-\nteristics and precision-recall curves of classifiers, respectively. For \nocular prognosis and oculomic prediction tasks, the AUROC and AUPR \nare calculated in a binary setting. For multiclass classification, such as \nfive-stage diabetic retinopathy and multicategory disease diagnosis, \nwe calculate the AUROC and AUPR for each disease category and then \naverage them to get the general AUROC and AUPR. For each task, we \ntrain the model with five different random seeds, determining the \nshuffling of training data. We calculate the mean and standard \n \ndeviation of the performance over the five iterations and calculate the \nstandard error by (standard deviation/ 5 ). We obtain the 95% CI by \nmeans of 1.96 × standard error. We use the two-sided t-tests between \nthe performance of RETFound and the most competitive comparison \nmodel to show whether significant differences exist.\nEthics statement\nThis study involves human participants and was approved by the \nLondon-Central Research Ethics Committee (18/LO/1163, approved \n1 August 2018), Advanced statistical modelling of multimodal data \nof genetic and acquired retinal diseases (20/HRA/2158, approved \n5 May 2020) and the Confidential Advisory Group for Section 251 \n \nsupport (18/CAG/0111, approved 13 September 2018). The National \nHealth Service Health Research Authority gave final approval on 13 \nSeptember 2018. Moorfields Eye Hospital NHS Foundation Trust vali-\ndated the de-identifications. Only de-identified retrospective data \nwere used for research, without the active involvement of patients.\nReporting summary\nFurther information on research design is available in the Nature  \nPortfolio Reporting Summary linked to this article.\nData availability\nThe MIDAS dataset consists of routinely collected healthcare data. \nOwing to its sensitive nature and the risk of reidentification, the dataset \nis subject to controlled access by means of a structured application \nprocess. Data access enquiries may be made to enquiries@insight.\nhdrhub.org and we will aim to respond within 2 weeks. Further details \nabout the data request pipeline may be found on the INSIGHT Health \nData Research Hub website https://www.insight.hdrhub.org. The AlzEye \ndataset is subject to the contractual restrictions of the data sharing \nagreements between National Health Service Digital, Moorfields Eye \nHospital and University College London, and is not available for access \nbeyond the AlzEye research team. National and international collabora-\ntions are welcomed, although restrictions on access to the cohort mean \nthat only the AlzEye researchers can directly analyse individual-level \nsystemic health data. More details can be found at https://reading-\ncentre.org/studies/artificial_intelligence/alzeye. UK Biobank data are \navailable at https://www.ukbiobank.ac.uk/. Data for ocular disease \nexperiments are publicly available online and can be accessed through \nthe following links: IDRID (https://ieee-dataport.org/open-access/\nindian-diabetic-retinopathy-image-dataset-idrid), MESSIDOR-2 (https://\nwww.adcis.net/en/third-party/messidor2/), APTOS-2019 (https://\nwww.kaggle.com/competitions/aptos2019-blindness-detection/data), \nPAPILA (https://figshare.com/articles/dataset/PAPILA/14798004/1), \nGlaucoma Fundus (https://dataverse.harvard.edu/dataset.\nxhtml?persistentId=doi:10.7910/DVN/1YRRAC), JSIEC (https://zenodo.\norg/record/3477553), Retina (https://www.kaggle.com/datasets/jr2ngb/\ncataractdataset) and OCTID (https://borealisdata.ca/dataverse/OCTID).\nCode availability\nThe code used to train, fine-tune and evaluate RETFound from Y .Z. \nis available at https://github.com/rmaphoh/RETFound_MAE, which \nis based on PyT orch. Furthermore, a Keras version implemented by \nY .K. is available at https://github.com/uw-biomedical-ml/RETFound_\nMAE. Please note that the reported results are obtained from PyT orch  \nmodels. Images were processed with automated retinal image analysis \ntool AutoMorph v.1.0 (https://github.com/rmaphoh/AutoMorph). \nImage data were extracted from Dicom files with Pydicom v.2.3.0. \nResults were further analysed and visualized with Python v.3.6, NumPy \nv.1.19.5, SciPy v.1.5.4, seaborn v.0.12.0, Matplotlib v.3.6.1, pandas v.1.5.0, \nScikit-Learn v.1.1.3 and Pillow v.9.2.0. Heatmaps were generated with \nRELPROP (https://github.com/hila-chefer/Transformer-Explainability).\n \n51. Kovalyk, O. et al. PAPILA: dataset with fundus images and clinical data of both eyes of the \nsame patient for glaucoma assessment. Sci. Data 9, 291 (2022).\n52. Ahn, J. M. et al. A deep learning model for the detection of both advanced and early \nglaucoma using fundus photography. PLoS ONE 14, e0207982 (2018).\n53. Cen, L.-P. et al. Automatic detection of 39 fundus diseases and conditions in retinal \nphotographs using deep neural networks. Nat. Commun. 12, 4828 (2021).\n54. Gholami, P. et al. OCTID: optical coherence tomography image database. Comput. Electr. \nEng. 81, 106532 (2020).\n55. Krause, J. et al. Grader variability and the importance of reference standards for \nevaluating machine learning models for diabetic retinopathy. Ophthalmology 125, \n1264–1272 (2018).\n56. International Statistical Classification of Diseases and Related Health Problems: \nAlphabetical Index (World Health Organization, 2004).\n57. Zhou, Y. et al. AutoMorph: automated retinal vascular morphology quantification via a \ndeep learning pipeline. Transl. Vis. Sci. Technol. 11, 12 (2022).\nArticle\n58. Dosovitskiy, A. et al. An image is worth 16×16 words: transformers for image recognition at \nscale. In Proc. 9th International Conference on Learning Representations (eds Hofmann, K.  \net al.) https://openreview.net/forum?id=YicbFdNTTy (ICLR 2021).\n59. Selvaraju, R. R. et al. Grad-CAM: visual explanations from deep networks via gradient- \nbased localization. In Proc. 2017 IEEE International Conference on Computer Vision \n(eds Cucchiara, R. et al.) 618–626 (IEEE, 2017).\nAcknowledgements We thank P. Rawlinson for project management, C. Green and L. Wickham \nfor information governance expertise, and A. Wenban, S. St John-Green and M. Barnfield for \ninformation technology support. This work is supported by Engineering and Physical Sciences \nResearch Council grant nos. EP/M020533/1, EP/R014019/1 and EP/V034537/1, as well as the \nNIHR UCLH Biomedical Research Centre. S.K.W. is supported by a Medical Research Council \nClinical Research Training Fellowship (grant no. MR/TR000953/1). P.A.K. is supported by a \nMoorfields Eye Charity Career Development Award (grant no. R190028A) and a UK Research & \nInnovation Future Leaders Fellowship (grant no. MR/T019050/1). For the purpose of open \naccess, the author has applied a Creative Commons Attribution (CC BY) licence to any Author \nAccepted Manuscript version arising.\nAuthor contributions Y.Z., M.X., E.J.T., D.C.A. and P.A.K. contributed to the conception and \ndesign of the work. Y.Z., M.A.C., S.K.W., D.J.W., R.R.S. and M.G.L. contributed to the data \nacquisition and organization. Y.Z. contributed to the technical implementation. M.A.C., S.K.W., \nA.K.D. and P.A.K. provided the clinical inputs to the research. Y.Z., M.A.C., S.K.W., M.S.A., T.L., \nP.W.-C., A.A., D.C.A. and P.A.K. contributed to the evaluation pipeline of this work. Y.Z., Y.K., \nA.A., A.Y.L., E.J.T., A.K.D. and D.C.A. provided suggestions on analysis framework. UK Biobank & \nEye Vision Consortium provided the UK Biobank. All authors contributed to the drafting and \nrevising of the manuscript.\nCompeting interests P.A.K. has acted as a consultant for DeepMind, Roche, Novartis, Apellis \nand BitFount, and is an equity owner in Big Picture Medical. He has received speaker fees from \nHeidelberg Engineering, Topcon, Allergan and Bayer.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-023-06555-x.\nCorrespondence and requests for materials should be addressed to Yukun Zhou or  \nPearse A. Keane.\nPeer review information Nature thanks Pranav Rajpurkar, Pamela Sankar and the other, \nanonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer \nreports are available.\nReprints and permissions information is available at http://www.nature.com/reprints.\nExtended Data Fig. 1 | Illustration of training pipeline of RETFound and \ncomparison baselines.  The compared baselines include SL-ImageNet, \nSSL-ImageNet, and SSL-Retinal. SL-ImageNet trains the model via supervised \nlearning on ImageNet-21k (14 million images and categorical labels); \nSSL-ImageNet trains the model on ImageNet-1k (1.4 million images) via SSL; \nSSL-Retinal trains the model on retinal images via SSL from scratch; RETFound \ntrains the model on retinal images via SSL from the weights of SSL-ImageNet. \n*kayak picture is used to illustrate the method pipeline.\nArticle\nExtended Data Fig. 2 | Performance (AUPR) on ocular disease diagnostic \nclassification.  a, internal evaluation, models are adapted to each dataset via \nfine-tuning and internally evaluated on hold-out test data. The dataset details \nare listed in Supplementary Table 1. b , external evaluation, models are fine- \ntuned on one diabetic retinopathy dataset and externally evaluated on the \nothers. c, performance on ocular disease prognosis. The models are fine-tuned \nto predict the conversion of fellow eye to wet-AMD in 1 year and evaluated \ninternally. For each task, we trained the model with 5 different random seeds, \ndetermining the shuffling of training data, and evaluated the models on the \ntest set to get 5 replicas. We derived the statistics with the 5 replicas. The error \nbars show 95% confidence intervals and the bars’ centre represents the mean \nvalue of the AUPR. We compare the performance of RETFound with the most \ncompetitive comparison model to check if statistically significant differences \nexist. p-value is calculated with the two-sided t-test and listed in the figure.\nExtended Data Fig. 3 | Performance (AUPR) on 3-year incidence prediction  \nof systemic diseases with retinal images. a, internal evaluation, models are \nadapted to curated datasets from MEH-AlzEye via fine-tuning and internally \nevaluated on hold-out test data. b , external evaluation, models are fine-tuned \non MEH-AlzEye and externally evaluated on UK Biobank. Data for internal and \nexternal evaluation is described in Supplementary Table 2. For each task, we \ntrained the model with 5 different random seeds, determining the shuffling of \ntraining data, and evaluated the models on the test set to get 5 replicas.  \nWe derived the statistics with the 5 replicas. The error bars show 95% \nconfidence intervals and the bars’ centre represents the mean value of the \nAUPR. We compare the performance of RETFound with the most competitive \ncomparison model to check if statistically significant differences exist. p-value \nis calculated with the two-sided t-test and listed in the figure.\nArticle\nExtended Data Fig. 4 | Adaptation efficiency in exemplar applications. \nAdaptation efficiency refers to the time required to achieve training convergence. \nWe show the performance on validation sets with the same hyperparameters \nsuch as learning rate. The gray dash lines highlight the time point when the \nmodel checkpoint is saved and the time difference between RETFound and the \nmost competitive comparison model is calculated. RETFound saves 80% of \ntraining time in adapting to 3-year incidence prediction of myocardial \ninfarction and 46% in diabetic retinopathy MESSIDOR-2. 95% confidence \nintervals of AUROC are plotted in colour bands and the mean values are shown \nas centre lines.\nExtended Data Fig. 5 | Comparison of different SSL strategies in RETFound \nframework.  We show AUROC of predicting ocular diseases and systemic \ndiseases by the models pretrained with different SSL strategies, including  \nthe masked autoencoder (MAE), SwAV, SimCLR, MoCo-v3, and DINO. The \ncorresponding quantitative results for the contrastive SSL approaches are \nlisted in Supplementary Table 4. For each task, we trained the model with 5 \ndifferent random seeds, determining the shuffling of training data, and \nevaluated the models on the test set to get 5 replicas. We derived the statistics \nwith the 5 replicas. The error bars show 95% confidence intervals and the bars’ \ncentre represents the mean value of the AUPR. We compare the performance of \nRETFound with the most competitive comparison model to check if statistically \nsignificant differences exist. p-value is calculated with the two-sided t-test and \nlisted in the figure.\nArticle\nExtended Data Fig. 6 | Qualitative results of RETFound. a, Reconstructed \ncolour fundus photographs and optical coherent tomography scans from \nhighly masked images in pretext task. Although with few patches visible, \nRETFound infers the retina-specific anatomical structures (e.g. optic nerve and \nretinal nerve fibre layer) and disease lesions, which are markers for multiple \ndiseases. b, Heatmaps highlighting the areas that contribute to the classification  \nof the models in various downstream tasks. Red colour indicates high \ncontribution. The well-defined pathologies of ocular diseases are identified \nand used for classification. For the prediction of systemic diseases, some \nanatomical structures associated with systemic conditions, e.g. optic nerve \nand vasculature on CFP and ganglion cell layer and macular area on OCT, are \nhighlighted.\nExtended Data Fig. 7 | Performance on various age distributions in \npredicting myocardial infarction.  The disease group remains unchanged \n(mean value of age is 72.1) while the four control groups are sampled with \nvarious age distributions (mean values of age are respectively 66.8, 68.5, 70.4, \nand 71.9). The X axis shows the age difference between disease group and \ncontrol groups. With each control group, we evaluate the performance of \npredicting myocardial infarction. The performance of RETFound remains \nrobust to age difference while that of compared models drops when the age \ndifference decreases. Logistic regression uses age as input. The logistic \nregression performs well when age difference is large (about 6) but clearly \nworse than SSL models when the difference becomes smaller. 95% confidence \nintervals are plotted in colour bands and the mean value of performances are \nshown as the band centres.\nArticle\nExtended Data Fig. 8 | Reliability diagrams and expected calibration error \n(ECE) for prediction models.  Reliability diagrams measure the consistency \nbetween the prediction probabilities of an event (e.g. myocardial infarction) \nwith the actual chance of observing the event. The dashed line (diagonal line) \nindicates a perfectly calibrated model and the deviation represents the \nmiscalibration. RETFound is closest to diagonal lines and the ECE is lowest \namong all models.\nExtended Data Fig. 9 | Performance in predicting heart failure across \nethnicities.  We show AUROC of predicting 3-year heart failure in subsets with \ndifferent ethnicity, including White, Asian or Asian British, and Black or Black \nBritish subgroups, the three largest major categories of ethnicity as described \nby the UK Government’s Office for National Statistics. Data is from MEH-AlzEye \ntest set. The first column shows the performance on all test data, followed by \nresults on three subgroups. The cohort quantity is listed in titles. We trained \nthe model with 5 different random seeds, determining the shuffling of training \ndata, and evaluated the models on the test set to get 5 replicas. We derived the \nstatistics with the 5 replicas. The error bars show 95% confidence intervals and \nthe bars’ centre represents the mean value of the AUPR. We compare the \nperformance of RETFound with the most competitive comparison model to \ncheck if statistically significant differences exist. p-value is calculated with the \ntwo-sided t-test and listed in the figure.\nArticle\nExtended Data Fig. 10 | Performance in predicting myocardial infarction \nacross ethnicities.  We show AUROC of predicting 3-year myocardial infarction \nin subsets with different ethnicity. Data is from MEH-AlzEye test set. The first \ncolumn shows the performance on all test data, followed by results on White, \nAsian or Asian British, and Black or Black British cohorts. The cohort quantity is \nlisted in titles. We trained the model with 5 different random seeds, determining  \nthe shuffling of training data, and evaluated the models on the test set to get 5 \nreplicas. We derived the statistics with the 5 replicas. The error bars show 95% \nconfidence intervals and the bars’ centre represents the mean value of the \nAUPR. We compare the performance of RETFound with the most competitive \ncomparison model to check if statistically significant differences exist. p-value \nis calculated with the two-sided t-test and listed in the figure.\nExtended Data Fig. 11 | Performance in predicting ischaemic stroke across \nethnicities.  We show AUROC of predicting 3-year ischaemic stroke in subsets \nwith different ethnicity. Data is from MEH-AlzEye test set. The first column \nshows the performance on all test data, followed by results on White, Asian or \nAsian British, and Black or Black British cohorts. The cohort quantity is listed in \ntitles. We trained the model with 5 different random seeds, determining the \nshuffling of training data, and evaluated the models on the test set to get 5 \nreplicas. We derived the statistics with the 5 replicas. The error bars show 95% \nconfidence intervals and the bars’ centre represents the mean value of the \nAUPR. We compare the performance of RETFound with the most competitive \ncomparison model to check if statistically significant differences exist. p-value \nis calculated with the two-sided t-test and listed in the figure.\nArticle\nExtended Data Table 1 | Confusion matrix on 3-year prediction of myocardial infarction\na, confusion matrix with CFP. b, confusion matrix with OCT. RETFound shows the highest sensitivity and specificity.\n1 nature portfolio  |  reporting summaryMarch 2021\nCorresponding author(s): Pearse A. Keane, Yukun Zhou\nLast updated by author(s): Aug 29, 2023\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection The code used to train, fine-tune, and evaluate RETFound from Yukun Zhou is available at https://github.com/rmaphoh/RETFound_MAE \nwhich bases on PyTorch. Additionally, a Keras version implemented by Yuka Kihara is available at https://github.com/uw-biomedical-ml/\nRETFound_MAE. Please note that the reported results are obtained from PyTorch models. Image data was extracted from Dicom files with \nPydicom v2.3.0 (https://pydicom.github.io). Images were processed with automated retinal image analysis tool AutoMorph v1.0 (https://\ngithub.com/rmaphoh/AutoMorph). \nData analysis Data was analysed with Python v3.6 (https://www.python.org/), NumPy v1.19.5 (https://github.com/numpy/numpy), SciPy v1.5.4 (https:// \nwww.scipy.org/), seaborn v0.12.0 (https://github.com/mwaskom/seaborn), Matplotlib v3.6.1 (https://github.com/matplotlib/matplotlib), \npandas v1.5.0 (https://github.com/pandas-dev/pandas), Scikit-Learn v1.1.3 (https://scikit-learn.org/stable), Pillow v9.2.0 (https://pypi.org/\nproject/Pillow). Heatmaps were generated with RELPROP (https://github.com/hila-chefer/Transformer-Explainability).\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\n2 nature portfolio  |  reporting summaryMarch 2021\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \n \nThe MIDAS dataset consists of routinely collected healthcare data. Due to its sensitive nature and the risk of reidentification, the dataset is subject to controlled \naccess via a structured application process. Data access enquiries may be made to enquiries@insight.hdrhub.org and we will aim to respond within two weeks. \nFurther details about the data request pipeline may be found on the INSIGHT Health Data Research Hub website https://www.insight.hdrhub.org. The AlzEye \ndataset is subject to the contractual restrictions of the data sharing agreements between National Health Service Digital, Moorfields Eye Hospital and University \nCollege London and are not available for access beyond the AlzEye research team. National and international collaborations are welcomed though restrictions on \naccess to the cohort mean that only the AlzEye researchers can directly analyse individual-level systemic health data. More details can be found at https://\nreadingcentre.org/studies/artificial_intelligence/alzeye. UK Biobank data is available at https://www.ukbiobank.ac.uk/. \n \nData for ocular disease experiments are publicly available online and can be accessed via the links: IDRID (https://ieee-dataport.org/open-access/indian-diabetic-\nretinopathy-image-dataset-idrid), MESSIDOR-2 (https://www.adcis.net/en/third-party/messidor2/), APTOS-2019 (https://www.kaggle.com/competitions/\naptos2019-blindness-detection/data), PAPILA (https://figshare.com/articles/dataset/PAPILA/14798004/1), Glaucoma Fundus (https://dataverse.harvard.edu/\ndataset.xhtml?persistentId=doi:10.7910/DVN/1YRRAC), JSIEC (https://zenodo.org/record/3477553), Retina (https://www.kaggle.com/datasets/jr2ngb/\ncataractdataset), OCTID (https://borealisdata.ca/dataverse/OCTID). \nHuman research participants\nPolicy information about studies involving human research participants and Sex and Gender in Research. \nReporting on sex and gender Biological sex information for MEH-MIDAS and MEH-AlzEye was collected via self-report. MEH-MIDAS includes 37,401 \npatients (16,429 female, 20,966 male, and 6 unknown) and MEH-AlzEye includes 353,157 patients (190,494 female and \n162,663 male). Experiments were conducted both on female and male. We used all MEH-MIDAS data to develop RETFound \nmodels and subsets of MEH-AlzEye for downstream validation (detailed in Supplementary Table 2).\nPopulation characteristics MEH-MIDAS is a retrospective dataset which includes the complete ocular imaging records of 37,401 patients with diabetes \nwho were seen at Moorfields Eye Hospital, London, United Kingdom between 2000 and 2022. The age distribution has a \nmean value of 64.5 and standard deviation of 13.3. The ethnicity distributes diversly: British (13.7%), Indian (14.9%), \nCaribbean (5.2%), African (3.9%), other ethnicity (37.9%), not stated (24.4%). MEH-MIDAS includes various imaging devices, \nsuch as topcon 3DOCT-2000SA (Topcon), CLARUS (ZEISS), and Triton (Topcon). \n \nMEH-AlzEye is a retrospective cohort study linking ophthalmic data of 353,157 patients, who attended Moorfields Eye \nHospital between 2008 and 2018, with systemic health data from hospital admissions across the whole of England. Systemic \nhealth data are derived from Hospital Episode Statistics (HES) data relating to admitted patient care (APC), with a focus on \ncardiovascular disease and all-cause dementia. More details can be found in the method section. Selections of study cohort \nwere shown in Supplementary Figure 2-6 and characteristics were listed in Supplementary Table 2.  \n \nThe UK Biobank includes 502,665 UK residents aged between 40 and 69 years who are registered with the National Health \nService. Among all participants, 82,885 get CFP and OCT examinations and a total of 171,500 retinal images are collected. \nSelections of study cohort were shown in Supplementary Figure 2-6 and characteristics were listed in Supplementary Table 2.\nRecruitment MEH-MIDAS is a retrospective dataset which includes the complete ocular imaging records of 37,401 patients with diabetes \nwho were seen at Moorfields Eye Hospital, London, United Kingdom between 2000 and 2022. MEH-AlzEye is a retrospective \ncohort study linking ophthalmic data of 353,157 patients who attended Moorfields Eye Hospital between 2008 and 2018. \nEthics oversight This study involves human participants and was approved by the London-Central Research Ethics Committee (18/LO/1163, \napproved 01/08/2018), Advanced statistical modelling of multimodal data of genetic and acquired retinal diseases (20/\nHRA/2158, approved 05/05/2020), and the Confidential Advisory Group for Section 251 support (18/CAG/0111, approved \n13/09/2018). The National Health Service Health Research Authority gave final approval on 13 September 2018. Moorfields \nEye Hospital NHS Foundation Trust validated the de-identifications. Only de-identified retrospective data was used for \nresearch, without the active involvement of patients.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\n3 nature portfolio  |  reporting summaryMarch 2021\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size Data for developing RETFound model was from MoorfIelds Diabetic imAge dataSet (MEH-MIDAS) and public data (totalling 904,170 CFPs and \n736,442 OCTs). Data for ocular disease diagnosis were from public datasets, detailed in Supplementary Table 1. Data for systemic disease \nprediction were from Moorfields AlzEye project and selected cohorts were introduced in Supplementary Table 2. Datasets were chosen based \non the availability of labels that would permit external validation of the different fine-tuned RETFound models, which is dependent on the \nspecific clinical task being evaluated. The chosen external validation datasets were deemed to be suitable based on their parameters, which \nare summarised Supplementary Information Table 1 Dataset characteristics. Formal sample size calculations were not performed due to the \nlack of established methods when applied to machine-learning classification studies.\nData exclusions Data failed image processing with AutoMorph were excluded. Data without systemic health labels were excluded. For more details please \nrefer to the method section.\nReplication All patients were randomly selected and were not correlated in any way. The replication of experiment results were confirmed in 5 times with \n5 different random seeds.\nRandomization The training/validation/testing data for downstream tasks were randomly splitted in ratio of 55%:15%:30%. For each patient, we only included \nthe left eye data from one visit to avoid potential bias by inconsistent individual visits.\nBlinding When assigning patients randomly to training, validation and testing groups investigators were blinded to patient covariates and all features in \nthe dataset not required to perform the research. \nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7455974221229553
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6729507446289062
    },
    {
      "name": "Generalizability theory",
      "score": 0.5476067662239075
    },
    {
      "name": "Retinal",
      "score": 0.49594125151634216
    },
    {
      "name": "Machine learning",
      "score": 0.48499175906181335
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.4460066556930542
    },
    {
      "name": "Disease",
      "score": 0.41018328070640564
    },
    {
      "name": "Medicine",
      "score": 0.18429753184318542
    },
    {
      "name": "Pathology",
      "score": 0.17971184849739075
    },
    {
      "name": "Neuroscience",
      "score": 0.11497992277145386
    },
    {
      "name": "Ophthalmology",
      "score": 0.08752584457397461
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210150574",
      "name": "Moorfields Eye Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1325675289",
      "name": "Moorfields Eye Hospital NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I28407311",
      "name": "University of Manchester",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I36234482",
      "name": "University of Bristol",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I205051169",
      "name": "Kingston University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I130828816",
      "name": "University of Leeds",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1324925910",
      "name": "St Thomas' Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I43439940",
      "name": "University of Southampton",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I126231945",
      "name": "Queen's University Belfast",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I177639307",
      "name": "University of Dundee",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2799390153",
      "name": "Leeds Teaching Hospitals NHS Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210111135",
      "name": "King's College Hospital NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I23923803",
      "name": "University of Exeter",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I124357947",
      "name": "University of London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I7882870",
      "name": "University of Glasgow",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I84884186",
      "name": "Newcastle University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802266942",
      "name": "Gloucestershire Hospitals NHS Foundation Trust",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I165862685",
      "name": "St George's, University of London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1118541",
      "name": "University of East Anglia",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802775644",
      "name": "Royal Liverpool University Hospital",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210126452",
      "name": "The Retina Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210140114",
      "name": "Scripps (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I150209017",
      "name": "Scripps Institution of Oceanography",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1308923049",
      "name": "University Hospitals Birmingham NHS Foundation Trust",
      "country": "GB"
    }
  ]
}