{
  "title": "EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling",
  "url": "https://openalex.org/W3201114717",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A542753336",
      "name": "Wang Jue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2467397172",
      "name": "Wang Haofan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355702414",
      "name": "Deng, Jincan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2652874598",
      "name": "Wu WeiJia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365459415",
      "name": "Zhang, Debing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3096655658",
    "https://openalex.org/W2100031962",
    "https://openalex.org/W2124219775",
    "https://openalex.org/W3177224328",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3177174258",
    "https://openalex.org/W3133825286",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2172191903",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3166049810",
    "https://openalex.org/W2253806798",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2996060033",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2147483361",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3035271324",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2768477045",
    "https://openalex.org/W2287418003",
    "https://openalex.org/W3156669901",
    "https://openalex.org/W2876111955",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2889968917",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "While large scale pre-training has achieved great achievements in bridging the gap between vision and language, it still faces several challenges. First, the cost for pre-training is expensive. Second, there is no efficient way to handle the data noise which degrades model performance. Third, previous methods only leverage limited image-text paired data, while ignoring richer single-modal data, which may result in poor generalization to single-modal downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble Confident Learning to obtain a less noisy data subset. Extra rich non-paired single-modal text data is used for boosting the generalization of text branch. We achieve the state-of-the-art performance on Chinese cross-modal retrieval tasks with only 1/10 training resources compared to CLIP and WenLan, while showing excellent generalization to single-modal tasks, including text retrieval and text classification.",
  "full_text": "EfÔ¨ÅcientCLIP: EfÔ¨Åcient Cross-Modal Pre-training by Ensemble ConÔ¨Ådent\nLearning and Language Modeling\nJue Wang‚Ä†‚Ä°, Haofan Wang‚Ä°, Jincan Deng‚Ä°, Weijia Wu‚Ä°¬∂, Debing Zhang‚Ä°\n‚Ä†Zhongnan University of Economics and Law, ¬∂Zhejiang University, ‚Ä°Kuaishou Technology\n201821090281@stu.zuel.edu.cn, {wanghaofan, dengjincan}@kuaishohu.com, weijiawu@zju.edu.cn ,\nzhangdebing@kuaishohu.com\nAbstract\nWhile large scale pre-training has achieved great achieve-\nments in bridging the gap between vision and language, it\nstill faces several challenges. First, the cost for pre-training\nis expensive. Second, there is no efÔ¨Åcient way to handle the\ndata noise which degrades model performance. Third, pre-\nvious methods only leverage limited image-text paired data,\nwhile ignoring richer single-modal data, which may result\nin poor generalization to single-modal downstream tasks. In\nthis work, we propose an EfÔ¨ÅcientCLIP method via Ensemble\nConÔ¨Ådent Learning to obtain a less noisy data subset. Extra\nrich non-paired single-modal text data is used for boosting\nthe generalization of text branch. We achieve the state-of-the-\nart performance on Chinese cross-modal retrieval tasks with\nonly 1/10 training resources compared to CLIP and WenLan,\nwhile showing excellent generalization to single-modal tasks\nincluding text retrieval and text classiÔ¨Åcation.\n1 Introduction\nPre-training has achieved great progress in Natural Lan-\nguage Processing (Brown et al. 2020; Devlin et al. 2018;\nLewis et al. 2019; Liu et al. 2019; Radford et al. 2019) and\nComputer Vision (Chen et al. 2020; Dosovitskiy et al. 2020;\nKolesnikov et al. 2019) tasks. Recently, multi-modal pre-\ntraining (Huo et al. 2021; Radford et al. 2021; Jia et al. 2021)\nattracts widespread attention, where large scale weakly cor-\nrelated multi-modal data on the internet is utilized to learn\ncross-modal representation by contrastive learning (Hadsell,\nChopra, and LeCun 2006). However, existing multi-modal\npre-training methods face several challenges. First, as the\nscale of data expands, pre-training requires expensive train-\ning resources, CLIP (Radford et al. 2021) costs 3584 GPU-\ndays and WenLan (Huo et al. 2021) costs 896 GPU-days\nboth on NVIDIA A100. Second, the raw internet data is\nnoisy (as shown in Appendix, Figure 7), which wastes train-\ning resources and extremely degrades the performance of\nmodel (Algan and Ulusoy 2021; Carlini and Terzis 2021;\nNorthcutt, Jiang, and Chuang 2021; Shen et al. 2020). Third,\nprevious multi-modal pre-training methods only use lim-\nited image-text pairs, while ignoring richer single-modal text\ndata, which results in poor generalization to many down-\nstream NLP tasks and scenes (Li et al. 2020).\nThe work is Ô¨Ånished when Jue Wang and Weijia Wu are interns.\nPerformancedegradation\nModel\nScoring &TrainingRanking & FilteringShadow UpdatingUpdate\nRe-TrainingData Cleaning\nEnhance GeneralizationModel\nMODEL\nModel\nModel\nHuge Text Dataset\nNLP Semi-Supervised task\nNoisy Dataset\nCorrespondingText+\n+CorrespondingText\nTraining\nECL\nFigure 1: Comparison of training processes.The Ô¨Çowchart\nabove shows a common procedure of previous cross-modal\npre-training methods. The Ô¨Çowchart below shows our differ-\nence, where the training dataset is progressively updated via\nan Ensemble ConÔ¨Ådent Learning (ECL) strategy.\nThere are numerous works (Northcutt, Jiang, and Chuang\n2021; Shen et al. 2020; Xie et al. 2020) that contribute to\ntraining noise-robust models or developing noise-free strate-\ngies for vision tasks. In contrast, few studies have focused on\nhandling noise in cross-modal pre-training. Inspired by con-\nÔ¨Ådent learning (Northcutt, Jiang, and Chuang 2021), we pro-\npose a novel method EfÔ¨ÅcientCLIP via an Ensemble Con-\nÔ¨Ådent Learning (ECL) strategy, where models at different\ntraining epochs are ensembled to estimate the data distribu-\ntion through several continuous iterative steps. We show that\nECL strategy helps model save training resources and speeds\nup convergence by building a smaller subset with less noisy\ndata for training. To further boost the generalization perfor-\nmance on downstream tasks and scenes, motivated by the\nsuccess of self-supervised tasks (Devlin et al. 2018; Dong\net al. 2019; Lewis et al. 2019; Gao, Yao, and Chen 2021)\nin NLP, we use extra non-paired single-modal data which\narXiv:2109.04699v2  [cs.CL]  22 Sep 2021\nis available in much richer scenes than cross-modal data,\nspeciÔ¨Åcally text data in our case, to enhance the text encoder\nvia self-supervised learning tasks. For the purpose of saving\nthe cost of training to the greatest extent, our image encoder\nis built on the top of CLIP (Radford et al. 2021) because of\nits robustness validated by (Shen et al. 2021). To make up\nfor the scarcity of the open Chinese multi-modal model and\nsolve various Chinese application scenarios, our model is\ntrained with data in Chinese. The key difference of the train-\ning process between our method and previous works can be\nfound in Figure 1.\nWe evaluate the performance on both cross-modal and\nsingle-modal tasks and achieve state-of-the-art (SOTA) re-\nsults on corresponding benchmarks. For instance, with only\n1/10 training resources compared to WenLan (SOTA in Chi-\nnese) and CLIP, EfÔ¨ÅcientCLIP outperforms WenLan (Huo\net al. 2021) on Recall@1 (R@1) and Recall@5 (R@5) by\n3.6% and 5.9% respectively, on the cross-modal retrieval\ntasks of AIC-ICC (Wu et al. 2019) (the largest Chinese cap-\ntion dataset). We also exceed CLIP (Radford et al. 2021) on\nR@1 and R@5 by 4.39% and 5.11% on COCO (Veit et al.\n2016) text-to-image retrieval task. Moreover, enriched by\nextra single-modal data, EfÔ¨ÅcientCLIP also shows great gen-\neralization on single-modal tasks. We outperform the SOTA\non text retrieval task and exceed benchmarks with a large\nmargin on text classiÔ¨Åcation task. All experiments are con-\nducted on zero-shot setting, in addition to the text classiÔ¨Åca-\ntion task. Details can be found in Sec 4.5. Our key contribu-\ntions are summarized as below:\n1. We propose EfÔ¨ÅcientCLIP via an Ensemble ConÔ¨Å-\ndent Learning (ECL)strategy for efÔ¨Åcient cross-modal pre-\ntraining, which pruning noisy data adaptively.\n2. We achieve the state-of-the-art on cross-modal retrieval\ntasks with only 1/10 training resourcescompared to bench-\nmarks such as CLIP and WenLan.\n3. We show the value of non-paired single-modal data\non cross-modal pre-training, and achieve great generaliza-\ntion ability to downstream single-modal tasks.\n2 Related Work\n2.1 Contrastive Learning\nContrastive learning (Hadsell, Chopra, and LeCun 2006; Le-\nKhac, Healy, and Smeaton 2020) is a kind of representa-\ntion learning by contrasting positive pairs against negative\npairs. Recent literatures (Chen et al. 2020; Gao, Yao, and\nChen 2021; He et al. 2020) have achieved great success in\nboth representation learning and unsupervised learning tasks\nby contrastive learning. Among them, SimCLR (Chen et al.\n2020) proposes to match positive sample pairs by various\ndata augmentations, and uses larger batch size with richer\nnegative samples for contrastive learning to get a robust vi-\nsual representation. MOCO (He et al. 2020) uses the dictio-\nnary as a queue to solve the dependence of contrastive learn-\ning on large batch size. In the Ô¨Åeld of NLP, SimCSE (Gao,\nYao, and Chen 2021) proposes to use dropout to make the\nsame text for mini data augmentation as a positive sample\npair, and uses different texts as negative samples.\n2.2 Two-Tower Structure Models\nTwo-tower structure models trained on large scale web\ndatasets have been successfully used in multi-modal tasks\nand achieve excellent results on many downstream tasks. In\ntwo-tower structures, visual and language information are\nencoded independently without any fusion via contrastive\nlearning scheme for efÔ¨Åcient discrimination. CLIP (Radford\net al. 2021) trains with 400 million image-text data pairs\nfrom the internet after simple data cleaning, it achieves ex-\ncellent performance on image-text retrieval tasks and zero-\nshot image recognition tasks. ALIGN (Jia et al. 2021) further\nexpands the scale of image-text paired data, and uses 1 bil-\nlion data to train without any cleaning, indicating that the ex-\npansion of the data scale can suppress the inÔ¨Çuence of noise\nfor the model to some extent. WenLan (Huo et al. 2021)\ntrains with Chinese paired data and achieves the best per-\nformance on image-text retrieval task in the Chinese scene.\n2.3 Learning with Noisy Data\nThere is a continued interest in community on training mod-\nels directly from enormous and low-cost web data. How-\never, a large scale of noisy data existing on the internet\ncause negative effects for model training. To utilize numer-\nous and low-cost web data, many researchers attempt to ex-\nplore noise-robust methods (Chen and Gupta 2015; Joulin\net al. 2016; Fergus et al. 2005; Schroff, Criminisi, and Zis-\nserman 2010) for training models. Fergus et al. (2005) ex-\nploit images from the Google search engine for image cat-\negorization based on an improved pLSA method. Schroff,\nCriminisi, and Zisserman (2010) propose an approach to au-\ntomatically harvest images on the internet for learning vi-\nsual recognition classiÔ¨Åers. Krause et al. (2016) show that\nmodels learning from a large scale of web data outperform\nthose learning from a limited amount of human-annotated\ndataset for classiÔ¨Åcation tasks. ConÔ¨Ådent Learning (North-\ncutt, Jiang, and Chuang 2021) achieves effective noise Ô¨Ålter-\ning through three continuous iterative steps of Count, Rank\nand Prune, and Re-Training, and trains a more robust model.\nHowever, these works are only designed for common ob-\nject detection and classiÔ¨Åcation tasks, which cannot directly\nbe applied effectively to cross-modal pre-training tasks. As\nfar as we know, current multi-modal pre-training methods\nstill lack an effective training method to effectively learn\nfrom noisy data. Thus, in this paper, we propose an Ensem-\nble ConÔ¨Ådent Learning (ECL) strategy to Ô¨Åll this gap.\n3 Methodology\nIn this section, we illustrate the steps of our efÔ¨Åcient train-\ning strategy. In Sec 3.1, we Ô¨Årst introduce how we transfer\nCLIP‚Äôs knowledge from English domain to Chinese domain.\nFrom Sec 3.2 to Sec 3.3, we demonstrate two core opera-\ntions, Ensemble ConÔ¨Ådent Learning and extra single-modal\nself-supervised learning respectively. We also describe a\nmemory queue method based on the idea of dictionary as\na queue (He et al. 2020) and the pseudocode of our ap-\nproach, which can be found in Appendix B and Figure 9\nin Appendix, respectively. The pipeline is shown in Figure\n2.\n‚Ñ±ùêº(‚àô) \nTraining Positive Samples\nTraining Negative Samples\n... ...\nPush to Image Queue\n‰∫¨ Ââß Áôæ Ëä± Ëµ† Ââë\nÁå´ Êúâ ‚Äî Êù° ÂëΩ\n‚Ñ±ùëá(‚àô) \n...\n‚ÑíùëÅùê∂ùê∏ \nÁå´\nÊúâ\n‰πù\nÊù°\nÂëΩ\n‚Ñíùê∂ùê∏ \nMasked Language\nÁå´\nÊúâ\n‚Äî\nÊù°\nÂëΩ\n‚ÑíùëÅùê∂ùê∏ \nText match Image InfoNCE Loss\nCross Entropy Loss\nÔºàaÔºâImage Encoder Ôºàb)Text Encoder\nFigure 2: Illustration of the whole pipeline. (a) Image Encoder: Image features are pushed to the image queue (see Appendix\nB for details of image queue). (b) Text Encoder: Text features are obtained by text encoder for contrastive learning with image\nqueue. Notation: FI(.) and FT(.) denote image encoder and text encoder (Dosovitskiy et al. 2020), respectively.\n3.1 Cross-language Knowledge Distillation\nIt is time-costing and expensive to train a cross-modal pre-\ntrained model from scratch. To conveniently obtain a text\nencoder in Chinese domain, we perform cross-language\nknowledge distillation. A transformer-based Chinese text\nencoder (refer to Appendix, Table 13 for detailed structure)\nis initialized as a student model and distills knowledge from\na teacher model (CLIP‚Äôs text encoder is adopted).\nLoss=\nN‚àë\ni=1\n(FC(T(i)\nc ) ‚àíFE(T(i)\ne ))2. (1)\nThe Chinese-English pairs are deÔ¨Åned as Tce =\n{(T(i)\nc ,T(i)\ne ),i ‚àà [1,N]}, where T(i)\nc and T(i)\ne represent\nChinese and English texts, respectively. Figure 3 shows the\nstructure of distillation model, we freeze the parameters of\nthe English teacher model FE, and only update the Chinese\nstudent model FC to minimize the distance between their\noutputs via an MSE loss formulated as above.\n‰∏Ä‰∏™Áî∑‰∫∫Ëµ∞Âú®Ë°ó‰∏ä\nAmanwalksonthestreet\nFigure 3: Cross-language distillation. FC represents the\nChinese Encoder, FE represents the English Encoder. The\nfrozen model is indicated by *. Our goal is to minimize the\ndistance between their outputs via an MSE loss.\n3.2 Ensemble ConÔ¨Ådent Learning\nLarge scale image-text datasets crawled from the inter-\nnet have been widely used in pre-training. As indicated\nby (Carlini and Terzis 2021; Northcutt, Jiang, and Chuang\n2021; Shen et al. 2020), excessive noisy data negatively\nimpacts the model‚Äôs performance and training efÔ¨Åciency.\nALIGN (Jia et al. 2021) and WenLan (Huo et al. 2021)\ndemonstrate that the large-scale pre-training with expensive\nresources can suppress the inÔ¨Çuence of noise to some ex-\ntent, but these training resources are usually not available\nfor general researchers. An alternative is to establish cleaned\ndatasets like COCO (Veit et al. 2016), or Conceptual Cap-\ntions (Sharma et al. 2018). However, as the high-quality\nmanual annotations or complex treatments are needed, they\nare often limited by their scales, which result in the poor\ngeneralization. Driven by these obstacles, a compromised\nway based on the idea of ConÔ¨Ådent Learning (Northcutt,\nJiang, and Chuang 2021) named as Ensemble ConÔ¨Ådent\nLearning (ECL) is designed. Instead of removing all noisy\npairs at once, ECL strategy adopts the same way as Con-\nÔ¨Ådent Learning and adaptively remove noisy data from the\ntraining set, as the distribution of dataset is hard to estimate.\nAs the model generally performs more discriminative\non those high-correlated pairs (related experiments can be\nfound in Appendix A.1), we propose to adaptively and it-\neratively remove the noisy pairs (low-correlated) by means\nof the discriminative ability of model to data distribution.\nFirst, we use the distillation model as initialization which\nis already equipped with the basic discriminative powers (it\nhas been proved in Appendix A.1), and additionally estab-\nlish a scoring shadow model that only updates parameters\nat the beginning of each epoch. For the Kth epoch, we de-\nÔ¨Åne the dataset as DK = {d1,d2...di...dn}where d is an\nimage-text pair, the score gained through the scoring shadow\nmodel (SK). The pre-trained model which keeps updating is\ndenoted as T. ECL strategy consists of three steps:\n(1) Scoring & Training: . For each image-text pair di,\nwe use the scoring shadow model to calculate its correlation\nscore SK(di) at the current epoch, and update the total score\nCK+1(di) based on the exponential smoothing algorithm:\nCK+1(di) =Œ±‚àóCK(di) +SK(di), (2)\nwhere Œ±refers to the smoothing factor. In this way, we get\nthe correlation score of each image-text pair, and train the\npre-trained model T with contrastive loss at the same time.\n(2) Ranking & Filtering: Based on the total score\nCK(di) of each pair, we reorder the dataset DK in descend-\ning order: D‚àó\nK = {d‚àó\n1,d‚àó\n2,¬∑¬∑¬∑ ,d‚àó\nn}. To Ô¨Ålter out the noisy\npairs, we set a Ô¨Åltered rank Œª and retain the training pairs\nwhose correlation ranks before the Œª. The obtained pairs are\nused as the training dataset at the (K+ 1)th epoch:\nDK+1 = {d‚àó\n1,d‚àó\n2,¬∑¬∑¬∑ ,d‚àó\nŒª√ón}. (3)\n(3) Shadow Updating: At the beginning of the next\nepoch, we update the parameters of the scoring shadow\nmodel SK+1 with the parameters of the pre-trained model\nT. Then, we return to step (1) for the training of next epoch.\nWe analyze the effectiveness of ECL in Appendix A.2 to\nshow the reason of using the distillation model, and the ef-\nfect of ‚Äùensemble‚Äù.\n3.3 Masked Language Model\nExisting cross-modal pre-trained models utilize large scale\nimage-text pairs from the internet, while not realizing that\nthose paired datasets are usually scene-limited. We observe\nthat image-text pairs are common in some speciÔ¨Åc domains,\nsuch as Wikipedia, News and several human-annotated pub-\nlic datasets. However, in other more professional domains\nsuch as Technology and Medical, paired data is scarce,\nsingle-modal non-paired data is abundant instead. Driven\nby the idea of multimodal few-shot learning (Tsimpoukelli\net al. 2021), we additionally leverage extra single-modal text\ndata from various scenes for self-supervised learning to en-\nhance the generalization of model to downstream tasks.\nWe adopt the Masked Language Model (MLM) task pro-\nposed by BERT (Devlin et al. 2018) for self-supervised\ntraining. MLM takes advantage of bidirectional seman-\ntic information, and only requires a simple masking op-\neration on the original text. Given a sequence of text as\n{t1,t2,t3,¬∑¬∑¬∑ ,tn}, we randomly replace the words with\n[mask] token. T and Tmask represent the unsubstituted and\nthe substituted tokens respectively. The optimization objec-\ntive of the text encoder is formulated as:\nW‚àó= arg max\nW\nP(Tmask|T,W ), (4)\nwhere W refers to the parameters of the text encoder.\nWe Ô¨Ånd that cross-modal pre-training can beneÔ¨Åt from ad-\nditional prevalent single-modal non-paired data. The reasons\ncan be explained from two aspects, as described next.\nFirst, the model gains more knowledge about rich scenes\nfrom additional text data, which helps avoid the problem of\nlimited distribution of image-text pairs. Second, the MLM\ntask facilitates the model to pay more attention to the re-\nlationship between words and helps the model avoid catas-\ntrophic forgetting of token-level knowledge, which results in\nthe improvement of transferring tasks (Gao, Yao, and Chen\n2021). Ablation study can be found in Sec 4.6.\n4 Experiment\nWe describe the training datasets in Sec 4.1 and implemen-\ntation details from Sec 4.2 to 4.4 before showing promising\nresults on the public evaluation datasets. We compare with\nthe state-of-the-art in Sec 4.5 and conduct comprehensive\nablation studies in Sec 4.6 to exhibit the effect of each mod-\nule. More results can be found in Appendix.\n4.1 Datasets\nWe establish 3 training sets and 1 validation set based on\npublic datasets (including Chinese-English text pairs, Chi-\nnese text data) and web-crawled datasets (including Image-\nText training set and validation set). The details of datasets\ncan be found in Appendix C.\n4.2 Knowledge Distillation\nTo conduct cross-language knowledge distillation, we build\nour distillation model on the top of frozen CLIP ViT-\nB/32 (Radford et al. 2021), and add additional transformer\nlayers on the text encoder for learning. Two models with\ndifferent hyperparameters are constructed as listed in Ap-\npendix, Table 13. We train the distilled Chinese text encoder\non a Chinese-English text paired dataset of 80 million size\nwith a batch size of 256. Due to the high correlation between\nChinese-English data, the convergence speed of knowledge\ndistillation is 2.5-3.5 times faster than regular contrastive\nlearning. In the experiment, we use 24 V100 GPUs training\nfor two days to converge to the local optimal value.\n4.3 Cross-modal Pre-Training\nPre-training with ECL\nQueue Sizes-Training TimeQueue Sizes-R@1\nFigure 4: Impact of queue sizes.The vertical axis is the size\nof memory queue. R@1 represents the recall 1 score on the\ntext-to-image task of AIC-ICC. Training time represents the\ntraining time (second) of models with different queue sizes\nfor training 100 steps.\nTo reduce the cost of pre-training, we use the distilla-\ntion model as initialization and introduce half-precision op-\ntimization based on Deepspeed (Rasley et al. 2020) frame-\nwork for efÔ¨Åcient distributed training. More tricks for speed-\ning up are illustrated in Appendix F.\nFor the choice of hyperparameters, the similar setting as\nMOCO (He et al. 2020) and CLIP (Radford et al. 2021) is\nadopted: we set the learning rate to 2e-3, weight decay to 1e-\n4, dropout to the default 0.1, and use cosine warmup sched-\nule as learning rate adjuster. To select the optimal size of\nthe queue, we experiment with several parameters including\n200, 2,000, 10,000, 20,000, 50,000, 100,000 and 500,000.\nAs shown in Figure 4, the larger the size of the queue, the\nhigher the accuracy of our model. However, when the query\nsize reaches 50,000, the model accuracy gets saturated and\nis no longer signiÔ¨Åcantly improved. Meanwhile, a too large\nqueue will bring about a great reduction in training efÔ¨Å-\nciency. Considering the trade-off between performance and\ntraining efÔ¨Åciency, the queue size is Ô¨Ånally set to 50000.\nWe use the ECL method to Ô¨Ålter data in each epoch and\nevaluate the model‚Äôs performance on the validation set every\n1000 steps. After 9 training epochs, there are only 120 mil-\nlion data left. We Ô¨Ånd that the score of the validation set does\nnot increase signiÔ¨Åcantly later, thus we stop the data Ô¨Ålter-\ning and continue training on the 120 million data. BeneÔ¨Åting\nfrom the effectiveness of adaptively Ô¨Åltering data, we only\nspend 7 days training our model. Compared with the model\npre-trained on 300 million data without ECL, our model sur-\npasses it by 8-14% on the R@1 on the test set of AIC-ICC.\nThe result can be found in Figure 5.\nECLWithout ECL\n1.5X efficiency\n2X performerce\nFigure 5: Impact of ECL on performance. The vertical\naxis is the training steps, and the horizontal axis is the per-\nformance of the f1 score of the training model. As shown,\nECL method helps to improve the convergence speed and\nmodel performance.\nTo select a suitable Ô¨Åltered rank for ECL, we experiment\nwith 4 values (0.7, 0.8, 0.9, 0.99), and use the R@1 of text-\nto-image task on AIC-ICC for evaluation. As illustrated in\nTable 1, when the Ô¨Åltered rank is 0.9, the model reaches the\nhighest R@1 of text-to-image task on AIC-ICC.\nWe claim that training directly on a large amount of noisy\ndataset, the model‚Äôs performance is constrained as shown in\nFigure 5. With the existence of ECL strategy which is used to\neffectively Ô¨Ålter out noisy data, our model can be re-trained\nfrom higher-quality data adaptively and shows faster train-\ning speed and better performance. More detailed analysis of\nECL strategy is conducted in Sec 4.6, which shows that the\nmethod of re-training from numerous noisy datasets to high-\nquality datasets can effectively improve the generalization\nability and performance of the model.\nFiltered rank 0.7 0.8 0.9 0.99\nR@1 17.66 17.82 18.02 14.70\nTable 1: Impact of Ô¨Åltered rank. As the Ô¨Åltered rank in-\ncreases from 0.7 to 0.9, the R@1 improves. However, once\nthe Ô¨Åltered rank is too high, the recall drops.\nCombined with MLM Task Training\nIn the pre-training process, we design two dataloaders for\nthe image-text pairs and text data. The batch size of image-\ntext data and text data are 180 and 40 respectively. The text\ndata is loaded similarly with BERT: the masking probability\nis set to 15% for each token, and each masked token has a\n20% chance to be replaced by other tokens.\nEfÔ¨ÅcientCLIP is a multi-task pre-trained model, and its\nÔ¨Ånal performance is affected by the weights between con-\ntrastive learning task and MLM task. In our experiment,\nwe set the weights of these two tasks to be proportional to\ntheir batch sizes. If the batch size of text data is too large, it\nwill impair our model‚Äôs performance on image-text retrieval\ntasks; if the batch size is too small, the introduction of MLM\ntask will hardly improve the model‚Äôs performance. In order\nto verify the effect of the weight of MLM task on model‚Äôs\nimage-text retrieval ability, we calculate the f11 score under\ndifferent text batch sizes on the validation set. The results\nare shown in Table 2.\nText batch size 30 40 50 60\nVal f1 score 0.452 0.460 0.444 0.442\nTable 2: Impact of the text batch size. When the batch size\nis less than 40, the MLM task is helpful for improving the\nmodel‚Äôs performance. But once the batch size is larger than\n40, the greater the weight of MLM task, the model will re-\nduce the learning ability on image-text retrieval tasks, and\nthe performance of image-text retrieval will decrease.\nIn order to make the model more focused on image-text\nretrieval tasks in the later stage, we choose to remove the\nMLM task after stopping data Ô¨Åltering.\n4.4 Adaptive Cleaning Data\nDataset size 300M 200M 100M\nBad case percent % 28.0 8.0 1.0\nGood case percent % 21.0 47.0 66.0\nTable 3: Performance of ECL to clean noise. Dataset size,\nbad case percent and good case percent represent the size of\nthe dataset we sample, the proportion of bad cases and good\ncases in the sampled data respectively.\nTo verify the effect of our model for adaptive Ô¨Åltering, we\nrandomly sample 1000 samples from the 300 million, 200\nmillion, and 100 million datasets (200 million and 100 mil-\nlion datasets are Ô¨Åltered by ECL on the 300 million dataset)\nrespectively, and label them manually. We count the propor-\ntion of bad cases and good cases in the samples and present\nit in Table 3. We also visualize the distribution of the man-\nual data, as shown in Figure 6. It can be seen that ECL has\na strong ability to remove noisy data and separate the good\ncases from the other cases.\n4.5 Comparison to State-of-the-art\nIn this section, we evaluate the effectiveness (generaliza-\ntion and discriminative ability) of EfÔ¨ÅcientCLIP under dif-\nferent scenarios. To measure the capability of task-agnostic\nmodels, zero-shot evaluation have been widely adopted and\n1f1 = 2¬∑precision¬∑recall\nprecision+recall\nGood DataClean DataNoisy Data\n Good DataClean DataNoisy Data\n Good DataClean DataNoisy Data\n300M TraingSet 200M TraingSet 100M TraingSet\nFigure 6: Distribution of training set with different sizes. The vertical axis and horizontal axis represent the score of the\ncorresponding epoch and the total score, respectively. Good cases, clean cases, noisy cases represent the highly correlated\npairs, correlated pairs, uncorrelated pairs (judge by human subjectivity), respectively. The Ô¨Årst Ô¨Ågure demonstrates the situation\nof Ô¨Årst epoch where the total score equal to single epoch score.\nproved as being much more representative of a model‚Äôs abil-\nity (Radford et al. 2021). Thus, we conduct cross-modal re-\ntrieval in both Chinese and English datasets under zero-shot\nsetting. To further illustrate the zero-shot transfer ability to\ndownstream task, we also evaluate on single-modal tasks,\nsuch as text classiÔ¨Åcation and text retrieval. The CLIP model\nused in this section is CLIP ViT-B/32 because our model is\nbased on this model to show the text encoder‚Äôs improvement\nby our approach.\nCross-Modal Retrieval in Chinese\nAIC-ICC (Wu et al. 2017) is the only publicly-available\nChinese multi-modal dataset. The training set contains\n210,000 images and 1.05 million descriptions (5 captions for\neach image), and the validation set contains 30,000 images\nand 150,000 descriptions. We evaluate the zero-shot transfer\nability on the test subset (10,000 data) following the same\nsetting as WenLan (Huo et al. 2021). To compare with other\nbenchmarks (mainly trained with English data), we trans-\nlate Chinese text into English via Google translation API.\nWe also distill CLIP into Chinese domain (details can be\nseen in Sec 4.2) to decrease the impact of translation, while\nUNITER (single tower) uses the translation directly.\nTable 4 presents the cross-modal retrieval results. We\ncan observe that our EfÔ¨ÅcientCLIP signiÔ¨Åcantly outperforms\nother benchmarks on both the text-to-image and image-to-\ntext retrieval subtasks, showing the effectiveness of the pro-\nposed approach in this paper.\nMethod Text2Image(%) Image2Text(%)\nR1 R5 R10 R1 R5 R10\nCLIP‚Ä† 7.80 18.50 25.00 13.40 27.30 35.10\nUNITER‚Ä† 9.80 23.30 31.40 14.80 29.80 37.90\nCLIP‚Ä° 11.06 24.89 33.28 18.33 34.45 43.23\nWenLan 14.40 30.40 39.10 20.30 37.00 45.60\nE-CLIP 18.02 36.33 45.91 26.33 44.73 52.95\nTable 4: Evaluation results for the cross-modal retrieval\ntasks on the AIC-ICC test subset. ‚Ä† and ‚Ä° means trans-\nlation and distillation, respectively. E-CLIP represents the\nEfÔ¨ÅcientCLIP.\nCross-Modal Retrieval in English\nBecause of language differences which come from the\ndifferent culture and style, transferring a model to another\nlanguage will pose a huge challenge to the generalization\nof model. When CLIP transfers to Chinese domain, its per-\nformance is far lower than the current Chinese model (Huo\net al. 2021). So, to provide the more comprehensively em-\npiricle evidence of EfÔ¨ÅcientCLIP‚Äôs generalization ability,\nwe directly compare with benchmark (Radford et al. 2021)\ntrained on English datasets. As the original EfÔ¨ÅcientCLIP is\ntrained on Chinese datasets, we transfer the EfÔ¨ÅcientCLIP\n(text encoder) into English domain through cross-language\nknowledge distillation (details are the same as the distillation\nof Chinese-to-English but the student and teacher models are\nexchanged). A common image caption dataset in English is\nadopted for evaluation.\nCOCO (Veit et al. 2016) is common adopted caption\ndatasets. We use the COCO2014 test set (5000 images and\n24716 captions) to evaluate our EfÔ¨ÅcientCLIP model. The\nresults are shown in Table 5. The distilled EfÔ¨ÅcientCLIP\noutperforms CLIP by nontrivial margin on text-to-image re-\ntrieval, while achieving competitive results on image-to-text\nretrieval tasks.\nMethod Text2Image(%) Image2Text(%)\nR1 R5 R10 R1 R5 R10\nCLIP 30.75 56.60 67.73 50.78 75.10 83.70\nE-CLIP‚Ä° 35.12 61.71 72.77 50.70 76.56 84.98\nTable 5: Evaluation results of the cross-modal retrieval\ntasks on the COCO2014 test set. ‚Ä° mean distillation. E-\nCLIP represents the EfÔ¨ÅcientCLIP.\nSingle-Modal Evaluation\nPrevious cross-modal pre-trained models (Radford et al.\n2021; Huo et al. 2021) usually cannot effectively adapt to\nsingle-modal (NLP) scenarios (Li et al. 2020). We bridge\nthis gap between cross-modal pre-training and its single-\nmodal (NLP) counterpart by validating the generalization\nability of EfÔ¨ÅcientCLIP on several NLP tasks.\n(1) Text ClassiÔ¨Åcation. To evaluate the NLU (Natu-\nral Language Understanding) capability on short texts, we\nadopt news classiÔ¨Åcation task (TNEWS) in Chinese dataset\nCLUE (Xu et al. 2020) as our benchmark. Each item in\nMethod Text Match(%)\nR1 R5 R10\nCLIP‚Ä† 22.94 36.25 42.65\nWenLan 31.44 45.32 52.98\nCLIP‚Ä° 37.34 53.26 60.64\nSimBERT 38.4 55.41 62.10\nSimCSE 39.64 56.49 63.24\nEfÔ¨ÅcientCLIP 43.48 60.36 67.74\nTable 7: Evaluation results for short text retrieval on\nAIC-ICC test subset. ‚Ä† and ‚Ä° means translation and dis-\ntillation, respectively.\nTNEWS has three attributes, NEWS ID, NEWS TYPE and\nNEWS TITLE. The training set contains 53,360 samples,\nand the test set and validation set both contain 10,000 sam-\nples. We compare with some popular benchmarks, such as\nXLNET (Yang et al. 2019), ALBERT-xxlarge (Lan et al.\n2019), and ROBERTA-xxlarge on the validation set. The re-\nsults are shown in Table 6.\nModel XLNET RoBERTa* ALBERTA* E-CLIP\nAcc(%) 56.10 58.61 59.50 67.20\nTable 6: Results on TNEWS dataset. * represents as ‚Äù-\nxxlarge‚Äù. E-CLIP represents the EfÔ¨ÅcientCLIP.\n(2) Text Retrieval. To measure the discriminative ability\nof text embedding and the zero-shot transfer ability of fac-\ning unseen tasks, we evaluate on AIC-ICC (Wu et al. 2017)\ntest subset (same as Section 4.5.1, but only use the texts)\nwhere each image has 5 corresponding descriptions. We ran-\ndomly select one of the 5 texts as the query, and the rest of\n4 texts as the key. We reproduce SimCSE (Gao, Yao, and\nChen 2021) and SIMBERT as our compared benchmarks\n(details can be found in supplementary materials Appendix\nE). For CLIP, we translate the Chinese text into English via\nGoogle translation API and use the distilled CLIP for com-\nparison. The results can be found in Table 7. We also eval-\nuate on AFQMC and LCQMC (Liu et al. 2018) which are\ncommonly used Chinese semantic matching datasets. The\nresults can be found in Appendix G, Table 15.\nWe also provide the results of comparison on text retrieval\ntask in English domain, which can be seen in Appendix G.1.\nAs illustrated above, EfÔ¨ÅcientCLIP shows advantages\nover all other competitors on several common NLP tasks,\nwhich validates that extra single-modal pre-training enriches\nthe discriminative power of single-modal (text) branch.\n4.6 Ablation Study\nIn the section, we investigate the effects of single-modal pre-\ntraining and Ensemble ConÔ¨Ådent Learning (ECL) strategy\non the overall performance on common benchmarks.\nSingle-Modal Pre-Training\nWe evaluate the impact of the extra single-modal pre-\ntraining branch on cross-modal and single-modal tasks. The\ncross-modal and text retrieval tasks are conducted on AIC-\nICC dataset, while the text classiÔ¨Åcation is evaluated on\nTNEWS dataset.The results are shown in Table 8.\nAs shown, the extra single-modal (text) pre-training\nbranch not only enhances model‚Äôs zero-shot transfer ability\non text classiÔ¨Åcation and text retrieval (single-modal) tasks,\nbut also improves the performance on text-to-image retrieval\n(cross-modal) tasks. As expected, the performance gains\nfrom text retrieval (3.18%‚Üë) and text classiÔ¨Åcation (3.80%‚Üë)\nare higher than cross-modal retrieval (0.75%‚Üë).\nModels T2I (R@1) TC (Acc) TR (R@1)\nE-CLIP (wo) 17.27 63.40 40.30\nE-CLIP (w) 18.02 67.20 43.48\nTable 8: Effect of single-modal pre-training. T2I, TC and\nTR denote text-to-image retrieval, text classiÔ¨Åcation and\ntext retrieval respectively. (w) and (wo) mean training with\nsingle-modal pre-training or not. E-CLIP is the EfÔ¨Åcient-\nCLIP.\nEnsemble ConÔ¨Ådent Learning\nThe other key part in our approach is the Ensemble Con-\nÔ¨Ådent Learning strategy, where we adaptively Ô¨Ålter data for\nimproving training efÔ¨Åciency and model performance. We\ntrain a series of four models under different settings and\nevaluate on the same tasks as Sec 4.6.1. The results are\nshown in Table 9.\nMethod T2I TC TR\nEfÔ¨ÅcientCLIP (300M) 10.77 65.20 32.70\nEfÔ¨ÅcientCLIP (200M*) 14.87 66.70 40.65\nEfÔ¨ÅcientCLIP (100M*) 16.72 65.92 40.38\nEfÔ¨ÅcientCLIP+ECL (300M) 18.02 67.20 43.48\nTable 9: Effect of ECL strategy. T2I, TC and TR denote\ntext-to-image retrieval (R@1), text classiÔ¨Åcation (Acc) and\ntext retrieval (R@1) respectively. M stands for a million of\nuncleaned data. * means the data is Ô¨Åltered with ECL strat-\negy.\nAs the table 9 shown, as the quality of data increases,\nwhile the scale of data decreases, the retrieval performance\nstill gains signiÔ¨Åcant improvement, but NLP downstream\nperformance degrades. Second, instead of only using the\nhighly correlated data (denoted with *), ECL strategy adopts\nan adaptive way to select training subset from huge noisy\ndataset, which is shown to be more generalized on both\ncross-modal and single-modal tasks.\n5 Conclusion\nIn this work, we introduce an efÔ¨Åcient cross-modal pre-\ntraining method called EfÔ¨ÅcientCLIP. We propose an En-\nsemble ConÔ¨Ådent Learning (ECL) strategy to adaptively Ô¨Ål-\nter the noisy dataset. We show the value of single-modal\nnon-paired data for improving the generalization perfor-\nmance. We claim that EfÔ¨ÅcientCLIP achieves the SOTA per-\nformance on Chinese cross-modal retrieval tasks, surpass-\ning CLIP in English scene, and outperforms benchmarks on\nsingle-modal downstream tasks.\nDisney PrincessFashion Couple Black Band Watch\n6271-78-9,6-adamas,71939c,01098673 S-guide, power amplification.\nScore:0.341 Score:0.357\nScore:0.102 Score:0.113\nCarniola bee David btyttowzirtualFouder\npasswordkeylapa,lange,game,lodge,,Marintal\nScore:0.367 Score:0.325\nScore:0.134 Score:0.128\nFigure 7: Example of raw text-image pairs on the internet (The captions are translated from Chinese by Google Translation\nAPI). The high-quality pairs are indicated by GREEN (The Ô¨Årst row of pictures), the noisy pairs are indicated by RED (The\nsecond row of pictures). The scores generated by our model shows semantic similarity for image-text pairs.\nA Analyses for ECL\nA.1 Related Experients of ECL\nF1 Score-ThresholdsLoss-Thresholds\nFigure 8: Model performance on validation sets with dif-\nferent thresholds. Higher f1 score or lower loss demon-\nstrate the better performance of model in the validation sets.\nHighly correlated (high quality) image-text pairs contain\nrich and useful supervision which can be sufÔ¨Åciently learned\nby model. In contrast, the knowledge of noisy pairs are hard\nto be absorbed, which leads to the model‚Äôs ability of discrim-\ninating the difference between high quality pairs and noisy\npairs in the training. We conduct experiments to investi-\ngate this perspective. We train a cross-modal model through\nweakly supervised contrastive learning on noisy 300 million\nimage-text pairs, and evaluate its performance on datasets\nof different qualities. We use the distillation model trained\nin Sec 3.1 to calculate the cosine similarity score of each\ntraining pair, and randomly select 20,000 pairs with cosine\nsimilarity score larger than the thresholds (we set to the ta-\nble 10) as validation sets. We assume that the validation\nset with a higher threshold approximately represents higher\nquality. As illustrated in Figure 8, the model performs bet-\nter on the higher quality (higher thresholds) validation sets.\nThe result indicates that the model performs better on those\nhigh-correlated data even if it is trained on the noisy dataset,\nand shows the crucial difference between the model‚Äôs per-\nformance on validation sets with different thresholds (quali-\nties). Thus, the above result provides a powerful evidence of\nthe training model‚Äôs ability to separate the high quality pairs\nfrom the noisy pairs.\nThreshold 0.28 0.30 0.32 0.34\nR@1 10.87 11.45 12.34 12.66\nTable 10: Impact of the setting of thresholds. The R@1\nrepresents Recall@1 (on the AIC-ICC text-to-image task)\nof models Ô¨Ånetuned with corresponding datasets.\nIn addition, to conÔ¨Årm that the similarity scores calculated\nby the distillation model are positively correlated with the\nground truth similarity, we Ô¨Ånetune the distillation model on\nthose datasets of different thresholds (each dataset is com-\nposed of 100,000 randomly selected pairs). We evaluate the\nÔ¨Ånetuned model on text-to-image task of the AIC-ICC (Wu\net al. 2019) test set. The results are shown in Table 10 (be-\ncause the number of data of the datasets with higher thresh-\nolds are lower than 100,000, so the highest threshold used is\n0.34). We Ô¨Ånd that the higher threshold is set to Ô¨Ålter data,\nthe higher R@1 score on the test set. It can be explained that\nbecause the model is Ô¨Ånetuned on a higher quality dataset\nwith higher threshold, the Ô¨Ånetuned model will perform bet-\nter on the test set. So, as illustrated in above analysis, what\nthe similarity scores produced by the distillation model are\npositively correlated with the correlation of image-text pairs\ncan be veriÔ¨Åed.\nA.2 Effectiveness of ECL\nRegarding the three-step process of Sec 3.2, we have two\nconsiderations: (1) A motivation to use the distillation model\nas the initial model is not only to save training cost, but also\nto endow the model with initial ability to judge data distri-\nbution. Thus, our model can give a score with high conÔ¨Å-\ndence even at the beginning. (2) If we only take the score\nof the distillation model as the total score, it is easy to Ô¨Ål-\nter out the strongly related image-text pairs with low scores.\nIn this way, the Ô¨Ånal model is easy to converge to the lo-\ncal optimum relying on the distillation model. Therefore, we\npropose to ensemble scores of multiple models through ex-\nponential smoothing. The details of the ablation experiment\ncan be found in Appendix H.\nBefore explaining the ECL, we Ô¨Årst give a priori Q(scor-\ning shadow model), and set the Ô¨Åltered rank as Œª(0 < Œª <\n1). Based on the prior Q, we can estimate the distribution of\nthe data, and after re-ranking, Ô¨Ålter out the data ranking af-\nter Œª. We set the number of clean data asn, the noisy data as\nm. After Ô¨Åltering, the number of clean data and noisy data\nchange to n* and m* respectively. Then, this process can al-\nways satisfy n‚àó\nn > m‚àó\nm . Related experiments can be seen in\nSec 4.4.\nLet us give an example: we assume that there are(n+ m)\ndog images Id in the dataset, while only n images corre-\nspond to the dog descriptions Dd (the dataset represents as\n{Dd ‚à©Id}), and the mimages correspond to the noisy de-\nscriptions Dn (the dataset represents as {Dn ‚à©Id}). Our\nideal optimization function isW‚àó= arg max\nW\nP(Dd|Id,W).\nHowever, based on the Monte Carlo method, the radio of\nsampling data of {Dd ‚à©Id}to {Dn ‚à©Id}in the train-\ning is n : m. So the real optimization function can be ap-\nproximately represented asW‚àó= arg max\nW\n(P(Dd|Id,W)+\nm\nnP(Dn|Id,W)) because of the training with noisy data.\nWe assume that after the Ô¨Årst round of Ô¨Åltering, the num-\nber of dataset {Dd ‚à©Id}is n‚àó = v1 √ón(0 < v1 < 1),\nand dataset {Dn ‚à©Id}is m‚àó = u1 √óm(0 < u1 < 1),\nand according to the prior, there always satisÔ¨Åes n‚àó\nn >\nm‚àó\nm (which also satisÔ¨Åes v1 > u1). Then, in the next\nepoch, the optimization function can be expressed as W‚àó=\narg max\nW\n(P(Dd|Id,W) +m\nn\nu1\nv1\nP(Dn|Id,W)). In this iter-\nation, after K epochs, the optimization function is W‚àó =\narg max\nW\n(P(Dd|Id,W) + m‚àèK\n1 ui\nn‚àèK\n1 vi\nP(Dn|Id,W)). We de-\nnote\n‚àèK\n1 ui‚àèk\n1 vi\n(0 <\n‚àèK\n1 ui‚àèk\n1 vi\n< 1) as a regular term, then we\ncan regard ECL as a regularization process. As the ECL is\nan iterative method, we can control the regular term to the\nmost suitable extent for the best model‚Äôs performance. ECL\ngreatly reduces the impact of the noisy sample during train-\ning, making the model more accurate to predict for input\nfrom the real world. Related experiments can be seen in Sec\n4.6.2 and Sec 4.3.\nB Memory Queue\nContrastive learning methods heavily depend on the number\nand quality of negative samples for generating high-quality\nrepresentations that are robust to noise interference and ben-\neÔ¨Åcial to performance improvement. In order to free nega-\ntive samples from the constraints of batch size, we introduce\na memory queue to expand the number of negative samples.\nIn MOCO (He et al. 2020), a momentum-based updating\nrule is designed for the key encoder to ensure the consis-\ntency of the dictionary of a queue. However, when the size\nof queue is too large, it is hard to maintain consistency due\nto the parameters update of key encoder which is not sync\nwith the query encoder. The discrepancy of negative sam-\nples‚Äô features might result in training instability and perfor-\nmance degradation (He et al. 2020). To alleviate the prob-\nlem, we utilize the frozen image encoder as the key encoder\nand only train the text encoder. Since the keys in memory\nqueue all come from the same image encoder, there is no\ninconsistency no matter how large the queue is. The con-\ntrastive loss in our method is formulated as following:\nE\nx,x+,x‚àí\n[\n‚àílog\n(\nef(x)T f(x+)\nef(x)T f(x+) + ef(x)T f(x‚àí)\n)]\n. (5)\nwhere (x,x+) and (x,x‚àí) are positive and negative pairs,\nrespectively. f(x) represents the encoder forward function.\nC Creating Datasets\nC.1 Public Datasets\nChinese-English Text Pairs . To conduct cross-language\nknowledge distillation, we collect Chinese-English paired\ndata from AI Challenge Machine Translation 2(denoted as\nAIC), WMT203, and other public Chinese-English transla-\ntion datasets, totaling 80 million translation pairs.\nChinese Text Data. For the extra single-modal branch, we\nadopt the single-modal text dataset from CLUE (Xu et al.\n2020), which is the largest Chinese language understand-\ning evaluation benchmark. We clean the dataset by removing\ndata with low Chinese character ratio (50% in our case) and\nmeaningless symbols. We Ô¨Ånally get a Chinese text dataset\nof 56G size, totaling 20,329,263 documents.\nC.2 Web-crawled Datasets\nImage-Text Pairs for training. To construct a large scale\nImage-Text dataset for contrastive learning, we establish a\nChinese word dictionary including 4 million Chinese vocab-\nulary. Each word in the dictionary is used as query to crawl\nimage-text pairs from Chinese Search Engine (Baidu Pic-\ntures and Baidu Baike). We simply clean the raw crawled\npairs as we did for text data and get a Chinese image-text\npaired training dataset of 300 million pairs. Details of pre-\nprocessing of the training set can be found in Appendix D.\nImage-Text Pairs for validation . To verify the effective-\nness of our method in time, we extra collect image-text pairs\nfrom various scenarios on the internet. We collect image-\ntext pairs from Baidu Pictures, Baidu Baike, Toutiao, hash-\ntag, and other sources. SpeciÔ¨Åcally, we Ô¨Årst use the distilla-\ntion model to calculate the cosine similarity score for each\nimage-text pair. Based on these scores, we sort the dataset\nand take out the Ô¨Årst 1 million of data for next cleaning\nstage.Then, in order to extract more general and common\ndata, we extract the data containing common words deÔ¨Åned\nin a 40 thousand entity vocabulary which is collected from\nGitHub. Finally, we clean out the top-10,000 (sorted by the\ncosine similarity score of pairs) image-text pairs as our val-\nidation set, which is not covered by the training set.The de-\ntails of sizes of these datasets can be found in Table 11.\n2https://challenger.ai\n3http://www.statmt.org/wmt20/\nDataset Name CET TD IMT IMV\nnumber of data 80M 20.3M 300M 10K\nsizes 14.2G 56G / /\nTable 11: Details of used datasets. CET, TD, IMT, IMV\nrepresent Chinese-English Translation, Text Data, Image-\nText Training Set and Validation Set respectively. M, K rep-\nresent a million and a thousand respectively.\nD Data Processing\nIn terms of the preprocessing of training set, we try a variety\nof methods, and prove that cleaning text branch of image-\ntext pairs has a considerably signiÔ¨Åcant improvement on the\nperformance of model. We Ô¨Årst train on the raw 300 mil-\nlion data, while the R@1 of trained model on text-to-image\ntask of AIC-ICC test set is just 6.58 which is far lower than\nthe current SOTA (Huo et al. 2021). Therefore, we try to\nperform preprocessing for text and compare the effects of\nvarious cleaning methods on model‚Äôs performance.\nFirst, we remove the redundant HTML, space symbols\n(such as ‚Äú. . . ‚Äù ‚Äú‚Äî‚Äù) and all emojis, which are meaningless\nin text. We notice that the crawled text data exists enormous\ninterval symbols such as ‚Äú&‚Äù, ‚Äú-‚Äù, etc. We replace these\ninterval symbols with ‚Äú,‚Äù to make sentences more cleaned\nand Ô¨Åt with general data. Although all we crawl are Chi-\nnese websites, some English sentences and extremely short\nsentences will inevitably appear in text data. Therefore, we\nperform rules with a Chinese character ratio of less than\n0.5 to remove English sentences, and a length which is less\nthan 4 to remove short sentences. We conduct related exper-\niments to explore the effects of removing these two kinds\nof sentences on the performance of model. We train models\nwith same hyperparameters and regular contrastive learning\nmethod on the raw 300 million data, the removal of English\nsentence, short sentence data, and the data removing two\nkinds of sentences, respectively. And we use the image-text\nretrieval tasks on AIC-ICC for evaluation. The details can be\nfound in Table 12.\nMethod Text2Image(%) Image2Text(%)\nR1 R5 R10 R1 R5 R10\nNo Clean 6.58 15.19 20.89 12.14 25.23 33.20\nC&S 10.29 22.98 30.74 20.73 37.27 46.00\nC&S&E 10.56 23.39 31.20 21.15 37.49 46.00\nC&E 10.77 23.99 31.86 21.50 38.30 46.83\nTable 12:Impact of data cleaning method.C&S, C&S&E,\nC&E represent cleaning short text, cleaning short text and\nEnglish text, cleaning English text respectively.\nThe above results suggest that when all English sentences\nare removed, the performance of model on the test set is\noptimal. So we remove all English sentences in the dataset,\nand retain extremely short sentences.\n# model: the EfficientCLIPmodel # texts[n, l], d_f: batch of aligned texts, dims of features  # t,ùúÜ, ùõº: temperature, filtered rank, smoothing value # W[d_f, vocab_size]: the mapping parameters for mlmtask # queue: memory queue with K keys [K, d_f] # n*: batch of texts for mlmtaskmodel.image_encoder= freeze_params(model.image_encoder) for iin range(epochs): score_encoder= freeze_params(model.text_encoder) for texts, images, indexes in sample(Dataset): I_f= model.image_encoder(images) # I_f: [n, d_f] I_f= cat([I_f,query], dim = 0) # I_f: [n+K, d_f]_, T_f= model.text_encoder(texts) # T_f: [n, d_f]_, T_f_s= score_encoder(texts) # T_f_s: [n, d_f]# scaled pairwise cosine similarities logits = dot(T_f, I_f.T) * exp(t) scores = dot(T_f_s, I_f.T) # get the correlation scores# update scores for corresponding data Dataset.scores[indexes] +=ùõº*scores # calculate contrastive loss  labels = arange(n+K) loss_c= cross_entropy_loss(logits, labels) # calculate mlmloss, T_emb: [n*, l, d_f]mask_texts, mlm_labels=get_batch() # masked_texts: [n*, l]T_emb, _= model.text_encoder(mask_texts)T_emb= matmul(T_emb, W)  #T_emb: [n*, l, vocab_size]loss_m= cross_entropy_loss(T_emb, mlm_labels) # AdamWupdate: model.text_encodernetwork  loss = loss_c+loss_mloss.backward() update(model.text_encoder)  enqueue(queue, I_f)  # enqueue the current minibatchdequeue(queue)  # dequeue the oldest minibatch# filter data with score rank behind ùúÜin Dataset rank(Dataset.scores) filter(Dataset.scores, ùúÜ)\nFigure 9: Pseudocode for an implementation of EfÔ¨Åent-\nCLIP.The text encoder have two outputs, including the em-\nbeddings of tokens (the Ô¨Årst output) and the features of sen-\ntences (the second output).\nE Reproduction of SimCSE\nFor reproducing SimCSE to a great extent, we refer to the\nhyperparameters in SimCSE paper (Gao, Yao, and Chen\n2021) and utilize 10 million sentences collected from Chi-\nnese Wikipedia (for unsupervised) and the BQ Corpus (Chen\net al. 2018) dataset (for supervised) to reproduce. In terms of\ndata preprocessing, we choose to divide the text into multi-\nple shorter texts with a length of less than 77 as input data\n(the same as EfÔ¨ÅcientCLIP), and use text data processing\nmethod the same as Appendix D to perform simple data\ncleaning. For better performance, we choose Roberta case\n(the best model mentioned in SimCSE paper) as the back-\nbone of our SimCSE model. We add [CLS] token to each\ntext, and use the representation at the position of [CLS] to-\nken as its sentence embedding. In the experiment, we set\nthe learning rate to 1e-5 and the weight decay to 0.0001.\nFor selection of dropout, we refer to the setting of SimCSE\npaper and choose the default 0.1. We use dropout noise as\ndata augmentation to create positive samples of text, and\nuse other randomly selected texts as negative samples for\ncontrastive learning. We refer to the recommendation of\nSimCSE, where we add Masked Language Model task as\nauxiliary loss, which helps model avoid catastrophic forget-\nting of token-level knowledge. For the purpose of speeding\nup the training of model, we also leverage MMAP method\nto load data and use half-precision optimization based on\nDeepspeed (Rasley et al. 2020) framework to accelerate dis-\ntributed training. Finally, we cost 24 GPU-days on V100\nto get a Chinese SimCSE model. After unsupervised train-\ning, we Ô¨Ånetune the model on the BQ Corpus for supervised\nlearning to obtain the best performance.\nWe also explore more data augmentation methods to en-\nhance the training effect of model. Because the efÔ¨Åciency of\ncalling Google Translate API is not high as our expectation,\nwe choose to use the Fairseq (Ott et al. 2019) framework to\ntrain a Chinese-to-English model and an English-to-Chinese\nmodel for ofÔ¨Çine back translation. We perform simple data\naugmentation through back translation to create a positive\nsample of text, and use the same training method as Sim-\nCSE to reproduce SimBERT.\nF Speeding up Training\nTo reduce the cost of pre-training, we use the distillation\nmodel as a coarse initialization. Further, in order to load\ndata with less time and memory cost, we utilize the image\nencoder of CLIP ViT-B/32 to extract image features before\ntraining and store them in a binary Ô¨Åle. While using image\nfeatures, we utilize the corresponding pointer to quickly ex-\ntract the features by MMAP (A method for disk mapping\nmemory).\nHyperparameters E-CLIP-small E-CLIP\nNumber of Layers 12 32\nHidden Size 512 512\nDropout 0.1 0.1\nAttention Dropout 0.1 0.1\nWeight Decay 0.0001 0.0001\nnumber of parameters 151M 225M\nR@1 16.82 18.02\nTable 13:Hyperparameters of two EfÔ¨ÅcientCLIP models.\nR@1 represents the R@1 score obtained by the model on\nthe text-to-image task of AIC-ICC, and E-CLIP represents\nthe EfÔ¨ÅcientCLIP.\nG Other Comparison of EfÔ¨ÅcientCLIP\nG.1 Text Retrieval in English\nTo further validate the discriminative ability of EfÔ¨Åcient-\nCLIP on text retrieval tasks, we also evaluate on COCO2014\ntest set (only use the captions). The results can be found in\nTable 14.\nMethod Text Match(%)\nR1 R5 R10\nCLIP 38.34 59.44 68.52\nEfÔ¨ÅcientCLIP 40.40 62.20 71.80\nTable 14: Evaluation results for short text retrieval on\nCOCO2014 test set.\nMethod AFQMC(%) LCQMC(%)\nR1 R5 R10 R1 R5 R10\nCLIP‚Ä† 6.43 14.80 19.96 57.17 78.22 82.74\nWenLan 9.72 18.68 23.92 66.27 87.01 90.82\nCLIP‚Ä° 12.03 24.59 31.54 74.53 93.50 96.18\nSimBERT 11.23 25.10 30.98 76.10 93.23 96.44\nSimCSE 13.34 27.22 33.78 78.56 95.52 97.32\nE-CLIP 15.77 30.72 36.54 81.58 98.10 98.88\nTable 15: Evaluation results for short text retrieval on\nAFQMC and LCQMC. ‚Ä†and ‚Ä°means translation and dis-\ntillation, respectively. E-CLIP represents the EfÔ¨ÅcientCLIP.\nH Ensemble Scoring Shadow Models\nTo provide a powerful evidence for the effectiveness of inte-\ngrating multiple models at different epochs during Ô¨Åltering,\nwe train a model by only using the distillation model as the\nscoring shadow model. The results are shown in Table 16.\nModels T2I (R@1) TC (Acc) TR (R@1)\nE-CLIP (wo) 17.03 67.20 42.04\nE-CLIP (w) 18.02 67.20 43.48\nTable 16: Effect of ensemble scoring shadow models. T2I,\nTC and TR denote text-to-image retrieval, text classiÔ¨Åcation\nand text retrieval respectively. E-CLIP represents the EfÔ¨Å-\ncientCLIP.\nReferences\nAlgan, G.; and Ulusoy, I. 2021. Image classiÔ¨Åcation with\ndeep learning in the presence of noisy labels: A survey.\nKnowledge-Based Systems, 215: 106771.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165.\nCarlini, N.; and Terzis, A. 2021. Poisoning and Backdooring\nContrastive Learning. arXiv preprint arXiv:2106.09667.\nChen, J.; Chen, Q.; Liu, X.; Yang, H.; Lu, D.; and Tang, B.\n2018. The bq corpus: A large-scale domain-speciÔ¨Åc chinese\ncorpus for sentence semantic equivalence identiÔ¨Åcation. In\nProceedings of the 2018 conference on empirical methods in\nnatural language processing, 4946‚Äì4951.\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020.\nA simple framework for contrastive learning of visual repre-\nsentations. In International conference on machine learning,\n1597‚Äì1607. PMLR.\nChen, X.; and Gupta, A. 2015. Webly supervised learning\nof convolutional networks. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, 1431‚Äì1439.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. UniÔ¨Åed language\nmodel pre-training for natural language understanding and\ngeneration. arXiv preprint arXiv:1905.03197.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFergus, R.; Fei-Fei, L.; Perona, P.; and Zisserman, A. 2005.\nLearning object categories from google‚Äôs image search. In\nTenth IEEE International Conference on Computer Vision\n(ICCV‚Äô05) Volume 1, volume 2, 1816‚Äì1823. IEEE.\nGao, T.; Yao, X.; and Chen, D. 2021. SimCSE: Simple Con-\ntrastive Learning of Sentence Embeddings. arXiv preprint\narXiv:2104.08821.\nHadsell, R.; Chopra, S.; and LeCun, Y . 2006. Dimension-\nality reduction by learning an invariant mapping. In 2006\nIEEE Computer Society Conference on Computer Vision\nand Pattern Recognition (CVPR‚Äô06), volume 2, 1735‚Äì1742.\nIEEE.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2020.\nMomentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 9729‚Äì9738.\nHuo, Y .; Zhang, M.; Liu, G.; Lu, H.; Gao, Y .; Yang, G.; Wen,\nJ.; Zhang, H.; Xu, B.; Zheng, W.; et al. 2021. WenLan:\nBridging vision and language by large-scale multi-modal\npre-training. arXiv preprint arXiv:2103.06561.\nJia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;\nLe, Q. V .; Sung, Y .; Li, Z.; and Duerig, T. 2021. Scaling\nup visual and vision-language representation learning with\nnoisy text supervision. arXiv preprint arXiv:2102.05918.\nJoulin, A.; Van Der Maaten, L.; Jabri, A.; and Vasilache, N.\n2016. Learning visual features from large weakly supervised\ndata. In European Conference on Computer Vision, 67‚Äì84.\nSpringer.\nKolesnikov, A.; Beyer, L.; Zhai, X.; Puigcerver, J.; Yung,\nJ.; Gelly, S.; and Houlsby, N. 2019. Big transfer (bit):\nGeneral visual representation learning. arXiv preprint\narXiv:1912.11370, 6(2): 8.\nKrause, J.; Sapp, B.; Howard, A.; Zhou, H.; Toshev, A.;\nDuerig, T.; Philbin, J.; and Fei-Fei, L. 2016. The unrea-\nsonable effectiveness of noisy data for Ô¨Åne-grained recog-\nnition. In European Conference on Computer Vision, 301‚Äì\n320. Springer.\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma,\nP.; and Soricut, R. 2019. Albert: A lite bert for self-\nsupervised learning of language representations. arXiv\npreprint arXiv:1909.11942.\nLe-Khac, P. H.; Healy, G.; and Smeaton, A. F. 2020. Con-\ntrastive representation learning: A framework and review.\nIEEE Access.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2019. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461.\nLi, W.; Gao, C.; Niu, G.; Xiao, X.; Liu, H.; Liu, J.; Wu, H.;\nand Wang, H. 2020. Unimo: Towards uniÔ¨Åed-modal under-\nstanding and generation via cross-modal contrastive learn-\ning. arXiv preprint arXiv:2012.15409.\nLiu, X.; Chen, Q.; Deng, C.; Zeng, H.; Chen, J.; Li, D.;\nand Tang, B. 2018. Lcqmc: A large-scale chinese question\nmatching corpus. In Proceedings of the 27th International\nConference on Computational Linguistics, 1952‚Äì1962.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nNorthcutt, C. G.; Jiang, L.; and Chuang, I. L. 2021. ConÔ¨Å-\ndent learning: Estimating uncertainty in dataset labels.Jour-\nnal of ArtiÔ¨Åcial Intelligence Research.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng,\nN.; Grangier, D.; and Auli, M. 2019. fairseq: A fast,\nextensible toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. arXiv preprint arXiv:2103.00020.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI blog, 1(8): 9.\nRasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y . 2020.\nDeepspeed: System optimizations enable training deep\nlearning models with over 100 billion parameters. In Pro-\nceedings of the 26th ACM SIGKDD International Confer-\nence on Knowledge Discovery & Data Mining, 3505‚Äì3506.\nSchroff, F.; Criminisi, A.; and Zisserman, A. 2010. Harvest-\ning image databases from the web. IEEE transactions on\npattern analysis and machine intelligence, 33(4): 754‚Äì766.\nSharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.\nConceptual captions: A cleaned, hypernymed, image alt-text\ndataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), 2556‚Äì2565.\nShen, S.; Li, L. H.; Tan, H.; Bansal, M.; Rohrbach, A.;\nChang, Z., Kai-Wei andf Yao; and Keutzer, K. 2021. How\nMuch Can CLIP BeneÔ¨Åt Vision-and-Language Tasks?arXiv\npreprint arXiv:2107.06383.\nShen, Y .; Ji, R.; Chen, Z.; Hong, X.; Zheng, F.; Liu, J.; Xu,\nM.; and Tian, Q. 2020. Noise-Aware Fully Webly Super-\nvised Object Detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n11326‚Äì11335.\nTsimpoukelli, M.; Menick, J.; Cabi, S.; Eslami, S.; Vinyals,\nO.; and Hill, F. 2021. Multimodal Few-Shot Learn-\ning with Frozen Language Models. arXiv preprint\narXiv:2106.13884.\nVeit, A.; Matera, T.; Neumann, L.; Matas, J.; and Belongie,\nS. 2016. Coco-text: Dataset and benchmark for text de-\ntection and recognition in natural images. arXiv preprint\narXiv:1601.07140.\nWu, J.; Zheng, H.; Zhao, B.; Li, Y .; Yan, B.; Liang, R.;\nWang, W.; Zhou, S.; Lin, G.; Fu, Y .; et al. 2017. Ai chal-\nlenger: A large-scale dataset for going deeper in image un-\nderstanding. arXiv preprint arXiv:1711.06475.\nWu, J.; Zheng, H.; Zhao, B.; Li, Y .; Yan, B.; Liang, R.;\nWang, W.; Zhou, S.; Lin, G.; Fu, Y .; et al. 2019. Large-scale\ndatasets for going deeper in image understanding. In 2019\nIEEE International Conference on Multimedia and Expo\n(ICME), 1480‚Äì1485. IEEE.\nXie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V . 2020. Self-\ntraining with noisy student improves imagenet classiÔ¨Åcation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 10687‚Äì10698.\nXu, L.; Hu, H.; Zhang, X.; Li, L.; Cao, C.; Li, Y .; Xu, Y .;\nSun, K.; Yu, D.; Yu, C.; et al. 2020. Clue: A chinese lan-\nguage understanding evaluation benchmark. arXiv preprint\narXiv:2004.05986.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. Advances in neural\ninformation processing systems, 32.",
  "topic": "Boosting (machine learning)",
  "concepts": [
    {
      "name": "Boosting (machine learning)",
      "score": 0.7936738729476929
    },
    {
      "name": "Computer science",
      "score": 0.7513433694839478
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7383300065994263
    },
    {
      "name": "Generalization",
      "score": 0.7070621252059937
    },
    {
      "name": "Modal",
      "score": 0.7018084526062012
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5712747573852539
    },
    {
      "name": "Training set",
      "score": 0.5664532780647278
    },
    {
      "name": "Machine learning",
      "score": 0.5332571864128113
    },
    {
      "name": "Ensemble learning",
      "score": 0.5054214000701904
    },
    {
      "name": "Bridging (networking)",
      "score": 0.4563683569431305
    },
    {
      "name": "Natural language processing",
      "score": 0.3988783359527588
    },
    {
      "name": "Mathematics",
      "score": 0.07642844319343567
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": []
}