{
    "title": "A natural language model to automate scoring of autobiographical memories",
    "url": "https://openalex.org/W4395467724",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Mistica, Meladel",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": null,
            "name": "Haylock, Patrick",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": null,
            "name": "Michalewicz, Aleksandra",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": null,
            "name": "Raad, Steph",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": null,
            "name": "Fitzgerald, Emily",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": null,
            "name": "Hitchcock, Caitlin",
            "affiliations": [
                "University of Melbourne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2907030468",
        "https://openalex.org/W2913584617",
        "https://openalex.org/W2944654332",
        "https://openalex.org/W4200560827",
        "https://openalex.org/W2143068456",
        "https://openalex.org/W4319315022",
        "https://openalex.org/W2065320600",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1969882436",
        "https://openalex.org/W4206951162",
        "https://openalex.org/W2024185813",
        "https://openalex.org/W2781550573",
        "https://openalex.org/W4245747635",
        "https://openalex.org/W3156933671",
        "https://openalex.org/W2564497866",
        "https://openalex.org/W2889234836",
        "https://openalex.org/W2885717556",
        "https://openalex.org/W4380358041",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W4376609254",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3127903797",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2160623437",
        "https://openalex.org/W2796806385",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W2473482051",
        "https://openalex.org/W2604746162",
        "https://openalex.org/W2887720804",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3047542501",
        "https://openalex.org/W4251602616",
        "https://openalex.org/W1992938996",
        "https://openalex.org/W4223610593",
        "https://openalex.org/W2012552301"
    ],
    "abstract": "Abstract Biases in the retrieval of personal, autobiographical memories are a core feature of multiple mental health disorders, and are associated with poor clinical prognosis. However, current assessments of memory bias are either reliant on human scoring, restricting their administration in clinical settings, or when computerized, are only able to identify one memory type. Here, we developed a natural language model able to classify text-based memories as one of five different autobiographical memory types (specific, categoric, extended, semantic associate, omission), allowing easy assessment of a wider range of memory biases, including reduced memory specificity and impaired memory flexibility. Our model was trained on 17,632 text-based, human-scored memories obtained from individuals with and without experience of memory bias and mental health challenges, which was then tested on a dataset of 5880 memories. We used 20-fold cross-validation setup, and the model was fine-tuned over BERT. Relative to benchmarking and an existing support vector model, our model achieved high accuracy (95.7%) and precision (91.0%). We provide an open-source version of the model which is able to be used without further coding, by those with no coding experience, to facilitate the assessment of autobiographical memory bias in clinical settings, and aid implementation of memory-based interventions within treatment services.",
    "full_text": "Behavior Research Methods (2024) 56:6707–6720\nhttps://doi.org/10.3758/s13428-024-02385-5\nORIGINAL MANUSCRIPT\nA natural language model to automate scoring of autobiographical\nmemories\nMeladel Mistica 1 · Patrick Haylock 2 · Aleksandra Michalewicz 1 · Steph Raad 2 · Emily Fitzgerald 1 ·\nCaitlin Hitchcock 2\nAccepted: 26 February 2024 / Published online: 25 April 2024\n© The Author(s) 2024\nAbstract\nBiases in the retrieval of personal, autobiographical memories are a core feature of multiple mental health disorders, and are\nassociated with poor clinical prognosis. However, current assessments of memory bias are either reliant on human scoring,\nrestricting their administration in clinical settings, or when computerized, are only able to identify one memory type. Here,\nwe developed a natural language model able to classify text-based memories as one of ﬁve different autobiographical memory\ntypes (speciﬁc, categoric, extended, semantic associate, omission), allowing easy assessment of a wider range of memory\nbiases, including reduced memory speciﬁcity and impaired memory ﬂexibility. Our model was trained on 17,632 text-based,\nhuman-scored memories obtained from individuals with and without experience of memory bias and mental health challenges,\nwhich was then tested on a dataset of 5880 memories. We used 20-fold cross-validation setup, and the model was ﬁne-tuned\nover BERT. Relative to benchmarking and an existing support vector model, our model achieved high accuracy (95.7%) and\nprecision (91.0%). We provide an open-source version of the model which is able to be used without further coding, by\nthose with no coding experience, to facilitate the assessment of autobiographical memory bias in clinical settings, and aid\nimplementation of memory-based interventions within treatment services.\nKeywords Autobiographical memory task · AMT · Large language models · Natural language processing\nIntroduction\nAutobiographical memory of one’s personal past plays a key\nrole in the formation of self-identity, and subsequently, men-\ntal health (Conway & Pleydell-Pearce, 2000; Dalgleish &\nHitchcock, 2023; Williams et al., 2007). Models of auto-\nbiographical memory (Conway & Pleydell-Pearce, 2000)\npropose that autobiographical knowledge is stored in a\nﬂuid manner, such that prior experience can be recalled\nas generalized summaries which characterize categories of\nevents (i.e., categoric memories) or extended periods of time\n(i.e., extended memories), or conversely, as speciﬁc, single-\nincident events which are isolated in space and time and\ncontain a high level of detail (i.e., speciﬁc memories). The\nability to retrieve speciﬁc memories has been implicated in\nB Meladel Mistica\nmisticam@unimelb.edu.au\nB Caitlin Hitchcock\ncaitlin.hitchcock@unimelb.edu.au\nExtended author information available on the last page of the article\nthe onset and maintenance of multiple mental health disor-\nders (Barry et al., 2021; Hallford et al., 2021a), including the\nprimary onset of symptoms (Askelund et al., 2019). Further,\nreduced ability to retrieve speciﬁc, single-incident memories\nappears to be a marker of higher chronicity of symptoms,\nas it is associated with increased frequency of depressive\nepisodes and suicide attempts (Williams & Broadbent, 1986).\nAs such, there is great potential for screening of autobio-\ngraphical memory retrieval in both a preventative context,\nto indicate those who may be at risk of developing mental\nillness, and in treatment settings, to indicate those likely to\nexperience chronicity and/or identify those likely to bene-\nﬁt from completing adjunctive memory-based interventions\n(Barry et al., 2019b; Dalgleish & Hitchcock, 2023).\nA key barrier to widespread assessment of autobiographi-\ncal memory retrieval is the scoring of the obtained memories,\nthat is, the classiﬁcation of responses as a given memory\ntype. The gold standard for assessing memory retrieval is\nthe Autobiographical Memory Task (AMT). This is tradi-\ntionally scored by hand, by trained researchers. This reliance\non human scoring limits how widely we can use the AMT,\n123\n6708 Behavior Research Methods (2024) 56:6707–6720\ndue to time constraints (each AMT takes approximately\n5 min to score), along with the reductions in the accu-\nracy of scoring caused by inter-rater variability and scorer\nfatigue. In response to this demand, Takano et al. ( 2019)\ndeveloped an automated scoring system by training a sup-\nport vector machine (SVM). However, this system simpliﬁed\ncertain details of the original AMT, conﬂating general, sum-\nmary memory types such that the prediction model only\ndistinguished between the binary class of speciﬁc versus non-\nspeciﬁc.\nIn the present study, a team of cognitive and clinical psy-\nchologists, data analysts, and computational linguists worked\ntogether to apply techniques employed in natural language\nprocessing (NLP) to develop a machine learning model\nable to classify text-based memory responses, advancing\nfrom binary classiﬁcation to a multiclass model. This model\nexpands the non-speciﬁc memory types so that there is a more\nﬁne-grained distinction of these memories. We provide an\nopen-source tool which can be used in research to enable\ncollection of very large datasets needed to answer impor-\ntant basic science questions regarding the processes through\nwhich autobiographical memory inﬂuences the development\nof mental illness. Further, the automated scoring model will\nfacilitate quick and easy assessment of autobiographical\nmemory within clinical services, thus taking important steps\ntoward implementing a personalized approach (e.g., by iden-\ntifying those individuals who might beneﬁt from adjunctive\nmemory interventions, in addition to usual care (Dalgleish &\nHitchcock, 2023)) to treatment of mental ill health.\nBackground\nThe AMT is a cued-recall task in which individuals verbally\nreport or type out a memory that comes to mind in response to\na cue word of neutral, positive or negative emotional valence\n(Williams & Broadbent, 1986). Original task instructions ask\nthe individual to provide a speciﬁc, single incident mem-\nory in response to each cue word (Williams & Broadbent,\n1986), however variations of the task have now been devel-\noped. This includes a Minimal Instructions version (Debeer\net al., 2009), where participants are simply asked to retrieve ‘a\nmemory’ (i.e., a speciﬁc event is not requested). This version\nhas been demonstrated as more sensitive to reduced mem-\nory speciﬁcity in community-based samples. An Alternating\nInstructions version (Dritschel et al., 2014) requires the par-\nticipant to alternate between retrieval of a speciﬁc memory in\nresponse to one cue, and a categoric memory in response to\nthe next cue. That is, the task requests retrieval of both spe-\nciﬁc and categoric memories. The Alternating version seeks\nto index memory ﬂexibility, that is, the ability to deliber-\nately retrieve any memory type on demand, as prior research\nhas suggested that poor mental health is characterized by\nreduced movement between speciﬁc and non-speciﬁc mem-\nory types (Hitchcock et al., 2018; Piltan et al., 2021). While\nearly versions of the AMT involved a researcher delivering\nthe task in person, with the participant verbally reporting\ntheir memories, later research has moved toward use of writ-\nten AMT instructions and text-based reporting of memories\nto enable group-based and online delivery, and improve the\nability to deliver the AMT, at-scale (though Wardell et al.\n(2021) have created a pipeline for transcription of verbally\nelicited responses into written form for scoring). For both\nverbal-report and text-based memories under each of these\nAMT versions, the classiﬁcation of an individual’s reported\nmemories has traditionally been reliant on human scoring of\nthe response, using a coding manual. The coding manual\n(Williams & Broadbent, 1986) details ﬁve different cate-\ngories of memory type; categoric, speciﬁc, extended (i.e.,\nreference to events which took place for longer than 1 day,\nsuch as a holiday or semester), semantic associate (i.e., infor-\nmation related to the cue word which is not a memory),\nor omission (i.e., text indicating that no memory has been\nretrieved, e.g., ‘I don’t know’).\nIn the ﬁrst effort to automate scoring of the AMT, Takano\net al. (2019) developed a support vector model (SVM) involv-\ning linguistically motivated feature engineering, which is\nable to classify text-based memories as either speciﬁc or\nnon-speciﬁc. The initial system was developed with Japanese\nmemories, and has now been extended (Takano et al., 2018)\nfor English-text memories written by children and English-\n, Dutch-, and Japanese-text memories written by adults\n(Takano et al., 2019). The model performs well. It reliably\ndistinguishes between speciﬁc and non-speciﬁc responses,\ndemonstrating the utility of a machine learning approach to\nscoring autobiographical memories. However, a key limita-\ntion of the SVM is the binary classiﬁcation of a memory as\nspeciﬁc or non-speciﬁc. That is, the current model is unable\nto identify categoric, extended, semantic associate, or omis-\nsion memories. Moving from human scorers to the binary\nclassiﬁcation produced by existing SVM therefore reduces\nthe richness of data. This is particularly important, as our\nresearch (Hitchcock et al., 2018; Piltan et al., 2021) has sug-\ngested that the ability to retrieve any memory type on demand\nmay be a better characterization of the memory impair-\nment experienced by those with poor mental health, than the\nability to simply retrieve speciﬁc memories. Indeed, early\nresearch (Hitchcock et al., 2018) has suggested that inter-\nventions which target retrieval of a variety of memory types\nmay potentially produce a larger effect on symptoms than\nthe effect size commonly seen for interventions which train\nretrieval of speciﬁc memories alone (Barry et al., 2019b).\nMultiple research studies (Piltan et al., 2021; Mang et al.,\n2018) have therefore begun to use the Alternating Instruc-\ntions adaption of the AMT to assess memory ﬂexibility, that\nis, the ability to move between retrieval of speciﬁc and cat-\n123\nBehavior Research Methods (2024) 56:6707–6720 6709\nTable 1 Dataset characteristics: In Datasets C and D, education and employment were calculated from participant reports of whether they were\nbest described as being currently in high school, tertiary education, taking time out for themselves, or employed\nDataset A Dataset B Dataset C Dataset D\nTakano et al. ( 2019) Marsh et al. ( 2023) (unpublished) (unpublished)\nNumber of participants 1000 62 256 244\nNumber of responses 10,000 1488 6144 5880\nMean Age (SD) 34.70 (15.4) 39.08 (15.01) 20.31 (2.82) 20.30 (3.01)\nPercentage female 60.30% 66.07% 74.31% 73.95%\nPercentage Caucasian 78.80% 65.60% 52.50% 49.52%\nPercentage completed or currently 65.40% 59.20% 50.00% ∗ 47.71%∗∗\ncompleting tertiary education\nPercentage currently 64.70% 65.11% 14.29% 16.06%\nemployed fulltime\nIntra- class correlation – 0.96 0.76-0.97 0.89\nco- efficient for human raters\nIn Dataset C, ∗ indicates that 30.95% of the participants were still in high school, while in Dataset D, ∗∗ indicates that 31.19% were in high school\negoric memory types, as opposed to indexing retrieval of\nspeciﬁc memories alone.\nApplications of NLP to memory scoring\nMethods applied within the ﬁeld of NLP provide an unprece-\ndented opportunity to improve consistency and speed of\nassessing clinically relevant cognitive processes. A recent\nsystematic review of NLP methods applied to mental health\nassessment (Zhang et al., 2022) indicated that most appli-\ncations have focused on identifying or categorizing men-\ntal health symptoms. Analyzed texts have primarily been\nderived from social media (approximately 80%), with a\nsmaller number of studies using text derived from clinical\ninterviews (7%) or narrative writing (2%) (Zhang et al.,\n2022). It therefore appears that there is considerable scope for\nexpanding NLP applications to the large volume of text that\nis collected during clinical assessments and research. Doing\nso may not just reﬁne ability to make diagnosis, as appears\nto be the aim of prior NLP applications, but also inform\nthe selection of treatment components for each individual,\nthereby improving the personalization of mental health care.\nThe automation of the AMT is an apt application for NLP\nand machine learning (ML) techniques.\nIn the recent decade, NLP has seen a seismic shift in\nthe methodologies that are applied in solving text- and\nlanguage-based problems. The shift derives from a few major\ncontributions. First, there has been a drastic shift away from\nsurface token representations as seen in more traditional\nmachine learning models as implemented by Takano et al.\n(2017, 2018, 2019) to a more distributional representa-\ntion (Pennington et al., 2014; Mikolov et al., 2013). This shift\nallowed the deﬁning of tokens and sentences in terms of the\ncontext in which they appear, rendering identifying semanti-\ncally related concepts a much easier task. In real terms, this\nmeans that if related but unexpected terminology is used,\nwe would expect that their representations will be similar,\nif they are near synonyms, which gives rise to more robust\nsystems. Second, advances in computer power allowed more\nintricate architectures to also be implemented, facilitating the\napplication of deep neural networks in sequence-to-sequence\nproblems (Sutskever et al., 2014) and paving the way for the\ndevelopment of large language models such as BERT (Devlin\net al., 2019). BERT is a masked bidirectional language model\nthat employs an attention model (V aswani et al., 2017).\nRecently, van Genugten and Schacter ( 2022) implemented a\nﬁne-tuned system over DistilBert (Sanh et al., 2019), a more\n‘compact’ version of BERT, to create an automated system\nthat scores the internal, episodic details within autobiograph-\nical memories; a construct closely related to the ability to\nretrieve speciﬁc, single-incident memories (for discussion on\nthe distinction between retrieval of discrete episodes, and the\nlevel of detail within memories, see Barry et al. ( 2021)). Our\naim is to also leverage these more recent advances applied in\nthe ﬁeld of NLP and employ these large language models to\ncreate a full pipeline that can expedite the process of scoring\nthe AMT.\nMethod\nIn this section, we outline the datasets and methods we used\nin building the model. In the building of a model or achiev-\ning a resulting system, it requires two major elements: the\ndata and the algorithm. We compare two types of algorithms.\nOne is based on deep learning algorithms pretrained on large\namounts of text data. We call these models ﬁne-tuned BERT\nmodels, which we compare with our baseline models, which\n123\n6710 Behavior Research Methods (2024) 56:6707–6720\nFig. 1 Breakdown of memory types used in the automatic classiﬁcation\nof the AMT. Binarized classes are either speciﬁc or non-speciﬁc.F i v e -\nway classes expand the non-speciﬁc categories into categoric, extended,\nassociate or omission\nare traditional machine learning algorithms. Depending on\nthe point of comparison of the resulting models or systems,\nthe data or the algorithm may be highlighted or discussed, but\neach model required both components. All memories were\nobtained via a written version of the Autobiographical Mem-\nory Task (AMT) for which participants were provided with\na positive, negative, or neutral cue word, and asked to type\nout a memory prompted by that word. The AMT is widely\naccepted as the gold-standard measure for autobiographical\nmemory speciﬁcity, with factor analysis suggesting a one fac-\ntor structure (Grifﬁth et al., 2009). Responses to word cues\nwere coded by human raters as one of ﬁve memory types: spe-\nciﬁc (i.e., an event that is located in time and place and lasted\nfor less than 1 day), categoric (i.e., an event that happened\non repeated occasion), extended (i.e., an event that occurred\nfor longer than 24 h, such as a holiday), semantic associate\n(information that related to the cue but is not a memory), or\nomission (i.e., no memory provided).\nFig. 2 ROC curve using cross-validation for the ‘combined’ dataset in\nTable 3\nData\nWe give a brief description of all the datasets used below.\nDataset A (Takano et al., 2019) and B (Marsh et al., 2023)\nadministered a minimal instructions version of the AMT,\nwhich did not ask participants to provide a certain memory\ntype. Dataset C and Dataset D (unpublished) asked partici-\npants to provide a certain memory type in response to each\ncue.\nFor all datasets, the AMT was administered as part of\na wider study which examined relationships between auto-\nbiographical memory retrieval and mental health. Sample\nstatistics and properties (which may inﬂuence linguistic char-\nacteristics) for each dataset are presented in Table 1.\nDataset A: takano\nThe ﬁrst dataset was 10,000 memories collected from 1000\nUSA-based participants by Takano et al. ( 2019) via Mechan-\nical Turk.\n1 In response to ten cue words (ﬁve positive and ﬁve\nnegative),2 participants were instructed to recall a personal\nevent, and provide as many details as they can in relation to\nthe event, but not to use an event from within the past week,\nor to repeat memories that had been mentioned for a prior\ncue word. This dataset was previously used by Takano et al.\n(2019) to train a SVM for scoring of the AMT.\nDataset B: marsh\nFor this dataset (Marsh et al., 2023),\n3 62 individuals were\nrecruited via the research volunteer panel of the MRC Cog-\nnition and Brain Sciences Unit, University of Cambridge,\nUnited Kingdom.\n4 All were experiencing a current major\ndepressive episode. Data collection was completed online.\nIn response to twelve cue words (six positive and six nega-\ntive), participants were asked to provide an event from their\npersonal past that the word reminded them of, but that it\ncould not be from today. Participants completed the AMT\ntwice, using different cue words, resulting in 24 memories\nper person for this dataset.\n1 Available via https://osf.io/ryshb/?view_only=1dc1501f2d1a46848\n22ea14ada15d995\n2 A positive or negative word cue refers to either an adjective, noun or\nother open word class that is supposed to bring to mind a positive or\nnegative experience, respectively. For example a positive cue could be\nhappy, joy, etc., while an example of a negative cue could be sad, loss.\n3 Available via https://osf.io/jqpek\n4 https://www.mrc-cbu.cam.ac.uk\n123\nBehavior Research Methods (2024) 56:6707–6720 6711\nDataset C: amt-ai (tranche 1)\nFor Dataset C, 256 individuals were recruited via online\nadvertisements in Australia. This dataset is currently unpub-\nlished.5 The age range for this study was 16–25 years.\nParticipants completed an online, Alternating Instructions of\nthe AMT, which required speciﬁc memories in response to a\nblock of six cues, categoric memories in response to a block\nof six cues, and for a block of 12 cues, to alternate between\nretrieval of speciﬁc and categoric memories. Positive, nega-\ntive, and neutral cue words were randomized to the speciﬁc,\ncategoric, and alternating blocks. Prior to the test trials, indi-\nviduals were provided with a deﬁnition and example of a\nspeciﬁc memory and a categoric memory. Participants were\nasked not to provide events from today.\nDataset D: amt-ai-2 (tranche 2)\nThis dataset is an extension of Dataset C: the same partic-\nipants were invited back to complete another AMT using\ndifferent cue words, 3 months later. As with Dataset C, these\nmemories are currently unpublished.\n6 The same AMT task\ninstructions were used. Of the 256 individuals in Dataset C,\n244 participants provided AMT responses for this dataset.\nOne participant provided two responses for this dataset, and\nall data from this individual were used.\nExperimental set-up\nIn this section, we outline how we prepared the datasets\ndescribed in the previous section for the ﬁnetuning exper-\niments detailed below. In addition, we detail our evaluation\nmethods here so that the results reported in the next section\ncan be easily interpreted.\nDataset preprocessing\nWhile each dataset had their own classiﬁcation scheme, they\nall followed the coding manual presented by Williams and\nBroadbent ( 1986) for the scoring of the AMT, and there-\nfore could be directly mapped to one another. We normalized\nthese datasets to align with the ﬁve-way schema followed by\nDataset B ( marsh). For the binarized version of the corpus (as\nused by Takano et al. ( 2019)) we combined the non-speciﬁc\ncategories (categoric, extended, associate and omission)i n t o\none category, resulting in a speciﬁc versus non-speciﬁc dis-\ntinction as illustrated in Fig. 1.\nData preprocessing also included removing any duplicates\nfrom the datasets (from when initial scorers had disagreed)\n5 Dataset is pre-registered at https://osf.io/y79b3\n6 The second tranche of this dataset is also pre-registered at https://osf.\nio/y79b3\nand resolving these disagreements with a deﬁnitive outcome\nand manually correcting any missing data or errors. The small\nnumber of errors identiﬁed frequently came as a result of\ndocument formatting, such as misalignments that were cor-\nrected and other preprocessing issues were hand-checked and\nresolved.\n7\nFor the baseline systems (applying traditional machine\nlearning), we used SpaCy 8 with the appropriate large English\nlanguage model 9 to automatically perform tokenization,\nwhich we used to identify word tokens as features. Tokeniza-\ntion can identify sentence breaks from abbreviations, so that\nsentence-ﬁnal words are not misconstrued as unique terms,\nand adding to data sparsity. These tokens were also case-\nfolded\n10 for the same reason. For ﬁne-tuned BERT models,\nwe employed the accompanying tokenizer 11 to preprocess all\nthe text after duplicates and errors were manually corrected,\nas above.\nFine-tuning experiments\nThe models were ﬁne-tuned over BERT (Devlin et al., 2019)\nusing the bert-base-uncased model .\n12 The compute\nresources used in the training of the system had an Intel(R)\nXeon(R) Silver 4214 CPU 2.20-GHz chip with an NVIDIA\nTesla V100 SXM2 GPU with 32 GB GPU RAM.\nFor the data, we had a maximum sequence length of 512.\nNote that on the whole the AMT responses in this instance\ncomprised of short answers of a couple of sentences. The\nbatch size was set to 12 and the maximum number of epochs\nset to 20. For the initial training, we implemented warm-up\nsteps, which allowed a lower rate of learning for the ﬁrst 20%\nof the training steps. Warm-up steps help reduce the need for\nadditional training epochs by slowly exposing the model to\nnew data and possibly large variances in the data. While we\nhad a maximum number of training epoch, we also imple-\nmented an early stopping criteria based on the validation loss\ncalculated, using binary cross entropy, with a patience of 2.\nFor the experimental setup, we had a 20-fold cross-\nvalidation set up, which means at each fold 5% of the data\n7 These misalignments were only found in the exporting of our data,\nnamely Dataset C and Dataset D. The results of these original assess-\nments were exported to a spreadsheet format, however in the exporting\nof the ﬁles some of the cells in the spreadsheets misaligned.\n8 https://spacy.io\n9 The en_core_web_lg model is downloadable on the English mod-\nels’ site:https://spacy.io/models/en\n10 Performed the operation lower() ensuring all tokens as lower-\ncased\n11 The BertTokenizer from huggingface:\nhttps://huggingface.co/docs/transformers/model_doc/bert#\ntransformers.BertTokenizer\n12 Available via https://huggingface.co\n123\n6712 Behavior Research Methods (2024) 56:6707–6720\nwas held out for testing while the other 95% was used for\ntraining and validation.\nBaseline comparison\nWe compare the results from the ﬁne-tuned model with a vari-\nety of baselines. The ﬁrst is a weak majority class baseline.\nAlthough this is a weak baseline, it does allow us to deter-\nmine how difﬁcult the task is, and how the classes in the gold\nstandard dataset are distributed. A gold-standard dataset in\nmachine learning refers to the data used for evaluation. In this\ninstance, our ‘gold-standard datasets’ are the human-scored\nmemories described in the previous section under the head-\ning ‘Data’. In addition, we compare this system to the SVM\nmodel developed by Takano et al. ( 2019) with the linguis-\ntically motivated features, and an SVM model without any\nfeature engineering.\n13\nEvaluation\nWe evaluate our systems both quantitatively and qualitatively\nso that future users of the model can gauge its strengths and\nlimitations, and the degree to which they can rely on the\noutput of the system. The qualitative analysis focuses more\non the shortcomings of the model to better understand what\ntypes of inputs tend to lead to errors and why, and to guide\npractical use of model results.\nQuantitative analysis\nThis section outlines the metrics we employ in our quanti-\ntative evaluation. Our ﬁrst set of metrics is borrowed from\ninformation retrieval and the machine learning community.\nIt is a set of three metrics called precision, recall and f-score,\nand the way in which they are calculated is shown in Eqs. 1,\n2, and 3 below.\nprecision = TP\nTP + FP (1)\nrecall = TP\nTP + FN (2)\nf 1-score = 2 ∗ precision ∗ recall\nprecision + recall (3)\nIn Eq. 1 above, TP above stands for true positive. This rep-\nresents the number of memories or responses that the model\nhad correctly predicted as a certain category. In our case,\nit could represent the number of instances that the model\nhad labeled ‘speciﬁc’ that was also labeled as speciﬁc in the\ngold-standard dataset. FP is false positive, which gives us the\n13 Employing the scikit-learn tools: https://scikit-learn.org/\nnumber of memories that the model had incorrectly marked\nas a particular category. The equation for precision has the\nsum of TP and FP as the denominator and TP as the numer-\nator, and this metric helps us gauge the rate at which the\nsystem is correct when it commits to a prediction.\nRecall in Eq. 2 differs slightly from precision because it\nmeasures ‘missed opportunity’. The denominator for recall\nhas two elements as well: TP and FN. FN stands for false\nnegative. If we take ‘speciﬁc’ again as an example, FN is the\nnumber of times the model should have marked a memory\nas ‘speciﬁc’ but did not. Therefore, recall measures the rate\nat which the model missed out on being able to identify and\ncorrectly mark a memory as speciﬁc.\nThe f-score deﬁned above is a balanced harmonic mean\nof precision and recall. There are variations of the f-score\nwhich can favor either recall or precision, however we opt\nfor the f1-score in our reporting which means we have an\noverall multiplier of 2 as shown in Eq. 3 above.\nIn addition to reporting on precision, recall and f-score, we\ngauge the performance of the model via the metrics Accuracy\nfor testing the held-out dataset, and ROC (receiver operat-\ning characteristics) and AUROC (the area under ROC, when\ngraphed) for showing the performance of our released model,\nwhich are in more standard usage in the ﬁeld of psychology.\nBrieﬂy, an ROC curve illustrates how the true positive rate\n(TPR) changes with respect to the false positive rate (FPR)\nfor various threshold settings. TPR is calculated in the same\nway as recall in Eq. 2 above, and is also called sensitivity.\nFPR on the other hand is equivalent to 1 − specificity ,\nwhere speciﬁcity is calculated in a similar fashion to Eq.\n1 but instead replacing TP with TN (true negative counts).\nAUROC (sometime referred to AUC), or calculating the area\nunder the ROC curve, is a good indicator of a classiﬁer’s per-\nformance, where 1.0 would be an impossibly perfect system,\nand 0.5 for a binary classiﬁer would represent a random sys-\ntem. The closer to 1.0 AUROC is the better the model is at\ndistinguishing classes.\nQualitative error analysis\nFor the qualitative analysis, we shift our focus to the errors\nthat were made by the system so that we can better ascer-\ntain whether the model is ﬁt for purpose. We identiﬁed all\nof the false-positive outcomes for each of the ﬁve categories\n(speciﬁc, categoric, extended, omission, and associate), and\nfurther grouped them into their correct classes, as deemed by\nthe human scorers. In addition, we examined the output for\nwhen the model was not conﬁdent enough to make a predic-\ntion (i.e., it does not meet or exceed the speciﬁed threshold).\nTwo researchers manually reviewed these error ﬁles to\ndetermine whether the classiﬁcation errors made by the\nmodel exhibited identiﬁable linguistic patterns. This qual-\n123\nBehavior Research Methods (2024) 56:6707–6720 6713\nTable 2 Results of the binary\nﬁnetuning experiments DatasetA: takano DatasetB: marsh DatasetC: amt-ai All: combined\ndataset size 10.0K 1.5K 6.1K 17.7K\npr f pr f pr f pr f\nBert 0.45 .91 .91 .91 .92 .92 .92 .90 .90 .90 .89 .89 .89\nBert 0.50 .91 .91 .91 .92 .92 .92 .89 .89 .89 .89 .89 .89\nBert 0.55 .91 .91 .91 .92 .92 .92 .89 .89 .89 .89 .89 .89\nTakano .80 .82 .81 .72 .71 .70 .72 .72 .71 – – –\nSVM .69 .71 .69 .61 .66 .63 .69 .68 .69 .78 .78 .78\nMajority .55 .55 .55 .50 .50 .50 .59 .59 .59 .50 .50 .50\nThere are no ‘All: combined’s c o r e sf o rt h eTakano system because this dataset includes the same instances.\nNote that the grayed out ﬁgures showing results for the system developed over ‘Dataset A ’ tested with the\nTakano data did not have training and testing mixed and were generously provided by Keisuke Takano via\npersonal communication\nitative procedure was performed iteratively over the course\nof three sessions to avoid mental fatigue.\nDuring this process, however, the researchers identiﬁed\ninconsistencies in the originally scored memories. That is, in\nsome cases where the model disagreed with the human scor-\ners, our own scoring of the memory agreed with the model,\nrather than the original scorer. This may indicate that the\nmodel was identifying human-made scoring errors in the\noriginal dataset (e.g., made due to inattention or fatigue).\nAlternatively, this may reﬂect slight differences in the scor-\ning manuals used between different research groups. For this\nreason, we performed an additional re-evaluation of a subset\nof our original collection of memories.\nFurther re-evaluation\nFor the instances where the model labeled a memory incor-\nrectly, or where the human-scored memory and the model\nprediction disagreed, we had two researchers re-score a sam-\nple of these memories to ascertain the extent to which the\nmodel could be identifying human error in the initial dataset.\nWhere there were model-to-human disagreements, we ran-\ndomly obtained 250 of these instances for each of the classes.\nThe rescoring was done in a two-stage process. In Stage I, the\nresearchers were presented with a memory and two possible\nmemory types to choose as the correct class. If the researcher\ndeemed neither of the classes as correct for the given mem-\nory, they moved to Stage II, which allowed them to specify\nthe correct memory type. The two possible memory types\nwere taken as the prediction from the model and the original\nhuman-scored label. The researcher was not aware of which\noutput was generated by the model or scored by the human.\nObtaining access to the model\nWe have made our ﬁnal model freely available via GitHub,\nhttps://github.com/autobiographical-memory-task/amt-2023-\n08-01. We include step-by-step instructions on how to run\nthe model. In doing so, we aim to make it simple for others\nto use the model to score their own data.\nResults\nTable2 shows the results for the binarized experiments where\nthe four types of non-speciﬁc memories ( categoric, extended,\nTable 3 Results of the ﬁve-way\nmulticlass ﬁnetuning\nexperiments\nDatasetA: takano DatasetB: marsh DatasetC: amt-ai All: combined\ndataset size 10.0K 1.5K 6.1K 17.7K\npr f pr f pr f pr f\nBert 0.45 .91 .89 .90 .80 .83 .82 .94 .93 .93 .87 .87 .87\nBert 0.50 .92 .88 .89 .83 .81 .82 .94 .92 .93 .88 .85 .87\nBert 0.55 .92 .86 .88 .84 .79 .81 .95 .92 .93 .91 .86 .89\nTakano ––– ––– ––– –––\nSVM .78 .78 .78 .76 .76 .76 .78 .78 .78 .68 .68 .68\nMajority .55 .55 .55 .50 .50 .50 .41 .41 .41 .50 .50 .50\nThe bolded ﬁgures achieve the highest scores for precision (P) and overall f1-score (F) for BERT 0.55 with\nthe Combined dataset. This model is made available\n123\n6714 Behavior Research Methods (2024) 56:6707–6720\nassociate and omission) were conﬂated into one class, as\nshown in Fig. 1. Table 3 has the results for the multiclass\nexperiments including all ﬁve classes: the four in parentheses\nabove as well as speciﬁc ROC curve is displayed in Fig. 2.\nBaseline systems\nFor both sets of results, we have three kinds of baselines to\ncompare our models to, which we call Takano, SVM and\nMajority. The latter is a simple majority class baseline.\nThe other two baselines are SVM models. Takano is the R\nModel by Takano et al. ( 2019) that has linguistically moti-\nvated features that were designed especially for this binary\ntask, and SVM does not involve any feature engineering; the\ntext was simply tokenized and case-folded. The results for\nboth the binary and multiclass systems convincingly outper-\nform the baseline systems, in some cases achieving over 20\npercentage points above the best-performing baseline. This\nis displayed in the results for the multiclass system for the\ncombined dataset as shown in Table 3, which achieves an f-\nscore of 0.89 when the threshold is 0.55 (BERT 0.55), while\nthe comparison baseline system achieves an f-score of only\n0.68 (SVM).\nA noteworthy comparison is the results for the binary sys-\ntems between Datasets B and C, marsh and amt-ai.W es e e\nthat the Majority class baseline for amt-ai is 0.59 for the\nbinary system and 0.41 for the multiclass system. This is\nbecause unlike the other datasets, amt-ai does not have the\nspeciﬁc memory class that makes up a clear majority for the\nbinary system – the baseline is made up of the heteroge-\nneous class making up the non-speciﬁc memories. The other\ndatasets have at least 50% of the one class, speciﬁc, that\nmakes up the majority of the memories in the dataset. There-\nfore, while we expect that a system that is trained on more\ninstances – or memories – to perform better than one that has\nfewer instances, we see that for the binary experiments, the\nmarsh system outperforms amt-ai. This is primarily because\nthe majority class does not form a homogeneous class with\nthe same linguistic characteristics.\nWe observe in the multiclass results in Table 3 that the\nbest-performing system is amt-ai. It outperforms the system\ntrained on the takano dataset, even though the latter has 10K\nmemories and the former has fewer with 6.1 K instances. This\nmay largely be due to the make-up of the datasets. Dataset A,\ntakano, had memories collected from diverse cohort whose\naverage age was 34.7 years of age but with a standard devia-\ntion of 15.4 years, while the average age of amt-ai was almost\n20 years, but with only a standard deviation of 0.67 years.\nWe would expect that the linguistic style of the memories\nfrom amt-ai would be less diverse than that of takano for this\nreason.\nThe grayed out ﬁgures in the Takano baseline for the\nbinarized task shows a precision, recall and f-score of 0.80,\n0.82, and 0.81, respectively.\n14 It is interesting to note that the\nSVM model reported in Table 2 has not had any linguistically\nmotivated feature engineering, unlike the Takano system\nbased on an SVM model as reported by Takano et al. ( 2019).\nWe can posit that the increase in performance between the\nsystems SVM and Takano is due to the feature engineering\ndesigned by Takano et al. ( 2019).\nFine-tuned BERT systems\nWe present three versions of our ﬁne-tuned models based\non a threshold of 0.45, 0.50, and 0.55. This means if the\nprobability that the model predicts for a class exceeds this\nthreshold then this prediction will hold. Otherwise, as a\npost-process, we disallow the model from committing to\na prediction should the probability fall below. This thresh-\nolding does not result in a marked difference in the binary\ntask, as shown by the consistent numbers in Table 2, as you\nread down each column representing the systems trained on\neach dataset. However, Table 3 illustrates that the thresh-\nolds increase precision for all four systems presented. For\nDatasets takano and marsh, this increase in precision as the\nthreshold increases adversely affects the overall f-score due\nto the decrease in recall, but for Dataset amt-ai and the system\nthat merges all three datasets ( takano, marsh, and amt-ai)t o\nform the combined system, we do not see a fall in the f-score.\nFurthermore, for the combined system, we see an increase in\nthe overall performance for threshold 0.55, with precision,\nrecall and f-score at 0.91, 0.86, and 0.89, respectively. In\npractical terms, this means that for every 100 predictions the\nsystem makes, it gets nine incorrect. However, the deteriora-\ntion of the f-score (0.89) is largely due to poorer performance\nin the recall, our measure of ‘missed opportunity’.\nWe present the ROC curve for the best-performing mul-\nticlass system in Fig. 2. The closer the ROC curve hugs the\nupper left corner of the graph, the more proﬁcient the model\nis in categorizing the data. To measure this, we can deter-\nmine the AUROC (area under the roc curve), which reveals\nthe portion of the graph that lies beneath the curve. This is\ndepicted for each of the classes in the Fig. 2 where we see the\nclass omission hugging the top left corner most closely with\nthe further curve belonging to the extended class . Overall,\nthe AUROC for the entire model for this ﬁve-way classiﬁca-\ntion system has a value of 0.976 and an accuracy of 0.9572\n(95.7%).\nFor the remainder of this section, we refer to the results\nfrom the system trained on the combined dataset with\n14 These ﬁgures were provided by Keisuko Takano via private commu-\nnication.\n123\nBehavior Research Methods (2024) 56:6707–6720 6715\nTable 4 Breakdown of classes\npr fD a t a\nSpeciﬁc .95 .92 .93 8802\nExtended .89 .77 .83 2852\nCategoric .86 .88 .87 4165\nOmission .99 .80 .88 687\nAssociate .84 .66 .74 1113\nthe threshold of 0.55 15 in the ‘Error analysis’ and ‘Re-\nevaluation’ sections below.\nError analysis\nThe aim of the qualitative error analysis was to identify what\ntypes of responses gave rise to errors in the model prediction.\nBy doing so, this will allow users of the model to gauge the\nutility of the system. Table 4 provides a quantitative descrip-\ntion of the precision, recall, and f-score metrics for each\nclass. We observe that the model performs well in each of\nthe speciﬁc, extended, categoric, and omissions classes, but\nthe model exhibits poorer performance in the associate class.\nThis quantitative error analysis coincides with this qualitative\nerror analysis.\nIn the qualitative error analysis, we identiﬁed three main\nerror types:\nI Short responses;\nII Present and future-oriented; and\nIII Ambiguous duration and frequencies\nError type I describes errors pertaining to short responses.\nWe limited our analysis speciﬁcally to single-word responses,\nand these are typically best classiﬁed as semantic associates.\nThe model was able to correctly classify many single-word\nresponses as semantic associates, however it could also cat-\negorize these responses as categoric (“sleep”), omission\n(“Die”), none (“Drone”), extended (“retirement”) or spe-\nciﬁc (“funeral”). Human raters also exhibit similar errors\nin categorizing single-word responses, indicating that this\nmay be an existing problem in scoring responses. Overall,\nsingle-word responses may beneﬁt from re-scoring by care-\nfully instructed human scorers. Further research on other\nshort responses (e.g., two- and three-word responses) may\nbe appropriate in future.\nError type II, present and future-oriented responses, is a\nresult of the model mis-classifying responses that describe\nevents that occurred ‘today’ or were anticipated to occur in\n15 This model is available via https://github.com/autobiographical-\nmemory-task/amt-2023-08-01\nthe future. In these instances, the participant had not fol-\nlowed the experiment instructions to recall a memory from a\nperiod prior to ‘today’ (in Dataset B and C) or the previous\nweek (in Dataset A). Under the scoring instructions used in\neach of the training datasets, these responses are treated as\nomissions (Takano et al., 2019). These responses can contain\nfuture and present markers (e.g., “today”, “will”), or hypo-\nthetical future scenarios (e.g., “If I don’t get this job that I\nam interviewing for”). The model can sometimes correctly\nidentify these types of responses as omissions (e.g., “I will\nfeel relieved after all the bills are paid.”) but the model also\ncategorized these responses as speciﬁc (“today after dinner”),\nor extended (“Moving to New Zealand later in the year”), or\nassociate (“Right now I’m bored”) or categoric (“When I\nhave kids”).\nError type III describes errors pertaining to responses that\nhave ambiguous duration and frequencies. The model could\nstruggle to classify these responses in agreement with the\nhuman scorer. For example, the response “swimming with\nturtles” was interpreted by the model as a categoric memory,\nand as a speciﬁc memory by a human scorer. These types\nof responses are inherently challenging for both models and\nhumans, as also observed in Takano et al. ( 2019). While a\nhuman scorer can use prior knowledge to reduce some ambi-\nguity (e.g., for most people swimming with turtles is a rare\noccurrence), the use of world knowledge is often not decisive\n(e.g., ‘Going to a carnival\" cannot be conﬁdently classiﬁed\nas a speciﬁc or categoric memory).\nWhile each error type is driven by unique features, we\nobserved that each of these errors was also made by humans.\nAs such, the model may partly reﬂect standard human scor-\ning approaches. For example, type I errors (short responses)\nmay be partly driven by some challenges humans scorers\ndisplayed in correctly categorizing single-word responses\nand semantic associates more generally, and type II errors\n(present and future-oriented responses) may be partly driven\nby similar difﬁculties humans exhibited in correctly scor-\ning future and present responses as omissions, for example,\n“When I graduate college” was incorrectly scored by an anno-\ntator as a speciﬁc memory.\nRe-evaluation\nThe process of the qualitative error analysis allowed us to\nrecognize that in some instances the model had identiﬁed\nerrors in the original manually scored corpus of memories\nand responses. In order to ascertain the extent to which the\nmodel identiﬁed human error, or whether these were iso-\nlated instances, we embarked on a process of re-evaluation\nas described in the Methods section (under ‘Further re-\nevaluation’). We had a primary annotator who re-evaluated\n826 memories that disagreed with the human scores. A sec-\n123\n6716 Behavior Research Methods (2024) 56:6707–6720\nondary annotator scored a large subset of these as a form of\nquality control.\nThrough this process, we found that the primary annotator\nagreed with the model’s prediction 64%; with the original\nhuman-scored annotation 31% of the time; and they rescored\nthe response as a different category altogether 5% of the\ntime. These proportions coincided well with the secondary\nannotator’s responses who agreed with the model 66% of the\ntime, with the human annotation 31%; and disagreed with\nboth 3% of the time.\nThe results of this re-evaluation demonstrate that, while\nthe model generally agreed with the gold-standard human\nscored responses – as evidenced by the high-precision score –\na majority of any disagreements once rescored by researchers\nactually found that the model picked up on human error. That\nis, the evaluation we have presented of the model may actu-\nally represent an underestimate of its true performance and\nmay overcome errors that eventuate due to either annotation\nfatigue or shortcomings in the current human-driven scoring\npractices.\nError analysis summary\nThe error analysis identiﬁed three types of errors made by the\nmodel: error type I (short responses); error type II (present\nand future-oriented responses) and error type III (ambiguous\ndurations and frequencies).\nThe error analysis also identiﬁed that a majority of dis-\nagreements between the model and human scorers were due\nto human-error. The model may therefore improve scoring\nrigor over existing human-driven practices.\nApplication to new data\nIn Table 5, we compare how our BERT-trained binary model\nfares against the system developed by Takano et al. ( 2019)\nlabeled as ‘SVM Model in R’ on a completely new dataset.\nThe system labeled ‘Dataset A (0.55)’ was trained on the\nsame data as the ‘SVM Model in R’ but this model was\nﬁne-tuned over BERT. For the system labeled ‘Dataset All\n(0.55)’, there are no results for the ﬁrst and second lines of\nthe table to test both the marsh and amt-ai datasets because\nthis system was trained on the memories from these corpora.\nThe only completely held-out dataset was amt-ai-2, which\nwe compare for all three systems in Table 5.\nWe observe that the held-out dataset fares the best for\nthe ‘Dataset All (0.55)’ system, which was trained on three\ndatasets. It performs over ten percentage points higher than\nthe SVM model reported in the table for this dataset.\nThis shows that while the ‘SVM Model in R’ performs\nwell, employing large language models that are ﬁne-tuned\noutperforms traditional machine learning systems in this\ninstance, with both ‘Dataset A ’ (ﬁne-tuned over the takano\ndataset) as well as our best-performing system ﬁne-tuned on\ntakano, marsh and amt-ai, validating that this methodology\nis apt for this application area.\nDiscussion\nBiased retrieval of autobiographical memories has been con-\nsistently demonstrated to predict depressive prognosis, and\nis increasingly recognized as feature of multiple other mental\nhealth disorders (Barry et al., 2021; Dalgleish & Hitchcock,\n2023). Memory-based interventions show promise as a low-\nintensity treatment option (Barry et al., 2019a; Hitchcock\net al., 2017), and can also be deployed as adjuncts to our\ncurrent gold-standard treatments (Dalgleish & Hitchcock,\n2023). Indeed, cognitive augmentations for psychological\ntherapies are receiving increasing attention for boosting treat-\nment effects (Nord et al., 2023). However, to effectively\nimplement memory interventions into clinical services, we\nneed fast, low-cost, and easy to implement tools for assessing\nmemory bias, in order to effectively identify those who are\nlikely to beneﬁt from memory-based intervention. Further, if\nwe are to better understand how memory bias inﬂuences treat-\nment response, and the mechanisms through which memory\nbias impairs mental health, we need large-scale datasets.\nHere, we have developed a machine learning model able to\nidentify multiple forms of memory bias, which can be used\nfor these purposes. Critically, the model is open-source (i.e.,\ndoes not require paid software), and does not require the user\nto have any coding experience, ensuring that it can be used\nby both clinicians and researchers alike, facilitating imple-\nmentation and scalability within real-world settings.\nTable 5 Model comparison for\nbinary systems SVM Model in R Dataset A (0.55) Dataset All (0.55)\npr fA c c pr fA c c pr fA c c\nB: marsh .72 .71 .70 .71 .83 .82 .83 .83 −−−−\nC: amt-ai .72 .72 .71 .72 .82 .83 .82 .83 −−−−\nD: amt-ai-2 .72 .72 .70 .72 .81 .81 .81 .81 .85 .85 .84 .85\nBoth the ‘SVM Model in R’ and ‘Dataset A (0.55)’ are developed from the takano (A) Dataset. ‘Dataset All\n(0.55)’ was developed from Datasets A, B, and C and tested on the held-out Dataset D\n123\nBehavior Research Methods (2024) 56:6707–6720 6717\nOur developed model offers a number of advantages over\nexisting scoring solutions. First, it is able to identify all ﬁve\nmemory types in the original AMT scoring manual (Williams\n& Broadbent, 1986), facilitating the ability to identify not\nonly reduced memory speciﬁcity, but also assess memory\nﬂexibility. The reliability and robustness of the model stems\nfrom the diversity of the data it is trained on with over 17,000\ntraining instances from three diverse sources. This will enable\nfurther large-scale research examining the role of varied and\nnon-speciﬁc memory types in the development of everyday\ncognitive skills (e.g., exploration of how categoric summaries\nof what usually works inﬂuence problem-solving skills).\nSecond, our model was trained on text from both community-\nbased and clinical samples collected from multiple countries,\nfrom participants aged 15 to 80 years, demonstrating that the\nmodel can be applied effectively to text composed by those\nwith and without mental health disorders (Smirnova et al.,\n2018). Finally, we demonstrated an improvement in accuracy\nwhen using a large language model. Our preferred sys-\ntem (System All) outperformed previous machine learning\napproaches while also utilizing the more nuanced ﬁve-way\nschema as compared to a binary schema. Only 5.5% of\nresponses fall below the 0.55 threshold and therefore require\nmanual scoring, and researchers will be able to use the prob-\nabilities assigned by the model to facilitate the process. This\nmodel is therefore useful for the rapid assessment of autobi-\nographical memory retrieval, in both a clinical and research\nsetting.\nThe model also appeared to exhibit advantages when com-\npared to human scorers. When the model and human scorers\ndisagreed, a re-evaluation of these disagreements favored\nthe model’s prediction approximately 65% of the time. This\nindicates that the model may be more robust to factors that\nmay otherwise drive errors in human scoring. These fac-\ntors include fatigue, which is not a concern for the model.\nLess obvious factors may include the additional informa-\ntion available to human scorers that the model does not have\naccess to. Human scorers will often, when faced with ambi-\nguity, refer to other information associated with the response\nincluding the associated cue words, the instructions provided\n(was the participant explicitly told to recall a speciﬁc mem-\nory?), patterns of responding in a single participant (does\nthis participant generally provide speciﬁc memories?), and\nthe scorer’s own experiences (are the described events com-\nmon or unusual in their own life?). As such, many sources\nof information are available to the human scorer, which can\nreduce the focus on the content of the response provided.\nWhile under certain circumstances this additional informa-\ntion may confer an advantage (e.g., when resolving events\nwith ambiguous durations), the focus on additional sources\nof information may contribute to variability in the human\nscoring processes. Our current model is trained on the scor-\ning practices of a large number of different human scorers,\nacross different countries, each with their own interpretive\nbias. The outcome of this training is that the model interprets\nresponses in a manner that approximates the output of a large\nnumber and variety of human scorers. Thus, moving towards\na machine scoring models likely minimizes multiple human\nbiases in scoring.\nIn addition to variability between human scorers, there\nare likely consistent differences in scoring between research\ngroups. Each research group appears to use slightly dif-\nferent judgments and thresholds when scoring ambiguous\nresponses (e.g., whether ’beneﬁt of the doubt’ is given, such\nthat ambiguous responses are scored as correctly providing\nthe requested memory type). Many of these small nuances in\nscoring arise from how new scorers are trained, and the errors\nthat we have identiﬁed here (i.e., when the human scorer did\nnot agree with the model) may not be considered as errors\nwithin some research groups. Datasets B, C, and D were all\nscored by our own research group, minimizing the impact of\nthis issue on model training. While we did not observe any\nsystematic differences in model accuracy between Dataset A\n(scored by another research group) and Datasets B, C, and D,\nsmall variation in scoring decisions between research groups\nwill impact what is considered an error. When moving from\nhuman-led scoring to computer-led scoring, we encourage\nconsideration of the likely systematic differences in the pro-\nduced scores.\nThere are other limitations which will impact the gener-\nalizability of the model. We trained on data obtained from\ncommunity samples or those with depression, and it will need\nto be determined whether the model applies as well to those\nexperiencing other mental health disorders, which are asso-\nciated with their own linguistic styles (e.g., psychosis). We\nalso made use of memories which were reported in written\nform. Steady technological advances are facilitating auto-\nmated text-based transcription of oral responses, allowing\nthe future application of our model to transcribed, orally pro-\nvided memories. However, it will need to be determined that\nmodel accuracy holds for orally reported memories, which\nlikely vary in memory structure and length, from typed mem-\nories. Similarly, while data was obtained from numerous\ncountries, all memories were in English. Reapplication of\nour code to new data will allow development of scoring mod-\nels in languages other than English, and this is an exciting\navenue for future research.\nFuture applications of this model may also wish to explore\nwhether it can be used to accurately score future-oriented spe-\nciﬁc episodes. The ability to retrieve past episodes and project\n123\n6718 Behavior Research Methods (2024) 56:6707–6720\nfuture events are closely related (Schacter & Addis, 2007),\nwith indications that the ability to imagine speciﬁc future\nevents is also associated with poor mental health (Hallford et\nal., 2018). Qualitative error analysis of our model suggested\nthat the model may inconsistently identify future oriented\nepisodes as omissions or speciﬁc. Further work to identify\ndisagreements between human-scorer and machine in scor-\ning of future episodes would produce a more detailed dataset\nthat could be used for future training. Although we were\nfocused on scoring memories for past events, and our model\nachieves high accuracy in doing so, additional further train-\ning may see that our model is able to be used in assessment\nof future thinking also.\nOur model can now be used to evaluate research questions\nwhich necessitate the use of large datasets. Meta-analysis has\nindicated a signiﬁcant prognostic effect of memory speci-\nﬁcity on depressive symptoms across studies (Hallford et\nal., 2021b), however, there are indicators that the predictive\neffect may be stronger for certain participant groups (e.g.,\nthose with prior trauma exposure, a family history of depres-\nsion) (Dalgleish & Hitchcock, 2023). Further understanding\nof moderators will help to identify who is most likely to\nbeneﬁt from memory-based interventions. Similarly, further\nunderstanding the mediating mechanisms underpinning the\neffect of memory speciﬁcity, or memory ﬂexibility, on men-\ntal health outcomes is now needed. Evaluation of mediators\nand moderators will require datasets with hundreds of par-\nticipants, assessed at multiple time points, which exceeds\ncapabilities of human scorers. Our provided model there-\nfore takes important steps toward enabling such necessary\nresearch.\nThe model also enables the secondary analysis of exist-\ning datasets. Current challenges in using existing data sets to\ncomplete new analyses often lie in the differences in how\nAMT responses are scored (e.g., in a binary or ﬁve-way\nschema) and reported (e.g., some researchers may combine\ncategorical and extended memories into ’overgeneral memo-\nries’ Barry et al. ( 2021)). The current model enables existing\ndatasets to be rapidly re-scored with a ﬁve-way schema and\nwith high consistency, allowing for a more detailed secondary\nanalysis of a broad set of autobiographical memory biases\nin existing datasets and for the pooling of these datasets to\nexplore new research questions.\nIn terms of practical use, as indicated by our results, the\nmodel is likely to be of high utility due to its high accu-\nracy. Lower, though still very good, accuracy was observed\nfor semantic associates and thus this should be taken into\naccount when interpreting results concerning semantic asso-\nciates. Much of the research to date has focused on the ability\nto retrieve speciﬁc, categoric, and extended memories, and\nour results suggest that the user can be conﬁdent in model\nscoring of these memories. In providing a model that requires\nno additional coding, our goal has been to offer an easy to\nimplement scoring system to assess autobiographical mem-\nory not only in future research, but also in clinical contexts.\nIncreasing identiﬁcation of those who are likely to bene-\nﬁt from autobiographical memory-based interventions is an\nimportant step toward clinical implementation of these basic\nscience-driven interventions, and ultimately, personalization\nof mental health care.\nOpenpracticestatement\nAll code, including instructions for use of the trained model,\nis available via GitHub, https://github.com/autobiographical-\nmemory-task/amt-2023-08-01 .\nFunding Open Access funding enabled and organized by CAUL and\nits Member Institutions.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\nAskelund, A. D., Schweizer, S., Goodyer, I. M., et al. (2019). Posi-\ntive memory speciﬁcity is associated with reduced vulnerability to\ndepression. Nature Human Behaviour , 3 (3), 265–273. https://doi.\norg/10.1038/s41562-018-0504-3\nBarry, T. J., Sze, W. Y ., & Raes, F. (2019a). A meta-analysis and system-\natic review of Memory Speciﬁcity Training (MeST) in the treat-\nment of emotional disorders. Behaviour Research and Therapy,\n116, 36–51. https://doi.org/10.1016/j.brat.2019.02.001. https://\nlinkinghub.elsevier.com/retrieve/pii/S000579671930018X\nBarry, T. J., Vinograd, M., Boddez, Y ., et al. (2019b). Reduced autobio-\ngraphical memory speciﬁcity affects general distress through poor\nsocial support. Memory, 27 (7), 916–923. https://doi.org/10.1080/\n09658211.2019.1607876\nBarry, T. J., Hallford, D. J., & Takano, K. (2021). Autobiographical\nmemory impairments as a transdiagnostic feature of mental illness:\nA meta-analytic review of investigations into autobiographical\nmemory speciﬁcity and overgenerality among people with psy-\nchiatric diagnoses. Psychological Bulletin, 147 (10), 1054–1074.\nhttps://doi.org/10.1037/bul0000345\nConway, M., & Pleydell-Pearce, C. (2000). The construction of auto-\nbiographical memories in the self memory system. Psychological\nReview, 107(2), 261–288.\nDalgleish, T., & Hitchcock, C. (2023). Transdiagnostic distortions in\nautobiographical memory recollection. Nature Reviews Psychol-\nogy, 2(3), 166–182. https://doi.org/10.1038/s44159-023-00148-1\n123\nBehavior Research Methods (2024) 56:6707–6720 6719\nDebeer, E., Hermans, D., & Raes, F. (2009). Associations between\ncomponents of rumination and autobiographical memory speci-\nﬁcity as measured by a Minimal Instructions Autobiographical\nMemory Test. Memory, 17 (8), 892–903. https://doi.org/10.1080/\n09658210903376243\nDevlin, J., Chang, M. W., Lee, K., et al. (2019). BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding.\nIn: Proceedings of the 2019 Conference of the North , (pp. 4171–\n4186). Association for Computational Linguistics, Minneapolis,\nMinnesota. https://doi.org/10.18653/v1/N19-1423\nDritschel, B., Beltsos, S., & McClintock, S. M. (2014). An “alternat-\ning instructions” version of the Autobiographical Memory Test\nfor assessing autobiographical memory speciﬁcity in non-clinical\npopulations. Memory, 22 (8), 881–889. https://doi.org/10.1080/\n09658211.2013.839710\nvan Genugten, R., & Schacter, D. L. (2022). Automated Scoring of the\nAutobiographical Interview with Natural Language Processing.\npreprint, PsyArXiv. https://doi.org/10.31234/osf.io/nyurm\nGrifﬁth, J. W., Sumner, J. A., Debeer, E., et al. (2009). An item\nresponse theory/conﬁrmatory factor analysis of the Autobiograph-\nical Memory Test. Memory, 17 (6), 609–623. https://doi.org/10.\n1080/09658210902939348\nHallford, D., Austin, D., Takano, K., et al. (2018). Psychopathology\nand episodic future thinking: A systematic review and meta-\nanalysis of speciﬁcity and episodic detail. Behaviour Research and\nTherapy, 102 , 42–51. https://doi.org/10.1016/j.brat.2018.01.003.\nhttps://linkinghub.elsevier.com/retrieve/pii/S0005796718300032\nHallford, D. J., Austin, D. W., Takano, K., et al. (2021a). Improving\nUsual Care Outcomes in Major Depression in Y outh by Targeting\nMemory Speciﬁcity: A Randomized Controlled Trial of Adjunct\nComputerised Memory Speciﬁcity Training (c-MeST). preprint,\nPsyArXiv. https://doi.org/10.31234/osf.io/vmurs\nHallford, D. J., Rusanov, D., Yeow, J. J. E., et al. (2021b).\nOvergeneral and speciﬁc autobiographical memory predict\nthe course of depression: an updated meta-analysis. Psy-\nchological Medicine, 51 (6), 909–926. https://doi.org/10.1017/\nS0033291721001343. https://www.cambridge.org/core/product/\nidentiﬁer/S0033291721001343/type/journal_article\nHitchcock, C., Werner-Seidler, A., Blackwell, S. E., et al. (2017).\nAutobiographical episodic memory-based training for the treat-\nment of mood, anxiety and stress-related disorders: A sys-\ntematic review and meta-analysis. Clinical Psychology Review,\n52, 92–107. https://doi.org/10.1016/j.cpr.2016.12.003. https://\nlinkinghub.elsevier.com/retrieve/pii/S0272735816301969\nHitchcock, C., Gormley, S., Rees, C., et al. (2018). A randomised con-\ntrolled trial of memory ﬂexibility training (MemFlex) to enhance\nmemory ﬂexibility and reduce depressive symptomatology in indi-\nviduals with major depressive disorder. Behaviour Research and\nTherapy, 110, 22–30. https://doi.org/10.1016/j.brat.2018.08.008\nMang, L., Ridout, N., & Dritschel, B. (2018). The inﬂuence of\nmood and attitudes towards eating on cognitive and autobi-\nographical memory ﬂexibility in female university students.\nPsychiatry Research, 269 , 444–449. https://doi.org/10.1016/\nj.psychres.2018.08.055. https://linkinghub.elsevier.com/retrieve/\npii/S0165178117320541\nMarsh, L. C., Patel, S. D., Smith, A. J., et al. (2023). From basic\nscience to clinical practice: Can cognitive behavioural ther-\napy tasks be augmented with enhanced episodic speciﬁcity?\nBehaviour Research and Therapy, 167 , 104352. https://doi.\norg/10.1016/j.brat.2023.104352. https://www.sciencedirect.com/\nscience/article/pii/S0005796723001018\nMikolov, T., Sutskever, I., Chen, K., et al. (2013). Distributed rep-\nresentations of words and phrases and their compositionality.\nIn: Proceedings of the 26th International Conference on Neural\nInformation Processing Systems , (V ol. 2, pp. 3111–3119). Curran\nAssociates Inc., Red Hook, NY , USA, NIPS’13.\nNord, C. L., Longley, B., Dercon, Q., et al. (2023). A transdiag-\nnostic meta-analysis of acute augmentations to psychological\ntherapy. Nature Mental Health, 1 (6), 389–401. https://doi.org/\n10.1038/s44220-023-00048-6 . https://www.nature.com/articles/\ns44220-023-00048-6\nPennington, J., Socher, R., & Manning, C. (2014). GloV e: Global\nvectors for word representation. In: Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language Processing\n(EMNLP), (pp. 1532–1543). Association for Computational Lin-\nguistics, Doha, Qatar. https://doi.org/10.3115/v1/D14-1162\nPiltan, M., Moradi, A. R., Choobin, M. H., et al. (2021). Impaired\nAutobiographical Memory Flexibility in Iranian Trauma Survivors\nWith Posttraumatic Stress Disorder. Clinical Psychological Sci-\nence, 9 (2), 294–301. https://doi.org/10.1177/2167702620953637\nSanh, V ., Debut, L., Chaumond, J., et al. (2019). DistilBERT, a dis-\ntilled version of BERT: smaller, faster, cheaper and lighter. In: 5th\nWorkshop on Energy Efﬁcient Machine Learning and Cognitive\nComputing @ NeurIPS 2019 . arXiv:1910.01108\nSchacter, D. L., & Addis, D. R. (2007). The cognitive neuroscience\nof constructive memory: remembering the past and imagining the\nfuture. Philosophical Transactions of the Royal Society B: Biolog-\nical Sciences, 362 (1481), 773–786. https://doi.org/10.1098/rstb.\n2007.2087\nSmirnova, D., Cumming, P ., Sloeva, E., et al. (2018). Language Pat-\nterns Discriminate Mild Depression From Normal Sadness and\nEuthymic State. Frontiers in Psychiatry, 9 , 105. https://doi.org/\n10.3389/fpsyt.2018.00105. https://journal.frontiersin.org/article/\n10.3389/fpsyt.2018.00105/full\nSutskever, I., Vinyals, O., & Le, Q. V . (2014). Sequence to sequence\nlearning with neural networks. In: Proceedings of the 27th Inter-\nnational Conference on Neural Information Processing Systems ,\n(V ol. 2, pp. 3104–3112). MIT Press, Cambridge, MA, USA,\nNIPS’14.\nTakano, K., Ueno, M., Moriya, J., et al. (2017). Unraveling the linguistic\nnature of speciﬁc autobiographical memories using a computerized\nclassiﬁcation algorithm. Behavior Research Methods, 49 (3), 835–\n852. https://doi.org/10.3758/s13428-016-0753-x\nTakano, K., Gutenbrunner, C., Martens, K., et al. (2018). Computerized\nscoring algorithms for the Autobiographical Memory Test. Psy-\nchological Assessment, 30 (2), 259–273. https://doi.org/10.1037/\npas0000472\nTakano, K., Hallford, D. J., V anderveren, E., et al. (2019). The com-\nputerized scoring algorithm for the autobiographical memory\ntest: updates and extensions for analyzing memories of English-\nspeaking adults. Memory, 27 (3), 306–313. https://doi.org/10.\n1080/09658211.2018.1507042\nV aswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all\nyou need. In: Proceedings of the 31st International Conference on\nNeural Information Processing Systems , (pp. 6000–6010). Curran\nAssociates Inc., Red Hook, NY , USA, NIPS’17.\nWardell, V ., Esposito, C. L., Madan, C. R., et al. (2021). Semi-automated\ntranscription and scoring of autobiographical memory narratives.\nBehavior Research Methods, 53 (2), 507–517. https://doi.org/10.\n3758/s13428-020-01437-w\nWilliams, J. M., & Broadbent, K. (1986). Autobiographical memory in\nsuicide attempters. Journal of Abnormal Psychology, 95 (2), 144–\n149. https://doi.org/10.1037/0021-843X.95.2.144\nWilliams, J. M. G., Barnhofer, T., Crane, C., et al. (2007). Autobio-\ngraphical memory speciﬁcity and emotional disorder. Psychologi-\ncal Bulletin, 133(1), 122–148. https://doi.org/10.1037/0033-2909.\n133.1.122\nZhang, T., Schoene, A. M., Ji, S., et al. (2022). Natural language pro-\ncessing applied to mental illness detection: a narrative review. Npj\nDigital Medicine, 5 (1), 46. https://doi.org/10.1038/s41746-022-\n00589-7\n123\n6720 Behavior Research Methods (2024) 56:6707–6720\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\nAuthorsandAﬃliations\nMeladel Mistica 1 · Patrick Haylock 2 · Aleksandra Michalewicz 1 · Steph Raad 2 · Emily Fitzgerald 1 ·\nCaitlin Hitchcock 2\nPatrick Haylock\nphaylock@student.unimelb.edu.au\nAleksandra Michalewicz\naleksm@unimelb.edu.au\nSteph Raad\nsraad@student.unimelb.edu.au\nEmily Fitzgerald\nﬁej@unimelb.edu.au\n1 Melbourne Data Analytics Platform (MDAP), University of\nMelbourne, Melbourne Connect, Carlton 3053, Victoria,\nAustralia\n2 Melbourne School of Psychological Sciences, University of\nMelbourne, Tin Alley, Parkville 3010, Victoria, Australia\n123"
}