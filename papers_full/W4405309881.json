{
  "title": "Establishing vocabulary tests as a benchmark for evaluating large language models",
  "url": "https://openalex.org/W4405309881",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2096779661",
      "name": "Gonzalo Martínez",
      "affiliations": [
        "Universidad Carlos III de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2095793212",
      "name": "Javier Conde",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2616944996",
      "name": "Elena Merino Gómez",
      "affiliations": [
        "Universidad de Valladolid"
      ]
    },
    {
      "id": "https://openalex.org/A2905126319",
      "name": "Beatriz Bermudez-Margaretto",
      "affiliations": [
        "Universidad de Salamanca"
      ]
    },
    {
      "id": "https://openalex.org/A2116184410",
      "name": "Jose Alberto Hernandez",
      "affiliations": [
        "Universidad Carlos III de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A1951092153",
      "name": "Pedro Reviriego",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A24323327",
      "name": "Marc Brysbaert",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A2096779661",
      "name": "Gonzalo Martínez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095793212",
      "name": "Javier Conde",
      "affiliations": [
        "Universidad Politécnica de Madrid",
        "European Telecommunications Standards Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2616944996",
      "name": "Elena Merino Gómez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2905126319",
      "name": "Beatriz Bermudez-Margaretto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116184410",
      "name": "Jose Alberto Hernandez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1951092153",
      "name": "Pedro Reviriego",
      "affiliations": [
        "European Telecommunications Standards Institute",
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A24323327",
      "name": "Marc Brysbaert",
      "affiliations": [
        "Ghent University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1983578042",
    "https://openalex.org/W4214751380",
    "https://openalex.org/W1992009922",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2415973339",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4378193514",
    "https://openalex.org/W4381733282",
    "https://openalex.org/W2164318591",
    "https://openalex.org/W2117031222",
    "https://openalex.org/W2157564630",
    "https://openalex.org/W4297998383",
    "https://openalex.org/W2014307400",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4387724042",
    "https://openalex.org/W4280598557",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W7020750956",
    "https://openalex.org/W2309528772",
    "https://openalex.org/W1964081397",
    "https://openalex.org/W1996979899",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4399598212",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4378501273",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4385889917",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4319323490",
    "https://openalex.org/W4378498597",
    "https://openalex.org/W2784963995",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4386043250",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4401798709",
    "https://openalex.org/W179875071",
    "https://openalex.org/W4387293246"
  ],
  "abstract": "Vocabulary tests, once a cornerstone of language modeling evaluation, have been largely overlooked in the current landscape of Large Language Models (LLMs) like Llama 2, Mistral, and GPT. While most LLM evaluation benchmarks focus on specific tasks or domain-specific knowledge, they often neglect the fundamental linguistic aspects of language understanding. In this paper, we advocate for the revival of vocabulary tests as a valuable tool for assessing LLM performance. We evaluate seven LLMs using two vocabulary test formats across two languages and uncover surprising gaps in their lexical knowledge. These findings shed light on the intricacies of LLM word representations, their learning mechanisms, and performance variations across models and languages. Moreover, the ability to automatically generate and perform vocabulary tests offers new opportunities to expand the approach and provide a more complete picture of LLMs’ language skills.",
  "full_text": "RESEA RCH ARTICL E\nEstablishing vocabulary tests as a benchmark\nfor evaluating large language models\nGonzalo Martı ´ nez\n1\n, Javier Conde\nID\n2\n, Elena Merino-Go ´ mez\nID\n3\n, Beatriz Bermu ´ dez-\nMargaretto\n4\n, Jose ´ Alberto Herna ´ ndez\n1\n, Pedro Reviriego\nID\n2\n*, Marc Brysbaert\nID\n5\n1 Departamen to de Ingenierı ´ a Telema ´ tica, Univers idad Carlos III de Madrid, Legane ´ s, Spain, 2 ETSI de\nTelecomuni cacio ´ n, Univers idad Polite ´ cnica de Madrid, Madrid, Spain, 3 Escuela de Ingenierı ´ as Industriales ,\nUniversidad de Valladolid , Valladolid , Spain, 4 Departamen to de Psicologı ´ a Ba ´ sica, Psicobiologı ´ a y\nMetodolo gı ´ a de las CC. del Compto, Univers idad de Salamanca , Salamanc a, Spain, 5 Department of\nExperime ntal Psychology , Ghent University, Ghent , Belgium\n* pedro. reviriego@upm .es\nAbstract\nVocabulary tests, once a cornerstone of language modeling evaluation, have been largely\noverlooked in the current landscape of Large Language Models (LLMs) like Llama 2, Mistral,\nand GPT. While most LLM evaluation benchmarks focus on specific tasks or domain-spe-\ncific knowledge, they often neglect the fundamental linguistic aspects of language under-\nstanding. In this paper, we advocate for the revival of vocabulary tests as a valuable tool for\nassessing LLM performance. We evaluate seven LLMs using two vocabulary test formats\nacross two languages and uncover surprising gaps in their lexical knowledge. These find-\nings shed light on the intricacies of LLM word representatio ns, their learning mechanisms,\nand performanc e variations across models and languages. Moreover, the ability to automat-\nically generate and perform vocabulary tests offers new opportunities to expand the\napproach and provide a more complete picture of LLMs’ language skills.\nIntroduction\nIn a seminal paper Landauer and Dumais [1] presented latent semantic analysis (LSA) as a\nnew theory of knowledge representation. Meaning was inferred from local co-occurrences of\nwords in representative text. The main idea was that one can learn the meaning of an unfamil-\niar word \"X\" from the words that frequently occur with \"X.\" Results with the LSA model sug-\ngested that English vocabulary could be acquired in that way at a rate comparable to\nschoolchildren, without prior linguistic or perceptual knowledge.\nLandauer and Dumais not only proposed the theory but also presented a mathematical\nmodel that, starting from the co-occurrences of words in the texts, constructed a matrix that\nwas then mapped to a space of reduced dimensions. In this space of a few hundred dimensions,\neach word was represented by a point and the distance between the points represented the dis-\ntance in meaning. The points were defined as semantic vectors and had about 300 dimensions.\nWords with similar meanings had semantic vectors that were close to each other.\nLandauer and Dumais evaluated their LSA model with a vocabulary test. They used 80\nitems from the synonymy section of the Test of English as a Foreign Language (TOEFL). In\nthis task, target words were presented with four response alternatives from which the correct\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 1 / 17\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Martı ´ nez G, Conde J, Merino-Go ´ mez E,\nBermu ´ dez-Margar etto B, Herna ´ ndez JA, Reviriego\nP, et al. (2024) Establishi ng vocabulary tests as a\nbenchmark for evaluating large language models.\nPLoS ONE 19(12): e0308259. https://do i.org/\n10.1371/ journal.pone. 0308259\nEditor: Jessie S. Barrot, National University\nPhilippines , PHILIPPIN ES\nReceived: June 26, 2024\nAccepted: October 4, 2024\nPublished: December 12, 2024\nCopyright: © 2024 Martı ´ nez et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All data is available\non a Github repository: https://github. com/\nWordsGPT/ LLM_Vocab ulary_Evaluati on.\nFunding: This work was partially supporte d by the\nproject CyberTutor: Asistente educativo\npersonaliz ado basado en Grandes Modelos de\nLenguaje (LLM), funded by “Primeros Proyectos”\ncall from ETSIT, UPM; by the FUN4DATE\n(PID2022-136 684OB-C22) and ENTRUDIT\n(TED2021- 130118B-I00 projects funded by the\nSpanish Agencia Estatal de Investigac io ´ n (AEI); by\none had to be chosen. Landauer and Dumais found that the semantic vector of the correct\nanswer was closest to that of the target word in 51 items (64% correct). This was comparable to\nUS college candidates from non-English-speaking countries who scored 65% correct on aver-\nage [1].\nAlthough the scores of the LSA model were not perfect (they were at the same level as\nspeakers of English as a second language), they were much better than what was available at\nthe time. Jarmasz and Szpakowicz reported that other algorithms for word meaning scored\nbarely above the chance level (25%) on the TOEFL test [2]. In subsequent years, authors tried\nto improve the performance of LSA-type models by optimizing the algorithm and training\nmaterials. Performance on the TOEFL test (as it became known) gradually increased until Bul-\nlinaria and Levy reported 100% correct performance for a model tweaked to the test [3, 4].\nLandauer and Dumais argued that their LSA model could be viewed as a simple three-layer\nneural network, an argument that was confirmed when real connectionist networks were\ndeveloped that quickly outperformed the co-occurrence statistics used in LSA [5–7].\nThe work of Landauer and Dumais showed how vocabulary can be learned from a large\ncorpus of text using only local relationships between words. This breakthrough was a funda-\nmental step in understanding how language can be acquired and how computer systems can\nbe implemented that can learn the meaning of words and text. These concepts and models can\nbe seen as forerunners of today’s Large Language Models (LLMs). Since vocabulary tests were\nused to evaluate the predecessors of LLMs, a research question which we try to answer in this\npaper naturally arises: are vocabulary tests still relevant to evaluate LLMs?\nLarge language models design and evaluation\nOver the past decade, artificial intelligence has made impressive progress in language modeling\n(and other areas). The improvements were possible due to the availability of huge text datasets,\nmore powerful computing and storage, and architectures that can implement very large mod-\nels efficiently (such as transformers). First, the introduction of the transformer [8] and then\nthe development of popular models based on it, such as BERT [9] or T5 [10], paved the way\nfor the development of LLMs with many billions of parameters, such as GPT4 [11]. Trans-\nformers are complex neural networks with many interconnected layers and novel mechanisms\nsuch as attentional focus that allow them to learn complex relationships, for example, between\nwords in text. By increasing the size of transformers and training datasets, unprecedented per-\nformance has been achieved in many language processing tasks, but more importantly, they\nhave been integrated into products such as ChatGPT, Gemini, or Bing, reaching hundreds of\nmillions of users [12]. The main features of these LLMs are the huge size of their training data-\nsets and number of model parameters, their ability to learn different languages (including pro-\ngramming languages) and perform a wide range of tasks such as generating text in genres,\ntranslating, answering questions and summarizing [13]. LLMs operate on units called tokens,\nwhich in some cases correspond to words but can also be sequences of a few letters from the\ntraining dataset. Texts are decomposed into tokens as input, and the model generates output\ntokens that are assembled into words and sentences. LLMs are models that predict the next\ntoken, and tokens are mapped into words. This process can be applied recursively to construct\ncomplex sentences or even longer texts or to build more powerful tools based on LLMs. A par-\nticularly attractive application of LLMs is the development of intelligent chatbots, such as the\nwell-known ChatGPT used by hundreds of millions of people worldwide. Chatbots are models\nthat use a base pre-trained LLM to specialize in answering questions. To do this, the model has\nbeen partially retrained (finetuned) with a dataset of question-answer pairs, allowing the neu-\nral network to learn how to process questions and respond to them.\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 2 / 17\nthe Chips Act Joint Undertakin g project SMARTY\n(Grant no. 101140087) and by the OpenAI API\nResearch Access Program. The funders had not\nplayed in study design, data collection and\nanalysis, decision to publish, or prepara tion of the\nmanuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nThe capabilities of LLM based chatbots make them attractive for use in L2 teaching [14, 15].\nFor example, they can be used to interact with students by engaging in conversations or gener-\nating questions to practice different skills and correct the mistakes of the students. They can\nalso assist students when writing. However, as with any new technology there are also draw-\nbacks and limitations. Chatbots can be used to complete assignments students should do and\nwrite essays for them. Chatbots can also instill a false confidence in students, reducing the\nincentive to learn the language. LLMs may further produce invalid and incorrect information\nthat is hard to detect as it is well written. Regardless of these limitations LLMs are poised to\nchange the way languages are learned in the near future.\nAs mentioned before, LLMs are trained to predict the next token in a sentence [16]. This is\ndone by taking existing texts, removing tokens from them and using these tokens as criteria\nfor the outcome to be predicted by the model. This approach makes it possible to use huge\ndatasets containing almost every text of interest on the Internet. The reason why models work\nwith tokens is that words in all languages can be generated with just a list of tens or hundreds\nof thousands of tokens. Otherwise, if they worked with all the words from every language, it\nwould make the models too heavy and slow. In contrast, if models operated with single charac-\nters, many predictions would be needed to construct long texts. The tokenization strategy\n(design of the token dictionary) is very relevant when designing an LLM. The most common\npractice is to assign tokens to the most frequent words. Likewise, many models have predomi-\nnantly been trained in English, tokenizers have also been designed to optimize generation in\nthis language. This means that generating the same text in two different languages may take\nmore time, cost, or produce responses with lower quality.\nInterestingly, predicting the next token assumes that languages and words can be learned\nfrom the words that appear in texts together with the target word. This is exactly the assump-\ntion Landauer and Dumais made in their LSA model. Therefore, LLMs can, in a way, be seen\nas descendants of LSA. The prediction of words based on surrounding words is not the only\nlink between LLMs and LSA. The use of a point in a multidimensional space to represent word\nmeanings, introduced by LSA, is also used by LLMs to map inputs to so-called \"embeddings.\"\nThese embeddings are vectors of values that correspond to a point in the space where the\nmeaning is located, so that, similar to semantic vectors in LSA, points that are close together\ncorrespond to similar meanings. In the case of LLMs, embeddings are usually larger than the\n300 dimensions used in LSA, with several thousand dimensions.\nTo evaluate LLM performance, several benchmarks have been proposed [11]. In most cases,\nthe test evaluates how well LLMs answer questions on almost any topic [17, 18], or are able to\nperform reasoning based on a given text [19]. For example, there are benchmarks with thou-\nsands of mathematical problems covering almost every discipline in mathematics [20]. There\nare also frameworks that can be easily extended so that new tasks or tests can be added at will\n(see for example https://github.com/open ai/evals). Those expanded benchmarks focus on\nquantifying how well LLMs perform on different knowledge tasks. Fig 1 shows the evolution\nof models over the years along with their sizes expressed in billions of parameters, since the\nintroduction of the transformers architecture in 2017 [21]. In November 2022, the trend of\nLLMs emerged, coinciding with OpenAI’s launch of ChatGPT.\nLarge language models produce language, which is likely to create a\nfeedback loop\nLLMs do not just answer questions or solve problems. At the same time, they produce lan-\nguage output. They create text when they provide answers or translations. In fact, they are\nalready being used to help write entire novels and textbooks. So, people and future LLMs will\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 3 / 17\nbe increasingly exposed to LLMs’ output, creating a feedback loop [22]. As a simple example,\ntake a word that is not produced by LLMs because a simpler synonym exists. This word will\nappear less and less in the language to which future people and LLMs are exposed until it\nbecomes extinct.\nSome research is beginning to appear on these subtler, as well as more fundamental, linguis-\ntic aspects of the use of LLMs, but it is still very limited compared to the large number of arti-\ncles evaluating the performance of LLMs on various knowledge tasks. There are some studies\nthat have focused on the linguistic characteristics of the text generated by LLMs. A comparison\nof the linguistic characteristics of humans and LLMs is presented in [23], which focuses on the\nanalysis of news generated by an open-source LLM. There are also studies that focus on pho-\nnological [24] and lexical [25] aspects of LLMs. Finally, the effect of LLMs in academic writing\nhas also been recently studied showing how some words are becoming more popular, probably\ndue to the use of generative AI tools by the authors when writing papers [26].\nIn this article, we look at how LLMs perform on vocabulary tests, what this tells us about\nhow LLMs learn language, and try to answer the question of whether vocabulary tests can be a\nuseful addition for evaluating LLMs.\nVocabulary tests and large language models\nAs discussed at the beginning of this article, the TOEFL test was an important tool for evaluat-\ning Landauer and Dumais’ LSA model and its later extensions. Vocabulary tests are also widely\nused to assess language proficiency in humans [27]. Surprisingly, current LLMs are no longer\ntested on vocabulary tests, probably because everyone assumes they will be error-free. Given\nthat LSA-type models were already achieving flawless performance in early 2010 and that cur-\nrent LLMs are vastly superior in design and the amount of training material, it seems a waste\nof time to test them on something as simple as the TOEFL test, which only taps into knowledge\nFig 1. Evolution of LLMs. Released models and model sizes over the years. The sizes of commerc ial models are not\nofficial.\nhttps://d oi.org/10.1371/j ournal.pon e.0308259.g0 01\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 4 / 17\nknown to undergraduates with English as a second language. Indeed, this is what the present\nauthors expected when they used the TOEFL test as the starting point of a study that would\nuse more taxing vocabulary tests.\nVocabulary tests traditionally consist of multiple-choice questions that require participants\nto choose the correct answer from a number of alternatives [28]. Indeed, this is the format of\nthe TOEFL test. The difficulty of the test then depends on the difficulty of the target words and\nthe number and difficulty of the answer alternatives. By varying these, it is possible to take\nmore demanding vocabulary tests than the TOEFL to see what level various LLMs achieve.\nLemho ¨ fer and Broersma introduced another format for vocabulary tests that may be of partic-\nular interest for LLM testing [29]. They presented participants with a list of letter strings and\nasked them to indicate which words they knew. To prevent participants from selecting all items\nwithout knowing them, legal letter strings were added that do not exist as words in English (such\nas \"plound\" or \"ternace\"; these are so-called non-words or pseudowords). Performance was esti-\nmated based on both word and non-word performance, so that a participant who selected all\nitems would receive a score of zero. The Yes/No format gained momentum when Lemho ¨ fer and\nBroersma published an English language proficiency test for use in psycholinguistic research,\nwhich they called LexTALE [30]. The Yes/No format is interesting because the test taker must\nrefrain from choosing the non-words. LLMs are known to tend to present nonexistent informa-\ntion based on word co-occurrences (so-called hallucinations), as discussed in [16]. So, we thought\nit would be interesting to see how LLMs would perform on tests with non-words.\nConsidering the evaluation of LLMs, vocabulary tests have a number of features that are of\ninterest:\n• First, vocabulary tests are pure language tests with stimuli that are not embedded in an infor-\nmative context. Thus, vocabulary tests could potentially be used to evaluate LLMs’ knowl-\nedge of languages. This is an important point since LLMs should perform equally well for all\nthe languages they claim to support (see [31], for evidence that this unfortunately is often\nnot the case yet).\n• Second, if a model makes errors, these errors can be analyzed to study how LLMs learn a lan-\nguage and to evaluate whether cognitive theories of language acquisition apply to LLMs. For\nexample, early acquired words are often well remembered by humans even though they\nalmost never occur in everyday language use [32]. Vocabulary tests allow fine-grained evalu-\nation of LLM, which facilitates analysis and understanding of the mechanisms and algo-\nrithms underlying the models.\n• Third, how do LLMs interpret non-words and are they able to distinguish them from valid\nwords? Do they follow the same mechanisms as humans (discussed in [33])? How does the\ntokenization that LLMs use affect word learning? To what extent does performance depend\non the specific question posed to the LLM?\n• A fourth important feature of vocabulary tests is that they can be automated, both for test\ngeneration and for test execution. This is especially true for the Yes/No format. There are\nsoftware tools that can be used to generate non-words in different languages [34] and even\nworkflows to generate entire test suites [35]. This makes it possible to generate large-scale\nvocabulary tests. Similarly, the execution of those tests in different LLMs can be automated,\nallowing the evaluation of multiple LLMs in multiple languages, without practical limitations\non the number of words tested [36].\nThese characteristics make vocabulary tests potentially interesting for evaluating LLMs.\nThe main question is: are vocabulary tests still relevant to evaluate LLMs? To try to answer it\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 5 / 17\nwe pose ourselves three more specific research questions to get a better understanding of the\npotential of vocabulary tests for LLM evaluation:\n• Research Question 1 (RQ1): What is the performance of current LLMs on existing vocabu-\nlary tests?\n• Research Question 2 (RQ2): Are vocabulary tests capable of discriminating LLM\nperformance?\n• Research Question 3 (RQ3): Which type of vocabulary tests are more relevant for LLM\nevaluation?\nIn the following we present an initial assessment of the use of existing vocabulary tests for\nevaluating LLMs so that we can answer those questions and decide whether or not it is a fruit-\nful approach.\nMaterials and methods\nTo assess the usefulness of vocabulary tests in LLM evaluation, we conducted several vocabu-\nlary tests on different LLMs. To have a representative sample of current LLMs, we selected two\ncompany-owned, commercial LLM tools: ChatGPT (based on GPT3.5 and GPT4 see https://\nopenai.com/blog/chatgpt) and Bard (based on PaLM 2, see https://ai.google/static/documents /\ngoogle-about-bard.pdf, now replaced by Gemini), together with two open source LLMs: Llama\n2 [37] and Mistral [38].\nChatGPT, developed by OpenAI, is the most popular LLM-based chatbot today and proba-\nbly the one that has shown the best performance across a variety of tasks. Two versions of\nChatGPT (with different numbers of parameters) were tested. Bard was developed by Google\nand is intended to compete with ChatGPT, so both are good examples of commercial chatbots.\nParameters and source code for these LLMs are not available, which makes them less interest-\ning for research purposes because they can be modified overnight without researchers being\nable to verify what was done. Still, because of their massive use by the public, it is worthwhile\nto determine their performance at some point. Llama 2, developed by Meta, is probably the\nbest known open-source LLM right now. Another open-source LLM with good performance\nis Mistral, developed by a startup of the same name. We tested three versions of Llama 2, of dif-\nferent sizes, to see to what extent performance improves as network complexity increases. The\nseven models considered in our evaluation are summarized in Table 1. They range from rela-\ntively small models to the largest models that were publicly accessible at the time of conducting\nthe experiments.\n• Mistral-7B. LLM developed by Mistral AI and released in September 2023. It has 7.3 billion\nparameters and is based on a transformer architecture. It stands out for its efficiency,\nTable 1. LLMs consider ed in the experimen ts.\nLLM Company Type Paramet ers\nMistral-7 B Mistral Opensourc e 7.3 billion\nLlama 2-7B Meta Opensourc e 7 billion\nLlama 2-13B Meta Opensourc e 13 billion\nLlama 2-70B Meta Opensourc e 70 billion\nPaLM 2 (Bard) Google Commerci al > 340 billion (non-offic ial)\nGPT-3.5-turbo (ChatGPT ) OpenAI Commerci al 175 billion\nGPT-4 (ChatGPT) OpenAI Commerci al > 1 trillion (non-officia l)\nhttps://do i.org/10.1371/j ournal.pone .0308259.t001\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 6 / 17\noutperforming larger models like Llama2 at the time of its release. It is an open-weight\nmodel, but the dataset used for its training is not public [38].\n• Llama 2 (7/13/70B). Developed by Meta AI and released in July 2023. It is available in ver-\nsions of 7B, 13B, and 70B parameters, utilizing a transformer architecture with techniques\nsuch as RMSNorm pre-normalization, SwiGLU activation function, and rotary positional\nembeddings [37]. Unlike other big tech companies, Meta has chosen to offer Llama 2 as\nopen source. It has been trained in over 20 languages, although most of the data is in English\n(89.7%), while other languages, such as Spanish, represent only 0.13% of the training data.\n• PaLM 2. Developed by Google AI and presented in May 2023, it is a lighter version com-\npared to its predecessor (PaLM) while offering better capabilities. The training dataset\nincludes more data in other languages to enhance multilingual capabilities compared with\nPaLM. Neither the training dataset nor the model weights have been released [39].\n• GPT3.5. Commercial model developed by OpenAI and a version of GPT-3. It is estimated to\nhave a size equal to GPT-3 (175 billion parameters), which was primarily trained in English\n(93%). GPT-3.5 is the model on which the first version of ChatGPT was based (November\n2022) [40].\n• GPT4: Latest version of the GPT family released in March 2023 [41]. The model adds image\nprocessing capabilities and improves its performance compared to its predecessor. Various\nsources indicate that it contains more than 1 trillion of parameters within a Mixture of\nExperts (MoE) architecture [42], which allows it to dynamically activate different subsets of\nits neural network depending on the input.\nThe tests were run automatically using the Application Programming Interfaces (APIs) of\nthe LLM-based chatbots to create the questions in each test and then produce an excel file with\nall the answers, as described in [36]. The only exception was Bard (now Gemini), whose API\nwas not accessible in Spain at the time of the evaluation, for Bard the user interface was used\nfor testing. Automation is interesting for running tests at scale, since we evaluated seven LLMs\non tests with dozens of questions each. In addition, the use of the API allowed control over\nLLM parameters, such as temperature, which adjust the variability of answers. For certain\nmodels, such as Llama 2, the responses included extra text along with the selected answer (A/\nB/C/D). When feasible, this text was automatically processed to isolate the answer and com-\npare it with the correct one to generate the evaluation metrics. In cases where this was not pos-\nsible, a manual analysis was performed to classify the response as correct or incorrect. During\nevaluation, LLMs were not given context information and default parameters were used except\ntemperature, which was set to zero if it was controllable to produce deterministic responses.\nThe prompts used to interrogate the chatbots were simple and similar to those used in the\nhuman tests (see below). The performance of LLMs can be improved by providing context or\nmore sophisticated prompts that force the LLMs to solve the questions step by step using a\nchain of thought [16]. However, our goal was to understand how LLMs perform in vocabulary\ntests when presented with the same questions as humans and not to modify the questions or\nprovide additional information to improve the LLMs’ answers.\nWe use several representative vocabulary tests, both with multiple choice and yes/no ques-\ntions, in our evaluation. In every case, our use of these tests adheres to the terms and condi-\ntions set forth by the sources. For example, for some of the tests, the questions cannot be made\npublicly available and thus are not included in the dataset that contains the results of the differ-\nent LLMs for the different tests.\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 7 / 17\nThe details of the multiple-choice and yes/no questions tests are summarized in Table 2.\nThe first test was the TOEFL introduced by Landauer and Dumais [1]. It contains 80 target\nwords with four alternatives to choose from (these are also single words). The difficulty of the\nitems varies, but the level is adapted to non-English-speaking students who want to study at\nEnglish-speaking universities. The test cannot be freely shared due to copyright restrictions,\nbut researchers can request access to the stimulus material, if use is strictly limited to research\npurposes. We were kindly granted access to the stimuli by the LSA research group at the Uni-\nversity of Colorado.\nThe second vocabulary test was the StuVoc test, published by Vermeiren et al. [43]. This\ntest contains three subtests with 50 validated English items each (thus 150 items in total).\nItems consist of target words in short neutral sentences along with four response alternatives.\nUnlike the TOEFL, the alternatives can include short descriptions of words. The first two sub-\ntests are difficult enough for English-speaking university students. The third subtest is easier\nand better suited for second-language speakers with high proficiency [27]. The level of the last\nsubtest is similar to the TOEFL, while the first two tests are more demanding.\nThe third vocabulary test was the Spanish adaptation of StuVoc. Bermu ´ dez-Margaretto &\nBrysbaert translated 146 of the English StuVoc items into Spanish and validated them on a\ngroup of adult native Spanish speakers [39]. They selected the 80 best items (good distribution\nof difficulty levels, good correlation between item performance and overall test performance,\nand a clear transition from unknown to known based on item response theory analysis). The\nremaining 66 items were considered less interesting for various reasons.\nThe fourth and fifth tests were yes/no tests. For English, we used LexTALE, proposed by\nLemho ¨ fer and Broersma [29]. The test contains 40 English words and 20 non-words. Since the\ntest is aimed at advanced second language speakers, the level is comparable to the TOEFL.\nNative speakers typically score more than 90% correct on the test (scores obtained after sub-\ntracting the % yes answers to non-words).\nFinally, we tested the models on a Spanish Yes/No test published by [40]. This test is more\ncomprehensive and more difficult than the English LexTALE because the authors wanted the\ntest to be usable by both native Spanish speakers and second-language speakers [41]. The test\ncontains 60 Spanish words and 30 non-words. We presented all vocabulary tests to all LLMs.\nFor the multiple-choice tests, we asked: “Answer the option which is the meaning of the fol-\nlowing word \"squelch\": a. suppress b. revive c. acquire d. dispute. Please, first just answer the\nletter of the option and below your explanation.\". For the Yes/No tests, we asked: \"Please\nanswer \"Yes\" or \"No\" to the following question: Is X an existing word in English (Spanish)?\".\nThe prompts were the same, in English for all tests to avoid prompts from causing differences\nin the results We have selected these five tests, which include two languages and two types of\nevaluations (multiple-choice and yes/no for words/pseudowords), because they allow to assess\ndifferent aspects of LLMs, such as their performance degradation when using a language other\nthan English and their ability to recognize pseudowords as invalid (which is a task they strug-\ngle with). Table 3 contains samples of all the test except for TOEFL as it is not public.\nTable 2. Vocabulary tests considered in the experimen t.\nTest Type Language Numbe r of items\nTOEFL Multiple choice English 80\nStuVoc-EN Multiple choice English 150\nStuVoc-SP Multiple choice Spanish 80\nLexTAL E-EN Yes/No English 60\nLexTAL E-SP Yes/No Spanish 90\nhttps://do i.org/10.1371/j ournal.pone .0308259.t002\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 8 / 17\nResults\nThe aim of our study is to determine whether vocabulary tests are still relevant for evaluating\nLLMs. First, we will analyze the performance of models using different types of vocabulary\ntests and in different languages (RQ1). Next, we will examine whether these tests can differen-\ntiate the quality of the models (RQ2). Finally, we will investigate which vocabulary tests are\nmost suitable for LLM evaluation (RQ3). The results obtained from the multiple-choice tests\nare summarized in Table 4.\nAs expected, most models performed well on the TOEFL test, and the larger Llama 2 mod-\nels outperformed the basic 7B version. Still, the performance was not flawless. Moreover, the\nmodels differed in the items they got wrong, suggesting that suboptimal performance was not\ndue to one or two weak items. The three commercial models failed on the item \"fashion,\"\nwhere they chose the option \"rage\" instead of \"manner\" (which Llama 2-7B and Mistral-7B did\nget right). For the item \"figure,\" GPT3.5, Bard and Mistral-7B chose the option \"express\"\ninstead of \"solve.\" GPT4 and Mistral-7B obtained the best scores. However, the differences\nwith PaLM 2 (Bard), Llama 2-70B and GPT3.5 were small. Unfortunately, no further informa-\ntion can be given for the TOEFL test as the items are copyrighted. For the other tests, the\nresults for each model and item are available in a public GitHub repository (available at\nhttps://github.com/WordsGPT /LLM_Vocabulary_Evaluation). This dataset contains for each\nitem on each test the response given by each of the LLMs in machine readable files. The links\nto the LLM models and versions used are also provided.\nDespite the fact that the StuVoc-EN is more demanding for human speakers than the\nTOEFL, LLMs’ performance on this test was generally higher than on the TOEFL. One reason\nTable 3. Samples of vocabula ry tests considere d in the experimen ts.\nTest Example of question Correct\nanswer\nStuVoc-EN Answer the option which is the meaning of the following word \"ablution\":\na. did all her duties as a minister\nb. washed herself to get ready\nc. played her set piece of music\nd. did her exercises to stay healthy\nThis is an exampl e sentence: She performe d her ablutions.\nPlease, first just answer the letter of the option and below your explanat ion.\nb\nStuVoc-S P Answer the option which is the meaning of the following word \"ablucio ´ n\":\na. tocar su pieza musical\nb. hizo sus ejercicios para manteners e saludable\nc. cumplio ´ con todos sus deberes como ministra\nd. se lavo ´ para prepararse\nThis is an exampl e sentence: Ella realizo ´ sus abluciones .\nPlease, first just answer the letter of the option and below your explanat ion.\nd\nLexTAL E-EN Please respond with \"Yes\" or \"No\" to the following question: Is \"mensible\" an\nexisting word in English?\nno\nLexTAL E-SP Please respond with \"Yes\" or \"No\" to the following question: Is \"terzo\" an existing\nword in Spanish?\nno\nhttps://d oi.org/10.1371/j ournal.pon e.0308259.t00 3\nTable 4. Performance of LLMs on the multiple-cho ice tests (percentag e of correct answers).\nTest Llama 2-7B Llama 2-13B Llama 2-70B Mistral-7 B GPT3.5 GPT4 PaLM 2\nTOEFL 76.3% 93.9% 96.3% 98.8% 96.3% 98.8% 97.5%\nStuVoc-EN 91.3% 96.0% 94.7% 99.3% 97.3% 100% 98.7%\nStuVoc-SP 73.8% 76.4% 61.3% 68.8% 93.8% 95.0% 96.3%\nhttps://do i.org/10.1371/j ournal.pone .0308259.t004\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 9 / 17\ncould be the availability of a short, neutral context sentence (given information about the part-\nof-speech). Another reason could be that multi-word descriptions describe the meaning of the\ntarget words better than one-word synonyms. Again, the best scores were obtained by GPT4\nand Mistral-7B, closely followed by PaLM 2 and GPT3.5. The Llama 2 models had lower\nscores, and Llama 2-13B performed better than Llama 2-70B. The item \"Let’s not pussyfoot\naround\" was censored by PaLM 2 and Llama 2-7B. What is further striking is that no item was\ncensored by all LLMs. Every item was scored correctly by at least 4 of the 7 LLMs. Three LLMs\nhad only two items wrong: \"The boy shuddered.\" where they selected \"almost fell\" instead of\n\"shook\" and for \"She parried the comments\" where the three failing models selected different\nanswers.\nThere was a significant performance drop for StuVoc-SP compared to StuVoc-EN, even\nthough the items were direct translations. The drop was especially large for the open-source\nmodels (Llama 2 and Mistral), which fell below 80%. For the commercial models, the decline\nwas smaller, but still noticeable. Bard (PaLM 2) achieved the highest score in this test.\nThe distribution of the failures among the words is shown in Fig 2. It can be observed that\nfor TOEFL there are words that are failed by five out of seven models and for StuVoc-SP even\nfor six out of seven models. Instead for StuVoc-EN the worst case are three failures for a word.\nGiven that TOEFL and StuVoc-EN have similar failing percentages, and both are low, the\nresults suggest that TOEFL has some specific questions that are harder for LLMs while that is\nnot the case for StuVoc-EN. For StuVoc-SP, the failure rates are much higher, and therefore, a\nfew questions failing on five or six models can be due to the large number of failures. In the\ncase of Llama2, the best model for StuVoc is the intermediate sized one (13B), not the largest\nas one might expect. The StuVoc-SP translation stands out, where 70B gets 61.3% of the ques-\ntions correct, while 13B gets 76.4% and 7B gets 73.8%. Comparing the Llama2 and StuVoc-SP\nversions, the 70B model makes 38.7% of its errors on its own (12 questions), while in 13B this\npercentage is much lower (5.3%, 1 question). For example, the word ablucio ´ n is correctly iden-\ntified by 13B and 7B, but incorrectly by 70B which identifies it as exercise. These results are\naligned with the poorer performance of the models in languages other than English, where\nmore powerful versions in English do not imply they are better in Spanish.\nAfter the analysis of the results of the multiple-choice tests we can go back to our research\nquestions:\n• RQ1 (What is the performance of current LLMs on existing vocabulary tests?): LLMs achieve\ngood performance on the vocabulary tests but no model got all the answers correctly except\nfor GPT4 on StuVoc-EN. The performance is significantly worse in Spanish than in English.\nFig 2. Distribution of the failures per word in TOEFL (left), StuVoc- EN (center) and StuVoc-SP (right).\nhttps://do i.org/10.1371/j ournal.pone .0308259.g00 2\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 10 / 17\n• RQ2 (Are vocabulary tests capable of discriminating LLM performance?): the tests are able\nto discriminate LLM performance and can be used to compare LLMs. For example, they\nshow the limitations of open models in Spanish.\nThe results for the Yes/No tests are presented in Table 5 and are reported independently for\nwords and non-words to better understand the results. Performance on LexTALE-EN was\nquite good and tended to be better for words than for non-words. Remember that this is a\nfairly easy test designed for second language speakers. Interestingly, for Llama 2 we see better\nperformance on words as the model gets larger and at the same time worse performance on\nnon-words. ChatGPT4 (GPT4) was flawless on all items, but Google Bard (PaLM 2) in its cur-\nrent version was poor (with Llama 2-70B) in hallucinating the existence of non-words.\nAnalyzing the data from LexTALE-SP, we saw that two of the non-words (vegada, capillo)\nare existing words in Spanish because they appear in some dictionaries. Therefore, we\nexcluded these two items from the analyses. Performance was good for the words (especially\nconsidering that some were more difficult than the LexTALE-EN words). When further asked\nabout the meaning of the words, most models gave the English translation. However, perfor-\nmance was very poor for the Spanish non-words, where the models not only gave \"yes\"\nanswers, but when asked, readily gave meanings and translations for letter combinations that\ndo not exist in Spanish. This was especially true for Llama 2 and Mistral. Mistral was unable to\nidentify even one non-word and performed like a boastful test taker, claiming to know all the\n\"words\" but in reality scoring zero points. Performance was best for GPT4, but even this model\npresented interpretations for 18% of the non-words. Performance was also poor for Bard,\nwhere interpretations were given for more than half of the non-words.\nThe distribution of the failures among the words, non-words and all test items is shown in\nFig 3. It can be observed that failures are concentrated on non-words. As for the multiple-\nchoice tests, the English version does not show strong correlation among model failures. For\nthe Spanish test, the failure rates for non-words are so high that again having a few questions\nfailing on most or all models is expected even with no correlation among failures. In this case,\nMistral7B is not capable of detecting non-words in LexTALE-SP. However, other models like\nGPT-3.5, which contains only 0.8% Spanish in its training dataset, can. Mistral was possibly\ntrained with a similar percentage of Spanish (the information is not public). Nevertheless, the\nsize of the training dataset is not the only factor that determines multilingual capabilities.\nOther aspects, such as the model’s architecture or the tokenization strategy, can also affect the\nmodel’s performance.\nAs with the multiple-choice tests, after the analysis of the results of the yes/no tests we can\ngo back to our research questions:\n• RQ1: LLMs also achieve good performance on the yes/no tests but again except for GPT4 on\nLex-TALE-EN scores are also below 100%. Results are worse for nonwords and for Spanish.\n• RQ2: again, the tests are able to discriminate LLM performance and can be used to compare\nLLMs. For example, they show the inability of Mistral-7B in recognizing nonwords in Spanish.\nTable 5. Performance of October 2023 LLMs on the Yes/No tests (percentage of correct answers).\nTest Type Llama 2-7B Llama 2-13B Llama 2-70B Mistral-7 B GPT3.5 GPT4 PaLM 2\nLexTAL E-EN Words 92.5% 87.5% 100% 100% 100% 100% 100%\nLexTAL E-EN Non-word s 100% 95% 85% 95% 95% 100% 85%\nLexTAL E-SP Words 96.7% 96.7% 100% 100% 96.7% 100% 100%\nLexTAL E-SP Non-word s 7.1% 60.7% 14.3% 0% 67.9% 82.1% 46.4%\nhttps://do i.org/10.1371/j ournal.pone .0308259.t005\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 11 / 17\nFinally, having all the results we can also go back to our last research question:\n• RQ3 (Which type of vocabulary tests are more relevant for LLM evaluation?): both tests,\nmultiple-choice and yes/no can be used to evaluate and compare LLMs. However, yes/no\ntests are more interesting as LLMs have a tendency to identify nonwords as words.\nThe Spanish results suggest that current LLMs are likely to perform less well in languages\nother than English, for which the training materials were most extensive. To better understand\nthis effect, the information on the training datasets for GPT3 (see https://github.com/openai/\ngpt-3/blob/master/dataset_ statistics/languages_by_word_c ount.csv), Palm [44] and Llama 2\n[37] is summarized in Table 6. For the other models, the information on the training dataset is\nnot publicly available as it is considered a key element of the design, and it is kept confidential\nfor commercial reasons (see https://huggingface .co/mistralai/Mistral-7B-v0.1/discu ssions/8).\nWhat is first noticeable is the small percentage of training materials in Spanish, even though\nSpanish is one of the most widely spoken languages in the world (and present in the USA). To\nsome extent, it is surprising that performance on StuVoc-SP and for some LLMs on LexTA-\nLE-SP was so good, given the limited amount of Spanish training material these models\nreceived. At the same time, the results clearly show that the use of LLMs for languages other\nthan English is oversold.\nDiscussion\nIn the early days of machine language models, developers tested the quality of their models\nwith a vocabulary test (specifically, the TOEFL test introduced by Landauer & Dumais [1]. At\npresent this is no longer done, possibly because developers assume that current models are\nerror-free given the improvements in design and amount of training over the past decade. In\nthis paper we have analyzed the performance of LLMs on several vocabulary tests to under-\nstand if they are still relevant as benchmarks for LLMs.\nWhen we tested the performance of available LLMs, however, we saw that most models did\nnot achieve 100% scores on the various vocabulary tests (RQ1) (the exception was GPT4 on\nStuVoc-EN and LexTALE-EN). Performance was especially poor in Spanish and in Yes/No\ntests. Therefore, the tests can still be used to discriminate LLM performance (RQ2).\nFig 3. Distribution of the failures per word in LexTale- EN (left) and LextTale- SP (right).\nhttps://do i.org/10.1371/j ournal.pone .0308259.g00 3\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 12 / 17\nThe origin of the poor performance in Spanish is not difficult to find. Given the small per-\ncentage of training material the models received in Spanish (Table 6), it is to some extent sur-\nprising that the models still performed so well (RQ1). At the same time, our findings suggest\nthat errors in Spanish are more likely than in English and that the quality of Spanish answers\nwill be linguistically worse than the quality of English answers (RQ2). Given that Spanish is\none of the most widely spoken languages in the world (and is present in the U.S.), it is to be\nexpected that performance will decline even further for languages with less training material.\nOur findings indicate that training datasets should be more balanced to avoid bias against lan-\nguages other than English, and they demonstrate the usefulness of vocabulary tests to perform\ncomparisons of language proficiency in LLMs.\nThe high number of non-words accepted as meaningful by LLMs (again, particularly in\nSpanish) is also important and worthy of further investigation. Needless to say, language mod-\nels that hallucinate meaning when there is none are poor assistants. One possible cause could\nbe that some models do not work with words as input and output units, but with tokens of a\ncertain length, regardless of word boundaries. In such models, words have no special status\nand non-words similar to existing words (as good pseudowords should be) can activate lexical\noutput. Even in humans, there is evidence that some non-words activate word-related mean-\nings [32]. Further factors contributing to hallucinations of non-words are likely to be cross-\nlanguage contamination (a non-word in one language may be a word in another language)\nand spelling errors in training materials. Therefore, again vocabulary tests are relevant to\nunderstand how LLMs interpret words.\nAnother question is whether the performance of the best English models is already good\nenough. Much here depends on what one wants (or claims) to achieve. If average human per-\nformance is the goal, then the current best models may already be good enough. However, if\nmodels are intended to improve human performance, then further progress still seems possi-\nble, even in English.\nPerhaps even more important than the results with the specific tests we used is the useful-\nness of vocabulary tests in general to examine the performance of LLMs. Many hypotheses in\ncognitive science linked to existing theories of human cognition can be related to the genera-\ntion of specific words (and non-words) that can be tested in LLMs, to see if the predicted accu-\nracy differences are obtained. This will likely lead to new theoretical developments that may be\napplicable to both LLMs and humans. By focusing on words rather than knowledge areas,\nvocabulary tests provide a fine-grained mechanism for investigating the fundamental cognitive\nmechanisms of language processing in LLMs (and humans). The vocabulary tests of Table 2\nincluded words that were good for testing language proficiency in general, but words (and\nnon-words) can also be selected to answer specific theoretical questions.\nAnother aspect that can be varied is the format of the vocabulary test. In this article, we\nhave discussed the multiple-choice format and the yes/no format (RQ3). Other formats are\nthose in which a correct definition or word must be generated (production rather than recog-\nnition). Such tests are more difficult for humans, but they can be easier for LLMs. Even within\na format, a few changes can make a difference [43]. In the results section, we wondered to\nwhat extent the availability of neutral carry phrase improves performance. This can be easily\nTable 6. Percentag e of the training dataset in English and Spanish for several LLMs.\nModel English Spanish\nGPT3 92.64% 0.77%\nPaLM 2 77.98% 2.11%\nLlama 2 89.70% 0.13%\nhttps://d oi.org/10.1371/j ournal.pon e.0308259.t00 6\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 13 / 17\ntested in LLMs and, if deemed interesting, later in humans. Similarly, we can examine the\nextent to which LLMs’ performance on Yes/No tests depends on the questions asked and what\nthis says about the underlying mechanisms.\nAnother finding from our study is that commercial models (PaLM2, GPT-3.5, and GPT-4)\nhave obtained the best results. This aligns with benchmarks where commercial models outper-\nform open models in most evaluations. To the best of our knowledge, vocabulary tests have\nnot been evaluated on them, but it is no surprise that they perform better.\nFinally, it is possible to automate the production of large-scale vocabulary tests (e.g., [34]).\nThis makes it possible to generate large numbers of stimuli based on different criteria, such as\nfrequency in texts, length, or even how letter sequences are tokenized by LLMs. In this way,\nvocabulary tests can be developed to evaluate LLMs at scale. This is particularly interesting\nbecause, unlike human participants, LLMs are not limited in the number of items they can\nprocess. Thousands of items can be easily tested in LLMs, allowing comprehensive evaluation\nof the entire vocabulary. This also solves the problem of experimenter bias in the selection of\nstimuli presented [45–48]. The evaluation of the results can also be automated as part of the\npipeline.\nIn summary, the results obtained and the answers to all the RQs allow us to confirm that\nvocabulary tests are relevant to evaluate the performance of LLMs and complement existing\nbenchmarks by providing a more fine-grained analysis of how LLMs interpret words.\nConclusion\nThe development of vocabulary tests for the evaluation of LLMs is an interesting area of\nresearch at the intersection of cognitive science, psycholinguistics, and artificial intelligence,\nwhich can provide valuable insights into both the operation of LLMs and theories of human\ncognition. Therefore, there is a strong case for designing such vocabulary tests to complement\nexisting benchmarks for evaluating LLMs.\nAt the same time, we would like to point out that the performance of LLMs is also useful for\ncreators of vocabulary tests for human participants. The models may not replace item selection\nbased on psychometric analysis, but they can point to ambiguities in item construction. To\navoid being overly influenced by one model, our data suggest that it is beneficial to test the per-\nformance of multiple models. For the good items we tested, most models gave the expected\nresult, at least as long as the items were existing words. LLM tests can thus be used to avoid\nproblems in new tests for humans. Also note that LLM testing alerted us to the fact that two of\nthe non-words in the Spanish Yes/No test were present in some Spanish dictionaries.\nThis paper is just an initial step on the study of the use of vocabulary tests to evaluate LLMs\nand has several limitations. The first one is the number of LLMs and languages evaluated. Eval-\nuating a larger number of LLMs and languages is needed to confirm the results and conclu-\nsions obtained in this work. A second limitation is the size of the tests that have only tens of\nitems as they have been designed for humans. The creation of larger tests with thousands of\nitems by relying on automated processes would enable a more comprehensive and focused\nevaluation and is also an interesting topic for future work. Finally, studying the links between\nthe results of LLMs in vocabulary tests and cognitive science theories [49] is also an interesting\nidea to explore in future works.\nAcknowledgmen ts\nWe thank the LSA research group at Colorado University and in particular Peter Foltz for\nkindly providing the items for the TOEFL test.\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 14 / 17\nAuthor Contributions\nConceptualization: Elena Merino-Go ´ mez, Jose ´ Alberto Herna ´ ndez, Pedro Reviriego, Marc\nBrysbaert.\nData curation: Gonzalo Martı ´ nez, Javier Conde, Beatriz Bermu ´ dez-Margaretto.\nInvestigation: Pedro Reviriego.\nMethodology: Elena Merino-Go ´ mez, Beatriz Bermu ´ dez-Margaretto, Pedro Reviriego, Marc\nBrysbaert.\nResources: Javier Conde.\nSoftware: Gonzalo Martı ´ nez, Javier Conde.\nSupervision: Elena Merino-Go ´ mez, Pedro Reviriego.\nValidation: Gonzalo Martı ´ nez, Javier Conde.\nVisualization: Gonzalo Martı ´ nez, Javier Conde.\nWriting – original draft: Gonzalo Martı ´ nez, Javier Conde, Elena Merino-Go ´ mez, Jose ´ Alberto\nHerna ´ ndez, Pedro Reviriego, Marc Brysbaert.\nWriting – review & editing: Gonzalo Martı ´ nez, Javier Conde, Elena Merino-Go ´ mez, Beatriz\nBermu ´ dez-Margaretto, Jose ´ Alberto Herna ´ ndez, Pedro Reviriego, Marc Brysbaert.\nReferences\n1. Landauer T. K., & Dumais S. T. A solution to Plato’s problem : The latent semant ic analysis theory of\nacquisition , induction , and representatio n of knowledge. Psychologi cal Review, 104 (2), 211–240.\n(1997).\n2. Jarmasz, M., and Szpakow icz, S. Roget’s thesaurus and semantic similarity, In Procee dings of the\nInternational Conferen ce on Recent Advances in Natural Language Processin g, pp. 212–219 (2003).\n3. Bullinaria J.A., and Levy J.P. Extracting semantic representat ions from word co-occurre nce statistics:\nstop-lists, stemmin g, and SVD. Behavior Research Methods, 44 (3), 890–907. (2012) https:// doi.org/\n10.3758/ s13428-011- 0183-8 PMID: 22258891\n4. Wiki ACL (2019, Septemb er 15). TOEFL Synonym Questio ns (State of the art). https:// aclweb.org /\naclwiki/T OEFL_Synon ym_Questio ns_(Sta te_of_the_ art) (2019)\n5. Mikolov T., Karafia ´ t M., Burget L., Cernocky ´ J., & Khudanpur S. Recurren t neural network based lan-\nguage model. In Proceeding of Interspee ch 2, pp. 1045–1048. (2010).\n6. Mikolov T., Chen K., Corrado G., & Dean J. Efficient estimation of word repres entations in vector space.\nPreprint at https://arxiv .org/abs/130 1.3781 (2013).\n7. Mandera P., Keuleers E., & Brysbaert M. Explaining human performanc e in psycholingu istic tasks with\nmodels of semantic similarity based on prediction and counting: A review and empirical validatio n. Jour-\nnal of Memory and Langua ge, 92, 57–78. (2017).\n8. Vaswani A. et al. Attentio n is All you Need. In Proceedings of the Neural Information Processin g Sys-\ntems (2017).\n9. Devlin J., Chang M., Lee K., and Toutanova K. BERT: Pre-train ing of deep bidirectional transfor mers for\nlanguage understa nding. In Proceedings of the North American Chapter of the Associat ion for Compu-\ntational Linguisti cs. (2019)\n10. Raffel C., Shazeer N., Roberts A., Lee K., Narang S., Matena M., et al. Exploring the limits of transfer\nlearning with a unified text-to-text transforme r. Journal of Machine Learning Research, 21 (140):1–67 .\n(2020).\n11. Chang Y., Wang X., Wang J., Wu Y., Zhu K., Chen H., et al. A survey on evaluatio n of large language\nmodels. Preprint at https://arxiv .org/abs/23 07.03109 (2023).\n12. Ray P. P. ChatGPT: A comprehe nsive review on backgrou nd, application s, key challeng es, bias, ethics,\nlimitation s and future scope. Interne t of Things and Cyber-Phy sical System s, 3:121–154. (2023).\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 15 / 17\n13. Sallam M. ChatGPT utility in healthcar e education, resear ch, and practice: Systemat ic review on the\npromising perspective s and valid concerns. Healthcare , 11, 887. (2023). https:// doi.org/10.33 90/\nhealthcare 11060887 PMID: 36981544\n14. Barrot J. S. Using ChatGP T for second language writing: Pitfalls and potentia ls. Assessing Writing , 57,\n100745. (2023).\n15. Barrot J. S. Google Bard as an automated written corrective feedback tool: possibiliti es and feedback .\nTESOL Journal, e805. (2024).\n16. Zhao W. X., et al. A survey of large language models. Preprint at https://arx iv.org/abs/2 303.18223\n(2023).\n17. Hendryck s D., Burns C., Basart S., Zou A., Mazeika M., Song D., et al. Measurin g massive multitask\nlanguage understa nding. Preprint at https://arxiv .org/abs/200 9.03300 (2020).\n18. Srivastava A., Rastogi A., Rao A., Shoeb A. A. M., Abid A., Fisch A., et al. Beyond the imitation game:\nQuantify ing and extrapolating the capabili ties of langua ge models. Preprint at https://arxiv.or g/abs/\n2206.04615 (2022).\n19. Zellers R., Holtzman A., Bisk Y., Farhadi A., & Choi Y. Hellaswag: Can a machine really finish your sen-\ntence? Preprint at https://arxiv.or g/abs/190 5.07830 (2019).\n20. Hendryck s D., Burns C., Kadavath S., Arora A., Basart S., Tang E., et al. Measurin g mathematic al prob-\nlem solving with the math dataset. Preprint at https://arxiv.or g/abs/210 3.03874 (2021).\n21. Vaswani A., et al \"Attentio n is all you need.\" Advances in Neural Informatio n Processin g Systems\n(2017).\n22. Martı ´ nez G., Watson L., Reviriego P., Herna ´ ndez J. A., Juarez M., & Sarkar R. Towards Understand ing\nthe Interplay of Generative Artificial Intellig ence and the Interne t. In International Workshop on Episte-\nmic Uncertainty in Artificial Intellig ence (pp. 59–73). Cham: Springer Nature Switzerlan d, 2023.\n23. Muñoz-Or tiz A., Go ´ mez-Rodrı ´ guez C., & Vilares D. Contrast ing linguistic pattern s in human and LLM-\ngenerated text. Preprint at https://arxiv .org/abs/23 08.09067 (2023).\n24. Toro J. M. Emerge nce of a phonologic al bias in ChatGPT . Preprint at https://arxiv. org/abs/2305. 15929\n(2023).\n25. Reviriego P., Conde J., Merino- Go ´ mez E., Martı ´ nez G., and Herna ´ ndez J. A. Playing with words: Com-\nparing the vocabular y and lexical richness of ChatGPT and humans. Preprint at https://arxiv .org/abs/\n2308.07462 (2023).\n26. Kobak D., Gonza ´ lez Ma ´ rquez R., Horva ´ t E. A., & Lause J. Delving into ChatGPT usage in academic\nwriting throug h excess vocabular y. arXiv e-prints , arXiv-2406 . (2024).\n27. Webb S., & Nation P. How vocabulary is learned. Oxford University Press. (2017).\n28. Vermeiren H., & Brysbaert M. How useful are native language tests for research with advanced second\nlanguage users? Bilingualism: Language and Cognition 27 (1):204–21 3. (2024).\n29. Meara P.M., & Buxton B. An alternative to multiple choice vocabulary tests. Language Testing, 4, 142–\n154. (1987)\n30. Lemho ¨ fer K., & Broersma M. Introducing LexTALE : A quick and valid lexical test for advanced learners\nof English. Behavior Research Methods, 44, 325–34 3. (2012). https://doi.or g/10.375 8/s13428-011-\n0146-0 PMID: 218981 59\n31. Petrov A., La Malfa E., Torr P. H., & Bibi A. Language Model Tokenizer s Introduce Unfairness Between\nLanguages . Preprint https://arxiv.or g/abs/230 5.15425 (2023).\n32. Stadthagen -Gonzalez H., Bowers J. S., & Damian M. F. Age-of-acqu isition effects in visual word recog-\nnition: Evidence from expert vocabular ies. Cognition , 93 (1), B11–B26. (2004). https://doi.or g/10.101 6/\nj.cognitio n.2003.10.009 PMID: 15110727\n33. Gatti D., Marelli M., & Rinaldi L. Out-of-vo cabulary but not meaningles s: Evidence for semantic-pr iming\neffects in pseudowor d processing . Journal of Experimen tal Psychology 152 (3), 851–863. (2023).\nhttps://doi.or g/10.103 7/xge000 1304 PMID: 36174173\n34. Keuleers E., & Brysbaer t M. Wuggy: A multiling ual pseudowor d generator. Behavior research methods,\n42, 627–633. (2010). https://doi.or g/10.3758/BR M.42.3.627 PMID: 208055 84\n35. van Rijn P., Sun Y., Lee H., Marjieh R., Sucholu tsky I., Lanzari ni F., et al. Around the world in 60 words:\nA generative vocabular y test for online research . Preprint at https://arxiv.o rg/abs/2302.01 614 (2023).\n36. Martı ´ nez G. et al. How many words does ChatGPT know? The answer is ChatWords. Preprint at\nhttps://arxiv .org/abs/2 309.16777 (2023).\n37. Touvron H., Martin L., Stone K., Albert P., Almahairi A., Babaei Y., et al. Llama 2: Open foundation and\nfine-tuned chat models. Preprint at https://arxiv. org/abs/2307. 09288 (2023).\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 16 / 17\n38. Jiang A. Q., Sablayr olles A., Mensch A., Bamford C., Chaplot D. S., et al. Mistral 7B. Preprint at https://\narxiv.org /abs/2310 .06825 (2023).\n39. Rohan A., et al. \"Palm 2 technical report.\" Preprint at https://arxiv .org/abs/2 305.10403 (2023).\n40. Brown T. B., et al. \"Language models are few-shot learners. \" Preprint at https://a rxiv.org/pdf/ 2005.\n14165 (2020).\n41. Achiam Josh, et al. \"Gpt-4 technical report.\" Preprint at https://a rxiv.org/abs /2303.08774 (2023).\n42. Liu Boan, et al. \"Diversify ing the mixture-o f-experts repres entation for language models with orthogonal\noptimizer.\" Preprint at https://arxiv. org/pdf/2310.0 9762 (2023).\n43. Vermeiren H., Vandendael e A., & Brysbaer t M. Validate d tests for language research with university\nstudents whose native language is English: Tests of vocabulary , general knowledge, author recognit ion,\nand reading comprehe nsion. Behavio r Research Methods, 55 (3), 1036–1068. (2023). https:// doi.org/\n10.3758/ s13428-022- 01856-x PMID: 35578105\n44. Chowdhery A., Narang S., Devlin J., Bosma M., Mishra G., Roberts A., et al. Palm: Scaling langua ge\nmodelin g with pathway s. Journal of Machine Learning Researc h, 24 (240), 1–113. (2023).\n45. Bermu ´ dez-Marg aretto B., & Brysbaer t M. How efficient is translation in language testing? Deriving valid\nSpanish tests from establishe d English tests. https://d oi.org/10.312 34/osf.io/yp u9w (2022).\n46. Izura C., Cuetos F., & Brysbaert M. Lextale-Es p: A test to rapidly and efficiently assess the Spanish\nvocabular y size. Psicolo ´ gica, 35 (1), 49–66. (2014).\n47. Ferre ´ P., & Brysbaert M. Can Lextale-Es p discrimina te between groups of highly proficie nt Catalan-\nSpanish bilinguals with differe nt language dominan ces? Behavio r Research Methods, 49, 717–72 3.\n(2017). https://doi.or g/10.375 8/s13428-016- 0728-y PMID: 27004486\n48. Forster K. I. The potential for experime nter bias effects in word recognit ion experiment s. Memory &\nCognition , 28 (7), 1109–1115. (2000). https://doi. org/10.3758/b f032118 12 PMID: 11185767\n49. Kuperman V. Virtual experime nts in megast udies: A case study of language and emotion. Quarterly\nJournal of Experi mental Psychology , 68 (8), 1693–1710. (2015). https://doi.or g/10.108 0/17470218.\n2014.98986 5 PMID: 25406972\nPLOS ONE\nEstablishing vocabular y tests as a benchmar k for evaluatin g large language models\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03082 59 December 12, 2024 17 / 17",
  "topic": "Vocabulary",
  "concepts": [
    {
      "name": "Vocabulary",
      "score": 0.8381476402282715
    },
    {
      "name": "Computer science",
      "score": 0.6574946045875549
    },
    {
      "name": "Cornerstone",
      "score": 0.6443518400192261
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4928765296936035
    },
    {
      "name": "Natural language processing",
      "score": 0.47936469316482544
    },
    {
      "name": "Test (biology)",
      "score": 0.4718708395957947
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43096688389778137
    },
    {
      "name": "Domain-specific language",
      "score": 0.4215957820415497
    },
    {
      "name": "Linguistics",
      "score": 0.3418697714805603
    },
    {
      "name": "Biology",
      "score": 0.10106509923934937
    },
    {
      "name": "Programming language",
      "score": 0.06832808256149292
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}