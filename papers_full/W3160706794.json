{
  "title": "Predicting enzymatic reactions with a molecular transformer",
  "url": "https://openalex.org/W3160706794",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2678847168",
      "name": "David Kreutter",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A4223443570",
      "name": "Jean-Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2678847168",
      "name": "David Kreutter",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A4223443570",
      "name": "Jean-Louis Reymond",
      "affiliations": [
        "University of Bern"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2751743037",
    "https://openalex.org/W2994728060",
    "https://openalex.org/W2767044445",
    "https://openalex.org/W2509639622",
    "https://openalex.org/W2621742623",
    "https://openalex.org/W2606363443",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W2799620402",
    "https://openalex.org/W2998659621",
    "https://openalex.org/W3040975947",
    "https://openalex.org/W3088265803",
    "https://openalex.org/W4249735123",
    "https://openalex.org/W2802005696",
    "https://openalex.org/W2924412896",
    "https://openalex.org/W3088602465",
    "https://openalex.org/W3120686965",
    "https://openalex.org/W2769423117",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2987091515",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W2481382694",
    "https://openalex.org/W2139138352",
    "https://openalex.org/W2128312954",
    "https://openalex.org/W2134441341",
    "https://openalex.org/W3095926507",
    "https://openalex.org/W2609516818",
    "https://openalex.org/W3105895642",
    "https://openalex.org/W3008588639",
    "https://openalex.org/W3123901912",
    "https://openalex.org/W2950955377",
    "https://openalex.org/W2905012389",
    "https://openalex.org/W3145257564",
    "https://openalex.org/W2941191803",
    "https://openalex.org/W2760832210",
    "https://openalex.org/W2061875821",
    "https://openalex.org/W2047346406",
    "https://openalex.org/W2982686216",
    "https://openalex.org/W2164336172",
    "https://openalex.org/W2414862901",
    "https://openalex.org/W1998326599",
    "https://openalex.org/W2199708805",
    "https://openalex.org/W2555782591",
    "https://openalex.org/W2749733387",
    "https://openalex.org/W2328819222",
    "https://openalex.org/W2492411637",
    "https://openalex.org/W2593519657",
    "https://openalex.org/W2057340712",
    "https://openalex.org/W2886801083",
    "https://openalex.org/W2151930622",
    "https://openalex.org/W2897535979",
    "https://openalex.org/W2767227490",
    "https://openalex.org/W2951422523",
    "https://openalex.org/W3211129705",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W3035847987",
    "https://openalex.org/W2899575547",
    "https://openalex.org/W2085126686",
    "https://openalex.org/W3103092523",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3102693939",
    "https://openalex.org/W2604561121",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "The enzymatic transformer was trained with a combination of patent reactions and biotransformations and predicts the structure and stereochemistry of enzyme-catalyzed reaction products with remarkable accuracy.",
  "full_text": "  Chemical\n  Science\nrsc.li/chemical-science\nVolume 12\nNumber 25\n7 July 2021\nPages 8573–8932\nISSN 2041-6539\nEDGE ARTICLE\nJean-Louis Reymond et al.\nPredicting enzymatic reactions with a molecular transformer\nPredicting enzymatic reactions with a molecular\ntransformer†\nDavid Kreutter, a Philippe Schwaller ab and Jean-Louis Reymond *a\nThe use of enzymes for organic synthesis allows for simpliﬁed, more economical and selective synthetic\nroutes not accessible to conventional reagents. However, predicting whether a particular molecule\nmight undergo a speciﬁc enzyme transformation is very diﬃcult. Here we used multi-task transfer\nlearning to train the molecular transformer, a sequence-to-sequence machine learning model, with one\nmillion reactions from the US Patent O ﬃce (USPTO) database combined with 32 181 enzymatic\ntransformations annotated with a text description of the enzyme. The resulting enzymatic transformer\nmodel predicts the structure and stereochemistry of enzyme-catalyzed reaction products with\nremarkable accuracy. One of the key novelties is that we combined the reaction SMILES language of\nonly 405 atomic tokens with thousands of human language tokens describing the enzymes, such that\nour enzymatic transformer not only learned to interpret SMILES, but also the natural language as used by\nhuman experts to describe enzymes and their mutations.\nIntroduction\nThe use of enzymes for organic synthesis, commonly referred to\nas the eld of biocatalysis, greatly contributes to organic\nsynthesis methodology by providing the possibility to carry out\nhighly chemo-, regio-, stereo- and enantio-selective trans-\nformations under mild and environmentally friendly condi-\ntions, o en allowing the redesign and simpli cation of\nsynthetic routes by enabling reactions that are not possible with\nconventional chemical reagents.\n1,2 The advent of directed\nenzyme evolution as a tool to increase enzyme performance has\nalso greatly contributed to improve the range and eﬃciency of\nenzyme catalyzed reactions for organic synthesis.\n3 However, the\nimplementation of biocatalytic steps in synthetic processes\nremains challenging because it is very di ﬃcult to predict\nwhether a particular substrate might actually be converted by an\nenzyme to the desired product.\nComputer-assisted synthetic planning (CASP) comprises\na range of articial intelligence approaches to predict reaction\nproducts from reactant or reagents, orvice versa, and to plan\nretrosynthesis.\n4–12 Here we asked the question whether CASP\nmight be exploited to predict the outcome of enzymatic reac-\ntions for organic synthesis. Recent eﬀorts in predicting enzy-\nmatic reactions focused on metabolic reactions from the KEGG\nenzymatic reaction database and predictions of drug\nFig. 1 General concept of the enzymatic transformer training. The\nUSPTO data set contains reactions SMILES describing reactants,\nreagents and products. The ENZR data set contains reaction SMILES as\nwell as an additional text component.\naDepartment of Chemistry, Biochemistry and Pharmaceutical Sciences, University of\nBern, Freiestrasse 3, 3012 Bern, Switzerland. E-mail: jean-louis.reymond@dcb.\nunibe.ch\nbIBM Research Europe, S¨aumerstrasse 4, 8803 R¨uschlikon, Switzerland\n† Electronic supplementary information (ESI) available. See DOI:\n10.1039/d1sc02362d\nCite this:Chem. Sci.,2 0 2 1 ,12, 8648\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 28th April 2021\nAccepted 24th May 2021\nDOI: 10.1039/d1sc02362d\nrsc.li/chemical-science\n8648 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nmetabolism,13–15 as well as retrosynthetic planning with enzy-\nmatic reactions using a template based approach.16 Here we\nconsidered the molecular transformer,17–19 which is a sequence-\nto-sequence prediction model operating on text representations\nof reactions as reaction SMILES (Simplied Molecular Input\nLine Entry System)\n20 including stereochemistry. We set out to\nuse multi-task transfer learning combining the USPTO dataset21\nas a source of general chemistry knowledge with a few thousand\nenzymatic reactions collected from the scientic literature as\na source of specialized knowledge (Fig. 1).\nWe used transfer learning previously to enable the molecular\ntransformer to predict complex regio- and stereo-selective\nreactions at the example of carbohydrates.\n22 In this former\nstudy transfer learning was performed on a dataset of reactions\ndescribed as SMILES, which are based on a vocabulary of only\na few hundred atomic tokens identical to the vocabulary\ndescribing the general USPTO dataset used for primary training.\nOne of the novelties of the present work on enzyme reactions is\nthat we combine SMILES language for the substrates with\nhuman language for the enzyme descriptions. Those more\ndiverse inputs result in an increase from 405 atomic tokens for\nSMILES only to a few thousand atomic and language tokens\nwhen describing enzyme reactions, implying that our trans-\nformer model had to learn to interpret not only the SMILES\nlanguage but also natural language, as used by human experts\nto describe enzymes and their mutations.\nResult and discussion\nReaction datasets\nAs a general chemistry dataset, we used the previously reported\n“USPTO stereo augmented” dataset derived from the patent\nmining work of Lowe, which contains, for each of the one\nmillion reactions in the USPTO dataset, the original reaction\nSMILES and a randomized SMILES version, both conserving\nstereochemical information.\n23,24 To compose a specialized\ndataset of enzymatic reactions, we extracted 70 096 reactions\nlabeled as “enzymatic reactions” from the Reaxys database.25\nWe collected the data columns corresponding to reactant\nSMILES, product SMILES, and enzyme description (“reaction”,\n“reagent” and “catalyst”). Canonicalizing all SMILES and\nremoving reactions lacking either reactants or products as well\nas duplicate entries (identical reactants, products and enzyme\ndescription) le  32 181 unique enzymatic reactions, each\nannotated with an enzyme description, referred to here as the\nENZR dataset.\nAlthough Reaxys does not cover the full spectrum of scien-\ntic literature about enzymes, the ENZR dataset contains\na broad range of enzymes covering diverse reaction types,\nincluding not only highly specic enzymes such as glucose\noxidases and dehydrogenases used in glucose monitoring\ndevices,\n26 but also enzymes with a documented broad substrate\nscope for organic synthesis including mechanistically promis-\ncuous enzymes,27 such as lipases used to promote aldol and\nMichael addition reactions, 28 or ene-reductases capable of\nreducing oximes,29 thus providing a broad basis for training our\nmodel about the scope and specicity of diﬀerent enzymes. We\ndid not consider the enzyme databases KEGG30 or BRENDA31\nbecause their data format is not homogeneous and many of the\nlisted reactions are template-based and not assigned to docu-\nmented examples.\nTo better understand our ENZR dataset, we analyzed enzyme\nreactions in terms of the frequency of occurrence of words with\nthe suﬃx “-ase”, which are the enzyme names, in the enzyme\ndescription. Across all enzyme reactions, 81.9% (26 348) con-\ntained a single“-ase” word, and 98.4% (31 663) contained one,\ntwo, or three“-ase” words (Fig. 2a). The largest group of single\n“-ase” word reactions involved a lipase (17%), a type of enzyme\nwhich is almost exclusively used alone. By contrast, dehydro-\ngenases and reductases were most frequent in reactions\ninvolving two or more “-ase” words, re ecting that such\nenzymes are oen used in processes involving enzyme-coupled\ncofactor regeneration systems. The ten most frequent “-ase”\nwords corresponded to well-known enzyme families and\ntogether covered 50.3% of all enzyme reactions (the 15 most\nfrequent “-ase” words covered 57.0% of all reactions, Fig. 2b). A\nner analysis of enzyme families considering the complete\nenzyme description, which typically includes the enzyme source\nand the substrate type, showed that each enzyme family\ncomprised a number of diﬀerent enzymes (Fig. S1†).\nTo visualize our ENZR dataset, we used our recently reported\nTMAP (tree-map) algorithm, a powerful tool to represent very\nlarge high-dimensional datasets containing up to millions of\ndatapoints as connected trees in two dimensions.\n32 In a rst\nTMAP, we connected enzymatic reactions, each represented as\na point, according to their similarity measured by the reaction\nngerprint RXNFP, a recently reported reaction ngerprint\nderived from a neural network trained to classify patent\nchemical reactions.\n33 This analysis considered the trans-\nformation of substrates into product molecules but not the\nenzyme description in each ENZR entry. Color-coding the TMAP\nby the 10 most frequent“-ase” words mentioned above, corre-\nsponding to the most abundant enzyme families in the ENZR\ndataset, showed that these enzyme families formed relatively\nwell separated clusters of reactions, illustrating that, similarly\nto organic reagents, enzymes carry out well-dened functional\ngroup transformations (Fig. 2c).\nIn a second color-coded version of the TMAP we labeled all\nenantioselective and kinetic resolution reactions, identied as\nreactions SMILES with no “@” characters in the reactants,\nindicating either the absence of chiral centers or an undened\nstereochemistry at chiral centers, but the presence of at least\none “@” character in the products SMILES, indicating a specic\nabsolute conguration for chiral centers.\n34 This color-code\nshowed that enantioselective and kinetic resolution reactions\nalso formed dened clusters corresponding to biotransforma-\ntions with mostly dehydrogenases, lipases and reductases\n(Fig. 2c, inset lower right).\nThe diﬀerent enzymes also formed identiable clusters in\nad iﬀerent TMAP grouping reactions by substructure similarity\nof the reacting substrates using the extended connectivity\nngerprint MHFP6 (Fig. S2†).\n35 This illustrated that enzymatic\nreactions in the ENZR dataset followed the well-known trend\nthat enzymes only react with certain types of substrates, in\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8649\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ncontrast to chemical reagents which are usually only specic for\nfunctional groups. The range of substrates utilized by the\nenzymes covered a broad range of sizes from very small mole-\ncules such as pyruvate up to relatively large peptides (Fig. S2,†\ninset).\nTaken together, the analysis above indicated that the ENZR\ndataset contained a diverse set of enzymatic reactions, with the\nexpected biases towards the most frequently used enzymes in\nthe eld of biocatalysis such as lipases and dehydrogenases.\nTraining and evaluation of transformer models for enzymatic\nreactions\nTraining a transformer model rst requires tokenizing the\ninput and output character strings to allow the model to learn\nwhich series of input tokens produces which series of output\ntokens. For the reaction SMILES in both USPTO and ENZR\ndatasets, we used the approach reported previously for the\ngeneral molecular transformer, which considers each character\nFig. 2 Analysis of the ENZR dataset. (a) Number of reactions depending on how many“-ase” words are present in the sentence. (b) Frequency of\nthe top 15“-ase” words depending on the count of enzyme name per reaction. (c) TMAP of reactions similarity color-coded by the 10 most\nfrequent “-ase” words as listed in (b) combinations. The“other” category groups reactions with“-ase” words other than the top 10“-ase” words as\nwell as reactions containing more than one“-ase” word. Inset lower right: TMAP highlighting enantioselective and kinetic resolution reactions.\n8650 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nFig. 3 (A) Top prediction accuracy and invalid SMILES on the enzyme reaction test set for various models. (a) USPTO model from Schwalleret al.trained\nwithout any enzymatic transfer learning and tested without enzyme sentence. (b) Enzymatic DB without USPTO dataset. (c) USPTO model transfer learned\n(sequential) to enzymatic DB trained without any enzyme description part.(d) USPTO model transfer learned (multi-task) to enzymatic DB trained without\nany enzyme description part. (e) Enzymatic DB without USPTO data set trained with‘-ase’ words only. (f) USPTO model transfer learned (sequential) to\nenzymatic DB trained with‘-ase’words only. (g) USPTO model transfer learned(multi-task) to enzymatic DB trained with‘-ase’words only. (h) Enzymatic\nDB without USPTO data set trained with enzyme full sentences. (i) USPTO model transfer learned (sequential) to enzymatic DB trained with enzyme full\nsentences. (j) USPTO model transfer learned (multi-task) to enzymatic DB trained with enzyme full sentences. (k) Best multi-task model tested by swapping\nenzyme full sentences between reactions of the test set. (B) Accuracy on the test set depending on how many“-ase” words are present in the sentence. (C)\nAccuracy on the test set depending on how frequent the“-ase” words combination from the sentences appears in the training set. (D) True predictions rate\nagainst conﬁdence scores, bins were adjusted to obtain an equal distribution of predictions over the bins. Vertical red bars represent our limits to indicate\ntrue or false predictions. (E) Top prediction accuracy and invalid SMILES on lipase reactions of the test set only. (F) Top prediction accuracy and invalid\nSMILES on enantiomeric resolution reactions of the test set only.\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8651\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nof the reaction SMILES as a separate token except Cl, Br, and\ncharacter strings in square brackets, which denote special\nelements.18 The set of tokens necessary for describing reaction\nSMILES in the USPTO amounted to 405 so-called atomic tokens,\nand did not increase for describing the reaction SMILES portion\nof our ENZR dataset, which we rst canonicalized using\nRDKit.\n36 To incorporate the enzyme information into our\nmodel, we tokenized the sentences describing the enzymes in\nthe ENZR dataset using the Hugging Face Tokenizers library,\n37\nwhich aer preprocessing resulted in a vocabulary of 3004\natomic and language tokens to describe the ENZR dataset.\nIn view of evaluating transformer models, we split the\nUSPTO stereo augmented dataset randomly into a training set\n(900 000 reactions, 90%, 1.8 million reactions aer adding for\neach canonical training reaction a duplicate using non-\ncanonical precursor SMILES), a validation and a test set (each\n50 000 reactions, 5%).\n24 For the ENZR dataset, werst grouped\nreactions having the same product in diﬀerent groups, and then\nsplit these groups into a training set (25 700 reactions, 80%),\na validation and a test set (each 3200 reactions, 10%). Distrib-\nuting these reaction groups rather than individual reactions\ninto the diﬀerent sets ensured that products which must be\npredicted in the validation or test sets have not been seen by the\ntransformer during training or validation sets, respectively.\nWe then trained various models using OpenNMT\n38 and\nPyTorch,39 and evaluated them by presenting them with\nsubstrate SMILES, optionally together with the partial or full\ndescription of the enzyme, for each of the 3200 reactions in the\ntest set. In each case, the model was challenged to write out the\nSMILES of the reaction product, including the correct stereo-\nchemistry, none of which had been seen by the model in the\ntraining or validation set. We analyzed whether the correct\nproduct was written out within therst one orrst two solu-\ntions proposed by the model, as well as the percentage of invalid\nproduct SMILES, detected using RDKit, appearing among the\nrst one or two solutions (top 1 and top 2 accuracy, blue and\ncyan bars, top 1 and top 2 invalid SMILES, red and orange bars,\nFig. 3A).\nWe rst evaluated if transformer models could be trained to\npredict reaction products from only the substrate by omitting\nany enzyme information during training. The UPSTO only\nmodel showed approximately 10% accuracy but a very low\nFig. 4 Examples of substrates applied to various enzymes using the MTL transformer with full sentences, which illustrate predictions of reactions\nfrom the test set not seen by the model during training. The color code indicates high conﬁdence predictions (score > 98%, black), uncertain\npredictions (score 80– 98%, blue), and low conﬁdence predictions (score < 80%), see Fig. 3D for discussion of conﬁdence scores. All enzymatic\nreactions are predicted correctly, however the conﬁdence score varies. The predictions of the MTL no text model are shown to illustrate what the\ntransformer predicts when the enzyme information is missing.\n8652 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\npercentage of invalid SMILES, indicating that this model\nunderstood chemistry but lacked expertise in biotransforma-\ntions (Fig. 3A, entry (a)). The ENZR only model also performed\npoorly (/C2420% accuracy) and produced/C2410% invalid SMILES,\nreecting that general chemistry training was insuﬃcient with\nthis relatively small dataset (Fig. 3A, entry (b)). Nevertheless,\ntraining with both models using sequential transfer learning\n(STL) or multi-task transfer learning (MTL) reached /C2450%\naccuracy, indicating that substrate structure was partially\npredictive of the outcome of enzymatic reactions even in the\nabsence of any enzyme information (Fig. 3A, entries (c) and (d)).\nThis partial prediction based on only the substrate reects the\nfact that certain types of substrate molecules are only docu-\nmented to react with specic enzymes in the ENZR dataset. For\nexample, many alcohols are only documented to react with\nalcohol dehydrogenases to produce the corresponding ketone,\nsuch that a transformer model trained with the reaction SMILES\nlearns to predict the ketone as the most likely product even\nwithout enzyme information, a prediction which is most of the\ntime the correct one.\nAdding enzyme information in form of“-ase” words alone\ndid not signicantly increase prediction performance when\nusing only ENZR, however combining the data with the USPTO\nby transfer learning increased in terms of top 1 accuracy to\n51.7% with STL and 54.0% with MTL (Fig. 3A, entries (e)–(g)).\nTop 1 prediction accuracy increased further up to 59.5% with\nSTL and 62.2% with MTL when using the complete enzyme\ninformation as full sentence (Fig. 3A, entry (j)). Note that the\nmodel trained with ENZR alone only reached 34.3% top 1\naccuracy with full enzyme names and produced/C2410% invalid\nSMILES, showing that the general chemistry training learned\nfrom USPTO was essential even with full enzyme information\n(Fig. 3A, entry (h)). Furthermore, testing the MTL with a test set\nin which the enzyme information was scrambled between\nreactions resulted in poor results (/C2415% accuracy), indicating\nthat the true enzyme information was required rather than the\npresence of random text information (Fig. 3A, entry (k)).\nExamples of the added value of enzyme information for pre-\ndicting the outcome of an enzyme reaction are provided with\nthe cases of linoleic acid conversion with various oxygenases\nand dehydrogenases, and the conversion of\nL-tyrosine by a lyase\nand a tyrosinase. These examples are taken from the test set and\nreect true predictions since they have not been seen by the\nmodel during training or validation (Fig. 4).\nAnalyzing the prediction performance of the enzymatic\ntransformer\nThe comparisons above showed that an excellent prediction\nperformance was reached by the transformer trained using MTL\ncombining the USPTO and the ENZR dataset using full enzyme\nnames as enzyme information. Retraining this model with\ndiﬀerent splits of training, validation and test sets gave indis-\ntinguishable results in terms of prediction accuracy. This model\nwas selected for further investigation and is referred to as the\n“enzymatic transformer”.\nConsidering that many reactions in the ENZR dataset\ncontain multiple enzymes, we wondered if our transformer\nmight be confused in such situations because the main enzyme\nand the cofactor regeneration enzyme are not labeled as such.\nIndeed, the prediction accuracy of the enzymatic transformer\nwas lower for reactions with multiple enzymes compared to\nreactions with a single enzyme (Fig. 3B). However, in many\ncases of multi-enzyme reactions including cofactor regenera-\ntion, the transformer provided the correct prediction when\nomitting the cofactor regenerating enzyme or swapping it for an\nequivalent one (glucose dehydrogenase to phosphite dehydro-\ngenase, Fig. S3†).\nSince transformer models require a large number of exam-\nples for good performance, we also tested prediction accuracy as\nfunction of the number of occurrences of the enzyme name in\nthe training set. Indeed, a prediction accuracy of almost 80%\nwas reached for lipases, which were the most abundant in the\ntraining set (Fig. 3C). Nevertheless, prediction accuracy reached\na good level (/C2460%) as soon as more thanve examples of\na particular enzyme were present in the training set.\nIn the best transformer model using MTL on full sentences,\nthere was a clear association of the prediction condence score\nwith accuracy, as observed with other transformer models\n(Fig. 3D).\n22 Overall, 85.5% of the predictions with condence\nscore > 98% were true and 75.6% of the predictions with\ncondence score < 80% were false, suggesting to use condence\nscore values > 98% or <80% as indicators for a true (the reaction\nis worth testing) or false (the reaction outcome is uncertain)\nprediction.\nSince the subset of the test set containing the word“lipase”\nperformed best (Fig. 3C), we evaluated this subset exhaustively\nwith all models (Fig. 3E). While models trained on the USPTO or\nENZR dataset without enzyme information performed poorly\n(Fig. 3E, entries (a) and (b)), combining both sets with STL\n(entry (c)) or MTL (entry (d)) reached an excellent accuracy\n(>70%), indicating that the presence of an ester functional\ngroup is su ﬃcient for the model to recognize a lipase\nbiotransformation even in the absence of the enzyme name.\nHowever, models trained with ENZR alone using only the“ase”\nword or the full sentence performed poorly (Fig. 3E, entries (e)\nand (h)), showing that this relatively small dataset contained\ninsuﬃcient general chemistry knowledge to training even for\nthe relatively simple lipase reaction. Overall, the model trained\non both datasets using STL and the full enzyme description\nperformed best for lipases, as observed in the entire dataset\n(Fig. 3E, entry (j)). However, scrambling the enzyme information\nbetween diﬀerent reactions in the lipase only test set did not\ndecrease prediction accuracy as dramatically as for the full set,\nreecting the fact that all lipases catalyze very similar reactions.\nIn addition, 36.89% of the lipase test set cases were reactions\nwith Candida antarctica lipase B, the most frequently used\nlipase in biotranformations, in which case swapping the\nenzyme information does not induce any change.\nEnzymatic reactions are oen used to perform kinetic reso-\nlutions, typically using hydrolase enzymes such as lipases, or to\ntransform achiral substrates into chiral products, typically to\nproduce chiral alcohols or amines from achiral ketone\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8653\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nprecursors. To evaluate the performance of the transformer on\nsuch reactions, we dened enantiomeric resolutions as enzy-\nmatic reactions containing chiral centers, identied by the\npresence of at least one“@” character in the SMILES, in the\nreaction products only, which corresponded to 6495 reactions\nin the entire ENZR dataset (20.18%), and 687 reactions in the\ntest set (21.35%). The relative performance of the diﬀerent\ntransformer models in this subset was comparable to that of the\nentire dataset, indicating that the transformer model was able\nto learn the enantiomeric preference of enantioselective\nenzymes as successfully as the overall enzymatic transformation\n(Fig. 3E).\nExamples of correct and incorrect predictions by the\nenzymatic transformer\nThe types of enzymatic reactions predicted correctly by the\nenzymatic transformer are well illustrated by selected cases\nFig. 5 Examples of successful predictions by the enzymatic transformer.\n8654 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n(Fig. 5). These include the correct product prediction including\nchirality for kinetic resolutions using lipases (reactions (1)40 and\n(2)),41 two enantioselective reductions of ketones using alcohol\ndehydrogenases (reaction (3)42 and (4)),43 an enantioselective\nimine reduction (reaction (5))44 and reductive amination with\na transaminase (reaction (6)).45\nConsidering that none of the products of these reactions\nhave been seen by the model during training, the ability of the\nenzymatic transformer to predict not only the correct reaction\nproduct but also the correct stereochemical outcome of the\nenantiomeric resolution reactions is remarkable. It must be\npointed out that the prediction is always done by analogy to\nexamples, including cases of engineered enzymes. For instance,\nin reaction (1) with a mutant CALB enzyme, the transformer has\nlearned from the training set that this triple mutant has an\naltered stereospecicity, and listing the mutation is suﬃcient\nfor the model to make the correct prediction in the example\nfrom the test set. The product structure prediction is still correct\nbut the stereoselectivity is lost when using simply “Candida\nantarctica lipase B” as enzyme description, which corresponds\nto the experimental result (Fig. S4†).\nCytochrome P450 mediated regioselective demethylation\n(reaction (7))\n46 or hydroxylations (reactions (8) 47 and (9))48\nfurther illustrate the predictive power of the enzymatic trans-\nformer. From the 405 cytochrome P450 mediated reactions in\nENZR, 316 were used in the training set and 46 in the validation\nset. The resulting enzymatic transformer correctly predicted the\nproduct structure of 17 (40%) of the 43 cytochrome P450 reac-\ntions in the test set considering the top 1 predictions and 22\n(51%) considering the top 2 predictions. The numbers\nincreased to 21 (49%) correct predictions for the top 1 and 25\n(58%) for the top 2 predictions when ignoring stereochemistry.\nThese prediction accuracies are far from perfect but still very\nremarkable considering that the reaction site and type of cyto-\nchrome P450 reactions transformation are diﬃcult to predict\nfor a chemist (Fig. S5 and S6†).\nIn the above examples, a shorter description of the enzyme\noen reduces the condence score and may induce errors in the\npredicted stereochemistry or product structure (red labels in\nFig. 5 and S4†). Such errors when using short enzyme names are\nnot surprising considering that models trained with only“-ase”\nwords performed worse than models trained with the full\nenzyme description (Fig. 3A).\nAnalyzing unsuccessful predictions by the enzymatic trans-\nformer in a random sample of 200 reactions from the test set\nselected to cover various reaction types and enzymes provides\nfurther insights (Fig. 6). Inaccurate predictions may sometimes\nsimply reect errors in database entries. For instance, the\nenzymatic transformer correctly predicts, with a high con-\ndence score, the formation of thymine from the hydrolysis of\na thymidine nucleoside analog by uridine phosphorylase,\nhowever the database entry wrongly recorded the isomeric 6-\nmethyl-uracil as the product (reaction (10)).\n49 The model also\ncorrectly predicts with high con dence score the alcohol\nhydrolysis product in the hydrolysis of ab-hydroxysulfone by\nporcine liver esterase. However, this product is unstable and\nspontaneously eliminates to form a styrene, which is the\nproduct isolated and recorded in the database (reaction (11)).\n50\nFurthermore, the model correctly predicts that 5-deoxy- b-D-\nribofuranose is the product formed by the action of a nucleosi-\ndase on the parent adenosine nucleoside, which it writes down\nin the cyclic hemi-acetal form, while the database entry recor-\nded the open-chain aldehyde form (reaction (12)).\n51\nOther examples reect true limitations of our model, for\nexample errors in the regioselectivity of hydroxylation of 7-\nmethoxy-3,4-dihydronaphthalen-1(2H)-one (reaction (13))\n52 and\na-naphthol (reaction (17))53 by cytochrome P450. In the case of\nthe formation of (+)-d-cadinene from geranyl pyrophosphate by\n(+) cadinene synthase, our model predicts the correct product\nstructure and stereochemistry, however the deuterium label,\nwhich is lost during cyclization, is wrongly incorporated into the\npredicted product (reaction (14)).\n54 The model may also predict\nthe correct product structure but the opposite enantiomer, as\nillustrated for the benzylic hydroxylation of ethylbenzene by\ncytochrome P450 (reaction (15)),\n55 or with missing stereo-\nchemistry, as illustrated for the biotransformation of 4-methyl-\ncyclohexanol by a sequence of an alcohol dehydrogenase and\na cyclohexanone monooxygenase to produce an enantiomeri-\ncally pure lactone (reaction (16)).\n56\nNote that the enzymatic transformer can only predict the\nstructure of reaction products based on what it has learned\nfrom examples in the ENZR source database. For example, the\nreaction rates of 49 diﬀerent alcohol substrates with a wild-type\ncholine oxidase (WT) and an engineered version with an\nexpanded substrate scope (M) have been reported with a broad\nrange of values.\n57 However, the Reaxys entry used for ENZR\nattributed each reaction only to one of the two enzymes, which\nwas in each case the faster reacting enzyme, even if the rates\nwere almost equal. The enzymatic transformer was trained with\na random subset of 32 reactions attributed to M andve reac-\ntions attributed to WT (Fig. S7†) and validated withve M and\ntwo WT cases (Fig. S8†). The model then correctly predicts the\ntwo WT and three M reactions in the test set, however in each\ncase the same product is predicted with very high condence for\nboth WT and M enzymes (Fig. S9†). This prediction is correct for\nthe two WT cases where the reported rates are almost equal for\nWT and M, but inaccurate for the three M cases where the\nactivity of WT is much lower, including one case where even\nthe M rate is impractically low, reecting the fact that the\ntraining data does not consider reaction rate information.\nHow to use the enzymatic transformer\nThe examples discussed above belong to the ENZR test set for\nwhich the product molecules have never been seen by the\nenzymatic transformer during training and validation, but they\nare recorded cases for which a look-up in the scientic literature\nwill give the answer. In a possible application, one might use\nthe enzymatic transformer to select which enzyme might be\nbest suited for a given biotransformation not yet recorded in the\ndataset. To carry out such prediction, one would analyze the\nproduct structures and condence scores returned by the model\nwhen presented with a given substrate and various enzymes.\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8655\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nAs a theoretical example, we consider the reduction of levu-\nlinic anilide to either enantiomer of the corresponding chiral\nalcohol, a reaction which is not present in the training set. We\nused the enzymatic transformer to predict which product would\nbe formed by exposing this ketone to 163 alcohol dehydroge-\nnases and 60 ketoreductases in the ENZR dataset. In this case,\nthe transformer model predicts with high con dence two\nexperimentally veried cases of two diﬀerent keto-reductases in\nthe test set forming either the (S) or the (R) enantiomeric\nalcohol enantioselectively. In addition, the transformer also\nproposes high con dence reactions to either enantiomers\ninvolving other ketoreductase and alcohol dehydrogenases\nenzymes, which could be considered for experimental testing\n(Fig. 7).\nOne might also use the enzymatic transformer to predict\nwhich substrates might be converted by a given enzyme. To\nillustrate this point, we considered the enzyme “\nD-glucose\ndehydrogenase alcohol dehydrogenase ymr226c from Saccha-\nromyces cerevisiae”, which is documented in six reactions of the\ntraining set to reduce various acetophenones enantioselectively\nand correctly predicts the product structure and stereochem-\nistry for the 2 examples in the test set (Fig. S10,† substrates D1\nand D2). One can then challenge the enzymatic transformer to\npredict which product might be formed with further ketone\nsubstrates and the same enzyme. The transformer predicts the\nprobably correct alcohol products with high condence scores\nfor ketones that are structurally related to the database exam-\nples (Fig. S10,† substrates D3–D15). Among further analogs that\nare less similar, three cases are predicted with high condence\n(Fig. S10,† substrates D16–D18), and the remainingve cases\nhave much lower con dence scores as well as sometimes\nunlikely product structure, indicating that the model is\nFig. 6 Examples of unsuccessful predictions by the enzymatic transformer.\n8656 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nuncertain about the possible outcome of these reactions\n(Fig. S10,† substrates D19–D22).\nConclusion\nWe had previously shown the principle of transfer learning to\nspecialize the general USPTO transformer model at the example\nof carbohydrate reactions, however this approach used SMILES\ninformation only and a limited set of 405 tokens.\n22 Here we\nshowed for therst time that the general USPTO transformer\nmodel can be used as a basis for transfer learning using a more\ncomplex language information, here an extended vocabulary of\nseveral thousand language and atomic tokens describing\nenzymatic reactions in text format. Despite of the relatively\nsmall size of the ENZR dataset of enzymatic reactions used here,\nthe resulting enzymatic transformer model predicted the\noutcome of enzymatic transformations including enantiose-\nlective reactions with excellent accuracy. This type of approach\nmight be extended in the future to incorporate additional\ninformation such as reaction conditions and experimental\nprocedures.\nIt should be noted that the text descriptions of enzymes used\nin our ENZR dataset most o en represent a rather plain\ndescription of the reaction and substrate involved,e.g. “tyrosine\ndecarboxylase”, which provides a direct hint for the enzymatic\ntransformer for proposing a product structure. Nevertheless,\nother descriptions of enzymes such as their EC number,\n14 their\namino acid sequence or a representation of the sequence\nproduced by an auto-encoder,58,59 might also be exploitable for\nthe enzymatic transformer if these would be available since\nthese descriptions in principle contain the same information,\neven if in a more indirect manner.\n62\nHere we demonstrated the feasibility of using a text\ndescription of an enzyme to train a transformer model to\npredict product structure given a substrate and the enzyme. The\nsame data type might be suitable to train a transformer to\npredict the substrate structure given a product and an enzyme\n(retro-synthesis) or to predict an enzyme name given a substrate\nand a product, however to succeed such models might require\nmuch larger datasets than the relatively small ENZR dataset\nused here.\nIn this study, we obtained the best prediction accuracies\nwhen using multi-task transfer learning based on the full\ndescription of the enzymes. However, model performance was\nlimited by database size and was lower with enzymes for which\nonly few examples were available. Furthermore, analysis of\nsuccesses and failures showed that model performance is also\nlimited by the occurrence of database entry errors. Model\nperformance can probably be increased by using larger and\nhigher quality training dataset. Furthermore, the performance\nof our enzymatic transformer model was highest with the\nenzymes that are most represented in the ENZR dataset, which\nwere lipases and dehydrogenases due to the historical nature of\nthe data source reecting which enzymes have been mostly used\nin the literature. Considering that transformer models learn\nfrom example, increasing the performance for other types of\nbiotransformations such as keto-reductases and mono-\noxygenases will critically depend on acquiring training data for\nsuch types of enzymes. Provided the availability of experimental\ntraining data, the transfer learning approach demonstrated\nhere should be optimally suited to integrate this data into\npredictive models capable of assisting chemists in implement-\ning biotransformations for chemical synthesis.\nMethods\nData collection\nThe USPTO data was downloaded from the patent mining work\nof Lowe.24 The ENZR data set was downloaded from Reaxys.25\nEnzymatic reactions were found querying“enzymatic reaction”\nkeywords directly in the searcheld.\nTransformer training\nThe enzymatic transformer model was trained based on the\nmolecular transformer work from Schwalleret al.18 The version\n1.1.1 of OpenNMT,38 freely available on GitHub,60 were used to\npreprocess, train and test the models. Minor changes were\nperformed based on the version of Schwalleret al.\n18 SMILES\nwere also tokenized using the same tokenizer as Schwaller\net al.18 The ENZR description sentences were tokenized by the\nHugging Face Tokenizers37 using a byte pair encoding61 result-\ning in a vocabulary of 6139 language tokens (top 40 most\nfrequent tokens in Fig. S11 †) for which the occurrence\nfrequencies follow a power-law distribution shown in Fig. S12.†\nFig. 7 Examples of usage of the enzymatic prediction model toﬁnd\nsuitable enzymes leading to diﬀerent enantiomers. Screening sen-\ntences were extracted from the entire dataset. Filtering was applied for\ndehydrogenases and ketoreductases from single enzyme systems and\nﬁltered for simple sentences (less than 5 words). Resulting in a total of\n223 sentences (163 dehydrogenases and 60 ketoreductases). Are\nshown the top 5 conﬁdence score sentences leading to both enan-\ntiomers. Red colored sentences were present in the test set providing\nexperimental proof.\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8657\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nFor our model, we used the 3000 most frequent tokens repre-\nsenting 97.4% of tokens found in ENZR sentences. The 3139\nremaining tokens only represent 2.6% of occurrences and have\nless important frequencies going from 7 to 1. The following\nhyperparameters were used for the multi-task model:\npreprocess.py -train_ids ENZR ST_sep_aug\n-train_src $DB/ENZR/src_train.txt $DB/ST_sep_aug/src-train.txt\n-train_tgt $DB/ENZR/tgt_train.txt $DB/ST_sep_aug/tgt-train.txt\n-valid_src $DB/ENZR/src_val.txt -valid_tgt $DB/ENZR/tgt_val.txt\n-save_data $DB/Preprocessed\n-src_seq_length 3000 -tgt_seq_length 3000\n-src_vocab_size 3000 -tgt_vocab_size 3000\n-share_vocab -lower\ntrain.py -data $DB/Preprocessed\n-save_model ENZR_MTL -seed 42 -train_steps 200000 -param_init 0\n-param_init_glorot -max_generator_batches 32 -batch_size 6144\n-batch_type tokens -normalization tokens -max_grad_norm\n0 -accum_count 4\n-optim adam -adam_beta1 0.9 -adam_beta2 0.998 -decay_-\nmethod noam\n-warmup_steps 8000 -learning_rate 4 -label_smoothing 0.0\n-layers 4\n-rnn_size 384 -word_vec_size 384\n-encoder_type transformer -decoder_type transformer\n-dropout 0.1 -position_encoding -global_attention general\n-global_attention_function somax -self_attn_type scaled-dot\n-heads 8 -transformer_ﬀ 2048\n-data_ids ENZR ST_sep_aug -data_weights 1 9\n-valid_steps 5000 -valid_batch_size 4 -early_stopping_criteria\naccuracy\nValidation\nCanonicalized SMILES were compared to assess the accuracy of\nthe models. Distribution of the training, validation and test set\nwas randomly distributed a er being grouped by reaction\nproduct multiple time resulting in constant accuracy.\nTMAPs\nTMAPs were computed using standard parameters.\n32 The reac-\ntion ngerprint (RXNFP)33 as well as the molecular substructure\nngerprint (MHFP6)35 was computed with a dimension of 256.\nAvailability of data and materials\nThe USPTO data is available from the patent mining work of\nLowe.24 Reactions from Reaxys are accessible with subscription.\nThe modied version of OpenNMT as well as the code for data\nextraction and preprocessing as well as to tokenize, train and\ntest the model are available from: https://github.com/reymond-\ngroup/OpenNMT-py.\nAuthor contributions\nDK designed and carried out the study and wrote the paper, PS\nprovided support on the transformer model and wrote the\npaper, JLR designed and supervised the study and wrote the\npaper.\nConﬂicts of interest\nThe authors declare that they have no competing interests.\nAcknowledgements\nThis work was supportednancially by Novartis. We would like\nto thank Dr Thierry Schlama, Dr John Lopez and Dr Radka\nSnajdrova for helpful discussions.\nReferences\n1 R. A. Sheldon and J. M. Woodley,Chem. Rev., 2018,118, 801–\n838.\n2 S. Wu, R. Snajdrova, J. C. Moore, K. Baldenius and\nU. T. Bornscheuer,Angew. Chem., Int. Ed. Engl., 2020,59,2 –\n34.\n3 F. H. Arnold,Angew. Chem., Int. Ed. Engl., 2018, 57, 4143–\n4148.\n4 J. N. Wei, D. Duvenaud and A. Aspuru-Guzik,ACS Cent. Sci.,\n2016, 2, 725–732.\n5 B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu\nNguyen, S. Ho, J. Sloane, P. Wender and V. Pande,ACS Cent.\nSci., 2017,3, 1103–1113.\n6 C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green and\nK. F. Jensen,ACS Cent. Sci., 2017,3, 434–443.\n7 M. H. S. Segler, M. Preuss and M. P. Waller,Nature, 2018,\n555, 604–610.\n8 C. W. Coley, W. H. Green and K. F. Jensen,Acc. Chem. Res.,\n2018, 51, 1281–1289.\n9 V. H. Nair, P. Schwaller and T. Laino,Chimia, 2019,73, 997–\n1000.\n10 S. Johansson, A. Thakkar, T. Kogej, E. Bjerrum, S. Genheden,\nT. Bastys, C. Kannas, A. Schliep, H. Chen and O. Engkvist,\nDrug Discovery Today: Technol., 2019,32–33,6 5–72.\n11 I. V. Tetko, P. Karpov, R. Van Deursen and G. Godin,Nat.\nCommun., 2020,11, 5575.\n12 W. W. Qian, N. T. Russell, C. L. W. Simons, Y. Luo,\nM. D. Burke and J. Peng, 2020, chemrxiv preprint, DOI:\n10.26434/chemrxiv.11659563.v1.\n13 Y. Cai, H. Yang, W. Li, G. Liu, P. W. Lee and Y. Tang,J. Chem.\nInf. Model., 2018,58, 1169–1181.\n14 N. Hadadi, H. MohammadiPeyhani, L. Miskovic, M. Seijo\nand V. Hatzimanikatis,Proc. Natl. Acad. Sci. U. S. A., 2019,\n116, 7298\n–7307.\n15 E. E. Litsa, P. Das and L. E. Kavraki,Chem. Sci., 2020, 11,\n12777–12788.\n16 W. Finnigan, L. J. Hepworth, S. L. Flitsch and N. J. Turner,\nNat. Catal., 2021,4,9 8–104.\n17 P. Schwaller, T. Gaudin, D. L´anyi, C. Bekas and T. Laino,\nChem. Sci., 2018,9, 6091–6098.\n18 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter,\nC. Bekas and A. A. Lee,ACS Cent. Sci., 2019,5, 1572–1583.\n8658 | Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 © 2021 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n19 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, inAdvances in\nneural information processing systems, 2017, pp. 5998–6008.\n20 D. Weininger,J. Chem. Inf. Comput. Sci., 1988,28,3 1–36.\n21 A. Thakkar, T. Kogej, J.-L. Reymond, O. Engkvist and\nE. J. Bjerrum,Chem. Sci., 2019,11, 154–168.\n22 G. Pesciullesi, P. Schwaller, T. Laino and J.-L. Reymond,Nat.\nCommun., 2020,11, 4874.\n23 D. M. Lowe, PhD thesis, University of Cambridge, 2012.\n24 D. Lowe, Chemical reactions, US Pat. (1976-Sep 2016), 2017,\nhttp://https://gshare.com/articles/\nChemical_reactions_from_US_patents.\n25 A. J. Lawson, J. Swienty-Busch, T. G´eoui and D. Evans, inThe\nFuture of the History of Chemical Information , American\nChemical Society, 2014, vol. 1164, pp. 127–148.\n26 S. Ferri, K. Kojima and K. Sode,J. Diabetes Sci. Technol., 2011,\n5, 1068–1076.\n27 O. Khersonsky and D. S. Tawk, Annu. Rev. Biochem., 2010,\n79, 471–505.\n28 K. Hult and P. Berglund,Trends Biotechnol., 2007, 25, 231–\n238.\n29 S. Velikogne, W. B. Breukelaar, F. Hamm, R. A. Glabonjat\nand W. Kroutil,ACS Catal., 2020,10, 13377–13382.\n30 M. Kanehisa,Methods Mol. Biol., 2017,1611, 135–145.\n31 A. Chang, L. Jeske, S. Ulbrich, J. Hofmann, J. Koblitz,\nI. Schomburg, M. Neumann-Schaal, D. Jahn and\nD. Schomburg,Nucleic Acids Res., 2021,49, D498–D508.\n32 D. Probst and J.-L. Reymond,J. Cheminf., 2020,12, 12.\n33 P. Schwaller, D. Probst, A. C. Vaucher, V. H. Nair, D. Kreutter,\nT. Laino and J.-L. Reymond,Nat. Mach. Intell., 2021,3, 144–\n152.\n34 C. W. Coley, W. H. Green and K. F. Jensen,J. Chem. Inf.\nModel., 2019,59, 2529–2537.\n35 D. Probst and J.-L. Reymond,J. Cheminf., 2018,10, 66.\n36 G. Landrum, et al. , RDKit: Open-Source Cheminformatics\nSoware, 2020.\n37 T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue,\nA. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite,\nJ. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest\nand A. M. Rush, 2019, arXiv:1910.03771 [cs].\n38 G. Klein, Y. Kim, Y. Deng, J. Senellart and A. Rush, in\nProceedings of ACL 2017, System Demonstrations ,\nAssociation for Computational Linguistics, Vancouver,\nCanada, 2017, pp. 67–72.\n39 A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,\nG. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison,\nA. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai and\nS. Chintala, in Advances in Neural Information Processing\nSystems 32, ed. H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alch´e-Buc, E. Fox and R. Garnett, Curran Associates,\nInc., 2019, pp. 8024–8035.\n40 J. Xu, Y. Cen, W. Singh, J. Fan, L. Wu, X. Lin, J. Zhou,\nM. Huang, M. T. Reetz and Q. Wu,J. Am. Chem. Soc., 2019,\n141, 7934–7945.\n41 Y.-H. Kim and S. Park,Bull. Korean Chem. Soc., 2017, 38,\n1358–1361.\n42 H. Ankati, D. Zhu, Y. Yang, E. R. Biehl and L. Hua,J. Org.\nChem., 2009,74, 1658–1662.\n43 W. Borze˛cka, I. Lavandera and V. Gotor,J. Org. Chem., 2013,\n78, 7312–7317.\n44 H. C. B¨uchsensch¨utz, V. Vidimce -Risteski, B. Eggbauer,\nS. Schmidt, C. K. Winkler, J. H. Schrittwieser, W. Kroutil\nand R. Kourist,ChemCatChem, 2020,12, 726–730.\n45 F. G. Mutti and W. Kroutil,Adv. Synth. Catal., 2012, 354,\n3409–3413.\n46 R. R. Chao, J. J. D. Voss and S. G. Bell,RSC Adv., 2016,\n6,\n55286–55297.\n47 K. Neufeld, J. Marienhagen, U. Schwaneberg and\nJ. Pietruszka,Green Chem., 2013,15, 2408–2421.\n48 P. Both, H. Busch, P. P. Kelly, F. G. Mutti, N. J. Turner and\nS. L. Flitsch,Angew. Chem., Int. Ed., 2016,55, 1511–1513.\n49 C. S. Alexeev, G. G. Sivets, T. N. Safonova and S. N. Mikhailov,\nNucleosides, Nucleotides Nucleic Acids, 2017,36, 107–121.\n50 W. Wang and B. Wang,Chem. Commun., 2017, 53, 10124–\n10127.\n51 H. A. Namanja-Magliano, C. F. Stratton and V. L. Schramm,\nACS Chem. Biol., 2016,11, 1669–1676.\n52 R.-J. Li, J.-H. Xu, Y.-C. Yin, N. Wirth, J.-M. Ren, B.-B. Zeng and\nH.-L. Yu,New J. Chem., 2016,40, 8928–8934.\n53 E. A. Hall, M. R. Sarkar and S. G. Bell,Catal. Sci. Technol.,\n2017, 7, 1537–1548.\n54 J. A. Faraldos, D. J. Miller, V. Gonz ´alez, Z. Yoosuf-Aly,\nO. Casc´on, A. Li and R. K. Allemann, J. Am. Chem. Soc.,\n2012, 134, 5900–5908.\n55 R.-J. Li, A. Li, J. Zhao, Q. Chen, N. Li, H.-L. Yu and J.-H. Xu,\nCatal. Sci. Technol., 2018,8, 4638–4644.\n56 S. Schmidt, H. C. B¨uchsensch¨utz, C. Scherkus, A. Liese,\nH. Gr¨oger and U. T. Bornscheuer,ChemCatChem, 2015, 7,\n3951–3955.\n57 R. S. Heath, W. R. Birmingham, M. P. Thompson,\nA. Taglieber, L. Daviet and N. J. Turner, ChemBioChem,\n2019, 20, 276–281.\n58 J. Wang, L. Zhang, L. Jia, Y. Ren and G. Yu,Int. J. Mol. Sci.,\n2017, 18, 2373.\n59 V. Gligorijevi´c, M. Barot and R. Bonneau, Bioinformatics,\n2018, 34, 3873–3881.\n60 OpenNMT/OpenNMT-py, https://github.com/OpenNMT/\nOpenNMT-py, accessed July 28, 2020.\n61 R. Sennrich, B. Haddow and A. Birch, inProceedings of the\n54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , Association for\nComputational Linguistics, Berlin, Germany, 2016, pp.\n1715–1725.\n62 D. Probst, M. Manica, Y. G. N. Teukam, A. Castrogiovanni,\nF. Paratore and T. Laino, Chemrxiv, 2021, preprint, DOI:\n10.26434/chemrxiv.14639007.v1.\n© 2021 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 1 ,12,8 6 4 8–8659 | 8659\nEdge Article Chemical Science\nOpen Access Article. Published on 25 May 2021. Downloaded on 11/5/2025 2:47:34 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5923969745635986
    },
    {
      "name": "Enzyme",
      "score": 0.4678923189640045
    },
    {
      "name": "Computational biology",
      "score": 0.4673991799354553
    },
    {
      "name": "Chemistry",
      "score": 0.4283002018928528
    },
    {
      "name": "Computer science",
      "score": 0.35443899035453796
    },
    {
      "name": "Combinatorial chemistry",
      "score": 0.34555375576019287
    },
    {
      "name": "Biochemistry",
      "score": 0.2811625003814697
    },
    {
      "name": "Biology",
      "score": 0.24414628744125366
    },
    {
      "name": "Engineering",
      "score": 0.1620599329471588
    },
    {
      "name": "Electrical engineering",
      "score": 0.06382158398628235
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118564535",
      "name": "University of Bern",
      "country": "CH"
    }
  ]
}