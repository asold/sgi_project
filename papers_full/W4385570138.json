{
  "title": "Code Execution with Pre-trained Language Models",
  "url": "https://openalex.org/W4385570138",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2149968155",
      "name": "Chen-xiao Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2113488890",
      "name": "Shuai Lu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2108390110",
      "name": "Wei‐Zhu Chen",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2123654898",
      "name": "Daxin Jiang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2627672354",
      "name": "Alexey Svyatkovskiy",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2175401833",
      "name": "Shengyu Fu",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1981173961",
      "name": "Neel Sundaresan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A1966337374",
      "name": "Nan Duan",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3134642945",
    "https://openalex.org/W4221166942",
    "https://openalex.org/W4297732705",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W2108557864",
    "https://openalex.org/W4297795493",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W4288089571",
    "https://openalex.org/W2963937837",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4319997768",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3034549508",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W3198188208",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3035116013",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4283794074",
    "https://openalex.org/W4300972995",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4308731473",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W4394664141",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2135841285",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2173051530",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W3119507053",
    "https://openalex.org/W3107418514",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4288376504",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3025298766",
    "https://openalex.org/W2343438912"
  ],
  "abstract": "Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4984–4999\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCode Execution with Pre-trained Language Models\nChenxiao Liu1∗, Shuai Lu2, Weizhu Chen2, Daxin Jiang2,\nAlexey Svyatkovskiy2, Shengyu Fu2, Neel Sundaresan2, Nan Duan2\n1 Peking University 2 Microsoft\njadecxliu@gmail.com\n{shuailu, wzchen, djiang}@microsoft.com\n{alsvyatk, shengyfu, neels, nanduan}@microsoft.com\nAbstract\nCode execution is a fundamental aspect of pro-\ngramming language semantics that reflects the\nexact behavior of the code. However, most\npre-trained models for code intelligence ignore\nthe execution trace and only rely on source\ncode and syntactic structures. In this paper,\nwe investigate how well pre-trained models\ncan understand and perform code execution.\nWe develop a mutation-based data augmenta-\ntion technique to create a large-scale and real-\nistic Python dataset and task for code execu-\ntion, which challenges existing models such\nas Codex. We then present CodeExecutor, a\nTransformer model that leverages code execu-\ntion pre-training and curriculum learning to en-\nhance its semantic comprehension. We evaluate\nCodeExecutor on code execution and show its\npromising performance and limitations. We\nalso demonstrate its potential benefits for code\nintelligence tasks such as zero-shot code-to-\ncode search and text-to-code generation. Our\nanalysis provides insights into the learning and\ngeneralization abilities of pre-trained models\nfor code execution.\n1 Introduction\nPre-trained models have achieved remarkable re-\nsults in natural language (NL) tasks (Radford et al.,\n2018; Devlin et al., 2019; Raffel et al., 2020), in-\nspiring the development of pre-trained models for\nprogramming language (PL) tasks (Kanade et al.,\n2020; Feng et al., 2020; Svyatkovskiy et al., 2020;\nWang et al., 2021b; Guo et al., 2021, 2022). These\nmodels leverage source code and code structures,\nsuch as abstract syntax tree (AST) (Wang et al.,\n2021a; Guo et al., 2022) and data flow (Guo et al.,\n2021), to learn code-related tasks. These structures,\nwhile useful, are not sufficient to represent the dy-\nnamic behavior of code during execution, which is\nreflected in the execution trace. Using Figure 1 as\n∗Work done during internship at Microsoft. Shuai Lu and\nNan Duan are corresponding authors.\nan example, the execution trace shows how code be-\nhaves during execution, reflecting the control flow\nand the state changes of variables. On the other\nhand, as stated by Casalnuovo et al. (2020), source\ncode contains two channels of information: natural\n& formal. The natural channel (Hindle et al., 2012),\nsuch as identifiers and comments, enables language\nmodels to be leveraged to understand code-related\ntasks. The formal channel is used by interpreters\nand compilers to specify execution and has precise\nsemantics. The formal channel is unique to code\nand is what makes it executable. Execution trace\nfalls into the second category since it reveals the\nformal channel of information that distinguishes\ncode from natural language, as well as enabling\ncode execution precisely (Casalnuovo et al., 2020;\nChakraborty et al., 2022).\nIn this work, we aim to teach pre-trained models\nthe real-world code execution process. We pro-\npose CodeExecutor, a Transformer-based model\nthat learns to execute arbitrary programs and pre-\ndict their execution traces. To support pre-training\non large-scale data, we construct the Python Co-\ndeNetMut dataset by producing mutations based\non submissions to competitive programming prob-\nlems from CodeNet (Puri et al., 2021), along with\nsingle-line Python transformations and programs\nadapted from Python official tutorial. We design\na pre-training task that predicts both the line order\nand the intermediate states of the execution trace,\nand apply curriculum learning to gradually increase\nthe difficulty of the programs.\nWe evaluate CodeExecutor on code execution\ntasks and show that it outperforms existing mod-\nels and demonstrates promising capabilities. We\nalso conduct an in-depth analysis of the model’s\nperformance and reveal its strengths and weak-\nnesses. Furthermore, we show that CodeExecutor\ncan improve downstream tasks like zero-shot code-\nto-code search and text-to-code generation, indicat-\ning the potential of leveraging execution trace to\n4984\nh = 3 w = 7n = 10for iin range(min(h, w)): n = n -max(h, w)      if n <= 0: print(i+ 1) break\n<line> 1<state> h : 3 <line> 2<state> h : 3 ; w : 7<line> 3<state> h : 3 ; w : 7 ; n : 10<line> 4<state> h : 3 ; w : 7 ; n : 10 ; i: 0<line> 5<state> h : 3 ; w : 7 ; n : 3 ; i: 0<line> 6<state> h : 3 ; w : 7 ; n : 3 ; i: 0<line> 4<state> h : 3 ; w : 7 ; n : 3 ; i: 1<line> 5<state> h : 3 ; w : 7 ; n : -4 ; i: 1<line>6<state> h : 3 ; w : 7 ; n : -4 ; i: 1<output> 2 <line> 7<state> h : 3 ; w : 7 ; n : -4 ; i: 1<line> 8<state> h : 3 ; w : 7 ; n : -4 ; i: 1\n12345678 (a)SourceCode (b)ExecutionTrace\nFigure 1: Sample source code and its execution trace in the code execution task.\nenhance code intelligence. Our models and datasets\nare publicly available1. In summary, the contribu-\ntions of this paper are:\n• We present the first attempt at building a large-\nscale pre-training dataset for real-world code\nexecution using a mutation-based data aug-\nmentation approach.\n• We propose a novel pre-trained model named\nCodeExecutor that learns to predict the execu-\ntion traces using a code execution pre-training\ntask and curriculum learning.\n• We conduct a comprehensive evaluation of\nCodeExecutor for code execution tasks, pro-\nviding a detailed understanding of the model’s\nperformance.\n• CodeExecutor significantly improves code in-\ntelligence tasks like zero-shot code-to-code\nsearch and text-to-code generation.\n2 Related Work\n2.1 Learning to Execute\nPrevious works form the learning to execute task\nas a problem that reads a program and com-\nputes the program’s output. These works lever-\nage architectures such as recurrent neural networks\n(Zaremba and Sutskever, 2014), graph neural net-\nworks (Bieber et al., 2020; Wang et al., 2020) and\nTransformers (Dehghani et al., 2019; Yan et al.,\n2020; Austin et al., 2021; Nye et al., 2021). An-\nother related task algorithm induction is to read a\nshort program, such as integer addition or polyno-\nmial evaluation, and computes the output. Algo-\nrithm induction task (Graves et al., 2014; Kurach\net al., 2016; Kaiser and Sutskever, 2016; Graves\n1https://github.com/microsoft/CodeBERT/tree/\nmaster/CodeExecutor\net al., 2016; Reed and de Freitas, 2016; Dehghani\net al., 2019; Velickovic et al., 2020a,b; Nye et al.,\n2021) targets a particular algorithm with direct\nalgorithm-specific supervision compared with arbi-\ntrary programs in our code execution task.\nSome emerging works also employ pre-trained\nmodels to tackle the two tasks. Lu et al. (2022)\nfine-tunes a small fraction of the weights in GPT-\n2 (Radford et al., 2019) on non-language tasks,\nincluding simple algorithm induction tasks like Bit\nXOR. Austin et al. (2021) evaluates models pre-\ntrained on web documents and dialog data ranging\nin size from 2 million to 137 billion parameters and\nshows that largest models are generally unable to\npredict the output of a program, whether few-shot\nor fine-tuning. Nye et al. (2021) uses a \"scratchpad\"\nto store intermediate computation steps to perform\nmulti-step computations, improving the ability of\nmodels in Austin et al. (2021).\nDifferent from previous works that predict pro-\ngram’s output and mainly deal with specific algo-\nrithms, we predict the program’s whole execution\ntrace and focus on imitating the real-world arbi-\ntrary program execution behavior. Besides, by us-\ning execution to capture code semantics, our work\nis beneficial for tasks related to code intelligence.\n2.2 Mathematical Problem Solving\nMathematical problem solving is a related domain\nof code execution. Recent works show the ability\nof language models to solve math problems, which\nrequires learning to execute a soft algorithm to ar-\nrive at a deterministic answer. Amini et al. (2019);\nLing et al. (2017) map math problems to opera-\ntion programs and focus on sequence-to-program\ngeneration. Saxton et al. (2019) introduce the Deep-\nMind Mathematics dataset, which contains plug-\nand-chug problems such as addition, list sorting,\nand function evaluation. Henighan et al. (2020)\n4985\nOperator Description\nCRP Constant Replacement Change numeric and string literals.\nAOD Arithmetic Operator Deletion Delete a unary arithmetic operator ‘+’ or ‘-’.\nAOR Arithmetic Operator Replacement Replace an arithmetic operator with another one. E.g. x * y can be\nmutated to x / y.\nASR Assignment Operator Replacement Substitute an extended assignment operator with another.\nBCR Break Continue Replacement Swap keywords break and continue in a loop body.\nCOD Conditional Operator Deletion Delete unary negation operator not or the negation of an membership\noperator not in.\nLCR Logical Connector Replacement Swap logical operators and with or and vice versa.\nROR Relational Operator Replacement Substitutes relational operators. E.g. x <= y can be mutated to x > y.\nSIR Slice Index Removal Delete one argument of collection[start:end:step].\nOIL One Iteration Loop Execute a loop only once by adding a break statement.\nRIL Reverse Iteration Loop Change direction of loop iteration by the function reversed().\nZIL Zero Iteration Loop Interrupt realization of a loop during its first iteration.\nTable 1: A set of mutation operators containing 12 operators we implement to mutate code examples.\nshows that the majority of problems in the Deep-\nMind Mathematics dataset can be straightforwardly\nsolved with large Transformers. Hendrycks et al.\n(2021) introduces the MATH dataset, consisting\nof competition math problems with step-by-step\nsolutions written in LATEX and natural languages.\nCobbe et al. (2021) releases GSM8K, including\ngrade school math questions and natural language\nsolutions. Recently, Zhou et al. (2022) proposes\nalgorithmic prompting to improve the performance\nof large language models on math problem solv-\ning, which starts from learning skills containing\naddition, subtraction, multiplication, and parity.\nCode execution involves calculations such as\naddition, subtraction, multiplication, division, ex-\nponentiation, and modulus, which are similar to\nsolving math problems. With the added complex-\nity of managing variables, data structures, control\nflows, and other programming concepts, learning\ncode execution requires a different set of skills and\nknowledge from learning mathematics, although\nsome overlap exists.\n3 Mutation-based Data Augmentation\nThe goal of code execution task is to learn to emu-\nlate the execution without running a program by an\ninterpreter. We treat the task as a generation task:\ngiven a source code c, the execution trace t is re-\nquired to be generated. Execution trace consists of\ntwo components: one is the order in which the com-\nputer executes statements, and the other is how the\nstates of the variables change when jumping from\none statement to another. Normally, the statements\ninside a program are not executed sequentially, es-\npecially in a real-world scenario where programs\nembody complex logic and rich semantics. More-\nover, variables relate to various types of data struc-\ntures with diverse characteristics and operations.\nGiven the complexity and difficulty of this task, it\nis of great importance to build a large-scale dataset\nand explore the capabilities and boundaries of large\nlanguage models for code execution.\n3.1 Mutating Source Code\nConstructing a large-scale Python dataset for real-\nworld code execution is very challenging. Pro-\ngrams retrieved from software development plat-\nforms such as GitHub2 are mostly not executable at\nscale, as they depend on specific external resources\nwhich are not easily available. Examples of exter-\nnal resources include program inputs, file contents,\nexternal modules, and third-party packages. For the\nsame reason, it is not practical to collect programs\nfrom posts in coding question-answering websites\nlike StackOverflow 3.\nWe build the Python code execution dataset\nbased on submissions to competitive programming\nproblems from CodeNet benchmark (Puri et al.,\n2021). We run each submission in a sandbox en-\nvironment to get the execution trace and filter out\nprograms that exceed time and trace limits or result\nin runtime errors.\nTo construct a large-scale dataset of executable\nprograms, we propose a mutation-based data aug-\nmentation approach. For each submission, the ap-\nproach modifies some parts of a program to gen-\nerate diverse mutants, leading to different execu-\ntion traces. Specifications of these modifications\nare called mutation operators. It is inspired by\nmutation testing (Hamlet, 1977; Jia and Harman,\n2https://github.com/\n3https://stackoverflow.com/\n4986\n2011) in software engineering, a popular technique\nthat supports the design of high-quality test suites\nfor programs. Following Derezi ´nska and Hałas\n(2014) that applies mutation testing technique to\nPython programs, we first present a set of muta-\ntion operators as shown in Table 1. Most of them\ncorrespond to selected operators used in strongly\ntyped general purpose languages and are adopted\nto the Python language. Operators designed for\nPython features are also included, such as Slice\nIndex Removal (SIR) and Reverse Iteration Loop\n(RIL). Then we convert a program into an AST\nand extract its node type information to get a candi-\ndate list of all mutable literals, operators and state-\nments. Finally, we generate mutants and eliminate\nthose that are not executable. We use the CodeNet\nMutants (CodeNetMut) to build the pre-training\ndataset. Greater detail of the dataset generation\nprocess can be found in Appendix A.\n3.2 Dataset Construction\nGiven the difficulty of training the model on real-\nworld complete programs, we build two simpler\ndatasets along with CodeNetMut for pre-training.\nThe first is the Python SingleLine dataset col-\nlected by Fraser Greenlee 4, which consists of\nnearly nine million examples of single-line transfor-\nmations. Each example contains several variables\nspecified in initial values, a single line of Python\ncode, and the new set of variables and values re-\nsulting from executing that line. We combine the\nfirst two as the input code, and use the last one as\nthe target trace. We do not re-execute the dataset.\nWhen pre-training on SingleLine data, we only ask\nthe model to predict the final states of the last code\nline without line-by-line illustration. Figure 2 (a)(b)\nshow examples of these data. Since individual lines\nof code constitute real-world complex programs,\nthe dataset serves as a foundation for learning about\ncode execution.\nThe second is the Python Tutorial dataset. This\ndataset is created by crawling and filtering all the\nexecutable code examples that appear in the official\nPython tutorial 5. The official tutorial introduces\nthe basic concepts and most noteworthy features of\nthe Python language. To generate this dataset, we\napply the Constant Replacement operator (first row\nin Table 1) to change numeric literals into diverse\nvalues. This approach results in 3.4 million pro-\n4https://www.kaggle.com/frasergreenlee/\npython-state-changes\n5https://docs.python.org/3/tutorial\nc = 98 z = 3 c += z123\n(a)\nCode:\nTrace:c : 101; z : 3 \nf = 'ifailuhkqq’ l = ['a', 'i’] x = 2 y = 5 l = list(f[x:y])\n12345\n(b)\nCode:\nTrace:f:‘ifailuhkqq’;l:[‘a’,‘i’,‘l’];x:2;y:5 \n1234567\n(c)\nCode:\nTrace:<line> 1<state> stack:[3, 866, -325] <line> 2<state> stack:[3, 866, -325,6] <line> 3<state> stack:[3, 866, -325,6,7] <line> 4<state> stack:[3, 866, -325,6] <line> 5<state> stack:[3, 866, -325] <line> 6<state> stack:[3, 866] <line> 7<state> stack:[3, 866] \nstack = [3, 866, -325] stack.append(6) stack.append(7) stack.pop() stack.pop() stack.pop() stack\nFigure 2: (a) and (b) are examples from the SingleLine\ndataset. (c) is an example from the Tutorial dataset.\ngrams. Figure 2 (c) shows an example of a mutant.\nWhile the Tutorial dataset is not comprehensive\nand does not cover every single feature, it provides\na good representation of Python’s flavor and style,\nwhich offers valuable supervision for modeling the\nexecution of commonly used code blocks.\nTherefore, the Python Code Execution datasets\nare a series of datasets following an easy-to-hard\nparadigm, including the SingleLine dataset, Tuto-\nrial dataset, and CodeNetMut dataset.\n4 CodeExecutor\nOur CodeExecutor utilizes a Transformer-based\nframework to learn code execution through pre-\ntraining. We will first describe the model architec-\nture (§4.1), then the pre-training task (§4.2), and\nfinally, the curriculum learning strategy (§4.3).\n4.1 Model Architecture\nThe model is based on Transformer and adopts the\nsame architecture as UniXcoder (Guo et al., 2022).\nUniXcoder is a unified cross-modal pre-trained\nmodel for programming language which has\nencoder-only, decoder-only and encoder-decoder\nmodes. It utilizes mask attention matrices (Dong\net al., 2019) with prefix adapters to control the\nbehavior. We take the encoder-decoder manner\nby using a special token [E2D] as the prefix in\nfront of the input. CodeExecutor consists of 12\nTransformer layers. Each transformer layer is ar-\nchitecturally identical, containing a multi-headed\nself-attention pooling (Vaswani et al., 2017) fol-\nlowed by a feed forward network.\n4987\n4.2 Pre-training Task\nWe propose a new pre-training task called code\nexecution. Our motivation for the task is to im-\nprove the ability of our model to understand and\nexecute code. Traditional pre-training tasks such as\nlanguage modeling or denoising objective do not\ninvolve code execution, and thus, models trained\non these tasks have limited ability to execute code.\nBy pre-training our model on the task of code ex-\necution, we aim to improve its ability by learning\nuseful patterns from bimodal data of code and trace.\nThis will enable our model to generate more accu-\nrate traces and understand the behavior of the code,\nwhich is crucial for a wide range of code intelli-\ngence applications that require code understanding.\nWith the knowledge of how the code works, the\nmodel can better understand the underlying logic\nof the code and use that understanding to better\nperform these tasks.\nWe continue pre-training UniXcoder on the\ntask. At the pre-training stage, our model receives\ncode as inputs and learns to generate traces.\nTo facilitate a better understanding of code,\nspecial tokens [i] indicating line numbers and\n[INDENT ] [DETENT ] indicating indentation\nare inserted into the code. Each line in trace can\nbe represented as [LINE ], [i], [STATE ], v1, :\n, s1, [DICTSEP ], ...,[DICTSEP ], vk, :, sk,\n[STATEEND ], where k denotes the number\nof variables and the state of k-th variable vk\nis sk. The symbol [DICTSEP ] separates the\npairs within the dictionary and [STATEEND ]\nindicates the end of the states. This representation\nallows our model to learn the state of variables\nat each step of the execution, which is crucial for\nunderstanding the behavior of the code.\n4.3 Curriculum Learning\nTo improve the generalization capacity, we follow\nthe curriculum learning strategy during pre-training.\nCurriculum learning (Bengio et al., 2009) (CL) is a\nlearning strategy that starts from easy instances and\nthen gradually handles harder ones, which imitates\nthe meaningful learning order in human curricula.\nIn our pre-training process, we organize the learn-\ning of the Python code execution datasets according\nto a curriculum that starts with simple instances, i.e.\nSingleLine data. First, we employ all the 9 million\nSingleLine transformations to pre-train CodeEx-\necutor until convergence. To achieve a balanced\ndataset, we then reserve 3 million instances in Sin-\nSingleLine Tutorial CodeNetMut\nDifficulty Level Easy Medium Hard\nLanguage Python Python Python\nPre-train # 8,950,959 3,422,943 2,838,644\nTest # 7,968 13,744 19,541\nAvg Code Len 3.28 4.90 8.26\nAvg Trace Len 1.00 11.89 22.80\nAvg State Num 2.44 1.34 3.67\nTable 2: Statistics of pre-training dataset. “Avg Code\nLen” and “Avg Trace Len” represent the average num-\nber of lines in a program and a trace, respectively. “Avg\nState Num” denotes the average of the maximum num-\nber of states reached per line in a trace.\ngleLine that are most difficult for our model to gen-\nerate and add Tutorial data into the pre-training cor-\npus. We further add CodeNetMut data into the pre-\ntraining corpus and pre-train the model to converge\non all the examples. To help distinguish difficulty\nlevel, we add a prefix p ∈ {[SINGLELINE ],\n[TUTORIAL ], [CODENETMUT ]} in front\nof the input, indicating the kind of data, e.g.\n[SINGLELINE ] means receiving SingleLine\ndata. More details about pre-training settings and\nmodel configurations can be found in Appendix B.\n5 Experimental Setup\n5.1 Dataset\nWe build our pre-training dataset as described in\nSection 3. Table 2 shows some basic statistics. The\n19,541 examples in CodeNetMut test split are from\n39 unseen programming problems in CodeNet and\nhave not undergone the mutation process. Addition-\nally, we held out 10k programs from each dataset as\na validation split during pre-training. For Tutorial\nand CodeNetMut, the ground truth trace is the exe-\ncution result of the whole program. For SingleLine,\nsince the instances are simple programs consisting\nof variable declarations and one-line transforma-\ntions, the model is only asked to predict the final\nstates of variables, which is presented in the form\nof a one-line trace. We observe the average length\nof code and trace in CodeNetMut are about twice\nas long as those in Tutorial. Also, executing pro-\ngrams in CodeNetMut requires managing a larger\nnumber of variables in varying states.\n5.2 Models\nWe evaluate several models on code execution\ntask. Codex model code-cushman-001 is a spe-\ncialized GPT model fine-tuned on GitHub code\n(Chen et al., 2021). We use few-shot learning\n4988\nDataset Model General Line Identifier\nOutput Acc. Trace Acc. Precision Recall F1 Precision Recall F1\nSingeLine Codex - 36.87 36.87 36.87 36.87 71.87 69.34 70.58\nCEL-S1 - 93.32 93.32 93.32 93.32 96.94 96.86 96.90\nCodeExecutor - 94.03 94.03 94.03 94.03 97.28 97.18 97.23\nTutorial\nCodex 13.07 - - - - - - -\nCEL-S2 79.51 85.59 95.94 84.24 89.71 97.29 87.30 92.02\nCEL-S3 7.89 8.35 26.58 21.33 23.67 26.36 19.47 22.40\nCodeExecutor 76.42 80.09 94.49 76.74 84.70 95.91 69.15 80.36\nCodeNetMut Codex 17.45 - - - - - - -\nCEL-S3 43.80 29.44 59.32 41.76 49.01 68.30 41.69 51.78\nCodeExecutor 48.06 33.38 58.70 43.48 49.96 67.81 45.29 54.31\n-w/o CL 45.93 30.98 60.21 42.45 49.79 68.55 41.58 51.76\nTable 3: Results on the code execution task. In the Tutorial and CodeNetMut datasets, Codex cannot generate\nexecution traces in a uniform format. Therefore, we only report the output accuracy of Codex in these datasets.\nby giving Codex three code and execution trace\npairs for the code execution task. CodeExecutor-\nLimited (CEL) is a three-stage model pre-trained\nwith the code execution objective. CEL can only\naccess limited data in each stage, as opposed to\nCodeExecutor which can utilize all the datasets\nsimultaneously (see Appendix C for a detailed com-\nparison). It is initialized using the publicly avail-\nable checkpoint of UniXcoder and continues to be\ntrained with SingleLine data, resulting in the model\nCodeExecutorLimited-Stage1, which we call CEL-\nS1. In the second stage, we initialize it with CEL-\nS1 and employ Tutorial data to pre-train, so we\nget the model CEL-S2. By continuing pre-training\nCEL-S2, we use CodeNetMut to improve the ca-\npacity of executing real-world programs at the third\nstage. CEL-S3 is produced after these stages men-\ntioned above. CodeExecutor without Curriculum\nLearning(CodeExecutor w/o CL) is a single-stage\nmodel trained on all three datasets together.\n5.3 Evaluation Metrics\nWe test model capabilities of executing code on the\ntest sets from three datasets. We measure functional\ncorrectness of the sampled trace from three perspec-\ntives. We report output accuracy and trace accuracy\nto evaluate the general aspect. Output accuracy\nchecks if the model prints the same message as\nthe code execution, calculated only for programs\nwith standard output. Trace accuracy checks if the\nmodel produces the same trace as the code execu-\ntion, regardless of the order of states in a line of the\ntrace. To evaluate the correctness of each line and\nthe states of identifiers in the trace, we also assess\nper-line score and identifier score. Line precision\nis determined by the ratio of correctly identified\nlines among all the lines in the traces generated\nby the model. Line recall is the ratio of correctly\nidentified lines predicted by the model among all\nthe lines in the ground truth traces. Similarly, we\nalso calculate scores for the identifiers in the trace.\nTo deepen our understanding of model behav-\nior and error modes, we also conduct a qualitative\nanalysis by examining samples.\nWe randomly sample 50 code-trace pairs from\nthe test set and ask two programmers with at least\n5 years of experience to evaluate whether CodeEx-\necutor executes a program correctly in 7 aspects.\nThe category Basic includes basic knowledge for\na Python beginner like math operators, augmented\nassignment operators, comparison operators, vari-\nables. The category Lists, Tuples, etc. consists of\ntypical Python data structures, such as lists, tuples,\ndictionaries, sets, and related manipulation func-\ntions. As shown in Table 4, we build the taxonomy,\nalong with a handbook to guide classification. Each\nreviewer examines the generated trace line by line\nand counts the occurrence frequency of each cate-\ngory. They count all these categories if a trace line\ninvolves multiple categories. When an error occurs,\nthey identify which kind of knowledge category\nthe model mistakes. Finally, they work together\nto discuss the divergence of error attribution and\ncome to an agreement.\n6 Results and Analysis\nIn this section, we evaluate CodeExecutor on code\nexecution task(§6.1), conduct an in-depth analy-\nsis to understand model behavior and error mode\n(§6.2), followed by two downstream tasks (§6.3).\n4989\nrec = ['10', '3', '5'] n, a, b = map(int, rec) nin= [a, b] nmax= min(nin) nmin= n -min(n, (n-nin[0])+(n-nin[1])) print(str(nmax) + \" \" + str(nmin))\n123456\nCode: Prediction:<line> 1<state> rec:[10, 3,5]  <line> 2<state> rec:[10, 3,5];n:10;a:3;b:5 <line> 3<state> rec:[10, 3,5];n:10;a:3;b:5;nin:[3,5] <line> 4<state> rec:[10, 3,5];n:10;a:3;b:5;nin:[3,5];nmax:3 <line> 5<state> rec:[10, 3,5];n:10;a:3;b:5;nin:[3,5];nmax:3;nmin:2<output> 3 2 <line> 6<state> rec:[10, 3,5];n:10;a:3;b:5;nin:[3,5];nmax:3;nmin:2\nFigure 3: An Example from CodeNetMut test split, where CodeExecutor produces an imperfect prediction, with the\nmistake highlighted by an underline.\n6.1 Overall Results\nWe evaluate the performance of models on Single-\nLine, Tutorial and CodeNetMut datasets.\nWe show the result of SingleLine in Table 3\n(top). CodeExecutor is able to execute around\n94% of single-line transformations correctly, while\nCodex fails to do so in most cases. CodeExecutor\nalso brings a 0.7% improvement over CEL-S1, indi-\ncating learning hard programs during pre-training\nhelps better solve easier examples. Since each Sin-\ngleLine program always produces a one-line trace\nwithout standard outputs, we do not report output\naccuracy, and the line precision/recall scores are\nequal to trace accuracy.\nFor the Tutorial experiments in Table 3\n(medium), CodeExecutor significantly outperforms\nCodex on output accuracy (76.42% vs.13.07%).\nThe lower score of CodeExecutor compared to\nCEL-S2 suggests a discrepancy between code ex-\namples in tutorials and CodeNet since the Tutorial\ndataset is composed of mutants from only a few\nprograms in tutorial websites, limiting its diversity.\nCEL-S3 struggles to produce traces, indicating that\nit forgets most knowledge acquired in Tutorial data\nin the last training stage.\nCodeNetMut results are much lower than those\nin SingleLine and Tutorial datasets, which shows\nthat it is more challenging to generate traces in\nreal-world scenarios. CodeExecutor produces the\ncorrect output for nearly half of the examples\n(48.06%), and about a third of the traces are the\nexact match for the ground truth (33.38%). By pre-\ntraining on the code execution task, CodeExecutor\nboosts the performance of output by 30.6% abso-\nlute points over Codex. Besides, CodeExecutor\nyields 4.3% output accuracy score and 3.9% trace\naccuracy score improvement than CEL-S3, which\nindicates the effectiveness of the training strategy\ndescribed in 4.3. After removing curriculum learn-\ning, the output accuracy score drops from 48.06%\nto 45.93% and the trace accuracy score drops from\n33.38% to 30.98%, which shows the contribution\nCategory Total Correct Accuracy\nBasic 204 183 89.71\nBuilt-in Functions 42 35 83.33\nLists, Tuples, etc. 44 34 77.27\nStrings 19 10 52.63\nConditional Statements 60 57 95.00\nLoops 25 21 84.00\nFunction Calls 5 5 100.00\nTable 4: Human evaluation results. To evaluate the ca-\npability of CodeExecutor, we classify Python program-\nming knowledge into seven categories and manually\nanalyze whether the generated trace is correct or wrong\nwhen dealing with these categories. The third category\nincludes Python data structures, such as lists, tuples,\ndictionaries and sets.\nof curriculum learning.\nThese results demonstrate that the code execu-\ntion task is challenging for pre-trained models on\nsource code like Codex. However, our CodeExecu-\ntor model can achieve high performance to execute\nsimple programs and are capable of predicting com-\nplex execution traces for real-world programs.\n6.2 In-depth Study on Model Performance\nWe conduct a qualitative analysis of model perfor-\nmance by examining samples (Table 4), resulting\nin the following findings. More examples can be\nfound in Appendix D.\nThe Model Typically Has a Basic Sense of Con-\ntrol Flows Conditional statements, loops, and\nfunction calls reveal the control flow of the pro-\ngram. Control flow reflects the order in which the\nprogram’s code executes. It is important for un-\nderstanding a program and is often complex, as\nit controls the code through certain decisions and\nmonitors which statements need to be executed\nand which should be skipped. From Table 4, we\nfind that CodeExecutor has a rudimentary under-\nstanding of high-level multi-line control flows, es-\npecially expert at conditional statements and func-\ntion calls. 57 out of 60 conditional statements and\nall 5 calls to user-defined functions are predicted\n4990\nModel MAP\nGraphCodeBERT 23.08\n+ CodeExecutor 55.94\nUniXcoder 71.86\n+ CodeExecutor 79.13\nTable 5: MAP score (%) on code-to-code search task\nin zero-shot setting.\ncorrectly. The accuracy of loops is 84%, while\nthe incorrect loops undergo wrong iterative times.\nTake Figure 1 (a) as an example. CodeExecutor\npredicts exactly the same trace as the ground truth\nin (b). Our model recognizes that the for loop oc-\ncurred on line 4 will execute several times. In the\nsecond iteration, “n” meets the condition of “n <=\n0”, resulting in the “break” statement and termi-\nnating the loop. The model behaves well on the\ncode block in the for loop, showing its capacity of\nunderstanding control flows.\nThe Model Struggles to Handle the Intricacies\nof Operations, Particularly in Relation to Data\nStructures Complex programs often involve mul-\ntiple categories of programming knowledge. Figure\n3 shows an example that uses lists and strings. It\ndetermines the maximum and minimum possible\nnumber of people among “n”, who subscribe to\nboth Newspaper I and II, given that “a” people\nsubscribe to I and “b” people subscribe to II. Code-\nExecutor incorrectly calculates “nmin” in line 5,\nexpected 0 but got 2. This calculation involves re-\ntrieving values from a list, performing additions,\nsubtractions, and using the \"min\" function. The\ncompositionality of these operations makes it chal-\nlenging for our model to fully comprehend the code\nand generate accurate states. Additionally, as pre-\nsented by the relatively low accuracy on “Lists,\nTuples, etc.” (77.27%) and “Strings” (52.63%) in\nTable 4, we observe that the model falls short of\nunderstanding data structures like lists and strings.\nThe understanding of data structures requires the\nmodel to learn the behavior of objects after they\nare created, modified, added or deleted. These\noperations can be changeable and challenging for\nthe model to grasp. This suggests that the model\nmay struggle with complex programs that involve\nmultiple operations and data structures.\n6.3 Downstream Tasks\nTo verify the effectiveness of CodeExecutor in rep-\nresenting code semantics, we apply it to two code\nModel Pass@1 Pass@10\nCodex 12.48 45.59\n+ CodeExecutor 17.87 49.69\nTable 6: Results on HumanEval benchmark for the\ntext-to-code generation task. 50 solutions are evaluated\nfor each problem in both settings.\nintelligence tasks – the zero-shot code-to-code-\nsearch task and text-to-code generation task.\nZero-shot Code-to-code Search The task is in-\ntroduced by Guo et al. (2022). To avoid duplication\nbetween the associate dataset and our pre-training\ncorpus, we construct a new dataset by collecting\n9,987 Python functions from CodeNet (Puri et al.,\n2021). Each function solves one of the 48 problems.\nGiven one function, we retrieve all the functions\nthat solve the same problem.\nWe first use the mean vectors of last hidden states\nof a baseline model to calculate the similarity be-\ntween two functions. To explore how code exe-\ncution facilitates code-to-code-search, we execute\neach function by providing a test case. We then\nutilize the program outputs extracted from the exe-\ncution trace generated by CodeExecutor, and sort\nthe candidates according to the edit similarity com-\npared with outputs of the query program.\nFrom table 5, we find that CodeExecutor boosts\nover 32.8 points compared with GraphCodeBERT\n(Guo et al., 2021), and provides about 7.2 points\nimprovement compared with UniXcoder, showing\nthat code execution can significantly enhance the\ncomprehension of code semantics.\nText-to-code Generation We use HumanEval\nbenchmark (Chen et al., 2021) which includes 164\nhuman-written programming problems.\nWe first leverage Codex ( code-cushman-001)\nto generate 200 solutions for each problem. Then\nwe use CodeExecutor to predict the outputs of each\nsolution by feeding example test cases in problem\ndescriptions. We rank the 200 solutions by the\nedit similarity between their outputs and expected\noutputs. Finally, we evaluate the correctness of\nthe first 50 solutions for each problem. Note that\ndifferent from other filtering strategies, our method\ndoesn’t need a real-world code executor but only\nuses models to predict the execution results.\nTable 6 demonstrates that with CodeExecutor as\na solution filter, the performance of text-to-code\ngeneration is improved, indicating CodeExecutor\n4991\nis beneficial to other code intelligence tasks.\n7 Conclusion\nWe propose a mutation-based data augmentation\nmethod to create a large and realistic Python code\nexecution dataset and task, which pose a signifi-\ncant challenge for current models such as Codex.\nWe develop CodeExecutor, a Transformer model\nthat leverages code execution as a pre-training ob-\njective and adopts a curriculum learning strategy.\nCodeExecutor not only outperforms existing mod-\nels on code execution, but also demonstrates its\ngeneralizability to downstream tasks such as code-\nto-code search and text-to-code generation. Our\nwork offers a novel and effective solution for code\nexecution and other code intelligence tasks.\nLimitations\nSeveral limitations of CodeExecutor, such as its\napplication to only Python, the lack of faithfulness\nin the results produced, and the maximum length\nlimit for trace generation, point toward interesting\ndirections for future work.\nProgramming Language One limitation of our\ncurrent model is that it is currently only applied\nto Python, which limits its use and effectiveness\nin executing programs written in other program-\nming languages. This highlights the need for future\nwork to expand the model’s applicability to other\nlanguages.\nFaithfulness The result may not be faithful\nenough when handling difficult examples, such\nas those with complex logic, long loops, or many\nbranches. For example, we observe that in two com-\nplicated programs that both contain the assignment\n“alpha = list(’abcdefg’)”, our model correctly pre-\ndicts the value of “alpha” in one case but incor-\nrectly in the other. The lack of faithfulness needs to\nbe studied for further research on code execution.\nGeneration Window Size We limit the length\nof generated trace to 1024 tokens. It can be a lim-\nitation for programs with long execution traces,\nparticularly those with loops. Improving the ability\nof Transformers to handle longer sequences (Tay\net al., 2021, 2022) would likely be beneficial for\nthe code execution task.\nEthical Statement\nThe work is conducted in compliance with ethical\nprinciples. The datasets introduced in this paper\nonly used publicly available data. The annotation in\nhuman evaluation was conducted by two authors of\nthe paper, and thus there are no associated concerns,\ne.g. regarding compensation. Therefore, there are\nno potential risks associated with the research.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 2357–2367. Association for Computational\nLinguistics.\nJacob Austin, Augustus Odena, Maxwell I. Nye,\nMaarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le,\nand Charles Sutton. 2021. Program synthesis with\nlarge language models. CoRR, abs/2108.07732.\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Con-\nference on Machine Learning, ICML 2009, Montreal,\nQuebec, Canada, June 14-18, 2009, volume 382 of\nACM International Conference Proceeding Series ,\npages 41–48. ACM.\nDavid Bieber, Charles Sutton, Hugo Larochelle, and\nDaniel Tarlow. 2020. Learning to execute programs\nwith instruction pointer attention graph neural net-\nworks. In Advances in Neural Information Process-\ning Systems 33: Annual Conference on Neural In-\nformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nCasey Casalnuovo, Earl T. Barr, Santanu Kumar Dash,\nPrem Devanbu, and Emily Morgan. 2020. A the-\nory of dual channel constraints. In ICSE-NIER 2020:\n42nd International Conference on Software Engineer-\ning, New Ideas and Emerging Results, Seoul, South\nKorea, 27 June - 19 July, 2020, pages 25–28. ACM.\nSaikat Chakraborty, Toufique Ahmed, Yangruibo Ding,\nPremkumar T. Devanbu, and Baishakhi Ray. 2022.\nNatgen: generative pre-training by \"naturalizing\"\nsource code. In Proceedings of the 30th ACM Joint\nEuropean Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineering,\nESEC/FSE 2022, Singapore, Singapore, November\n14-18, 2022, pages 18–30. ACM.\n4992\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. CoRR, abs/2110.14168.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nAnna Derezi´nska and Konrad Hałas. 2014. Operators\nfor mutation testing of python programs. Res. Rep.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada, pages\n13042–13054.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020\nof Findings of ACL, pages 1536–1547. Association\nfor Computational Linguistics.\nAlex Graves, Greg Wayne, and Ivo Danihelka. 2014.\nNeural turing machines. CoRR, abs/1410.5401.\nAlex Graves, Greg Wayne, Malcolm Reynolds,\nTim Harley, Ivo Danihelka, Agnieszka Grabska-\nBarwinska, Sergio Gomez Colmenarejo, Edward\nGrefenstette, Tiago Ramalho, John P. Agapiou,\nAdrià Puigdomènech Badia, Karl Moritz Hermann,\nYori Zwols, Georg Ostrovski, Adam Cain, Helen\nKing, Christopher Summerfield, Phil Blunsom, Ko-\nray Kavukcuoglu, and Demis Hassabis. 2016. Hybrid\ncomputing using a neural network with dynamic ex-\nternal memory. Nat., 538(7626):471–476.\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming\nZhou, and Jian Yin. 2022. Unixcoder: Unified cross-\nmodal pre-training for code representation. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 7212–7225. Association for Computa-\ntional Linguistics.\nDaya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu\nTang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-\natkovskiy, Shengyu Fu, Michele Tufano, Shao Kun\nDeng, Colin B. Clement, Dawn Drain, Neel Sundare-\nsan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021.\nGraphcodebert: Pre-training code representations\nwith data flow. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nRichard G. Hamlet. 1977. Testing programs with the aid\nof a compiler. IEEE Trans. Software Eng., 3(4):279–\n290.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and Ja-\ncob Steinhardt. 2021. Measuring mathematical prob-\nlem solving with the MATH dataset. In Proceedings\nof the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets\nand Benchmarks 2021, December 2021, virtual.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B. Brown, Prafulla Dhariwal, Scott Gray, Chris\nHallacy, Benjamin Mann, Alec Radford, Aditya\nRamesh, Nick Ryder, Daniel M. Ziegler, John Schul-\nman, Dario Amodei, and Sam McCandlish. 2020.\nScaling laws for autoregressive generative modeling.\nCoRR, abs/2010.14701.\nAbram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel,\nand Premkumar T. Devanbu. 2012. On the natural-\nness of software. In 34th International Conference\non Software Engineering, ICSE 2012, June 2-9, 2012,\nZurich, Switzerland, pages 837–847. IEEE Computer\nSociety.\n4993\nYue Jia and Mark Harman. 2011. An analysis and sur-\nvey of the development of mutation testing. IEEE\nTrans. Software Eng., 37(5):649–678.\nLukasz Kaiser and Ilya Sutskever. 2016. Neural gpus\nlearn algorithms. In 4th International Conference\non Learning Representations, ICLR 2016, San Juan,\nPuerto Rico, May 2-4, 2016, Conference Track Pro-\nceedings.\nAditya Kanade, Petros Maniatis, Gogul Balakrishnan,\nand Kensen Shi. 2020. Learning and evaluating con-\ntextual embedding of source code. In Proceedings of\nthe 37th International Conference on Machine Learn-\ning, ICML 2020, 13-18 July 2020, Virtual Event ,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 5110–5121. PMLR.\nKarol Kurach, Marcin Andrychowicz, and Ilya\nSutskever. 2016. Neural random-access machines.\nIn 4th International Conference on Learning Repre-\nsentations, ICLR 2016, San Juan, Puerto Rico, May\n2-4, 2016, Conference Track Proceedings.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 158–167. Association\nfor Computational Linguistics.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch. 2022. Frozen pretrained transformers as uni-\nversal computation engines. In Thirty-Sixth AAAI\nConference on Artificial Intelligence, AAAI 2022,\nThirty-Fourth Conference on Innovative Applications\nof Artificial Intelligence, IAAI 2022, The Twelveth\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2022 Virtual Event, February 22 -\nMarch 1, 2022, pages 7628–7636. AAAI Press.\nMaxwell I. Nye, Anders Johan Andreassen, Guy Gur-\nAri, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten\nBosma, David Luan, Charles Sutton, and Augustus\nOdena. 2021. Show your work: Scratchpads for inter-\nmediate computation with language models. CoRR,\nabs/2112.00114.\nRuchir Puri, David S. Kung, Geert Janssen, Wei Zhang,\nGiacomo Domeniconi, Vladimir Zolotov, Julian\nDolby, Jie Chen, Mihir R. Choudhury, Lindsey\nDecker, Veronika Thost, Luca Buratti, Saurabh Pujar,\nShyam Ramji, Ulrich Finkler, Susan Malaika, and\nFrederick Reiss. 2021. Codenet: A large-scale AI for\ncode dataset for learning a diversity of coding tasks.\nIn Proceedings of the Neural Information Process-\ning Systems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nScott E. Reed and Nando de Freitas. 2016. Neural\nprogrammer-interpreters. In 4th International Con-\nference on Learning Representations, ICLR 2016,\nSan Juan, Puerto Rico, May 2-4, 2016, Conference\nTrack Proceedings.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020. Intellicode compose:\ncode generation using transformer. In ESEC/FSE\n’20: 28th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of\nSoftware Engineering, Virtual Event, USA, November\n8-13, 2020, pages 1433–1443. ACM.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transformers.\nIn 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComputing Surveys, 55(6):1–28.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nPetar Velickovic, Lars Buesing, Matthew C. Overlan,\nRazvan Pascanu, Oriol Vinyals, and Charles Blun-\ndell. 2020a. Pointer graph networks. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nPetar Velickovic, Rex Ying, Matilde Padovano, Raia\nHadsell, and Charles Blundell. 2020b. Neural ex-\necution of graph algorithms. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\n4994\nXin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao\nWan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang.\n2021a. Syncobert: Syntax-guided multi-modal con-\ntrastive pre-training for code representation. arXiv\npreprint arXiv:2108.04556.\nYu Wang, Ke Wang, Fengjuan Gao, and Linzhang Wang.\n2020. Learning semantic program embeddings with\ngraph interval neural network. Proc. ACM Program.\nLang., 4(OOPSLA):137:1–137:27.\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021b. Codet5: Identifier-aware uni-\nfied pre-trained encoder-decoder models for code\nunderstanding and generation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 8696–8708. Association for Computa-\ntional Linguistics.\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy\nRanganathan, and Milad Hashemi. 2020. Neural\nexecution engines: Learning to execute subroutines.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nWojciech Zaremba and Ilya Sutskever. 2014. Learning\nto execute. CoRR, abs/1410.4615.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron C.\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022. Teaching algorithmic reasoning via in-context\nlearning. CoRR, abs/2211.09066.\nA Dataset Detail\nTo obtain executable programs, we build the Python\nCode Execution dataset based on submissions to\ncompetitive programming problems from CodeNet\n(Puri et al., 2021). These human-written programs\nwith real-world complexity are derived from online\njudge websites AIZU 6 and AtCoder 7. CodeNet\ncontains 240k Python submissions, aiming to solve\n8,00 distinct programming problems. Each sub-\nmission is a single-file Python program that reads\nfrom stdin and writes to stdout. Each programming\nproblem provides at least one sample input and at\nmost four sample inputs. Since executing a pro-\ngram relies on an input, we replace the statements\nthat read from input streams with assignment state-\nments that assign input values to variables. We\nrun each submission in a sandbox environment to\nget the execution trace for that program. Programs\nare restricted to one second of execution time and\n1024 lines of execution trace, and will be filtered\n6https://onlinejudge.u-aizu.ac.jp/\n7https://atcoder.jp/\nout if they exceed the limits. We also remove the\nprograms that result in runtime errors during pars-\ning or execution, by catching Python exceptions\nraised in programs. This results in a dataset of 387k\nexecutable programs, each paired with a trace.\nTo construct a large-scale dataset of executable\nprograms, we propose a mutation-based data aug-\nmentation approach. we first present a set of muta-\ntion operators as shown in Table 1. Most of them\ncorrespond to selected operators used in strongly\ntyped general purpose languages and are adopted\nto the Python language. Operators designed for\nPython features are also included, such as Slice\nIndex Removal (SIR) and Reverse Iteration Loop\n(RIL). Then we leverage the tree-sitter8 to convert\na program into an abstract syntax tree and then ex-\ntract its node type information to get a candidate\nlist of all mutable literals, operators and statements.\nFor each mutable candidate, we apply the related\nmutation operators with 50% probability. Specifi-\ncally, we change a numeric literal x into a random\nnumber from a Gaussian distribution with mean\nx and standard deviation 100. We either extend a\nstring with one or two random characters or shorten\na string. We randomly pick one of the three loop-\nrelated operators or keep it as it is when handling\neach loop. All operators can be applied before a\nmutated program execution, and possible mutants\nwith errors are to be detected and eliminated during\nexecution. By mutating each program 20 times, we\nobtain 3.2M deduplicated programs, each paired\nwith a trace.\nWe use the CodeNet Mutants (CodeNetMut) to\nbuild the pre-training dataset. To prevent data leak-\nage, all submissions to the same problem become\npart of the same split. We use submissions of\n710 problems with their mutants to build the pre-\ntraining dataset. Since mutation greatly enhances\ndiversity, these programs embody rich semantics\nand complex operations. Other submissions (with-\nout mutations) are used to build the validation and\ntest dataset. These human-authored programs en-\nsure the quality of evaluation data.\nB Model Configurations\nWe build our model based on 12 layers of Trans-\nformer with 768 dimensional hidden states and 12\nattention heads. We add 210 additional special\ntokens into the vocabulary to represent 200 line\nnumbers, 3 pre-training dataset names, and trace\n8https://tree-sitter.github.io/tree-sitter/\n4995\nModel Stage1 (S1) Stage2 (S2) Stage3 (S3)\nCEL SingleLine Tutorial CodeNetMut\nCodeExecutor SingleLine SingleLine (3M), Tutorial SingleLine (3M), Tutorial, CodeNetMut\nTable 7: Datasets that CEL and CodeExecutor use for three-stage pre-training. “SingleLine (3M)” denotes 3 million\ninstances within SingleLine that are most difficult for CodeExecutor to generate.\nFigure 4: An example from CodeNetMut test split, which covers all the categories of Python programming\nknowledge. CodeExecutor gives the correct prediction.\nstructure described in §4.2. During pre-training,\nwe set the max length of input sequence and batch\nsize to be 1024 and 256, respectively. We use the\nAdam optimizer to update model parameters with\n4e-4 learning rate. We first employ the SingleLine\ndataset to pre-train the model with the code execu-\ntion objective for 500k steps. We then reserve 3\nmillion instances in SingleLine that are most diffi-\ncult for our model to generate and add Tutorial data\ninto the corpus, pre-training for 300k steps. We add\nCodeNetMut into the corpus and further pre-train\nfor 300k steps. We pre-train the model on a cluster\nof 16 NVIDIA Tesla V100 with 32GB memory\nand the total training time is about a month. For\ninference, we set beam search as 10.\nC Three-stage Pre-training\nIn table 7, we list the datasets that CodeExecutor-\nLimited (CEL) and CodeExecutor use for three-\nstage pre-training, respectively.\nThe first stage of pre-training for CEL uses the\nSingleLine dataset, resulting in the model CEL-S1.\nIn the second stage, CEL is initialized with CEL-S1\nand pre-trained with the Tutorial dataset, resulting\nin the model CEL-S2. In the third stage, CEL is\ninitialized with CEL-S2 and pre-trained with the\nCodeNetMut dataset, resulting in the model CEL-\nS3.\nOn the other hand, CodeExecutor is first pre-\ntrained with the SingleLine dataset, then the 3 mil-\nlion most challenging SingleLine data is selected\nfor later training stages based on the model’s loss.\nIn the second stage, CodeExecutor is pre-trained\nwith the 3 million difficult SingleLine data, along\nwith the Tutorial dataset. In the third stage, Code-\nExecutor is pre-trained with the 3 million difficult\nSingleLine data, the entire Tutorial dataset, and the\nCodeNetMut dataset.\nD Qualitative Examples\nAdditional examples are shown here.\nFigure 4 shows an example that covers all the\ncategories of Python programming knowledge in\nTable 4. CodeExecutor generates the same trace as\nground truth.\nFigure 5 is an example of performing division\ncalculations with decimals. CodeExecutor is able\nto produce the correct first fifteen digits and makes\nerrors in the remaining two digits.\n4996\nFigure 5: An example of division calculations with dec-\nimals, where CodeExecutor correctly produce the first\nfifteen digits, with mistakes highlighted by an underline.\n4997\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThe \"Limitations” section, which is after the conclusion.\n□\u0013 A2. Did you discuss any potential risks of your work?\nThe \"Ethical Statement” section, which is before the references.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and the ﬁrst section.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3, 4 and 5.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3, 4 and 5.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection 3, 4 and 5.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 3, 4 and 5.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe \"Ethical Statement” section, which is before the references.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 5.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 5.\nC □\u0013 Did you run computational experiments?\nSection 5 and 6.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4998\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5 and appendix B.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 6.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3 and appendix A.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 5.\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 5 and a handbook in the data package of supplemental material.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 5, the \"Ethical Statement” section and a handbook in the data package of supplemental\nmaterial.\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nSection 5 and a handbook in the data package of supplemental material.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n4999",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8882925510406494
    },
    {
      "name": "Programming language",
      "score": 0.6892392039299011
    },
    {
      "name": "KPI-driven code analysis",
      "score": 0.6207356452941895
    },
    {
      "name": "Python (programming language)",
      "score": 0.5969834923744202
    },
    {
      "name": "Code generation",
      "score": 0.5488288998603821
    },
    {
      "name": "Code (set theory)",
      "score": 0.47395849227905273
    },
    {
      "name": "Source code",
      "score": 0.4662111699581146
    },
    {
      "name": "Code review",
      "score": 0.45353129506111145
    },
    {
      "name": "Static program analysis",
      "score": 0.4383566379547119
    },
    {
      "name": "Redundant code",
      "score": 0.4376324415206909
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37955692410469055
    },
    {
      "name": "Software",
      "score": 0.20987790822982788
    },
    {
      "name": "Software development",
      "score": 0.18723368644714355
    },
    {
      "name": "Operating system",
      "score": 0.0933331549167633
    },
    {
      "name": "Key (lock)",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 15
}