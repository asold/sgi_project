{
  "title": "Dense Transformer Networks for Brain Electron Microscopy Image Segmentation",
  "url": "https://openalex.org/W2965279912",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5100362041",
      "name": "Jun Li",
      "affiliations": [
        "Washington State University"
      ]
    },
    {
      "id": "https://openalex.org/A5100371672",
      "name": "Yongjun Chen",
      "affiliations": [
        "Washington State University"
      ]
    },
    {
      "id": "https://openalex.org/A5081396646",
      "name": "Lei Cai",
      "affiliations": [
        "Washington State University"
      ]
    },
    {
      "id": "https://openalex.org/A5030289182",
      "name": "Ian Davidson",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A5052278550",
      "name": "Shuiwang Ji",
      "affiliations": [
        "Mitchell Institute",
        "Texas A&M University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1889898024",
    "https://openalex.org/W2128409098",
    "https://openalex.org/W2259303769",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2022508996",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W1905829557",
    "https://openalex.org/W2279221249",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W603908379",
    "https://openalex.org/W4298236736",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2953133772",
    "https://openalex.org/W4297688279",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W2963542991",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W1910657905",
    "https://openalex.org/W2167510172",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W2963591054"
  ],
  "abstract": "The key idea of current deep learning methods for dense prediction is to apply a model on a regular patch centered on each pixel to make pixel-wise predictions. These methods are limited in the sense that the patches are determined by network architecture instead of learned from data. In this work, we propose the dense transformer networks, which can learn the shapes and sizes of patches from data. The dense transformer networks employ an encoder-decoder architecture, and a pair of dense transformer modules are inserted into each of the encoder and decoder paths. The novelty of this work is that we provide technical solutions for learning the shapes and sizes of patches from data and efficiently restoring the spatial correspondence required for dense prediction. The proposed dense transformer modules are differentiable, thus the entire network can be trained. We apply the proposed networks on biological image segmentation tasks and show superior performance is achieved in comparison to baseline methods.",
  "full_text": "Dense Transformer Networks for Brain Electron Microscopy Image Segmentation\nJun Li1 , Yongjun Chen1 , Lei Cai1 , Ian Davidson2 and Shuiwang Ji3\u0003\n1Washington State University\n2University of California, Davis\n3Texas A&M University\nfjun.li3, yongjun.chen, lei.caig@wsu.edu, davidson@cs.ucdavis.edu, sji@tamu.edu\nAbstract\nThe key idea of current deep learning methods for\ndense prediction is to apply a model on a regu-\nlar patch centered on each pixel to make pixel-\nwise predictions. These methods are limited in the\nsense that the patches are determined by network\narchitecture instead of learned from data. In this\nwork, we propose the dense transformer networks,\nwhich can learn the shapes and sizes of patches\nfrom data. The dense transformer networks em-\nploy an encoder-decoder architecture, and a pair of\ndense transformer modules are inserted into each\nof the encoder and decoder paths. The novelty of\nthis work is that we provide technical solutions for\nlearning the shapes and sizes of patches from data\nand efﬁciently restoring the spatial correspondence\nrequired for dense prediction. The proposed dense\ntransformer modules are differentiable, thus the en-\ntire network can be trained. We apply the proposed\nnetworks on biological image segmentation tasks\nand show superior performance is achieved in com-\nparison to baseline methods.\n1 Introduction\nIn recent years, deep convolution neural networks (CNNs)\nhave achieved promising performance on many artiﬁcial in-\ntelligence tasks, including image recognition [LeCun et al.,\n1998], object detection [Sermanet et al., 2014], and segmen-\ntation [Chen et al., 2015]. Among these tasks, dense predic-\ntion tasks take images as inputs and generate output maps\nwith similar or the same size as the inputs. For example,\nin image semantic segmentation, we need to predict a la-\nbel for each pixel on the input images [Long et al., 2015;\nNoh et al., 2015 ]. Other examples include depth estima-\ntion [Laina et al., 2016], image super-resolution [Dong et\nal., 2016], and surface normal prediction [Eigen and Fergus,\n2015]. These tasks can be generally considered as image-to-\nimage translation problems in which inputs are images, and\noutputs are label maps [Isola et al., 2017].\nGiven the success of deep learning methods on image-\nrelated applications, numerous recent attempts have been\n\u0003Contact Author\nmade to solve dense prediction problems using CNNs. A\ncentral idea of these methods is to extract a square patch\ncentered on each pixel and apply CNNs on each of them to\ncompute the label of the center pixel. The efﬁciency of these\napproaches can be improved by using fully convolutional or\nencoder-decoder networks. Speciﬁcally, fully convolutional\nnetworks [Long et al., 2015] replace fully connected layers\nwith convolutional layers, thereby allowing inputs of arbi-\ntrary size during both training and test. In contrast, decon-\nvolution networks [Noh et al., 2015 ] employ an encoder-\ndecoder architecture. The encoder path extracts high-level\nrepresentations using convolutional and pooling layers. The\ndecoder path uses deconvolutional and up-pooling layers to\nrecovering the original spatial resolution. In order to trans-\nmit information directly from encoder to decoder, the U-\nNet [Ronneberger et al., 2015] adds skip connections [He et\nal., 2016 ] between the corresponding encoder and decoder\nlayers. A common property of all these methods is that the\nlabel of any pixel is determined by a regular (usually square)\npatch centered on that pixel. Although these methods have\nachieved considerable practical success, there are limitations\ninherent in them. For example, once the network architec-\nture is determined, the patches used to predict the label of\neach pixel is completely determined, and they are commonly\nof the same size for all pixels. In addition, the patches are\nusually of a regular shape, e.g., squares.\nIn this work, we propose the dense transformer networks\nto address these limitations. Our method follows the encoder-\ndecoder architecture in which the encoder converts input im-\nages into high-level representations, and the decoder tries to\nmake pixel-wise predictions by recovering the original spatial\nresolution. Under this framework, the label of each pixel is\nalso determined by a local patch on the input. Our method al-\nlows the size and shape of every patch to be adaptive and data-\ndependent. In order to achieve this goal, we propose to insert\na spatial transformer layer [Jaderberg et al., 2015] in the en-\ncoder part of our network. We propose to use nonlinear trans-\nformations, such as these based on thin-plate splines [Book-\nstein, 1989 ]. The nonlinear spatial transformer layer trans-\nforms the feature maps into a different space. Therefore, per-\nforming regular convolution and pooling operations in this\nspace corresponds to performing these operations on irregular\npatches of different sizes in the original space. Since the non-\nlinear spatial transformations are learned automatically from\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2894\ndata, this corresponds to learning the size and shape of each\npatch to be used as inputs for convolution and pooling opera-\ntions.\nThere has been prior work on allowing spatial transfor-\nmations or deformations in deep networks [Jaderberg et al.,\n2015; Dai et al., 2017], but they do not address the spatial\ncorrespondence problem, which is critical in dense predic-\ntion tasks. The difﬁculty in applying spatial transformations\nto dense prediction tasks lies in that the spatial correspon-\ndence between input images and output label maps needs to\nbe preserved. A key innovation of this work is that we provide\na new technical solution that not only allows data-dependent\nlearning of patches but also enables the preservation of spa-\ntial correspondence. Speciﬁcally, although the patches used\nto predict pixel labels could be of different sizes and shapes,\nwe expect the patches to be in the spatial vicinity of pixels\nwhose labels are to be predicted. By applying the nonlinear\nspatial transformer layers in the encoder path as described\nabove, the spatial locations of units on the intermediate fea-\nture maps after the spatial transformation layer may not be\npreserved. Thus a reverse transformation is required to re-\nstore the spatial correspondence.\nIn order to restore the spatial correspondence between in-\nputs and outputs, we propose to add a corresponding decoder\nlayer. A technical challenge in developing the decoder layer\nis that we need to map values of units arranged on input regu-\nlar grid to another set of units arranged on output grid, while\nthe nonlinear transformation could map input units to arbi-\ntrary locations on the output map. We develop a interpolation\nmethod to address this challenge. Altogether, our work re-\nsults in the dense transformer networks, which allow the pre-\ndiction of each pixel to adaptively choose the input patch in a\ndata-dependent manner. The dense transformer networks can\nbe trained end-to-end, and gradients can be back-propagated\nthrough both the encoder and decoder layers. Experimental\nresults on biological images demonstrate the effectiveness of\nthe proposed dense transformer networks.\n2 Spatial Transformer Networks Based on\nThin-Plate Spline\nSpatial transformer networks[Jaderberg et al., 2015] are deep\nmodels containing spatial transformer layers. These layers\nexplicitly compute a spatial transformation of the input fea-\nture maps. They can be inserted into convolutional neural net-\nworks to perform explicit spatial transformations. The spatial\ntransformer layers consist of three components; namely, the\nlocalization network, grid generator and sampler.\nThe localization network takes a set of feature maps as in-\nput and generates parameters to control the transformation. If\nthere are multiple feature maps, the same transformation is\napplied to all of them. The grid generator constructs trans-\nformation mapping between input and output grids based on\nparameters computed from the localization network. The\nsampler computes output feature maps based on input fea-\nture maps and the output of grid generator. The spatial trans-\nformer layers are generic and different types of transforma-\ntions, e.g., afﬁne transformation, projective transformation,\nand thin-plate spline (TPS), can be used. Our proposed work\nis based on the TPS transformation, and it is not described in\ndetail in the original paper [Jaderberg et al., 2015]. Thus we\nprovide more details below.\n2.1 Localization Network\nWhen there are multiple feature maps, the same transfor-\nmation is applied to all of them. Thus, we assume there\nis only one input feature map below. The TPS transforma-\ntion is determined by 2K ﬁducial points among which K\npoints lie on the input feature map and the other K points\nlie on the output feature map. On the output feature map,\nthe K ﬁducial points, whose coordinates are denoted as ~F =\n[ ~f1; ~f2; \u0001\u0001\u0001 ; ~fK] 2R2\u0002K, are evenly distributed on a ﬁxed\nregular grid, where ~fi = [~xi; ~yi]T denotes the coordinates of\nthe ith point. The localization network is used to learn the K\nﬁducial points F = [ f1; f2; \u0001\u0001\u0001 ; fK] 2R2\u0002K on the input\nfeature map. Speciﬁcally, the localization network, denoted\nas floc(\u0001), takes the input feature maps U 2RH\u0002W\u0002C as in-\nput, where H, W and C are the height, width and number of\nchannels of input feature maps, and generates the normalized\ncoordinates F as the output as F = floc(U).\nA cascade of convolutional, pooling and fully-connected\nlayers is used to implement floc(\u0001). The output of the ﬁnal\nfully-connected layer is the coordinatesF on the input feature\nmap. Therefore, the number of output units of the localization\nnetwork is 2K. In order to ensure that the outputs are normal-\nized between \u00001 and 1, the activation functiontanh(\u0001) is used\nin the fully-connected layer. Since the localization network is\ndifferentiable, the K ﬁducial points can be learned from data\nusing error back-propagation.\n2.2 Grid Generator\nFor each unit lying on a regular grid on the output fea-\nture map, the grid generator computes the coordinate of the\ncorresponding unit on the input feature map. This corre-\nspondence is determined by the coordinates of the ﬁducial\npoints F and ~F. Given the evenly distributed K points\n~F = [ ~f1; ~f2; \u0001\u0001\u0001 ; ~fK] on the output feature map and the K\nﬁducial points F = [f1; f2; \u0001\u0001\u0001 ; fK] generated by the local-\nization network, the transformation matrix T in TPS can be\nexpressed as follows:\nT =\n\u0012\n\u0001\u00001\n~F \u0002\n\u0014\nFT\n03\u00022\n\u0015\u0013T\n2R2\u0002(K+3); (1)\nwhere \u0001 ~F 2R(K+3)\u0002(K+3) is a matrix determined only by\n~F as\n\u0001 ~F =\n2\n4\n1K\u00021 ~FT R\n01\u00021 01\u00022 11\u0002K\n02\u00021 02\u00022 ~F\n3\n52R(K+3)\u0002(K+3); (2)\nwhere R 2RK\u0002K, and its elements are deﬁned as ri;j =\nd2\ni;j ln d2\ni;j, and di;j denotes the Euclidean distance between\n~fi and ~fj.\nThrough the mapping, each unit (~xi; ~yi) on the output fea-\nture map corresponds to unit(xi; yi) on the input feature map.\nTo achieve this mapping, we represent the units on the regular\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2895\nHigh-level \nFeature\nSpatial \nTransformer\nSpatial \nDecoder\nEncoder Path Decoder Path\nInput\nImage\nOutput\nLabel\nMap\nA B\nCD\nP\nS1S2S3S4T A B\nCD\nP\nT\nS3S4S2S1\nFigure 1: The proposed dense transformer networks.\noutput grid by f~pig\n~H\u0002 ~W\ni=1 , where ~pi = [~xi; ~yi]T is the (x; y)-\ncoordinates of the ith unit on output grid, and ~H and ~W are\nthe height and width of output feature maps. Note that the\nﬁducial points f~figK\ni=1 are a subset of the points f~pig\n~H\u0002 ~W\ni=1 ,\nwhich are the set of all points on the regular output grid.\nTo apply the transformation, each point ~pi is ﬁrst\nextended from R2 space to RK+3 space as ~qi =\n[1; ~xi; ~yi; si;1; si;2; \u0001\u0001\u0001 ; si;K]T 2 RK+3; where si;j =\ne2\ni;j ln e2\ni;j, and ei;j is the Euclidean distance between ~pi and\n~fj. Then the transformation can be expressed as\npi = T ~qi; (3)\nwhere T is deﬁned in Eq. (1). By this transformation, each\ncoordinate (~xi; ~yi) on the output feature map corresponds to\na coordinate (xi; yi) on the input feature map. Note that the\ntransformation T is deﬁned so that the points ~F map to points\nF.\n2.3 Sampler\nThe sampler generates output feature maps based on input\nfeature maps and the outputs of grid generator. Each unit\n~pi on the output feature map corresponds to a unit pi on the\ninput feature map as computed by Eq. (3). However, the co-\nordinates pi = ( xi; yi)T computed by Eq. (3) may not lie\nexactly on the input regular grid. In these cases, the output\nvalues need to be interpolated from input values lying on reg-\nular grid. For example, a bilinear sampling method can be\nused to achieve this. Speciﬁcally, given an input feature map\nU 2 RH\u0002W , the output feature map V 2 R ~H\u0002 ~W can be\nobtained as\nVi =\nHX\nn=1\nWX\nm=1\nUnm max(0; 1\u0000jxi\u0000mj) max(0; 1\u0000jyi\u0000nj)\n(4)\nfor i = 1 ; 2; \u0001\u0001\u0001 ; ~H \u0002 ~W, where Vi is the value of pixel\ni, Unm is the value at (n; m) on the input feature map,\npi = ( xi; yi)T , and pi is computed from Eq. (3). By us-\ning the transformations, the spatial transformer networks have\nbeen shown to be invariant to some transformations on the in-\nputs. Other recent studies have also attempted to make CNNs\nto be invariant to various transformations [Jia et al., 2016;\nHenriques and Vedaldi, 2016; Cohen and Welling, 2016;\nDieleman et al., 2016].\n3 Dense Transformer Networks\nThe central idea of CNN-based method for dense prediction\nis to extract a regular patch centered on each pixel and apply\nCNNs to compute the label of that pixel. A common property\nof these methods is that the label of each pixel is determined\nby a regular (typically square) patch centered on that pixel.\nAlthough these methods have been shown to be effective on\ndense prediction problems, they lack the ability to learn the\nsizes and shapes of patches in a data-dependent manner. For\na given network, the size of patches used to predict the labels\nof each center pixel is determined by the network architec-\nture. Although multi-scale networks have been proposed to\nallow patches of different sizes to be combined[Farabet et al.,\n2013], the patch sizes are again determined by network archi-\ntectures. In addition, the shapes of patches used in CNNs\nare invariably regular, such as squares. Ideally, the shapes\nof patches may depend on local image statistics around that\npixel and thus should be learned from data. In this work, we\npropose the dense transformer networks to enable the learn-\ning of patch size and shape for each pixel.\nAs illustrated in Figure 2, features extracted by the origi-\nnal convolutional operation corresponds to a regular path on\ninput feature maps. We employ a nonlinear TPS transforma-\ntion in our model. Therefore, a regular patch ef1 ef2 ef3 ef4 cor-\nresponds to an area f1f2f3f4 with different shape and size.\nThe parameters of nonlinear transformation are learned by a\nnetwork based on inputs. Therefore, our model can learn an\nappropriate transformation to achieve better performance. To\ntackle the spatial correspondence problem, a reverse trans-\nformation is used to restore the spatial correspondence. The\nreverse TPS transformation share parameters with the TPS\ntransformation in the encoder network.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2896\nFigure 2: Illustration of the dense transformer networks.\n3.1 An Encoder-Decoder Architecture\nIn order to address the above limitations, we propose to de-\nvelop a dense transformer network model. Our model em-\nploys an encoder-decoder architecture in which the encoder\npath extracts high-level representations using convolutional\nand pooling layers and the decoder path uses deconvolution\nand un-pooling to recover the original spatial resolution[Noh\net al., 2015; Ronnebergeret al., 2015; Badrinarayananet al.,\n2015]. To enable the learning of size and shape of each patch\nautomatically from data, we propose to insert a spatial trans-\nformer module in the encoder path in our network. As has\nbeen discussed above, the spatial transformer module trans-\nforms the feature maps into a different space using nonlinear\ntransformations. Applying convolution and pooling opera-\ntions on regular patches in the transformed space is equiv-\nalent to operating on irregular patches of different sizes in\nthe original space. Since the spatial transformer module is\ndifferentiable, its parameters can be learned with error back-\npropagation algorithms. This is equivalent to learning the size\nand shape of each patch from data.\nAlthough the patches used to predict pixel labels could be\nof different sizes and shapes, we expect the patches to include\nthe pixel in question at least. That is, the patches should be\nin the spatial vicinity of pixels whose labels are to be pre-\ndicted. By using the nonlinear spatial transformer layer in\nencoder path, the spatial locations of units on the intermedi-\nate feature maps could have been changed. That is, due to this\nnonlinear spatial transformation, the spatial correspondence\nbetween input images and output label maps is not retained\nin the feature maps after the spatial transformer layer. In or-\nder to restore this spatial correspondence, we propose to add a\ncorresponding decoder layer, known as the dense transformer\ndecoder layer. This decoder layer transforms the intermedi-\nate feature maps back to the original input space, thereby re-\nestablishing the input-output spatial correspondence.\nThe spatial transformer module can be inserted after any\nlayer in the encoder path while the dense transform decoder\nmodule should be inserted into the corresponding location\nin decoder path. In our framework, the spatial transformer\nmodule is required to not only output the transformed feature\nmaps, but also the transformation itself that captures the spa-\ntial correspondence between input and output feature maps.\nThis information will be used to restore the spatial corre-\nspondence in the decoder module. Note that in the spatial\ntransformer encoder module, the transformation is computed\nin the backward direction, i.e., from output to input feature\nmaps (Figure 1). In contrast, the dense transformer decoder\nmodule uses a forward direction instead; that is, a mapping\nfrom input to output feature maps. This encoder-decoder pair\ncan be implemented efﬁciently by sharing the transformation\nparameters in these two modules.\nA technical challenge in developing the dense transformer\ndecoder layer is that we need to map values of units arranged\non input regular grid to another set of units arranged on reg-\nular output grid, while the decoder could map to units at ar-\nbitrary locations on the output map. That is, while we need\nto compute the values of units lying on regular output grid\nfrom values of units lying on regular input grid, the mapping\nitself could map an input unit to an arbitrary location on the\noutput feature map, i.e., not necessarily to a unit lying exactly\non the output grid. To address this challenge, we develop a\nsampler method for performing interpolation. We show that\nthe proposed samplers are differentiable, thus gradients can\nbe propagated through these modules. This makes the entire\ndense transformer networks fully trainable. Formally, assume\nthat the encoder and decoder layers are inserted after the i-th\nand j-th layers, respectively, then we have the following rela-\ntionships:\nUi+1(p) = SamplingfUi(Tp)g; Uj+1(Tp) = Uj(p);\nUj+1(p) = SamplingfUj+1(Tp)g; (5)\nwhere Ui is the feature map of the i-th layer, p is the coor-\ndinate of a point, T is the transformation deﬁned in Eq. (1),\nwhich maps from the coordinates of the(i+1)-th layer to the\ni-th layer, Sampling(\u0001) denotes the sampler function.\nFrom a geometric perspective, a value associated with an\nestimated point in bilinear interpolation in Eq. (4) can be in-\nterpreted as a linear combination of values at four neighbor-\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2897\ning grid points. The weights for linear combination are ar-\neas of rectangles determined by the estimated points and four\nneighboring grid points. For example, in Figure 1, when a\npoint is mapped toP on input grid, the contributions of points\nA, B, C, and D to the estimated point P is determined by the\nareas of the rectanglesS1, S2, S3, and S4. However, the inter-\npolation problem needs to be solved in the dense transformer\ndecoder layer is different with the one in the spatial trans-\nformer encoder layer, as illustrated in Figure 1. Speciﬁcally,\nin the encoder layer, the points A, B, C, and D are associ-\nated with values computed from the previous layer, and the\ninterpolation problem needs to compute a value for P to be\npropagated to the next layer. In contrast, in the decoder layer,\nthe point P is associated with a value computed from the pre-\nvious layer, and the interpolation problem needs to compute\nvalues for A, B, C, and D. Due to the different natures of\nthe interpolation problems need to be solved in the encoder\nand decoder modules, we propose a new sampler that can ef-\nﬁciently interpolate over decimal points in the following sec-\ntion.\n3.2 Decoder Sampler\nIn the decoder sampler, we need to estimate values of regular\ngrid points based on those from arbitrary decimal points, i.e.,\nthose that do not lie on the regular grid. For example, in Fig-\nure 1, the value at point P is given from the previous layer.\nAfter the TPS transformation in Eq. (3), it may be mapped\nto an arbitrary point. Therefore, the values of grid points A,\nB, C, and D need to be computed based on values from a set\nof arbitrary points. If we compute the values from surround-\ning points as in the encoder layer, we might have to deal with\na complex interpolation problem over irregular quadrilater-\nals. Those complex interpolation methods may yield more\naccurate results, but we prefer a simpler and more efﬁcient\nmethod in this work. Speciﬁcally, we propose a new sam-\npling method, which distributes the value of P to the points\nA, B, C, and D in an intuitive manner. Geometrically, the\nweights associated with points A, B, C, and D are the area\nof the rectangles S1, S2, S3, and S4, respectively (Figure 1).\nIn particular, given an input feature map V 2R ~H\u0002 ~W , the\noutput feature map U 2RH\u0002W can be obtained as\nSnm =\n~H\u0002 ~WX\ni=1\nmax(0; 1 \u0000jx i \u0000mj)\n\u0002max(0; 1 \u0000jy i \u0000nj); (6)\nUnm = 1\nSnm\n~H\u0002 ~WX\ni=1\nVi max(0; 1 \u0000jx i \u0000mj)\n\u0002max(0; 1 \u0000jy i \u0000nj); (7)\nwhere Vi is the value of pixel i, pi = (xi; yi)T is transformed\nby the shared transformation T in Eq. (1), Unm is the value\nat the (n; m)-th location on the output feature map,Snm is a\nnormalization term that is used to eliminate the effect that dif-\nferent grid points may receive values from different numbers\nof arbitrary points, and n = 1; 2; \u0001\u0001\u0001 ; N; m= 1; 2; \u0001\u0001\u0001 ; M.\nMore details are given in Figure 2.\nIn order to allow the backpropagation of errors, we deﬁne\nthe gradient with respect to Unm as dUnm. Then the gradient\nwith respect to Vnm and xi can be derived as follows:\ndVi =\nHX\nn=1\nWX\nm=1\n1\nSnm\ndUnm max(0; 1 \u0000jx i \u0000mj)\n\u0002max(0; 1 \u0000jy i \u0000nj); (8)\ndSnm =\u0000dUnm\nS2nm\n~H\u0002 ~WX\ni=1\nVi max(0; 1 \u0000jx i \u0000mj)\n\u0002max(0; 1 \u0000jy i \u0000nj); (9)\ndxi =\nHX\nn=1\nWX\nm=1\n\u001adUnm\nSnm\nVi + dSnm\n\u001b\nmax(0; 1 \u0000jy i \u0000nj)\n\u0002\n( 0 if jm\u0000xij\u0015 1\n1 if m \u0015xi\n\u00001 if m \u0014xi\n: (10)\nA similar gradient can be derived for dyi. This provides us\nwith a differentiable sampling mechanism, which enables the\ngradients ﬂow back to both the input feature map and the sam-\npling layers.\n4 Experimental Evaluation\nWe evaluate the proposed methods on two image segmenta-\ntion tasks. The U-Net [Ronneberger et al., 2015] is adopted\nas our base model in both tasks, as it has achieved state-of-\nthe-art performance on image segmentation tasks. Speciﬁ-\ncally, U-Net adds residual connections between the encoder\npath and decoder path to incorporate both low-level and high-\nlevel features. Other methods like SegNet [Badrinarayanan\net al., 2015], deconvolutional networks [Zeiler et al., 2010]\nand FCN [Long et al., 2015] mainly differ from U-Net in\nthe up-sampling method and do not use residual connec-\ntions. Experiments in prior work [Ronneberger et al., 2015;\nZeiler et al., 2010; Longet al., 2015] show that residual con-\nnections are important while different up-sampling methods\nlead to similar results. The network consists of 5 layers in the\nencoder path and another corresponding 5 layers in the de-\ncoder path. A stack of two 3\u00023 convolutional layers have the\nsame receptive ﬁeld as a 5\u00025 convolutional layer, but with\nless parameters [Simonyan and Zisserman, 2015]. Therefore,\nwe use 3\u00023 kernels and one pixel padding to retain the size\nof feature maps at each level.\nIn order to efﬁciently implement the transformations, we\ninsert the spatial encoder layer and dense transformer decoder\nlayer into corresponding positions at the same level. Using\nspatial transformation layers at a deep position in the encoder-\ndecoder network can increase the receptive ﬁeld of the spatial\ntransformation operation. Therefore, the layers are applied\nto the 4th layer, and their performance is compared to the\nbasic U-Net model without spatial transformations. As for\nthe transformation layers, we use 16 ﬁducial points that are\nevenly distributed on the output feature maps. In the dense\ntransformer decoder layer, if there are pixels that are not se-\nlected on the output feature map, we apply an interpolation\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2898\nRaw image Ground truth U-Net output DTN output\nFigure 3: Example results generated by the U-Net and the proposed DTN models for the SNEMI3D data set.\nDATA SET MODEL TRAINING PREDICTION\nSNEMI3D U-N ET 14M18S 3M31S\nDTN 15 M41S 4M02S\nTable 1: Training and prediction time on the two data sets using a\nTesla K40 GPU. We compare the training time of 10,000 iterations\nand prediction time of 40 (SNEMI3D) images for the base U-Net\nmodel and the DTN.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nFalse Positive Rate\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nTrue Positive Rate\nU-Net\nDTN\nU-Net AUC: 0.86761\nDTN AUC: 0.89532 \nFigure 4: Comparison of the ROC curves of the U-Net and the pro-\nposed DTN model on the SNEMI3D data set.\nstrategy over its neighboring pixels on previous feature maps\nto produce smooth results.\n4.1 Brain Electron Microscopy Image\nSegmentation\nWe evaluate the proposed methods on brain electron mi-\ncroscopy (EM) image segmentation task [Lee et al., 2015;\nCiresan et al., 2012 ], in which the ultimate goal is to re-\nconstruct neurons at the micro-scale level. A critical step in\nneuron reconstruction is to segment the EM images. We use\ndata set from the 3D Segmentation of Neurites in EM Im-\nages (SNEMI3D, http://brainiac2.mit.edu/SNEMI3D/). The\nSNEMI3D data set consists of 100 1024\u00021024 EM image\nslices. Since we perform 2D transformations in this work,\neach image slice is segmented separately in our experiments.\nThe task is to predict each pixel as either a boundary (denoted\nas 1) or a non-boundary pixel (denoted as 0).\nOur model can process images of arbitrary size. How-\never, training on whole images may incur excessive mem-\nory requirement. In order to accelerate training, we randomly\npick 224\u0002224 patches from the original images and use it\nto train the networks. The experimental results in terms of\nROC curves are provided in Figure 4. We can observe that\nthe proposed DTN model achieves higher performance than\nthe baseline U-Net model, improving AUC from 0.8676 to\n0.8953. These results demonstrate that the proposed DTN\nmodel improves upon the baseline U-Net model, and the use\nof the dense transformer encoder and decoder modules in the\nU-Net architecture results in improved performance. Some\nexample results along with the raw images and ground truth\nlabel maps are given in Figure 3.\n4.2 Timing Comparison\nTable 1 shows the comparison of training and prediction time\nbetween the U-Net model and the proposed DTN model on\nthe two data sets. We can see that adding DTN layers leads to\nonly slight increase in training and prediction time.\n5 Conclusion\nIn this work, we propose the dense transformer networks to\nenable the automatic learning of patch sizes and shapes in\ndense prediction tasks. This is achieved by transforming the\nintermediate feature maps to a different space using nonlinear\ntransformations. A unique challenge in dense prediction tasks\nis that, the spatial correspondence between inputs and outputs\nshould be preserved in order to make pixel-wise predictions.\nTo this end, we develop the dense transformer decoder layer\nto restore the spatial correspondence. The proposed dense\ntransformer modules are differentiable. Thus the entire net-\nwork can be trained from end to end. Experimental results\nshow that adding the spatial transformer and decoder layers\nto existing models leads to improved performance. To the\nbest of our knowledge, our work represents the ﬁrst attempt\nto enable the learning of patch size and shape in dense pre-\ndiction. The current study only adds one encoder layer and\none decoder layer in the baseline models. We will explore\nthe possibility of adding multiple encoder and decoder layers\nat different locations of the baseline model. In this work, we\ndevelop a simple and efﬁcient decoder sampler for interpo-\nlation. A more complex method based on irregular quadri-\nlaterals might be more accurate and will be explored in the\nfuture.\nAcknowledgments\nThis work was supported in part by National Science Foun-\ndation grants IIS-1615035 and DBI1641223.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2899\nReferences\n[Badrinarayanan et al., 2015] Vijay Badrinarayanan, Alex\nKendall, and Roberto Cipolla. Segnet: A deep convolu-\ntional encoder-decoder architecture for image segmenta-\ntion. arXiv preprint arXiv:1511.00561, 2015.\n[Bookstein, 1989] Fred L. Bookstein. Principal warps: Thin-\nplate splines and the decomposition of deformations.IEEE\nTransactions on pattern analysis and machine intelli-\ngence, 11(6):567–585, 1989.\n[Chen et al., 2015] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-\nmantic image segmentation with deep convolutional nets\nand fully connected CRFs. In Proceedings of the Interna-\ntional Conference on Learning Representations, 2015.\n[Ciresan et al., 2012] Dan Ciresan, Alessandro Giusti,\nLuca M Gambardella, and J ¨urgen Schmidhuber. Deep\nneural networks segment neuronal membranes in electron\nmicroscopy images. In Advances in neural information\nprocessing systems, pages 2843–2851, 2012.\n[Cohen and Welling, 2016] Taco Cohen and Max Welling.\nGroup equivariant convolutional networks. InProceedings\nof The 33rd International Conference on Machine Learn-\ning, pages 2990–2999, 2016.\n[Dai et al., 2017] Jifeng Dai, Haozhi Qi, Yuwen Xiong,\nYi Li, Guodong Zhang, Han Hu, and Yichen Wei.\nDeformable convolutional networks. arXiv preprint\narXiv:1703.06211, 2017.\n[Dieleman et al., 2016] Sander Dieleman, Jeffrey De Fauw,\nand Koray Kavukcuoglu. Exploiting cyclic symmetry in\nconvolutional neural networks. InProceedings of The 33rd\nInternational Conference on Machine Learning, pages\n1889–1898, 2016.\n[Dong et al., 2016] Chao Dong, Chen Change Loy, Kaiming\nHe, and Xiaoou Tang. Image super-resolution using deep\nconvolutional networks. IEEE transactions on pattern\nanalysis and machine intelligence, 38(2):295–307, 2016.\n[Eigen and Fergus, 2015] David Eigen and Rob Fergus. Pre-\ndicting depth, surface normals and semantic labels with a\ncommon multi-scale convolutional architecture. In Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision, pages 2650–2658, 2015.\n[Farabet et al., 2013] Clement Farabet, Camille Couprie,\nLaurent Najman, and Yann LeCun. Learning hierarchi-\ncal features for scene labeling. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 35(8):1915–1929,\n2013.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016.\n[Henriques and Vedaldi, 2016] Jo˜ao F Henriques and An-\ndrea Vedaldi. Warped convolutions: Efﬁcient invariance to\nspatial transformations. arXiv preprint arXiv:1609.04382,\n2016.\n[Isola et al., 2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou,\nand Alexei A Efros. Image-to-image translation with con-\nditional adversarial networks. arXiv preprint, 2017.\n[Jaderberg et al., 2015] Max Jaderberg, Karen Simonyan,\nAndrew Zisserman, et al. Spatial transformer networks. In\nAdvances in neural information processing systems, pages\n2017–2025, 2015.\n[Jia et al., 2016] Xu Jia, Bert De Brabandere, Tinne Tuyte-\nlaars, and Luc V Gool. Dynamic ﬁlter networks. In Ad-\nvances in Neural Information Processing Systems, pages\n667–675, 2016.\n[Laina et al., 2016] Iro Laina, Christian Rupprecht, Vasileios\nBelagiannis, Federico Tombari, and Nassir Navab. Deeper\ndepth prediction with fully convolutional residual net-\nworks. In 3D Vision (3DV), 2016 Fourth International\nConference on, pages 239–248. IEEE, 2016.\n[LeCun et al., 1998] Y . LeCun, L. Bottou, Y . Bengio, and\nP. Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324,\nNovember 1998.\n[Lee et al., 2015] Kisuk Lee, Aleksandar Zlateski, Vish-\nwanathan Ashwin, and H Sebastian Seung. Recursive\ntraining of 2D-3D convolutional networks for neuronal\nboundary prediction. In Advances in Neural Information\nProcessing Systems, pages 3573–3581, 2015.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 3431–\n3440, 2015.\n[Noh et al., 2015] Hyeonwoo Noh, Seunghoon Hong, and\nBohyung Han. Learning deconvolution network for se-\nmantic segmentation. In Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, pages 1520–1528,\n2015.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, pages 234–241. Springer, 2015.\n[Sermanet et al., 2014] Pierre Sermanet, David Eigen, Xi-\nang Zhang, Michael Mathieu, Rob Fergus, and Yann Le-\nCun. OverFeat: Integrated recognition, localization and\ndetection using convolutional networks. In Proceedings\nof the International Conference on Learning Representa-\ntions, April 2014.\n[Simonyan and Zisserman, 2015] Karen Simonyan and An-\ndrew Zisserman. Very deep convolutional networks for\nlarge-scale image recognition. In Proceedings of the Inter-\nnational Conference on Learning Representations, 2015.\n[Zeiler et al., 2010] Matthew D Zeiler, Dilip Krishnan, Gra-\nham W Taylor, and Rob Fergus. Deconvolutional net-\nworks. In Computer Vision and Pattern Recognition\n(CVPR), 2010 IEEE Conference on, pages 2528–2535.\nIEEE, 2010.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n2900",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.7315987944602966
    },
    {
      "name": "Computer science",
      "score": 0.7139269709587097
    },
    {
      "name": "Transformer",
      "score": 0.6692890524864197
    },
    {
      "name": "Segmentation",
      "score": 0.6397035121917725
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6329745650291443
    },
    {
      "name": "Pixel",
      "score": 0.5917038917541504
    },
    {
      "name": "Image segmentation",
      "score": 0.4755503833293915
    },
    {
      "name": "Novelty",
      "score": 0.474395751953125
    },
    {
      "name": "Computer vision",
      "score": 0.4331998825073242
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3869534730911255
    },
    {
      "name": "Engineering",
      "score": 0.08712807297706604
    },
    {
      "name": "Electrical engineering",
      "score": 0.07437106966972351
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}