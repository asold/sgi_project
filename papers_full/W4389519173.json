{
  "title": "Exploring Transformers as Compact, Data-efficient Language Models",
  "url": "https://openalex.org/W4389519173",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Clayton Fields",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2274008004",
      "name": "Casey Kennington (Mentor)",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3213014097",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3105069964",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W4318621130",
    "https://openalex.org/W4365600532",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3213012718",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W2605959375",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2996728628"
  ],
  "abstract": "Large scale transformer models, trained with massive datasets have become the standard in natural language processing. The huge size of most transformers make research with these models impossible for those with limited computational resources. Additionally, the enormous pretraining data requirements of transformers exclude pretraining them with many smaller datasets that might provide enlightening results. In this study, we show that transformers can be significantly reduced in size, with as few as 5.7 million parameters, and still retain most of their downstream capability. Further we show that transformer models can retain comparable results when trained on human-scale datasets, as few as 5 million words of pretraining data. Overall, the results of our study suggest transformers function well as compact, data efficient language models and that complex model compression methods, such as model distillation are not necessarily superior to pretraining reduced size transformer models from scratch.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 521–531\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n521\nExploring Transformers as Compact, Data-efficient Language Models\nClayton Fields\nDepartment of Computer Science\nBoise State University\nclaytonfields@u.\nboisestate.edu\nCasey Kennington\nDepartment of Computer Science\nBoise State University\ncaseykennington\nboisestate.edu\nAbstract\nLarge scale transformer models, trained with\nmassive datasets have become the standard in\nnatural language processing. The huge size of\nmost transformers make research with these\nmodels impossible for those with limited com-\nputational resources. Additionally, the enor-\nmous pretraining data requirements of trans-\nformers exclude pretraining them with many\nsmaller datasets that might provide enlighten-\ning results. In this study, we show that trans-\nformers can be significantly reduced in size,\nwith as few as 5.7 million parameters, and\nstill retain most of their downstream capabil-\nity. Further we show that transformer mod-\nels can retain comparable results when trained\non human-scale datasets, as few as 5 million\nwords of pretraining data. Overall, the results\nof our study suggest transformers function well\nas compact, data efficient language models\nand that complex model compression methods,\nsuch as model distillation are not necessarily\nsuperior to pretraining reduced size transformer\nmodels from scratch.\n1 Introduction\nIn the space of a few years, transformers have revo-\nlutionized natural language processing. Their suc-\ncess has been driven by increasingly large models\nand more training data. Sizes of the most power-\nful language models have ballooned to billions of\nparameters and are pretrained with (in some cases)\ntrillions of tokens of text (Hoffmann et al., 2022;\nChowdhery et al., 2022). However, the size and\ndata input requirements of transformers limit their\nreach as research tools in two key ways:\nFirst, training transformers usually requires ac-\ncess to powerful compute resources. For instance,\nthe creators of the PALM model (Chowdhery et al.,\n2022), used 6,144 TPUv3 chips for pretraining. At\nthe time of this writing, the on-demand cost of this\nmuch compute would be a little less than $20,000\nper hour.1 Even the moderately sized BERT (De-\nvlin et al., 2018) model required 16 TPU chips for\npretraining, putting such a task beyond the meager\nmeans of many researchers. Costs this high make\nresearch on end-to-end pretraining impossible for\npotentially timely and impactful academic research\n(Togelius and Yannakakis, 2023).\nSecond, large models require pretraining with\nlarge datasets that can generally only be obtained\nfrom data extracted from the internet. BERT, for\ninstance, was trained on a 3.3 billion word web-\nbased corpus. In contrast, datasets derived from\nother sources, human speech for instance, are nec-\nessarily much smaller and sometimes contain only\na few million words. Using data that is not based on\ninternet text can offer insight into how the nature of\nlanguage data affects language model performance.\nCurrently, such efforts to create language models\nfrom smaller, alternative data sources are of grow-\ning interest in computational linguistics (Warstadt\net al., 2023; Huebner et al., 2021).2,3\nMost research for creating efficient transformers\nhas focused on distillation, which trains a smaller\nstudent model using output from a large, pretrained\nteacher model (Sanh et al., 2019; Wang et al., 2020;\nSun et al., 2020; Jiao et al., 2019). While these\nefforts have produced more efficient models, they\nrequire the same large datasets and the use of larger\nteacher models which themselves require ample\ncompute power during training, even though the\nend goal is a smaller model. Remarkably, there has\nto date been little research into simply reducing the\nsize of transformers, pretraining them from scratch\nand fine-tuning them on downstream tasks. The\nprocess of increasing the size of transformer mod-\nels and their data inputs are well explored (Kaplan\net al., 2020; Hoffmann et al., 2022). However, it\n1https://cloud.google.com/tpu/pricing\n2https://babylm.github.io\n3https://sites.google.com/view/learning-with-small-\ndata/home\n522\nis still an open question to what degree the trans-\nformer architecture can function as a lightweight,\ndata-friendly research tool.\nIn this paper, we offer a preliminary study to-\nward addressing these issues. In contrast to pre-\nvious studies that have approached these topics,\nwe forego the use of knowledge distillation and\nother complex compression techniques. Rather we\npretrain various configurations of the ELECTRA\n(Clark et al., 2020) transformer in search of parame-\nter and data efficient models. We conduct all of our\nexperiments using a single 12GB GPU to demon-\nstrate the computational efficiency of the models\nwe train. The main contributions of our study are:\n• We show that compact transformers can re-\ntain a surprising amount of capability on the\nGLUE benchmark (Wang et al., 2018) when\ntrained with only 5 million word tokens. Fur-\nther, we show that when training with such a\nsmall dataset, several model dimensions can\nbe significantly reduced with little ill-effect.\n• We show that when using such a small dataset\nwe can shrink transformers to as few as 5.7\nmillion parmeters and train them faster, us-\ning less compute, while retaining much of the\nperformance of much larger models.\n• We show that with suitable changes to model\nconfiguration, compact variants of the ELEC-\nTRA model trained on the moderately sized\nOpenWebText (Gokaslan and Cohen) corpus\ncan perform on par with compact transform-\ners trained with complex distillation methods\nsuch as DistilBERT (Sanh et al., 2019). Fur-\nther they can do so with significantly fewer\nparameters and computational requirements.\n2 Related Work\nThe excessive compute requirements of transform-\ners has led to the creation of a sizable body of\nresearch into reducing their size and memory foot-\nprint. The most well explored strategy is knowl-\nedge distillation, a process whereby a full-sized\nteacher network is used to train a smaller student\nnetwork. DistilBERT (Sanh et al., 2019), Tiny-\nBERT (Wang et al., 2020), MiniLM (Jiao et al.,\n2019) and MobileBERT (Sun et al., 2020) are pop-\nular examples of compact transformers distilled\nusing full sized BERT models as teachers. These\nmethods produce effective smaller models, how-\never they don’t directly address the amount of input\ndata required and the training process still requires\nusing a full-sized teacher model to train the student\nmodel.\nPruning is another popular model compression\nmethod in which some fraction of the trained\nmodel’s parameters are set to zero. Li et al. (2020)\nand Sanh et al. (2020) use unstructured pruning\nmethods to eliminate a large percentage of weights\nthroughout transformer models with small corre-\nsponding reductions in performance. Structured\npruning methods such as Fan et al. (2019) set the\nparameter values of entire regions of the model\nto zero; in this case whole transformer layers are\npruned. Michel et al. (2019) showed that a large\npercentage of BERT’s attention heads can be en-\ntirely removed before testing without a significant\ndecrease in performance. However, these tech-\nniques are are all premised on pretraining full-sized\nmodels and then reducing the model size prior to\ninference time, therefore still have the same pre-\ntraining data and compute requirements.\nThere has also been some research directly ad-\ndressing the size of pretraining datasets for trans-\nformers. Micheli et al. (2020) and Martin et al.\n(2019) experimented with reducing the absolute\namount of training data in French language models.\nThey showed that full sized French language trans-\nformer models can perform well on select tasks\nwith significantly less pretraining data. Warstadt\net al. (2020b) and Zhang et al. (2020) investigated\nthe effect of different pretraining data volumes on\nthe grammatical knowledge of the RoBERTa-base\nmodel using probing techniques.\nHuebner et al. (2021) experimented with using\nAOCHILDES, the 5 million word dataset com-\nposed of child directed speech for pretraining and\nevaluated their results using a grammatical bench-\nmark based on BLIMP (Warstadt et al., 2020a).\nThis study is notable because the authors used a\nvery small pretraining dataset derived from human\nspeech and opted to use a scaled-down version of\nthe RoBERTa model (Liu et al., 2019) to accom-\nmodate it. Unfortunately, the resulting model was\nonly tested on narrow set of grammatical learning\ntasks, using a specialized dataset for evaluation.\n3 Data and Evaluation Criteria\n3.1 Pretraining Data\nThe ELECTRA model was originally pretrained\nwith the 3.3 billion word corpus used to train BERT\n(Devlin et al., 2018). This dataset, however, is not\n523\nModel Params COLA MRPC QNLI MNLI QQP SST2 STSB RTE Avg.\nELECTRA 13.6M 0.570 0.907 0.883 0.814 0.894 0.858 0.822 0.657 0.801\nMobileBERT 15.1M 0.531 0.881 0.908 0.814 0.858 0.901 0.874 0.592 0.794\nDistillBERT 67M 0.496 0.869 0.886 0.824 0.866 0.901 0.864 0.585 0.783\nTable 1: Results for downstream tasks with compact, pretrained models downloaded from the Huggingface library.\npublicly available. Fortunately, there are a variety\nof open source alternatives freely available for re-\nsearch purposes. We use a web-sourced, public\ntext corpus, or a subset of it, called OpenWebText\n(Gokaslan and Cohen) for pretraining in all of our\nexperiments. The OpenWebText corpus was cre-\nated as a publicly available reproduction of Ope-\nnAI’s WebText corpus that was used in the training\nof GPT-2. It consists of over 38GB of text data\nscraped from over 8 million internet documents. It\nis a popular choice for pretraining language models.\nWe chose this dataset, specifically because it con-\ntains text from a wide variety of sources and will\nprepare our models for the diverse range of tasks\ncontained in the GLUE benchmark (Wang et al.,\n2018).\nIn the first two of our three experiments we aim\nto test models trained with scarce data, specifically\nwe use approximately 5 million words of pretrain-\ning data. 5 million words is a rough estimate of how\nmany words an American child might hear before\nthey begin speaking (Gilkerson et al., 2017). In\nthat sense it represents a realistic size for a human\nscale dataset. To obtain a corpus of suitable size for\nthis experiment we randomly selected documents\nfrom OpenWebText until we had a set with just\nover 5 million words and 306,462 unique words\nincluding names of websites such as \"tumblr\" and\nnon-English words and phrases. In terms of disk\nspace it requires only 43MB to store. In our third\nand final experiment we make use of all 38GB of\nthe OpenwebText corpus. The scale and diversity\nof the full dataset are similar to those used to train\nmodels such as BERT and will allow us to compare\nour compact model variations to other pretrained\ncompact models.\n3.2 Finetuning Data & Tasks\nGLUE To evaluate our pretrained models we\nfine-tune them on the GLUE tasks introduced in\nWang et al. (2018). The GLUE benchmark consists\nof nine supervised sentence-level tasks and their\nassociated datasets that cover a variety of natural\nlanguage understanding domains. We chose GLUE\nas a benchmark because it spans several tasks and\nFigure 1: The ELECTRA model is a Generator-\nDiscriminator ensemble. The Discriminator is tasked\nwith determining if the Generator properly guessed a\nmasked word; borrowed from (Clark et al., 2020).\nbecause its popularity in NLP research allows us\nto directly compare the performance of our models\nwith previously published results. Following De-\nvlin et al. (2018) and Clark et al. (2020) we exclude\nthe WNLI task from our consideration. COLA is a\ngrammatical acceptability task, SST-2 a sentiment\nclassification, QQP, MRPC, STS-B are sentence\nsimilarity tasks and MNLI, QNLI, and RTE are\ninference tasks. Our evaluation metrics are Spear-\nman correlation for STS-B, Matthews correlation\nfor CoLA, F1 score for QQP and MRPC and accu-\nracy for the remainning tasks. All of the reported\nresults were obtained by evaluating on the dev sets\nof the tasks described and are fine-tuned for 10\nepochs. In general, the standard practice for GLUE\nfine-tuning is to train for 3 epochs with a batch size\nof 32 and a learning rate of 2e-5. However, Clark\net al. (2020) noted that ELECTRA performs better\non select GLUE tasks when trained for 10 epochs.\nWe found that since overfitting is not a concern for\nthe small variants of ELECTRA that we trained,\nour models benefited from training for 10 epochs\non all of the GLUE tasks.\n4 Language Model: ELECTRA-small\nIn this section we describe the ELECTRA-small\nmodel (Clark et al., 2020) and the rationale behind\nusing it as the basis for our experiments. In place of\nmasked language modeling, ELECTRA pretrains\na transformer encoder stack, structurally identical\nto BERT’s, by replacing some input tokens with\nplausible alternative words sampled from a small\ngenerator network. A larger discriminator model\nthen predicts whether or not each input token has\n524\nintr emb\nsize size lyrs prms COLA MRPC QNLI MNLI QQP SST2 STSB RTE Avg.\n1024 128 12 13.6M 0.417 0.825 0.818 0.755 0.836 0.838 0.802 0.596 0.736\n768 128 12 12.0M 0.422 0.841 0.823 0.755 0.838 0.849 0.800 0.556 0.736\n512 128 12 10.4M 0.425 0.832 0.822 0.757 0.838 0.807 0.800 0.570 0.731\n256 128 12 8.82M 0.343 0.833 0.828 0.758 0.838 0.830 0.794 0.588 0.727\n128 128 12 8.03M 0.379 0.861 0.819 0.750 0.839 0.825 0.815 0.592 0.735\n64 128 12 7.64M 0.366 0.852 0.816 0.753 0.838 0.819 0.813 0.639 0.737\n1024 96 12 12.5M 0.362 0.818 0.821 0.752 0.840 0.823 0.807 0.588 0.726\n1024 64 12 11.5M 0.346 0.824 0.820 0.746 0.830 0.820 0.784 0.552 0.715\n1024 32 12 10.5M 0.246 0.822 0.798 0.725 0.816 0.831 0.757 0.563 0.695\n1024 128 10 12.0M 0.415 0.834 0.827 0.746 0.840 0.844 0.807 0.599 0.739\n1024 128 8 10.4M 0.442 0.851 0.826 0.748 0.839 0.826 0.806 0.585 0.740\n1024 128 6 8.81M 0.367 0.814 0.826 0.746 0.835 0.852 0.787 0.516 0.718\n1024 128 4 7.2M 0.28 0.823 0.819 0.740 0.832 0.818 0.791 0.581 0.710\nTable 2: Results for downstream tasks with reduced model dimensions. Note that the top row represents the\nfull-sized ELECTRA-small model. All results were trained with a 5M word subset of openwebtext trained for\n100,000 steps with batch size of 128. Reduced parameter settings are shown in bold.\nbeen replaced. See Figure 1 for an illustration of\nthe ELECTRA model. Clark et al. (2020) show\nthat this strategy leads to better results with less\ndata and less compute than causal language model-\ning or standard masked language modeling. After\ntraining, the generator is discarded and the discrim-\ninator is used for downstream tasks.\nThe ELECTRA-small model also has the advan-\ntage of beginning with only 13.6 million parame-\nters and performs favorably to similarly sized mod-\nels. Further, it can be pretrained without the use\nof model distillation. As the purpose of our study\nis to train transformers in data and resource scarce\nsettings, it is desirable to use a model that doesn’t\nrequire a teacher model pretrained on a massive\ntext corpus, and can be trained on a single GPU.\nTo ensure that ELECTRA-small can produce re-\nsults on par with other compact models we use\nthe pretrained models from the Huggingface trans-\nformer hub and test them on our selected down-\nstream tasks.4 The results are summarized in Ta-\nble 1. We tested three pretrained models, ELEC-\nTRA, MobileBERT and DistilBERT. Of the three,\nELECTRA is the smallest model in terms of abso-\nlute number of parameters with only 13.6 million.\nDespite its small size, it achieves the best average\nresults on GLUE. Notably it does so using only\npretraining and fine-tuning without the benefit of\nknowledge distillation from a larger model. These\nfeatures harmonize well with the goals of our study\nand make the ELECTRA-small model the logical\nchoice on which to base our succeeding experi-\nments.\n4https://huggingface.co/\n5 Experiment 1: Reducing Individual\nModel Dimensions in a Low Data\nSetting\nIn the first set of experiments that we conduct, we\ntest varying the size and configuration of the ELEC-\nTRA model using the 5 million word subset of\nopenwebtext described in Section 3.1 as the pre-\ntraining dataset for each model variation. We begin\nby changing only a single dimension of the model’s\nconfiguration. The goal of this series of experi-\nments is to determine which parts of the model’s\narchitecture can be reduced and what effect these\nreductions have on performance. In the process,\nwe hope to provide some insight into how the size\nof each dimension of the transformer model con-\ntributes to its downstream performance.\n5.1 Procedure\nThe basic architecture of transformer models is best\ndescribed by Vaswani et al. (2017) and consists of\nan embedding layer followed by stacked attention\nlayers, each composed of a multi-head attention\nmechanism followed by a feed-forward neural net-\nwork sub-layer. All of the stacked layers have the\nsame dimension, but have varying weights. The\nembedding size, vocab size, hidden state size, the\nfeed-forward network’s intermediate sizeand the\nabsolute number of layerscan all be altered. The\nnumber of attention heads per layer and the max-\nimum sequence length can also be varied, though\nthese changes don’t affect the overall number of\nmodel parameters. We first test the effect of reduc-\ning the size of each of the these parameters and\nthen pretrain a given model on our 5 million word\n525\nsubset of openwebtext. Each model is trained for\n100,000 steps with a batch size of 128 and a learn-\ning rate of 5e-4. The generator network used for\ntraining is one quarter the size of the discriminator\nnetwork.\nThe downstream tasks on which we fine-tune\nand evaluate our resulting models are the GLUE\ntasks described in Section 3, including an Average\nof all scores ( Avg.). For each task we fine-tune\nmodels for 10 epochs, with a learning rate of 2e-5\nand a batch size of 32. Four of these tasks, QQP,\nQNLI, SST-2 and MNLI are associated with rela-\ntively large datasets and the results are fairly robust\nto changes in the model’s size. The remainder of\nthe GLUE tasks on the other hand, use very small\ntraining sets leading to wide variation, even scores\nat the level of chance when model capacity is suffi-\nciently degraded.\n5.2 Results\nThe results for the models with a single dimension\nreduced are discussed in this section and summa-\nrized in Table 2. We had the most success in reduc-\ning the intermediate size, the embedding size and\nthe number of attention layers and we provide dis-\ncussion for each below. The results of modifying\nthe hidden size, vocabulary size and number of at-\ntention heads were less successful and are available\nin Appendix A.\nIntermediate Size The intermediate size refers\nto the dimension of the hidden layer in the feed-\nforward network (FFN) contained in each attention\nlayer. Following Vaswani et al. (2017), ELEC-\nTRA’s default intermediate size is 4 times that of\nthe the hidden size, which yields an intermediate\nsize of 1024 for ELECTRA-Small. Our results\nindicate that the number of these parameters can\nbe dramatically decreased with relatively little ef-\nfect on the model’s capability when training with\na small dataset. Downstream performance shows\nessentially no loss with as few as 64 parameters in\neach FFNs hidden layer, nearly a 16-fold reduction\nin size. This is a remarkable result as the model\nperforms nearly identically with 6 million fewer\nparameters. Most transformer architectures also\nuse an intermediate size 4 times that of the hidden\nsize of the attention layers. These results suggest\nthat the intermediate stage of transformer’s FFNs\nmay be over-parameterized. In the final experiment\nwe address how well these results hold for models\ntrained on large-scale datasets.\nEmbedding Size In transformer models, the em-\nbedding size refers to the length of each vocabulary\nword’s embedding vector. The default size of the\nembedding vectors for ELECTRA is 128. Like the\nintermediate layer size, the embedding size can be\nsubstantially decreased while retaining most of the\nmodel’s downstream performance. We see from\nour results that an embedding size of 96 has vir-\ntually the same capability as a full-sized model.\nAn embedding size of 64 shows slightly reduced\nperformance on most of the GLUE tasks with a 2\npercent drop in average score. This is a notable\nresult and it suggests that the embedding layer may\nalso be over parameterized in a low data setting.\nModel Depth Finally, we also reduce the depth\nof the model, and its number of parameters by sim-\nply decreasing the number of attention layers in the\nmodel. The number of hidden layers in ELECTRA-\nsmall is 12 by default. The results for reducing\nmodel depth are included in Table 2. We see that\ndecreasing the number of layers to 10 or 8 actually\nimproves the model’s performance on most of our\ndownstream tasks with a 1.5 million and 2.3 mil-\nlion decrease in their respective parameter counts.\nFurther decreasing the number of layers to 6 or\n4, with 3-5 million fewer parameters, shows only\nsmall decreases in the overall GLUE score.\n6 Experiment 2: Reducing Overall Model\nSize in a Low Data Setting\nGuided by our results from the previous experi-\nments, we now aim to find an overall configuration\nof ELECTRA-small that has significantly fewer\nparameters than its default of 13.6 million and re-\ntains most of its downstream performance. Using\nthe same 5 million word dataset, we train models\nwith any number of their dimensions reduced or\nmodified. In essence, we trained and evaluated a\nlarge number of models with various combinations\nof our most successful modifications from our pre-\nvious experiments in search of a robust and well\nfunctioning model.\nWe found early in our efforts that simply re-\nducing model size while keeping the dimensions\nproportionate produced poor results. Our results\nfrom the previous section suggest that this may be\ndue to the models low tolerance for decreases in\nits hidden size. We did however find several alter-\nnative configurations with parameter counts that\nranged from 5.7 to 10 million that retained most of\nthe performance of the full-sized ELECTRA-small\n526\nModel Hidden Size Inter Size Layers Emb Size Params time 100k time 1M\nELECTRA-\nsmall 256 1024 12 128 13.7M 16h26m 6d21h\nModel 1 256 1024 8 128 10.4M 11h15m 4d17h\nModel 2 256 256 16 64 8.4M 16h11m 6d5h\nModel 3 256 128 14 96 7.7M 13h 5d18h\nModel 4 256 64 12 128 7.6M 11h58m 5d7h\nModel 5 196 128 18 64 5.7M 13h48m 5d17h\nTable 3: Model Key Dimensions for 5 smaller model configurations of ELECTRA. Training times for 100k and\n1M training steps with a batch size of 128 on a 12GB GPU included. Note that the top row represents the full-sized\nELECTRA-small model for reference. Reduced parameter settings are shown in bold.\nModel COLA MRPC QNLI MNLI QQP SST2 STSB RTE Avg.\nELECTRA-Small 0.417 0.825 0.818 0.755 0.836 0.838 0.802 0.596 0.736\nModel 1 0.442 0.851 0.826 0.748 0.839 0.826 0.806 0.585 0.740\nModel 2 0.383 0.834 0.833 0.752 0.841 0.826 0.815 0.614 0.737\nModel 3 0.366 0.852 0.816 0.753 0.838 0.819 0.813 0.639 0.737\nModel 4 0.386 0.849 0.832 0.751 0.839 0.842 0.817 0.567 0.735\nModel 5 0.334 0.838 0.819 0.736 0.827 0.815 0.794 0.614 0.722\nTable 4: Low Data Setting Results for select models trained with the 5M word subset of OpenWebText corpus for\n100k steps. Results for MobilBERT and DistilBERT are appended for the sake of comparison.\nmodel trained on our 5 million word set. The most\nsuccessful configurations modified some combina-\ntion of intermediate size, embedding size or layer\ncount. We discovered that we could improve per-\nformance relative to parameter count by decreasing\nthe width ( intermediate size and hidden size) of\nthe model while increasing its depth ( number of\nlayers). We had less success increasing the width\nand decreasing the number of layers. Turc et al.\n(2019) observed a similar phenomenon leveraging\nknowledge distillation on pretrained compact mod-\nels.\nThough we trained several dozen model varia-\ntions, we present only our 5 most succesful. The\ndimensions and parameter counts of these mod-\nels are described in Table 3. Two of the models,\nModel 1 and Model 4, feature only a single modi-\nfied dimension and were mentioned in the previous\nexperiment. Model 1 has 8 layers and Model 4 has\nan intermediate size of only 64 parameters. These\nmodifications led to good results in our previous\nexperimental settings and a sizeable reduction in\nmodel size. The remaining models feature a de-\ncrease in the model width and the size of the em-\nbedding layer with an increase in model depth.\nProcedure As in our previous experiments where\nwe altered only a single model dimension, we pre-\ntrain all of our models using the 5 million word\nsubset of openwebtext for 100,000 steps. We eval-\nuate our models using the same metrics and hyper-\nparameters as the previous experiment in order to\ncompare our results.\nResults The downstream results for these mod-\nels are summarized in Table 4 and discussion is\nprovided below. In this low data setting, our small\nModels 1-4 have essentially the same performance\nas the original ELECTRA-Small model configura-\ntion trained with the same data and settings. Model\n5 performed only slightly worse, despite having\nonly 5.7 million parameters. These results sug-\ngest that when using small datasets, small-scale\ntransformers may perform as well as their compu-\ntationally more expensive larger cousins.\nMoreover, the reduction in size can be performed\nin a variety of ways. The results for Model 1 show\nthat we can also decrease the model depth by 4\nlayers without ill-effect. Doing so cuts our training\ntime nearly in half and reduces our model size by\n3 million parameters. Alternatively, increasing the\ndepth to compensate for loss of width and embed-\nding size was also very effective in lowering overall\nmodel size. Models 2, 3 and 5 made use of this\nstrategy to varying degrees and produced similar\nresults. Increasing model depth, however, comes\n527\nModel COLA MRPC QNLI MNLI QQP SST2 STSB RTE Avg.\nELECTRA-Small 0.591 0.908 0.875 0.812 0.856 0.885 0.857 0.632 0.802\nModel 1 0.487 0.886 0.865 0.788 0.847 0.894 0.842 0.61 0.777\nModel 2 0.504 0.896 0.859 0.784 0.848 0.853 0.841 0.621 0.776\nModel 3 0.478 0.881 0.854 0.792 0.847 0.885 0.842 0.632 0.776\nModel 4 0.409 0.868 0.846 0.774 0.836 0.858 0.849 0.643 0.760\nModel 5 0.444 0.906 0.860 0.784 0.846 0.859 0.834 0.661 0.774\nMobileBERT 0.510 0.880 0.908 0.831 0.873 0.917 0.874 0.625 0.802\nDistilBERT 0.527 0.826 0.889 0.818 0.870 0.896 0.865 0.585 0.785\nTable 5: High Data Setting Results for select models trained with the full OpenWebText corpus for 1 million steps.\nResults for MobilBERT and DistilBERT are appended for the sake of comparison.\nat the cost of slower training times, presumably\nbecause of the increased number of non-linear acti-\nvation functions. Though smaller, Models 2, 3 and\n5 required longer train times than did models 1 and\n4, which had fewer layers. Model 2 required nearly\nas much time to train than ELECTRA-small.\n7 Experiment 3: Reducing Model Size in\na High Data Setting\nIn this experiment, we pretrain a selection of mod-\nels using the full OpenWebText corpus as the pre-\ntraing dataset and training for a million steps. We\nuse the same five models described in Table 3.\nBecause the training times in this experiment are\nmuch longer, we will not repeat the exhaustive\nstudy of the effects of changing individual model di-\nmensions as we did in the low data setting. Rather,\nwe only pretrain and evaluate the 5 models consid-\nered in Experiment 2. The goal of this experiment\nis determine to what degree the results of Exper-\niments 1 and 2 will hold with a full-size dataset\ntrained for an extended time. Given that the models\nconsidered contain so few parameters, it is a natural\nquestion as to whether or not they can adequately\nmake use of the additional information provided\nby more data and longer pretraining. The results\nof this experiment will also be more readily com-\npared to other compact transformers which are also\ntrained on full-sized datasets. We use the same eval-\nuation criteria as that performed in Experiments 1\nand 2.\nResults The results of fine-tuning these models\non the GLUE corpus are summarized in Table 5\nand discussion is provided below. As opposed to\nthe scarce data setting, the larger ELECTRA-Small\nmodel is able to make greater use of more data and\nincreased training time and outperforms its smaller\ncounterparts to a noticeable degree. This was an\nexpected result given the abundance of training\ndata used. Over the course of a million training\nsteps, the differences in training times are consider-\nable. Model 1 requires almost 2 days fewer to train.\nModels 3, 4 and 5 all require a day less in training\ntime. The slow training of Model 2 is again on\ndisplay, requiring over six days of training time.\nOf the small models we tested, all had quite\nsimilar performance, though Model 4 showed a\nslight drop relative to the other models. In our\nhigh data setting, with longer training time, our\nsmallest model, Model 5 performs as well as the\nother small model variants. This is a change from\nthe low data setting where it lagged slightly behind.\nModels 2 and 3 also perform well in this setting\nsuggesting that increasing model depth to offset\nreductions in other dimensions scales fairly well\nto larger datasets. Notably Model 1, with 8 layers\nof the original ELECTRA-small dimensions, had\nsimilar performance and a favorable training time.\nThough it contains more parameters than the other\nsmall models, its reduced depth markedly reduces\ntraining time, requiring less than 5 days to train for\na million steps.\nIt is not immediately clear why these particu-\nlar distributions of parameters perform well. Most\ntransformer architectures feature a roughly 2:1 ratio\nof parameters between their feed-forward networks\nand their multi-head attention mechanisms. Our\nresults suggest that this ratio might be open to sig-\nnificant modification. The theoretical purpose of\nthe FFN is to introduce non-linearity. The fact that\nincreasing the number of layers, and therefore the\nnumber of non-linear activation functions, seems\nto offset reductions in the size of the FFN lends cre-\ndence to that theory. MobileBERT also has a long,\nthin architecture. Its creators, however, felt com-\n528\nFigure 2: Relative size comparison of Electra-small\n(blue) with Electra-tiny (red). Electra-tiny has smaller\nembeddings, hidden size, and intermediate size, but has\nmore hidden layers.\npelled to stack additional FFNs to restore the 2:1\nparameter ratio. We suggest that this may not be\nnecessary. In general, we also advocate for a more\nthorough investigation of how parameters are dis-\ntributed within the transformer architecture. While\nthe focus of this study was in low data settings and\nsmall models, even small improvements in param-\neter efficiency could be of great consequence for\nvery large models.\n7.1 Our Smallest Model: ELECTRA-tiny\nThe smallest model configuration we found that\ndidn’t experience large reductions in performance\nwas Model 5. It had a hidden size of 196, reduced\nfrom 256, intermediate size of 128, decreased from\n1024, an embedding size of 64, decreased from\n128 and 18 layers, increased from 12. We call\nthis model ELECTRA-Tiny and it contains just 5.7\nmillion parameters. Figure 2 shows visually how\nELECTRA-Tiny compares to ELECTRA-Small.\nELECTRA-Tiny is an extremely small given the\nmodel’s performance on a diverse set of tasks such\nas GLUE. When training the ELECTRA-Small\nmodel, the largest batch size that our 12GB GPU\ncould accommodate was 128. Because of the small\nsize of ELECTRA-Tiny, we could train at batch\nsizes of up to 256; alternatively we might have\ntrained ELECTRA-Tiny at a batch size 128 on an\neven smaller GPU. The compactness, low compute\nrequirements and favorable training times make a\nmodel like this ideal for researchers without access\nto multiple GPUs. The model weights from Exper-\niment 2, trained with the full OpenWebText for 1\nmillion steps, are available on the Huggingface.5\nFor the sake of comparison, we have added the\nresults of two compact transfomers trained with\ndistillation to Table 5, DistilBERT (Sanh et al.,\n2019) and MobileBERT (Sun et al., 2020). We\nagain downloaded the pretrained weights for these\nmodels from Huggingface. This time however, we\nfinetuned the models for 10 epochs and the same\nfine-tuning parameters as the previous experiments\nin order to fairly compare the results to the com-\npact ELECTRA variants we trained. Though dif-\nferences in training data and training times make\nthis comparison somewhat inexact, the results are\nstill illuminating. We see that ELECTRA-Tiny pro-\nduces scores only slightly below that of the Distil-\nBERT model, despite being a tenth of the size and\nbeing trained without complex distillation losses.\nMobileBERT performs slightly better, on par with\nthe ELECTRA-Small model. MobileBERT has 15\nmillion parameters, slightly more than ELECTRA-\nSmall and 3 times as many as ELECTRA-Tiny. All\ntold, our data suggest that complex compression\ntechniques like distillation may be less profitable\nthan simply starting with much smaller models and\npretraining them on a suitable training corpus with\na data efficient proxy task such as the discrimina-\ntive loss of ELECTRA.\n8 Conclusion\nIn this study, we have shown that the transform-\ners, specifically ELECTRA, can function as com-\npact data-efficient models. Our results suggest that\nwhen training with small datasets, the intermedi-\nate size, embedding size and number of layers can\nall be reduced with little ill-effect. Additionally,\nwe presented the GLUE results for 5 model vari-\nations that significantly reduce the overall size of\nthe ELECTRA-Small model. In the final phase of\nour experiments, we tested the same five models\ntrained with the full OpenWebText corpus. We\nshowed that several compact transformer architec-\ntures can function on par with larger models trained\nusing complex distillation methods. Finally, we\npresent a compact configuration of ELECTRA we\ncall ELECTRA-Tiny with just 5.7 million param-\neters that performs remarkably well on the GLUE\nbenchmark given its small size, requires less com-\npute and can be trained end to end on a single 12GB\nGPU.\n5https://huggingface.co/claytonfields/electra-\ntiny/tree/main\n529\nLimitations\nOne of the primary limitations of our study was\nthat of computational resources. Had we had more\ncompute, we would to have been able to conduct\nmore exhaustive studies of our models in high data\nscenarios with extended training times. There are\nseveral model compression methods such as quan-\ntization (Bondarenko et al., 2022) and adaptive\nsequence length reduction (Guskin et al., 2021)\nthat would have been compatible with the models\nthat we trained. An exhaustive study of these tech-\nniques applied to the type of small models we used\nin this study could potentially have produced even\nmore efficient models.\nEthics Statement\nThere are no compelling ethical conflicts to report\nfor this study.\nAcknowledgements\nThanks to the anonymous reviewers for their very\nuseful feedback. This material is based upon work\nsupported by the National Science Foundation un-\nder Grant No. 2140642.\nReferences\nAlexander Bondarenko, Magdalena Wolska, Ste-\nfan Heindorf, Lukas Blübaum, Axel-Cyrille\nNgonga Ngomo, Benno Stein, Pavel Braslavski,\nMatthias Hagen, and Martin Potthast. 2022.\nCausalQA: A benchmark for causal question\nanswering. In Proceedings of the 29th Interna-\ntional Conference on Computational Linguistics,\npages 3296–3308, Gyeongju, Republic of Ko-\nrea. International Committee on Computational\nLinguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nJill Gilkerson, Jeffrey A Richards, Steven F Warren, Ju-\ndith K Montgomery, Charles R Greenwood, D Kim-\nbrough Oller, John HL Hansen, and Terrance D Paul.\n2017. Mapping the early language environment using\nall-day recordings and automated analysis. American\njournal of speech-language pathology, 26(2):248–\n265.\nAaron Gokaslan and Vanya Cohen. Openwebtext cor-\npus.\nShira Guskin, Moshe Wasserblat, Ke Ding, and Gyuwan\nKim. 2021. Dynamic-TinyBERT: Boost TinyBERT’s\ninference efficiency by dynamic sequence length.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training Compute-Optimal\nlarge language models.\nPhilip A Huebner, Elior Sulem, Fisher Cynthia, and\nDan Roth. 2021. Babyberta: Learning more gram-\nmar with small-scale child-directed language. In Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 624–646.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt\nKeutzer, Dan Klein, and Joseph E Gonzalez. 2020.\nTrain large, then compress: Rethinking model size\nfor efficient training and inference of transformers.\narXiv preprint arXiv:2002.11794.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de La Clergerie, Djamé Seddah, and Benoît\nSagot. 2019. Camembert: a tasty french language\nmodel. arXiv preprint arXiv:1911.03894.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nVincent Micheli, Martin d’Hoffschmidt, and François\nFleuret. 2020. On the importance of pre-training data\n530\nvolume for compact language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7853–7858, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander Rush. 2020.\nMovement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems,\n33:20378–20389.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nJulian Togelius and Georgios N Yannakakis.\n2023. Choose your weapon: Survival strate-\ngies for depressed ai academics. arXiv preprint\narXiv:2304.06035.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776–5788.\nAlex Warstadt, Leshem Choshen, Aaron Mueller, Adina\nWilliams, Ethan Wilcox, and Chengxu Zhuang. 2023.\nCall for papers – the BabyLM challenge: Sample-\nefficient pretraining on a developmentally plausible\ncorpus.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nAlex Warstadt, Yian Zhang, Haau-Sing Li, Haokun Liu,\nand Samuel R Bowman. 2020b. Learning which\nfeatures matter: Roberta acquires a preference for lin-\nguistic generalizations (eventually). arXiv preprint\narXiv:2010.05358.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nYian Zhang, Alex Warstadt, Haau-Sing Li, and\nSamuel R Bowman. 2020. When do you need bil-\nlions of words of pretraining data? arXiv preprint\narXiv:2011.04946.\nA Additional Results\nA.1 Experiment 2: Hidden Size\nWe found that small reductions in hidden size re-\nsult in significantly fewer model parameters and\nnotable effects on downstream performance. Low-\nering the hidden size from 256 to 192 results in\ntolerable losses in performance, even on our low\ndata tasks COLA and BLiMP. However, further\nreductions show sizable drops in downstream per-\nformance, especially for COLA and BLiMP. As\nwas mentioned in section 6.2, the effect of decreas-\ning hidden size can be offset by increasing mode\ndepth.\nA.2 Experiment 2: Vocabulary Size\nAltering the vocabulary size is somewhat more in-\nvolved than changing the other dimensions. The\nvocab is produced by the WordPiece algorithm (Wu\net al., 2016) and must be trained on a corpus of text.\nThe number of words in the vocab is chosen prior\nto training and the algorithm determines the opti-\nmum choice of word pieces. In order to form a fair\ncomparison with the original vocabulary we elected\nto train various tokenizers on a large fraction of the\nopenwebtext data. In contrast to embedding size,\nwe see significant effect from lowering the vocab\nsize relative to the decrease in parameter count. As\nsuch, decreased vocabulary size did not figure into\nour most effective reduced model configurations.\nA.3 Experiment 2: Attention Heads\nFinally, we tried altering the number of attention\nheads per layer from the defualt number of 4. Since\nthe number of attention heads does not affect the\nnumber of parameters in the model, we also tried\nincreasing the number to 8 (the number of attention\nheads must evenly divide the attention layer hidden\nsize). Our results show that doing so did not greatly\nimpact model performance.\n531\nhid voc atn\nsize size hds Prms COLA MRPC QNLI MNLI QQP SST2 STSB RTE Avg.\n256 30,522 4 13.6M 0.417 0.825 0.818 0.755 0.836 0.838 0.802 0.596 0.736\n192 30,522 4 10.6M 0.369 0.824 0.824 0.752 0.839 0.833 0.789 0.567 0.725\n128 30,522 4 7.9M 0.284 0.828 0.824 0.738 0.826 0.815 0.716 0.534 0.696\n64 30,522 4 5.8M 0.176 0.815 0.773 0.696 0.79 0.803 -0.107 0.505 0.556\n32 30,522 4 4.8M 0.0 0.812 0.657 0.655 0.753 0.763 -0.139 0.52 0.503\n256 28,672 4 13.3M 0.339 0.841 0.811 0.74 0.832 0.807 0.789 0.585 0.718\n256 24,576 4 12.8M 0.275 0.838 0.818 0.745 0.836 0.813 0.765 0.552 0.705\n256 20,480 4 12.3M 0.294 0.842 0.813 0.744 0.837 0.828 0.794 0.599 0.719\n256 16,384 4 11.7M 0.33 0.821 0.821 0.742 0.837 0.821 0.779 0.578 0.716\n256 12,288 4 11.2M 0.335 0.818 0.824 0.74 0.839 0.798 0.809 0.534 0.712\n256 8,192 4 10.7M 0.279 0.844 0.819 0.735 0.84 0.817 0.807 0.545 0.711\n256 30,522 8 13.5M 0.381 0.828 0.824 0.749 0.842 0.831 0.765 0.552 0.722\n256 30,522 2 13.5M 0.385 0.848 0.815 0.752 0.839 0.844 0.801 0.581 0.733\n256 30,522 1 13.5M 0.401 0.803 0.819 0.746 0.838 0.838 0.788 0.574 0.726\nTable 6: Additional results for downstream tasks with reduced model dimensions. Note that the top row represents\nthe full-sized ELECTRA-small model. All results were trained with a 5M word subset of openwebtext trained for\n100,000 steps with batch size of 128. Modified parameter settings are shown in bold.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7661412954330444
    },
    {
      "name": "Computer science",
      "score": 0.6516473889350891
    },
    {
      "name": "Language model",
      "score": 0.4519079923629761
    },
    {
      "name": "Data modeling",
      "score": 0.414777547121048
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38528138399124146
    },
    {
      "name": "Machine learning",
      "score": 0.3398614525794983
    },
    {
      "name": "Engineering",
      "score": 0.18792343139648438
    },
    {
      "name": "Database",
      "score": 0.15355053544044495
    },
    {
      "name": "Electrical engineering",
      "score": 0.12012064456939697
    },
    {
      "name": "Voltage",
      "score": 0.0990903377532959
    }
  ]
}