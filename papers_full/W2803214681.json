{
  "title": "Are All Languages Equally Hard to Language-Model?",
  "url": "https://openalex.org/W2803214681",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2148165152",
      "name": "Ryan Cotterell",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2798366445",
      "name": "Sebastian J. Mielke",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2023211357",
      "name": "Jason Eisner",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2165864196",
      "name": "Brian Roark",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2625014264",
    "https://openalex.org/W1769936356",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W2741021341",
    "https://openalex.org/W3141884538",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1994616650",
    "https://openalex.org/W2963899393",
    "https://openalex.org/W2044916741",
    "https://openalex.org/W2963022149",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2161278536",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2609370997",
    "https://openalex.org/W2807188009",
    "https://openalex.org/W2524429381",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2250525902",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2574640638",
    "https://openalex.org/W58893626"
  ],
  "abstract": "Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, Brian Roark. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.",
  "full_text": "Are All Languages Equally Hard to Language-Model?\nRyan Cotterell1 and Sabrina J. Mielke1 and Jason Eisner1 and Brian Roark2\n1 Department of Computer Science, Johns Hopkins University 2 Google\n{ryan.cotterell@,sjmielke@,jason@cs.}jhu.edu roark@google.com\nAbstract\nFor general modeling methods applied to di-\nverse languages, a natural question is: how\nwell should we expect our models to work on\nlanguages with differing typological proﬁles?\nIn this work, we develop an evaluation frame-\nwork for fair cross-linguistic comparison of\nlanguage models, using translated text so that\nall models are asked to predict approximately\nthe same information. We then conduct a study\non 21 languages, demonstrating that in some\nlanguages, the textual expression of the infor-\nmation is harder to predict with both n-gram\nand LSTM language models. We show com-\nplex inﬂectional morphology to be a cause of\nperformance differences among languages.\n1 Introduction\nModern natural language processing practitioners\nstrive to create modeling techniques that work well\non all of the world’s languages. Indeed, most meth-\nods are portable in the following sense: Given ap-\npropriately annotated data, they should, in princi-\nple, be trainable on any language. However, despite\nthis crude cross-linguistic compatibility, it is un-\nlikely that all languages are equally easy, or that\nour methods are equally good at all languages.\nIn this work, we probe the issue, focusing onlan-\nguage modeling. A fair comparison is tricky. Train-\ning corpora in different languages have different\nsizes, and reﬂect the disparate topics of discussion\nin different linguistic communities, some of which\nmay be harder to predict than others. Moreover,\nbits per character, a standard metric for language\nmodeling, depends on the vagaries of a given ortho-\ngraphic system. We argue for a fairer metric based\non the bits per utterance using utterance-aligned\nmulti-text. That is, we train and test on “the same”\nset of utterances in each language, modulo transla-\ntion. To avoid discrepancies in out-of-vocabulary\nhandling, we evaluate open-vocabulary models.\nWe ﬁnd that under standard approaches, text\ntends to be harder to predict in languages with ﬁne-\ngrained inﬂectional morphology. Speciﬁcally, lan-\nguage models perform worse on these languages,\nin our controlled comparison. Furthermore, this\nperformance difference essentially vanishes when\nwe remove the inﬂectional markings.1\nThus, in highly inﬂected languages, either the ut-\nterances have more content or the models are worse.\n(1) Text in highly inﬂected languages may be in-\nherently harder to predict (higher entropy per utter-\nance) if its extra morphemes carry additional, un-\npredictable information. (2) Alternatively, perhaps\nthe extra morphemes are predictable in principle—\nfor example, redundant marking of grammatical\nnumber on both subjects and verbs, or marking of\nobject case even when it is predictable from seman-\ntics or word order—and yet our current language\nmodeling technology fails to predict them. This\nmight happen because (2a) the technology is bi-\nased toward modeling words or characters and fails\nto discover intermediate morphemes, or because\n(2b) it fails to capture the syntactic and semantic\npredictors that govern the appearance of the extra\nmorphemes. We leave it to future work to tease\napart these hypotheses.\n2 Language Modeling\nA traditional closed-vocabulary, word-level lan-\nguage model operates as follows: Given a ﬁxed set\nof words V, the model provides a probability distri-\nbution over sequences of words with parameters to\nbe estimated from data. Most ﬁxed-vocabulary lan-\nguage models employ a distinguished symbol UNK\nthat represents all words not present in V; these\nwords are termed out-of-vocabulary (OOV).\nChoosing the set V is something of a black\nart: Some practitioners choose the k most com-\n1One might have expected a priori that some difference\nwould remain, because most highly inﬂected languages can\nalso vary word order to mark a topic-focus distinction, and\nthis (occasional) marking is preserved in our experiment.\n1\nProceedings of NAACL-HLT 2018, pages 536–541\nNew Orleans, Louisiana, June 1 - 6, 2018. c⃝2018 Association for Computational Linguistics\nmon words (e.g., Mikolov et al. (2010) choose\nk = 10000 ) and others use all those words that\nappear at least twice in the training corpus. In gen-\neral, replacing more words with UNK artiﬁcially\nimproves the perplexity measure but produces a\nless useful model. OOVs present something of a\nchallenge for the cross-linguistic comparison of lan-\nguage models, especially in morphologically rich\nlanguages, which simply have more word forms.\n2.1 The Role of Inﬂectional Morphology\nInﬂectional morphology can explode the base vo-\ncabulary of a language. Compare, for instance, En-\nglish and Turkish. The nominal inﬂectional system\nof English distinguishes two forms: a singular and\nplural. The English lexeme BOOK has the singular\nform book and the plural form books. In contrast,\nTurkish distinguishes at least 12: kitap, kitablar,\nkitabı, kitabın, etc.\nTo compare the degree of morphological inﬂec-\ntion in our evalation languages, we use count-\ning complexity (Sagot, 2013). This crude metric\ncounts the number of inﬂectional categories distin-\nguished by a language (e.g., English includes a cat-\negory of 3rd-person singular present-tense verbs).\nWe count the categories annotated in the language’s\nUniMorph (Kirov et al., 2018) lexicon. See Table 1\nfor the counting complexity of evaluated languages.\n2.2 Open-Vocabulary Language Models\nTo ensure comparability across languages, we re-\nquire our language models to predict every charac-\nter in an utterance, rather than skipping some char-\nacters because they appear in words that were (arbi-\ntrarily) designated as OOV in that language. Such\nmodels are known as “open-vocabulary” LMs.\nNotation. Let ˙∪denote disjoint union, i.e., A ˙∪\nB = C iff A∪B = C and A∩B = ∅. Let Σ be\na discrete alphabet of characters, including a dis-\ntinguished unknown-character symbol ⋆.2 A char-\nacter LM then deﬁnes p(c) = ∏|c|+1\ni=1 p(ci |c<i),\nwhere we take c|c|+1 to be a distinguished end-of-\nstring symbol EOS . In this work, we consider two\nopen-vocabulary LMs, as follows.\nBaseline n-gram LM. We train “ﬂat” hybrid\nword/character open-vocabulary n-gram models\n(Bisani and Ney, 2005), deﬁned over strings Σ+\n2The set of graphemes in these languages can be assumed\nto be closed, but external graphemes may on rare occasion\nappear in random text samples. These are rare enough to not\nmaterially affect the metrics.\nfrom a vocabularyΣ with mutually disjoint subsets:\nΣ = W ˙∪C ˙∪S, where single characters c∈Care\ndistinguished in the model from single character\nfull words w∈W, e.g., a versus the word a. Spe-\ncial symbols S = {EOW,EOS }are end-of-word\nand end-of-string, respectively. N-gram histories\nin H are either word-boundary or word-internal\n(corresponding to a whitespace tokenization), i.e.,\nH = Hb ˙∪Hi. String-internal word boundaries are\nalways separated by a single whitespace character.3\nFor example, if foo, baz ∈W but bar ̸∈W,\nthen the string foo bar baz would be gener-\nated as: foo b a r EOW baz EOS . Possible\n3-gram histories in this string would be, e.g., [foo\nb] ∈Hi, [r EOW ] ∈Hb, and [EOW baz] ∈Hb.\nSymbols are generated from a multinomial given\nthe history h, leading to a new history h′that now\nincludes the symbol and is truncated to the Markov\norder. Histories h ∈ Hb can generate symbols\ns ∈W ∪C ∪{EOS }. If s = EOS , the string is\nended. If s ∈W, it has an implicit EOW and the\nmodel transitions to history h′ ∈Hb. If s ∈C,\nit translitions to h′ ∈Hi. Histories h ∈Hi can\ngenerate symbols s ∈C∪{EOW}and transition\nto h′∈Hb if s= EOW, otherwise to h′∈Hi.\nWe use standard Kneser and Ney (1995) model\ntraining, with distributions at word-internal histo-\nries h∈Hi constrained so as to only provide prob-\nability mass for symbols s∈C∪{EOW}. We train\n7-gram models, but prune n-grams hswhere the\nhistory h∈Wk, for k> 4, i.e., 6- and 7-gram his-\ntories must include at least ones̸∈W. To establish\nthe vocabularies W and C, we replace exactly one\ninstance of each word type with its spelled out ver-\nsion. Singleton words are thus excluded from W,\nand character sequence observations from all types\nare included in training. Note any word w ∈W\ncan also be generated as a character sequence. For\nperplexity calculation, we sum the probabilities for\neach way of generating the word.\nLSTM LM. While neural language models can\nalso take a hybrid approach (Hwang and Sung,\n2017; Kawakami et al., 2017), recent advances indi-\ncate that full character-level modeling is now com-\npetitive with word-level modeling. A large part of\nthis is due to the use of recurrent neural networks\n(Mikolov et al., 2010), which can generalize about\n3The model can be extended to handle consecutive whites-\npace characters or punctuation at word boundaries; for this pa-\nper, the tokenization split punctuation from words and reduced\nconsecutive whitespaces to one, hence the simpler model.\n2\n0 50 100 150 200 250\nMorphological Counting Complexity\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nCost of Modeling Words (BPEC)\nn-gram\nLSTM\n(a) BPEC performance of n-gram (blue)\nand LSTM (green) LMs over word se-\nquences. Lower is better.\n0 50 100 150 200 250\nMorphological Counting Complexity\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\nCost of Modeling Lemmata (BPEC)\n(b) BPEC performance of n-gram (blue)\nand LSTM (green) LMs over lemma se-\nquences. Lower is better.\n0 50 100 150 200 250\nMorphological Counting Complexity\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nCost of Modeling Inﬂection (BPEC)\n(c) Difference in BPEC performance of\nn-gram (blue) and LSTM (green) LMs\nbetween words and lemmata.\nFigure 1: The primary ﬁndings of our paper are evinced in these plots. Each point is a language. While the LSTM outperforms\nthe hybrid n-gram model, the relative performance on the highly inﬂected languages compared to the more modestly inﬂected\nlanguages is almost constant; to see this point, note that the regression lines in Fig. 1c are almost identical. Also, comparing Fig. 1a\nand Fig. 1b shows that the correlation between LM performance and morphological richness disappears after lemmatization of\nthe corpus, indicating that inﬂectional morphology is the origin for the lower BPEC.\nhow the distribution p(ci |c<i) depends on c<i.\nWe use a long short-term memory (LSTM) LM\n(Sundermeyer et al., 2012), identical to that of\nZaremba et al. (2014), but at the character-level.\nTo achieve the hidden state hi ∈Rd at time step\ni, one feeds the left context ci−1 to the LSTM:\nhi = LSTM (c1,...,c i−1) where the model uses\na learned vector to represent each character type.\nThis involves a recursive procedure described in\nHochreiter and Schmidhuber (1997). Then, the\nprobability distribution over the ith character is\np(ct |c<i) = softmax (W hi + b), where W ∈\nR|Σ|×d and b ∈R|Σ|are parameters.\nParameters for all models are estimated on the\ntraining portion and model selection is performed\non the development portion. The neural models\nare trained with SGD (Robbins and Monro, 1951)\nwith gradient clipping, such that each component\nhas a maximum absolute value of 5. We optimize\nfor 100 iterations and perform early stopping (on\nthe development portion). We employ a character\nembedding of size 1024 and 2 hidden layers of size\n1024.4 The implementation is in PyTorch.\n3 A Fairer Evaluation: Multi-Text\nEffecting a cross-linguistic study on LMs is com-\nplicated because different models could be trained\nand tested on incomparable corpora. To avoid this\nproblem, we use multi-text: k-way translations of\nthe same semantic content.\n4As Zaremba et al. (2014) indicate, increasing the number\nof parameters may allow us to achieve better performance.\nWhat’s wrong with bits per character? Open-\nvocabulary language modeling is most commonly\nevaluated under bits per character (BPC) =\n1\n|c|+1\n∑|c|+1\ni=1 log p(ci | c<i).5 Even with multi-\ntext, comparing BPC is not straightforward, as it\nrelies on the vagaries of individual writing systems.\nConsider, for example, the difference in how Czech\nand German express the phoneme /tS /: Czech uses\nˇc, whereas German tsch. Now, consider the Czech\nword puˇc and its German equivalent Putsch. Even\nif these words are both predicted with the same\nprobability in a given context, German will end up\nwith a lower BPC. 6\nBits per English Character. Multi-text allows\nus to compute a fair metric that is invariant to\nthe orthographic (or phonological) changes dis-\ncussed above: bits per English character(BPEC).\nBPEC = 1\n|cEnglish|+1\n∑|c|+1\ni=1 log p(ci |c<i), where\ncEnglish is the English character sequence in the\nutterance aligned to c. The choice of English is\narbitrary, as any other choice of language would\nsimply scale the values by a constant factor.\nNote that this metric is essentially capturing the\noverall bits per utterance , and that normalizing\nusing English characters only makes numbers in-\ndependent of the overall utterance length; it is not\ncritical to the analysis we perform in this paper.\n5To aggregate this over an entire test corpus, we replace\nthe denominator and also the numerator by summations over\nall utterances c.\n6Why not work with phonological characters, rather than\northographic ones, obtaining /putS / for both Czech and Ger-\nman? Sadly this option is also fraught with problems as many\nlanguages have perfectly predictable phonological elements\nthat will artiﬁcially lower the score.\n3\nBPEC /∆BPC (·e-2)\ndata (M) hybridn-gram LSTM\nlang wds / chMCC form lemma form lemma\nbg 0.71/4.3 96 1.13/ 4 1.03/ 1 0.95/ 3 0.80/ 1\ncs 0.65/3.9 195 1.20/ -81.05/-120.97/ -60.83/ -9\nda 0.70/4.1 15 1.10/ -11.06/ -40.85/ -10.82/ -3\nde 0.74/4.8 38 1.25/ 171.18/ 131.04/ 140.90/ 10\nel 0.75/4.6 50 1.18/ 131.08/ 5 0.90/ 100.82/ 4\nen 0.75/4.1 6 1.10/ 0 1.08/ -30.85/ 0 0.83/ -3\nes 0.81/4.6 71 1.15/ 121.07/ 7 0.87/ 9 0.80/ 5\net∗ 0.55/3.9 110 1.20/ -81.11/-150.97/ -60.89/-12\nﬁ∗ 0.52/4.2 198 1.18/ 2 1.02/-111.05/ 1 0.79/ -9\nfr 0.88/4.9 30 1.13/ 171.06/ 130.92/ 140.78/ 10\nhu∗ 0.63/4.3 94 1.25/ 5 1.12/ -91.09/ 5 0.89/ -7\nit 0.85/4.8 52 1.15/ 161.08/ 140.96/ 140.79/ 10\nlt 0.59/3.9 152 1.17/ -61.12/ -70.93/ -50.88/ -6\nlv 0.61/3.9 81 1.15/ -61.04/ -90.91/ -50.81/ -7\nnl 0.75/4.5 26 1.20/ 111.16/ 4 0.92/ 8 0.91/ 4\npl 0.65/4.3 112 1.21/ 6 1.09/ -10.97/ 5 0.84/ -1\npt 0.89/4.8 77 1.17/ 161.09/ 9 0.88/ 120.82/ 7\nro 0.74/4.4 60 1.17/ 8 1.09/ 0 0.90/ 6 0.84/ 0\nsk 0.64/3.9 40 1.16/ -61.06/-110.92/ -50.87/ -9\nsl 0.64/3.8 100 1.15/-101.02/-100.90/ -80.80/ -7\nsv 0.66/4.1 35 1.11/ -21.06/ -80.86/ -20.83/ -7\nTable 1: Results for all conﬁgurations and the typological\nproﬁle of the 21 Europarl languages. All languages are Indo-\nEuropean, except for those marked with ∗ which are Uralic.\nMorpholical counting complexity (MCC) is given for each\nlanguage, along with bits per English character (BPEC) and\nthe ∆BPC, which is BPEC minus bits per character (BPC).\nThis is blue if BPEC > BPC and red if BPEC < BPC.\nA Potential Confound: Translationese. Work-\ning with multi-text, however, does introduce a new\nbias: all of the utterances in the corpus have a\nsource language and 20 translations of that source\nutterance into target languages. The characteris-\ntics of translated language has been widely studied\nand exploited, with one prominent characteristic of\ntranslations being simpliﬁcation (Baker, 1993).\nNote that a signiﬁcant fraction of the original\nutterances in the corpus are English. Our analysis\nmay then have underestimated the BPEC for other\nlanguages, to the extent that their sentences consist\nof simpliﬁed “translationese.” Even so, English had\nthe lowest BPEC from among the set of languages.\n4 Experiments and Results\nOur experiments are conducted on the 21 languages\nof the Europarl corpus (Koehn, 2005). The corpus\nconsists of utterances made in the European par-\nliament and are aligned cross-linguistically by a\nunique utterance id. With the exceptions (noted in\nTable 1) of Finnish, Hungarian and Estonian, which\nare Uralic, the languages are Indo-European.\nWhile Europarl does not contain quite our de-\nsired breadth of typological diversity, it serves our\npurpose by providing large collections of aligned\ndata across many languages. To create our experi-\nmental data, we extract all utterances and randomly\nsort them into train-development-test splits such\nthat roughly 80% of the data are in train and 10%\nin development and test, respectively. 7 We also\nperform experiments on lemmatized text, where we\nreplace every word with its lemma using the UD-\nPipe toolkit (Straka et al., 2016), stripping away\nits inﬂectional morphology. We report two evalua-\ntion metrics: BPC and BPEC (see §3). Our BPEC\nmeasure always normalizes by the length of the\noriginal, not lemmatized, English.\nExperimentally, we want to show: (i) When eval-\nuating models in a controlled environment (multi-\ntext under BPEC), the models achieve lower per-\nformance on certain languages and (ii) inﬂectional\nmorphology is the primary culprit for the perfor-\nmance differences. However, we repeat that we do\nnot in this paper tease apart whether the models are\nat fault, or that certain languages inherently encode\nmore information.\n5 Discussion and Analysis\nWe display the performance of the n-gram LM and\nthe LSTM LM under BPC and BPEC for each of\nthe 21 languages in Fig. 1 with full numbers listed\nin Table 1. There are several main take-aways.\nThe Effect of BPEC. The ﬁrst major take-away\nis that BPEC offers a cleaner cross-linguistic com-\nparison than BPC. Were we to rank the languages\nby BPC (lowest to highest), we would ﬁnd that\nEnglish was in the middle of the pack, which is\nsurprising as new language models are often only\ntuned on English itself. For example, BPC surpris-\ningly suggests that French is easier to model than\nEnglish. However, ranking under BPEC shows that\nthe LSTM has the easiest time modeling English it-\nself. Scandinavian languages Danish and Swedish\nhave BPEC closest to English; these languages are\ntypologically and genetically similar to English.\nn-gram versus LSTM. As expected, the LSTM\noutperforms the baseline n-gram models across\nthe board. In addition, however, n-gram modeling\nyields relatively poor performance on some lan-\nguages, such as Dutch, with only modestly more\ncomplex inﬂectional morphology than English.\n7Characters appearing < 100 times in train are ⋆.\n4\nOther phenomena—e.g., perhaps, compounding—\nmay also be poorly modeled by n-grams.\nThe Impact of Inﬂectional Morphology. An-\nother major take-away is that rich inﬂectional mor-\nphology is a difﬁculty for both n-gram and LSTM\nLMs. In this section we give numbers for the\nLSTMs. Studying Fig. 1a, we ﬁnd that Spear-\nman’s rank correlation between a language’s BPEC\nand its counting complexity ( §2.1) is quite high\n(ρ = 0.59, signiﬁcant at p <0.005). This clear\ncorrelation between the level of inﬂectional mor-\nphology and the LSTM performance indicates that\ncharacter-level models do not automatically ﬁx the\nproblem of morphological richness. If we lemma-\ntize the words, however (Fig. 1b), the correlation\nbecomes insigniﬁcant and in fact slightly negative\n(ρ= −0.13, p≈0.56). The difference of the two\nprevious graphs (Fig. 1c) shows more clearly that\nthe LM penalty for modeling inﬂectional endings\nis greater for languages with higher counting com-\nplexity. Indeed, this penalty is arguably a more\nappropriate measure of the complexity of the in-\nﬂectional system. See also Fig. 2.\nThe differences in BPEC among languages are\nreduced when we lemmatize, with standard devia-\ntion dropping from 0.065 bits to 0.039 bits. Zoom-\ning in on Finnish (see Table 1), we see that Finnish\nforms are harder to model than English forms, but\nFinnish lemmata are easier to model than English\nones. This is strong evidence that it was primarily\nthe inﬂectional morphology, which lemmatization\nstrips, that caused the differences in the model’s\nperformance on these two languages.\n6 Related Work\nRecurrent neural language models can effec-\ntively learn complex dependencies, even in open-\nvocabulary settings (Hwang and Sung, 2017;\nKawakami et al., 2017). Whether the models are\nable to learn particular syntactic interactions is an\nintriguing question, and some methodologies have\nbeen presented to tease apart under what circum-\nstances variously-trained models encode attested\ninteractions (Linzen et al., 2016; Enguehard et al.,\n2017). While the sort of detailed, construction-\nspeciﬁc analyses in these papers is surely informa-\ntive, our evaluation is language-wide.\nMT researchers have investigated whether an En-\nglish sentence contains enough information to pre-\ndict the ﬁne-grained inﬂections used in its foreign-\nlanguage translations (see Kirov et al., 2017).\n0.9 1.0 1.1\nBPEC (LSTM over forms)\n0.80\n0.85\n0.90\n0.95\nBPEC (LSTM over lemmata)\nen\nda\nnl\nfr\nsv\nde\nsk\nel\nit\nro\nes\npt\nlv\nhu\nbgsl\net\npl\nlt\ncs\nﬁ\nFigure 2: Each dot is a language, and its coordinates are the\nBPEC values for the LSTM LMs over words and lemmata.\nThe top and right margins show kernel density estimates of\nthese two sets of BPEC values. All dots follow the blue\nregression, but stay below the green line ( y = x), and the\ndarker dots—which represent languages with higher counting\ncomplexity—tend to fall toward the right but not toward the\ntop, since counting complexity is correlated only with the\nBPEC over words.\nSproat et al. (2014) present a corpus of close\ntranslations of sentences in typologically diverse\nlanguages along with detailed morphosyntactic\nand morphosemantic annotations, as the means\nfor assessing linguistic complexity for compara-\nble messages, though they expressly do not take an\ninformation-theoretic approach to measuring com-\nplexity. In the linguistics literature, McWhorter\n(2001) argues that certain languages are less com-\nplex than others: he claims that Creoles are simpler.\nM¨uller et al. (2012) compare LMs on EuroParl, but\ndo not compare performance across languages.\n7 Conclusion\nWe have presented a clean method for the cross-\nlinguistic comparison of language modeling: We\nassess whether a language modeling technique can\ncompress a sentence and its translations equally\nwell. We show an interesting correlation between\nthe morphological richness of a language and the\nperformance of the model. In an attempt to explain\ncausation, we also run our models on lemmatized\nversions of the corpora, showing that, upon the\nremoval of inﬂection, no such correlation between\nmorphological richness and LM performance exists.\nIt is still unclear, however, whether the performance\ndifference originates from the inherent difﬁculty of\nthe languages or with the models.\n5\nReferences\nMona Baker. 1993. Corpus linguistics and translation\nstudies: Implications and applications. Text and\nTechnology: In Honour of John Sinclair 233:250.\nMaximilian Bisani and Hermann Ney. 2005. Open vo-\ncabulary speech recognition with ﬂat hybrid models.\nIn INTERSPEECH. pages 725–728.\n´Emile Enguehard, Yoav Goldberg, and Tal Linzen.\n2017. Exploring the syntactic abilities of RNNs\nwith multi-task learning. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) . Association for Compu-\ntational Linguistics, pages 3–14. https://doi.\norg/10.18653/v1/K17-1003.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation\n9(8):1735–1780.\nKyuyeon Hwang and Wonyong Sung. 2017. Character-\nlevel language modeling with hierarchical recurrent\nneural networks. In International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, pages 5720–5724.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to create and reuse words in open-\nvocabulary neural language modeling. In Proceed-\nings of the 55th Annual Meeting of the Associ-\nation for Computational Linguistics (ACL) . Asso-\nciation for Computational Linguistics, Vancouver,\nCanada, pages 1492–1502. http://aclweb.\norg/anthology/P17-1137.\nChristo Kirov, Ryan Cotterell, John Sylak-Glassman,\nG´eraldine Walther, Ekaterina Vylomova, Patrick\nXia, Manaal Faruqui, Arya McCarthy, Sabrina J.\nMielke, Sandra K ¨ubler, David Yarowsky, Jason Eis-\nner, and Mans Hulden. 2018. Unimorph 2.0: Uni-\nversal morphology. In Proceedings of the Ninth In-\nternational Conference on Language Resources and\nEvaluation (LREC). European Language Resources\nAssociation (ELRA).\nChristo Kirov, John Sylak-Glassman, Rebecca\nKnowles, Ryan Cotterell, and Matt Post. 2017.\nA rich morphological tagger for English: Ex-\nploring the cross-linguistic tradeoff between\nmorphology and syntax. In Proceedings of the\n15th Conference of the European Chapter of\nthe Association for Computational Linguistics:\nVolume 2, Short Papers . Association for Com-\nputational Linguistics, pages 112–117. http:\n//aclweb.org/anthology/E17-2018.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In In-\nternational Conference on Acoustics, Speech, and\nSignal Processing, ICASSP. pages 181–184.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In MT Summit. vol-\nume 5, pages 79–86.\nTal Linzen, Emmanuel Dupoux, and Yoav Gold-\nberg. 2016. Assessing the ability of LSTMs\nto learn syntax-sensitive dependencies. Transac-\ntions of the Association of Computational Linguis-\ntics 4:521–535. http://www.aclweb.org/\nanthology/Q16-1037.\nJohn McWhorter. 2001. The world’s simplest gram-\nmars are creole grammars. Linguistic Typology\n5(2):125–66.\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Re-\ncurrent neural network based language model.\nIn INTERSPEECH 2010, 11th Annual Confer-\nence of the International Speech Communica-\ntion Association . pages 1045–1048. http:\n//www.isca-speech.org/archive/\ninterspeech_2010/i10_1045.html.\nThomas M¨uller, Hinrich Sch¨utze, and Helmut Schmid.\n2012. A comparative investigation of morpho-\nlogical language modeling for the languages of\nthe European Union. In Proceedings of the\n2012 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies . Association\nfor Computational Linguistics, Montr ´eal, Canada,\npages 386–395. http://www.aclweb.org/\nanthology/N12-1043.\nHerbert Robbins and Sutton Monro. 1951. A stochas-\ntic approximation method. The Annals of Mathemat-\nical Statistics pages 400–407.\nBenoˆıt Sagot. 2013. Comparing complexity mea-\nsures. In Computational Approaches to Morpholog-\nical Complexity.\nRichard Sproat, Bruno Cartoni, HyunJeong Choe,\nDavid Huynh, Linne Ha, Ravindran Rajakumar, and\nEvelyn Wenzel-Grondie. 2014. A database for mea-\nsuring linguistic information content. In LREC.\nMilan Straka, Jan Hajiˇc, and Jana Strakov´a. 2016. UD-\nPipe: Trainable pipeline for processing CoNLL-U\nﬁles performing tokenization, morphological analy-\nsis, POS tagging and parsing. In LREC.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Thirteenth Annual Conference of the Interna-\ntional Speech Communication Association.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\nCoRR abs/1409.2329. http://arxiv.org/\nabs/1409.2329.\n6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6606999635696411
    },
    {
      "name": "Computational linguistics",
      "score": 0.5943857431411743
    },
    {
      "name": "Linguistics",
      "score": 0.4421624541282654
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.42290329933166504
    },
    {
      "name": "Programming language",
      "score": 0.40342408418655396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38818037509918213
    },
    {
      "name": "Natural language processing",
      "score": 0.35922056436538696
    },
    {
      "name": "Philosophy",
      "score": 0.1569407880306244
    },
    {
      "name": "Physics",
      "score": 0.06189659237861633
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}