{
    "title": "Toward Foundational Deep Learning Models for Medical Imaging in the New Era of Transformer Networks",
    "url": "https://openalex.org/W4307923847",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2168382548",
            "name": "Martin J. Willemink",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2609450958",
            "name": "Holger R. Roth",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2188421051",
            "name": "Veit Sandfort",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3014974815",
        "https://openalex.org/W2984306354",
        "https://openalex.org/W2979708377",
        "https://openalex.org/W3007935259",
        "https://openalex.org/W3012501605",
        "https://openalex.org/W4242308908",
        "https://openalex.org/W3119358336",
        "https://openalex.org/W2982036758",
        "https://openalex.org/W3097588083",
        "https://openalex.org/W3086590218",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W4310895557"
    ],
    "abstract": "Deep learning models are currently the cornerstone of artificial intelligence in medical imaging. While progress is still being made, the generic technological core of convolutional neural networks (CNNs) has had only modest innovations over the last several years, if at all. There is thus a need for improvement. More recently, transformer networks have emerged that replace convolutions with a complex attention mechanism, and they have already matched or exceeded the performance of CNNs in many tasks. Transformers need very large amounts of training data, even more than CNNs, but obtaining well-curated labeled data is expensive and difficult. A possible solution to this issue would be transfer learning with pretraining on a self-supervised task using very large amounts of unlabeled medical data. This pretrained network could then be fine-tuned on specific medical imaging tasks with relatively modest data requirements. The authors believe that the availability of a large-scale, three-dimension-capable, and extensively pretrained transformer model would be highly beneficial to the medical imaging and research community. In this article, authors discuss the challenges and obstacles of training a very large medical imaging transformer, including data needs, biases, training tasks, network architecture, privacy concerns, and computational requirements. The obstacles are substantial but not insurmountable for resourceful collaborative teams that may include academia and information technology industry partners. Â© RSNA, 2022 <b>Keywords:</b> Computer-aided Diagnosis (CAD), Informatics, Transfer Learning, Convolutional Neural Network (CNN).",
    "full_text": null
}