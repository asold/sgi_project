{
  "title": "A Dimension Centric Proximate Attention Network and Swin Transformer for Age-Based Classification of Mild Cognitive Impairment From Brain MRI",
  "url": "https://openalex.org/W4388642353",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2182242712",
      "name": "T. Illakiya",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2109846087",
      "name": "R Karthik",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963168174",
    "https://openalex.org/W2010643121",
    "https://openalex.org/W2966607877",
    "https://openalex.org/W3126177281",
    "https://openalex.org/W4285246255",
    "https://openalex.org/W3160208258",
    "https://openalex.org/W2927655793",
    "https://openalex.org/W4380077983",
    "https://openalex.org/W4323534860",
    "https://openalex.org/W3171990888",
    "https://openalex.org/W4284973298",
    "https://openalex.org/W3044140833",
    "https://openalex.org/W3109659125",
    "https://openalex.org/W4387272159",
    "https://openalex.org/W2993219936",
    "https://openalex.org/W4224246814",
    "https://openalex.org/W2942882625",
    "https://openalex.org/W4381188046",
    "https://openalex.org/W3025729836",
    "https://openalex.org/W4313597920",
    "https://openalex.org/W3194360768",
    "https://openalex.org/W4214900691",
    "https://openalex.org/W4319030137",
    "https://openalex.org/W4379985122",
    "https://openalex.org/W3120309683",
    "https://openalex.org/W2901932536",
    "https://openalex.org/W2945065837",
    "https://openalex.org/W4377007227",
    "https://openalex.org/W4378697095",
    "https://openalex.org/W2916257687",
    "https://openalex.org/W3178350790",
    "https://openalex.org/W3093116876",
    "https://openalex.org/W4290713633",
    "https://openalex.org/W3047901047",
    "https://openalex.org/W4321457755",
    "https://openalex.org/W4386295689",
    "https://openalex.org/W3185098617",
    "https://openalex.org/W3033107563",
    "https://openalex.org/W4367174198",
    "https://openalex.org/W4380483648",
    "https://openalex.org/W4206382244",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W3019196002",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4385878937",
    "https://openalex.org/W3110420020",
    "https://openalex.org/W4285600294",
    "https://openalex.org/W4386075553",
    "https://openalex.org/W2179352600",
    "https://openalex.org/W4295009181",
    "https://openalex.org/W4206270325",
    "https://openalex.org/W4225114743",
    "https://openalex.org/W2602552939",
    "https://openalex.org/W3198401109",
    "https://openalex.org/W4316369019",
    "https://openalex.org/W3098722327",
    "https://openalex.org/W3136333197"
  ],
  "abstract": "The early identification and treatment of Mild Cognitive Impairment (MCI) play a crucial role in managing the risk of Alzheimer&#x2019;s disease (AD). However, current methods for categorizing progressive MCI and stable MCI based on brain MRI scans have proven insufficient due to the subtle nature of the features involved. This research aims to improve the effectiveness of MCI classification through the utilization of a Deep Learning (DL) network. The primary objective of this work is to improve the feature representation of brain MRI scans for more accurate classification. The proposed model is a hybrid MCI classification system that integrates three components: the Swin Transformer, the Dimension Centric Proximity Aware Attention Network (DCPAN), and the Age Deviation Factor (ADF). The proposed network achieves better classification results through a unique feature fusion approach that combines global, local, proximal features, and dimensional dependencies. It effectively combines fine-grained details with broader contextual information to extract discriminative features. Experimental results demonstrate the effectiveness of the proposed network, achieving an accuracy of 79.8&#x0025;, precision of 76.6&#x0025;, recall of 80.2&#x0025;, and an F1-score of 78.4&#x0025; when evaluated on the ADNI dataset.",
  "full_text": " \nVOLUME XX, 2017                                                                                                                                                                                                                                                                                \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000. \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \n \n \nA Dimension Centric Proximate Attention \nNetwork and Swin Transformer for  \nAge-based Classification of Mild \nCognitive Impairment from Brain MRI \n \nT. Illakiya1, R. Karthik2, for the Alzheimer's Disease Neuroimaging Initiative3 \n1 School of Computer Science and Engineering, Vellore Institute of Technology, Chennai, Tamil Nadu, India.  \n2Centre for Cyber Physical Systems, Vellore Institute of Technology, Chennai, Tamil Nadu, India. \n3Data used in the preparation of this article were obtained from the Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). \nAs such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in \nanalysis or writing of this report. A complete listing of ADNI investigators can be found at:   \nhttp://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf. \n \nCorresponding author: R. Karthik (e-mail: r.karthik@vit.ac.in). \n \nABSTRACT The early identification and treatment of Mild Cognitive Impairment (MCI) play a crucial role \nin managing the risk of Alzheimer's disease (AD). However, current methods for categorizing p rogressive \nMCI and stable MCI based on brain MRI scans have proven insufficient due to the subtle nature of the features \ninvolved. This research aims to improve the effectiveness of MCI classification through the utilization of a \nDeep Learning (DL) network. The primary objective of this work is to improve the feature representation of \nbrain MRI scans for more accurate classification. The proposed model is a hybrid MCI classification system \nthat integrates three components: the Swin Transformer, the Dimension Centric Proximity Aware Attention \nNetwork (DCPAN), and the Age Deviation Factor (ADF). The proposed network achieves better \nclassification results through a unique feature fusion approach that combines global, local, proximal features, \nand dimensional dependencies. It effectively combines fine -grained details with broader contextual \ninformation to extract discriminative features. Experimental results demonstrate the effectiveness of the \nproposed network, achieving an accuracy of 79.8%, precision of 76.6%, recall of 80.2%, and an F1 -score of \n78.4% when evaluated on the ADNI dataset. \nINDEX TERMS  Alzheimer's Disease, Magnetic Resonance Imaging, Mild Cognitive Impairment, \nConvolutional Neural Network, Image Classification. \nI. INTRODUCTION \nAlzheimer's disease (AD) is a chronic irreversible \nneurodegenerative condition characterized by the atrophy \nand impairment of neurons in the brain. It is the most \nprevalent form of dementia and results in significant deficits \nin cognition, behavior, and so cial functioning, ultimately \naffecting an individual's capacity for living independently. \nThe progression of AD is characterized by early, middle, and \nlate phases. As the disease advances, alleviating its \nsymptoms becomes increasingly challenging. However,  \nprompt diagnosis during the preclinical phase known as Mild \nCognitive Impairment (MCI) can effectively slow down its \nprogression [1]. In contrast to the typical decline in cognitive \nabilities associated with aging, MCI represents an \nintermediate phase bet ween aging and AD. Research \nfindings suggest conversion rates from MCI to AD ranging \nfrom 10.2% to 33.6% within one year and from 9.8% to \n36.3% within two years, highlighting the importance of early \ndetection in achieving more favorable outcomes [2]. Thus, \nMCI can be considered an intervention point to slow or \nreverse this degenerative disease.  \n     Progressive MCI (pMCI) and stable MCI (sMCI ) are the \ntwo most common subtypes of MCI. The term pMCI refers \nto a form of MCI in which cognitive function decreases over \ntime. The brain region responsible for c ognitive functions \nsuch as memory, attention, language, and executive \nfunctioning will be affected. The risk of developing AD is \nhigher with pMCI, although the disease does not develop in \nall cases . In contrast, sMCI does not decline cognitive \nfunction significantly and does  not progress to AD.  For \nseveral reasons, it is crucial to distinguish progressive from \nstable MCI.  First and foremost, it allows clinicians and \n1 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nresearchers to identify individuals whose AD risk is higher \nand who may benefit from early intervention. Secondly, it \ncontributes to the development of targeted treatments and \ninterventions for MCI by providing insight into underlying \nmechanisms. Hence, it is crucial to classify pMCI and sMCI \naccurately. When MCI is detected early, interventions and \ntreatments can be offered to slow its progression and \nimprove quality of life. \n    The diagnosis of MCI involves a variety of assessments \ndesigned to evaluate memory and cognitive function. \nPhysical examinations, neuropsychological testing, and \nneuroimaging are common methods for detecting MCI. \nPatients are assessed physically for reflex es, muscle tone, \nvision, and functional mobility during the physical \nexamination. Additionally, memory and other cognitive \nfunctions can be assessed through a brief mental status \nevaluation. It is possible to diagnose and track the \nprogression of the disea se using these assessments  but they \nare subjective in nature. \nAdditionally, AD and MCI can be detected through \ngenetic testing, blood tests, and other biomarker tests [3, 4]. \nOne of the widely used diagnostic tool  for MCI is \nneuroimaging. Generally, Structural Magnetic Resonance \nImaging (sMRI), Functional Magnetic Resonance Imaging \n(fMRI), and Positron Emission Tomography (PET) are used \nto diagnose MCI. A neuroimaging technique can provide \nvaluable insight into structural and functional changes in the \nbrain, aiding in the early detection and classification of MCI \n[5, 6]. One of the primary neuroimaging techniques used in \nthe classification of MCI is sMRI [7]  [8]. It allows \nvisualization of structural brain abnormalities and detection \nof MCI -related changes. In order to distinguish between \npatients with MCI and those with normal cognitive abilities, \nMRI measures the volume and integrity of brain regions. \nAlthough these images are difficult to interpret, advanced \nMachine Learning ( ML) and DL techniques are being \nexplored to improve diagnostics. However, these \nmethodologies encounter several challenges, including \ndistinguishing between an AD -affected brain and an MCI-\naffected brain, accurately determining the disease stage in \nMCI/AD patients, and predicting the progression of MCI [9]. \nFurthermore, the similarity in MRI findings between \nprogressive pMCI and sMCI poses a significant challenge in \ndistinguishing MCI patients who will ultimately progress to \nAD. Hence, the main objective of this research is to develop \na customized DL network that classifies pMCI and sMCI \neffectively. The proposed work integrates features from the \nSwin Transformer, Dimension Centric Proxim ate Attention \nNetwork (DCPAN), and incorporates an Age Deviation \nFactor (ADF) to enhance its predictive capabilities. \nII. RELATED WORKS \nThe early detection of MCI is important as it can help in \ntimely intervention to delay its progression to AD. Various \nstudies have been conducted to classify MCI using \ntechniques like ML and DL methods . Traditional ML \nalgorithms often struggle with feature extraction due to the \ncomplex nature of cognitive decline. Hence, most of the \nresearch works involve deep learning methods which do not \nrequire manual feature extraction. One of the reasons deep \nlearning excels in this domain is its ability to automatica lly \nlearn hierarchical features from raw data, eliminating the \nneed for feature engineering  [10]. Deep learning methods \nsuch as Convolutional Neural Network ( CNN),  Recurrent \nNeural Network (RNN) plays an vital role in image vision \ntasks such as classification, prediction, object recognition \netc.,[11 - 14]. This section focuses on the latest studies and \nresearch related to the classification of MCI using DL \ntechniques. Oh et al. used an approach for the classification \nof MCI using a volumetric CNN and transfer learning [ 15]. \nFor the classification of pMCI vs sMCI, transfer learning was \nemployed. For imp roving generalization, the visual \nrepresentations obtained in AD vs NC classification were \ntransferred to learning models of pMCI vs. sMCI. \n     Along with the CNN, few works have incorporated Long \nShort-Term Memory (LSTM). For instance, Sun et al. used \na CNN with residuals combined with a multiple LSTM to \npredict the progression of MCI to AD [ 16]. Similar to the \nprevious work, Feng et al. used a combination of 3D -CNN \nand FSBi -LSTM [ 17]. It uses a 3D -CNN architecture to \nextract deep feature representations from both MRI and PET \ndata. To further improve the performance, a Fully Stacked \nBidirectional LSTM (FSBi-LSTM) is applied to the hidden \nspatial information from the deep feature maps. Zhang et al. \nused a population-based Graph Convolution Network (GCN) \nmethod that allows nodes in a graph to aggregate and \npropagate information from their neighboring nodes, \nenabling them to capture relationships and dependencies \nwithin the graph structure [ 18]. Few works have used \nensemble modelling to enhance accuracy, reduce overfitting, \nand increase the robustness of the model. For example, Pan \net al. combined CNN and ensemble learning which involves \ntraining many CNN models using sagittal, coronal, or axial \nMRI slices and then integrating these models into a single \nensembled model [19]. A similar Siamese ensemble model \nwas used by Arco et al. , where  Siamese networks are \nconstructed by combining pairs of regions of interest in the \nbrain scans [20]. Features are extracted from the embeddings \nof the final linear layer, and classification is performed by \ncombining the characteristics of each pair of regions into an \nensemble architecture. Oh et al. presented a learning -based \nframework that generates counterfactual maps that identify \nhypothetical abnormalities in normal brain images that may \nlead to AD [21]. Han et al. used broad learning system which \nincludes feature mapping and the feature enhancement \nmodule for the AD diagnostic task [22]. \n      Focusing on brain regions, such as hippocampal atrophy, \nis used in some of the research works [23] [24]. For example, \nLi et al. used a multi -channel cascaded CNN to learn \nhierarchical representations of hippocampus shapes and \nabnormalities for MCI classification [ 25]. A similar \nhippocampal analysis was carried out by Cui et al. The \nmethod involves extracting patches from 3D image data and \nusing the DenseNet model for classification [ 26]. The \n2 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nexperiment tests the effects of different patch sizes on \nclassification performance, with larger patches covering \nmore neighboring information of the hippocampus. \n     Further, Li et al. used DenseNet to learn intensity and \nshape information from both interior and exterior \nhippocampal patches [ 27]. The features from the left and \nright hippocampus are then combined to extract high -level \nfeatures for disease classification using a RNN. Different \nmodalities offer unique insights into the progression of MCI. \nFew research works have included multimodal data for the \nclassification of MCI [ 28]. Zhang et al. used a 3D -Mixer \nmodel for predicting AD status using PET data and s MRI \npatches [29]. Huang et al. incorporated a CNN model that \nintegrates T1-weighted MRI and FDG -PET images for the \ndiagnosis of AD [ 30]. As it is most frequently studied and \nconsidered to have the highest clinical value, the \nhippocampal ROI was chosen for MRI. Abdelaziz et al. used \na DL framework for the diagnosis of MCI using incomplete \nmultimodal data, such as genetic and neuroimaging data, \nthrough the use of CNNs [ 31]. The framework aims to \naddress the challenges of missing data and heterogeneity \nbetween biomarkers. Shi et al. used a learning -based model \non both MRI and PET scans for the diagnosis of MCI and \nperformed adaptive similarity learning and feature selection \nsimultaneously. The model learns a similarity matrix by \njointly considering different modalities [32].  \n      Classification tasks that use medical images like MRI , \nPET scans have been improved using attention models. \nThese images contain comprehensive information that may \nnot be pertinent to the specific classification task. \nIncorporating attention models allows the classification \nprocess to selectively emphasize ar eas that contribute to \naccurate predictions by emphasizing the most informative \nareas [ 33]. In recent studies, attention -based models have \nbeen used to diagnose MCI. To improve diagnostic \nperformance, Zhang et al. employed an 3D Residual \nAttention Deep Neural Network (3D ResAttNet) that gathers \nlocal, global, and spatial information from MR images [ 34]. \nSimilarly, Luo et al. utilized Class Activation Attention \nTransfer (CAAT) to predict the progression of MCI to AD \n[35]. CAAT uses a 3D CNN to learn regions of interest \n(ROIs) from a source task and transfers attention maps \ninstead of model weights to the target classification task. \nZhang et al. used a disease-related Attentional UNet \nFramework that consists of three components: weakly \nsupervised detection of disease-related locations using a full \nCNN, extraction of local and global features relevant to these \nlocations using a hybrid network, and fusion of these features \nfor the final identification of MCI [ 36]. Gao et al. utilized \nTask-Induced Pyramid and Attention-GAN based on the U-\nnet structure and includes contracting and expanding parts \n[37]. It uses pyramid convolution blocks to capture different \nlevels of details from MRI images for image imputation.  \n      The concept of a ge biomarker for the diagnosis of MCI \nhas gained significant prominence in recent research, aiming \nfor early detection of this cognitive condition. It is a valuable \nindicator that indicates how far an individual's biological \naging process differs from their chronological age. There \nhave been recent studies demonstrating the value of this age \nfactor as a biomarker, particularly for predicting dementia \nprogression in patients with MCI. As part of their fine-tuning \nprocess, Gao e t al. ex tract and transfer features and \nknowledge, including an age -related surrogate biomarker, \nfrom a pre-training stage to the fine-tuning stage [38]. A pre-\ntrained network performs feature extraction and age \nprediction, and a fine-tuned network handles MCI converter \nprediction. Similar work was carried out by Gao et al. The \nnetwork that uses a multi -scale attention convolution learns \nfeature maps with adaptively aggregated multi-scale kernels, \ncapturing local variations [ 39]. The aging prediction \nsubnetwork emb eds age information into image features, \ncapturing dependencies between subjects at different ages. \n     The advantages of deep learning, such as its ability to \nhandle complex data, extract non -linear patterns, and \nleverage transfer learning, make it a compelling choice for \nthe classification of pMCI versus sMCI, particularly when \ndealing with diverse and int ricate cognitive data. However, \nit is essential to acknowledge that deep learning is not \nwithout its challenges. In the classification of MCI, it is \nimportant to consider that subtle or less visually prominent \nfeatures may possess significant discriminativ e information \nfor distinguishing between pMCI and sMCI cases [ 40]. \nAdditionally, the performance of attention models can be \nhindered if the training dataset does not adequately capture \nthe full diversity and variability of pMCI and sMCI cases. \nThis can result in difficulties in generalizing the model to \nunseen or sligh tly different cases, leading to diminished \nclassification performance. Therefore, despite the potential \nbenefits, attention models face certain limitations that should \nbe addressed when applyi ng them to the classification of \nMCI. \nA. RESEARCH GAPS AND MOTIVATION \nThe following are the research gaps addressed by the \nproposed work in pMCI and sMCI classification. \n1) It is difficult for existing deep learning models to \ndistinguish between pMCI and sMCI because of their \nlimited ability to capture subtle variations in brain \nstructure. Hence, there is a necessity for the \ndevelopment of advanced deep learning models that \ncan effectively capture fine -grained features and \nrefine structural differences within the brain.  \n2) The spatial and dimensional dependencies that exist \nacross various brain regions have not been captured \nadequately in most of the existing methods for the \nclassifications of MCI. Therefore, it is difficult to \ndistinguish between sMCI and pMCI with good \naccuracy. \n3) Although image biomarkers have been promising in \nidentifying potential neuroimaging-based biomarkers \nfor distinguishing individuals with AD, recent \nresearch suggests that utilizing neuroimage as an only \ndiagnostic tool may not be sufficient to classify \npatients with pMCI and sMCI . It is therefore \n3 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nnecessary to develop more efficient DL methods for \nidentifying PMCI by incorporating the relevant \nbiomarkers. \nB. RESEARCH CONTRIBUTIONS \nThe following key contributions are proposed to address the \nresearch gaps. These contributions play a significant role in \nthe utilization of DL models for improved MCI \nclassification. \n1) The proposed network aims to enhance the \nclassification accuracy for pMCI versus sMCI. This \nis achieved by using long -range dependencies and \nspatial features derived from the fusion of Swin \nTransformer and DCPAN. \n2) A novel approach for the MCI classification task is \npresented by combining dimensional features and \nproximal features, offering a precise representation \nfor improved prediction. \n3) The performance of the MCI classification model is \nincreased by incorporating the ADF into image \nfeatures extracted from a Swin transformer with \nDCPAN. In this innovative approach, both image -\nbased and age -related information was used to \nimprove MCI classification accuracy. \n \nIII. MATERIALS AND METHODS \nThe section describes the dataset, pre -processing & \naugmentation methods and novel blocks used in the \nproposed model, specifically the DCPAN track, the Swin \ntransformer track, spatial pyramid pooling, and the \ndetermination of the ADF. \nA. MATERIALS  \nThis section provides an overview of the dataset and the \npreprocessing and data augmentation steps used in the study. \n \n1) DATASET DESCRIPTION \nThe Alzheimer's Disease Neuroimaging Initiative \n(ADNI) dataset is a widely recognized and valuable data \nsource in the field of neuroimaging and AD and MCI \nresearch. It was launched in 2004 as a collaborative effort \namong academic and medical institutions, the \npharmaceutical industry, and governmental agencies. There \nis a wide range of clinical, genetic, imaging, and biomarker \ndata in the ADNI dataset that were collected from individuals \nwith AD and MCI, as well as healthy controls [41]. The study \nprovides a unique opportunity for researchers to identify \npotential biomarkers and develop new diagnostic tools and \ntherapeutic interventions for AD. T1 -weighted sMRI scans \nwere used in this study to perform classification. According \nto information obtained from the ADNI website \n(http://adni.loni.usc.edu/), the sMRI were obtained using a \nvolumetric 3D MPRAGE captured using 1.5T scanners. The \nacquisition of MRI data was done axially at ADNI, and \nadditional details about the methodology can be found on the \nwebsite. Study participants include 172 individuals with \npMCI and 204 individuals with sMCI.  Table 1  shows the \ndemographic details of the subjects used in the research \nwork.  \nTABLE 1. Demographic details of the subjects. \n \nDiagnostic \nType \nNumber of \nsubjects \nNumber \nof 2D \nslices \nAge Gender \nM/F \npMCI 172 15480 75.3¬±7.4 67/105 \nsMCI 204 18360 72.08¬±7.6 98/106 \n \nFIGURE 1. Graphical representation of skull stripping and \naugmentation. \n \n2) PREPROCESSING AND AUGMENTATION \n To obtain precise classification results, the preprocessing \nand data augmentation pipeline begins with raw brain MRI \nimages in the Neuroimaging Informatics Technology \nInitiative (NIfTI) format. The 3D MRI volumes are \ntransformed into 2D slices to simplify the data \nrepresentation. Subsequently, non -brain structures are \nremoved through skull stripping. The dataset is then \nsubdivided into training, validation, and testing sets in a \n60:20:20 ratio. Figure 1 depicts the skull stripping and data \naugmentation.  \n      In order to boost the model's generalizability and \nintroduce variability during training, online data \naugmentation techniques  are applied  to training and \nvalidation sets. This includes random geometric \ntransformations such as horizontal and vertical flipping, each \nwith a 50% probability and random rotations ranging from \n0¬∞ to 90¬∞. These augmentations enrich the dataset with \ndiverse perspectives, enhancing the capacity of the model to \ndistinguish critical features [ 42]. Together, these \npreprocessing and augmentation steps equip the proposed  \nnetwork to deliver precise classification results while \naccommodating variations in the input data. \n \n4 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nB. METHODS \nIn recent years, the development of advanced deep \nlearning techniques has greatly contributed to the accuracy \nand effectiveness of medical diagnosis [43]. In the context of \nage-related neurological diseases like MCI, accurate \nclassification plays a crucial role in early detection and \nintervention. A novel classification model that combines the \npotential of the Swin Transformer and the DCPAN \narchitecture along with the ADF is proposed. \n     The proposed model follows a  dual-track approach to \nleverage both spatial and global information for accurate \nclassification. In the first track, the Swin Transformer, a \nstate-of-the-art DL network is incorporated that had shown \nremarkable performance in image analysis tasks. The Swin \nTransformer captures long -range dependencies and \neffectively models the interactions between different regions \nof the brain. This track focuses on extracting high -level \nspatial features from t he input data. In the second track, the \nDCPAN architecture is used, which is designed to enhance \nthe discriminative power of deep CNNs. It leverages the \nconcept of dimensional convolution and proximal attention \nto emphasize informative regions of the brain  images. By \nincorporating proximal attention, the DCPAN track provides \na comprehensive understanding of both spatial and proximal \nfeatures. The outputs from these two tracks are then fused, \nwhich combines the complementary information learned by \nthe Swin Transformer and the DCPAN. The workflow of the \nproposed methodology is presented in Figure 2. \n \nFIGURE 2. Workflow diagram of the proposed methodology.  \n \n     This fusion enhances the overall representational \ncapability of the proposed model, enabling it to capture \nspatial features, proximal features, dimensional and long-\nrange dependency from the brain images. To improve the \naccuracy of the classification model, a Spatial Pyramid \nPooling (SPP) layer is incorporated. This block ensures that \nthe model can capture relevant features at various levels of \ngranularity, further improvi ng its ability to differentiate \nbetween different cognitive states. One unique feature of the \nproposed model is the incorporation of an ADF into the \ntransformer-based CNN model. The ADF represents the \ndifference between the biological age and the chronolog ical \nage of MCI patients. By considering age as an influential \nfactor, the model provides a more accurate and personalized \nMCI classification. \n \n1) SWIN TRANSFORMER TRACK \nSwin Transformer is a cutting -edge deep learning \narchitecture tailored for object detection and image \nclassification tasks [44, 45]. It adopts a hierarchical approach \nwhere the input image is divided into non -overlapping \nwindows, each treated as an individual token for processing. \nNotably, Swin Transformer employs a shift  window \nmechanism in each layer, allowing it to capture contextual \ninformation between neighboring windows. Its architecture \nconsists of multiple stages, each combining attention and \nconvolutional layers, enabling the gradual reduction of \nspatial resolution like CNNs while preserving fine -grained \nspatial details.  \n        Swin Transformer starts by dividing the input image \ninto non-overlapping patches. Let us denote the input image \nas X with dimensions height, width, and number of channels \n(H√óW√óC). These patches are represented as P, and the patch \nsize is denoted as S. The number of patches in the height and \nwidth dimensions are Hp and Wp, respectively: \n \nùêªùëù =  \nùêª\nùëÜ         (1) \n \nùëäùëù =  \nùëä\nùëÜ         (2) \n \n        Transformer blocks in the Swin architecture replace \nmulti-head self -attention (MSA) modules with shifted \nwindows modules. The Swin Transformer consists of an \nMSA module with shifted windows, followed by a 2 -layer \nMLP with GELU activation function. The calculation \nprocess of a Swin Transformer block is structured as follows: \nLet Zl represent the output of the previous layer. The next \nmodule calculates Zl+1 as follows: First, it applies Window -\nbased Multi -Head Self -Attention (WMSA) on Zl and \ncharacterizes it with the output of a Multilayer Perceptron \n(MLP) module. The result is denoted as ùëçÃÇùëô and it is \ncalculated by the following relation. \n \nùëçÃÇùëô  =  WMSA(LN(Zùëô‚àí1)) +  Zùëô‚àí1       (3) \n \n where LN denotes Layer Normalization.  \n \nùëçùëô  =  MLP (LN(ZÃÇ ùëô)) +  ZÃÇ ùëô     (4) \n \nThen, it moves to the shifted window using Shifted Window-\nbased Multi -Head Self -Attention (SWMSA) on Z l and \ncharacterizes it with another MLP module, yielding ZÃÇ ùëô+1 by \nthe relation, \n \nZÃÇ ùëô+1  =  SWMSA(LN(Zùëô)) +  Zùëô     (5) \n \nùëçùëô+1  =  MLP (LN(ZÃÇùëô+1)) +  ZÃÇ ùëô+1    (6) \n5 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nFIGURE 3. Schematic block diagram of Swin transformer. \nFIGURE 4. Architecture of Swin transformer block. \n \nFigure 3 and 4 depict the working of the swin  transformer. \nThe Swin-Transformer's self-attention mechanisms allow for \nthe establishment of global relationships between pixels in \nan image, allowing the model to better extract long -range \ndependencies. These long-range dependencies are crucial in \nunderstanding the complex structures and patterns present in \nmedical images. Moreover, by utilizing the Swin -\nTransformer as the backbone network for feature extraction, \nhierarchical features can be effectively captured from input \nimages. \n \n2) DCPAN TRACK \nThe DCPAN track consists of two major operations: 1. \nTernary partitioning and 2 . Tri-branch Attention \naggregation. The application of convolutional operations to \neach dimension of an image, namely height, width, and \ndepth, expands the capabilities of CNNs. The ternary \npartitioning approach uses convolutional filters that are \ndesigned to operate along different dimensions of the data \nsimultaneously. This helps to produce three feature matrices \nbased on the dimension. Dimension -wise convolutions are \nused to encode spatial and channel -wise information in the \ninput tensor efficiently [ 46, 47]. The input tensor M has \ndimensions w, h,  and d, where w represents width, d \nrepresents depth, and h represents height. Using depth-wise, \nwidth-wise, and height-wise convolutional kernels, the three \nbranches, namely channel features, horizontal spatial \nfeatures, and vertical spatial features, are generated. Depth -\nwise convolution applies a separate kernel along the depth \ndimension for each input channel. Here, FD is the filter for \ndepth-wise convolution and it is of the size 1√ón√ón, and M is \nof size d√óh√ów.  \nThese convolutional operations are applied independently \nalong their respective dimensions, resulting in three sets of \nfeature maps: FMD, FMH, and FMW. \n \nùêπùëÄùê∑[ùëë, ‚Ñé, ùë§] = ‚àë ‚àë ùêπùê∑[1, ùëñ, ùëó]ùëõ\nùëó=1\nùëõ\nùëñ=1 ‚àó ùëÄ      (8)\n  \nWhere, FMD is the output feature map for depth -wise \nconvolution. d, h, and w  are indices for depth, height, and \nwidth, respectively.  FD[1, i, j] represents the value of the \ndepth-wise convolutional kernel at position (i, j). \n      Width-wise convolution applies kernels along the width \ndimension, and here, FW is of size n√ón√ó1. The convolution \noperation is defined as: \n \nùêπùëÄùëä[ùëë, ‚Ñé, ùë§] = ‚àë ‚àë ùêπùëä[ùëñ, ùëó, 1]ùëõ\nùëó=1\nùëõ\nùëñ=1 ‚àó ùëÄ        (9) \n \nWhere, FMW is the output feature map for width -wise \nconvolution. FD [i, j, 1] represents the value of the width -\nwise convolutional kernel at position (i, j). \n       Height-wise convolution applies kernels along the \nheight dimension, and FH is of size n√ó1√ón. The convolution \noperation is defined as: \n \nùêπùëÄùêª[ùëë, ‚Ñé, ùë§] = ‚àë ‚àë ùêπùêª[ùëñ, 1, ùëó]ùëõ\nùëó=1\nùëõ\nùëñ=1 ‚àó ùëÄ    (10) \n \n Where, FMH is the output feature map for height -wise \nconvolution. FMH [i, 1, j] represents the value of the height-\nwise convolutional kernel at position (i, j). \n     Thus, a channel descriptor, a spatial -vertical descriptor \nand a spatial-horizontal descriptor is obtained. In the case of \nheight-wise, depth-wise, and channel-wise convolutions, the \nmodel can better understand the spatial relationships, depth \nvariations, and channel interactions within the data. Tri -\nbranch Attention aggregation operation involves applying \nneighborhood attention to the three branches FMD, FMH, and \nFMW  derived from the ternary partitioning to capture the \ncontextual information based on the proximity of the features \n[48]. This helps in combining the complementary \ninformation effectively, allowing the model to leverage the \nstrengths of each dimension. Figure 3 and 4 illustrates a \nworking of DCPAN.  \n      The linear projections of input FMD  are defined as query \nQ, key K, value V, and relative biases R(i,j). The attention \nweight Aki for the ith input is the sum of the dot product of \nbetween the query projection Qi and the transposed key \nprojections KTœÜj(i) by the corresponding relative positional \n6 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nbias R(i,  œÜj(i)). It can be represented by the following \nrelation,  \n \nùê¥ùëòùëñ = ‚àë (Qùëñ ‚ãÖ ùêæœÜj(i)\nùëá + R(i, œÜj(i)))\nùëò\nùëó=1\n    (11) \n \nThen the neighboring values Vki is defined as a matrix with \nrows being the ith input's k nearest neighboring value \nprojections and calculated by the following relation,   \n \n                      ùëâùëòùëñ =  |\nùëâœÜ1(ùëñ)\nùëá\nùëâœÜ2(ùëñ)\nùëá\nùëâœÜ3(ùëñ)\nùëá\n|                                             (12)    \nWhere VœÜj(i) represents the value projection for the jth nearest \nneighbour of the ith input. This method introduces local \nbiases and maintains translational equivariance by localizing \nattention to a proximal token. Then, the feature maps are \nconcatenated for further processing. The working of DCPAN \nis presented in Figure 5 and Figure 6. \nFIGURE 5. Architectural overview of DCPAN. \n \nFIGURE 6. Working of Neighborhood Attention block [41]. \n \n       Incorporating proximal information and dimension-wise \nfeatures helps in enhancing the spatial contextual \ninformation. This enhanced spatial context can lead to more \neffective feature representation. DCPAN focuses on \ncapturing local and proximal dependencies within each \nfeature matrix. This can help the model capture fine-grained \ndetails and patterns within the data.  \n \n3) SPATIAL PYRAMID POOLING \nSpatial Pyramid Pooling (SPP) is used in DL network  to \ngenerate fixed -length representations regardless of image \nsize or scale [49]. In addition to the last convolutional layer, \nan SPP layer can be used to aggregate information between \nconvolutional layers and fully -connected layers. In each \ndivision, local features are aggregated, and the image is \ndivided at different levels. A mult i-scale approach is \nachieved by dividing the input into spatial grids or regions of \ndifferent sizes and positions. Grid sizes are determined based \non the original image dimensio ns, enabling flexibility, with \ncommon choices including 1 √ó 1, 2 √ó 2, 4 √ó 4. Feature \nrepresentations are condensed using a pooling operation like \nmax-pooling within each grid. Lastly, the feature vectors \nfrom all grids are combined to capture spatial information at \ndifferent scales and positions within the input image. Deep \nCNNs benefit from this pooling strategy because it generates \nfixed-length outputs, uses multi -level spatial bins, and can \npool features at different scales. It significantly accelerates \nthe feature computation in object detection tasks, making it \nmore efficient than previous methods like R-CNN.  \n \n4) DETERMINATION OF AGE DEVIATION FACTOR \nADF is a deviation between chronological age and biological \nage, which can help identify MCI patients at a higher risk of \ndeveloping dementia. It serves as an indicator of the variance \nbetween an individual's chronological age and their \nbiological age. Res earchers have shown that the mismatch \nbetween chronological age and biological age can be used as \na biomarker for ageing -related diseases like MCI and \ndementia [50 - 53]. By incorporating this factor as a feature \nin conjunction with the image features extracted from a CNN \nmodel, accurate and robust classification of MCI patients can \nbe achieved. The concept of biological age refers to the \nassessment of an individual's age based on the physiological \ncharacteristics of the brain rather than their actual \nchronological age. A transformer-based prediction is applied \nto find the biological age. By leveraging the transformer \nmodel's ability to capture complex patterns and relationships \nwithin data, it becomes possible to estimate the biological \nage of an individual 's brain based on sMRI. The brain age \nestimation based on brain MRI used in this work is inspired \nfrom the Global-Local Attention transformer [54].  \n A global track and a local track are used to extract global \ncontext information and fine -grained details from the input \nimage. As shown in Figure 7, the attention mechanism fuses \nthe fine -grained information from local patches with the \nglobal context information. Both the global and local tracks \nconsist of eight blocks, each composed of three essential \ncomponents: a convolutional layer employing a 3 √ó3 kernel \n7 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nand padding layer, a batch normalization layer, and a \nRectified Linear Unit (ReLU) activation layer. In order to \nreduce spatial dimensions progressively, a max -pooling \nlayer with a 2 √ó2 kernel and a stride of 2 is introduced after \nevery two blocks. Global context information is calculated \nby weighting the global features according to their similarity. \nOn the channel dimension, global -local attention is applied \nin parallel parts, and the output values are concatenated and \nprojected into one feature of the same size as the input. A \nfeed-forward block allows global context and local -feature \ninformation to be fused, and this process can be repeated \niteratively to integrate global context and local -detail \ninformation. This fusion process can be repeated iteratively \nto integrate the global context and local detailed information.  \n \n \n \nFIGURE 7. A Visual representation of age prediction model [47]. \n      The Global -Local Transformer achieves improved \nperformance in brain age estimation compared to other state-\nof-the-art methods, reducing mean absolute error, and \nincreasing correlation coefficient. Once the biological age is \npredicted, the ADF can be calculated by subtracting the \nbiological age from the chronological age. This factor is then \nadded as a feature along with the image features extracted \nfrom a CNN  model. The final model can then be fed with \nboth biological and image features to classify MC I patients' \nrisk of developing dementia. When combined, the ADF and \nthe image features extracted using the CNN model provide a \ncomprehensive set of information for the classification of \nMCI patients.  \nIV. RESULTS AND ANALYSIS  \nThis section comprises several key sections that provide a \ncomprehensive analysis of the findings. In the following \nparagraphs, Section 4.1 discusses the environmental setup, \nthe experimental conditions and the resources utilized during \nthe research process. Section 4 .2 presents the \nhyperparameter tuning used to enhance the model's \nperformance are presented. In section 4.3, the results of the \nablation study are discussed. Further, a comparative analysis \nwith state -of-the-art networks aiming to highlight the \nstrengths a nd weaknesses of the proposed approach is \ndiscussed in Section 4.4. Finally, a performance comparison \nwith existing works to showcase the advancements and \ncontributions made by the proposed work is explained in \nSection 4.5.  \nA. ENVIRONMENTAL SETUP AND HYPERPARAMETER \nTUNING \nIn this research work , AWS EC2 instance with a 16GB \nNVIDIA T4 GPU, 4 AMD vCPUs, and 32GB of RAM is \nused to process images and train the model. To speed up the \ntraining while maintaining its efficiency, the model is trained \nfor 50 epochs, used mixed -precision training, and \nimplemented gradient scaling during backpropagation. To \nimprove the model's accuracy, hyperparameter tuning is \nperformed using the Grid Search algorithm on the Ray Tune \nframework. Hyperparameters are parameters set before \ntraining, which heavily affect the model's performance. The \nweight decay, learning rate, batch size, dropout, and epochs  \nare adjusted, and Table 2 shows the ranges of values that are \niterated through to achieve optimal tuning. Proper tuning of \nthese hyperparameters significantly impacts the model's \naccuracy, speed of convergence, and ability to generalize \nwell.  \nTABLE 2. Optimal hyperparameter values for model training. \n \nParameter Search Space Optimal value \nWeight decay [0, 1e-3, 1e-4, 1e-5] 1e-3 \nLearning rate [1e-3, 1e-4, 1e-5] 1e-3 \nBatch size 32 or 64 32 \nDropout [0 ‚Äì 0.33] 0.15 \nEpochs [30 ‚Äì 100] 50 \nOptimizer [SGD, BGD, ADAM] ADAM \nB. ABLATION STUDIES \nThe objective of this section is to assess the efficiency of the \nkey components in the proposed architecture in improving \nperformance. The efficacy of the following blocks was \nassessed: (1) Swin transformer, (2) DCPAN, (3) Swin \nTransformer + DCPAN, and (4) the proposed model. In order \nto thoroughly evaluate the efficiency of this architectur e, a \nseries of experiments were carried out to assess the \nperformance of each block as well as the final proposed \nmodel. \n \n1) ANALYSIS OF THE SWIN TRANSFORMER TRACK \nThe efficiency of the Swin transformer  is examined at \nextracting features for MCI classification. The performance \nof the Swin transformer was evaluated as a standalone \ncomponent in the evaluation. Its ability to capture long-range \ndependencies and efficiently process complex images \ncontributed to its effectiveness. For further enhancement of \nthe performance of the network, the last few layers are fine -\n8 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \ntuned to adapt to the pMCI vs sMCI classification task. \nFigure 8 illustrates the training process of the models, with a \ntesting accuracy of 70.21%. Furthermore, the average \nprecision, recall, and F1 score were 67.64%, 66.86%, and \n67.25%, respectively. Tabl e 3 provides metrics for each \nclass. \n \n \n \nFIGURE 8. Observations of training the Swin Transformer track in terms \nof accuracy. \n \nFIGURE 9. Observations of training the Swin Transformer track in terms \nof loss. \n \nTABLE 3. Performance Metrics for the Swin Transformer Architecture. \n \n \n \n \n \n \n \n2) ANALYSIS OF THE DCPAN TRACK \nDCPAN involves two major operations: ternary partitioning \nand tri-branch attention aggregation. Feature representation \nin this block is improved by enhancing spatial contextual \ninformation and capturing local dependencies within the \nfeature matrix. Implementation of DCPAN can be found in \nthe second branch of the proposed system. Here, the \nefficiency of the DCPAN is analyzed as an independent \nmodule. The performance evaluation of DCPAN was \nconducted, and the findings are illustrated in Figure 9. The \nobtained results on the test data showed an accuracy of \n74.46%, precision of 71.59%, F1 score of 72.41%, and recall \nof 73.25%. Table 4 provides an overview of the class -wise \nmetrics. The DCPAN enhanced the model's performance, \nhighlighting its crucial role in improving feature \nrepresentation and capturing proximal and spatial \ninformation.  \n \n \nFIGURE 10. Observations of training the DCPAN track in terms of \naccuracy. \n \nFIGURE 11. Observations of training the DCPAN track in terms of \naccuracy. \n \nTABLE 4. Performance Metrics for the DCPAN. \n \nClass Precision in \n% \nRecall \nin % \nF1-Score \nin % \npMCI 73.12 75.14 72.08 \nsMCI 70.06 71.36 72.74 \nOverall 71.59 73.25 72.41 \n \n3) ANALYSIS OF THE SWIN TRANSFORMER WITH \nDCPAN \nClass Precision in \n% \nRecall in \n% \nF1-Score \nin % \npMCI 68.04 67.03 69.03 \nsMCI 67.24 66.7 65.47 \nOverall 67.64 66.86 67.25 \n9 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nThe combination of the Swin transformer and DCPAN (track \none and track two) was evaluated to assess the synergistic \neffect of these two components. The results demonstrated \nthat their integration led to further performance \nimprovements. The Swin transforme r's ability to capture \nglobal dependencies combined with the DCPAN's feature \nrepresentation capabilities resulted in enhanced performance \nin extracting the subtle features of pMCI and sMCI images. \nThe Swin Transformer with DCPAN configuration \noutperformed both the standalone Swin transformer and \nDCPAN models, indicating the effectiveness of their \ncombination. By utilizing both global and local features, the \nproposed model captures fine details, inter -dimensional \nassociations, and long-range dependencies concurrently. The \nimplementation of the model included 50 epochs of training \nand validation and the observations illustrated in Figure 10. \nThe model attained an accuracy of 76.32% o n the testing \ndataset, with an average precision, F1 score, and recall of \n72.92%, 74.78%, and 76.74%, respectively, as demonstrated \nin Table 5. \n \nFIGURE 12. Observations of training the Swin Transformer with DCPAN \nin terms of accuracy. \nFIGURE 13. Observations of training the Swin Transformer with DCPAN \nin terms of loss. \n \n \n \n \n \n \n \n \n \n \n \n \nTABLE 5. Performance Metrics for the Swin Transformer with DCPAN \n \n \n \n \n \n \n \n \n \n4) EVALUATION OF THE PROPOSED MODEL \nBy leveraging the strengths of the Swin transformer, \nDCPAN, and age deviation factor, the proposed model \nachieved state -of-the-art results in the ADNI dataset. Its \nability to capture global dependencies, extract proximal local \nfeatures, and model long -range dependencies made it an \ninfluential architecture for classifying pMCI and sMCI. This \nablation study proves that the integration of the age deviation \nfactor, along with the features extracted from the combined \nswin transformer and DCPAN, synergistically improved the \nperformance of the model. The proposed network converged \nafter 50 epochs of training, as depicted in Figure 11. \nSubsequently, the test data achieved an accuracy of 79.8%. \nThe average precision, recall, and F1 score were recorded as \n76.66%, 80.23%, and 78.4% respectively. Detailed class -\nwise metrics can be found in Table 6. \n \n \n \n \n \n \n \n \n \nFIGURE 14. Observations of training the proposed model in terms of \naccuracy. \n \n \n \nClass Precision \nin % \nRecall \nin % \nF1-Score \nin % \npMCI 74.35 78.27 76.57 \nsMCI 71.50 75.21 72.99 \nOverall 72.92 76.74 74.78 \n10 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \n \nFIGURE 15. Observations of training the proposed model in terms of \nloss. \n \nTABLE 6. Performance Metrics for the Proposed model. \n \nClass Precision in \n% \nRecall in \n% \nF1-Score \nin % \npMCI 76.43 81.07 78.4 \nsMCI 76.89 79.39 78.4 \nOverall 76.66 80.23 78.4 \n \nTABLE 7. Analysis of the ablation studies. \n \n \n \n \n \n \n        \nIn conclusion, the evaluation of the key components in the \nproposed architecture highlights their effectiveness in \nimproving performance. The Swin transformer, DCPAN, \nSwin Transformer with DCPAN, and the proposed model all \ndemonstrated significant contributions to the overall \nperformance of the architecture. These findings validate the \nimportance of carefully selecting and integrating key \ncomponents to optimize the performance of MCI diagno sis \ntasks. The performance analysis of various blocks in the \nproposed model is summarized in Table 7. \n \n5) EFFECT OF CROSS VALIDATION \nTo evaluate the reliability of the proposed network on \nvarious samples, a four -fold cross-validation approach was \nemployed. The outcomes of assessing the proposed network \nusing this four -fold cross-validation technique is presented \nin Table 8. \n \n \nTable 8.  Evaluation metrics of the proposed model on the validation set     \nfor each fold. \nC. COMPARATIVE ANALYSIS WITH STATE-OF-THE-\nART NETWORKS \nPre-trained models such as AlexNet, VGGNet, ResNet, \nDenseNet, and EfficientNet are increasingly popular due to \ntheir high efficiency in various applications. This section \ncompares the performance of several popular pre -trained \nmodels with the proposed model designed specifically for \nthe MCI classification t ask. Pre -trained models have the \nadvantage of providing a solid starting point, but their size, \ncomputational costs, and overall accuracy may vary [ 55]. \nEfficientNet-B0 scored the highest accuracy of 73.1% \namong the pre -trained models, followed by DenseNet -169 \nwith 71.8%. However, performance of ResNet-18 was \nsignificantly lower, with a 56.6% accuracy rate. In Table 9, \nit is evident that the proposed model outperforms existing \nstate-of-the-art architectures, emphasizing its superiority in \nclassification. \n \nTABLE 9. Performance analysis of the proposed architecture with state-\nof-the-art methods. \n \nS. \nNo \nModel Trained Accuracy in \n(%) \n1 AlexNet 65.4 \n2 VGGNet-16 69.8 \n3 ResNet-18 56.6 \n4 ResNet-34 68.2 \n5 ResNet-50 69.6 \n6 ResNet-152 70.6 \n7 DenseNet-121 68.1 \n8 DenseNet-169 71.8 \n9 DenseNet-201 70.1 \n10 EfficientNet-B0 73.1 \n11 Proposed Model 79.8 \n \n      The objective of this comparative performance analysis \nis to provide insight into the trade-offs associated with using \npre-trained models versus the proposed model that is tailored \nto suit pMCI and sMCI classification tasks. \n \nD. PERFORMANCE COMPARISON WITH EXISITNG \nWORKS \nThis section analyses the performance of the proposed model \nand the existing works. By comparing existing work and the \nproposed work, we can evaluate the advantages and potential \nArchitecture Accuracy in \n(%) \nSwin Transformer 70.21 \nDCPAN 74.46 \nSwin Transformer with \nDCPAN \n76.32 \nProposed model 79.87 \nFolds Accuracy \nin (%) \nPrecision \nin (%) \nRecall \nin (%) \nF1-\nScore \nin (%) \nFold 1 79.9 77.3 79.6 78.4 \nFold 2 79.4 76.7 78.7 78.5 \nFold 3 79.9 76.9 79.4 77.9 \nFold 4 80 76.2 79.5 78.7 \nAverage 79.8 76.77 79.30 78.37 \nStandard \nDeviation 0.270 0.457 0.408 0.340 \n11 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nimprovements that are offered by the novel techniques. We \nconsidered the related work s that performed binary \nclassification on the same ADNI dataset using DL \ntechniques to ensure a fair comparison. Study selection \nincluded various DL approaches, including ensemble \nlearning, custom 2D and 3D CNNs, and transfer feature \nlearning. This analysis aims to provide insights into the \neffectiveness of different deep learning techniques and their \noptimal utilization for improving MCI detection. \nTABLE 10. Assessing the performance of the proposed method with \nexisting works. \n \n     From the Table 10, it was inferred that the proposed \nmodel outperformed the existing networks with an accuracy \nof 79.8%. These findings highlight the potential of tailored \nDL models and techniques for improving the accuracy of \nMCI classification, contributing to the field of \nneuroimaging-based diagnostics and patient care.                \nE. Limitations and Future work \nThis section discusses the limitations of the proposed work \nand the potential directions for future research and \ndevelopment. \n \n1. Integrating multimodal imaging data, such as fMRI, PET, \ncan provide a more extensive range of information compared \nto using a single modality MRI. This integration has the \npotential to improve the performance of the DL model.  \nTherefore, our forthcoming study will focus on incorporating \nmultimodal brain data into the analysis. \n \n2. We have exclusively utilized datasets obtained from \nADNI in this study. However, incorporating additional data \nsources would enable a more comprehensive feature  \nrepresentation. As a result, our future work aims to expand \nthe study to encompass a broader range of data. This \napproach will increase the diversity of the sample set, leading \nto more robust findings. \n \nV. CONCLUSION \nA novel DL network is proposed for classifying pMCI and \nsMCI based on brain MRI. The proposed network is \ndeveloped to efficiently extract the subtle differences in the \npMCI and sMCI images  using a dual track feature fusion \nnetwork. The first track of this network incorporates the \nSwin Transformer, designed to extract long -range \ndependencies present within the input MRI data. In parallel, \nthe second track leverages the DCPAN to capture spatial and \nproximal features. The fusion of feat ures from both tracks \nresults in the extraction of global, proximal, and dimensional \nfeatures. Furthermore, this study includes the integration of \nan age deviation factor into the fused image features, aimed \nat enhancing the accuracy of pMCI vs. sMCI classification.  \nThe holistic fusion of information enables the proposed \nnetwork to detect complex patterns of pMCI and sMCI. \nExperimental and evaluation results on the ADNI dataset \nindicate the proposed framework attained an accuracy rate of \n79.8%. These findings contribute significantly to the field of \nneuroimaging-based diagnostics, offering new opportunities \nfor identifying and managing MCI cases. The proposed work \nhas the potential to extend to other neurodegenerative \ndiseases, offering a versatile framework for broader \napplications in this field.  \n \nACKNOWLEDGMENT \n     Data collection and sharing for this project was funded by \nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) \n(National Institutes of Health Grant U01 AG024904) and \nDOD ADNI (Department of Defense award number \nW81XWH-12-2-0012). ADNI is funded by the N ational \nInstitute on Aging, the National Institute of Biomedical \nImaging and Bioengineering, and through generous \ncontributions from the following: AbbVie, Alzheimer‚Äôs \nAssociation; Alzheimer‚Äôs Drug Discovery Foundation; \nAraclon Biotech; BioClinica, Inc.; B iogen; Bristol -Myers \nSquibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan \nPharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; \nF. Hoffmann -La Roche Ltd and its affiliated company \nGenentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; \nJanssen Alzh eimer Immunotherapy Research & \nDevelopment, LLC.; Johnson & Johnson Pharmaceutical \nResearch & Development LLC.; Lumosity; Lundbeck; \nMerck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx \nResearch; Neurotrack Technologies; Novartis \nPharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; \nServier; Takeda Pharmaceutical Company; and Transition \nTherapeutics. The Canadian Institutes of Health Research is \nproviding funds to support ADNI clinical sites in Canada. \nPrivate sector contributions are facilitated by the Foundation \nfor the National Institutes of Health (www.fnih.org). The \ngrantee organization is the Northern California Institute for \nResearch and Education, and the study is coordinated by the \nAlzheimer‚Äôs Therapeutic Research Institute at the University \nof Southern California. ADNI data are disseminated by the \nLaboratory for Neuro Imaging at the University of Southern \nCalifornia. \nSource Method Accuracy \nIn (%) \nLuo et al. [35] Attention Based Transfer \nLearning 74.61 \nArco et al. [20] Custom CNN 78.4 \nOh et al. [21] Reinforcement Learning 77 \nHan et al. [22] 3D CNN Learning System 75.5 \nShi et al. [32] Multimodal Feature \nSelection 69.4 \nZhang et al. [18] Custom GCN 76.37 \nGao et al. [39] Attention Based Classifier 73.6 \nZhang et al. [29] 3D Mixer Neural Network 75 \nGao et al. [37] Attention Based Classifier 75.3 \nSun et al. [16] Custom CNN 75.5 \nZhang et al. [36] Custom CNN 76.88 \nProposed Work \nAge based classification \nusing DCPAN with Swin \ntransformer \n79.8 \n12 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \n \nREFERENCES \n[1] T. E. Kam, H. Zhang, Z. Jiao, and Di. Shen, ‚ÄúDeep Learning of Static \nand Dynamic Brain Functional Networks for Early MCI Detection,‚Äù \nIEEE Trans Med Imaging, vol. 39, no. 2, pp. 478 ‚Äì487, Feb. 2020, doi: \n10.1109/TMI.2019.2928790. \n[2] A. Ward, S. Tardiff, C. Dye, and H. M. Arrighi, ‚ÄúRate of Conversion \nfrom Prodromal Alzheimer‚Äôs Disease to Alzheimer‚Äôs Dementia: A \nSystematic Review of the Literature,‚Äù Dement Geriatr Cogn Dis Extra, \nvol. 3, no. 1, pp. 320‚Äì332, Sep. 2013, doi: 10.1159/000354370. \n[3] J. Yang, X. Feng, A. F. Laine, and E. D. Angelini, ‚ÄúCharacterizing \nAlzheimer‚Äôs Disease with Image and Genetic Biomarkers Using \nSupervised Topic Models,‚Äù IEEE J Biomed Health Inform, vol. 24, no. \n4, pp. 1180‚Äì1187, Apr. 2020, doi: 10.1109/JBHI.2019.2928831. \n[4] A. Muscari et al., ‚Äú‚Äò2-step MCI-AD‚Äô: a simple scoring system to predict \nrapid conversion from mild cognitive impairment to Alzheimer \ndementia,‚Äù Arch Gerontol Geriatr, vol. 94, May 2021, doi: \n10.1016/j.archger.2021.104359. \n[5] F. U. R. Faisal and G. R. Kwon, ‚ÄúAutomated Detection of Alzheimer-s \nDisease and Mild Cognitive Impairment Using Whole Brain MRI,‚Äù \nIEEE Access, vol. 10, pp. 65055 ‚Äì65066, 2022, doi: \n10.1109/ACCESS.2022.3180073. \n[6] N. Goenka and S. Tiwari, ‚ÄúDeep learning for Alzheimer prediction \nusing brain biomarkers,‚Äù Artif Intell Rev, vol. 54, no. 7, pp. 4827‚Äì4871, \nOct. 2021, doi: 10.1007/s10462-021-10016-0. \n[7] W. Zhao, X. Wang, C. Yin, M. He, S. Li, and Y. Han, ‚ÄúTrajectories of \nthe hippocampal subfields atrophy in the alzheimer‚Äôs disease: A \nstructural imaging study,‚Äù Front Neuroinform, vol. 13, Feb. 2019, doi: \n10.3389/fninf.2019.00013. \n[8] T. Ghosh, M. I. A. Palash, M. A. Yousuf, Md. A. Hamid, M. M. \nMonowar, and M. O. Alassafi, ‚ÄúA Robust Distributed Deep Learning \nApproach to Detect Alzheimer‚Äôs Disease from MRI Images,‚Äù \nMathematics, vol. 11, no. 12. MDPI AG, p. 2633, Jun. 09, 2023. doi: \n10.3390/math11122633. \n[9] T. Illakiya and R. Karthik, ‚ÄúAutomatic Detection of Alzheimer‚Äôs \nDisease using Deep Learning Models and Neuro -Imaging: Current \nTrends and Future Perspectives,‚Äù Neuroinformatics, vol. 21, no. 2. \nSpringer, pp. 339‚Äì364, Apr. 01, 2023. doi: 10.1007/s12021-023-09625-\n7. \n[10] A. Ebrahimi, S. Luo, and R. Chiong, ‚ÄúDeep sequence modelling for \nAlzheimer‚Äôs disease detection using MRI,‚Äù Computers in Biology and \nMedicine, vol. 134. Elsevier BV, p. 104537, Jul. 2021. doi: \n10.1016/j.compbiomed.2021.104537. \n[11] S. Akbar, M. Hayat, M. Tahir, S. Khan, and F. K. Alarfaj, ‚ÄúcACP -\nDeepGram: Classification of anticancer peptides via deep neural \nnetwork and skip -gram-based word embedding model,‚Äù Artificial \nIntelligence in Medicine, vol. 131. Elsevier BV, p. 102349, Sep. 2 022. \ndoi: 10.1016/j.artmed.2022.102349. \n[12] S. Akbar, S. Khan, F. Ali, M. Hayat, M. Qasim, and S. Gul, ‚ÄúiHBP -\nDeepPSSM: Identifying hormone binding proteins using PsePSSM \nbased evolutionary features and deep learning approach,‚Äù \nChemometrics and Intelligent Laboratory Systems, vol. 204. Elsevier \nBV, p. 104103, Sep. 2020. doi: 10.1016/j.chemolab.2020.104103. \n[13] A. Ahmad et al., ‚ÄúDeep-AntiFP: Prediction of antifungal peptides using \ndistanct multi -informative features incorporating with deep neural \nnetworks,‚Äù Chemometrics and Intelligent Laboratory Systems, vol. \n208. Elsevier BV, p. 104214, Jan. 2021. doi: \n10.1016/j.chemolab.2020.104214. \n[14] S. Akbar et al., ‚ÄúpAtbP-EnC: Identifying Anti-tubercular Peptides using \nMulti-Feature Representation and Genetic Algorithm based Deep \nEnsemble model,‚Äù IEEE Access. Institute of Electrical and Electronics \nEngineers (IEEE), pp. 1‚Äì1, 2023. doi: 10.1109/access.2023.3321100. \n[15] K. Oh, Y. C. Chung, K. W. Kim, W. S. Kim, and I. S. Oh, \n‚ÄúClassification and Visualization of Alzheimer‚Äôs Disease using \nVolumetric Convolutional Neural Network and Transfer Learning,‚Äù Sci \nRep, vol. 9, no. 1, Dec. 2019, doi: 10.1038/s41598-019-54548-6. \n[16] H. Sun, A. Wang, and S. He, ‚ÄúTemporal and Spatial Analysis of \nAlzheimer‚Äôs Disease Based on an Improved Convolutional Neural \nNetwork and a Resting -State FMRI Brain Functional Network,‚Äù Int J \nEnviron Res Public Health, vol. 19, no. 8, Apr. 2022, doi: \n10.3390/ijerph19084508. \n[17] C. Feng et al., ‚ÄúDeep Learning Framework for Alzheimer‚Äôs Disease \nDiagnosis via 3D -CNN and FSBi -LSTM,‚Äù IEEE Access, vol. 7, pp. \n63605‚Äì63618, 2019, doi: 10.1109/ACCESS.2019.2913847. \n[18] Y. Zhang, L. Qing, X. He, L. Zhang, Y. Liu, and Q. Teng, ‚ÄúPopulation-\nbased GCN method for diagnosis of Alzheimer‚Äôs disease using brain \nmetabolic or volumetric features,‚Äù Biomed Signal Process Control, vol. \n86, Sep. 2023, doi: 10.1016/j.bspc.2023.105162. \n[19] D. Pan, A. Zeng, L. Jia, Y. Huang, T. Frizzell, and X. Song, ‚ÄúEarly \nDetection of Alzheimer‚Äôs Disease Using Magnetic Resonance Imaging: \nA Novel Approach Combining Convolutional Neural Networks and \nEnsemble Learning,‚Äù Front Neurosci, vol. 14, May 2020, doi: \n10.3389/fnins.2020.00259. \n[20] J. E. Arco, A. Ortiz, D. Castillo -Barnes, J. M. G√≥rriz, and J. Ram√≠rez, \n‚ÄúEnsembling shallow siamese architectures to assess functional \nasymmetry in Alzheimer‚Äôs disease progression,‚Äù Appl Soft Comput, \nvol. 134, Feb. 2023, doi: 10.1016/j.asoc.2023.109991. \n[21] K. Oh, J. S. Yoon, and H. Il Suk, ‚ÄúLearn -Explain-Reinforce: \nCounterfactual Reasoning and its Guidance to Reinforce an \nAlzheimer‚Äôs Disease Diagnosis Model,‚Äù IEEE Trans Pattern Anal \nMach Intell, vol. 45, no. 4, pp. 4843 ‚Äì4857, Apr. 2023, doi: \n10.1109/TPAMI.2022.3197845. \n[22] R. Han, Z. Liu, and C. L. P. Chen, ‚ÄúMulti-scale 3D convolution feature-\nbased Broad Learning System for Alzheimer‚Äôs Disease diagnosis via \nMRI images,‚Äù Applied Soft Computing, vol. 120. Elsevier BV, p. \n108660, May 2022. doi: 10.1016/j.asoc.2022.108660. \n[23] M. Dang, C. Yang, K. Chen, P. Lu, H. Li, and Z. Zhang, \n‚ÄúHippocampus-centred grey matter covariance networks predict the \ndevelopment and reversion of mild cognitive impairment,‚Äù Alzheimers \nRes Ther, vol. 15, no. 1, Dec. 2023, doi: 10.1186/s13195-023-01167-z. \n[24] Y. Pusparani et al., ‚ÄúDiagnosis of Alzheimer ‚Äôs Disease using \nConvolutional Neural Network with Select Slices by Landmark on \nHippocampus in MRI Images,‚Äù IEEE Access, 2023, doi: \n10.1109/ACCESS.2023.3285115. \n[25] A. Li, F. Li, F. Elahifasaee, M. Liu, and L. Zhang, ‚ÄúHippocampal shape \nand asymmetry analysis by cascaded convolutional neural networks for \nAlzheimer‚Äôs disease diagnosis,‚Äù Brain Imaging Behav, vol. 15, no. 5, \npp. 2330‚Äì2339, Oct. 2021, doi: 10.1007/s11682-020-00427-y. \n[26] R. Cui and M. Liu, ‚ÄúHippocampus Analysis by Combination of 3 -D \nDenseNet and Shapes for Alzheimer‚Äôs Disease Diagnosis,‚Äù IEEE J \nBiomed Health Inform, vol. 23, no. 5, pp. 2099 ‚Äì2107, Sep. 2019, doi: \n10.1109/JBHI.2018.2882392. \n[27] F. Li and M. Liu, ‚ÄúA hybrid Convolutional and Recurrent Neural \nNetwork for Hippocampus Analysis in Alzheimer‚Äôs Disease,‚Äù J \nNeurosci Methods, vol. 323, pp. 108 ‚Äì118, Jul. 2019, doi: \n10.1016/j.jneumeth.2019.05.006. \n[28] B. Guelib, K. Zarour, H. Hermessi, B. Rayene, and K. Nawres, ‚ÄúSame-\nSubject-Modalities-Interactions: A Novel Framework for MRI and PET \nMulti-Modality Fusion for Alzheimer‚Äôs Disease Classification,‚Äù IEEE \nAccess, vol. 11, pp. 48715 ‚Äì48738, 2023, doi: \n10.1109/ACCESS.2023.3276722. \n[29] Z. C. Zhang, X. Zhao, G. Dong, and X. M. Zhao, ‚ÄúImproving \nAlzheimer‚Äôs Disease Diagnosis with Multi -Modal PET Embedding \nFeatures by a 3D Multi -task MLP -Mixer Neural Network,‚Äù IEEE J \nBiomed Health Inform, Aug. 2023, doi: 10.1109/JBHI.2023.3280823. \n[30] Y. Huang, J. Xu, Y. Zhou, T. Tong, and X. Zhuang, ‚ÄúDiagnosis of \nAlzheimer‚Äôs disease via multi -modality 3D convolutional neural \nnetwork,‚Äù Front Neurosci, vol. 13, no. MAY, 2019, doi: \n10.3389/fnins.2019.00509. \n[31] M. Abdelaziz, T. Wang, and A. Elazab, ‚ÄúAlzheimer‚Äôs disease diagnosis \nframework from incomplete multimodal data using convolutional \nneural networks,‚Äù J Biomed Inform, vol. 121, Sep. 2021, doi: \n10.1016/j.jbi.2021.103863. \n[32] Y. Shi et al., ‚ÄúASMFS: Adaptive -similarity-based multi -modality \nfeature selection for classification of Alzheimer‚Äôs disease,‚Äù Pattern \nRecognit, vol. 126, Jun. 2022, doi: 10.1016/j.patcog.2022.108566. \n[33] Y. Wu, Y. Zhou, W. Zeng, Q. Qian, and M. Song, ‚ÄúAn Attention-Based \n3D CNN with Multi -Scale Integration Block for Alzheimer‚Äôs Disease \nClassification,‚Äù IEEE J Biomed Health Inform, vol. 26, no. 11, pp. \n5665‚Äì5673, Nov. 2022, doi: 10.1109/JBHI.2022.3197331. \n[34] X. Zhang, L. Han, W. Zhu, L. Sun, and D. Zhang, ‚ÄúAn Explainable 3D \nResidual Self -Attention Deep Neural Network for Joint Atrophy \nLocalization and Alzheimer‚Äôs Disease Diagnosis Using Structural \n13 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n VOLUME XX, 2017 \nMRI,‚Äù IEEE J Biomed Health Inform, vol. 26, no. 11, pp. 5289 ‚Äì5297, \nNov. 2022, doi: 10.1109/JBHI.2021.3066832. \n[35] M. Luo, Z. He, H. Cui, Y. P. P. Chen, and P. Ward, ‚ÄúClass activation \nattention transfer neural networks for MCI conversion prediction,‚Äù \nComput Biol Med, vol. 156, Apr. 2023, doi: \n10.1016/j.compbiomed.2023.106700. \n[36] Z. Zhang, L. Gao, P. Li, G. Jin, and J. Wang, ‚ÄúDAUF: A disease-related \nattentional UNet framework for progressive and stable mild cognitive \nimpairment identification,‚Äù Comput Biol Med, vol. 165, p. 107401, Oct. \n2023, doi: 10.1016/j.compbiomed.2023.107401. \n[37] X. Gao, F. Shi, D. Shen, M. Liu, and T. Alzheimer‚Äôs Disease, ‚ÄúTask -\nInduced Pyramid and Attention GAN for Multimodal Brain Image \nImputation and Classification in Alzheimer‚Äôs Disease,‚Äù IEEE J Biomed \nHealth Inform, vol. 26, no. 1, 2022, doi: 10.1109/JBHI.2020.3097721. \n[38] F. Gao et al., ‚ÄúAD-NET: Age-adjust neural network for improved MCI \nto AD conversion prediction,‚Äù Neuroimage Clin, vol. 27, Jan. 2020, doi: \n10.1016/j.nicl.2020.102290. \n[39] X. Gao, H. Cai, and M. Liu, ‚ÄúA Hybrid Multi -scale Attention \nConvolution and Aging Transformer Network for Alzheimer‚Äôs Disease \nDiagnosis,‚Äù IEEE J Biomed Health Inform, Jul. 2023, doi: \n10.1109/JBHI.2023.3270937. \n[40] T. Illakiya, K. Ramamurthy, M. V. Siddharth, R. Mishra, and A. \nUdainiya, ‚ÄúAHANet: Adaptive Hybrid Attention Network for \nAlzheimer‚Äôs Disease Classification Using Brain Magnetic Resonance \nImaging,‚Äù Bioengineering, vol. 10, no. 6, p. 714, Jun. 2023, doi: \n10.3390/bioengineering10060714. \n[41] C. J. Weber et al., ‚ÄúThe Worldwide Alzheimer‚Äôs Disease Neuroimaging \nInitiative: ADNI-3 updates and global perspectives,‚Äù Alzheimer‚Äôs and \nDementia: Translational Research and Clinical Interventions, vol. 7, no. \n1, 2021, doi: 10.1002/trc2.12226. \n[42] C. Shorten and T. M. Khoshgoftaar, ‚ÄúA survey on Image Data \nAugmentation for Deep Learning,‚Äù J Big Data, vol. 6, no. 1, Dec. 2019, \ndoi: 10.1186/s40537-019-0197-0. \n[43] S. Al -Shoukry, T. H. Rassem, and N. M. Makbol, ‚ÄúAlzheimer‚Äôs \nDiseases Detection by Using Deep Learning Algorithms: A Mini -\nReview,‚Äù IEEE Access, vol. 8. Institute of Electrical and Electronics \nEngineers (IEEE), pp. 77131 ‚Äì77141, 2020. doi: \n10.1109/access.2020.2989396. \n[44] Z. Liu, et al., \"Swin Transformer: Hierarchical Vision Transformer \nusing Shifted Windows,\" in 2021 IEEE/CVF International Conference \non Computer Vision (ICCV), Montreal, QC, Canada, 2021 pp. 9992 -\n10002. doi: 10.1109/ICCV48922.2021.00986. \n[45] Y. Huang and W. Li, ‚ÄúResizer Swin Transformer -Based Classification \nUsing sMRI for Alzheimer‚Äôs Disease,‚Äù Applied Sciences (Switzerland), \nvol. 13, no. 16, Aug. 2023, doi: 10.3390/app13169310. \n[46] S. Mehta, H. Hajishirzi and M. Rastegari, \"DiCENet: Dimension-Wise \nConvolutions for Efficient Networks,\" in IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, vol. 44, no. 5, pp. 2416 -2425, 1 \nMay 2022, doi: 10.1109/TPAMI.2020.3041871.  \n[47] R. Mo, S. Lai, Y. Yan, Z. Chai, and X. Wei, ‚ÄúDimension -aware \nattention for efficient mobile networks,‚Äù Pattern Recognit, vol. 131, \nNov. 2022, doi: 10.1016/j.patcog.2022.108899. \n[48] Hassani, A., Walton, S., Li, J., Li, S., & Shi, H. (2023). Neighborhood \nattention transformer. In Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition (pp. 6185-6194). \n[49] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúSpatial Pyramid Pooling in Deep \nConvolutional Networks for Visual Recognition,‚Äù Jun. 2014, doi: \n10.1007/978-3-319-10578-9_23. \n[50] A. Taylor et al., ‚ÄúInvestigating the temporal pattern of neuroimaging -\nbased brain age estimation as a biomarker for Alzheimer‚Äôs Disease \nrelated neurodegeneration,‚Äù Neuroimage, vol. 263, Nov. 2022, doi: \n10.1016/j.neuroimage.2022.119621. \n[51] N. Adluru et al., ‚ÄúCharacterizing brain age in the Alzheimer‚Äôs disease \nconnectome project using a deep neural network pre‚Äêtrained on the UK \nBiobank,‚Äù Alzheimer‚Äôs & Dementia, vol. 17, no. S5, Dec. 2021, doi: \n10.1002/alz.057535. \n[52] P. Giannakopoulos et al., ‚ÄúAlzheimer resemblance atrophy index, \nBrainAGE, and normal pressure hydrocephalus score in the prediction \nof subtle cognitive decline: added value compared to existing MR \nimaging markers,‚Äù Eur Radiol, vol. 32, no. 11, pp. 7833 ‚Äì7842, Nov. \n2022, doi: 10.1007/s00330-022-08798-0. \n[53] J. H. Cole et al., ‚ÄúBrain age predicts mortality,‚Äù Mol Psychiatry, vol. \n23, no. 5, pp. 1385‚Äì1392, May 2018, doi: 10.1038/mp.2017.62. \n[54] S. He, P. E. Grant, and Y. Ou, ‚ÄúGlobal -Local Transformer for Brain \nAge Estimation,‚Äù IEEE Trans Med Imaging, vol. 41, no. 1, pp. 213 ‚Äì\n224, Jan. 2022, doi: 10.1109/TMI.2021.3108910. \n[55] I. Thayumanasamy and K. Ramamurthy, ‚ÄúPerformance Analysis of \nMachine Learning and Deep Learning Models for Classification of \nAlzheimer‚Äôs Disease from Brain MRI,‚Äù Traitement du Signal, vol. 39, \nno. 6, pp. 1961‚Äì1970, Dec. 2022, doi: 10.18280/ts.390608. \nMs. T. Illakiya  received M.Tech. degree in \nInformation Technology from Anna University, \nIndia. Currently, she is currently pursuing Ph.D \nin the School of Computer Science and \nEngineering in Vellore Institute of Technology, \nChennai, India.  Her research interest includes \nDeep Learning, Computer Vision, and Medical \nImage Analysis. \nDr. R. Karthik  obtained his Doctoral degree \nfrom Vellore Institute of Technology, India \nand Master‚Äôs degree from Anna University, \nIndia. Currently, He serves  as Associate \nProfessor in the Research Centre  for Cyber \nPhysical Systems, Vellore Institute of \nTechnology, Chennai. His research interest \nincludes Deep Learning, Computer Vision, \nDigital Image Processing, and Medical Image \nAnalysis. He has published around 75 papers in peer reviewed journals and \nconferences. He is  an active reviewer for journals publi shed by Elsevier, \nIEEE, Springer and Nature.  \n \n \n14 \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3332122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Discriminative model",
  "concepts": [
    {
      "name": "Discriminative model",
      "score": 0.7905151844024658
    },
    {
      "name": "Computer science",
      "score": 0.6673808097839355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5961534976959229
    },
    {
      "name": "Recall",
      "score": 0.5123497247695923
    },
    {
      "name": "Cognitive impairment",
      "score": 0.4940853416919708
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4767083525657654
    },
    {
      "name": "Machine learning",
      "score": 0.4234585165977478
    },
    {
      "name": "Dimension (graph theory)",
      "score": 0.4186256527900696
    },
    {
      "name": "Cognition",
      "score": 0.4150465726852417
    },
    {
      "name": "Psychology",
      "score": 0.10081362724304199
    },
    {
      "name": "Neuroscience",
      "score": 0.0793103277683258
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I876193797",
      "name": "Vellore Institute of Technology University",
      "country": "IN"
    }
  ]
}