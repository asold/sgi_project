{
  "title": "Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs",
  "url": "https://openalex.org/W4382239609",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2001693504",
      "name": "Fang Wu",
      "affiliations": [
        "Westlake University",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2913816252",
      "name": "Dragomir Radev",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2436786329",
      "name": "Stan Z. Li",
      "affiliations": [
        "Westlake University"
      ]
    },
    {
      "id": "https://openalex.org/A2001693504",
      "name": "Fang Wu",
      "affiliations": [
        "Westlake University",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2913816252",
      "name": "Dragomir Radev",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2436786329",
      "name": "Stan Z. Li",
      "affiliations": [
        "Westlake University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2153693853",
    "https://openalex.org/W89429679",
    "https://openalex.org/W2949095042",
    "https://openalex.org/W3110563343",
    "https://openalex.org/W2461620095",
    "https://openalex.org/W6736685754",
    "https://openalex.org/W6770914289",
    "https://openalex.org/W2986662312",
    "https://openalex.org/W3114474797",
    "https://openalex.org/W6765474767",
    "https://openalex.org/W3165665091",
    "https://openalex.org/W3005111505",
    "https://openalex.org/W2809216727",
    "https://openalex.org/W2290847742",
    "https://openalex.org/W2903800307",
    "https://openalex.org/W2548003600",
    "https://openalex.org/W2785947426",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W3102797483",
    "https://openalex.org/W2778051509",
    "https://openalex.org/W6803993145",
    "https://openalex.org/W6648496940",
    "https://openalex.org/W6746034047",
    "https://openalex.org/W6683933068",
    "https://openalex.org/W6734561206",
    "https://openalex.org/W2986232138",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4312039930",
    "https://openalex.org/W4286905681",
    "https://openalex.org/W3034379414",
    "https://openalex.org/W3126679164",
    "https://openalex.org/W3036737467",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W3196791236",
    "https://openalex.org/W3169622372",
    "https://openalex.org/W3213940558",
    "https://openalex.org/W1738019091",
    "https://openalex.org/W2788775653",
    "https://openalex.org/W4304697394",
    "https://openalex.org/W4225726305",
    "https://openalex.org/W3007488165",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2962876364",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2951148835",
    "https://openalex.org/W2997021962",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2957874522",
    "https://openalex.org/W2997945091",
    "https://openalex.org/W4210861939",
    "https://openalex.org/W4225091467",
    "https://openalex.org/W3095883070",
    "https://openalex.org/W4286715520",
    "https://openalex.org/W2948990653",
    "https://openalex.org/W4287586570",
    "https://openalex.org/W2963716836",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W4287828570",
    "https://openalex.org/W2968734407",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964113829",
    "https://openalex.org/W4294974645",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W2161917555",
    "https://openalex.org/W1993046136"
  ],
  "abstract": "Procuring expressive molecular representations underpins AI-driven molecule design and scientific discovery. The research mainly focuses on atom-level homogeneous molecular graphs, ignoring the rich information in subgraphs or motifs. However, it has been widely accepted that substructures play a dominant role in identifying and determining molecular properties. To address such issues, we formulate heterogeneous molecular graphs (HMGs) and introduce a novel architecture to exploit both molecular motifs and 3D geometry. Precisely, we extract functional groups as motifs for small molecules and employ reinforcement learning to adaptively select quaternary amino acids as motif candidates for proteins. Then HMGs are constructed with both atom-level and motif-level nodes. To better accommodate those HMGs, we introduce a variant of the Transformer named Molformer, which adopts a heterogeneous self-attention layer to distinguish the interactions between multi-level nodes. Besides, it is also coupled with a multi-scale mechanism to capture fine-grained local patterns with increasing contextual scales. An attentive farthest point sampling algorithm is also proposed to obtain the molecular representations. We validate Molformer across a broad range of domains, including quantum chemistry, physiology, and biophysics. Extensive experiments show that Molformer outperforms or achieves the comparable performance of several state-of-the-art baselines. Our work provides a promising way to utilize informative motifs from the perspective of multi-level graph construction. The code is available at https://github.com/smiles724/Molformer.",
  "full_text": "Molformer: Motif-Based Transformer on 3D Heterogeneous Molecular Graphs\nFang Wu1,3, Dragomir Radev2, Stan Z. Li1*\n1 School of Engineering, Westlake University\n2 Department of Computer Science, Yale University\n3 Institute of AI Industry Research, Tsinghua University\nfw2359@columbia.edu, dragomir.radev@yale.edu, stan.zq.li@westlake.edu.cn\nAbstract\nProcuring expressive molecular representations underpins\nAI-driven molecule design and scientiÔ¨Åc discovery. The re-\nsearch mainly focuses on atom-level homogeneous molecu-\nlar graphs, ignoring the rich information in subgraphs or mo-\ntifs. However, it has been widely accepted that substructures\nplay a dominant role in identifying and determining molec-\nular properties. To address such issues, we formulate hetero-\ngeneous molecular graphs (HMGs), and introduce a novel ar-\nchitecture to exploit both molecular motifs and 3D geometry.\nPrecisely, we extract functional groups as motifs for small\nmolecules and employ reinforcement learning to adaptively\nselect quaternary amino acids as motif candidates for pro-\nteins. Then HMGs are constructed with both atom-level and\nmotif-level nodes. To better accommodate those HMGs, we\nintroduce a variant of the Transformer named Molformer,\nwhich adopts a heterogeneous self-attention layer to distin-\nguish the interactions between multi-level nodes. Besides,\nit is also coupled with a multi-scale mechanism to capture\nÔ¨Åne-grained local patterns with increasing contextual scales.\nAn attentive farthest point sampling algorithm is also pro-\nposed to obtain the molecular representations. We validate\nMolformer across a broad range of domains, including quan-\ntum chemistry, physiology, and biophysics. Extensive experi-\nments show that Molformer outperforms or achieves the com-\nparable performance of several state-of-the-art baselines. Our\nwork provides a promising way to utilize informative motifs\nfrom the perspective of multi-level graph construction. The\ncode is available at https://github.com/smiles724/Molformer.\nIntroduction\nThe past decade has witnessed the extraordinary success\nof deep learning (DL) in many scientiÔ¨Åc areas. Inspired\nby these achievements, researchers have shown increas-\ning interest in exploiting DL for drug discovery and ma-\nterial design with the hope of rapidly identifying desirable\nmolecules. A key aspect of fast screening is how to represent\nmolecules effectively, where graphs are a natural choice to\npreserve their internal structures. As a consequence, a num-\nber of graph neural networks (GNNs) (Gilmer et al. 2017;\nIshida et al. 2021) have been invented and applied to molec-\nular representation learning with noticeable performance.\n*The corresponding author.\nCopyright ¬© 2023, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nHowever, most existing GNNs only take atom-level infor-\nmation in homogeneous molecular graphs as input, which\nfails to adequately exploit rich semantic information in\nmotifs. Remarkably, motifs are signiÔ¨Åcant subgraph pat-\nterns that frequently occur, and can be leveraged to un-\ncover molecular properties. For instance, a carboxyl group\n(COOH) acts as a hydrogen-bond acceptor, which con-\ntributes to better stability and higher boiling points. Besides,\nsimilar to the role of N-gram in natural language, molecu-\nlar motifs can promote the segmentation of atomic semantic\nmeanings. While some regard motifs as additional features\nof atoms (Maziarka et al. 2020, 2021), these methods in-\ncrease the difÔ¨Åculty to separate the semantic meanings of\nmotifs from atoms explicitly and hinder models from view-\ning motifs from an integral perspective. Others (Huang et al.\n2020) take motifs as the only input, but ignore the inÔ¨Çuence\nof single atoms and infrequent substructures.\nTo overcome these problems, we formulate a novel het-\nerogeneous molecular graph (HMG) that is comprised of\nboth atom-level and motif-level nodes as the model input.\nIt provides a clean interface to incorporate nodes of differ-\nent levels and prevents the error propagation caused by in-\ncorrect semantic segmentation of atoms. As for the determi-\nnation of motifs, we adopt different strategies for different\ntypes of molecules. On the one hand, for small molecules,\nthe motif lexicon is deÔ¨Åned by functional groups based on\nchemical domain knowledge. On the other hand, for proteins\nthat are constituted of sequential amino acids, a reinforce-\nment learning (RL) motif mining technique is introduced to\ndiscover the most meaningful amino acid subsequences for\ndownstream tasks.\nIn order to better align with HMGs, we present Mol-\nformer, an equivariant geometric model based on Trans-\nformer (Vaswani et al. 2017). Molformer differs from pre-\nceding Transformer-based models in two major aspects.\nFirst, it utilizes heterogeneous self-attention (HSA) to dis-\ntinguish the interactions between nodes of different levels\nand incorporates them into the self-attention computation.\nSecond, an Attentive Farthest Point Sampling (AFPS) algo-\nrithm is introduced to aggregate node features and obtain a\ncomprehensive representation of the entire molecule.\nTo summarize, our contributions are as follows:\n‚Ä¢ To the best of our knowledge, we are the foremost to in-\ncorporate motifs and construct 3D heterogeneous molec-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n5312\nular graphs for molecular representation learning.\n‚Ä¢ We propose a novel Transformer architecture to perform\non these heterogeneous molecular graphs. It has a mod-\niÔ¨Åed self-attention to take into account the interactions\nbetween multi-level nodes, and an AFPS algorithm to in-\ntegrate molecular representations.\n‚Ä¢ We empirically outperform or achieve competitive re-\nsults compared to state-of-the-art baselines on several\nbenchmarks of small molecules and proteins.\nPreliminaries\nProblem DeÔ¨Ånition. Suppose a molecule S = (P ;H)\nhas N atoms, where P = fpigN\ni=1 2RN\u00023 describes 3D\ncoordinates associated to each atom and H = fhigN\ni=1 2\nRN\u0002h contains a set of h-dimension roto-translationally in-\nvariant features (e.g. atom types and weights). hi can be\nconverted to a dense vector xi 2R embed via a multi-layer\nperceptron (MLP). A representation learning model f acts\non S, and obtains its representation r = f(S). Then r is\nforwarded to a predictor g and attains the prediction of a\nbiochemical property ^y= g(r).\nSelf-Attention Mechanism. Given input fxigN\ni=1, the\nstandard dot-product self-attention layer is computed as fol-\nlows:\nqi = fQ(xi); ki = fK(xi); vi = fV(xi) (1)\naij = qikT\nj =\np\n model; zi =\nNX\nj=1\n\u001b(aij)vj (2)\nwhere ffQ;fK;fVgare embedding transformations, and\nfqi;ki;vigare respectively the query, key, and value vec-\ntors with the same dimension  model. aij is the attention that\nthe token ipays to the token j. \u001bdenotes the Softmax func-\ntion and zi is the output embedding of the token i. This for-\nmula conforms to a non-local network (Wang et al. 2018),\nwhich indicates its poor ability to capture Ô¨Åne-grained pat-\nterns in a local context.\nPosition Encoding. Self-attention is invariant to the per-\nmutation of the input, and position encoding (PE) is the only\ntechnique to reveal position information. PE can be based on\nabsolute positions or relative distances. The former uses raw\npositions and is not robust to spatial transformations. The\nlatter manipulates the attention score by incorporating rela-\ntive distances: aij = qikT\nj =p\n model + fPE(pi\u0000pj), where\nfPE is a translation-invariant PE function. The rotation in-\nvariance can be further accomplished by taking a L2-norm\ndij = jjpi \u0000pjjj2 between the i-th and j-th atom.\nHeterogeneous Molecular Graphs\nMotifs are frequently-occurring substructure patterns and\nserve as the building blocks of complex molecular struc-\ntures. They have great expressiveness of the biochemical\ncharacteristics of the whole molecules. In this section, we\nÔ¨Årst describe how to extract motifs from small molecules\nand proteins respectively, and then present how to formulate\nHMGs.\nReward\nPolicy Network\n(e.g., ProtBERT)\nMotif Lexicon Molformer\nDownstream Tasks\n(e.g., PDBbind)\nAction\nInput\nModel? ? ? ?\nùê∂160,000\nùêæ\nAction Space\nFigure 1: The workÔ¨Çow of RL motif mining method in pro-\nteins. In each iteration, the policy network is responsible\nfor producing a motif lexicon. Molformer‚Äôs performance on\ndownstream tasks and the diversity of the lexicon are con-\nsidered as the reward.\nMotifs in Small Molecules\nIn the chemical community, a set of standard criteria have\nbeen developed to recognize motifs with essential func-\ntionalities in small molecules. There, we build motif tem-\nplates of four categories of functional groups, containing\ngroups that contain only carbon and hydrogen (Hydrocar-\nbons), groups that contain halogen (Haloalkanes), groups\nthat contain oxygen, and groups that contain nitrogen (see\nAppendix for more discussion). Practically, we rely on RD-\nKit to draw them from SMILES of small molecules.\nMotifs in Proteins\nIn large protein molecules, motifs are local regions of 3D\nstructures or amino acid sequences shared among proteins\nthat inÔ¨Çuence their functions (Somnath, Bunne, and Krause\n2021). Each motif usually consists of only a few elements,\nsuch as the ‚Äôhelix-turn-helix‚Äô motif, and can describe the\nconnectivity between secondary structural elements. The de-\ntection of protein motifs has been long studied. Neverthe-\nless, existing tools are either from the context of a pro-\ntein surface (Somnath, Bunne, and Krause 2021) or are\ntask-independent and computationally expensive (Macken-\nzie, Zhou, and Grigoryan 2016). On the basis of this pecu-\nliarity, we design an RL mining method to discover task-\nspeciÔ¨Åc protein motifs heuristically.\nTherefore, we consider motifs with four amino acids be-\ncause they make up the smallest polypeptide and have spe-\ncial functionalities in proteins. For instance, \f-turns are\ncomposed of four amino acids and are a non-regular sec-\nondary structure that causes a change in the direction of the\npolypeptide chain. Each amino acid can be of 20 different\npossibilities, such as Alanine, Isoleucine, and Methionine,\nso there are 1:6 \u0002105 (= 204) potential quaternary motifs.\nOur goal is to Ô¨Ånd the most effective lexicon V\u0003 2 V\ncomposed of K quaternary amino acid templates, where V\ndenotes the space of all CK\n1:6\u0002105 potential lexicons. Since\nwe aim to mine the optimal task-speciÔ¨Åc lexicon, it is prac-\ntically feasible to only consider the existing quaternions in\n5313\nthe downstream datasets instead of all 1:6 \u0002105 possible\nquaternions.\nIn each iteration of the parameter update, we use a pre-\ntrained ProtBERT (Elnaggar et al. 2020) with an MLP as\nthe policy network \u0019\u0012. SpeciÔ¨Åcally, all possible quaternions\nare fed into ProtBert to obtain their corresponding repre-\nsentations feig1:6\u0002105\ni=1 , which are subsequently sent to the\nMLP to acquire their scores fsig1:6\u0002105\ni=1 . These scores il-\nlustrate each quaternion‚Äôs signiÔ¨Åcance to beneÔ¨Åt the down-\nstream tasks if they are chosen as a part of the vocabulary.\nThen top-K motifs with the highest scores are selected to\ncomprise V 2V in accordance with fsig1:6\u0002105\ni=1 , and V is\nused as templates to extract motifs and construct HMGs in\ndownstream tasks. After that, a Molformer is trained based\non these HMGs. Its validation performance is regarded as\nthe reward rto update parameters \u0012by means of policy gra-\ndients. With adequate iterations, the agent can select the op-\ntimal task-speciÔ¨Åc quaternary motif lexicon V\u0003.\nRemarkably, our motif mining process is a one-step game,\nsince the policy network\u0019\u0012 only generates the vocabularyV\nonce in each iteration. Thus, the trajectory consists of only\none action, and the performance of Molformer based on the\nchosen lexicon V composes a part of the total reward. More-\nover, we also consider the diversity of motif templates within\nthe lexicon and calculate it as:\nddiv(V) = 1\njVj\nX\nmi2V\nX\nmj2V\ndlev(mi;mj) (3)\nwhere dlev is the Levenshtein distance of two quaternary se-\nquences mi and mj. The Ô¨Ånal reward, therefore, becomes\nR(V) = r+ \rddiv(V), where \r is a weight to balance two\nreward terms, and the policy gradient is computed as the fol-\nlowing objective:\nr\u0012J(\u0012) = EV2V[r\u0012log \u0019\u0012(V)R(V)] (4)\nFormulation of Heterogeneous Molecular Graphs\nMost prior studies (Maziarka et al. 2020; Rong et al. 2020;\nMaziarka et al. 2021) simply incorporate motifs into atom\nfeatures. For instance, they differentiate carbons into aro-\nmatic or non-aromatic and deem them as extra features. We\nargue its ineffectiveness for two reasons. First, the fusion\nof multi-level features increases the difÔ¨Åculty to summarize\nthe functionality of motifs. Second, it hinders models to see\nmotifs from a unitary perspective. To Ô¨Åll these gaps, we sep-\narate apart motifs and atoms, regarding motifs as new nodes\nto formulate an HMG. This way disentangles motif-level and\natom-level representations, thus alleviating the difÔ¨Åculty for\nmodels to properly mine the motif-level semantic meanings.\nSimilar to the relation between phrases and single words\nin natural language, motifs in molecules carry higher-level\nsemantic meanings than atoms. Therefore, they play an es-\nsential part in identifying the functionalities of their atomic\nconstituents. Inspired by the employment of dynamic lattice\nstructures in named entity recognition, we treat each cate-\ngory of motifs as a new type of node and build HMGs as\nthe input of our Molformer. To begin with, motifs are ex-\ntracted according to a motif vocabulary V. We assume M\nmotifs fmigM\ni=1 are detected in the molecule S. Conse-\nquently, an HMG includes both the motif-level and atom-\nlevel nodes as fx1;:::; xN;xm1 ;:::; xmM g, where xmi 2\nR embed is obtained through a learnable embedding matrix\nWM 2 RC0\u0002 embed and C0 denotes the number of mo-\ntif categories. As for positions of each motif, we adopt\na weighted sum of 3D coordinates of its components as\npmi = P\nxi2mi\n\u0010\nwi\nP\nxi2mi wi\n\u0011\n\u0001pi, where wi are the atomic\nweights. Analogous to word segmentation, our HMGs com-\nposed of multi-level nodes avoid error propagation due to\ninappropriate semantic segmentation while leveraging the\natom information for molecular representation learning.\nMolformer\nIn this section, we propose Molformer, which modiÔ¨Åes\nTransformer with several novel components speciÔ¨Åcally de-\nsigned for 3D HMGs. First, we build a motif vocabulary\nand match each molecule with this lexicon to obtain all its\ncontained motifs. Then both atoms and motifs acquire their\ncorresponding embeddings and are forwarded into L fea-\nture learning blocks. Each block consists of an HSA, a feed-\nforward network (FFN), and two-layer normalizations. After\nthat, an AFPS is followed to adaptively produce the molec-\nular representation, which is later fed into a dense predictor\nto forecast properties in a broad range of downstream tasks.\nHeterogeneous Self-attention\nAfter formulating an HMG with N atom-level nodes and M\nmotif-level nodes, it is important to endow the model with\nthe capacity of separating the interactions between multi-\norder nodes. To this end, we utilize a function \u001e(i;j) :\nR(N+M)\u0002(N+M) ! Z, which identiÔ¨Åes the relationship\nbetween any two nodes into three sorts: atom-atom, atom-\nmotif, and motif-motif. Then a learnable scalarb\u001e(i;j) : Z !\nR indexed by \u001e(i;j) is introduced so that each node can\nadaptively attend to all other nodes according to their hier-\narchical relationship inside our HMGs.\nIn addition, we consider exploiting 3D molecular geome-\ntry (see Figure 2). Since robustness to global changes such\nas 3D translations and rotations is an underlying principle\nfor molecular representation learning, we seek to satisfy\nroto-translation invariance. There, we borrow ideas from\nSE(3)-Transformer (Sch¬®utt, Unke, and Gastegger 2021) and\nAlphaFold2 (Jumper et al. 2021), and apply a convolu-\ntional operation to the pairwise distance matrix D =\n[di;j]i;j2[N+M] 2R(N+M)\u0002(N+M) as ^D = Conv2d(D),\nwhere Conv2d denotes a 2D shallow convolutional network\nwith a kernel size of1\u00021. Consequently, the attention score\nis computed as follows:\n^aij =\n\u0010\nqikT\nj =\np\n model\n\u0011\n\u0001^dij + b\u001e(i;j); (5)\nwhere ^di;j 2 ^D controls the impact of interatomic distance\nover the attention score, and b\u001e(i;j) is shared across all lay-\ners.\nMoreover, exploiting local context has proven to be im-\nportant in sparse 3D space. However, it has been pointed out\n5314\nHeterogeneous Attention \nLayer Norm\nFFN\nL √ó\nGlobal Pooling\nProperties\nLayer Norm\nVirtual Atom \nH\nAttentive FPS\nMLP\nN\nO\nC\nMulti-scale Mechanism\nLocal Global\nUnselected Atom \nMotif   Extraction\nC C H‚Ä¶\nEmbedding Layer \nMotifsAtoms\nFigure 2: The architecture of Molformer. Given a heterogeneous molecular graph with both atom-level and motif-level nodes,\nstacked feature learning blocks composed of a heterogeneous self-attention module and an FFN compute their updated features.\nAfterward, an attentive subsampling module integrates the molecular representation for downstream predictions. Local features\nwith different scales are purple and orange; yellow corresponds to global features.\nthat self-attention is good at capturing global data patterns\nbut ignores local context (Wu et al. 2020). Based on this\nfact, we impose a distance-based constraint on self-attention\nin order to extract multi-scaled patterns from both local and\nglobal contexts. Guo et al. (2020) propose to use integer-\nbased distances to limit attention to local word neighbors,\nwhich cannot be used in molecules. This is because different\ntypes of molecules have different densities and molecules of\nthe same type have different spatial regularity, which results\nin the non-uniformity of interatomic distances. Normally,\nsmall molecules have a mean interatomic distance of 1-2 ÀöA\n(Angstrom, 10\u000010m), which is denser than large molecules\nlike proteins with approximately 5 ÀöA on average. To tackle\nthat, we design a multi-scale methodology to robustly cap-\nture details. SpeciÔ¨Åcally, we mask nodes beyond a certain\ndistance \u001cs at each scale s. The attention calculation is mod-\niÔ¨Åed as:\na\u001cs\nij = ^aij \u00011fdij<\u001csg;z\u001cs\ni =\nNX\nj=1\n\u001b\n\u0000\na\u001cs\nij\n\u0001\nvj; (6)\nwhere 1fdij<\u001csg is the indicator function. Notably, Equa-\ntion 6 can be complementarily combined with Equation 5.\nThen, features extracted from S different scales f\u001csgS\ns=1,\nas well as the informative global feature, are concatenated\ntogether to form a multi-scale representation, denoted by\nz0\ni = z\u001c1\ni \b:::\bz\u001cS\ni \bzglobal\ni 2R(S+1) model . After that,\nz0\ni is forwarded into a FFN to obtain z00\ni with the original\ndimension  model.\nAttentive Farthest Point Sampling\nAfter having the node embeddingsfz00\nigN+M\ni=1 , we study how\nto obtain the molecular representation r. For GNNs, several\nreadout functions such as set2set (Vinyals, Bengio, and Kud-\nlur 2015) and GG-NN (Gilmer et al. 2017) are invented. For\nTransformers, one way is via a virtual node. Though Ying\net al. (2021) state that it signiÔ¨Åcantly improves the perfor-\nmance of existing models in the leaderboard of Open Graph\nBenchmark (OGB), this way concentrates more on its close\nadjacent nodes and less on distant ones, and may lead to\ninadvertent over-smoothing of information propagation. Be-\nsides, it is difÔ¨Åcult to locate a virtual node in 3D space and\nbuild connections to existing vertices. The other way selects\na subset of nodes via a downsampling algorithm named Far-\nthest Point Search (FPS), but it ignores nodes‚Äô differences\nand has the sensitivity to outlier points as well as uncon-\ntrollable randomness. To address these issues, we propose a\nnew algorithm named AFPS. It aims to sample vertices by\nnot merely spatial distances, but also their signiÔ¨Åcance in\nterms of attention scores.\nSpeciÔ¨Åcally, we choose the virtual atom x# as the\nstarting point and initialize two lists P = fx#g and\nM= fxigN+M\ni=1 to store remaining candidate points. Then\nthe process begins with the attention score matrix ^A =\n[^ai;j]i;j2[N+M] 2R(N+M)\u0002(N+M) and the interatomic dis-\ntance matrix D 2R(N+M)\u0002(N+M). It can be easily proved\nthat each row of ^A sums up to 1 after the Softmax op-\neration along columns, i.e. P\nj ^aij = 1; 8i 2 [N + M].\n5315\nAlgorithm 1: Attentive Farthest Point Sampling\nInput: The attention score matrix A 2R(N+M)\u0002(N+M),\na Euclidean distance matrix D 2R(N+M)\u0002(N+M).\nOutput: Ksampled points.\n~A  P\ni^aij 2RN+M .sum up along rows\n~D  D\u0000min D\nmax D\u0000min D 2R(N+M)\u0002(N+M) .normalize the\ndistance matrix\nP= fx#g\nM= fxigN+M\ni=1\nwhile length(P) <k do\nxnew  argmax\ni2M\n(min\nj2P\n~Dij + \u0015~Ai) .pick up the node\nthat maximize the objective\nP.append(xnew)\nM.remove(xnew)\nend whilereturn P\nIn order to obtain the importance of each atom in the self-\nattention computation, we accumulate ^A along rows and get\n~A = P\ni^aij 2 RN+M. Besides, we adopt the min-max\nnormalization to rescale the distance matrix D into values\nbetween 0 and 1, and obtain ~D = D\u0000min D\nmax D\u0000min D .\nAfter the above preprocess, we repeatedly move a point\nxnew from Mto P, which ensures that xnew is as far from\nPas possible by maximizing ~Dij and also plays a crucial\nrole in attention computation by maximizing ~Ai. Mathemat-\nically, the AFPS aims to achieve the objective:\nmax\nX\ni2M\n( min\nj2Pnfig\n~Dij + \u0015~Ai); (7)\nwhere \u0015 is a hyperparameter to balance those two differ-\nent goals. This process is repeated until Phas reached K\npoints. Algorithm 1 provides a greedy approximation solu-\ntion to this AFPS optimization objective for sake of compu-\ntational efÔ¨Åciency.\nAfter that, sampled features fz00\nigi2P are gathered by a\nGlobal Average Pooling layer to attain the molecular repre-\nsentation r 2R model .\nRemarkably, our proposed AFPS has considerable differ-\nences and superiority over a body of previous hierarchical\napproaches (Eismann et al. 2021). Their subsampling oper-\nations are mainly designed for protein complexities, which\noften have uniform structures. To be speciÔ¨Åc, they hierarchi-\ncally use alpha carbons as the intermediate set of points and\naggregate information at the level of those carbons for the\nentire complex. However, the structures of small molecules\nhave no such stable paradigm, and we provide a universal\nmethod to adaptively subsample atoms without any prior as-\nsumptions on the atom arrangement.\nExperiments\nWe conduct extensive experiments on 7 datasets of both\nsmall and large protein molecules from three different do-\nmains, including quantum chemistry, physiology, and bio-\nphysics. The appendix summarises statistical information of\nthese 7 benchmark datasets, such as the number of tasks and\ntask types, the number of molecules and atom classes, the\nminimum and the maximum number of atoms, and the den-\nsity (mean interatomic distances) of all molecules.\nDatasets. We test Molformer on QM7 (Blum and Rey-\nmond 2009), QM8 (Ramakrishnan et al. 2015), QM9 (Ra-\nmakrishnan et al. 2014), BBBP (Martins et al. 2012),\nClinTox (Gayvert, Madhukar, and Elemento 2016), and\nBACE (Subramanian et al. 2016) 1. QM7 is a subset of\nGDB-13 with 7K molecules. QM8 and QM9 are subsets of\nGDB-17 with 22K and 133K molecules respectively. BBBP\ninvolves records of whether a compound carries the perme-\nability property of penetrating the blood-brain barrier. Clin-\nTox compares drugs approved through FDA and drugs elimi-\nnated due to toxicity during clinical trials. BACE is collected\nfor recording compounds that could act as the inhibitors of\nhuman \f-secretase 1 (BACE-1).\nWe also inspect Molformer‚Äôs ability to learn mutual re-\nlations between proteins and molecules on the PDBbind\ndataset (Wang et al. 2005). Following Townshend et al.\n(2020), we split protein-ligand complexes by protein se-\nquence identity at 30%. As for the target, we predict pS =\n\u0000log(S), where S is the binding afÔ¨Ånity in Molar unit. In\naddition, we only use the pocket of each protein and put\npocket-ligand pairs together as the input.\nFor QM9, we use the exact train/validation/test split\nas Townshend et al. (2020). For PDBbind, 90% of the data\nis used for training and the rest is divided equally between\nvalidation and test like Chen et al. (2019). For others, we\nadopt the scaffold splitting method with a ratio of 8:1:1 for\ntrain/validation/test as Rong et al. (2020). More implement-\ning details can be found in Appendix.\nBaselines For small molecules, baselines are as fol-\nlows. TF\nRobust (Ramsundar et al. 2015) takes molecu-\nlar Ô¨Ångerprints as the input. Weave (Kearnes et al. 2016),\nMPNN (Gilmer et al. 2017), Schnet (Sch ¬®utt et al. 2018),\nMEGNet (Chen et al. 2019), GROVER (Rong et al.\n2020), DMPNN (Yang et al. 2019), MGCN (Lu et al.\n2019), AttentiveFP (Xiong et al. 2019), DimeNet (Klicpera,\nGro√ü, and G ¬®unnemann 2020), DimeNet++ (Klicpera et al.\n2020), PaiNN (Sch ¬®utt, Unke, and Gastegger 2021), and\nSphereNet (Liu et al. 2021) are graph convolutional mod-\nels. Graph Transformer (Chen, Barzilay, and Jaakkola 2019),\nMAT (Maziarka et al. 2020), R-MAT (Maziarka et al.\n2021), SE(3)-Transformer (Fuchs et al. 2020), and LieTrans-\nformer (Hutchinson et al. 2021) are Transformer-based\nEquivariant Neural Networks (ENNs) (Thomas et al. 2018).\nFor PDBbind, we choose seven baselines. Deep-\nDTA ( ¬®Ozt¬®urk, ¬®Ozg¬®ur, and Ozkirimli 2018) and DeepAfÔ¨Ån-\nity (Karimi et al. 2019) take in pairs of ligand and protein\nSMILES as input. Cormorant (Anderson, Hy, and Kondor\n2019) is an ENN that represents each atom by its abso-\nlute 3D coordinates. HoloProt (Somnath, Bunne, and Krause\n2021) captures higher-level Ô¨Ångerprint motifs on the pro-\ntein surface. Schnet, 3DCNN and 3DGCN (Townshend et al.\n2020) are 3D methods.\n1For BBBP, ClinTox, and BACE, we use RDKit to procure 3D\ncoordinates from SMILES.\n5316\nMethod QM7 QM8 BBBP ClinT\nox BACE\nTF-Robust 120.6 0.024 0.860 0.765\n0.824\nWeave 94.7 0.022 0.837 0.823\n0.791\nMPNN 113.0 0.015 0.913 0.879\n0.815\nSchnet 74.2 0.020 0.847 0.717\n0.750\nDMPNN 105.8 0.014 0.919 0.897 0.852\nMGCN 77.6 0.022 0.850 0.634\n0.734\nAttentive FP 126.7 0.028 0.908 0.933 0.863\nGraph Transformer 47.8 0.010 0.913 - 0.880\nMAT 102.8 - 0.728 -\n0.846\nR-MAT 68.6 - 0.746 -\n0.871\nGROVERlarge 89.4 0.017 0.911 0.884\n0.858\nMolformer 43.2 0.009 0.926 0.937\n0.884\nTable 1: For regression tasks in QM7 and QM8, lower MAE\nis better. For classiÔ¨Åcation tasks in BBBP, ClinTox, and\nBace, higher values are better.\nOverall Results on Benchmark Tasks\nMolecules. Table 1 and Table 2 document the overall re-\nsults on small molecules datasets, where the best perfor-\nmance is marked bold and the second best is underlined\nfor clear comparison. It can be discovered that Molformer\nachieves the lowest MAE of 43.2 on QM7 and 0.009 on\nQM8, beating several strong baselines including DMPNN\nand Graph Transformer. Besides, Molformer offers compet-\nitive performance in all property regression tasks on QM9.\nParticularly, we outperform all Transformer-based ENNs,\nincluding SE(3)-Transformer and LieTransformer. As for\nclassiÔ¨Åcation problems, we surpass all baselines mostly by a\nfairly large margin.\nProteins. Table 3 reports the Root-Mean-Squared Devia-\ntion (RMSD), the Pearson correlation (Rp), and the Spear-\nman correlation (Rs) on PDBbind. Molformer achieves the\nlowest RMSD and the best Pearson and Spearman correla-\ntions. As Wu et al. (2018) claim, appropriate featurization\nwhich holds pertinent information is signiÔ¨Åcant for PDB-\nbind. However, an important observation in our work is that\ndeep learning approaches with the exploitation of 3D ge-\nometric information can perform better than conventional\nmethods like DeepDTA and DeepAfÔ¨Ånity which use a set\nof physicochemical descriptors but ignore 3D structures.\nAblation Study and Discussion\nWhat Is the Effect of Each Component? We investigate\nthe effectiveness of each component of our Molformer in\nTable 4. It can be observed that HSA along with HMGs\nsubstantially boosts the model‚Äôs performance compared with\nthe naive method that immediately adds 3D coordinates as\nthe atom input feature. MAE declines from 132.2 to 46.5\nin QM7 while decreasing from 0.0205 to 0.0097 in QM8. In\naddition, AFPS produces better predictions than the counter-\npart that utilizes the virtual node as the molecular representa-\ntion (a case study of AFPS is in the Appendix). We also dis-\ncover that the multi-scale mechanism signiÔ¨Åcantly reduces\nRMSD from 50.1 to 46.5 on QM7, but its improvements in\nQM8 are much smaller. This phenomenon indicates that the\nNo Motif +Hy. +Ha. +O. +N.\n40\n60\n80\n100\n120MAE\nQM7\nBBBP\n0.85\n0.86\n0.87\n0.88\n0.89\n0.90\n0.91\n0.92\n0.93\nROC\nFigure 3: The ablation study of consecutively adding motif\ngroups (from left to right) in the QM7 and BBBP datasets.\nmulti-scale mechanism is an appropriate way to alleviate the\nproblem of inadequate training in small datasets. It endows\nMolformer with the capability to extract local features by\nregulating the scope of self-attention. However, as the data\nsize gets larger, Molformer does not require the assistance\nof the multi-scale mechanism to abstract local patterns, since\nthe parameters of convolution operators are properly trained.\nWhat Is the Contribution of HMGs? The ideology of\nconstructing heterogeneous graphs has achieved success\nin not only chemical knowledge graphs but named entity\nrecognition (Li et al. 2020). The former views the chemi-\ncal characteristics obtained from domain knowledge of el-\nements as shared nodes, while the latter converts the lattice\nstructure into a Ô¨Çat structure consisting of spans. To verify its\nefÔ¨Åcacy, we compare our motif-based HMGs with the naive\nfusion of multi-level features. Table 5 shows a noticeable\nimprovement in our HMGs over the other two variants.\nHave We Found Good Candidates of Motifs? How\nto determine motifs is critical to HMGs. Concerning\nsmall molecules, we deÔ¨Åne motifs on the basis of func-\ntional groups, which refers to a substituent or moiety\nthat causes molecules‚Äô characteristic chemical reactions.\nTo further explore their contributions, we divide func-\ntional groups into four categories: Hydrocarbons (Hy.),\nHaloalkanes (Ha.), groups that contain oxygen (O.), and\ngroups that contain nitrogen (N.) and please refer to Ap-\npendix for more details. The ablation studies (see Fig-\nure 3) demonstrate that Molformer can gain improvements\nfrom all four groups of motifs, where Hydrocarbons and\nHaloalkanes are the most and the least effective types re-\nspectively. This is in line with the fact that Hydrocar-\nbons occur most frequently in organic molecules. More-\nover, the best performance is achieved when all categories\nare considered, implying a promising direction to dis-\ncover more effective motifs. As for proteins, motifs discov-\nered by our RL mining method share the same backbone\nas CC(C(NC(C)C(O)=O)=O)NC(CNC(CN)=O)=O (see\nAppendix), which is a hydrogen bond donor and implies a\nmark to distinguish potential binding site. Moreover, the por-\ntion of those motifs in the pocket (1:38%) is nearly twice\nthat in other locations (0:73%), conforming to the fact that\npockets are the most preferable part for ligands to bind with.\n5317\nTarget \u000fHOMO \u000fLUMO \u0001\u000f \u0016 \u000b R2 ZPVE U0 U\nH G c v\nUnit eV eV eV D\nbohr3 a2\n0 meV meV meV meV meV cal=mol K\nMPNN .043 .037 .069 .030\n.092 .150 1.27 45 45 39 44 .800\nSchnet .041 .034 .063 .033\n.235 .073 1.7 14 19 14\n14 .033\nMEGNet full .038 .031 .061 .040\n.083 .265 1.4 9 10 10 10 .030\nMGCN .042 .057 .064 .056 .030 .110 1.12 12.9\n14.4 14.6 16.2 .038\nDimeNet .027 .019 .034 .028\n.046 .331 1.29 8.02 7.89 8.11 8.98 .024\nDimeNet++ .024 .019 .032 .029 .043 .331 1.21 6.32 6.28 6.53 7.56 .023\nSphereNet .024 .019 .032 .026 .047 .292 1.12 6.26 7.33 6.40 8.0 .021\nPaiNN .028 .020 .046 .012 .045 .066 1.28 5.85 5.53\n5.98 7.35 .024\nSE(3)-Transformer .035 .033 .053 .051\n.142 ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì\nLieTransformer .033 .029 .052 .061\n.104 2.29 3.55 17 16 27 23 .041\nMolformer .025 .026 .039 .028\n.041 .350 2.05 7.52 7.46 7.38 8.11 .025\nTable 2: Comparison of MAE on QM9. The last three methods are Transformer-based methods.\nMethod Geometry RMSD Rp Rs\nDeepDTA Non-3D 1.565 0.573 0.574\nDeepAfÔ¨Ånity Non-3D 1.893 0.415 0.426\nSchnet 3D 1.892 0.601 -\nCormorant 3D 1.429 0.541 0.532\n3DCNN 3D 1.520 0.558 0.556\n3DGCN 3D 1.963 0.581 0.647\nHoloProt 3D 1.464 0.509 0.500\nMolformer 3D 1.386 0.623 0.651\nTable 3: Comparison of RMSD, Rp, and Rs on PDBbind.\nHSA AFPS HMG QM7 QM8 PDBbind\n1 - - - 132.2 0.0205 1.925\n2 X - X 46.5 0.0097 1.441\n3 X X X 43.2 0.0095 1.386\nTable 4: Effects of each module on QM7, QM8 and PDB-\nbind (RMSD).\nRelated Works\nMotifs in Molecular Graphs. Motifs have been proven\nto beneÔ¨Åt many tasks from exploratory analysis to trans-\nfer learning. Various algorithms have been proposed to ex-\nploit them for contrastive learning (Zhang et al. 2020), self-\nsupervised pretraining (Zhang et al. 2021), generation (Jin,\nBarzilay, and Jaakkola 2020), protein design (Li et al. 2022)\nand drug-drug interaction prediction (Huang et al. 2020).\nBut none of them take advantage of motifs to build a het-\nerogeneous graph for molecular property prediction. More-\nover, previous motif mining methods either depend on ex-\nact counting (Cantoni, Gatti, and Lombardi 2011) or sam-\npling and statistical estimation (Wernicke 2006) to extrat\nmotifs. No prior studies extract task-speciÔ¨Åc motifs to en-\nhance model performance.\nMolecular Representation Learning. Molecules are usu-\nally represented as 1D sequences, including amino acid se-\nQM7 QM8 PDBbind\nNo Motifs 132.2 0.0205 1.925\nMulti-Lev\nel Fusion 89.7 0.0154 1.427\nHeterogeneous Graphs 43.2 0.0095 1.386\nTable 5: Comparison of HMGs with simple feature fusion\non QM7, QM8 and PDBbind (RMSD).\nquences and SMILES (Xu et al. 2017), and 2D graphs (Du-\nvenaud et al. 2015). Despite that, more evidence indicates\nthat 3D structures lead to better modeling and superior\nperformance. 3D CNNs (Anand-Achim et al. 2021) and\nGNNs (Cho and Choi 2018) become popular to capture\nthese complex geometries. Nonetheless, the aforementioned\nmethods are inefÔ¨Åcient at grabbing local contextual feature\nand long-range dependencies.\nTransformer attempts to address that issue, assuming full\nconnection and using self-attention to capture long-term de-\npendencies. Some researchers feed SMILES in Transformer\nto obtain representations (Honda, Shi, and Ueda 2019) and\nconduct pretraining (Chithrananda, Grand, and Ramsun-\ndar 2020). Others employ Transformer to solve generative\ntasks (Ingraham et al. 2019) or fulÔ¨Åll equivariance (Fuchs\net al. 2020) via spherical harmonics. However,they are ei-\nther incapable to encode 3D geometry, non-sensitive to local\ncontextual patterns, or inefÔ¨Åcient to aggregate atom features.\nMore essentially, they are not specially designed to operate\non heterogeneous graphs of molecules.\nConclusion\nThis paper presents Molformer for 3D molecular representa-\ntion learning on heterogeneous molecular graphs. First, we\nextract motifs to formulate heterogeneous molecular graphs.\nAfter that, Molformer adopts a heterogeneous self-attention\nto distinguish the interactions between multi-level nodes.\nThen a simple but efÔ¨Åcient downsampling algorithm is intro-\nduced to better accumulate molecular representations. Ex-\nperiments demonstrate the superiority of Molformer in vari-\nous domains, which beats a group of baseline algorithms.\n5318\nReferences\nAnand-Achim, N.; Eguchi, R. R.; Mathews, I. I.; Perez,\nC. P.; Derry, A.; Altman, R. B.; and Huang, P. 2021. Protein\nsequence design with a learned potential. bioRxiv, 2020‚Äì01.\nAnderson, B.; Hy, T.-S.; and Kondor, R. 2019. Cormorant:\nCovariant molecular neural networks. arXiv preprint\narXiv:1906.04015.\nBlum, L. C.; and Reymond, J.-L. 2009. 970 million drug-\nlike small molecules for virtual screening in the chemical\nuniverse database GDB-13. Journal of the American Chem-\nical Society, 131(25): 8732‚Äì8733.\nCantoni, V .; Gatti, R.; and Lombardi, L. 2011. 3D Protein\nSurface Segmentation through Mathematical Morphology.\nIn International Joint Conference on Biomedical Engineer-\ning Systems and Technologies, 97‚Äì109. Springer.\nChen, B.; Barzilay, R.; and Jaakkola, T. 2019. Path-\naugmented graph transformer network. arXiv preprint\narXiv:1905.12712.\nChen, C.; Ye, W.; Zuo, Y .; Zheng, C.; and Ong, S. P. 2019.\nGraph networks as a universal machine learning framework\nfor molecules and crystals. Chemistry of Materials, 31(9):\n3564‚Äì3572.\nChithrananda, S.; Grand, G.; and Ramsundar, B. 2020.\nChemberta: Large-scale self-supervised pretraining\nfor molecular property prediction. arXiv preprint\narXiv:2010.09885.\nCho, H.; and Choi, I. S. 2018. Three-dimensionally em-\nbedded graph convolutional network (3dgcn) for molecule\ninterpretation. arXiv preprint arXiv:1811.09794.\nDuvenaud, D.; Maclaurin, D.; Aguilera-Iparraguirre, J.;\nG¬¥omez-Bombarelli, R.; Hirzel, T.; Aspuru-Guzik, A.;\nand Adams, R. P. 2015. Convolutional networks on\ngraphs for learning molecular Ô¨Ångerprints. arXiv preprint\narXiv:1509.09292.\nEismann, S.; Townshend, R. J.; Thomas, N.; Jagota, M.;\nJing, B.; and Dror, R. O. 2021. Hierarchical, rotation-\nequivariant neural networks to select structural models of\nprotein complexes. Proteins: Structure, Function, and\nBioinformatics, 89(5): 493‚Äì501.\nElnaggar, A.; Heinzinger, M.; Dallago, C.; Rihawi, G.;\nWang, Y .; Jones, L.; Gibbs, T.; Feher, T.; Angerer, C.;\nSteinegger, M.; et al. 2020. ProtTrans: towards cracking\nthe language of Life‚Äôs code through self-supervised deep\nlearning and high performance computing. arXiv preprint\narXiv:2007.06225.\nFuchs, F. B.; Worrall, D. E.; Fischer, V .; and Welling, M.\n2020. Se (3)-transformers: 3d roto-translation equivariant\nattention networks. arXiv preprint arXiv:2006.10503.\nGayvert, K. M.; Madhukar, N. S.; and Elemento, O. 2016. A\ndata-driven approach to predicting successes and failures of\nclinical trials. Cell chemical biology, 23(10): 1294‚Äì1301.\nGilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; and\nDahl, G. E. 2017. Neural message passing for quantum\nchemistry. In International conference on machine learn-\ning, 1263‚Äì1272. PMLR.\nGuo, Q.; Qiu, X.; Liu, P.; Xue, X.; and Zhang, Z. 2020.\nMulti-scale self-attention for text classiÔ¨Åcation. In Proceed-\nings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, vol-\nume 34, 7847‚Äì7854.\nHonda, S.; Shi, S.; and Ueda, H. R. 2019. Smiles trans-\nformer: Pre-trained molecular Ô¨Ångerprint for low data drug\ndiscovery. arXiv preprint arXiv:1911.04738.\nHuang, K.; Xiao, C.; Hoang, T.; Glass, L.; and Sun, J. 2020.\nCaster: Predicting drug interactions with chemical substruc-\nture representation. In Proceedings of the AAAI Conference\non ArtiÔ¨Åcial Intelligence, volume 34, 702‚Äì709.\nHutchinson, M. J.; Le Lan, C.; Zaidi, S.; Dupont, E.; Teh,\nY . W.; and Kim, H. 2021. Lietransformer: Equivariant self-\nattention for lie groups. In International Conference on Ma-\nchine Learning, 4533‚Äì4543. PMLR.\nIngraham, J.; Garg, V . K.; Barzilay, R.; and Jaakkola, T.\n2019. Generative models for graph-based protein design.\nAdvances in neural information processing systems.\nIshida, S.; Miyazaki, T.; Sugaya, Y .; and Omachi, S. 2021.\nGraph Neural Networks with Multiple Feature Extraction\nPaths for Chemical Property Estimation. Molecules, 26(11):\n3125.\nJin, W.; Barzilay, R.; and Jaakkola, T. 2020. Hierarchi-\ncal generation of molecular graphs using structural motifs.\nIn International Conference on Machine Learning, 4839‚Äì\n4848. PMLR.\nJumper, J.; Evans, R.; Pritzel, A.; Green, T.; Figurnov, M.;\nRonneberger, O.; Tunyasuvunakool, K.; Bates, R.; ÀáZ¬¥ƒ±dek,\nA.; Potapenko, A.; et al. 2021. Highly accurate protein struc-\nture prediction with AlphaFold. Nature, 596(7873): 583‚Äì\n589.\nKarimi, M.; Wu, D.; Wang, Z.; and Shen, Y . 2019. Deep-\nAfÔ¨Ånity: interpretable deep learning of compound‚Äìprotein\nafÔ¨Ånity through uniÔ¨Åed recurrent and convolutional neural\nnetworks. Bioinformatics, 35(18): 3329‚Äì3338.\nKearnes, S.; McCloskey, K.; Berndl, M.; Pande, V .; and Ri-\nley, P. 2016. Molecular graph convolutions: moving beyond\nÔ¨Ångerprints. Journal of computer-aided molecular design,\n30(8): 595‚Äì608.\nKlicpera, J.; Giri, S.; Margraf, J. T.; and G ¬®unnemann,\nS. 2020. Fast and uncertainty-aware directional message\npassing for non-equilibrium molecules. arXiv preprint\narXiv:2011.14115.\nKlicpera, J.; Gro√ü, J.; and G ¬®unnemann, S. 2020. Direc-\ntional message passing for molecular graphs. arXiv preprint\narXiv:2003.03123.\nLi, A. J.; Sundar, V .; Grigoryan, G.; and Keating, A. E. 2022.\nTERMinator: a neural framework for structure-based pro-\ntein design using tertiary repeating motifs. arXiv preprint\narXiv:2204.13048.\nLi, X.; Yan, H.; Qiu, X.; and Huang, X. 2020. FLAT:\nChinese NER using Ô¨Çat-lattice transformer. arXiv preprint\narXiv:2004.11795.\nLiu, Y .; Wang, L.; Liu, M.; Zhang, X.; Oztekin, B.; and Ji,\nS. 2021. Spherical message passing for 3d graph networks.\narXiv preprint arXiv:2102.05013.\n5319\nLu, C.; Liu, Q.; Wang, C.; Huang, Z.; Lin, P.; and He, L.\n2019. Molecular property prediction: A multilevel quan-\ntum interactions modeling perspective. In Proceedings of\nthe AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33,\n1052‚Äì1060.\nMackenzie, C. O.; Zhou, J.; and Grigoryan, G. 2016. Ter-\ntiary alphabet for the observable protein structural universe.\nProceedings of the National Academy of Sciences, 113(47):\nE7438‚ÄìE7447.\nMartins, I. F.; Teixeira, A. L.; Pinheiro, L.; and Falcao, A. O.\n2012. A Bayesian approach to in silico blood-brain barrier\npenetration modeling. Journal of chemical information and\nmodeling, 52(6): 1686‚Äì1697.\nMaziarka, ≈Å.; Danel, T.; Mucha, S.; Rataj, K.; Tabor, J.; and\nJastrzebski, S. 2020. Molecule attention transformer. arXiv\npreprint arXiv:2002.08264.\nMaziarka, ≈Å.; Majchrowski, D.; Danel, T.; Gai ¬¥nski, P.;\nTabor, J.; Podolak, I.; Morkisz, P.; and Jastrzebski, S.\n2021. Relative Molecule Self-Attention Transformer. arXiv\npreprint arXiv:2110.05841.\n¬®Ozt¬®urk, H.; ¬®Ozg¬®ur, A.; and Ozkirimli, E. 2018. DeepDTA:\ndeep drug‚Äìtarget binding afÔ¨Ånity prediction.Bioinformatics,\n34(17): i821‚Äìi829.\nRamakrishnan, R.; Dral, P. O.; Rupp, M.; and von Lilienfeld,\nO. A. 2014. Quantum chemistry structures and properties of\n134 kilo molecules. ScientiÔ¨Åc Data, 1.\nRamakrishnan, R.; Hartmann, M.; Tapavicza, E.; and\nV on Lilienfeld, O. A. 2015. Electronic spectra from TDDFT\nand machine learning in chemical space. The Journal of\nchemical physics, 143(8): 084111.\nRamsundar, B.; Kearnes, S.; Riley, P.; Webster, D.; Konerd-\ning, D.; and Pande, V . 2015. Massively multitask networks\nfor drug discovery. arXiv preprint arXiv:1502.02072.\nRong, Y .; Bian, Y .; Xu, T.; Xie, W.; Wei, Y .; Huang, W.; and\nHuang, J. 2020. Self-supervised graph transformer on large-\nscale molecular data. arXiv preprint arXiv:2007.02835.\nSch¬®utt, K. T.; Sauceda, H. E.; Kindermans, P.-J.;\nTkatchenko, A.; and M ¬®uller, K.-R. 2018. Schnet‚Äìa\ndeep learning architecture for molecules and materials. The\nJournal of Chemical Physics, 148(24): 241722.\nSch¬®utt, K. T.; Unke, O. T.; and Gastegger, M. 2021.\nEquivariant message passing for the prediction of ten-\nsorial properties and molecular spectra. arXiv preprint\narXiv:2102.03150.\nSomnath, V . R.; Bunne, C.; and Krause, A. 2021. Multi-\nScale Representation Learning on Proteins. In Thirty-Fifth\nConference on Neural Information Processing Systems.\nSubramanian, G.; Ramsundar, B.; Pande, V .; and Denny,\nR. A. 2016. Computational modeling of \f-secretase 1\n(BACE-1) inhibitors using ligand based approaches.Journal\nof chemical information and modeling, 56(10): 1936‚Äì1949.\nThomas, N.; Smidt, T.; Kearnes, S.; Yang, L.; Li, L.;\nKohlhoff, K.; and Riley, P. 2018. Tensor Ô¨Åeld networks:\nRotation-and translation-equivariant neural networks for 3d\npoint clouds. arXiv preprint arXiv:1802.08219.\nTownshend, R. J.; V ¬®ogele, M.; Suriana, P.; Derry, A.;\nPowers, A.; Laloudakis, Y .; Balachandar, S.; Anderson,\nB.; Eismann, S.; Kondor, R.; et al. 2020. ATOM3D:\nTasks On Molecules in Three Dimensions. arXiv preprint\narXiv:2012.04035.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nVinyals, O.; Bengio, S.; and Kudlur, M. 2015. Order\nmatters: Sequence to sequence for sets. arXiv preprint\narXiv:1511.06391.\nWang, R.; Fang, X.; Lu, Y .; Yang, C.-Y .; and Wang, S. 2005.\nThe PDBbind database: methodologies and updates.Journal\nof medicinal chemistry, 48(12): 4111‚Äì4119.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 7794‚Äì\n7803.\nWernicke, S. 2006. EfÔ¨Åcient detection of network mo-\ntifs. IEEE/ACM transactions on computational biology and\nbioinformatics, 3(4): 347‚Äì359.\nWu, Z.; Liu, Z.; Lin, J.; Lin, Y .; and Han, S. 2020. Lite\ntransformer with long-short range attention. arXiv preprint\narXiv:2004.11886.\nWu, Z.; Ramsundar, B.; Feinberg, E. N.; Gomes, J.; Ge-\nniesse, C.; Pappu, A. S.; Leswing, K.; and Pande, V . 2018.\nMoleculeNet: a benchmark for molecular machine learning.\nChemical science, 9(2): 513‚Äì530.\nXiong, Z.; Wang, D.; Liu, X.; Zhong, F.; Wan, X.; Li, X.;\nLi, Z.; Luo, X.; Chen, K.; Jiang, H.; et al. 2019. Pushing the\nboundaries of molecular representation for drug discovery\nwith the graph attention mechanism. Journal of medicinal\nchemistry, 63(16): 8749‚Äì8760.\nXu, Z.; Wang, S.; Zhu, F.; and Huang, J. 2017. Seq2seq\nÔ¨Ångerprint: An unsupervised deep molecular embedding for\ndrug discovery. In Proceedings of the 8th ACM interna-\ntional conference on bioinformatics, computational biology,\nand health informatics, 285‚Äì294.\nYang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao,\nH.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, M.;\net al. 2019. Analyzing learned molecular representations for\nproperty prediction. Journal of chemical information and\nmodeling, 59(8): 3370‚Äì3388.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.;\nShen, Y .; and Liu, T.-Y . 2021. Do Transformers Really\nPerform Bad for Graph Representation? arXiv preprint\narXiv:2106.05234.\nZhang, S.; Hu, Z.; Subramonian, A.; and Sun, Y . 2020.\nMotif-driven contrastive learning of graph representations.\narXiv preprint arXiv:2012.12533.\nZhang, Z.; Liu, Q.; Wang, H.; Lu, C.; and Lee, C.-K. 2021.\nMotif-based Graph Self-Supervised Learning for Molecular\nProperty Prediction. arXiv preprint arXiv:2110.00987.\n5320",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6826481223106384
    },
    {
      "name": "Motif (music)",
      "score": 0.6307764053344727
    },
    {
      "name": "Exploit",
      "score": 0.5453472137451172
    },
    {
      "name": "Transformer",
      "score": 0.47706806659698486
    },
    {
      "name": "Theoretical computer science",
      "score": 0.45785626769065857
    },
    {
      "name": "Homogeneous",
      "score": 0.4435258209705353
    },
    {
      "name": "Molecular graph",
      "score": 0.4137105345726013
    },
    {
      "name": "Graph",
      "score": 0.39542528986930847
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35162800550460815
    },
    {
      "name": "Mathematics",
      "score": 0.11979907751083374
    },
    {
      "name": "Physics",
      "score": 0.09471362829208374
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3133055985",
      "name": "Westlake University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ],
  "cited_by": 36
}